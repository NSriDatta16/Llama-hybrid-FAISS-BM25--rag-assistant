[site]: datascience
[post_id]: 6349
[parent_id]: 
[tags]: 
What should I care about while stacking as an ensemble method?

I'm using SMO, Logistic Regression, Bayesian Network and Simple CART algorithms for classification. Results form WEKA: Algorithm Sensitivity (%) Specificity (%) Overall accuracy (%) Bayesian Network 57.49 76.09 65.24 Logistic Regression 64.73 69.86 66.87 SMO 54.32 79.20 64.69 Simple CART 71.88 61.51 67.56 SMO gives the best result for my classification problem, since it correctly classify the 79.20% of the class which is important for me. I want to increase this accuracy by stacking. I tried to combine some of them. In most of the cases I couldn't increase the accuracy but stacking SMO with Logistic Regression made a little increment in accuracy. How can I explain why stacking SMO with Logistic Regression is better than others? Is there any generalization such as combining tree classifiers gives good result in stacking? What should I care about while stacking? EDIT: Bayesian Network Logistic Reg. SMO CART Kappa statistic 0.3196 0.3367 0.3158 0.3335 Mean absolute error 0.3517 0.4164 0.3531 0.4107 Root mean squared error 0.5488 0.4548 0.5942 0.4547 Relative absolute error (%) 72.3389 85.65 72.6299 84.477 Root relative squared error (%) 111.3076 92.2452 120.5239 92.2318 Weighted Avg. of F-Measure 0.653 0.671 0.676 92.2318 ROC Area 0.725 0.727 0.668 0.721 Total number of instance is 25106. 14641 of them is class a, and 10465 of them belong to class b. === Confusion Matrix of Simple CART === a b Since SMO is successful at class b and CART is successful at class a, I tried to ensemble these two algorithms. But I couldn't increase the accuracy. Then I tried to combine SMO with Logistic Regression, the accuracy is increased a little bit. Why ensembling SMO with Logistic Regression is better than ensebling SMO with CART, is there any explanation?
