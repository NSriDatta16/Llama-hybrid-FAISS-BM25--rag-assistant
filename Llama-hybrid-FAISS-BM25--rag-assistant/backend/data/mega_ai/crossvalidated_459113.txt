[site]: crossvalidated
[post_id]: 459113
[parent_id]: 
[tags]: 
Asymptotics of Marginal Likelihood

I'm working with Bayes factors, and I want to develop some intuition for the result $$ \frac{m_1(\mathbf{X})}{p_n(\mathbf{X}|\hat\theta_n)}\xrightarrow{p}\frac{\pi_1(\theta_0)\sqrt{2\pi}}{\sqrt{\mathbf{I}(\theta_0)}}, $$ where $\mathbf{X}$ is a vector of iid random variables; $m_1(\mathbf{X})$ is the marginal likelihood defined as $m_1(\mathbf{X}):=\int_\Theta p_n(\mathbf{X}|\theta)\pi_1(\theta)d\theta$ ; $\theta_0$ is taken as the true value of $\theta$ ; $\hat\theta_n$ is the MLE, and $\mathbf{I}$ is the Fisher Information. I'd like to understand this so I can make claims about how the marginal distribution behaves as $n$ grows. I've seen Walker's 1969 paper cited. Unfortunately, the paper is very technical, and the result I care about appears to only be briefly mentioned at the end. The main theorem of the paper is a Bernstein-von Mises result showing the asymptotic normality of the posterior under suitable regularity conditions. In most Bayesian textbooks, I've seen the asymptotic distribution of the posterior justified heuristically through a Taylor expansion. Does a similar intuition exist for the claim about the marginal likelihood?
