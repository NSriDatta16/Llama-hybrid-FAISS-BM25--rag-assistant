[site]: crossvalidated
[post_id]: 618317
[parent_id]: 
[tags]: 
In the context of machine learning, at what stage is the normalization process implemented?

I have several data types and I want to use them as features for binary classification task. The data is as follows: 1- Genes: I have several datasets containing bulk RNA-seq data. Some of these datasets have already undergone normalization through TPM (Transcripts Per Million) normalization, while others are still in their raw form. Additionally, a few datasets have been normalized using unconventional methods. I plan to merge all these datasets into a single dataset and then select only 20 features from the pool of 11,000. Since the data has been sourced from different places, and because some datasets have undergone different normalization processes, I intend to perform upper quantile normalization to bring all samples to the same scale. My question is: Should I perform this normalization before splitting the data into training and testing sets? Alternatively, should I first split the data into training and testing sets and then perform the upper quantile normalization separately on each set? Another thought is to perform the upper quantile normalization for each data set separatly (there are 30 data set approximately) and only then I combine them and split the data to training set and testing set. There are just too many options I don't know which one is statistically correct. Morover, should I apply batch effect correction (cause of different data sets as source)? 2- Deconvolution: I am utilizing the dataset of genes I mentioned (with 11,000 genes) to execute a deconvolution algorithm, which will yield scores for each sample. I am uncertain about the ideal timing for running the deconvolution process. Should I conduct this process before or after dividing the data? Alternatively, is it more effective to run the algorithm on each dataset separately, then merge them into a single dataset, and finally split it into different subsets?
