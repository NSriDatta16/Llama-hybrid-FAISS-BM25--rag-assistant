[site]: crossvalidated
[post_id]: 93020
[parent_id]: 
[tags]: 
Why features compression is good?

I'm reading about deep learning and that in principles it's a features compression technique and that is why it works. Now my question is why compressing features from 200 or so into 4 is better? How and why does it influence classifiers performance or the construction of the decision boundary? Look at the following picture for example: Assume that blue points are class-0 and red points are class-1. A classifier might see that any instance that has an $x_2$ lower than $0.5$ is of class-0 and any instance that has $x_2$ above $0.5$ is class-1. At this time $x_1$ is redundant. But now look at this example: In this example $x_1$ becomes vital for a good performance. So why features compression is better? Or could (if not always better) lead to a better performance?
