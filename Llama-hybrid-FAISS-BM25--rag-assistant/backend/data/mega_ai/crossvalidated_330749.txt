[site]: crossvalidated
[post_id]: 330749
[parent_id]: 330686
[tags]: 
As @Flom mentioned, if a feature (dimension) is not contributing to the results, perhaps this is what you need to pay attention to instead of forcing it to have a noticeable affect on the results. Not all features play a significant role in every single analysis. This could be a clue from reducing the dimension of your dataset, or looking for other features. But what make sense to me is applying PCA , to find a new space where you can look at your data from a better angle or in other words, to find better attributes. If the first column (NIR) has a very low variance, it may have a much larger variance (hence, information) if looked at from a different angle. I am trying to elaborate on the idea of Principal Components (PC) here. For example, in the scatter plot below, for a 2 dimensional dataset, you see that the original X and Y axes would let us look at our data from 2 orthogonal perspectives with a certain variance for each of them. But if we find the eigen-vectors of our data matrix, we can find new axes (a new space with up to 2 dimensions). Now $X=PC1$ and $Y = PC2$. In the new space, variance of the components are in total maximized. This is a classic dimensionality reduction task. But what is important to note before you decide whether you want to use it, is that your space will be transformed. Meaning that, each of the new dimensions are a combination of the old dimensions and perhaps, not perfectly interpretable. Unless you find a way to transform the results back to the original space. Plot from a useful post on StatistiXL.
