[site]: datascience
[post_id]: 57019
[parent_id]: 57000
[tags]: 
Let's take an example : "I went to the shop." Let's say you want to predict "to" and "the". With bidirectionality, you will predict : p(to | I, went, the, shop) : No problem here. p(the | I, went, to, shop) : Here we have a problem, because we already saw the word 'the' while predicting 'to'. It's trivial for the model to predict 'the'. With MLM, if we take the same example : p(to | I, went, [MASK], [MASK], shop) p(the | I, went, [MASK], [MASK], shop) There is no more problem because the word cannot see itself in other predictions. It's more difficult to predict, but anyway only 15% of the time the word is masked. Edit In MLM, BERT will see all the words of the sentence, including the word to predict itself. This is because BERT will create representations not only for the [MASK] token, but also for the other tokens of the sentence (but in this case we are interested only in [MASK]). Note : It is also because BERT will create representations of all tokens that we need to replace [MASK] sometimes by the original token, sometimes by a random token. You can find more detailed information in this blog in the "Pretraining" section.
