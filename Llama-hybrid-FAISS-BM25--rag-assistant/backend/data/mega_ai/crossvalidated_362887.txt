[site]: crossvalidated
[post_id]: 362887
[parent_id]: 362883
[tags]: 
When training neural networks, backpropagation requires the computation of many gradients and it can be computationnally heavy. In order to minimize that load, the weights are only updated (i.e. the backpropagation is done) after a certain amount of samples, that's called batch (or mini-batch, if number of samples is low) gradient descent. So, the loss function is of course computed after each training example and summed up for all the samples in the batch, then backpropagation is applied on the overall loss of the batch. You can also update the weights after each sample, this is called stochastic gradient descent
