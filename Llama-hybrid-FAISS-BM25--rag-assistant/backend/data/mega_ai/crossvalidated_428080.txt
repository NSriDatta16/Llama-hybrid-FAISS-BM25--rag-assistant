[site]: crossvalidated
[post_id]: 428080
[parent_id]: 
[tags]: 
How to use data from other policies in order to find optimal policy in model-free rl?

I am struggling to understand whether experience from one policy can be used to find optimal policy. Suppose that I have gathered many data (state, action, reward, next_state) by following random policy. How can I take advantage of those data in order to find the optimal policy in a model-free off-policy setting? For example in DYNA-Q algorithm after applying the Q-Learning update we save the experience by following for example e-greedy policy and then take random samples of the history in order to update the Q-values. If I already have many many data can I use them in same way? Can I compute state transition probabilities and rewards and apply dynamic programming to find optimal q-values in order to have better initial q-values and then apply Q-Learning? In the following article the author states that the reason that q-learning and its variants work consistently is because they are not pure off-policy algorithms due to the fact that the behavioral policy (e-greedy) is correlated with the target policy (greedy). In my case the data gathered may be uncorrelated with the target/optimal policy so I am not sure I can use them. https://towardsdatascience.com/the-false-promise-of-off-policy-reinforcement-learning-algorithms-c56db1b4c79a
