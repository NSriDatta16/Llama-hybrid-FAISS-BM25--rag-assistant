[site]: crossvalidated
[post_id]: 39025
[parent_id]: 39024
[tags]: 
The problem with dimensionality reduction and number of variables >> number of observations is that the $k$ observations that you have define an at most $k-1$ dimensional hyperplane on which the objects perfectly are located on. So yes, anything more than 9 dimensions still has proven redundancies. Many dimension reduction techniques - in particular PCA, SVD, but probably also MDS etc. - will essentially try to preserve this hyperplane. Don't you have a way to reduce the number of dimensions that uses domain information that you have ? I.e. if you know that your dimensions are expected to be highly correlated, remove dimensions that are the most correlated (pairwise is probably best). But note that even correlation is not very stable to compute when you have just 10 observations. You lose one degree of freedom for the mean, for example, which you can't really afford.
