[site]: datascience
[post_id]: 123832
[parent_id]: 
[tags]: 
Using text embeddings directly to compute similarity vs using them as features for a model that predicts similariy

Say you have a problem where you have a query and a set of result documents and you want to rank the result documents according to the query. Say also you have embeddings for the query and for the documents. Would it be better to A) rank the documents according to the similarity of their embeddings or B) train a model that takes the query embedding and the document embedding as inputs and predicts relevance then rank by that prediction?
