[site]: crossvalidated
[post_id]: 530600
[parent_id]: 530588
[tags]: 
When finding an extremal point of a function $f$ (let's say in one dimension and we look at all of $\mathbb{R}$ ) then we know from 10th grade that if $f$ has an extremal point (i.e. maximum or minimum) at some $x_0$ then $f'(x_0)=0$ . When we only allow $x \in [a,b]$ then this is not necessarily true: in $(a,b)$ we can apply the same logic but it could also be that $f(a)$ or $f(b)$ is an extremal point without having $f'(a)=0$ and/or $f'(b)=0$ simply because we cannot compute neither $f'(a)$ nor $f'(b)$ . So in one dimension we just need to do the regular thing (finding point where $f'=0$ in the interior) and then comparing to $f(a)$ and $f(b)$ . In more dimensions the situation is not that simple because the 'boundary' of sets can look really ugly. However, there is hope because of the theorem of Lagrange (see https://en.wikipedia.org/wiki/Lagrange_multiplier ). There is a special instance in the context of optimization (see https://en.wikipedia.org/wiki/Duality_(optimization) ). Applying the theorem of Lagrange in optimization leads to a different formulation (which nevertheless is as expressive as the original one in the sense that if we solve one then we also solve the other) of the problem called dual problem. In SVM it turns out that this dual problem can be written down with only finitely many features (see https://en.wikipedia.org/wiki/Support-vector_machine#Dual ). It's quite involved actually: we know the weights using this dual problem and then we can also write down the prediction function and it only uses the weights, not the vector $w$ itself! That's why it's called a trick :-)
