[site]: crossvalidated
[post_id]: 578249
[parent_id]: 264717
[tags]: 
Q1: Why the first line of V(G,D) can be changed to second line of V(G,D) The change of random variable is a standard result from probability theory (see e.g. Papoulis' book Probability, Random Variables, and Stochastic Processes ). In the 1-D case, for some arbitrary function $f(z)$ the expectation is given by: ${\rm E}_z[f(z)] = \int_{-\infty}^{\infty} f(z){\rm p}_z(z) dz$ if we apply an invertible change of variable by letting $x=g(z)$ where $x$ is a scalar random variable, where the PDFs satisfy ${\rm p}_x(x) dx={\rm p}_z(z) dz$ , we obtain: ${\rm E}_z[f(z)]=\int_{-\infty}^{\infty} f(g^{-1}(x)) {\rm p}_x(x) dx={\rm E}_x[f(g^{-1}(x))]$ The result generalises to (i) many-to-one scalar transformations (ii) invertible vector transformations $G(\cdot)$ with ${\rm dim}(x)={\rm dim}(z)$ ; and (iii) vector transformations where ${\rm dim}(x) . In case (i), the PDF is composed of a sum of terms each of which corresponds to a root of the equation $g(z)=0$ . In case (ii), the two PDFs are linked via the determinant of the Jacobian. In the case where ${\rm dim}(x)>{\rm dim}(z)$ , the PDF ${\rm p}_x(x)$ is non-unique and degenerate (it contains contains delta functions). This situation is illustrated by Example 2.1 in the paper (IEEE Access, Dec. 2021) https://www.researchgate.net/publication/356815736_Convergence_and_Optimality_Analysis_of_Low-Dimensional_Generative_Adversarial_Networks_using_Error_Function_Integrals Q2: In my own trial to change the V(G,D), the above condition was needed. Is it appropriate condition?! You need to pay attention to the dimensions of $x$ and $z$ . For a scalar change of variables $x=g(z)$ , clearly $dx/dz=g'(z)$ so ${\rm p}_x(x)={\rm p}_z(z) dz/dx$ , which is the same as your equation. This requires $x$ and $z$ to be scalar random variables . For vectors of the same dimension, you can use the Jacobian. However, as explained above, when ${\rm dim}(x)>{\rm dim}(z)$ , the PDF of $x$ is degenerate. Although the expectation result still holds, the method used to obtain the optimal discriminator does not. This is because the latter uses calculus of variations, which requires continuously differentiable integrands. The integrand as a function of $x$ and $D$ is not continuously differentiable when ${\rm dim}(x)>{\rm dim}(z)$ , so the optimal discriminator does not exist in this case. This case is actually the one of practical interest and the counter-examples provided in the reference (based on the arguments in this post) invalidate the theoretical results in Goodfellow et al's 2014 GAN paper where they are based on Proposition 1 [Optimal Discriminator]. This is not to say the algorithms don't work: they clearly do in many cases and are extremely useful for machine learning; however the theory does not stand up to scrutiny when the data dimension ${\rm dim}(x)$ exceeds the latent variable dimension ${\rm dim}(z)$ . These points are explained further in the paper "Convergence and Optimality Analysis of Low-Dimensional Generative Adversarial Networks using Error Function Integrals" for which the link was given above. A practical demonstration appeared in a paper by Qin et al. (NIPS 2020) "Training Generative Adversarial Networks by Solving Ordinary Differential Equations." They showed on CIFAR-10 with ${\rm dim}(x)=3072>{\rm dim}(z)=256$ that neither the discriminator nor the generator losses converged to the predicted "Nash equilibrium" values, whereas the Nash equilibrium values were obtained on a ${\rm dim}(x)=2\leq {\rm dim}(z)=32$ Gaussian mixture simulation.
