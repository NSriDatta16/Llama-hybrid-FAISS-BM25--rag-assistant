[site]: crossvalidated
[post_id]: 563912
[parent_id]: 
[tags]: 
Are consistently negative Efron's pseudo-r2 in logistic regression possible?

I am conducting logistic regression and looking to calculate pseudo-R2 values alongside AIC and BIC for model evaluation. I selected Efron's pseudo-R2 because of its simple calculation and the similarity to a proper R2 value. When I run a series of logistic regressions, it produces a negative value. However, it is commonly stated that pseudo-R2 values fall between 0 and 1 ( example here ). Am I calculating something wrong, or is this 0 to 1 range false for Efron's pseudo-R2? The equation I am using for Efron's pseudo-R2 is: $ R^2 = 1 - \frac{\sum(y_i-\pi_i)^2}{\sum(y_i-\bar y)^2} $ Where: $ y $ is an array of 1s and 0s, representing the true outcome labels in the data $ \pi $ is an array of 1s and 0s, representing the predicted outcome labels as a result of the logistic regressions $ \bar y $ is the arithmetic mean of $ y $ , calculated as $ \frac{\sum y_i}{n} $ and equivalent to $ p $ or the probability of a 1 outcome This seems to be the definition consistently given online. (I don't have access to the foundational journal article .) Looking at the equation, this appears to be happening for three reasons: (1) because of large sample size (n = 4,000), (2) relative inaccuracy of the logistic regression, and (3) the fact that the value for p (mean y = ) is close to 0.5. The large sample size combined with frequent error blow up the numerator and having p close to 0.5 shrinks the denominator. Indeed, when I calculate the value on a subset of my data (about the first 100 rows), I receive a positive pseudo-R2. Again, though, pseudo-R2 are discussed as being consistently between 0 and 1. It seems there are two possibilities: (1) this range is a simplification and it is the case that a bad model can be negative (as with a conventional R2) or (2) there is something wrong with my understanding. More information on implementation, although I believe my implementation is correctly reflecting the equation above: I am using the python sklearn implementation, which has limited metrics for inference, so I am writing functions to calculate pseudo-R2 values. I wrote the function based on the formula above and confirmed another function from this site returns the same values, leading me to believe this is a characteristic of the metric and not of an incorrect implementation of the equation.
