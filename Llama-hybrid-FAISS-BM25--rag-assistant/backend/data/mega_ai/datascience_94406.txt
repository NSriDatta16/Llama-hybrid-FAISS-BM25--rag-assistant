[site]: datascience
[post_id]: 94406
[parent_id]: 
[tags]: 
Transformer: where is the output of the last FF sub-layer of the encoder used?

In the "Attention Is All You Need" paper, the decoder consists of two attention sub-layers in each layer followed by a FF sub-layer. The first is a masked self attention which gets as an input the output of the decoder in the previous step (and the first input is a special start token). The second, 'encoder-decoder', attention sub-layer gets as an input queries from the lower self-attention sub-layer and keys & values from the encoder. I do not see the use of the output of the FF sub-layer in the encoder; can someone explain where is it used? Thanks
