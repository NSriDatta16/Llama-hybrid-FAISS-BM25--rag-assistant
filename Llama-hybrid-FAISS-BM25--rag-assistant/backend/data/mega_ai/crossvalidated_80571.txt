[site]: crossvalidated
[post_id]: 80571
[parent_id]: 
[tags]: 
Quality of a model and the bias-variance tradeoff

Take linear regression as the example, given one specific data set $D_1=\{(x_1,y_1),...(x_n,y_n)\}$, we could train a model with one specific parameter estimate $\hat\theta_1$, if we do the training on a new data set $D_2$, we will have new estimate $\hat\theta_2$, and so on. For input data $x$, I could predict the target value $\hat y$. My problems are, I thought the term bias and variance are only used when talking about the parameter estimate $\hat\theta$ of model, not used to describe the random variable of the predicted data, which are $\hat y$, right? So we have $bias(\hat\theta),var(\hat\theta)$, but don't have $bias(\hat y)$ or $var(\hat y)$. If I'm wrong, then what are bias and variance here? To judge the quality of the one specific model with parameter estimate $\hat\theta_1$, I check the mean squared error over all the training data points of $D_1$, $$MSE_1=\frac{1}{n}\sum_i(y_i-\hat y_i)^2$$, but for different training data sets $D_i$, we have different $\hat\theta_i$, then different $MSE_i$, so how do I determine what $\hat\theta$ should be? The one with the smallest $MSE$? As I read the bias-variance tradeoff , it's said that the expectation of the $MSE$ is of special interest , why should we pay special attention to the $E[MSE]$? I mean I could also pay attention to $E[\hat y_i]$ for each data point. Moreover, $MSE$ is calculated on the observed target value $y_i$ and its prediction $\hat y_i$, so I thought it has to with the $bias(\hat y_i)$ and $var(\hat y_i)$, but not the bias or variance of $\hat\theta$? I totally got lost when trying to figure out what the terms bias and variance are aimed at. I hope anyone of you can help me out.
