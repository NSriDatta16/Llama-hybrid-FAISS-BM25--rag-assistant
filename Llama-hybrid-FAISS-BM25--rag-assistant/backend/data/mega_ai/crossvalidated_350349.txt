[site]: crossvalidated
[post_id]: 350349
[parent_id]: 
[tags]: 
Is there any point in using MSE loss in modern deep neural networks?

Is there any point in using MSE loss -- (a-b)^2 instead of L1 loss -- abs(a-b) in modern DNN/CNN architectures which use ReLU/ReLU-like activations? If so, why?
