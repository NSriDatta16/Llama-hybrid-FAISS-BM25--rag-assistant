[site]: crossvalidated
[post_id]: 572875
[parent_id]: 487182
[tags]: 
There's a problem here: Testing the difference in variances when ? The time series you've shown are clearly non-stationary. What this means is that there is a pattern over time. More formally, a time series is nonstationary if the distribution of the variables is changing over time. Letting $X_i$ be the $i$ th observation: $$ P(X_i \le x) \ne P(X_{j} \le x) $$ One common example of nonstationary time series is time series with seasonal trends. The data you've shown clearly exhibit seasonal trends, i.e. the mean changes over time. Now we see the problem: We can't take the variance for these time series, because for the variance to exist, the mean has to exist, since we define the variance as $E((X - \mu)^2)$ ). However, asking what "The" mean of this time series is doesn't make sense, because the mean is a moving target. The mean in January is clearly different from the mean in May. Luckily, the time series looks like it's stationary after removing seasonal trends. I'd suggest the following procedure: Consider your null hypothesis carefully -- what do you really want to test? Do you want to test that, after removing seasonal trends, the time series have the same variance? Do you want to test whether the seasonal component for one model is bigger than for the other? Build a null model that makes the assumptions you want to test. This model will likely take the form of a SARIMA model. Choose some test statistic. A good test statistic might be something like the sample variance after detrending your data. Simulate the distribution of your test statistic under your null model.
