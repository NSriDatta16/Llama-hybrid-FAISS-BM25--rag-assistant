[site]: crossvalidated
[post_id]: 370277
[parent_id]: 
[tags]: 
The ways to normalize the likelihood in EM algorithm

In Wikepedia it states that: In the simplest cases, normalization of ratings means adjusting values measured on different scales to a notionally common scale, often prior to averaging. And the likelihood funciton: $\mathcal{L}(\theta; x_1,...,x_n) = f(x_1,...,x_n; \theta) = \prod_{j} f(x_j; \theta)$ As asked in this answer and this question , likelihood function is a function of the parameter only, with the data held as a fixed constantfunction and is not a pdf. And in this tutorial of EM , he normalize the pair of likelihoods of a binomial distribution into a probability distribution simply by mean. I thought the two probabilities can be viewed as how likely the sample is from A or from B. I wonder if there are other methods to normalize the likelihoods to scale them into numbers between 0 and 1. And what these two normalized likelihood stands for. Am I right that they represent how likely the sample is generated by coin A or coin B?
