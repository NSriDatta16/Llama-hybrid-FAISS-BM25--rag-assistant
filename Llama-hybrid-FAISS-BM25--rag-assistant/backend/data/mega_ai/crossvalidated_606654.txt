[site]: crossvalidated
[post_id]: 606654
[parent_id]: 606642
[tags]: 
Short answer : there is no inconsistency. I want to make the following two remarks: Merely from the differentiation perspective, your confusion is understandable. For example, consider a bivariate function $F(x_1, x_2) = x_1x_2, (x_1, x_2) \in \mathbb{R}^2$ , in differential form, the total derivative of $F$ is \begin{align} dF(x_1, x_2) = x_2dx_1 + x_1dx_2, \end{align} which in appearance is completely different from $f(x_1, x_2)dx_1dx_2 = dx_1dx_2$ . Such difference may cause your confusion. However, the above perspective is not the right way to understand the expectation formula, which essentially concerns integration instead of differentiation . It is a common misconception that " $dF_A(a)$ " standalone is "evaluable", and it equals to " $f_A(a)da$ " when the density exists (when $f_A$ exists, the Radon-Nikodym notation $f_A = \frac{d\mu}{da}$ is standard, but its correct interpretation is $(6)$ below, which has nothing to do with differentiation in calculus). Every piece in the notation " $\int_{\mathbb{R}^n} h(a)dF_A(a)$ " and " $\int_{\mathbb{R}^n} h(a)f_A(a)da$ " should be interpreted jointly rather than separately -- in particular, the integration operator " $\int$ " should not be dropped. Longer answer : Below I will replace your r.v. $A$ with $X$ (because by convention, $A$ is more used to stand for events/sets instead of random variables), which is naturally defined on a probability space $(\Omega, F, P)$ . By definition, the expectation of a r.v. $Y$ is the integral of $Y$ with respect to the probability measure $P$ : \begin{align} E[Y] = \int_\Omega Y(\omega)P(d\omega). \tag{1} \end{align} When $Y$ is non-negative, the precise meaning (which is still a definition) of the right-hand side of $(1)$ is \begin{align} \sup\sum_i \inf_{\omega \in A_i}Y(\omega)P(A_i), \tag{$*$} \end{align} where $\{A_i\}$ is a finite decomposition of $\Omega$ into $\mathscr{F}$ -sets, and the supremum extends over all finite decomposition of $\Omega$ into $\mathscr{F}$ -sets. As you can see, although the notation " $d\omega$ " appeared in this measure-theoretic definition, it has nothing to do with the elementary "differentiation" concept seen in calculus. It merely is a notation. If $X = (X_1, \ldots, X_n)$ is a random vector, and $h$ is a function from $\mathbb{R}^n$ to $\mathbb{R}$ , then $h(X)$ is still a r.v. from $\Omega$ to $\mathbb{R}$ . Therefore applying $(1)$ to $h(X)$ yields \begin{align} E[h(X)] = \int_\Omega h(X(\omega))P(d\omega). \tag{2} \end{align} (Side comment on the $"E_A"$ notation in your post: Although it is seen in some literature, I always found that using the " $E_X[h(X)]$ " to denote the right-hand side of $(2)$ redundant and misleading -- given the integrand is a function of $X$ , the subscript " $X$ " doesn't provide any new information hence should be dropped.) As you may know, based on the probability measure $P$ , $X$ induces a new probability measure $\mu$ , on the Borel sets over $\mathbb{R}^n$ , called the distribution of $X$ , which is \begin{align} \mu(A) = P[X \in A] = P(X^{-1}(A)). \end{align} The Change of Variable Theorem in probability theory connects $P$ and $\mu$ as follows: \begin{align} \int_\Omega h(X(\omega))P(d\omega) = \int_{\mathbb{R}^n}h(x)\mu(dx). \tag{3} \end{align} Note that the precise meaning of the right-hand side of $(3)$ is exactly the same the measure-theoretic interpretation of $(1)$ , just with a different measure $\mu$ . When $A = (-\infty, x_1] \times \cdots \times (-\infty, x_n]$ , the set function $\mu(\cdot)$ can be converted to a point function $F(x_1, \ldots, x_n)$ , which is called the distribution function of $X$ . Because of the defining relation $F(x_1, \ldots, x_n) = \mu((-\infty, x_1] \times \cdots \times (-\infty, x_n])$ , the " $\mu(dx)$ " in the right-hand of $(3)$ is often replaced by " $dF(x)$ ", whence $(2)$ and $(3)$ together can be written as \begin{align} E[h(X)] = \int_\Omega h(X(\omega))P(d\omega) = \int_{\mathbb{R}^n}h(x)\mu(dx) = \int_{\mathbb{R}^n} h(x)dF(x). \tag{4} \end{align} Although the right-hand side of $(4)$ also admits the "Riemann-sum" interpretation as for the one-dimensional Riemann integral (under which it is usually referred as Riemann-Stieltjes integral ), it is recommended to interpret it just as a notational variant for $\int h(x)\mu(dx)$ , as Billinglsley noted in Section 17 of Probability and Measure : ...Since these distinctions are unimportant under the context of general measure theory $\int f(x)dF(x)$ and $\int fdF$ are best regarded as merely the notational variants for $\int f(x)\mu(dx)$ and $\int fd\mu$ . The point is: throughout the whole process $(2)$ -- $(4)$ , there is really no place to " evaluate $dF(x)$ " -- " $dF(x)$ " is a notational variant for $\mu(dx)$ , which in turn has to be interpreted as a part of measure-theoretic integral definition (i.e., there is no rigorous mathematical meaning for the standalone notation " $\mu(dx)$ "). Finally, let's talk about density $f$ of $X$ . While $(4)$ holds for any random vector $X$ , the relation \begin{align} \int_{\mathbb{R}^n}h(x)\mu(dx) = \int_{\mathbb{R}^n}h(x)f(x)dx \tag{5} \end{align} only holds when $\mu$ further admits the representation (a sufficient condition for $(6)$ to hold is that $\mu$ is dominated by $\lambda$ , for which case $f$ is the Radon-Nikodym derivative) \begin{align} \mu(A) = \int_A f dx, \tag{6} \end{align} where " $dx$ " is essentially " $\lambda(dx)$ ", which means the Lebesgue measure on $\mathbb{R}^n$ . As before, both $(5)$ and $(6)$ are measure-theoretic integrals (with respect to the Lebesgue measure $\lambda$ ) and shared the same interpretation as $(1)$ . In particular, " $dx$ " is not a vector in $\mathbb{R}^n$ (if you really want to make some geometric sense of it, in view of the " $P(A_i)$ " in $(*)$ , it is slightly better to call it a small rectangle in $\mathbb{R}^n$ , although I still discourage such isolated interpretation as stressed throughout this answer).
