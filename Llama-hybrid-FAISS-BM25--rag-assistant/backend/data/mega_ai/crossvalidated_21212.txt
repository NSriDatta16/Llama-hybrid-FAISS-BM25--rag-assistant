[site]: crossvalidated
[post_id]: 21212
[parent_id]: 
[tags]: 
Is additive logistic regression equivalent to boosted decision stumps?

Are additive logistic regression and boosted decision stumps (where a decision stump is a one-node decision tree) equivalent in some sense? I thought not, but if I google for "LogitBoost" algorithms, I find: http://weka.sourceforge.net/doc/weka/classifiers/meta/LogitBoost.html , which says it's a "class for performing additive logistic regression" http://rss.acs.unt.edu/Rdoc/library/caTools/html/LogitBoost.html , which says "Train logitboost classification algorithm using decision stumps (one node decision trees) as weak learners."
