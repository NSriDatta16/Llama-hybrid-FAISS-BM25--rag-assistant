[site]: crossvalidated
[post_id]: 437955
[parent_id]: 144475
[tags]: 
This post was brought to my attention just a few days ago. Thank you for your interest. Question 1: What useful statistical inferences can be made using a model that makes no assumptions at all? Before I answer this question we should agree on a definition of the word model: A common definition of a statistical model is the set of possible probability distributions or densities of the observed data. In addition to formulating a statistical model, one might make additional assumptions that do not restrict the distribution of the data such as missing at random, coarsening at random, or randomization assumptions in so called censored or missing data models. These latter type of assumptions are typically non-testable, i.e. they do not put restrictions on the distribution of the data and can thus not be tested based on data. For example, one commonly represents the observed data as a missing or censored data structure on a full-data random variable, and defines the target quantity of interest as some feature of the full-data distribution. To establish identification of this target quantity of the full-data distribution, one needs to make certain assumptions such as the ones I mention above. These assumptions allow us to define an estimand (i.e. feature of the distribution of the observed data) that equals the desired target quantity, even though these assumptions do not put any restrictions on the distribution of the data. These non-testable assumptions do not affect the statistical estimation or statistical properties of estimators of the target estimand, but they do affect the interpretation of the target estimand and the degree one feels comfortable extending the purely statistical interpretation to a causal or full-data distribution interpretation. I am going to focus on the notion of a statistical model. If we make no assumptions at all, then the statistical model would be all possible probability distributions. I agree that in this case we cannot do anything. This could happen, but still might be a useful realization, making us careful to over-interpret results that will be derived from statistical models that make assumptions. For example, if one observes a single microarray of gene expressions, then one might have to acknowledge that there is no basis for statistical inference without making very strong, unrealistic assumptions. In many studies we know or feel highly confident (based on understanding of the experiment) that the data set is the result of independent and identical experiments, in which case we view our data as n independent and identically distributed random variables with a common probability distribution. In other cases, one might condition on the units and treat the data observed on these units as the result of independent experiments, one for each unit. This does not only apply to experiments that involve random sampling of units from a population. For example, a study that enrolls patients that satisfy some eligibility criterion and then tracks patients longitudinally over time could be thought of as independent experiments, maybe identical, or maybe only independent. Our statistical model may then assume nothing else. Still, this is a real statistical model that allows us to formulate estimators with statistical inference based on asymptotic linearity of estimators and central limit theorems (that work under only assuming independence, or even weaker forms of independence assumptions). We might be able to make more assumptions, such as the treatment variable given the set of observed pre-treatment covariates only depends on a certain subset of the covariates (something that one might learn from talking to the people who made the treatment decision). In our research we carry out a lot of work on sample size one problems such as observing a single time series over many time points (assuming some form of stationarity), a single community of individuals connected through a network (assuming that the data at next time point on a subject is conditionally independent of data collected on other subjects at that time point, given the data we have observed on the friends of that subject), or a sequential adaptive trial in which the next experiment (e.g. randomly sampling a next group of subjects) is set in response to what is observed in the previous experiments. Again, such types of studies satisfy conditional independence assumptions that allow for estimators with asymptotic statistical inference. For me, a realistic statistical model is a model that is known to contain the true probability distribution of the data, or at least it can be sensibly defended as a truthful statistical model. One should be ready to defend a statistical model. Note that, using your language, an "exact model" would be a model that contains the true probability distribution of the data, but has nothing to do with making a lot of assumptions. The only hope to succeed in formulating such a model is to work hard on understanding the data generating experiment, learning about independence and conditional independence assumptions. There are cases where one cannot be sure (e.g. models for a single time series that avoid making parametric assumptions but still need a form of stationarity), even when posing a highly nonparametric model, and, the assumed model might be going as far as possible while still being able to obtain statistical inference (based on state of the art advances in probability theory). Even then (heavily advancing on current statistical methods), it is fair and necessary to criticize and be fully aware of the assumptions, while still moving forward with valid statistical estimators for such a model. This still represents important advances relative to working with parametric models that are known to be false from the start and cannot be defended at all. It might motivate us to more carefully design experiments for which this same model will be known to be valid, where we now know that we actually have valid powerful methods that handle such highly challenging statistical models. The selection of a statistical model should be distinguished from the construction of an estimator that might try out many working models and machine learning algorithms as a way to approximate the true distribution of the data. The fit of a data distribution is not a model, but just the realization of an estimator of the data distribution. Another important benefit by having defined the statistical estimation problem realistically is that one can set up simulation studies to evaluate the behavior of estimators (and data set competitions), refine them, learn the weak spots, and propose a bootstrap respecting the true experiment to further improve on finite sample inference. In the end, the asymptotic results are a must, but all that matters is finite sample inference, so one should always aim to work on finite sample improvements without affecting asymptotic optimality. Even such finite sample improvements are often guided by theory. Question 2: Does there exist a case study, with important, real data in the use of targeted maximum likelihood? Are these methods widely used and accepted? There is a growing literature on Targeted Learning. TMLE started with a 2006 article, and we published two books on the topic (van der Laan, Rose, 2011 and 2018) including contributions from a variety of authors working in the area. I just found out that the 2018 book (Targeted Learning in Data Science) is the top 1 in the Springer Series of Statistics over last three years, while the 2011 book is in top 3% overall going back to beginning of this series. Similarly, we see a great demand for workshops on the topic which we are giving regularly. We recently gave a workshop at the Bill and Melinda Gates Foundation on Targeted Learning and it included an initial presentation which showcased case studies in journals such as the New England Journal of Medicine, among others. There will be a link posted since it was recorded, feel free to contact me about it. Of course, these papers can all be Googled but this may still be helpful. Overall, I now regularly encounter articles by authors I do not know (e.g. not former students, postdocs, etc.). This is a good thing, and it is a joy to see new Ph.D. students at other places contributing new insights. Sometimes it is painful to see how some of such contributions are simply not understanding the material and are confusing the literature. Still, many of such authors are making a concerted effort, so they will get there eventually. Question 3: Are all inexact models indeed useless? If we define "inexact models" as statistical models that can be defended but for which we have no guarantee that the true data distribution is in it, then in my answer to Question 1 I clarify that such models are still useful. Work in such models advance the literature: the assumptions are transparent and for anybody to criticize and evaluate; and once one realizes the kind of assumptions ond needs to worry about, these are realistic enough so that one can expect future applications in which they can be applied. A model where not a single person on earth believes them is not helpful at all. For example, we teach our students that in GEE using a parametric regression model for a multivariate outcome that, for each choice of covariance matrix of the residuals, the estimator of the coefficients is consistent and asymptotically linear; and if we estimate the covariance matrix consistently then the estimator is efficient. These statements are predicated on the regression model being correct, but since they are not, they actually teach the wrong thing. In the real world, 1) the choice of covariance matrix defines the projection of the true regression curve on the parametric working model, and thereby affects the target estimand (so confidence intervals for two different covariance matrices will be non-overlapping for large enough sample size); 2) the variability of the estimator of the covariance matrix heavily contributes to the variability of the estimator of the coefficients, so that more nonparametric (and thus consistently) estimation of the covariance matrix typically heavily increases the actual variance of the estimator. The majority that one is taught in statistics is based on such unrealistic assumptions and is actually wrong when applied to the real world. This is just one of the million examples in which what we teach based on these models is not even representative of what happens in the real world when applying these methods. Question 4: Is it possible to know that you have the exact model other than in trivial cases? Not at all, as I explain in my response to Question 1. Either way, it is clearly my philosophy that one should make a sincere effort to define the real statistical estimation problem as accurately as possible, making assumptions that are reasonable and one can also defend. To me, this honest formulation is better than posing models in which the whole world knows the assumptions are plain wrong -- the confidence intervals have asymptotic coverage zero and the p-values result in testing procedures that have asymptotic type I error equal to 1. In addition, when people use these wrong models they typically play with them and try out many (I cannot blame them when that is only tool one has available), resulting in additional bias beyond the issue of using a statistical method with asymptotic coverage zero (for the assumed question of interest) and type I error 1. You wrote: "If this is too opinion-based and hence off-topic, where can it be discussed? Because Dr van der Laan's article definitely does need some discussion." This gets to the essence of statistical learning. Yes, this is incredibly important and it changes the way one approaches statistics. We often refer to the following steps as the roadmap of statistical learning (the answer to a statistical query): 1) Define data; 2) Define probability distribution of data and our knowledge about the data generating experiment; 3) Define target estimand (possibly augmenting its statistical interpretation with a causal/enhanced interpretation under specified non-testable assumptions); 4) Define estimator that is asymptotically valid under the statistical model assumptions; 5) Obtain inference based on sampling distribution of estimator; 6) Interpret the results (e.g. augmenting with sensitivity analysis to allow for interpretation of target estimand going in between purely statistical and purely causal). By taking these steps seriously, one often ends up with new statistical estimation problems. That itself can be an important contribution. In addition, many times new identification results (i.e, causal inference), new estimators and new theory needs to be developed (e.g. statistical estimators developed within the TMLE template), but this only happens because one has defined the precise challenge so that expertise and brainpower can be brought in by the general scientific community to solve it. If we replace the real problem by a toy problem, we avoid the real challenges.
