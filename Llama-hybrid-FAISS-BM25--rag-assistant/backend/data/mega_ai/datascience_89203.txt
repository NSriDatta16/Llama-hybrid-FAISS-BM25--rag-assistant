[site]: datascience
[post_id]: 89203
[parent_id]: 89132
[tags]: 
Do the undersampling after the train/test split and only in the train split: you want to somehow "weigth" your learning algorithm in order to prevent it to be biased towards the majority class, indeed, other techniques can be applied, and always on the train split, what eventually is the data for the learning algorithm. But... your test set should represent the real data distribution. In this case, you will asses if whatever technique you have applied to prevent a biased algorithm towards the majority class has not eventually affected its performance on it. Some other approaches for this problem (fraud vs non-fraud): Anomally detection One-class classifies: One Class SVM
