[site]: datascience
[post_id]: 51933
[parent_id]: 41873
[tags]: 
While it's true that increasing the batch size will make the batch normalization stats (mean, variance) closer to the real population, and will also make gradient estimates closer to the gradients computed over the whole population allowing the training to be more stable (less stochastic), it is necessary to note that there is a reason why we don't use the biggest batch sizes we can computationally afford. Let's say that our hardware configuration allows us to train using batches of 10K samples. Sure, our BN would get more precise stats, and our gradients would be more accurate and closer to the real gradient. However, this is not necessarily good because mini -batch SGD is proven to work better when the batches aren't big. As batches get remarkably bigger, mini-batch SGD becomes more and more like its father gradient descent, and this ancient monster isn't good for non-convex optimization problems like deep neural networks for both computational reasons and local-minima-related reasons.
