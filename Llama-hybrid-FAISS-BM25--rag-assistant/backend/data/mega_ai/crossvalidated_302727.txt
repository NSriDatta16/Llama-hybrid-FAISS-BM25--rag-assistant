[site]: crossvalidated
[post_id]: 302727
[parent_id]: 
[tags]: 
Spikes in loss function of deep neural network training

I'm training a simple feed-forward Deep Network with standard relu/sigmoid activation functions, using Tensorflow. I tried several optimizers and I noticed that those belonging to the Adam family show a strange behavior in loss trend, as you can see in the picture. I've read about some similar issues , but their loss trends appear to be quite different than mine. Have you any idea of the possible source of this problem? Can you suggest some possible references to investigate this issue? However, I verified that using SGD or RMSprop as optimizer the problem disappeared, but the obtained final loss value was much higher.
