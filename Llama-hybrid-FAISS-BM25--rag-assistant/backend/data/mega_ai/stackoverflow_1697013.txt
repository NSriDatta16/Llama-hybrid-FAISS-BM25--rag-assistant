[site]: stackoverflow
[post_id]: 1697013
[parent_id]: 
[tags]: 
How do I efficiently estimate a probability based on a small amount of evidence?

I've been trying to find an answer to this for months (to be used in a machine learning application), it doesn't seem like it should be a terribly hard problem, but I'm a software engineer, and math was never one of my strengths. Here is the scenario: I have a (possibly) unevenly weighted coin and I want to figure out the probability of it coming up heads. I know that coins from the same box that this one came from have an average probability of p , and I also know the standard deviation of these probabilities (call it s ). (If other summary properties of the probabilities of other coins aside from their mean and stddev would be useful, I can probably get them too.) I toss the coin n times, and it comes up heads h times. The naive approach is that the probability is just h/n - but if n is small this is unlikely to be accurate. Is there a computationally efficient way (ie. doesn't involve very very large or very very small numbers) to take p and s into consideration to come up with a more accurate probability estimate, even when n is small? I'd appreciate it if any answers could use pseudocode rather than mathematical notation since I find most mathematical notation to be impenetrable ;-) Other answers: There are some other answers on SO that are similar, but the answers provided are unsatisfactory. For example this is not computationally efficient because it quickly involves numbers way smaller than can be represented even in double-precision floats. And this one turned out to be incorrect.
