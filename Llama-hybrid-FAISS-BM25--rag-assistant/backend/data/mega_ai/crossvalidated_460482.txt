[site]: crossvalidated
[post_id]: 460482
[parent_id]: 
[tags]: 
What is the proper formula to highlight variance in utilization

I have $160$ machines that are given tasks over time. Each machine is expected to perform $\frac{1}{160}$ of the overall tasks, however if one encounters a fault, it could have some period of inactivity, shifting the average away from $\frac{1}{160} \approx 0.625\%$ I am displaying the data as an $8$ by $20$ grid, where each cell in the grid displays $\frac{\text{Tasks Completed}}{\text{Total Tasks Among All Machines}}$ What I have calculated currently are the Average number of tasks $\frac{\text{Average(Tasks Per Machine)}}{\text{Total Tasks}} \cdot 100 \%$ And the standard deviation $\sqrt{\sum\left(x_i - \mu\right)^2}$ So for a given cell I can determine the number of standard deviations from the mean for an individual machines utilization. I wanted to color the cells on a scale, where those with a Z-score of 0 would be green ( #00FF00 ). Would it be appropriate to treat the change in Z-score as a linear transition along a scale of Green ( #00FF00 ) to Yellow ( #FFFF00 ) to Red ( #FF0000 )? Should the Min and Max value in the table of data below have any factor on the scale, or would the range between colors essentially be arbitrary based on what 'resolution' I want in the results? At a very trivial level this is splitting the cells into either Red/Yellow/Green where x is the value of the cell If (x > (mean - stdD)) And (x (mean - (stdD * 6))) Then r = 255 g = 255 b = 0 Else If (x > (mean + stdD)) And (x *The black cell was due to one machine not having any tasks for the entire time range.
