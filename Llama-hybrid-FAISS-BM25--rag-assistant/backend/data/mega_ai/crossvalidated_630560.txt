[site]: crossvalidated
[post_id]: 630560
[parent_id]: 
[tags]: 
Why relu not used after the second FCL in the feed forward neural network part of the transformer

I found that after the second fully connected layer of feed forward neural network, transformer doesn't apply ReLU on it. If the purpose of ReLU after the first FCL, the ReLU should be applied after the second FCL, but it does not. Is there any reason?
