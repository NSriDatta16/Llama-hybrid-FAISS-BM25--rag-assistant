[site]: crossvalidated
[post_id]: 574359
[parent_id]: 574333
[tags]: 
There are two very good answers as of writing this, and so let me add a needlessly complex yet interesting approach to this problem. I think one way to operationalize the human generated vs truly random question is to ask if the flips are autocorrelated. The hypothesis here being that humans will attempt to appear random by not having too many strings of one outcome, hence switching from heads to tails and tails to heads more often than would be observed in a truly random sequence. Whuber examines this nicely with a 2x2 table, but because I am a Bayesian and a glutton for punishment let's write a simple model in Stan to estimate the lag-1 autocorrelation of the flips. Speaking of Whuber, he has nicely laid out the data generating process in this post . You can read his answer to understand the data generating process. Let $\rho$ be the lag 1 autocorrelation of the flips, and let $q$ be the proportion of flips which are heads in the sequence. A fair coin should have 0 autocorrelation, so we are looking for our estimate of $\rho$ to be close to 0. From there, we only need to count the number of occurrences of $H,H$ , $H, T$ , $T, H$ and $T,T$ in the sequence. The Stan model is shown below data{ int y_1_1; //number of concurrent 1s int y_0_1; //number of 0,1 occurrences int y_1_0; //number of 1,0 occurrences int y_0_0; //number of concurrent 0s } parameters{ real rho; real q; } transformed parameters{ real prob_1_1 = q + rho*(1-q); real prob_0_1 = (1-q)*(1-rho); real prob_1_0 = q*(1-rho); real prob_0_0 = 1 - q + rho*q; } model{ q ~ beta(1, 1); target += y_1_1 * bernoulli_lpmf(1| prob_1_1); target += y_0_1 * bernoulli_lpmf(1| prob_0_1); target += y_1_0 * bernoulli_lpmf(1| prob_1_0); target += y_0_0 * bernoulli_lpmf(1| prob_0_0); } Here, I've placed a uniform prior on the autocorrelation $$ \rho \sim \mbox{Uniform}(-1, 1) $$ and on the probability of a head $$ q \sim \operatorname{Beta}(1, 1) $$ Our likelihood is Bernoulli, and I have weighted the likelihood by the number of occurrences of each pair of outcomes. The probabilities of each outcome (e.g. probability of observing a heads conditioned on the previous flip being a heads) is provided by Whuber in his linked answer. Let's run our model and compare posterior distributions for the two sequences The estimated auto correlation for sequence 1 is -0.36, and the estimated autocorrelation for sequence 2 is -0.02 (close enough to 0). If I was a betting man, I'd put my money on sequence 1 being the sequence generated by a human. The negative autocorrelation means that when we see a heads/tails we are more likely to see a tails/heads! This observation lines up nicely with the 2x2 table provided by Whuber. Code The plot I present is made in R, but here is some python code to do the same thing since you asked import matplotlib.pyplot as plt import cmdstanpy # You will need to install cmdstanpy prior to running this code # Write the stan model as a string. We will then write it to a file stan_code = ''' data{ int y_1_1; //number of concurrent 1s int y_0_1; //number of 0,1 occurences int y_1_0; //number of 1,0 occurences int y_0_0; //number of concurrent 0s } parameters{ real rho; real q; } transformed parameters{ real prob_1_1 = q + rho*(1-q); real prob_0_1 = (1-q)*(1-rho); real prob_1_0 = q*(1-rho); real prob_0_0 = 1 - q + rho*q; } model{ q ~ beta(1, 1); target += y_1_1 * bernoulli_lpmf(1| prob_1_1); target += y_0_1 * bernoulli_lpmf(1| prob_0_1); target += y_1_0 * bernoulli_lpmf(1| prob_1_0); target += y_0_0 * bernoulli_lpmf(1| prob_0_0); } ''' # Write the model to a temp file with open('model_file.stan', 'w') as model_file: model_file.write(stan_code) # Compile the model model = cmdstanpy.CmdStanModel(stan_file='model_file.stan', compile=True) # Co-occuring counts for heads (1) and tails (0) for each sequence data_1 = dict(y_1_1 = 46, y_0_0 = 49, y_0_1 = 102, y_1_0 = 102) data_2 = dict(y_1_1 = 71, y_0_0 = 75, y_0_1 = 76, y_1_0 = 77) # Fit each model fit_1 = model.sample(data_1, show_progress=False) rho_1 = fit_1.stan_variable('rho') fit_2 = model.sample(data_2, show_progress=False) rho_2 = fit_2.stan_variable('rho') # Make a pretty plot fig, ax = plt.subplots(dpi = 240, figsize = (5, 3)) ax.set_xlim(-1, 1) ax.hist(rho_1, color = 'blue', alpha = 0.5, edgecolor='k', label='Sequence 1') ax.hist(rho_2, color = 'red', alpha = 0.5, edgecolor='k', label='Sequence 2') ax.legend() ```
