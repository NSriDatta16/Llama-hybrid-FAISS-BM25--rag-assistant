[site]: crossvalidated
[post_id]: 341058
[parent_id]: 
[tags]: 
Difference in Gradient Boosting Models

In the past year, I've seen a number of different gradient boosting machines emerging from different groups. I normally used Xgboost since it trained faster than sklearn's gradient boosting machine. Last year I found people using Lightgbm from microsoft and catboost. Secondly when I use these different methods I get different results apart from speed. In case of random forest the results are mostly same whether I use python or R. How are these models different from one another and how to determine which one to use for a given problem?
