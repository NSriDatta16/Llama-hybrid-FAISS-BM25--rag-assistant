[site]: crossvalidated
[post_id]: 442110
[parent_id]: 442108
[tags]: 
I don't think the objection is to just the term "statistically significant" but to the abuse of the whole concept of statistical significance testing and to the misinterpretation of results that are (or are not) statistically significant. In particular, look at these six statements: P-values can indicate how incompatible the data are with a specified statistical model. P-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone. Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold. Proper inference requires full reporting and transparency. A p-value, or statistical significance, does not measure the size of an effect or the importance of a result. By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis. So, they are recommending a more comprehensive way of doing and reporting analysis than simply just giving a p value, or even a p value with a CI. I think this is wise and I don't think it ought to be controversial. Now, going from their statement to my own views, I'd say that we often shouldn't mention the p value at all. In many cases, it doesn't provide useful information. Nearly always, we know in advance that the null is not exactly true and, quite often, we know it is not even close to true. What to do instead? I highly recommend Robert Abelson's MAGIC criteria: Magnitude, Articulation, Generality, Interestingness and Credibility. I say much more about this in my blog post: Statistics 101: The MAGIC criteria . (My views, unlike those of the ASA, are controversial. Many people disagree with them).
