[site]: crossvalidated
[post_id]: 294297
[parent_id]: 
[tags]: 
Off-policy evaluation of reinforcement learning: How to compute importance weights

I am working on a project that will use reinforcement learning to recommended products to customers in a mobile app. We have a few years of historical data available, which I would like to use for evaluation. I would like to do what they are doing in this paper: https://www.ijcai.org/Proceedings/15/Papers/257.pdf Specifically, I want to compute the importance weighted returns given by: $\hat{\rho}(\pi_e|\tau_i, \pi_i) = R(\tau_i)\prod_{t=1}^T \frac{\pi_e(a_t^{\tau_i}|s_t^{\tau_i})}{\pi_i(a_t^{\tau_i}|s_t^{\tau_i})}$ where $\pi_e$ is the policy to be evaluated, and $\pi_i$ is the policy that generated the data. $R(\tau_i)$ is the reward of the trajectory $\tau$. $a_t$ is the action at time t, and $s_t$ is the state at time t. Now, my question is: How do I compute $\pi_i(a_t^{\tau_i}|s_t^{\tau_i})$ ? In the paper, this is not explained, so I suspect the solution may be simple. I just don't understand exactly how this distribution should be estimated given the historical data. If anyone has an example of code that does this, or just an answer to the question, it would be highly appreciated. Thanks, Esben
