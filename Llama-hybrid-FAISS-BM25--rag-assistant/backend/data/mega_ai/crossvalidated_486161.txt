[site]: crossvalidated
[post_id]: 486161
[parent_id]: 486158
[tags]: 
We do this simply because the variance of a random variable is constrained to be positive (i.e. $\sigma^2 \in \mathbb{R}^+$ ) and so if we were to try to learn the variance we would have to constrain somehow the output of a neural network to be positive. A simple way around this is to learn the logarithm instead since $\log(\sigma^2) \in \mathbb{R}$ ensures that $\exp(\log(\sigma^2)) \in \mathbb{R}^+$ . The $\sigma$ in the first equation is the standard deviation which as you know is the square root of the variance. Then you can see that the multiplication of 0.5 outside the log equates to raising the variance inside of the log to the power of 0.5: $$ e^{0.5\log(var)} = e^{\log(var^{0.5})} = var^{0.5} = \sigma $$ Given the answer to (2) it should be clear they are the same.
