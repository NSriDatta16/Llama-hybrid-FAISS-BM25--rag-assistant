[site]: crossvalidated
[post_id]: 630277
[parent_id]: 440633
[tags]: 
For the linear model $$ f(x, \alpha) = \sum_{n=1}^{N} \alpha_n k(x, x_n) \tag{1} $$ we can write it in matrix form as $$ \hat y = K\alpha \tag{2} $$ where we have some training set of $N$ elements $x_n$ with labels $y_n$ , we can collect the $x_n$ in a Gram matrix $K$ such that $K_{ij}=\exp\left(-\gamma\|x_i-x_j\|_2^2\right)$ . We have $N$ equations with $N$ unknowns, and we know that the columns of $K$ form a basis, so we can directly solve the linear system $$ \hat \alpha = (K+\lambda I_N)^{-1}y \tag{3} $$ for some regularization hyper-parameter $\lambda \ge 0$ . But we still need $N$ evaluations of $k$ to make predictions, and solve an $N \times N$ linear system in some way to get $\alpha$ , which is generally expensive. Rahimi and Recht's early papers show that a random Fourier basis $Z \in [-1,1]^{D \times N}$ is a good approximation: $K \approx Z^\top Z.$ The paper and blog post don't really spell it out, but I think this line in " Reflections on Random Kitchen Sinks " is crucial: A linear combination of a linear combination is another linear combination, but with this new linear combination has many fewer $(D)$ parameters. This is somewhat cryptic if you don't already know how random kitchen sinks works. I think the more explicit way to state this is "We can replace solving a large linear system exactly with solving a small linear system approximately , with little loss of precision due to the approximation." We can do this by making the simple and direct substitution $\beta = Z\alpha$ in the original problem, and then solve for $\beta$ . Specifically: $$\begin{align} \hat y &= K\alpha \tag{2} \\ &\approx Z^\top Z\alpha \tag{4} \\ &= Z^\top \beta \tag{5} \end{align}$$ for which $\hat \beta$ minimizes square error with regularization $\lambda \ge 0$ : $$ \hat \beta = (ZZ^\top + \lambda I_D)^{-1}Zy. \tag{6} $$ In other words, we can just hide the old vector $\alpha$ (with $N$ elements) "under the covers" of the factorization, and work with the more convenient $\beta$ (with $D$ elements). In this way, the problem is reduced to OLS, which is cheaper to solve. And this is essentially what the authors write in "Random Features for Large-Scale Kernel Machines": In this set of experiments, we trained regressors and classifiers by solving the least squares problem $$ \min_w \| Z^\top w - y \|_2^2 + \lambda \| w^\top w \|_2^2, $$ where $y$ denotes the vector of desired outputs and $Z$ denotes the matrix of random features. To evaluate the resulting machine on a datapoint $x$ , we can simply compute $w^\top z(x)$ . And this result also appears in Rahimi and Recht's guerilla marketing leaflet in "Reflections on Random Kitchen Sinks." This is just OLS with basis expansion! This isn't an SVM (it doesn't maximize a margin or anything), but it is a kernel regression. You could quite reasonably ask "Why is this interesting? As a matter of principle, it's always true that we can pick some new basis (e.g. polynomials or thin-plate splines, or whatever) for our data, and then estimate a linear model." The key insights in Rahimi and Recht's papers are The Fourier basis approximates the radial basis function. The explicit feature space of the RBF kernel is infinite, but this approximation can achieve high-quality results with small $D$ . The approximation error is small. Rahimi and Recht show that there are nice probability bounds on the error, and that the approximation gets exponentially better as you increase $D$ . (Of course would never solve this system $(6)$ directly, but instead use a convenient factorization of $Z$ , such as $QR$ decomposition or singular value decomposition . Or we could even just use mini-batch methods directly, if the amount of data are truly enormous.) If we choose the basis dimension $D \ll N$ , then we've made some progress to simplifying the task: We only need $D$ evaluations of our kernel basis, and they're cheap to do. Decomposing $Z$ is cheaper than solving $K^{-1}y$ , especially for large $N$ . We have only $D$ elements in $\beta$ , compared to $N$ elements in $\alpha$ , so predictions require fewer multiplications. In the $D \ll N$ setting, the "big idea" is that we don't need to worry about a Gram matrix if we have a good basis. It just so happens that a random Fourier basis is a good basis and also it has a good approximation to RBF. Of course, you're hardly restricted to $D \ll N$ . Rahimi and Recht write By this third paper, we’d entirely stopped thinking in terms of kernels, and just fitting random basis function to data. We put on solid foundation the idea of linearly combining random kitchen sinks into a predictor. Which meant that it didn’t really bother us if we used more features than data points . Naturally you can use as rich a basis as you like, even $P \gg N$ , and then regularize the result with $\lambda$ . There's also a secret third use-case for random Fourier features. In any case that you need to work with an RBF kernel, but forming the $N\times N$ matrix $K$ is too expensive, it's convenient to work instead with $b^\top K b = b^\top Z^\top Z b$ or $K b = Z^\top Z b$ . The approximate factor $Z$ is cheap to compute with a much smaller memory requirement than $K$ , and you can choose an expedient order in which to compute the desired product.
