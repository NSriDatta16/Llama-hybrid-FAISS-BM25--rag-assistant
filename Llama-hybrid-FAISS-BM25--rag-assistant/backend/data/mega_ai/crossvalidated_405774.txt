[site]: crossvalidated
[post_id]: 405774
[parent_id]: 
[tags]: 
Label smoothing formula

I recently came across the paper Regularizing Neural Networks by Penalizing Confident Output Distributions . In section 3.2, it claims that label smoothing loss is equivalent to adding the KL divergence of the uniform distribution $u$ from the networkâ€™s learned distribution $p$ . $$L(\theta) = -\sum \log(p_\theta(y|x))-D_{\mathrm{KL}}(u \parallel p_\theta(y|x))$$ On the other hand, we know that the label smoothing gives a probability of $1-\epsilon$ for the target and $\epsilon/(V-1)$ for the rest of the labels. Given this, I can't infer the formula above from the label smoothing value. Can anyone knows how to solve this problem?
