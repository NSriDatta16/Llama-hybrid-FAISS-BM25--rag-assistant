[site]: datascience
[post_id]: 11436
[parent_id]: 11432
[tags]: 
Re-scaling or any other form of standardization/normalization is very useful when dealing with models that are trained with gradient descent: (SVM,NN, LogReg). This question explains the effect of normalization on the gradient rather well: https://stats.stackexchange.com/questions/111467/is-it-necessary-to-scale-the-target-value-in-addition-to-scaling-features-for-re Decision trees for example are invariant to linear transformations, therefore feature scaling, in theory, does not effect the model in any way. In the case of your set-up, you have a binary classification problem. I would recommend trying out first a linear classifier on your data. Logistic regressions is a good first choice. If you decide that LogReg is not working as well as you would like it to then you can move on to more complicated models. I would recommend using decision trees coupled with a gradient boosting model. In problems that are similar to what you described, specifically when there is no inherent structure in the data (as there is in image classification, speech recognition, etc.), gradient boosting models tend to outperform neural networks. Hope this helps.
