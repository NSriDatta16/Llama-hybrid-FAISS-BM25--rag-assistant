[site]: crossvalidated
[post_id]: 601055
[parent_id]: 601049
[tags]: 
Attention in NLP is determined by two key components: the token ID (unique identifier for each word or subword-unit) the position of the token in the document Each of (1) and (2) is represented by a unique vector for each token & position, which are added together to get an embedding for token T at position P in a document. There may be multiple types of embedding vectors for token T at position P. One of these will be used by the attention operator, which compares the similarity of this vector against the corresponding vector for another token T' at position P' (eg. dot product similarity). These scalar similarity scores are computed for every alternate token/position in the document (more specifically the attention context window) T' & P' and then renormalized to sum to 1. This produces what we refer to as the "attention weights" which people think of as what the model is "focused on" when computing a particular representation/prediction. What is learned by the model is the embedding vectors themselves (via backpropagation). The embedding vectors fully determine what attention will later focus on, so the model is not directly learning what to look at per-say, but rather learning how to represent words/positions in a way that computing simple similarity between these representations leads to useful weights that can emphasize a particular part of the input (and what gets emphasized differs between different documents based on the words that occur at different locations).
