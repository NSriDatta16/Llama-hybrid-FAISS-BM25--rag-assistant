[site]: crossvalidated
[post_id]: 482320
[parent_id]: 482304
[tags]: 
Creating some aggregate (such as a moving average) of features as a new feature is a perfectly fine idea. I believe your question is perhaps not that much about 'convention' (there aren't any) but best practices or the most pragmatic approach. To rephrase the question: what size of the moving window would be most effective, to increase quality of the predictions? You have to experiment but here are some clues. the window size may need to be much longer than the prediction delay . Here, the delay is 1 hour. Then the window size should be a few-hours-long at least. Imagine an opposite situation: a 10-minute window is unlikely to contain a causal relationship with whatever happens 1 hour later. the number of events (records) in that time window are key. If their frequency is constant, you are safe with time-constrained time window, but if the frequency is variable , consider to delimit the window by number of events it contains, rather than time. Otherwise you may end up with bogus time windows containing zero events. Both approaches have obvious downsides. The window size needs to be in some reasonable proportion to the variability of the target . This is related to the classic bias vs variance problem: imagine your true data points differ by 10% between the subsequent measurements. Then if your moving average differs by 200% between measurements, then maybe the time window is too small, allowing too big randomness. On the other hand, if the moving average ends up mostly constant then the window may be too large and not useful to any predictions. And then, going a bit beyond the original question, more strategically: The ground truth (target) measurement outliers, if they exist, will make a problem and confuse the classifier. As remedy, enlarging the window may help, but also consider other metrics, such as the moving median Beyond the moving average, additional metrics built on top are often useful. For instance, do you expect to discover linear trends ? Then gradient (derivative) of growth of the average could be used. Or a simple difference between moving_average(n) and moving_average(n+1) could do seasonability : do you have daily (24-h) cycles? Then maybe you want to look at different type of moving average. Instead of moving average of the last 7 hours, consider using "selective" moving average of the most related measurements going further into the past. For instance, to predict event at 6:00 am today, create an average of all 6:00 measurements over the last 14 days) How about creating all those derivative features and seeing which one performs best. In the end each of those statistics will contain a different piece of information, complementary to each other.
