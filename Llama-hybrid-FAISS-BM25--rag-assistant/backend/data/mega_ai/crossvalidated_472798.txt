[site]: crossvalidated
[post_id]: 472798
[parent_id]: 472797
[tags]: 
From my understanding, people are trying different things on different tasks. And there are no universal guidelines to pick the "right" pre-trained model. It is better to choose the pretained model, that training data is close to your data. For example, if your task is working on images with cat and dogs, it is better to use a pretrained model that is also trained with cats and dogs instead of medical images. It will always to be the case that pre-trained model's predicted class is different on your task. And this does not matter at all, because we just use pre-trained model to generate features, i.e., we will strip out the final layers to predict classes, but use the middle layer as "embedding" on new task. To answer your questions in the comment You can search for pre-trained network to get the data used and the problem it was trying to solve. For example, if I search VGG16, I got VGG16 is a convolutional neural network model proposed by K. Simonyan and A. Zisserman from the University of Oxford in the paper “Very Deep Convolutional Networks for Large-Scale Image Recognition”. The model achieves 92.7% top-5 test accuracy in ImageNet, which is a dataset of over 14 million images belonging to 1000 classes. So, I know the training data is ImageNet and it is a multi class classification setting. You can further search ImageNet to see how the images and classes look. The key point of the transfer learning is using the pre-trained model to generate features. Let me use another example to illustrate this idea: think about word embedding in NLP, you may view the word embedding vector is a pre-trained model. And we will build "second half" of the neural network to use them for the new task. the weights of pre trained model will be only for that pre-trained model's class. Yes, but we are adding additional layers, and these layer can "adopt" and use the "weights for pre-trained class" to perform the new tasks.
