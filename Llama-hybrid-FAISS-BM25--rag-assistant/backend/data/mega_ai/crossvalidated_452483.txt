[site]: crossvalidated
[post_id]: 452483
[parent_id]: 
[tags]: 
Why Some Algorithms Produce Calibrated Probabilities

I have seen that some algorithms especially the ones that are linear produce well-calibrated probabilities. For example, Logistic Regression or the Multi-Layer Perceptron produce highly calibrated probabilities whereas non-linear algorithms like SVM, Random Forest or KNN don't. Gradient Boosting Trees, which is non-linear, on the other hand, produces a very well calibrated class probabilities. Can someone please explain why is this the case with different algorithms? Many thanks in advance!
