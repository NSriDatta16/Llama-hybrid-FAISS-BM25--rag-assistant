[site]: crossvalidated
[post_id]: 449411
[parent_id]: 449386
[tags]: 
One thing I've figured out after a couple of consultations with researchers is that, while the language people to use to describe their experimental design is done with the best of intentions (use common set of terms), a more down-to-earth explanation sometimes helps. Please do not smile at my oversimplification, but can I try putting your experiment into simpler language? Let me know if I'm misunderstanding things: You took a "experimental culture" and a "control culture" and worked them up identical ways except you "treated" the experimental culture with some cells. You did this two more times, for a total of 3 treated cultures and 3 control cultures. You took 100 measurements on each culture, for a total of 600 measurements. So, to your question (I'm paraphrasing, please make sure I got it right): is it appropriate to group all of the results (n = 300) from the treated cultures and compare them to the results from the control cultures (n = 300) . I would say: Maybe yes, but only you can answer this question. Yeah that's a bit of a cop out; grouping all of the controls and all of the treatment cultures means that you are effectively ignoring any between-repetition variance. Is it appropriate to assume that the experimental culture from Repetition 1 and the culture from Repetition 2 are the same? Only you know. In some experiments, this sort of between-repetition variance can be quite large and it depends on the laboratory conditions. For example, did you do the repetitions on different days? Did you use different "batches" of reagents? Grouping all of your repetitions together -- though it may be reasonable -- constitutes an assumption that these sources of variance are zero (or negligible). If you aren't comfortable with this assumption then you might be more interested in estimating a mixed-effects model. ADDON: At this point, I can't say whether or not certain assumptions (such as normality) are met for your data because I haven't seen your data. Means and SDs might not be good descriptions of your results if they are count data (e.g., number of cells visualized or something). However, you have a lot of measurements (n = 600) so it is likely that your data are approximately normal. You will have to check this. However, I think we might be getting ahead of ourselves here, and this is partly my fault. Your question was about whether it is appropriate to group the controls and tests together and compare a group of 300 results against the other 300 results. Instead of asking "why?" you should ask "why not?" Why not group the results together? When you group them, do you fail to find reasonable results? It's true that there are more sophisticated techniques that can make adjustments for variance among control runs 1-3 (and/or test runs 1-3), but in applied research it's typically better to start small and progressively seek out more advanced analyses when one of the following happens: Your preliminary results from the simpler (grouped) approach do not address your research question adequately; OR You have a distinct theoretical reason that the control/test runs should not be treated as a two homogeneous groups (or some other more advanced analysis is needed). You've implied that there is not a theoretical reason to think of the replicates as distinct, so we should let the data do the talking. Here's what I suggest: Group the data together as 300 control runs and 300 test runs. Perform a nonparametric test: the Mann-Whitney U test on the difference between the two groups. If this test fails to be significant, a parametric test may get you the additional power you need: use a Welch-Satterthwaite "independent" t-test, but first you need to check some distributional assumptions. Examine two separate histograms: one of the 300 control runs and another of the 300 test runs. If the data appear normally distributed (tests such as the Shapiro-Wilk test, Levene's test, etc. can be used to "check" these assumptions to some extent but they have limitations of their own), go ahead and perform the above-mentioned t test ont he grouped data. If the p-value suggests no significant difference, then re-evaluate your assumptions: Are there outliers? Are the data normally distributed? is there significant between-repetition variance that you feel is messing up the analysis? If you are still struggling to get that p
