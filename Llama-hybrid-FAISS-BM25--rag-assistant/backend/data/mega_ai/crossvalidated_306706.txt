[site]: crossvalidated
[post_id]: 306706
[parent_id]: 
[tags]: 
How is a vector differentiation by a matrix defined?

I am a beginner of the machine learning. And at the same time, this is the first time to ask a question in this site, so I might have a problem with this question in terms of understandability. And when I learned backpropagation, I saw this type of formula. E: scholar function(error function), $\mathbf{W}$: matrix(Weight matrix), $\mathbf{p}$: vector, $\mathbf{x}$: input vector, $\mathbf{b}$: bias vector, $\mathbf{W}^T$: transposed matrix of $\mathbf{W}$ $\mathbf{p} := \mathbf{Wx} + \mathbf{b}$ $\frac {\partial E}{\partial \mathbf{W}} = \frac{\partial E}{\partial \mathbf{p}} \cdot \frac{\partial \mathbf{p}}{\partial \mathbf{W}^T} = \frac{\partial E}{\partial \mathbf{p}} \mathbf{x}^T$ This is the first time for me to see the differentiation of vector by the matrix. But, how is it defined? Althogh I coundn't find any useful information, could anyone explain this?
