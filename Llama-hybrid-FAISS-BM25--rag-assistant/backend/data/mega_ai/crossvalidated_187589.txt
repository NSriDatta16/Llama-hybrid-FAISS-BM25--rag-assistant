[site]: crossvalidated
[post_id]: 187589
[parent_id]: 178957
[tags]: 
L in LSI as you know stands for latent. So, LSI finds latent concepts in a given document term matrix (or matrix in general). In essence, LSI uses SVD. SVD is a matrix decomposition. If your input matrix is $A$ then the SVD of $$A = U\Sigma V^T.$$ Here $A$ is $m \times n$. $U$ is $m \times r$, indicating $m$ documents and $r$ concepts. This is a document to concept similarity matrix. $\Sigma$ is $r \times r$. This is a diagonal matrix whose entries from upper left to lower right are positive and in decreasing order. These values are the eigenvalues of $A$. A higher value indicates the strength of the concept. $V$ is $n \times r$ (note the transpose makes it $r \times n$), indicating $n$ terms and $r$ concepts. This is a term to concept similarity matrix. So, SVD provides both concept strength in terms of documents ($U$), but also terms ($V$). Like PCA, SVD can be used for dimensionality reduction. To do this you would reduce the square matrix $\Sigma$ of size $r$ to $\tilde{r}$ where $\tilde{r} So, to summarize. The eigenvalue ordering indicates (latent) concept strength. $U$ and $V$ are similarity matrices of document to concept and term to concept respectively.
