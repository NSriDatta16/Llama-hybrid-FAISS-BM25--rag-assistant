[site]: crossvalidated
[post_id]: 592216
[parent_id]: 
[tags]: 
Does this look like overfitting or something else?

The input are a timeseries of 1x41x41 geospatial images (so, 5x1x41x41 for example). I managed to achieve an MSE of 0.53 with PCA and Random Forest. But I thought to use ConvLSTM since my input size is large and knowing that I can manipulate many variables to prevent overfitting.But as you can see from the image below, I'm having a hard time. My current model is as follows: model = Sequential( ConvLSTM(INPUT_DIM=1, 128, KERNEL_SIZE=[3,3], NUM_LAYERS=1, batch_first=True, bias=True, return_all_layers=False), #torch.nn.BatchNorm3d(5), torch.nn.MaxPool3d((8,8,8)), torch.nn.ReLU(), torch.nn.Flatten(), torch.nn.Dropout(p=0.2), torch.nn.Linear(2000,20)).to(DEVICE) Before, the training loss would start similar to validation loss but keep on decreasing while the valid loss stays constant or increases, so I thought of data augmentation and added the following transform functions to my training loader: class CustomDataset(torch.utils.data.Dataset): def __init__(self, inputs, labels, transform=True, target_transform=None): self.inputs = inputs self.labels = labels self.transform = transform self.target_transform = target_transform def __len__(self): return len(self.labels) def __getitem__(self, idx): image = self.inputs[idx] label = self.labels[idx] if self.transform: r = np.random.choice([0,1,2,3,4]) if r == 2: image = np.flip(image, axis=(2)).copy() elif r == 3: image = np.flip(image, axis=(3)).copy() elif r == 4: image = np.flip(image, axis=(2,3)).copy() image = torch.tensor(image) label = torch.tensor(label) return image.to(DEVICE), label.to(DEVICE) That might explain why my training loss starts higher than my valid loss since I'm not applying such transformations for the latter. Training batch size is 32 and validation batch size is 1. I'm normalizing my input values as follows: X = zscore(np.array(X).reshape(5599,5,1,41,41)) I'm using adam with lr=1e-3. Any suggestions? Is my reasoning flawed? Thanks in advance.
