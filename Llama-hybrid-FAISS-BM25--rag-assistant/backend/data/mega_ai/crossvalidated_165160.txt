[site]: crossvalidated
[post_id]: 165160
[parent_id]: 165063
[tags]: 
A partial answer to this is found in Gelman et al., Bayesian Data Analysis , 3rd ed. Jeffreys' principle can be extended to multiparameter models, but the results are more controversial. Simpler approaches based on assuming independent noninformative prior distributions for the components of the vector parameter $\theta$ can give different results than are obtained with Jeffreys' principle. When the number of parameters in a problem is large, we find it useful to abandon pure noninformative prior distributions in favor of hierarchical models, as we discuss in Chapter 5. When Gelman writes that the results are "controversial," I believe he means that a prior that is noninformative in one dimension tends to become strongly informative in several. If memory serves, this was a claim made in the same section of BDA 2nd ed., but I don't have a copy with me at the moment.
