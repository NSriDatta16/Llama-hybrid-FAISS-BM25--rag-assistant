[site]: datascience
[post_id]: 56574
[parent_id]: 13216
[tags]: 
Honestly there is no intuitive way to understand why NCE loss will work without deeply understanding its math. To understand the math, you should read the original paper. The reason why NCE loss will work is because NCE approximates maximum likelihood estimation (MLE) when the ratio of noise to real data $k$ increases. The TensorFlow implementation works in practice. Basically, for each data $(x, y)$ , $y$ is the labeled class from the data, TensorFlow NCE loss samples $k$ classes from noise distributions. We calculate a special version of the digits for each of the classes (1 from data + $k$ from noise distributions) using equation $$\Delta s_{\theta^0}(w,h) = s_{\theta^0}(w,h) - \log kP_n(w)$$ Where $P_n(w)$ is the noise distribution. With the digits for each classes calculated, TensorFlow use the digits to compute softmax loss for binary classification (log loss in logistic regression) for each of the classes, and add these losses together as the final NCE loss. However, its implementation is conceptually wrong because the ratio of noise to real data $k$ is different to the number of classes $n$ sampled from noise distributions. TensorFlow NCE does not provide a variable for the noise to data ratio, and implicitly assumes $n=k$ which I think is conceptually incorrect. The original NCE papers skipped the derivations of the proof a lot so that it is really hard to understand NCE. To understand the math about NCE easier, I have a blog post on this annotating the math from the NCE papers: https://leimao.github.io/article/Noise-Contrastive-Estimation/ . College sophomore or above should be able to understand it.
