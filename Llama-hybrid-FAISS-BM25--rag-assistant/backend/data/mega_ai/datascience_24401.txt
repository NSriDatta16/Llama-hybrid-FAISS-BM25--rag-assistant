[site]: datascience
[post_id]: 24401
[parent_id]: 
[tags]: 
How to develop small workable embeddings for debugging

Processing language data with deep learning models often involves a lookup of a pre-trained embedding model. In the model development phase, it's very annoying that every time the entire embeddings are loaded, as embeddings could be very big (e.g., Glove ) and would consume a lot of timing loading it. Is there a way to build/find a smaller and workable embedding just for debugging? Right now what I've done is shrink the dimensions of Glove , for example, reduce the dimension of embeddings from 300d to 30d by taking the first 30 dims, but that might potentially induce a risk that some of words will share the same embedding.
