[site]: datascience
[post_id]: 11881
[parent_id]: 11880
[tags]: 
Are there machine learning models commonly accepted as representing a good tradeoff between the two? I assume that by being good at prediction you mean being able to fit nonlinearities present in the data while being fairly robust to overfitting. The tradeoff between interpretability and being able to predict those nonlinearities depends on the data and question asked. There really is no free lunch in data science and no single algorithm can be considered to be the best for any set of data (and the same applies for interpretability). The general rule should be that the more algorithms you know the better it is for you as you can adopt to your specific needs more easily. If I had to pick my favorite for classification task that I often use in business environment I would pick elastic-net for logistic regression . Despite strong assumption about the process that generates the data it can easily adopt to data thanks to the regularization term maintaining its interpretability from basic logistic regression. Is there are any literature enumerating the characteristics of algorithms which allow them to be explainable? I would suggest you to pick a well written book that describes the commonly used machine learning algorithms and thier pros and cons in diffrent scenarios. An example of such book can be The Elements of Statistical Learning by T. Hastie, R. Tibshirani and J. Friedman
