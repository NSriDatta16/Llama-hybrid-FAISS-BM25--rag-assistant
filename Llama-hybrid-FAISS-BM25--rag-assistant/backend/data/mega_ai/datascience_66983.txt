[site]: datascience
[post_id]: 66983
[parent_id]: 66978
[tags]: 
For any CNN layer $n$ with $F_n$ filters and kernel size $K_n^{height},K_n^{width}$ the number of parameters to train is $$F_{n-1} \cdot F_n \cdot K_n^{height} \cdot K_n^{width} + F_n$$ where the first summand stands for the weights and the second for the bias parameters (if $n$ is the first CNN layer then $F_{n-1}$ equals the number of input channels). And for the special case of $F_{n-1} = F_n$ (i.e. same number of filters for layer $n$ and layer $n-1$ ) and $K_n^{height} = K_n^{width}$ (i.e. square shaped Kernel) the number of parameters for layer $n$ is $F_n^2 \cdot K_n^2 + F_n$ . Accordingly, you have a non-linear dependency of trainable parameters with regards to the number of filters.
