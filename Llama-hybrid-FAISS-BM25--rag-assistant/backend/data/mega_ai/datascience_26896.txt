[site]: datascience
[post_id]: 26896
[parent_id]: 26894
[tags]: 
actually you are right, you have to make use of the test set the less times you can if you want to be quite sure it will stay a test set, and not another dev set. Said that, when you see that the performances on the test set are much worse than the dev set, you have other options than touch the size of the dev set: 1) add more penalties in the model, like a form of regularization (L1, L2, etc.); 2) decrease the degrees of freedom of your model, like the number of layer/nodes in case of NN, or the number of trees in case of decision trees et similia; 3) decrease the learning epochs; 4) try some (more aggressive) data augmentation technique; You can also use data augmentation in the test set: for each sample of the test set you can use the same form of data augmentation you used on the training set, and finally taking as result the average of the various single results. Even if this technique do not resolve a very big performance difference from the dev set and the test set, it can make your model acceptable. An in this last case, you have of course to use it also on the real data when the model is deployed. And in general if you are in need of accessing the test set several times, in order to have a less biased model you can reshuffle all the data, and then divide it again in train/dev/test. Of course, the rule of accessing the test set the less times possible stays true.
