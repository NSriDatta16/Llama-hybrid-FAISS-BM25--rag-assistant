[site]: crossvalidated
[post_id]: 452757
[parent_id]: 452730
[tags]: 
Jan has actually captured the essence of the answer in his comment, but I'll try to make it more explicit. This is not specific to parameter sharing; it is what neural networks typically do. What this operation does is to produce a non-linear (assuming $\sigma$ being a non-linear function) dimensionality augmentation of the data. Now, why would you want to do that? For classification, your data might fall into linearly non-separable classes in the one-dimensional space. By appropriate transformation into a higher-dimensional space, they might become linearly separable. Below is a toy example: The classes A (red) and B (blue) cannot be linearly separated only based on their x-coordinate. If you artificially generate an y-coordinate, as $x^2$ (non-linear!), you can easily separate them: The same argument holds also for regression: You may not be able to find a good regression line (linear function) in the one-dimensional space, but in a higher dimensional space it might work. This is analogous to introducing non-linearly transformed values of the original independent variable as additional independent variables in linear regression, e.g.: $$y = \beta_0 + \beta_1 x + \beta_2 x^2$$ Parameter sharing here means that you are applying the same transformation on all components of your input vector. This can make sense when the absolute position of a value in the vector is irrelevant, but instead only its relative distance from other values matters. You can think of it as of, instead of presenting your network a single, $B$ -dimensional input, you present it a sequence of one-dimensional values (scalars), which your network then augments into multi-dimensional vectors, always applying the same transformation, and stores them for further collective processing.
