[site]: crossvalidated
[post_id]: 326299
[parent_id]: 
[tags]: 
In seq2seq, how is the attention vector combined with the hidden state of the decoder?

My understanding of attention is that a weighted combination of a set of vectors is somehow combined with the decoder's hidden state. How exactly is it combined? Is it added to the hidden state before it enters the cell at each time step? In particular, I'm using LuongAttention , an AttentionWrapper , and a GRUCell .
