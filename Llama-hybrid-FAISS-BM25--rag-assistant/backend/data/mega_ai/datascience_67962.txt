[site]: datascience
[post_id]: 67962
[parent_id]: 
[tags]: 
Sentences language translation with neural network, with a simple layer structure (if possible sequential)

Context: Many language sentences translation systems (e.g. French to English) with neural networks use a seq2seq structure: "the cat sat on the mat" -> [Seq2Seq model] -> "le chat etait assis sur le tapis" Example: A ten-minute introduction to sequence-to-sequence learning in Keras , Python for NLP: Neural Machine Translation with Seq2Seq in Keras I noticed that in all these examples the structure of the neural network is not done by using a Sequential structure with consecutive layers, but rather a more complex structure like this: Question: are there successful attempts of doing sentence language translation with classic Sequential layers? i.e.: Input layer: word-tokenized sentence in english, zero padded: "the cat sat on the mat" => x = [2, 112, 198, 22, 2, 302, 0, 0, 0, 0, 0, 0, 0, 0, ...] Output layer: word-tokenized sentence in french, zero padded: "le chat etait assis sur le tapis" => y = [2, 182, 17, 166, 21, 2, 302, 0, 0, 0, 0, 0, 0, 0, 0, ...] What would you use as layers? I think we could start with: model = Sequential() # in shape: (None, 200) model.add(Embedding(max_words, 32, input_length=200)) # out shape: (None, 200, 32) model.add(LSTM(100)) # out shape: (None, 100) .... what here? ... but then how to have a second Embedding for the output language and reverse it? from a 200x32 embedding (floats) to an integer list like this [2, 182, 17, 166, 21, 2, 302, 0, 0, 0, 0, 0, 0, 0, 0, ...] ? Also, how to measure the loss in this situation, mean squared error ? More generally, what is the simplest structure you can think of (even if it does not give the best results), working for language translation? (it's ok even if not sequential)
