[site]: datascience
[post_id]: 15308
[parent_id]: 15301
[tags]: 
Boruta is by universal reputation dog-slow and not very good. Boruta runs take many hours or days. VIF feature-selection algorithm is not objective, anyway. You can program your own feature-selection that runs faster. I ran Boruta a few times on various datasets and it wasted 4 days of my time, and the result was inconclusive. Here's the fast-and-dirty not-100%-scientific approach, culled from consensus in a lot of Kaggle competitions and suchlike. It's 10-1000x faster than Boruta and probably more accurate. Run some fast exploratory trees (RF/XGB), i.e. not very deep, not too many trees. Extract feature importances. Repeat for several random seeds and look at average or absmax feature importances. For now, throw out very-high-cardinality features (e.g. user-ID, zipcode etc.), since trees won't tend to split on them until very deep and small nodes, so initially exclude them. Let's define "Very-high-cardinality" as >= ~0.3 distinct values per record, or less in a larger dataset. Throw out all near-zero features, zero- and very-low-importance features ( Measure the matrix of which features are highly correlated ( Rerun, your CV accuracy should have gone way up, and your training time will be way faster (since tree training is quadratic to no. of features). Revisit choices of borderline-low-important or correlated feature, and again use multiple random seeds. For exploratory, 5-fold CV is fine, but if training time is an issue, 3-fold works. 10-fold is unnecessary. For the subset of remaining features, use a 'respectable' method. See the excellent posts on CrossValidated in feature-selection. Somebody with a PhD in Stats will probably wag their finger at me for this :)
