[site]: crossvalidated
[post_id]: 144174
[parent_id]: 144121
[tags]: 
1) Yes. You can aggregate/de-aggregate (?) binomial data from individuals with the same covariates. This comes from the fact that the sufficient statistic for a binomial model is the total number of events for each covariate vector; and the Bernoulli is just a special case of the binomial. Intuitively, each Bernoulli trial that makes up a binomial outcome is independent, so there shouldn't be a difference between counting these as a single outcome or as separate individual trials. 2) Say we have $n$ unique covariate vectors $x_1, x_2, \ldots, x_n$, each of which has a binomial outcome on $N_i$ trials, i.e. $$Y_i \sim \mathrm{Bin}(N_i, p_i)$$ You've specified a logistic regression model, so $$\mathrm{logit}(p_i) = \sum_{k=1}^K \beta_k x_{ik}$$ although we'll see later that this isn't important. The log-likelihood for this model is $$\ell(\beta; Y) = \sum_{i=1}^n \log {N_i \choose Y_i} + Y_i \log(p_i) + (N_i - Y_i) \log(1-p_i)$$ and we maximise this with respect to $\beta$ (in the $p_i$ terms) to get our parameter estimates. Now, consider that for each $i = 1, \ldots, n$, we split the binomial outcome into $N_i$ individual Bernoulli/binary outcomes, as you have done. Specifically, create $$Z_{i1}, \ldots, Z_{iY_i} = 1$$ $$Z_{i(Y_i+1)}, \ldots, Z_{iN_i} = 0$$ That is, the first $Y_i$ are 1s and the rest are 0s. This is exactly what you did - but you could equally have done the first $(N_i - Y_i)$ as 0s and the rest as 1s, or any other ordering, right? Your second model says that $$Z_{ij} \sim \mathrm{Bernoulli}(p_i)$$ with the same regression model for $p_i$ as above. The log-likelihood for this model is $$ \ell(\beta; Z) = \sum_{i=1}^n \sum_{j=1}^{N_i} Z_{ij}\log(p_i) + (1-Z_{ij})\log(1-p_i) $$ and because of the way we defined our $Z_{ij}$s, this can be simplified to $$ \ell(\beta; Y) = \sum_{i=1}^n Y_i \log(p_i) + (N_i - Y_i)\log(1-p_i) $$ which should look pretty familiar. To get the estimates in the second model, we maximise this with respect to $\beta$. The only difference between this and the first log-likelihood is the term $\log {N_i \choose Y_i}$, which is constant with respect to $\beta$, and so doesn't affect the maximisation and we'll get the same estimates. 3) Each observation has a deviance residual. In the binomial model, they are $$ D_i = 2\left[Y_i \log \left( \frac{Y_i/N_i}{\hat{p}_i} \right) + (N_i-Y_i) \log \left( \frac{1-Y_i/N_i}{1-\hat{p}_i} \right)\right] $$ where $\hat{p}_i$ is the estimated probability from your model. Note that your binomial model is saturated (0 residual degrees of freedom) and has perfect fit: $\hat{p}_i = Y_i/N_i$ for all observations, so $D_i = 0$ for all $i$. In the Bernoulli model, $$ D_{ij} = 2\left[Z_{ij} \log \left( \frac{Z_{ij}}{\hat{p}_i} \right) + (1-Z_{ij}) \log \left(\frac{1-Z_{ij}}{1-\hat{p}_i} \right)\right] $$ Apart from the fact that you will now have $\sum_{i=1}^n N_i$ deviance residuals (instead of $n$ as with the binomial data), these will each be either $$D_{ij} = -2\log(\hat{p}_i)$$ or $$D_{ij} = -2\log(1-\hat{p}_i)$$ depending on whether $Z_{ij} = 1$ or $0$, and are obviously not the same as the above. Even if you sum these over $j$ to get a sum of deviance residuals for each $i$, you don't get the same: $$ D_i = \sum_{j=1}^{N_i} D_{ij} = 2\left[Y_i \log \left( \frac{1}{\hat{p}_i} \right) + (N_i-Y_i) \log \left( \frac{1}{1-\hat{p}_i} \right)\right] $$ The fact that the AIC is different (but the change in deviance is not) comes back to the constant term that was the difference between the log-likelihoods of the two models. When calculating the deviance, this is cancelled out because it is the same in all models based on the same data. The AIC is defined as $$AIC = 2K - 2\ell$$ and that combinatorial term is the difference between the $\ell$s: $$AIC_{\mathrm{Bernoulli}} - AIC_{\mathrm{Binomial}} = 2\sum_{i=1}^n \log {N_i \choose Y_i} = 9.575$$
