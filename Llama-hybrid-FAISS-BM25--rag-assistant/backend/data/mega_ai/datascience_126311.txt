[site]: datascience
[post_id]: 126311
[parent_id]: 
[tags]: 
Understanding the concepts of word embedding in GPT-2

I have a program that calculate the word embedding using GPT-2 specifically the GPT2Model class: from transformers import GPT2Tokenizer, GPT2Model import torch tokenizer = GPT2Tokenizer.from_pretrained('gpt2') model = GPT2Model.from_pretrained('gpt2') caption = "this bird is yellow has red wings" encoded_caption = tokenizer(caption, return_tensors='pt') input_ids = encoded_caption['input_ids'] with torch.no_grad(): outputs = model(input_ids) word_embeddings = outputs.last_hidden_state I have a few questions about this: When calculating the word embedding using outputs.last_hidden_state , does this mean that the word embedding only uses the token embedding and positional embedding of GPT-2, without feeding them to the decoder blocks after that ? Is this embedding also known as contextualized embedding ? How does this embedding better than RNN architectures, such as LSTM or bidirectional-LSTM embedding ?
