[site]: crossvalidated
[post_id]: 308898
[parent_id]: 308870
[tags]: 
If your distribution has a closed form density, then it would make sense to just compare KL divergences between your input and the two output distributions and then settle on the one that gives you less KL divergence. The case with no closed form density is more interesting. If you can easily generate samples from either distribution (or have a pre-labeled training set), then you could try a simple mixture-of-experts architecture, where the input is a single vector (row) and the output is a 0-1 prediction. You then average predictions over your full $X$ and use logistic loss on the final result. In other words, we're training a function NN for each individual sample $x_i$, such that our output for $X$ is $\frac{1}{n}\sum_{i=1}^nNN(X_i)$, where $X_i$ is the $i$'th row of $X$, for a total of $n$ rows. You can then train $NN$ by generating random samples from either distribution and labelling them $0,1$, and using something like binary-loss. Personally I don't see any advantage of using a convolutional or RNN architecture for this, because presumably your samples are iid, and the output should be invariant under permutations of the rows of $X$.
