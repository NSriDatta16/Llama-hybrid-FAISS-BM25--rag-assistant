[site]: datascience
[post_id]: 64684
[parent_id]: 
[tags]: 
Can BERT/ELMo be used (or retrained) to generate a text in both directions?

Text generation is perhaps one of the fun things to do with old NGram or new BERT/ELMo models. I am wondering can BERT be used to generate text from the end of a sentence, or better in both directions. That is instead of giving some starting words, we give some ending words.
