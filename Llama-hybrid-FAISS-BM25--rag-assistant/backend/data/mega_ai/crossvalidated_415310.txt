[site]: crossvalidated
[post_id]: 415310
[parent_id]: 415308
[tags]: 
First of all I would recomment this book here to you: Hands-On Machine Learning with Scikit-Learn and TensorFlow There you can read about overfitting, underfitting and the reasons. You can imagine overfitting as a process where the model remembers the training data to detailed and can not generalize to the test or validation data. In summary you can say that "simple" models tend to underfit. Because they can not "memorise" so well. Complex and bigger models tend to overfit. To fight overfitting you need regularisation or / and early stopping. In general you want to find a model that overfits (or increase the potential of the model so it overfits) and then apply regularisation. For gradient boosted trees I love to use lightGBM. Its awsome. Here you have so called hyperparameters (many) that can be canged to fight overfitting. Gradient Boosted trees can handle missing values and there is no need to feature select. And no need for feature scaling. Here is an example of lightGBM usage. Maybe you want to try this with your own data: https://github.com/PhilipMay/fraud-detection/blob/master/LightGBM_with_Hyperopt_no_oversampling_0.896.ipynb Deal with overfitting in lightGBM: https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html By the way: In my experience lightGBM is one of the best tools to do classification or regression with plain data. Better then neural networks. Neural Networks are better when it comes to Images, Time Series or sequence data or NLP. And lightGBM is like 1000 times faster then neural networks.
