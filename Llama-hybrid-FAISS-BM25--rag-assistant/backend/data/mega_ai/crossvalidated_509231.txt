[site]: crossvalidated
[post_id]: 509231
[parent_id]: 
[tags]: 
Train and test score - overfitting?

I have hourly time series data with a range of two years. I want to test my model when predicting my target variable (continuous) for a specific week. I'm doing the following: Splitting my data into training and test. (training data is all of the data before the week I want to predict) (testing data is the week I want to predict) I'm using GridSearchCV to find parameters with Cross-Validation (it splits the training data into combinations of training and validation data with CV). After I have the best parameters, I train my model with the training data (all of the data before the week I want to predict). Then I finally predict the final week (X_test) I'm getting good results, but how can I check for overfitting? After step 2, I've plotted the cv_results_['mean_train_score'] and cv_results_['mean_test_score'] from the GridSearchCV and got the following: (The 'test' in the plot is refering to the validation datas in the cross validation) Does this indicate overfitting? If it does, how does it happen since my test data (the week I predict later) is not even included in the training/cross-validation phase and how can I prevent it? I think the test/validation value is way lower than the training value because this is a time series and when we are doing cross-validation (splitting data in blocks), sometimes we are predicting the beginning (first days/months) with later data which probably affects the result. Edit : The plot I display here is BEFORE predicting the test week. That plot is obtained after GridSearchCV. When I predict my X_test (unseen week) I get a value of really low value of MAPE, which is very good. Does this discard overfitting? AFTER predicting my week I've got the following: from slearn import metrics print(metrics.r2_score(y_train,model.predict(X_train))) print(metrics.r2_score(y_test,model.predict(X_test))) 0.9693362391994333 #TRAIN 0.9558915555507633 #TEST Does this mean anything?
