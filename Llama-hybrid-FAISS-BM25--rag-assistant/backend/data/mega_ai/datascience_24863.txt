[site]: datascience
[post_id]: 24863
[parent_id]: 
[tags]: 
Connect output node to next hidden node in RNN

I'm trying to build a neural network with an unconventional architecture and a having trouble figuring out how. Usually we have connections like so, where $X=$ input, $H=$ hidden layer, $Y=$ output layer: $X_t \rightarrow H_t \rightarrow Y_t$ and $H_t \rightarrow H_{t+1}$ Normal Tensorflow or Keras nodes build above, where we end up with weight matrices connecting each component, i.e. $W_{xh}, W_{hh}, W_{hy}$. I would also like to introduce a connection: $Y_t \rightarrow H_{t+1}$, defined by a new weight matrix $W_{yh}$. I have looked at the Tensorflow and Keras implementations and there is a self.kernel ($W_{xh}$) and self.recurrent_kernel ($W_{hh}$), where the second is defined in terms of the previous hidden node's output prev_output = states[0] , defined here. Does anyone have a sense of how to access the output of the output layer in the subsequent step? Is it states[1] ? How are states defined, and where is that documented? Thanks for any insight you can give! Edit: Before anyone asks yes, I recognize that such a connection is not strictly necessary. I am trying to understand information flow through network nodes.
