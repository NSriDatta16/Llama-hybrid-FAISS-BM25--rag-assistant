[site]: crossvalidated
[post_id]: 289234
[parent_id]: 89792
[tags]: 
The amount of wiggle in your loss is related to the size of your batch. If you compute the loss over a larger batch size you should see a smoothened loss function. This means that if your validation loss was calculated on a larger batchsize, you won't see the sporadicity that you observe. Here is an excerpt from Stanford's CNN course notes The amount of “wiggle” in the loss is related to the batch size. When the batch size is 1, the wiggle will be relatively high. When the batch size is the full dataset, the wiggle will be minimal because every gradient update should be improving the loss function monotonically (unless the learning rate is set too high). You can also check out this link which gives some more insight on training loss: http://cs231n.github.io/neural-networks-3/
