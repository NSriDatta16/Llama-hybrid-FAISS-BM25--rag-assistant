[site]: crossvalidated
[post_id]: 218238
[parent_id]: 
[tags]: 
Relationship of the standard deviation of a distribution to a derived/calculated value of the distribution

I have a random variable $z$ for which I've calculated the sample mean $x = \frac{1}{n} \sum z_i$ and the sample standard deviation $s$. How can I calculate the standard deviation of $\frac{1,000,000}{x}$ ? Edit: More concise and accurate question thanks to Matthew Gunn Original verbose question: I have a data set/sample of values and I calculate the average and standard deviation in the end. Now I have a calculated/derived value of this data set/sample that is calculated as $1000000/x$ (with $x$ being the average in the end). Now the question is, what is the standard deviation of my derived value? I thought I could take the percentage of the standard deviation (standard_dev/average) and just multiple my derived average by it but that doesn't seem right... Is there any relationship between them at all or do I need to compute a new standard deviation by first converting my distribution to my derived/calculated value? (calculate $1000000/x$ for each value and then compute the standard deviation there) Why do I need this? I want to graph the results with error bars, and I want to graph my derived value as well as the original value. Further details/domain: This is benchmarking. I measure run times in Î¼s but want to also display/graph them in iterations per second ("how often could this be run within one second?" which is nice as higher -> better). And of course, I want to show error bars. Here is an example of values I got (markdown tables don't work here, hope it's good enough): | Name | Iterations per Second | Average | Standard Deviation | Standard Deviation Ratio | Median | | map.flatten | 1451.6712445317 | 688.8612030905 | 173.9981946453 | 0.2525881758 | 583 | | flat_map | 911.7426322002 | 1096.8007469244 | 77.6401735316 | 0.0707878562 | 1056 |
