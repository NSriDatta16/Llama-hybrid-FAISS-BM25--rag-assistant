[site]: crossvalidated
[post_id]: 471955
[parent_id]: 
[tags]: 
Intra-training loss as proxy for post-training loss

(not a great question title, I know) Background I'm training a neural net on a classification problem with Keras and am consistently noticing that during the training process, the test error is lower than the training error. I thought this was odd and might be unique to the data I'm working with. However, Keras' documentation has the following image, which leads me to believe it isn't uncommon: (the validation error does cross back over the training error towards the end of the training in the image, but imagine it didn't). At first glance, this seem to be contradictory to the core concept of statistical learning, that being that the training error will almost always underestimate the testing error. The below image, taken from 'The Elements of Statistical learning", shows this concept: At the end of my training process, I do see what I would expect; the training error is lower than the testing error when evaluated on the entire data sets (train and test respectively). Questions Given the above, I have a few questions: Is each point in the Keras loss plot the average loss for that epoch, averaged across each mini-batch in that epoch? I assume that the error in the second plot (ESL plot) is the loss evaluated over the entire data set (train and test, respectively). Is this the correct interpretation? If the second plot is the loss evaluated over the entire data sets, this leads me to believe that the difference between the intra-training train and test loss is not a good estimator of the post-training difference. Is this true? If the above is true, why is the intra-training error not a good estimator of the post-training error (evaluated on the entire data sets)?
