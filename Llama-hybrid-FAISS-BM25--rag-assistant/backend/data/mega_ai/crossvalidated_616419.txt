[site]: crossvalidated
[post_id]: 616419
[parent_id]: 419830
[tags]: 
No, not only does cross-validation make no sense here, but no analysis makes any sense at all. You cannot validly analyze 250 features on a sample size of 16. Cross-validation cannot validly help you with that. Let's go to the raw, absolute minimum: suppose you had only one predictor and you want to estimate an outcome with only 16 samples. That alone would be almost definitely an insufficient sample size for any valid conclusions. A sample is supposedly a representative sample from some larger population with the hope that you would want to extrapolate your findings to the larger population. To what kind of larger population could you validly extrapolate 16 cases? I do not know of any statistical test (not to talk of machine learning algorithm) for which a sample of 16 would be valid--and I'm still talking about the minimum case of just one predictor. 250 predictors make your sample 250 times as insufficient as for one predictor, so you can only imagine the magnitude of the problem. (That is why I am not even bothering to mention the possibility of dimension reduction--which you should probably do with 250 predictors--because that would not solve the problem even if you could reduce all 250 predictors into just one dimension.) CV would make the problem only worse by reducing your insufficient sample from 16 to something smaller. Even if you carried out leave-one-out cross-validation, each of the 16 training samples of size 15 would be invalid. 15 wrongs do not magically make a right. Sorry; this is probably not what you want to hear, but I think your best solution is to collect a truly large sample. Remember that as far as the computer is concerned, machine learning is just playing with numbers. If you give it data, it will try to give you a response according to some mathematical rules. The computer cannot tell you whether its response is good or completely worthless--only a proper study design can assure that.
