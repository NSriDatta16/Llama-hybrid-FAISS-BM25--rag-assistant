[site]: crossvalidated
[post_id]: 436319
[parent_id]: 
[tags]: 
Relationship between measurement precision and the precision of the mean of repeated measurements

Measurement precision can be improved by measuring the same quantity multiple times and taking the average. In other words, the average of several measurements of the same thing is more likely to be closer to the true value than a single measurement. The more measurements you average, the better the precision of the average -- the average of two measurements is more precise (i.e., probably closer to the true value) than one measurement. The average of three measurements should be more precise than the average of two, and so on. My question is, how much more precise? What is the mathematical relationship between the number of measurements averaged and the precision? In other words, how much more precise is the mean of $n$ measurements than a single measurement, where $n>1$ ? Given given some previously established precision metric (such as SD, CV, or 95% CI), how would one calculate the precision of measuring the same thing $n$ times and finding the mean of those measurements? My best stab at how to do this is the apply the standard error of the mean (SEM) formula using the documented SD and the number of measurements I'm taking as the sample size. I know that the SEM formula is to divide the population SD by the square root of the sample size, and if I understand correctly that tells you the SD of all possible sample means, i.e. the precision of the sample mean relative to the true mean. If the documented SD is based on large scale testing and can be regarded as the "population" SD (i.e. the actual measurement precision) for practical purposes, then it seems to me that dividing the documented SD by the square root of the number of measurements should give me the standard error of the mean of those measurements, which is the SD of that mean relative to the true mean, which in this case is what the measurement would be with zero random error (the actual quantity being measured plus the bias). Is that reasoning sound? If not, what's the correct approach to quantifying the uncertainty of x number of measurements, not based on the statistical spread of just those few measurements themselves, but based on the documented precision of the instrument? Here is a concrete example to illustrate what I'm asking: Suppose the documented SD of an electronic scale is 2.82g*. That tells me the probability distribution of a measurement with that scale. If I weigh an object once, and the result is 250g, there is a ≈68% probability that 250g is within 2.82g of the true weight, and a ≈95% probability that it's within 5.53g of the true weight (2.82 * 1.96), assuming a normal distribution. If I weigh the same object three times, and my results are 250g, 248g, and 255g, each of those measurements by itself has the same ≈68% probability of being with 2.82g of the true weight, and ≈95% probability of being within 5.53g. However, the mean of those three measurements (251) should have a higher precision than any one measurement by itself. What would be the precision of the mean of those three measurements, expressed in terms of SD? Note that I am not asking what is the SD of those three measurements, based on their variation from each other, or how to estimate the precision of the measurement instrument based on the sample of three measurements. I am asking what is the precision of the mean of those those measurements, expressed as SD, relative to the previously established measurement precision? If the precision in terms of SD for any one measurement by itself is 2.82g, how precise is the mean of three measurements? If one measurement has a ≈68% probability of being within 2.82g of the true value, the mean of three measurements of the same object has a ≈68% chance of being within how many grams of the true value? If I weigh the object four times, what would be the precision of the mean of those measurements? It should have a better precision than the mean of three measurements, just as the mean of three measurements should have a better precision than the mean of two measurements, and the mean of two measurements should be more precise than one measurement, since the theoretical precision should improve the more measurements are averaged. * Disregard any possible imprecision in the documented SD itself, or any issues that might make the precision of my scale different than the documented SD, such as calibration or drift, and assume that the precision is fixed throughout the measurement range. I'm stipulating that the documented SD is trusted to be for all intents and purposes the "population SD" for my scale, i.e. the SD that would theoretically be obtained from an infinite number of measurements of the same immutable object. I understand that those are issues to take into account in a practical situation, but my question is theoretical, so please disregard any and all what-ifs that might affect the answer in practice. I'm asking only about the mathematical relationship between the precision of the mean of multiple measurements of the same quantity and the precision of a single measurement. Based on my reasoning above, since the documented SD is for practical purposes being regarded as the population SD, I should be able to divide the documented SD by the square root of the number of measurements I'm taking, and that would tell me the SD of the mean of those measurements (again, not the SD of those measurements themselves, but as a means of expressing the probability distribution of their mean as an estimate of the true weight of the object). So, for three measurements, that would be $\frac{2.82g}{\sqrt{3}} \approx 1.63g$ , which would mean that the mean of three measurements would have a ≈68% probability of being within 1.63g of the true value, and a 95% CI of ±3.19. For five measurements, it would be $\frac{2.82g}{\sqrt{5}} \approx 1.26g$ , and a 95% CI of ±2.47g. Is this correct? If not, what is?
