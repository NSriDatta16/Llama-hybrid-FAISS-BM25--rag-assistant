[site]: crossvalidated
[post_id]: 299693
[parent_id]: 274317
[tags]: 
+1 to Tim's answer . There are quite a number of published "benchmarks" of forecast accuracy out there, but in my opinion, they are all misleading. Typically, they will be based on responses to informal surveys at practitioner forecasting conferences and similar, and if you think about things for a little, you will notice that industries are widely different, time series are widely different (high noise vs. low noise, strong seasonality vs. no seasonality, promotional or other external drivers or not, ...) and so forth. Plus people will even differ on how they calculate the MAPE ( Green & Tashman, 2009. Percentage Error: What Denominator? Foresight: The International Journal of Applied Forecasting , 12, 36-40 ). These differences mean that there simply cannot be useful external benchmarks for forecasting accuracy. I summarized my arguments in a little paper ( Kolassa, 2008. Can We Obtain Valid Benchmarks from Published Surveys of Forecast Accuracy? Foresight: The International Journal of Applied Forecasting , 11, 6-14 ) which may be helpful. Instead, I'd recommend that you look at internal benchmarks: throw different forecasting algorithms at your data and see how they compare. Consider averaging different forecasts, which often improves accuracy. Think about whether you have modeled all important drivers, like trend, seasonality or promotions. (Conversely, don't model weak effects, which may make your forecasts worse.) Look at badly forecasted series: why are they badly forecasted? Do they exhibit some kind of unmodeled regularity?
