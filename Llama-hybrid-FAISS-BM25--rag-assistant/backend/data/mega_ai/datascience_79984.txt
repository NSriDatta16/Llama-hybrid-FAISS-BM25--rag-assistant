[site]: datascience
[post_id]: 79984
[parent_id]: 74328
[tags]: 
This is the Neural Network - $\hspace{5cm}$ This is the equation - $\hspace{5cm}$$\frac{\partial J}{\partial w_{12}^{(2)}} = \frac{\partial J}{\partial h_{1}^{(3)}} \frac{\partial h_{1}^{(3)}}{\partial z_{1}^{(2)}} \frac{\partial z_{1}^{(2)}}{\partial w_{12}^{(2)}}$ $J$ = The calculated Loss $w_{12}^{(2)}$ - Weight for which the rate of Loss is to be calculated $h_{1}^{(3)}$ - Output after the Activation of Output Neuron [ This will be used with True value to get the Loss ] $z_{1}^{(2)}$ - This is Output before the Activation functions We create chain so that each individual partial derivative can be easily calculated and we get the derivative of two variables which are not directly connected i.e. inner layer weights and the Loss First term - Derivative of Loss w.r.t Output - $J$ = $y_{true}^2$ - ${h_{1}^{(3)}}^2$ [ Assuming a square loss ] Deriative will be - 2* ${h_{1}^{(3)}}$ Second term - Derivative of output after activation w.r.t Output before activation - $h_{1}^{(3)}$ = $z_{1}^{(2)}$ [ Assuming a Linear o/p activation ] Deriative will be - 1 If it is Sigmoid, Let's assume sigmoid as $f(x)$ $h_{1}^{(3)}$ = $f(z_{1}^{(2)})$ We know the derivative of $Sigmoid = f(x)(1 - f(x))$ Read here So the derivative is - $f(z_{1}^{(2)})$ (1 - $f(z_{1}^{(2)}))$ Third term - Derivative of output before activation w.r.t one of the participating three weights - $z_{1}^{(2)}$ = $h_{1}^{(2)}$ * $w_{11}^{(2)}$ + $h_{2}^{(2)}$ * $w_{12}^{(2)}$ + $h_{3}^{(2)}$ * $w_{13}^{(2)} + b$ First and last two terms will will 0. Deriative of mid term will be - $h_{2}^{(2)}$
