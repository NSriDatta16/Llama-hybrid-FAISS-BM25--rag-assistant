[site]: crossvalidated
[post_id]: 255678
[parent_id]: 255599
[tags]: 
$4$ points? Did you mean $4$ different problems that you want to link together and learn the same set of hyperparameters? Then your new objective function would be the sum of $4$ (log marginal) likelihoods (Instead of optimizing the each of the $4$ likelihoods independently). To do so one can use gradient descent (which searches for $0$ in the sum of the $4$ gradients). This is what the paragraph discusses. If you only have $4$ points just use the standard GP which involves optimizing a single likelihood function. There are ways that people have used the derivatives to treat ill-posed problems. But this is not what the paragraph is about.
