[site]: datascience
[post_id]: 24685
[parent_id]: 
[tags]: 
Confusion with cosine similarity

In information retrieval when we calculate the cosine similarity between the query features vector and the document features vector we penalize the unseen words in the query. Example if we have two documents with features vectors d1 = [1,1,1,0,0] d2 = [0,1,1,1,0] We can see that the two documents have the second feature so if we want to search for the second feature with query vector: q = [0,1,0,0,0] then the cosine similarity between q and d1,d2 will be $1/âˆš3$, and not 1 because that we penalize the other features that we have not mention in the query. From this discussion I don't understand why penalize it is a good Idea. Is penalizing unseen features good? Is there another similarity measure that does not penalize them?
