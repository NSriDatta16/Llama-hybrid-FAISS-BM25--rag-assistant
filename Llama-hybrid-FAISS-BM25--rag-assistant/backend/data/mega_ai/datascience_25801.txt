[site]: datascience
[post_id]: 25801
[parent_id]: 24860
[tags]: 
I would suggest you to split your problem into two: How to train? How to make inference when resources are limited? Very common pattern is to train model on large data (by renting AWS server, for instance) and then use its predictions to train much smaller network. Here is fundamental paper on the subject from Hinton et al. This way you can stick to more conventional object detection method without stacking involved and then deploy 'squeezed' network to your device. But back to your question. If you are rigid in your approach for various reasons and can't do like I described above, then lots of end-to-end approaches are available in literature, take a look at this to get started. And even code . I will just cite the paper: A unified deep neural network, denoted the multi-scale CNN (MS-CNN), is proposed for fast multi-scale object detection. The MS- CNN consists of a proposal sub-network and a detection sub-network. In the proposal sub-network, detection is performed at multiple output layers, so that receptive fields match objects of different scales. These complementary scale-specific detectors are combined to produce a strong multi-scale object detector. The unified network is learned end-to-end, by optimizing a multi-task loss. Feature upsampling by deconvolution is also explored, as an alternative to input upsampling, to reduce the memory and computation costs. State-of-the-art object detection performance, at up to 15 fps, is reported on datasets, such as KITTI and Caltech, containing a substantial number of small objects. But nevertheless feel free to experiment with network compression (take a look at this ). It typically has high degree of redundacy, and by being creative in exploring this redundacy it's possible to have many gains
