[site]: crossvalidated
[post_id]: 271514
[parent_id]: 271500
[tags]: 
Firstly, see the answer to this post Furthermore, to expand on that answer, if all hidden neurons and subsequently all output neurons have the same activation values, there would only be $k$ different updates for the weights between the hidden and the output layer where $k$ is the number of output neurons. This is easily understandable by looking at the update rule. This is bad because the power of neural networks comes from their ability to associate different weights to different neurons (parameters) and by initializing the network with equal weights, you are forcing it to yield groups of same-valued weights.
