[site]: crossvalidated
[post_id]: 61369
[parent_id]: 60743
[tags]: 
this paper might be of interest: http://arxiv.org/pdf/0906.4032v1.pdf It gives a nice summary of some frequentist and Bayesian approaches to the two sample problem, and discusses both the parametric and nonparametric cases. It might add something to the other answers to give a simple example. Say you have two data sets $\mathbf{x}$ and $\mathbf{y}$ where each $x_i$ and each $y_j$ is either a $0$ or a $1$. You assume an iid Bernoulli model in both cases, so each $x_i\sim Bern(p)$ and each $y_i\sim Bern(q)$. Your hypothesis testing scenario in both the frequentist and Bayesian settings can be: $\mathcal{H}_0: \: \: p=q$ $\mathcal{H}_1: \: \: p,q$ not necessarily equal. The likelihoods for the data in each case are: Under $\mathcal{H}_0$ : $L_0(p) = f(\mathbf{x},\mathbf{y};p) = \prod_i p^i (1-p)^{1-i} \prod_j p^j(1-p)^{1-j}$ Under $\mathcal{H}_1$ : $L_1(p,q) = f(\mathbf{x},\mathbf{y};p,q) = \prod_i p^i (1-p)^{1-i} \prod_j q^j(1-q)^{1-j}$ (since under $\mathcal{H}_0 \:\: q=p$). A frequentist approach to the problem might be to do a Likelihood ratio test, whereby you calculate the statistic: $W = -2\log\left\{ \frac{L_0(p_{max})}{L_1(p_{max},q_{max})}\right\},$ where $p_{max},q_{max}$ denote the maximum likelihood estimates for $p$ and $q$ under the relevant hypothesis (so $p_{max}$ in the numerator may not be the same as $p_{max}$ in the denominator). $W$ asymptotically follows a $\chi^2_1$ distribution (see e.g. Pawitan, 2001), so you would specify a significance level and reject/fail to reject $\mathcal{H}_0$ as appropriate. Traditionally, in the Bayesian approach the test statistic would be the Bayes factor. First you would assume some relevant priors $p\sim \pi_0$ under $\mathcal{H}_0$ and $p,q\sim \pi_1$ under $\mathcal{H}_1$. The Bayes factor is the ratio of marginal likelihoods, given by: $BF = \frac{ f(\mathbf{x},\mathbf{y}|\mathcal{H}_0) }{f(\mathbf{x},\mathbf{y}|\mathcal{H}_1)} = \frac{ \int_0^1 L_0(p)\pi_0(p)dp}{\int_0^1 \int_0^1 L_1(p,q)\pi_1(p,q)dpdq}$. The Bayes factor can be combined with some prior beliefs on the probability of $\mathcal{H}_0$ or $\mathcal{H}_1$ being true, to give the probability of $\mathcal{H}_0$ versus $\mathcal{H}_1$ after seeing the data . If we assume apriori that each hypothesis is equally likely, so $p(\mathcal{H}_0)=p(\mathcal{H}_1) = 1/2$, then this gives: $\frac{p(\mathcal{H}_0|\mathbf{x},\mathbf{y})}{p(\mathcal{H}_1|\mathbf{x},\mathbf{y})} = BF \times \frac{p(\mathcal{H}_0)}{p(\mathcal{H}_1)} = BF \times \frac{1/2}{1/2} = BF.$ Intuitively, if this ratio is $>1$, then the posterior probability of $\mathcal{H}_0$ is larger than $\mathcal{H}_1$, so you would say that $\mathcal{H}_0$ has a higher probability of being true under these assumptions for the prior and model. One nice thing about the Bayes factor is how it automatically penalises more complex models (such as $\mathcal{H}_1$ here). A nice paper offering some more intuition is here: http://quasar.as.utexas.edu/papers/ockham.pdf . Hope that helps along with the other answers already posted.
