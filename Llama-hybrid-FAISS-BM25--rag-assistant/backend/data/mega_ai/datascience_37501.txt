[site]: datascience
[post_id]: 37501
[parent_id]: 37488
[tags]: 
The original paper does a lot of hand-waving on the implementation of inference step and is not clear. So your confusion is justified. I'll explain at high level below. I'm assuming only PV-DBOW model. Training Phase In this model, we forget word ordering information and setup a very simple neural network. Represent all input document tags in a vocabulary to get unique ID for each document tag. Do the same for words. For a given document ID, the input is one-hot encoded representation of document tag IDs. Output is one hot encoded representation of a randomly selected word. We want to setup training such that for a given tag ID, a randomly selected word from that document will be predicted with high probability. So the neural network getting trained transforms one hot encoded representation to document/tag vector. The tag vector is passed through another layer with softmax at the output. Both set of weights are adjusted during training phase. What training achieves: It represents all document tags in a new space such that probabilities for randomly selected words in each document are maximized starting from that vector space representation to softmax output. Important to note that there are two set of weights . Input to hidden layer and hidden to softmax output. Inference Stage Words are unique across document. So a word 'India' in training and inference stage gets mapped to same ID in vocabulary. But there's not such concept for document ID since documents are assumed to be unique and there's no shared ID between training and inference. Inference stage runs a (sort of) reverse calculation. What vector space representation is most appropriate for this document, if I use the same set of weights from hidden space to output layer? The weights from hidden layer to softmax output are kept constant! Select a word from new document at random. Start with a random representation for the document vector (input to hidden layer). Pass it through neural network from hidden to softmax output with constant weights (learnt during training). Adjust the randomly initialized weights such that softmax probability is maximized for the selected word. Repeat this process many times. This is the reason why you need stochastic gradient descent and need to specify number of steps. Summary Document IDs are not common from training and inference stage (like word IDs in w2v). There are two set of weights, keep one constants during inference stage based on learning from training phase. Adjust the other using SGD. You mapping new document to document vector space such that documents which had similar words during the training phrase are close in the document vector space. This is a very good tutorial on the implementation by Richard Berendsen . I have ignored additional complexities like negative sampling, context window from the explanation.
