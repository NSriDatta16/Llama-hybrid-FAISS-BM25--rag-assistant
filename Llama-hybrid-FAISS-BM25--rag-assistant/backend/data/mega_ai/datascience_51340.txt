[site]: datascience
[post_id]: 51340
[parent_id]: 51330
[tags]: 
What you are looking for is called "decision boundary", which is the set of (extreme) points laying on the boundary between classes. Decision boundary of NB is a set of points $\boldsymbol{x}$ that satisfy at least one of these $K(K-1)/2$ conditions $$i\neq j \in [1, K]:{\Bbb P}(\boldsymbol{x}, C_i)={\Bbb P}(\boldsymbol{x}, C_j) \overset{\forall k}{\geq} {\Bbb P}(\boldsymbol{x}, C_k)$$ Generally, Naive Bayes does not learn an explicit decision boundary (specially when categorical features are involved), however, for example here is the derivation of NB boundary for continuous variables when ${\Bbb P}(\boldsymbol{x}|C_i)$ is chosen from exponential family (e.g. a Gaussian). Nonetheless, you could still select a set of random points, then assign a class to each, and finally plot them using something like t-SNE to get a sense of decision boundaries. For example, a tool like this interactive t-SNE plot would be helpful: Or a library like mlxtend.plotting that carries out a similar procedure: On the contrary, models like multi-class logistic regression and SVM both have explicit decision boundaries, which are equations of learned parameters $\boldsymbol{w}$ such as $$w_0+w_1\text{fat}+w_2\text{salt}+w_3\text{sugar}=0$$ that you can access via common libraries.
