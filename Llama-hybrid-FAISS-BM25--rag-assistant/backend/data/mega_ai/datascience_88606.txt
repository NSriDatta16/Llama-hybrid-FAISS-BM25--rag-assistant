[site]: datascience
[post_id]: 88606
[parent_id]: 88572
[tags]: 
In my opinion, you need to understand the models, plots, calculation routines to be able to draw the right conclusions out of it. There is absolutely no use to have, let's say a Q-Q-Plot, and lack the statistic knowledge to interpret it. A few other examples: "cleaning" data for nice and round-up plotting requires either good knowledge of the data or else, if done automatically, assumes statistical properties such as normality, non-collinearity etc. An automatic algorithm either introduces a large bias by transforming the data or will give you a false impression or relations/correlations etc. F.i. when checking for (multi-)collinearity of data, you can use the correlation coefficient. This is typically the Pearson-correlation. But Pearson is only for linearly related data. Now you most often have non-linearly related data, so Pearson coeff. is low and you say: Ok, no collinearity. Instead, with proper statistic knowledge, you can select the correct coefficient and then extend the analysis with other measures such as VIF, condition index, partial pairwise correlation etc... also to know what you can get out of data visualization, you need to know at least the basic set of data visualization methods. F.i. when you want to analyse sales dependig on the size of your stores . But now all your large stores are in low income regions. Plotting a scatter plot of it you see no relation/correlation. So your assumption is: There is no use to build larger stores. But in fact, the interaction between store size and average regional income is concealing the "real" influence. You need to know which methods are good to find this "hidden" information and how to apply them. And, in my opinion, finding this "hidden" information is what discerns a real data scientist from the nowadays everyone-is-a-datascientist-hipster predictions are highly biased towards preprocessing of input data, the type of the model and the model parameters. You almost never find the best model on the first try. last but not least: If something is "super robust" you can be 99% certain, that some assumptions have been made, introducing bias, cutting outliers which are not outliers, applying overly strong regularizations/transformations etc... If something is super robust : Either know why it is super robust and have the comparison to non-"super"-robust, or be overly skeptical about the resulting data quality. Robustness is never for free , so you should better know the backgrounds of it. In my opinion, the problem is that everyone wants to be a data scientist, but no one wants to learn statistics. So we've got tons of data scientist university courses and degrees of some sub-prime universities doing data science name-dropping, but now covering the statistic background. So everyone is applying something but barely anyone knows what he/she is doing. It has come so far, that I've seen two "real" statistic oriented data scientists with a master in computer science or engineering, both having a quite solid statistical background and working with data science on a day-to-day basis (but have no degree explicitly stating "Data Scientist"), being told when applying for a data scientist job: "You've got a too low experience with data science, we've hired someone with a bachelor degree in data science." From which you know exactly: Those hiring guys have no idea about data science and they just want to get "something" done... Well... This is Germany and we are known for having an extremely unflexible job market... ;) So: Learn your statistics or stop "being" a data scientist. :)
