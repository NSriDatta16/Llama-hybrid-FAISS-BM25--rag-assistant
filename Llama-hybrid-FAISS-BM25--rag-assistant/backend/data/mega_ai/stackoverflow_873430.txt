[site]: stackoverflow
[post_id]: 873430
[parent_id]: 872563
[tags]: 
Ok, I found another algorithm: the alias method (also mentioned in this answer ). Basically it creates a partition of the probability space such that: There are n partitions, all of the same width r s.t. nr = m . each partition contains two words in some ratio (which is stored with the partition). for each word w i , f i = ∑ partitions t s.t w i ∈ t r × ratio(t,w i ) Since all the partitions are of the same size, selecting which partition can be done in constant work (pick an index from 0...n-1 at random), and the partition's ratio can then be used to select which word is used in constant work (compare a pRNGed number with the ratio between the two words). So this means the p selections can be done in O(p) work, given such a partition. The reason that such a partitioning exists is that there exists a word w i s.t. f i , if and only if there exists a word w i' s.t. f i' > r , since r is the average of the frequencies. Given such a pair w i and w i' we can replace them with a pseudo-word w' i of frequency f' i = r (that represents w i with probability f i /r and w i' with probability 1 - f i /r ) and a new word w' i' of adjusted frequency f' i' = f i' - (r - f i ) respectively. The average frequency of all the words will still be r, and the rule from the prior paragraph still applies. Since the pseudo-word has frequency r and is made of two words with frequency ≠ r, we know that if we iterate this process, we will never make a pseudo-word out of a pseudo-word, and such iteration must end with a sequence of n pseudo-words which are the desired partition. To construct this partition in O(n) time, go through the list of the words once, constructing two lists: one of words with frequency ≤ r one of words with frequency > r then pull a word from the first list if its frequency = r, then make it into a one element partition otherwise, pull a word from the other list, and use it to fill out a two-word partition. Then put the second word back into either the first or second list according to its adjusted frequency. This actually still works if the number of partitions q > n (you just have to prove it differently). If you want to make sure that r is integral, and you can't easily find a factor q of m s.t. q > n , you can pad all the frequencies by a factor of n , so f' i = nf i , which updates m' = mn and sets r' = m when q = n . In any case, this algorithm only takes O(n + p) work, which I have to think is optimal. In ruby: def weighted_sample_with_replacement(input, p) n = input.size m = input.inject(0) { |sum,(word,freq)| sum + freq } # find the words with frequency lesser and greater than average lessers, greaters = input.map do |word,freq| # pad the frequency so we can keep it integral # when subdivided [ word, freq*n ] end.partition do |word,adj_freq| adj_freq
