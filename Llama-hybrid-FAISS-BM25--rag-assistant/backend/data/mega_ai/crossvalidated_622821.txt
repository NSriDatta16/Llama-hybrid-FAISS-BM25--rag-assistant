[site]: crossvalidated
[post_id]: 622821
[parent_id]: 
[tags]: 
How do I incorporate sample size as a random variable in a latent interval censored model

I've got a dataset wherein only every third success in a series of n trials is observed. I had originally used the following Bayesian model: $$ y_i \sim \mathtt{floor}(z_i / 3) \\ z_i \sim B(n_i, p_z) \\ p_z \sim \text{Beta}(\alpha, \beta) $$ Where $y_i$ is the number of observed successes and $z_i$ is the latent count of all successes. However, when considering my data, I realized that in most cases, the trials behaved like a negative binomial distribution and stopped as soon as $y_i$ was 1 but occasionally continued on (I'd estimate about 90% of the time it ends with the success, but the dataset doesn't include this information). After some thought I came up with the model: $$ y_i \sim \mathtt{floor}(z_i / 3) \\ z_i = \begin{cases} w_i & w_i 5 | u_i = 0 \\ 3 & \text{otherwise} \end{cases} \\ w_i \sim B(n^*_i, p_z) \\ u_i \sim B(1, p_u)\\ n^*_i \sim \text{Geo}(p_n) + 1 \\ n^u_i \sim \text{trunc}(\text{NB}(3, p_z), \text{upper} = \text{max}(n_i^* - 3, 0)) + 3\\ n_{obs} = \begin{cases} n_i^* & w_i 5 | u_i = 0 \\ n_i^u & \text{otherwise} \end{cases} \\ p_z \sim \text{Beta}(\alpha_z, \beta_z) \\ p_u \sim \text{Beta}(\alpha_u, \beta_u) \\ p_n \sim \text{Beta}(\alpha_n, \beta_n) $$ Where, as before, $y_i$ is the number of observed successes and $z_i$ is the latent count of all successes, but that there was some probability $p_u$ $(p_u = p(u_i = 1))$ that would cause trials to end abruptly in which case the number of trials wouldn't be the preallocated $n_i^*$ but the observed $n_i^u$ which is a shifted, truncated negative binomial random variable representing when the final success occured. The observed sample size, $n_{obs}$ would then be a random variable that would be the full preallocated $n_i^*$ unless $u_i = 1$ and $y_i = 1$ . I was feeling pretty clever until I realized I couldn't figure out the complete conditional for $u_i$ as it was now a dependency for $n_{obs}$ and I was in over my head. I tried to sidestep the issue with JAGS but I couldn't figure out how to reparameterize $n_{obs}$ so that it wasn't deterministic (as observed data has to be stochastic). model { for(i in 1:N){ y[i] ~ dinterval(z[i], breaks) z[i] = 6, w[i], 3) u[i] ~ dbern(p_u) n_star_fail[i] ~ dnegbin(p_n, 1) n_star[i] = 6, n_star[i], n_u[i]) n[i] ~ dbin(1, n_get[i]) } p_u ~ dbeta(1, 1) p_n ~ dbeta(1, 1) p_z ~ dbeta(1, 1) } data $drops, n = dat_mod$ kills, N = 16249 ) inits How can I model this? Do I need to rethink what I've done or is there a math trick that would make this simpler? Dataset can be found here . For context it's the kill and drop rates for some new items in Old School RuneScape with an interesting drop mechanic I was hoping to learn more about. Data was scraped from collectionlog.net such that the players included were sampled [mostly] independently of whether or not they recieved the drop. Most players stop killing bosses once they get the drop so it has presented quite the modeling conundrum.
