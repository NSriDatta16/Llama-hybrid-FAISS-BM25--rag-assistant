[site]: crossvalidated
[post_id]: 294761
[parent_id]: 294756
[tags]: 
If there are a lot of parameters to optimize like in deep neural network, Hessian can be computationally expensive. Here is Mark's comment in this question. Is that true Newton's Method / Quasi Newton Method are not widely used in deep neutral network training? The L, as in L-BFGS, can deal with a larger number of variables than BFGS. L stands for limited memory, and the Hessian approximation is never explicitly formed with L-BFGS., rather, some small number of the most recent gradients are retained to allow the computations needed with the Hessian approximation. But L eventually runs out of gas as well.
