[site]: datascience
[post_id]: 114457
[parent_id]: 114455
[tags]: 
UPDATE (I WAS WRONG) Maximizing correlation misses a lot and makes for a terrible function to optimize. For instance, correlation will not detect if you consistently predict too high or too low. For every real $a$ : $$ corr(y, \hat y) = corr(y, a+\hat y) $$ In fact, for any real $a$ and positive $b$ : $$ corr(y,\hat y) = corr(y,a+b\hat y) $$ Concretely, if your true values are $y=(1,2,4,6,5)$ , $\hat y=(11, 21, 41, 61,51)$ makes for terrible predictions but does have a perfect correlation with $y$ . (In this example, $a=1$ and $b=10$ .) Minimizing square loss implies a maximization (not minimization) of correlation between observed and predicted values, but the reverse implication does not apply. However, since minimizing square loss implies a maximization of correlation (at least in linear models), you can minimize square loss and get maximized correlation. Correlation will not be perfect unless square loss is zero, but you can still maximize the correlation in the sense that no other regression coefficients will do better (though some will result in equal correlation but worse or equal square loss). Minimizing square loss is what happens in ordinary least squares linear regression and is completely standard in statistics and machine learning.
