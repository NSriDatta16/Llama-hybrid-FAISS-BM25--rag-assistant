[site]: crossvalidated
[post_id]: 92496
[parent_id]: 
[tags]: 
Likelihood vs. Probability

I have difficulties with Likelihoods . I do understand Bayes' Theorem $$p(A|B, \mathcal{H}) = \frac{p(B|A, \mathcal{H}) p(A|\mathcal{H})}{p(B|\mathcal{H})}$$ which can be directly deduced from applying $p(A,B) = p(B) \cdot p(A|B) = p (A) p(B|A) = p(B,A)$. Thus in my interpretation, the $p(\cdot)$ functions in Bayes Theorem are somehow all probabibilities, either marginal or conditional. So I have actually thought that Likelihood as a concept was more of a frequentist view of the inverse probability. However, I have now repeatedly seen statements in Bayesianists' books that say that the likelihood is not a probability distribution. Reading MacKay's book yesterday, I stumbled over the following statement "[...] it is important to note that the terms likelihood and probability are not synonyms. The quantity $P(n_b|u,N)$ is afunction of both $n_B$ and $u$. For fixed $u$, $P(n_b|u,N)$ defines a probability over $n_B$, for fixed $n_B$, $P(n_B|u,N)$ defines the likeihood of $u$." I understand this as follows: $p(A|B)$ is a probability of $A$ under given $B$, thus a function $\text{probability} : \mathcal{A}\to [0,1]$. But considering a given value $a \in A$ and evaluating $p(A=a|B)$'s dependency on different $b\in\mathcal{B}$'s we are actually using a different function $L : \mathcal{B}\to[0,1]$. Is this interpretation correct? Can one then say that maximum likelihood methods could be motivated by the Bayesian theorem, where the prior is chosen to be constant?
