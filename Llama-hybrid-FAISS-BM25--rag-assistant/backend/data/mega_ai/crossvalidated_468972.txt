[site]: crossvalidated
[post_id]: 468972
[parent_id]: 
[tags]: 
Large action and state spaces in reinforcement learning

In an MDP, I have an action space and a state space that are very large (exponential), in the order of $\mathcal{O}(m^n)$ for some $n$ and $m$ . I know that in such a situation, Deep Reinforcement Learning (DRL) uses function approximation to deal with such large spaces. For example, Deep Q Learning (DQL) uses Deep Q Network (DQN) to approximate the value function Q. When I was trying to implement DQL using DQN I noticed that I need to specify the action and state spaces. However, just by specifying them, it takes a lot of time, especially when $m$ and $n$ are large. Am I missing something or it is just it and we have to specify these large states anyway?
