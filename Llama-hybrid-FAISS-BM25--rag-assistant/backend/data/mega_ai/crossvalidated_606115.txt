[site]: crossvalidated
[post_id]: 606115
[parent_id]: 606024
[tags]: 
In principle we will have to handle ties in the data, so let's adopt a convenient notation. Let the data have values $x_1\lt x_2 \lt \cdots \lt x_d$ where the value $x_k$ appears with multiplicity $\nu_k \ge 1.$ Write $n_k = \nu_1 + \nu_2 + \cdots + \nu_k$ for the partial sums, so that $n = \nu_d$ is the dataset size. You describe a random variable created by sampling $m = n$ values $X_1,X_2,\ldots, X_m$ from these data (with replacement) and computing their $q = 1/2$ quantile $X.$ Given any index $k,$ what is the chance that $x_k$ is this quantile? Let's say that a "success" at step $i$ is that $X_i \le x_k.$ This event has probability $n_k/n.$ Because $X \le x_k$ exactly when there have been $qm$ or more successes when drawing the sample of size $m,$ $$\Pr(X \le x_k) = S\left(qm; m, n_k/n\right)$$ where $S$ is the cumulative Binomial survival function (computable as an incomplete Gamma integral or, for small $m,$ directly using Binomial coefficients). Abbreviating the right hand side as $p_k,$ we obtain $\Pr(X = x_k) = p_k - p_{k-1}$ and the expectation is $$E[X] = \sum_{i=1}^k x_k\left(p_k - p_{k-1}\right).$$ This is (explicitly) a linear combination of the order statistics of the original $n$ data values $y_1, y_2, \ldots, y_n$ where the coefficient of $y_i$ is $(p_k-p_{k-1})/\nu_k$ when $y_i = x_k.$ The highest weights tend to be concentrated near the $q$ quantile of the data. To illustrate, on the left is a bar chart of $n=24$ data showing $(x_k, \nu_k).$ In the middle is a plot of the distribution $(x_k, p_k - p_{k-1})$ for $q = 30\%.$ This shows how the weights are concentrated around a few data values. At the right is a histogram of the $30^\text{th}$ percentiles of a thousand independent bootstrap samples (using the default quantile calculation in R ). The difference between the average of these thousand quantiles and the computed expectation is insignificant ( $Z = -0.005$ ). In an application with no ties likely and predictable dataset sizes $n$ (such as the five to ten in the question) you can precompute the $p_k$ for each possible $n,$ making the calculation extremely efficient. If there aren't too many ties in any case you wouldn't be wrong to just sort the data and use the precomputed $p_k.$ This R code generated the figures. # # Make some data. # set.seed(17) nu
