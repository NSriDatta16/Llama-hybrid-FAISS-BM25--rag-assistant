[site]: crossvalidated
[post_id]: 234578
[parent_id]: 207794
[tags]: 
If you are using keras, just put sigmoids on your output layer and binary_crossentropy on your cost function. If you are using tensorflow, then can use sigmoid_cross_entropy_with_logits . But for my case this direct loss function was not converging. So I ended up using explicit sigmoid cross entropy loss $(y \cdot \ln(\text{sigmoid}(\text{logits})) + (1-y) \cdot \ln(1-\text{sigmoid}(\text{logits})))$ . You can make your own like in this Example Sigmoid, unlike softmax don't give probability distribution around $n_{classes}$ as output, but independent probabilities. If on average any row is assigned less labels then you can use softmax_cross_entropy_with_logits because with this loss while the classes are mutually exclusive, their probabilities need not be. All that is required is that each row of labels is a valid probability distribution. If they are not, the computation of the gradient will be incorrect.
