[site]: crossvalidated
[post_id]: 4051
[parent_id]: 1164
[tags]: 
So 'classical models' (whatever they are - I assume you mean something like simple models taught in textbooks and estimated by ML) fail on some, perhaps many, real world data sets. If a model fails then there are two basic approaches to fixing it: Make fewer assumptions (less model) Make more assumptions (more model) Robust statistics, quasi-likelihood, and GEE approaches take the first approach by changing the estimation strategy to one where the model does not hold for all data points (robust) or need not characterize all aspects of the data (QL and GEE). The alternative is to try to build a model that explicitly models the source of contaminating data points, or the aspects of the original model that seems to be false, while keeping the estimation method the same as before. Some intuitively prefer the former (it's particularly popular in economics), and some intuitively prefer the latter (it's particular popular among Bayesians, who tend to be happier with more complex models, particularly once they realize they're going to have use simulation tools for inference anyway). Fat tailed distributional assumptions, e.g. using the negative binomial rather than poisson or t rather than normal, belong to the second strategy. Most things labelled 'robust statistics' belong to the first strategy. As a practical matter, deriving estimators for the first strategy for realistically complex problems seems to be quite hard. Not that that's a reason for not doing so, but it is perhaps an explanation for why it isn't done very often.
