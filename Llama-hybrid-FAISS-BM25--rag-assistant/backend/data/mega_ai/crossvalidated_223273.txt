[site]: crossvalidated
[post_id]: 223273
[parent_id]: 223258
[tags]: 
Let $x$ be the original time series and $x_m$ be the result of smoothing with a simple moving average with some window width. Let $f(x, \alpha)$ be a function that returns a smoothed version of $x$ using smoothing parameter $\alpha$. Define a loss function $L$ that measures the dissimilarity between the windowed moving average and the exponential moving average. A simple choice would be the squared error: $$L(\alpha) = \|x_m - f(x, \alpha)\|^2$$ If you want the error to be invariant to shift/scaling, you could define $L$ to be something like the negative of the peak height of the normalized cross correlation. Find the value of $\alpha$ that minimizes $L$: $$\underset{\alpha}{\min} L(\alpha)$$ Here's an example using a noisy sinusoidal signal and the mean squared error as the loss function: Another example using white noise as the signal: The loss function appears to be well behaved and have a single global minimum for these two different signals, suggesting a standard 1d optimization solver could work (as I used to select $\alpha$ here). But, I haven't verified that this must be the case. If in doubt, plot the loss function and use a more sophisticated optimization method if necessary. Edit : Here's a plot of the optimal alpha (for exponential smoothing) as a function of window size (for simple moving average). Plotted for each of the signals shown above.
