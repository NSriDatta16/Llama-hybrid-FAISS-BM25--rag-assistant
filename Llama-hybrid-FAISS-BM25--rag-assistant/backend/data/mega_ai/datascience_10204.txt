[site]: datascience
[post_id]: 10204
[parent_id]: 
[tags]: 
Should I take random elements for mini-batch gradient descent?

When implementing mini-batch gradient descent for neural networks, is it important to take random elements in each mini-batch? Or is it enough to shuffle the elements at the beginning of the training once? (I'm also interested in sources which definitely say what they do.)
