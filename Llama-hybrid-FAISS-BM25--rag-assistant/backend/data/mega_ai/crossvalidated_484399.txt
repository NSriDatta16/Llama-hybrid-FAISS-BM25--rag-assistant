[site]: crossvalidated
[post_id]: 484399
[parent_id]: 484378
[tags]: 
Before answering your question, let's first explain some basic Bayesian mindset. In Bayesian statistics, everything is a random variable, the only difference between these random variables is whether they are observed or hidden . Say for example if you believe $X$ follows a distribution defined by $\theta$ , denote $$ X \sim P(X|\theta) $$ Where $\theta$ is the parameter of the distribution, from Bayesian perspective it's also a random variable. Usually in this case random variable $X$ is observed and $\theta$ is not, and you want to infer/learn/esitmate $\theta$ based on your observations. In such situations there's no matter of "prior", "marginal" or "posterior" The term "prior", "marginal" or "posterior" matters when you believe $\theta$ follows some other distribution $$ \theta \sim P(\theta|\gamma) $$ Then we call this "other distribution" the prior , more specifically it's the piror distribution for $\theta$ . Among all three random variables $X$ , $\theta$ and $\gamma$ , usually $X$ and $\gamma$ are observed, $\theta$ is not, and you want to estimate $\theta$ based on the observed $X$ and $\gamma$ . So yes the term "prior" is usually on hidden random variables, of course you can believe there's a prior distribution for $\theta$ even when it is observed, but usually nobody do so(why would anyone esitimate something that is already observed?). And, if you can't observe $\gamma$ , you can even assume $\gamma$ follows a distribution defined by another random variable $\eta$ , then $P(\gamma | \eta)$ will be the prior for $\gamma$ . Hope this answers your question regarding to "prior". Now let's talk about "marginal". In previous example people usually interested in the distribution of $X$ (while $\theta$ is hidden), given $\gamma$ , the distribution $$ X \sim P(X|\gamma) $$ is called the "marginal distribution". The term "marginal" came from the fact that $P(X|\gamma)$ is acquired by marginalizing out $\theta$ from the joint distribution: $$ p(X|\gamma) = \int_\theta p(X|\theta)p(\theta|\gamma) $$
