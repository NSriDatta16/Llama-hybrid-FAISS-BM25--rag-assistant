[site]: datascience
[post_id]: 108875
[parent_id]: 
[tags]: 
No feedback in Transformers

Newbie question about transformers. I am referring to the paper https://arxiv.org/pdf/1706.03762.pdf . Figure 1 (bottom-right) says: "Outputs (shifted right)". To me, during generation (not training), the n-th output of the network seems to be computed by consuming the n-1-th output, and possibly earlier output entries as well. Is my understanding correct? If this is the case, that would be a feedback, no? But then why transformers are said to be feed-forward? Is it because the output is "read-only" during training? Is it correct to say that transformers have feedback at generation time?
