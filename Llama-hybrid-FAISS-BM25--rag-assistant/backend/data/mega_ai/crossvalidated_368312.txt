[site]: crossvalidated
[post_id]: 368312
[parent_id]: 367962
[tags]: 
What you have in the OP is not exactly true, but there is a correct way to do this sort of thing. Suppose that I have parameters $\theta_1, \ldots, \theta_N \sim g(\theta \mid\eta)$ given $\eta$ , and I use a prior $\eta \sim F$ . Then the joint distribution of the $\theta$ 's after integrating out $\eta$ is $$ m(\pmb \theta) = \int \left(\prod_{i = 1}^N g(\theta_i \mid \eta) \right) \, F(d\eta). \tag{$\dagger$} $$ A "folk-theorem" in MCMC is that chains based on $(\dagger)$ leads to better mixing, i.e., it is better to marginalize things out if you can. It is not strictly true, as there are some situations where parameter-expansion ideas lead to better mixing, and it also matters what exactly you do with $m(\pmb \theta)$ to sample it, but it is a useful rule of thumb. So, for example, it would be valid, in STAN to do target += log_m(theta) assuming you can write a function log_m that computes the log of $(\dagger)$ . That's the tricky part, of course, since log_m may not be easy to evaluate to a given accuracy. Actually, this is how the makers of STAN recommend that one incorporates discrete parameters, because HMC cannot deal with discrete parameters directly. They recommend summing out the discrete parameter, which is equivalent to $(\dagger)$ when $\eta$ is discrete (e.g., it might be a mixture component indicator).
