[site]: crossvalidated
[post_id]: 524665
[parent_id]: 524599
[tags]: 
You have essentially answered your own question. In a nutshell, models with random slopes are often not supported by the data. It's as simple as that. In principle, random slopes often seem like a good idea because it is easy to argue that different subjects may respond differently to an intervention, or other exposure(s), and that's precisely what random slopes are supposed to capture. The problem is that in practice, random slopes often lead to an overfitted model, which means that if the model converges, it is often to a singular fit. Even if the random structure is not over-specified, the more complex the model, the more likely it is for the software to have problems finding a/the solution. Generalised linear mixed models (I assume you are talking about a GLMM based on your earlier question) are very complex models to begin with, so when the random structure is complex this makes the problem even harder. Also, bear in mind that most frequentist approachhes to fitting mixed models assume that the random effects are multivariate normal - which in may be very far from the truth. With random intercepts only this may be justifiable, but adding random slopes just compounds the problem. I am very fond of the very famous quote by Albert Einstein: "Everything should be made as simple as possible, but no simpler." When fitting a mixed model, this means a model with random intercepts only ! So if you absolutely need to justify this, quote Einstein :) Of course, it is just an articulation of Occam's razor ie. Parsimony. And on the topic of parsimony in mixed models, I would highly recommend the following paper: Bates, D., Kliegl, R., Vasishth, S. and Baayen, H., 2015. Parsimonious mixed models. arXiv preprint arXiv:1506.04967. https://arxiv.org/pdf/1506.04967.pdf If you really want to fit a model with random slopes and your (frequentist) software of choice is unable to oblige, then try a Bayesian approach. I hope this helps.
