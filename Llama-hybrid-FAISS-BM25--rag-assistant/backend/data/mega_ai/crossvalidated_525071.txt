[site]: crossvalidated
[post_id]: 525071
[parent_id]: 525063
[tags]: 
The standard error that you want is the standard deviation of the estimate $\hat\pi$ at a fixed point in the sequence, over multiple experiments . This standard error will give you a confidence interval that includes the actual value $\pi$ in 95% of experiments. You don't need a huge sample size to get reasonable estimation of the standard error; you need a huge sample size to get an accurate estimate of $\pi$ . I'm going to do the code in R; translating to Python is left as an exercise for the reader. First, let's have a look at multiple runs: 100 runs of a 1000-step simulation hatpi You can see that the variation along each curve (which is what your first method uses) is smaller than the variation between the curves. That's because points on the same curve are correlated: the estimates at 800 and 900 throws share the first 800 throws and so have a correlation of 800/900, or nearly 0.9. Your standard error formula didn't take this into account Now, I'll run a slightly larger set of experiments, 100, and estimate the standard deviation at 100,200,...,1000 throws experiments The standard deviations look more plausible now; the $\pm 1.96$ interval around the true value covers all 10 curves nearly all the way from 100 to 1000 (so the corresponding intervals around each curve would cover the true value). We can actually do the calculations analytically here. The variance of the binary in_circle variable is $(\pi/4)\times(1-\pi/4)$ , so the standard deviation of the estimate of $\pi$ based on $n$ throws is $4\sqrt{(\pi/4)\times(1-\pi/4)/n}$ truesd Even 100 experiments gave a reasonable estimate of the standard errors. Estimating the standard errors is relatively easy, because you'll probably be happy with the standard error being within about 10% of the truth, but you're trying to get $\pi$ to much more than one digit accuracy. Now, a larger simulation: 1000 experiements with 100,000 throws each (but plotting only every 1000th throw) experiments I've only drawn 10 of the experiments, but you can still see the pattern. The estimated and analytic standard errors are almost identical (the green and orange curves are superimposed) and 9 of the 10 lines stay inside the interval. The standard error is only 0.005 after 100,000 throws, so you'll need many more than that for a good value of $\pi$ . The confidence intervals were well estimated with only 100 experiments, and 1000 experiments is overkill. Finally, could you estimate the standard error from a single run? Yes, you could, but not the way you were doing it. Averages (and variances) of the binary in_circle variable along one run estimate the same thing as averages and variances across experiments. But averages and variances of the cumulative estimator don't -- they're increasingly correlated. hatpi_with_sd It works! The bootstrap idea is a slightly more general version of this, where the estimate_sd is based on empirical standard deviations over intervals of more than one time point, so it would work for, eg, autocorrelation estimates as well as for means.
