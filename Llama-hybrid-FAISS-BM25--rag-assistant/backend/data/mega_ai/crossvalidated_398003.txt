[site]: crossvalidated
[post_id]: 398003
[parent_id]: 
[tags]: 
Naive Monte Carlo, MCMC and their use in Bayesian Theory

So let's suppose I have a random variable X which follows a PDF fX(x) which is known. I can use the Naive Monte Carlo method (with unfiltered random sampling) to obtain n samples of fX(x) and get empirical PDF and estimates of its parameters. Now suppose we have three random variables ( X , Y and Z ). First case: Suppose we know the relationship (function) M between X , Y and Z so that Z=M(X,Y) . Suppose we know the marginals of X and Y and the covariance matrix between X and Y . In this case we can use Naive Monte Carlo method to get empirical PDF of Z and estimates of its parameters. Second case: 2.A Suppose we only know the marginals of X and Y and how the PDF of Z is related to the PDFs of X and Y . In this case we can get the empirical PDF of Z and estimates of its parameters directly (although not knowing M ). Although I'm not sure how the dependency between X and Y is taken into account... 2.B Suppose we only know the marginals of X and Y and how the PDF of Z is related to the PDFs of X and Y (up to a normalizing constant). This in the context of Bayesian Theory is equivalent of knowing the prior and the likelihood and the Bayes rule. In this case we can no longer use the Naive Monte Carlo method to get empirical PDF of Z and estimates of its parameters since we don't know M(X,Y) . Here we need to resort to MCMC. Is the above reasoning right? Third case: Suppose we only know the marginals of X and Y , and some observations of Z. What methods can one apply to estimate not only the PDF of Z but also the function M assuming the marginals of X and Y are general and representative.
