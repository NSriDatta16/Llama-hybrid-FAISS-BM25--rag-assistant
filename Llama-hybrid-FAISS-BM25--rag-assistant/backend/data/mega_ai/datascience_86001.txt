[site]: datascience
[post_id]: 86001
[parent_id]: 66913
[tags]: 
To answer in the simplest way possible - let the model learn the attention weights by training itself. We do that by defining a Dense single layer MLP with 1 unit which 'transforms' each word in the input sentence in such a way that when a dot product of this transformation with the last decoder state is taken, the resulting value is high if the word in question needs to be considered when translating the next word. So at the decoder end, before translating each word, we now know what all words in the input sequence need to be given importance - all we have to do is to take the last hidden state of the decoder and dot product it with all the 'transformed' words in the input sequence and softmax the result. As to how the weights are learnt during training - it is learnt the same way that any layer weights in a NN are learnt - using the standard gradient descent, backprop concepts etc
