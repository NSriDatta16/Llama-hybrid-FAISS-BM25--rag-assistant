[site]: crossvalidated
[post_id]: 295698
[parent_id]: 234314
[tags]: 
It incorporates "prior knowledge" by training the network over a training dataset which will update the weights of the convolution filters. This is how most neural networks are trained with standard backprop. Where the loss to be backproped is based on the segmenation loss in this case. Here's a link to better show a deconvolution visualization viz . It doesn't show how it is trained because that is the same as how regular convolution is trained and there are other resources for that such as here backprop .
