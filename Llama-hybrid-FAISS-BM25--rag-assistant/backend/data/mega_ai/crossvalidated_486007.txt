[site]: crossvalidated
[post_id]: 486007
[parent_id]: 
[tags]: 
Measuring predictive uncertainty with Negative Log Likelihood (NLL)?

I see that in many papers about prediction uncertainty and calibration of neural networks, methods are compared in terms of the negative log-likelihood. What does it represent in this context? And why is it used as a metric for calibration? Some example papers: Maddox et al. 2019, A Simple Baseline for Bayesian Uncertainty in Deep Learning Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles
