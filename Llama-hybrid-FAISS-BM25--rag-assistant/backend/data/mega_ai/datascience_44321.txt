[site]: datascience
[post_id]: 44321
[parent_id]: 44303
[tags]: 
A typical model you could use is shown below- Input Text -> Word Embedding -> Bidirectional LSTM -> Dense output layer Word embedding layer - maps the words from the vocabulary into vectors of real numbers. Bidirectional LSTM - since they can preserve information from both the past and the future they can understand context better as compared to unidirectional LSTM. Checkout the following links for more details- https://machinelearningmastery.com/what-are-word-embeddings/ https://machinelearningmastery.com/develop-bidirectional-lstm-sequence-classification-python-keras/
