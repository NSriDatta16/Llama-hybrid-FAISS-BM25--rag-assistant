[site]: datascience
[post_id]: 62079
[parent_id]: 62035
[tags]: 
Interesting question! I think re-using your model in a DQN setting could be interesting. Even more so if you retrain your supervised model every now and then to update the DQN model (although in that case you'd have to figure out how to re-use what your model learned from DQN vs what it learned supervised). I think to get you started you would have to define your context that you are using RL on. Basic elements that are needed for RL are: an Agent (whoever is making a decision on what to do) a State (how can you describe a snapshot of your current environment that has been influenced by an agent's action) an Action (so something your Agent can actively "do" to induce another state in its environment) a Reward (something you can "give" your agent to learn if he has chosen something good or bad) Assuming you want to use something like DQN you have to define those things to be able to run the algorithm. Your current supervised model would then be the starting status of your NN that is used to choose your Agent's decisions. The keyword for what you are looking for is "Transferlearning", which describes how to use trained models for other cases. In your case even other learning methods.
