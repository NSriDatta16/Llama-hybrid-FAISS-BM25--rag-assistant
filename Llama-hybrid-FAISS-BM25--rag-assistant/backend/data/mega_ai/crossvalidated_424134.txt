[site]: crossvalidated
[post_id]: 424134
[parent_id]: 422890
[tags]: 
Just an example why people want to apply masks to encoders. There're unsupervised language models pre-trained with an unidirectional mask, for example GPT . If we want to leverage this pre-trained language model to build a encoder-decoder based machine translation model, we might want to apply the unidirectional mask in the same fashion it's pre-trained with.
