[site]: datascience
[post_id]: 68745
[parent_id]: 54751
[tags]: 
Please consider the following: 1) Random forest algorithm can be used for both classifications and regression task. 2) It typically provides very high accuracy. 3) Random forest classifier will handle the missing values and maintain the accuracy of a large proportion of data. 4) If there are more trees, it usually wonâ€™t allow overfitting trees in the model. 5) It has the power to handle a large data set with high dimensionality Ultimately, what algo you choose to work with is up to you. you definitely want the predictive capabilities of the algo to be pretty high (over 90%). Sometimes other algos beat the RF algo, but I have found that often the RF is quite good! Usually, I start with RF, and if I see decent performance, I am done. I believe, in around at least 80% of the time, I'm done. If you are not getting good results from the RF algo, test some others. This gives a nice comparison of a few different algos. import pandas import matplotlib.pyplot as plt from sklearn import model_selection from sklearn.linear_model import LogisticRegression from sklearn.tree import DecisionTreeClassifier from sklearn.neighbors import KNeighborsClassifier from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.naive_bayes import GaussianNB from sklearn.svm import SVC # load dataset url = "https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv" names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] dataframe = pandas.read_csv(url, names=names) array = dataframe.values X = array[:,0:8] Y = array[:,8] # prepare configuration for cross validation test harness seed = 7 # prepare models models = [] models.append(('LR', LogisticRegression())) models.append(('LDA', LinearDiscriminantAnalysis())) models.append(('KNN', KNeighborsClassifier())) models.append(('CART', DecisionTreeClassifier())) models.append(('NB', GaussianNB())) models.append(('SVM', SVC())) # evaluate each model in turn results = [] names = [] scoring = 'accuracy' for name, model in models: kfold = model_selection.KFold(n_splits=10, random_state=seed) cv_results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring) results.append(cv_results) names.append(name) msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std()) print(msg) # boxplot algorithm comparison fig = plt.figure() fig.suptitle('Algorithm Comparison') ax = fig.add_subplot(111) plt.boxplot(results) ax.set_xticklabels(names) plt.show() Reference: How To Compare Machine Learning Algorithms in Python with scikit-learn
