[site]: crossvalidated
[post_id]: 535624
[parent_id]: 
[tags]: 
Are SHAP values potentially misleading when predictors are highly correlated?

Are SHAP (SHapley Additive exPlanations) values potentially misleading when predictors are highly correlated? How and why? If so, is there any guidance on when not to use SHAP? Are there any rules of thumb based on $\mathbf{Var}\left[X\right]$ telling us when features are too correlated for SHAP? I'm interested in a regression setting where $X \in \mathbb{R}^p$ is a $p$ -dimensional vector of predictors (aka features), and we are using SHAP to understand the behavior of a nonlinear regression model $f(X)$ which allows interactions. Suppose $f$ is a gradient boosted regression tree, for example. Motivation: https://christophm.github.io/interpretable-ml-book/shapley.html#disadvantages-13 states that "Like many other permutation-based interpretation methods, the Shapley value method suffers from inclusion of unrealistic data instances when features are correlated. To simulate that a feature value is missing from a coalition, we marginalize the feature. This is achieved by sampling values from the feature's marginal distribution. This is fine as long as the features are independent. When features are dependent, then we might sample feature values that do not make sense for this instance." https://youtu.be/B-c8tIgchu0 (a presentation about SHAP given by Scott Lundberg) touches on this question. Around 10:50, the presenter says "in practice what happens is you often assume independence between different input features in order to calculate the conditional expectation." https://arxiv.org/abs/1705.07874 (A Unified Approach to Interpreting Model Predictions by Scott Lundberg and Su-In Lee) notes that many methods "assume feature independence" in equation 11. Screenshot: Edit: Mase et al.'s paper (linked by cardinal in a comment below) is quite relevant to this question: Mixing and matching the components of $x_t$ and $x_b$ presents some problems. The variables $x_{ij}$ and $x_{ik}$ may show a strong correlation over subjects i = 1, . . . , n. Putting $x_{tj}$ and $x_{bk}$ into a single hypothetical point may produce an input combination far from any that has ever been seen. Beyond being unusual, some combinations are physically or even logically impossible. The changes might produce a hybrid data point representing a patient whose systolic blood pressure is lower than their diastolic blood pressure. Somebody’s birth date could follow their graduation date. When hospital records show minimum, maximum and average levels of blood oxygen, the hybrid point could have mean O2 below minimum O2, or it could have minimum and maximum that differ along with a variable saying they were only measured once (or never). There could be important reasons to understand effects of longitude and latitude separately, but some combinations will not make sense, perhaps by placing a dwelling within a body of water. When the function $f(·)$ that made a decision was trained, it would have seen few if any impossible or extremely unlikely inputs. As a result, its predictions there cannot have been regularized suitably. Investigators should be able to choose an importance measure that does not rely on any such values.
