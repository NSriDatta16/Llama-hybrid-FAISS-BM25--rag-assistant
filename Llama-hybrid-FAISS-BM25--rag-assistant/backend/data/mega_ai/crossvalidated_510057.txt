[site]: crossvalidated
[post_id]: 510057
[parent_id]: 489988
[tags]: 
This may be due to your dev set and test set not being identically distributed. One way to test this is to train a classifier that discriminates between the training/dev vs the test set. If your dataset is small you should definitely check if the drop from dev/test metrics is consistent between splits. If the drop varies, you should do a nested cross validation. That way you average over the splits (which is random) and get a better estimate of the true performance.
