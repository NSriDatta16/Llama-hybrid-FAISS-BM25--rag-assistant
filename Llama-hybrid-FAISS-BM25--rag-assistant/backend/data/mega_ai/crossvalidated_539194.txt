[site]: crossvalidated
[post_id]: 539194
[parent_id]: 
[tags]: 
How to do feature engineering with scikit learn pipelines?

When doing preprocessing I've always used pandas to impute, encode, or scale my data. In other words, I've done all of the steps "manually". However, this takes a long time, generates a lot of code, and makes the whole workflow much more complicated in my opinion. An alternative is using a scikit-learn pipeline that allows chaining multiple (preprocessing) steps to make the process more efficient, write less code, and avoid data leakage. However, I've run across a problem when using pipelines: Since most feature engineering cannot be done without preprocessing the data, e.g. because of missing values, I have to do feature engineering after preprocessing. Okay, no problem so far. Now, when I do the preprocessing with a scikit learn pipeline, it returns a NumPy array with the imputed and, for example, one-hot-encoded features. It's now impossible to understand (or at least much harder to understand) which features are which. Especially because we don't have the column labels after they've been one-hot-encoded. We also cannot simply display the dataframe and think about what features could be engineered. Now my questions: The above being the case, what is the best way to integrate feature engineering with a scikit learn pipeline? p.s. the same question applies to feature selection.
