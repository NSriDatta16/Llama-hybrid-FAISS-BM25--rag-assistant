[site]: datascience
[post_id]: 102852
[parent_id]: 72167
[tags]: 
Be careful with feature selection! Do not rely solely on feature selection techniques. They might be misleading sometimes. Here is the process I usually follow: 1.) The very first thing to do is build a baseline model where you consider all the features and record the performance. This will give you a baseline score to compare with. (Do not perform hyperparameter tuning here!) 2.) Now you perform feature engineering , where you see if you can combine multiple feature into one single one. For example you have 3 features as date, month and year of sale of a car. You can combine all 3 of them into a single feature age. This will reduce the dimension of your dataset. 3.) Here you try to remove any outliers/nonsensical values from features. For example in the case of predicting the price of car, you have year of car as 1900. This is a nonsensical value and won't help the model. You can safely remove it. (be carefull how you deal with outliers as removing them is not the only solution but thats a whole another topic in itself!) 4.) Now you can perform feature selection. There a quite a few techniques you can use like filter based, wrapper based and hybrid techniques. But don't just blindly use of these as they might be misleading. Instead use subject matter expertise first to remove any redundant features (which is what I usually do). Applying all the above things will usually result in the removal of redundant features. If not only then go for feature selection techniques mentioned in point 4.). Hope it helps you!
