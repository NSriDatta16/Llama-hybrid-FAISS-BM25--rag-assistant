[site]: crossvalidated
[post_id]: 311118
[parent_id]: 310942
[tags]: 
Consider the simpler example of a two-part experiment in which we first select one of two differently -biased coins (say $P(\mathrm{Heads}) = p_0$ for one coin and $P(\mathrm{Heads}) = p_1$ for the other, where $p_0\neq p_1$) and then toss the selected coin $n$ times. The choice of coin to be tossed is random: the two coins are picked with probabilities $\pi_0$ and $\pi_1 = 1-\pi_0$ respectively. We also assume that the tosses of the coin are independent sub-experiments of the main experiment, that is, the tosses are conditionally independent regardless of which coin has been selected for tossing. If we define $X_i$ as having value $1$ or $0$ according as the $i$-th toss resulted in Heads or Tails respectively, then all the above can be summarized by saying that the $X_i$'s are conditionally independent Bernoulli random variables with parameter $p_0$ or $p_1$ depending on which coin was selected. But the $X_i$ are not unconditionally independent random variables. It is easy to calculate (via the law of total probability) that for each $i$, $$ P\{X_i = 1\} = p_0\cdot \pi_0 + p_1\cdot (1-\pi_0),$$ that is, the $X_i$ are identically distributed random variables. But, $$ P\{X_i = 1, X_j = 1\} = (p_0)^2\cdot \pi_0 + (p_1)^2\cdot (1-\pi_0) \neq P\{X_i = 1\}\cdot P\{X_j = 1\}$$ and so , but the $X_i$ are not unconditionally independent random variables even though they are conditionally independent random variables under both the possible conditions. All the above is the probabilist's take on what has been described thus far. Statisticians, on the other hand, are skeptical people who have different axes to grind. You are correct in that if the results of the $n$ tosses are the only observations that you will ever have, then whether you think of the $X_i$'s as conditionally independent or unconditionally independent is totally irrelevant. But then, the statistician who knows which coin produced the given results of the $n$ tosses but is skeptical of what he has been told are the values of $p_0$ and $p_1$ might be more interested in estimating the value of $p_0$ (or $p_1$ as the case might be) from the given results of the $n$ tosses. If the $n$ tosses resulted in $k$ Heads, then the maximum-likelihood estimate of $p_i$ would be $\frac kn$. Alternatively, the statistician could devise a confidence interval for the estimate of $p_i$. Or, if he does not know which coin produced the given results, then he might be interested in making a decision as to which coin produced the observed results of $k$ Heads in $n$ tosses. The maximum-likelihood decision rule would choose between the two coins according as whichever of the two likelihoods $p_1^k(1-p_1)^{n-k}$ and $p_0^k(1-p_0)^{n-k}$ was larger, whereas the Neyman-Pearson decision rule would find the decision rule that had the largest power among all decision rules that achieved Type I error probability $\alpha$ or less. In all of these methods, the fact that the experiment is really a two-part experiment which the coin to be tossed has been picked randomly with probability $\pi_i$ of selecting the coin with $P(\mathrm{Heads}) = p_i$ is ignored. All we are doing is making various inferences from the data that $n$ tosses produced $k$ Heads. To take into account the prior probabilities $\pi_0$ and $\pi_1 = 1-\pi_0$ of the the coin selection, a Bayesian statistician would use a decision rule that minimizes the average error probability , or more generally, results in the smallest possible average cost . It is well known that the minimum average error probability decision rule is the maximum a posteriori probability (MAP) rule: pick the coin that has the larger conditional probability given the data , and that the MAP rule reduces to comparing the likelihood ratio $\frac{f_1(x)}{f_0(x)}$ to the threshold $\frac{\pi_0}{\pi_1}$ and deciding that it is the coin with $P(\mathrm{Heads})=p_1$ if and only if $$\frac{f_1(x)}{f_0(x)} = \frac{p_1^k(1-p_1)^{n-k}}{p_0^k(1-p_0)^{n-k}} > \frac{\pi_0}{\pi_1}.$$ In contrast, the maximum-likelihood decision rule compares the likelihood ratio to $1$. Similarly, the Bayesian statistician can make (Bayesian) estimates of $p_i$ or come up with credible intervals in which the parameter being estimated must lie with high probability and these can be different from the more common frequentist estimates found in most texts.
