[site]: crossvalidated
[post_id]: 522375
[parent_id]: 522366
[tags]: 
The answer is (as usual!) provided in Devroyes' Non-uniform random variate generation (1986, p.396). The principle for the Ahrens-Dieter (1972) algorithm is a result due to George Marsaglia (1961): Theorem IX.2.1 $\ \ \ $ If $U_1,U_2,\ldots$ is a series of iid $\mathcal U(0,1)$ random variables, if $Z$ is an independent positive Poisson $\mathcal P_+(\mu)$ random variable, and if $M$ is an independent Geometric $\mathcal G(1-e^{-\mu})$ random variable, then $$X = \mu(M+\min(U_1,\ldots,U_Z))\sim\mathcal E(1)$$ The proof proceeds as follows (reproducing from Devroye, p.395 ): \begin{align*} \mathbb P(\mu\min(U_1,\ldots,U_Z)\le x) &=\mathbb E^Z[\mathbb P(\mu\min(U_1,\ldots,U_z)\le x|Z=z)]\\ &=\mathbb E^Z[1-(1-x/\mu)^Z)]\\ &=1-\dfrac{e^{\mu-x}-1}{e^{\mu}-1}\\ &=\dfrac{1-e^{-x}}{1-e^{-\mu}} \end{align*} which is the cdf of the exponential distribution truncated to $(0,\mu)$ (see here ). A preliminary result $^1$ due to Von Neumann (Lemma IV.2., p.125 & p.393) is that, when $$Z\sim \mathcal G(1-e^{-\mu})\qquad Y\sim \dfrac{e^{-y}}{1-e^{-\mu}}\mathbb I_{(0,1)}(y)$$ then $$\mu(Z-1)+Y\sim\mathcal E(1)$$ This follows from considering the moment generating function (for $t ) $$\mathbb E[e^{tX}]=\mathbb E[e^{t\mu(Z-1)}]\mathbb E[e^{tY}]= \frac{1-e^{-\mu}}{1-e^{-\mu(1-t)}}\frac{1-e^{-\mu(1-t)}}{(1-e^{-\mu)}(1-t)}=\frac{1}{1-t}$$ which concludes the proof. Relating to the graph included in the question, this means that $\mu M$ corresponds to the area under the step function, while $\mu\min(U_1,\ldots,U_Z))$ corresponds to the residual area between the step function and the Exponential $\mathcal E(1)$ density, which is therefore independent from $M$ . Marsaglia then derives his Exponential algorithm from Theorem IX.2.1: Generate a Geometric $M\sim\mathcal G(1-e^{-\mu})$ variable Generate two Uniform $U$ and $V$ Set $Y=V$ and $Z=1$ While $U>F_\mu(Z)$ , increase $Z$ to $Z+1$ and decrease $Y$ to $\min(Y,W)$ , where $W$ is Uniform Return $\mu(M+Y)$ as an Exponential $\mathcal E(1)$ variate. and Ahrens and Dieter (1972) consists in an optimisation of the above, for instance by choosing $^2$ $\mu=\log(2)$ , refining the generation of $M$ , and storing the cdf $F(\cdot)$ . A detailed explanation of Ahrens and Dieter (1972) version: The prerecorded table corresponds to the first terms of the cdf of the Poisson $\mathcal P_+(\log 2)$ distribution (with q[0] equal to $\mu$ ) The first loop returns a rescaled Geometric $\mathcal G(1-e^{-\mu})$ variate $\mu M$ as a (using a sequential search inversion method and the property that $F_M(i)=1-2^{-i}$ when $\mu=\log 2$ ) The residual u-1 produces $^3$ an independent uniform variate $V$ The case u corresponds to $Z=1$ and avoids running the second loop The second loop while (u > q[i]) produces the Poisson $\mathcal P_+(\log 2)$ variate $Z$ as i-1 and the associated $Y=\min(U_1,\ldots,U_Z)$ as umin The outcome a+umin*q[0] is indeed $X = \mu(M+\min(U_1,\ldots,U_Z))\sim\mathcal E(1)$ $^1$ An ingenious algorithm for generating from the Exponential distribution is derived from this lemma and consists in only producing sequences of Uniform variates $U_0,U_1,\ldots$ . $^2$ When $\mu=\log 2$ , the geometric random variate corresponds to the number of $0$ before the first $1$ in the binary expansion of $U\sim\mathcal U(0,1)$ . Sampling directly these bits proves to be much faster than the first loop until (u > 1.) . $^3$ Devroye remarks that "Ahrens and Dieter squeeze the first uniform [0,1] random variate $U$ dry". The efficiency of the method is such that it requires on average $1+\log(2)$ uniforms.
