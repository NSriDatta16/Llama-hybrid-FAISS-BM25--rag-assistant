[site]: crossvalidated
[post_id]: 365039
[parent_id]: 365004
[tags]: 
Your question is presently ill-posed, and it really depends on what kind of "aggregation" you have in mind. Each Bayesian has a legitimate posterior estimate based on observed data and a prior belief, so presumably if you want "aggregate" these posterior estimates, you want to use a prior that is related somehow to the priors of the Bayesians in your question (e.g., a combination of their priors). One way to specify the "aggregation" is to specify how your own prior relates to the priors of these Bayesians, which in turn determines how your posterior relates to their posteriors. I will show you how you can do this below. Since you have not specified to the contrary, I am going to assume that all the Bayesians in your question observe the same data $\mathbf{x}$, which they all take to come from the same sampling distribution with likelihood function $L(\theta)$. I will also assume that, conditional on the unknown parameter $\theta \in \Theta$, the probability of the event of interest is $0 \leqslant h(\theta) \leqslant 1$. This setup means that all the Bayesians perform the same inference, but they may each have different priors for the unknown parameter. (And hence, their estimated probabilities of the event may differ.) Now, suppose that your Bayesians have priors $\pi_1,...,\pi_N$. Then for Bayesian $i$ the posterior probability of the event of interest is: $$H_i = \mathbb{E}(h(\theta)| \mathbf{x},\pi_i) = \frac{\int_\Theta h(\theta) L(\theta) \pi_i(\theta) d\theta}{\int_\Theta L(\theta) \pi_i(\theta) d\theta}.$$ Now, how you aggregate these probabilities depends on how your own prior relates to the priors of the $N$ Bayesians in the analysis. For example, if your prior is some convex combination of priors of these Bayesians, then you have $\pi_* = \sum_{i=1}^n \lambda_i \pi_i$ and your corresponding posterior probability is: $$H_* = \frac{\int_\Theta h(\theta) L(\theta) \pi_*(\theta) d\theta}{\int_\Theta L(\theta) \pi_*(\theta) d\theta} = \frac{\sum_{i=1}^n \lambda_i \int_\Theta h(\theta) L(\theta) \pi_i(\theta) d\theta}{\sum_{i=1}^n \lambda_i \int_\Theta L(\theta) \pi_i(\theta) d\theta} = \sum_{i=1}^n \lambda_i^* H_i,$$ where the updated posterior weights $\lambda_1^*,...,\lambda_N^*$ are given by: $$\lambda_i^* = \frac{\lambda_i \int_\Theta L(\theta) \pi_i(\theta) d\theta}{\sum_{i=1}^n \lambda_i \int_\Theta L(\theta) \pi_i(\theta) d\theta}.$$ This gives you a rule for "aggregating" the posterior estimates of your Bayesians, based on an assumption that your preferred prior is some combination of their priors. As you can see, the updated posterior weightings are related to the prior weightings by a linear weighting that depends on the likelihood and priors. In the case where the Bayesians are "equally trustworthy" you might decide that your prior is a simple average of their priors,so that $\lambda_1 = ... = \lambda_N = \tfrac{1}{N}$. In this special case, the posterior weightings for the estimated probability of the event are: $$\lambda_i^* = \frac{\int_\Theta L(\theta) \pi_i(\theta) d\theta}{\sum_{i=1}^n \int_\Theta L(\theta) \pi_i(\theta) d\theta}.$$ This result could arguably meet your requirement to "aggregate" the posterior estimates based on each Bayesian being "equally trustworthy". As you can see, even with the simplification of equal weighting on the priors, the aggregation is still more complicated than taking a simple mean, median or geometric mean.
