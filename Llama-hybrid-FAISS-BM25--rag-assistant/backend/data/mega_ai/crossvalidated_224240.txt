[site]: crossvalidated
[post_id]: 224240
[parent_id]: 224237
[tags]: 
It's hard to know for sure with such a terse and pithy description, but here's a shot at what he may likely be getting at. Say you have a very high cardinality feature $x$ with some giant set of possible levels $l_1, l_2, \cdots, l_n$. These can be difficult to use in a model directly. One approach to deriving a feature from such a predictor is to take some other data set that is not used in training, and compute the average values of the response within each group determined by the predictor $$ x'_i = \frac{1}{\# \{ x_j = x_i \} } \sum_{x_j = x_i} y_i $$ Then you can use this new predictor in a learner, but you have reduced the many binary features of the categorical into one new feature in the model. There are caveats. First, you absolutely must use data that abstains from training to calculate the group level averages, or you have leaked the thing you are trying to predict directly into your predictors, and your model is worthless. Second, if you want your model to explain much of anything this is a pretty bad approach, as your new predictor basically says "y happened to x because y happened to x some other data set." If you don't have a free data set hanging around to compute the group level averages, one pretty effective technique is to add noise to group level averages computed from your training data $$ x'_i = \frac{ \sum_{x_j = x_i} y_i + \text{laplace}(0, \alpha)} {\# \{ x_j = x_i \} + \text{laplace}(0, \alpha)} $$ where $\text{laplace}(0, \alpha)$ is random noise generated from a laplace distribution . This technique is derived from research into differential privacy.
