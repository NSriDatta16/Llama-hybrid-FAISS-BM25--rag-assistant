[site]: crossvalidated
[post_id]: 591970
[parent_id]: 
[tags]: 
Logistic regression with many predictors opposed to statistical testing

Suppose we have a two groups of people, A and B and we have measured 50 features, e.g. blood markers in these people and we would like to know which features are different between the two groups and statistically significant. Usuaully I would apply a statistics test like Mann-Whitney-U on the distribution of each feature between the two groups, and then use a procedure to correct for multiple hypothesis testing on the p-values obtained from this. Another way could be to make a binary logistic classifier, e.g. glm in R to classify if a person belongs to group A or not. This model would then report p-values for the coefficient for each feature. First question: should these p-values be corrected for multiple hypothesis testing? I would think so. Second question: Instead of supplying all features of the data to the logistic model, what if I fit a model for each feature, obtain the p-value for that feature, and then correct these p-values for multiple hypothesis testing. Will this yield the same result - p-values - as supplying all features to the model? I guess not - and then why is this last approach not correct? It seems like this corresponds more to the first 'simple' statistical test approach? Would I be violating some assumption of the model when doing this?
