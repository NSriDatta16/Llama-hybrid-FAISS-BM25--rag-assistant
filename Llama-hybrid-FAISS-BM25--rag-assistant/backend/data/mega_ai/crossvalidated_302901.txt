[site]: crossvalidated
[post_id]: 302901
[parent_id]: 302897
[tags]: 
In a parametric regression scenario where residuals are significantly autocorrelated, some issues may arise when using ordinary least squares: Estimated model coefficients remain unbiased but do not possess the minimum variance property (correct values, bad confidence intervals). The MSE may seriously underestimate the true variance of the errors. The standard error of the regression coefficients may seriously underestimate the true standard deviation of the estimated regression coefficients. Statistical intervals and inference procedures are no longer strictly applicable. In a machine learning scenario (usually nonparametric), where statistical inference is not applicable, all but the first of the above points are unimportant. The trained model remains unbiased , which means that its predictive accuracy is not directly affected by the effects of autocorrelation. However, what does have an indirect impact on predictive accuracy is the fact that the degree of autocorrelation constitutes a proportion of the overall variation of the response. In other words, having a high degree of autocorrelation implies that you have some "hidden" features integrated in your dependent variable and unless you extract them somehow (e.g. time series decomposition) you will not be able to regress that variable sufficiently using exogenous input alone.
