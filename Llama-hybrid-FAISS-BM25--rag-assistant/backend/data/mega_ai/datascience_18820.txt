[site]: datascience
[post_id]: 18820
[parent_id]: 
[tags]: 
Attempt to reuse RNNCell with a different variable scope

I'm using Tensorflow 1.1.0 with GPU support and I have this function: def get_init_cell(batch_size, rnn_size, keep_prob=0.75, layers=2): """ Create an RNN Cell and initialize it. :param batch_size: Size of batches :param rnn_size: Size of RNNs :return: Tuple (cell, initialize state) """ # Basic LSTM cell # lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size) lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size, forget_bias=0.0, state_is_tuple=True, reuse=tf.get_variable_scope().reuse) # Add drop to the cell drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob) # Stack multiple LSTM layers rnn_cell = tf.contrib.rnn.MultiRNNCell([drop for _ in range(layers)], state_is_tuple=True) # Getting an initial state of zeros initial_state = rnn_cell.zero_state(batch_size, tf.float32) # Set the name initial_state = tf.identity(initial_state, 'initial_state') return rnn_cell, initial_state The previous code works perfectly with Tensorflow 1.0.0 if I use lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size) but now I have changed the way I get the lstm cell. And I get this error: ValueError: Attempt to reuse RNNCell with a different variable scope than its first use. First use of cell was with scope 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell', this attempt is with scope 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell'. Please create a new instance of the cell if you would like it to use a different set of weights. If before you were using: MultiRNNCell([BasicLSTMCell(...)] * num_layers), change to: MultiRNNCell([BasicLSTMCell(...) for _ in range(num_layers)]). If before you were using the same cell instance as both the forward and reverse cell of a bidirectional RNN, simply create two instances (one for forward, one for reverse). In May 2017, we will start transitioning this cell's behavior to use existing stored weights, if any, when it is called with scope=None (which can lead to silent model degradation, so this error will remain until then.) I have changed get_init_cell with this one: def lstm_cell(rnn_size, keep_prob=0.75): # Basic LSTM cell lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size, reuse=tf.get_variable_scope().reuse) # Add drop to the cell return tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob) def get_init_cell(batch_size, rnn_size, keep_prob=0.75, layers=2): """ Create an RNN Cell and initialize it. :param batch_size: Size of batches :param rnn_size: Size of RNNs :return: Tuple (cell, initialize state) """ # Stack multiple LSTM layers rnn_cell = tf.contrib.rnn.MultiRNNCell([lstm_cell(rnn_size, keep_prob) for _ in range(layers)]) # Getting an initial state of zeros initial_state = rnn_cell.zero_state(batch_size, tf.float32) # Set the name initial_state = tf.identity(initial_state, 'initial_state') return rnn_cell, initial_state And I still getting the same error. I have also try it with the same error: def get_init_cell(batch_size, rnn_size, keep_prob=0.75, layers=2): """ Create an RNN Cell and initialize it. :param batch_size: Size of batches :param rnn_size: Size of RNNs :return: Tuple (cell, initialize state) """ def lstm_cell(): # Basic LSTM cell return tf.contrib.rnn.BasicLSTMCell(rnn_size, reuse=tf.get_variable_scope().reuse) def attn_cell(): return tf.contrib.rnn.DropoutWrapper(lstm_cell(), output_keep_prob=keep_prob) # Stack multiple LSTM layers rnn_cell = tf.contrib.rnn.MultiRNNCell([attn_cell() for _ in range(layers)], state_is_tuple=True) # Getting an initial state of zeros initial_state = rnn_cell.zero_state(batch_size, tf.float32) # Set the name initial_state = tf.identity(initial_state, 'initial_state') return rnn_cell, initial_state Any idea?
