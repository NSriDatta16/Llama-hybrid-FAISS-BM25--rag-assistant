[site]: crossvalidated
[post_id]: 76432
[parent_id]: 76376
[tags]: 
The explanations are both right, but they are for different situations. As usual, it all boils down to the question how to obtain statistically independent splits of your data. The image you linked and your description is for a situation where you have repeated measurements of time series . In this situation you can leave out complete time series from your data. Imagine you want to predict some property based on a new complete measurement of another time series, e.g. classification of EEG readings. You can assume EEG readings of different patients to be statistically independent and a scenario where only complete readings are used is sensible. In that case the natural way of splitting the data would be by patient. Hyndman discusses a situation where you essentially have only one (ongoing) measurement of a time series , and you want to predict future values of the time series from past measurements . Thus, you split by time, and the future implies that none of the following time points is known. In the EEG example, this corresponds to trying to predict what the next seconds/minutes of the EEG of the given patient would be. This type of splitting is also important when you want to measure how long a model is valid, see e.g. Esbensen, K. H. and Geladi, P.: Principles of Proper Validation: use and abuse of re-sampling for validation, J Chemom, 2010, 24, 168-187 . Another situation where you'd need to split by time and also by case would be: imagine you'd like to do predictions on future value of stocks. Again, you need to split by case (stock). But of course, the tested stock's value at a given time may (is probably) also be correlated with the value of other stocks at that time. Thus, you also need to leave out all "future" data of all stocks from model training.
