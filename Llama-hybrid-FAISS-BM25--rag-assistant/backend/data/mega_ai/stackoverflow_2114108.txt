[site]: stackoverflow
[post_id]: 2114108
[parent_id]: 2113601
[tags]: 
I recently had a debate with someone about this at work. The person raising the issue had a fair point but, ultimately, unit testing is a practical, proven technique. TDD adds a further "test first" aspect which further reduces the chance of the test being incorrect, as it should fail before any production code is written. While it's true that tests can be bugged, in my eyes, that issue is more of a philosophical debate than a practical barrier. The argument that you need test code to test the tests only holds any weight with me when the code in question forms a re-usable framework or utility testing class that other unit test fixtures will use. Example : Prior to NUnit 2.5, we had re-usable data-driven testing class. It was used by and relied upon by many other tests, plus it was fairly complicated. Certainly complicated enough to have bugs. So we unit tested the testing class. .. and then when NUnit 2.5 came out, we swapped all the tests to use it and threw away our own implementation. On the other hand, well written test code is straightforward, contains no special logic / branching and helps build a scaffolding around the production code. Where possible, we should always leverage the functionality in other, well-tested frameworks rather than doing any logic in our test class. If regular unit test code gets complicated enough to make someone think it may require testing, I would argue the approach is likely flawed.
