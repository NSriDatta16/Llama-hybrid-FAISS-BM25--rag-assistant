[site]: crossvalidated
[post_id]: 586726
[parent_id]: 
[tags]: 
time series forecasting: how to evaluate performance on test set

Performance during the training of a model for time series forecasting can be computed simply considering the difference at each time $t$ between the real value $y_t$ and the model predicted value $\hat{y}_t$ . The prediction of $\hat{y}_t$ can be made considering as model input the real $x_t$ , corresponding to $y_{t-1}$ , i.e. $x_t=y_{t-1}$ , since it is a time series so the input at each step is the output of the previous one. However, to evaluate the model, in the test set the situation is different, since we should hypothesize that, for all the $t$ greater than the ones used in the training stage, the real input series values are unknown. We could use each previous model prediction as input, i.e $x_t=\hat{y}_{t-1}$ , but if the prediction $\hat{y}_{t-1}$ is wrong, it will affect all subsequent results, causing performances to collapse. On the other side, if we use the real $y_t$ values of the test set as inputs, the results may be too optimistic. The question is: is there a correct way to build the test set in evaluating time series forecasting? Is there a book, a paper, or any kind of document that tackle this point? The only reference that I found about the problem is https://towardsdatascience.com/proper-validation-of-a-time-series-model-5c1b54f43e60 (section "the gap in validation data"). Is there anything else or an established way of dealing with the problem?
