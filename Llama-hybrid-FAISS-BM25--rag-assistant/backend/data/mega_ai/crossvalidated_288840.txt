[site]: crossvalidated
[post_id]: 288840
[parent_id]: 287929
[tags]: 
Disclaimer: I do not generally endorse the use automatic feature selection algorithms. They are prone to overfitting, emphasising nonsensical associations, over-reliance on a particular metric used and commonly present a good tool for senseless data analysis. They are probably useful for variable importance at some settings where data collection cost is an issue... :) To your question now: The choice of AUC-ROC is just a standard choice and you should not read too much into it. AUC-ROC can be seen as a metric that does not depend on particular probability thresholds in the case of probabilistic classification . In addition it has a nice association with the Mann-Whitney U-test (ie. can be easily interpreted as the probability that the classifier will assign a higher probability to a randomly chosen positive item than a randomly chosen negative item) and it is not too affected by imbalanced data. For these reasons it is the established vanilla ice-cream of classification metrics. As you correctly assess maximising AUC-ROC does not guarantee that we will get the better combination of sensitivity and specificity or any other particular metric. (If you are interested in the combination of the two you might want to consider using F-score ) In most applications defining a correct metric is half the battle. For a probabilistic classifier, I would suggest you look into using a proper scoring rule like the Brier score. Please note that accuracy is usually not a good metric because it is an improper rule (ie. maximising it does not guarantee we are as close as possible to the true probability distributions of the data). Additional notes particular to the use case present: For Support Vector Machines AUC-ROC is a bit fuzzy as a concept. SVMs natively do hard class-assignment and have to rely to isotonic regression or Platt scaling approaches to generate probability distributions over classes. This adds an additional layer of complexity. At first instance, I could suggest using a "standard" regularisation approach like Elastic-net or Least-angle regression (to directly control for the $n \ll p$ scenario you outline. The techniques will do the "variable selection" internally through regularisation. The sensitivity and specificity values you see in the output of rfe are related to a $0.5$ probability threshold. It is plausible you can get better performance using another threshold. AUC-ROC is probably better than just using sensitivity and specificity. Both sensitivity and specificity are improper scoring rules that are hard to properly optimise for. For example, correctly increasing the probability of an item belonging to particular class does not increase neither sensitivity or specificity unless the increase passes an arbitrary threshold (usually $0.5$). This correct change in the estimated probability will be directly reflected on the AUC-ROC value. ROC , ie. Receiver operating characteristic is used instead of simply AUC Area-Under-the-Curve because AUC can be easily defined for many other metrics too. A common one is AUC-PR, the Area-Under-the-Curve for the Precision Recall curve - a quite popular metric for imbalanced learning (eg. see Davis and Goadrich's The Relationship Between Precision-Recall and ROC Curves for more on this.)
