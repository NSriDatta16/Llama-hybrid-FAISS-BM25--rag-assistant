[site]: crossvalidated
[post_id]: 361667
[parent_id]: 361656
[tags]: 
In frequentist statistics, we assume we don't know anything about the parameter(s) of interest prior to seeing the data. In Bayesian statistics, we relax this assumption by acknowledging we might know something about the possible values of the parameter(s) of interest prior to seeing the data - for example, how likely it is that the parameter might take on values in a certain range. After we see the data, we can refine/update this prior knowledge. So the prior distribution(s) of the parameters(s) of interest encapsulate our knowledge/belief about the possible values of this(these) parameters(s) prior to seeing the data, while the corresponding posterior distributions refine this knowledge/belief with input from the data. Both frequentist and Bayesian statistics assume a parameter is an unknown quantity which must be estimated. But they diverge in terms of the inputs required for estimating that quantity: data only for frequentist statistics versus prior knowledge/belief plus data for Bayesian statistics. When Bayesians report the mean/median/mode of a posterior distribution for a specific parameter, they do so because they believe that is the most likely value of the parameter (i.e., unknown quantity to be estimated). But the parameter is unknown, so a measure of uncertainty around this most likely value (e.g., 95% credible interval) gives a sense of where the true value of the parameter may live. As a frequentist approaching the Bayesian field, I found it helpful to think of the parameter as something unknown we are trying to estimate and the prior and posterior distributions as tools for encapsulating our state of knowledge/belief about this unknown entity before and after we see the data, respectively. While frequentists adopt the hard-line approach that we don't know anything about the unknown entity we are chasing after, Bayesians adopt a more realistic and nuanced approach that we may actually know something and that, once we see the data, we will improve upon that knowledge. So we care about distributions in the Bayesian context because they are convenient vehicles for expressing the current state of knowledge/belief (for prior distributions) and the hopefully improved state of knowledge/belief achieved after seeing the data (for posterior distributions). Addendum: Personally, I subscribe to the view captured so eloquently by Sander Greenland in the article Bayesian perspectives for epidemiological research: I. Foundations and basic methods , International Journal of Epidemiology, Volume 35, Issue 3, 1 June 2006, Pages 765–775 (which is available at https://academic.oup.com/ije/article/35/3/765/735529 ): " It is often said (incorrectly) that ‘parameters are treated as fixed by the frequentist but as random by the Bayesian’. For frequentists and Bayesians alike, the value of a parameter may have been fixed from the start or may have been generated from a physically random mechanism. In either case, both suppose it has taken on some fixed value that we would like to know. The Bayesian uses formal probability models to express personal uncertainty about that value. The ‘randomness’ in these models represents personal uncertainty about the parameter’s value; it is not a property of the parameter (although we should hope it accurately reflects properties of the mechanisms that produced the parameter)." See http://thestatsgeek.com/2015/04/22/bayesian-inference-are-parameters-fixed-or-random/ for more musings on this topic.
