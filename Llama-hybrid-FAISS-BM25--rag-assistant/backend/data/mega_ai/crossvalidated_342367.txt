[site]: crossvalidated
[post_id]: 342367
[parent_id]: 
[tags]: 
Is feature transformation (power, log, Box-Cox) necessary in deep learning?

I've read that it's beneficial to apply certain common feature transformations on datasets before they hit machine learning models. These based on the distributions of the dataset's features; eg, applying log transforms to skewed normally-distributed features. Some examples here . Now as I understand, a main boon of deep learning is "automatic feature engineering" (aka, "feature learning"). I know that includes feature combinations; but my hunch says that also includes learned feature transformations per the above? So when using deep networks with well-tuned hypers, can feature-transformations safely be removed from the human's responsibilities - that is, throw all this log/square/box-cox stuff away? [Edit] Extra: does this also handle "feature selection" (deciding which inputs not to include) for you?
