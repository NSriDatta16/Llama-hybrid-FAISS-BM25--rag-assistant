[site]: crossvalidated
[post_id]: 602392
[parent_id]: 
[tags]: 
Why is it easy for the Gibbs sampler to take long time to converge to target distribution?

This is related to Gelman's Bayesian Data Analysis 3rd Edition pg 300 first paragraph of Section 12.4. The book says the following. "An inherent inefficiency in the Gibbs sampler and Metropolis algorithm is their random walk behavior ... can take a long time zigging and zagging while moving through the target distribution." For the Metropolis algorithm, the reason for taking a long time to zigging and zagging to converge is not surprising due to jumping distribution choice with narrow "width". This will produce local random walk behavior. I cannot see why the Gibbs sampler has this inherent problem. Suppose I have $p(\theta_1,\dots,\theta_n|y)$ posterior. Denote $\theta_{-i}$ as all $\theta$ 's other than $\theta_i$ . Denote $\theta=(\theta_1,\dots,\theta_n)$ . The Gibbs sampler basically iteratively picks $\theta_1$ from $p(\theta_1|\theta_{-1},y)$ , $\theta_2$ from $p(\theta_2|\theta_{-2},y)$ ,... Each step changes only one component of $\theta$ . Question: What property of the Gibbs sampler drives inefficiency? For Gaussian $p(\theta_1,\dots,\theta_n|y)$ case, it is clear as given in https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Conditional_distributions's covariance and mean of conditional, by noting attenuation of covariance of conditionals. Then inefficiency is apparent. However, I do not find that argument is applicable in full generality as Metropolis algorithm's inefficiency by bad choice of jumping distribution.
