[site]: datascience
[post_id]: 45326
[parent_id]: 45319
[tags]: 
The vanilla decision tree algorithm is prone to overfitting. That's kind of why we have those ensembled tree algorithm. The classics include Random Forests, AdaBoost, and Gradient Boosted Trees. All of those are implemented in sklearn. There are other more advanced variation/implementation outside sklearn, for example, lightGBM and xgboost etc. If you must use the vanilla decision tree, trying to reduce the dimensionality of your inputs might help to reduce overfitting.
