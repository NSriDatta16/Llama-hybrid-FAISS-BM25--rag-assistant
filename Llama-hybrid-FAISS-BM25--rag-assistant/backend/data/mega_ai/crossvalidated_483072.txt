[site]: crossvalidated
[post_id]: 483072
[parent_id]: 
[tags]: 
Why do model weights become less explainable with more training?

For example, I was using a logistic regression on the Fashion-MNIST dataset. This is using sklearn, which uses an iterative training approach, and I was experimenting with the number of iterations. (This is with no regularisation, and using the "multinomial" approach to multiclass, though one-versus-rest yields the same thing.) Here are what the coefficients look like for each class, for different numbers of training iterations: The range of coefficients is increasing with number of iterations: And here are the training and validation accuracies at each number of iterations: What surprises me is that the coefficients become less explainable and intuitive with more iterations of training. For example, for T-shirts, after one training iteration, the coefficients take the shape of a T-shirt. It makes intuitive sense that the dot product of this with an image of a shirt will result in a high value. However, with more training iterations, this shape fades away until the coefficients look like unintepretable noise. One might suspect the model is simply overfitting, but validation accuracy does not start decreasing until after iteration 200, at which point the T-shirt shape is indistinguishable, and even then the overfitting is slight after that point. What is the explanation for this phenomenon? Has the effect been discussed in the literature (does it have a name)? The coefficients after a large number of iterations remind me of the coefficients that are typically learned by neural networks; is there a connection there?
