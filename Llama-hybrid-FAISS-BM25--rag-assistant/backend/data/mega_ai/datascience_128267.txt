[site]: datascience
[post_id]: 128267
[parent_id]: 128266
[tags]: 
For normalizing the data it depends heavily on what you want to do downstream. If you want to use machine learning models based on linear algebra, then normalizing makes sense because otherwise, you can skew your model towards the larger data values. But if the downstream is a tree model or something then it doesn't really matter how you normalize the continuous and categorical features. As for dimension reduction, the main purpose is to reduce the computation cost for the downstream task. So for the linear algebra-based models PCA would be the good call, as the model itself will be correlation based. Whereas if you are going to use a tree for the downstream, then a different criteria could be used, such as using correlation between features or entropy between the feature and target. As this would pre-emptively reduce the amount of computations. All of the techniques should be tailored to the specific purpose of the downstream, and will perform differently depending on the downstream task.
