[site]: crossvalidated
[post_id]: 396987
[parent_id]: 
[tags]: 
Weights not converging while cost function has converged in neural networks

I'm talking in an ideal scenario where a validation set isn't used. Without validation, as many epochs as possible are calculated. Training stops and finishes only when the loss function is minimized and converged. Activation is sigmoid. My loss function is mean squared error, weights being in the squared error, so the loss function is a quadratic function of the weights. There's no hidden nodes. My cost/loss function drops drastically and approaches 0, which looks a sign of convergence. But the weights are still changing in a visible way, a lot faster than the cost function. Why do the weights keep changing?
