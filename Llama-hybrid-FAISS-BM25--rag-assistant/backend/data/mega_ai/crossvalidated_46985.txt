[site]: crossvalidated
[post_id]: 46985
[parent_id]: 46855
[tags]: 
wx + b = -1 and wx + b = 1; These equations represent two parallel hyperplanes that are formed based on samples class (-1, +1). These two hyperplanes are used to optimize the distance between classes and to get optimal hyperplane. For optimization problem Lagrange multipliers are used. You can find brief description on resources listed below. Online course on Machine Learning by Andrew Ng is a great place to understand SVM and other ML algorithms: Machine Learning - Andrew Ng Hyperplane is thoroughly explained. In order to better understand math behind the SVM, learning Optimization is the right choice. There is a great free ebook by S.Boyd: Optimization - Boyd Here you can find SVM papers: svms.org/tutorials/
