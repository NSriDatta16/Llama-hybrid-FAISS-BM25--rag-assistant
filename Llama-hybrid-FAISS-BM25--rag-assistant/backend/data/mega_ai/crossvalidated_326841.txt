[site]: crossvalidated
[post_id]: 326841
[parent_id]: 326840
[tags]: 
As long as you dont train on your test set at any time, any approach is valid. You'll measure the effectiveness of your approach using your dev or test set. What you probably want to do to see if you are getting a benefit from the approach above (which is totally possible, since neural nets are highly empirical, theory lags our results considerably...), is to use cross-validation. Take your training set, set aside eg 20% as a test set. Dont touch this. Then run eg 5-fold or 10-fold cross-validation using the remaining 80% of your test set. In each fold, you'll divide this remaining 80% into 80% train, and 20% dev, run your full procedure above, measure the accuracy. Repeat for the other folds. Take the average accuracy. Is it better/worse/same as what you originally intended to do? There's no obvious way to predict the results of doing this without trying it, in general.
