[site]: datascience
[post_id]: 56055
[parent_id]: 56053
[tags]: 
This is called "catastrophic forgetting" and can be a serious problem in many RL scenarios. If you trained a neural network to recognise cats and dogs and did the following: Train it for many epochs on a full dataset until you got a high accuracy. Continue to train it, but remove all the cat pictures. Then in a relatively short space of time, the NN would start to lose accuracy. It would forget what a cat looks like. It would learn that its task was to switch the dog prediction as high as possible, just because on average everything in the training population was a dog. Something very similar happens in your DQN experience replay memory. Once it gets good at a task, it may only experience success. Eventually, only successful examples are in its memory. The NN forgets what failure looks like (what the states are, and what it should predict for their values), and predicts high values for everything. When something bad happens and the NNs high predicted value is completely wrong, the error can be high, and the NN may have incorrectly "linked" its state representations so that it cannot distinguish which parts of the feature space are the cause of this. This creates odd effects in terms of what it learns about values of all states. Often the NN will behave incorrectly for a few episodes but then re-learn optimal behaviour. But it is also possible that it completely breaks and never recovers. There is lots of active research into catastrophic forgetting and I suggest you search that term to find out some of the many types of mitigation you could use. For Cartpole, I found a very simple hack made the learning very stable. Simply keep aside some percentage of replay memory stocked with the initial poor performing random exploration. Reserving say 10% to this long term memory is enough to make learning in Cartpole rock solid, as the NN always has a few examples of what not to do. The idea unfortunately does not scale well to more complex environments, but it is a nice demonstration. For a more sophisticated look at similar solutions you could see the paper " The importance of experience replay database composition in deep reinforcement learning "
