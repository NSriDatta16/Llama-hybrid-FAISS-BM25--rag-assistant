[site]: datascience
[post_id]: 38669
[parent_id]: 
[tags]: 
Architecture Advice for training a GAN

I'm trying to create a model that generates worlds for a game. The game is 2-dimensional and small worlds sizes. I've sampled about 15,000 worlds that are about 200x200 blocks. Each block has its own id ranging from 0 to around 1500, although not all ids are used. Right now I'm training with 64x64 samples. Here's where I'm seeking advice. Right now I'm normalizing all the blocks into a sequential hash before normalizing them from -1 to 1. If I'm training with ids 0, 2, 3 I would first hash them into 0->0, 2->1, 3->2, then into 0->-1, 2->0, 3->1. I don't know if that's right, because my generator is getting completely weak after a few epochs and the discriminator loss is converging to 0 usually pretty quickly. I know I haven't given much information, but I'm just wondering if another approach might be easier to train. My other idea would be to have a generator that outputs a 64x64x300 where each 64x64 dimension corresponds to a list of probabilities for each x, y for that block (like a softmax?) Any sort of advice would be appreciated, as I'm very much an amateur with machine learning. Thanks, and let me know if you'd like more information for context (I'd thought I would spare the potentially unnecessary details)
