[site]: crossvalidated
[post_id]: 52864
[parent_id]: 52657
[tags]: 
Further to Glen's answer, I think it is nice to think of the problem in terms of the volume/concentration of high dimensional space. Directly from the wikipedia article: There is an exponential increase in volume associated with adding extra dimensions to a mathematical space. For example, $10^2$=100 evenly-spaced sample points suffice to sample a unit interval (a "1-dimensional cube") with no more than $10^{-2}$=0.01 distance between points; an equivalent sampling of a 10-dimensional unit hypercube with a lattice that has a spacing of $10^{-2}$=0.01 between adjacent points would require $10^{20}$ sample points. So basically as the dimension increases, the number of points required to provide the same coverage of space increases exponentially with the dimension. This means that for non-parametric methods, which rely on there being points locally to base an estimator on, far more points are required as the volume explodes. The curse is also often considered in terms of computational feasibility of trying to estimate functions in high dimensions, maybe you could look into this if you're still looking for insight.
