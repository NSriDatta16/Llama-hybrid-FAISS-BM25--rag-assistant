[site]: datascience
[post_id]: 113688
[parent_id]: 74351
[tags]: 
I once wondered the same in case of LightGBM and got this answer from its creator, Guolin Ke: I think in both XGBoost and LightGBM, the CV will use the average scores from all folds, and use this for the early stopping. Therefore, best_iteration is the same in all folds. I think this is more stable since the average_score is computed over all data samples, not just over the current fold. [ https://github.com/microsoft/LightGBM/pull/3204#issuecomment-692455136 ] Relaxing this restriction and letting early stopping rounds number differ between folds gives more accurate CV metrics (averaged across all folds), but it later becomes impractical to try and deploy all 5+ CV models into production, so in practice a single deployment model has to be re-fitted on all data, using the optimal number of rounds established using CV (i.e. taken from all folds models and simply averaged, because arithmetic mean gives you an unbiased estimator of expected value). Optionally one can also use a special correction for an increased number of observations in the training set of the final model (given that we use all available data there, not leaving any for a validation set - see source ): single_model_best_iter = cv_best_iter_mean / (1 - 1 / n_folds)) And finally, trying to use even a small validation set (for early stopping) when training the final deployment model, will most likely not help, and potentially give you a less accurate model (despite guaranteed optimal level of under- vs. overfitting) due to the reduction of the training set size (i.e. training on more data by avoiding any validation sets typically brings more benefits ("more data gives better models") than fine-tuning early stopping rounds to a smaller data set, and the smaller your overall data set, the more true this rule will be, because then every observation counts more).
