[site]: datascience
[post_id]: 6927
[parent_id]: 6925
[tags]: 
There has/is been a lot of hype around the word "Big Data". So, as someone who has been setting up an analytics infrastructure at a fast growing company, I would like to share my experiences. You would like to check this out, which I have written for an almost similar question on CV. As the tools like H2O, R, Julia, Spark, etc; are being evolving for dealing and playing with Big Data, the size of the data is becoming a less of a problem these days. As there is a lots of data available, and a wonderful set of tools at disposal, the art of analytics and data science is becoming more of a pleasure for data scientists rather than pain. But, the real challenge is the availability of people who know what they are doing, who know how to use these tools for mining gold from the data. Just anyone who has run a Linear Regression on a data set can't call himself a data scientist. One has to explain what the data is, what can be pulled out of it, and why is the algorithm applied on it, and why not something else. How do they differ from a small amount of data? In my experience, there isn't any huge differences. The only differences which I have found are algorithms which help find the needle in the hay stack, like MinHash, HyperLogLog, etc; which had to evolve for dealing with the size.
