[site]: crossvalidated
[post_id]: 303278
[parent_id]: 291787
[tags]: 
The same question came into my mind when I did the course. From my understanding, It's mean that we can user a tensor with an extra dimension to deal with all m examples at the same time using the same vectorization approach, instead of looping over the m training examples in the backpropagation method. Let's say we have 5000 training examples and a digit classification problems, so the output of the FNN for the looping approach would be (10,) (a vector). Now with the full vectorized approach, the output will be (5000 x 10) matrix, each row containing the i-th training result. A piece of python code from my Neural Network class looks like: def forward(self, X): """Make the forward step, storing the value of the hypothesis H in each layer for backpropagation. I decided to store the H values (H = sigmoid(Z)) instead the Z as many people do for saving computation time as coping and assign 1 in a column (the bias term) is faster than evaluate the sigmoid(Z) again to compute the derivate for 2nd time. """ # initial H is X + bias term H = np.insert(X, 0, values=1, axis=len(X.shape) > 1) # axis 0 or 1 self.Hs = [H] for Theta in self.Theta: # make the logistic regression for this layer # and store H values for backpropagation H = np.copy(H) H[:, 0] = 1 Z = H.dot(Theta) Z = np.insert(Z, 0, values=1, axis=len(Z.shape) > 1) # axis 0 or 1 H = sigmoid(Z) # Z --> H self.Hs.append(H) # store H with all sigmoid values done self.H = H[:, 1:] # remove the bias term in last step return self.H # return the hypothesis and def grad_backprop(self, X, Y, lamb=0.0): "Compute the gradients using Back Propagation algorithm" self.forward(X) outputs = list() delta = self.H - Y for i in reversed(range(len(self.Theta))): H = self.Hs[i] # we need to add the full bias term # so make a copy and overwrite the 1st column H = np.copy(H) H[:, 0] = 1 DTheta = np.einsum('...i,...j->...ij', H, delta) if len(DTheta.shape) > 2: # average of all gradients per sample DTheta = np.average(DTheta, axis=0) outputs.append(DTheta) # we can avoid the rest of the computation # in the last step if i == 0: break # sigmoid gradient H = self.Hs[i] sigrad = H * (1 - H) Theta = self.Theta[i] delta = delta.dot(Theta.T) * sigrad delta = delta[:, 1:] # remove the bias outputs.reverse() outputs = np.array(outputs) # regularization if lamb > 0.0: m = Y.shape[0] Theta = np.copy(self.Theta) for th in Theta: th[0] = 0 outputs += Theta * lamb / m return np.array(outputs) I hope this helps Tom Don't hesitate to contact me if you want to see the full code or whatever you need.
