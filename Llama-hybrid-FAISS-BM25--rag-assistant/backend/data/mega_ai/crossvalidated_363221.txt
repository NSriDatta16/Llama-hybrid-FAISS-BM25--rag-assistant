[site]: crossvalidated
[post_id]: 363221
[parent_id]: 363208
[tags]: 
Interesting question! I would say from a statistical modelling perspective, all data are assumed to come from a combination of systematic components and a stochastic component, which means that data-generating processes as modeled by statistics, are assumed to be non-deterministic in nature. Wikipedia even states that: A statistical model is a special class of mathematical model. What distinguishes a statistical model from other mathematical models is that a statistical model is non-deterministic . I will also give you my perspective as a former biologist, where we usually explain that statistical modelling (at least frequentist) is something like the following: Which ironically you could interpret as the stochastic part being only 'apparently stochastic', since it relies on things that we did not or cannot measure.$^*$ Even 'random mutations' are eventually caused by a large sum of things we cannot measure, such as exposure to sunlight, failure of repair mechanisms, etc., but for all practical intents and purposes, it might as well be non-deterministic. However, even if we could measure all those things perfectly, all down to the molecular level, we would have to concede that effects on a molecular scale are in turn influenced by things on a quantum scale, and... well, there are limits to the precision with which quantities can be measured (uncertainty principle). We run into the uncertainty principle, which would imply that in the end, the processes we model with statistics are random (if my understanding of it is correct). That being said, a model is just that, a model . And I don't think statistics as a field takes a point of view on the nature of the universe. After all, that is not our field of research. At best you could argue that you implicitly assume the data-generating process is random by making use of statistical models. $^*$ In other words, the sum of a large number of unobserved, independent effects create an (apparently) random deviation from the systematic effects. (In case of a large sum of uniformly distributed effects this nicely explains why we often assume normality, but I'm getting off track.) Where in the methodology does it matter? Well, basically for everything! A model without an stochastic part is not considered a statistical model. This even applies to those who do not consider machine learning and statistics to be the same. Even from the point of stochastic optimization (the name gives it away a bit) , a model cannot be trained further from new data if the loss function is already exactly zero.
