[site]: crossvalidated
[post_id]: 627105
[parent_id]: 626996
[tags]: 
I think there are a lot of things to touch on here. To start with, obviously this framework mostly comes up in Bayesian statistics, where $f(\theta)$ is the prior/posterior and $p_f(x) := \int_\Theta f(\theta)b_\theta(x) d\theta$ would be prior/posterior predictive distribution ( https://en.wikipedia.org/wiki/Posterior_predictive_distribution ). Now the important thing to know is that the Bernoulli distributions are a very special case and in general $ p_f \not\in \{b_\theta: \theta \in \Theta\}!$ For example if you take the classic set up of $\Theta$ being binomial distributions and $f$ being a beta-distribution, then $p_f$ will be a beta-binomial distribution ( https://en.wikipedia.org/wiki/Beta-binomial_distribution ). Very much not the same thing! There are a few more exceptions, especially with normal distributions, but generally speaking the normally used parameter/distribution spaces are not closed under this conflating and therefore your mapping does not exist. In closing the "statistics part" of Bayes we change $f$ , but $\Theta$ stays the same, so why conflate them? It's just unnecessary complexity. Afterwards, if we want to bet on the super bowl or something, we do put them together. If two different priors/posteriors truly produce identical predictions over all possible scenarios, then I would call them equivalent, but this would mostly imply that $\Theta$ was unnecessarily complex to begin with.
