[site]: crossvalidated
[post_id]: 598398
[parent_id]: 
[tags]: 
Is XGBoost too much to apply on my data?

My data has 1530 samples and about 50 features. Not all features are used, some are removed after a feature selection process. Now I'm facing overfitting, and one solution to overfitting is regularization, making the model simpler and forcing it to behave in some way. [My train set gets AUC of 0.9 and the test set bearly gets an AUC of 0.65]. XGBoost is a powerful algorithm, and it is basically a gradient-boosting algorithm but with regularization added, which is great. However, someone once told me, using XGBoost on such small data is like dropping a nuclear bomb to destroy a building. So it is too powerful that it will not only fit the data but also the noise, meaning it will learn the data too well, leading to overfitting eventually. Is this correct? I've used gradient boosting until now, and I tried XGboost once but I didn't really give the effort to make it work. I'm asking here because I got some mixed opinions about this, should or should I not use this algorithm? I need some non-linear algorithm that adds regularization to the problem. Thanks!!
