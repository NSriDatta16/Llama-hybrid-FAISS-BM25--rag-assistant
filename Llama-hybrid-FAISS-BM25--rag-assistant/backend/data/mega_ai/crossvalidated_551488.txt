[site]: crossvalidated
[post_id]: 551488
[parent_id]: 
[tags]: 
What to consider when choosing between f-divergence measures? (e.g.: kl-divergence, chi-square divergence, etc.)

I have some baseline population, and I have a non random sample from that population. For both the population and the sample I have observation of some measure (for simplicity, let's say age). I would like to measure how "un-similar" my sample is from the target population, and would like to use a divergence measure (in which the $P$ is the distribution of age in the population and $Q$ the distribution of age in the sample). I know that the f-divergence includes various known options such as kl-divergence , chi-square divergence, and others. But it's not clear to me how to prefer or decide which divergence to choose. Are there some known considerations on when using one over the other is beneficial? (in terms of interpretation, or decision making?) For example, I know that various machine learning algorithms might use kl-divergence (or cross entropy) for optimizing, but it's not clear to me why that is better or worse then, say, chi-square divergence. Thanks.
