[site]: crossvalidated
[post_id]: 416246
[parent_id]: 416242
[tags]: 
In a rather narrow sense, you are correct (1): if $$\pi(\theta|x) \propto L(\theta|x)$$ then $$\underbrace{\arg \max_\theta \pi(\theta|x)}_\text{MAP estimate} = \underbrace{\arg \max_\theta L(\theta|x)}_\text{ML estimate}$$ However, this coincidence is not of considerable interest as: [Re. (1) and (3):] It is not invariant by repameterisation, i.e., the flat prior is only flat for one pameterisation. If one considers $\xi=h(\theta)$ as the new pameterisation of the model, the prior of $\xi$ is no longer flat and the MAP then differs from the MLE. In your example, the prior on $\sigma^2$ is $\sigma^{-2}$ not a [Re. (2):] The purpose of Bayesian analysis is not to return point estimates but a whole distribution on the parameter, conditional on the data. The posterior distribution provides optimal decisions (based on a loss function) and uncertainty quantification. [Re. (2):] The MAP estimator is fringe Bayesian in that there does not exist a conventional loss function that returns the MAP as the optimal decision. It further depends on the choice of the dominating measure. [Re. (3):] My almost prehistorical understanding of BUGS (i.e., circa 1992) is not accepting flat priors as far as I know. [Re. (3):] Regarding the R code, the function mnf returns the log-likelihood, not the maximum of the likelihood.
