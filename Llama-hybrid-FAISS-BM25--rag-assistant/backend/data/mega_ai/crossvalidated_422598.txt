[site]: crossvalidated
[post_id]: 422598
[parent_id]: 422414
[tags]: 
Quantifying dependence, given particular values of $Z_1,Z_2$ First, consider the case where $Z_1,Z_2$ are known to take particular values $z_1,z_2$ . Given this knowledge, the conditional dependence between $X_1$ and $X_2$ can be quantified using the KL divergence between the joint distribution and the product of the marginal distributions: $$D_{KL} \big( p(x_1,x_2 \mid z_1,z_2) \parallel p(x_1 \mid z_1,z_2) p(x_2 \mid z_1,z_2) \big) \tag{1}$$ You can think of this as measuring the dissimilarity between the true joint distribution and what it would be if $X_1$ and $X_2$ were conditionally independent (i.e. the product of the marginals). Notice the similarity between this quantity and the ordinary (unconditional) mutual information. The only difference is that, here, that we're conditioning on $Z_1=z_1, Z_2=z_2$ . Alternatively, using the link between KL divergence and conditional entropy, expression $(1)$ is equivalent to: $$H(X_1 \mid Z_1=z_1, Z_2=z_2) \ - \ H(X_1 \mid X_2, Z_1=z_1, Z_2=z_2) \tag{2}$$ That is: suppose we already know values $z_1,z_2$ . Now, we are additionally told the value of $X_2$ . How much does this reduce our uncertainty about $X_1$ on average? (where the average is taken over all possible values of $X_2$ ). Conditional mutual information Expression $(1)$ above is perfectly satisfactory if we're interested in particular values of $Z_1,Z_2$ . Alternatively, we might like to quantify the conditional dependence between $X_1,X_2$ over all values of $Z_1,Z_2$ . The dependence structure may change depending on these values. This leads to the definition of the conditional mutual information: $$I(X_1, X_2 \mid Z_1, Z_2) = $$ $$E_{p(z_1, z_2)} \Big[ D_{KL} \big( p(x_1, x_2 \mid z_1, z_2) \parallel p(x_1 \mid z_1, z_2) p(x_2 \mid z_1, z_2) \big) \Big] \tag{2}$$ The conditional mutual information simply averages the KL divergence in expression $(1)$ over all possible values of $Z_1,Z_2$ , weighted by the probability of each. The expression for conditional mutual information given in the question (for discrete variables) is equivalent to expression $(2)$ here. You can verify this by writing out the definition of the KL divergence, then rearranging things using the definition of conditional probability. The quantity $I'$ Maybe I should just use the different quantity $I'(X_1,X_2 \mid Z_1,Z_2)$ [.....] does this quantity have a name? I'm not aware of a name for this quantity, but it's proportional to the KL divergence in expression $(1)$ above: $$p(z_1, z_2) \ I'(X_1,X_2 \mid Z_1,Z_2) =$$ $$D_{KL} \big( p(x_1,x_2 \mid z_1,z_2) \parallel p(x_1 \mid z_1,z_2) p(x_2 \mid z_1,z_2) \big)$$ $p(z_1, z_2)$ is a constant, since $z_1,z_2$ are considered fixed values here. So, as in $(1)$ , your quantity $I'$ is measuring the dependence between $X_1,X_2$ given particular values for $Z_1,Z_2$ . But, I think $(1)$ is a bit more natural to interpret than $I'$ .
