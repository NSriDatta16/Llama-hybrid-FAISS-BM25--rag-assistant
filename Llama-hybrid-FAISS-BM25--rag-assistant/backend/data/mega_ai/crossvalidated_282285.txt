[site]: crossvalidated
[post_id]: 282285
[parent_id]: 282087
[tags]: 
Yes, I think the estimate holds for "you can achieve this performance* by training a model that auto-tunes hyperparameters this way". Still, there's always the question: if optimiziation is not stable, did it work at all? So the important point is that you need to think what this instability means. You may have a situation where there truly is no single best set of hyperparameters. While you could reasonably expect that for a single hyperparameter like the penalty of a LASSO a (global) optimum exists. But when comparing different algorithms you may have very similar performance achieved by different algorithms (think e.g. LDA, logistic regression and linear SVM). Best case: easy problem, different hyperparameter sets achieve perfect classification a not so good variant of this is: splitting into the CV sets does not yield independent sets of cases (e.g. because of hierarchical/clustered data structure) -> there may be heavy overfitting as a consequence -> if the inner CV sees only "perfect" classification, it doesn't know how to choose. => check inner performance estimates and use proper scoring rules for optimization not perfect, but e.g. the 3 linear models mentioned above all show up in the winner parameters -> linear model achieves xxx performance, and it doesn't matter which one you choose. * IMHO you need to put all interpretations into relation of what you know about "this performance": measurement of generalization error has variance uncertainty (and bias) like any other measurement. Bias is going to be slightly pessimistic if the splitting is done properly in your case, but variance uncertainty crucially depends on the number of cases available. So the questions are: Is the uncertainty on the final (outer) performance estimate small enough to distinguish whether the model answers your application problem well or not so well or not at all? Is the uncertainty on the inner performance estimates small enough to make selection of an apparently best model a sensible approach? One question I'd always ask as well: compared to a simple "baseline" model where you fix all hyperparameters in advance by your knowledge about the problem, did the optimization yield any improvement (keeping in mind the uncertainty)? (In my field I encounter problems where - due to the very small sample size that is available - I can say after checking such a baseline model performance: I don't need to start an optimization because I won't be able to show that the "optimal" model is better than the one I already have unless I get far more cases.)
