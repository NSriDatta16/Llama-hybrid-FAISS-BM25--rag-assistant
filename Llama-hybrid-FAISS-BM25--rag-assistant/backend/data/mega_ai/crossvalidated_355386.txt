[site]: crossvalidated
[post_id]: 355386
[parent_id]: 
[tags]: 
Why do the terms in L2 Regularized likelihood represent Gaussian distributions?

Studying machine learning, I've made it to the point where I've exponentiated my L2 Regularization loss function. We began with a simple ordinary least squares loss function, and added a penalty term proportional to the squared weights of the coefficients, as seen below: Because minimizing the least squared loss function is equal minimizing the negative log-likelihood, we flipped the signs so that maximizing the negative loss function is equal to maximizing the log likelihood. Then we've exponentiated to get rid of the log function, yielding the $\exp{\{-J\}} $term below. Now I'm told that these represent two gaussians. I have the following two expressions: I know that $J = $ ($-$ log likelihood), thus $ -J =$ (log likelihood), thus $\exp{\{-J\}} = $ likelihood. What I'm confused by is how do the expressions in the first image represent the gaussians below? Or rather, why do/can I add $\frac{1}{2\sigma^2}$ inside the exponentiation and normalize by the constant? I'm missing the connection between them. Side note: as I was following my material I thought I had the connection, but I got bogged down in some computations and believe I lost sight of the connection here. Note: This question follows from my other post in which I was trying to prove what the instructor states, which is that the second expression in the $\exp{\{-J\}}$ term represents a Gaussian with $\mu = 0$ and $\sigma^2 = \frac{1}{\lambda}$, but I was getting a different answer. I can prove this about the Prior probability at the bottom, but the instructor said it referring to the second expression in $\exp{\{-J\}}$
