[site]: crossvalidated
[post_id]: 498859
[parent_id]: 
[tags]: 
Determinant =1 constraint in PCA reconstruction Error

Let $q\leq p$ . As in Tibshirani's statistical learning book, one can describe the PCA problem as optimizing the $q$ -dimensional reconstruction error, given on a dataset $\{x_n\}_{n=1}^N$ in $\mathbb{R}^p$ by $$ \inf_{\mu\in \mathbb{R}^p,\lambda \in \mathbb{R}^q,V_qV_q^{\top}=I_d}\, L(\mu,V_q,\lambda) \triangleq \frac1{N}\sum_{n=1}^N\left\| x_n - V_q\lambda - \mu \right\|, $$ for the parameters $\mu\in \mathbb{R}^p$ , $\lambda \in \mathbb{R}^q$ , and (most notable for this question) the orthogonal matrix $V_q$ . My question is, is there any benefit to further constraining $V_q$ to have determinant $1$ (called a special orthogonal matrix)? That is, do people consider the constrained PCA problem $$ \inf_{\mu\in \mathbb{R}^p,\lambda \in \mathbb{R}^q,V_qV_q^{\top}=I_d,\, \det(V_q)=1}\,L(\mu,V_q,\lambda) \triangleq \frac1{N}\sum_{n=1}^N\left\| x_n - V_q\lambda - \mu \right\|? $$ If so, what is the advantage/interpretation over classical PCA?
