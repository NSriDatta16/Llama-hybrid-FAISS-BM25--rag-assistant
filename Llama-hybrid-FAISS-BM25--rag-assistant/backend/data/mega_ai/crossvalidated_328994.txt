[site]: crossvalidated
[post_id]: 328994
[parent_id]: 
[tags]: 
Panel data which number do I report as sample size?

I have a panel data-set across three waves (year 0, year 5 and year 10) which includes the health outcomes and behaviors of respondents. In Stata I do the following in order to prepare my data for analysis: reshape long bin_residence_y psum_unemployed_total_cont_y household_income_y health_y current_county_y binary_health_y bmi_y binbmi_overweight_y binbmi_underweight_y binbmi_obese_y ord_bmi_y any_tobacco_y only_cigarettes_y occ_cigarettes_y reg_cigarettes_y no_cigs_cons_deflated_y no_cigs_cons_more10_y triedstopsmoking_y times_quit_cigarettes_y smokeintention_y smokeintention_binary_y lastdrank_y usually_drink_y days_drink_y drink_count_y prescribed_medication_y bin_mild_ex_y mild_exercise_y bin_moderate_ex_y no_activity_y strenuous_exercise_y bin_strenous_ex_y moderate_exercise_y residence_y accommodation_y home_owner_y bin_home_owner_y health_insurance_y own_education_y age_y ord_age_y medical_card_y employment_y binary_employment_y binmartatus_y, i(id) j(year) xtset id year quietly tab1 year, gen(yr) drop if year == 10 Looking at the excel file that this dataset is made from, I know that the number of people to report their self-rated health (binary_health_y) in year 0 is 1095 and in year 5 is 558, I am not interested in year 10. When I run the following regression of binary self-rated health as good or bad (binary_health_y) and the total number of people unemployed in the county in which a person lives (psum_unemployed_total_cont_y) xtlogit binary_health_y psum_unemployed_total_cont_y Fitting comparison model: Iteration 0: log likelihood = -973.25989 Iteration 1: log likelihood = -973.24175 Iteration 2: log likelihood = -973.24175 Fitting full model: tau = 0.0 log likelihood = -973.24175 tau = 0.1 log likelihood = -969.80706 tau = 0.2 log likelihood = -966.3578 tau = 0.3 log likelihood = -962.98739 tau = 0.4 log likelihood = -959.84848 tau = 0.5 log likelihood = -957.20783 tau = 0.6 log likelihood = -955.57269 tau = 0.7 log likelihood = -956.02716 Iteration 0: log likelihood = -955.57204 Iteration 1: log likelihood = -946.86439 Iteration 2: log likelihood = -946.5548 Iteration 3: log likelihood = -946.55444 Iteration 4: log likelihood = -946.55444 Random-effects logistic regression Number of obs = 1,613 Group variable: id Number of groups = 1,077 Random effects u_i ~ Gaussian Obs per group: min = 1 avg = 1.5 max = 2 Integration method: mvaghermite Integration pts. = 12 Wald chi2(1) = 0.02 Log likelihood = -946.55444 Prob > chi2 = 0.8855 ---------------------------------------------------------------------------------------------- binary_health_y | Coef. Std. Err. z P>|z| [95% Conf. Interval] -----------------------------+---------------------------------------------------------------- psum_unemployed_total_cont_y | -.0071684 .0497788 -0.14 0.885 -.1047331 .0903964 _cons | 1.384735 .4005664 3.46 0.001 .5996391 2.16983 -----------------------------+---------------------------------------------------------------- /lnsig2u | 1.187308 .2524483 .6925184 1.682097 -----------------------------+---------------------------------------------------------------- sigma_u | 1.810592 .2285404 1.413769 2.318798 rho | .4991151 .0631119 .3779334 .6204009 ---------------------------------------------------------------------------------------------- LR test of rho=0: chibar2(01) = 53.37 Prob >= chibar2 = 0.000 What I'm confused about is, what do I say when I'm asked about my sample size? Is it the 1,077 reported in the logit model? Or is a number which comes from the excel file (i.e. only those respondents who have data across both waves analysed). And what is the justification for what I should report? Obviously my preference is to report the number from my xtlogit regression because it is much larger than the 500 people or so that the excel file suggests exist across both waves but is there a way that I can adequately justify this? i.e. does the random effects model in itself support this in some theoretical way?
