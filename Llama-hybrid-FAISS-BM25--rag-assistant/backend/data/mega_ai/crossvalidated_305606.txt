[site]: crossvalidated
[post_id]: 305606
[parent_id]: 
[tags]: 
How to determine the optimal ratio of a type I and type II error in a given business context?

Let us assume I have developed two designs (A, B) of a product and I want to see which one performs better in terms of sales. So I run an experiment where some customers can buy A whilst others can buy B. This gives me two means (e. g. the A-customers bought on average 5x, the B-customers 7x) with which I perform a t-test. Let us further assume that the consequences of a type I error and a type II error are as follows: type I error: I change the design of the product but (contrary to what I believe) this has absolutely no effect on sales. type II error: I do not change the product's design although I should have done so because it would have earned me some additional cash, say $100 in each of the upcoming months. Under the assumption that changing the manufacturing process to produce the new design creates no additional costs, the type II error is obviously more critical than the type I error. So my inclination would be to set the significance level $\alpha$ higher than the usual $\alpha=0.05$ and certainly higher than $\beta$ (which of course I have to control via $n$ and the effect size). But by how much? What would be the optimal ratio $q=\beta/\alpha$? EDIT: @whuber You seem to think that my question is unclear in a general sense or needs a data background, but the exact same question is also discussed elsewhere, see for example Which Statistical Error Is Worse: Type 1 or Type 2? (albeit without the concrete business context I'm offering here). 2. EDIT: Here is a study that tackles the above problem: Setting an Optimal Î± That Minimizes Errors in Null Hypothesis Significance Tests .
