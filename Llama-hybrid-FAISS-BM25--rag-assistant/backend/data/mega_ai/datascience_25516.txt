[site]: datascience
[post_id]: 25516
[parent_id]: 16225
[tags]: 
How is your experience using feature normalization with boosted trees does it in general improve our models? My rather limited experience with scaling of features suggests that it has virtually no impact on xgboost results. I suppose by normalisation you mean subtracting the mean and then dividing by standard deviation. If you calculated the statistics based on entire dataset (including holdout) you would get data leakage, which might indeed, at least theoretically, degrade the performance on holdout. According to my understanding of xgboost, the correctly performed scaling should have no impact on the performance. I suggest you double check your implementation or provide more details on how you do it, preferably with including a reproducible example.
