[site]: crossvalidated
[post_id]: 553631
[parent_id]: 553624
[tags]: 
All of F1, recall, precision (and others) rely crucially on two-class classification. Essentially, they need a notion of true/false positive/negative, which only makes sense if you have one target class and "everything else". Thus, in a multiclass scenario, you can assess (say) the F1 score of classifying one of your class, which then is the target class, and everything else is the non-target class. And of course you can do this exercise with every separate one of your classes. This is exactly the output you are getting. Thus, there is simply no notion of "overall" F1, precision, recall etc. What you can do is to calculate the averages of these KPIs, possibly weighted by how often the target class appears in your test set. Note that every single criticism of accuracy at the following thread applies equally to F1, precision, recall etc.: Why is accuracy not the best measure for assessing classification models? Specifically, optimizing any of these will give you biased predictions of the true probabilities of class memberships, and suboptimal decisions, and the same applies to optimizing weighted or unweighted averages of these KPIs. Instead, use probabilistic classifications and assess these using proper scoring rules - and note also that proper scoring rules have no problems whatsoever with multiclass situations.
