[site]: crossvalidated
[post_id]: 529723
[parent_id]: 313857
[tags]: 
Support Vector Machines come in a few flavors. Hard margin SVMs demand linear separability to construct the equidistant margins parallel to the hyperplane where all samples must sit either on or outside the margins. Soft margin SVMs allow samples to cross the margins by giving them 'slack'. In a soft margin SVM the problem involves minimising slack while maximising margin distance from the hyperplane. In a one class SVM problem used to detect outliers the opposite is observed. This problem involves minimising both slack and margin distance, such that any samples which step outside the margins can be classified as outliers. By controlling hyperparameters C and gamma you have control over how the SVM priorities minimisation of slack and minimisation of margin distance. If we assume the data is standardised with zero mean and unit variance we know that the 'hyperplane' will go through the origin, but this hyperplane isn't used for classification, it simply orients the margins, and the position of the samples relative to the margins is what 'classifies' samples as outliers. To me it would be more intuitive to create a boundary around the positive class from all direction not just from the origin. Again, SVMs work best with standardised data anyway, so you should be feeding that to your SVM anyway. And remember, SVMs don't necessarily create just linear hyperplanes and margins, through the use of the kernel trick with something like the RBF kernel it's possible for the 'outlier' boundary to very much be non-linear.
