[site]: crossvalidated
[post_id]: 401011
[parent_id]: 400998
[tags]: 
Expanding a little bit what Sycorax said, the basic recurrent cell is something like $$ h_t = \tanh(W_{hh}h_{t-1} + W_{xh}x_t) $$ It is a function of previous hidden state $h_{t-1}$ and current input $x_t$ , and returns current hidden state $h_t$ . Same applies to LSTM cell that is a special kind of RNN. So it does not "look" directly at any input in the past, the information from the past is passed only through the hidden states $h_t$ . What follows, in theory all the history to some extent contributes to $h_t$ . If you have multiple such cells, then they do not look directly at different points in time, but just use different weights. Of course, it can be the case that some of the hidden states will carry more information from points that are further in time, as compared to other hidden states, but the kind of information that is carried is something that is learned from the data, rather then forced.
