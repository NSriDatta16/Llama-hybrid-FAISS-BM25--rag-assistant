[site]: crossvalidated
[post_id]: 207300
[parent_id]: 206997
[tags]: 
Probabilistic canonical correlation analysis (probabilistic CCA, PCCA) was introduced in Bach & Jordan, 2005, A Probabilistic Interpretation of Canonical Correlation Analysis , several years after Tipping & Bishop presented their probabilistic principal component analysis (probabilistic PCA, PPCA). Very briefly, it is based on the following probabilistic model: \begin{align} \newcommand{\z}{\mathbf z} \newcommand{\x}{\mathbf x} \newcommand{\y}{\mathbf y} \newcommand{\m}{\boldsymbol \mu} \newcommand{\P}{\boldsymbol \Psi} \newcommand{\S}{\boldsymbol \Sigma} \newcommand{\W}{\mathbf W} \newcommand{\I}{\mathbf I} \newcommand{\w}{\mathbf w} \newcommand{\u}{\mathbf u} \newcommand{\0}{\mathbf 0} \z &\sim \mathcal N(\0,\I) \\ \x|\z &\sim \mathcal N(\W_x \z + \boldsymbol \m_x, \P_x)\\ \y|\z &\sim \mathcal N(\W_y \z + \boldsymbol \m_y, \P_y) \end{align} Here noise covariances $\P_x$ and $\P_y$ are arbitrary full rank symmetric matrices. If we consider 1-dimensional latent variable $z$ , assume that all means are zero $\m_x=\m_y=0$ , and combine $\x$ and $\y$ into one vector, then we get: $$\begin{pmatrix} \x\\ \y\end{pmatrix}\sim\mathcal N (\0,\S),\quad\quad\quad\S=\begin{pmatrix}\w_x\w_x^\top+\P_x & \w_x\w_y^\top \\ \w_y\w_x^\top & \w_y\w_y^\top+\P_y\end{pmatrix}.$$ Bach & Jordan proved that this is equivalent to standard CCA. Specifically, the maximum likelihood (ML) solution is given by $$\w_i = \S_i\u_i m_i,$$ where $\S_i$ are sample covariance matrices of both datasets, $\u_i$ is the first canonical pair of axes, and $m_x m_y = \rho_1$ are arbitrary numbers (both between $0$ and $1$ ) giving first canonical correlation as a product. As you see, $\w_i$ are not directly equal to the CCA axes, but are given by some transformation of those. See Bach & Jordan for more details. I don't have a good intuitive grasp of PCCA. As you can see, the cross-covariance matrix between $X$ and $Y$ is modeled by $\w_x \w_y^\top$ , so one could naively expect $\w_i$ to rather yield PLS axes. The ML solution is however related to the CCA axes. It probably is somehow due to the block-diagonal structure of $\P=\begin{pmatrix}\P_x & \0\\ \0 & \P_y\end{pmatrix}$ . I am not aware of any similar probabilistic versions of RRR or PLS, and have failed to come up with any myself. Note that if $\P$ is diagonal then we obtain factorial analysis (FA) on the combined $X+Y$ dataset, and if it is diagonal and isotropic then we get PPCA on the combined dataset. So there is a progression from CCA to FA to PPCA, as $\P$ gets more and more constrained. I don't see what other choices of $\P$ can be reasonable.
