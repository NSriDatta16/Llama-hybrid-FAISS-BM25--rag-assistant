[site]: crossvalidated
[post_id]: 359875
[parent_id]: 
[tags]: 
PCA and data normalization

I have a database where the number of features is about 60 times greater than the number of samples (60000 to 1000). I want to use some dimensional reduction techniques, including PCA. Data are all on the same 'unit of measure' (they are RNA-exp data), but they have very different scales (std from about 0 to about 1e9) and magnitudes. Obviously, scaling or not scaling the data leads to very different results for PCA (in particular, only about 15 components are enough to explain 99% of variance if one does not scale the data, while about 500 are needed if one does). I understand the reason behind this behavior. My question is: in this setting (same "unit", different magnitude), is it generally recommended to scale the data or not? The ultimate goal is to use PCs as features of a classifier. Is there a general rule of thumb? How should one decide? Should one consider something else in case another technique (say LDA) is used for dimensionality reduction? Thanks
