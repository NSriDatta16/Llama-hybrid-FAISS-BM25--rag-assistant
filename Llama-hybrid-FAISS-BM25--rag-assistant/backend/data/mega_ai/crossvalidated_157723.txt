[site]: crossvalidated
[post_id]: 157723
[parent_id]: 157712
[tags]: 
You are right, and one source is the original LASSO paper by Tibshirani (JRSSB 1996) . In the LASSO, choose $\hat\beta^L_\lambda$ so as to minimize $$ (\tilde{y}-X\beta)'(\tilde{y}-X\beta)+\lambda\sum_{j=1}^{K}|\beta_j|, $$ where $\tilde{y}_i$ is $y_i$ demeaned and the $x_{ij}$ are standardized. The Bayesian interpretation is as follows. Consider independent mean zero Laplace priors for $\beta$ with common scale parameter $b=2/\lambda$. Assume $\sigma^2=1$ to be known for simplicity (see Park and Casella, JASA 2008 for the more general case, which implies some nontrivial additional issues!). The posterior then is proportional to \begin{eqnarray*} \pi(\beta|\tilde{y},X)&\propto&\exp\left\{-\frac{1}{2}(\tilde{y}-X\beta)'(\tilde{y}-X\beta)\right\}\prod_{j=1}^{K}\frac{\lambda}{4}\exp\left\{-\frac{\lambda}{2}|\beta_j|\right\}\\ &\propto&\exp\left\{-\frac{1}{2}(\tilde{y}-X\beta)'(\tilde{y}-X\beta)-\frac{\lambda}{2}\sum_j|\beta_j|\right\} \end{eqnarray*} Hence, $\hat\beta^L_\lambda$ is the MAP under this Laplace prior, as the $\beta$ that maximizes this expression minimizes $$(\tilde{y}-X\beta)'(\tilde{y}-X\beta)+\lambda\sum_j|\beta_j|.$$
