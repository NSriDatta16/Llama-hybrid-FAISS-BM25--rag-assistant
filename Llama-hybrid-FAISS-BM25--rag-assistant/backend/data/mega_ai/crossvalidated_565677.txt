[site]: crossvalidated
[post_id]: 565677
[parent_id]: 
[tags]: 
Definition of autocorrelation?

EDIT: Apparently I was not thorough enough in my search for previous questions on this topic. This is pretty much the same question, and has been answered: Auto-correlation vs Auto-covariance The answer seems to be cultural, as I was hypothesizing myself. Now the dilemma is whether, as a teacher in an engineering degree, I should continue the tradition, or tell them the statistics definition ;) I will let the question be, perhaps it will make it easier to google it (since I didn't find the other one when I was searching). To begin with, this is what I have always thought of as covariance vs correlation: $$ cov(X,Y)=E[(X-E[X])(Y-E[Y])] \\ corr(X,Y)=\frac{cov(X,Y)}{\sqrt{\sigma_X\sigma_Y}} $$ where the $\sigma$ 's are the standard deviations. Autocovariance and autocorrelation are then simply the covariance and correlations between the same time series at different times. However, according to this book: https://www.cambridge.org/highereducation/books/applied-digital-signal-processing/D8CDA7C7B784A7602E072B6C4A986D23#overview Autocorrelation is defined as simply $$ E[X[n]X[m]] $$ , whereas autocovariance is (still) $$ E[(X[n]-E[X[n]])(X[m]-E[X[m]])] $$ This conflicts with my previous statistics upbringing, where correlation has always been the scaled version of covariance. Looking at wikipedia ( https://en.wikipedia.org/w/index.php?title=Autocorrelation ) , I find this (very confusing) definition: In statistics, the autocorrelation of a real or complex random process is the Pearson correlation between values of the process at different times, as a function of the two times or of the time lag. Let $\left\{X_{t}\right\}$ be a random process, and $t$ be any point in time ( $t$ may be an integer for a discrete-time process or a real number for a continuous-time process). Then $X_{t}$ is the value (or realization) produced by a given run of the process at time $t$ . Suppose that the process has mean $\mu_t$ and variance $\sigma_t^2$ at time $t$ , for each $t$ . Then the definition of the auto-correlation function between times $t_1$ and $t_2$ is: $$ R_{XX}(t_1,t_2)=E[X_{t_1}\bar{X_{t_2}}] $$ So that seems to match the book I'm reading, but the wikipedia text leading up to it seems to better match my definition (both by invoking Pearson and by the mention of $\mu$ and $\sigma$ ). Wikipedia cites two books which seem to be from engineering, and so is the one I'm reading, so I was wondering if this is a culture thing? At some point an engineer forgot that he was assuming $\mu=0, \sigma=1$ , and now the definitions differ between fields? As a further sanity check, I looked up the definition in matlab, and found that it matches my own version: https://se.mathworks.com/help/econ/autocorr.html Has anyone come across this before? Is the point that there is some subtle, historical difference between correlation and correlation coefficient that is confusing me and the people at mathworks?
