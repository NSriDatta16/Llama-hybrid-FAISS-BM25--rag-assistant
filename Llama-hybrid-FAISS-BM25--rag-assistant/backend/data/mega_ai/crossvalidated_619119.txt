[site]: crossvalidated
[post_id]: 619119
[parent_id]: 
[tags]: 
Is there a natural extension of the ACF/PACF to more general measures of dependency?

Assume that we have a time series that we want to model as a stationary real-valued stochastic process: $$X_t , t \in \{0, 1, \dots \}.$$ Two complementary measures of linear dependence between the lagged versions of $X_t$ $(X_{t-1}, X_{t-2}, \dots)$ and $X_t$ would be the autocorrelation and partial autocorrelation functions of $X_t$ with lag $k \in \{0, 1, \dots\}$ . For a lag $k$ , the autocorrelation function of $X_t$ is defined as $\text{corr}(X_{t-k}, X_t)$ which, in practice, can be estimated by calculating the correlation between a vector of observations over time lagged by $k$ and a vector of the observations over time without lag. For a given lag $k$ , the partial autocorrelation function of $X_t$ is defined as the correlation between $X_{t-k}$ and $X_t$ when the linear dependence of lags $1$ to $k-1$ are removed. An intuitive appoach to calculation/understanding the PACF is detailed in this post where an AR( $k$ ) model of $X_t$ is constructed and the PACF of $X_t$ at lag $k$ is shown to be the $(k+1)$ -th coefficient of the AR model. This can be estimated in practice in a way similar to the ACF, among other ways. Is there a natural extension of this method to find out what dependence a process has on lags of said process for measures of dependence that take non-linearity/other dependencies into account? If we choose the Mutual Information as our dependency measure, the Auto Mutual Information function (obtained by replacing the correlation function from the ACF with the Mutual Information between the two random variables) evaluated at various lags seems to be an approachable enough extension to estimate from data as there are various ways to approximate Mutual Information, but how would one approach the Partial Auto Mutual Information which controls for the potentially nonlinear effect of intermediary lags? Or what of a more general similarity metric such as the Normalized Compression Distance (detailed in papers such as this ) which measures similarity between string/binary representations of data through an approximation of Kolmogorov Complexity by lossless compression? Again, the non-partial case seems to come out rather naturally, but my intuition breaks down on the partial cases for these two dependency measures. Any references/input on this sort of "generalization" of auto-dependency functions would be greatly appreciated. Thanks in advance. :)
