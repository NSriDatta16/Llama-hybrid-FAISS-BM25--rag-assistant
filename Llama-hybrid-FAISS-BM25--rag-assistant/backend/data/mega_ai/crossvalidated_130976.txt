[site]: crossvalidated
[post_id]: 130976
[parent_id]: 
[tags]: 
Bayesian update with multiple parameters

In the past I have been able to do Bayesian updating when there is just one parameter which I am trying to estimate. I know a bit about Bayesian methods but I am confused by how to extend them to multiple parameters. One parameter is X which can take the values $[x_1, x_2, x_3]$ They have a prior probability $[0.2, 0.2, 0.6]$ The other parameter in the model is Y which can take the values $[y_1, y_2, y_3]$ with a prior probability $[0.1, 0.5, 0.4]$ There are two ways I see of treating this problem and I'm not sure which is correct. Method 1. I treat the problem by updating both parameters together so that there are 9 states which the parameters can take (any combination of X and Y values). If E is the evidence, $P(x,y)$ is the prior probability of x and y, and $P^*(x,y)$ is the posterior probability of x and y then $P^*(x_1,y_1)= \frac{P(x_1,y_1) P(E|x_1,y_1)}{P(E)}$ Method 2. I treat the problem by updating each parameter individually. In this case I have to consider the probability of the evidence given $x_i$ and any value of Y so I sum all terms over Y. $P^*(x_1)= \frac{P(x_1) \sum_{j=1}^3 P(E|x_1, y_j)}{P(E)}$ And I use a similar approach for updating probabilities of y Both of these methods have downsides, I am not sure if both are correct. Could someone shed some light on which of these is normally used?
