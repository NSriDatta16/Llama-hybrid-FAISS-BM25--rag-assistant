[site]: crossvalidated
[post_id]: 175187
[parent_id]: 
[tags]: 
MCMC and approximate inference for Gaussian processes

I am reading a paper that is doing approximate inference over GP models. The idea is that when the likelihood is non-Gaussian than the inference of the posterior is not tractable and we need to use approximate inference schemes to compute the posterior distribution. There is one bit in the paper that I do not follow. The author says: In order to benefit from the advantages of MCMC it is necessary to develop efficient sampling strategies. This has proved to be particularly difficult in many GP applications that involve the estimation of a smooth latent function. Given that the latent function is represented by a discrete set of values, the posterior distribution over these function values can be highly correlated. The more discrete values used to represent the function, the worse the problem of high correlation becomes. Perhaps this is very naive but why does the discreteness of the function increase the correlation?
