[site]: datascience
[post_id]: 81151
[parent_id]: 81126
[tags]: 
In the evaluation type B approach, your neural network weights and biais are not reset before each loop of cross-validation. The neural network is then learning from one loop to another, so you see the MAE continuously decreasing. A solution is to store your weights and biais before fitting the model and load them at each loop so they have the same init. You can use those methods to do so: model.save_weights('model.h5') # right after model instantiation model.load_weights('model.h5') # in the loop before fitting In evaluation type A , because you instantiate the model in the loop, weights and biais are reset so you don't see the phenomenon.
