[site]: crossvalidated
[post_id]: 530735
[parent_id]: 
[tags]: 
Prove two orthogonal contrasts are statistically independent

Linear combination $C=\sum_{i=1}^{n} a_i \bar{X}_i$ is called a (estimated) contrast if $\sum_{i=1}^{n} a_i=0$ . Two contrasts are called orthogonal if $\sum_{i=1}^{n} a_i b_i = 0$ ; simplest example would be $C_1=\bar{X}_1 - \bar{X}_2$ and $C_2=\bar{X}_1 + \bar{X}_2 - 2\bar{X}_3$ . I have read the claim Two orthogonal contrasts are statistically independent in Statistical Design and Analysis of Experiments by Robert L. Mason (2003, 2nd edition, Page 248) . I have searched the web for a proof with no success. I have tried to prove $$f\left( \sum_{i=1}^{n} a_i \bar{X}_i = v_2 \mid \sum_{j=1}^{n} b_j \bar{X}_j = v_1\right) = f\left(\sum_{i=1}^{n} a_i \bar{X_i} = v_2\right) \tag 1 $$ or even uncorrelatedness (a weaker property) $$\mathbb{E}\left [\left(\sum_{i=1}^{n} a_i \bar{X}_i\right)\left(\sum_{j=1}^{n} b_j \bar{X}_j\right)\right ]=\mathbb{E}\left[\sum_{i=1}^{n} a_i \bar{X}_i\right]\mathbb{E}\left[\sum_{j=1}^{n} b_j \bar{X}_j\right] \tag 2$$ again with no success. It seems there are more intricacies involved which I am missing. Edit: I have managed to prove the uncorrelatedness $(2)$ using kjetil b halvorsen 's advice. For now, let's assume there is only one factor, then $Y_{il} = \mu_{i} + \epsilon_{il}; \epsilon_{il} \overset{\text{iid}}{\sim} \mathcal{N}(0, \sigma^2)$ is the assumed model for i-th factor-level and l-th repeat test. Now let $\bar{X}_i \triangleq \frac{1}{r}\sum_{l=1}^r Y_{il}$ , where $r$ is the number of repeat tests in a balanced experiment. Therefore, $$\begin{align*} &\mathbb{E}\left [\left(\sum_{i=1}^{n} a_i \bar{X}_i\right)\left(\sum_{j=1}^{n} b_j \bar{X}_j\right)\right ]=\sum_{i,j=1}^n a_ib_j \mathbb{E}\left [\bar{X}_i\bar{X}_j \right] \\ &\overset{\bar{X}_i \perp \bar{X}_j \text{ for } i \neq j}{=} \sum_{i\neq j} a_ib_j \mu_i \mu_j + \sum_{i=1}^n a_ib_i \mathbb{E}\left [\bar{X}_i^2 \right] \\ \end{align*}$$ By using $$\begin{align*} \mathbb{E}\left [\bar{X}_i^2 \right] &= \mathbb{E}\left [\left(\frac{1}{r}\sum_{l=1}^r Y_{il}\right) \left(\frac{1}{r}\sum_{l'=1}^r Y_{il'}\right)\right] = \frac{1}{r^2}\sum_{l,l'=1}^r\mathbb{E}\left [Y_{il} Y_{il'}\right] \\ &\overset{Y_{il} \perp Y_{il'} \text{ for } l \neq l'}{=} \frac{1}{r^2}\left(\sum_{l\neq l'} \mathbb{E}[Y_{il}]\mathbb{E}[Y_{il'}] + \sum_{l=1}^r \mathbb{E}\left [Y_{il}^2 \right]\right) \\ &= \frac{1}{r^2}\left( \sum_{l\neq l'} \mu_i^2 + \sum_{l=1}^r \mathbb{E}\left [(\mu_i + \epsilon_{il})^2 \right] \right) \\ &= \frac{1}{r^2}\left( r(r-1)\mu_i^2 + \sum_{l=1}^r \left (\mu_i ^ 2 + \sigma ^2 \right) \right) = \mu_i^2 + \frac{1}{r} \sigma^2 \end{align*}$$ We arrive at $$\begin{align*} &\mathbb{E}\left [\left(\sum_{i=1}^{n} a_i \bar{X}_i\right)\left(\sum_{j=1}^{n} b_j \bar{X}_j\right)\right ]= \sum_{i,j=1}^n a_ib_j \mu_i \mu_j + \frac{\sigma^2}{r}\sum_{i=1}^n a_ib_i \\ &\overset{\sum_{i=1}^n a_ib_i=0}{=} \sum_{i,j=1}^n a_ib_j \mathbb{E}[\bar{X}_i] \mathbb{E}[\bar{X}_j] = \mathbb{E}\left[\sum_{i=1}^{n} a_i \bar{X}_i\right]\mathbb{E}\left[\sum_{j=1}^{n} b_j \bar{X}_j\right] \end{align*}$$ Extending this proof to arbitrary $n$ factors and averages is straightforward; it can be messy though. For example, for a 3-factor experiment, any average $\bar{X}_{i'}$ can be expressed as $\bar{X}_{i'} \triangleq \sum_{ijkl} a_{ijk}Y_{ijkl}$ , which can be replaced in the proof. There is only one question left. Is "statistically independent" is used only loosely to mean "uncorrelated"? Or $(1)$ and $(2)$ can be proved to be equivalent in this context?
