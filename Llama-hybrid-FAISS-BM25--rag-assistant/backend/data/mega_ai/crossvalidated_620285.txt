[site]: crossvalidated
[post_id]: 620285
[parent_id]: 
[tags]: 
Derivation of cross entropy loss in machine learning

Given a dataset $\mathcal{D} = \{ (x_1, y_1),\cdots, (x_n, y_n)\}$ , let's say we want to approximate the conditional probability $p(y|x)$ , and we parameterized it as $p_{\theta}(y|x)$ . So,for a particular $x_i$ , the true distribution will be $p(y|x_i)$ and the approximated distribution will be $p_{\theta}(y|x_i)$ , the cross entropy will be $H(p,p_{\theta}) = -\sum_{y} p(y|x_i)\log p_{\theta}(y|x_i)$ . Now assume that $y_i\sim p(y|x_i)$ , how to prove that $H(p,p_{\theta}) \approx -\log p_{\theta}(y_i|x_i)$ ?
