[site]: crossvalidated
[post_id]: 290584
[parent_id]: 
[tags]: 
Why adding more samples to dataset make the model's performance weak?

I am using xgboost to train a classification model. Say, there are 1 million samples totally and my model's f1 score is 0.36. But when I add more 0.5 million samples which are close to some entries of my current dataset, the f1 score is below 0.34. Actually, the additional 0.5 million samples are close to (not equal to) some samples. For example, one additional sample is [1,0.2,4,0.7,0,0,0,0,0,9] and it is close to the sample [1,0.2,4,0.4,0,0,1,0,0,8] in original dataset. So why the larger dataset of 1.5 million make the model weaker ? Besides, I want to add some explanation of my question: original 1 million sample is from 1 million people, everybody's action feature output 1 entry to my dataset. But half of them has two actions in history and hence it can produce two entries. So from the half people, I can get more 0.5 million entries. My original intention is just make the dataset larger. the model is based only on the action features to predict the label, for example, he/she will make purchase or not.
