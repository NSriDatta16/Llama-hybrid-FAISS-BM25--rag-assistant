[site]: datascience
[post_id]: 109710
[parent_id]: 75449
[tags]: 
What you could do, is to rewrite your favorite preprocessing functions into new custom transformers. This might take time to rewrite, but it surely is helpful when you want everything to be as a dataframe. For example consider an example of a StandardScaler: class DFStandardScaler(TransformerMixin): def __init__(self): self.ss = None self.mean_ = None self.scale_ = None def fit(self, X, y=None): self.ss = StandardScaler() self.ss.fit(X) self.mean_ = pd.Series(self.ss.mean_, index=X.columns) self.scale_ = pd.Series(self.ss.scale_, index=X.columns) return self def transform(self, X) -> pd.DataFrame: # assumes X is a DataFrame Xss = self.ss.transform(X) Xscaled = pd.DataFrame(Xss, index=X.index, columns=X.columns) return Xscaled def __str__(self): return "DF_StandardScaler" def __repr__(self): return "DF_StandardScaler" Using the following as DFStandardScaler().fit_transform(df) would return the same dataframe which was provided. The only issue is that this example would expect a df with column names, but it wouldn't be hard to set column names from scratch. Here's the sklearn's documentation on custom transformers: https://scikit-learn.org/stable/modules/preprocessing.html#custom-transformers
