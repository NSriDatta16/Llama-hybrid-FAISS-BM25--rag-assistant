[site]: crossvalidated
[post_id]: 598639
[parent_id]: 463224
[tags]: 
Precision, recall, F1, ROC/AUC, and other metrics like specificity/sensitivity that you mentioned can be good for multi-class imbalanced metrics. If you want to emphasize the undersampled classes, use macro weighting (arithmetic average). If not, use micro average, which is weighted by number of samples. Another metric I don't see people often talking about is Cohen's Kappa. I like to think of it like accuracy, but taking into account the "no information rate" or random guessing baseline. It will give you a score similar to accuracy, though I believe it has some flaws in certain situations. In general, I've found Cohen's Kappa to work well. Others include the litany of metrics listed on Wikipedia's confusion matrix page, such as Matthew's correlation coefficient (MCC) and others.
