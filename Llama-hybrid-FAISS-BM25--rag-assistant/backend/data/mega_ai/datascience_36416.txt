[site]: datascience
[post_id]: 36416
[parent_id]: 36410
[tags]: 
It sounds like you essentially have a sparse input problem, similar to doing something like a recommendation system. Imagine trying to recommend a film to somebody based on films they have already watched and rated. There would be many many films they have not seen, so you (in a sense) have missing data. This was the case in the Netflix Prize challenge. Facorisation methods such Single Value Decomposition are common approaches in these cases. There is a nice summary of such methods used in the Netflix Prize. Using a neural network Auto-Enconder will be reliant on relatively large numbers of samples, which is something to bear in mind when assessing performance. Just one other idea that might be worth trying out, depending on how many features you have and the general sparsity, could be to replace the missing values with fixed values on a feature-level. This could be as simple as replacing the missing values e.g. in "relationship status" to , and then in general "feature X" to just be . This is a kind of trick that is often used in NLP (natural language processing) to allow filler-words to encode some useful information. In your case, this might at least help any model distinguish between missing values across features. I have not tested something like this myself, so unfortunately I cannot cite references nor point to results.
