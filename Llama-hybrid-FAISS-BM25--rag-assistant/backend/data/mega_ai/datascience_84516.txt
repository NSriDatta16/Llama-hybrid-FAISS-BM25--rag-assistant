[site]: datascience
[post_id]: 84516
[parent_id]: 
[tags]: 
Normalise the scores by small and large numbers division

I am working on a scoring problem use-case where I will score each task based on few aspects and take an average of scores of tasks in each experiment. For each experiment I will have 1 to N number of tasks where N ranges upto 100k. So, this averaged score help me to rank the experiments to understand their efficiency. Here, the problem I am facing is taking the average ie., since as said, each experiment have 1 to N number of tasks. Lets say we have two cases where Case 1: . Experiment X having 10 tasks. Each task is rated 7 out of 10. So the average is 10*7/10 = 7 Case 2: Experiment Y having 1000 tasks. Each task is rated 7 out of 10. So the average is 1000*7/1000 = 7 Now, ideally since the scores are equal, we can conclude that X==Y. But the fact is Y is better than X because after 1000 tasks it is still having 7 average rating and the experiment Y is chosen 1000 times which itself says that it is more preferred and trusted. Now, here I want to rate Experiment Y is higher than X by taking no of tasks into account taking into count that the score should not exceed the range 0-10. And, the range of tasks in the experiments can be 0 to infinity. I tried multiplying the final average score of the experiment with ((N-1)/N) but the dispersion is very small for large numbers of N.
