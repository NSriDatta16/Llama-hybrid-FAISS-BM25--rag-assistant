[site]: crossvalidated
[post_id]: 220903
[parent_id]: 
[tags]: 
High variance of the distribution of p-values (an argument in Taleb 2016)

I'm trying to understand the big picture claim made in Taleb, 2016, The Meta-Distribution of Standard P-Values . In it, Taleb makes the following argument for the unreliability of the p-value (as I understand it): An estimation procedure operating on $n$ data points coming from some distribution $X$ outputs a p value. If we draw n more points from this distribution and output another p value, we can average these p-values obtaining in the limit the so-called "true p-value". This "true p-value" is shown to have a disturbingly high variance, so that a distribution+procedure with "true p value" $.12$ will 60% of the time report a p-value of Question : how can this be reconciled with the traditional argument in favor of the $p$-value. As I understand it, the p-value is supposed to tell you what percentage of the time your procedure will give you the correct interval (or whatever). However, this paper seems to argue that this interpretation is misleading since the p-value will not be the same if you run the procedure again. Am I missing the point?
