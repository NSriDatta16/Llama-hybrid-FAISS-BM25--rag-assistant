[site]: crossvalidated
[post_id]: 298666
[parent_id]: 298485
[tags]: 
I currently work in the data science field, and before then I worked in education research. While at each "career" I've collaborated with people who did not come from a formal background in statistics, and where emphasis of statistical (and practical) significance is heavily placed on the p-value . I've learned include and emphasize effect sizes in my analyses because there is a difference between statistical significance and practical significance. Generally, the people I worked with cared about one thing "does our program/feature make and impact, yes or no?". To a question like this, you can do something as simple as a t-test and report to them "yes, your program/feature makes a difference". But how large or small is this "difference"? First, before I begin delving into this topic, I'd like to summarize what we refer to when speaking of effect sizes Effect size is simply a way of quantifying the size of the difference between two groups. [...] It is particularly valuable for quantifying the effectiveness of a particular intervention, relative to some comparison. It allows us to move beyond the simplistic, 'Does it work or not?' to the far more sophisticated, 'How well does it work in a range of contexts?' Moreover, by placing the emphasis on the most important aspect of an intervention - the size of the effect - rather than its statistical significance (which conflates effect size and sample size), it promotes a more scientific approach to the accumulation of knowledge. For these reasons, effect size is an important tool in reporting and interpreting effectiveness. It's the Effect Size, Stupid: What effect size is and why it is important Next, what is a p-value , and what information does it provide us? Well, a p-value , in as few words as possible, is a probability that the observed difference from the null distribution is by pure chance. We therefore reject (or fail to accept) the null hypothesis when this p-value is smaller than a threshold ($\alpha$). Why Isn't the P Value Enough? Statistical significance is the probability that the observed difference between two groups is due to chance. If the P value is larger than the alpha level chosen (eg, .05), any observed difference is assumed to be explained by sampling variability. With a sufficiently large sample, a statistical test will almost always demonstrate a significant difference, unless there is no effect whatsoever, that is, when the effect size is exactly zero; yet very small differences, even if significant, are often meaningless. Thus, reporting only the significant P value for an analysis is not adequate for readers to fully understand the results. And to corroborate @DarrenJames's comments regarding large sample sizes For example, if a sample size is 10 000, a significant P value is likely to be found even when the difference in outcomes between groups is negligible and may not justify an expensive or time-consuming intervention over another. The level of significance by itself does not predict effect size. Unlike significance tests, effect size is independent of sample size. Statistical significance, on the other hand, depends upon both sample size and effect size. For this reason, P values are considered to be confounded because of their dependence on sample size. Sometimes a statistically significant result means only that a huge sample size was used. [There is a mistaken view that this behaviour represents a bias against the null hypothesis. Why does frequentist hypothesis testing become biased towards rejecting the null hypothesis with sufficiently large samples? ] Using Effect Sizeâ€”or Why the P Value Is Not Enough Report Both P-value and Effect Sizes Now to answer the question, are effect sizes superior to p-values ? I would argue, that these each serve as importance components in statistical analysis that cannot be compared in such terms, and should be reported together. The p-value is a statistic to indicate statistical significance (difference from the null distribution), where the effect size puts into words how much of a difference there is. As an example, say your supervisor, Bob, who is not very stats-friendly is interested in seeing if there was a significant relationship between wt (weight) and mpg (miles per gallon). You start the analysis with hypotheses $$ H_0: \beta_{mpg} = 0 \text{ vs } H_A: \beta_{mpg} \neq 0 $$ being tested at $\alpha = 0.05$ > data("mtcars") > > fit = lm(formula = mpg ~ wt, data = mtcars) > > summary(fit) Call: lm(formula = mpg ~ wt, data = mtcars) Residuals: Min 1Q Median 3Q Max -4.5432 -2.3647 -0.1252 1.4096 6.8727 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 37.2851 1.8776 19.858 From the summary output we can see that we have a t-statistic with a very small p-value . We can comfortably reject the null hypothesis and report that $\beta_{mpg} \neq 0$. However, your boss asks, well, how different is it? You can tell Bob, "well, it looks like there is a negative linear relationship between mpg and wt . Also, can be summarized that for every increased unit in wt there is a decrease of 5.3445 in mpg " Thus, you were able to conclude that results were statistically significant, and communicate the significance in practical terms. I hope this was useful in answering your question.
