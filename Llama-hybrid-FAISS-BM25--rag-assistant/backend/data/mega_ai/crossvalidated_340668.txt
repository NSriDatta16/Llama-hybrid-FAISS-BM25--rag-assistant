[site]: crossvalidated
[post_id]: 340668
[parent_id]: 340584
[tags]: 
In a vanilla RNN, there is only the activation path (also often referred to as the hidden-state $h_t$). LSTM added on the cell memory $c_t$ as a way to store information over long time-spans in particular. GRU, which was developed later, simplifies the LSTM by combining both the cell memory and the hidden-state. Therefore the intuition is that the cell memory stores longer-term information while the hidden-state is still used the same way as it is used in vanilla RNN, but later on, we discovered that it's possible to combine the two without too much degradation in performance. In other words, GRU is a sort of refinement on the ideas from LSTM. The cell memory is used to modify the activation path before the output, so intuitively, whatever relevant information in the long-term memory can be dumped into the short-term memory (the activations) right before an output needs to be extracted
