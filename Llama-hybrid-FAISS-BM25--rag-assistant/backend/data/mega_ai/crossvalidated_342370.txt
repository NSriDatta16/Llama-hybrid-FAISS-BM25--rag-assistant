[site]: crossvalidated
[post_id]: 342370
[parent_id]: 342365
[tags]: 
It seems like a lot of work to me for minimal benefit. Having one more layer to backprop through when there may already be many tens of layers means that it doesn't help to hard-code the filters performance wise. In addition, you only artificially limit yourself -- we know CNNs tend to learn gabor filters in the first layer -- but what if you have an unusual dataset? Or you're exploring a new architecture? Etc. At best, you can hope that by hard-coding in the first layer, you're not missing out on an even better solution in parameter-space. And I think this is what you meant when you said transfer learning, but taking the pretrained weights from one model and using it to jumpstart the weights of another network is an effective strategy, and pretty similar to the static filters idea without the downsides.
