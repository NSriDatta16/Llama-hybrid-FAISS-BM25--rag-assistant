[site]: datascience
[post_id]: 21
[parent_id]: 19
[tags]: 
As you rightly note, these days "big data" is something everyone wants to say they've got, which entails a certain looseness in how people define the term. Generally, though, I'd say you're certainly dealing with big data if the scale is such that it's no longer feasible to manage with more traditional technologies such as RDBMS, at least without complementing them with big data technologies such as Hadoop. How big your data has to actually be for that to be the case is debatable. Here's a (somewhat provocative) blog post that claims that it's not really the case for less than 5 TB of data. (To be clear, it doesn't claim "Less than 5 TB isn't big data", but just "Less than 5 TB isn't big enough that you need Hadoop".) But even on smaller datasets, big data technologies like Hadoop can have other advantages, including being well suited to batch operations, playing well with unstructured data (as well as data whose structure isn't known in advance or could change), horizontal scalability (scaling by adding more nodes instead of beefing up your existing servers), and (as one of the commenters on the above-linked post notes) the ability to integrate your data processing with external data sets (think of a map-reduce where the mapper makes a call to another server). Other technologies associated with big data, like NoSql databases, emphasize fast performance and consistent availability while dealing with large sets of data, as well also being able to handle semi-unstructured data and to scale horizontally. Of course, traditional RDBMS have their own advantages including ACID guarantees (Atomicity, Consistency, Isolation, Durability) and better performance for certain operations, as well as being more standardized, more mature, and (for many users) more familiar. So even for indisputably "big" data, it may make sense to load at least a portion of your data into a traditional SQL database and use that in conjunction with big data technologies. So, a more generous definition would be that you have big data so long as it's big enough that big data technologies provide some added value for you. But as you can see, that can depend not just on the size of your data but on how you want to work with it and what sort of requirements you have in terms of flexibility, consistency, and performance. How you're using your data is more relevant to the question than what you're using it for (e.g. data mining). That said, uses like data mining and machine learning are more likely to yield useful results if you have a big enough data set to work with.
