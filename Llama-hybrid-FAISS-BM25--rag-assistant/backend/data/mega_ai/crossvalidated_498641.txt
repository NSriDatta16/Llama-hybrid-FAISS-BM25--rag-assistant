[site]: crossvalidated
[post_id]: 498641
[parent_id]: 
[tags]: 
Confusion on terminology for a variational autoencoder

I've read the original paper and many blogs and it only confused me more due to various conflicting notations being used. So I'm looking for a canonical answer here hopefully. All the following statements are based on a standard variational autoencoder. Confusions: I've seen the word prior and posterior thrown around a lot. I've seen posterior refer to the distribution $p(x)$ . But it is actually referring to the reconstructed input $x$ , not the input that is fed to the encoder. Similarly, the prior is $p(z)$ . From my understanding of priors and posteriors in a general sense, the prior should be the original input and the posterior is after new information has been incorporated. So let $x, z, \tilde x$ refer to the input, latent variable, and reconstruction. Which of these variables would you say is the prior, posterior, etc? The "posterior" is also referred to as the "evidence" since it is the evidence of the "prior." Can the "prior" be considered the evidence with regards to the input? Why is it called evidence "lower bound?" I get the loss function and how it is used, e.g. maximizing the log liklihood or minimzing the negative log liklihood, etc. But why exactly are we calling it "evidence lower bound" for the loss function function of a VAE? The decoder seems just as important as the encoder. But why are all the diagrams of VAE only concerned with $p(z | x)$ and not so much on $p( \tilde x | z)$ ? Maybe I just didn't catch the discussion surrounding this.
