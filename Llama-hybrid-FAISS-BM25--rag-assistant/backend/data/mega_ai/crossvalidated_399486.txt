[site]: crossvalidated
[post_id]: 399486
[parent_id]: 399430
[tags]: 
Machine learning algorithm is a function of the inputs, that predicts the outputs. There are many different algorithms. You seem to assume linear model, where $y = X\beta + \varepsilon$ , so the result of $y$ would linearly depend on $X$ . Notice however, that even with such model if $\beta$ is negative, then decreasing $X$ would lead to increasing $y$ . Moreover, most of the machine learning models learn non-linear functions, so there is no such linear dependence. For example, decision tree is a series of if ... else ... statements based on learned thresholds if X > c then ... else ... , so it doesn't matter for the model what are the actual values as long as it can pack them into meaningful "buckets" of similar values. Neural networks achieve non-linearity by using redundant weights and stacking multiple layers. If you use $k$ nearest neighbors , it only looks at similarities between your samples, so bigger/smaller relation does not affect it in this case. Finally, normalization/standardization does not affect ordering of values. So if $x_1$ is larger then $x_2$ , after normalization or standardization they both would have potentially different values, but the relation between them would not change.
