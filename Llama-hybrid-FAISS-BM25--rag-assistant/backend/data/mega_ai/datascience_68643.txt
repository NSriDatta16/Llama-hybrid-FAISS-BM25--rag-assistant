[site]: datascience
[post_id]: 68643
[parent_id]: 51490
[tags]: 
This looks like a two class boosted decision classifier. That should perform quite well. I used it several times about 2-3 months ago and got excellent results. I'm guessing you are not training/testing the right features. What is your target variable? Is it 'hasLead'? I'm not sure what will be statistically influential on that dependent variable, but you can run a feature engineering exercise to see what independent variables have the most influence. Run the code below; change to suit your specific needs (obviously, use your specific data). import pandas as pd import numpy as np import matplotlib.pyplot as plt # matplotlib inline df = pd.read_csv("https://rodeo-tutorials.s3.amazonaws.com/data/credit-data-trainingset.csv") df.head() from sklearn.ensemble import RandomForestClassifier features = np.array(['revolving_utilization_of_unsecured_lines', 'age', 'number_of_time30-59_days_past_due_not_worse', 'debt_ratio', 'monthly_income','number_of_open_credit_lines_and_loans', 'number_of_times90_days_late', 'number_real_estate_loans_or_lines', 'number_of_time60-89_days_past_due_not_worse', 'number_of_dependents']) clf = RandomForestClassifier() clf.fit(df[features], df['serious_dlqin2yrs']) # from the calculated importances, order them from most to least important # and make a barplot so we can visualize what is/isn't important importances = clf.feature_importances_ sorted_idx = np.argsort(importances) padding = np.arange(len(features)) + 0.5 plt.barh(padding, importances[sorted_idx], align='center') plt.yticks(padding, features[sorted_idx]) plt.xlabel("Relative Importance") plt.title("Variable Importance") plt.show()
