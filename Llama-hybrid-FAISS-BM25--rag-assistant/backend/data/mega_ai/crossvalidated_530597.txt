[site]: crossvalidated
[post_id]: 530597
[parent_id]: 530594
[tags]: 
There was an interesting paper What is the Effect of Importance Weighting in Deep Learning? by Byrd and Lipton, where they've shown the following: We demonstrate the surprising finding that for regularized neural networks optimized by stochastic gradient descent (SGD) , the impact of importance weighting diminishes over epochs of training. We show that L2 regularization and batch normalization, (but not dropout) interact with importance weights, restoring (some) impact on learned models. We replicate our results across a variety of networks, tasks, and datasets. Our results call into question the standard application of importance weighting when applied to deep networks, a finding with practical consequences on the fields of causal inference, domain adaptation, and off-policy reinforcement learning. So it works for traditional machine learning models (say, logistic regression), there are mixed results when using it for neural networks. Notice that the results show something opposite to what you suggested. Moreover, this has nothing to do with one-hot-encoding.
