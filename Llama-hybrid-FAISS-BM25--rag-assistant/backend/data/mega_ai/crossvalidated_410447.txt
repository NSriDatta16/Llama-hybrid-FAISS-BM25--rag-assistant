[site]: crossvalidated
[post_id]: 410447
[parent_id]: 
[tags]: 
How valid is this Stacking Model (input features to weak learners are different)?

I have a set of features with 6 of them being categorical, 1 continuous and 2 textual in type . I have to predict the labels ( 10 in number) for them. I tried applying several models and came to a conclusion that one predictive model is not enough to get the required efficiency. So I applied a stacking model using Random Forest, K- Nearest Neighbors and Multinomial Naive Bayes algorithms. Since RF and KNN can take only integer/float values, I was able to encode my 6 categorical features to give as input. But as for the 2 text fields, i had to use NLP libraries(nltk) in python and apply Naive Bayes to get some predictions. However, it has less efficiency. Nonetheless, I took the predictions of all of the above three models and fed it to a third model (again RF or KNN). The prediction outputs on the test set looks good with up to 98% accuracy score. So, in short, I am sending different features as inputs to the weak learners, and then combining them for input to the stronger model. Is this a valid way of stacking?
