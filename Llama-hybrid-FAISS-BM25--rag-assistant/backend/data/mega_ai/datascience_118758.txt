[site]: datascience
[post_id]: 118758
[parent_id]: 
[tags]: 
How can I deal with a computationally expensive simulator method in Sequential Monte Carlo/Approximate Bayesian Computation?

I am doing Approximate Bayesian Computation with Sequential Monte Carlo with PyMC in a way that is similar to what is described in this example of the PyMC documentation . The motivation for choosing this approach is that I have a simulator method (as it is called in the the documentation) that is complicated and computationally very expensive to evaluate, and this is a setting for which this approach is well suited. For the simplified form of the model, this works very well. However the full version of the simulator method takes very long to evaluate, on the order of ~1-10s. This makes the Sequential Monte Carlo prohibitively slow. I already tried to tune the number of draws and epsilon in pymc.Simulator() to reduce the runtime while still giving a meaningful result, which already helped, but not enough. Naturally, I am also working on optimising the simulation function, however it is very likely impossible to reduce the runtime fundamentally. What are approaches that I could try to deal with such an expensive simulation function? How could I optimise the number of times this simulation function needs to be called? Or is there an entirely different approach, which is better suited to this issue? (My apologies if I am missing something obvious, I am quite new to Bayesian inference.)
