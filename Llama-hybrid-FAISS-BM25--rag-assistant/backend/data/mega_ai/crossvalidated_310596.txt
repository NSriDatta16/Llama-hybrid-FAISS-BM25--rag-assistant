[site]: crossvalidated
[post_id]: 310596
[parent_id]: 
[tags]: 
Why can we assume independence Bayesian inference?

From https://en.wikipedia.org/wiki/Bayesian_inference Scenario: A series of N points(${x_{1,..,n}}$) are drawn from distribution p(x|$\theta$),here $\theta$ is a parameter for distribution and itself being a random variable. After getting evidence from N data points, we get a better(less variability) distribution for $\theta$,in short we are guessing value of $\theta$ using Bayes theorem $$ P(\theta \mid {x_{1,..,n}}) = \frac{P(x_{1,..,n} \mid \theta) \, P(\theta)}{P(x_{1,..,n})} $$ I get from other literature that we calculate $P(x_{1,..,n} \mid \theta)$ as $\prod_{k=1}^n P(x_{k} \mid \theta) $ ,this assumes that observations are independent How well is the assumption justified? Since each observation adds some information to guess $\theta$,next observation totally depends on previous one, in fact exploiting this evidence clues we even try to predict what'd be distribution for next observation by marginalising posterior over prior $$P(x_{new} \mid x_{1,..,n}) =\int_\theta P(x \mid \theta) P(\theta \mid {x_{1,..,n}}) d\theta$$
