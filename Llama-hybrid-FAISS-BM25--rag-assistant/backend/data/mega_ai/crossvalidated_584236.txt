[site]: crossvalidated
[post_id]: 584236
[parent_id]: 
[tags]: 
On a survey, when checking for effect some answers have on other answers, do we always need to check for effect of random question sets?

Say someone runs a survey with, 100 binary question to which 10,000 randomly selected people responded. For 5 of those each questions (#20 to #24), answers looked like this (lets call this set of questions "A"): Question 20: 75% yes Question 21: 70% yes Question 22: 71% yes Question 23: 72% yes Question 24: 73% yes Then a researcher, looks a second set of questions, and wonders, what is the effect they have on the first set of questions. This other "B" set of questions looks like this: Question 0: 67% yes Question 1: 64% yes Question 2: 69% yes Question 3: 63% yes Question 4: 62% yes When looking again at the set A, conditioned by only people that answered yes on set B, answers look like this: Question 20: 94% yes Question 21: 92% yes Question 22: 96% yes Question 23: 98% yes Question 24: 90% yes The researcher is excited, clearly answering yes to set A drives up answers on set B. Right? But then, a friend of the researcher runs script, randomly selecting 20 sets of 5 questions and discovering that all of them drive the average of answers to the "B" set from the around ~65% to around ~95%, and says: what you found is meaningless, I just got the same result with 20 random "A" sets! Questions: Is what the researcher found indeed meaningless as his friend says? Why? If it is indeed meaningless, what is the name of the statistical mistake the researcher is making? Does the technique that the friend of the researcher is using to push-back on the results have a name? Does the interpretation change if the 10,000 people, instead of being a random sample, are a total population (say for example, 100% of the students in school?)
