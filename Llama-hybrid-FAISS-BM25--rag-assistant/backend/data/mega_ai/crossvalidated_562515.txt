[site]: crossvalidated
[post_id]: 562515
[parent_id]: 540454
[tags]: 
You could consider the model itself a hyperparameter as well. If you optimize the hyperparameter using the test set, and then choose the best model, you overfit with the human in the loop. I like the sklearn documentation on model selection which sports the following chart: And futher states: When evaluating different settings (“hyperparameters”) for estimators, such as the C setting that must be manually set for an SVM, there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. This way, k nowledge about the test set can “leak” into the model and evaluation metrics no longer report on generalization performance . To solve this problem, yet another part of the dataset can be held out as a so-called “validation set”: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set. Additionally, I recommend you to read: How is cross validation different from data snooping? Bonferroni Correction & machine learning Note that while you call your holdout set test , that is what others call evaluation set. In this context, have a look at MFML 069 - Model validation done right
