[site]: crossvalidated
[post_id]: 461354
[parent_id]: 
[tags]: 
How does regularization part of w help in XGBoost

In the regularization part of XGBoost objective function, it contains gamma T and also lambda square(W). I understand gamma is the minimum node split criteria and T is number of leaves and regularizing them lead to a simpler model (not many splits/leaves). However, I don't understand why regularizing /penalizing w will help in simpler model or what it does ? Since we are fitting regression trees for both regression and classification in XGBoost, how does having a small score on the leaf help with regularization of the tree structure? Or w is just there not regularize the tree structure but to keep the leaf weights low (if so, why)? Thank you for any responses.
