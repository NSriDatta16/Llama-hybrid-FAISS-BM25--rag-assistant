[site]: crossvalidated
[post_id]: 534385
[parent_id]: 534372
[tags]: 
A basic statistical model could be a model for whether a product was bought by a specific user in that month with random effects that reflect that some users buy some product more and perhaps also a more specific product. You'd also want to have effects for product, month, and possibly with an interaction between product and month. Perhaps you also want some long-term trends. To use R model syntax something like (I used the Bayesian brms package - there's also rstanarm , but you can also try this with e.g. lme4 or mgcv ): library(tidyverse) library(brms) example = expand_grid(user=1:50, year=2018:2020, month=1:12, product=c("A", "B", "C", "D", "E")) %>% mutate(bought_previously = rbinom(n=n(), size=1, prob = plogis( 0.1*(year+month/12-2018) + user*0.05 + product %in% c("A", "D") )), bought = rbinom(n=n(), size=1, prob = plogis( 0.1*(year+month/12-2018) + user*0.05 + product %in% c("A", "D") + bought_previously ))) brmfit_bought = brm(bought ~ (1+product|user) + product + month + product*month + bought_previously, data=example, cores=4, family=bernoulli) summary(brmfit_bought) Obviously, there's a lot of interesting feature engineering for features like bought_previously (Did the user buy previously, ever, perhaps it matters whether they did so recently? Or perhaps how often?). One just has to be very, very careful to not use information from the future when building features. The other tricky bit is how to evaluate such models. You should spend a lot of time thinking about that. E.g. do you want to predict for existing customers based on their past history? Then tune hyperparameters and evaluate past-vs.-future splits. Do you want to predict for new customers? Then look at splitting by customer (i.e. validation/test sets should be completely separate customer ids from the training data). Another obvious baseline approaches are (a lot of these may not explicitly reflect whether records are for the same user, but capture user characteristics via features you engineer and you avoid overfitting by (cross-)validating appropriately): gradient boosted trees (e.g. xgboost or lightgbm - or perhaps catboost to deal with categories such as products) tabular neural networks with embeddings for users and products neural networks for the time series (e.g. LSTMs) of one user at a time (again, with embeddings for users and possibly products)
