[site]: crossvalidated
[post_id]: 172733
[parent_id]: 
[tags]: 
Lower classification accuracy after dimensionality reduction

Generally feature selection and dimensionality reduction are recommended to raise classification accuracy. Currently, I am working with datasets from neuroscience which are huge. I have 84 patients, half of them suffering under condition A and the other does not. Each patient has a 1x20 000 feature vector. Computational cost is no problem, but if I use a linear SVM for classification, the results for a two class classification are very good (ca. 80% accuracy, true-positive and false-negative rates each also ca. 80%). This is before doing any feature reduction. My question: Can this number of subjects/number of features disbalance bias the classification accuracy to be actually better ? How could I systematically remove this bias, if so. Edit: I know of the danger of overfitting, but I thought that would be less of a problem with a linear SVM and crossvalidation.. Thanks
