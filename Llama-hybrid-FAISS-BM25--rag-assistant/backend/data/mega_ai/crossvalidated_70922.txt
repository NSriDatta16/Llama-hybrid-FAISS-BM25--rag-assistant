[site]: crossvalidated
[post_id]: 70922
[parent_id]: 70870
[tags]: 
Use logistic regression with a Gaussian link. The count of simultaneous responses for a given value of $x$, written $y(x)$, is the outcome of $n=100$ independent Bernoulli trials whose chance of success is given by the Gaussian function. Letting $\theta$ stand for the three parameters (unknown, to be estimated), let's write the value of that Gaussian at $x$ as $\mu(x, \theta)$. Then the probability of observing $y(x)$ successes, with $0 \le y(x) \le n$, is $${\Pr}_\theta(y(x)) = \binom{n}{y(x)} \mu(x,\theta)^{y(x)} \left(1-\mu(x,\theta)\right)^{n-y(x)}.$$ The likelihood of a set of independent observations arising from the same underlying Gaussian is the product of these expressions. The part of the product that varies with $\theta$ is obtained from the last two factors. They will tend to be extremely small (because we are dropping the binomial coefficients), so about the only reasonable way to handle them on a computer is through their logarithms. Thus the part of the log likelihood that varies with $\theta$ is $$\Lambda(\theta) = \sum_{x}\left(y(x)\log(\mu(x,\theta)) + (n-y(x))\log(1-\mu(x,\theta))\right).$$ Logistic regression with a Gaussian link maximizes this log likelihood. To make sure the Gaussian peak does not exceed $1$, we might choose to parameterize its amplitude using a function that ranges from $0$ to $1$. Here is one convenient parameterization: $$\mu(x, \theta) = \mu(x, (m,s,a)) = \frac{\exp(-\frac{1}{2} \left(\frac{x-m}{s}\right)^2)}{1 + \exp(-a)}.$$ The parameter $m$ is the mode of the Gaussian, $s$ (a positive number) is its spread, and $a$ (some real number) determines the amplitude, increasing with increasing $a$. Use a multivariate nonlinear optimization procedure suitable for smooth functions. (Avoid specifying any constraints at all by using $s^2$ instead of $s$ as a parameter, if necessary.) Given some vaguely reasonable estimates of the parameters, it should have no trouble finding the global optimum. As an example, here is a detailed implementation of the fitting procedure in R using data from the question. It is modified from code for a four-parameter least-squares fit of a Gaussian shown in an answer at Linear regression best polynomial (or better approach to use)? . The fit is good: the standardized residuals do not become extreme and given the small amount of data, they are reasonably close to zero. The estimated values are $\hat{m} = -0.033$, $\hat{s} = 0.127$, and $\hat{a} = 16.8$ (whence the estimated amplitude is $1/(1+\exp(-\hat{a})) = 1 - 0.00000005$). y (Although R has a way of performing these calculations automatically (by means of a custom link function for its glm Generalized Linear Model function), the interface is so cumbersome and so uninstructive I have elected not to use it for this illustration.)
