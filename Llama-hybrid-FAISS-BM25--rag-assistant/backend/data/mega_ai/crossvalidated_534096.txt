[site]: crossvalidated
[post_id]: 534096
[parent_id]: 534086
[tags]: 
KLT is basically PCA in infinite-dimensions and PCA learns an "orthogonal dictionary". If the dimension of the space is $d$ (in the case of finite-dimensional PCA), then you can have at most $d$ orthogonal vectors, i.e., the size of the dictionary is at most $d$ . Dictionary learning dispenses with the orthogonality assumption in favor of obtaining sparser representations for most vectors/signals of interest. You usually work with an overcomplete dictionary, one whose number of elements is larger (or much larger) than the dimension $d$ of the space.
