[site]: crossvalidated
[post_id]: 496190
[parent_id]: 496170
[tags]: 
They are very different concepts, that unfortunately share a common term. For convolutions, the kernel is a tensor that indicates how the value at a particular index is calculated from a neighborhood of that index. For example, you can have a kernel that calculates the average of the 9 pixel neighborhood (for an image) by using the kernel [[1/9, 1/9, 1/9], [1/9, 1/9, 1/9], [1/9, 1/9, 1/9]] . For kernel PCA, the "kernel" is the same as the concept for "kernel based methods" such as support vector machines. They are functions that can stand in for the vector dot-product (or equivalent operations for whatever space your samples live in), with some particular properties. Such a kernel corresponds to a mapping of two samples to a different vector space, and taking the regular dot-product in that space. The neat thing is that the transformation does not need to be explicit: as long as you have the kernel function, you can do all the necessary calculations. So, kernel PCA is like regular PCA but with all vector in-products replaced by the kernel function of your choice.
