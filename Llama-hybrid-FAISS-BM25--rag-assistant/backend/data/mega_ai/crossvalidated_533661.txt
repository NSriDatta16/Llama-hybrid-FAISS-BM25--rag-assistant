[site]: crossvalidated
[post_id]: 533661
[parent_id]: 533659
[tags]: 
Whenever performance is too good to be true, the big two suspects are usually an improper training/validation/test-split (the typical problem is not splitting time-series as past vs. future: when you randomly select validation records ignoring time, the model only needs to interpolate rather than to learn to predict into the future), and target leakage, where something in the predictors gives away the answer, but cannot be use in real-life for prediction (e.g. predicting death and using whether the date of death field is missing (or not) as a feature, calculating some feature using future values e.g. when interpolating missing values etc.). It's hard to tell whether (2) could be occuring in your case, but assuming that your time series is ordered by time in the original data matrix m item (1) may not be the problem. Why do I sound like I expect that there is a problem? Well, I find that when a model performance is too good to be true, it is very often one of these two things (rather than a surprisingly good model performance).
