[site]: crossvalidated
[post_id]: 476694
[parent_id]: 476677
[tags]: 
It may help to think of the standard deviation as a measure of central tendency . Any normal Gaussian distribution will tend to cluster towards the mean (lets assume the clustering is symmetric to the left and right of the mean).The standard deviation tells us the degree of clustering relative to the mean. For example, a mean of 5 and a standard deviation of 41 suggests little clustering while a mean of 41 and a standard deviation of 5 suggests that the data is heavily clustered around the mean. Now a key feature of the normal Gaussian is that it is not bounded - there are no limits to the left and right of the distribution. It is always possible to have a data say 5 standard deviations away from the mean. It is increasingly unlikely though. If all data fell within 1 standard deviation, what would that tell us? It would neither suggest the presence of outliers nor provide a measure of 'clustering' around the mean. I think you are overly focused your definition of standard deviation as "how much the data differs from its mean on average". But if we were to focus on this definition, then 100% of the data cannot fall within 1 standard deviation by the definition of an average. When we say that a car goes 50 mph on average you know that some cars are going slower and some are going faster. I know I'm horribly simplifying everything, but I'm just making a general point. I hope this rant helps a bit.
