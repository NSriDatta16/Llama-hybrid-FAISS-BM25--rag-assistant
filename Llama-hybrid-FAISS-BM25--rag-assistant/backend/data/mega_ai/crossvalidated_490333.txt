[site]: crossvalidated
[post_id]: 490333
[parent_id]: 211436
[tags]: 
There are two aspects to this topic: Normalization to keep all data in the same scale --> the outcome is going to be similar when normalizing both on a per-image basis or across the entire image data set Preservation of relative information --> this is where doing normalization on a per-image or per-set basis makes a big difference For example if you want to learn a CNN to recognize night scenes vs. daytime scenes and you normalize on a per-image basis, the network will fail miserably because all the images will be scaled equally. Another pitfall of per-image normalization is that you may be artificially gaining up image sensor shot noise (e.g. for very dark scenes) and this will throw off the CNN in confusing such noise as useful information. Last word of caution on normalization: if it is done incorrectly it can lead to unrecoverable loss of information, for example image clipping (generating values that are below the valid range of the image datatype) or saturation (above the valid range). This is a classic mistake when operating with uint8 variables to represent images and values either go below 0 or exceed 255 due to normalization / pre-processing operations. Once this happens, image information is lost and it cannot be recovered, so the CNN will fail to learn any useful information from those image pixels.
