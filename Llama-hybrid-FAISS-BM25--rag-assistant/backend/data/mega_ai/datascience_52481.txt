[site]: datascience
[post_id]: 52481
[parent_id]: 52441
[tags]: 
I did this once, in a position similar to yours. My constraints were, People skeptical of machine learning Required interpretability Required very fast inference Explainable in text documentation Implementable in any language Traditionally, this would have been done by an expert system, so training a decision tree was essentially just one step further. On my end, all of the work was being done in Python. I tried a handful of ML models, with and without hand-crafted features. A single decision tree performed nearly as well as anything I tried, and outperformed a fully handcrafted expert system. I then created an "inference model" in the form of if-statements for each split in the tree to confirm performance was equal to the decision tree itself. I even manually ignored some low entropy splits or where a single feature was underutilized, finding that for the same number of branches, a larger trained tree manually retaining fewer nodes outperformed a small tree where I replicated it exactly. Some of the splits were unintuitive, but that's partially the point of throwing a model at it. Be sure to really dig into the failure modes of every alternative to help everyone involved have confidence in your work. Given the constraints, it was a great solution to the problem.
