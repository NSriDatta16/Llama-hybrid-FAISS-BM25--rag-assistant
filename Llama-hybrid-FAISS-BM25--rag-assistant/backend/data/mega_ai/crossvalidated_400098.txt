[site]: crossvalidated
[post_id]: 400098
[parent_id]: 400087
[tags]: 
it sort of follows from the definitions of an MDP. The equation simply says that the number of visits to a state $s$ is the sum of the probability of starting in state $s$ and the probability of moving to state $s$ from state $\hat{s}$ . $p(s|\hat{s},a)$ denotes the probability of moving into state $s$ conditional on being in state $\hat{s}$ and performing action $a$ . It is essentially a model of the environment where the next state ( $s$ ) can be predicted from the current state ( $\hat{s}$ ) and the currently performed action which is determined by the policy ( $\pi(a|\hat{s})$ ), which determines from the the probability of performing action $a$ given in state $\hat{s}$ .
