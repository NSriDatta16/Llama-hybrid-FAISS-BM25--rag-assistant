[site]: crossvalidated
[post_id]: 424969
[parent_id]: 
[tags]: 
Autoencoders linear latent space

According to "Linear interpolation in latent space" in https://hackernoon.com/latent-space-visualization-deep-learning-bits-2-bd09a46920df and others, the latent space representation of an autoencoder is linear in that if I average out two representations and propogate back to input space, I will get a real looking object with the features of the two. What proof is there to show the space is indeed linear? What is the intution behind in?
