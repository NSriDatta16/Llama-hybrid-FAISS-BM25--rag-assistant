[site]: datascience
[post_id]: 97975
[parent_id]: 97921
[tags]: 
Graph embedding gives an embedding/vector per node. That is analogous to word embedding in NLP, which gives one vector per word (often methods are quite related, e.g. word2vec vs node2vec, deepwalk etc). If you want to embed paths, that sound analogous to "sentence embedding". There are a bunch of methods you could find for that (inc RNNs etc), but it is often found they don't do that much better than just obtaining node embeddings and then take the average of all node vectors in a path as its vector/embedding. (e.g. see this paper and others by Wieting)
