[site]: crossvalidated
[post_id]: 449622
[parent_id]: 
[tags]: 
Why and when CNNs are used with text embeddings?

I am working on a subtask of QA in NLP where the objective is to determine how well a question and its answer are correlated. I have generated both sentence embeddings(USE4) and word embeddings(BERT) for the questions and answers in the corpus and stacked them horizontally along with dot product between them i.e input = USE_embedding+BERT_embedding+dotproduct(USE_embedding) and fed it as input to a feedforward neural network whose task is to give the probability of relevance between the question and answer, however, the NN doesn't seem to work well on the test set(please note that I have fine-tuned BERT). After searching the web, I came across publications that are applying the convolutional layer as the first layer of NN to the embeddings, this is the part that is inscrutable to my Neural-Network(myself); paraphrasing,why is convolution layer favoured to detect useful pattern among embeddings? Thank you in advance!
