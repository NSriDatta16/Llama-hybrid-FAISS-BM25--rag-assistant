[site]: crossvalidated
[post_id]: 193656
[parent_id]: 
[tags]: 
Bose-Einstein in Marketing

I have been reading "Entropy Optimization Principles with Applications" by Kapur and Kesavan. In the book they derive some distribution of statistical mechanics and then show how they can be applied to everyday scenarios not involving particles and energy. Below is a quick over-view of their derivation of BE distribution, an application relating to marketing, and my question. The question is not so much of statistical mechanics although one of its distributions is used, but in when to know to use BE versus other structures of constraints in relation to what information may be known (marketing, urban planning, etc.) 3.2.2 The Bose-Einstein (BE) Distribution In deriving the Maxwell-Boltzmann distribution, we considered a system with $n$ possible states with energies $\epsilon_1,\epsilon_2, \dots, \epsilon_n$. There we assumed only one moment constraint arising from the knowledge of the mean energy of the particles, $\hat{\epsilon}$. In the present case, we also assume a second moment constraint, in addition to the first arising from a knowledge of the expected number of particles in the system, N (not the actual number of particles that must thus be assumed to vary between zero and infinity). On the basis of these two constraints, we proceed to apply MaxEnt for deriving the distribution Let $p_{ij}$ be the probability of there being $j$ particles in state $i$ of the $n$ states. Since it is certain that the number of particles in any one state will be between zero and infinity, the normalizing constraints are given by the following equations $\sum_{j=0}^\infty p_{ij} = 1$ The expected number of particles and the expected energy, which are the prescribed moments, are given by the following equations $\sum_{i=1}^n \sum_{j=0}^\infty j p_{ij} = A$ $\sum_{i=1}^n \epsilon_i \sum_{j=0}^\infty j p_{ij} = B$ [...] the total entropy is $- \sum_{i=1}^n \sum_{j=0}^\infty p_{ij} \ln(p_{ij}) $ [...] (solving for maximum entropy) yields $p_{ij} = (1 - e^{-\lambda - \mu \epsilon_i}) e^{-j (\lambda + \mu \epsilon_i)}$ The expected number of particles in the $i$th state, $\bar{N_i}$, is given by $ \bar{N_i} = \sum_{j=0}^\infty j p_{ij} = 1 / \left( e^{-\lambda + \mu \epsilon_i} -1 \right) $ The constraints can then be rephrased as (*) $\sum_{i=1}^n \bar{N_i} = A$ $\sum_{i=1}^n \epsilon_i \bar{N_i} = B$ They then give a great example illustrating the distribution with product purchases Example 3.2: There are three products whose costs are \$50, \$7.5, and \$1, respectively. If the number of the products required per month is 275 and the total cost average is $800, find the average number of each product required per month. Solution 3.2: This is a problem on Bose-Einstein distribution. Here we are given $\epsilon_1=50.0, \epsilon_2=7.5, \epsilon_3 = 1.0, A=275, B=800$ with solution $\hat{n_1}=5.6$ $\hat{n_2}=38.6$ $\hat{n_3}=230.8$ QUESTION Before I read that section I would have solved the similar problem differently. Since we know 275 products were purchased, we can solve for the proportion of how many were in each category. $n_1 + n_2 + n_3 = 275$ $\epsilon_1 n_1 + \epsilon_2 n_2 + \epsilon_3 n_3 = 800$ (Which is analogous to (*) above). Dividing by 275 yields $p_1 + p_2 + p_3 = 1$ $\epsilon_1 p_1 + \epsilon_2 p_2 + \epsilon_3 p_3 = (800/275)$ Which when trying maximize the entropy $- \sum_{i=1}^n p_{i} \ln(p_{i}) $ has a solution of $\hat{p} = \{.0008,.2879,.7114\}$ and multiplying by 275 to get the estimated number $\hat{n_1}=0.21$ $\hat{n_2}=79.16$ $\hat{n_3}=195.63$ The answers are dramatically different. In what situations is BE distribution applicable, and which is the solving the proportions directly applicable? They yield different answers, so the assumptions are different - but as now I don't see a reason why I couldn't have just solved the proportions problem directly.
