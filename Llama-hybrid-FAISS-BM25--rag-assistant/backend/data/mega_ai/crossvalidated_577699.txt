[site]: crossvalidated
[post_id]: 577699
[parent_id]: 
[tags]: 
Should Bayesian probability of model given the data coincide with intuitive estimation of this probability?

I have $N$ real valued targets ordered by one single real valued feature. I would like to build a simple probabilistic model that states that all the targets corresponding to the features that are smaller than $f_{split}$ are coming from one normal distribution given by mean $\mu_1$ and dispersion $\sigma_1$ and all the other targets coming from another normal distributions with another mean $\mu_2$ and another dispersion $\sigma_2$ . So, my model is given by 5 parameters (split value, two means and two dispersion). When I search the best model by maximization of likelihood, the best model splits the data such that in one basket of targets I have just 1 point and the corresponding distribution has mean equal to the value of that single target ( $\mu_1 = t_1$ ) and dispersion is equal to zero ( $\sigma_1 = 0$ ). I do understand why it is happening. The single target gets this way a huge (practically infinite) probability density and therefore the total likelihood is very large. My question is: What is the best way to avoid this problem. From an intuitive point of view, if I see by eyes that there is a split (targets on the left hand side and on the right hand side clearly have different averages and dispersion), I would not believe that the real model is the one which contains just one target on the left. So, why my intuitive estimation of model given data do not coincide with the Bayesian one (I assume that all the $a\ priori$ models have the same probability). I guess my intuition says that it is very unlikely that just by chance we had a model whose mean is equal to the target and whose $\sigma$ is so close to zero. But I do not know how to formalize it.
