[site]: crossvalidated
[post_id]: 4984
[parent_id]: 4978
[tags]: 
MaxEnt and Bayesian inference methods correspond to different ways of incorporating information into your modeling procedure. Both can be put on axiomatic ground (John Skilling's "Axioms of Maximum Entropy" and Cox's "Algebra of Probable Inference" ). Bayesian approach is straightforward to apply if your prior knowledge comes in a form of a measurable real-valued function over your hypothesis space, so called "prior". MaxEnt is straightforward when the information comes as a set of hard constraints on your hypothesis space. In real life, knowledge comes neither in "prior" form nor in "constraint" form, so success of your method depends on your ability to represent your knowledge in the corresponding form. On a toy problem, Bayesian model averaging will give you lowest average log-loss (averaged over many model draws) when the prior matches the true distribution of hypotheses. MaxEnt approach will give you lowest worst-case log-loss when its constraints are satisfied (worst taken over all possible priors) E.T.Jaynes, considered a father of "MaxEnt" methods also relied on Bayesian methods. On page 1412 of his book , he gives an example where Bayesian approach resulted in a good solution, followed by an example where MaxEnt approach is more natural. Maximum likelihood essentially takes the model to lie inside some pre-determined model space and trying to fit it "as hard as possible" in a sense that it'll have the highest sensitivity to data out of all model-picking methods restricted to such model space. Whereas MaxEnt and Bayesian are frameworks, ML is a concrete model fitting method, and for some particular design choices, ML can end up the method coming out of Bayesian or MaxEnt approach. For instance, MaxEnt with equality constraints is equivalent to Maximum Likelihood fitting of a certain exponential family. Similarly, an approximation to Bayesian Inference can lead to regularized Maximum Likelihood solution. If you choose your prior to make your conclusions maximally sensitive to data, result of Bayesian inference will correspond to Maximum Likelihood fitting. For instance, when inferring $p$ over Bernoulli trials, such prior would be the limiting distribution Beta(0,0) Real-life Machine Learning successes are often a mix of various philosophies. For instance, "Random Fields" were derived from MaxEnt principles. Most popular implementation of the idea, regularized CRF, involves adding a "prior" on the parameters. As a result, the method is not really MaxEnt nor Bayesian, but influenced by both schools of thought. I've collected some links on philosophical foundations of Bayesian and MaxEnt approaches here and here . Note on terminology: sometimes people call their method Bayesian simply if it uses Bayes rule at some point. Likewise, "MaxEnt" is sometimes used for some method that favors high entropy solutions. This is not the same as "MaxEnt inference" or "Bayesian inference" as described above
