[site]: stackoverflow
[post_id]: 4133066
[parent_id]: 4132712
[tags]: 
When you say "... or query the last timestamp for each value" is this what you had in mind? select max(timestamp) from T where value = ? If you have millions of records, and the above is what you meant (i.e. value is alone in the WHERE clause), then you'd need an index on the value column, otherwise you'd have to do a full table scan. But if queries will ALWAYS have [timestamp] column in the WHERE clause, you do not need an index on [value] column if there's an index on timestamp. You need an index on the timestamp column if your users will issue queries where the timestamp column appears alone in the WHERE clause: select * from T where timestamp > x and timestamp You could index all three columns, but you want to make sure the writes do not slow down because of the indexing overhead. The rule of thumb when you have a very large database is that every query should be able to make use of an index, so you can avoid a full table scan. EDIT: Adding some additional remarks after your clarification. I am wondering how you will know the id? Is [id] perhaps a product code? A single simple index on id might not scale very well if there are not many different product codes, i.e. if it's a low-cardinality index. The rebalancing of the trees could slow down the batch inserts that are happening every x milliseconds. A composite index on (id,timestamp) would be better than a simple index. If you rarely need to sort multiple products but are most often selecting based on a single product-code, then a non-traditional DBMS that uses a hashed-key sparse-table rather than a b-tree might be a very viable even a superior alternative for you. In such a database, all of the records for a given key would be found physically on the same set of contiguous "pages"; the hashing algorithm looks at the key and returns the page number where the record will be found. There is no need to rebalance an index as there isn't an index, and so you completely avoid the related scaling worries. However, while hashed-file databases excel at low-overhead nearly instant retrieval based on a key value, they tend to be poor performers at sorting large groups of records on an attribute, because the data are not stored physically in any meaningful order, and gathering the records can involve much thrashing. In your case, timestamp would be that attribute. If I were in your shoes, I would base my decision on the cardinality of the id: in a dataset of a million records, how many DISTINCT ids would be found? YET ANOTHER EDIT SINCE THE SITE IS NOT LETTING ME ADD ANOTHER ANSWER: Simplest way is to have two tables, one with the ongoing history, which is always having new values inserted, and the other, containing only 250 records, one per part, where the latest value overwrites/replaces the previous one. Update latest set value = x where id = ?
