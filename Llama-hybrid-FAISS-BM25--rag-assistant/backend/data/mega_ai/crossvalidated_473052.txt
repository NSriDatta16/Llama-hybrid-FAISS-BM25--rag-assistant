[site]: crossvalidated
[post_id]: 473052
[parent_id]: 473044
[tags]: 
You have to find out what they mean by "held-out". Usually, the "held-out" set is a validation set, a separate dataset that is used to estimate the test set error. For example, it is useful to estimate the test set error to do model selection on different hyperparameters and/or early stopping*. That is very common because ML algorithms often have many hyperparameters that are not fit during learning (the "K" in K-means, the regularisation coefficient in logistic regression, the number of iterations for iterative algorithms in general...). However, "held-out" literally means "taken out of" so depending on the context, I could see how the authors of the paper could denote the test set by it. You can trust the comparison if the authors do not use the held-out set for learning parameters or choosing hyperparameters. That is the right way to do this comparison. * early stopping can be seen as nothing more than hyperparameter search on the duration of training.
