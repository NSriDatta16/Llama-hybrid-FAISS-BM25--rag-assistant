[site]: crossvalidated
[post_id]: 562049
[parent_id]: 
[tags]: 
Outputs of model evaluation function

I am working on a 3-class ML problem and I am in the phase where I have to write an "evaluation" function for my training dataset (i.e. function that performs a cross-validation method to split the dataset into train/validation and returns optimal(s) classifier(s) for the entire training dataset). I would like your opinion on the most "correct"/convenient/usual way to define such a function. If we say for instance that I want to consider k-NN, SVM and Decision Tree classifiers for my problem, I am thinking of two different approaches to define such a function : On the one hand I could compare all classifiers of interest (for all hyper-parameters) inside the function and define it to return only the "optimal" as output (for instance output "SVM with C=1"). Then the ROC curves (over some test set) would describe which class is best classified by the classifier (is this correct ?) On the other hand I could define it in such a way that it returns many "optimal" classifiers for any algorithm type (for instance output "SVM with C=1", "k-NN with k=10" and "DT with max_depth = 10") and then choose among these classifiers using averages of AUC values (over some test set) for the different classes per classifier. By "optimal" on the above bullets, I mean with respect to a metric such as highest mean validation accuracy or mean f1 score. Still I haven't found a clear answer on the above thus I would like your perspective as well. I am quite new to the field so sorry if this is already answered somewhere. Thank you in advance.
