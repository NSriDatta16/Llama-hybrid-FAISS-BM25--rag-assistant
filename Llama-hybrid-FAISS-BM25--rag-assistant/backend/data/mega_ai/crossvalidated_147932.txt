[site]: crossvalidated
[post_id]: 147932
[parent_id]: 147786
[tags]: 
By definition, when training accuracy (or whatever metric you are using) is higher than your testing you have an overfit model . In essence, your model has learned particulars that help it perform better in your training data that are not applicable to the larger data population and therefore result in worse performance. I’m not sure why you say k-fold validation wouldn’t be helpful. Its’ purpose is to help avoid over fitting your models. Perhaps you don’t have enough data? A statement like this is important, especially if you are going to defend any research when such cross-validation methods are highly recommended. You say you aren’t able to use the test set just once (again I assume smaller sample size?). In my experience the most common path followed is k-fold cross-validation of you model. Let’s take an example with 10-fold CV for a sample size of 100 and assume your classification problem is binary to make the calculations simple. I therefore have split my data in to 10 different folds . I then fit my model to 9/10 folds and then predict the 1/10 I left out. For this first run, the resulting confusion matrix is: 0 1 0 4 1 1 2 3 I then repeat this analysis again with the next 1/10 fold left out and train on the other 9/10. And get my next confusion matrix. Once completed, I have 10 confusion matrices. I would then sum these matrices (so I had all 100 samples predicted) and then report my statistics (Accuracy, PPV, F1-score, Kappa, etc.). If your accuracy is not where you want it to be there are many other possibilities. Your model needs be improved (change parameters) You may need to try a different machine learning algorithm (not all algorithms created equal) You need more data (subtle relationship difficult to find) You may need to try transforming your data (dependent upon algorithm used) There may be no relationship between your dependent and independent variables The fact of the matter is, a lower testing metric (e.g. accuracy) than your training is indicative of overfitting your model not something you want when trying to create a new predictive model.
