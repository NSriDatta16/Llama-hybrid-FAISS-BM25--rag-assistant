[site]: crossvalidated
[post_id]: 398780
[parent_id]: 
[tags]: 
Understanding the log-likelihood (score) in scikit-learn GMM

I have been training a GMM (Gaussian Mixture, clustering / unsupervised) on two version of the same dataset: one training with all its features and one training after a PCA truncated to its 2 first principal components. Then I have been plotting their respective log-likelihood, given by .score() in scikit-learn api, against the number of clusters. I struggle to understand why the axis are not of the same range. I mean, why does the PCA model give me so small values (around -4) compared to the full model which gives values around 80? I have actually no guess if the log-likelihood should lie in a particular range since we are dealing with probabilities. Thanks
