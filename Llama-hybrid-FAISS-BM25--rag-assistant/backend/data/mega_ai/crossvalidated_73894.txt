[site]: crossvalidated
[post_id]: 73894
[parent_id]: 73858
[tags]: 
Since you are using the ROC, I presume that you are running 5 classifiers. Frank is right about the ROC, that's not the way people compare models. For the linear, and generalized linear models you can apply the likelihood ratio test. However, in case you are after the best prediction performance, and particularly in case you are not using a parametric model, but say a random forest classifier, I would do the following: generate data split it randomly into a training and testing set train all your 5 models and test their performance repeat the entire procedure for as many time as the run time permits and store all 5 ROC curves (I would pick a 1000, or 10000 as a minimum, depending on the convergence of the mean predictions) report the means of the 5 ROC curves together with a 90% pointwise confidence interval around them The idea is of course that you pick a model that seems like the best combination of high AUC and low variance (narrow intervals around the mean) of the estimates. Best
