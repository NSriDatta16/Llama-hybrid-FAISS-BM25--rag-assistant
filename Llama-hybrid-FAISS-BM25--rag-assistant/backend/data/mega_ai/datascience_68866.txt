[site]: datascience
[post_id]: 68866
[parent_id]: 68862
[tags]: 
I just googled: A Convolutional Encoder Model for Neural Machine Translation , by Gehring et al., link Convolutional Sequence to Sequence Learning , by Gehring et al. link Pervasive Attention: 2D Convolutional Neural Networks for Sequence-to-Sequence Prediction , by Elbayad et al. link All the implementations I found on GitHub are in PyTorch. I'm not surprised I didn't find much: the application of CNNs to NLP was interesting, but they never beat RNNs at that. After the advent of Attention models, and especially Transformers, these kind of models have not been developed further. In fact the latest paper I linked you is from 2018 - i.e. an Ice Age ago for the pace of NLP. If you really want to go deeper on this topic of Convolutional NMTs, I suggest you to check the available torch-based code and try to make a replica in Tensorflow/Keras. It's hard work, but a fancy model nonetheless. Good luck!
