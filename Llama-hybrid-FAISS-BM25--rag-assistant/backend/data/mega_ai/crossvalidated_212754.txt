[site]: crossvalidated
[post_id]: 212754
[parent_id]: 212635
[tags]: 
It is a common misconception that a model is overfit when the training and hold out error metrics are divergent. This does tend to happen when models become overfit, but it is not a sufficient condition. A better definition of overfitting is the following A model is overfit when a (small) decrease in model complexity results in an increase in hold out performance. Models with quite different training and hold out performance can be underfit (a small increase in complexity improves hold out performance). So for your case, where you want all and only the model with the best hold out performance, you should focus on this aim. If you are confident in your estimate of the hold out performance of your neural network, and there are no barriers to using it in your problem domain, go with that.
