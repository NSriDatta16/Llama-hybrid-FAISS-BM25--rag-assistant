[site]: crossvalidated
[post_id]: 573688
[parent_id]: 
[tags]: 
Metric to quantify randomness

I have an $L \times L$ matrix representing a $2D$ region. Each entry of this matrix is a real number lying in the closed interval [0,1]. I want to quantify how different is this from a similar $L \times L$ matrix with completely random entries (say when each entry is sampled independently from a continuous uniform distribution between $[0,1]$ ). A naive way I can immediately think of is to use Monte Carlo methods - Sample a random matrix as described above and use the difference of matrix norms (e.g. Frobenius norm, L2 norm) as the metric. This can be averaged over say 50,000 such iterations to get a measure of how different the matrix is from a completely random one. Is there a more principled metric which can be used to compare such matrices based on how different they are from a random matrix? I feel a more information-theoretic approach that measures the "informational content" of the matrix in Shannon's sense will be relevant here. Also, need we define the random matrix in a different way (perhaps to make it easier to compute this metric)? Context - The entries of the matrix represent the population of the region (which has been discretized into grid cells). I want to study how population changes in each cell over time. I want to be able to quantify how complex is this evolution of the population (for example, if one can identify some complex behavior representative of a complex system). Note that it is important to compare matrices as such (and not flatten them to 1D arrays) because adjacent cells in the matrix are spatially correlated and by flattening we will lose this crucial information
