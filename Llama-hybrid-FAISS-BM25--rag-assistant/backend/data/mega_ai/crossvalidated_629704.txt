[site]: crossvalidated
[post_id]: 629704
[parent_id]: 629671
[tags]: 
Omitted Part of Original Answer I originally deleted this part of the answer based on commentary below. However, I believe that it decontextualizes the comments so I have added it back in so it makes sense. The biggest issue you have here is that you likely have too few variables to meaningfully produce three factors. I think at a minimum you should have at least three or four items per factor, but it is much safer to simply create many items and see if they produce a reasonable factor solution. Doing anything with six items is difficult enough if there are problems with your items, but made even more complex if you are trying to spread that out between three factors, particularly if they poorly load onto a latent factor already. That is probably the most important nugget. While you can theoretically obtain a factor with just two items, they need to load onto the latent variable pretty well (aka be strongly correlated with each other) for that to happen. Since you can't retest these people, you are probably stuck with either a two- or one-factor solution. Whether that is achievable is another story, but I think in your situation there is no other option. That likely isn't a major problem if you can get it to work, but you may need to adapt your theoretical explanation of why this may be the case. Anyway, that may ultimately be a good thing, because it can be difficult to meaningfully interpret a two-item factor anyway if the items don't provide a solid explanation of what people are being tested on. Core Answer There is certainly something wrong with your factor analysis and given the fit statistics look wonky (and as whuber notes the implausibility due to singularity), I would abandon ship with the factor solution you got. There are a series of steps you can employ to check what's going on. I tried to put them in some kind of order, but some of these you may need to go back to after accomplishing other steps. A large culprit can be a lack of variation in items. Check your items for linear constants (those which literally only have one response) or near linear constants (items which have extremely low variation). This can be simply achieved by tabulating counts or running bar plots of the items. If you have a binary item with bar plots that just include $1$ as the recorded response, this item is basically useless. From my experience, this has been a common but substantial problem in factor analyses I have run in the past. Thankfully, it is also the most easily fixable problem if you have an adequate number of items. Check the determinant or the eigenvalues of your matrix . This is easily accomplished in R. If you have a correlation matrix named mat , you can simply run det(mat) or eigen(mat) to obtain the determinant and the eigenvalues (though see the comments below for limitations to the former, in which the eigenvalues may provide a more useful check). If the determinant is extremely close to zero, this can cause some major instability in estimation and if it is exactly zero this means the matrix is singular. This often can crop up when you have perfect or near perfect collinearity between items, but the opposite can also happen if you have very poor correlations between items, particularly if there are a lot of them. Inspect your item correlation matrix. I find this to be bewilderingly uncommon when people estimate factor analyses, and yet it is often one of the clearest steps to checking if your items are behaving like you expect them to. If for example you get two distinct color patterns in a correlation heatmap (one group of items clusters together a lot in correlation strength while another is barely correlated), this will often clue you in on how many factors are sensible. This will also be important if the above determinant check fails, as you can identify which items are the main culprits. The number of items is important. If you have a ton of items on a test, this can sometimes complicate matters (note: this is a much better problem than having too few items, where you sometimes end up with catastrophic dumpster fires that simply can't be put out). If you have many zero or near-zero elements in your matrix (which contributes to the issue I highlighted above), this can cause it to become very unstable with many items. There are multiple options here. If you have a lot of items, you can simply remove some which are especially problematic. If you need to retain as many items as possible or removing items doesn't fix the issue, another method is to employ a PCA smoothing of the correlation matrix. This can be accomplished with the cor.smooth function in the psych package and works remarkably well. Factor solution is simply poor. If you notice after checking your correlations there are patterns as I noted, I would definitely suggest changing your factor solution to include more/less based on what you see. If its not totally clear from the others checks what makes sense, one can also run a parallel analysis and compare it to the original raw data analysis (it appears from your syntax you already have, so I would plot that and see what that looks like).
