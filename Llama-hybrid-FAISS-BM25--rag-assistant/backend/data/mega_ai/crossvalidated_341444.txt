[site]: crossvalidated
[post_id]: 341444
[parent_id]: 341433
[tags]: 
At each node, a subset of the full set of predictors is evaluated for their strength of association with the dependent variable. The strength of association may be measured using a correlation coefficient, or some other metric (necessary if there are both categorical or continuous predictors). The most strongly associated predictor is then used to split the data. This implies that variables that occur closer to the root are more important, in the sense that they are most strongly associated with the dependent variable in each of the bootstrap data subsets. times_a_root and the mean_min_depth are straightforward ways of measuring importance in this sense: a variable that is closer to the root, or one that on average occurs closer to the root, is one that is strongly associated with the dependent variable. no_of_nodes is distinct from the other two. Picture the dependent variable being a strong sinusoidal function of one predictor. The predictor will not necessarily appear as being important on the previous two metrics because of the lack of a clear trend/direction/non-zero linear slope in a bivariate plot. However, eventually the trees will start to split on this predictor, and will then continue to split on it a very large number of times to approximate the sinusoidal function. no_of_nodes will capture the importance of this and (I suspect) other nonlinear predictors (without a clear trend/direction/non-zero linear slope) better than the earlier metrics. That said, I think that accuracy_decrease (for classification) and mse_increase (for regression) are far better metrics of importance than the rest. They measure the decrease in the forest's predictive performance if a particular predictor is permuted. Correlated predictors will influence this, but they will also affect the other importance metrics as well. And whether that matters depends on what your goals for the analysis are.
