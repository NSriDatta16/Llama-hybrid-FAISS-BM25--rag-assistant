[site]: datascience
[post_id]: 111357
[parent_id]: 111342
[tags]: 
A simple way to estimate the loss of data due to the normalization/scaling, is to apply the inverted algorithm to see how different it is from the raw data. If the data loss is very low (ex: 0.1%), scaling is not an issue. On the other hand, if your clusterization works very well for 10k customers, it shall work well for 1 million. Generally speaking, it is better to have a very good model on a small random dataset and then increase it progressivelly until you reach the production scale. You can either make clusters from one feature, or several features. Due to the problem complexity, it is generally better to start with one feature, and then extend to several features. Making clusters from several features works better with dimensional reduction algorithms (ex: UMAP), because you project all your dimensions in a 2D plan automatically and make interesting correlation studies for all customers. If you apply a good multi dimensional clustering, all the features are taken into account and every point is represented by a customer id. If you select a cluster through a cluster technique (ex: DBSCAN), you just have to extract the list of the customers from this cluster, filter the raw data with this list, and start your data analysis to answer q1,q2 or q3. Note that normalization depends on the dimensional reduction algorithm you are using. UMAP wouldn't require data normalisation, whereas t-SNE or PCA requires it. https://towardsdatascience.com/tsne-vs-umap-global-structure-4d8045acba17 Finally, clusters' interpretation should be backed by actual proofs: even if algorithms are often very efficient in clustering data, it is crucial to add indicators to check if the data has been well distributed (for instance comparing mean or standard deviation values between clusters). In some cases, if the raw data have a too wide distribution, it could be interesting to apply a log but you might loss information.
