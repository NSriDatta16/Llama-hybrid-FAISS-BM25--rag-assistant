[site]: datascience
[post_id]: 124449
[parent_id]: 
[tags]: 
For a fine-tuning a transformer to type like a specific person, should I use sentence semantic embeddings or word semantic embeddings

I'm not clear on the pros and cons of each one for this particular task. Is there even a meaningful difference? My guess is using semantic embeddings for words will be better in nearly all cases because if you use semantic embeddings for the entire sentence you will still need to tokenize every word anyway. Moreover, sentence semantic embeddings seem like they'd require a much larger vector to be useful given the wide variety of sentence combinations that exist. So even though each word needs its own embedding vs each sentence, if your sentences are all under like 20 words the word embeddings would need less computational power. But that's just my intuition. Because sentence semantic embeddings really only got popular this year, I can't find a clear answer anywhere so I'm asking.
