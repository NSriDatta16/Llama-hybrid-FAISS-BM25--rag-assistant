[site]: crossvalidated
[post_id]: 621802
[parent_id]: 
[tags]: 
Continue train xgboost specifically on misclassified observations?

I'm considering integrating the Boosting technique into a basic XGBoost classification model, in which I'd focus on misclassified instances. Assuming I have already used hyperopt to build a basic XGBoost model ( let us name it xgboost_model ), I want to understand if the following approach is sound: a. Identify and isolate observations where the XGBoost model's predictions are incorrect. For example, in a classification model, if y_pred probability is less than 0.1 when y_true is 1. b1. Increase the weight of misclassified observations and randomly sample from the original dataset to generate the next training set, x_train_2 and y_train_2 . b2. Continuously train the existing model using model = model.fit(x_train_2, y_train_2, xgb_model=xgboost_model.get_booster()) . In my experiments, I've noticed a slight improvement in the aucpr on the validation dataset. Since my dataset is highly imbalanced, I've been using aucpr instead of auc . However, I'm uncertain if these results are due to luck or if this strategy is theoretically sound. I'm particularly concerned about potential overfitting . Is this a valid concern?
