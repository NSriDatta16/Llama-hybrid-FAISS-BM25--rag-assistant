[site]: datascience
[post_id]: 54031
[parent_id]: 54026
[tags]: 
I am using svm.SVR() from scikit-learn to apply Logistic Regression on my training data to solve similarity problem. Wait a second, if you're using support-vector regression, then you're not using logistic regression. These two are very different algorithms. They aren't even applicable to the same type of problem. Support-vector regression is used when you are predicting a continuous target, whereas logistic regression (despite the name) is a classification algorithm. However, for some of the test data, the prediction contains value in negative (less than 0) and for all the same vs same comparison it returns the value as 1.09469178. There's nothing unusual about this if you're using support-vector regression. A support-vector machine can (in theory) output any real number. Logistic regression, on the other hand, is a sigmoid function . It will take as input any real number and output a result between 0 and 1. Maybe you meant to use scikit's LogisticRegression model rather than SVR? The reason I am using SVM-Regression is to find similarity between two inputs. Could you expound on your use-case a bit more? Support-vector regression isn't really meant to be used to compute similarity. I'm speculating that you have a training set of (X, y) pairs (where y is a label between 0 and 1). You are training a model to output $\hat{y}$ , a prediction of y. To find the similarity of two inputs, $X_i$ and $X_j$ , you pass them both through the model and measure the difference in the $\hat{y}_i$ and $\hat{y}_j$ . Is that right? If so, I think this is a really roundabout way of computing similarity, and is unlikely to yield better results than a more straightforward method. I'm not sure that you need a machine learning algorithm at all. Have you looked into other similarity metrics that might be suitable for your problem? For similarity based on Euclidean distance, you can compute $\frac{1}{1 + d(X_i, X_j)}$ (where $d$ is the euclidean distance function). Cosine similarity might be a good choice if you care more about the similarity in direction of two input vectors.
