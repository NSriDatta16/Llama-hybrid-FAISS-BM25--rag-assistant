[site]: crossvalidated
[post_id]: 60573
[parent_id]: 60559
[tags]: 
(I wouldn't call this a simple problem. Certainly my suggested solution isn't simple.) From your description you have no way to be certain if a particular observation of 4.7 is genuine or not; if the series is near 4.7 you could have any number of 'legitimate' 4.7's mixed with any number of 'bad' ones. As such, any assessment would probably be best regarded probabilistically. I'd be inclined toward a Bayesian (indeed, MCMC) approach that has a model for the generation of the 'bad' 4.7-s and (from the model for the rest of the data) is able to infer the lost information on any iteration it decides an observation near 4.7 is bad. You'd end up with some points near 4.7 being always or almost always regarded as 'bad' (the ones where it's obviously bad) and then it's treated like missing data, and some points near 4.7 that are mostly regarded as 'good', which are occasionally replaced with a value suggested by the other data and your model (which will also turn out to be more or less near 4.7), and some data that's 'in between' (where you might suspect it could be bad but wouldn't be sure), which is often but not always replaced.
