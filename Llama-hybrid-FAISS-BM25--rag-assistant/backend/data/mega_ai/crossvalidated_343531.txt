[site]: crossvalidated
[post_id]: 343531
[parent_id]: 
[tags]: 
Are boosted trees always better than bagging trees?

I've observed this on quite a few different types of datasets, GBM or XGBOOST always perform better than Random Forests . The accuracy jump on the test set is always more than 5%, for the datasets I've worked on. Is it true that boosting is always a better option or it's just overfitting that I'm seeing, otherwise why would we choose Random Forest ? Note: I use SMOTE to balance out classes as I always get imbalanced data, not sure if this may be a factor
