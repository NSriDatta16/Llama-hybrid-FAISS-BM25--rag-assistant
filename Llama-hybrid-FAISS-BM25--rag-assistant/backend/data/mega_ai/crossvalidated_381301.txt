[site]: crossvalidated
[post_id]: 381301
[parent_id]: 
[tags]: 
Does cross validation say anything about parsimony?

Suppose I had a set of models that all attempt to explain some phenomena. According to a sensible—and appropriately cross-validated—performance metric, all of the models perform comparably well. The models' predictions, while not totally identical, are fairly well correlated. However, some models are more complex than others (e.g., have more parameters and hyperparameters to optimize) and they may even use different type—and number—of predictors as input. Furthermore, the metric used to evaluate the models does not penalize model complexity (e.g., cross-entropy, rather than BIC), What, if anything, does this say about the models' explanatory power? I ask because I've recently encountered this situation in a few talks, and there seem to be two opposing arguments. My gut feeling is essentially Occam's Razor: the best way to demonstrate our understanding of a phenomenon is to build a simple but effective model from explicit input features. For example, a model that explicitly states "mutation in gene X leads to disease Y" seems to demonstrate more understanding of the gene/disease than using a huge neural network to pull the same information out of the raw reads. This is especially true when the NN does not perform much better than the simple model. Both speakers' counter-argument was that, no, the most complicated model is the most interesting: it had the capacity to overfit the data, but judging by their equivalent cross-validated performance, it apparently did not. Thus, we should be investigating the structure of the complex model instead, since the true phenomena is probably complex and not captured by the simpler models.
