[site]: crossvalidated
[post_id]: 43363
[parent_id]: 41837
[tags]: 
You could certainly arbitrarily assign outliers to have the conversion system that reduces their outlier-ness. But I think this would be more of an arbitrary decision than an improvement. You may also be seeking a simple test for whether your distribution of cups of a given ingredient in a given recipe category is unimodal or not (after normalizing for servings and other ingredients). Hartigan’s dip test is one such test, but I doubt it will be helpful here. You could certainly make a simulated best-case scenario dataset to run the diptest on and get a better sense of its utility here. My inclination is to treat this as a data mining problem in which we seek to find which other features in the recipe are associated with the different kinds of cups. I would hypothesize that cup type is associated with region/culture and that region/culture is best identified by linguistic features in the recipes. I would to train a classifier, perhaps just Naïve Bayes, with a dataset in which the cup type is known. This may rule out online recipe websites, many of which automatically translate from metric to imperial units. Ideally, you would scrape the contents of several regional cookbooks. I’d probably run a quick sanity check to see if there actually was any regional variation in units. Who knows, variation in measurement units may be better explained by whether the recipe is targeted to casual cooks (unlikely to own scales) or serious ones. Ratios of ingredients may be helpful, but it is perhaps too complicated to be worth pursuing. Most recipes are tolerant of imprecision. Within less tolerant categories (say cakes) the crucial factor is not the raw ingredients, but the ingredients of the ingredients. That is, fat, protein, etc. So you would ideally lookup ingredients’ nutritional profiles in any of the many online sources. I think it would probably be easier and more successful to see if we can classify based on the words in the recipes, including the title, ingredients, or even step-by-step instructions. For example, “caster sugar” (British English) vs. “fine sugar” (American) or “self-raising flour” (a British product). To identify these shibboleth features, you might train random forests to predict region or measurement unit system when given a TF-IDF (term frequency – inverse document frequency) matrix made by counting words in recipes. In R, the varSelRF and Boruta packages are useful for variable selection via random forest. But I do think the key is obtaining a data set with known ground truth.
