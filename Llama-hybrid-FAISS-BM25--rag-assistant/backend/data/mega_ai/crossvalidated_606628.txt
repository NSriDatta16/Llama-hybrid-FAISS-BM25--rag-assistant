[site]: crossvalidated
[post_id]: 606628
[parent_id]: 
[tags]: 
Latent Semantic Indexing vs. PCA

I am trying to understand how Latent Semantic Analysis works, reading demonstrations based on singular value decomposition. Let's denote $X$ a $N \times D$ document-term matrix. The $D$ rows of $X$ represent documents, and the $W$ columns represent words. Using SVD, we can write $X = U \Sigma V^T$ . To me, this formulation is nothing more than a non-centered PCA. So then $WordSim = U \cdot S$ gives the word similarity matrix , where the rows of $WordSim $ represent different words and $DocSim= S \cdot V^T$ gives the document similarity matrix where the columns of $DocSim$ represent different documents. This would be akin to projecting the rows or columns onto the principal components of the $X^{T}X$ or $XX^{T}$ , which is just like PCA. Assume we have a new document $d$ . We would like to get a rank $k$ approximation of $d$ by applying the same mapping that transformed rows in $X$ into rows in $\tilde{X}$ and get $\hat{d}$ . Wikipedia has it that $\hat{d} = \Sigma^{-1}_k U_k^T d$ , but I can't find a full, formal mathematical demonstration behind this equation. I don't understand why we can't just directly project $d$ onto $U^{T}$ , as suggested here ?
