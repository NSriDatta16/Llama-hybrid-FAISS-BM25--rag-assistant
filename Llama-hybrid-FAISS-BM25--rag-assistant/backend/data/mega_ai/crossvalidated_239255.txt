[site]: crossvalidated
[post_id]: 239255
[parent_id]: 239023
[tags]: 
In addition to @Firebug's answer, As for imputation for the test data set: IMHO it is legitimate to use training data to do this: This would be an "imputation model" trained on training data and applied to test data, and in principle not different from deriving, say, the mean from training data and then applying this e.g. for centering to the test data as well. However, you need to make sure no test data leaks into the training set: the test data needs to stay unknown . From my application-centered point of view, pre-processing is part of the model building and needs to be included in the validation (just like hyperparameter tuning is), i.e. also needs to be re-calculated inside the cross validation loop. Some preprocessing steps can be pulled out of the validation loop without risking a data leak: the step in question calculates strictly within each case (i.e. does not consider any other cases but the one that is processed at the moment), and there was no preprocessing step that considered more than one case up to this point. In other words, the cross validation loop needs to start before the first (pre)processing step that uses more than one case. If you need a citation for this, we wrote it e.g. in Dochow, S.; Beleites, C.; Henkel, T.; Mayer, G.; Albert, J.; Clement, J.; Krafft, C. & Popp, J.: Quartz microfluidic chip for tumour cell identification by Raman spectroscopy in combination with optical traps. Anal Bioanal Chem, 2013, 405, 2743-2746. DOI: 10.1007/s00216-013-6726-3
