[site]: crossvalidated
[post_id]: 578524
[parent_id]: 115981
[tags]: 
First note that a neural network usually has more than one activation functions (the activation function in hidden layers is often different from that used in the output layer). Any function that is continuous can be used as an activation function, including linear function g(z)=z, which is often used in an output layer. Since only activation function can introduce nonlinearity to a network, and nonlinearity is indispensable for a neural network to make untrivial predictions, activation functions in hidden layers are usually nonlinear, e.g. relu, which is a piecewise linear function that introduces the most simple nonlinearity.
