[site]: crossvalidated
[post_id]: 402344
[parent_id]: 402333
[tags]: 
The two worlds that you describe aren't really two different kinds of statistician , but rather: "statistics on rails," to coin a phrase: an attempt to teach non-technical people enough to be able to use statistics in a few narrow contexts. statistics proper, as understood by mathematicians, statisticians, data scientists, etc. The deal is this. To understand statistics in even moderate depth, you need to know a considerable amount of mathematics. You need to be comfortable with set theory, outer product spaces, functions between high dimensional spaces, a bit of linear algebra, a bit of calculus, and a smidgen of measure theory. It's not as bad as it sounds: all this is usually covered adequately in the first 2-3 years of undergraduate for hard science majors. But for other majors... I can't even formally define a random variable or the normal distribution for someone who doesn't have those prerequisites. Yet, most people only need to know how to conduct a simple A/B test or the like. And the fact is, we can give someone without those prerequisites a set of formulas and look-up tables and tell them to plug-and-chug. Or today, more commonly a user-friendly GUI program like SPSS. As long as they follow some reasonable rules of experiment design and follow a step-by-step procedure, they will be able to accomplish what they need to. The problem is that without a fairly in-depth understanding, they: are very likely to misuse statistics can't stray from the garden path Issue one is so common it even gets its own Wikipedia article , and issue two can only really be addressed by going back to fundamentals and explaining where those tests came from in the first place. Or by continually exhorting people to stay within the lines, follow the checklist, and consult with a statistician if anything seems weird. The following poem comes to mind: A little learning is a dangerous thing; Drink deep, or taste not the Pierian spring: There shallow draughts intoxicate the brain, And drinking largely sobers us again. - Alexander Pope, A Little Learning I would liken the "on rails" version of statistics that you see in AP stats or early undergraduate classes for non-majors as the difference between WebMD articles and going to med school. The information in the WebMD article is the most essential conclusion and summary of current medical recommendations. But its not intended as a replacement for medical school, and I wouldn't call someone who had read an WebMD article "Doctor." What do you consider as must to know in statistics and machine learning? The Kolmogorov axioms , the definition of a random variable (including random vectors, matrices, etc.) the algebra of random variables , the concept of a distribution and the various theorems that tie these together. You should know about moments . You should know the law of large numbers , the various inequality theorems such as Chebyshev's inequality and the central limit theorems , although if you want to know how to prove them (optional) you will also need to learn about characteristic functions , which can occasionally be useful in their own right if you ever need to calculate exact closed form distributions for say, a ratio distribution . This stuff would usually be covered in the first (or maybe second?) semester of a class on mathematical statistics. There is also a reasonably good and completely free online textbook which I mainly use for reference but which does develop the topic starting from first principles. There are a few crucial distributions everyone must know: Normal, Binomial, Beta, Chi-Squared, F, Student's t, Multivariate Normal. Possibly also Poisson and Exponential for Poisson processes, Multivariate/Dirichlet if you work with multi-class data a lot, and others as needed. Oh, and Uniform - can't forget Uniform! At this point, you're ready to learn the basic structure of a hypothesis test ; which is to say, what a " sample " is, and about null hypothesis and critical values, etc. You will be able to use the algebra of random variables and integrals involving distributions to derive pretty much all of the statistical hypothesis tests you've seen in AP stats. But you're not really done, in fact we're just getting to the good part: fitting models to data. There are various procedures, but the first one to learn is MLE . For me personally, this is the only reason why developed all the above machinery. The key thing to understand about fitting models is that we pose each one as an optimization problem where we (or rather, very powerful computers) find the "best" possible set of "parameters" for the model that "fit" a sample. The resulting model can be validated, examined and interpreted in various ways. The first two models to learn are linear regression and logistic regression , although if you've come through the hard way you might as well study the GLM (generalized linear model) which includes them both and more besides. A very good book on using logistic regression in practice is Hosmer et al. . Understanding these models in detail is very demanding, and encompasses ANVOA , regularization and many other useful techniques. If you're going to go around calling yourself a statistician, you will definitely want to complement all that theoretical knowledge with a solid, thorough understanding of the design of experiments and power analysis. This is one of the most common thing statisticians are asked to provide input on. Depending on how much model building you're doing, you may also need to know about cross validation , feature selection, model selection, etc. Although maybe I'm biased towards model building and you could get away without this stuff? In any case, a reasonably good book, especially if you're using R, is Applied Predictive Modeling by Max Kuhn. At this point you'll have the "must know" knowledge you asked about. But you'll also have learned that inventing a new model is as easy as adding a new term to a loss function, and consequently a huge number of models and approaches exist. No one can learn them all. Sometimes it seems as if which ones are in fashion in a given field is completely arbitrary, or an accident of history. Instead of trying to learn them all, rest assured that you can you the foundation to built to understand any particular model you need if a few hours of study, and focus on those that are commonly used in your field or which seem promising to you. What tests/ methods would you put in your toolbox? All right, laundry list time! A lot of these come from The Elements of Statistical Learning, by Hastie, Tibshirani, and Friedman which is a very good book by three highly respected authors. Another good resource is scikit-learn , which tends to most of the most mature and popular models. Ditto for R's caret package , although it's really focused on predictive modeling. Others are just models I've seen mentioned and/or used frequently. In roughly descending order of popularity: Ridge, Lasso, and ElasticNet Regression Local Regression (LOESS) Kernel Density Estimates PCA Factor Analysis K-means GMM (and other mixture models) Decision Trees, Random Forest, and XGBoost Time Series Analysis: ARIMA, possible exponential smoothing SVM (Support Vector Machines) Hidden Markov Models GAM (General Additive Models) Bayes Networks and Structual Equation Modeling Robust Regression Imputation Neural Nets, CNNs (for images), RNN (for sequences). See the Deep Learning Book by Goodfellow, Bengio, and Courville. Bayesian Inference with MCMC a la Stan Survival Analysis (Cox PH, Kaplan-Meier estimator, etc.) Extreme value theory Vapnikâ€“Chervonenkis theory Causality Pairwise/Perference modling e.g. Bradley-Terry IRT (item response theory, used for surveys and tests) Martingales Copulas This is a pretty idiosyncratic list. Certainly I don't know everything on that, and even where I do my knowledge level varies from superficial to long experience. That's going to be true for everyone. Everyone is going to have their own additions to this list, and above all their own priorities. Some people will tell you to dive right in to neural nets and ignore the rest. Some people (actuaries) spend their entire career focusing on survival analysis and extreme value theory. I can't give you any real guidance except to study techniques that are used in your field and apply to your problems.
