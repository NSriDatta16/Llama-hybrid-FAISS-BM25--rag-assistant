[site]: datascience
[post_id]: 66840
[parent_id]: 66745
[tags]: 
It is therefore a problem of detecting the importance of the variables. You have many solutions based on the analysis of a regression model , Stepwise regression algorithms (backwards, forwards), random forest ... However, the simplest solution is probably to use a decision tree : the results obtained are presented under a graphical form that is simple to interpret, and is an effective aid to decision support. Once the graph is obtained, we can already have an idea of the importance of the variables by looking at the first nodes (it often happens that we use the conditions of these first nodes to create synthetic features that we reinject in a logistic regression). Then to obtain more precise results, we can compute the total reduction of the criterion brought by that feature in the splits. In python, with scitkit-learn, you could use the feature_importances_ attribute of a DecisionTreeClassifier to get the details of the importance of the variables: import matplotlib.pyplot as plt from sklearn.tree import DecisionTreeClassifier, plot_tree # init the classifier clf = DecisionTreeClassifier() # don't forget that you need to one-hot encode categorical # variables before you fit a tree with sklearn! clf.fit(X, Y) # plot the graph plot_tree(clf, filled=True) plt.show() print(clf.feature_importances_) Explanation : The objective of each split is to find the division, or more precisely the variable and the division rule, which will contribute to the strongest decrease of the heterogeneity of the son nodes on the left $\kappa_l$ and on the right $\kappa_r$ . In the case where Y is a qualitative variable, several heterogeneity functions can be defined for a node: a criterion defined from the notion of Entropy or from the Gini concentration (actually there is also the CHAID criterion which is based on the statistical test of $\chi^2$ ). In practice, it turns out that the choice of the criterion has little influence, and it is often Gini that is chosen by default. In the node $\kappa$ , where $p_\kappa^l$ is the number of element of the class $l$ in the node $\kappa$ , and $m$ the number of classes. \begin{align*} \textit{Entropy} : &S_\kappa = - \sum_{l=1}^{m} p_\kappa^l log(p_\kappa^l) \\ \textit{Gini} : &G_\kappa = \sum_{l=1}^{m}p_\kappa^l(1 - p_\kappa^l) = 1 - \sum_{l=1}^{m}(p_{\kappa}^{l})^2 \end{align*}
