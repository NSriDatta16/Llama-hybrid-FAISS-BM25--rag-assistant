[site]: crossvalidated
[post_id]: 615159
[parent_id]: 611723
[tags]: 
Given the large amount of comments I thought that it might be better when I place some of them in an answer. What I understand from the comments following the question is that the idea is about performing some form of MCMC-sampling, but with a Kernel that adapts after each new sample. And, the motivation is to ensure that the sample will satisfy a certain condition. Simple example A simple example would be the sampling of a distribution symmetric around zero (e.g. a standard normal distribution) but with the constraint that the sample mean needs to be zero (typically, if the distribution has zero mean, a sample from it does not need to be zero). If we have a kernel that proposes every time that the sample mean is unequal to zero a sample whose value makes the sample mean equal to zero. Then we ensure that this property is fulfilled (at least every odd step). The set of samples that can be sampled will not be iid variables and will not be the same as sampling the target distribution without the constraint. However, if the target distribution is symmetric around zero, then this algorithm will generate a sample whose empirical distribution approaches the true distribution. Below is a code example that has the proposed sample based on the complete history $$x^\star|x_0,x_1,\dots,x_t \sim \begin{cases} -\sum_{i=0}^t x_i & \qquad \text{if $\sum_{i=0}^t x_i \neq 0$} \\ N(x_t,0.04) & \qquad \text{if $\sum_{i=0}^t x_i = 0$} \\ \end{cases}$$ Below is an example of the histogram of a sample of size 50000 when the target function is a standard normal. This sample is in not a typical sample from a normal distribution. The sample will be having zero mean with probability 1, whereas a sample from a normal distribution will be having zero mean with probability 0 (and also the samples will be relatively symmetric). However, the empirical distribution will approach the the distribution function of the target distribution. So in that sense this sampling 'works'. For different more complex cases it will depend. For example, when we use the method above with a non-symmetric distribution, then it stops 'working'. set.seed(1) newsample = function(old_sample, LikelihoodFunction) { L = length(old_sample) m = sum(old_sample) ### if the current sample has not zero mean then the suggestion will always be a sample that makes the mean zero if (m!=0) { suggest = -m } else { suggest = rnorm(1,old_sample[L],0.2) } ### compare the likelihood and base the next sample on it u = runif(1) l1 = LikelihoodFunction(suggest) l2 = LikelihoodFunction(old_sample[L]) if (u
