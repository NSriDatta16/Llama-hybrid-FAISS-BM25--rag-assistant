[site]: crossvalidated
[post_id]: 643022
[parent_id]: 
[tags]: 
Good way to select reference and/or performing normalization of data points across experiments?

I have a set of objects S1 = {a1, b1, c1, d1, ..., z1} that perturbed a reference object q and were tested for effects in Experiment 1. The measurements for S1 were normalized to q . From this dataset, I derived modifications S2 = {a2, b2, c2, d2, .., z2} for further testing using machine learning, and included as references R = {q, a1, b1, z1} from the original set due to limited experimental capacity, i.e. R != S1 . What is the correct way to normalize S2 such that S2 and S1 measurements are comparable? I am guessing normalizing S2 w.r.t q alone won't suffice and that learning a regression curve between {q, a1, b1, z1} values from S1 and S2 might allow me to learn the shift if there is any, due to batch effects and such. Is this sufficient? Or an overkill, since normalizing S2 to q alone would be sufficient? Is there a definitive or standard way to plan the normalization so that it is robust across experimental rounds? A naive solution would be to test at each round the union of all objects from earlier rounds and then normalizing everything w.r.t. q in current round to propose further modifications in the next round. But, of course, this is expensive.
