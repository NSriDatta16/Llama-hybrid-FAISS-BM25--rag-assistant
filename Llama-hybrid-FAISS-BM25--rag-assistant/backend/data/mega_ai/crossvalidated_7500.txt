[site]: crossvalidated
[post_id]: 7500
[parent_id]: 7497
[tags]: 
Definitely not, although they are frequently used interchangeably. A vague prior (relatively uninformed, not really favoring some values over others) on a parameter $\theta$ can actually induce a very informative prior on some other transformation $f(\theta)$. This is at least part of the motivation for Jeffreys' prior, which was initially constructed to be as non-informative as possible. Vague priors can also do some pretty miserable things to your model. The now-classic example is using $\mathrm{InverseGamma}(\epsilon, \epsilon)$ as $\epsilon\rightarrow 0$ priors on variance components in a hierarchical model. The improper limiting prior gives an improper posterior in this case. A popular alternative was to take $\epsilon$ to be really small, which results in a prior that looks almost uniform on $\mathbb{R}^+$. But it also results in a posterior that is almost improper, and model fitting and inferences suffered. See Gelman's Prior distributions for variance parameters in hierarchical models for a complete exposition. Edit: @csgillespie (rightly!) points out that I haven't completely answered your question. To my mind a non-informative prior is one that is vague in the sense that it doesn't particularly favor one area of the parameter space over another, but in doing so it shouldn't induce informative priors on other parameters. So a non-informative prior is vague but a vague prior isn't necessarily noninformative. One example where this comes into play is Bayesian variable selection; a "vague" prior on variable inclusion probabilities can actually induce a pretty informative prior on the total number of variables included in the model! It seems to me that the search for truly noninformative priors is quixotic (though many would disagree); better to use so-called "weakly" informative priors (which, I suppose, are generally vague in some sense). Really, how often do we know nothing about the parameter in question?
