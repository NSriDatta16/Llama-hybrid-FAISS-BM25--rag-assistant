[site]: crossvalidated
[post_id]: 455078
[parent_id]: 
[tags]: 
What is difference between attention and restricted-Boltzmann machine (RBM)?

Attention has attracted lots of interest recently. However, when I looked at the details of calculation, I recognize a lot of similarity between RBM and attention. Self-attention (key=value) is calculated as z = softmax{k^{T} Q}k which reminds me of RBM layer, where v = P(h,v)h v and h denote visible and hidden units, and P(u,v) = softmax{-h^T W v- a^T v -b^T h} . Capital letters (i.e, Q,W,P) are matrix and the rest are vectors (i.e., z,k,v,h,a,b). Now, I am wondering what is the crucial difference of two layers?
