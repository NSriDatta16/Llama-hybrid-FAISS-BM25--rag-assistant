[site]: crossvalidated
[post_id]: 311226
[parent_id]: 311005
[tags]: 
I'm not sure what is meant by "a more objective criterion" as multiple metrics are possible and the decision of which one(s) to use can be motivated by a number of considerations. In other words, there are more than a few ways to analyze your information. First of all, with strictly positive information truncated at zero, gaussian assumptions are clearly not relevant. Given that, the arithmetic mean is not appropriate. Next, that the pdf(s) of your data are skewed or fat-tailed also has important consequences. Putting these two observations together leads one to an initial assumption that the information may be lognormally distributed. Given that, estimating a lognormal mean would be more appropriate. This wiki article discusses approaches for that ( https://en.wikipedia.org/wiki/Log-normal_distribution ). It may also be the case that with time series data such as this, the lognormal geometric mean would be a good estimator (also discussed in the wiki link). However, lognormality should only be an initial assumption and subject to some confirmation. One useful visual heuristic supporting (disconfirming) lognormality is the extent to which the empirical data fits a 45 degree, straight line in a Q-Q plot. The key thing to look for is the fit (or lack of it) in the extreme tails. If there is evidence for "lack of fit," deeper analyses are possible to help ferret out just how skewed or fat-tailed your data is. For instance, estimating a tail exponent would provide empirical evidence for fitting distributions other than lognormal, e.g., fat-tailed, extreme valued, power law distributions. A simple approach to estimating a tail index is discussed in this paper by Xavier Gabaix, Rank 1/2: A Simply Way to Improve the OLS Estimation of Tail Exponents ( http://www.nber.org/papers/t0342.pdf ). Basically, by taking the log of the metric as well as the log of its rank order (an ordinal, integer value), and using log-log OLS regression of the one on the other, the absolute value of the resulting beta coefficient is a heuristic proxy for a tail exponent. The closer that exponent is to 1.0, the greater the likelihood that the data is lognormally distributed. For power law distributions more extreme than lognormal, the "Examples" table from this wiki article discussing Tweedie distributions ( https://en.wikipedia.org/wiki/Tweedie_distribution#Examples ) can be used to identify and fit a variety alternative distributions. However, if the tail exponent from this simple heuristic is much greater than 1.0, then consequences need to be considered. First, as the magnitude of the exponent diverges from 1.0, the less reliable simple log-log OLS estimation becomes. In other words, as your data becomes increasingly extreme valued, lognormal (log-log) assumptions become increasingly unreliable estimators of the real magnitude or nature of these extreme values. At some point, more rigorous, accurate maximum likelihood methods for tail exponent estimation such as Hill's or Pickands' methods, need to be employed. Next, as the information conforms more and more to extreme valued, power law assumptions, it may be the case that the moments (mean, std dev, etc.) are infinite and/or undefined. For instance, the Cauchy distribution is one example of a symmetric, extreme value, power law distribution with a single parameter -- the mean -- its std dev is undefined and/or infinite. Two things about the Cauchy, it's not well represented by an arithmetic mean and, next, since it is symmetric around that mean, as Gelman has suggested for use with data like yours, employing a "half-Cauchy" to capture its greater than zero pdf would be recommended. All of this is likely confusing to you, at least wrt estimating the mean of your strictly positive, skewed data. Hopefully, your data provides an adequate fit to lognormal assumptions, making your life much easier. If not and given that mean estimation is so fraught with complications, it would be much simpler and more straightforward to estimate the median(s) and employ nonparametric methods for between group comparisons. The Mann-Whitney U test is for use when comparing two groups while the Kruskal-Wallis test, a generalization of the Mann-Whitney, can be used with multiple groups. Unfortunately, neither test was designed for use with time series data and would, therefore, require some simplifying assumptions wrt the presence of the classic issues wrt time series modeling: nonstationarity, autocorrelation, cointegration, and so on, to make them appropriate tests. Other CV participants may have better recommendations and suggestions wrt dealing with all of the statistically focused, time series issues your data presents, particularly in the case where lognormal assumptions no longer apply. I would be interested in those suggestions. My recommendation would be to leverage information-theoretic approaches that are not based or rooted in the moments of any statistical distribution and are appropriate for use with time series data. One good example of such an approach is permutation distribution clustering (PDC). There are a number of articles and R modules out there which discuss this method, most authored by Andreas Brandmaier at the Max Planck Institute, e.g., https://cran.r-project.org/web/packages/pdc/pdc.pdf . Here is the abstract to this paper: Permutation Distribution Clustering (pdc) represents a complexity-based approach to clustering time series. Clustering comprises methods that recover similarities in a dataset and represent the findings in group structures. Important applications of clustering include the creation of taxonomies, the discovery of anomalies, or the the discovery of reliably different subgroups for differential analysis or treatment. A crucial parameter in clustering is the choice of the similarity measure between objects. Permutation Distribution Clustering finds similarity in time series based on differences in their permutation distribution as a proxy for differences in their complexity. The permutation distribution is obtained by counting the frequency of distinct order patterns in an m-embedding of the original time series. An embedding of dimension m allows for m! different order patterns. The choice of the embedding dimension crucially influences the clustering result. A small embedding dimension might lead to a permutation distribution with a low representational power, while a large embedding dimension quickly leads to a large permutation distribution that cannot reliably be estimated. With the Minimum Entropy Heuristic (MinE), the embedding dimension can automatically be chosen, thus making the algorithm a parameter-free clustering approach. For clustering timeseries, the similarity between two time-series is defined as the divergence between two permutation distributions. PDC is particularly apt for the analysis of psychophysiological time-series because it is efficient (the time complexity is linear in the time-series length), it is robust to drift, time-series of differing length can be compared, and it is invariant to differences in mean and variance of the time-series (choosing a normalization is not essential). PDC creates a matrix of comparisons based on the Kullback-Leibler divergence metric as a measure of similarity. If one so chooses, at that point, standard statistical tests of multiple group differences can be employed, e.g., Student-Newman-Keuls or some other multiple group, ANOVA-type test for contrasts between matrix elements.
