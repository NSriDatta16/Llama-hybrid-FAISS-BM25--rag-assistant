[site]: crossvalidated
[post_id]: 585618
[parent_id]: 
[tags]: 
How to explain the high accuracy and F1 score on the test set with a huge binary crossentropy loss?

I'll provide a little of introduction based on my example. I have a small collection of RGB (but 'gray-looking') brain MRI photos, divided into 2 classes: healthy and tumor. My data split looks like this: Training: healthy: 444 tumor: 444 Validation: healthy: 55 tumor: 55 Testing: healthy: 54 tumor: 54 Before I did the splitting shown above, I've made sure that the images aren't duplicated or too much similar (the original dataset was bigger, about 4k imgs in total, but I found that most of images were very similar. Like, the same pictures, but different quality; the same pictures but one lighter and the other darker). I left only unique images and images flipped or rotated (flipped and rotated images don't have duplications also, of course). I've used dedicated software for finding duplicates and similar images, so it was easy to do and I'm preatty sure that now, my small dataset has no duplicates. My aim was to build several made-from-scratch simple CNN models with tensorflow keras, compare results, take one "best budding" model (for sure overfitted) and further work on this one selected model by adding different optimizations, like batch normalization, dropouts, L1/L2 regularization etc. to improve it and achieve higher acc, f1 and lower loss. Due to the simplicity of this particular classification problem - (at least this is my explanation, because there are only 2 classes, and even unique pictures are all very similar to each other within this brain MRI domain) - even the simpliest CNN (Input > Conv2D[32, 3x3] > MaxPool[2x2] > Flatten > Dense[64] > Dense[32] > Sigmoid) gives over 80% F1 on test set and the model starts to overfit after 4th - 6th epoch. So now the main part of my question. Among the several models I have written, the most "complex" is the one with this architecture: Model: "sequential" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 222, 222, 8) 224 _________________________________________________________________ conv2d_1 (Conv2D) (None, 220, 220, 16) 1168 _________________________________________________________________ conv2d_2 (Conv2D) (None, 218, 218, 32) 4640 _________________________________________________________________ conv2d_3 (Conv2D) (None, 216, 216, 64) 18496 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 108, 108, 64) 0 _________________________________________________________________ conv2d_4 (Conv2D) (None, 106, 106, 128) 73856 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 53, 53, 128) 0 _________________________________________________________________ conv2d_5 (Conv2D) (None, 51, 51, 256) 295168 _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 25, 25, 256) 0 _________________________________________________________________ conv2d_6 (Conv2D) (None, 23, 23, 512) 1180160 _________________________________________________________________ max_pooling2d_3 (MaxPooling2 (None, 11, 11, 512) 0 _________________________________________________________________ flatten (Flatten) (None, 61952) 0 _________________________________________________________________ dense (Dense) (None, 64) 3964992 _________________________________________________________________ dense_1 (Dense) (None, 32) 2080 _________________________________________________________________ dense_2 (Dense) (None, 1) 33 ================================================================= Total params: 5,540,817 Trainable params: 5,540,817 Non-trainable params: 0 As stated in this source: Cross-Entropy = 0.00: Perfect probabilities. Cross-Entropy Cross-Entropy Cross-Entropy Cross-Entropy > 0.30: Not great. Cross-Entropy > 1.00: Terrible. Cross-Entropy > 2.00 Something is broken. From above model, I obtained a result that I cannot interpret. On testing, never seen during learning samples it gave me 87,38% F1 and a high loss equals to 1.5996 . (And an other, slightly simpler model with 76% F1 gave me a loss of... 2.72 , btw!). I attach a picture with train/val accuracy plot and train/val loss plot (binary crossentropy), and testing confusion matrix and results. I would be grateful for the explanation of how to interpret such a result if it does make sense. And what would you suggest to do to improve this model and outcomes.
