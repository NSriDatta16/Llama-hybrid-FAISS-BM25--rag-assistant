[site]: crossvalidated
[post_id]: 603309
[parent_id]: 
[tags]: 
RNN's Output Layer: How does it learn from its prev iterations if each Activation Vector is processed in parallel?

First of all, are the activation vectors processed in parallel? If so: That doesnt make sense since each previous activation vector feeds into the RNN as input. So if you're processing all activation vectors in parallel, its not learning from the previous activation vector? I'm thinking of a RNN similar to how boosting behaves, where the activation vector "is updated" after each iteration. Please help me understand how the RNN can process the activation vectors in parallel even though its learning from the previous vector (as input) after each iteration.
