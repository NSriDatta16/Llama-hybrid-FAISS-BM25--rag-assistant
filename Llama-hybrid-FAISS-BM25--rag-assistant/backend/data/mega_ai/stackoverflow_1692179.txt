[site]: stackoverflow
[post_id]: 1692179
[parent_id]: 1692160
[tags]: 
This highly depends on many factors: What file system are you using? How large is each file? What type of drives are you using? What are the access patterns? Accessing files purely at random is really expensive in traditional disks. One significant improvement you can get is to use solid state drive. If you can reason an access pattern, you might be able to leverage locality of reference to place these files. Another possible way is to use a database system, and store these files in the database to leverage the system's caching mechanism. Update: Given your update, is it possbile you consolidate some files? 1k files are not very efficient to store as file systems (fat32, ntfs) have cluster size and each file will use the cluster size anyway even if it is smaller than the cluster size. There is usually a limit on the number of files in each folder, with performance concerns. You can do a simple benchmark by putting as many as 10k files in a folder to see how much performance degrades. If you are set to use the trie structure, I would suggest survey the distribution of file names and then break them into different folders based on the distribution.
