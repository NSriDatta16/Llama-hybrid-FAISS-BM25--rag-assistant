[site]: crossvalidated
[post_id]: 454775
[parent_id]: 
[tags]: 
Moving estimators for nonstationary time series, like loglikelihood: l_T=sum_{t<T} a^{t-T} ln(rho(x_t))?

While in standard ("static") e.g. ML estimation we assume that all values are from a distribution of the same parameters, in practice we often have nonstationary time series: in which these parameters can evolve in time. It is usually considered by using sophisticated models like GARCH conditioning sigma with recent errors and sigmas, or Kalman filters - they assume some arbitrary hidden mechanism. I have recently worked on a simpler and more agnostic way: use moving estimator , like loglikelihood with exponentially weakening weights of recent values: $$\theta_T=\arg\max_\theta l_T\qquad \textrm{for}\qquad l_T= \sum_{t intended to estimate local parameters, separately on each position. We don't assume any hidden mechanism, only shift the estimator. For example it turns out that EPD ( exponential power distribution ) family $\rho(x) \propto \exp(-|x|^\kappa)$ , which covers Gaussian ( $\kappa=2$ ) and Laplace ( $\kappa=1$ ) distributions, can have cheaply made such moving estimator (plots below), getting much better loglikelihood for daily log-returns of Dow Jones companies (100 years DJIA, 10 years individual), even exceeding GARCH: https://arxiv.org/pdf/2003.02149 - just using the $(\sigma_{T+1})^\kappa=\eta (\sigma_{T})^\kappa+(1-\eta)|x-\mu|^\kappa$ formula: replacing estimator as average with moving estimator as exponential moving average : I have also MSE moving estimator for adaptive least squares linear regression: page 4 of https://arxiv.org/pdf/1906.03238 - can be used to get adaptive AR without Kalman filter , also analogous approach for adaptive estimation of joint distribution with polynomials: https://arxiv.org/pdf/1807.04119 Are such moving estimators considered in literature? What applications they might be useful for?
