[site]: crossvalidated
[post_id]: 317547
[parent_id]: 
[tags]: 
In Logistic Regression, how do we do cross validation when using gradient descent?

Unlike linear regression, we do not have close-form result for the weights when training a logistic regression model. So I learned that we can either apply Newton's method or (stochastic) gradient descent when minimizing the binary cross entropy loss together with l1/l2 regularization. E.g., if we use Lasso, the negative log likelihood becomes $$NLL(w) = - log P(Y|X,w) - \lambda \sum_{i = 1}^{K} w_i ^2$$ If we use gradient descent to find the global minimum, for a specific $\lambda$, we will get one curve for training loss and one curve for validation loss like the pic shown below (I plotted curves for 2 different $\lambda$) Then how do we select the best $\lambda$? Do we still do N-fold cross validation? I guess yes? Then do we compare loss at the two 'X' points where validation loss is minimized or do we compare loss at two 'O' points where training loss is minimized? From my view, I prefer the first scenario to compare the 2 crosses but I don't know what is actually implemented in scikit learn or other packages? (For Newton's method, it is very fast to converge to the minimum so we will not have the smooth curve plotted here.)
