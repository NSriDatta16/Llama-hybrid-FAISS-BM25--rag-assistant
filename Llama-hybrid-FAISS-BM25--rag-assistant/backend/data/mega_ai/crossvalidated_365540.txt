[site]: crossvalidated
[post_id]: 365540
[parent_id]: 
[tags]: 
Computing recall on classification task

I have done google searches on computing "recall @ X" several times in the last few months, and every page I read gives me the same answer: $\frac{TP}{TP + FN}$ ... but the story I get from my PI and from the papers I read hints at something further. My PI just gave me the following instructions for recall, with the caveat: "we could be wrong so please google this." Can anyone enlighten me? Is the following correct? My application is image classification (well, actually visual relationship classification). Given a relationship, you generate the top 50 (or whatever "x" is) predictions for each test image and then count how many ground-truth instances were found within those total predictions (50 * number of test images). You then divide that by the total number of ground truth instances of that relationship across the entire test set. The key points in my uncertainty are: Is my PI's instruction above in fact a correct implementation for this standard metric? Should I compute a recall for each of the $K$ classes into which I am classifying entities in the image, then average those recall values? Or should I treat all of the predictions the same, regardless of class (thus creating a bias in favour of classes with more examples in my dataset)? Do I need to use the top $X$ predictions per image ? Or do I need to use the top $X$ per class per image ? (The latter would assume there are a great many predictions, to be sure.)
