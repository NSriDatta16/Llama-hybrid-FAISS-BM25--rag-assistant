[site]: crossvalidated
[post_id]: 580182
[parent_id]: 580168
[tags]: 
The answer to questions like this is nearly always some combination of "It depends" and "Try it and see". To your point about face recognition: 16 pixels is plenty if I only need to discriminate between 2 people and one has a massive red beard. In general it's really hard to make assertions about arbitrary tasks and datasets. For example, here are some relevant questions: How big is the corpus? How big is the vocabulary? How valuable are the small documents? What insights do you want from the final distance matrix? The good news is that you can build your term frequency matrix no problem. The issues start when you try to use that matrix for things, like measuring the distances between documents (which are now vectors). Thanks to the 'curse of dimensionality' , distances in high-dimensional spaces are weird. In particular, points (i.e. documents) tend to be very far apart. Very short documents will naturally tend to cluster around the origin, and may seem very far from all other documents... except other short ones. There are ways to try to account for document length (e.g. in Gensim and you could potentially come up with some mitigation specific to your task. For example, you could normalize the document vectors so they are all the same length, or use cosine similarity to compare them. Depending on your dataset and your purpose, machine learning and statistics gives us lots of quantitative tools for figuring out empirically what the best approach is. I'm afraid it may not be possible to get to good answers without experimenting.
