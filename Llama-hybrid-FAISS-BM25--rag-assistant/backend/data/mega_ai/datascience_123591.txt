[site]: datascience
[post_id]: 123591
[parent_id]: 
[tags]: 
Appropriate input size for nn.Embedding

I’m quite new to using Pytorch and deep learning. What size of unique categories of a categorical variable is appropriate for applying the nn.Embedding ideally (best practices)? for example, if a feature has only two unique values, does that mean that it’s better to use one-hot encoding instead of teaching nn.Embedding (1) ? or I should just initialize nn.Embedding with one-hot matrix [[0, 1], [1,0]] (2) ? The question is which of three approaches ( (3) - learn nn.Embedding without my initialization) are preffered for small category space
