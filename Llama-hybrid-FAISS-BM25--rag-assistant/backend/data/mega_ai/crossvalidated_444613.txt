[site]: crossvalidated
[post_id]: 444613
[parent_id]: 444600
[tags]: 
There are alternatives, for example, you can use constrained optimization, or regularization. Notice however, that in most cases those approaches can be thought as Bayesian inference in disguise. For example, constraining range of the parameter during optimization, is the same as using flat prior over this range. Using $L_2$ regularization is the same as using Gaussian priors. Moreover, in Bayesian inference you don't need nomalization as well. For both MCMC and optimization, you can work with unnormalized densities. With Approximate Bayesian Computation you can even solve problems where likelihood is not specified as a probability distribution. Finally, one of the reasons for the popularity of Bayesian approach, is that you end up with a probability distribution for the estimates (posterior), that quantifies uncertainty about the estimates. This is not directly available in other approaches.
