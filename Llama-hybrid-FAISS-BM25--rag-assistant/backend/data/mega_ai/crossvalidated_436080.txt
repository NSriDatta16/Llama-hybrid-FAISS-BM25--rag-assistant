[site]: crossvalidated
[post_id]: 436080
[parent_id]: 436006
[tags]: 
Some theoretical considerations. The book "Probabilistic Learning" states that "Neural networks with one hidden layer are universally consistent if the parameters are well-chosen." They mean that, as training set size goes to infinity, the error rate converges to error of the Bayes classifier. The book "Understanding machine learning" gives an estimate of VC-dimension of the class of hypotheses of NN. Using Fundamental Theorem of Learning may give an idea, how big shall be a training set to get the accuracy you want. Usually, it is huge. Neither of these results mean that if you increase training set 10 times, the accuracy will be better. It only mean that if you increase your training set indefinitely, then, eventually, the results will get better. But then, they do not say how to select the "well - chosen" parameters. So, yes, increasing size of data 10 times may get worse results from theoretical point of view.
