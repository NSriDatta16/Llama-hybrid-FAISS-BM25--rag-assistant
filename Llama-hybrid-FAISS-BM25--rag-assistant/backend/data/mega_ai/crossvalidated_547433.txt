[site]: crossvalidated
[post_id]: 547433
[parent_id]: 
[tags]: 
How to avoid selection bias while updating lead scoring (predictive) model with new data

We developed a standard lead scoring model using logistic regression on couple of months worth data. The model has been working and we have been pushing only top 1/3 leads to sales team basis that. The model is giving around 40% lift. This model is already 2.5 months old and we are planning to update / retrain the model after adding new data along with conversion results. I am concerned that since we were only pushing qualified leads to sales team, we do not have the conversion result for low quality leads and hence they will have to be excluded from the model but this in turn would mean that the model will get trained on a dataset which is systematically different from ground reality - how to fix this?
