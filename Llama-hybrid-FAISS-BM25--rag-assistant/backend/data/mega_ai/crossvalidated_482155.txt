[site]: crossvalidated
[post_id]: 482155
[parent_id]: 
[tags]: 
What is the relationship between Curse of Dimensionality and isotropic neighborhoods?

I am currently reading The Elements of Statistical Learning by Hastie, Tibshirani and Friedman. At the end of the very insightful section 2.7 the authors say this. Any method that attempts to produce locally varying functions in small isotropic neighborhoods will run into problems in high dimensions—again the curse of dimensionality. And conversely, all methods that overcome the dimensionality problems have an associated—and often implicit or adaptive—metric for measuring neighborhoods, which basically does not allow the neighborhood to be simultaneously small in all directions. I am not at all clear what the connection between isotropic neighborhood and curse of dimensionality is. The authors have previously presented linear regression as a model that does not suffer from curse of dimensionality, but k-nearest neighbor suffers heavily from it. How does isotropic/non-isotropic neighborhood fit into that picture? What about non-linear models such as neural networks and random forest?
