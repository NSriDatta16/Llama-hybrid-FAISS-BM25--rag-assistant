[site]: crossvalidated
[post_id]: 337159
[parent_id]: 337034
[tags]: 
Bias is a relative term, meaning approximately How far on average is the estimated thing from the truth. Depending on what we are assuming the word "truth" means, we have different conceptions of bias. You are experiencing that two of those conceptions are relevant for linear regression, and they can come to opposite conclusions about the model. The least squares is said to give us unbiased estimates during linear regression When we say this, we are assuming that the truth has a specific structure $$ y \mid X \sim \beta_0 + \beta_1 X_1 + \cdots + \beta_k X_k + \epsilon $$ (where $\epsilon$ is a random noise term which does not depend on $X$) and we are using the model to try to uncover something about the numbers $\beta_0, \beta_1, \ldots, \beta_k$. The unbiasedness of linear regression in this scenario says that on average, when we use linear regression to estimate the $\beta$s, we get the correct answer. The assumption here is strong , we need to be willing to accept that, in truth, the conditional mean of $y$ is a linear function of $X$. If we weaken this assumption... but still we refer linear regression to have a high bias because of its assumption Here we are making much weaker assumption about what the truth looks like. We are just assuming that there is some function $f$ such that $$ y \mid X \sim f(X) + \epsilon $$ Since the only shapes our fit model can assume is a line, but it is possible in this case that $f$ is very not a line, it is impossible for our fit linear regression to assume, on average, the correct shape. In this setup, we may say that linear regression is biased (*). (*) Note though, in the case where $f$ really is a linear function, we would not say that linear regression is biased, even in the second setup.
