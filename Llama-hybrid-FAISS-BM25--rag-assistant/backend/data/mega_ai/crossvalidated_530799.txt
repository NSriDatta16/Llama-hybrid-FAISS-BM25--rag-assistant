[site]: crossvalidated
[post_id]: 530799
[parent_id]: 530693
[tags]: 
Random forests shouldn't be immune to this kind of bias. An overrepresented data segment will be overrepresented in the splitting criterion, and so the trees will tend to favor splits that perform well for that segment at the expense of other segments. That's not to say the result will be poor, but there will be a bias. In the particular case of the classes, the final leaf scores will (on average) be biased in exactly the same way your data is. If you can quantify the extent to which data segments are under/over-represented, then you can add weights to the random forest to counteract that effect (see wikipedia ). Similarly, if you can quantify the class balance change that arises from sampling, you can apply class weights to get the leaf scores back to the right proportions. You can also apply a post-model adjustment for the class balance issues on the final scores, see e.g. Convert predicted probabilities after downsampling to actual probabilities in classification , but I don't think there's an analogue for data segments.
