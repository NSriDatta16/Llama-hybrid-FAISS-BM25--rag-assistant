[site]: crossvalidated
[post_id]: 604712
[parent_id]: 604577
[tags]: 
I interpreted visually well-defined segments as "separable in some (natural) space parametrization" . I assume that for you, this is not a case of visually well-defined segments : Further, your first image seems to suggest a GMM-type geometry, which is a natural choice. Since you already know the categorical part of your data $D$ (acting here as class assignment $k_i\in\{1...K\}$ ), you also have the (MLE) Gaussian Mixture Model fit $\hat{M}_D$ (no EM algorithm needed). Now you could compute a goodness of fit $T_{\hat{M}_D}$ of your model $\hat{M}_D$ by summing all pairwise Kullback-Leibler divergences $\text{KL}(\mathcal{N(\mu_i,\Sigma_i)}\,||\,\mathcal{N}(\mu_j,\Sigma_j)),\,\,(1\leq i, j \leq K)$ of Gaussians (which is never $\infty$ due to infinite support). Two datasets $D_1$ , $D_2$ with the same number of "classes" $K$ should be comparable by $T_{\hat{M}_{D_1}}$ and $T_{\hat{M}_{D_2}}$ , where higher values of this "test statistic" (I have reasons not to call it that) indicate better visual separation. EDIT: Normalization w.r.t. $K$ could be achieved by taking the average or maximum over the KL divergences (instead of summing). If you have a lot of datasets, you could also compare these aggregations against your (empirical) distribution of $T_{\hat{M}_{D_i}}|D_i$ to arrive at an absolute threshold/measure, similar to a p-value.
