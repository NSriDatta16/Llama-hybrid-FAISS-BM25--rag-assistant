[site]: crossvalidated
[post_id]: 212219
[parent_id]: 
[tags]: 
Decreasing the number of negative examples with respect to the positive examples produces good prediction with SVM

Inroduction We have a binary classification problem and we want to learn a linear SVM (support vector machine). The dataset is composed of: 502 positive examples 5020 negative examples An example is a tfidf vector of real numbers of dimension 169647 computed on wikipedia texts. The dataset can be downloaded from E0518966_100 . In order to test our model we use the Liblinear library. Problem For a given query represented by the following vector . If we train the classifier by using 502 positive examples and 5020 negative examples. The classification will produce the following probabilities: labels 1 0 0 0.236806 0.763194 Meaning that we didn't classify the query correctly. On the other hand when we use 502 positive examples and we randomly select 50 negative examples. The classification produces: labels 1 0 1 0.758789 0.241211 Meaning that the classifier gave us the right prediction. Question I can't explain why when we use less negative examples than positive examples we manage to get a good prediction. Implementation Step 1 : Split file into positive and negative datasets cat E0518966_100.txt | awk 'NR >= 1 && NR positive_2.txt cat E0518966_100.txt | awk 'NR >= 503 && NR negative_2.txt Step2: Randomly select 50 negative examples and combine files sort -R negative_2.txt | head -n 50 > sub_negative_2.txt cat positive_2.txt sub_negative_2.txt > train_2.t Step3: Classification #Select random subset of negative examples ./train -s 0 train_2.t train_model_1 ./predict -b 1 query_test2.txt train_model_1 prob_1.txt #NO random subset selection of negative examples ./train -s 0 E0518966_100.txt train_model_1 ./predict -b 1 query_test2.txt train_model_1 prob_1.txt
