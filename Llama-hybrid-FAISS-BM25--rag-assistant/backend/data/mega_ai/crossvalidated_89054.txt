[site]: crossvalidated
[post_id]: 89054
[parent_id]: 89036
[tags]: 
Bagging technique uses bootstraps (random samples of the same length with replacement) to train each tree from the assembly. Thus, the samples used to build each individual tree comes from the same population as the original sample. This is why the input and target variables are called ID (identically distributed = same distribution). More than that, because the samples are drawn randomly, the samples are also independent (knowing elements of a sample does not give hints on the elements of another sample). This is usually denoted as IID (independent and identically distributed). The expectation of the mean is preserved because input and target variables are IID (samples are independent and are drawn from the same population). [see Law of Large numbers] Because trees are basically a piece-wise constant approximation, what those trees can learn are constant averages on various regions. The trees only define input space regions (the leaf nodes), but on those regions approximates with an average. Those constants are averages of some sort (mean, median) depending on the loss function. So what you can say about averages from the input and target variables, you can say about trees themselves (that they preserve the expectation of the average). The bagging is used to reduce variance by averaging the models, while they preserve as much as possible the expectation of those variables. I hope I was clear somehow, I will retry later when I will have the chance, to improve this, eventually.
