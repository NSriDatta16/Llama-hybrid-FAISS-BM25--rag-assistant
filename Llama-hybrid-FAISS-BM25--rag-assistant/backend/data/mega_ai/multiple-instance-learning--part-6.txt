 also been adapted to a multiple-instance context under the standard assumption, including Support vector machines Artificial neural networks Decision trees Boosting Post 2000, there was a movement away from the standard assumption and the development of algorithms designed to tackle the more general assumptions listed above. Weidmann proposes a Two-Level Classification (TLC) algorithm to learn concepts under the count-based assumption. The first step tries to learn instance-level concepts by building a decision tree from each instance in each bag of the training set. Each bag is then mapped to a feature vector based on the counts in the decision tree. In the second step, a single-instance algorithm is run on the feature vectors to learn the concept Scott et al. proposed an algorithm, GMIL-1, to learn concepts under the GMIL assumption in 2005. GMIL-1 enumerates all axis-parallel rectangles { R i } i ∈ I {\displaystyle \{R_{i}\}_{i\in I}} in the original space of instances, and defines a new feature space of Boolean vectors. A bag B {\displaystyle B} is mapped to a vector b = ( b i ) i ∈ I {\displaystyle \mathbf {b} =(b_{i})_{i\in I}} in this new feature space, where b i = 1 {\displaystyle b_{i}=1} if APR R i {\displaystyle R_{i}} covers B {\displaystyle B} , and b i = 0 {\displaystyle b_{i}=0} otherwise. A single-instance algorithm can then be applied to learn the concept in this new feature space. Because of the high dimensionality of the new feature space and the cost of explicitly enumerating all APRs of the original instance space, GMIL-1 is inefficient both in terms of computation and memory. GMIL-2 was developed as a refinement of GMIL-1 in an effort to improve efficiency. GMIL-2 pre-processes the instances to find a set of candidate representative instances. GMIL-2 then maps each bag to a Boolean vector, as in GMIL-1, but only considers APRs corresponding to unique subsets of the candidate representative instances. This significantly reduces the memory and computational requirements. Xu (2003) proposed several algorithms based on logistic regression and boosting methods to learn concepts under the collective assumption. Metadata-based (or embedding-based) algorithms By mapping each bag to a feature vector of metadata, metadata-based algorithms allow the flexibility of using an arbitrary single-instance algorithm to perform the actual classification task. Future bags are simply mapped (embedded) into the feature space of metadata and labeled by the chosen classifier. Therefore, much of the focus for metadata-based algorithms is on what features or what type of embedding leads to effective classification. Note that some of the previously mentioned algorithms, such as TLC and GMIL could be considered metadata-based. One approach is to let the metadata for each bag be some set of statistics over the instances in the bag. The SimpleMI algorithm takes this approach, where the metadata of a bag is taken to be a simple summary statistic, such as the average or minimum and maximum of each instance variable taken over all instances in the bag. There are other algorithms which use more complex statistics, but SimpleMI was shown to be surprisingly competitive for a number of datasets, despite its apparent lack of complexity. Another common approach is to consider the geometry of the bags themselves as metadata. This is the approach taken by the MIGraph and miGraph algorithms, which represent each bag as a graph whose nodes are the instances in the bag. There is an edge between two nodes if the distance (up to some metric on the instance space) between the corresponding instances is less than some threshold. Classification is done via an SVM with a graph kernel (MIGraph and miGraph only differ in their choice of kernel). Similar approaches are taken by MILES and MInD. MILES represents a bag by its similarities to instances in the training set, while MInD represents a bag by its distances to other bags. A modification of k-nearest n