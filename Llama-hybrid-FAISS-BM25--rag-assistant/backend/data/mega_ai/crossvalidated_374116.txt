[site]: crossvalidated
[post_id]: 374116
[parent_id]: 374090
[tags]: 
The proof that this is the correct gradient to use in order to improve utility from following a policy function is called the Policy Gradient Theorem . In brief, in that derivation, the $\text{log}$ function appears due to a division by the probability of taking the action (i.e. the same policy function). This occurs when writing out the gradient of the utility of the policy as an expectation, which you wish to do, so you can take samples based on that expectation, and use them to estimate the gradient. Rather than write $\frac{\nabla x}{x}$ you can write $\nabla \text{log}(x)$ . In addition, as the gradient is scaled by $z_t$ (the value of the outcome), then gradients will be larger and positive when the probability of taking an action was low, but the outcome was good - increasing the probability here in the update step will be good for future outcomes. Whilst if $z_t$ is negative (the game was eventually lost), probabilities of the chosen actions will be reduced. Often you also want to offset $z_t$ factor by some baseline measurement (such as average reward, or current estimated value of a state, or use the "advantage" function instead of state value) to stabilise a policy gradient algorithm - else it can become a race between relative values with the preferences for all actions becoming higher and higher, and with better ones increasing faster (until non-favourable outcomes are so rare they stop happening anyway). With the simple reward structure and zero-sum maths of a two-player game, this seems not to be necessary for AlphaGo.
