[site]: crossvalidated
[post_id]: 526026
[parent_id]: 69831
[tags]: 
If you can't assume independence of the data splits (which in many scenarios you can't), here's a method that allows for the computation of "valid" confidence intervals around your error. It was recently published by Stanford (2021) so there still aren't python packages, but they did create an R package. I was interested in the topic so I made a less technical writeup , but the paper tells the full story. Paper Info (in case the link dies): Name: Cross-validation: what does it estimate and how well does it do it? Authors: Stephen Bates, Trevor Hastie, and Robert Tibshirani Year: 2021 Key conclusions: "We have made two main contributions. First, we discussed point estimates of prediction error via subsampling techniques. Our primary result is that common estimates of prediction error—cross-validation, bootstrap, data splitting, and covariance penalties—cannot be viewed as estimates of the prediction error of the final model fit on the whole data. ... Secondly, we discuss inference for cross-validation, deriving an estimator for the MSE of the CV point estimate, nested CV."
