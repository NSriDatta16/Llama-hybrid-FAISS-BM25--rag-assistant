[site]: crossvalidated
[post_id]: 593805
[parent_id]: 
[tags]: 
Confused about posterior collapse in variational autoencoders

I've been training a $\beta$ -VAE with a 5-dimensional latent space on some physics simulation data with 2000 samples. As I increase $\beta$ , I notice that an increasing number of the latent variables will be given by $\mu_{d} \approx$ 0 and $\sigma^{2}_{d} \approx 1$ for all of the training samples, where $d$ is the latent variable index. On the other hand, the other latent variables will produce different values of $\mu_{d}$ and about constant $\sigma^{2}_{d} \approx 0$ over all of the training samples. I see that this is commonly referred to as posterior collapse, where the posterior of some latent variables matches that of the isotropic Gaussian prior. This makes sense to me. What I'm confused about is that if I calculate only $\mu_{d}$ for all of my training samples for each latent variable, the collapsed latent variables will consistently be $\mu_{d} = 0$ . Over all of my training data, the variance of these values is then also 0. For the non-collapsed latent variables, I see that while $\mu_{d}$ is different for each training sample, the mean of these different values is actually 0 and the variance of these values is close to 1 and the distribution of $\mu_{d}$ is Gaussian, unlike the collapsed latent variables which seems to be just noise very close to $\mu_{d} = 0$ . My question is, why is the distribution of $\mu_{d}$ Gaussian for non-collapsed latent variables but not for the collapsed latent variables? This seems completely contrary to the collapsed latent variables matching the Gaussian prior. And why is $\sigma^{2}_{d} \approx 0$ for all of the training samples of non-collapsed latent variables? Shouldn't the KL divergence make this value closer to 1?
