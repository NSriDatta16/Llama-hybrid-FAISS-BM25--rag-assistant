[site]: crossvalidated
[post_id]: 486071
[parent_id]: 485797
[tags]: 
Your question runs to the heart of the difference between education and training. Instead of statistics, consider pharmacy and medicine. A pharmacist has to have extensive coursework in chemistry and biology, yet their primary function or their nearly exclusive function is to count manufactured pills. Very few pharmacists compound drugs anymore. And, while their advising role couldnâ€™t be substituted for by someone else, most of their advice is repetitive. Likewise, for a general practitioner, in terms of frequency of behaviors, their most-used skills are taking blood pressure, looking in your mouth, and taking your pulse. That is hardly a good use for the calculus, chemistry, biology, and higher-end medical training that they received. Indeed, the reason that physician assistants and pharmacy assistants exist in U.S. medicine is that most things of importance can be trained into a person and do not need higher-end reasoning. The Central Limit Theorem is that sort of thing. If you completely skipped it, you could still do a t-test, estimate a Bayesian posterior density, find the sample median, or perform the Kolmogorov-Smirnov test. For 95% of the applications out there, you would be skilled enough, and you would be competent enough to provide advice to others as well. The difficulty would happen when you believed you knew what to do, but you were wrong. For example, there are distributions where the assumptions of the Central Limit Theorem are strongly violated, and the sample mean is without meaning at all. In some areas of knowledge, that is a common problem. In other areas, it is never a problem. The Central Limit Theorem, at its most basic application, lets you know that sampling distributions exist as a concept. At the advanced level, it will keep your work from imploding. EDIT For The Comments Consider prices set in a double auction, $p_1$ and $p_2$ with quantities $q_1$ and $q_2$ . Return is defined as $$r_1=\frac{p_2}{p_1}\times\frac{q_2}{q_1}-1.$$ Let us define $R=r+1$ . For brevity, let us ignore dividends and when $q_2=0$ due to bankruptcy and when $q_2^j=kq_1^j$ and for mergers, or this will go on for about forty pages. In a double auction there is no winner's curse, so the rational action of each actor is to bid their expectation as to its value. Again, for brevity as this is not required if we can go on for forty pages, let us assume there are very many actors. The limit book, which in later operations will be scaled by the variance, should be normally distributed around an equilibrium price $p^*$ . Ignoring stock splits and stock dividends, $q_1=q_2$ , so $$R=\frac{p_2}{p_1}.$$ Now, noting that $R$ is a slope, we can find the ratio distribution of the slopes. Unfortunately, if you do that in Cartesian coordinates around $(0,0)$ you end up with a messy mixture distribution of a Cauchy distribution and a distribution with finite variance. It isn't useful, at least in economics, because it requires data that could not be reached because the necessary extra data was never recorded. However, if you integrate around the equilibrium prices $(p^*_1,p^*_2)$ and formally account for the cost of liquidity and the effect of bankruptcy, then you end up with a distribution that looks like real world data. Note that $\Re^2$ is not an ordered set, so the idea of $(0,0)$ is a bit arbitrary. You would then transform the distribution by adding back in the equilibrium return of $\frac{p_2^*}{p_1^*}.$ It is easier if you think about this as a vector in polar coordinates. The distribution of the slopes of the vector of bivariate shocks $(\epsilon_1,\epsilon_2)$ has no mean or variance. The shocks, individually, are normally distributed. As a visual example, consider the distribution of daily returns for Carnival Cruise Lines below. The process gets complex when you consider annual returns instead because equity returns are not scale-invariant. You can see multi-week long shifts in the location of the supply and demand curves and those long shifts can be observed in annual returns sometimes as multiple peaks or splits in the scale parameter. The red line is the fitted line. Because the distribution lacks a first moment, standard tools such as least-squares will produce spurious results. That is the source of the failure of models like the Capital Asset Pricing Model or Ito models such as Black-Scholes, or time-series tools like GARCH to fail in validation over the population of data. In fact, when Fama and MacBeth decisively falsified models like the CAPM in 1973, one would have thought they would have gone away. Indeed, the third to the last paragraph in Black and Scholes seminal paper on options pricing states they tested their model and it failed to pass validation. Likewise, the paper introducing GARCH as a concept tested the tool on equity returns and found the assumptions so strongly violated that they stated it shouldn't be used for equities. However, what every economist learns is that $\hat{\beta}=(X'X)^{-1}(X'Y)$ and it or a cousin, such as FGLS, fills the literature. The Central Limit Theorem doesn't apply to a range of real data types, other than equity securities. If you do not know that, your field can produce 3800 papers on one small anomaly in options pricing as finance has. Just a final note on the picture above, it is possible to improve the fit. The solution I used was a bit crude but vastly superior to assuming normality. Hundreds of thousands of hours have been spent in research in finance, financial economics, and macroeconomics by ignoring the fact that returns are not data. Prices are data. Volumes are data. Returns are a statistic and a function of prices, volumes, and dividends. It is no more proper to assume a statistic's distribution than it would be proper to assume the sampling distribution of the difference of two means is the $\chi^2$ distribution because you didn't check to see if that was correct. You can find examples of this type of phenomenon in physics, hydrology, biology and medicine. The Central Limit Theorem not only says what happens when it works, but it also sets the conditions of when it does not work. It is both a blessing and a warning. You are correct, there are practical limitations on the CLT, but technicians never know that. Personally, I have yet to be given an infinitely large data set. My guess is that my laptop is happier with that state of affairs anyway.
