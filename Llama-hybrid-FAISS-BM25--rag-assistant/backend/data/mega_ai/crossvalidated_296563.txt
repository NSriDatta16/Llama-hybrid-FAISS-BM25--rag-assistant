[site]: crossvalidated
[post_id]: 296563
[parent_id]: 
[tags]: 
What is the labels for SVM classification when we firstly run LDA (lda->SVM)

I am using LDA (Latent Dirichlet Allocation) to extract topics. I want to do topic modelling and use the topics as features to do document classification. the reason for doing classification is to evaluate my LDA model. the same as this link lda , my question is that when I generated my topics, what should be the labels for the classification method? I was supposed it has to be the topics generated by the LDA, would you please provide me with your idea, suppose we have six topics generated: topi0: computer, internet, net, technology topic1: politicians, political, barack, topic2: music, hobby,... topic3:... topic4: topic5: ... topic6: ... so what is the labels for the classification part? sv=SVC() labels = [???] sv.fit(lda_x,labels) predictclass = sv.predict(lda_x) testLables=[???] from sklearn import metrics yacc=metrics.accuracy_score(testLables,predictclass) Update my dataset is 20news group, my aim is to use LDA to extract for example 6 topic(also each topic has its own distribution of terms) then use one classification method to evaluate that. Update 2 code part this is the output for the matric to send to classification method. I now think there is something wrong with that, [[ 9.29756948e-07 9.99991632e-01 9.29752853e-07 9.29718960e-07 9.29696185e-07 9.29741802e-07 9.29735986e-07 9.29749709e-07 9.29697295e-07 9.29716150e-07]] from __future__ import print_function import nltk import numpy as np from time import time import gensim from gensim import corpora from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer from sklearn.decomposition import NMF, LatentDirichletAllocation from sklearn.datasets import fetch_20newsgroups from nltk.tokenize import RegexpTokenizer, sent_tokenize, word_tokenize from nltk.stem.porter import PorterStemmer from sklearn.naive_bayes import MultinomialNB from sklearn.preprocessing import LabelEncoder n_samples = 2000 n_features = 1000 n_topics = 10 n_top_words = 20 def print_top_words(model, feature_names, n_top_words): for topic_idx, topic in enumerate(model.components_): print("Topic #%d:" % topic_idx) print(" ".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])) print() # Load the 20 newsgroups dataset and vectorize it. We use a few heuristics # to filter out useless terms early on: the posts are stripped of headers, # footers and quoted replies, and common English words, words occurring in # only one document or in at least 95% of the documents are removed. texts=[] print("Loading dataset...") t0 = time() dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes')) data_samples = dataset.data[:n_samples] print("done in %0.3fs." % (time() - t0)) # Use tf-idf features for NMF. print("Extracting tf-idf features for NMF...") tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=n_features, stop_words='english') t0 = time() tfidf = tfidf_vectorizer.fit_transform(data_samples) p_stemmer = PorterStemmer() str=''.join(data_samples) sent = sent_tokenize(str) str2=''.join(sent) words = word_tokenize(str2) filtered_sentence = [w for w in words] stemmed_tokens = [p_stemmer.stem(i) for i in filtered_sentence] # add tokens to list texts.append(stemmed_tokens) values = ' '.join(' '.join(elems) for elems in texts).lower() print (values) print("done in %0.3fs." % (time() - t0)) # Use tf (raw term count) features for LDA. print("Extracting tf features for LDA...") tf_vectorizer = CountVectorizer(max_df=3, min_df=0.02, max_features=n_features, stop_words='english') t0 = time() y=[values] tf = tf_vectorizer.fit_transform(y) print("done in %0.3fs." % (time() - t0)) # Fit the NMF model print("Fitting the NMF model with tf-idf features, " "n_samples=%d and n_features=%d..." % (n_samples, n_features)) t0 = time() nmf = NMF(n_components=n_topics, random_state=1, alpha=.1, l1_ratio=.5).fit(tfidf) print("done in %0.3fs." % (time() - t0)) print("\nTopics in NMF model:") tfidf_feature_names = tfidf_vectorizer.get_feature_names() print_top_words(nmf, tfidf_feature_names, n_top_words) print("Fitting LDA models with tf features, " "n_samples=%d and n_features=%d..." % (n_samples, n_features)) lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5, learning_method='online', learning_offset=50., random_state=0) t0 = time() lda_x=lda.fit_transform(tf) from sklearn.svm import SVC for i in lda_x: print (i) print("done in %0.3fs." % (time() - t0)) print("\nTopics in LDA model:") sv=MultinomialNB() labels = [0] sv.fit(lda_x,labels) predictclass = sv.predict(lda_x) testLables=[0] from sklearn import metrics yacc=metrics.accuracy_score(testLables,predictclass) print (yacc) n=dataset.target[:20] print (n)
