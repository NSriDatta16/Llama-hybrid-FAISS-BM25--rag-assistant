[site]: stackoverflow
[post_id]: 1381796
[parent_id]: 1381607
[tags]: 
Two comments: 1) To generate samples from a random process, whether or not a certain choice is quite (>50%) likely, and others less so, just requires a weighted "coin flip": generate a random real number uniformly on [0,1), and consider the possibilities in the same fixed order, keeping a sum of probabilities so far. As soon as that sum exceeds your randomly chosen number, select that choice. If your choices have unnormalized (not summing to 1) probabilities, you first need to compute the sum of probabilities s, and either divide them all by s, or choose your random number on [0,s) 2) To prevent overfitting when estimating your model from a small amount of sample training data (compared to the number of parameters), use Bayesian priors on the model parameters. For a really cool example of this, where the number of model parameters (history size) isn't fixed to any finite number in advance, see the Infinite HMM . If you don't use Bayesian methods, then you'll want to choose the history length appropriately for the amount of training data you have, and/or implement some ad-hoc smoothing (e.g. linear interpolation between an order-2 and order-1 model).
