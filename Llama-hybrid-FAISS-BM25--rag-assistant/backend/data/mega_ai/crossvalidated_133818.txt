[site]: crossvalidated
[post_id]: 133818
[parent_id]: 133804
[tags]: 
Not really; logistic regression is a linear model, in which the outputs are a monotonic function of $\sum_i w_i x_i$ (where $x_i$ is the $i$th feature and $w_i$ the corresponding weight). How you deal with these effects depends on the problem you're trying to solve and how much effort you want to put into it: In text classification, one way to resolve the different meaning of "base" when seen with "acid" than with "structure" is word-sense disambiguation . You could then treat the two senses of the word as completely separate entities. n-grams don't really solve this problem, though they can sort of help with it a bit. For the hot-in-January situation, you might represent the variable as difference from a seasonal mean, or something like that. You could try to account for any such relationship using nonlinear methods. Kernel methods, deep neural networks, Bayesian modeling, or density estimation-based approaches might all be reasonable, depending on what exactly you're trying to do.
