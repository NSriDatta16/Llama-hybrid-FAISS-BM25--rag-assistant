[site]: crossvalidated
[post_id]: 152742
[parent_id]: 
[tags]: 
Testing a power-law hypothesis from the averaged distribution

A way to test the existence of power-law in the distribution is given in the following paper: http://arxiv.org/abs/0706.1062 The gist of the whole procedure is as follows: Let $\textbf{X}$ be the set or vector of observations of discrete or continuous variable and we want to check whether these elements have come from the power-law distribution $k^{-\alpha}$. The most basic (and worst) procedure would be to find (with an appropriate binning) the distribution of numbers from $\textbf{X}$ and plot it on log-log scale. If the points in the plot seem to be present approximately on straight line, then the distribution is power-law. Then one can visually fit the straight line to the approximately straight part of the plot and find out index $\alpha$. A better way is to use either a cumulative distribution function or logarithmic binning and again fit a straight line visually. Finally, one can use one of the above and then use say 'least square fitting' to find the exponent $\alpha$. However, all these methods assume that the underlying distribution is indeed a power-law. This may not be always true. A better way that is described in the above mentioned paper is like this: Using a method of maximum likelihood, one first finds a lowest point $k_{min}$ above which power-law holds and the corresponding index $\alpha$. Then we generate a large number of synthetic power-law distributed samples using this value of $\alpha$ and compare their fluctuations with the fluctuation of the actual data. If the fraction of synthetic data sets with more fluctuation than original data is large ($>0.1$) then usually one considers this as an indicative of a power-law. This whole process, however, uses an actual vector of observations $\textbf{X}$ and not the distribution generated using values of $\textbf{X}$. Now consider a following situation: I have a simulation of a random process and I would like to check the existence of power-law in that process. However, if this process indeed could be simulated, I would simulate it not once but say $1000$ times and average all the resulting distributions to see the actual shape of the underlying distribution. So far, so nice. But this invites a devil now. While averaging the distributions, we have simply thrown away individual vectors $\textbf{X}$ and they can't be averaged since every permutation of a vector $\textbf{X}$ contains the same information. Hence the method described above can't be used since it uses $\textbf{X}$ to establish a power-law. One way to solve the problem would be the following: After we find the average distribution $p(k)$, construct a dummy vector $\textbf{X}_d$ that contains values $k$ and the number of values would be proportional to $p(k)$. Generally we would choose closest integer to $nP(k)$ where $n$ is the sample size. One obvious problem with this approach is that when values of $p(k)$ are very small, it is not very clear how many values of $k$ should be included in $\textbf{X}_d$ ($0$ or $1$). Both choices seem to give bias over the actual underlying distribution. How can one solve this problem without great computational efforts?
