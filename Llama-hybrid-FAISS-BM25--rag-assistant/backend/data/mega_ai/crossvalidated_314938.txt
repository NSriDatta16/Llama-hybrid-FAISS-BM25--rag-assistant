[site]: crossvalidated
[post_id]: 314938
[parent_id]: 314915
[tags]: 
Regardless of how these devices behave, an additive model of variability provides useful insight. Such a model supposes that the response of an instrument is the sum of three independent quantities (none of which we necessarily know): The true value it is trying to measure, $\mu$. A random measurement error $X$ with mean $0$ and variance $\sigma^2$. $\sigma$ measures the imprecision of the instrument. A fixed error $Y$ which, because we do not know it, we also model as a random variable with mean $0$ and variance $\tau^2$. One way to view this is to suppose there is a large bin of instruments you could have used and the one(s) you are using have been pulled randomly from that bin. Overall these instruments are accurate (that's the mean $0$ assumption) but they do vary systematically from one to another (that's what $\tau$ measures). Although this model is rarely exactly right, it typically holds to a sufficiently good approximation that we can use it to find near-optimal combinations of measurements. This is part of the theory of experimental design. Suppose--this requires an assumption that's often not quite true, but is useful to get started--the results of the two instruments are independent and that the results of repeated measurements by one instrument are independent. Consider two possibilities: Repeated measurements by one instrument. Assumptions 1-3 enable us to view each measurement as a sum $$Z_i = \mu + X_i + Y$$ where $i$ is an index denoting the measurement and ranges from $1$ through $n$. Notice that $Y$ has no subscript because it is a property of the instrument itself : it doesn't change from one measurement to the other. We may compute the variance of the average of the measurements--conceived of as an average of these random variables $Z_i$--as $$\operatorname{Var}(\bar Z) = \frac{1}{n}\sigma^2 + \tau^2.$$ As $n$ gets larger, $\sigma^2/n$ grows smaller. Moreover, if we take expectations in the sense of what an arbitrarily large number of measurements would produce on average, $$E[\bar Z] = \mu + Y$$ shows that even the average is biased (unless you were lucky enough to draw an instrument with $Y\approx 0$--but you can't know that). The moral of this calculation is that averaging measurements from one instrument reduces the imprecision but has no effect on the accuracy. Independent measurements by multiple instruments. Now $i$ indexes both the measurement and the instrument . Accordingly, $$Z_i = \mu + X_i + Y_i.$$ Now $$\operatorname{Var}(\bar Z) = \frac{1}{n}\sigma^2 + \frac{1}{n}\tau^2$$ and (in the same sense as before, taking an arbitrarily large number of instruments ), $$E[\bar Z] = \mu.$$ As $n$ gets larger, both $\sigma^2/n$ and $\tau^2/n$ grow smaller. Regardless, the expected value of the measurement is correct: $\bar Z$ is more likely to be accurate in this case. Thus, averaging measurements from multiple instruments reduces the imprecision and improves the accuracy. The decision seems clear: when you have the choice, use multiple instruments. Making repeated measurements from the same instrument is no substitute. When you have even more time and budget to design your experiment, you may combine both approaches: use multiple instruments and repeat the measurements made with each instrument. Such data can be analysed with an Analysis of Variance (ANOVA) to estimate $\sigma^2$ and $\tau^2$, the components of variance. You can use this information, along with what you know about the costs of making measurements and buying more instruments, to estimate the best balance between the numbers of instruments and numbers of repeated measurements made by each. The calculations aren't really any more difficult than illustrated here.
