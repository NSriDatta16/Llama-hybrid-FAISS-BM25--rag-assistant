[site]: crossvalidated
[post_id]: 230294
[parent_id]: 
[tags]: 
Regression Methodology help

Problem Statement: We currently field a psychological survey with what we believe to be too many questions. 1) We would like to remove the questions that have the least predictive power on brand purchases. 2) We would also like to introduce new questions that are at the cutting edge of pyschographic research. To accomplish these two goals, we have been advised to run multiple regressions on all of our current questions against all of the brands we measure. We take the average $R^2$ and $Adjusted-R^2$ to determine the worst performing questions for removal. And for adding new questions, we run regressions of brand purchases against the new questions fielded in a new survey. The basic idea in matrix form: We measure about 1,000 different brands: $B_{m~\rm{x}~1}=$$\begin{bmatrix} B_1=Brand_1 \\ B_2=Brand_2\\.\\.\\B_m=Brand_m\end{bmatrix}$ And the sets of predictors. There are about 20 questions in each group. He asked that we combine sets of predictors also. $Z_{5~\rm{x}~1}=$$\begin{bmatrix} Z_1=Old~Questions_{n\approx20} \\ Z_2=New~Questions_{n\approx20} \\Z_3=Demographics_{n\approx20}\\Z_4=Old~and~New~Questions_{n\approx40}\\Z_5=All~Predictors~Combined_{n\approx60}\end{bmatrix}$ Therefore, we run each brand against the set of old questions, then each brand against the set of new questions, next the brands against the demos, and so on. $All~Regressions_{m~\rm{x}~5}=$$\begin{bmatrix} B_1=Z_1\beta+\epsilon & B_1=Z_2\beta+\epsilon & ...&B_1=Z_5\beta+\epsilon \\ B_2=Z_1\beta+\epsilon & B_2=Z_2\beta+\epsilon & ...&B_2=Z_5\beta+\epsilon \\ . & .&...&.\\B_m=Z_1\beta+\epsilon & B_m=Z_2\beta+\epsilon & ...&B_m=Z_5\beta+\epsilon \end{bmatrix}$ We will run about $m~X~5\approx5000$ regressions, taking the average $R^2$ and removing the lowest performers. But this approach seems to ignore the marginal effects of one question on another. Especially when looking at $Z_5$, it is a mixture of old, new, and demographic questions. Also, the $R^2$ looks at the overall accuracy of the set, not how one particular predictor performed. For this, each $\beta$ value should be judged. But does this become problematic since it is looking at a particular question holding all other questions constant? What do you think of this approach? How does it compare to other techniques like choice modeling?
