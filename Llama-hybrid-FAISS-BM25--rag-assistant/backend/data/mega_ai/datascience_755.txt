[site]: datascience
[post_id]: 755
[parent_id]: 750
[tags]: 
I expected more from random trees : With random forests, typically for N features, sqrt(N) features are used for each decision tree construction. Since in your case N =20, you could try setting max_depth (the number of sub-features to construct each decision tree) to 5. Instead of decision trees, linear models have been proposed and evaluated as base estimators in random forests, in particular multinomial logistic regression and naive Bayes. This might improve your accuracy. On MNIST kNN gives better accuracy, any ideas how to get it higher? Try with a higher value of K (say 5 or 7). A higher value of K would give you more supportive evidence about the class label of a point. You could run PCA or Fisher's Linear Discriminant Analysis before running k-nearest neighbour. By this you could potentially get rid of correlated features while computing distances between the points, and hence your k neighbours would be more robust. Try different K values for different points based on the variance in the distances between the K neighbours.
