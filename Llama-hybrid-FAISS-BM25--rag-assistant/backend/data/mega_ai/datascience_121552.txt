[site]: datascience
[post_id]: 121552
[parent_id]: 
[tags]: 
Do I need to standardize time series data in change point detection?

I have process data in time series data(0min, 1min, ... 999min). I don't know what does the variables mean. They are just written in X1, X2, ... X52. Each row means the data at the time. At certain point, process becomes abnormal. Then the data after the point are all abnormal data. If normal data class value is 0 and 1 for abnormal data, the label would be like below. 0 0 0 . . . 1 1 1 1 So I want to know when the value is changed to 1. I will use change point detection (with python 'ruptures' library). In this situation, should I standardize my data? I wonder (1) whether standardization is needed or not, and (2) if the standardization downgrades performance of the model, why is it? (3) If the standardization is not needed, is there any advantage for log transformation of data? As far as I know, log transformation is conducted to make the distribution be similar to standard distribution. (Correcting skewness) Is it true? I would appreciate to the answers for only the part of questions.
