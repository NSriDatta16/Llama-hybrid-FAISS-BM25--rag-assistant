[site]: crossvalidated
[post_id]: 353874
[parent_id]: 353081
[tags]: 
Don't choose, aggregate. If you can afford to fit multiple forecasts, then you also can aggregate them. You can take simple arithmetic average, or do something more clever, but on both cases should work well. The M4 forecasting competition has just ended (see Makridakis, Spiliotis and Assimakopoulos, 2018 ) and the results show that combinations of forecasts were most common among the best performing solutions. While the winning solution by Slawek Smyl was a hybrid approach that combined exponential smoothing and recurrent neural networks in a single model, the second best solution was by the Montero-Manso, Talagala, Hyndman, and Athanasopoulos that used a weighted combination of standard forecasting methods, where the weights were learned by XGBoost algorithm. Many of other top scoring solutions used some kind of combined forecasts. There was also another solution by the team that included Rob Hyndman that used FFORMS algorithm implemented in their seer package , which was choosing among multiple forecast by using Random Forest (you can here more about both solutions in here ), but it was further away in the ranking. Results of M4 fit many other similar competitions and surveys that show that combinations of forecasts usually perform better then any single forecasting method alone. Same is true about most of the winning Kaggle solutions. On another hand, if you have multiple series that come from same hierarchy (departments of store etc.), then it could be reasonable to use hierarchical forecast (see slides , book chapter , paper , and corresponding hts package by Rob Hyndman et al), since it should work better then making only the individual forecasts alone. Makridakis, S., Spiliotis, E., & Assimakopoulos, V. (2018). The M4 Competition: Results, findings, conclusion and way forward. International Journal of Forecasting.
