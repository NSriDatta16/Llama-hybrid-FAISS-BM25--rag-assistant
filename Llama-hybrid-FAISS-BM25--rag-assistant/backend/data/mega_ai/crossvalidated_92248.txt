[site]: crossvalidated
[post_id]: 92248
[parent_id]: 92213
[tags]: 
I respect @Glen_b's stance on the right way to answer here (and certainly don't intend to detract from it), but I can't quite resist pointing to a particularly entertaining example that's close to my home. At the risk of politicizing things and doing this question's purpose a disservice, I recommend Wagenmakers, Wetzels, Boorsboom, and Van Der Maas (2011) . I cited this in a related post on the Cognitive Sciences beta SE ( How does cognitive science explain distant intentionality and brain function in recipients? ), which considers another example of "a dart hitting the cat". Wagenmakers and colleagues' article comments directly on a real "howler" though: it was published in JPSP (one of the biggest journals in psychology ) a few years ago. They also argue more generally in favor of Bayesian analysis and that: In order to convince a skeptical audience of a controversial claim, one needs to conduct strictly confirmatory studies and analyze the results with statistical tests that are conservative rather than liberal. I probably don't need to tell you that this didn't exactly come across as preaching to the choir. FWIW, there is a rebuttal as well (as there always seems to be between Bayesians and frequentists; ( Bem, Utts, & Johnson, 2011 ) , but I get the feeling that it didn't exactly checkmate the debate . Psychology as a scientific community has been on a bit of a replication kick recently, partly due to this and other high-profile methodological shortcomings. Other comments here point to cases similar to what were once known as voodoo correlations in social neuroscience (how's that for politically incorrect BTW? the paper has been retitled; Vul, Harris, Winkielman, & Pashler, 2009 ). That too attracted its rebuttal , which you can check out for more debate of highly debatable practices. For even more edutainment at the (more depersonalized) expense of (pseudo)statisticians behaving badly, see our currently 8th-most-upvoted question here on CV with another (admittedly) politically incorrect title, " What are common statistical sins? " Its OP @MikeLawrence attributes his inspiration to his parallel study of psychology and statistics. It's one of my personal favorites, and its answers are very useful for avoiding the innumerable pitfalls out there yourself. On the personal side, I've been spending much of my last five months here largely because it's amazingly difficult to get rock-solid statistics on certain data-analytic questions. Frankly, peer review is often not very rigorous at all, especially in terms of statistical scrutiny of research in younger sciences with complex questions and plenty of epistemic complications. Hence I've felt the need to take personal responsibility for polishing the methods in my own work. While presenting my dissertation research , I got a sense of how important personal responsibility for statistical scrutiny is. Two exceptional psychologists at my alma mater interjected that I was committing one of the most basic sins in my interpretations of correlations. I'd thought myself above it, and had lectured undergrads about it several times already, but I still went there, and got called out on it (early on, thank heavens). I went there because research I was reviewing and replicating went there! Thus I ended up adding several sections to my dissertation that called out those other researchers for assuming causality from quasi-experimental longitudinal studies (sometimes even from cross-sectional correlations) and ignoring alternative explanations prematurely. My dissertation was accepted without revisions by my committee, which included another exceptional psychometrician and the soon-to-be-president of SPSP (which publishes JPSP), but to be frank once more, I'm not bragging in saying this. I've since managed to poke several rabbit holes in my own methods despite passing the external review process with perfectly good reviewers. I've now fallen into the deep end of stats in trying to plug them with methods more appropriate for predictive modeling of Likert ratings like SEM, IRT, and nonparametric analysis (see Regression testing after dimension reduction ). I'm opting voluntarily to spend years on a paper that I could probably just publish as-is instead...I think I even have a simulation study left to do before I can proceed conscientiously. Yet I emphasize that this is optional – maybe even overzealous and a costly luxury amidst the publish-or-perish culture that often emphasizes quantity over quality in early-career work records. Misapplication of parametric models for continuous data to assumption-violating distributions of ordinal data is all too common in my field, as is the misinterpretation and misrepresentation of statistical significance (see Accommodating entrenched views of p-values ). I could totally get away with it (in the short term)...and it's not even all that hard to do better than that. I suppose I have several recent years of amazing advances in R programs to thank for that though! Here's hoping the times are changing. References · Bem, D. J., Utts, J., & Johnson, W. O. (2011). Must psychologists change the way they analyze their data? Journal of Personality and Social Psychology, 101 (4), 716–719. Retrieved from http://deanradin.com/evidence/Bem2011.pdf . · Vul, E., Harris, C., Winkielman, P., & Pashler, H. (2009). Puzzlingly high correlations in fMRI studies of emotion, personality, and social cognition. Perspectives on Psychological Science, 4 (3), 274–290. Retrieved from http://www.edvul.com/pdf/VulHarrisWinkielmanPashler-PPS-2009.pdf . · Wagenmakers, E. J., Wetzels, R., Borsboom, D., & Van der Maas, H. (2011). Why psychologists must change the way they analyze their data: The case of psi. Journal of Personality and Social Psychology, 100 , 426–432. Retrieved from http://mpdc.mae.cornell.edu/Courses/MAE714/Papers/Bem6.pdf .
