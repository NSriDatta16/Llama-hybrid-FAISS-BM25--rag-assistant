[site]: crossvalidated
[post_id]: 353695
[parent_id]: 
[tags]: 
Why we say that In GP, choice of kernel reflects the belief of smoothness of the process?

In Gaussian Processes in Machine Learning (chapter 4 pdf) , the book shows that the smoothness of kernel is corresponding to the mean square smoothness. (I mean continuity or differentiability) I know that the mean square smoothness of the process will lead to the smoothness of the mean function. But it does not imply sample path continuity by mean square continuity or vice versa. See discussion here . However, In GPML figure 4.1, the realization of the process do reflect some smoothness depending on the Matern $\nu$ and $\nu$ is related to the smoothness of the kernel function. How can I rigorously state the smoothness relationship between kernel function and the Gaussian Process beyond mean square convergence?
