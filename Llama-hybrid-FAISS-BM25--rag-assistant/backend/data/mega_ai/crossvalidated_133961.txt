[site]: crossvalidated
[post_id]: 133961
[parent_id]: 133943
[tags]: 
Penalizing as you would in l1 regression is difficult to reconcile with simple algorithms for greedy tree growth. You'd need to do some sort of global optimization of the tree which becomes unwieldy. Pruning is the a common approach in single-decisiont-tree analysis but is rarely used in ensemble-of-tree methods like Random Forest (which this question is tagged with) where overfitting can be combated by either increasing the randomization of the models (via bagging, decreasing the number of features examined for each split or reducing the number of potential splits examined for each tree as in Extremely Randomized Trees). You can also limit the complexity of the tree with a max depth or leaf size parameter. In a Random Forest model I would do a parameter sweep of the number of in bag cases, the number of features examined and the leaf size or max tree depth and see if that achieves what you want.
