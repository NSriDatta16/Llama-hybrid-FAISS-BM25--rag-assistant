[site]: crossvalidated
[post_id]: 147203
[parent_id]: 
[tags]: 
Overtraining Neural Networks?

I am currently exploring the training of Neural Networks. I have some toy data and I've trained a NN with 2 hidden layers on it and I get 99 % accuracy on the test set. But the problem is that if I add an additional layer, my training set accuracy drops drastically to around 56 %. Thus I wanted to ask If anybody might know why that might be? Is it likely that I've coded something wrong (however I had no problem going from 1 -> 2 hidden layers), or it could be that if I try to train the a simple set on a very complex model (at least for the set), it could give me such erroneous results. Thank you very much.
