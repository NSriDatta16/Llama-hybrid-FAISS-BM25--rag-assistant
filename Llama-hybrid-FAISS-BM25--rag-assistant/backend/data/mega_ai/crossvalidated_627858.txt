[site]: crossvalidated
[post_id]: 627858
[parent_id]: 
[tags]: 
Overparameterization at the final layer of a neural network

I'm considering a multi-class classifying neural network with softmax on the final layer. Should one of the classes of the weight matrix of the final layer not be optimised over to account for the fact that the probabilities have to sum to one? I don't see any mention of this in the textbooks I've been reading on neural networks (e.g. Andreas Lindholm et al's Machine Learning - A First Course for Engineers and Scientists ). Is it not so much of a problem if you're using SGD rather than a quasi-Newton method?
