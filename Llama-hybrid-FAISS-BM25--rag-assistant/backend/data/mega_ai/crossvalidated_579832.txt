[site]: crossvalidated
[post_id]: 579832
[parent_id]: 
[tags]: 
self study: why is my neural network so much worse than my random forest

Playing around with machine learning in R. Say I have this arbitrary function: set.seed(123) n = 1e3 x = rnorm(n) y = 1 + 3*sin(x/2) + 15*cos(pi*x) + rnorm(n = length(x)) df = data.frame(y,x) # train/test df$train = sample(c(TRUE, FALSE), length(y), replace=TRUE, prob=c(0.7,0.3)) df_train = subset(df, train = TRUE) df_test = subset(df, train = FALSE) take a look: plot(df_train$x,df_train$y) a random forest fits nicely: library(randomForest) model_rf = randomForest(y~x, data = df_train) yh = predict(m, newdata = df_test) plot(df_test$x,df_test$y) points(df_test$x,yh, col = 'orange') but a neural net does terribly, regardless of how many layers I add (did 10 here): library(nnet) nn = nnet(y~x, data = df_train, size = 2) yh2 = predict(nn, newdata = df_test) plot(df_test$x,df_test$y) points(df_test$x, yh2, col = 'blue') Is this an artifact of the data? Or am I doing something blatantly wrong?
