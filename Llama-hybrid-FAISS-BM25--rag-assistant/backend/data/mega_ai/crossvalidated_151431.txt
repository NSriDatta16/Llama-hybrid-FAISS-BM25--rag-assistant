[site]: crossvalidated
[post_id]: 151431
[parent_id]: 151429
[tags]: 
To answer your second question, classification trees/random forests are probably a good way to go, and are usually a worthwhile tool for exploratory analysis. As far as your approach, depending on the data, you could try just running your algorithm on a few independent samples from the data set, where the sample size is a function of the running time of your classification. Taking multiple samples has the additional benefit of being able to quantify the variability of the classifier (This is bootstrapping in a sense but a bit more classical). As has been stated by other's, and what I tried to elude to with "depending on the data"; if you have lot's of variables, more than likely you have lot's of variables that are correlated in some respect with each other. PCA may be one option, but interpretability of the classification may be problematic. Bagging, in particular with random forests as suggested by (@aginensky) can also be useful in addressing the issue of dimension reduction, or at least help ensure you are not overfitting your tree.
