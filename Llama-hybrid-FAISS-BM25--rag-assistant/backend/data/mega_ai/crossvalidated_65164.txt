[site]: crossvalidated
[post_id]: 65164
[parent_id]: 
[tags]: 
A test of whether a sample has a specified distribution

Suppose we have an iid sample $X_1, \dots, X_n$. We want to test if it has a standard normal distribution. One test statistic I learned from Brockwell and Davis's Introduction to Time Series and Forecasting is to first find an interval $(-b, b)$ which has a probability $95\%$ under the standard normal distribution, then compute the ratio $r$ of the sample points falling into $(-b,b)$. The book continues to say if $r$ is not equal to $95\%$, then reject the null. But I think it is not right. What I think should be done instead is to find the distribution of $r$ under the null (and then find the rejection region of $r$). but I am not sure how to do that? This test should have very low power,isn't it? Because the test statistic only captures a little information about the sample's distribution? Thanks and regards! PS: in case my understanding is incorrect, the original text from the book says: Note that the book tells how to test if a sample is iid, but its underlying idea is to test if the sample autocorrelations is iid with distribution $N(0,1/n)$, which is the same question as in my post.
