[site]: crossvalidated
[post_id]: 588583
[parent_id]: 588474
[tags]: 
Pre-Analysis I have had to do this for a lab in the past, so I can probably provide some guidance. First off, your very first resource for such an extensive dataset is the group of people who came up with it in the first place. If you can contact the people who made it, they can provide a lot of background information on what variables are important, what they mean, how they were devised, etc. They may even have a handbook or manual that outlines every variable, which would be extremely helpful. If that information is not accessible, you can also see if the dataset has been cited in articles already, and from there you can also derive some insights. Finding What's Useful For the actual data side of things, once you have established which variables are important, I recommend condensing or splitting the data into meaningful parts. For example, if you have 50 variables for Variable X and Variable Z, but you also have 450 nonsense variables that are dichotomous, you can use a code like this: #### Load Data Wrangling Package #### library(tidyverse) #### Split Data Meaningfully #### data.split % select(contains("Variable_X"), contains("Variable_Y")) Perhaps all of these dichotomous variables are in wide format (it sounds like it from what you explained) and you need to convert them to long format (for an explanation of what this means, there is a good article on this by Hadley Wickham . One example is R's relig_income dataset, which has many income brackets: relig_income %>% head() Which is messy: # A tibble: 6 × 11 religion ` $10-2…¹ $ 20-3…² $30-4…³ $ 40-5…⁴ $50-7…⁵ $ 75-1…⁶ 1 Agnostic 27 34 60 81 76 137 122 2 Atheist 12 27 37 52 35 70 73 3 Buddhist 27 21 30 34 33 58 62 4 Catholic 418 617 732 670 638 1116 949 5 Don’t kn… 15 14 15 11 10 35 21 6 Evangeli… 575 869 1064 982 881 1486 949 # … with 3 more variables: `$100-150k` , `>150k` , # `Don't know/refused` , and abbreviated variable names # ¹​`$10-20k`, ²​`$20-30k`, ³​`$30-40k`, ⁴​`$40-50k`, ⁵​`$50-75k`, # ⁶​`$75-100k` But is easier read in long format: relig_income %>% pivot_longer(cols = 2:11, names_to = "Income_Bracket", values_to = "Amount") %>% head() And now looks easier to read: religion Income_Bracket Amount 1 Agnostic $10k 27 2 Agnostic $ 10-20k 34 3 Agnostic $20-30k 60 4 Agnostic $ 30-40k 81 5 Agnostic $40-50k 76 6 Agnostic $ 50-75k 137 Visualization There are also some visualization codes you can use to look at the data that you think is worth running. For example, the skimr package has a good quick look at how your data is distributed: library(skimr) skim(relig_income) Which gives you this: ── Data Summary ──────────────────────── Values Name relig_income Number of rows 18 Number of columns 11 _______________________ Column type frequency: character 1 numeric 10 ________________________ Group variables None ── Variable type: character ─────────────────────────────────────── skim_variable n_missing complete_rate min max empty n_unique 1 religion 0 1 5 23 0 18 whitespace 1 0 ── Variable type: numeric ───────────────────────────────────────── skim_variable n_missing complete_rate mean sd p0 p25 1 $10k 0 1 107. 169. 1 12.2 2 $ 10-20k 0 1 154. 255. 2 14.8 3 $20-30k 0 1 186. 310. 3 17 4 $ 30-40k 0 1 183. 291. 4 15.8 5 $40-50k 0 1 171. 271. 2 15 6 $ 50-75k 0 1 288. 458. 7 34.2 7 $75-100k 0 1 222. 345. 3 25.2 8 $ 100-150k 0 1 178. 276. 4 22.5 9 >150k 0 1 145. 205. 4 23.8 10 Don't know/refused 0 1 340. 531. 8 41.2 p50 p75 p100 hist 1 20 170 575 ▇▁▁▁▁ 2 27 193 869 ▇▁▁▁▁ 3 33.5 192 1064 ▇▁▁▁▁ 4 40 199. 982 ▇▁▁▁▁ 5 34 167. 881 ▇▁▁▁▁ 6 66.5 202. 1486 ▇▁▁▁▁ 7 65.5 129. 949 ▇▁▁▁▂ 8 48.5 104. 792 ▇▁▁▁▂ 9 53.5 134. 634 ▇▁▁▁▁ 10 74.5 295. 1529 ▇▁▁▁▂ Maybe you are only interested in the numeric data and you have a ton of uninteresting categorical variables. Using the starwars dataset, we can see that there are a lot of non-numeric values that we may not be interested in. glimpse(starwars) Which you can see: Rows: 87 Columns: 14 $ name "Luke Skywalker", "C-3PO", "R2-D2", "Darth Vade… $ height 172, 167, 96, 202, 150, 178, 165, 97, 183, 182,… $ mass 77.0, 75.0, 32.0, 136.0, 49.0, 120.0, 75.0, 32.… $ hair_color "blond", NA, NA, "none", "brown", "brown, grey"… $ skin_color "fair", "gold", "white, blue", "white", "light"… $ eye_color "blue", "yellow", "red", "yellow", "brown", "bl… $ birth_year 19.0, 112.0, 33.0, 41.9, 19.0, 52.0, 47.0, NA, … $ sex "male", "none", "none", "male", "female", "male… $ gender "masculine", "masculine", "masculine", "masculi… $ homeworld "Tatooine", "Tatooine", "Naboo", "Tatooine", "A… $ species "Human", "Droid", "Droid", "Human", "Human", "H… $ films $ vehicles , <>, … $ starships , <>, <>, "TIE A… You can gather the data that's useful to you and visualize their distributions that way: starwars %>% keep(is.numeric) %>% gather() %>% ggplot(aes(x=value))+ geom_density(fill="green", alpha= .4)+ facet_wrap(~key, scales = "free")+ labs(title = "Star Wars Numeric Data", x="Value", y="Density") There are probably a lot of other ways you can look at data. Something that may help is reading the book R for Data Science . I think it does a good job of explaining how to wrangle data, even if learning it for the first time can be frustratingly difficult as a beginner. Also, this guide may be helpful if you want something shorter. Though it is more focused on Python, the actual process of exploring and cleaning data is well explained there. Hope that is helpful and let me know if you have questions. Analysis As for the analysis part...it doesn't appear clear what you are actually trying to do with the data yet. This is common with people who first analyze large datasets, but the problem is largely alleviated when you know which direction to walk. I would first figure out what questions you are trying to answer with your data...are you trying to look at differences in height between genders? Are you looking to employ some form of machine learning to estimate what predicts hiring and firing in a corporation? It helps to have a solid preliminary reason for why you are even looking at the numbers in front of you. Additionally, if you have questions about a specific stats method, its better to ask about that rather than analysis in general, as that can be a years-long topic given the vast depth of statistical topics out there.
