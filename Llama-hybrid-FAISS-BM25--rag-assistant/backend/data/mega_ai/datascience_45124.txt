[site]: datascience
[post_id]: 45124
[parent_id]: 
[tags]: 
Why is PyTorch's DataLoader not deterministic?

I've set the seeds like this (hoping to cover all bases): random.seed(666) np.random.seed(666) torch.manual_seed(666) torch.cuda.manual_seed_all(666) torch.backends.cudnn.deterministic = True The below code will still output DIFFERENT batches for both namesTrainLoader1 and namesTrainLoader2 but they should really be the same . How come creating the model is affecting the deterministic values? namesDataset = NamesDataset() namesTrainLoader1 = DataLoader(namesDataset, batch_size=5, shuffle=True) for each in namesTrainLoader1: print(each) model = TorchRNN(inputSize, hiddenSize, outputSize) namesTrainLoader2 = DataLoader(namesDataset, batch_size=5, shuffle=True) for each in namesTrainLoader2: print(each) Output for namesTrainLoader1 : ('saiki', 'close', 'sloan', 'horos', 'roman') ... Output for namesTrainLoader2 : ('david', 'abeln', 'hatit', 'holan', 'protz') ... I also tried using worker_init_fn (e.g. with lambda x: 0) in the DataLoader , but that made no difference. Why is this not deterministic? How can I make it deterministic? i.e. reset the internal seed of the DataLoader ?
