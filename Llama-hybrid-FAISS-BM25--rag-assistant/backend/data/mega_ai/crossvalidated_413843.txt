[site]: crossvalidated
[post_id]: 413843
[parent_id]: 
[tags]: 
In an RNN, if the gradients don't vanish for long/distant terms, won't the derivative of the error be either divergent to infinity or oscillatory?

In my question below, I'll refer to the following well known paper " On the difficulty of training Recurrent Neural Networks " by Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. Assume we're working on somewhat simplified RNN, where, by their equation (2) on page 1, we have the following dynamical systems of their hidden states $\{x_t\}$ : $x_{t}=W_{R}x_t + W_I \sigma(x_{t-1}) + b$ , where I'm using $W_R$ in place of their $W_{rec}$ . Let $\theta = (W_R, W_I,b)$ the parameter set of this dynamical system. Please observe equations (3), (4) and (5) on page 2 that're basically back propagation through time in an unrolled RNN. If I understand correctly, the gradient vanishing problem is really not the vanishing of the term $\frac{\partial E_t}{\partial \theta}$ , but rather vanishing of the terminal summands $\frac{\partial E_t}{\partial x_t}\frac{\partial x_t}{\partial x_k} \frac{\partial x_k}{\partial \theta}$ , where $k$ is much small compared to $t$ , e.g. for $k=1$ . Correct? If so, I'm having a problem! Assume that the above "vanishing gradient problem", as per the definition above, does not happen. Then the above terminal summand, i.e. $\frac{\partial E_t}{\partial x_t}\frac{\partial x_t}{\partial x_k} \frac{\partial x_k}{\partial \theta}$ does not vanish when $k$ is fixed but $t \to \infty$ . But this implies that the sum $\frac{\partial E_t}{\partial \theta}=\Sigma_{k}\frac{\partial E_t}{\partial x_t}\frac{\partial x_t}{\partial x_k} \frac{\partial x_k}{\partial \theta}$ diverges or oscillates as $t \to \infty$ . (This should be clear by the contrapositive of the following statement:for a convergent infinite series $\Sigma_{t=0}^{\infty} r_t$ , we must have $lim_{t \to \infty} r_t = 0.$ ). But if the sum $\frac{\partial E_t}{\partial \theta}$ diverges or oscillates and as $t \to \infty$ , then that'd also be a problem right? Because in this case, converging to a local/global minima can be prevented by this divergence or oscillatory behavior of $\frac{\partial E_t}{\partial \theta}$ for large $t$ , making "learning" $\theta$ difficult. So why is that not a problem?
