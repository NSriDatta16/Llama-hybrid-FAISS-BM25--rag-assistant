[site]: crossvalidated
[post_id]: 324082
[parent_id]: 261918
[tags]: 
What it sounds like you are describing is that you are performing Batch Gradient Descent in your matrix implementation, and Mini Batch (size = 1) in your vector (iterative) implementation. Batch Gradient Descent: you run all the training data through the network, compute the total error, then propagate the error back. Mini Batch Gradient Descent: You run a subset of training data through the network compute the error within that subset and propagate the error back. Stochastic Gradient Descent: A special case of Mini Batch Gradient Descent where you select a random subset of training data for each iteration. Note that Batch size can be 1 or more. It is common for matrix implementations to bundle all training data into a single matrix. This is likely because it is appealing to take advantage of the fact that you can feed-forward and back propagate, each with a single operation instead of doing a for-loop over each training sample. It can also makes the code read more concisely (less code, no for-loops, etc). As you have noticed, this doesn't necessarily lead to more optimal training. The solution is to not pass the entire training data matrix through your network. Instead, assuming that each row in the matrix represents a single training sample, iterate over each row, and pass it independently through your network and perform error calculation and back propagation one sample at a time. If you choose to use Stochastic Gradient Descent, randomly select a single row to train on each iteration. Below is a random sample of my code for training MNIST digits. The training matrix consists of 60,000 samples. Each row being a separate image. I am training via Stochastic Gradient Descent (randomly selecting training data, back propagate after each sample.) val mnistImageLoader = MNISTImageLoader() // load a double matrix of mnist data, each row is a sample val xs = mnistImageLoader.loadIdx3("/data/mnist/train-images-idx3-ubyte").div(255.0) val visibleSize = 28 * 28 val hiddenSize = (visibleSize * 0.75).toInt() val layer = AutoEncoder( learningRate = 0.1, visibleSize = visibleSize, hiddenSize = hiddenSize) val rand = Random() (0.. 10000).forEach { i -> // select a single random row of image data val x = xs.getRow(rand.nextInt(xs.rows)) // feed-forward, calculate error, back-propagate for singe sample layer.learn(x, 1) // learn one-step // compute error for current training sample val error = Errors.compute(x, layer.feedForward(x)) println(error) }
