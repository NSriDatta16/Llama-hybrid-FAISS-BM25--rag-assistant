[site]: crossvalidated
[post_id]: 483939
[parent_id]: 458057
[tags]: 
LASSO is indeed used for computing weights of forecast combinations; see Diebold & Shin (2019) . The twist is that forecast weights are shrunk towards $1/k$ (where $k$ is the number of forecasts being averaged) rather than $0$ as LASSO normally would. The rationale for shrinking the combination weights towards $1/k$ is the empirical fact that equally-weighted forecast combinations tend to outperform unequally-weighted forecast combinations, known as the forecast combination puzzle . The reason behind the latter is estimation imprecision (due to high variance of estimators) of theoretically optimal weights; see Claeskens et al. (2016) or Diebold (2017) Chapter 12, among other studies. LASSO allows reducing estimation variance at a cost of increasing bias. As long as the variance reduction is greater than the increase in the square of the bias (which it typically is for suitably selected penalization intensity), the mean squared error is reduced (hint: bias-variance decomposition and trade-off). References Claeskens et al. "The forecast combination puzzle: A simple theoretical explanation" (2016) Diebold "Forecasting in Economics, Business, Finance and Beyond" (2017) Diebold & Shin "Machine learning for regularized survey forecast combination: Partially-egalitarian LASSO and its derivatives" (2019)
