[site]: datascience
[post_id]: 20011
[parent_id]: 
[tags]: 
Intuition for Logistic Regression Performance

I have a bunch of time series data I'm doing classification on. I used TPOT with a custom cv (walkforward time series split) to find the best performing model. Logistic regression was selected. I'm a bit surprised by this, given that most kaggle competitions are won by ensembles of newer models like XGB, etc. In fact, not only was logistic regression the best model but it was actually the only one that learned something meaningful (outperformed the others like RF, XGB, etc. by > 5% where 60% is the best accuracy it achieved). My expertise is mostly in deep networks, not as much the stuff dealt with by sklearn, but in this case the training data is very limited and quite noisy. What should I take away from logistic regression being the best classifier? Can I use this knowledge to inform my design of other classifiers that could perform better? Again, I have a lot less experience with this side of ML, so I'm really looking for an intuition as to how I can take this knowledge and use apply it to build better models or ensembles.
