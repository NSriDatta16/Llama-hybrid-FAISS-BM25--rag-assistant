[site]: crossvalidated
[post_id]: 626368
[parent_id]: 626346
[tags]: 
There are two aspects: Purpose of the test data: Use it mainly as information, not for decision. So in your case, you would typically go for the new model, even if its test performance is lower. The test performance helps you to judge how well the new model is expected to perform on data not used for modeling. Aggressive tuning: The more intensely you use the validation data for hyper-parameter optimization, the more biased will be the validation performance. This starts to become a real problem when you compare the performance of such a model with a simpler one (e.g., with your first model or an additive model). The comparison is unfair. One way to overcome this problem is to use cross-validation for hyper-parameter tuning, and the validation data only to compare the tuned algoritms (e.g., XGBoost versus additive model). Side remark: model selection should be done on a proper scoring rule like log loss. It is one of the most strange things that people use log loss/cross-entropy as loss/objective, and then make decisions with a very different and worse quantity.
