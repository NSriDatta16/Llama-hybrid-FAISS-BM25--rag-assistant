[site]: crossvalidated
[post_id]: 145078
[parent_id]: 
[tags]: 
Deep Learning with few features available

I was asked to employ deep learning on some seismic simulation data. Visually, the data is a cube, 1000 x 1000 x 1000. For each point in the cube, there are 3 numeric features [1, 0]. Some of it is labeled (so can be used for training), some is not. The idea is to create better representations than the existing 3 features provide using autoencoder or RBM. I currently use grid search on Bernoulli RBM (which is appropriate since data is 0 to 1) to find the appropriate number of hidden layers/hidden units per layer, and other parameters. Then I have simple logistic regression classifier take the outputs of the last hidden layer and do classification. The problem is: logistic regression on original features does as well or better. When I add more hidden layers, performance of deep system degrades. I have read that additional hidden layers are guaranteed to improve performance only if each succeeding layer has less than the previous one (Hinton). Is that true or am I misunderstanding something? In this case, does deep learning only work will for data with lots of features (images, etc.)? How could I go about my problem (the task was given by someone not familiar with Machine Learning so I cannot just say "it doesn't work this way")?
