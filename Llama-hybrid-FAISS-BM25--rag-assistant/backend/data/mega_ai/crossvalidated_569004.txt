[site]: crossvalidated
[post_id]: 569004
[parent_id]: 568780
[tags]: 
Mean squared error and mean absolute error are common. Then there are some that arise from time series forecasting but might make sense for you. The easiest to understand is mean absolute percentage error (MAPE), despite its problems. That page also mentions four alternatives to MAPE: Mean Absolute Scaled Error (MASE) Symmetric Mean Absolute Percentage Error (sMAPE) Mean Directional Accuracy (MDA) Mean Arctangent Absolute Percentage Error (MAAPE) Regarding $R^2$ and adjusted $R^2$ , there is a natural way to use the idea of $R^2$ for out-of-sample data. Regular $R^2$ compares the MSE of your model to the MSE of a model that guesses the mean of $y$ every time, regardless of what the features are. $$ R^2 = 1 - \dfrac{ \sum_{i=1}^n \big(y_i - \hat y_i\big)^2 }{ \sum_{i=1}^n \big(y_i - \bar y\big)^2 } $$ Apply this same idea to out-of-sample truth values $y_i$ and predicted values $\hat y_i$ , making sure to still use a denominator model that always guesses the same $\bar y$ that you calculated on the in-sample data. $$ R^2_{out} = 1 - \dfrac{ \sum_{i=1}^n \big(y_i - \hat y_i\big)^2 }{ \sum_{i=1}^n \big(y_i - \bar y_{in}\big)^2 } $$ Adjusted $R^2$ on test data makes less sense, since the reason for having the penalty for a high parameter count in adjusted $R^2$ is to catch if the model only fits the data used to train the model, rather than being able to generalize. Checking the ability to generalize, rather than just fitting the training data, is exactly what an out-of-sample metric checks.
