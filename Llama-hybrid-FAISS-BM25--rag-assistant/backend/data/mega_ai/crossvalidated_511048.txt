[site]: crossvalidated
[post_id]: 511048
[parent_id]: 
[tags]: 
Why is KL divergence between a standard normal, and normal distribution 0 if standard deviation is 0.37?

I wanted to see a graph about how KL divergence between a standard normal distribution, and any other normal distributions with 0 mean, and standard deviation being $x$ varies. I mostly need it for variational autoencoder loss calculation, and so far I learnt that it needs to be calculated like: $$D_{KL} = 0.5 * (\sigma^2 + \mu^2 - 1 - log \space \sigma^2)$$ If we pretend we know that $\mu$ is 0, I'm assuming we can simply skip that step, so we end up with: $$D_{KL} = 0.5 * (\sigma^2 - 1 - log \space \sigma^2)$$ In this graph you can see that the KL divergence is 0 if $x$ is -1, 1, -0.37, 0.37. If my equation is right, it should mean how different a normal distribution is with $x$ standard deviation from a standard normal distribution. I don't understand the negative values, but certainly don't understand the 0.37 value. Did I mess up my equation, or it is expected?
