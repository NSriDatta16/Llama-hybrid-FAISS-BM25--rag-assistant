[site]: crossvalidated
[post_id]: 193041
[parent_id]: 193029
[tags]: 
Let's start from a different scenario: a set of points that is not linearly separable . A projection (mapping) onto a higher dimensional space can allow us linearly separate our data. Let's stretch this idea. Let's project data onto a extremely high dimensional data. The planar separator in the extremely-high-dimensional space of feature vectors will be a curved (highly non-linear) separator in the low dimensional space . We are therefore very likely introducing over-fitting. What we have described is a clear trade-off between bias and variance . I forgot to mention that there's a bound in the parametrization of the separating hyperplanes: if we are in a space $\mathcal{H}$, the set of hyperplanes is parametrized by $dim( \mathcal{H} ) + 1$. However, given the form of solution [of the SVM], there are at most $l$ + 1 adjustable parameters (where $l$ is the number of training samples) (source) Note that there's a relationship between the number of support vectors and model complexity: Now, the number of support vectors still depends on how much slack we allow, but it also depends on the complexity of our model. Each twist and turn in the final model in our input space requires one or more support vectors to define. ... The number of support vectors can range from very few to every single data point if you completely over-fit your data. This tradeoff is controlled via C and through the choice of kernel and kernel parameters. (source) This is the higher level big picture on the trade-off and the risk of poor generalization in a high dimensional problem. See the answer below by Dikran Marsupial for a more detailed description of the role of feature selection in this context.
