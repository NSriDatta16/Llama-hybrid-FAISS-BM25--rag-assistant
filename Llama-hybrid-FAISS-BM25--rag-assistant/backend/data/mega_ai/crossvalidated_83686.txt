[site]: crossvalidated
[post_id]: 83686
[parent_id]: 
[tags]: 
The curse of dimensionality? (linear SVMs)

How do you know whether you suffer from it? Let's suppose I have a 2 class problem - 2000 training examples and 30 features. While it works good for the most part, sometimes I get edge cases that are so horribly misclassified. I feel like it's because there is a lot of dimensions and too little examples. I feel like the dimensionality is affecting my results - is there a way to check if it's true? Do I need to have several examples for each combination of the feature values? With 30 features from which each can have hundreds of values, this would require ENORMOUS amount of data... How much improvement can I anticipate if I were to increase the training set size to 4000 examples, 8000 examples, 16000 examples etc. - is there a rule of thumb when trying to figure this out?
