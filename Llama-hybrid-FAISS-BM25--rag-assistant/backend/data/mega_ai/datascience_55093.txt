[site]: datascience
[post_id]: 55093
[parent_id]: 
[tags]: 
Training Deep learning and validation loss

I'm trying to replicate result of a paper. The paper is a U-net for De-noising of some images. So basically I have a simple U-net that I give noisy data as input and have denoised data as the wanted output (use l2/MSE loss) . So, in the paper and generally in most papers like this (deep learning applied to medical imaging), they say they run the model for something like 300 epochs or they say they run it like ~50 hours. My question is that aren't they supposed to put call backs for the validation loss, so once the validation loss stops improving they stop the training, otherwise the model will overfit? Also does the number of batches in an epoch matter. If because of memory constraints I use 2 batch sizes per epoch, would it be a bad practice?
