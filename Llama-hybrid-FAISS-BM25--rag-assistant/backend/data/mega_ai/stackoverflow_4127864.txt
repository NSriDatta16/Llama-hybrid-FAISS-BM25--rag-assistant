[site]: stackoverflow
[post_id]: 4127864
[parent_id]: 
[tags]: 
How to handle large (1m+ rows/daily) mySQL databases & transactions

I have a web service that handles 500k+ unique hits a day (will be bumped up to 4m). There is a lot of log data per visitor (~5 rows/visit) to log various information about each visit (useragent, IP, location, etc). Every day at 1am I have PHP & mySQL summarize all the data in the log tables (# uniques, us uniques, average time) into another summary table. Each visitor is associated with 1 of about 1k different "groups" when they visit the site, depending upon certain characteristics (user agent, OS, location) Summarizing all the data takes a really long time and sometimes kills the DB server as we run the summary query for each of the 1k groups, and then insert the data into the summary table. Is there a more efficient way of storing and summarizing extremely large amounts of log data in a mySQL db?
