[site]: datascience
[post_id]: 106755
[parent_id]: 
[tags]: 
What are the best methods to reduce the bag of words dimensionality?

I have a small text dataset with 600 comments. My task is a binary classification one. In order to train my model, I transformed the comments into a bag of words using sklearn CountVectorizer. The vocabulary has 1800 words, which is way bigger than the amount of comments. So, how can I reduce the dimensionality of my dataset? I want to know this because I believe that this bigger vocabulary is a problem for the model. I know methods to reduce dimentionality, like PCA, but I do not know if it is as useful in the context of text classification as it is in a tabular dataset. I am also new to nlp, so I wonder if there is a better way to reduce dimensionality, like a way to choose the best words in the vocabulary and use only those.
