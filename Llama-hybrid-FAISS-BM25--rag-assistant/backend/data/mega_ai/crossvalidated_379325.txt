[site]: crossvalidated
[post_id]: 379325
[parent_id]: 
[tags]: 
Bayes theorem: a function of D, or theta, or both?

In the Bayes formula as written for machine learning applications, $$ p(\theta|D) = \frac{ p(D|\theta) p(\theta) }{ p(D) } $$ where $D$ is the data, $\theta$ are the model parameters. Commonly $p(\theta)$ is labeled the prior, $p(D|\theta)$ is called the likelihood, and $p(D)$ is called the evidence (or marginal likelihood I think). The question: I am bothered by calling $p(D|\theta)$ as a likelihood. I believe a likelihood is not a probability density, meaning that $\int p(D|\theta) d\theta$ does not integrate to one. Wheras I think $\int p(D|\theta) d\,D$ does integrate to one. So it seems strange to include something that is not a probability in the formula. I think maybe my confusion is about whether the overall Bayes formula should be considered as "a function of" $D$ , or $\theta$ , or either or both?
