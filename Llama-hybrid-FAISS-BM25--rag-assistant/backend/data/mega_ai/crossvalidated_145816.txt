[site]: crossvalidated
[post_id]: 145816
[parent_id]: 
[tags]: 
Neural networks: how can convex optimization produce different weights each time?

I am training a multilayer perceptron with a logistic activation function by backpropagation. The weights are not unique - each time I redo the fit, I get a different set of weights. However the optimization function (sum squared error in my case) is convex, which means there is one global minima. So I suppose this minima has several equivalent sets of weights that map to it. So how exactly do I describe this minima and what are its properties? How do I What is the name of this phenomenon and where can I learn more about it?
