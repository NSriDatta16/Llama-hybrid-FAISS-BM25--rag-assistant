[site]: crossvalidated
[post_id]: 166441
[parent_id]: 163463
[tags]: 
"Information gain" seems to be an overloaded name that corresponds to multiple formulas. The non-ambiguous names appear to be: The mutual information linking two random variables X and Y: $$ MI(X,Y) = H(X) + H(Y) - H(X,Y) = H(Y) - H(Y|X) = H(X) - H(X|Y) $$ where $H$ is the entropy of the random variable, and the Kullback-Leibler (KL) divergence, which measures the difference between two probability laws or probability density functions: $$ KL(p,q) = \int p(x) \log \frac{p(x)}{q(x)}dx. $$ These two quantities are linked. After straightforward manipulations from $ MI(X,Y) = H(Y) - H(Y|X)$, we find: $$ MI(X,Y) = \int p(x) KL(P(Y=y|X=x) | P(Y=y)) dx, $$ where the right hand side is the average KL divergence between a random variable's marginal and conditional distributions.
