[site]: crossvalidated
[post_id]: 325723
[parent_id]: 325711
[tags]: 
Your expectation that the posterior probability should generally increase while the Markov chain has not converged yet is false . This can be seen in the following simple example. Let your chain sample a random variable (prior and likelihood do not matter here, so I give just the posterior) which is in state 0 with a probability of $\frac{1}{4}$ and in each of 300 different other states with probability $\frac{1}{400}$. Then you will see the chain (which in this case is might even be IID) will be in $log(p) \approx -6.0$ states for most of the time, and when it's not, it is very likely that you will observe a decrease from $-1.4$ to $-6.0$. So if you have many states each having a probability density of those 1000 log units lower, and only relatively few at the higher log posterior level, such a decrease in posterior probability is not by itself a concern â€“ you mention that you sample a tree space, and as you will know, tree spaces are huge, so don't take this as an alarm signal all by itself.
