[site]: crossvalidated
[post_id]: 297877
[parent_id]: 142215
[tags]: 
I'd like add one additional point: the reason for some of the confusion rests on what it means to be performing "Naive Bayes classification". Under the broad topic of "Gaussian Discriminant Analysis (GDA)" there are several techniques: QDA, LDA, GNB, and DLDA (quadratic DA, linear DA, gaussian naive bayes, diagonal LDA). [UPDATED] LDA and DLDA should be linear in the space of the given predictors. (See, e.g., Murphy , 4.2, pg. 101 for DA and pg. 82 for NB. Note: GNB is not necessarily linear. Discrete NB (which uses a multinomial distribution under the hood) is linear. You can also check out Duda, Hart & Stork section 2.6). QDA is quadratic as other answers have pointed out (and which I think is what is happening in your graphic - see below). These techniques form a lattice with a nice set of constraints on the "class-wise covariance matrices" $\Sigma_c$: QDA: $\Sigma_c$ arbitrary: arbitrary ftr. cov. matrix per class LDA: $\Sigma_c = \Sigma$: shared cov. matrix (over classes) GNB: $\Sigma_c = {diag}_c$: class wise diagonal cov. matrices (the assumption of ind. in the model $\rightarrow$ diagonal cov. matrix) DLDA: $\Sigma_c = diag$: shared & diagonal cov. matrix While the docs for e1071 claim that it is assuming class-conditional independence (i.e., GNB), I'm suspicious that it is actually doing QDA. Some people conflate "naive Bayes" (making independence assumptions) with "simple Bayesian classification rule". All of the GDA methods are derived from the later; but only GNB and DLDA use the former. A big warning, I haven't read the e1071 source code to confirm what it is doing.
