[site]: stackoverflow
[post_id]: 385393
[parent_id]: 385213
[tags]: 
The best use for Key Performance Indicators is for driving (or steering, if your prefer). For course-corrections in real-time . (See Dashboards are for Driving for more blather about this sub-topic. Caveat: I am the author of the blathering article.) So, the question back to you is: are you trying to evaluate performance after the fact , when it is too late to do anything about it , or are you trying to find KPIs that can help you stay on course ? If the former, any metric your organization cares about (bug count, ship-date slippage, lines of code with comments, customer return percentages, etc.) will be fine. Measure away and good luck getting any better in between shipping products and upgrades ;-) If the latter, choose velocity . Assuming you are using test-driven development (TDD) of course. EDIT: so it's the former. Well, here's why you are probably out of luck: Suppose that you decide that "quality" is best quantified by measuring the number of bugs reported by customers as your post-process KPI. Let's assume that you are using TDD, and say that your team delivers Product #1 in 6 months, and after 6 months in the field you find that you have 10 customer-reported bugs. So now what, exactly, are you going to do to improve your process? Test more? Test specifically for more things like the causes of the bugs that were reported? It seems to me that you would already be testing, and when bugs are discovered - whether by the customer or not - you add a regression test for the specific bug and additional unit tests to make sure that there are no more similar bugs. In other words, your post-process improvement response will be no different than your in-process improvement response , so this KPI is really of no significant help in improving your process. The point is that the way you improve your process remains the same regardless of whether the bugs are discovered 6 months after release or two days into coding. So while this might be a shiny KPI to put on a manager's wall or a departmental newsletter, it really will not change your process-improvement mechanisms. (And beware of putting too much stock in this KPI because it can be wildly influenced by factors beyond your control!). In short, knowing the number of bugs does not help you improve . (There is another danger here, one commonly found not just in business but also in the military, and that is the illusion that the post-mortem analysis revealed valuable information, so the lessons learned post-mortem are vigorously applied to the next project, which is probably not the same as the last project . This is known as "fighting the last war".) Suppose the number of customer returns/refunds is your KPI of choice for "quality" - if this number is 5, what does this tell you? The specific reasons why customers requested a refund may be some indication of quality problems ("too slow", "doesn't interface with XYZ system", etc.), but the mere number of such incidents tell you nothing. A variance against an expected return percentage might tell you if quality was improving, but again the number does not help you improve . You need more information than the number can give you. So for "timeliness of releases" what measurement would be appropriate? Number of days of ship-date slippage? Percent overrun based on original estimates? It doesn't matter , because again the numbers do not help you improve . If you can measure "productivity" after the product is done then you can probably measure it while the product is being developed (e.g. velocity), the difference is that productivity less than expected during development can be improved immediately, while an overall productivity number measured after development is completed is too gross, too averaged, to be of any use. One could only guess at why it was lower than expected 6 months later... I have no idea how one might measure "flexibility", that sounds like marketing jargon ;-) I hope I haven't pounded this nail too hard or too far, but I don't think that there is anything useful that you can measure after-the-fact that you cannot measure while in progress . And there are a lot of after-the-fact measurements that are useless without knowing the causes.
