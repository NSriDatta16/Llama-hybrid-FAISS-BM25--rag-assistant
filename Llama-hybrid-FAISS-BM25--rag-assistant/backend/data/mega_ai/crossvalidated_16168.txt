[site]: crossvalidated
[post_id]: 16168
[parent_id]: 16146
[tags]: 
If predictive accuracy is the main objective, then it is generally better to use regularisation to address problems such as correlated predictor variables and not perform any feature selection. This is because feature selection is difficult. Most often feature selection is peformed by optimising some feature selection criterion evaluated over a finite dataset. Since only a finite dataset is used, the feature selection criterion has a non-zero variance, and hence it is possible to over-fit the feature selection criterion (and get a set of features that is optimal for this particular sample of data, but not for the true underlying distribution and hence generalisation is poor). Over-fitting is always most dangerous when you have many degrees of freedome with which to optimise the criterion, and in feature selection, there is one per feature. For regularisation (e.g. ridge regression or regularised logistic regression) there is only one degree of freedom (the ridge parameter) and so the risk of over-fitting is generally lower (but it doesn't go away completely). This is the advice given in the appendix of Millar's monograph "subset selection in regression" (but without the reasoning IIRC). If you can identify the variables that are the causal "parents" of the quantity you seek to predict, then using only those featrues has the advantage that the model will still work well when extrapolating or under covariate shift (e.g. the sampling of the data uses a different distribution), as your model will represent the true causal structure, rather than mere correllations. So if extrapolation or covariate shift is an issue, causal feature selection may be helpful (although in practice identifying causal relationships is unreliable). Isabelle Guyon has much to say that is well worth listening to on this topic (just found a videolecture here that I am going to watch now). There is no need for the same model to be used for explication and for prediction, so I would say fit two models, one with feature selection to help you understand the problem/data and a second model with no feature selection but with properly tuned regularisation to use for prediction.
