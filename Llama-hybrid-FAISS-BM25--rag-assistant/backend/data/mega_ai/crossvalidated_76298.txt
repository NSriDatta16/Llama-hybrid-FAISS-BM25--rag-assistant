[site]: crossvalidated
[post_id]: 76298
[parent_id]: 76288
[tags]: 
This should be a comment, but I'm still trying to work out how to do that on this site. If you've tried genetic algorithms, I assume you've tried a lot of other options. Might be helpful listing them. It sounds from the question, but not clear, that you have (1) a p>n problem (2) collinearity problem (3) are ignoring interactions. Based on that elastic-net (glmnet package) might be useful. You want a subset selection method, but given p>n you'd prefer a shrinkage method, and given collinearity elastic-net might be better than lasso. Dimension reduction methods (like PCA) usually still use all the variables to generate the principal components - so doesn't seem that useful to you. I wasn't that clear about the arbitrary cut-off of 10-20 variables. If you do just want 10-20 variables and that's it, you could rank predictive importance and then choose the top 10-20. This can be done after many learning models - but think easiest after boosted regression.
