[site]: datascience
[post_id]: 126604
[parent_id]: 
[tags]: 
Why Batch Normalization is undesirable?

In Let's build GPT: from scratch, in code, spelled out. , Andrej Karpathy says that no one likes Batch Normalization layer and people want to remove it. He also said it brings so many bugs and he shot his foot by this. But I am not sure why Batch Normalization is so undesirable. Kindly give concrete examples and mechanism that cause the harms for each situation.
