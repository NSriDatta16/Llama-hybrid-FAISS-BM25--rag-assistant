[site]: datascience
[post_id]: 20588
[parent_id]: 
[tags]: 
Update statement for Logistic Regression

I don't know which the update statement for Stochastic Gradient descent is the right one for the Newton update of the coefficients. Does it use the sum of the derivatives for ALL variables to update all the coefficients or is it iterating through each coefficient and using the sum of the derivatives for only that particular variable to update that coefficient? On page 120 (pg 139 on pdf) of the Elements of Statistical Learning book https://web.stanford.edu/~hastie/Papers/ESLII.pdf The Newton update statement (4.23) looks like all coefficients are updated with the sum of the derivatives of all rows Xi for ALL the variables In the update statement in this link http://machinelearningmastery.com/logistic-regression-tutorial-for-machine-learning/ the coefficients are updated with the sum of the derivatives for each that variable and not the sum of the derivatives for all variables. The update statement on slide # 23, pg 21 for http://www.hlt.utdallas.edu/~vgogate/ml/2015s/lectures/lr-nb-lec6.pdf I read the update statement to say the new coefficient for the given variable is the old weight + the derivative of that given variable.
