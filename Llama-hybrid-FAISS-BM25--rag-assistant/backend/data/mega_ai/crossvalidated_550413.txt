[site]: crossvalidated
[post_id]: 550413
[parent_id]: 550273
[tags]: 
So far I cannot appreciate your idea about using of PCA here, as it is unclear to me because you are showing only code and not the results themselves, and you are not explaining the idea. Hence, I'm answering to your problem without considering your recipe. Provided you have features data for every point (instance), you can select a distance (or similarity) measure you like and compute the points by points distance matrix. Then, because you need the distance matrix classes by classes , you have to select a rule how to calculate the set distances from the point distances , that is, to compute a distance between every two groups of points given distances between individual points. These rules majorily coincide with the linkage methods utilized in hierarchical clustering . There are two ways to technically get k x k class distances (prespecified classes) from a n x n point distances according to the rule selected. Compute manually or write a program doing it, or find a program doing it. My SPSS macro !KO_ASSCLU does right that job. (So welcome if you use SPSS). Use a hierarchical cluster analysis program which have the options: a) constraining option to cluster points first in the groups specified, before going on to cluster among the groups, b) option to stop agglomeration and output the distance matrix as it is at the moment of stop. My macro !KO_HIECLU does it. (To get the macros or to read, what and how they do, go to my web-page, linked to in the Profile, and get the "Clustering" collection.) Results for Iris dataset, using !KO_ASSCLU : Euclidean distances between all the data points, fragment of the matrix (features were z-standardized before computing distances): Distances between the three species (Between-group average linkage rule used): Distances between the three species (Hausdorff distance linkage rule used):
