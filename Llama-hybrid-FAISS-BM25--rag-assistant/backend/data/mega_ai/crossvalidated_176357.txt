[site]: crossvalidated
[post_id]: 176357
[parent_id]: 176286
[tags]: 
K-Nearest Neighbor is an instance-based learning algorithm that, as the name implies, looks at the K neighbors nearest to the current instance when deciding on a classification. In order to determine which neighbors are nearest, you need a distance measure. In the sample data set above, the distance measure that makes the most sense is the # of matching nominal values/# of attributes (4 in this case). For Record ID 1, we can calculate the distance between this instance and the instances between Record ID 2-10 as follows: ID2: 3 matching attribute values/4 attributes = 0.75 ID3: 3/4 = 0.75 ID4: 2/4 = 0.5 ID5: 3/4 = 0.75 ID6: 2/4 = 0.5 ID7: 2/4 = 0.5 ID8: 1/4 = 0.25 ID9: 3/4 = 0.75 ID10: 2/4 = 0.5 Thus, it is clear that instances ID2,ID3,ID5, and ID9 are ID1's nearest neighbors. However, since K is 3 in this case, we will only consider ID2,ID3, and ID5's labels: ID2: Soft contact ID3: Noncontact ID5: Noncontact Therefore, the assigned classification for ID1 will be "Noncontact" as this was the majority label/classification for ID1's K-nearest neighbors, which turns out to be the correct label after all. As K-NN is an instance-based learner, the algorithm does not need to be trained in the way a neural network or decision tree does, and can thus assign classifications for all 20 instances within this dataset using the algorithm I have outlined.
