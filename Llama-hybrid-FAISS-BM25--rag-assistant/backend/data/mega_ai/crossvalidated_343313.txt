[site]: crossvalidated
[post_id]: 343313
[parent_id]: 342219
[tags]: 
Given an intractable likelihood $f(x|\theta)$ of a generative model, associated with an observation $x^\text{obs}$, ABC produces simulations from the joint distribution $$\pi^\text{ABC}_{\epsilon}(\theta,z)\propto \pi(\theta)f(z|\theta) \mathbb{I}_{(0,\epsilon)}d(z,x^\text{obs})$$ or in the kernel generalisation (e.g., Fernhead and Prangle, 2012) $$\pi^\text{ABC}_{\epsilon}(\theta,z)\propto \pi(\theta)f(z|\theta) K(d(z,x^\text{obs})/\epsilon)$$ where $z$ denotes the pseudo-observation and $\epsilon$ the tolerance. There exist many ways of implementing this approach, but a core notion is that it always constitutes a simulation methodology on the parameter x pseudo-observation pair and hence that the parameter must be endowed with a probabilistic structure. Changing or updating the prior is of course feasible by different means, but I do not see a way to reinterpret the ABC output away from a Bayesian perspective. For instance, reweighing each point $\theta_i$ of an ABC sample by the inverse of the prior density $\pi(\theta_i)^{-1}$ means replacing the initial prior $\pi(\cdot)$ with a flat (and possibly improper) prior. But not leaving the Bayesian paradigm. As pointed out by Corey Yanofski in the above comment, a related resolution that does not involve a prior is indirect inference , where simulation is "only" used to find a closest value of the parameter.
