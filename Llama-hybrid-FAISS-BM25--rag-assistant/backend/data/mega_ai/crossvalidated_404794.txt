[site]: crossvalidated
[post_id]: 404794
[parent_id]: 404789
[tags]: 
The R function func2 produces the vector $$(\alpha,1-\alpha) \mathbf P^n$$ which is the marginal distribution of the Markov chain after $n$ steps, given that the initial state is generated with distribution $(\alpha,1-\alpha)$ . To generate the Markov chain, one needs to proceed one step at a time: generate $X_0$ equal to $1$ with probability $\alpha$ and $2$ with probability $1-\alpha$ generate $X_1$ which is equal to $2$ if $X_0=2$ and to $1$ with probability $1/2$ and $2$ with probability $1/2$ if $X_0=1$ generate $X_2$ which is equal to $2$ if $X_1=2$ and to $1$ with probability $1/2$ and $2$ with probability $1/2$ if $X_1=1$ generate $X_3$ which is equal to $2$ if $X_2=2$ and to $1$ with probability $1/2$ and $2$ with probability $1/2$ if $X_2=1$ generate $X_4$ which is equal to $2$ if $X_3=2$ and to $1$ with probability $1/2$ and $2$ with probability $1/2$ if $X_3=1$ generate $X_5$ which is equal to $2$ if $X_4=2$ and to $1$ with probability $1/2$ and $2$ with probability $1/2$ if $X_4=1$ Each transition can be simulated by a one-line code such as tranz leading to a five step Markov chain in a similarly easy coding such as marx Thus one recovers one realisation of the sequence $X_0,X_1,X_2,X_3,X_4,X_5$ . Rerunning the algorithm 99 times produces in total 100 realisations of the sequence $X_0,X_1,X_2,X_3,X_4,X_5$ . Not necessarily all different (actually certainly not all different!), but iid replicas of such realisations, from which $\mathbb P(X_1 = 1|X_0 = 1)$ and $\mathbb P(X_5 = 1|X_0 = 1)$ can be estimated by the Law of Large Numbers. Here is an illustration for 100 replications when $\alpha=.7$ : where the 6x100 values have been jittered and the curve corresponds to the 6 means of the simulated $X_t$ 's.
