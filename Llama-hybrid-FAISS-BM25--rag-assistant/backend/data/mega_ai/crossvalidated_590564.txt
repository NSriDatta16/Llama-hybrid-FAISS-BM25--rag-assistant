[site]: crossvalidated
[post_id]: 590564
[parent_id]: 590323
[tags]: 
A ReLU network is ultimately a piecewise-linear continuous function. Each neuron in the first hidden layer is just a shifted and scaled ReLU. Taking a linear combination of those produces a piecewise linear function, with (at most) as many hingepoints as there are neurons. Applying ReLU to that can create new hingepoints whenever the function crosses 0, but this is at most once per linear segment, so you end up with at most twice as many hingepoints as neurons. Having established that, the end behavior is linear. If the slope is nonzero, the function goes to infinity, and the supremum of errors is also infinite; if the slope is zero, the supremum of errors is at least 1. ... a shallow neural network Actually, this isn't an important assumption. Adding layers can increase the number of hinge points multiplicatively, but at the end you're still stuck with a finite number of hinges and hence a bounded good-estimation range.
