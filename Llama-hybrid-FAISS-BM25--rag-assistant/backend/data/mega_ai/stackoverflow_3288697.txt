[site]: stackoverflow
[post_id]: 3288697
[parent_id]: 3288662
[tags]: 
Stack Overflow presumably checks two things when deciding who gets access to the sitemaps: The USER_AGENT string The originating IP address both will probably be matched against a database of known legitimate bots. The USER_AGENT string is pretty easy to check in a server side language; it is also very easy to fake. More info: For how to check the USER_AGENT string Way to tell bots from human visitors? For instructions on IP checking Google: Google Webmaster Central: How to verify Googlebot Related: Allowing Google to bypass CAPTCHA verification - sensible or not?
