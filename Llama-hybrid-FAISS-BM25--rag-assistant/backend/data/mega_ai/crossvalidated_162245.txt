[site]: crossvalidated
[post_id]: 162245
[parent_id]: 
[tags]: 
Deriving Linear Regression update rule: Why do we set the derivative of cost funtion to 0 to minimize it?

(This question may be more about Math in general but I have a specific context in mind and hence am asking this here as opposed to math.stackexchange) I'm going through Andrew Ng's lecture notes on Machine Learning. While deriving the update rule for linear regression, the cost function's derivative is set to 0. I understand the cost function needs to be minimized. But setting the derivative as 0 means either you're considering the maxima, or the minima. So my question is: How can we be sure that setting the derivative to zero in this case will lead to considering the minima and not the maxima? (I'm not sure, but my guess is that it's because we already somehow know that the cost function is convex and will only have a minima and no maxima? In that case I guess I'd want to ask: a. How can you tell if the function is convex or concave? b. And also, if the function is neither convex nor concave, then how do we figure if setting the derivative to zero leads to a maxima or a minima?) EDIT: Another follow-up question. On the same notes, under 'Another algorithm for maximizing L(theta), we are introduced to Newton's method to find the minimum of a function. And this is used to find the minimum of the derivative of the likelihood. There's a self exercise in the notes that says this: Something to think about: How would this change if we wanted to use Newtonâ€™s method to minimize rather than maximize a function? And in the lecture here between (16:00 and 16:40) Prof. Ng mentions that this wouldn't change at all. So, my question remains, how would we know if we're considering the maximum of the function or the minimum? Thanks!
