[site]: crossvalidated
[post_id]: 372531
[parent_id]: 372471
[tags]: 
The Bayes factor $B^\pi_{01} (x)$ is, from a decision-theoretic point of view, under $a_0-a_1$ loss, that is, penalising the (wrong) decision of choosing $H_0$ when $H_1$ is true by $a_0$ , the (wrong) decision of choosing $H_1$ when $H_0$ is true by $a_1$ , and the (right) decision of choosing $H_i$ when $H_i$ is true by $0$ , completely equivalent to the posterior probability of the null hypothesis as $H_0$ is accepted when and only when \begin{equation} B^\pi_{01} (x) \ge {a_1\over a_0} \Big/ {\rho_0 \over \rho_1} = {a_1\rho_1 \over a_0\rho_0}, \end{equation} where the prior weights are defined as \begin{eqnarray} \rho_0 &=& \pi(\theta\in\Theta_0) \quad \hbox{ and } \nonumber\\ \rho_1 &=& \pi(\theta\in\Theta_1)=1-\rho_0. \end{eqnarray} Thus this is a Bayesian optimal decision, in the above decision theoretic sense, which should suffice by itself as a justification. In the case when $H_0$ and $H_1$ are made of a single parameter value each, the Bayes factor of course reduces to the likelihood ratio. Otherwise, it weights the likelihood according to the prior for each hypothesis, resulting in $$B^\pi_{01} (x)=\int_{\Theta_0} \pi_0 (\theta) f(x|\theta) d\theta \Bigg/ \int_{\Theta_1} \pi_1 (\theta) f(x|\theta) d\theta$$ as an averaged likelihood ratio (or rather a ratio of averaged likelihoods). This is coherent from a Bayesian view that all parameters are integrated out and weighted by the prior, much more that plugging in an estimate for each hypothesis since this is a random variable and its variability impacts the denominator differently from the numerator. It is with good reasons that the Bayes factor is seen as a modern version of Ockham's razor. More advanced arguments show that integrating out the parameter penalises the dimension of the hypothesis (or model) as in the BIC or Swartz' criterion, while taking the ratio of two likelihoods always favours the most complex of the two hypotheses (or models).
