[site]: crossvalidated
[post_id]: 104608
[parent_id]: 104582
[tags]: 
The best argument for your tree model would be that it makes better predictions. To show that, you would need training and test sets. Build a regression and a tree model from your training set, then see how well each one does on the test set. "Well" means "what proportion of items are correctly classified?". According to Leo Breiman , both logistic regression and classification trees can produce eccentric results when the number of predictor variables is large. He developed random forests to improve the stability of RT's. You might want to look at that option as well. If you find that logistic regression and RT's do just as well, then your best argument would be the interpretability of regression trees --- unless you are dealing with medical doctors, who find odds ratios intuitively appealing and are reluctant to consider anything else, in my experience.
