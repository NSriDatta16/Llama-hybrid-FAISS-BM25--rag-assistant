[site]: crossvalidated
[post_id]: 175774
[parent_id]: 175771
[tags]: 
A linear regression / classifier can absolutely be overfit if used without proper care. Here's a small example. Let's create two vectors, the first is simply $5000$ random coin flips: set.seed(154) N The second vector is $5000$ observations, each randomly assigned to one of $500$ random classes: N.classes There should be no relation between our flips y and our random classes rand.class , they were determined completely independently. Yet, if we attempt to predict the random flip with the random class using logistic regression (a linear classifier), it sure thinks there is a relationship M The true value of every one of these coefficients is zero. But as you can see, we have quite a spread. This linear classifier is for sure overfit. Note: The extremes in this histogram, where the coefficients have wandered to $-15$ and $15$, are cases where a class had either no observations with y == 1 or no values with y == 0 . The actual estimated values for these coefficients are plus and minus infinity, but the logistic regression algorithm is hard coded with a bound of $15$. "overfitting" does not seem to be formally defined. Why is that? Overfitting may be best understood within the context of a class of models which has some complexity parameter. In this case, a model could be said to be overfit when decreasing the complexity slightly results in better expected out of sample performance. It would be very difficult to precisely define the concept in a model independent way. A single model is just fit, you need something to compare it to for it to be over or under fit. In my example above this comparison was with the truth, but you usually don't know the truth, hence the model! Wouldn't some distance measure between training and test set performance allow for such a formalisation? There is such a concept, it's called the optimism. It's defined by: $$ \omega = E_{\text{test}} - E_{\text{train}} $$ where $E$ stands for error, and each term is averaged over all possible training and testing sets for your learning algorithm. It doesn't quite get at the essence of overfitting though, because the performance on a test set can be quite a bit worse than the train, even though a model of higher complexity decreases both .
