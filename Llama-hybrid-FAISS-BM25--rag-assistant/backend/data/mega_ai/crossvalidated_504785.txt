[site]: crossvalidated
[post_id]: 504785
[parent_id]: 504784
[tags]: 
I didn't come across with it anywhere to be honest. But, if you apply the activation function to the input layer, it'll correspond to a feature transformation, e.g. it's like taking the cube of all your features and inputting them to the neural network. It may not ruin your data always but note that neural nets typically benefit from zero-mean standardised inputs, because the hidden neurons tend to activate more in the linear region of sigmoid. And, that's useful for gradient descent.
