[site]: crossvalidated
[post_id]: 387547
[parent_id]: 
[tags]: 
Validation ROC AUC not improving with validation cross-entropy loss?

I am training a neural network that is doing binary image classification on several thousand images. I am running 5 fold cross validation (train on 4, validate on 1) with cross entropy (CE) loss. I am primarily concerned with optimizing ROC AUC but as this is non-differentiable, CE will do. Below is what I am seeing for ROC AUC (top) and CE loss (bottom). There are 10 lines on the plot, the 5 training plots are the smooth ones that are higher on the ROC AUC plot and lower on the loss plot, not surprising. What I am wondering, is why is the validation CE loss continuing to decrease while the validation ROCAUC loss is so unpredictable? Is there a way to smooth/decrease this validation ROC AUC loss variation? My possible explanations for the jumpy validation ROC AUC compared to mostly smooth validation CE loss: CE loss drives unsure predictions towards a prediction of 0.5 in order to minimize potential errors, as opposed to predicting like 0.2 or 0.8. The error of predicting .5 is about 50% as bad as incorrectly predicting 0.2 or 0.8, so that could be part of it. However, ROCAUC may not benefit from this optimization. To explain the validation ROC AUC random variation, perhaps there are some incorrectly classified examples deep into the wrong class hyperspace. This is leading to weird curves in the decision boundary. In other words overfitting? I don't think it's really overfitting though. The CE val loss continues to decrease for a long time. It could also be related to the fact that the validation set is fairly small and ROC AUC is a step function so it is inherently jumpy... Thank you!
