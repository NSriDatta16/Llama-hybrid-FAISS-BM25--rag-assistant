[site]: datascience
[post_id]: 126514
[parent_id]: 
[tags]: 
How the retriever model (Query encoder) is end-to-end trained in Retrieval Augmented Generation (RAG)?

RAG architecture from the original paper Since loss is calculated at the output layer of the generator, how the gradients are back propagated to the retriever model? Because the input to the Generator is pure text i.e. text of retrieved document + question. It states that the entire architecture is fine-tuned end-to-end. Notice in the figure "end-to-end backprop through q" where q is the Query Encoder. My question is how does error is back propagated to q? Because while calculating the loss at the output of Generator, the Query Encoder model plays no role.
