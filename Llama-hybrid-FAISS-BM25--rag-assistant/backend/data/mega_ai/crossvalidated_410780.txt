[site]: crossvalidated
[post_id]: 410780
[parent_id]: 410673
[tags]: 
In k-means, you can ignore the missing attributes both when computing the cluster assignment and when recomputing the cluster averages. But of course it only works for continuous data. In k-modes you could likely do the same - skip the missing value when finding the best assignment, as well as when finding the mode. Implementing them with support for NAs is a mess, and will make the code much slower, so people didn't do it. You'll have to do it yourself. There are a number of ugly corner cases, too: what if all points assigned to a cluster have NA in the same attribute? For many other methods you need to provide a distance function or matrix. So you "just" need to decide how to compute this. Beware that certain approaches (such as skipping attributes in the all-NA case) cause problematic points to become too central and violate the triangle inequality. Most clustering algorithms don't require the triangle inequality though. But imputation is an easy way that doesn't have this problem. Last but not least, you could do uncertain clustering . For points with missing values, you assume some prior distribution over the possible values, then do several runs to find the most likely assignments. Similarly, you could model a distribution over the distances. But this is quite complicated. The easiest way is to perform some kind of "random imputation" many times and find the consensus clustering.
