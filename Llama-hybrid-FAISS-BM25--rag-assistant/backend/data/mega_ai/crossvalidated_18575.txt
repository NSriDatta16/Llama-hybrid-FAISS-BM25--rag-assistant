[site]: crossvalidated
[post_id]: 18575
[parent_id]: 18566
[tags]: 
Not exactly a ready-made solution, and there are as always a lot of things not accounted for here (regarding the way the treatments were randomised over you subsets), but: Build a logistic regression model with dummies for treatments B and C (or any set of two that will make interpretation the easiest) and dummies for all but 1 of your subset indicators. If you have reason to think the treatments may work differently in the different subsets (and it is on the safe side to think so), also include the interactions. In R, this is done through mdl1 Then build a model not holding the treatments: mdl2 Now you can do a likelihood ratio test between the two models (check ?anova ) to see if there is a reason to include any of the treatment effects/interactions (perhaps you could do this in two steps to first try to eliminate the interactions). If you want to answer other research questions (e.g. differences per group), you need to compare other sets of models, but the idea is always the same: compare models with and without the variable that represents what you are interested in. When you do this repeatedly, you are indeed reusing the same data to some extent to check different questions, so you need to do some form of multiple testing correction. Depending on your specific research questions (and of your design), (orthogonal) contrasts may be of aid to you. If not, you can resort to Family Wide Error Rate (if your sample sizes are large enough, the power may still be OK) or, if this is an option, to False Discovery Rate. I repeat my warning that there are many issues with the methods presented here, but it should get you started.
