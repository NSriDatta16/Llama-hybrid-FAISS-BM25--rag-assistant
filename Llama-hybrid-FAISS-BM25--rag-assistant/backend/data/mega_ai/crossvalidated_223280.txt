[site]: crossvalidated
[post_id]: 223280
[parent_id]: 
[tags]: 
neural network for data set with large number of samples

What are the rules of thumb for neural network configurations with large number of samples? I have a dataset with 200k samples, 400 features, and binary label classification problem. How should I choose the hidden layer size? I'm concerned that during training the network will gradually forget the older samples, and will overfit over the samples fed to it recently. How can I make sure this doesn't happen, and the network nicely generalizes over entire data set? Particularly I'm interested in choosing the right number of hidden layers (probably 1 is enough), number of hidden nodes, minibatch size, etc.
