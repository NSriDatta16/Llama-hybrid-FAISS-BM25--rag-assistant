[site]: crossvalidated
[post_id]: 628212
[parent_id]: 628173
[tags]: 
For a Bayesian, the prior describes his knowledge of the "fairness property" of the coin. That knowledge can be vague if the coin is new and unfamiliar, or very precise if he has thrown the specific coin or an identical one in the past. As the coin gets flipped more and more, your knowledge of its property converges to a point. This can probably best be understood with the Beta-binomial model , where your prior of $p$ is $$\pi(p) = \frac{1}{B(\alpha,\beta)} p^{\alpha-1}(1-p)^{\beta-1}.$$ After tossing the coin $n$ times and seeing $x$ heads, your posterior is $$\pi(p|n,x) = \frac{1}{B(\alpha+x,\beta+n-x)} p^{\alpha+x-1}(1-p)^{\beta+(n-x)-1}$$ The larger $n$ and $x$ get, the tighter $\pi(\cdot)$ will wrap around a specific value of $p$ , which is to say your uncertainty regarding the fairness of the coin shrinks. What this also shows, though, is that $\alpha$ and $\beta$ are essentially a reflection of past tosses. A Bayesian being sure that the coin is fair is equivalent of him being "familiar" with the coin, which means $\alpha$ and $\beta$ are very large. Then, $\pi(p)$ will have converged to a Gaussian and ultimately to a (Dirac) point mass at $p=\frac{1}{2}$ .
