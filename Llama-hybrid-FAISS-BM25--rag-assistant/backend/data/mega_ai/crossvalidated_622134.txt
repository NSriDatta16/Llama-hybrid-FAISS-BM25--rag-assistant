[site]: crossvalidated
[post_id]: 622134
[parent_id]: 622101
[tags]: 
does data preprocessing (handling null values, standardization and so on..) comes before data exploration? If you don't know (yet) anything about your data, how would you know what preprocessing it needs? To know if and what kind of handling of missing values is needed, you need to know first if there are missing values in the data, how many there are, if there are any patterns in the missingness, etc. You learn all of this through exploratory data analysis. Before you do exploratory data analysis you cannot do anything with your data unless your job is only sending the data to some kind of AutoML software that will do the job, but without exploratory data analysis, you wouldn't even know how good or bad your data was, hence if the results can be trusted at all. while doing data exploration, how do I gain insight that will assist me for model selection? what insights for example helps me to decide whether to use KNN or SVM? I would appreciate if someone has some sources/articles on this matter. Sometimes yes, sometimes no. To make use of EDA in the choice of model candidates you additionally need an in-depth understanding of different machine learning models and their pros and cons in different scenarios. EDA will inform such decisions. For example, when you see that your features are strongly correlated you know that you need to use a model that can deal with it and does some kind of regularization. If during EDA you will learn that the only useful features in the data are categorical and binary, you know that you could limit yourself to models designed especially for such data (e.g. LCA for clustering). Moreover, if we are talking about machine learning models like $k$ NN vs SVD, in most of the cases what you would do is you would run both and check which did better on the test set. I'm not suggesting here a shotgun approach, where you would try all the models available in scikit-learn and pick the best one. In many cases, there would be the "usual suspects", e.g. if you need a cheap and simple model for tabular data, you could pick logistic regression, for a more expensive model you could try something like XGBoost, if you need a cheap model for NLP problem, naive Bayes would be a good start, and for a more sophisticated model, you would usually look at deep learning, etc. The choice of the models would depend not only on EDA but also on external limitations (if you don't have the budget for cloud computing, you would not use deep learning), your experience, and other factors. Another thing that you could, and should, do during EDA is to try to "manually classify" the data. If you can easily create good classification rules by hand only after looking at the plots and statistics, you would be inclined to put more effort into trying simple models like decision trees or linear regression. If it is hard to find any reasonable patterns in the data, you also wouldn't be surprised when the simple models would not perform great. On another hand, it would tell you that you should pay attention to overfitting--there are no obvious patterns in the data, so if the model performs too well, it's suspicious. So the better you know your data, the more informed decisions you will make.
