[site]: datascience
[post_id]: 103778
[parent_id]: 103713
[tags]: 
Dense layer | Feature in-dependency There are a couple of things that may be going on in your experiment. First of all, it may as well be that the particular keyword you are seeing to play the most important role is intrinsically correlated with a correct prediction. That aside, by feeding the network words in the form of a dense layer, these words are introduced with no intrinsic inter-dependency and are treated as orthogonal in the forward pass. RNN layer | Feature step-wise dependency force it to consider entire sentence I think what you may be saying is that, some there is some "sense" in the ordering or temporal structure of your text, and that you would like to impose that as an intrinsic characteristic of your data as it is being passed over to your network. The above is exactly what a recurrent layer is meant to be doing: impose a step-wise dependency between the words/features as they take place in time and only allow your data to be seen in this particular recurrence structure. In this way, a fixed-length representation of your target/input sequence is downprojected into the hidden state vector of the last timestep of your sequence. Your model will get adjusted based on this particular last-hidden-state distillation of your input texts, and thus enforced to look at the entirety of sentences by proxy. If you try this, beware of vanishing gradients ;)
