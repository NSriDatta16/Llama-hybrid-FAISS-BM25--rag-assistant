[site]: crossvalidated
[post_id]: 264578
[parent_id]: 264567
[tags]: 
"Systematic departures from hypotheses" in this context means we have some hypothesis but in gathering data, we observe consistent evidence that this is not true. The word systematic here implies that we have enough evidence to conclude what we observed was not just randomness. For example, suppose we have a coin and our hypothesis is that it is fair (P(heads) = 0.5). If we flipped a coin 20 times and got 11 heads, the MLE estimate of the probability of heads is 0.55, which is different than our hypothesis of a fair coin (0.5). But this could easily be observed due to the randomness of a fair coin, so we may be hesitant to conclude that it's not fair. But if we observe 19 / 20 heads, using the article's language, we would say we observe a systematic difference. I'll also comment that in this article, this distinction between data science and statistics is a bit misleading. This paragraph seems to imply that statistics stops at hypothesis testing while data science takes the next step of inferring the consequences of departures from the null. On the contrary, I would say it irks statisticians to no end that in non-statistical scientific literature, the use of statistics often stops at hypothesis testing. So while it's true that many users of statistics may stop at hypothesis testing, statistics does not. This is similar to going into a 2nd grade class and saying "the problem with the English language is that it only allows up to 5 letter words". EDIT: The OP asked why the author of the article stated that data science focuses on the implications of departures from the null, while traditional statistics does not. I think we're all on the same page that statistical methods do study the implications of departures from the null. But the point that the article author is trying to make, and is worth discussing, is that in many non-statistical scientific articles, the use of statistics does stop at testing the null hypothesis (i.e. "women prefer apples over oranges more frequently than men (p = 0.02)" or something silly like that). In contrast, most people who can call themselves data scientists are actually tasked with going beyond this (i.e. "women prefer apples over oranges more frequently than men (p = 0.02). Based on our estimates of differences of preferences, targeting apple marketing specifically towards women should increase revenue by 0.010% $\pm$ 0.004%"). Just looking at the p-value alone only tells you the strength of the evidence against the null in your data set. But just looking a p-value and deciding that it's low enough to reject the null tells you very little about the consequences of departure from the null! In my silly example, looking at the prediction interval for differences of the two marketing strategies studies the implications of departures from the null. For the record, there is a very strong movement in the non-statistical scientific community to demand more from statistical analyses than just p-values. But this movement has not yet reached all its goals yet.
