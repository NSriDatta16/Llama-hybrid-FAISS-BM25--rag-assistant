[site]: stackoverflow
[post_id]: 2229818
[parent_id]: 
[tags]: 
Need advice in Efficiency: Scanning 2 very large files worth of information

I have a relatively strange question. I have a file that is 6 gigabytes long. What I need to do, is scan the entire file, line by line, and determine all rows that match an id number of any other row in the file. Essentially, its like analyzing a web log file where there are many session ids that are organized by the time of each click rather than by userID. I tried to do the simple (dumb) thing, which was to create 2 file readers. One that scans the file line by line getting the userID, and the next to 1. verify that the userID has not been processed already and 2. If it hasn't been processed, read every line that begins with the userID that is contained in the file and store (some value X, related to the rows) Any advice or tips on how I can make this process work more efficiently?
