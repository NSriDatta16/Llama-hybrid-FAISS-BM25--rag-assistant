[site]: crossvalidated
[post_id]: 97932
[parent_id]: 97926
[tags]: 
My understanding is that this is an active area of research in the machine learning community and there are no great answers, but rather a large and growing number of potential solutions. You're likely going to get better answers if you specify the specific algorithms you're considering. If you're using a parametric model (logistic regression) this should be less of an issue and you can just vary the threshold based on your loss function (cost of false negatives to false positives) If you're using machine learning algorithms this might be trickier. Max Kuhn does a fair attempt at summarizing the issue in Chapter 16 of "Applied Predictive Modeling". But challenging topic to summarize. If you don't want to buy the book, R code is available in the AppliedPredictiveModeling package for this chapter and may be sufficient depending on your familiarity with R and the algorithms used. Usually the discussing revolves around undersampling/oversampling +/- cost-sensitive algorithms. With variations like jous-boost also possible. An example of this sort of discussion: Chen et al "Using Random Forest to Learn Imbalanced Data" http://statistics.berkeley.edu/sites/default/files/tech-reports/666.pdf
