[site]: crossvalidated
[post_id]: 535082
[parent_id]: 
[tags]: 
Neural network initialization behavior: low initial loss that spikes, then slowly decreases

The embedded image is the training loss of my inception V3 architecture that I am training from scratch using the sigmoid cross-entropy loss function and the Tensorflow RMSProp optimizer with two classes in my data. The x axis is batch index. I am initializing my inception V3 using tf.keras.applications.inception_v3.InceptionV3 (with random weights). However, I have also seen this loss curve behavior in other architectures of neural network with random weight initialization, Tensorflow versions, and datasets. My question is why the loss starts out so low, and then spikes up only to decrease again? Wouldn't this mean that the weights are improperly initialized? Update: the loss is spiking at every start of an epoch of my dataset. This must be my user error. My dataset is well-shuffled. Does anyone know what I'm doing wrong here?
