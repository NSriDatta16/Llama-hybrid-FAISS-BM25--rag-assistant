[site]: crossvalidated
[post_id]: 158590
[parent_id]: 158583
[tags]: 
A decision tree works by recursive partition of the training set. Every node $t$ of a decision tree is associated with a set of $n_t$ data points from the training set: You might find the parameter nodesize in some random forests packages, e.g. R : This is the minimum node size , in the example above the minimum node size is 10. The minimum node size is a single value: e.g. 10. If splitting a node generates two nodes for which one is smaller than nodesize then the node is not split, and it becomes a leaf node. It is actually a stopping criterion. This parameter implicitly sets the depth of your trees. Setting this number to bigger values causes smaller trees to be grown (and thus take less time). Note that the default values are different for classification (default 1 in R) and regression (default 5 in R). In other packages you directly find the parameter depth , e.g. WEKA : -depth from WEKA random forest package The maximum depth of the trees, 0 for unlimited. (default 0)
