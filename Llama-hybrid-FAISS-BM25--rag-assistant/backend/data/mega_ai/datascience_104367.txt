[site]: datascience
[post_id]: 104367
[parent_id]: 66319
[tags]: 
One option is to train a single embedding space with all the information. If you use Word2Vec in Genism, positive and negative operations are built-in. That is similar to how word analogies are calculated. The code would be something like: import gensim word2vec_model = gensim.models.Word2Vec(docs) word2vec_model.most_similar(positive=['Total War', 'WARHAMMER'], negative=['Fantasy'])
