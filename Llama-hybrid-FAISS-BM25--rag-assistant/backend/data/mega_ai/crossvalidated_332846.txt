[site]: crossvalidated
[post_id]: 332846
[parent_id]: 332833
[tags]: 
It may also mean that your error model is wrong - maybe the assumption of a normally distributed error is not warranted? (hard to guess without knowing more about the data) You can also try to fit a hierarchical/mixed effects model with a per-subject random effect (intercept) this would look something like: $Y_i \sim N(\mu_i,\sigma)$ $\mu_i = \beta X + \alpha_i$ $\alpha_i \sim N(\gamma, \tau)$ plus some priors on all parameters to have good regularization. Here $\alpha_i$ is the subject-specific intercept. In this context, the single outlier will more influence the corresponding $\alpha_i$ and the between-subject variability $\tau$ and will have smaller effect on both the average intercept $\gamma$ and the linear coefficients $\beta$. Note that $\gamma$ and $\tau$ bind all the $\alpha_i$ together and thus the model will not overfit even though we have more parameters than data points. Such models are best fit with INLA or rstanarm (both provide fully Bayesian treatment and will give you correct uncertainty estimates for all parameters). I would expect hierarchical model with those packages to give similar results regardless of inclusion of the outlier. If you want to stay within the frequentist framework, you may try the package lme4 , but I've heard its estimates can be unstable with few observations (I have no personal experience with lme4, but have good experience with both INLA and rstanarm).
