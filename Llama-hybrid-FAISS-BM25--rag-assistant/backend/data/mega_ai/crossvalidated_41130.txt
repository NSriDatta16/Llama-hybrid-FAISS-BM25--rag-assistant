[site]: crossvalidated
[post_id]: 41130
[parent_id]: 40808
[tags]: 
Is a strong background in maths a total requisite for ML? – an answer and some speculation for ML conceptualized as being statistics ;-) Around 1990 I had hopes for computer algebra being of assistance, I think it is but it is fairly limited. But it certainly helps with speeding up the learning of math (less need to develope manipulatory skills by practice or try to get by with just being able to do the simple exercises). I found Fred Szabo's Linear Algebra with Mathematica an excellent example of this (but I had already taken an advanced theory level linear algebra course.) I have been working since 1988 (Utilizing Computer Intensive Methods to "Concretize" Theorems and Principles from Statistics – Precisely) to make the answer no or at least not necessary (for statistics). One will always be able to understand more quickly and more generally with additional mathematical skill and understanding. I think I am starting to get close, but one needs a manipulate-able representation of probability generating models and inference that is valid and useful for more than just toy problems. Should I try and fill in the blanks of my maths before continuing with ML? That’s a hard endeavour – in MHO almost everyone who understands statistics got there by being very comfortable manipulating the standard and especially not so standard mathematical representations of probability generating models and mathematical characterizations of inference (the top x% of mathematical statistics Phds). So it’s not just getting the basics but being real comfortable with the math. (As an aside, for me Fourier Theory was essential.) Why are these representations hard (even with lots of math)? Gerd Gigerenzer has pretty much established that there is no challenge with the simple disease positive/negative given test positive/negative problem using _natural frequencies”. A reference from the linked question seems to make good use of that http://www.autonlab.org/tutorials/prob18.pdf Why is this hard to generalize? For k tests (repeated and or different ) – 2^k For tests that take v values – v^k So for binary unknown – 2 * v^k sample path probabilities For p multiple binary unknowns 2^p * v^k For p multiple rational unknowns Q^p * v^k One quickly moves to math with countable and uncountable infinities to cope with this, which even with mathematical expertise leads to many misunderstandings and seeming paradoxes (e.g. Borel’s paradox?) Additionally there is linear to non-linear hazardous misunderstandings (e.g. Hidden Dangers of Specifying Noninformative Priors Winbugs and other MCMC without information for prior distribution ) and interactions and random effects, etc.
