[site]: crossvalidated
[post_id]: 509859
[parent_id]: 
[tags]: 
Joining train and test dataset when preprocessing data with encoders

So I've been looking at several beginner Kaggle kernels for the 'Titanic - Machine Learning from Disaster' competition. In these kernels, I noticed that sometimes they combine train and test data, often using pd.concat() , when they are preprocessing data such as using encoders to fit and transform. ( https://www.kaggle.com/jeffd23/scikit-learn-ml-from-start-to-finish ) In this case : data_train = pd.read_csv('../input/train.csv') data_test = pd.read_csv('../input/test.csv') def encode_features(df_train, df_test): features = ['Fare', 'Cabin', 'Age', 'Sex', 'Lname', 'NamePrefix'] df_combined = pd.concat([df_train[features], df_test[features]]) for feature in features: le = preprocessing.LabelEncoder() le = le.fit(df_combined[feature]) df_train[feature] = le.transform(df_train[feature]) df_test[feature] = le.transform(df_test[feature]) return df_train, df_test data_train, data_test = encode_features(data_train, data_test) Specifically, they would fit the encoder using the combined dataset but transform train/test independently. In other cases, the encoder would only be fit using train data but transform would still be done independently on both train and test set. In both cases, they would then separate features and labels from the training dataset and use train_test_split(). I am confused what the exact difference these two approaches make, and why and when would one choose a certain method over the other. Some notebook(Titanic Top 4% with ensemble modeling) say they combined test and train set " to obtain the same number of features during categorical conversion, " but I have yet to figure it out what that means. This is my first question here so I'm not sure if I've worded the question properly or have provided enough context, but I would really appreciate it if anyone could help me understand this material easier!
