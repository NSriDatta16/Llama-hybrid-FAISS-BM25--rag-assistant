[site]: crossvalidated
[post_id]: 497616
[parent_id]: 497352
[tags]: 
The reason is not that computing $p(e|f)$ would not be possible. The reason for the factorization is that you want to bring in the language mode $p(e)$ that can be used in decoding. At the time, SMT was invented, $n$ -gram language models were quite good and it would be a pity not to use them. The probability $p(f|e)$ is factorized over phrases (in fact frequent n-grams). The phrase pairs and their probabilities are stored in a so-called phrase table. The phrase table is obtained by computing word alignment on the parallel corpus (which gives you probability scores of words being aligned) and joining the words into more common n-grams. So, at this stage, the difference between $p(e|f)$ and $p(f|e)$ is how you normalize the scores. Computing the conditional probability in this way tells you how to score a sentence pair (look-up phrases in the tables and multiply their scores), but it does not really tell you how to decode a sentence. This is why the language model comes into play. For a source sentence, you pick up some candidate phrases (scored with the reversed probability direction) and using the language model scores, you try to build a sentence as fluent as possible.
