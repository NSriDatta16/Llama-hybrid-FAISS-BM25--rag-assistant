[site]: crossvalidated
[post_id]: 502821
[parent_id]: 
[tags]: 
No Activation Function on Output Layer for Binary Classification

In this pytorch example , the output layer does not have an activation function even though the neural network is being used for a binary classification task (i.e. ground truth values are either 0 = negative or 1 = positive). After inspecting the output, I can see that there are values such as -13.02 or 4.56, which are obviously not bounded between 0 and 1. Also, after adding a sigmoid activation function in myself, the performance seems to be worse than without the activation function. Thus, I have two questions: Would 0 be the threshold that determines if the predicted output is the negative or positive class? In other words, is any output value $\hat{y}$ , $ \hat{y} is negative and $\hat{y}>= 0 $ is positive? Why does not using an activation function lead to better performance when this is a binary classification problem, not a regression problem? Is it specific to this example?
