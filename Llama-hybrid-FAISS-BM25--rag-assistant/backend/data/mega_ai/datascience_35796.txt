[site]: datascience
[post_id]: 35796
[parent_id]: 35771
[tags]: 
A very straightforward explanation is they can reduce both bias and variance to limit overfitting . Below points from that link: Reduce bias , each model learns something slightly different and by taking an ensemble, can cancel out any crazy/spurious things one model learns. Reduce variance , each model is suscptible to slightly different noise, and so by taking an ensemble you can diversify away the noise. Reduce overfitting , it is harder for an ensemble to overfit, since they don't necessarily all see the same data and the individual models learn different things Search around for "ensemble techniques", which covers this sort of thing. Random Forests are probably the best known kind.
