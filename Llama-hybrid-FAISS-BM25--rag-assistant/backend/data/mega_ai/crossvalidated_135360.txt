[site]: crossvalidated
[post_id]: 135360
[parent_id]: 135351
[tags]: 
If we take a frequentist decision-theoretic approach to the problem, it means evaluating estimators of $\mu$ by the zero-one loss function$$L(\mu,\hat\mu)=\mathbb{I}_{|\mu-\hat\mu|>L}=\begin{cases}1 &\text{if }|\mu-\hat\mu|>L\\0 &\text{if }|\mu-\hat\mu|\le L\\\end{cases}$$In the normal setting assumed here, sufficiency arguments lead to consider a single observation $X\sim\mathcal{N}(\mu,1)$. The estimator $\hat{\mu}_0(x)=x$ has a constant frequentist risk:$$R(\hat{\mu}_0,\mu)=\mathbb{P}_\mu(|\mu-X|>L)=\mathbb{P}_{\mu=0}(|X|>L)$$ If we can establish $\hat{\mu}_0$ is a Bayes estimator, then it will be minimax. This estimator is associated with the flat prior, since it is easy to show that the Bayes estimator associated with a posterior $\pi(\mu|x)$ satisfies$$\pi(\hat{\mu}+L|x)=\pi(\hat{\mu}-L|x)$$which means that the posterior density is the same on both values (and requires the posterior to be unimodal, I believe). When using the flat prior, $\pi(\mu|x)=\varphi(\mu-x)$, which leads to $\hat{\mu}^\pi(x)=x=\hat{\mu}_0(x)$. The sample mean is therefore a limit of Bayes estimators and the standard proof of minimaxity under quadratic loss extends to this case with the same type of arguments, namely to consider the sequence of normal priors $\mathcal{N}(0,n)$ denormalised by $\sqrt{n}$. See, e.g., my book, The Bayesian Choice, Chapter 2 .
