[site]: crossvalidated
[post_id]: 493291
[parent_id]: 
[tags]: 
Noise in regression problems and ways to reduce it

In the theory of bias-variance decomposition for regression problems ( this page is a very nice reference on this theory) the noise is defined as $$\mathrm{Noise} = \mathrm{E}_{X,Y}[(Y - \mathrm{E}[Y|X])^2],$$ where $(X,Y)$ is a pair of random variables taken from the given distribution $p$ on $\mathcal{X}\times\mathcal{Y}$ (here $\mathcal{X}$ is a feature space and $\mathcal{Y}$ is a label space). We assume that all observations are generated by $p$ in our regression problem. How can we reduce the noise? It is easy to show that the noise is just a conditional variance $\mathrm{Var}(Y|X)$ , averaged on $X$ : $$\mathrm{Noise} = \mathrm{E}_{X,Y}[(Y - \mathrm{E}[Y|X])^2] = \mathrm{E}_{X}\mathrm{E}_{Y|X}[(Y - \mathrm{E}[Y|X])^2|X] = \mathrm{E}_{X}\mathrm{Var}(Y|X).$$ So, we should decrease this conditional variance (this will lead to noise reduction). Obviously, for a fixed distribution $p(x, y)$ , the noise is a constant. This means that if someone provides us with the data and prohibits changing it in any way, we simply can't do anything with the noise. But if we somehow change our features or labels (for example, generate some new features from the old ones, this is a typical data mining process), we will implicitly move from the original distribution $p(x,y)$ on $\mathcal{X} \times \mathcal{Y}$ to new distribution $\tilde{p}(\tilde{x},\tilde{y})$ on $\tilde{\mathcal{X}} \times \tilde{\mathcal{Y}}$ . My question is â€“ what are the ways to move to this new distribution, so that its noise will be lower that the noise of original distributiion $p(x, y)$ . I see two main ways to do this: Move to new feature space $\mathcal{X}'$ and new label space $\mathcal{Y}'$ , in which the same features and labels are measured more precisely than in original $\mathcal{X}$ and $\mathcal{Y}$ respectively. This is not always possible. ( If features in $\mathcal{X}$ and/or labels in $\mathcal{Y}$ are measured very inaccurately, then $X$ and $Y$ will be approximately independent, hence $\mathrm{E}[Y|X] \approx \mathrm{E}[Y]$ (where $\mathrm{E}[Y]$ is a population average of $Y$ ) and noise will be large. ) Move to new, more rich feature space $\tilde{\mathcal{X}}$ , which contains more important features than the original feature space $\mathcal{X}$ . This is also not always possible. Are there any other ways to decrease noise in regression problems? Note. In many ML texts (for example in the "Elements of Statistical Learning") formula for the noise is given for a fixed $X=x_0$ , i.e. $\mathrm{Noise} = \mathrm{E}_{Y}[(Y - \mathrm{E}[Y|x_0])^2|x_0] = \mathrm{Var}(Y|x_0)$ . But in my formulas above I didn't fix $X$ (i.e. I considered more general case).
