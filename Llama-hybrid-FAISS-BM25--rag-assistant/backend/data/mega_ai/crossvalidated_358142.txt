[site]: crossvalidated
[post_id]: 358142
[parent_id]: 
[tags]: 
What is the interest to put more cells than the sequence length in LSTM?

Recently, I dug into RNN and LSTM and, if I understood well, the memory is passing through the network. So I wonder why in some architectures, the number of cells in the first LSTM layer is higher than the length sequence ? Would that mean that the "exceeding cells" (w.r.t. to the sequence length) have no other input than the memory of the previous cell (but no "data" input) ? According to me, the only effect of adding cells (or a second layer) would thus be to increase non-linearity and "combinations" but there is no improvement in terms of memory and "temporal knowledge"... Am I right ?
