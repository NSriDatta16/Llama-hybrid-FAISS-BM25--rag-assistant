[site]: crossvalidated
[post_id]: 342874
[parent_id]: 342838
[tags]: 
It seems to me that the time-series are loading on a common trend somehow, and this is generating this behavior in the correlations. If your time series A and B share a common trend, their absolute values will be highly correlated (decreasing with the variance of the individual series' noise term). However, once you differentiate the series, the only correlation that remains is in the noise term, which will be low if the noise terms are iid. This should expain (1). A simple code in R shows what I mean: x (2) is a little harder (and maybe farfetched, but I dont know your data and all I am saying is that this is a plausible explanation - its likelihood depends on your data) If D,C load on a trend with opposing signs but they load on a correlated noise term with the same sign, then this pattern could be observed as well. A more realistic pattern would be if they would each have a random iid noise term. This would generate high correlations in returns - due to the correlated noise term and low correlation in absolute value - due to the trend with opposing signs. More code to exemplify: rnd You asked for an explanation - I gave an example of how it can happen. Not sure if it answers your question, but I hope it helps you pinning down what may be the case.
