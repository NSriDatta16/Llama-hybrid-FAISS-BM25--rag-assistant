[site]: datascience
[post_id]: 85758
[parent_id]: 85731
[tags]: 
If your algorithm is based on gradient descent optimization, you can use embeddings, which are dense representation spaces for discrete elements. Embeddings are supported by most deep learning frameworks such as pytorch or tensorflow. Update: the fact that you want to have multiple discrete values does not prevent the possibility of using embeddings: you can just add all vectors together in a single value. The most straightforward approach for this would be to have a constant length for the list (equal to the maximum number of elements in all lists, or a sensible maximum value), filling with "padding" items the positions that are not needed. If you want to take the sequential appearance of the elements into account, instead of adding the vectors together you could apply convolutional layers or an LSTM over the embedded vectors.
