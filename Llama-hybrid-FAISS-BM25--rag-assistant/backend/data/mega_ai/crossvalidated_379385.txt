[site]: crossvalidated
[post_id]: 379385
[parent_id]: 379383
[tags]: 
What the author is getting at is that Naive Bayes implicitly treats all features as being independent of one another, and therefore the sorts of curse-of-dimensionality problems which typically rear their head when dealing with high-dimensional data do not apply. If your data has $k$ dimensions, then a fully general ML algorithm which attempts to learn all possible correlations between these features has to deal with $2^k$ possible feature interactions, and therefore needs on the order of $2^k$ many data points to be performant. However because Naive Bayes assumes independence between features, it only needs on the order of $k$ many data points, exponentially fewer. However this comes at the cost of only being able to capture much simpler mappings between the input variables and the output class, and as such Naive Bayes could never compete with something like a large neural network trained on a large dataset when it comes to tasks like image recognition, although it might perform better on very small datasets.
