[site]: datascience
[post_id]: 115134
[parent_id]: 90870
[tags]: 
"The parameter sharing used in recurrent networks relies on the assumption that the same parameters can be used for different time-steps For each time-step, the hidden state i.e., " memory " of the RNN is calculated as $$ h_t = f(U * x_t + W * h_{tâˆ’1})$$ For $n$ tokens in our input sequence that would look like: ## Pseudocode for simple rnn initialise(hidden, U, W, V) # forward pass for t in range(sequence_length): hidden[n] = rnn_forward(x[t], W, U, hidden[n-1]) # backpropagation tt for t in range(sequence_length): dW_t, dU_t = rnn_backward(hidden[-t], U, W) dW += dW_t dU += dU_t optimizer.step(dW, dU) What you can see in the process above is that the RNN layer shares the same parameters $(U, V, W)$ across all steps . It may help to think the RNN in its unrolled version (figure below). This reflects the fact that we are performing the same task at each step inside a for loop, just with different inputs. This greatly reduces the total number of parameters we need to learn. Anyway, hope this helps. References: https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-1/ https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html
