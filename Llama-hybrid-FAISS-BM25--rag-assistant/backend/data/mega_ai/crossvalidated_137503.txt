[site]: crossvalidated
[post_id]: 137503
[parent_id]: 137481
[tags]: 
The bias you are talking about is still mainly connected to overfitting. You can keep the risk low by evaluating only very few models for fixing the regularization hyperparameter plus going for a low complexity within the plausible choice. As @MarcClaesen points out, you have the learning curve working for you, which will somewhat mitigate the bias. But the learning curve is typically steep only for very few cases, and then also overfitting is much more of a problem. In the end, I'd expect the bias to depend much on the data (it's hard to overfit a univariate problem...) and your experience and modeling behaviour: I think it is possible that you'd decide on a roughly appropriate complexity for your model if you have enough experience with both the type of model and the application and if you are extremely well behaved and do not yield to the temptation for more complex models. But of course, we don't know you and therefore cannot judge how conservative your modeling is. Also, admitting that your fancy statistical model is highly subjective and you don't have cases left to do a validation is typically not what you want. (Not even in situations where the overall outcome is expected to be better.) I don't use LASSO (as variable selection does not make much sense for my data for physical reasons), but PCA or PLS usually work well. A ridge would be an alternative that is close to LASSO and more appropriate for the kind of data. With these data I have seen an order of magnitude more misclassifications on the "shortcut-validation" vs. proper independent (outer) cross validation. In these extreme situations, however, my experience says that the shortcut-validation looked suspiciously good, e.g. 2 % misclassifications => 20 % with proper cross validation. I cannot give you real numbers that directly apply to your question, though: So far, I did care more about other types of "shortcuts" that happen in my field and lead to data leaks, e.g. cross validating spectra instead of patients (huge bias! I can show you 10% misclassification -> 70% = guessing among 3 classes), or not including the PCA in the cross validation (2 - 5% -> 20 - 30%). In situations where I have to decide whether the one cross validation I can afford should be spent on model optimization or on validation, I always decide for validation and fix the complexity parameter by experience. PCA and PLS work well as regularization techniques is that respect because the complexity parameter (# components) is directly related to physical/chemical properties of the problem (e.g. I may have a good guess how many chemically different substance groups I expect to matter). Also, for physico-chemical reasons I know that the components should look somewhat like spectra and if they are noisy, I'm overfitting. But experience may also be optimizing model complexity on an old data set from a previous experiment that is similar enough in general to justify transferring hyperparameters and then just use the regularization parameter for the new data. That way, I cannot claim to have the optimal model, but I can claim to have reasonable estimate of the performance I can get. And with the patient number I have, it is anyways impossible to do statistically meaningful model comparisons (remember, my total patient number is below the recommended sample size for estimating a single proportion [according to the rule of thumb @FrankHarrell gives here]). Why don't you run some simulations that are as close as possible to your data and let us know what happens? About my data: I work with spectroscopic data. Data sets are typically wide: a few tens of independent cases (patients; though typically lots of measurements per case. Ca. 10Â³ variates in the raw data, which I may be able to reduce to say 250 by applying domain knowledge to cut uninformative areas out of my spectra and to reduce spectral resolution.
