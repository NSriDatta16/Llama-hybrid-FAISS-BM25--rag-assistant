[site]: crossvalidated
[post_id]: 220547
[parent_id]: 149231
[tags]: 
I agree with the other answers that usually, the reward function is not learned directly. The transition probabilities also don't have to be learned. The agent can learn directly the action values, or even directly the policy, with policy gradien method for instance. There are, however techniques for which the reward and the transition probabilities have to be learned. For example, the dyna-Q algorithm (described in Sutton & Barto) maintains a model of the environment. At each time step, the agent uses the reward and state information received from the environment to update the action-values, just like in Q-learning for instance. But it also update its own model of the environment, and then performs N other action-values updates based on that model. The supposition is that the acting->sensing loop takes some non-null time, time that we can put to good use by improving the action-values with simulated samples.
