[site]: crossvalidated
[post_id]: 551055
[parent_id]: 551031
[tags]: 
Take as an example the simple beta-binomial model, where we observed $k$ successes in $n$ trials and want to learn the probability of success $p$ . We start with a beta prior for $p$ . $$\begin{align} p &\sim \mathsf{Beta}(\alpha, \beta) \\ k &\sim \mathsf{Binomial}(n, p) \end{align}$$ in such a case, the posterior distribution would be $$ p|k,n \sim \mathsf{Beta}(\alpha + k, \, \beta + n - k) $$ with the expected value $$ E[p] = \frac{\alpha + k}{\alpha + \beta + n} $$ Notice that in such a case the $\alpha$ and $\beta$ hyperparameters can be thought as pseudocounts , so before seeing the data we assume to have observed $\alpha$ successes and $\beta$ failures. We are augmenting the data with the pseudocounts. While it may be less obvious, the same thing happens when you use other Bayesian models. By assuming priors you start with pre-calculated parameters that are updated by applying Bayes theorem. The more data you gather, the less impactful the priors would be. If you have good prior knowledge, it may help with overcoming some drawbacks of the data. On another hand, as noticed in comments by @Dave, if you start with priors that are wrong, they would also bias your estimates.
