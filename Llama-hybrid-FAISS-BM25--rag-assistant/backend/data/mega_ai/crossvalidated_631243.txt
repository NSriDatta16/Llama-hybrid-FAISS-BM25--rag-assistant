[site]: crossvalidated
[post_id]: 631243
[parent_id]: 464201
[tags]: 
In my opinion, why the chosen token is not always replaced by the special token [mask] is the following: suppose the token x, and its context variable denoted c(x); BERT model tries to learn the function f(x, c(x)) to get a context-sensitive representation of token x; But if the selected token is 100% replaced by the token [mask], then the input x is always [mask], making the input x redundant for learning.
