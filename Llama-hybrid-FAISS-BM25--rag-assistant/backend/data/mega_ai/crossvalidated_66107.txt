[site]: crossvalidated
[post_id]: 66107
[parent_id]: 49010
[tags]: 
As the question is still not answered, here are my 2ct: I think here are two different topics mixed into this question: How can I calculate the sensitivity and specificity (or analogous measures) of a continuous diagnostic test in predicting a continuous outcome (e.g., blood pressure) without dichotomizing the outcome? I take it that you want to measure the performance of the model. The model predicts continuous (metric) outcome from some kind of input (happens to be metric in your example as well, but that doesn't really matter here). This is a regression scenario, not a classification. So you better look for performance measures for regression models, sensitivity and specificity are not what you are looking for*. Some regression problems have a "natural" grouping into presence and absence of something, which gives a link to classification. For that you may have a bimodal distribution: lots of cases with absence, and a metric distribution of values for cases of presence. For example, think of a substance that contaminates some product. Many of the product samples will not contain the contaminant, but for those that do, a range of concentrations is observed. However, this is not the case for your example of blood pressure (absence of blood pressure is not a sensible concept here). I'd even guess that blood pressures come in a unimodal distribution. All that points to a regression problem without close link to classification. * With the caveat that both words are used in analytical chemistry for regression (calibration), but with a different meaning: there, the sensitivity is the slope of the calibration/regression function, and specific sometimes means that the method is completely selective , that is it is insensitive to other substances than the analyte, and no cross-sensitivities occur. A. D. McNaught und A. Wilkinson, eds.: Compendium of Chemical Terminology (the “Gold Book”). Blackwell Scientific, 1997. ISBN: 0-9678550-9-8. DOI: doi:10.1351/ goldbook. URL: http://goldbook.iupac.org/ . Analogues of sensitivity and specificity for continuous outcomes On the other hand, if the underlying nature of the problem is a classification, you may nevertheless find yourself describing it better by a regression: the regression describes a degree of belonging to the classes (as in fuzzy sets). the regression models (posterior) probability of beloning to the classes (as in logistic regression ) your cases can be described as mixtures of the pure classes (very close to "normal" regression, the contamination example above) For these cases it does make sense to extend the concepts behind sensitivity and specificity to "continuous outcome classifiers". The basic idea is to weight each case according to its degree of belonging to the class in question. For sensitivity and specificity that refers to the reference label, for the predictive values to the predicted class memberships. It turns out that this leads to a very close link to regression-type performance measures. We recently described this in C. Beleites, R. Salzer and V. Sergo: Validation of Soft Classification Models using Partial Class Memberships: An Extended Concept of Sensitivity & Co. applied to Grading of Astrocytoma Tissues Chemom. Intell. Lab. Syst., 122 (2013), 12 - 22. The link points to the home page of the R package implementing the proposed perfromance measures. Again, the blood pressure example IMHO is not adequately described as classification problem. However, you may still want to read the paper - I think the formulation of the reference values there will make clear that blood pressure is not sensibly described in a way that is suitable for classification. (If you formulate a continuous degree of "high blood pressure" that would itself be a model, and a different one from the problem you describe.) I had only a quick glance at the paper you linked, but if I understood correctly the authors use thresholds (dichotomize) for both modeling strategies: for the continuous prediction is further processed: a prediction interval is calculated and compared to some threshold. In the end, they have a dichotomous prediction, and generate the ROC by varying the specification for the interval. As you specify that you want to avoid this, the paper doesn't seem to be overly relevant.
