[site]: crossvalidated
[post_id]: 639328
[parent_id]: 
[tags]: 
Possible reason for the Central Limit Theorem not to apply?

I have a multi-generator process which generates around 1200 data points per minute consisting of ~80 processes which contribute more or less evenly to the data points. Each singular process should in theory produce the same measurements, in reality they differ but not in an extreme way. Due to the amount of data, each minute these data points are averaged and saved. A few example KDE plots are in the attached image. Now I plan to apply statistical process control to this process in the form of process capability and control charts. Most of these methods however require some form of normality to be valid measures. I assumed that a group size of 1200 should be large enough for the CLT to hold such that my averages are normal distributed, this is not the case however. My distributions are narrower, instead of 68.27% of data being present within 1 standard deviation there are around 76%. The amount of data present within 2 or 3 standard deviations however is much closer. I tried to transform the data but I was not able to make it more normal. Since non-parametric control charts are not really a popular thing I want to make sure I did not make any wrong assumptions leading to this point. I believe the problem lies in my data points not being i.i.d. since they are temporally correlated as process changes are neither completely random nor instantenious. Could this be a reasonable assumption? And if so, are there some further things I could do? I already plan on applying percentile-based analysis to the data which is well documented for process capability but not so much in the case of control charts.
