[site]: stackoverflow
[post_id]: 1897987
[parent_id]: 1897972
[tags]: 
Try Using Curl: /** * Get a web file (HTML, XHTML, XML, image, etc.) from a URL. Return an * array containing the HTTP server response header fields and content. */ function get_web_page( $url ) { $options = array( CURLOPT_RETURNTRANSFER => true, // return web page CURLOPT_HEADER => false, // don't return headers CURLOPT_FOLLOWLOCATION => true, // follow redirects CURLOPT_ENCODING => "", // handle all encodings CURLOPT_USERAGENT => "spider", // who am i CURLOPT_AUTOREFERER => true, // set referer on redirect CURLOPT_CONNECTTIMEOUT => 120, // timeout on connect CURLOPT_TIMEOUT => 120, // timeout on response CURLOPT_MAXREDIRS => 10, // stop after 10 redirects ); $ch = curl_init( $url ); curl_setopt_array( $ch, $options ); $content = curl_exec( $ch ); $err = curl_errno( $ch ); $errmsg = curl_error( $ch ); $header = curl_getinfo( $ch ); curl_close( $ch ); $header['errno'] = $err; $header['errmsg'] = $errmsg; $header['content'] = $content; return $header; } Just call that function as-is with your url and it should echo out the whole webpage into the php page. However, you may need to rewrite links to assets, such as stylesheets and images using some regex. (Replace "/image.jpg" with " http://mydomain.com/image.jpg "). Curl usually is installed on shared hosts. If you want to just get the page's body, or head, you can use simplexml or regex expressions for that. (If the html is well-formed, simplexml is great for traversing the DOM).
