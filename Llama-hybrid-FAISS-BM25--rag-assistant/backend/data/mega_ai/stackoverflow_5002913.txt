[site]: stackoverflow
[post_id]: 5002913
[parent_id]: 
[tags]: 
Advantages of creating own corpus in NLTK

I have a large amount of text in Mysql tables. I want to do some statistical analysis and later on some NLP on my text using the NLTK toolkit. I have two choices: Extract all the text at once from my DB table (maybe putting them in a file if needed) and use the NLTK functions Extract the text and turning it into a "corpus" that can be used with NLTK. The latter seems quite complicated and I haven't found any articles that actually describes how to use it I only found this: Creating a MongoDB backed corpus reader which uses MongoDB as its database and the code is quite complicated and also requires knowing MongoDB. On the other hand, the former seems really straightforward but results in an overhead extracting the texts from DB. Now the question is that what are the advantages of corpus in NLTK? In other words, if I take the challenge and dig into overwriting NTLK methods so it can read from MySQL database, would it be worth the hassle? Does turning my text into a corpus give me something that I cannot (or with a lot of difficulty) do with ordinary NLTK functions? Also if you know something about connecting MySQL to NLTK please let me know. Thanks
