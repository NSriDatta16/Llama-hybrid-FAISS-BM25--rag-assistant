[site]: crossvalidated
[post_id]: 639713
[parent_id]: 
[tags]: 
Proposal parameterization accuracy for Importance Sampling

Suppose I am fitting a Bayesian mixture model that's structured as follows: $$ Y_i | (z_i = k) \sim \mathcal{N}(\mu_k, \sigma_k^2), \quad k = 1, \cdots, K $$ $$ z_i \sim \text{Mult}(1; w_{i1}, \cdots, w_{iK}) $$ $$ alr((w_{i1}, \cdots, w_{iK})) \sim \mathcal{N}((\mu_i1, \cdots, \mu_{i,K-1}), \Sigma) $$ $$ \mu_k = \beta_0 + \gamma_{ik} $$ Here $\text{Mult}(\cdot)$ is a multinomial distribution we draw once from. The $alr(\cdot)$ is a a specific transformation used in compositional data analysis to move the weights from the simplex to the real numbers. Here's the crucial part: the $\gamma_{ik}$ is a spatially structured random effect. To be precise, it's a random effect drawn from a conditional autoregressive model. (Omitted priors as they are not relevant for my question). My question has to do with finding the weight and latent class parameters using Importance Sampling (IS). To do so, I am using a form of Bayesian model averaging to repeatedly adjust the proposal distribution across a large batch of models. Fitting all these models is by itself computationally challenging. What I now encountered is that I need to, in addition, perform Bayesian model averaging for thousands of marginal distributions, each coming from one specific $\gamma_{ik}$ . As the number of observations, categories, and iterations of IS grow, so does the computational overhead of this procedure. For my question: suppose I were to leave out the spatial random effect in parameterizing the proposal distribution. Instead I just use the other effects estimated in the alr-regression for shaping it. Would this heavily bias the IS algorithm? Or would it just take more iterations to converge? These questions probably stem from my misunderstanding of the IS algorithm. As a a note, I really wish to continue using the IS algorithm already implemented in my code. Changing it at this point in my research is not a viable option. Thank you in advance for your thoughts. Edit : It's probably good to mention that I am using INLA to fit the remaining parts of this model. This allows me to quickly get marginal distributions across all parameters except the weights and latent classes. These two are within the IS algorithm.
