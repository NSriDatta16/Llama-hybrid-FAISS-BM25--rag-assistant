[site]: crossvalidated
[post_id]: 612352
[parent_id]: 
[tags]: 
How to deal with the fact that almost all univariate time series models have autocorrelated correlated residuals?

When studying the relationship between multiple time series often the first step is to determine stationarity of the individual time series. Given one of the time series, one can check for stationarity by assuming an AR(p) model for some p, and then applying an ADF test. However, from the outset one expects that the series is related to some other series. This means that: One would expect significant autocorrelation in the residuals of the univariate analysis, and The univariate model is misspecified I'm wondering how to make sense of this situation then. Is there a way to take into account that the time series is expected to be driven partially be external forces when doing an ADF test? If so, how? And which, if any, problems can arise from failing to take this into account? edit: To make this more precise, suppose that the data is generated by a VAR(p) process with 2 variables. If we then do a univariate AR(p) model fit on one of these two variables, the residuals need not be white noise, and can be auto-correlated. How does this affect the applicability of the ADF test? and more generally, the ADF test is really a 'unit root' test, not a 'stationarity' test. Meaning, that when the data generating process is not an AR(p), then simply fitting an AR model to it anyway and testing if that has a unit root does not seem to make sense to me... If the process is not even an AR, then the concept of a unit root is not relevant to begin with, so why test for it?
