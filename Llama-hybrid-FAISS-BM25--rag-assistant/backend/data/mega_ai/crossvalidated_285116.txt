[site]: crossvalidated
[post_id]: 285116
[parent_id]: 
[tags]: 
"Wrong polarity" of SVM in Bottou & Lin (2006)

I am reading the paper "Support Vector Machine Solvers" by Bottou and Lin (2006). On page 10, the authors engage in a discussion about the asymptotic number of support vectors as a function of the training sample size. Their argument goes as follows $-$ my emphasis: Let $\mathcal{B}$ represent the best error achievable by a linear decision boundary in the chosen feature space for the problem at hand. When the training set size $n$ becomes large, one can expect about $\mathcal{B}n$ misclassified training examples [...]. All these misclassified examples $^3$ are bounded support vectors. Therefore the number of bounded support vectors scales at least linearly with the number of examples. So far so good: the more data we have, the better our fitted model and thus the closer to the best possible error rate we can expect to be, hence we can expect to misclassify $\mathcal{B}n$ points asymptotically, and all those misclassified vectors will be support vectors (SVs). However, it is their footnote #$3$ that I do not understand: $3$ : A sharp eyed reader might notice that a discriminant function dominated by these $\mathcal{B}n$ misclassified examples would have the wrong polarity. About $\mathcal{B}n$ additional well classified support vectors are needed to correct the orientation of $w$. I think I understand their notion of " wrong polarity ": as SVs determine entirely the decision function of the SVM, if those misclassified SVs were the only SVs, then the SVM would be biased towards the class where these SVs were wrongly classified. Is my reasoning correct? However, what I do not fully understand is their last sentence: About $\mathcal{B}n$ additional well classified support vectors are needed to correct the orientation of $w$. Is this something that is automatically achieved when solving for the SVM? Or just some property that might be desirable?
