[site]: crossvalidated
[post_id]: 133273
[parent_id]: 133261
[tags]: 
Your question is essentially about model selection. When you are building a statistical model, you might not want to just consider the predictive ability of your model. Conventionally, the goodness of a statistical model is evaluated by the following three attributes. Parsimony or Interpretability , i.e., the simplicity of your model. A parsimonious model usually have better interpretations and many other advantages. Everything should be made as simple as possible, but no simpler. – Albert Einstein Goodness-of-fit , i.e., how good your model fits the current data at hand. Generalizability , that is, the ability of the fitted model to describe or predict new unknown data. Because of the above, many model selection criteria have been proposed to address the model selection problems in different aspects. Above all , it should be pointed out that conducting variable selection solely based on the significance level (p value) of a variable can cause a lot of issues. The following is quoted from a report " Scientific method: Statistical errors " published in Nature. The paper addresses some serious problems in scientific research caused by the p-value criterion. P values, the 'gold standard' of statistical validity, are not as reliable as many scientists assume. ...... Perhaps the worst fallacy is the kind of self-deception for which psychologist Uri Simonsohn of the University of Pennsylvania and his colleagues have popularized the term P-hacking; it is also known as data-dredging, snooping, fishing, significance-chasing and double-dipping. “P-hacking,” says Simonsohn, “is trying multiple things until you get the desired result” — even unconsciously. ...... “That finding seems to have been obtained through p-hacking, the authors dropped one of the conditions so that the overall p-value would be less than .05”, and “She is a p-hacker, she always monitors data while it is being collected.”
