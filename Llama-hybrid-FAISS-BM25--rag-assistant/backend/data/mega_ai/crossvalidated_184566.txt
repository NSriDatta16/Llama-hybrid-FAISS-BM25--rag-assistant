[site]: crossvalidated
[post_id]: 184566
[parent_id]: 
[tags]: 
ROC_AUC is better then ballanced accuracy

I'm testing different algorithms like: logistic regression, SVM's.... with a 10 fold nested Cross-validation. I'm using scikit-learn with gridsearchCV. But it is a little bit strange, that nearly every roc_AUC is better then the ballanced accuracy. Maybe I did something wrong? There are 73% of 1 values and 27% of 0 values. The ROC_AUC is computed with: fpr, tpr, thresholds = metrics.roc_curve(expected, predicted1) fpr1, tpr1, thresholds = metrics.roc_curve(expi, predi, pos_label=1.0) spec=1-fpr1[1] sens=tpr1[1] auc=metrics.auc(fpr,tpr) ballanced_accuracy=(sens+spec)/2 I'm using 2 kinds of roc_curces, du to im getting with the fpr and tpr an array about y predictions e.g. fpr has for example 230 values, same with tpr. For fpr1 and tpr1I have 1 value, with them i'll calculate the specificity and sensitivity. The expected and expi vector, have the same values. Predicted1, have the y-predict_proba scores and predi has just the classification, based on the predict_proba function ( 1 or 0 ) Random Forest: ballanced accuracy: 0.76 +/- 0.07 roc_auc: 0.83 +/- 0.05 SVM: ballanced accuracy: 0.77 +/- 0.06 roc_auc: 0.83 +/- 0.06 Decision Tree Classifier: ballanced accuracy: 0.69 +/- 0.05 roc_auc: 0.69 +/- 0.05 AdaBoost-Decision Tree Classifier: ballanced accuracy: 0.75 +/- 0.06 roc_auc: 0.80 +/- 0.06
