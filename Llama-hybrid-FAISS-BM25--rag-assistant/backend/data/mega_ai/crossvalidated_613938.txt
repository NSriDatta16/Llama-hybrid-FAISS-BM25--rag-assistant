[site]: crossvalidated
[post_id]: 613938
[parent_id]: 613928
[tags]: 
Until Breiman and the 21st c the historic barrier for working with massively categorical features was computational, for example in ANOVA, inverting a cross-products matrix with too many categories was infeasible. That said, it's useful to distinguish between massively categorical features vs targets. It's true that random forests (RFs) have trouble modeling targets with more than a few dozen levels. This is not true for massively categorical features. Breiman's intent with RFs was to redress criticisms of his original 'single iteration' approach to classification and regression trees as being unstable and inaccurate. What Breiman didn't realize was that any multivariate modeling engine could be plugged into his RF framework, e.g., ANOVA, multiple regression, logistic regression, k-means, and so on, to arrive at an approximating, iterative solution. Breiman did his work in the late 90s on a single CPU when massive data meant a few dozen gigs and a couple of thousand features processed over a couple of thousand iterations of bootstrapped resampling of observations and features. Each iteration built a mini-RF model, the predictions from which were aggregated into an ensemble prediction of the target. Today there are dozens of workarounds to modeling massively categorical features which extend Breiman's approach to breaking a large model down to many bite-sized, smaller models, sometimes known as divide and conquer algorithms. A paper by Chen and Xie discusses D&Cs, A Split-and-Conquer Approach for Analysis of Extraordinarily Large Data https://www.jstor.org/stable/24310963 Another good review is McGinnis' Beyond One-Hot: an exploration of categorical variables https://www.kdnuggets.com/2015/12/beyond-one-hot-exploration-categorical-variables.html Related to this is the suggestion of impact coding , e.g., Zumel's Modeling Trick: Impact Coding of Categorical Variables with Many Levels https://win-vector.com/2012/07/23/modeling-trick-impact-coding-of-categorical-variables-with-many-levels/ A completely different, non-frequentist approach was made in marketing science wrt hierarchical Bayesian modeling: Ainslie and Steenburgh's Massively Categorical Variables: Revealing the Information in Zip Codes . Their model is easily programmed in software such as STAN. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=961571 Hope this helps address your query. Afterthought FWIW...having poked around with a few of the approaches to massive categorical information including one-hot encoding, hierarchical bayes and impact coding, I came to the opinion that impact coding offered the best results in several nonsignificantly better ways: strongest holdout metrics wrt dependence, minimized metrics of dispersion and easiest to code.
