[site]: datascience
[post_id]: 100663
[parent_id]: 100662
[tags]: 
Since you partly overfit with RF, first try to get the RF hyperparameter right. You could do a grid search like: rf = RandomForestClassifier(...) param_grid = { 'n_estimators': [200,300], 'max_features': [10,20,30] } cv = GridSearchCV(estimator=rf, param_grid=param_grid, cv= 5) cv.fit(xtrain, ytrain) In RandomForestClassifier max_depth and max_features are of particular interest. More trees ( n_estimators ) tend to be "better". (Single) Decision Trees are usually not a good estimator. Once you have tuned your RF properly, you could also try to "stack" KNN, logistic regression, and RF since all three of them are not too bad. Sklearn comes with a conveniance function for stacking, namely StackingClassifier .
