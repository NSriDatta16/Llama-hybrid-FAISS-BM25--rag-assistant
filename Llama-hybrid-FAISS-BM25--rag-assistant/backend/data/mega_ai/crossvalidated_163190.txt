[site]: crossvalidated
[post_id]: 163190
[parent_id]: 163184
[tags]: 
The syntax for cv.glm is clouding the issue here. In general, one divides the data up into $k$ folds. The first fold is used as the test data, while the remaining $k-1$ folds are used to build the model. We evaluate the model's performance on the first fold and record it. This process is repeated until each fold is used once as test data and $k-1$ times as training data. There's no need to fit a model to the entire data set. However , cv.glm is a bit of a special case. If you look at its the documentation for cv.glm , you do need to fit an entire model first. Here's the example at the very end of the help text: require('boot') data(mammals, package="MASS") mammals.glm The 4th line does a leave-one-out validation (each fold contains one example), while the last line performs a 6-fold cross-validation. This sounds problematic: using the same data for training and testing is a sure-fire way to bias your results, but it is actually okay. If you look at the source (in bootfuns.q , starting at line 811), the overall model is not used for prediction. The cross-validiation code just extracts the formula and other fitting options from the model object and reuses those for the cross-validation, which is fine* and then cross-validation is done in the normal leave-a-fold-out sort of way. It outputs a list and the delta component contains two estimates of the cross-validated prediction error. The first is the raw prediction error (according to your cost function or the average squared error function if you didn't provide one) and the second attempts to adjust to reduce the bias from not doing leave-one-out-validation instead. The help text has a citation, if you care about why/how. These are the values I would report in my manuscript/thesis/email-to-the-boss and what I would use to build an ROC curve. * I say fine, but it is annoying to fit an entire model just to initialize something. You might think you could do something clever like my.model but it doesn't actually work because it uses the $y$ values from the overall model instead of the data argument to cv.glm , which is silly. The entire function is less than fifty lines, so you could also just roll your own, I guess.
