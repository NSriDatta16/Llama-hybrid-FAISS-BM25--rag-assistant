[site]: crossvalidated
[post_id]: 130596
[parent_id]: 
[tags]: 
How do CNN's avoid the vanishing gradient problem

I have been reading a lot about convoloutional neural networks and was wondering how they avoid the vanishing gradient problem. I know deep belief networks stack single level auto-encoders or other pre-trained shallow networks and can thus avoid this problem but I don't know how it is avoided in CNNs. According to Wikipedia : "despite the above-mentioned "vanishing gradient problem," the superior processing power of GPUs makes plain back-propagation feasible for deep feedforward neural networks with many layers." I don't understand why GPU processing would remove this problem?
