[site]: crossvalidated
[post_id]: 626124
[parent_id]: 
[tags]: 
Should we increase the number of samples when adding more classes?

Assume we are solving a $k$ -class classification problem, $k \geq 2$ , and we have a trained classifier $\phi$ from a family of generative or discriminative classifiers $\Phi$ minimizing an objective $\ell$ . On a held-out set or via cross-validation, it produces a classification report $R$ consisting of one or more evaluated metrics. If we decide to add more classes to the data (e.g., an entertainment topic to the existing politics , economy , and sports ) or split one of the classes into two or more (e.g., the presence of a disease into its severities), should we also add more examples with the previous classes in order to prevent regressions? If yes, is there any theoretical justification for this and/or rule of thumb on the number of additional samples per class? If you believe the question is too general and the answer depends on multiple factors like $\Phi$ , $R$ , and $\ell$ , then let's assume additionally that we have a limited amount of data (say, a couple of thousands of texts and dozens of topics); train a shallow neural network minimizing the negative log-likelihood loss; evaluate weighted $F_1$ .
