[site]: crossvalidated
[post_id]: 473493
[parent_id]: 
[tags]: 
Increase in SVM Classifier performance after binning

I've been working on a classification problem, and I ran into something rather strange. The original problem has continuous features and three labels. I then mapped the continuous features to binary using binning (more on this below), and the performance of my model actually increased. I am aware that binning is generally a bad idea, and that information is lost when you perform binning. This loss should result in a lower performance, but that isn't what I am seeing here. Now for the details of what I have done so far. The data I'm using contains continuous features and three classes. First, I normalized the features to $[-1, 1]$ by dividing by the max value of the dataset, and split the data 50/50 into training and test sets. I then fit the data with an SVM using an RBF kernel, and One-Vs-Rest to deal with the issue of having multiple classes. As a note, I'm using the area underneath the ROC curve (ROC-AUC) to evaluate the performance of the model. For this model, I get an ROC-AUC of 0.98, 0.93, and 0.99 for each of the three individual classes. Now, I needed to convert these continuous features to binary. As mentioned above, this is considered to be a terrible idea, but is necessary for the project I'm working on. I decided to try this by binning the data. Now, this data is not evenly or even normally distributed. Most of the data lies very close to zero. If we plot a histogram of this data using 256 bins, we get: In order to fix this issue, I re-binned using hyperbolic bin positions. Again with 256 bins but the new scale: I noticed that the standard procedure for binning involves essentially one-hot encoding the bin number. In this case, that would result in a ridiculous amount of features (the original dataset has 202). To get around this, I instead used the binary representation of the bin number, ie a feature that fell into bin 7 would yield 00000111 (assuming 256 bins, so $\log_2(256) = 8$ new features). This is significantly more compact. Finally, I performed the same fit procedure as on the continuous feature data (RBF SVM, etc) and get an ROC-AUC of 1.0, 0.97, and 0.99 for the three classes. This is a reasonably significant increase in performance. However, binning should decrease the performance as information is lost. My instinct tells me that this increase is likely due to the hyperbolic scaling, although I don't have any evidence to back this up. Any thoughts on why I get better results after losing information? As a few final notes, I'm using Scikit Learn in Python for the SVM routines. Also, the performance values given above are from a single trial, but I have run this multiple times with different permutations of the train/test split, always with randomly chosen elements. Although the performance numbers differ slightly across trials (as you would expect), the increase in performance is consistent throughout.
