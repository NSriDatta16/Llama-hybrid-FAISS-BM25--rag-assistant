[site]: crossvalidated
[post_id]: 445261
[parent_id]: 421935
[tags]: 
See Attention is all you need - masterclass , from 15:46 onwards Lukasz Kaiser explains what q, K and V are. So basically: q = the vector representing a word K and V = your memory, thus all the words that have been generated before. Note that K and V can be the same (but don't have to). So what you do with attention is that you take your current query (word in most cases) and look in your memory for similar keys. To come up with a distribution of relevant words, the softmax function is then used.
