[site]: datascience
[post_id]: 107143
[parent_id]: 107134
[tags]: 
My experience is oversampling with replacement may gives better classification performance than SMOTE on imbalanced data although the latter is considered more advanced than the former. If the minority classes are too small, the synthetic data generated by SMOTE can have wrong labels i.e. a synthetic instance is class 0 (a minority class) but should be class 1 (a majority class). This is because SMOTE labels the synthetic instances based on their K nearest neighbors in the training set. When the class 0 instances and the class 1 instances of the training set are mixed together in the multi-dimensional space of the features i.e. both classes can not be separated either linearly or non-linearly, SMOTE would label the synthetic instances wrong which leads to the poor classification performance of the trained classifier. Boosting can leads to over-fitting of ensemble model to the training set and poorer generalization than bagging. Because the base classifiers of boosting are trained one by one on those training instances which are classified wrong by the preceding base classifier. This over-fits to the training set. The balanced random forest algorithm of imbalanced-learn library normally give promising results on imbalanced datasets. Balanced bagging classifier are worth trying. https://imbalanced-learn.org/stable/references/generated/imblearn.ensemble.BalancedRandomForestClassifier.html https://imbalanced-learn.org/stable/references/generated/imblearn.ensemble.BalancedBaggingClassifier.html#imblearn.ensemble.BalancedBaggingClassifier
