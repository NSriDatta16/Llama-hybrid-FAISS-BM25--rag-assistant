[site]: datascience
[post_id]: 88216
[parent_id]: 88186
[tags]: 
You either of them: If you use the output of the LSTM, you will have contextual embeddings, like BERT or ELMo. This means that each time you want to use your embeddings to encode some input text, you need to pass the input text to your encoder and take the output of the LSTM. If you use the embedding table, you will have non-contextual embeddings, like word2vec. This means that each time you want to use your embeddings to encode some input text, you just need to look up the appropriate vector in the embedding table.
