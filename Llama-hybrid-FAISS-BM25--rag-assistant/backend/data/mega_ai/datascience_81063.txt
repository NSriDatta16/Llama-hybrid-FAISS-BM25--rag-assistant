[site]: datascience
[post_id]: 81063
[parent_id]: 80729
[tags]: 
As stated in 3.2 Model setup The generator G is made up of embedding layer, one bidirectional long short-term memory (BLSTM) [21] layer, one fully connected (FC) layer. And Gaussian noise is 10-dim vector concatenated with the output of BLSTM So the noise is concatenated to the embeddings computed by the BLSTM for each time step. I think they concatenate the same 10-dim vector to each embedding output of the BLSTM but this is not clear. The obtained vector (embedding concatenated with noise) is fed to the Fully-connected layer with softmax activation to compute the probability that the word should be translated or not. Concerning page 2 footnote about "ignoring the noise" I think they simply ignore it in their explanations of the model, but it is effectively used in the model by simply appending a gaussian noise vector to the embeddings output of the BLSTM. As you see in the following figure they do not show the noise, but it is actually concatenated to the output embeddings of the BLSTM in the Generator.
