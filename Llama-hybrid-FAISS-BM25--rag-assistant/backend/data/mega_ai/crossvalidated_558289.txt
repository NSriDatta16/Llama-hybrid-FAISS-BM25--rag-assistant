[site]: crossvalidated
[post_id]: 558289
[parent_id]: 558225
[tags]: 
There are two forms of the geometric distribution . The one we use here counts the number $X$ of Bernoulli trials until the first Success occurs, where the Success probability is $p,$ so that the PDF is $$f_X(x) = p(1-p)^x,$$ for $x = 1, 2, 3, \dots$ and $$E(X) = \mu_X = 1/p.$$ [The alternative version counts the number of Failures before the first Success.] It is not trivial to show that $$E(X) = \sum_{x=1}^\infty xf_X(x) =\sum_{x=1}^\infty xp(1-p)^{x-1} = 1/p.$$ [The Wikipedia article linked above shows a formal derivation for the alternative version. A slight modification works for our version.] However, the terms of the sum decrease markedly as $x$ increases, so that one does not need to sum a huge number of terms to get a good approximation. For example, let $p = 1/6.$ p = 1/6; x = 0:100; f = p*(1-p)^{x-1} mu = sum(x*f) [1] 6 In your problem about rolling a fair die, the probabilty of getting a 3 on any one roll is $p = 1/6,$ so the expected number of rolls of the die until a 3 occurs is $6.$ Notes: (1) One way to show that $\mu_X = 1/p$ is to use moment generating functions. The proof if the Wilkipedia article uses an analogous differentiation method. (2) The geometric distribution has the memoryless property: $P(X > m+n | X > m) = P(X > m),$ for positive integers $m, n.$ So the average number of rolls after the first 5 observed until we get a 3 is also $6.$ (3) An approximate simulation of a million waiting times for the first 3 shows that the average wait is $6.$ [Extremely rare waits longer than 100 trials are ignored.] set.seed(2021) w = replicate(10^6, match(3, sample(1:6, 100, rep=T))) mean(w) [1] 6.003519
