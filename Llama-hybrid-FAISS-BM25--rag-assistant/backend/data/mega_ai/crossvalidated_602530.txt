[site]: crossvalidated
[post_id]: 602530
[parent_id]: 602492
[tags]: 
In general, it is best if the loss function in fitting a model is as close as possible to the real cost your predictions would incur. Yes, these costs are often very hard to operationalize. I would argue that this is a Good Thing: once you realize it is hard to pin an exact real-life cost to your model's output, you understand that it may not be worthwhile trying to tease the last bit of performance out of your model using your pretend-cost. I am still rather sceptical that the only cost that matters is the penalty. I'll accept that the penalty is the same whenever the percentage error is exceeded, whether by 11% or by 50%... but I find it hard to believe that there are truly no other costs at all. But if this is a reasonable modeling assumption, then you can simply use a custom loss function that is zero if the prediction is less that 10% from the actual, and one otherwise. I went and simulated this a bit, similar to finding the optimal point prediction if your actuals are conditionally gamma or conditionally lognormal under various loss functions. Playing around a bit with this loss function, it often turns out that the optimal prediction is pretty close to the expectation, at least for the parameters I fed into my simulations. So you might start out with a straightforward model trained on squared errors (which elicits the expectation) - this might already give pretty good results. For instance, the plot below shows what loss you can expect for given forecasts if your data is conditionally lognormal with log-mean 1 and log-SD 0.2, and the vertical line gives the expectation, which is pretty close to the loss-minimizing forecast (R code at bottom): (These simulations also show that there is some limit to the possible accuracy. In the plot above, you won't get below incurring that penalty in 60% of cases. If there is a lot of residual noise, then no matter what you forecast, you have a good chance of incurring that penalty often. There is simply a limit to forecastability, which depends on the process we are forecasting. See How to know that your machine learning problem is hopeless? and Ways to increase forecast accuracy .) R code: set.seed(1) # for reproducibility # sims 0] # in case we simulate something below zero forecasts
