[site]: crossvalidated
[post_id]: 525751
[parent_id]: 477046
[tags]: 
The two models have distinct levels of uncertainty. Once you have the parameters and inputs of an RNN, all hidden states of the RNN are uniquely determined. For an HMM, even if you know the parameters and emissions, you don't know the hidden states. The forward-backward algorithm is needed to integrate out the uncertainty over the hidden states of the HMM. You can see the difference in the likelihood function for an HMM compared to an RNN: the HMM likelihood has a sum over hidden states, while the RNN does not. When you take the gradient of the HMM likelihood, this sum remains, and you get the forward-backward algorithm. If you were doing Bayesian inference in an RNN, then the hidden states would be uncertain (since the parameters are uncertain) and there would be a reason to do message passing/expectation propagation.
