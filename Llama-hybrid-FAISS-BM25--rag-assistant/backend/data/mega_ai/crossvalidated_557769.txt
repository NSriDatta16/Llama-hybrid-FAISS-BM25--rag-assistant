[site]: crossvalidated
[post_id]: 557769
[parent_id]: 
[tags]: 
Confusion about the training procedure while using transfer learning

Suppose that we have a trained CNN, there is 5 conv layers and 3 fully connected layers. We take the first 5 conv layers as it is (with their parameter settings: like kernel size, activation etc) and their weights and biases which are trained before by using another dataset. If we want to profit from this knowledge in our new model (which has the same 5 conv layers at the beginning but later is different), do we: Continue to train the whole model (at the beginning: 5 conv layers is initialized to the parameters found in the previous training and the later layers some other initialization (e.g. he initialization for weights and 0 for biases)) with our data? or We keep the first 5 conv layers as in the test mode (no more update) and only update/train the parameters in later layers? Which one is understood/done when we talk about transfer learning? Note: I am not very familiar with all the terminology used in deep learning. However, I hope that I could at least explain my question.
