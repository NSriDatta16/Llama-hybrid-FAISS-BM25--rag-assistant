[site]: datascience
[post_id]: 17965
[parent_id]: 17964
[tags]: 
Latent Semantic Analysis (LSA) relies on linear-algebraic decompositions (e.g. SVD), which in turn involve eigenvectors/values (see here ). Not sure if this is quite what your question is driving at, but in general, eigenvectors are useful in data analysis because they define some "natural" direction in the data. For example, the eigenvectors of the covariance matrix of some data are all perpendicular to one another, and the i'th one points in the direction of i'th greatest variance in the data. In other words, the first eigenvector points in the direction where the data's variance is greatest, the second one in (perpendicular) direction of second-greatest variance, and so on. This is why eigenvector-based decompositions can be so useful in feature selection and dimensionality reduction. In informal terms, they transform the data into perpendicular features that are aligned with some "natural" axes in the data.
