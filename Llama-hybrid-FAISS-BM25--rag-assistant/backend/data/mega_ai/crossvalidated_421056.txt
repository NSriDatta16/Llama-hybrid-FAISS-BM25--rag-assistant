[site]: crossvalidated
[post_id]: 421056
[parent_id]: 421050
[tags]: 
Let's say that we've measured subjects on variables $X_1$ , $X_2$ , and $X_3$ . However, for our regression or classification equation, we want to use $X_1$ , $X_3$ , $X_1X_3$ , $X_1^2$ , and $cos(X_1)$ as the predictors ( $X_2$ omitted intentionally). Attribute space: $X_1$ , $X_2$ , and $X_3$ Feature space: $X_1$ , $X_3$ , $X_1X_3$ , $X_1^2$ , and $cos(X_1)$ The attributes are what you measure. The features are what you put into the regression. You may elect to exclude attributes. You may elect to transform attributes, such as interaction terms, squaring (cubing, etc), trig functions, or anything else you find interesting. That's the feature extraction part that your linked answer mentions. If you know what PCA means, I propose the following exercise. Exercise : You measure subjects on 10,000 attributes. Then you use PCA and decide to include 15 PCs in your regression equation. What is your feature space? Edit You wanted to discuss iris in particular. There are four attributes: $X_1$ : length of pedals $X_2$ : width of pedals $X_3$ : length of sepals $X_4$ : width of sepals Let's say that you're convinced that some explicit measure of pedal area should make it into your regression. Sure, you don't think pedals are rectangular, but $X_1X_2$ ought to give some idea of pedal area. Then define $X_5 = X_1X_2$ . Now run your regression. $$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + \beta_4X_4 + \beta_5X_5$$
