[site]: datascience
[post_id]: 20232
[parent_id]: 19980
[tags]: 
The ten times rule seems like a rule of thumb to me, but it is true that the performance of your machine learning algorithm may decrease if you do not feed it with enough training data. A practical and data-driven way of determining whether you have enough training data is by plotting a learning curve, like the one in the example below: The learning curve represents the evolution of the training and test errors as you increase the size of your training set. The training error increases as you increase the size of your dataset, because it becomes harder to fit a model that accounts for the increasing complexity/variability of your training set. The test error decreases as you increase the size of your dataset, because the model is able to generalise better from a higher amount of information. As you can see on the rightmost part of the plot, the two lines in the plot tend to reach and asymptote. Therefore, you eventually will reach a point in which increasing the size of your dataset will not have an impact on your trained model. The distance between the test error and training error asymptotes is a representation of your model's overfitting. But more importantly, this plot is saying whether you need more data. Basically, if you represent test and training error for increasing larger subsets of your training data, and the lines do not seem to be reaching an asymptote, you should keep collecting more data.
