[site]: crossvalidated
[post_id]: 374322
[parent_id]: 374312
[tags]: 
Shameless plug for my paper: Machine Learning Methods for Crop Yield Prediction and Climate Change Impact Assessment in Agriculture First, reshape your data such that each observation is a unit-period. I.e.: take it from wide to long format. gather in tidyverse -speak. If you were using classical statistics, and you had repeated observations of many units over time, you might use a model that looks like this: $$ y_{it} = \alpha_i + \mathbf{X}_{it}\beta +\epsilon_{it} $$ where the $\alpha$ is a fixed or random intercept (capturing unobserved cross-sectional heterogeneity), and the covariates $\mathbf{X}_{it}$ can include things that you think or know are important, like linear/quadratic/low-dimensional time trends that are additively separable but yet estimated simultaneously with everything else. If you knew from theory or whatever that this model was correctly specified up to a parameter vector, you'd just fit OLS or maybe ridge. If you believed that the true data-generating process was nested in the above equation, you'd run LASSO. But if you believed that the model was useful but mis-specified, you might consider fitting $$ y_{it} = \alpha_i + \mathbf{X}_{it}\beta + f(\mathbf{Z}_{it})+\epsilon_{it} $$ where $f()$ is a neural net. In principle, it could be any nonparametric algorithm, but a neural net is convenient because both the parametric and nonparametric part have gradients and can be optimized -- simultaneously -- by gradient descent. (Other approaches to models like this include the backfitting algorithm, but I haven't had much success using the backfitting algorith where $f()$ is a random forest or xgboost ). The addition of parametric structure to the neural net improves efficiency. If $N\rightarrow \infty$ , the net would probably figure out the time trends and individual heterogeneity, and any other parametric structure. But in finite samples, it can really help. While I've done this in pure R, it can also be done in modern deep learning libraries like Keras/Tensorflow, with a bit of hacking. If you had a dynamic process, you could apply similar logic to augment an autoregressive model with a RNN or LSTM, though dynamic models have their own considerations that don't always apply in the panel context.
