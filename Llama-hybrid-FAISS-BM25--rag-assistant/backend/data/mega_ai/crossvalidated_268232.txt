[site]: crossvalidated
[post_id]: 268232
[parent_id]: 268202
[tags]: 
If anyone is concerned, this "answer" will never become the accepted answer. It's more like some notes on the issue, possibly helping other people as well. This post is very useful: $f$ is the array of class scores for a single example (e.g. array of 3 numbers here): $f= X\cdot W + b$, then the Softmax classifier computes the loss for that example as: $L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right)$ Recall also that the full Softmax classifier loss is then defined as the average cross-entropy loss over the training examples and the regularization: $L = \underbrace{ \frac{1}{N} \sum_i L_i }_\text{data loss} + > \underbrace{ \frac{1}{2} \lambda \sum_k\sum_l W_{k,l}^2 > }_\text{regularization loss} \\\\$ Lets introduce the intermediate variable $p$, which is a vector of the (normalized) probabilities. The loss for one example is: $$p_k = \frac{e^{f_k}}{ \sum_j e^{f_j} } \hspace{1in} L_i =-\log\left(p_{y_i}\right)$$ We now wish to understand how the computed scores inside $f$ should change to decrease the loss $L_i$ that this example contributes to the full objective. In other words, we want to derive the gradient $\partial L_i/\partial f_k$. The loss $L_i$ is computed from $p$, which in turn depends on $f$. It’s a fun exercise to the reader to use the chain rule to derive the gradient, but it turns out to be extremely simple and interpretible [sic] in the end, after a lot of things cancel out: $$\frac{\partial L_i }{ \partial f_k } = p_k - \mathbb{1}(y_i = k)$$ Notice how elegant and simple this expression is. Suppose the probabilities we computed were $p = [0.2, 0.3, 0.5]$, and that the correct class was the middle one (with probability $0.3$). According to this derivation the gradient on the scores would be $\text{df} = [0.2, -0.7, 0.5]$. Recalling what the interpretation of the gradient, we see that this result is highly intuitive: increasing the first or last element of the score vector f (the scores of the incorrect classes) leads to an increased loss (due to the positive signs $+0.2$ and $+0.5$) - and increasing the loss is bad, as expected. However, increasing the score of the correct class has negative influence on the loss. The gradient of $-0.7$ is telling us that increasing the correct class score would lead to a decrease of the loss $L_i$, which makes sense. The code implementation of scores in Python (the example in this linked post has no hidden layer) is: # compute class scores for a linear classifier scores = np.dot(X, W) + b This corresponds to the line providing the net input for the outer layer on the example in the OP: score Given the array of scores we’ve computed above, we can compute the loss. First, the way to obtain the probabilities is straight forward: # get unnormalized probabilities exp_scores = np.exp(scores) # normalize them for each example probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) This corresponds to the lines in the OP: # softmax score.exp All of this boils down to the following code. Recall that probs stores the probabilities of all classes (as rows) for each example. To get the gradient on the scores, which we call dscores , we proceed as follows: dscores = probs dscores[range(num_examples),y] -= 1 dscores /= num_examples This is the equivalent in the OP to: dscores Lastly, we had that scores = np.dot(X, W) + b , so armed with the gradient on scores (stored in dscores ), we can now backpropagate into W and b: dW = np.dot(X.T, dscores) db = np.sum(dscores, axis=0, keepdims=True) dW += reg * W # don't forget the regularization gradient Where we see that we have backpropped through the matrix multiply operation, and also added the contribution from the regularization. Note that the regularization gradient has the very simple form reg*W since we used the constant $0.5$ for its loss contribution (i.e. $\frac{d}{dw} ( \frac{1}{2} \lambda w^2) = \lambda w$). This is a common convenience trick that simplifies the gradient expression. Here the matrix of weights is updated, and that's it, because the example quoted has no hidden layer. However, in the OP, there is a hidden layer, which explains that the first matrix of weights being updated are $W_2$ connecting the output of the hidden layer to the net input of the outer layer: Remembering that: hidden.layer is the activation function or the output of the hidden layer... dW2
