[site]: crossvalidated
[post_id]: 68276
[parent_id]: 
[tags]: 
Reducing size of dataset to a fixed size - retaining maximum information in all dimensions

I was wondering about about the following problem: I have a set of $N=10^5$ observations with dimensionality $D=2$, and I would like to reduce it to a set of size with $M=10^3$, or some other $(M \ll N)$, while retaining most of the information contained in it. I assume M is fixed in advance. (The observations serve as input for an algorithm in a vision problem, but consider that irrelevant.) What I mean exactly is that I would like to eliminate all observations that are highly correlated with some other observation in my 2 dimensional feature space and keep only the ones that aren't. Ideally I would like the points to be uniformly spread across each dimension. I think this could be achieved by running a k-mean algorithm with M centroids and by keeping only 1 point for each cluster, the one that is closest to the centroid mean. However, I was wondering whether something like this could also be achieved using the PCA. We can use PCA to reduce the dimensionality of the data to a subset of existing variables, for example by selecting the variable with the highest absolute coefficient for each principle component (PC) - as suggested by Jolliffe. I was thinking about transposing the data matrix (treating the observations as variables) and solving PCA (SVD).Would the coefficients in the PC then tell us the contribution of each observation to a particular PC? Do similar coefficients for 2 observations in a certain PC tell us the correlation of the data in the direction of the PC? Ultimately I would like to use a sorting procedure based on the similarity of these scores. My problems are: I am not 100% sure if any of the above makes sense for what I want to do. My other problem is scaling. I can't scale the transposed data - as I have only two points. I am thinking about scaling it in the two true dimensions, then transposing it and calculating the eigenvectors. However, the problem of running prcomp(x,scale=F,center=F) in R is that it does a simple SVD, which is fine, but in my case the linear subspace spanned by the transposed data matrix has dim = 1. So I should be only getting one PC with non-zero variance and not two, as in this case - I'm not sure whether I can simply ignore the second PC, and how can that be justified formally... Thanks for your insights!
