[site]: crossvalidated
[post_id]: 88164
[parent_id]: 88082
[tags]: 
i would think of spectral analysis. the plain text should probably have some patterns in it. the encrypted text should remove the patterns. so if you draw the spectrum using something like FFT ( fast fourier transform) of two files, they should probably look distinctly different. the original text should have some spikes in the spectrum, while the encrypted one should look like all noise. maybe wavelet spectrum would be even better than FFT UPDATE: another way of capturing the patterns is something called Markov chains, see this interesting paper Using Markov Chains for Identification of Writers . In the simplest case you build the table where the rows are the first character, and the columns are the second character in all two-character sequences of the text, the intersection cell contains the frequency. For instance, ABCABAC would generate 3x3 table with rows {0 2 1}{0 0 1}{1 0 0} corresponding to {A B C}. The original text should have an interesting table, while the encrypted text should have a "boring" table, because it presumably removed all patterns.
