[site]: crossvalidated
[post_id]: 83694
[parent_id]: 83640
[tags]: 
Option 1 (Choose $\lambda=\lambda^*$ to minimize the average OOS mean square error) should not be over optimistic. The point of estimating the OOS error is to select to the $\lambda$ that best fits an independent set of data from the same distribution. A $\lambda$ that results in an overly complex model will overfit the training set and therefore perform poorly on the out of sample set, and so won't be selected. See Chapter 7 of Elements of Statistical Learning for a more detailed explanation of the validity of model selection based on out of sample error, and on the benefits of cross-validation. The philosophy behind Option 2 (Choose the largest $\lambda$ that is within one standard error (taken over all cross validation sets) of the $\lambda$ that minimizes the average OOS mean square error.) is to select a model whose performance is not substantially worse than option 1, but with a simpler (more parsimonious result). Essentially you are trading accuracy of the fit for interpretability. In summary, Option 1 is the best choice for predictive performance of your model (and closeness to the true data generating distribution. Option 2 (and similar modifications) might lead to a model that is easier to interpret and therefore more useful in describing the true distribution in a meaningful way.
