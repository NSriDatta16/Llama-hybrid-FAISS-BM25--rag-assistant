[site]: crossvalidated
[post_id]: 429918
[parent_id]: 137590
[tags]: 
There is already a good answer to this question, but I would like to add something more to it. The gamma(discounting factor) is a reflection of how you value your future reward. Choosing the gamma value=0 would mean that you are going for a greedy policy where for the learning agent, what happens in the future does not matter at all. The gamma value of 0 is the best when unit testing the code, as for MDPs, it is always difficult to test the behavior of an agent when number of horizons increases. On the other hand, choosing the gamma value=1 would mean that you don't have any preference about when you want to get the reward; that is to say, for example, if you are given a reward in time step 3 or in time step 7 would be of equal value to your agent. These are the edge scenarios, but you would like to have something in between to find an optimal policy. In my several years experience with MDPs, the most common value of gamma I have seen, is 0.9 , though the value always depends on the requirements of the problem domain.
