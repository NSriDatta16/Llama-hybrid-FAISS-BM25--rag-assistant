[site]: datascience
[post_id]: 100155
[parent_id]: 100153
[tags]: 
Sigmoid $f(x) = \frac{1}{1+e^{-x}}$ $f'(x) = f(x)(1-f(x)) $ When the value of sigmoid function is either too high or too low, the derivative becomes too small(close to zero). While error is back propagating in sigmoid activated neural networks, gradient degradation happens and it results in vanishing gradient. Relu $f(x) = max(0,x) $ $f'(x) = (0~if~x 0 ) $ While error is back-propagating in relu activated neural networks, gradient is not getting degraded.
