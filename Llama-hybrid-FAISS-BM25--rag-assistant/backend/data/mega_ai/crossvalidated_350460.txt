[site]: crossvalidated
[post_id]: 350460
[parent_id]: 350455
[tags]: 
The question in the title is a little different from what the quotation addresses. You ask What's the problem of evaluating a global metric at the end of each epoch in deep neural network? The answer is that there isn't one -- evaluating some metric at the end of an epoch, after all updates are applied, gives you an unbiased estimate of the model's performance given that data for the parameters obtained at the end of that epoch. This is what you want, and it's what Keras was previously attempting to approximate using a running average of the minibatch loss. (Doing so isn't free, however -- it requires that you feed your training data to the model once to compute parameter updates and once again to compute the loss, i.e. evaluate the neural network twice against the training data.) The Keras quotation is about how the end-of-batch metrics were approximated. The way this worked was that each minibatch computed some loss, and that loss was used to compute a per-epoch running average. (Did you ever notice how early in an epoch, the loss would fluctuate more than at the end of the epoch? This is why.) The problem with this is that each minibatch is computed for a different set of parameters (because the model is training). So minibatch $i$ has its loss computed with parameters $\theta_i$, and minibatch $i+1$ is computed with parameters $\theta_{i+1}$ and so on. This is biased, in the sense that if your model is continuing to learn, that implies that the loss will be stochastically decreasing, so you don't care about minitbatch $i$'s loss at $\theta_i$, you care about how it did using $\theta_M$ the final parameter estimate for the epoch (because these parameters ought to be "better" than they were at previous epochs if your model is improving). Another source of bias in this procedure is that using regularization during training will tend to inflate training loss at each minibatch. This is either because you're using some sort of weight penalty, so that the loss function looks like $$ \mathcal{L}(\theta | x, y, \lambda) = \text{Loss}(x,y | \theta) + \lambda \text{Penalty}(\theta) $$ or because using something like dropout is systematically disrupting the model's ability to fit the data well. Recording the loss for each training batch will include these biases, and tend to inflate the reported training loss.
