[site]: crossvalidated
[post_id]: 600909
[parent_id]: 600888
[tags]: 
This is a well-known result of the non-linear relationships in your model. I.e. even if on some suitable scale your posterior distribution for the parameters is a set of independent normal distributions, the uncertainty around them will no longer cancel out once you apply non-linear functions to them. A way of getting the same results as you did from conditional_effects(mod) is to take the posterior MCMC samples and to calculate the curve across values of x for the parameters in each MCMC sample (of course, that's 4000 curves across however many values of x you want to evaluate the curve at). Then, you could take the median of all curves (plus quantiles for credible intervals). E.g. with brms + tidyverse functions you could do that like this: library(tidyverse) mod %>% as_draws_df() %>% mutate(logy_pred = pmap(list(b_a1_Intercept, b_a2_Intercept, b_k1_Intercept, b_k2_Intercept), function(a1, a2, k1, k2) tibble(x=seq(0, 30, 0.1)) %>% mutate(logy_pred = log(a1 * exp(-k1 * x) + a2 * exp(-k2 * x))))) %>% unnest(logy_pred) %>% group_by(x) %>% summarize(median = median(logy_pred), lcri = quantile(logy_pred, probs=0.025), ucri = quantile(logy_pred, probs=0.975)) %>% ggplot(aes(x=x, y=median, ymin=lcri, ymax=ucri)) + geom_ribbon() + geom_line() + ylab("log(y)") That exactly reproduces the plot you obtained originally. Note that another confusing thing was that one of your plots is on the $y$ -scale and the other on the $log(y)$ -scale. However, you can calculate quantiles (median etc.) before and after the transformation without it making a difference in this case.
