[site]: crossvalidated
[post_id]: 100317
[parent_id]: 
[tags]: 
Bernoulli mixture models for image classification, pathological cases

I'm trying to use a Bernoulli mixture model to classify MNIST images, and I'm running into pathological cases which screw up my calculations. The pdf of a multidimensional (let's say N dimensions) Bernoulli distribution is as such: $P(x|\mu) = \prod\limits_{i=0}^{N}\mu_{i}^{x_i}(1 - \mu_{i})^{(1 - x_i)}$ The pdf of a mixture of thoses being a weighted sum of them. Dealing with small probabilities, I naturally turned to log probabilities, and transformed the equation as such: $log(P(x|\mu) = \sum\limits_{i=0}^{N}log(\mu_{i}^{x_i}) + log((1 - \mu_{i})^{(1 - x_i)})$ My problem is, what happens when $\mu_i$ becomes either 0 or 1 during training? I end up with $log(0^1)$, which yields -Inf and ruins my classification. Are those case pathological, or is there a way to represent them in a sane way? I thought of considering those cases a pathological, because they typically represent corners of the image on which no pixel ever wanders, and running a PCA to reduce dimensionality, but I'm not sure how I can proceed from there: I can understand reducing my dataset to D' dimensions, and so having my centers of D' dimensions, but how can I use them with an image from my validation set, which is still D dimensions? I feel using the coefficients from my training set will introduce bias. Thanks for your help, and cheers!
