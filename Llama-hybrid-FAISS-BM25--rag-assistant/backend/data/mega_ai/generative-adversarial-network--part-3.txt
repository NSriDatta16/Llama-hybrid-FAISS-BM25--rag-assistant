parametrization The GAN architecture has two main components. One is casting optimization into a game, of form min G max D L ( G , D ) {\displaystyle \min _{G}\max _{D}L(G,D)} , which is different from the usual kind of optimization, of form min θ L ( θ ) {\displaystyle \min _{\theta }L(\theta )} . The other is the decomposition of μ G {\displaystyle \mu _{G}} into μ Z ∘ G − 1 {\displaystyle \mu _{Z}\circ G^{-1}} , which can be understood as a reparametrization trick. To see its significance, one must compare GAN with previous methods for learning generative models, which were plagued with "intractable probabilistic computations that arise in maximum likelihood estimation and related strategies". At the same time, Kingma and Welling and Rezende et al. developed the same idea of reparametrization into a general stochastic backpropagation method. Among its first applications was the variational autoencoder. Move order and strategic equilibria In the original paper, as well as most subsequent papers, it is usually assumed that the generator moves first, and the discriminator moves second, thus giving the following minimax game: min μ G max μ D L ( μ G , μ D ) := E x ∼ μ ref , y ∼ μ D ( x ) ⁡ [ ln ⁡ y ] + E x ∼ μ G , y ∼ μ D ( x ) ⁡ [ ln ⁡ ( 1 − y ) ] . {\displaystyle \min _{\mu _{G}}\max _{\mu _{D}}L(\mu _{G},\mu _{D}):=\operatorname {E} _{x\sim \mu _{\text{ref}},y\sim \mu _{D}(x)}[\ln y]+\operatorname {E} _{x\sim \mu _{G},y\sim \mu _{D}(x)}[\ln(1-y)].} If both the generator's and the discriminator's strategy sets are spanned by a finite number of strategies, then by the minimax theorem, min μ G max μ D L ( μ G , μ D ) = max μ D min μ G L ( μ G , μ D ) {\displaystyle \min _{\mu _{G}}\max _{\mu _{D}}L(\mu _{G},\mu _{D})=\max _{\mu _{D}}\min _{\mu _{G}}L(\mu _{G},\mu _{D})} that is, the move order does not matter. However, since the strategy sets are both not finitely spanned, the minimax theorem does not apply, and the idea of an "equilibrium" becomes delicate. To wit, there are the following different concepts of equilibrium: Equilibrium when generator moves first, and discriminator moves second: μ ^ G ∈ arg ⁡ min μ G max μ D L ( μ G , μ D ) , μ ^ D ∈ arg ⁡ max μ D L ( μ ^ G , μ D ) , {\displaystyle {\hat {\mu }}_{G}\in \arg \min _{\mu _{G}}\max _{\mu _{D}}L(\mu _{G},\mu _{D}),\quad {\hat {\mu }}_{D}\in \arg \max _{\mu _{D}}L({\hat {\mu }}_{G},\mu _{D}),\quad } Equilibrium when discriminator moves first, and generator moves second: μ ^ D ∈ arg ⁡ max μ D min μ G L ( μ G , μ D ) , μ ^ G ∈ arg ⁡ min μ G L ( μ G , μ ^ D ) , {\displaystyle {\hat {\mu }}_{D}\in \arg \max _{\mu _{D}}\min _{\mu _{G}}L(\mu _{G},\mu _{D}),\quad {\hat {\mu }}_{G}\in \arg \min _{\mu _{G}}L(\mu _{G},{\hat {\mu }}_{D}),} Nash equilibrium ( μ ^ D , μ ^ G ) {\displaystyle ({\hat {\mu }}_{D},{\hat {\mu }}_{G})} , which is stable under simultaneous move order: μ ^ D ∈ arg ⁡ max μ D L ( μ ^ G , μ D ) , μ ^ G ∈ arg ⁡ min μ G L ( μ G , μ ^ D ) {\displaystyle {\hat {\mu }}_{D}\in \arg \max _{\mu _{D}}L({\hat {\mu }}_{G},\mu _{D}),\quad {\hat {\mu }}_{G}\in \arg \min _{\mu _{G}}L(\mu _{G},{\hat {\mu }}_{D})} For general games, these equilibria do not have to agree, or even to exist. For the original GAN game, these equilibria all exist, and are all equal. However, for more general GAN games, these do not necessarily exist, or agree. Main theorems for GAN game The original GAN paper proved the following two theorems: Interpretation: For any fixed generator strategy μ G {\displaystyle \mu _{G}} , the optimal discriminator keeps track of the likelihood ratio between the reference distribution and the generator distribution: D ( x ) 1 − D ( x ) = d μ ref d μ G ( x ) = μ ref ( d x ) μ G ( d x ) ; D ( x ) = σ ( ln ⁡ μ ref ( d x ) − ln ⁡ μ G ( d x ) ) {\displaystyle {\frac {D(x)}{1-D(x)}}={\frac {d\mu _{\text{ref}}}{d\mu _{G}}}(x)={\frac {\mu _{\text{ref}}(dx)}{\mu _{G}(dx)}};\quad D(x)=\sigma (\ln \mu _{\text{ref}}(dx)-\ln \mu _{G}(dx))} where σ {\displaystyle \sigma } is th