[site]: crossvalidated
[post_id]: 321342
[parent_id]: 
[tags]: 
Question about batch normalization implementation, does each batch belong to the output of each neuron over the whole training set?

I'm trying to follow this paper https://www.arxiv-vanity.com/papers/1502.03167/ http://proceedings.mlr.press/v37/ioffe15.pdf Since the full whitening of each layerâ€™s inputs is costly and not everywhere differentiable, we make two necessary simplifications. The first is that instead of whitening the features in layer inputs and outputs jointly, we will normalize each scalar feature independently, by making it have the mean of zero and the variance of 1. For a layer with d-dimensional input we will normalize each dimension where the expectation and variance are computed over the training data set. As shown in (LeCun et al., 1998b), such normalization speeds up convergence, even when the features are not decorrelated. So from what I understand, to implement batch normalization, first you need to run the whole training set over the first layer, collect the outputs of each neuron in the first layer, and when apply the normalization over all the outputs of each neuron; each neuron has it's own batch to normalize over? The normalization doesn't occur for all the outputs in the first layer. Then, once each neuron in the first layer has their outputs normalized, then those outputs can be fed into the 2nd layer. So, the 2nd layer of the neural network can't receive any input until the whole training set has been run through the first layer, since the whole training set is needed to normalize all the batches from the first layer? Is this summary accurate?
