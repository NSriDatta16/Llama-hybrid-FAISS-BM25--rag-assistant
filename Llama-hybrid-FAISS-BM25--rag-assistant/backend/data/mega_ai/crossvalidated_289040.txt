[site]: crossvalidated
[post_id]: 289040
[parent_id]: 289036
[tags]: 
The answer you seek is highly domain dependent. But before I get into that I want to introduce some terminology that might be useful. Transfer learning is the improvement of learning in a new task through the transfer of knowledge from a related task that has already been learned. Reference for Transfer Learning Boosting, bagging and randomization are methods to improve model performance but on samples of same data. Boosting and bagging are more specifically ensemble methods that create a number of classifiers and then combine them using various methods to get an improved model - or fine tuning as you say. The reason the distinction between transfer learning vs. other techniques is important - problem domain gets involved. In deep learning, let us say the new data is similar but not exactly the same, even the response categories can vary. For this scenario you would use transfer learning and not the other methods ( Tensorflow tutorial ). But if you're predicting say ham / spam and find that your algorithm is not working well on a small population of spam mails or if a certain population of your spam mails is too small to be significant for your classifier, say logistic regression- you would benefit from using an algorithm that accommodates resampling techniques such as boosting bagging ( example XGBoost - one of the frequently winning algorithms on Kaggle). Theoretically algorithms like XGBoost are robust against overfitting, but there's a possibility to overfit. Related answer explores overfitting aspect more.
