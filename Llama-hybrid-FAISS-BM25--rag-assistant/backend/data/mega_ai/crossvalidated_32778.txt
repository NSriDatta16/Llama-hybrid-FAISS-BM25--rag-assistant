[site]: crossvalidated
[post_id]: 32778
[parent_id]: 31830
[tags]: 
Let's say your estimate of a distribution is pretty much right. It looks like a log normal distribution with parameters 0 and 0.7: The mean of a variable with this distribution is 1.278 and the median is 1.00 (fitting with your estimate of the 50% likelihood value); the variance is 1.032 (more than you would estimate from the PERT method if I understand it correctly). If we took 40,000 independent instances of this variable and added them together we would get a new variable that is normally distributed (because of the central limit theorem), with mean $40000\times 1.278=51100$ and a standard distribution of only $\sqrt{40000*1.032}=203$. In other words, you are virtually certain the needed amount is within say 600 of 51100. This is the "flaw of averages" issue - if the random variables are independent then you really can say with a lot of certainty what is going to happen when you add enough of them up together; you don't need to add the worst likely scenario n times. But lets take another possibility - there's no individual level randomness at all, all the users will end up using exactly the same amount of storage, perhaps because storage is dictated by environmental factors eg what software they have to use. This isn't as odd as it may sound, because technological change is surely a big source of the uncertainty in how much storage is needed, and its something that impacts on each individual in a common way. In this case, effectively your initial estimates just get multiplied by 40,000; if your worst case scenario of 5MB turns out to be true, it's true for everyone, every year and you need 200,000 MB of storage. These two scenarios look like the following: Of course, the reality probably lies somewhere between these two. There's a chunk of uncertainty that impacts on everyone equally (including your own estimation skills); and then some individual randomness. I appreciate this hasn't exactly answered your question, and I haven't engaged with the Mathematica or PERT issues at all. From what I can understand of your question and the other answer, it looks like the relevant Mathematica function is more in line with my cautious "everyone makes the same decision" approach. This seems to (sensibly) put a lot of weight on the uncertainty about the whole distribution and external environment. I suppose my answer's theme, such as it is is that there is a lot of uncertainty about the uncertainty here - and that is just when we deal with the known unknowns. If we add into this any uncertainty about your initial parameter estimates, things get even wilder. So I would be very cautious about relying on the maths other than as a ballpark
