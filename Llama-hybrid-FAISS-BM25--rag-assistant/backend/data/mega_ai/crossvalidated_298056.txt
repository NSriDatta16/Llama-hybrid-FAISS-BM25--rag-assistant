[site]: crossvalidated
[post_id]: 298056
[parent_id]: 298053
[tags]: 
On Wikipedia that quote is flagged as "further explanation needed", and I would say that backpropagation is only tangentially related to solving XOR. In fact you can construct a 2-layer feedforward network that solves XOR quite easily by hand, although of course that has not "learned" XOR. Various optimisation search methods such as simulated annealing (invented 1953) and genetic algorithms (1970s) were known at the time. The innovation required to go from perceptrons that cannot approximate XOR to multi-layer perceptrons that can is the extra layers. I don't know if the article writer had something specific in mind, but I think they have conflated the idea of multi-layer networks and backpropagation. However, the two are strongly linked, as the invention of backpropagation is what makes training of multi-layer neural networks generally feasible. Other approaches are possible, but are much less efficient, especially when the number of parameters (and layers) grows.
