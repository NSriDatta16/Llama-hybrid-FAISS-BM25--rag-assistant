[site]: datascience
[post_id]: 13629
[parent_id]: 
[tags]: 
Highly accurate convnets appear to have random-looking visualized weights?

I'm building a TensorFlow convoluted neural network that isn't getting the accuracy that I hoped for. So I figured I would visualize the learned weights to see where the network might be stumbling. As a benchmark I started visualizing the weights for a totally different project, Google's MNIST convnet example , which has very high accuracy (99.2%). I assumed that a very accurate model would have intuitive weights, but in reality I got weights that looked completely random. Other people seem to get similarly random results . See martin-gorner's comment with visuals on Mar 18. His are results are similar to mine. More Googling shows that other people see essentially random behavior for non-MNIST datasets. Is this common? If so, it would seem that visualizing weights is a fruitless exercise unlikely to bring any helpful insights to the modeler. Is this true? If not, why do so many people use this visual (clear edges in the first layer, and composite images in subsequent layers) when describing how convnets work? Stanford's course material also seems to at least show edges (you have to scroll down about half of the page) when visualizing the weights. I'm not sure what to think.
