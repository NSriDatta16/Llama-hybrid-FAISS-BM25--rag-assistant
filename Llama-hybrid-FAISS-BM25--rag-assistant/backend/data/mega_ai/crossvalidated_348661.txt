[site]: crossvalidated
[post_id]: 348661
[parent_id]: 
[tags]: 
Posterior distribution regression model

Suppose we have the regression model $$y_t=x_t'\beta +\lambda_t\epsilon_t$$ with flat prior $p(\beta) \propto 1$ , i.i.d $\epsilon_t \sim N(0,1)$, $\lambda_t$ i.i.d and fixed regressors $x_t$. Now suppose I am interested in the posterior $p(\beta| \lambda, y) $ where $y$ and $\lambda$ are the vectors with $y_t$ and $\lambda_t$ stacked over time. Because we are looking at the posterior of $\beta$ conditional on $\lambda$, it seems that we could look at usual the regression model with constant error variance $$\frac{y_t}{\lambda_t}=\frac{x_t}{\lambda_t}\beta+\epsilon_t$$ and make use of the known bayesian regression model result which then suggests that $p(\beta| \lambda, y)$ is normal with mean $(\sum_{t=1}^T\lambda^{-2}_tx_tx_t')^{-1}\sum_{t=1}^T\lambda^{-2}_tx_ty_t$ and covariance matrix $(\sum_{t=1}^T\lambda^{-2}_tx_tx_t')^{-1}$. How can we rigorously justify this step?
