[site]: crossvalidated
[post_id]: 126251
[parent_id]: 
[tags]: 
How do I force the L-BFGS-B to not stop early? Projected gradient is zero

I'm trying to use the SciPy implementation of the fmin_l_bfgs_b algorithm using the following code: imgOpt, cost, info = fmin_l_bfgs_b(func, x0=img, args=(spec_layer, spec_weight, regularization), approx_grad=1,bounds=constraintPairs, iprint=2) The variable img is simply a vector containing 784 pixels, where all the corners are set to 0 and the middle part is initialized randomly between 0 and 255. The bounds for corners are (0,0) and for the middle part (0, 255). The function is the weighted input of a hidden neuron in my neural network. None of this should be special in any way. However, when I run the algorithm it stops immediately because the projected gradient is zero. How can I help the algorithm find a proper gradient-estimate so it doesn't stop immediately? Output: RUNNING THE L-BFGS-B CODE it = iteration number nf = number of function evaluations nseg = number of segments explored during the Cauchy search nact = number of active bounds at the generalized Cauchy point sub = manner in which the subspace minimization terminated: con = converged, bnd = a bound was reached itls = number of iterations performed in the line search stepl = step length used tstep = norm of the displacement (total step) projg = norm of the projected gradient f = function value * * * Machine precision = 2.220D-16 N = 784 M = 10 it nf nseg nact sub itls stepl tstep projg f 0 1 - - - - - - 0.000D+00 1.694D+00 CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_
