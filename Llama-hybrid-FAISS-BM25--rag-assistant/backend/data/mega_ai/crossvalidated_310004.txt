[site]: crossvalidated
[post_id]: 310004
[parent_id]: 
[tags]: 
Why does the paper Layer Normalization discuss the invariance properties of different normalization schemes?

When I read Related Work part in the paper Layer Normalization , it just analyses its invariance properties and compares it with those of others normalization methods, like batch normalization. But this paper does not talk how this property effect the entire neural networks, like CNN or RNN, specifically. Can you tell me more details about the effects or importance of invariance properties of normalization in NN?
