[site]: crossvalidated
[post_id]: 632072
[parent_id]: 
[tags]: 
Estimating expected value with respect to posterior

I have a neural network and I need to calculate the following: $$\mathbb{E}_{P(\theta|D)}[f(\theta)]=\frac{\sum_\theta P(D|\theta)P(\theta)f(\theta)}{\sum_\theta P(D|\theta)P(\theta)}$$ Where $f$ , generally, can be any function, $\theta$ is the parameters of the neural network, and consequently, $P(D|\theta)$ is defined by the neural network (let's say a CNN), and $D$ is my dataset. I define $P(\theta)$ to be a gaussian with a fixed variance, but in general it can be anything really. Since we're talking about a deep neural network, this is not easy to compute even with MCMC. So, one possibility I could think of for estimating this value is to try to estimate the sums in both the nominator and the denominator with as many large terms in them as possible: Basically, assuming $P(D,\theta)=P(D|\theta)P(\theta)$ takes either very large values or very small values, I train the network with SGD on my data $D$ to maximize the objective function $P(D,\theta)$ , and repeat this $N$ times. Each of those $N$ times, SGD lands on a different set of parameters $\theta_i$ for $1\leq i\leq N$ . If we can assume (☆) the probability of landing on $\theta_i$ is proportional to $P(D,\theta_i)$ , then the following would be the estimate of the desired value: $$\frac{\sum_i f(\theta_i)}{N}$$ The idea here is basically to directly sample from the posterior, which is usually done via more popular sampling methods such as MCMC, but here, because it would take a long time due to the dimensionality of the neural network, I do it with another "sampling" algorithm (i.e. multiple runs of SGD). But since I am not sure of that assumption (☆), instead I assume in the region of the parameter space where $P(D,\theta)$ is large, SGD can land on any parameters with the same probability. So, I use the following as my estimate. $$\frac{\sum_i P(D,\theta_i)f(\theta_i)}{\sum_i P(D,\theta_i)}$$ How good or bad is this estimation? Is there an upper bound or lower bound on its error? Also, is there a better way for doing this that can work on neural networks in the order of tens of thousands of parameters? I feel like this problem must be popular enough to have better solutions but I am not even sure where to look.
