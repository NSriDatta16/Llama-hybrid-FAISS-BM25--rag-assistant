[site]: crossvalidated
[post_id]: 306002
[parent_id]: 
[tags]: 
Measuring the size of a molecule with a ruler and enough independent measurements?

This question arose from the conversation on a previous question of mine. Also, I purposely came up with a ridiculous attention-grabbing title and question to facilitate the discussion more easily. I am not actually interested in measuring the size of a molecule with a ruler. [Here is the seeming paradox]: Give a ruler to somebody and try to measure the size of a molecule with a ruler. They will get some reading $x=s+\delta$, where $s$ is the true value and $\delta$ is the error. If there are no systematic errors, then the error $\delta$ solely comes from instrument imprecision and human clumsiness/incapacity, and thus it is random. Now give rulers to a huge number of people, enough for the law of large numbers to well be valid, and get an independent measurement from each of them. The error in the average that you obtain from this huge sample goes down by $1/\sqrt{N}$. Thus, since you are certain no systematic average-shifting errors are present, the average of all those measurements gives you the true value of the size of the molecule with a precision of $1/\sqrt{N}$. [Here is my attempted resolution]: Although each size-measurement is in principle a random drawing from a continuous distribution, the finite resolution of our measurement device effectively puts each size-measurement into a bin, making each measurement discrete in a certain sense. For example, if our resolution was $1\text{mm}$, this is the distribution we would get: In this case, I believe all we can is that the probability of receiving a measurement $x=0\text{mm}$ is $100\%$, and we can say that with a relative precision of $1/\sqrt{N}$. Since the resolution of our ruler was $1\text{mm}$ to start with, we can therefore say the true molecule-size is within the range $(0\text{mm},1\text{mm})$ with $1/\sqrt{N}$ precision, which is equivalent to just saying our uncertainty in measuring the molecule-size is $1\text{mm}$, as it intuitively was all along. [My question:] I know all of this may seem stupid and contrived, but I am really struggling to understand how in this particular case my reasoning is flawed, i.e. how we really can measure the size of a molecule with enough people and rulers. I believe I am wrong because every well-qualified source ( see linked question ) seems to say otherwise, which I personally don't understand. [Important Qualifiers]: To be totally clear, I am not talking about estimating the probability of obtaining discrete measurements (e.g. $0\text{mm}$, $1\text{mm}$, etc.). That is obviously a measurement that can be refined indefinitely with more samples (in principle). I am talking about refining a histogram, which from the outset is partitioned into slots of size equal to the measurement resolution, into a finer partitioning through repeated measurements. To me this doesn't seem to be possible, but once again, all sources seem to indicate otherwise. Please do not just say "Sorry bud, it's a mathematical fact that repeated measurements refine the precision of the final averaged measurement. It's called standard error of the mean, and it drops off as $1/\sqrt{N}$. Look at how precisely scientists have measured $X$ phenomenon." I obviously know that. That is why I've constructed a single contrived case to highlight my misunderstanding. The context of my question is that I am a TA for a freshman physics lab, and many students have asked whether instrument precision is a statistical error or systematic error. I told them it is a statistical error, since its effect when determining derived quantities is diminished through repeated measurements, or more simply put, you can handle it with statistics. But now after thinking more about it, I'm a little confused myself, and that made me post the linked question , which led me to this question.
