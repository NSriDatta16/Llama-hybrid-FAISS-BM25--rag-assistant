[site]: crossvalidated
[post_id]: 329628
[parent_id]: 329114
[tags]: 
The code looks fine. Without access to your data, at least in standardized/anonymized form, it's impossible to give a definitive answer. However, when in this kind of predicament there is a standard set of checks worth going through 1 : some of these steps don't apply to you, but I'll include them anyway to make the list more comprehensive. As a best practice these checks should be done before training the DNN, but they can be of help also when you have already used $k-$fold CV and/or training/validation/test set split to estimate correctly the generalization error, and you found a suspiciously low value. check that all data transforms (or estimators, in Tensorflow lingo), are fit on the training set and applied to the test set, for each fold of $k-$fold CV. Using pipelines, as you did, is a simple way to make sure of this. Just check that all operations on data are performed inside the pipeline. check that you're using the right metric for classification. In your case, accuracy makes sense because the two classes are reasonably balanced. But on a medical data set, where the incidence of the disease in the population is $0.1\%$, you will get $99.9\%$ accuracy on a representative data set, just by classifying each point to the majority class. In this case, $99.9\%$ accuracy is nothing to be surprised of, and actually accuracy is not the right metric to use here. check that the problem is not overly simple. For example, it could be linearly separable . Some folks think this can be seen with PCA: PCA has nothing to do with linearly separable classification problems. Instead, use linear discriminant analysis or linear SVM: both will fit a linearly separable problem perfectly (or nearly perfectly), and get excellent out-of-sample performance. Alternatively, you can use regularized logistic regression (unregularized LR estimated with MLE is ill-posed for linearly separable problems). Plotting decision regions would also help a lot: mlxtend 's plot_decision_regions is a simple and convenient way to do this. Of course when you have more than 2 predictors you can only visualize 2D projections of the decision boundary, on the planes defined by the various pairs of input features. create a correlation matrix including inputs and the class label: of course, computing the Pearson correlation coefficient between a continuous and a categorical variable doesn't make sense, but most good visualization tools (i.e., not matplotlib ) will automatically switch to a visualization which makes sense. If there are a few inputs which are very correlated with the class label, then, even if there is significant correlation among inputs, a neural network is likely to do well. Note : after performing check 3, this check may seem redundant, but I believe visualization never hurts. an heuristic I've found useful sometimes is to refit the NN on shuffled (permutated) labels. In this case the neural network has only one way to get good training set error: memorize the training set. This will usually manifest in much longer training time. Also, it has no way to get good test set error. If you still get excellent performance on the test set, then either there is something seriously wrong with your code (consider writing unit tests for each method you defined) or with the framework you're using. Consider opening a GitHub issue at this point. Go to the extreme opposite: train on a single (or very few) data points. You should get $\sim 100\%$ accuracy on the training set (and extremely low on the test set). If you don't, again there's something seriously flawed with your code or framework. Finally, try to obtain a few new test points, which you've never personally seen (let alone used in training) until now. The new test points must be representative of your real life usage of the NN. If you still get excellent accuracy, then get used to it, you're just very good at Deep Learning ;-) 1 though I frankly would love to be in this kind of predicament more often, when my NN does greatly both under $k-$fold CV and on a well-designed validation set, without having to touch a single line of my architecture ;-)
