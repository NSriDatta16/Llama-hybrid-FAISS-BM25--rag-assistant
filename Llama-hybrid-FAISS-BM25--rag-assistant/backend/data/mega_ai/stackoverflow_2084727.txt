[site]: stackoverflow
[post_id]: 2084727
[parent_id]: 2076398
[tags]: 
This style of (attempted) unit testing where you try to cover an entire huge code base through a single public method always reminds me of surgeons, dentists or gynaecologists whe have perform complex operations through small openings. Possible, but not easy. Encapsulation is an old concept in object-oriented design, but some people take it to such extremes that testability suffers. There's another OO principle called the Open/Closed Principle that fits much better with testability. Encapsulation is still valuable, but not at the expense of extensibility - in fact, testability is really just another word for the Open/Closed Principle . I'm not saying that you should make your private methods public, but what I am saying is that you should consider refactoring your application into composable parts - many small classes that collaborate instead of one big Transaction Script . You may think it doesn't make much sense to do this for a solution to a single vendor, but right now you are suffering, and this is one way out. What will often happen when you split up a single method in a complex API is that you also gain a lot of extra flexibility. What started out as a one-off project may turn into a reusable library. Here are some thoughts on how to perform a refactoring for the problem at hand: Every ETL application must perform at least these three steps: Extract data from the source Transform the data Load the data into the destination (hence, the name ETL ). As a start for refactoring, this give us at least three classes with distinct responsibilities: Extractor , Transformer and Loader . Now, instead of one big class, you have three with more targeted responsibilities. Nothing messy about that, and already a bit more testable. Now zoom in on each of these three areas and see where you can split up responsibilities even more. At the very least, you will need a good in-memory representation of each 'row' of source data. If the source is a relational database, you may want to use an ORM, but if not, such classes need to be modeled so that they correctly protect the invariants of each row (e.g. if a field is non-nullable, the class should guarantee this by throwing an exception if a null value is attempted). Such classes have a well-defined purpose and can be tested in isolation. The same holds true for the destination: You need a good object model for that. If there's advanced application-side filtering going on at the source, you could consider implementing these using the Specification design pattern. Those tend to be very testable as well. The Transform step is where a lot of the action happens, but now that you have good object models of both source and destination, transformation can be performed by Mappers - again testable classes. If you have many 'rows' of source and destination data, you can further split this up in Mappers for each logical 'row', etc. It never needs to become messy, and the added benefit (besides automated testing) is that the object model is now way more flexible. If you ever need to write another ETL application involving one of the two sides, you alread have at least one third of the code written.
