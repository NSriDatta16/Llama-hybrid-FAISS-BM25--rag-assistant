[site]: crossvalidated
[post_id]: 46791
[parent_id]: 46785
[tags]: 
For the hard-margin support vector machine, there is no real means of controlling the bias-variance trade-off. For that you need to use the soft-margin formulation, where the bias-variance trade-off is controlled by the regularisation parameter, $C$. The leave-one-out error is bounded by a quantity that depends on the margin (see the radius-margin bound and the span bound). The SVM is essentially an approximate implementation of an upper bound on the generalisation error, which is defined in terms of the margin. However, I suspect that the reason the SVM works well in practice on most occasions is because it encourages the user to at least think about avoiding over-fitting by tuning the regularisation parameters. The same good performance is often just as easily obtained using [kernel] ridge regression, which suggests the hinge loss doesn't make a great deal of difference (average case).
