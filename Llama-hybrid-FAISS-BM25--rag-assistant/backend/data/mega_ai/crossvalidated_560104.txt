[site]: crossvalidated
[post_id]: 560104
[parent_id]: 560098
[tags]: 
No, it is almost never a problem. First of all, there's a measurement error—even physicists account for it, while the rest of us rarely can be lucky enough for as precise measurements as theirs. Second, you are dealing with sampled data, so there is error due to sampling. Finally, we have all kinds of biases and noise in the data. In the end, we are usually far from having precise data, so we don’t need algorithms more precise than the data itself. More than this, there is research showing that you can train neural networks with low (8-bit, 2-bit) precision without performance drop . Some argue that this might even have a regularizing effect. It can probably be extended to some degree to other models.
