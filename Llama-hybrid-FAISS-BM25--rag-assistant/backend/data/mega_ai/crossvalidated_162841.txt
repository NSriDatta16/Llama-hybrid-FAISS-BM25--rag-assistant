[site]: crossvalidated
[post_id]: 162841
[parent_id]: 162833
[tags]: 
In a nutshell, in order to account for errors associated with sampling and measurement, you need to define a statistical model that describes both processes and how they interact. The $Likelihood$ framework allows you to take that statistical model and then compute the estimated errors with some assumptions. When you are defining your statistical model you will have invoked parameters that represent your sample size and measurement error. These parameters will show up in the computations in the $Likelihood$ and you can see how the total error is affect by each processes. This is how i would break down your question: 1) first I would want to understand how the heck do I even derive estimated errors. There are several ways to go about this, but the $Likelihood$ approach is the most principled. AWF Edwards gives a clear introduction in his classic text. There is even a $Bayesian$ way about it, and I find Sivia to give a clear exposition. 2) Then i would go through a classic derivation of estimating the sampling error given normally distributed data. (i think this example is covered in both Edwards and Sivia, plus there are tons of online resources) Your goal is to understand where standard error of the mean comes from. 3) Then I would figure out how measurement noise affects your data. A classic derivation is additive Gaussian noise to your measurement. It would look something like this: $Z=X+Y$ where $Z$ is what you measured, $X$ is your true sample, $Y$ is your additive Gaussian noise. At this point you should have a new formula that almost looks like standard error of the mean, but additional components coming from your measurement error. (Sivia or Edwards might have already done these derivations for you)
