[site]: crossvalidated
[post_id]: 555839
[parent_id]: 
[tags]: 
When interpreting machine learning models, should preprocessing steps be considered as part of "model"?

Suppose I have some inputs on which I first apply some feature engineering and then apply a machine learning algorithm such as random forest to make predictions. Now, if I want to interpret/explain the model for example using perturbation importance or partial dependence plot etc, should I consider the whole feature engineering step and random forest as one model or should I only try to explain the random forest using the model explainability methods. The problem with the second approach is that the features going into the random forest are transformed and it is difficult to interpret them. What is the formal way?
