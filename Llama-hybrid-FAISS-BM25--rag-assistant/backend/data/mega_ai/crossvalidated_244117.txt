[site]: crossvalidated
[post_id]: 244117
[parent_id]: 244093
[tags]: 
Let's have some simple examples to show the differences. Our example have a single independent variable x and a single dependent variable - either real y or categorical z : x y z ... 0 0.01 A 1 1.98 A 2.01 4.02 B 2.99 6.01 B ... One can see that y grows as x grows and that z=B for values of x grater than something around 1.5. That is an example when GLM and similar methods rock. It is easy to make good predictions for both y and z for a new value of x . For example, for x=0.5 you would predict y=1 and z=A ("You can understand the direct influence, and direction, of each variable" as @HEITZ wrote) x y z ... 0 0.01 A 1 1.98 B 2.01 0 A 2.99 2.01 B 4 0 A 5.01 2.00 B ... We can see a clear pattern in the data again, however GLM and similar methods cannot, the connection between x and y or z is not linear nor even additive. That is when other methods as random forests needs to be used. Prediction based on GLM for x=3 would be y=1 and more or less randomly z=A or z=B . However RF or similar methods can predict what we would expect: y=2 and z=B . Generally I would do: Try GLM and similar, if they model the data well, use it, because you can understand the model well; if not Try other methods as RF (or neural networks, etc.)
