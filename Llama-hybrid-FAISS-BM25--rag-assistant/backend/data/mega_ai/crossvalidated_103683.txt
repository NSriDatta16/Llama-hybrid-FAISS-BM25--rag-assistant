[site]: crossvalidated
[post_id]: 103683
[parent_id]: 95250
[tags]: 
In a word, yes, but it wouldn't be logistic regression anymore. The logistic regression loss function (i.e., the negative log likelihood), is essentially a regression in the log odds. Changing that to a least squares loss function will make it linear regression, which will lose three things: (i) the interpretation of the regression coefficients in terms of log odds and (ii) interpretation of the model predictions as log odds (or when exponentiated, as probabilities) (iii) that linear regression does not bound the model predictions between 0 and 1, so it can easily make predictions 1. That said, for many real world applications, simply minimizing the least squares criteria using a 0/1 response (output, DV) works quite well, because there enough noise etc so that the model never gets close to 0 or 1 (the sigmoid function around 0.5 well approximated by a straight line). Plus in terms of things like ROC performance, that doesn't matter since all you care about is the ranks of the predictions. For the misclassification error, that loss function is not differentiable and not convex (also called 0/1 loss), so it's very hard to minimize effectively. Hence, both support vector machines and logistic regression minimize two convex proxy loss functions, the hinge loss and the logistic loss, respectively, which can be seen as approximations to the 0/1 loss (convex relaxations).
