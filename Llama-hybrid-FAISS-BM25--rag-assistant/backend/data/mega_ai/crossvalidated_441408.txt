[site]: crossvalidated
[post_id]: 441408
[parent_id]: 441209
[tags]: 
We can get variable importance from XGBoost (and gradient boosting procedures in general). There are a few ways it can be computed (e.g. # of times a particular variable was used for splitting (commonly referred as Frequency ), the total gains of splits which use a particular variable (commonly referred as Gain ) and # of observations related to this features (commonly referred as Coverage )). R's xgboost package contains a method called xgb.importance that allows us to compute different feature importances in a model. That said, you might also want to explore the concept of Shapley values and how they can be used to find the most impactful features for a given model (and/or particular observation if needed). Assuming one is using R, the package iml is a great tool to first check. I would also suggest exploring in a bit more detail the reason why glm returns this warning. Note that probabilities numerically equal to 0 or 1 are not categorical evidence that the GLM fitting procedure failed. I would suggest looking into this very informative CV.SE thread on: Unstable logistic regression when data not well separated . Aside that I would suggest considering the use of a regularised logistic regression (e.g. ridge regression through glmnet with alpha=0 ). Computationally, this allow the iterative model fitting procedure used by a GLM to exhibit "higher convexity", i.e. the ML procedure is able to converge to a minimum. Again, CV.SE has a great thread on this matter here: Regularization methods for logistic regression .
