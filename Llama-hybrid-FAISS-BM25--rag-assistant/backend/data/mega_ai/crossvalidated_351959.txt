[site]: crossvalidated
[post_id]: 351959
[parent_id]: 350869
[tags]: 
OP has answered this question by stating I think I have found what I was looking for. I can approximate my $ y = f(x_1, x_2, x_3, x_4, x_5, x_6) = g(x_1, x_2, x_3, x_4, x_5, x_6) + \varepsilon $ using an Artificial Neural Net, then from there using a genetic algorithm to optimize the output. but I don't think that this is a good solution. Using a neural network inherently introduces lots of hard problems: selecting how many neurons in what configuration, an initialization method, an activation function, an optimizer, a learning rate, regularization strategies (L1, L2, dropout, mixup, max norm...) -- to succeed, all of these things must be chosen to be "just right" for whatever problem you're trying to solve. Then the genetic algorithm step adds more complexity, and more tuning, on top of that. If you generalize from using a neural network to any function approximation method, you are employing a surrogate surface , which you denote as $g$ , and performing the optimization over that surface. There's no particular need to use a neural network. A popular method to use in place of a neural network is a Gaussian process. See: Jones et al (1998), " Efficient Global Optimization of Expensive Black-Box Functions ." One desirable attribute that you would want your surrogate surface interpolates well between your data points, neither changing values too quickly or too slowly. A well-studied category of surrogate surfaces for this type of problem is the Gaussian process; these methods are usually characterized in terms of a length-scale parameter which exactly controls the behavior between data points. In this sense, there is essentially only one "knob," the length-scale, that you have to fiddle with. Once you're satisfied that you have a "good" surrogate surface, you can directly optimize it. The values $g$ and their gradients are easily computed and cheap to evaluate, so you can quite readily use multi-start optimization, or really any global optimization method, to find optima on the surface.
