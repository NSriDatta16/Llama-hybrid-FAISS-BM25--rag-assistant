[site]: crossvalidated
[post_id]: 159528
[parent_id]: 158382
[tags]: 
The model that is closest to the true data generating process will always be best and will beat most ensemble methods. So if the data come from a linear process lm() will be much superior to random forests, e.g.: set.seed(1234) p=10 N=1000 #covariates x = matrix(rnorm(N*p),ncol=p) #coefficients: b = round(rnorm(p),2) y = x %*% b + rnorm(N) train=sample(N, N/2) data = cbind.data.frame(y,x) colnames(data) = c("y", paste0("x",1:p)) #linear model fit1 = lm(y ~ ., data = data[train,]) summary(fit1) yPred1 =predict(fit1,data[-train,]) round(mean(abs(yPred1-data[-train,"y"])),2)#0.79 library(randomForest) fit2 = randomForest(y ~ ., data = data[train,],ntree=1000) yPred2 =predict(fit2,data[-train,]) round(mean(abs(yPred2-data[-train,"y"])),2)#1.33
