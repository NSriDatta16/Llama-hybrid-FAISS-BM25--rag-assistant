[site]: crossvalidated
[post_id]: 465882
[parent_id]: 465870
[tags]: 
You are right that deleting a feature which has a large weight will cause a decrease in performance. The weight does signify the importance of that feature. A highly positive weight signifies a strong positive correlation between the variables. To understand this better, a simple equation of a neural network is given by, $$Z=x1w1+x2w2+x3w3$$ where w1, w2 and w3 are the weights, x1, x2 and x3 are the features and Z is the output. Higher the weight of one feature, more the output is influenced by it. This holds true when you use an activation function like sigmoid as well. You can also check this paper out - http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.45.9756&rep=rep1&type=pdf . It has a few points that you might find useful.
