[site]: crossvalidated
[post_id]: 571202
[parent_id]: 570851
[tags]: 
I believe refitting a trained random forest with sufficient number of deep trees on a subset of high-impurity features will reduce variance and introduce minor bias. But is it worth computationally - training two ensembles on thousands of features? I'd rather combine tree-based ensembles with, say, linear models like RidgeClassifier or LogisticRegression(penalty='l1') , having selected $p or $p \leq n$ features, respectively (although I don't have theoretical justifications of combining impurity- and residual-based models). is it possible to use my trained Classifier (trained using all genes) and only provide it with the 10 features that I am interested in. You should provide exactly the same number of features every time you make predictions, otherwise, scikit-learn will throw an exception. If you want your final predictor to use only informative features, consider using SelectFromModel along with Pipeline . Speaking of linear models with $L_1$ penalty and similar models updating coefficients coordinate-wise by applying soft-thresholding, the nonzero coefficients tend to be biased toward zero, so debiasing the model by refitting LinearRegression or Ridge with smaller penalty on the chosen feature subset makes sense: from sklearn.feature_selection import SelectFromModel from sklearn.linear_model import Lasso, Ridge model = make_pipeline(SelectFromModel(Lasso()), Ridge(alpha=1e-4))
