[site]: crossvalidated
[post_id]: 390940
[parent_id]: 390829
[tags]: 
(My solution is basically the same as what @JimB suggested in his comment.) First, some theory: We can think of $[x]=x+\epsilon$ where $f(\epsilon) \sim (1-\delta)+2\delta\epsilon$ for $1/2\le \epsilon \le 1/2$ . This is a sloping distribution that approximates $N(x|\mu,\sigma^2)$ ) by a line with slope $2\delta=N'([x]|\mu,\sigma^2)=-\frac{[x]-\mu}{\sigma^2} N([x]|\mu,\sigma^2)$ . We see that $\delta$ is nearly zero when $x \sim \mu$ or when $x\gg\mu$ or $x\ll\mu$ . $\delta$ is a maximum value around the inflection points $[x] \sim \pm \sigma$ , which is where a lot of the action happens. The mean of $y$ is found to be an unbiased estimator of $\mu$ . $$E(y)=E([x])=E(x+\epsilon)=E(x)+E(\epsilon)=E(x)=\mu$$ where I have used $E(\epsilon)=0$ since $E(\delta)=0$ by symmetry. For variance, $$var(y)=var([x])=var(x+\epsilon)=var(x)+var(\epsilon)$$ where $var(\epsilon)$ is estimated as a function of $\delta$ to be $$var(\epsilon) \sim 1/12 - \delta^2/36=\frac{1}{12} (1-\delta^2/3).$$ This would now have to be averaged over the distribution of values of $\delta,$ which is doable but tedious. On 10,000 simulations of 1000 random normals with means around 50 and variances around 100, I find which contains at least one big surprise. The average value is $1/12$ as perhaps expected, but this figure shows that for about half of the simulations, the effect of rounding the data to integer is to underestimate $\sigma^2$ rather than to overestimate it, as would be expected from basic probability theory. Perhaps this is because there is assumption that $x$ and $\epsilon$ are independent is false! Regardless, now to the optimization problem in $\mu$ and $\sigma$ Here is an example, in R. rm(list=ls()) mu=50 sigma=10 N=1000 data=rnorm(N,mu,sigma) idata=round(data,0) indices One example of this gives [1] "target variance 127.743486150357" [1] "sample variance 124.050532385344" [1] "sample rounded variance 124.173364364364" [1] "optimized variance 123.965644616766" In this one case, we see that rounding increases the variance and that optimizing reduces the estimate to a value closer to the original sample variance, which is the best linear unbiased estimator of the target variance.
