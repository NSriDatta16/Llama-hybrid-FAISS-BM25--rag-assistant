[site]: crossvalidated
[post_id]: 567045
[parent_id]: 
[tags]: 
Adam is an adaptive learning rate method, why people decrease its learning rate manually?

Adam optimizer is an adoptive learning rate optimizer that is very popular for deep learning, especially in computer vision. I have seen some papers that after specific epochs, for example, 50 epochs, they decrease its learning rate by dividing it by 10. I do not fully understand the reason behind it. How do we do that in Pytorch?
