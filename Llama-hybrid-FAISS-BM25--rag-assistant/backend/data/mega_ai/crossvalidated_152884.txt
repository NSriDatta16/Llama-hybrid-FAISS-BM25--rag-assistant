[site]: crossvalidated
[post_id]: 152884
[parent_id]: 
[tags]: 
Limitations of ensemble selection from libraries

Question related to the approach in Caruana's paper: "Ensemble Selection from Libraries of Models" (linked below) http://www.cs.cornell.edu/~caruana/ctp/ct.papers/caruana.icml04.icdm06long.pdf Seems like this approach should be applicable generically to any classification/regression problem, and the results provided do seem to indicate that the performance of bagged ensemble selection is better than the best individual model instance. I havent seen much critical evaluation of the ideas in that paper, which makes me feel that after obtaining a data set (size of data ignored for the present), if enough data is available, then Split as train/validate/test Maybe do some feature engineering? Generate huge library of models Bag and forward select the best model incrementally Profit! What if only a small fraction of the generated library of models are indeed of meaningful accuracy? I found the paper somewhat light on these details (note: i might well need to understand the material better and the previous statement might not be accurate). Seems to take the art (perhaps good if that is indeed what it does?) of finding the right class of model out of the process. When should such an approach fail? **Update I'd also like to know what the relation to model selection driven overfitting is in the scenario above. For example, per Cawley's paper "On overfitting in Model Selection and Subsequent Selection Bias in Performance Evaluation", i would expect that the approach in Caruana's paper to be prone to overfitting (driven by greedy model selection). Is this true, and if so, how can Caruana's approach be updated to include nested cross-validation?
