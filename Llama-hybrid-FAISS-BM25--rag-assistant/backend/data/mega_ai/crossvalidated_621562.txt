[site]: crossvalidated
[post_id]: 621562
[parent_id]: 
[tags]: 
OOB in Random Forests - Detailed Calculation

In the book An introduction to statistical learning, it is mentioned that: One can show that on average, each bagged tree makes use of around two-thirds of the observations. The remaining one-third of the observations not used to fit a given bagged tree are referred to as the out-of-bag (OOB) observations. We can predict the response for the ith observation using each of the trees in which that observation was OOB. This will yield around B/3 predictions for the ith observation. In order to obtain a single prediction for the ith observation, we can average these predicted responses (if regression is the goal) or can take a majority vote (if classification is the goal). This leads to a single OOB prediction for the ith observation. An OOB prediction can be obtained in this way for each of the n observations, from which the overall OOB MSE (for a regression problem) or classification error (for a classification problem) can be computed. The resulting OOB error is a valid estimate of the test error for the bagged model, since the response for each observation is predicted using only the trees that were not fit using that observation. I think I understand the overall intuition, however, to be sure, I would greatly appreciate if someone could refer me to a tutorial/paper/etc (available online) that illustrates this with a simple example and a small forest (perhaps 2-3 trees). I have gone over other posts such as this one but these are mostly high-level. Thanks in advance
