[site]: crossvalidated
[post_id]: 169351
[parent_id]: 169156
[tags]: 
The curse of dimensionality is somewhat fuzzy in definition as it describes different but related things in different disciplines. The following illustrates machine learning’s curse of dimensionality: Suppose a girl has ten toys, of which she likes only those in italics: a brown teddy bear a blue car a red train a yellow excavator a green book a grey plush walrus a black wagon a pink ball a white book an orange doll Now, her father wants to give her a new toy as a present for her birthday and wants to ensure that she likes it. He thinks very hard about what the toys she likes have in common and finally arrives at a solution. He gives his daughter an all-coloured jigsaw puzzle. When she does not like, he responds: “Why don’t you like it? It does contain the letter w. ” The father has fallen victim to the curse of dimensionality (and in-sample optimisation). By considering letters, he was moving in a 26-dimensional space and thus it was very likely that he would find some criterion separating the toys liked by the daughter. This did not need to be a single-letter criterion as in the example, but could have also been something like contains at least one of a, n and p but none of u, f and s. To adequately tell whether letters are a good criterion for determining which toys his daughter likes, the father would have to know his daughter’s preferences on a gargantuan amount of toys¹ – or just use his brain and only consider parameters that are actually conceivable to affect the daughter’s opinion. ¹ order of magnitude: $2^{26}$, if all letters were equally likely and he would not take into account multiple occurrences of letters.
