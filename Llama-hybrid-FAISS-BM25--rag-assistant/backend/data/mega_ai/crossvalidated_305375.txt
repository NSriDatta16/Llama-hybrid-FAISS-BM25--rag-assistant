[site]: crossvalidated
[post_id]: 305375
[parent_id]: 238268
[tags]: 
Question: Is it feasible to estimate this kind of confidence? In general, the idea is reasonable in my opinion. However, especially in an active learning setting with its naturally incurred sampling bias to the model, you have to consider what such a confidence measure means - a notion of confidence of your model, based on the data it has seen so far . Of course, the models uncertainty is one of the factors considered in active learning literature. With probabilistic models it can directly be taken from the model, while with SVMs for example, the distance to the decision hyperplane(s) serves as a proxy. But what does the confidence/uncertainty of the model tell us? It gives us a measure of how confident we can be in the models predictions. Consider the example below: with the two classes blue & red, the circles indicating labeled instances and the small Xs indicating unlabeled data - how confident are we in the models prediction? If we estimate how well our model explains the labeled data we have sampled so far (here with uncertainty sampling and random instances as initialization) we will probably be happy with the result. However , without any notion of how well our sampling actually covers the available data, we might be deceived by the confidence in the model. Of course, in the sketched example it is easy to see - with data of higher dimensionality and less nicely separated clusters, this is hardly possible.
