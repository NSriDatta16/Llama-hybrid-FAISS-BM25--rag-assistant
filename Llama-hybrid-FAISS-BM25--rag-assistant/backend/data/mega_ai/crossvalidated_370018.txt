[site]: crossvalidated
[post_id]: 370018
[parent_id]: 369500
[tags]: 
Quite simple, yet actionable approach: Collect your data, preprocess them to get categorical features $X$ . Create, tune, optimize the Bayesian network for $X$ with bnlearn. As a result we have practically a probability distribution $p(X)$ . Take all your observations and calculate their likelihoods $L_i=p(x_i)$ . Based on the likelihoods define a threshold $\theta$ for false negatives, i.e. if the desired sensitivity is e.g. 95%, you should take the likelihood that corresponds to the 5th quantile. The resulting classifier is then: $p(X)>\theta$ . The trick is that you never know how similar are the unobserved counter examples. However, based on some ex-post observations, you can tune the threshold also with the respect to specificity .
