[site]: crossvalidated
[post_id]: 579930
[parent_id]: 579927
[tags]: 
Likely it should not matter. If taking every 100th point is essentially a random sample from the larger population, the means and variance estimates should be more or less the same. If there is significant difference, that may be an indication that taking every 100th point is not really random (e.g. your data is autocorrelated at that lag), and thus should be avoided. The other question is why do you need them standardized? If it is just to make the variables comparable so that the algorithm does not prioritize one over another - then it is ok if they are approximately comparable and precise standardization is not required. If for whatever reason you do require precise standardization of the sample, then of course you normalize by the variance of the resulting sub-sampled dataset. Thus, my recommendations are: If you want to stick with taking every 100th point, use either estimate, but make sure to compare the two approaches for validation purposes. As a good measure, I would also compute autocorrelation at that lag, hopefully finding that it is zero, or reconsidering your life choices If I was you, I would instead take averages over every 100 points. While it blurs data significantly, at the same time it reduces instrument noise and is much less vulnerable to autocorrelation effects. In this case you would standardize after taking averages, because variance of the mean is not the same as variance of the sample.
