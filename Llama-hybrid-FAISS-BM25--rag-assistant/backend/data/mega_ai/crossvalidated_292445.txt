[site]: crossvalidated
[post_id]: 292445
[parent_id]: 
[tags]: 
How to implement "Bilinear loss" function in R/tensorflow

I was reading Resheff et al 2017[1] and trying to implement their idea for a loss function in R. I am not sure if I understand correctly. It appears simple enough: Specifically, let $a_{i,j}$ denote the relative cost associated with assigning the label $j$ to an example whose correct label is $i$, and let $A = {a_{i,j}} \in \mathscr{R}^{k \times k}$ denote the penalty matrix. Using $A$ we define two related loss functions: The Bilinear loss is defined as: $L_{B} = y^{T} A \hat{y}$ I did the following, but am not sure: R code: set.seed(1234) # Helper softmax function softmax = function(logits){ exp(logits)/sum(exp(logits)) } # Define penalty matrix (multipliers for the loss) penaltyMat = matrix(1, nrow = 3, ncol = 3) penaltyMat[1, 3] = penaltyMat[3, 1] = 2 rownames(penaltyMat) = colnames(penaltyMat) = c("A", "B", "C") # Generate target (true classes) targ = matrix(0, nrow = 10, ncol = 3) sel = sample(3, nrow(targ), replace = T) for(i in 1:length(sel)){ targ[i, sel[i]] = 1 } # Generate logits (for predicted classes) logits = matrix(runif(length(targ), -10, 10), nrow = 10, ncol = 3) # Calculate Bilinear Loss BilinearLoss = targ*(softmax(logits) %*% penaltyMat) Results: > penaltyMat A B C A 1 1 2 B 1 1 1 C 2 1 1 > targ [,1] [,2] [,3] [1,] 1 0 0 [2,] 0 1 0 [3,] 0 1 0 [4,] 0 1 0 [5,] 0 0 1 [6,] 0 1 0 [7,] 1 0 0 [8,] 1 0 0 [9,] 0 1 0 [10,] 0 1 0 > logits [,1] [,2] [,3] [1,] 3.8718258 -3.6677509 -0.8781704 [2,] 0.8994967 -3.9461326 -4.6962666 [3,] -4.3453283 -6.8190799 -3.9065559 [4,] 8.4686697 -9.2000816 0.1461374 [5,] -4.1536832 -5.6240092 -6.3780758 [6,] 6.7459126 6.2119710 5.1934127 [7,] -4.2755343 0.5139509 -5.9750392 [8,] -4.6635844 8.2931633 -4.8238036 [9,] -6.2655442 6.6269009 9.8430084 [10,] -5.3554818 -9.0845947 6.1470468 > BilinearLoss A B C [1,] 1.608557e-03 0.000000e+00 0.000000e+00 [2,] 0.000000e+00 8.182605e-05 0.000000e+00 [3,] 0.000000e+00 1.124311e-06 0.000000e+00 [4,] 0.000000e+00 1.567648e-01 0.000000e+00 [5,] 0.000000e+00 0.000000e+00 1.208276e-06 [6,] 0.000000e+00 5.032116e-02 0.000000e+00 [7,] 5.563614e-05 0.000000e+00 0.000000e+00 [8,] 1.314996e-01 0.000000e+00 0.000000e+00 [9,] 0.000000e+00 6.443034e-01 0.000000e+00 [10,] 0.000000e+00 1.537728e-02 0.000000e+00 Also, eventually I would use this in tensorflow, which I am guessing is: R: BilinearLoss = targ*(tf$matmul(tf$softmax(logits), penaltyMat)) Python: BilinearLoss = targ*(tf.matmul(tf.softmax(logits), penaltyMat)) Does this look correct? [1] "Here we present the bilinear-loss (and related log-bilinear-loss) which differentially penalizes the different wrong assignments of the model." https://arxiv.org/abs/1704.06062 Edit: Well I messed with the first row of logits and it affected the other rows so my above interpretation must be wrong: logits[1,] = c(2, 0, 0) targ*(softmax(logits) %*% penaltyMat) Edit 2: Ok... well looking at it again in the morning I see I had a dumb bug where I was not applying softmax by row. The softmax should look like this instead: # Helper softmax function softmax = function(logits){ t(apply(logits, 1, function(x) exp(x)/sum(exp(x)))) } The final result is then: > BilinearLoss A B C [1,] 1.008573 0 0.000000 [2,] 0.000000 1 0.000000 [3,] 0.000000 1 0.000000 [4,] 0.000000 1 0.000000 [5,] 0.000000 0 1.747393 [6,] 0.000000 1 0.000000 [7,] 1.001505 0 0.000000 [8,] 1.000002 0 0.000000 [9,] 0.000000 1 0.000000 [10,] 0.000000 1 0.000000 So, for example, sample 7 has a true class of "A", and predictions for each class (softmax) are 8.235724e-03, 9.902590e-01, and 1.505276e-03. So even though the wrong class (B) is predicted with high confidence, we have a low bilinear loss since the penalty for confusing A with B is low? The same thing can be seen for sample 8. For sample 1 the correct class was predicted and the loss is also low, while for sample 5 The correct class was C, but A was predicted so the loss is relatively high. It seems to roughly make sense. Here are the class probabilities I was using in the paragraph above: > softmax(logits) [,1] [,2] [,3] [1,] 9.909002e-01 5.267849e-04 8.572999e-03 [2,] 9.885562e-01 7.772689e-03 3.671067e-03 [3,] 3.794965e-01 3.197946e-02 5.885240e-01 [4,] 9.997571e-01 2.120573e-08 2.429208e-04 [5,] 7.473933e-01 1.717888e-01 8.081794e-02 [6,] 5.561712e-01 3.260773e-01 1.177515e-01 [7,] 8.235724e-03 9.902590e-01 1.505276e-03 [8,] 2.360229e-06 9.999956e-01 2.010813e-06 [9,] 9.706548e-08 3.856405e-02 9.614359e-01 [10,] 1.010441e-05 2.426485e-07 9.999897e-01
