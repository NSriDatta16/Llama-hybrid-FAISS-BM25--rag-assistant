[site]: datascience
[post_id]: 111943
[parent_id]: 
[tags]: 
KL divergence loss first decreases and then increases in VAE training

I am training a VAE on CelebA HQ (resized to 256x256). The training is going well, the reconstruction loss is decreasing and reconstructions are also meaningful. But, the problem is with KL divergence loss. The KL divergence loss first drops, then start to increase. training logs file I have attached link to training logs file.
