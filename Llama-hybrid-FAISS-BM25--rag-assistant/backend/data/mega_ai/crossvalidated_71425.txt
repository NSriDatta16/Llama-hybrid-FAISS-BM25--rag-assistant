[site]: crossvalidated
[post_id]: 71425
[parent_id]: 71363
[tags]: 
In order to obtain reproducible randomizations you have to set up a random seed beforehand, e.g. in R you do set.seed(17) where 17 is just a number I made up. When you do it you should get the same accuracy every random forest run when you fix the number of trees. I don't think your accuracy really decreases when you increase the number of trees. The accuracy results with a few trees have higher variances and even if the mean value is slightly higher (in your case little higher) this does not mean they are better. Your results might be due to different randomization of the data. Try to set up a random seed beforehand. If you don't have a specific set of records to use as test set you might obtain more stable results computing the accuracy by cross validation . A 10-fold cross validation is usually the measure of choice. For a better estimate you can also average the results of many cross validation with different randomizations. It might be time consuming, an average among 5 randomizations of 2-fold cross validation might be good practice if you don't have too few records. If you have a few records, for example less then 100, you can try the leave one out method. If you want to learn more about classifier evaluation the book Evaluating Learning Algorithms: A Classification Perspective by Nathalie Japkowicz is a good reference.
