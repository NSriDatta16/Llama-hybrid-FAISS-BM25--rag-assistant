[site]: crossvalidated
[post_id]: 638616
[parent_id]: 
[tags]: 
How to normalise outputs of neural networks with different distribution?

I have a NN model that predicts 8 different variables. I use a multi-task learning approach, where I compute the loss between predictions and targets for each of the 8 variables, and then I add all the 8 losses together. The sum of the losses is then minimised using gradient descent. The output variables all have different physical units and ranges. Therefore, I want to normalise the data first before computing the loss. But the problem is, some of the variables are highly skewed, while some are normally (or close to it) distributed. I need to make sure that all outputs are normalised so that they have values between specific range (e.g. 0 and 1). Because, the original output values for some variables are between 0 and 1, and for some it can be as high as 2000, and for some it can be 50-100. My question is, should I use the same normalisation algorithm or can I use different normalisation algorithms for different outputs? Which algorithms are recommended for extremely left skewed data?
