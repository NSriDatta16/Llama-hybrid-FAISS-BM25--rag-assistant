[site]: crossvalidated
[post_id]: 179495
[parent_id]: 178704
[tags]: 
As @wolfgang says, it's a good thing that this is very difficult. Kind of the point of SEM is that you start with a specified model, and then you test it, and use the chi-square (and other fit statistics) to decide if you have a good fit. IN a chapter called "Testing structural equation models" Joreskog talks about this as the 'strictly confirmatory' approach. A second approach is what Joreskog calls 'alternative models'- you have several theoretically defined models and see which one fits best. But what everyone almost always ends up doing is a 'model generating' approach. You can see the pages in the book here: https://goo.gl/zGjAsr (I think). But as he says there, the goal of the analysis is to find a model that fits the data well, but also that "every parameter of the model can be given a substantively meaningful interpretation". (It's a good chapter, in a good book - it's worth reading the whole chapter, and book.) So the exploratory approach that you are suggesting kind of goes against the whole principle of SEM. But Imagine we have four variables, a, b, c and d. How many possible models could there be? They could all covary. They could all be indicators of a factor. They could be indicators of two factors (there are three ways to do this). Any three could be indicators, and the fourth could be causal. One factor, with any pair of variables having a correlation (that's 6 models). a -> b-> c-> d b -> a -> c-> d ... And so on, and that's only with 4 variables. There are ways of being a bit automated and exploratory, but you always need to specify the basic model. AMOS allows 'optional' parameters. It fits a model with every possible combination of optional parameters. That rapidly leads to an awful lot of models - if you have 10 optional parameters that's $2^10=1024$ models. And I had 10 more than 10 models in my list above, with only four variables. I've used R to write and then run Mplus files, and then read and store the results (I think there's a package that helps with that now). For reasons I completely forget, I wanted to find the best combination of 10 (I think) predictors, where the predictors were constrained to one or zero. That's about a thousand models to compare. If I'd had 20 variables, I would have had just over a million models to compare (and if each model had taken 5 seconds to run, it would have taken 576 days). Finally LISREL has (or did have) the AM option, which automodifies the model. It estimates a model, finds the largest modification index, frees that parameter, reruns the model, etc. When I was in graduate school we would joke that if your model didn't fit, you could put AM on the OU line, go for lunch, and come back to find it fitted. However, there are papers (which I can't recall the authors of) which showed that even if your model was close to the correct model, this approach rarely ended up with the correct model. If you start with nothing, I hate to think what you would end up with.
