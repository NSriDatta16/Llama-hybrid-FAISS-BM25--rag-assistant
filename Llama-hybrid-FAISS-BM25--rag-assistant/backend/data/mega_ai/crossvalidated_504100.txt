[site]: crossvalidated
[post_id]: 504100
[parent_id]: 503790
[tags]: 
Ridge regression would be a useful approach here. It allows you to include all predictors in the model while penalizing their coefficient values to minimize overfitting. A standard approach is to use cross validation to choose the level of penalty that provides the optimal performance (for example, gauged by deviance in logistic regression). You could get into trouble, however, if you apply this approach to setting up and comparing a set of multiple different models to identify the "best" one. As the answer from Demetri Pananos emphasizes, inference is difficult at best with penalized methods, and bootstrapping won't get around the problem of too few events. At this stage of your study, use your knowledge of the subject matter to identify potentially important predictors and use them all in a single penalized ridge-regression model. That should help you start "to properly grasp the phenomenon" you're studying and guide you toward studies that can provide more data efficiently.
