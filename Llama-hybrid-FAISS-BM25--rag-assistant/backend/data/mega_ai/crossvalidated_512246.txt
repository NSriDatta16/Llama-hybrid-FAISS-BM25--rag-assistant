[site]: crossvalidated
[post_id]: 512246
[parent_id]: 
[tags]: 
Why is there a discrepancy between the eigenvalues of the covariance matrix (PCA) and the eigenvalues of the kernel matrix (kernel PCA)?

I've done PCA on my data matrix $ \mathbf{X} $ which gives me i.a. the eigenvalues $ \lambda $ and eigenvectors $ v $ of the data covariance matrix $ C=\mathbf{X}^T \mathbf{X} $ . I'm now extending my analysis to also apply kernel PCA. Now, it can be shown that the eigenvalues of $ C $ should be equal to the eigenvalues of the kernel matrix $ \mathbf{K} $ : $$ \mathbf{K} \alpha = \lambda \alpha \\ \Leftrightarrow \mathbf{X} \mathbf{X}^T \alpha=\lambda \alpha \\ \Rightarrow \mathbf{X}^T \mathbf{X} \mathbf{X}^T \alpha=\lambda \mathbf{X}^T \alpha \\ \Leftrightarrow Cv=\lambda v $$ With $ \alpha $ being the eigenvector of $ \mathbf{K} $ and $ v:= \mathbf{X}^T \alpha $ being the eigenvector of $ C $ . After applying kernel PCA with a linear kernel (equivalent to "standard" PCA), however, the eigenvalues are not equal. I see, however, a (maybe general) relationship between $ \lambda_{PCA} $ and $ \lambda_{KPCA} $ , because $ \overline{\lambda}_{PCA, i} = \overline{\lambda}_{KPCA, i} $ with $ \overline{\lambda}_i = \frac{\lambda_i}{\sum_{k=1}^n \lambda_k} $ for the $ i $ -th of the $ n $ eigenvalues. So why are the eigenvalues not equal? I'm using Python with sklearn.decomposition.PCA and sklearn.decomposition.KernelPCA .
