[site]: crossvalidated
[post_id]: 377080
[parent_id]: 377076
[tags]: 
A common approach to using unlabeled data to pre-train a network is by first contructing a denoising auto-encoder. Corrupted versions of the unlabeled data are then reconstructed by the autoencoder (source task), after which the encoding part of the network can be used for a regression or classification task (target task). The reason this sometimes benefits the target task is because the early layers of the network are already trained to recognize useful features of the input data. It also acts as a regularizer, constraining the parameter space for the target task. Have a look here for example. Figure 6 demonstrates the regularizing ability of this approach. Linked article: Erhan D. et al. (2010): Why Does Unsupervised Pre-training Help Deep Learning?
