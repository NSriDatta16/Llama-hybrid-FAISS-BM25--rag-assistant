[site]: crossvalidated
[post_id]: 618516
[parent_id]: 
[tags]: 
In the case of the beta-binomial model, how to compute the marginal likelihood?

I have a question about Bayesian statistics. In a book called PML, I came across the following description of how to calculate the marginal likelihood of a beta-binomial distribution. In the case of the betaBernoulli model, the marginal likelihood is proportional to the ratio of the posterior normalizer to the prior normalizer. To see this, recall that the posterior for the beta-binomial models is given by $p(\theta|D)=Beta(\theta|a', b')$ , where $a'=a+N_1$ and $b'=b+N_0$ . We know the normalization constant of the posterior is $B(a', b')$ . Hence $p(\theta|D)=\begin{pmatrix}N \\ N_1 \end{pmatrix}\frac{1}{p(D)}\frac{1}{B(a, b)}[\theta^{a+N_1-1}(1-\theta)^{b+N_0-1}]$ so $\frac{1}{B(a+N_1, b+N_0)}=\begin{pmatrix}N \\ N_1 \end{pmatrix}\frac{1}{p(D)}\frac{1}{B(a, b)}$ , $p(D)=\begin{pmatrix}N \\ N_1 \end{pmatrix}\frac{B(a+N_1, b+N_0)}{B(a, b)}.$ suppose we toss a coin N times. $N_1$ : the number of heads. $N_0$ : the number of tails. Why can the marginal likelihood be calculated in this way? What does it mean that the marginal likelihood is proportional to the ratio of the posterior normalizer to the prior normalizer?
