[site]: crossvalidated
[post_id]: 198829
[parent_id]: 
[tags]: 
How does one know if normalizing is improves reconstructions in the task of auto-encoding?

I wanted to understand the performance on an algorithm in the auto-encoding task and compare understand if normalizing the data was a good idea or not and compare the performance when the data is normalized vs when its not (normalize as in $\frac{x}{\| x \|}$ ). The algorithm is trained via stochastic gradient descent (say its some neural network) and I train it via with the data and when the data is not normalized. Naturally, when the data is normalized, its likely to see smaller errors (just because we are dividing by a quantity $ \| x \| \geq 1$) so nearly by definition the error "seems" smaller because the data has been shrunk. However, this doesn't mean that the algorithm necessarily performed better (or worse). Therefore, I was trying to understand how should compare the performance of the algorithm on the same task when the data is normalized and when its not. Naturally, if the problem I was trying to solve was instead classification, the problem would be very easy to solve, because, if the accuracy (in terms of correct labels predicted) increases, then we know that the algorithm is performing better when the data (say images) are normalized. But in the case of auto-encoding, its harder to see this. Is the issue of having a hard time to compare the performance inherit in that I am considering the wrong loss function (i.e. $ L(f(x), y) = \| f(x) - y \|^2$ doesn't capture the information I want to learn) or what is the issue? How do people overcome this issue in practice and know if its worth normalizing in the case of auto-encoding task? Obviously, one can just observe the reconstruction for say, a fraction of the images and the human can asses which one is better, but this approach doesn't seem to scale. Is there a better way to do this or has this been addressed in practice or theoretically?
