[site]: crossvalidated
[post_id]: 485553
[parent_id]: 
[tags]: 
Word embeddings - Pre-trained tokenizers vs more involved methods

I'm drowning under all the various methods of converting my text corpora into embeddings. I'm currently using the HuggingFace Tokenizer ( https://github.com/huggingface/tokenizers ) to do this, using the encode_plus method, which seems to do a great job of converting my text data into embeddings to be passed to my neural network. However, I've noticed the vocabulary size of the tokenizer is about 30k, whereas word2vec vocab size is 3m. I have hardly any experience of using other methods than the HF Tokenizer, but there still seems to be lots of posts being written on blog sites about how to use W2V (etc). If the HF implementation is so great, why isn't everyone just using that? Is there any known advantage to obtaining embeddings the W2V way for a simple multiclass text classification problem? Yes, HF is used primarily for converting to the very niche requirements of BERT (et al) inputs, but you can easily turn off the more complex outputs and just have a simple list of embeddings returned. I originally started out using BERT but found it too complex a model for my needs, so I just use the HF Tokenizer, and then a vanilla LSTM.
