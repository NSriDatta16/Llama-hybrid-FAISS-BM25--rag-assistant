[site]: crossvalidated
[post_id]: 363076
[parent_id]: 
[tags]: 
SARSA with Linear Function Approximation weight overflow

I'm trying to solve the CartPole problem, implemented in OpenAI Gym. In each state the agent is able to perform one of 2 actions move left or right. The reward is always +1. The epsiode ends after 500 steps or when the pole falls over. The state consists of 4 values the cart's position, velocity, the pole angle and the angleRate (I'm not quite sure what the last one means). I've omitted the position because it makes no difference where the cart is except for some rare edge cases, when the cart would go out of bounds. I tried to solve this problem by approximating the state action value $Q(s,a)$ with SARSA and a linear function for each action. The problem is that the algorithm is able to learn how to balance the pole for 500 steps but then it jumps back to around 100. It proceeds to cycle while the weights start to grow rapidly until they overflow. The state contains no information about the amount of steps until the episode ends and maybe because of that it starts to approximate an infinite reward. But even with the time included the weights overflow. I've also tried to use the sklearn LinearRegressor to check if my regression implementation is wrong. The Regressor didn't overflow but it did not even come close to a reward of 500. The last thing I tried was to use combined states like $positon * velocity$ and $velocity * pole angle$ etc. But that didn't fix the overflowing it just helped to reach the reward of 500 more often. So I'm out of ideas why my weights keeps overflowing. Below is a shorter but working version of my code. I do not think that there is a problem with my code but it seems like I'm doing something fundamentaly wrong. import random as rand import numpy as np import gym import sys class SemiGradientSarsaAgent: def __init__(self, stateSize, actions, explorationRate, stepSize, rewardDiscount): self.actions = np.array(actions) self.weights = np.zeros((len(actions), stateSize), dtype=np.float32) self.explorationRate = explorationRate self.stepSize = stepSize self.rewardDiscount = rewardDiscount def chooseAction(self, state): return self.__chooseActionInternal__(state)[1] def learn(self, state, action, newState, reward, done): prevQValue = self.__getQValue__(state, action) if done: step = self.stepSize * (reward - prevQValue) else: nextQValue = self.__chooseActionInternal__(newState)[0] step = self.stepSize * (reward + self.rewardDiscount * nextQValue - prevQValue) # Gradient descent: The Gradient for the weights is the state itself gradient = state self.weights[action] += step * gradient def __chooseActionInternal__(self, state): # Epsilon Greedy qValues = np.array([self.__getQValue__(state, action) for action in self.actions]) if rand.random()
