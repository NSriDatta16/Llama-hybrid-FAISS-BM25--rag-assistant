[site]: datascience
[post_id]: 120087
[parent_id]: 
[tags]: 
Random Forest on high correlated data

I am relatively new on data science sector. I started tampering with random forest models and some strange occasions aroused. To be precise, I have data from sensors that record pollutants and a column that acts as the labels populated with data that shows if a person has developed allergies or not depicted with 0 or 1. The amount of data is 1300 people. So i have run some random forest classifier models and started to remove or add some columns. What i dont understand is that: 1. In a model with all pollutants as features and the column SYMPT as labels when i remove a random column (ex styrene) the order of the feature importance changes significantly, 2. In a model with all pollutants as features and the column SYMPT as labels when i remove a large number of columns or even let just 1 column of the pollutants the accuracy remains the same (about 71%) and lastly 3. when i added the column with AA (increasing serial number) data which was originaly excluded from the model along with SBR column, the accuracy increased. What can you understand from these results?
