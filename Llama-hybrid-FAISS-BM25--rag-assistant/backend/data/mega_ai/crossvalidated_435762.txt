[site]: crossvalidated
[post_id]: 435762
[parent_id]: 435097
[tags]: 
What is the most appropriate way to validate prediction models with clustered data? For internal validation, I indeed recommend cross validation splitting by clinical trial and further known influencing factors that are crossed with the trials (e.g. patients paticipating in more than one trial). In that case, the splitting will produce 3 subsets for each "fold": CV-training and CV-test subsets are independent of each other plus there may be a do-not-use-for-this-fold set that is independent neither of the training nor of the test set. If the other influencing factors are nested within trial, then splitting by trial will automaticall achieve independence for them as well. Exhaustive cross validation running though all combinations of these factors may be too tedious (too many combinations), so you may consider running only a subset of the possible splits or to run a number of train/test/do-not-use splits according to those principles (set validation). This way, you'll keep the full information of the trials (which would be lost by pooling) and have independent splits. Independence can anyways be ensured only in so far as you are aware of influencing factors, and this situation is in my experience fairly common. I'd also consider the splitting strategy fairly standard in the sense that I've been using such splits at the highest level of the data structure for clustered* data for some 15 years now, i.e. since I was working with data where we had repeated measurements for each patient during my Diplom thesis. * we've been calling this hierarchical data structure, nested would be another term that I'd prefer nowadays. Internal vs. External Validation In my field, analytical chemistry (and AFAIK also for clinical chemistry), the difference between internal and external validation is Internal validation is performed by a lab for their method. Whereas for External validation (method proficiency), the validation is done by an organizer outside the lab in question, for example by participating in round robin tests where an organizer sends blinded samples to the lab and then compares the lab's results to the ground truth. So, as you have access to the full set of studies and do training and validation, you can only perform internal validation . To have an external validation, you'd need to find someone else to provide you with blinded samples that you predict with your final method. However, if the different trials are implementing the same method in different labs, i.e. you are looking at an inter-laboratory study in analytical-chemical terminology or a multicentric study in medical terminology, you may argue that while you do not have the extra blinding level an external validation would provide nor the ongoing performance estimation that repeatedly participating in a round robin provides, you do estimate an generalization error for "unknown" labs. In any case, I recommend to clearly spell out the splitting procedure rather than assuming that everyone understands the same under the heading of internal vs. external validation. The more so, as resampling procedures (such as cross validation, set validation or hold-out validation) belong to verification and thus can provide only a part of what is needed for full [method] validation. internal/external validation may spell out quite differently in different fields (e.g. a psychologist discussing internal and external validity of their method may not be concerned about different labs).
