[site]: crossvalidated
[post_id]: 139449
[parent_id]: 139327
[tags]: 
First note that the availability of a closed-form posterior, which essentially amounts to conjugacy as you notice, depends also on $\sigma^2$ and on what prior distribution it is assigned. If $\sigma^2$ is known, then your prior $\beta \sim N(\beta_0, \Sigma_0)$ is indeed conjugate, otherwise if $\sigma^2$ is random, you need an inverse gamma prior on it, and you also need to change your prior on $\beta$ as $$\beta\vert\sigma^2 \sim N(\beta_0, \sigma^2\Sigma_0).$$ Showing conjugacy requires some tedious algebra, a crucial point in the algebra is to use the least squares estimate $$\hat \beta = (XX^T)^{-1}X^Ty$$ in order to center the posterior distribution of $\beta$. You can show that the posterior is $$\beta\vert \sigma^2, Y, X\sim N(\beta_n, \sigma^2\Sigma_n)$$ where $$\Sigma_n = (X^TX+\Sigma_0^{-1})^{-1},\quad \beta_n = \Sigma_n((X^TX)\hat\beta+\Sigma_0^{-1}\beta_0).$$ The steps are well detailed on the Wikipedia page of Bayesian linear regression, see here: http://en.wikipedia.org/wiki/Bayesian_linear_regression#Posterior_distribution PS: mind your expression of the posterior distribution for $\beta$ where you are missing some terms. It should read $$\Bigg(\prod_{i=1}^N \exp -\frac{w_i}{2 \sigma^2}\big(y_i - \beta^Tx_i\big)^T\big(y_i - \beta^Tx_i\big)\Bigg)\exp -\frac{1}{2 \sigma^2}(\beta-\beta_0)^T\Sigma_0^{-1}(\beta -\beta_0)$$
