[site]: crossvalidated
[post_id]: 490123
[parent_id]: 
[tags]: 
Reformulation - Probability of correctly inferring whether a coin can produce heads

[This is a repost, since the original post specified a slightly different problem than I actually had in mind] Suppose you have a coin which you know could be biased towards tails with unknown probability $p_{\mathrm{t}}\ge0.5$ . Your task is to find out whether the coin can produce heads at all . To do so, you are allowed to make $n$ observations of coin flips. You observe $k$ times heads. If $k>0$ , you conclude that the coin can produce heads. If $k=0$ , you randomly guess with equal probability for "can produce heads" and "can not produce heads". Given your single experiment of $n$ observations, you are now asked to provide an estimate $\hat{p}(\mathrm{correct})$ of the probability of having made a correct response such that for a large number of experiments your estimate would approach your objective performance for any given true $p_\mathrm{t}$ . A response is termed 'correct' when you say the coin can produce heads and indeed this is the case (i.e. $p_{\mathrm{t}} ) or when you say the coin cannot produce heads and indeed $p_{\mathrm{t}} = 1$ . Objectively, the proportion correct (I think) is $ p(\mathrm{correct}) = 1 - p(\mathrm{incorrect}) = 1 - 0.5\cdot p_{\mathrm{t}}^n \tag{1} $ In other words, I can be wrong only when all coin flips give a tail, and even then, I'm only wrong with a probability of 0.5. Here's how I approached the problem so far. I thought, to approximate $p(\mathrm{correct})$ , the observer has to make an estimate $\hat{p}_{\mathrm{t}}$ of the probability for tails. I have thought of different ways of how the observer could estimate $\hat{p}_{\mathrm{t}}$ : a) The simplest estimate for $p_{\mathrm{t}}$ would be to just count the number of tails and divide by $n$ , which gives: $ \hat{p}(\mathrm{correct}) = 1-0.5\cdot(1-\frac{k}{n})^n \tag{2} $ b) You could use the knowledge that the probability distribution for $p_{\mathrm{t}}$ given $n-k$ observations of tails is given by the Beta distribution: $ f(p_{\mathrm{t}}) = \mathrm{Beta}(p_{\mathrm{t}}; n-k+1, k+1) $ We can then compute the full integral: $ \begin{aligned} \hat{p}(\mathrm{correct}) = 1-0.5\cdot \int\limits_{p_\mathrm{t}=0}^1 \mathrm{Beta}(p_{\mathrm{t}}; n-k+1, k+1)\cdot p_{\mathrm{t}}^n = \\ 1-0.5\cdot\frac{1}{\mathrm{B}(n-k+1, k+1)}\int\limits_{p_\mathrm{t}=0}^1 p_\mathrm{t}^{n-k} (1-p_\mathrm{t})^k \,p_\mathrm{t}^n =\\ 1-0.5\cdot\frac{1}{\mathrm{B}(n-k+1, k+1)}\int\limits_{p_\mathrm{t}=0}^1 p_\mathrm{t}^{2n-k} (1-p_\mathrm{t})^k =\\ 1-0.5\cdot\frac{\mathrm{B}(2n-k+1, k+1)}{\mathrm{B}(n-k+1, k+1)} = 1-0.5\cdot \frac{(2n-k)!(n+1)!}{(2n+1)!(n-k)!} \end{aligned} \tag{3} $ c) Realizing that the distribution of $p_{\mathrm{t}}$ is given by a Beta distribution, we could also use the mean of the distribution rather than the mode (which would be equivalent to a)). Since the mean of a Beta distribution is given by $\frac{\alpha-1}{\alpha+\beta-2}$ , we arrive at: $ \hat{p}(\mathrm{correct}) = 1-0.5\cdot(1-\frac{k+1}{n+2})^n \tag{4} $ However, all approaches mis-estimate $p(\mathrm{correct})$ . And indeed, intuitively it might seem impossible to provide an estimate of $p(\mathrm{correct})$ that is unbiased for all underlying values of $p_\mathrm{t}$ . For instance, when true $p_\mathrm{t}=1$ , it must be $p(\mathrm{correct})=0.5$ since I will always only observe tails (i.e. $k=0$ ). However, when $k=0$ , my formulas (except a), but a) has other problems) will have results above 0.5 â€“ and rightfully so from their perspective, because $k=0$ in one instance of the experiment does not imply $k=0$ in every other experiment. So my question is then: is it indeed impossible to provide an unbiased estimate of $p(\mathrm{correct})$ for any true $p_\mathrm{t}$ , such that in the long run, the average of my estimates $\hat{p}(\mathrm{correct})$ would approach the true $p(\mathrm{correct})$ ? Or would it be possible to provide an appropriate correction to my estimates? Also I think (in hindsight) that my approach taken with a), b), c) will actually not lead to the solution that I have in mind. I think what it would take would be a full model of the expected performance for any true $p_\mathrm{t}$ and then kind of invert the model to provide a $\hat{p}(\mathrm{correct})$ that would converge to the $p(\mathrm{correct})$ . At the moment though I have no idea how to approach this. Edit: Well, there's one 'trivial' solution, which is: $$ \hat{p}(\mathrm{correct}) = \begin{cases} 0.5 & \text{if $k=0$}\\ 1 & \text{else} \end{cases} $$ This estimate would actually be unbiased. But it would also not be an interesting estimate for my purpose since it is not a sufficiently "graded" estimate of $p(\mathrm{correct})$ . But maybe this is the answer? There is no unbiased estimate that is also graded? It seems like this. An unbiased estimate for $p_\mathrm{t}=1$ requires that I set $\hat{p}(\mathrm{correct})=0.5$ . However, if this hard rule should not lead to an understimation for all other $p_\mathrm{t}$ , I must set $\hat{p}(\mathrm{correct})=1$ in literally every other case. So there is no 'wiggle room' for more graded estimates.
