[site]: crossvalidated
[post_id]: 179315
[parent_id]: 
[tags]: 
How do I know a simple validation result is statistically significant when comparing machine learning algorithms?

In computer science literature, we always see different algorithms are trained with a lot of data (n=100,000), and then they are tested on a test set (n=10,000). Then, often,if one algorithm NUMERICALLY is slightly bigger than another algorithm in term of accuracy (e.g., 79.1% vs 78.1%), then the author will claim his/her method is better (beat the state of the art). However, how do we know the difference in term of accuracy is significant enough? In computer science literature, they often don't do statistical test. So, if I am in this situation, what should I do to compare the significance of the difference between two algorithms and what tests should be used?
