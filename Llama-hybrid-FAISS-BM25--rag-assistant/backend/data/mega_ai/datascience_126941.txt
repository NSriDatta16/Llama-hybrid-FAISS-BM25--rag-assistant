[site]: datascience
[post_id]: 126941
[parent_id]: 126940
[tags]: 
An embedding layer is just a building block to be used as part of neural architectures. It is just a lookup table whose purpose is to represent tokens as vectors, and to learn these vectors as part of the training of the model in certain task (e.g. masked language modeling). The input of the embedding layer are token indices, and its output are the vectors associated to those tokens. Actually, an embedding layer is the first layer of most NLP models, including RoBERTa. What we call "RoBERTa" embeddings are not the internal embedding layer inside the model, but the hidden states of the self-attention layer of the model.
