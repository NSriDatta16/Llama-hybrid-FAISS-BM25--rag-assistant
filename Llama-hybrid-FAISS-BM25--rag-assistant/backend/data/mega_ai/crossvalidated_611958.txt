[site]: crossvalidated
[post_id]: 611958
[parent_id]: 
[tags]: 
xgboost model for binary classification produces predicted probabilities between 0.49 and 0.51: how to analyze where the model fails

I recently trained an xgboost model for a binary prediction task; the dataset had rougly 900 class 1 and 100 class 0 rows. The model didn't fare too well (AUC 0.64) and none of the features had SHAP value to speak of (below 0.01 for every feature). All the predictions were between 0.49 and 0.51, so I got the impression the model was basically useless, a coinflip if you will. What got me thinking is that on the same dataset, I also trained two binary classifications using xgboost as well with different target variables, with much better results (AUC ~ 0.75, reasonable feature importances, predicted probabilities all over the range 0-1). All three targets were similiar in nature (patient survey results, all from the same surgical procedure, basically asking the patients in diffeent ways how they felt and how well they function). Now, I need to find why the model (and also lots of variations I tested) fails on this one particular target variable. How could i do that? The feature variable data seems fine (as it works with different targets); how could I analyze the data quality for the remaining target? Are there other sources where something could have gone wrong? Basically, I'm asking what can be done as a sort of data/model "autopsy" to find what killed my modeling for this particular target variable.
