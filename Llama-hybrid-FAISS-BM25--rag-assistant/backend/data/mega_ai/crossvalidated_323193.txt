[site]: crossvalidated
[post_id]: 323193
[parent_id]: 315497
[tags]: 
The topic you are addressing is known as model explanation or model interpretation and quite an active topic in research. The general idea is to find out, which features contributed to the model, and which not. You already mentioned some popular techniques, such as the Partial Dependence Plots (PDP) or LIME. In a PDP, the influence of a features' value to the model output is displayed by creating new instances from the data that have a modified feature value and predict them by the model. LIME creates a local approximation of the model by sampling instances around a requested instance and learning a simpler, more interpretable model. In the naive method you described, the impact of a feature is neutralised by setting it to the population mean. You are absolutely right, that this is not an appropriate method, as the prediction of the mean value is not probably not the mean prediction. Also, it does not reflect the feature distribution and does not work for categorical attributes. Robnik-Sikonja and Kononenko [1] addressed this problem. Their basic idea is the same: the prediction difference between the unchanged instance, and an instance with a neutralised feature. However, instead of taking the mean value to get rid of the features' impact, they create several instance copies, each with a different value. For categorical values, they iterate over all possible categories; for numerical values, they discretise the data into bins. The decomposed instances are weighted by the feature value frequency in the data. Missing data can be ignored by using classifiers that can handle it, or imputing it, e.g. by setting the values to the mean. Conditional importance has been addressed in a second publication by Strumbelj et al [2]. They extended the original approach by not only creating decomposed instances of a single feature, but observed how the prediction changes for each subset of the power set of feature values. This is of course computationally very expensive (as they mention themselves and tried to improve by smarter sampling in Strumbelj and Kononenko [3]). By the way: for binary data, this problem becomes much easier, as you just have to compare the prediction between attribute is present and not present . Martens and Provost [4] discussed this for document classification. Another approach for finding groups of meaningful features has been proposed by Andreas Henelius in [5] and [6]. The idea of his GoldenEye algorithm is to permute the data within-class and feature group. Imagine a data table where each row represents an instance and each column is a feature. In each column, all rows that share the same class are permuted. Features are grouped, i.e. permuted together. If the classification on the permuted data is very different (worse) than the original data, the current grouping did not reflect the true grouping. Check out the publications, its better described there. This approach becomes computationally expensive , too. I'd also like to refer to the publications by Josua Krause [7], [8]. He developed interactive visual analytics workflows for the analysis of binary instance-based classification problems, including an enhanced PDP. They are well-written and an interesting read. [1] Robnik-Šikonja, M. (2004, September). Improving random forests. In European conference on machine learning (pp. 359-370). Springer, Berlin, Heidelberg. [2] Štrumbelj, E., Kononenko, I., & Šikonja, M. R. (2009). Explaining instance classifications with interactions of subsets of feature values. Data & Knowledge Engineering, 68(10), 886-904. [3] Štrumbelj, E., & Kononenko, I. (2014). Explaining prediction models and individual predictions with feature contributions. Knowledge and information systems, 41(3), 647-665. [4] Martens, D., & Provost, F. (2013). Explaining data-driven document classifications. [5] Henelius, A., Puolamäki, K., Boström, H., Asker, L., & Papapetrou, P. (2014). A peek into the black box: exploring classifiers by randomization. Data mining and knowledge discovery, 28(5-6), 1503-1529.# [6] Henelius, A., Puolamäki, K., Karlsson, I., Zhao, J., Asker, L., Boström, H., & Papapetrou, P. (2015, April). Goldeneye++: A closer look into the black box. In International Symposium on Statistical Learning and Data Sciences (pp. 96-105). Springer, Cham. [7] Krause, J., Perer, A., & Ng, K. (2016, May). Interacting with predictions: Visual inspection of black-box machine learning models. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (pp. 5686-5697). ACM. [8] Krause, J., Dasgupta, A., Swartz, J., Aphinyanaphongs, Y., & Bertini, E. (2017). A Workflow for Visual Diagnostics of Binary Classifiers using Instance-Level Explanations. arXiv preprint arXiv:1705.01968.
