[site]: crossvalidated
[post_id]: 260438
[parent_id]: 
[tags]: 
High coefficient values in Bayesian compared to non-Bayesian logistic regression

I'm trying to move from a non-Bayesian logistic regression model to a Bayesian one (using R + jags). The model in this test scenario contains one categorical predictor. Here's some dummy data: testdata Now, using testdata\$y as the predicted variable and testdata\$x as a predictor, let's run a simple logistic regression: nonbayesian.model This gives the following results: Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) -1.8153 0.4076 -4.454 8.43e-06 *** xB -0.3819 0.6231 -0.613 0.54 This is how I am currently trying to construct a Bayesian version: writeLines("model{ for (i in 1:Ntotal) { y[i] ~ dbern(p[i]) logit(p[i]) Looking at the summary of the Bayesian version, I get something like: Mean SD Naive SE Time-series SE beta.x[1] 5.056 3.818 0.05400 1.209 beta.x[2] 4.642 3.803 0.05378 1.200 What puzzles me is why the means in the summary of the Bayesian model are so high . I suspect I'm doing something terribly unreasonable in setting the priors in the jags model specification (I was just trying to get an uninformative starting point by setting up normally distributed priors with a small sd).
