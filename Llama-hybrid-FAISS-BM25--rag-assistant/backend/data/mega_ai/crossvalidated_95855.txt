[site]: crossvalidated
[post_id]: 95855
[parent_id]: 
[tags]: 
Neural network packages which allow shared weights and parallel training

I'm curious if there are any neural network packages out there that easily allow one to construct feed forward neural networks with shared weights, but also allow for the training to be done in parallel. Torch7 allows for easy construction of shared weights, although the parallel training support is either not there (or not documented well enough to make it obvious that it is). If it interfaced to Python, that would be even better, but this is not a requirement.
