[site]: crossvalidated
[post_id]: 508000
[parent_id]: 507992
[tags]: 
Generally if your training error is much lower than the test error, it indeed suggests overfitting. However, the training error would almost always be smaller than the test error, and Random Forest generally doesn't overfit (as long as the bootstrap samples and mtry ratio are good- rule of thumb- 2/3, and sqrt(# variables) respectively). You could try different parameters for number of trees, bootstrap size and mtry as well as trees max depth (the deeper, the more prone to overfitting). Keep in mind that if anything, Random Forest tends to underfit (i.e too low variance in predictions) Furthermore, because Random Forest uses bootstrap samples for training you don't need to perform cross validation as you get the 'out-of-bag error' - this is the error computed on the samples that were not selected in the bootstrap samples in each iteration. I am usually using R and not python so I can't tell you what is the name of the object but I'm sure it exists in sklearn.
