[site]: crossvalidated
[post_id]: 181561
[parent_id]: 181501
[tags]: 
@NickCox has presented a good way to visualize your data. I take it you want to find a rule for deciding when to classify a value as condition1 vs condition2. In an earlier version of your question, you wondered if you should call any value greater than the median of condition1 as a member of condition2. This is not a good rule to use. Note that by definition, $50\%$ of a distribution is above the median. Therefore, you will necessarily misclassify $50\%$ of true condition1 members. Based on your data, I gather you will also misclassify $18\%$ of your true condition2 members. A way to think through the value of a rule like yours is to form a confusion matrix . In R, you can use ?confusionMatrix in the caret package . Here is an example using your data and your suggested rule: library(caret) dat = stack(list(cond1=Cond.1, cond2=Cond.2)) pred = ifelse(dat $values>median(Cond.1), "cond2", "cond1") confusionMatrix(pred, dat$ ind) # Confusion Matrix and Statistics # # Reference # Prediction cond1 cond2 # cond1 20 7 # cond2 19 32 # # Accuracy : 0.6667 # ... # # Sensitivity : 0.5128 # Specificity : 0.8205 # Pos Pred Value : 0.7407 # Neg Pred Value : 0.6275 # Prevalence : 0.5000 # Detection Rate : 0.2564 # Detection Prevalence : 0.3462 # Balanced Accuracy : 0.6667 I bet we can do better. A natural approach is to use a CART ( decision tree ) model, which (when there is only one variable) simply finds the optimal split. In R, you can do that with ?ctree from the party package . library(party) cart.model = ctree(ind~values, dat) windows() plot(cart.model) You can see that the model will call a value "condition1" if it is $\le5.7$ , and "condition2" otherwise (note that the median of condition1 is $3.9$ ). Here is the confusion matrix: confusionMatrix(predict(cart.model), dat$ind) # Confusion Matrix and Statistics # # Reference # Prediction cond1 cond2 # cond1 39 15 # cond2 0 24 # # Accuracy : 0.8077 # ... # # Sensitivity : 1.0000 # Specificity : 0.6154 # Pos Pred Value : 0.7222 # Neg Pred Value : 1.0000 # Prevalence : 0.5000 # Detection Rate : 0.5000 # Detection Prevalence : 0.6923 # Balanced Accuracy : 0.8077 This rule yields an accuracy of $0.8077$ , instead of $0.6667$ . From the plot and the confusion matrix, you can see that true condition1 members are never misclassified as condition2. This falls out of optimizing the accuracy of the rule and the assumption that both types of misclassification are equally bad; you can tweak the model fitting process if that isn't true. On the other hand, I would be remiss if I didn't point out that a classifier necessarily throws away a lot of information and is typically suboptimal (unless you really need classifications). You may want to model the data so that you can get the probability a value will be a member of condition2. Logistic regression is the natural choice here. Note that because your condition2 is much more spread out than condition1, I added a squared term to allow for a curvilinear fit: lr.model = glm(ind~values+I(values^2), dat, family="binomial") lr.preds = predict(lr.model, type="response") ord = order(dat$values) dat = dat[ord,] lr.preds = lr.preds[ord] windows() with(dat, plot(values, ifelse(ind=="cond2",1,0), ylab="predicted probability of condition2")) lines(dat$values, lr.preds) This is clearly giving you more, and better, information. It is not recommended that you throw away the extra information in your predicted probabilities and dichotomize them into classifications, but for the sake of comparison with the rules above, I can show you the confusion matrix that comes from doing so with your logistic regression model: lr.class = ifelse(lr.preds The accuracy is now $0.859$ , instead of $0.8077$ .
