[site]: crossvalidated
[post_id]: 112440
[parent_id]: 112339
[tags]: 
It depends a lot on your data. Skewed features space make Gradient Descent way slower and sub-optimal (in general). If they are heterogeneous kind of data, scaling may help. Let's say your features are Area (m^2), Temperature (Â°K) and so on. You have features of different size, so it may help convergence to make them of comparable size. Moreover, in real applications, you may find useful to use dimensionality reduction with really high variance retain (say 0.99) to make your classifier more robust to noise and may help generalization. If heterogeneous data are processed with dimensionality reduction without scaling before it, meaningful data with low variance may be just dropped, if you do not properly scale before to apply it. However, suppose that you are using color histogram features. You want to apply PCA at the 0.999 variance retain in order to avoid unpredictable behaviours of your classifier in real world deployment. If you perform features scaling before to apply PCA, you are not deleting anymore components with less variance, thus the one more affected by "noise". In this case I would not apply features scaling before dimensionality reduction. I would apply it after PCA, and before to feed features in the NN, always for easing convergence. What about if you have histogram features and shape features? In this case, I would scale the two bunch of features separately. I would apply dimensionality reduction on them separately, and then I would scale them all together. When dimensionality reduction is involved, you should apply features scaling always after its application, while you have to apply it before depending on the type of data. Just for completeness, in few certain practical cases I encountered, not scaling features at all gave me slightly better results. The best advice I can give you is to try.
