[site]: crossvalidated
[post_id]: 518674
[parent_id]: 421935
[tags]: 
Q ueries is a set of vectors you want to calculate attention for. K eys is a set of vectors you want to calculate attention against. As a result of dot product multiplication you'll get set of weights a (also vectors) showing how attended each query against K eys. Then you multiply it by V alues to get resulting set of vectors. Now let's look at word processing from the article "Attention is all you need". There are two self-attending (xN times each) blocks, separately for inputs and outputs plus cross-attending block transmitting knowledge from inputs to outputs. Each self-attending block gets just one set of vectors (embeddings added to positional values). In this case you are calculating attention for vectors against each other. So Q=K=V . You just need to calculate attention for each q in Q . Cross-attending block transmits knowledge from inputs to outputs. In this case you get K=V from inputs and Q are received from outputs. I think it's pretty logical: you have database of knowledge you derive from the inputs and by asking Q ueries from the output you extract required knowledge. How attention works: dot product between vectors gets bigger value when vectors are better aligned. Then you divide by some value (scale) to evade problem of small gradients and calculate softmax (when sum of weights=1). At this point you get set of weights sum=1 that tell you for which vectors in K eys your q uery is better aligned. All that's left is to multiply by V alues.
