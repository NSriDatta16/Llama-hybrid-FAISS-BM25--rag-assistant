[site]: crossvalidated
[post_id]: 338186
[parent_id]: 211962
[tags]: 
I also wonder what does an LSTM have to do with Attractors. Look at the paper "An Empirical Exploration of Recurrent Network Architectures", Proceedings of the 32 nd International Conference on Machine Learning, Lille, France, 2015 by Rafal Jozefowicz, Wojciech Zaremba Ilya Sutskever. In the 2nd paragraph of Section 2.1 they conclude that LSTMs are not "compatible" with attractor systems based on a result of Bengio: Bengio et al. (1994) showed that any RNN that stores onebit of information with a stable attractor must necessarilyexhibit a vanishing gradient. As Theorem 4 of Bengio et al.(1994) does not make assumptions on the model architec-ture, it follows that an LSTM would also suffer from van-ishing gradients had it stored information using attractors.But since LSTMs do not suffer from vanishing gradients,they should not be compatible with attractor-based mem-ory systems.
