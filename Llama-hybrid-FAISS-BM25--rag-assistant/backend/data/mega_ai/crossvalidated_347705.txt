[site]: crossvalidated
[post_id]: 347705
[parent_id]: 
[tags]: 
How can feature relevance be assessed with categorical data?

THE PROBLEM Consider data such as feature_1 feature_2 feature_3 feature_4 0 0.192812 f2_class_1 0.274992 f4_class_1 1 0.456625 f2_class_0 0.284048 f4_class_2 2 0.989948 f2_class_0 0.194613 f4_class_0 3 0.233459 f2_class_0 0.646692 f4_class_0 4 0.107654 f2_class_1 0.281131 f4_class_1 where features 1 and 3 are numerical and features 2 and 4 are categorical. Assuming that the features 2 and 4 are drawn from a pool of 2 and 3 categories, respectively, then one-hot encoding for the above data gives feature_1 f2_0 f2_1 feature_3 f4_0 f4_1 f4_2 0 0.192812 0 1 0.274992 0 1 0 1 0.456625 1 0 0.284048 0 0 1 2 0.989948 1 0 0.194613 1 0 0 3 0.233459 1 0 0.646692 1 0 0 4 0.107654 0 1 0.281131 0 1 0 So, effectively, the machine learning is done on, not 4 features, but 7: feature_1 , f2_0 , f2_1 , feature_3 , f4_0 , f4_1 , and f4_2 . THE QUESTION Naively running a function such as scikit-learn's SelectKBest() will return a relevance score, say, a p-value, of the 7 features, whereas in reality, what I want is an ordering of the 4 original features. How do I go about that? Also, assuming I want to process this data using a feed-forward neural network (aka. multi-layer perceptron), is it enough to provide it with the 7 features at the input layer or is there more processing that needs to be done to account for the fact that f2_0 and f2_1 "belong together" (and similarly for the f4 's)?
