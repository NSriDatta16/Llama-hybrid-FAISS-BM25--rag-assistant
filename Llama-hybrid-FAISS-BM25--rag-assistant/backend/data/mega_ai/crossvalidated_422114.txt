[site]: crossvalidated
[post_id]: 422114
[parent_id]: 
[tags]: 
The correct implementation of momentum method and NAG

Recently started a Coursera course on Deep Learning. In the optimization video, momentum and NAG were not very clearly explained so, I searched and came across the paper On the importance of initialization and momentum in deep learning , (Sutskever et al., 2013) which has a really nice explanation on it. But I realized that the algorithm was different compared to that on most websites such as http://ruder.io/optimizing-gradient-descent/index.html#momentum $$ \begin{aligned} v_{t+1} &=\mu v_{t}-\eta \nabla f\left(\theta_{t}\right) \\ \theta_{t+1} &=\theta_{t}+v_{t+1} \end{aligned} $$ And here's the one used widely, $$ \begin{aligned} v_{t+1} &=\mu v_{t}+\eta \nabla_{\theta} J(\theta) \\ \theta_{t+1} &=\theta_{t}-v_{t+1} \end{aligned} $$ Clearly, they are different as the $\mu v_{t}$ term is added in the first update while being substracted in the second. Similarily for NAG updates, the paper implements $$ \begin{aligned} v_{t+1} &=\mu v_{t}-\eta \nabla f\left(\theta_{t}+\mu v_{t}\right) \\ \theta_{t+1} &=\theta_{t}+v_{t+1} \end{aligned} $$ While the commonly used version is, $$ \begin{aligned} v_{t+1} &=\mu v_{t}+\eta \nabla_{\theta} J\left(\theta-\eta v_{t}\right) \\ \theta_{t+1} &=\theta_{t}-v_{t+1} \end{aligned} $$ I have tried the implemented the second algorithm in the Coursera course and it works nicely but surely the one in the paper has also been implemented. So, are both correct and or the newer version has some computational advantages?
