[site]: crossvalidated
[post_id]: 260730
[parent_id]: 260590
[tags]: 
Naturally, I find an answer after posting the question. Wu et al published a paper in 2015 titled "Flip-Rotate-Pooling Convolution and Split Dropout on Convolution Neural Networks for Image Classification" ( https://arxiv.org/abs/1507.08754 has the PDF) that uses an approach very similar to what I described. They use the term "Rotate-Pooling Convolution" (RPC) to describe rotating the weights of a convolution tensor (which gives 4 variants when using 90-degree rotation increments as opposed to their 45-degree rotation increments) and the term "Flip-Rotate-Pooling Convolution" (FRPC) to describe flipping the weights. They use a stochastic approach in which they randomly select some weights to rotate and others to flip. I have not read the paper thoroughly yet, so I do not know whether they allow flipped filters to also be rotated. It is worth noting that their approach to handling flipping will try at most one flip (left-right or up-down) randomly for some features and then pool its results from the original orientation. What I proposed above simultaneously combines four 90-degree rotations with the four states of reflection (none, left-right, up-down, left-right-and-up-down) to create eight rotationally and reflexively invariant filters. Plus, they found that randomly selecting 25% of the final convolution layer's filters for rotating or flipping provided the best results (when combined with their sDropout training policy) as opposed to using this behavior in "lower" filters. This seems to make some sense for image detection where eliminating data from a large input space is vital, but may not be as important for the much smaller 19x19xCHANNELS domain of Go. They also note that they were able to use Theano to code their RPC and FRPC modifications.
