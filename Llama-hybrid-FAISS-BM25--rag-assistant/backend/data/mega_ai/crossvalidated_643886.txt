[site]: crossvalidated
[post_id]: 643886
[parent_id]: 
[tags]: 
Training a model where true value is only known in batches

I would like a model to learn a function $f(x) = y$ based on some data, but suppose that I only know the true value in small batches. More precisely, each training sample consists of $x_1,\ldots,x_n$ for some $n$ (which can vary) and I only know the value of $y_1+\cdots+y_n$ . My goal would be to have a function $f$ which minimizes $$|\sum_{i=1}^n (f(x_i) - y_i)|$$ for a random batch $(x_i,y_i)$ . Can I train such a model using xgboost (or some other model)?
