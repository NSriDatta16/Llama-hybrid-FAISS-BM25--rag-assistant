[site]: datascience
[post_id]: 118238
[parent_id]: 118219
[tags]: 
Any text-generating neural network nowadays generates tokens in a discrete space of approximately that size (30k-50k), which is analogous to a classifier of that number of classes. With a normal Transformer encoder (e.g. BERT) with a classification head (i.e. linear projection + softmax), you should be able to handle that in terms of model capacity, without needing high-end GPUs. A quick google search will give you multiple tutorials on how to achieve this. If you have hardware limitations, I suggest you read the article Cramming: Training a Language Model on a Single GPU in One Day . For even larger output spaces, there are other options like sampled softmax or hierarchical softmax. This answer provides more details and some pointers about that. One aspect that you should take into account is data sparsity. Your training data should cover profusely the label space. Otherwise, your model will not perform well.
