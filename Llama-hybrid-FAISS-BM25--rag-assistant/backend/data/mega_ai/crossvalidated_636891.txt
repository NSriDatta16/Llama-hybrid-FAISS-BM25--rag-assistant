[site]: crossvalidated
[post_id]: 636891
[parent_id]: 539312
[tags]: 
is it always the case with the more parameter you trained, the more training time you need when training a deep learning model. No See for an example Sycorax's answer here to a question "Aren't my iterations needed to train NN for XOR with MSE But generally speaking, more parameters, requires more computation effort. If there is an improvement then this comes from the network working better. (In the example it helps to avoid a local optimum that is difficult to escape)
