[site]: stackoverflow
[post_id]: 4440129
[parent_id]: 4439031
[tags]: 
I want to develop a very robust method to detect only a few top search engines spiders such as googlebot and let them access content on my site, otherwise usual user registeration/login required to view that content. The normal approach to this is to configure a robots.txt file to allow the crawlers that you want, and disallow the rest. Of course, this does depend on crawlers following the rules, but for those that don't you can fall back on things like user-agent strings, ip address checking, etc. The nice things about "robots.txt" are: It is simple to set up. It has minimal impact on your site. A well behaved crawler would fetch the file, and (assuming you disallowed the crawler) just go away. You can specify what parts of your site can be crawled. Note that I also make use of cookies to let users access some content without being registered. So if cookies are disabled on client browser, no content except front page is offered. But I heard search engine spiders dont accept cookies so this will also shut out legitimate search engine bots. Is this correct? I believe so. See Google's view on what you are doing. One suggestion I heard is to do reverse lookup from ip address and if it resolves to for example googlebot.com, then do a forward dns lookup and if get back the original ip, then its legitimate and not some one impersonating as googlebot. It probably will, but it is rather expensive. Robots.txt is a simpler approach, and easy to implement in the first instance.
