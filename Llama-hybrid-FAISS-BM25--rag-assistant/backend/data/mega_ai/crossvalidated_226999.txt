[site]: crossvalidated
[post_id]: 226999
[parent_id]: 
[tags]: 
Should one normalize both the input and the target distribution when training an auto-encoder?

I was trying to train an auto-encoder. Specifically, I was trying to train an MNIST auto-encoder. Auto-encoders usually minimize the reconstruction error. When I train my auto-encoder should I normalize the input data and the output data or just the input data and then leave the target data normalized? So train with normalized MNIST but reconstruct the original MNIST? Or both normalized? Formally: Let $N(x) = \frac{x - \mu}{ \sigma}$ denote the normalizing transform. If I have a training data set $X_{train}$ (what I call input) and the labels (or target) $Y_{train}$, usually in the auto-encoder setting $X_{train} = Y_{train}$. Intuitively we want $ F_{autoencoder} (X_{train}) \approx Y_{train} $ where $F$ denotes the auto-encoder making a reconstruction. We could train the auto-encoder with $N(X_{train})$ as training data but leave $Y_{train}$ unnormalized (so its the same as the original "virgin" data). In this setting we want $ F_{autoencoder} ( N(X_{train}) ) \approx Y_{train} $. When training there are 4 options. We could have: Input: $N(X_{train})$, Target: $Y_{train} = X_{train}$ Input: $N(X_{train})$, Target: $Y_{train} = N(X_{train})$ Input: $X_{train}$, Target: $Y_{train} = X_{train}$ Input: $X_{train}$, Target: $Y_{train} = N(X_{train})$ which of the four options is correct? My hunch is that 1 and 3 are the only sound ones. 4 sounds like the worst idea. Though 2 does sound tempting but I am not 100%. Which do people usually do? Specially if I want to, say, compare my auto-encoder with state of the art?
