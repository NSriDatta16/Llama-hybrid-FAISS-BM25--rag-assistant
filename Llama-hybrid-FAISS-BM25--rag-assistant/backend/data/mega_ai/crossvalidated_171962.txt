[site]: crossvalidated
[post_id]: 171962
[parent_id]: 171951
[tags]: 
In practice you never know the likelihood; IMO it's better thought of as a useful model for your data. In fact, I often find it helpful to replace the words 'prior' and 'likelihood' with 'parameter model' and 'data model' respectively. If a gamma distribution or what have you seems to model your data well, then it would be a prime choice for a likelihood function. You could indeed then use maximum likelihood to estimate the parameters - that is, find the parameter values that maximize the likelihood function. You can usually do this via some optimization algorithm if your likelihood is reasonably well-behaved. In R you could use for example optim() . If you want to do a Bayesian treatment you'll want to specify a prior (a parameter model) in addition to your likelihood (your data model). In the case of a $\text{gamma}(\alpha, \beta)$ distribution that means you'd want to specify distributions for $\alpha$ and $\beta$ as well. But you don't usually 'estimate the likelihood function' so to speak; you specify it, and then conduct your analysis around that assumption.
