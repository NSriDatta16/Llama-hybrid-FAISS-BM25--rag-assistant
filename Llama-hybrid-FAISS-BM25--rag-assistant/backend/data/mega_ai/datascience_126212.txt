[site]: datascience
[post_id]: 126212
[parent_id]: 126206
[tags]: 
The link you provide states multiple reason why, namely: "We know from GPT-2 and 3 that models trained on such data can achieve compelling zero shot performance; however, such models require significant training compute. To reduce the needed compute, we focused on algorithmic ways to improve the training efficiency of our approach." CLIP is meant to show that good zero-shot performance is achievable without requiring large and expensive datasets or long and costly compute. Seeing if an LLM outperforms CLIP could be an interesting research question, but then it would definitely need to factor in the difference in compute and data required.
