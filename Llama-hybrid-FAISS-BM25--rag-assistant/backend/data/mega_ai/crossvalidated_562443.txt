[site]: crossvalidated
[post_id]: 562443
[parent_id]: 
[tags]: 
Did Early AI Researchers Understand Why the Perceptron "Worked"?

I was reading about the development of the original Perceptron Algorithm (i.e. the First Neural Network) - if I understand this correctly, a Perceptron is a One Layer and One Neuron "Neural Network". The first perceptron was able to successfully classify pictures of shapes and letters (around 1958). I am interested in learning about to what extent did Early AI Researchers understand the mathematical limits and abilities of the Perceptron. For instance, I know that many years later, an important theorem in AI called the "Universal Approximation Theorem" was established, that showed a Neural Network in "theory" could perfectly learn any function - but the earliest version of this theorem was established in 1989 ( https://en.wikipedia.org/wiki/Universal_approximation_theorem ) - 30 years prior to the successful implementation of the first Perceptron. While doing some reading online, the only theoretical results I came across that suggest early AI Researchers understood the "mathematical abilities" of the Perceptron were: "X-OR" Problem : It was theoretically demonstrated that a simple Perceptron can not perfectly separate data that is non-linearly separable. Perceptron Convergence Theorem : A theoretical result was found that suggests "a finite number of iterations are required to update the weights of the perceptron such that all data points (provided they are linearly separable) will be perfectly classified" ( https://web.mit.edu/course/other/i2course/www/vision_and_learning/perceptron_notes.pdf ). I have the following question : Did the early AI Researchers have "confidence in the mathematical abilities" of the first Perceptron? Or was this more of a trial-and-error experiment which had no mathematical justifications as to if such an experiment was even theoretically possible - let alone in practice? Thanks!
