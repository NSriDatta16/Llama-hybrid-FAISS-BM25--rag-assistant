[site]: crossvalidated
[post_id]: 395032
[parent_id]: 394296
[tags]: 
The KL divergence tells us how well the probability distribution Q approximates the probability distribution P by calculating the cross-entropy minus the entropy. Intuitively, you can think of that as the statistical measure of how one distribution differs from another. In VAE, let $X$ be the data we want to model, $z$ be latent variable, $P(X)$ be the probability distribution of data, $P(z)$ be the probability distribution of the latent variable and $P(X|z)$ be the distribution of generating data given latent variable In the case of variational autoencoders, our objective is to infer $P(z)$ from $P(z|X)$ . $P(z|X)$ is the probability distribution that projects our data into latent space. But since we do not have the distribution $P(z|X)$ , we estimate it using its simpler estimation $Q$ . Now while training our VAE, the encoder should try to learn the simpler distribution $Q(z|X)$ such that it is as close as possible to the actual distribution $P(z|X)$ . This is where we use KL divergence as a measure of a difference between two probability distributions. The VAE objective function thus includes this KL divergence term that needs to be minimized. $$ D_{KL}[Q(z|X)||P(z|X)] = E[\log {Q(z|X)} âˆ’ \log {P(z|X)}] $$
