[site]: datascience
[post_id]: 67414
[parent_id]: 67395
[tags]: 
Depends on the question you're asking, I suppose. You are right that you need some kind of significant test. The null hypothesis is that performance is "as expected," hasn't changed, and that any deviation from that is just sampling error or noise. But what is "as expected"? if the business process were indeed static, it'd be straightforward. You know the distribution of measurements and can compute p-values or something. But you have seasonality, and variation by some grouping. You want to control for that? sure. Doing that is also building some kind of time series model. xgboost isn't particularly suited to timeseries forecasting, in the way that maybe FB prophet is. Also, you really want some distribution over predictions to assess how unusual the actual value is. prophet should give you that, but you would need to run xgboost with k-fold CV or something to start to infer the distribution of its prediction. Like, I'm wondering what it means to compare one xgboost prediction to the actual result. What is "too different" and how would you know? You're sort of on the same track but yes I think you are looking for a significance test at some level, and probably a better tool than xgboost.
