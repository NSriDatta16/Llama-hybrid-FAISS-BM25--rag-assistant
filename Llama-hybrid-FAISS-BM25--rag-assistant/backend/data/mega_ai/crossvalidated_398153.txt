[site]: crossvalidated
[post_id]: 398153
[parent_id]: 270618
[tags]: 
The reason for using $\epsilon$ -greedy during testing is that, unlike in supervised machine learning (for example image classification), in reinforcement learning there is no unseen, held-out data set available for the test phase. This means the algorithm is tested on the very same setup that it has been trained on. Now the paper mentions (section Methods, Evaluation procedure): The trained agents were evaluated by playing each game 30 times for up to 5 min each time with different initial random conditions (‘no- op’; see Extended Data Table 1) and an $\epsilon$ -greedy policy with $\epsilon = 0.05$ . This procedure is adopted to minimize the possibility of overfitting during evaluation. Especially since the preprocessed input contains a history of previously encountered states the concern is that, instead of generalizing to the underlying gameplay, the agent just memorizes optimal trajectories for that specific game and replays them during the testing phase; this is what is meant by "the possibility of overfitting during evaluation" . For deterministic environments this is obvious but also for stochastic state transitions memorization (i.e. overfitting) can occur. Using randomization during the test phase, in form of no-op starts of random length as well as a portion of random actions during the game, forces the algorithm to deal with unforeseen states and hence requires some degree of generalization. On the other hand $\epsilon$ -greedy is not used for potentially improving the performance of the algorithm by helping it get unstuck in poorly trained regions of the observation space. Although a given policy can always only be considered an approximate of the optimal policy (for these kind of tasks at least), they have trained well beyond the point where the algorithm would perform nonsensical actions. Using $\epsilon = 0$ during testing would potentially improve the performance but the point here is to show the ability to generalize. Furthermore in most of the Atari games the state also evolves on a no-op and so the agent would naturally get "unstuck" if that ever happened. Considering the elsewhere mentioned labyrinth example where the environment doesn't evolve on no-ops, the agent would quickly learn that running into a wall isn't a good idea if the reward is shaped properly (-1 for each step for example); especially when using optimistic initial values the required exploration happens naturally. In case you still find find your algorithm ever getting stuck in some situations then this means you need to increase the training time (i.e. run more episodes), instead of introducing some auxiliary randomization with respect to the actions. If you are however running in an environment with evolving system dynamics (that is the underlying state transitions or rewards change over time) then you must retain some degree of exploration and update your policy accordingly in order to keep up with the changes.
