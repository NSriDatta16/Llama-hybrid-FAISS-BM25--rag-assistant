[site]: crossvalidated
[post_id]: 451020
[parent_id]: 
[tags]: 
Modeling time series with Gaussian Mixture Model

I'm reading Song and Wang's paper on incremental estimation of GMM for online data streaming clustering. I assumed that we could apply the same idea to model time series, as a time series is a data stream. That said, the paper explicitly mentions that all data points are independently and identically distributed right before its Theorem 1 (Estimator Updating Theorem). My question is, does this assumption of data points being i.i.d make GMM not suitable for modeling time series, because data points in a time series are usually not independent of each other? For instance, we often use autocorrelation to model time series? Since people do use GMM to model time series for tasks like forecasting, clustering, and outlier detection, is it because either My understanding is incorrect as data points in a time series can be considered i.i.d? We can ignore the assumption of i.i.d as this assumption is not that important in practice? Embedded data points or segments into a different space, in which the embedded vectors can be treated as i.i.d? If so, is there any well accepted paper that discusses how to do such embedding? Thanks,
