[site]: datascience
[post_id]: 11066
[parent_id]: 11057
[tags]: 
As far as I understand your question, the difference between those two approaches lies in the field of limits you apply to the model. During ensemble learning, say xgboost, you will be training multiple boosted trees models, each of which would be trained on a (random) subset of features and (random) subset of species from your dataset. That way, you will get N classifiers, for example, each of which would have slightly different experience about your problem, but every one of them would be trained on randomly limited (with no specific limit) chunk of data. If you apply clustering (for any reason, be it diversity of classifiers or reduction of dataset to fit it into memory) before training, you fist rely on a clustering algorithm to find some sort of communities in your dataset and then you train separate classifiers for every cluster. How they are used in your example further, I am not completely sure about, but a concise way would be assignment of a particular cluster for a new data and using 'right' classifier for that instance. For a real-world example, take loan approval problem. In ensemble learning you will have three persons sitting in the same office, talking to a constant flow of customers in need of money, and every credit approver would have similar experience, but will tend to look into particular data they are familiar with, like job, marital status or any other 'feature'. In clustering , you will have three persons approving loans, but one will be working strictly with farmers, other will be looking into cases of recently fired people and third would be dealing with students in urge to buy a new smartphone. All three will be working on the same problem, but their experience will be focused on a subset of people and a student-handler wouldn't be a good choice to make a decision on a farmer loan since he has no experience with appraising their financial situation. Clustering approach makes more sense when you have a diverse data which would contradict itself in terms of training (load approval criterion would be different for gender-based, marital status based and so on, and married persons would be approved with higher probability than single once due to responsibilities and stuff even if their income would be lower than single person's) - but for every cluster you would have a single working model, other-cluster model would provide random results since it hasn't been trained on other-cluster data. Ensemble learning works great in a way that you get a set of models with different 'experience' about your problem. Each classifier would have its own precision/recall/general performance metric, and if three classifiers with different feature and example set would agree on an outcome, generally it is safe to say that if three classifiers say you it is an apple, it should be an apple (because, error rate of 1% for each classifier would give you 1%*1%*1% = 0.0001% chance that all three classifiers are incorrect). The arithmetic of joint classifier error rate isn't that straightforward since usually ensemble sub-models (talking about xgboost here) aren't independent as they share some features, but general idea is like that.
