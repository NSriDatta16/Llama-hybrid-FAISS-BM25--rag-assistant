[site]: crossvalidated
[post_id]: 384378
[parent_id]: 384353
[tags]: 
Normalization has to be the same for all data points in every mini-batch that you use. Imagine that you have one input feature, let's say it's 3 and you calculated min=0 and max=10 , and after normalization you get value 0.3 ( (3 - 0) / (10 - 0) ). Now, let's say in the next iteration you have input 5 , but this time you calculated different min and max, let say min=2 and max=12 . You will get the same normalized value as before 0.3 ( (5 - 2) / (12 - 2) ). You will end up having the same representation for different numbers. Even worse case, when the same value gets different signs from different normalizations. One solution will be to pre-compute min and max and re-use these values in your training. It might take awhile, but you have to do it only once. L-BFGS works only in full-batch training, which means that it hasn't been designed for mini-batch training. If you cannot afford using all samples at once for training than BFGS probably not such a good choice for your problem. You can try to use simple random search or more sophisticated methods like Tree-structured Parzen Estimators (TPE). More information about hyperparameter tuning for neural networks you can find in the article Hyperparameter optimization for Neural Networks
