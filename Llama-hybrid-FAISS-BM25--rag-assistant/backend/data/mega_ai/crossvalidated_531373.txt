[site]: crossvalidated
[post_id]: 531373
[parent_id]: 329017
[tags]: 
Yes, you are correct that infinite horizon MDPs admit a stationary policy (which means time-invariant policy). There are two questions: Why do finite horizon MDPs have a nonstationary policy while infinite horizon MDP has a stationary policy. Why do we use episodic tasks than finite horizon MDPs in practice. Your first question is easy to answer. Consider a 1 length horizon and a 10 length horizon. If you have only 1 step left, it makes sense to immediately take the action that gives you the highest expected reward. However, if you have more steps, it might be possible to take any action that gives you a lesser reward at the moment but would take you to a better state from where you could get better rewards. For example, if you have an exam tomorrow, you will be busy studying for your exam, and you will keep all other stuff away. If you follow cricket, consider the twenty-twenty matches. The batsman would hit the ball differently for the same kind of ball in the first and the last over. However, in the case of an infinite horizon MDP, how many ever steps you have already taken, you have an infinite number of steps remaining. That means you are exactly in the same situation as the previous time you visited the same state. If you are aware of infinite series sums, consider x = 1+ 1/2 + 1/4 + 1/8 + ...=1 + 1/2(1 + 1/2 + 1/4 + 1/8 +...) = 1 + x/2. The same stuff has happened here, because of which you replace the infinite series in the bracket with s. If you considered a test match to be a decent approximation for an infinite horizon case within the first 20 overs, you would notice that the style of play is consistent over these 20 overs (unlike the case in a 20-20 match). Coming to your second question, episodic tasks are goal-oriented. If you reach a particular state, you can end. No limit is directly imposed on the number of steps the agent can take(in fact you may not even know the time that is required beforehand), but just that he needs to finish the job. Such situations are best modelled as episodic tasks. Another reason from a computational perspective is that policy planning on infinite horizon MDPs is easier than that of finite horizon MDPs.
