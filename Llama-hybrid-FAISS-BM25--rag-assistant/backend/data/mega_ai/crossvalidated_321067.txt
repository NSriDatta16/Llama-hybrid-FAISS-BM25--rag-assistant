[site]: crossvalidated
[post_id]: 321067
[parent_id]: 320918
[tags]: 
Let's use a scale with values 1 through 5, where 1 is least inclined toward risk and 5 is most inclined. If you are willing to take the scale as numerical, you might try to use a two-sample t test, even though such a scale hardly produces normal data. A Mann-Whitney-Wilcoxon rank sum test can deal with ordinal data, but would probably not be useful for your data because of it will have many ties. A one-sided two-sample permutation test might be the best choice. It can compare responses of men and women using either differences in means (numerical) or differences in medians (ordinal). Theoretically, it is possible to find the exact permutation distribution under the null hypothesis by using combinatorial methods, but this can be tedious. And a good approximation of the permutation distribution can be found using simulation. Example. Suppose that the proportional choices of responses 1 through 5 are $(1, 1, 2, 3, 2)$ for men and $(1, 3, 2, 1, 1)$ for women. Randomly generated data for 50 men and 50 women according to these proportions might be tabled as follows: m (men) 1 2 3 4 5 3 9 12 14 12 w (women) 1 2 3 4 5 7 23 12 2 6 Thus the observed sample means for men and women are $\bar M = 3.46$ and $\bar W = 2.54,$ respectively. ]A one-sided, pooled 2-sample t test has P-value 0.0001; if it is valid, this P-value provides strong evidence that men tend to have higher scores.] A permutation test begins by finding $\bar D_{obs} = \bar M - \bar W = 0.92.$ Then the 100 observations for men and women are randomly permuted. Taking the first 50 observations to be for 'men' and the last 50 to be for 'women', we get an average difference $\bar D_{prm}.$ (If the null hypothesis is true then the permutation should not make an important difference.) If this permutation procedure is repeated many times, we can simulate the permutation distribution of $\bar D$ and then see whether the observed value 0.92 is an unusual value for the permutation distribution. The P-value is the proportion of $\bar D_{prm}$-values that exceed 0.92. For the fake data shown above, the P-value is about 0.0001. If data are taken to be ordinal (not numerical) one can use medians throughout, and the P-value is 0.03, so the null hypothesis is still rejected at the 5% level of significance. You are not the first person to consider testing such data and several other tests have been proposed for ordinal data. You can google around to see various proprietary software packages for such analyses. Textbooks in psychology and sociology provide additional sources. I prefer the permutation test. (Perhaps other Answers will have different suggestions.) Note: A short program using R statistical software for a permutation test using the difference in means as the 'metric' is shown below. Remove set.seed(1801) for different fake data; remove set.seed(1118) for a different approximation of the P-value. Change mean to median on lines with #* to use the difference in medians as the metric. (In R, more elegant programs can be written for this procedure, but the one below should be easy to understand, even for those not familiar with R.) set.seed(1801) # generate fake data mp = c(1,1,2,3,2); mw=c(1,3,2,1,1) m = sample(1:5, 50, rep=T, prob=mp) # R scales `mp` so elements sum to 1 w = sample(1:5, 50, rep=T, prob=wp) set.seed(1118) # one-sided permutation test: matric=means mw = c(m,w); df.obs = mean(m)-mean(w) #* B = 10^5; df.prm = numeric(B) for(i in 1:B) { mw.prm=sample(mw,100) df.prm[i] = mean(mw.prm[1:50])-mean(mw.prm[51:100]) #* } mean(df.prm > df.obs) # mean of logical vector is its proportion of TRUE's [1] 9e-05 summary(df.prm) Min. 1st Qu. Median Mean 3rd Qu. Max. -1.2000000 -0.1600000 0.0000000 0.0001532 0.1600000 1.0800000
