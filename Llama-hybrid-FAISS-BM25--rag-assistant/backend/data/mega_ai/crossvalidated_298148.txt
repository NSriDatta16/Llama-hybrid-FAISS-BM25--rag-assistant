[site]: crossvalidated
[post_id]: 298148
[parent_id]: 298060
[tags]: 
The way you tried to perform principal component regression didn't work because you didn't include a constant term in the model. You defined $y = X \beta$, so regressing $y$ against $X$ without a constant term works in this case (but isn't a good idea in general; it won't work if $y = X \beta + c$ with nonzero $c$). You then computed the PCA projections $W$, which are a translation of $X$ (subtracting the mean), followed by a rotation. Because of the translation, $y$ can't be expressed as a linear combination of the columns of $W$. It will work if you include a constant term in the model: $y \approx W \hat{\beta} + c$ Why the last step worked In the last step, you performed: $$y_3 = X V_2 \beta_1$$ The way you computed $\beta_1$ using the SVD is equivalent to: $\beta_1 = W^+ y$, where $W^+$ is the Moore-Penrose pseudoinverse of $W$. Plug this in: $$X V_2 W^+ y$$ You computed $W$ as: $W = X_c V_2$. Plug this in: $$X V_2 (X_c V_2)^+ y$$ Rewrite this as: $$X V_2 V_2^+ X_c^+ y$$ $V_2$ is orthonormal, so $V_2 V_2^+$ is the identity matrix: $$X X_c^+ y$$ You defined $y$ as $y = X \beta$. Plug this in: $$X X_c^+ X \beta$$ $X_c$ is a centered version of $X$, so we can write $X = X_c + \vec{1} \mu^T$, where $\vec{1}$ is a column vector containing $n$ ones (one for each data point), and $\mu^T$ is a row vector containing the mean over rows of $X$. Plug this in for the second instance of $X$: $$X X_c^+ (X_c + \vec{1} \mu^T) \beta$$ Expand: $$X X_c^+ X_c \beta + X X_c^+ \vec{1} \mu^T \beta$$ By the definition of the pseudoinverse, $X_c^+ X_c$ is the identity matrix: $$X \beta + X X_c^+ \vec{1} \mu^T \beta$$ $X_c^+ \vec{1}$ can be re-written as $(\vec{1}^+ X_c)^+$. Note that $\vec{1}^+ X_c$ is the mean of $X_c$, which is zero. Therefore, the second term above reduces to zero and we're left with: $$X \beta$$ which is equal to $y$ (in this case; as above, this won't work if $y = X \beta + c$)
