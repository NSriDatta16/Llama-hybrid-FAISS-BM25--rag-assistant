[site]: crossvalidated
[post_id]: 263657
[parent_id]: 
[tags]: 
Sensitivity Analysis in Deep Neural Networks

Following a question already answered ( Extracting weight importance from One-Layer feed-forward network ) I am looking for inference about relevance of inputs in neural networks. Considering a deep net, where reconstructing the input importance by going backward through the layers from the output node of interest may be difficult or time consuming, I was wondering whether there was some theoretical framework in performing sensitivity analysis for neural network, basically slightly change an input and consider how the ouptut node of interest changes. Is there a canonical way of performing some sort of sensitivity analysis in neural networks? I would really welcome some Python code to do so, if there is any
