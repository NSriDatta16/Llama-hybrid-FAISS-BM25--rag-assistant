[site]: crossvalidated
[post_id]: 539126
[parent_id]: 
[tags]: 
Bayesian update (considering normal distributions as model) always decreases the standard deviation

I am trying to refine a sensor measurement using Bayes rule to update the sensor value $x$ after consecutive measurements. I model the sensor value with a normal distribution $P(x) = N(x; \mu, \sigma)$ . Also the probability density function of the observation given the sensor value $x$ is modeled with a normal distribution $P(x_{obs} | x) \propto N(x_{obs}; x, \sigma_{obs})$ ; with $\sigma_{obs}$ , a standard deviation which changes for each data point. Using Bayes rule, we have that $ P(x | x_{obs}) \propto P(x_{obs} | x) P(x)$ , and considering that we have normal distributions $P(x | x_{obs})$ is also a normal distribution with standard deviation given by $\sigma_{new} = \sqrt(\frac{\sigma_{obs}^{2} \sigma^{2} }{ \sigma_{obs}^{2} + \sigma^{2} })$ . Now I assume that I am applying Bayes rule correctly. However, what troubles me is that regardless on how close each consecutive measurements are to each other, the standard deviation always decreases after each update. To check this, I have the following code: def compute_var(sigma_1, sigma_2): sigma_c = np.sqrt( np.power(sigma_1, 2) * np.power(sigma_2, 2) / (np.power(sigma_1, 2) + np.power(sigma_2, 2))) return sigma_c s = [] q = [] sigma_2 = 3 for sigma_1 in tqdm(np.linspace(0.0001, 10, 1000)): s.append(compute_var(sigma_1, sigma_2)) q.append(compute_var(sigma_1, sigma_2) / np.minimum(sigma_1, sigma_2)) plt.plot(q) For the problem that I am considering this seems a bit strange behavior. Hence, I have the following questions: am I applying Bayes update rule in a wrong fashion ? if I applying correctly, is my reasoning about the decrease in standard deviation correct? and if I would like to prevent the standard deviation to always decrease, how could I go about that?
