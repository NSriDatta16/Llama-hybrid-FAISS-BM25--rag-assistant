[site]: crossvalidated
[post_id]: 195288
[parent_id]: 195287
[tags]: 
(I am the OP.) For example, my answers would be: Slice sampling as per Neal (2003). It requires very little tuning; you only need to very vaguely know the length scales of your problem (and you can get those wrong with little harm, as long as you err on the larger side). Some type of Hamiltonian Monte Carlo . I would probably first try no-U-Turn Sampling by Homan and Gelman (2014). This is an active area of research with many interesting proposals (e.g., Riemannian Monte Carlo by Girolami and Calderhead (2011)) but I am not in the field and don't know what would be the established state-of-the-art. I don't know if knowledge of the normalization constant would allow for a more powerful method (e.g., would it allow to be smarter in how you slice your distribution?). Same as above. Don't know either, but I have the intuition that you should be able to gain something by knowing the factorization of $f$ (perhaps with additional assumptions on how the $f_i$ are related). My gut feeling is based on the fact that, for example, WAIC is more powerful than DIC exactly because it does not take $f(x)$ as a black-box but computes the contribution of each data point separately. Interestingly, Metropolis-Hastings still works if you have a noisy but unbiased estimate of $f(x)$ (although you'll have to fight a large increase in variance and worse mixing properties); see this post at Darren Wilkinson's blog for a good discussion. An accepted paper by Murray and Graham (2016) extends slice sampling to the case of a noisy, unbiased estimate of $f(x)$ via a relatively efficient pseudo-marginal method (I have not tried it yet). I assume that you can perhaps attack it with something like (6) plus a convexity correction. Edit: I just found out a previous question on CrossValidated that addresses this issue. References: Neal, R. M. (2003). "Slice Sampling". Annals of Statistics 31 (3): 705â€“767. Homan, M. D., & Gelman, A. (2014). "The no-U-turn sampler: Adaptively setting path lengths in Hamiltonian Monte Carlo". The Journal of Machine Learning Research, 15(1), 1593-1623. Girolami, M., & Calderhead, B. (2011). "Riemann manifold langevin and hamiltonian monte carlo methods". Journal of the Royal Statistical Society: Series B (Statistical Methodology), 73(2), 123-214. Murray, I., & Graham, M. M. (2016). "Pseudo-Marginal Slice Sampling" To appear in AISTATS 2016.
