[site]: crossvalidated
[post_id]: 608654
[parent_id]: 436154
[tags]: 
To unbiasedly estimate a model's training loss at the end of an epoch, do exactly what you do to estimate its validation loss at the end of an epoch: set the model in evaluation mode (to disable training-only computations like dropout), and apply the model to the sample. Not all of the training sample has to be used here; a random subsample should do just fine. This procedure is less biased than the typical procedure you mentioned: sum the [training] loss of each batch and divide by the number of batches analyzed for getting the loss of the current epoch The typical procedure saves time because there are no extra model calls; it just sums what was already computed. But it's a biased estimator of training loss because: See Sycorax's answer Training-only computations were applied to get the loss for each training batch, since (presumably) this loss is recycled from the forward pass during backpropagation. Applying dropout, for example, causes loss to be overestimated. There are other training-only computations, e.g., feeding the model back true rather than predicted sequence elements as in a "teacher forcing" architecture for language translation (e.g., Figure 10.6 here 1 ), which cause training loss to be underestimated. I personally prefer to compute an unbiased estimate of training loss and error because it's insightful to see how training and validation loss and error compare between different models. One can iterate a model more easily by understanding how much certain interventions affect its bias and variance. The typical procedure is fine if your only goal is to sanity check that optimization is working, i.e., training loss consistently goes down. Training error doesn't have to be estimated at all to select the best performing model. Only validation error needs to be estimated. References Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.
