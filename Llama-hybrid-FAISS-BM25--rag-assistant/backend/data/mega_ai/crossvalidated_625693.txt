[site]: crossvalidated
[post_id]: 625693
[parent_id]: 
[tags]: 
Are embedding in GPT models trainable model parameters?

I have tried to search from a few sources, but I did not see any one of them specifically talking about this issue. For example This blog post seems to imply that the embedding used in transformer is learned separately (in other words fixed during transformer training. As is the case in NLP applications in general, we begin by turning each input word into a vector using an embedding algorithm. but it is rather vague. But in nano GPT code it seems embedding layer is part of the model, which means it will be updated during the model training. So which case is it? Is the embedding for GPT a pre-processing stage or part of the trainable parameters?
