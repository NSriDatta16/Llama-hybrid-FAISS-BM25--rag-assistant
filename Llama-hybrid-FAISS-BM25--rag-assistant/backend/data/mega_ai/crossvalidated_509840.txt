[site]: crossvalidated
[post_id]: 509840
[parent_id]: 369418
[tags]: 
In this case, I find it easier to start from a simple case and then build up in complexity. The simplest case is to consider $J=1$ . $$ \begin{align} -\int_{-\infty}^{\infty} p(z) \log(q(z)) dz &= \frac{1}{2}\log(2\pi\sigma_2^2) - \int p(z) \left(-\frac{\left(z - \mu_2\right)^2}{2 \sigma_2^2}\right)dz \\ &= \frac{1}{2}\log(2\pi\sigma_2^2) + \frac{\mathbb{E}_{z\sim p}[z^2] - 2 \mathbb{E}_{z\sim p}[z]\mu_2 +\mu_2^2} {2\sigma_2^2} \\ &= \frac{1}{2}\log(2\pi\sigma_2^2) + \frac{\sigma_1^2 + \mu_1^2-2\mu_1\mu_2+\mu_2^2}{2\sigma_2^2} \\ &= \frac{1}{2}\log(2\pi\sigma_2^2) + \frac{\sigma_1^2 + (\mu_1 - \mu_2)^2}{2\sigma_2^2}\\ \int_{-\infty}^{\infty} p(z) \log(q(z)) dz = V &= -\frac{1}{2}\log(2\pi\sigma_2^2)-\frac{\sigma_1^2 + (\mu_1 - \mu_2)^2}{2\sigma_2^2} \end{align} $$ The key is recognizing that we can expand the quadratic in the first line; this gives us a sum of several integrals. Then we apply the law of the unconscious statistician, and we use the fact that $\text{Var}(z)=\mathbb{E}[z^2]-\mathbb{E}[z]^2$ . The rest is just rearranging. In this special case, we know that $q$ is a standard normal, so $$ \begin{align} -\frac{1}{2}\log(2\pi\sigma_2^2)-\frac{\sigma_1^2 + (\mu_1 - \mu_2)^2}{2\sigma_2^2} &= -\frac{1}{2} \log(2\pi) -\frac{1}{2}\left(\sigma_1^2 + \mu_1^2\right) \end{align} $$ How can we generalize this to $J>1$ ? Consider the case of a diagonal covariance matrix. In this case, the $z_j$ are independent. So the solution arises from the sum of $V_j$ . If you're not convinced, then you'll need to crank through the matrix arithmetic, using multivariate normal $p$ and $q$ . It's not particularly hard, it's just tedious. Here's some threads to get started: KL-Divergence of $Q(z|X)$ and $P(z)$ in Variational Autoencoder (VAE)
