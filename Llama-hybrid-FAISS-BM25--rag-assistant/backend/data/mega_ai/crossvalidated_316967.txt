[site]: crossvalidated
[post_id]: 316967
[parent_id]: 
[tags]: 
Multinomial vs Gaussian Naive Bayes Performance in Scikit Learn for Word Embedding Features

I am running some experiments using word embedding features with Multinomial and Gaussian Naive Bayes in Scikit learn. As far as I know, Multinomial Naive Bayes works on features with distribution like word frequencies, it may work with tf-idf as well (according to Scikit learn documentation). On the other hand in Gaussian Naive Bayes the data distribution in features is assumed to be a normal distribution and the values can be continuous. I was surprised to see Multinomial NB performed better than Gaussian NB in a multilabel textmining task with a OneVsAll classifier. I am not sure why. Is their anyone who can put an insight. Also, can tell me what NB should be used with Word Embedding features?
