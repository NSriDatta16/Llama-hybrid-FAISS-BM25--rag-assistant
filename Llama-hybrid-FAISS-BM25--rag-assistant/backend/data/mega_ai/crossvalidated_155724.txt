[site]: crossvalidated
[post_id]: 155724
[parent_id]: 152620
[tags]: 
Doing a 3-sample $\chi^2$ test ( prop.test function in R) the variability of each mother is not taken into account. Right. The $\chi^2$ test requires independent sampling for each cell. With a Wilcoxon-Mann-Whitney I would not be actually testing proportions, right? You would be testing proportions, but after they are ranked. Also, this test is a two-sample test, so you would have to go through some additional work. With these data, you are starting to leave the world of specific statistical tests. If the proportion of male+female is not too low or high, you can model this using a generalized linear mixed effect model as a starting point. We can set up some fake data to see. First, we'll get some tools. # Load handy packages. library(ggplot2) library(plyr) library(lme4) library(lsmeans) library(car) library(multcomp) Then, set up the structure of the data. # Generate some fake data. Let's say that # the proportion of male+female is higher on # day #1 than on days #2 and #3. Let's also # say that there is variation among mothers. set.seed(875487) K Now calculate proportions and fix up the data set so the analysis of variance will work correctly. Data$p Just to see if it makes sense: # Look at the data. ggplot(Data, aes(x=Day, y=p, group=Mother)) + geom_line() ddply(Data, .(Day), summarise, Mean=mean(p)) # Day Mean #1 1 0.6223710 #2 2 0.5607464 #3 3 0.5853103 Now, analyze the data. One way to do it is to have a binomial response using the logit link function. Here is an implementation that includes Day as a fixed effect, Mother as a random effect, and Mother by Day interaction as a random effect. # Analyze the data with a random effect model. fit Chisq) #Day 6.9907 2 0.03034 * #--- #Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 There is a Day effect. One way to go is to look at pairwise comparisons. # Perform pairwise comparisons. tuk However, in your case you specified that you were interested in comparing day #1 versus the day #2 and day #3 --- presumably, the average of the two is similar to what you meant. # Test the contrast of interest, day #1 - (day #2 + day #3)/2. # The contrast coefficients look a little weird, but it's due to the # coding used. con |z|) #1 == 0 0.2082 0.0823 2.53 0.0114 * #--- #Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #(Adjusted p values reported -- single-step method) One source of confusion with the estimates and least squares means is the scale on which the analysis is performed. # Look at the estimated marginal means. lsmeans(fit, "Day") # Day lsmean SE df asymp.LCL asymp.UCL # 1 0.4895782 0.07094520 NA 0.35051135 0.6286451 # 2 0.2435273 0.07391394 NA 0.09864109 0.3884135 # 3 0.3192616 0.07409900 NA 0.17401260 0.4645105 # #Confidence level used: 0.95 These are all on the logistic scale. We can transform them back. lsm Pretty good agreement with the raw means. That does not have to happen. You could also use the nlme package and model the $R$-side correlation structure, perhaps with an AR(1) process as an example.
