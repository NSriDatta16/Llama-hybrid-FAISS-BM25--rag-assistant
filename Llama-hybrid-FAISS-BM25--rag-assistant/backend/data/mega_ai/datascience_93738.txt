[site]: datascience
[post_id]: 93738
[parent_id]: 
[tags]: 
How to interpret stagnant validation curve

I'm new to deep learning, so I'm just learning how to interpret my models. I'm creating a mixed-convolutional neural net to classify melanoma images . Here's the model structure: Model: "model" __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_2 (InputLayer) [(None, 80, 120, 3)] 0 __________________________________________________________________________________________________ conv2d (Conv2D) (None, 80, 120, 32) 896 input_2[0][0] __________________________________________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 40, 60, 32) 0 conv2d[0][0] __________________________________________________________________________________________________ conv2d_1 (Conv2D) (None, 40, 60, 64) 18496 max_pooling2d[0][0] __________________________________________________________________________________________________ max_pooling2d_1 (MaxPooling2D) (None, 20, 30, 64) 0 conv2d_1[0][0] __________________________________________________________________________________________________ conv2d_2 (Conv2D) (None, 20, 30, 32) 18464 max_pooling2d_1[0][0] __________________________________________________________________________________________________ max_pooling2d_2 (MaxPooling2D) (None, 10, 15, 32) 0 conv2d_2[0][0] __________________________________________________________________________________________________ input_1 (InputLayer) [(None, 2065)] 0 __________________________________________________________________________________________________ flatten (Flatten) (None, 4800) 0 max_pooling2d_2[0][0] __________________________________________________________________________________________________ dense (Dense) (None, 64) 132224 input_1[0][0] __________________________________________________________________________________________________ dense_2 (Dense) (None, 1024) 4916224 flatten[0][0] __________________________________________________________________________________________________ dense_1 (Dense) (None, 1) 65 dense[0][0] __________________________________________________________________________________________________ dense_3 (Dense) (None, 1) 1025 dense_2[0][0] __________________________________________________________________________________________________ average (Average) (None, 1) 0 dense_1[0][0] dense_3[0][0] ================================================================================================== Total params: 5,087,394 Trainable params: 5,087,394 Non-trainable params: 0 __________________________________________________________________________________________________ I then compile and fit: model.compile('adam', loss='binary_crossentropy', metrics=[auc]) epochs = 15 history = model.fit([meta_train, img_train], y_train, epochs=epochs, batch_size=300, validation_data=([meta_test, img_test], y_test)) When I run the model, I get an increase in AUC for the training data, but the validation AUC is stagnant--it starts higher than the training AUC, but never moves: Is it correct to attribute this to overfitting? Wouldn't the validation AUC go lower if it was overfitting, rather than remaining relatively stagnant? How do I interpret this? EDIT Metadata includes age bins and dummied categorical variables of patient ID, sex, and location on body (torso, upper/lower extremity, etc.)
