[site]: datascience
[post_id]: 123183
[parent_id]: 123181
[tags]: 
What is meant by "high dimensional" vectors? Does that just mean the array of numbers is a big array? Why is that important / highlighted? Yes an array of numbers (floats) of a fixed length. For example 256, 512, 1024 or 12888 (GPT-4). How is text, for example, converted to a vector of numbers (at a high level)? Neural networks learn to map tokens words or texts into arrays of numbers by performing stochastic gradient descent optimization for certain simple problems (for example predict the next word in the text). Why do they call them vector "embeddings"? One of the meanings in mathematics of this word is just mapping an object to a point in a space, in this case, for example the space $\mathbb{R}^{512}$ . I don't need to learn the theory of vectors, how can I understand vector databases practical function and use (as someone who wants to build an AI assistant), without learning about the theory of vectors exactly? You can view them as traditional databases that store embeddings in tables that have $d$ columns, one for each dimension plus an identifier for an object and perhaps some extra metadata. The main advertised advantage is their support for similarity search. In machine learning or information retrieval, when you have a vector (an array of numbers), representing an object or a query, there is often a need to find another vector representing a document that is most similar to the query vector. The neural networks that learn to produce those embeddings from texts or other objects often are such (or can be made such by certain techniques) that if objects x and y are similar in a certain semantic sense (humans would label them as similar), then for some simple mathematical function similarity , the value of similarity(embedding(x), embedding(y)) is very large. I know you don't want to learn, but the terms are cosine similarity, Euclidean distance. The problem that similarity search indices solve is this: in order to find a document $y$ that maximizes similarity(embedding(x), embedding(y)) you would have to scan the whole table, which would be much too slow for applications that operate with millions of objects or more. Recently a number of practically fast and very different methods have become popular (hnsw - a graph-based algorithm with an open source library, product quantization - a method based on some mathematical properties of vectors, developed by Google researchers and implemented by several companies commercially and also as open source (FAISS), Annoy - an open source method developed by an ex-developer in Spotify, etc.) that solve this "nearest neighbour" search problem not exactly, but approximately, guaranteeing, for example, that the most similar object will be found for 95% of queries. There is also another (older) group of methods known as random projections/locality sensitive hashing developed by computer scientists, which are simple and have been proved to be optimal in theory, however they don't seem to be as successful in practice for this task. So, while I have never used them, I assume that what the vector databases do, is they implement those indices and similarity search so that they it works with very large scale, distributed data, etc., and support insertion, deletion, updates, persisting to disk, and other essential database operations.
