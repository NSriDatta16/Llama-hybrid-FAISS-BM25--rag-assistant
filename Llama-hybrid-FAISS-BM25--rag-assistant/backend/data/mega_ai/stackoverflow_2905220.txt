[site]: stackoverflow
[post_id]: 2905220
[parent_id]: 2905059
[tags]: 
Your code looks OK to me. Since ngram extraction and similarity comparison are used very often. You should consider some efficiency issues here. The MapReduce pattern is very suitable for your frequency counting problem: get a string, emit (word, 1) pairs out do a grouping of the words and adds all the counting together. let wordCntReducer (wseq: seq ) = wseq |> Seq.groupBy (fun (id,cnt) -> id) |> Seq.map (fun (id, idseq) -> (id, idseq |> Seq.sumBy(fun (id,cnt) -> cnt))) (* test: wordCntReducer [1,1; 2,1; 1,1; 2,1; 2,2;] *) You also need to maintain a map during your ngram building for a set of strings. As it is much more efficient to handle integers rather than strings during later processing. (2) to compare the distance between two short strings. A common practice is to use Edit Distance using a simple dynamic programming. To compute the similarity between articles, a state-of-the-art method is to use TFIDF feature representation. Actuallym the code above is for term frequency counting, extracted from my data mining library. (3) There are complex NLP methods, e.g. tree kernels based on the parse tree, to in-cooperate the context information in.
