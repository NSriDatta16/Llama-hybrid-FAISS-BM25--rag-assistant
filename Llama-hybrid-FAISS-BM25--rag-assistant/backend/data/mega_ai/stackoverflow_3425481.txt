[site]: stackoverflow
[post_id]: 3425481
[parent_id]: 3411909
[tags]: 
Two things come to mind: Anything that requires real-time / interactive / low latency response times. There is a fixed cost incurred for any job submitted to Hadoop. Any problem that is not embarrassingly parallel . Hadoop can handle a lot of problems that require some simple interdependency between data, since records are joined during the reduce phase. However, certain graph processing and machine learning algorithms are difficult to write in Hadoop because there are too many operations that are dependent on one another. Some machine learning algorithms require very low latency, random access to a large set of data, which Hadoop does not provide out of the box.
