[site]: crossvalidated
[post_id]: 501225
[parent_id]: 
[tags]: 
Decoder's autoregressive output keeps outputting the same token

I'm currently working on a personal reimplementation of the Transformer model from the paper Attention is All You Need (Vaswani et al., 2017) and had a question. Right now the way that I'm getting the output is as follows: import torch import torch.nn.functional as F def decode_autoregressive(model, src): outputs = torch.ones(size=(src.shape[0],)).reshape(-1, 1) * 2 for _ in range(src.shape[1]): prediction = F.softmax(model(src, outputs), dim=2) prediction = torch.argmax(prediction, dim=2)[:, -1] outputs = torch.cat((outputs, prediction.view(-1, 1)), dim=-1) return outputs[:, 1:] To add some explanation, outputs is our output. It's initialized with the EOS/SOS token (which is 2 in the tokenizer I'm using). The src is the source language and it's shape is [batch_size, sequence_len] . I initialized outputs so that it has as many sample numbers as src (i.e., src.shape[0] ). The model outputs values of shape [batch_size, output_seq_len, vocab_size] . The softmax is put on the vocab_size dimension so that we have a probability distribution over the vocabulary. argmax is subsequently used so that we choose the vocabulary word with the highest probability. That's then concatenated to outputs which is again fed into the model in an autoregressive manner. I wrote the code to return the outputs excluding the EOS/SOS token because the ground-truth target tokens doesn't have them. So this is basically a naive way to implement the decoding, which may be the reason why the decoder output is pretty much terrible, resulting in bad BLEU scores as well. I'm wondering what I may be able to do her? I've read somewhere that I need to implement beam search, and another thing I've read is that rather than using a deterministic method like argmax I could select the top-k tokens and sample from there. I'd like to know if anyone else might have any opinions though. Thanks!
