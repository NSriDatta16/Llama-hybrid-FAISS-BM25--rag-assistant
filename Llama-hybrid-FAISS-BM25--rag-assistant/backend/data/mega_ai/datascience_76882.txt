[site]: datascience
[post_id]: 76882
[parent_id]: 45223
[tags]: 
Looking at each model's accuracy is a good place to start, of course, but you should also look at the relationship between their predictions. I'd start with something simple, like checking only the prediction correctness (similar to a confusion matrix) - i.e. both correct / only A correct / only B correct / both incorrect. This would allow you to immediately see if, for example, model B only gets it right when model A does (or more generally, to see if the models are actually quite dependent). If you have many classes, it'll be a bit hard to do the same but by class. Another thing to look at is to see where the averaged model is different than model A; if both model A and the averaged model are very similar in accuracy, there'll be only a few examples when their predictions are different - and you can inspect those predictions. And another thought, that might not apply to your case but might apply to other readers - when looking at accuracy, it's good to have an idea of your class sizes. I'm guessing that for your dataset you don't have 35% of your examples in one class, but it could be that one model just outputs a very high score for the largest class, and averaging it with another model does not change the result.
