[site]: datascience
[post_id]: 100192
[parent_id]: 100188
[tags]: 
Feature engineering has at least two important benefits: You can simplify the task for your model by including interactions between features which the model otherwise would have to learn Beyond simplification, you can inject prior knowledge (e.g. expert knowledge) into the data and eventually the model Here are two examples: The non-linear case - Suppose you're developing a model to predict the risk of severe Covid19 (e.g. defined as patients being hospitalized or dying). If your datasets includes features, such as weight and height , your model could learn the association between these features and the outcome. However, given that empirical evidence demonstrated an increased risk of severe Covid19 for obese people you could feature engineer an independent variable BMI or obesity . If you do so your model does not need to learn the, most likely, non-linear relationship between height and weight with regards to the target variable. The linear case - Suppose your developing a model to predict whether a household will purchase a luxury car. If your dataset includes income from salary , income from investments and other income . Then feature engineering a linear combination of these 3 total income can make it easier for your model to learn the relationship between total income and the target variable. To make it more concrete: Take, for example, a decision tree. Without total income it might need several split points including the different income variables to derive a prediction. In contrast, splitting on total income might result in a prediction with much fewer nodes required. The non-linear case is usually more relevant though.
