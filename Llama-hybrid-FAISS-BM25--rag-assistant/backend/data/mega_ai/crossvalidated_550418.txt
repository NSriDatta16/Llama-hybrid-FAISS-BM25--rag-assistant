[site]: crossvalidated
[post_id]: 550418
[parent_id]: 550308
[tags]: 
Anything can be seen as a "metric", and both groups, statisticians and machine-learners, use plenty of those: accuracy, mean value, estimated parameter of a model, etc. Hypothesis testing is done on top of these "metrics" in order to measure their uncertainty. For example, if you have 5 male and 5 female students you can measure their heights and get a "metric" for the average height difference between males and females. But the number you get will not reflect the real average difference between all males and all females. Hypothesis testing tries to check if a hypothesis about a population is consistent with the observed "metric". Same holds for accuracy measurements in machine learning. You build a model and, using a test set of, say, 100 samples, you get an accuracy of 88%. But this is just a measure of accuracy on 100 samples, and not the true accuracy. If you used another 100 samples you would get a slightly different number. So given this accuracy on a set of 100 samples - what can we say about the true accuracy of this classifier? This is where hypothesis testing comes in. And it allows us to answer questions like "how surprising would it be to get an accuracy of 88% on my 100 samples, if the true accuracy of a model is 75%".
