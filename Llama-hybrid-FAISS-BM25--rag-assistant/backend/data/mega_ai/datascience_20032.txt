[site]: datascience
[post_id]: 20032
[parent_id]: 
[tags]: 
Audio Spectrum Normalization for NeuralNetwork Classification

in my free time I am working on a little machine learning project with focus on NeuralNetworks. It is all about Chord-Classification from an acoustic guitar through a microphone. So here is what I got so far: I am using a simple FeedForward-Network (for first) and train it using the Backpropagation algorithm. First I record some sounds of me strumming simple chords (once, no complex strumming). This data is transformed in realtime with the FourierTransformation into a nice spectrum (this is done every 10ms) and saved in a file. After that I load these files containing the information about which chord was played and the spectrum data through time (every 10ms I got a full spectrum). This data then is normalized by the highest amplitude of all spectrum data (of all chord recordings). The now normalized data is used to train a simple neural network using batch training and backpropagation (with momentum). The activation function is the default sigmoid-function. After some epochs (about 30) the accuracy is mostly around 95%, enough for me! So then I export the network into a file, just to keep it stored. Now when I play some chords and want them to be classified, I load the NeuralNetwork. Before I can start playing, I have to calibrate the program for normalization reasons: I play a few chords and the program transforms it into a spectrum just like the training data. But this information is now used to normalize the real time data of me playing chords recorded through the mic. This basically works well. The program does what it should: Classifies the chords I play (mostly) correctly. So whats my problem? I don't like the way I am normalizing my data, as for now it depends on how loud I play the chords (highest amplitude). The problem is, if I train the network for example with chords played in a mid range volume it has problems classifing me playing very loud. Same problem would be with more quiet playing. Training the Network with loud and quiet data doesn't solve this either as the more quiet data (smaller amplitudes) is kinda getting lost by the loud data (high amplitudes). This is where you come in: I want to know how I could optimize that. How can I normalize the spectrum data so that my network will classify loud data as well as quiet data correctly. (I know there may be better techniques like LSTM-ReccurentNetworks, but I want this to work properly, before I go onto that topic) Thanks in advance!
