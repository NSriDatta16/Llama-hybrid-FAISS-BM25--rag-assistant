[site]: crossvalidated
[post_id]: 635738
[parent_id]: 
[tags]: 
Residuals and "error terms" in time series

I'm self-studying and I see “residuals” seems to be what is left, after we take away non-random components. So if we have additive decomposition : $$ Series = Constant + Trend\text{ }_t + Seasonality\text{ }_t + RandomComponent\text{ }_t $$ $$ = \alpha + \beta t + S_t + u_t $$ The residual is $u_t$ . But now if we want to fit an ARMA on the residual $u_t$ , say, an ARMA(2,2), then the MA(2) is supposed to be the “error term”. So it can be considered a residual too (residuals are observed errors). Peraphs wrongly, I have deduced from this terminology that MA sould be understood as the "residual" of the ARMA process : $$ ARMA(2,2) : $$ $$u_t = X_t = \sum_{i=1}^2 {\phi_{t-i}X_{t-i}} + \sum_{i=1}^2 \theta_{t-i}\varepsilon_{t-i}$$ $$\rightarrow X_t = AR(2) + error\_terms$$ $$\rightarrow X_t = AR(2) + residuals $$ In this context the residual would be the $MA(2)$ . I know the case can exist that there is no MA, so no "error term", because it can happen that we fit an ARMA(p,0) instead. So then the residual would the white noise $\varepsilon$ that appears as residual of the AR process. So whenever we talk about the "residuals", for instance when we perform tests of Box-Jenkins algorithm, what do we mean ? Is it : the entire ARMA ( $u_t$ residuals of decomposition) or should we zoom in the MA or white noise component of $u_t$ (error terms) ?
