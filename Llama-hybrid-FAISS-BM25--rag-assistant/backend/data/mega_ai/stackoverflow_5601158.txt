[site]: stackoverflow
[post_id]: 5601158
[parent_id]: 5600839
[tags]: 
You've touched on a few things here. Let me try to cover them all ... If you've written an in-house test framework, then it should have some tests around it. If you're not using the off-the-shelf frameworks and tools, then you need to know that the framework itself is working the way it is expected to. Tests will provide that. Tests are a double-check of the code. It is better to test the code again from a different angle than to write a test of the test. I would spend time expanding my test data set or building a fuzz tester or some other such exercise to expand test coverage than spend time writing code to test the tests themselves. You should formalize the testing process. Having a test plan (even if it is just a list of scenarios with repro steps in a wiki) is a huge step up from where most people are in their testing efforts. You should also have some form of code review for tests before/when they are checked into the code repository. This will help catch little errors like, "Oh ... you didn't realize that when x is true that it always returns five?" When a test fails, one of the first things that a tester should check is if the test is correct. (Often because that's the first thing the developer is going to insist, that the test is wrong and their code is right.) Swallow your pride and verify the test is 100% correct, then try to figure out if there is a bug in the shipping code. You are using bug tracking software, right? When a test is broken, a bug is filed, isn't it? Sharing these test bugs helps everyone learn how to be better testers. And finally, when all the tests are passing is when the tester should be the most vigilant! Double-check everything again and use this time to expand the test coverage in new ways, like fuzz testing, model-based testing, etc.
