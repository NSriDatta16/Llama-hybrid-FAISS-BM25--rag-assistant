[site]: crossvalidated
[post_id]: 393886
[parent_id]: 
[tags]: 
Uncertainty estimation in high-dimensional inference problems without sampling?

I'm working on a high-dimensional inference problem (around 2000 model parameters) for which we are able to robustly perform MAP estimation by finding the global maximum of the log-posterior using a combination of gradient-based optimisation and a genetic algorithm. I'd very much like to be able to make some estimate of the uncertainties on the model parameters in addition to finding the MAP estimate. We are able to efficiently calculate the gradient of the log-posterior with respect to the parameters, so long-term we're aiming to use Hamiltonian MCMC to do some sampling, but for now I'm interested in non-sampling based estimates. The only approach I know of is to calculate the inverse of the Hessian at the mode to approximate the posterior as multivariate normal, but even this seems infeasible for such a large system, since even if we calculate the $\sim 4\times10^{6}$ elements of the Hessian I'm sure we couldn't find its inverse. Can anyone suggest what kind of approaches are typically used in cases like this? Thanks! EDIT - additional information about the problem Background This is an inverse-problem related to a large physics experiment. We have a 2D triangular mesh which describes some physical fields, and our model parameters are the physical values of those fields at each vertex of the mesh. The mesh has about 650 vertices, and we model 3 fields, so that's where our 2000 model parameters come from. Our experimental data is from instruments which do not measure these fields directly, but quantities that are complicated non-linear functions of the fields. For each of the different instruments we have a forward-model which maps the model parameters to predictions of the experimental data, and a comparison between the prediction and the measurement yields a log-likelihood. We then sum up the log-likelihoods from all these different instruments, and also add some log-prior values which apply some physical constraints to the fields. Consequently I doubt this 'model' falls neatly into a category - we don't have a choice of what the model is, it is dictated by how the actual instruments function that collect our experimental data. Data set The data set is composed of 500x500 images, and there is one image for each camera so total data points is 500x500x4 = $10^6$ . Error model We take all errors in the problem to be Gaussian at the moment. At some point I might try to move over to a student-t error model just for some extra flexibility, but things still seem to function well with just Gaussians. Likelihood example This is a plasma physics experiment, and the vast majority of our data comes from cameras pointed at the plasma with particular filters in front of the lenses to look only at specific parts of the light spectrum. To reproduce the data there are two steps; first we have to model the light that comes from the plasma on the mesh, then we have to model that light back to a camera image. Modelling the light that comes from the plasma unfortunately depends on what are effectively rate coefficients, which say how much light is emitted by different processes given the fields. These rates are predicted by some expensive numerical models, so we have to store their output on grids, and then interpolate to look up values. The rate function data is only ever computed once - we store it then build a spline from it when the code starts up, and then that spline gets used for all the function evaluations. Suppose $R_1$ and $R_2$ are the rate functions (which we evaluate by interpolation), then the emission at the $i$ 'th vertex of the mesh $\mathcal{E}_i$ is given by $$ \mathcal{E}_i = R_1(x_i, y_i) + z_i R_2(x_i, y_i) $$ where $(x,y,z)$ are the 3 fields we model on the mesh. Getting the vector of emissions to a camera image is easy, it's just multiplication with a matrix $\mathbf{G}$ which encodes what parts of the mesh each camera pixel looks through. Since the errors are Gaussian the log-likelihood for this particular camera is then $$ \mathcal{L} = -\frac{1}{2} (\mathbf{G}\vec{\mathcal{E}} - \vec{d})^{\top}\mathbf{\Sigma}^{-1} (\mathbf{G}\vec{\mathcal{E}} - \vec{d}) $$ where $\vec{d}$ is the camera data. The total log-likelihood is a sum of 4 of the above expressions but for different cameras, which all have different versions of the rate functions $R_1, R_2$ because they're looking at different parts of the light spectrum. Prior example We have various priors which effectively just set certain upper and lower bounds on various quantities, but these tend not to act too strongly on the problem. We do have one prior that acts strongly, which effectively applies Laplacian-type smoothing to the fields. It also takes a Gaussian form: $$ \text{log-prior} = -\frac{1}{2}\vec{x}^{\top}\mathbf{S}\vec{x} -\frac{1}{2}\vec{y}^{\top}\mathbf{S}\vec{y} -\frac{1}{2}\vec{z}^{\top}\mathbf{S}\vec{z} $$
