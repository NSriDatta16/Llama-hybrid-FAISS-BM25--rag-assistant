[site]: crossvalidated
[post_id]: 413826
[parent_id]: 
[tags]: 
Neural network model has way more features than samples but yields good test accuracy

I am recently doing a bioinformatic machine learning project. We have over 470,000 features and only 700 training samples and 300 test samples. We used a perceptron with 1 hidden layer to train. However, the result is surprisingly well, even though the number of features are way more than that of data. Intuitively, we used PCA, lasso regularization and elastic net regularization for dimension reduction. The prediction accuracy for 40 selected PCA is higher, but the addition of lasso or elastic net penalty even reduces performance slightly, although the feature space is over 470,000. The whole thing is really counter-intuitive. It should be the case that when you have so much features, the model will not yield a consistent unique solution and the model is doomed to overfit. However, the test set results is surprisingly well.The prediction accuracy for age ranged from 10 to 100 is less than 4. We try to print the weight vector but it doesn't give us much information. It seems that the neural network reduces many of those weights very close to zero. Lasso reduces even more weights close or equal to 0, yet the result is worse. Are there possible explanations to explain this? Or are there some approaches we can use to see what is going on?
