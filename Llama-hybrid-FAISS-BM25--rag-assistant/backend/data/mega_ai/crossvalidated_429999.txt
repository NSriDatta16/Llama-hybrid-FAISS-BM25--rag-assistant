[site]: crossvalidated
[post_id]: 429999
[parent_id]: 429979
[tags]: 
It's better to do the standard $10$ -fold cross-validation in your case, since you have $n=260$ observations. Leave-one-out cross-validation is more appropriate if the $n$ is smaller. You're completely right, after the cross-validation you need to train a new model on the full data with the hyperparameter $\lambda$ found by the cross-validation. If you're using R for this analysis (which is highly recommended), everything should be easy to implement with the cv.glmnet() function from the glmnet package, with parameter family="binomial" for the logistic regression. Take care that you normalized your gene expression values reasonably. Look up "RPKM" and "TPM" for a further understanding of how to model with gene expression values. Below a minimum working example based on your comments, this should give you a smooth start with the cv.glmnet function (X is your gene expression matrix, y your 0-1-encoded disease status): library(glmnet) my_glmnet $lambda==my_glmnet$ lambda.min) # this gives you the index of the optimal lambda (you can also try lambda.1se) opti_coefficients $glmnet.fit$ beta[, which_opti_lambda] # get the coefficients of the final run for the optimal lambda which(opti_coefficients!=0) # these are the genes that were chosen by lasso cv logistic regression
