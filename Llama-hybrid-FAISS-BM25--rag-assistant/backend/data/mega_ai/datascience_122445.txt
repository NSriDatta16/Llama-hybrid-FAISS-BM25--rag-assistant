[site]: datascience
[post_id]: 122445
[parent_id]: 43933
[tags]: 
The accepted answer is good, but I wanted more. First I should say this is a good guess, but it's not correct AFAIK: Is it because they can't accept floats on their input? Both types of data preprocessing ( n-gram data prep and sequence data prep) can produce integers and/or floats. Both types of models ( MLP's and SVM's , as well as RNN's ), can accept integers and/or floats. Here's what I learned about sequences: when preparing the data, only certain preprocessing preserves order information when choosing a model, only certain models can understand order information When preparing the data... for the sequence model Note this example, which is how the Google tutorial prepares data for the sequence model (where order matters) : from tensorflow.keras.preprocessing import text from tensorflow.keras.preprocessing import sequence ts = ['The mouse ran up the clock', 'The mouse ran down'] tokenizer = text.Tokenizer(num_words=10) tokenizer.fit_on_texts(ts) sequence.pad_sequences(tokenizer.texts_to_sequences(ts), 6) array([[1, 2, 3, 4, 1, 5], [0, 0, 1, 2, 3, 6]], dtype=int32) ... as shown in Google's image:... for the bag-of-words aka n-gram model.. Note this example, which is how the Google tutorial prepares its data for the n-gram model (although I use unigrams); unigrams don't remember anything about word order. Notice these sentences which have different order (resulting in opposite meaning), appear to be the same as far as TfidfVectorizer is concerned; the two rows are identical: from sklearn.feature_extraction.text import TfidfVectorizer v = TfidfVectorizer() ts = ['Cat beats mouse', 'Mouse beats cat'] print(v.fit_transform(ts).todense()) [[0.57735027 0.57735027 0.57735027] [0.57735027 0.57735027 0.57735027]] As @Dmytro / accepted answer says, we could use bigrams (n-gram of size 2), or increasing-size-grams. The effect is that the two different sentences (different only in word order) are encoded differently (so there is some understanding these sentences have different meaning), as shown in the example below (the two rows are not identical) from sklearn.feature_extraction.text import TfidfVectorizer # Notice now we allow bi-grams ----v v = TfidfVectorizer(ngram_range=(1,2)) ts = ['Cat beats mouse', 'Mouse beats cat'] print(v.fit_transform(ts).todense()) # Notice the bigram "beats mouse" is different than "beats cat", # each of those bigrams appear in only one of the rows, making the rows' meaning distinct [[0.37930349 0. 0.53309782 0.37930349 0.53309782 0.37930349 0. ] [0.37930349 0.53309782 0. 0.37930349 0. 0.37930349 0.53309782]] However, increasing n-gram size won't remember the entire sentence order (unlike the text_to_sequence example above). Moreover, increasing n-gram size produces increasingly rare combinations that you may discard from your vocabulary during feature selection (because your vocabulary; the count of distinct n-grams) is too large, as @Dmytro says. Therefore only certain types of preprocessing will preserve order in the first place! If you don't preserve order, it doesn't matter what model you choose, there is no order to learn from! When choosing a model This is your main question: Why some ML model (Simple multi-layer perceptrons, gradient boosting machines and support vector machines models) cannot leverage any information about text ordering? models that don't understand order can still try to use the data... Could you pass the texts_to_sequences data into an MLP model? Sure! It's a matrix. Revisit the example above; this array can be passed into an MLP model (or an SVM model, or even other types of model). # ['The mouse ran up the clock', 'The mouse ran down'] array([[1, 2, 3, 4, 1, 5], [0, 0, 1, 2, 3, 6]], dtype=int32) This is where I struggled with @Dmytro's language ... In case of RNNs, words (in form of embeddings) are fed to the model sequentially ... In the phrase "words ... are fed to", I think "words" refers to the data in the matrix matrix; but the data would be "fed to" an MLP the same way it would be fed to an "RNN". The key difference is that MLP interprets each column as a separate feature; (as @Dmytro said; interprets feature values "independently"); so it doesn't learn "word 1 is often followed by word 2, then word 3, word 4, etc.", as represented in the first row: [1, 2, 3, 4, 1, 5] In fact MLP may even learn correlations between features (when the first column is "1" the second column is "2"), but what about a shift/padding for a simpler sentence 'The mouse ran up'. Now it's the third column with word "1" and the fourth column with word "2": [0, 0, 1, 2, 3, 4] But only certain models (like RNN's ) are able to understand order... The special thing about RNN's is that they can learn order within a row ; they're good at sequences. (Conversely, if you don't care about order, you probably don't need an RNN ) And why is it that RNN's "can do order" whereas other models like MLP cannot? Because RNN's have "memory"; they use backpropagation . This means the same row can be reprocessed in different time steps. At the first timestep , the first word is processed (red), and the output of that processing can be considered at the second timestep, when the second word is processed (blue) For me this helps clarify how the architecture of RNN is specifically designed for a "memory" across the "order" of feature values within a row , whereas other simpler models don't work that way (and thus can only understand "order" if it is encoded in new features (bi-grams, tri-grams, etc))
