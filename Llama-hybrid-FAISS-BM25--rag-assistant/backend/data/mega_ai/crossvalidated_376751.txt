[site]: crossvalidated
[post_id]: 376751
[parent_id]: 
[tags]: 
What are paralell training and attention mechanism?

I read a quite interesting paper here: http://hanj.cs.illinois.edu/pdf/kdd18_cyang.pdf Accordingly, the basic idea is to combine clustering and churn prediction so that it can imply some insight from churn prediction. However, I don't really understand the concept "k-sub LSTM parallel training" and "attention mechanism" in this context. Both LSTM and attention mechanism are mostly used in NLP which I am not familiar with (I don't really know much about NN either), so I am quite curious how they worded here. Does the following procedure make sense and follow what the author did in his paper? 1) Using K-means clustering to devide the data into k clusters 2) Using LSTM to classify the training data by k above label 3) Assigning weights to each cluster and continue LSTM for predicting churn in the last layer And how did the author calculate the weights for each cluster here? I really appreciate any help with interpreting the idea. (May it be possible to even combine NN with logistic and clustering in the similar way?)
