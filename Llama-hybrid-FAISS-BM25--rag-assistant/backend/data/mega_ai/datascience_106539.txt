[site]: datascience
[post_id]: 106539
[parent_id]: 
[tags]: 
Alternatives to negative sampling in word2vec

In word2vec, the natural negative log likelihood has a term of the shape $$\log \sum_{w \in V} \exp (v_w \cdot v_c')$$ where $V$ is the set of vocabulary, $v_w$ is embedding for word, and $v_c'$ is embedding for context. A commonly-cited reason to use negative sampling (thus changing the training objective slightly) is that the sum above could be very long, since it involves $|V|$ terms, and thus causes training to be slow. (For example, this is mentioned at p.9/10 of these notes ) Question : Are there methods that directly address derivative involving the above term? Why is negative sampling preferred over those methods - is it empirical, or is there a conceptual explanation? Thanks! For example, one method I can think of is: pre-compute word frequency, sample $F$ words based on the frequency, and approximate $$\log \sum_{w \in V} \exp (v_w \cdot v_c') \sim \log \frac{|V|}{F} \sum_{f=1}^F \exp(v_{w_f} \cdot v_c')$$ Its derivative can also be approximated by only updating the embeddings of the $F$ sampled words. There's probably some error bounds one can compute that depends on $F$ , but this looks feasible on a first glance.
