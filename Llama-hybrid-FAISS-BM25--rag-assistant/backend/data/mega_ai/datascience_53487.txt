[site]: datascience
[post_id]: 53487
[parent_id]: 
[tags]: 
Exponential Linear Units (ELU) vs $log(1+e^x)$ as the activation functions of deep learning

It seems ELU (Exponential Linear Units) is used as an activation function for deep learning. But its' graph is very similar to the graph of $log(1+e^x)$ . So why has $log(1+e^x)$ not been used as the activation functions instead of ELU? In other words what is the advantage of ELU over $log(1+e^x)$ ?
