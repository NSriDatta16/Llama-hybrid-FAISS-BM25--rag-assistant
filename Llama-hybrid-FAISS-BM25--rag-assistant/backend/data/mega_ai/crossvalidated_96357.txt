[site]: crossvalidated
[post_id]: 96357
[parent_id]: 
[tags]: 
Does a Neural Network actually need an activation function or is that just for Back Propagation?

I have a feed forward neural network (1 hidden layer with 10 neurons, 1 output layer with 1 neuron) with no activation function (only transfer by weight + bias) that can learn a really wonky sin wave (using a 2in1out window) with production usable accuracy trained via stochastic climbing in a couple seconds: for (int d = 0; d I'm probably just drunk, but if you don't use an activation function do you lose that universal function approximator status? Or is it just for gradient descent / back propagation etc. to act as a differentiable function? Alternatively, have I probably just overlooked a bug and am actually secretly activating without knowing it? source in C# (draws on a form)
