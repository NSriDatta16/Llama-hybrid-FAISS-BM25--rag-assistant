[site]: crossvalidated
[post_id]: 140453
[parent_id]: 139906
[tags]: 
In addition to my comments above and links within, I'd like to share a bit more information, which is hopefully relevant and helpful. It seems that ensemble methods , such as ensemble Bayesian model averaging (EBMA) , indeed improve predictive ability of individual models as well as offer certain other benefits. For example, Montgomery and Hollenbach (2012) write: Yet, combining forecasts, and ensemble methods in particular, have been shown to substantially reduce prediction error in two important ways. First, across subject domains, ensemble predictions are usually more accurate than any individual component model. Second, they are significantly less likely to make dramatically incorrect predictions (Bates and Granger 1969; Armstrong 2001; Raftery et al. 2005). Combining forecasts not only reduces reliance on single data sources and methodologies (which lowers the likelihood of dramatic errors), but also allows for the incorporation of more information than any one model is likely to include in isolation. A paper by Singh, Mishra and Ruskauf (2010) provides an interesting comparison of a subset of model averaging techniques , which include three types: frequentist, Bayesian and information theory-based. Finishing on a practical note, I would like to share a page from a popular Python machine learning library scikit-learn , dedicated to several frequentist ensemble methods . References Montgomery, J. M., & Hollenbach, F. (2012). Improving predictions using ensemble Bayesian model averaging. [Working paper] Retrieved from http://pages.wustl.edu/montgomery/ebma Singh, A., Mishra, S., & Ruskauff, G. (2010). Model averaging techniques for quantifying conceptual model uncertainty. Ground Water, 48 (5), 701-715.
