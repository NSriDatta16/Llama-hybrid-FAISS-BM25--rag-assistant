[site]: datascience
[post_id]: 33397
[parent_id]: 33392
[tags]: 
Ensemble learning combines predictions from multiple learners. Boosting methods are one way to form an ensemble. Stacking is another. The important difference between boosting and stacking (and other ensemble methods) is that boosting applies a number of weak learners sequentially and then produces a final result via a weighted majority vote. The learners in stacking can also be combined as a weighted average or vote (or by another "meta" model) but they can be more or less independent. In boosting, each weak learner (usually the same, yes) modifies the data for the next learner in the sequence. E.g. in AdaBoost the weight of each learner and also each sample in the data depends on the misclassification rate of the previous weak learner to the effect that the next learner focuses more on previously misclassified samples. Boosting methods typically vary by how these weight updates are performed.
