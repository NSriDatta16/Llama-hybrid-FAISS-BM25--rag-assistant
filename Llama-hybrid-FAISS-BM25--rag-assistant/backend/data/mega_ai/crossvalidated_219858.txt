[site]: crossvalidated
[post_id]: 219858
[parent_id]: 219373
[tags]: 
Curse of dimensionality in machine learning is more often the problem of exploding empty space between the few data points that you have. Low manifold data can make it even worse. Here is an example setup with 10000 samples where I try to do kNN with 1 neighbor. from numpy.random import normal from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import precision_score import matplotlib.pyplot as plt import numpy as np from math import sqrt from scipy.special import gamma N=10000 N_broad=2 scale=20 dims=[] precs=[] def avg_distance(k): return sqrt(2)*gamma((k+1)/2)/gamma(k/2) for dim in range(N_broad+1,30): clf = KNeighborsClassifier(1, n_jobs=-1) X_train=np.hstack([normal(size=(N,N_broad)), normal(size=(N,dim-N_broad))/avg_distance(dim-N_broad)/scale]) y_train=(X_train[:,N_broad]>0).astype(int) clf.fit(X_train, y_train) X_test=np.hstack([normal(size=(N,N_broad)), normal(size=(N,dim-N_broad))/avg_distance(dim-N_broad)/scale]) y_test=(X_test[:,N_broad]>0).astype(int) y_test_pred=clf.predict(X_test) prec=precision_score(y_test, y_test_pred) dims.append(dim) precs.append(prec) print(dim, prec) plt.plot(dims, precs) plt.ylim([0.5,1]) plt.xlabel("Dimension") plt.ylabel("Precision") plt.title("kNN(1) on {} samples".format(N)) plt.show() You didn't like fully uniform distributions, so I've made this a 2D manifold with smaller dimensions (reduced by scale ) sprinkled around the 2D plane of the first two coordinates. As it happens, one of the smaller dimensions is predictive (the label is 1 when that dimension is positive). The precision drops rapidly with increasing dimension. Of course, precision=0.5 would be random guessing. With a decision surface, which is more complicating than a plane, it would get even worse. It's like the kNN balls are too sparse to be helpful at probing a smooth hyperplane. With higher dimensions they feel increasingly more lonely. On the other hand, methods like SVM have a global view and do much better.
