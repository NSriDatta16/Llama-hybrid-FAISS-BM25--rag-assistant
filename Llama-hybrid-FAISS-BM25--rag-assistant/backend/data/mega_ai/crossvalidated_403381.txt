[site]: crossvalidated
[post_id]: 403381
[parent_id]: 
[tags]: 
Using Random Forest variable importance for feature selection

I'm currently trying to convince my colleague that his method of doing feature selection is causing data leakage and I need help doing so. The method they are using is as follows: They first run a random forest on all variables and get the feature importance measure; MeanDecreaseAccuracy. They then remove all variables that score low on this measure and re-run the forest and report the out of bag error rate as the error for the model. They argue that since the MeanDecreaseAccuracy measure is calculated using the bootstrap and out of bag records that there is no data leakage. I am trying to convince them that since the variable importance measure uses ALL data (in bag records to build the trees and out of bag records to obtain the decrease in accuracy) there is data leakage if they use this measure to do feature selection in this manner. My solution for them was that they cannot use the out of bag error measure if they want to do feature selection, they will have to set up a proper cross validation split and perform the feature selection on the training sets only. Am I incorrect here? Can anyone think of a convincing argument (example or paper) that I can show my colleague?
