[site]: crossvalidated
[post_id]: 285509
[parent_id]: 
[tags]: 
Periodical loss in test data with l2 regularization

I'm using Tensorflow to implement a RNN (LSTM) network to classify a very small dataset (1500 samples) of timeseries (64 steps, 72 features) which represents movement. These samples could come from the same origin so you can have in both in train and test. Not the same movement but a similar one (because the same origin). I'm using a stack of 4 basic LSTM with 24 hidden units: # Linear activation _X = tf.matmul(_X, _weights['hidden']) + _biases['hidden'] _X = tf.split(axis=0, num_or_size_splits=n_steps, value=_X) lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True) lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True) lstm_cell_3 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True) lstm_cell_4 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True) lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2, lstm_cell_3, lstm_cell_4], state_is_tuple=True) # Get LSTM cell output outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X, dtype=tf.float32) # Linear activation result = tf.matmul(outputs[-1], _weights['out']) + _biases['out'] My loss function is: class_weights = tf.constant([12.0, 26.0, 43.0, 17.0]) # deduce weights for batch samples based on their true label weights = tf.reduce_sum(class_weights * y, axis=1) unweighted_losses = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred) weighted_losses = unweighted_losses * weights #L2 Regularization l2 = lambda_loss_amount * sum( tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()) # Final loss cost = tf.reduce_mean(weighted_losses) + l2 And the problem is that I'm getting a normal behaviour in the train data, but my test data is acting weird. . As you can see I having these ups and downs in loss and I don't know why. The first thing that I don't totally understand is how accuracy on test (blue) can stay the same while loss is growing that much? and the second thing is about how the machine learns and learns.. and then it fails? should I have try with smaller values of learning rates and still the same. Maybe you could give me some ideas about.
