[site]: crossvalidated
[post_id]: 278779
[parent_id]: 278755
[tags]: 
In short, suppose we want to solve the linear regression problem with squared loss $$\text{minimize}~ \|Ax-b\|^2$$ We can set the derivative $2A^T(Ax-b)$ to $0$ , and it is solving the linear system $$A^TAx=A^Tb$$ In high level, there are two ways to solve a linear system. Direct method and the iterative method. Note direct method is solving $A^TAx=A^Tb$ , and gradient descent (one example iterative method) is directly solving $\text{minimize}~ \|Ax-b\|^2$ . Comparing to direct methods (Say QR / LU Decomposition). Iterative methods have some advantages when we have a large amount of data or the data is very sparse. Suppose our data matrix $A$ is huge and it is not possible to fit in memory, stochastic gradient descent can be used. I have an answer to explain why How could stochastic gradient descent save time compared to standard gradient descent? For sparse data, check the great book Iterative Methods for Sparse Linear Systems On the other hand, I believe one of the reasons Andrew Ng emphasizes it is because it is a generic method (most widely used method in machine learning) and can be used in other models such as logistic regression or neural network.
