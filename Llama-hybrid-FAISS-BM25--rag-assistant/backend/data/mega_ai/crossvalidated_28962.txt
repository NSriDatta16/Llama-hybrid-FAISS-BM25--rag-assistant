[site]: crossvalidated
[post_id]: 28962
[parent_id]: 28885
[tags]: 
The fundamental idea behind importance sampling is to be able to estimate better the tails of the distribution. It is a variance reduction trick in Monte Carlo. Generally you need a large sample to get observations in the tale of the distribution. Importance sampling gives more weight to the tails or in general any part of the distribution that needs more attention than it would get from basic (sometimes called naive) Monte Carlo. The term is due to Hammersley and Handscombe who wrote a book on Monte Carlo and variance reduction techniques way back in the 1950s. Because you know how you biased the sampling with the weights you chose you can determine weights to get unbiased estimates. There is no free lunch though. While you gain accuracy for some estimates you will lose for others. For example if the distribution has a finite mean the adjusted unbiased estimate will have a larger variance than what you would have gotten taking an unweighted average using naive Monte Carlo. When talking about bootstrap you are usually referring to the process of drawing inference about a population based on the mechanism of sampling with replacement from the original data set. In theory it does not involve Monte Carlo. In practice you cannot usually get bootstrap estimates without a Monte Carlo approximation. I sense that your understanding of bootstrap may be fuzzy. To learn more about the bootstrap Tim Hesterberg's chapter in David Moore's introductory statistics book can be very helpful. Also the book by Efron and Tibshirani or one of mine explain the bootstrap well. Hesterberg's new Mathematical Statistics book also covers bootstrap.
