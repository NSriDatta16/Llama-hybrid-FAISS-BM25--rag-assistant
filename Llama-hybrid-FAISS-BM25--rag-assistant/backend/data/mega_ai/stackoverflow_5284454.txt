[site]: stackoverflow
[post_id]: 5284454
[parent_id]: 5281301
[tags]: 
in abstract, you have a sort of queue that forks. programs on the host produce output that gets enqueued at the head and users each consume from their own tail. the "consume" part is a little funny, because you don't want to "forget" content until all interested users have seen it. Tim Post suggests using a simple queue per user-stream, with the producer enqueueing to all "subscribed" queues. The other approach is a queue-per-producer, with a cursor to mark how far a user has already seen. The latter approach is parsimonious in that it doesn't replicate anything, but does force you to figure out when everyone has seen trailing content so you can forget it. I think I'd try the latter approach, and just punt on forgetting. that is, always store all output forever. this is potentially quite attractive to users, as well - after all, what terminal emulator doesn't have a generous-sized scroll-back buffer? it's more storage, but storage is embarassingly cheap these days. I don't think using a DB is necessarily a bad idea. you might, for instance, want to associate a timestamp with output as it is produced, or to perform searches within the content. tracking how far in a stream users have seen would most likely be done by rowid, but users might appreciate a "show me output for the last hour" kind of interface. storing each queue as a table, with rows indexed by time would be a natural workload for a DB, pretty efficient in space and time. but the basic no-forget queue could be done very simply as a file-per-stream with user positions stored as offset. Tim's approach, with per-user streams, does a bit more work in the producer's context, depending on how often a producer has multiple subscribers. not a viable approach if your producers are like twitter: being watched by millions of users ;) but his stream-per-user approach could also not bother garbage-collecting, could also include the DB and timestamp ideas.
