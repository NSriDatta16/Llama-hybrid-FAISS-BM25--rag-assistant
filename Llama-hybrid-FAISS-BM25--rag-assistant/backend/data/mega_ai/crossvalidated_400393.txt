[site]: crossvalidated
[post_id]: 400393
[parent_id]: 394879
[tags]: 
That is an interesting take because the corresponding points are indeed "Missing at Random" (MAR), i.e. the missingness is due to something happening on a particular day of the week. Of course, this is not fully testable but from is described it seems perfectly reasonable. We need to control for this; before continuing here is a simplified example of why this is a problem: If the " Friday night " users are the least like to buy our services (e.g. shop at a furniture shop) and Friday night is the time of the week our system dropped, our metric will look biased due to selection bias. i.e. if we simply said: " Let's drop Friday night users from both arms and analyse the remaining data. ", we would have an overoptimistic estimate as our "worse customers" would be excluded. (Partial) Solution: Do not use a $\chi^2$ test but rather run a full (logistic) regression where the time of the week as well as the A/B group membership are used as explanatory variables - both set of variables will have to be included, buy/no buy is our response variable. If indeed there is a weekly trend this formulation should allow us account for it. To that extend, using a regression model would let us to control for other covariates as well (e.g. age) - this is optional; their inclusion if they are relevant should increase the precision of our estimate. Usual ways of computing confidence (or prediction) intervals can then be used to quantify the effect of A/B group membership. CV.SE has some very good threads on this I would recommend looking into the threads: " Why is there a difference between manually calculating a logistic regression 95% confidence interval, and using the confint() function in R? " and " Why do my p-values differ between logistic regression output, chi-squared test, and the confidence interval for the OR? " they both very instructive on the matter. The corresponding confidence will allows us to visualise the results directly. Side-note : For future reference, we also might want to initialise our testing first with an A/A test (i.e. we just record how the two randomly assigned groups behave in the absence of any treatment) so we have an idea of the inherit variability and probably catch such collection errors before our actual A/B phase. (Full) Solution: Repeat the experiment. This is a bit heavy-handed but realistically the experiment is contaminated. Re-running the A/B test, if possible, will alleviate concerns about the influence of this kind of collection errors in the data.
