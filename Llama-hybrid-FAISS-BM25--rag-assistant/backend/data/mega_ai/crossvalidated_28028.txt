[site]: crossvalidated
[post_id]: 28028
[parent_id]: 28018
[tags]: 
There are numerous examples of Bayesian analysis of census data in the book Bayesian Data Analysis by Gelman, Carlin, Rubin, and Stern, especially in chapters 5-8. In fact, whenever the census data collection mechanism is not ignorable, this can often be a crucial part of the analysis. Consider, for example, prior beliefs about demographics that might be under-reported due to unusual household living situations. If you were targeting only a few specific covariates from the census data to estimate something about migrant workers, say, taking this into account would be extremely important. That example might not be very realistic, but surely there are other examples with census data that highlight a similar point: Bayesian methods allow you to account for hierarchical aspects of data collection and to concentrate your assumptions into well-articulated priors. This would be important when seeking models for underrepresented demographics or non-ignorable designs. More directly to your three questions: (1) This seems too simplistic. I don't think you just "run the data through Bayes' rule". Bayesian analysis is the process of articulating a prior distribution (it could come from the past census, but probably you also know pieces of information that lead you to have a current prior belief that's somewhat different from just the most recent census (say, regarding job loss or something)), and articulating a likelihood model, and then using computational approaches to construct the posterior. You don't "run" the current census data "through" Bayes' theorem, unless that is your prior and you have already articulated some likelihood model for the problem you're trying to perform inference on. I guess to answer (1) for you, we'd need to know more about what you're thinking of using as a prior, how you are splitting the data analysis out (are you setting up a hierarchical model, with hyperparameters sampled from some meta-prior, etc.?), what sorts of parameters / test statistics you are interested in estimating, and what your likelihood model is. (2) You should probably be modeling things that effect the response rate. (3) I think you are confused a little here. You seem to think that either last year's census or some polling data should be the "prior" and that this year's census should be some sort of check on the posterior. It's like you want to specify a model that converts old census data to new census data, but this doesn't make much sense to me. I could be misunderstanding you, but it seems like you're thinking of Bayes' theorem as a black box that you drop data into... but it's certainly not that. You use Bayes' theorem to test your likelihood model of the data, incorporated with prior beliefs on the parameters of the model. Usually this is done in hierarchical stages to sequester different parts of the model from each other, generally because some subset of the data collection can be treated as ignorable conditioned on knowing certain hyper-parameters.
