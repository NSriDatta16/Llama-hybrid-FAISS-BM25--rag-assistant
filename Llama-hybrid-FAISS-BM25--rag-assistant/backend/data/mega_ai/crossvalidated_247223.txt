[site]: crossvalidated
[post_id]: 247223
[parent_id]: 
[tags]: 
Accounting for uncertainty in a fit coefficient

So given some empirical data of $I_T$ vs $r$ I'd like to fit that to some model given by $$ I_T=\frac{I_0}{1+F \sin\bigg(\frac{2\pi d}{\lambda}\frac{f}{f^2+r^2}\bigg)} $$I am trying to analyze data from a Fabry-Perot interferometer where I measure the intensity profile of light from a helium neon laser in order to calibrate it to measure the hyperfine structure of mercury's green transition line. So, I have the following image, which has a radial intensity profile that looks like My goal is to then fit that intnesity profile to the following function, which is supposed to describe these ring patterns from a FP interferometer $$ I_T=\frac{I_0}{1+F \sin\bigg(\frac{2\pi d}{\lambda}\frac{f}{f^2+r^2}\bigg)} $$ where $I_T$ is intensity at a given value of $r$, $F$ is known as the coefficient of finesse which is related to the resolving power of the interferometer, $d$ is the plate separation of the interferometer, $f$ is the focal length of the lens I've used and $\lambda$ is the wavelength of my helium neon laser. I have measurements of $\lambda$ and $f$. So, I would like to fit for $d$ and $F$. The concern I have run into though, is that I realize now that my measurement of $f$ has a huge amount of experimental error in it and I'd like to account for that in my fit, however I don't really understand how. My knowledge of curve fitting has only informed me of what do to with errors in my measurements of $I_T$ and $r$. I sense a couple ways to deal with this but would like some advice on the best way to proceed. One thing to do is Monte-Carlo simulations. I could run the fit 1000 times for $f$ and $\lambda$ with a gaussian error in them with width proportional to my estimate of their uncertainty, and then measure the variance of the set of Monte-Carlo fit parameters for $d$. This seems like a bit of a pain in the neck, but also doable. Another approach would be to propagate my estimated error in $f$ into my estimated error in $I_T$, ie. do the following transformation to my estimated error in $I_T$ $$ \delta I_t \rightarrow \sqrt{(\delta I_T)^2+\bigg(\frac{\partial I_T}{\partial f}\delta f\bigg)^2+\bigg(\frac{\partial I_T}{\partial \lambda}\delta \lambda\bigg)^2} $$ and then just weigh my data by this transformed uncertainty in my fit. I'm wondering if any of you have opinions on which of these is the 'better' method here. I understand that subjective questions are discouraged here, so let me phrase this as follows: Should the two methods I discussed give the same or similar results? Why or why not? Is there a more correct or less labour intensive way to account for error $f$? I have measurements of $\lambda$ and $f$. So, I would like to fit for $d$. The concern I have run into though, is that I realize now that my measurement of $f$ has a huge amount of experimental error in it and I'd like to account for that in my fit, however I don't really understand how. My knowledge of curve fitting has only informed me of what do to with errors in my measurements of $I_T$ and $r$. I sense a couple ways to deal with this but would like some advice on the best way to proceed. One thing to do is Monte-Carlo simulations. I could run the fit 1000 times for $f$ and $\lambda$ with a gaussian error in them with width proportional to my estimate of their uncertainty, and then measure the variance of the set of Monte-Carlo fit parameters for $d$. This seems like a bit of a pain in the neck, but also doable. Another approach would be to propagate my estimated error in $f$ into my estimated error in $I_T$, ie. do the following transformation to my estimated error in $I_T$ $$ \delta I_t \rightarrow \sqrt{(\delta I_T)^2+\bigg(\frac{\partial I_T}{\partial f}\delta f\bigg)^2+\bigg(\frac{\partial I_T}{\partial \lambda}\delta \lambda\bigg)^2} $$ and then just weigh my data by this transformed uncertainty in my fit. I'm wondering if any of you have opinions on which of these is the 'better' method here. I understand that subjective questions are discouraged here, so let me phrase this as follows: Should the two methods I discussed give the same or similar results? Why or why not? Is there a more correct or less labour intensive way to account for error $f$?
