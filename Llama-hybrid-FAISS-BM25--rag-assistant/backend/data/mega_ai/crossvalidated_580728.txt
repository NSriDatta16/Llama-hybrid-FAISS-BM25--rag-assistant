[site]: crossvalidated
[post_id]: 580728
[parent_id]: 580647
[tags]: 
"What's complicated about regression to the mean" in most realistic cases is that, unlike rolling dice of completely known characteristics, (a) previous data points usually give us information about future data points, e.g. by allowing us to make (or improve upon) estimates of the statistical parameters of the underlying data-generating process (DGP), and (b) the information we gain about the DGP generally leads us to expect "like will be followed by like". For example, the taller I know a father is, then based on the genetic heritability of height, the taller I should expect his son to be. This is obviously in pedagogical tension with most people's intuition of regression to the mean: if heights of fathers and sons were independent random variables, as in your dice experiment, then a tall father is expected to be followed by a shorter* son, and indeed for each inch taller the father is, then the expected height difference between them grows by one inch also. The resolution to this tension requires me to synthesise these two apparently contradictory effects into a more sophisticated mental model: one in which knowing a father is tall leads me to expect his son will be tall too (because of what I've gleaned about genetic factors influencing the son's height), but probably not as tall as the father (due to regression to the mean). For example, a son-versus-father regression line with slope between zero and one would be consistent with both these facts. For simplicity let us assume mean height $m$ is consistent across generations, so the son's expected height $\mathbb{E}(y)$ and father's height $x$ are related by: $$ \mathbb{E}(y) - m = \beta (x-m),\quad 0 Then for every inch taller that the father is, the son's expected height $\mathbb{E}(y)$ increases by $\beta$ inches, a less than one-for-one rise. A tall father is expected to be followed by a son who is tall both in absolute terms and relative to the average height. Yet the extent to which the son is expected to be above average height by, $\mathbb{E}(y) - m$ , is less (since we multiply by a number between zero and one) than his father stood above the average by, $x - m$ . Relative to the father, the expected height differential is: $$ \mathbb{E}(y) - x = -\gamma x + \gamma m,\quad \gamma = 1 - \beta, \quad 0 That is, knowing that a father is one inch taller, leads me to expect the son to be an additional $\gamma = 1 - \beta$ inches shorter compared to his father . Note that in the above formula the expected father-son height differential is zero when $x=m$ : there is no "regression towards the mean" effect if we're at the mean already. Also, if we put $\beta = 0$ so $\gamma = 1$ , you can see we recover the case discussed above where the son's height is unrelated to his father's. The son's expected height is simply the mean height, and each additional inch of height for a taller-than-average father leads us to expect the son will be one more inch shorter than him. Perhaps the above seems trivial to you, but I hope it illustrates why the mental gymnastics required are more strenuous than the dice example suggests. And even this is really just a jumping-off point: why, if this procedure is iterated over many generations, do all the great-great-...-grandchildren not converge towards the average height? To understand this, we need to extend our model to show the whole distribution of heights at each generation. What if we allow for there to be an underlying trend in mean height over generations, with mean height rising due to improved population health and nutrition? If you use independent rolls of a 0-10 di(c)e as your mental model for regression to the mean, you miss out on a lot of complexity. (You called the die "Gaussian" which contradicts its distribution of scores being over a bounded interval, but I'll assume you just mean "central values are more likely, with probability falling off in the tails", like a symmetric Beta distribution with $\alpha = \beta > 1$ that's been rescaled and translated as necessary.) If someone scores a 9 they're well into the upper tail and very likely to do worse the next time. But what if there are two possible dice and you don't know which one they are using? Let's say the dice run 0-10 ("low" with mean 5) and 5-15 ("high" with mean 10). If initially we rate it as a 50:50 chance whether the the low or high die is being used, the expected score is 7.5 so a 9 is still a higher-than-average score. But our calculus for what we expect to happen on the next roll must take into account that a score of 9 is indicative evidence — albeit not definitive — that the higher-scoring of the two dice is in play. Indeed a score of 11 would have told us this for sure, and the expected score on the next roll would clearly be 10. For 9, we could update our assessment of the probability we assigned to each die using a Bayesian method, as suggested in some of the other answers here, and work out the next roll's expected score from there. If we replace the high/low dichotomy by a whole spectrum of possible distributions, this resembles many real-life situations where previous data hints what distribution is being sampled from. Our response to a student acing a test, or a runner's great time in a race, can't simply be "well, they're unlikely to repeat that kind of success". We must also incorporate the evidence we've seen that we have encountered a student or runner of high ability, and update our expectations of their future performance accordingly. If our student is a fighter pilot in training, as Kahneman discusses, the effect of trends is important, since (hopefully!) we would not expect their performance to remain the same over the course of their training. If we learn a pilot has previously had a run of high scores, does that lead us to expect the next score will be (i) higher since we have evidence suggesting the pilot has high ability, (ii) lower due to regression to the mean, (iii) higher since with additional training the pilot's scores should trend upwards, or (iv) some combination of the above effects? (Worth pondering the implicit higher/lower than what ?) And this is before we consider that some pilots will be faster learners than others, that individual trends could be estimated from each pilot's previous testing data, and that a pilot with an apparently outstanding record of improvement to date might be a brilliantly fast learner yet might also be about to hit the regression to the mean effect on their trend , not just on their level... Two final points re "what's complicated". Plenty of people think they understand regression to the mean because "obviously if a student's aced ten tests in a row, or a runner's had a series of good times, they must be overdue a bad result". In other words, they have conflated regression to the mean with the gambler's fallacy . I think this is the most common misconception encountered when teaching the topic. However, it isn't the most dangerous. As Kahneman points out, there's a particular problem when you anticipate a trend or change in level due to interventions targeted on the basis of previous performance . Suppose you scold pilots who performed badly on their last training mission, and praise those who did well. Even if these interventions have no effect at all, we would expect the scolded group to improve on their previous performance and the praised group to get worse, purely due to regression to the mean. Similarly, set up an intervention group for students who did poorly in a test at school: give them extra tuition, or feed them more fresh fruit or fish oil, or just teach them to stand on one leg for thirty seconds before exams. Their performance in the next test (both relative to their previous performance and relative to the rest of the class) is likely to improve. I scarcely need to mention the potential for misleading effects in medical studies which pick out high-risk groups as a target for (what's intended to be a) protective intervention. The problem is not only that we fool ourselves when we observe how our brilliant intervention for low-performers "succeeded" (or for high-performers "failed counterproductively"). We can "fool" the standard statistical tests too: SPSS/Stata/R will quite likely churn out a highly significant p-value when you compare the intervention group's before/after scores, or compare the intervention group's improvement to the non-intervention group's improvement. The issue is we are making the wrong comparison: ideally we need a control group who were eligible for the intervention but were randomly assigned not to receive it. The control group should experience similar regression to the mean effects as the intervention group, but not the (dis)advantages of the intervention itself, allowing these effects to be distinguished. But unless we are undertaking a proper study with some intent to write it up, how often would we do this? Would it even be ethical if the intervention was believed to be beneficial? A class teacher tasked with ensuring all students made the required grade would be most unlikely to refuse extra support to a randomly chosen subset of low-achieving students just to confirm the effectiveness of their methods: as a result, it's quite likely the teacher's impression of the value of their intervention is over-inflated. Even people who understand the maths of regression to the mean are prone to this kind of error, a trap we likely fall into daily... ( $*$ ) "Shorter" compared to the father, not in absolute terms or relative to the average height! Thinking relative to the correct reference level is important here, and a point of confusion in itself — certainly not something all students get right.
