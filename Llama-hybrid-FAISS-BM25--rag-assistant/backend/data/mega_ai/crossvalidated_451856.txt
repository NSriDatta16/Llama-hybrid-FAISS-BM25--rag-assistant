[site]: crossvalidated
[post_id]: 451856
[parent_id]: 
[tags]: 
Any methods for sanity checks on refreshed machine learning models?

I am wondering if there are any best practices for validating ML models that are trained on new data. Apart from the validation metrics used on test data, are there any other recommended approaches which compare predictions from previously trained model outputs? For example, I have an outlier detection problem, and have streaming data coming in every month (my ML model is trained at a monthly cadence). I have ML models trained on Jan-2018, Feb-2018, up to Jan-2020. If I train a new model on Feb-2020 data, are there common methods used for sanity-checks by comparing model predictions of previous version to the latest one?
