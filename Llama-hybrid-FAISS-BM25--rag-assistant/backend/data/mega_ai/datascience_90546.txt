[site]: datascience
[post_id]: 90546
[parent_id]: 
[tags]: 
Finding the dual to an optimization problem on an unsupervised dataset

We consider the unsupervised dataset $x_1,..x_N \in R^d$ and the optimization problem: $$min_w \,\frac{1}{2}{\left\lVert w \right\rVert}^2,$$ subject to constraints: $$\forall_{i=1}^N: \phi(x_i)^Tw\geq1, \text{with} \,\phi: \mathbb R^d \rightarrow \mathbb R^h \,\text{is a nonlinear feature map}. $$ Here are my questions: What is the geometric interpretation of the optimization problem ? One can think of maximal separating hyperplane between the data point and the origin, right ? How one can derive the dual formulation of the optimization problem ? I guess one will have to maximize a certain quantity, but how one can find it and specify the constrains, I am not sure. I was told class 1 support vector machines (SVM) can be used, but I do not know how to do that. Can somebody provide some hint or a solution to the problem (in the best case by using first class 1 SVM) ? Many thanks.
