[site]: datascience
[post_id]: 100397
[parent_id]: 100392
[tags]: 
There is a general principle in linguistics and consequently in NLP: the meaning of a word is represented by the context of the word, i.e. the words around it. [edit] In NLP this principle is the basis of distributional semantics , which is used in every NLP application involving semantics (almost of them). This means that statistically the meaning of a word can be represented by a distribution of frequencies/probabilities over the vocabulary of all its possible context words. This principle is generalized to a full text: the meaning of the text is represented as the frequency distribution of the words it contains. Thus it's very meaningful to use the word frequency: it represents the "importance" of the word in the text and taking together the "importance" of all the words gives a representation of the meaning of the text.
