[site]: crossvalidated
[post_id]: 523497
[parent_id]: 523472
[tags]: 
I wouldn't treat online tutorials as a source of truth about the best practices. The point of the tutorials is to show how to do something. Usually, they do not offer a comprehensive, in-depth review of the available options, but rather show you how to use the available tools. Are you sure that the author of the tutorial claimed that $k$ -NN is the best of all the possible algorithms? It would be a ridiculous claim. First of all, did they check every other possible algorithm and every possible combination of hyperparameters? Likely not. But let's say that $k$ -NN achieves perfect accuracy, this still does not prove that there is no other algorithm that can achieve this result. Last but not least, the results would depend on many technical details: did you use the same versions of the packages as in the tutorial, exactly the same code, same procedures, hyperparameters, random seeds, etc? If not, results can differ because most of the machine learning algorithms are sensitive to such technical details. They can even differ between different implementations, for example, they can use different rules for tie-breaking, resulting in "the same" $k$ -NN algorithm giving different results. All this is amplified by the fact that Iris is a small, toy dataset, so even small discrepancies may have a significant impact on the overall result. Finally, there is a lot of bad and outdated tutorials. I cannot count the cases where the code I found in the online tutorials didn't work. If your results don't match the tutorial, double-check the code, but don't trust at face value something you've read on the Internet.
