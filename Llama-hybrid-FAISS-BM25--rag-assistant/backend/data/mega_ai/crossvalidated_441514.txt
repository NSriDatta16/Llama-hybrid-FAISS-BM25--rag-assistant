[site]: crossvalidated
[post_id]: 441514
[parent_id]: 
[tags]: 
Detecting anomalies in system logs

Chances are this may be closed off as too broad, but I'll try to be as specific as I can. I am currently working with API logs with categorical features separated by 1ms intervals, an example: {#id}, {dd-mm-yy}, {bool}, {feature_one}, {feature_two} . Now such logs have no underlying distribution or output class, therefore I cannot treat this as a supervised problem. Having categorical features where some may have >40 possible outcomes, in my opinion makes it difficult to use conventional encoding schemes. I've gone through a couple of ways to tackle this problem. OneHotEncode the features, reduce dimensionality using PCA or t-SNE and clustering the data using DBSCAN. FrequencyEncoding the features and running the same pipeline as above Taking every line from the log file, splitting it by feature, using Word2Vec and clustering the vector embeddings. Now I read a couple of papers, most notably ( https://qspace.library.queensu.ca/bitstream/handle/1974/1217/Memon_Ahmed_U_200805_MSC.pdf?sequence=1 ) and ( https://arxiv.org/pdf/1812.07136v1.pdf ) and I've been wondering if converting the logs of a 'normal' day's working to vector embeddings, feeding that to an autoencoder and using the reconstruction error from feeding anomalous logs as a metric for tagging anomalies could work. Am I missing something obvious?
