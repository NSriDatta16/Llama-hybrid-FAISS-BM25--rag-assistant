[site]: datascience
[post_id]: 27137
[parent_id]: 27132
[tags]: 
The metrics you calculate are of two types, metrics that depict the entire prediction model you have built like accuracy which will be same in both the cases of your pseudo code. While the others like precision says how precise are you in explaining particular class of interest ( accuracy can also be expressed this way in multi-class classification, see the diagram). This score depends on which class you had selected as a positive one. If you put positive class as face of your model, then it is called ppv or precision and npv , if vice versa. Coming to the multi-class classification, the core definition holds the same. Now the matrix will be n x n ( n being number of classes). The sample matrix looks likes this. The diagonal elements explains the number of 1 class 's predicted as 1 . Now there are n precision values for each class. Precision for class 1 is how many values were truly predicted as 1 divided by how many are predicted as 1 (FP's included), which is the sum of the first column. Not but not the least if you adamantly want a precision like metrics for the entire model, you got micro & macro averaging methods, which are helpful in giving a combined metric. This blog post explains it pretty well. Hope this clears something.
