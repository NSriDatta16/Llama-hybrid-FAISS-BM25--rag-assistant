[site]: stackoverflow
[post_id]: 1176499
[parent_id]: 1176444
[tags]: 
There is more to this than meets the eye...which I am thinking that you already knew! What sort of files are you talking about? If they are anything even remotely big or in such quantity that the group of files could be big I would immediately suggest that you add some flexibility to your approach. create a table that stores the root paths to various file stores (this could be drives, unc paths, what ever your environment supports). It will initially have one entry in it which will be your first storage location. An nice attribute to maintain with this data is how much room can be stored here. maintain a table of file related data (id {guid}, create date, foreign key to path data, file size) write the file to a root that still has room on it (query all file sizes stored in a root location and compare to that roots capacity) write the file using a GUID for the name (obfuscates the file on the file system)..can be written without the file extension if security requires it (sensitive files) write the file according to its create date starting from the root/year{number}/month{number}/day{number}/file.extension With a system of this nature in place - even though you won't/don't need it up front - you can now more easily relocate the files. You can better manage the files. You can better manage collections of files. Etc. I have used this system before and found it to be quite flexible. Dealing with files that are stored to a file system but managed from a database can get a bit out of control once the file store becomes so large and things need to get moved around a bit. Also, at least in the case of windows...storing zillions of files in one directory is usually not a good idea (the reason for breaking things up by their create date). This complexity is only really needed when you have high volumes and large foot prints.
