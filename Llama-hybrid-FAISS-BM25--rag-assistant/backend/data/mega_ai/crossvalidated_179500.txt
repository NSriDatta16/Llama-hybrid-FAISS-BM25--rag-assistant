[site]: crossvalidated
[post_id]: 179500
[parent_id]: 178870
[tags]: 
There is a whole field called nonparametric statistics that avoids the use of strong models. However, your concern about fitting models, per se, is valid. Unfortunately there is no mechanical procedure to fitting models that would be universally accepted as "optimal". For example, if you want to define the model that maximizes the likelihood of your data, then you will be led to the empirical distribution function. However, we usually have some background assumptions and constraints, such as continuous with finite first and second moments. For cases like these, one approach is to choose a measure like Shannon Differential Entropy and maximize it over the space of continuous distributions that satisfy your boundary constraints. What I'd like to point out is that if you don't just want to default to the ECDF, then you'll need to add assumptions, beyond the data, to get there, and that requires subject matter expertise, and, yes, the dreaded..... professional judgement So, is there a guaranteed stopping point to modeling...the answer is no. Is there a good enough place to stop? Generally, yes, but that point will depend on more than just the data and some statistical desiderata, you're usually going to take into account the risks of different errors, the technical limitations to implementing the models, and the robustness of its estimates, etc. As @Luca pointed out, you can always average over a class of models, but, as you rightly pointed out, that will just push the question up to the next level of hyperparameters. Unfortunately, we seem to live within an infinitely layered onion...in both directions!
