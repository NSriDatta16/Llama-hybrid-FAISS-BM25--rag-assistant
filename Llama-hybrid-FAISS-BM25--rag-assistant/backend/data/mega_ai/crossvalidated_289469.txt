[site]: crossvalidated
[post_id]: 289469
[parent_id]: 
[tags]: 
Why would the sampled softmax work? [word2vec]

I'm following Udacity Deep learning course and the instructor briefly explained about the "Sampled Softmax" used in word2vec. In tensorflow word2vec_basic.py , the implementation is like below. for j in range(num_skips): while target in targets_to_avoid: target = random.randint(0, span - 1) # randomly choose a word from the context of target word targets_to_avoid.append(target) I'm having hard time understanding why randomly remove some words would work well even if it reduces the computaion. Can someone shed some light on it?
