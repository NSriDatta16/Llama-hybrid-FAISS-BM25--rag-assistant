[site]: crossvalidated
[post_id]: 314161
[parent_id]: 313996
[tags]: 
they usually only seem to show how the dot product give you a similarity score between two data points not how it partitions a set of data into two categories. Linear SVM partitions space using a hyperplane. Hyperplane can be represented as $$H = \{x \in \mathbb{R}^k | x \cdot n = b \}$$ for some $n \in \mathbb{R}^k $. $H$ partitions $\mathbb{R}^k$ according to the sign of $x \cdot n - b$. But I don't see an explanation about how this product can be used to determine similarity between the two points You can calculate Euclidean distance between two points using dot products: $$\|x - y\|^2 = (x - y)\cdot (x-y) = x \cdot x - 2 x \cdot y + y \cdot y$$ Kernel trick lets you to separate data which is not linearly separable in some other space - you just need the kernel to actually fit SVM, since its training objective is expressed solely using dot products (and not explicitly using coordinates from the space into which you embed your data).
