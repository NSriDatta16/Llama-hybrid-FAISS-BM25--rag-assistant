[site]: crossvalidated
[post_id]: 432805
[parent_id]: 
[tags]: 
What is the Cost Function for Neural Network with Dropout Regularisation?

For some context, I shall outline my current understanding: Considering a Neural Network, for a Binary Classification problem, the Cross-entropy cost function, J, is defined as: $ J = \frac{-1}{m} \sum_{i=1}^m y^i*log(a^i) + (1-y^i)*log(1-a^i) $ m = number of training examples y = class label (0 or 1) a = output prediction (value between 0 and 1) Dropout regularisation works as follows: For a given training example, we randomly shut down some nodes in a layer according to some probability. This has the effect of keeping the weights low during training and hence regularises the network and prevents overfitting. I have learnt that if we do apply dropout regularisation, the cross entropy cost function is no longer easy to define due to all the intermediate probabilities. Why is this the case? Why doesn't the old definition still hold? As long as the network learns better parameters, won't the cross entropy cost decrease on every iteration of Gradient Descent? Thanks in advance.
