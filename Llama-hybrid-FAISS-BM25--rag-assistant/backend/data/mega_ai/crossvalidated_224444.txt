[site]: crossvalidated
[post_id]: 224444
[parent_id]: 
[tags]: 
Making the model explicit in Bayes' rule

I'm reading a book about Bayesian statistics and at some stage it explain the Bayes' rule as follow: $$p(\theta|D) = \frac{p(D|\theta)\,p(\theta)}{p(D)}$$ Where $\theta$ is the model parameter and D is the data. Until here I can understand. Then it says "We talk about parameter values $\theta$ only in the context of a particular model that gives the meaning to the parameter. In some application it can help to make the model explicit in Bayes' rule. Let's call the model $M$. Then, because all probabilities are defined given that model, we can rewrite Eq 4.4 as:" $$p(\theta|D,M) = \frac{p(D|\theta, M)\,p(\theta|M)}{p(D|M)}$$ Until now my understanding is that $P(A,B)$ is the co-joint probability of two events happening at the same time, so I intuitively interpret $p(\theta|D,M)$ as the probability of $\theta$ given $D$ in the context of the model $M$. The same applies for $p(D|\theta,M)$. I'm lost when it multiplies that value by $p(\theta|M)$, where does it come from? I would expect that to be $p(\theta,M)$ (i.e.: $\theta$ in the context of model $M$). It is very likely that my intuitively interpretation is wrong, could you please help me understand how the first equation connects to the second?
