[site]: crossvalidated
[post_id]: 172903
[parent_id]: 
[tags]: 
Predictive uncertainty vs test input magnitude

I am reading the chapter on regression in the Gaussian Process for machine learning book. The author introduces Bayesian linear regression and he describes that the distribution is normally distributed with some mean and variance. He makes a statement like: "The predictive variance is a quadratic form of the test input with the posterior covariance matrix, showing that the predictive uncertainties grow with the magnitude of the test input, as one would expect for a linear model." This is related to eq (2.9) in the link. This bit has me confused. Why should the posterior uncertainty be a function of the magnitude of the test input points? The test input points are where we want to make the predictions and if we have had some training data around these points, I can see that the uncertainty will be small but do not see what the relation with the magnitude of this input point is...
