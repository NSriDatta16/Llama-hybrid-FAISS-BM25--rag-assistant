[site]: crossvalidated
[post_id]: 570906
[parent_id]: 
[tags]: 
PCA Minimizing Reconstruction Error

In PCA, we want to fit the best $k$ -dimensional subspace to given data $x_1,\dots,x_n\in\mathbb{R}^d$ . Specifically, we want to approximate each $x_i$ by $\mu + V_k\alpha_i$ , where $\mu \in \mathbb{R}^d $ and $\alpha_i \in\mathbb{R}^k$ are parameters we wish to estimate for $i=1,\dots,n$ . $V_k$ is a given, fixed orthogonal rank $k$ matrix in $\mathbb{R}^{d\times k}$ . An objective function for this problem is minimize the loss $L(\mu,\{\alpha_i\}) = \sum_{i=1}^n||X_i-\mu-V_k\alpha_i||^2$ , and I am given one possible optimum is $\hat{\mu}=\bar{x}$ and $\hat{\alpha_i}=V_k^t(x_i - \bar{x})$ . I'm trying to find this optimum and understand why it's only one of multiple possibilities. So far I've been able to calculate $\frac{\partial L}{\partial \mu}= -2\sum_ix_i+2n\mu+2\sum_iV_k\alpha_i \implies\mu=\frac{1}{n}\sum_i(x_i-V_k\alpha_i)$ , and $\frac{\partial L}{\partial \alpha_i}=-2V_k^tx_i+2V_k^t\mu+2\alpha_i \implies \alpha_i = V_k^t(x_i-\mu)$ . Here I'm a bit stuck as to how to proceed, since these solutions depend on each other. However, I do notice that $\mu=\frac{1}{n}\sum_ix_i = \bar{x}$ IF $V_k\alpha_i=0$ , so perhaps under the constraint $V_k\alpha_i=0$ for all $i$ we get the desired optimum. Perhaps this is why $\hat{\mu} = \bar{x}$ is not unique. Thanks for any help and thoughts with these questions.
