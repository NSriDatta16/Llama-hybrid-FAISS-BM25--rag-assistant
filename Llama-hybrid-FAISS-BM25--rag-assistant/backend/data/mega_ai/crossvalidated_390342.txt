[site]: crossvalidated
[post_id]: 390342
[parent_id]: 
[tags]: 
Why is the risk equal to the empirical risk when taking the expectation over the samples?

From Understanding Machine Learning: From theory to algorithms: Let $S$ be a set of $m$ samples from a set $Z$ and $w^*$ be an arbitrary vector. Then $\Bbb E_{S \text{ ~ } D^m}[L_S(w^*)] = L_D(w^*)$ . Where: $L_S(w^*) \equiv \frac{1}{m}\sum_{i=1}^ml(w^*, z_i)$ and $z_i \in S$ , $L_D(w^*) \equiv \Bbb E_{z \text{ ~ }D}[l(w^*, z)]$ , $D$ is a distribution on $Z$ , and $l(\text{_},\text{_} )$ is a loss function. I see that $$\Bbb E_S[L_S(w^*)] = \Bbb E_S[\frac{1}{m}\sum_{i=1}^ml(w^*, z_i)] = \frac{1}{m}\sum_{i=1}^m \Bbb E_S[l(w^*, z_i)]$$ and $$L_D(w^*) = \Bbb E_z[l(w^*, z)] = \sum_{z \in Z} l(w^*, z)D(z)$$ But how are these two equal? $\Bbb E_S$ is an expectation over samples $S$ of size $m$ whereas $\Bbb E_z$ is an expectation over all samples in $Z$ .
