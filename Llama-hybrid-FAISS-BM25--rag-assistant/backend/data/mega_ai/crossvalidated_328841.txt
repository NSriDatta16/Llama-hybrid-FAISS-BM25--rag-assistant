[site]: crossvalidated
[post_id]: 328841
[parent_id]: 328795
[tags]: 
Tree depth determines how flexible the model is. A deeper tree can fit more complicated functions. Therefore, increasing tree depth should increase performance on the training set. But, increased flexibility also gives greater ability to overfit the data, and generalization performance may suffer if depth is increased too far (i.e. test set performance may decrease). Another way of saying this is that increasing depth decreases bias at the expense of increasing variance. Random forests can combat this increase in variance by averaging over multiple trees, but are not immune to overfitting. Getting the best generalization performance typically requires tuning the tree depth to achieve a proper balance between bias and variance (e.g. by optimizing the out-of-bag error).
