[site]: crossvalidated
[post_id]: 396075
[parent_id]: 
[tags]: 
How bias arises in using maximum likelihood to determine the variance of a Gaussian?

While reading Christopher M. Bishop Pattern Recognition and Machine Learning , I ran into the following explanation for why there is an error in estimating the variance of Gaussian distribution using Maximum Likelihood. On page 48 , Illustration of how bias arises in using maximum likelihood to determine the variance of a Gaussian. The green curve shows the true Gaussian distribution from which data is generated, and the three red curves show the Gaussian distributions obtained by fitting to three data sets, each consisting of two data points shown in blue, using the maximum likelihood results (1.55) and (1.56). Averaged across the three data sets, the mean is correct, but the variance is systematically under-estimated because it is measured relative to the sample mean and not relative to the true mean. I'm pasting figure 1.15 and the required equations (1.55, 1.56) here for convenience. $\mu_{ML} = \frac{1}{N}\sum_{n=1}^{N}x_n$ $\sigma_{ML}^2 = \frac{1}{N}\sum_{n=1}^{N}(x_n - \mu_{ML})^2$ I can imagine how maximum likelihood estimation would look pictorially. However, I'm not able to understand the figure or the explanation . I would appreciate if someone could label the figure or explain what points are considered in each part of the figure and why the curve could look like that in each case. I looked up exercise 1.12 , and I'm able to understand why there is a factor of $\frac{N-1}{N}$ between the true $\sigma^2$ and $\sigma_{MLE}^2$ . ${\rm I\!E}[\sigma_{MLE}^2]= (\frac{N-1}{N})\sigma^2 $
