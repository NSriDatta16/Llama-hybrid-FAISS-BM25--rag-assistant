[site]: crossvalidated
[post_id]: 451868
[parent_id]: 
[tags]: 
Calculating the value of $b^{*}$ in an SVM

In Andrew Ng's notes on SVMs , he claims that once we solve the dual problem and get $\alpha^*$ we can calculate $w^*$ and consequently calculate $b^*$ from the primal to get equation (11) (see notes) $$b^* = -\frac{\max_{i:y^{(i)} = -1}{w^*}^Tx^{(i)} + \min_{i:y^{(i)} = 1}{w^*}^Tx^{(i)}}{2}$$ I am not sure how this was derived from the primal. The generalized lagrangian is (see equation 8) $$\mathcal{L}(w, b, \alpha) = \frac{1}{2}w^Tw - \sum_{i=1}^m\alpha_i\left[y^{(i)}\left(w^Tx^{(i)} + b\right) - 1\right]$$ and the primal is, by definition, $$\theta_{\mathcal{P}}(w, b) = \max_{\alpha\geq0} \mathcal{L}(w, b, \alpha)$$ To find $b^*$ we must the optimal solution of $$\min_{w, b}\theta_{\mathcal{P}}(w, b)$$ Since we know $w^*$ we can write this as $$\min_{w, b}\theta_{\mathcal{P}}(w, b) = \min_{b}\theta_{\mathcal{P}}(w^*, b)\tag{$*$}$$ Further, note that $\theta_{\mathcal{P}}(w^*, b) = \infty$ if for any $i$ , $y^{(i)}\left({w^*}^Tx^{(i)} + b\right) . Otherwise, $\theta_{\mathcal{P}}(w^*, b) = \frac{1}{2}{w^*}^T{w^*}$ . Hence, the solution to $(*)$ must be $$\min_{b}\theta_{\mathcal{P}}(w^*, b) = \frac{1}{2}{w^*}^Tw^*$$ and the optimal solution $b^*$ must be such that $y^{(i)}\left({w^*}^Tx^{(i)} + b^*\right) \geq 1$ for each $i$ . This only gives a range of values of $b^*$ and not a particular value. How do I mathematically get to equation (11)? More generally, how can I get $b^*$ for the soft-margin classifier?
