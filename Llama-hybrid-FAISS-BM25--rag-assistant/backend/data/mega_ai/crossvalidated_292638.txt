[site]: crossvalidated
[post_id]: 292638
[parent_id]: 292575
[tags]: 
In my experience, CHAID often gives results that are hard to understand. However, it is worth remembering that the natural greediness and tendency towards overfitting of single decision trees is especially pronounced in CHAID, and that as a result, where there are cross correlations the first variable in the split will often effectively steal explanatory power from subsequent variables. Hence for example, if you have an age variable and a gender variable, and those two variables are not independent, some of the relationship between the dependent variable and the second variable will appear under the guise of the more influential variable, and as a result, the relationship between the second variable and the dependent variable won't be what you expect. Hence, in your example, one possible explanation is that there is one or more stronger variables in your model other than your Likert scales, and the Likert scale is itself partially dependent on those variables, so that you are not really modelling the effect of the weaker variables, but their effect conditional on the stronger variables, which may be very different to what you expect. Another possibility is that the relationship is not actually a straight line - for example, people with polarised opinions are more alike each other than people so the relationship between the likert scale and the dependent variable resemebles a parabola. In that case, the possibility exists that CHAID is actually doing a reasonable job of modelling what is really happening. There seem to be a few paths forward. Firstly, you could use another algorithm. If you have some good reasons to prefer a single decision over, for example, a Random Forest or a GLM, the paper by Loh,'Fifty Years of Classification and Regression Trees" (2014) - http://www.stat.wisc.edu/~loh/treeprogs/guide/LohISI14.pdf- compares a number of different tree algorithms, the differences in how they work and what they're good at. To address the first possible problem mentioned above, it might be useful to use something like a scatter plot matrix to better understand cross-correlations. Assuming there are cross-correlations, in the past, where I have had a CHAID model produce difficult to understand results due to the 'winner-takes-all' effect, I have had some success at reducing it by adding white noise to the most influential variables, which reduces their importance, and makes the less influential variables behave more intuitively. In respect of the second problem, creating models which highlight the non-linear aspects of your data may be useful. A generalised additive model is often useful here. Alternatively, you could create a decision tree model with binary splits. This will make it more likely that responses at 4 & 6 appear in different nodes even if they actually result in a similar output. Hence, this approach may help validate whether their appearing on the same node under CHAID matches the underlying data.
