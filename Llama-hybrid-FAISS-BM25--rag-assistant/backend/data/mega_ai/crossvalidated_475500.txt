[site]: crossvalidated
[post_id]: 475500
[parent_id]: 469799
[tags]: 
The asymptotic nature refers to the logistic curve itself. The optimizer, if not regularized, will enlarge the weights of the logistic regression to put $wx$ as far as possible to the left or right per sample to reduce the loss maximally. Lets assume one feature that provides perfect separation, one can imagine $wx$ getting larger and larger on each iteration. Optimization will fail in this case, that is unless the solution is regularized. $$\frac{1}{1 + e^{wx}}$$ A high dimensional model creates a large hypothesis space for the possible set of parameters. The optimizer will capitalize on that by choosing the solution with the highest weights. Higher weights will reduce the loss, which is the task of the optimizer, steepen the logistic curve, and give a higher conditional likelihood of the data. The model is overconfident, a paraphrase for overfitting in this setting. If there are several parameter configurations that have the same binary performance measure, the optimizer will always choose the configuration with the lowest loss. Due to the asymptotic nature of the logistic curve, the loss function can be reduced beyond the information provided by the binary labels. More pragmatic, regularization, which makes the coefficients smaller, can help to reduce overfitting. A more formal explanation of the relationship between unconstrained weights, regularization and overfitting can be found using Bayesian theory.
