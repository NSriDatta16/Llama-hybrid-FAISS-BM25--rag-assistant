[site]: crossvalidated
[post_id]: 279137
[parent_id]: 279000
[tags]: 
The phrase from the book doesn't sound right to me, but maybe I don't have the whole context. Let's assume that we split your whole data set into two: train and test (for example 70%, 30%). If we apply a cross validation scheme to the training set (not touching test set) by splitting it into several sections (folds), and run the machine learning algorithm with the intention to find the values of hyperparameters that produce the lowest training error, we of course end up with a model (meaning that we also find the values of the parameters of the model). If we think for a moment why we are doing cross validation, we see that the reason is to reduce the number of freely adjusting parameters. That's why the number of hyperparameters are almost always lower than the number of model parameters. We do this because the number of free parameters increases the bias (very optimistic) in estimation of the expected training error. Having said that, the cross validation still has freely varying hyperparameters. Therefore the bias is reduced but not completely eliminated. For this reason when you actually run the model emitted by the cross validation on the test set (30% part) the performance will still be worse "on the average" compared to the training. That's why it is a good idea to still keep a test set aside even if you are building your model with cross validation. The error of the model on the test set is the most honest estimate of expected error.
