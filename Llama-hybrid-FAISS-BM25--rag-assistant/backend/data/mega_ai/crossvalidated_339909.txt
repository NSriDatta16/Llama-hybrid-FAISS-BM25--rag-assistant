[site]: crossvalidated
[post_id]: 339909
[parent_id]: 339897
[tags]: 
Loss In machine learning applications, such as neural networks, the loss function is used to assess the goodness of fit of a model. For instance, consider a simple neural net with one neuron and linear (identity) activation that has one input $x$ and one output $y$: $$y=b+wx$$ We train this NN on the sample dataset: $(x_i,y_i)$ with $i=1,\dots,n$ observations. The training is trying different values of parameters $b,w$ and checking how good is the fit using the loss function. Suppose, that we want to use the quadratic cost : $$C(e)=e^2$$ Then we have the following loss : $$Loss(b,w|x,y)=\frac 1 n \sum_{i=1}^n C(y_i-b-wx_i)$$ Learning means minimizing this loss: $$\min_{b,w} Loss(b,w|x,y)$$ MLE connection You can pick the loss function which ever way you want, or fits your problem. However, sometimes the loss function choice follows the MLE approach to your problem. For instance, the quadratic cost and the above loss function are natural choices if you deal with Gaussian linear regression. Here's how. Suppose that somehow you know that the true model is $$y=b+wx+\varepsilon$$ with $\varepsilon\sim\mathcal N(0,\sigma^2)$ - random Gaussian error with a constant variance. If this is truly the case then it happens so that the MLE of the parameters $b,w$ is the same as the optimal solution using the above NN with quadratic cost (loss). Note, that in NN you're not obliged to always pick cost (loss) function that matches some kind of MLE approach. Also, although I described this approach using the neural networks, it applies to other statistical learning techniques in machine learning and beyond.
