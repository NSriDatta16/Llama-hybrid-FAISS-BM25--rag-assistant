[site]: crossvalidated
[post_id]: 381823
[parent_id]: 341027
[tags]: 
You are correct that experience replay (ER) makes the implementation of eligibility traces impossible. States are not processed in the order they are visited, which is a requirement for the backward view of TD(位) . Yet it would be nice to somehow achieve the benefits of eligibility traces in sparse-reward environments when using DQN, as you suggest. This problem has been the primary focus of my own research. In my paper Reconciling 位-Returns with Experience Replay , I propose a solution that uses offline 位-return calculation (aka the forward view ) to emulate eligibility traces. I describe a procedure that allows this to be done efficiently for DQN while retaining ER. I found that learning speed could be increased compared to n -step returns when playing Atari games, and I expect these results to generalize to other domains. My code can be found here . You can also read this paper for another approach to rectifying eligibility traces with Deep Q-learning. However, its major limitations are that it is compatible only with Deep Recurrent Q-Networks (DRQN) and that the 位-return calculation must be truncated to the length of the RNN training sequence. Finally, it is important to note that there are other benefits to ER than just decorrelating the training experience. For example, ER can greatly improve sample efficiency, which is explored in this paper . However, ER cannot be called an "accepted standard." Many policy gradient methods like TRPO, PPO, and ACKTR do not use ER. In the case of DQN, ER was primarily a design decision to help prevent overfitting when estimating Q-values and yield superior empirical performance.
