[site]: crossvalidated
[post_id]: 462720
[parent_id]: 429015
[tags]: 
If you fit a random forest composed $B$ trees to your dataset, the OOB error on sample $z_j$ is computed using only those trees that did not have $z_j$ included in their bootstrapped training set. So for a given loss function $L$ the OOBE is actually: $\epsilon^{OOB} = \frac{1}{N^{OOB}} \sum_{j=1}^{N} L(y_j, \frac{1}{|B_j|}\sum_{b \in B_j}{T(x_j | \theta_b, Z_b)} )$ . where $T( |\theta_b,Z_b)$ represents simply your b-th grown tree with parameters $\theta_b$ trained on the bootstrapped training set $Z_b$ . $B_j = \{b=1,2 .. B | z_j \notin Z_b\}$ is the set of all the trees that did not contain $z_j$ in their training set. And finally $N^{OOB}$ is the number of samples that do not belong to, at least, one of the bootstrapped training sets $Z_b$ $b=1,2...B$ . Meanwhile for k-fold CV: $\epsilon^{CV} = \frac{1}{N} \sum_{j=1}^{N} L(y_j, \frac{1}{B}\sum_{b=1}^B{T(x_j | \theta_b^{-k(j)}, Z_b^{-k(j)})} )$ where $k(j)$ is the fold which does not contain sample $z_j$ , $\theta_b^{-k(j)}$ are the parameters of the b-th tre at the $k(j)$ round of CV, and $Z_b^{-k(j)}$ is a bootstrap sample of the training set with the $k(j)$ fold excluded and So meanwhile in CV the ensemble of all $B$ trees is tested against the hold-out fold, with OOB an ensemble of a subset of $|B_j|$ trees is considered for each sample $z_j$ . Even though each OOB sample is evaluated on a minor number of trees $B_j , the training data this sub-ensemble collectively sees is actually more than what all the $B$ trees collectively see in CV since in this case every tree is trained on bootstrapped dataset of only $k-1$ folds of the original training set. In mathematical form: $|\bigcup_{b=1}^{B} Z_b^{-k(j)}| This implies that the OOBE estimate of the error should be less biased with respect of CV, however the fact that we are bagging less trees which collectively see overlapping training sets may cause the variance to be larger. It can actually be proved that for $B \to \infty$ the OOBE converges to the k-fold CV estimate with $k=n$ [1] which is an unbiased estimator of the error, but with usually higher variance than CV with $k . The advantage of OOBE is computational: basically you can train and get a hold out error estimate (almost equivalent to n-fold CV) at the same time, avoiding all the burden of re-training your random forest for every CV fold. [1] Hastie, T., Tibshirani, R., Friedman, J. (2001). The Elements of Statistical Learning.
