[site]: crossvalidated
[post_id]: 554098
[parent_id]: 553871
[tags]: 
Regarding exponential smoothing yielding autocorrelated residuals, it depends on how strong the autocorrelation is (magnitude, not only statistical significance). If it is strong, consider using another model or modelling the residuals and adjusting your forecasts accordingly. Thank you very much dear Mr. Hardy. I managed to fix this by playing with $p$ , $d$ and $q$ parameters. I would like to ask would it be weird if I could use a model like Arima(ts_d2, order = c(20, 2, 2), lambda = 0.3) for example with a $p$ of 20 so that all residuals become independent with $p$ -value also greater than 0.05 ? There is the bias-variance trade-off. If you have a complex model like yours, its bias may be low but variance will be high, unless you time series is very long. That will have a negative impact on forecast accuracy. Consider using regularized estimation or setting some coefficients to zero to mitigate that. Consider using auto.arima for model selection, as that model selection algorithm is tuned specifically with forecast accuracy in mind. I used auto.arima to get an ARIMA(0,1,1) and my $p$ -value is now 0.4505 and AICc is around 61.05 with only one lag outside the blue dotted lines in ACF diagram. Based on what you mentioned above so with this new $p$ -value if we have only one or two lags outside the blue dotted lines, does it mean that our model fitted the data quite well? which one actually defines the situation better? The new model with its diagnostics sounds quite good to me. If the lags that stick out are distant lags (not the first few) and/or they stick out only a little bit, then they are probably harmless. Note that under the null hypothesis of no autocorrelation, on average 1 out of 20 lags would stick out of the confidence bounds by pure chance. Is it really essential to partition the data to train and test for ARIMA models? Because in this case I didn't do that. In a time series setting, this is quite costly, as you generally cannot do leave-one-out or $k$ -fold cross validation like you could in a cross section. But for a fair evaluation of predictive performance, splitting the data into training and test subsamples is hard to beat. If you were not doing any model selection, AIC would be a fair estimate of the model's expected loss on new data in terms of $-2n\ \times$ the log-likelihood. But when doing model selection, the selected model suffers from the winner's curse, so the AIC of the winner is overly optimistic. Thus better consider using a test subset to evaluate your model.
