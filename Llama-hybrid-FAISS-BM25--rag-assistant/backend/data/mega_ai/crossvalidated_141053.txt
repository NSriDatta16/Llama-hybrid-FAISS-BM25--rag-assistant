[site]: crossvalidated
[post_id]: 141053
[parent_id]: 94316
[tags]: 
How much you can conclude depends on the assumptions you are willing to make about the underlying distribution. Even with no assumptions, though, you can still conclude something, although it might not be a whole lot. For example, the sample data are not plausibly consistent with an underlying distribution whose mean is $100$ and standard deviation is $10$: Chebyshev's inequality tells us these data must have extremely low likelihood in that case. The most general answer will be given in the form of a p-box . This is a simple device to describe a range of distribution functions. It consists of two graphs, one lying above the other, which thereby determine a region $\mathcal P$ in Cartesian coordinates. Any CDF whose graph would lie entirely within $\mathcal P$ is an element of that p-box. As an example, let's construct a (nonparametric) 90%-confidence p-box for the sample data, $(\lt 5, \lt 5, \lt 5, \lt 10, \lt 10, \lt 10)$. This p-box should be as "small as possible" (in some sense) while containing any distribution law $F$ for which the probability of the data, given $F$, is at least $1 - 90/100 = 1/10$. Let us (temporarily) call these the "plausible laws." "As small as possible" can be rigorously defined. Let $\mathbb{P}$ be the set of all distribution functions. Associated with any subset $\Omega\subset \mathbb{P}$ is the set $C(\Omega)$ consisting of all points $(x,y)$ such that $x$ is a real number and there exist $F,G\in \Omega$ with $F(x) \le y \le G(x)$. Letting $\Omega$ be the set of plausible laws, we take the p-box to be $C(\Omega)$. These data, which have no quantified results at all, tell us just two things about the plausible laws. When a sample $X_1, X_2, \ldots, X_6$ is taken from such a law $F$, then (1) it should be unlikely that one or more of the $X_i$ equal $10$ or greater and (2) it should be unlikely that four or more of the $X_i$ should equal $5$ or greater. These are binomial events governed by the parameters $p=F(10)$ and $q=F(5)$, respectively. Interpreting "unlikely" as being a chance of $100-90 = 10\%$ or less, these criteria uniquely determine $p$ and $q$. For these data, $p = (1/10)^{1/6} \approx 0.681$ and $q \approx 0.201$. The p-box therefore contains the graphs of all distribution laws $F$ for which (1) $q \le F(5) \le 1$ and (2) $p \le F(10) \le 1$. A part of this p-box (between $x=0$ and $x=15$) is shaded in gray: Within this p-box I have drawn the graphs of some distributions that come close to its boundary. All three are two-parameter Exponential distributions. The dark red one is determined by the two corners it passes through. The other two (dashed gray ones) show a distribution with a large standard deviation and one with a small standard deviation. These illustrations should make it clear that if you wished to construct a parametric p-box encompassing only two-parameter Exponential distributions, then it would exclude the region between the thick red curve and the lower line segment between $x=5$ and $x=10$, but it would have to include everything else. We don't necessarily achieve a lot by making a strong parametric assumption in this example. What happens when the amount of data increases? Suppose as you collect more data that all results continue to be either $\lt 5$ or $\lt 10$ and that their proportions remain roughly 1:1. Then the value of $p$ approaches $1.0$--there can't be much of a chance of observing anything greater than $10$--and the value of $q$ approaches $1/2$, the proportion of $\lt 5$s in the population. Making an Exponential distribution assumption allows you to conclude a little more: the chance of observing anything above $5$ grows vanishingly small, too. Here is the situation upon observing $3000$ values of "$\lt 5$" and $3000$ values of "$\lt 10$": Nevertheless, there is a huge set of possible distributions consistent with such data, even when the dataset becomes enormous. Intuitively this should be clear: for example, any distribution supported on a set of numbers less than $5$ will be consistent with such data and its graph will fit within this p-box. This approach to data analysis is not as powerful as, say, a Bayesian analysis. However, it requires minimal assumptions: we only suppose that the data are identically and independently distributed. There is no need to adopt a prior and justify your choice of it, for instance. (Such checking is rather difficult when all the data are censored!) If the p-box is sufficiently narrow to let you draw a firm conclusion, then you're done and you know your conclusion is extremely robust. Otherwise, the p-box can be a useful tool to help you explore the impact of potential additional assumptions on your conclusions. Here is some R code to play with. k1
