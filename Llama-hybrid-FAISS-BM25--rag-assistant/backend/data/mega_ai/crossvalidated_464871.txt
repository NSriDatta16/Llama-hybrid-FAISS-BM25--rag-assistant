[site]: crossvalidated
[post_id]: 464871
[parent_id]: 
[tags]: 
Multiple Regression - Normal equations and features normalization (whitening)

In the multivariate regression analysis, it is easy and natural to conclude that the coefficients of the regression are given by the so-called normal equation $\hat{\beta}=(X^TX)^{-1}X^T y$ My doubt is related to the role of the term $(X^TX)^{-1}$ . On Flach's Machine learning book , it is stated that $(X^TX)^{−1}$ acts as a transformation that decorrelates, centres and normalises the features , and I quote: Let us try to understand the term $(X^TX)^{−1}$ a bit better. Assume that the features are uncorrelated (meaning the covariance between every pair of different features is 0) in addition to being zero-centred. ... the covariance matrix $\Sigma$ is diagonal with entries $\sigma_{jj}$ . Since $(X^TX)= n(\Sigma+M)$ , and since the entries of $M$ are 0 because the columns of $X$ are zero-centred, this matrix is also diagonal with entries $n\sigma_{jj}$ – in fact, it is the matrix S referred to above. In other words, assuming zero-centred and uncorrelated features, $(X^TX)^{−1}$ reduces to our scaling matrix $S^{−1}$ .In the general case we cannot make any assumptions about the features, and $(X^TX)^{−1}$ acts as a transformation that decorrelates, centres and normalises the features . I am aware that to decorrelate and normalize a data set is known as whitening. A whitening matrix $W$ is such that $Z=WX$ decorrelates $X$ ; i.e., even if $X$ is correlated, the covariance of $Z$ will be diagonal. Usually, $W$ is determined via the eigen-decomposition of $\Sigma$ or the Cholesky decomposition of $\Sigma ^{-1}$ , among other procedures, but nothing like $(X^TX)^{−1}$ (not that I am aware of). Intrigued with what, I ran some simulations in Matlab where some random (and correlated) multivariate matrices were transformed by using the transformation $W_{Flach}=(X^TX)^{−1}$ and also $W_{Flach}=\Sigma^{-1}_X$ (the latter corresponds to the "cov" function in Matlab, which returns the covariance matrix of a matrix of data). It didn't work in either way: $Z=W_{Flach}X$ was surely transformed, but remained correlated. I also tried the ZCA whitening (sometimes called Mahalanobis whitening , here ), which uses the transformation $W_{ZCA}=\Sigma_x^{\frac{-1}{2}}$ in my simulations and, not surprisingly, it worked as expected: $Z=W_{ZCA}X$ becomes uncorrelated. Finally, it is also clear that $W_{Flach}$ does not comply with the definition of a whitening transformation - if $W$ is a whitener, then $W^T W=\Sigma^{-1}$ . Well, whereas $W_{ZCA}^T W_{ZCA}$ is identical to $\Sigma^{-1}$ , $W_{Flach}^T W_{Flach}$ is obviously not. So, it is crystal clear that $W_{Flach}$ can not be a whitening transformation. And that is driving me crazy: as far as I know, to state that $W_{Flach}$ " decorrelates the features " is plainly wrong - if it was, it would decorrelate $X$ , right? So, why on Earth Flach says that in his book? One point caught my attention. Later on in his book, Flach defines the Mahalanobis distance as $Dis_M=(x,y|\Sigma)=\sqrt{(x-y)^T\Sigma^{-1}(x-y)}$ and states that using the covariance matrix in this way has the effect of decorrelating and normalising the features, as we saw in Section 7.1 ("Section 7.1" is the quotation I made at the beginning). Now, the Mahalanobis distance is applied in a different context: it takes the difference between two vectors, calculates the weighted product of this difference by itself and then takes the square root of the result; i.e., computes a normalized distance (the weighting factor is $(X^T X)^{-1} X$ ). While that is certainly a normalized measure, it is not the same as whitening $X$ . Computing $\sqrt{((x-y)^T\Sigma^{-1}(x-y))}$ sounds pretty different from taking the product of $(X^T X)^{-1}$ by $X$ . For a start, Flach doesn't assert that $X$ is centered, so $(X^T X)^{-1} X$ is not $N\Sigma^{-1}$ . Peter Flach is a renowned author and his book is well accepted in the academia, so probably I am missing something. I refuse to believe that Flach confused $\Sigma^{-1}$ for $\Sigma^\frac{-1}{2}$ and, in fact, he speaks about the decorrelation proprieties of $(X^TX)^{−1}$ along his book several times. Anyone could shed some light on that?
