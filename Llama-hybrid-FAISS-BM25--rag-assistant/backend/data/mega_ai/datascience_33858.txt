[site]: datascience
[post_id]: 33858
[parent_id]: 33857
[tags]: 
I think you might be a little confused with the terminology here. Whether you train locally on your own computer or in the cloud (via AWS, Microsoft Azure, GoogleCloud, FloydHub, ...) there are only two three options: using a CPU $\rightarrow$ whatever is in your computer! using a GPU $\rightarrow$ probably an Nvidia card supportung CUDA using a TPU $\rightarrow$ a type of GPU to accelerate Machine Learning) Your deep learning framework of choice (Pytorch, Tensorflow, CNTK, ...) will make use of the hardware with which you provide it. All cloud services will offer instances with or without GPUs (they will almost vertainly have a CPU). Training on a GPU is of course much faster thatn a CPU, as it can parallelise many of the operations that are performed (especially for CNNs, compared to e.g. LSTMs). It is of course possible to offload certain parts on training onto each of them, so you would be using both together. A GPU in your computer at home and GPU in a big server-rack in an AWS data-center would both perform as equally as can be hoped for. The only differences will be cost, but that is a different discussion.
