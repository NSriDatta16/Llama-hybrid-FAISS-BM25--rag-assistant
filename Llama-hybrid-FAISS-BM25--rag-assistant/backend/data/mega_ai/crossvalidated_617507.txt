[site]: crossvalidated
[post_id]: 617507
[parent_id]: 617270
[tags]: 
$\mathbf X$ is a random $n$ -vector that models the observation $\mathbf x$ in $\mathbb R^n$ . The decision rule is specified by partitioning $\mathbb R^n$ into disjoint subsets $R_1, R_2, \cdots$ : if the observation $\mathbf X$ is an element of $R_j$ , then we decide or declare that $\mathsf H_j$ is the true hypothesis: $\mathsf H_j$ might or not be the true hypothesis and thus our decision can be in error, or be correct. $L_{k,j}$ is the cost of deciding/declaring that $\mathsf H_j$ is the true hypothesis when in fact, $\mathsf H_k$ is the true hypothesis. Now, Let $f_{\mathbf X\mid \mathsf H_k}(\mathbf x\mid \mathsf H_k)$ denote the conditional density of $\mathbf X$ given that $\mathsf H_k$ is the true hypothesis. Then, given that $\mathsf H_k$ is the true hypothesis, \begin{align} P(\mathbf X \in R_j\mid \mathsf H_k ) &= P(\text{declare that }R_j~\text{is true when in fact }\mathsf H_k~\text{is true})\\ &= \int_{R_j}_{∣\mathsf H_}(∣\mathsf H_)\, \mathrm d\mathbf x. \end{align} Now, declaring that $\mathsf H_j$ is true given that $\mathsf H_k$ is in fact true costs $L_{k,j}$ , and so the average cost of making a decision when $\mathsf H_k$ is the true hypothesis is $\displaystyle \sum_j L_{k,j}\int_{R_j}_{∣\mathsf H_}(∣\mathsf H_)\, \mathrm d\mathbf x$ . Since the probability that $\mathsf H_k$ is the true hypothesis is $P(\mathsf H_k)$ , we incur this cost with probability $P(\mathsf H_k)$ , and thus the average loss is \begin{align} E[L] &= \sum_k P(\mathsf H_k) \sum_j L_{k,j}\int_{R_j}_{∣\mathsf H_}(∣\mathsf H_)\, \mathrm d\mathbf x\\ &= \sum_k \sum_j L_{k,j}\int_{R_j}_{∣\mathsf H_}(∣_)\cdot P(\mathsf H_k)\, \mathrm d\mathbf x. \end{align} Bishop writes $p(\mathbf x, C_k)$ for what I have written as $ _{∣\mathsf H_}(∣\mathsf H_)\cdot P(\mathsf H_k)$ which is analogous to writing $P(A\mid B)\cdot P(B)$ as $P(A,B)$ instead of the more formal $P(A\cap B)$ . Turning to the OP's question, suppose that we are given that $\mathbf X \in R_j$ , and so the decision (or declaration or classification) is that $\mathsf H_j$ is the true hypothesis. If in fact $\mathsf H_k$ is the true hypothesis, then we incur a cost of $L_{k,j}$ . So, what is the average cost of deciding that $\mathsf H_j$ is the true hypothesis? Well, we are given that $\mathbf X \in R_j$ and so the average cost of deciding that $\mathsf H_j$ is the true hypothesis is $$\text{average cost of deciding }\mathsf H_j~\text{is true} = \sum_k L_{k,j}\cdot P(\mathsf H_k\mid \mathbf X \in R_j). $$ Bishop expresses this as $\displaystyle \sum_k L_{k,j}\cdot P(\mathsf C_k\mid \mathbf x)$ which does not mention the condition that the observation $\mathbf X$ must belong to $R_j$ . In my opinion, this lacuna just serves to confuse the issues.
