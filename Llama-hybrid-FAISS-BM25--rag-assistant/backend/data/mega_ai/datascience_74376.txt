[site]: datascience
[post_id]: 74376
[parent_id]: 74372
[tags]: 
In that tutorial, they created a new BahdanauAttention() class that is supposed to be inserted into the Decoder() object. Attention is something the Decoder uses, not the Encoder. The model as it is isn't complete. Add a Decoder and change its input shape to make it work. However, let me conclude with some thoughts on this implementation: I don't think you need bidirectionality when you use Attention mechanisms. The goal of attention is to allow for a useful signal that is far away back in time to travel to the output quicker (without having to pass through all the RNN cells and get lost mid way). For that reason, before the rise of Attention mechanisms, Bidirectional RNN layers used to be the best tool to achieve this higher "fully-connectedness" of Seq2seq Networks. But now that you have attention, you don't need that anymore since you have a tool that does that in a more flexible and powerful way. By adding bidirectionality, you are forcing the model to distribute its attention on a duplicate trend, the Decoder would receive two copies of the same signal (left-to-right and right-to-left) but with one attention to distribute on all. I think this is counterintuitive and undesirable, the very concept of Attention is messed up. I suggest you to drop bidirectionality and use just plain Attention. It would be interesting to compare its performance against a model with bidirectionality and no attention (I'm pretty sure attentional models would win hands down). -.-.-.-.-.- PS: Since tensorflow 2.1, the class BahdanauAttention() is now packed into a keras layer called AdditiveAttention() , that you can call as any other layer, and stick it into the Decoder() class. There is also another keras layer simply called Attention() that implements Luong Attention; it might be interesting to compare their performance.
