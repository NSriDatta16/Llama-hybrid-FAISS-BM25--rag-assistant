[site]: crossvalidated
[post_id]: 577526
[parent_id]: 
[tags]: 
Does XGBoost use gradient descent?

XGBoost is said to be based on "gradient tree boosting" in the original paper . Reading the paper and the introduction on the official website , it seems to me that the algorithm does not use gradient descent. As far as I can tell, the second-order (Taylor expansion) approximation of the loss of the whole ensemble is used in each step, which corresponds to Newton-Raphson optimization (also known as Newton boosting in this context). In contrast, other gradient-boosting methods use the negative gradient of the previous loss function as a pseudo-response variable for the optimization of a new base learner, thus performing an additive version of gradient descent optimization. Is XGBoost a gradient boosting method simply because it utilizes the gradient of the loss? Is it a gradient boosting method because it does use gradient descent, but I have failed to see it? Or is it, strictly speaking, not a gradient boosting method at all? EDIT: The question's marked duplicate contains the answer indirectly. My question was aimed more towards terminology than the method itself. XGBoost, in strict terms, does not perform gradient boosting but Newton boosting (in function space). The answer can now also be found on Wikipedia .
