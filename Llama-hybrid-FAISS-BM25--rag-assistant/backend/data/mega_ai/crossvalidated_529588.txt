[site]: crossvalidated
[post_id]: 529588
[parent_id]: 208529
[tags]: 
A simplified answer, as indicated by Cagdas Ozgenc, might be: whenever you do not aim for the true predictive distribution. A second aspect is the difference between fitting/estimation, inference, and forecast comparison. When you fit by minimizing a proper scoring rule and then add a penalty to deal with overfitting, your objective is usually no longer a proper scoring rule. Thirdly, I'm not aware of use cases where you want a predictive distribution but not the true one, or as close as possible. Often in practice, however, you are content with predicting a certain functional of the predictive distribution, i.e. a point forecast like the expected value or a quantile. In those cases, the usage of proper scoring functions is advisable, unless there is a clear (business) objective that you want to optimize directly. Also note that the notions of scoring rule and scoring function for the expectation coincide for binary targets. This is the general direction of Cagdas Ozgenc's answer , on which I'd like to comment (as I can't comment directly...yet): The same parametric model should have been used for log-loss and hinge loss. I'm sure, a logistic regression with intercept plus feature $I_{x>0}$ would not be worse than the hinge example. "However it does perfect classification." It does not. BTW, by which notion of "good classification"? I guess by "classification", a concrete decision based on a (probabilistic) forecast is meant, have a look at https://stats.stackexchange.com/q/312787 for more details on that distinction.
