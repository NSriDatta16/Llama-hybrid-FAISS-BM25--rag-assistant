[site]: crossvalidated
[post_id]: 421837
[parent_id]: 421745
[tags]: 
Most of what I've read seems to be concerned with constructing posterior predictive distributions for new $y$ (denoting as $y_{\text{new}}$ ), as in $p(y_{\text{new}}|y) = \int p(y_{\text{new}}|\theta) p(\theta|y)d\theta$ . But don't I just want the parameter-weighted distribution $f(\theta)p(\theta|y)$ without the measurement noise $\epsilon$ for the best estimate of $f(\theta)$ ? Expected value from the posterior distribution is "best" only in terms of minimizing squared error (for details, check The Bayesian Choice , already mentioned by you), but you could want instead look at other statistics, for example median of the posterior distribution (minimize absolute loss), or mode ( maximum a posteriori estimate), etc. If you have estimate of the posterior distribution, you can easily estimate any of those quantities. Letting $z=g(\theta)$ , is it necessary to specify a probability density function $p(z|\theta)$ so that new predictions of $z$ are defined as the marginal distribution $p(z|y) = \int p(z|\theta) p(\theta|y)d\theta$ (e.g., Robert, The Bayesian Choice , 2007)? Can I not just construct a probability distribution for the second physical model from $g(\theta)p(\theta|y)$ ? For instance using MCMC I could construct the probability distribution from the sequence $\{g(\theta_1), g(\theta_2), \ldots, g(\theta_t)\}$ . By the law of unconscious statistician $$ E[g(x)] = \int g(x)\,p(x)\, dx $$ If you have samples from the posterior distribution of $\tilde\theta_1,\tilde\theta_2,\dots,\tilde\theta_n \sim p(\theta|y)$ , then to obtain the samples of their transformations, you just need to transform the samples; to get estimate of the expected value of the transformation of $\theta$ , just calculate the empirical mean of the transformed MCMC samples.
