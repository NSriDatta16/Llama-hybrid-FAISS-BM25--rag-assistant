[site]: crossvalidated
[post_id]: 602221
[parent_id]: 
[tags]: 
Single input - multiple outputs with different loss functions in Keras: how is the gradient computed?

I've implemented a neural network with single input - multiple outputs using Keras API. The general structure of the network is like in this figure: Because each branch does a different task, I choose different loss functions (cross-entropy for the classifier and MSE for the regressor). The code is quite lengthy, but I can summarize it as follows: # define the input layer inputs = Input(shape=(..)) # define the common part (from the input layer) common = make_common_part(inputs) # model with a single input until the common part, then branches out model = Model(inputs=inputs, outputs=[make_branch_1(common), make_branch_2(common)]) # compile the model with MSE for model.compile(loss=['categorical_crossentropy', 'mean_squared_error'], optimizer='adam') The code works and returns the results as expected, but I want to understand how Keras works in the background. My intuition is that this model can learn the features (in the common part) that are good for both the classification and regression tasks because its weight matrix $W$ is updated by the gradients of the losses from the 2 branches. I guess the gradient used for backpropagation is computed on the sum of two losses: $$ L_{total} = L_{crossentropy} + L_{mse} = \sum_{i=1}^{N} CE(y_{reg}^{(i)}, \hat{y}_{reg}^{(i)}) + \sum_{i=1}^{N} MSE(y_{clf}^{(i)}, \hat{y}_{clf}^{(i)}) $$ I tried to confirm this by reading the code of keras and tensorflow but it's still unclear to me. It seems to me the losses are computed separately but I don't understand how the gradients in form of tensors are computed in tensorflow... Are the gradient of both losses summed up or are they left separated? Any insights you could give me are very much appreciated! Thank you very much in advance!
