[site]: datascience
[post_id]: 63886
[parent_id]: 63875
[tags]: 
Yes, it's common to consider that the prediction is made of multiple answers (typically top N most relevant answers) and use a performance measure based on that. Currently you're treating the problem as a classification problem but logically this is more like a recommendation problem or an information retrieval problem (like results from a search engine). Usually for this kind of problem the gold answer would also consists of a list of several items, but apparently your dataset contains a single answer for every instance. Answer to comment: a couple of papers using some top N performance measures (note: it's just a quick selection based on the keyword "information retrieval") https://www.microsoft.com/en-us/research/publication/letor-benchmark-collection-research-learning-rank-information-retrieval/ http://informationr.net/ir/18-2/paper582.html The CLEF series of Shared Tasks have proposed many datasets and evaluation measures across the years, it's probably a good source for resources and papers... if you have a bit of time to explore it ;)
