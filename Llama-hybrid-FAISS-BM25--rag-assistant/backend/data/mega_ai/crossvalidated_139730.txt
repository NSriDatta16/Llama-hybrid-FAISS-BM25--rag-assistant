[site]: crossvalidated
[post_id]: 139730
[parent_id]: 
[tags]: 
Creating a copy/simulation of a dataset respecting statistical properties?

I often need to work with and analyse electronic medical record data which cannot be moved from the servers in the organisation where it resides, due to ethical reasons. Is anyone aware of a method or approach that could create a copy/simulation of the (large) data which respects the statistical properties of original dataset, but where the values of variables are simulated. This would allow a simulated copy could then be used locally to work out analysis coding before applying the the 'real' data? I guess kind of like multiple imputation but for the entire dataset. Is it even possible? I maybe should explain further that the purpose of the data copy is to work out data handling and analysis code (R, SAS, etc) then take that to the real data. Not to make inference from the copy. UPDATE Maybe it's a case of suggesting an approach. Could multiple imputation be adapted to impute everything? Or maybe data mining techniques to first reduce Colinear variables to single component?
