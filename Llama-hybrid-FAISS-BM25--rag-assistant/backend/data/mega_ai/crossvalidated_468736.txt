[site]: crossvalidated
[post_id]: 468736
[parent_id]: 
[tags]: 
Maths for deeply understanding backpropgation

I have been trying to develop a deeper understanding of Neural Networks so I can understand the libraries such as tensorflow and others. I have had good success with pereceptron models, and have a good enough understanding of linear algebra, and the other mechanics behind the networks as well. But now as I am getting into multi-layer Perceptrons. The concept of backpropogation is just above me. I have little to no understanding of basic calculus concepts. (I will only start taking pre-calc/Calculus1 next year.) So I wish to start studying up on the concepts now. I know I need to have a good understanding of derivatives, the chain rule, and gradient descent. But what all concepts of calculus do I need to understand in order better under stand the pseudo code, lectures I have been reading up on. I have watched 3blue1brown's and The Coding Train's neural network series, but the concepts for backpropogation are a little over my head. Additionally, any resources for learning would be greatly appreciated.
