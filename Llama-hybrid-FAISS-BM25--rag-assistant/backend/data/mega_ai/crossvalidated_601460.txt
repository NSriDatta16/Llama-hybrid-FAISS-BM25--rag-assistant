[site]: crossvalidated
[post_id]: 601460
[parent_id]: 
[tags]: 
Convergent and Divergent Prediction Intervals in Time series models

My question is similar to this one Whether increasing the sample size influences the prediction interval? and it's also related with Rob Hyndman - Forecasting: Principles and Practice (2nd Edition), but I want to gather some more insights: As far as I understand, when we fit a time-series model: If the model is stationary, the prediction intervals are not divergent, no matter the prediction length we are talking about: From "Forecasting: Principles and Practice, 2018": In general, prediction intervals from ARIMA models increase as the forecast horizon increases. For stationary models (i.e., with d=0) they will converge, so that prediction intervals for long horizons are all essentially the same. For d>1, the prediction intervals will continue to grow into the future. Why is this the case? Even if fitted residuals are normal, will this always be the case? What if residuals are not-normal and we need to use bootstrap? How can we cope bootstraping with data transformations? On what data set do we make the predictions? Furthermore, forgetting normality of residuals, bootstrap, transformations and all that, shouldn't prediction intervals be always divergent, due to uncertainty of fitted parameters, for instance in ARIMA models? I am asking this because in Rob Hyndman book, there are multiple examples where prediction intervals are shown to be convergent or divergent for many models (both ETS and ARIMA) and I don't clearly see what is the rational behind such results...
