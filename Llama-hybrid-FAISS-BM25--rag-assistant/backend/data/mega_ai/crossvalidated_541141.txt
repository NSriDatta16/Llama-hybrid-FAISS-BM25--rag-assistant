[site]: crossvalidated
[post_id]: 541141
[parent_id]: 541134
[tags]: 
No, if your number of records changes, AIC is not appropriate. There's multiple idea for fixing this: deal with the missing informaiton to have the same number of records so that missing lags are no longer an issue (e.g. multiple imputation, single imputation of some form perhaps with a missing flag for those records, etc. or use an algorithm that intrinsically can deal with missingness like xgboost) use the smallest subset you can use all methods for (if you don't want to do (1) and still want to compare using AIC) do not use AIC (e.g. use cross-validation or some past-vs.-future split if this is a time series problem, i.e. not evaluating on the training data). AIC vs. AICc is sort of irrelavant for this topic. AICc is meant to be better for small sample situations (although there seems to be disagreement on whether it truly is). Finally, there's the question whether model selection is a sensible thing to do (esp. if the models are so close to each other that it's not really possible to be sure whether one is truly better than the other) or whether you want model averaging instead.
