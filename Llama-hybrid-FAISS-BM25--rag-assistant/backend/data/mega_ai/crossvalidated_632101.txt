[site]: crossvalidated
[post_id]: 632101
[parent_id]: 
[tags]: 
What are the layer normalization dimensions in transformer?

In transformer training, the activations have three dimensions: batch, feature (i.e. embedding) and time (i.e. token). Layer normalization is applied, calculating statistics (mean, standard deviation) and using them to standardise the activations before using learned parameters to scale ( $*\gamma$ ) and shift ( $+\beta$ ) them. Which dimensions are normalized over, and which are independent? Presumably the batch dimension is independent. I also presume the feature dimension is normalized. But what about the time dimension? (As it doesn't have a defined constant length, I suppose the separate time steps can't have separate learned parameters, so there's just one $\gamma$ and $\beta$ for the layer.) Different sources seem to disagree on the definition of layer normalization. And I feel the original transformer paper ( Attention Is All You Need ) and its layer normalization citation ( Layer Normalization ) are not explicit about this.
