[site]: crossvalidated
[post_id]: 71419
[parent_id]: 
[tags]: 
Average over two variables: Why do standard error of mean and error propagation differ and what does that mean?

I'm doing an experiment with a cryostat to determine the critical temperature for lead. To avoid asymmetries, I determine the critical temperature both through heating (going from 2 K to 10 K) and cooling (10 K -> 2 K). Now I have two values, that differ slighty and I average them. So a measurement of (6.942 $\pm$ 0.020) K and (6.959 $\pm$ 0.019) K gives me an average of 6.951 K. Now the question is: what is the error of that average? One way to do it would be to calculate the variance of this sample (containing two points), take the square root and divide by $\sqrt{2}$. This gives me an SEM of 0.0085 K, which is too low for my opinion (where does this precision come from?) The other way is to say the the mean is a function of two variables, $\bar{T} = \frac{T_1 + T_2}{2}$, therefore by error propagation the error is $\Delta T = \frac12\sqrt{(\Delta T_1)^2+(\Delta T_2)^2}$, and that gives me a much more rational value of 0.014. I see how those values differ in terms of numbers, but which one is correct when talking about the correct estimate for the standard deviation?
