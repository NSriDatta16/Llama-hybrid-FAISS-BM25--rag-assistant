[site]: crossvalidated
[post_id]: 107529
[parent_id]: 
[tags]: 
Bayes decision theory: Classification error probability

In Bayesian decision theory: Given $\omega_1$ and $\omega_2$ as two classes for classification, $P\left( \omega_1 \right)$ and $P\left( \omega_2\right)$ their prior probabilities, $x$ the feature vector representing an unknown pattern, $P\left( \omega_1 | x\right)$ and $P\left( \omega_2 | x\right)$ as posteriori probabilities, $p\left( x |\omega_i\right)$ the likelihood function of $\omega_i$ with respect to $x$; and $R_i$ the region of feature space where decision is in favor of $\omega_i$ why is it that the minimization of the probability error $P_e$ , given by: \begin{align*} P_e &= P\left( x \in R_1, \omega_2 \right) + P\left( x \in R_2, \omega_1 \right)\\ &=P\left(\omega_2 \right) \int \limits_{R_1}p\left( x |\omega_2 \right) dx + P\left(\omega_1 \right) \int \limits_{R_2}p\left( x |\omega_1 \right)dx\\ &= \int \limits_{R_1}P\left( \omega_2 | x\right)p(x) dx + \int \limits_{R_2}P\left(\omega_1 | x \right)p(x)dx \end{align*} by choosing regions $R_1$ and $R_2$ of feature space so that: \begin{align} R_1 &: P\left(\omega_1 | x \right) > P\left( \omega_2 | x\right)\\ R_2 &: P\left(\omega_2 | x \right) > P\left( \omega_1 | x\right) \end{align} is said to be not always the best for minimizing $P_e$ ?
