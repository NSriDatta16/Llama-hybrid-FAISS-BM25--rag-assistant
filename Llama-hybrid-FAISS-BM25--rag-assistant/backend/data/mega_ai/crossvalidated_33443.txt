[site]: crossvalidated
[post_id]: 33443
[parent_id]: 33435
[tags]: 
I have seen two arguments advanced that Bayesian analysis is a generalization of a frequentist analysis. Both were somewhat tongue-in-cheek, and more getting people to recognize the assumptions about regression models by using priors as a context. Argument 1: Frequentist analysis is Bayesian analysis with a purely uninformative prior centered on zero (yes, it doesn't matter where its centered, but ignore that). This provides both the context for which a Bayesian might extract the results of a frequentist analysis, explains why you can get away with using some "Bayesian" techniques like MCMC to extract frequentist estimates in situations where say, maximum likelihood convergence is tough, and gets people to recognize that when they say "The data speak for themselves" and the like, what they're actually saying is that beforehand, all values are equally likely. Argument 2: Any regression term you don't include in a model has, in effect, been assigned a prior centered on zero with no variance. This one isn't so much a "Bayesian analysis is a generalization" as much as "There are priors everywhere , even in your frequentist models" argument.
