[site]: crossvalidated
[post_id]: 498754
[parent_id]: 
[tags]: 
Question on how Experience Replay solves the traditional instability of online Q-learning (benefits of Experience replay)

My question is in regards to how Experience replay solves a traditional instability problem of RL combined with NN. Experience replay solves several instability issues, but the part that I am trying to understand is the following: Cause for instability: the fact that small updates to Q may significantly change the policy and therefore change the data distributions. Solution: With experience replay, the behavior distribution is averaged over many of its states, smoothing out learning and avoiding oscillations or divergence in the parameters. My interpretation of those cause and solution is now: Interpretation for Cause for instability: With learning online (with non-stationary training sample distribution which next training sample is determined by agent's current action), "small" updates to Q-values potentially can abruptly change the ongoing policy (neural networks try to converge toward some good policy). Abrupt change in the ongoing policy also abruptly changes the next time-step's training sample distribution (as in an 'online-setting', now's action determined by now's policy determines next training sample ). And this can potentially lead to "unwanted feedback loops" (which means that the neural network can't converge but diverge/oscillate due to these loops of abrupt changes in policy) Interpretation for Solution: Experience replay resolves the problem of such potential abrupt changes in ongoing policy(which can lead to unwanted feedback loops -> divergence of NN ), because it averages out policy exploration (behavior distribution) over "many" previous states (mini-batch). Unlike online Q-learning, which policy is determined by one most recent training sample, Experience replay uses multiple training samples (of previous states) as the size of predetermined mini-batch. This enables smoothing out the learning of weights and policy and can avoid abrupt changes in ongoing policy. Please tell me if I am understanding correctly.
