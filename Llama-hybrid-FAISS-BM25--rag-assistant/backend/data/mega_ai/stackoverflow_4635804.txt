[site]: stackoverflow
[post_id]: 4635804
[parent_id]: 4634787
[tags]: 
NLTK's FreqDist accepts any iterable. As a string is iterated character by character, it is pulling things apart in the way that you're experiencing. In order to do count words, you need to feed FreqDist words. How do you do that? Well, you might think (as others have suggested in the answer to your question) to feed the whole file to nltk.tokenize.word_tokenize . >>> # first, let's import the dependencies >>> import nltk >>> from nltk.probability import FreqDist >>> # wrong :( >>> words = nltk.tokenize.word_tokenize(p) >>> fdist = FreqDist(words) word_tokenize builds word models from sentences. It needs to be fed each sentence one at a time. It will do a relatively poor job when given whole paragraphs or even documents. So, what to do? Easy, add in a sentence tokenizer! >>> fdist = FreqDist() >>> for sentence in nltk.tokenize.sent_tokenize(p): ... for word in nltk.tokenize.word_tokenize(sentence): >>> fdist[word] += 1 One thing to bear in mind is that there are many ways to tokenize text. The modules nltk.tokenize.sent_tokenize and nltk.tokenize.word_tokenize simply pick a reasonable default for relatively clean, English text. There are several other options to chose from, which you can read about in the API documentation .
