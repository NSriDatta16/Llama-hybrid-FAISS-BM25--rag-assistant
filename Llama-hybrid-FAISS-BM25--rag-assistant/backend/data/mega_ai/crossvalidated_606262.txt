[site]: crossvalidated
[post_id]: 606262
[parent_id]: 
[tags]: 
Are bias and variance used as metrics to evaluate estimators in Bayesian inference?

Consider the parameter $\theta$ , which is a deterministic unknown in the frequentist paradigm. Given a random variable $X \sim p_X(x ; \theta)$ , consider the estimator $\Theta(X)$ of $\theta$ , and the error defined as $$e = \Theta(X) - \theta$$ The bias of $\Theta$ is defined as $$\text{Bias}(\Theta) = E[e] = E[\Theta(X)] - \theta$$ and the variance of $\Theta$ is defined as \begin{align} \text{Var}(\Theta) &= \text{Var}(\Theta(X) - \theta) \\ &= \text{Var}(e) \\ &= E[e^2] - E[e]^2 \\ &= \text{MSE}(\Theta) - \text{Bias}(\Theta)^2 \end{align} where $\text{MSE}(\Theta) = E[e^2]$ is the mean squared-error associated with $\Theta$ . We often choose $\Theta$ that minimizes the mean squared-error, which in turn minimizes the bias and variance associated with $\Theta$ . Now consider the Bayesian paradigm, where $\theta$ is a random parameter with prior $p_\theta$ . Given an observation $X \sim p_{X \mid \Theta}(x \mid \theta)$ , we estimate $\theta$ as $\Theta(X)$ . Are there similar definitions of bias and variance in the Bayesian paradigm? I'm guessing that the answer is no, as the definitions above would not be helpful in evaluating $\Theta$ . More precisely, let the error be defined as $$e = \Theta(X) - \theta$$ In this case, the randomness of $e$ is due to both $\Theta(X)$ and $\theta$ , and so the moments of $e$ , such as its mean (bias in the frequentist case) and variance (variance of $\Theta$ in the frequentist case) would not be useful to us. I’m aware that loss functions are preferred to evaluate Bayesian estimators. However, I’m not sure why these would be preferred over a similar definition of bias and variance above.
