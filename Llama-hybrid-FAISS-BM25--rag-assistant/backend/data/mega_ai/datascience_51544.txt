[site]: datascience
[post_id]: 51544
[parent_id]: 
[tags]: 
Smaller network width than output size?

I am trying to figure out if it makes sense that the width of the network could be smaller than the input/output size? So for example, I am giving the Neural Network 2048 numbers, and I am expecting 2048 numbers back. I would also like to use LSTM's, which take a lot of time/space etc to train, and having one or a few 2048 or larger LSTM layers connected to Dense layers would take a lot of space/time to train. But maybe that is the way to go? Does anyone have any experience with this type of problem? Thank you!
