[site]: datascience
[post_id]: 63312
[parent_id]: 
[tags]: 
How can I regularize the output of a layer from scratch (without using Keras)?

I am trying to build a Convolutional Neural Network after reading notes from Stanford's cs231n course. I use ELU activation as activation function, and SoftMax as my classifier. Architecture is simple: Convolution(5 x 5 x 32) - ELU - MaxPool(2 x 2) - Convolution(5 x 5 x 64) - ELU - MaxPool(2 x 2) - FullyConnected(hidden units = 256) - SoftMax I wrote the architecture without using API's and it turns out fine. When I apply L2 regularization to the weights, I get test accuracy of 57% (I didn't do hyperparameter tuning; learning rate = 0.001, regularization strength = 0.001). Recently, I found out that in keras API, there is something call activity regularization, which puts penalty based on the output(activation) of a layer. I tried to do so in my model, by putting the regularization term to my loss: L2_regularization_weight = np.sum(W1 ** 2) + np.sum(W2 ** 2) + np.sum(W3 ** 2) + np.sum(W4 ** 2) activity_regularization = np.sum(conv_out_1 ** 2) + np.sum(conv_out_2 ** 2) loss += 0.5 * self.reg * (activity_regularization + L2_regularization_weight) _, dW1, _ = conv_backward_fast(conv_out_1, conv_cache_1) _, dW2, _ = conv_backward_fast(conv_out_2, conv_cache_2) grads['W1'] += self.reg * (dW1 + W1) grads['W2'] += self.reg * (dW2 + W2) grads['W3'] += self.reg * W3 grads['W4'] += self.reg * W4 W1, W2 , W3, W4 are weights of first convolution layer, second convolution layer, Linear filter in fully connected layer, and linear filter in classification layer. Here I am regularizing based on L2 of weights and L2 of convolution layers' output. Moreover, here is my documentation for conv_backward_fast function: conv_backward_fast(dout, cache) Input : dout = gradient w.r.t layers after this convolution layer in the model (e.g. If the function is called by second convolution layer, then dout is the "product" of rates of change of loss w.r.t ELU - MaxPool - FullyConnected - SoftMax.) cache = All inputs to current convolution layer Output : dx = gradient w.r.t input dw = gradient w.r.t weights db = gradient w.r.t bias So, my first question is " Am I doing it correctly? " (Although I feel like I don't since the validation accuracy and training accuracy fall down to 27%). My second question would be " Any suggestion to do activity regularization and backward calculation correctly? "
