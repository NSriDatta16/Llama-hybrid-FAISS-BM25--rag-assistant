[site]: crossvalidated
[post_id]: 478598
[parent_id]: 
[tags]: 
What happens if validation/real data has values outside training set limits for continuous data?

Describing Example: If feature X1 in training data has values inside [0,1] However, X1 in real/validation data has values [-1, 2[ What happens then? Previously discussed On previous discussions with colleagues, it was suggested that it varies a lot depending on the model. A linear model could explode this feature as it gets higher and the opposite for lower values. A neural network would depend on its activation functions. ReLU based architectures would be more sensitive to these out-of-bounds values, and sigmoid based architectures would be less. However, we could not come to a conclusion when the architecture was a mix of both. Literature Are there any papers discussing this?
