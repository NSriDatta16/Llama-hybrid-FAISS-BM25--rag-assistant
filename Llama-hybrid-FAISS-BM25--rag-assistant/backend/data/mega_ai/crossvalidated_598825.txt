[site]: crossvalidated
[post_id]: 598825
[parent_id]: 598816
[tags]: 
It sounds like you're doing some practice experiments so it's not that big of a deal, but in general you want to be pretty confident in your choice of hyperparameters before you even start touching the test set. Strictly, you should choose your hyperparameters only based on the training set, evaluate on the test set, and the number you get is the number you get, no backsies. SVMs are mostly differentiable in their parameters, so the CV performance should be mostly smooth w.r.t. these parameters. Try plotting the CV performance against one parameter at a time and look for patterns. Since you do k-fold CV, you get k performance estimates per hyperparameter value, and the 99.8 % sounds like the mean of those k values. But you might want to play it more safe than by going by the mean, e.g. looking at the mean minus a few standard deviations, or the 5th or 10th percentile of the performance estimates, or similar. This depends on the distribution of you performance estimates, how bad a bad model is considering your application, and so on. The training score will usually be better than the test score, because the best hyperparameters are the best partially due to luck. This luck effect will on average disappear when you look at the test set, this is called regression toward the mean.
