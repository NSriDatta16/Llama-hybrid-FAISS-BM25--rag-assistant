[site]: crossvalidated
[post_id]: 405368
[parent_id]: 
[tags]: 
Hypothesis testing control vs treatment when the outcome tends to vary over time

I want to test the effect of a treatment on a continuous outcome (specifically interested in increase in average in the treatment group). The experiment is run over a period of time. But the problem here is that the relative values of the outcome (and hence the daily average) tend to vary over time (and is mostly random variation day over day). What I am interested is, finding out if there is a significant increase in the mean of the outcome due to the treatment. The sample size for the treatment is relatively large enough for a single day, but are not constant over days (ratio of treatment size to control size is constant however). What is the best way to test this, given the random time effect on both treatment and control ? Can the percentage change relative to the control, be used a metric in some appropriate test, given that the treatment sample size is not constant over days ? EDIT: Further clarification of the situation It'is better not to think of this as an individual. Let's say there is an environment and lot of variables that operate within it. The environment has an outcome. (Think of it as production volume, revenue, sales, visits etc,..). Now we are trying a treatment on the environment by affecting one variable and want to see if that variable has a positive effect on the outcome. We are measuring the outcome on a daily basis and want to test if the mean value of the outcome is increased in the treatment group. But the problem is that, even in a situation where you don't have a treatment, the net outcome and the mean outcome fluctuate on a daily basis by random factors. So the issue at hand is to devise a proper test to study the treatment, while somehow accounting for this random variability in the outcome. Would appreciate if there are any suggestions for this case.
