[site]: crossvalidated
[post_id]: 71284
[parent_id]: 71260
[tags]: 
Gung's answer is good (+1). There is another way of looking at it, though. Imagine that the covariance between $X_1$ and $X_2$ were to be positive. What does it mean for $\sigma_{1,2}>0$? Well, it means that when $X_2$ is above $X_2$'s mean, $X_1$ tends to be above $X_1$'s mean, and vice versa . Now suppose I told you that $X_2=\mathit{x}_2>\mu_2$. That is, suppose I told you that $X_2$ is above its mean. Wouldn't you conclude that $X_1$ is likely above its mean (since you know $\sigma_{1,2}>0$ and you know what covariance means)? So, now, if you take the mean of $X_1$, knowing that $X_2$ is above $X_2$'s mean, you are going to get a number above $X_1$'s mean. That is what the formula says: \begin{equation} E\{X_1 | X_2=\mathit{x}_2\} = \mu_1 + \frac{\sigma_{1,2}}{\sigma_{2,2}}\left( \mathit{x}_2-\mu_2\right) \end{equation} If the covariance is positive and $X_2$ is above its mean, then $E\{X_1 | X_2=\mathit{x}_2\} > \mu_1$. The conditional expectation takes the form above for the normal distribution, not for all distributions. This seems a little strange given that the reasoning in the paragraph above seems pretty compelling. However, (almost) no matter what the distributions of $X_1$ and $X_2$ this formula is right: \begin{equation} BLP\{X_1 | X_2=\mathit{x}_2\} = \mu_1 + \frac{\sigma_{1,2}}{\sigma_{2,2}}\left( \mathit{x}_2-\mu_2\right) \end{equation} Where $BLP$ means best linear predictor. The normal distribution is special in that conditional expectation and best linear predictor are the same thing.
