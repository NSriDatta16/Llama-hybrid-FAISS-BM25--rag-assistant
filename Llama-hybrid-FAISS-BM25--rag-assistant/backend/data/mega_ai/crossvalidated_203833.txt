[site]: crossvalidated
[post_id]: 203833
[parent_id]: 
[tags]: 
Derivation of the gradient of log likelihood of the Restricted Boltzmann Machine using free energy method

I am reading "Machine Learning: A Probabilistic Perspective". While I understand the other proof in 27.7.2.1: Deriving the gradient using $p(\boldsymbol{h}, \boldsymbol{v}|\boldsymbol{\theta})$, I don't quite understand the next section 27.7.2.2 Deriving the gradient using $p(\boldsymbol{v}|\boldsymbol{\theta})$. In section 27.7.2.2, equation 27.114 ~ 27.117 \begin{align} & F(\boldsymbol{v}) \\ = & \sum_\boldsymbol{h} E(\boldsymbol{v}, \boldsymbol{h}) \\ = & \sum_\boldsymbol{h} \exp\Bigg(\sum_{r=1}^{R}\sum_{k=1}^{K}v_r h_k W_{rk}\Bigg) \\ = & \quad ... \\ = & \prod_{k=1}^{K}\Bigg(1+\exp\bigg(\sum_{r=1}^{R}v_r W_{rk}\bigg)\Bigg) \\ \end{align} and equation 27.122 $$ \frac{\partial}{\partial w_{rk}} F(\boldsymbol{v}) = -\mathbb{E}[v_r h_k| \boldsymbol{v}, \boldsymbol{\theta}] $$ I have little idea why the gradient of the free energy $ F(\boldsymbol{v}) $ turns out to be like that. Also why does the $E(\boldsymbol{v}, \boldsymbol{h})$ expands into a $\exp(\cdot)$ thing while none of the previous pages of that do?
