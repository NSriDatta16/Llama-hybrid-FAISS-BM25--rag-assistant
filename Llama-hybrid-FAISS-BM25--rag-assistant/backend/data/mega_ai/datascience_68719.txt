[site]: datascience
[post_id]: 68719
[parent_id]: 
[tags]: 
Understanding pseudocode for co-democratic learning

I am reading this blog from Sebastian Ruder blog-link , researcher scientist at Deepmind, and having problems understanding this pseudocode for Democratic Co-learning. Can somebody help me understand what is going on; specifically, I'm just trying to understand line 1-6. Description of Democratic Co-learning from blog: Democratic Co-learning Rather than treating different feature sets as views, democratic co-learning (Zhou and Goldman, 2004) [18] employs models with different inductive biases. These can be different network architectures in the case of neural networks or completely different learning algorithms. Democratic co-learning first trains each model separately on the complete labelled data L. The models then make predictions on the unlabelled data U. If a majority of models confidently agree on the label of an example, the example is added to the labelled dataset. Confidence is measured in the original formulation by measuring if the sum of the mean confidence intervals w of the models, which agreed on the label is larger than the sum of the models that disagreed. This process is repeated until no more examples are added. The final prediction is made with a majority vote weighted with the confidence intervals of the models. The full algorithm can be seen below. M is the set of all models that predict the same label j for an example x. I guess line 1-3 is the training of several models on labeled data. Is i each sample in the labeled dataset? Is mi a trained model using all the samples from the labeled dataset? I guess that line 4-6 is going through each sample in the unlabeled dataset but what's j? Is it iterating through each label? This line I'm especially confused about is M
