[site]: datascience
[post_id]: 124816
[parent_id]: 
[tags]: 
Prefix tuning in LLM uses learnable vectors to fine tune the model

I would like to implement a new architecture for Transformer. Below description is my thought. Prefix tuning in LLM uses learnable vectors to fine tune the model. Is there a way to use the output generated by the Neural network as prefix? Thanks
