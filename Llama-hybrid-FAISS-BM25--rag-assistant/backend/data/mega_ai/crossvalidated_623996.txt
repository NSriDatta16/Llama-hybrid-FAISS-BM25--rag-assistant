[site]: crossvalidated
[post_id]: 623996
[parent_id]: 
[tags]: 
Ideal loss for a multi-label problem with soft targets

Given an input X, my goal is to predict a list of probabilities for n factors, where the factors could be attributes like feasibility , comfort , ease etc. using a neural network. I have soft targets for each of these classes that I have computed for each of these targets. The factors are not necessarily related to each other (and even if they are, an earlier part of the neural network can take care of that). What would the best loss for this situation be? Pytorch examples would be a bonus! Thanks
