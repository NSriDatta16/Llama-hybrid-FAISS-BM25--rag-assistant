[site]: crossvalidated
[post_id]: 379677
[parent_id]: 379666
[tags]: 
I totally agree with @Tim's comment about variance but I felt motivated to go a step further, as is my want. I took the 20 values and mused as to what AUTOBOX would do with these values essentially revisiting this post How to calculate the standard average of a set excluding outliers? . AUTOBOX delivered the following adjustments to cleanse the data using procedures developed here http://docplayer.net/12080848-Outliers-level-shifts-and-variance-changes-in-time-series.html . Additionally identifying an unusual value at period 4 and a persistant level/step shift starting at period 8. The plot of the Actual and Cleansed Data is educational as to what the human eye sees and what it doesn't see . . What we miss initially is the subtle but significant anomaly at period 4 and the persistent level shift at period 8 BECAUSE we are focused on the overwhelming pulse impacts at periods 5 and 18. Going one step further ( always dangerous with small samples but not necessarily so when there is strong signal ) the model's residuals suggest a constant/persistant blurring (inncreased error variance ) from period 7 to 20 The question I really answered "Is there a better process ?" in terms of making data smoother ? i.e. less effected by blurring . Or is it possible to further reduce the variance (non-systematic behavior )?
