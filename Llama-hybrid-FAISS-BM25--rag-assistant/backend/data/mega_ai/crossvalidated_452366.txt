[site]: crossvalidated
[post_id]: 452366
[parent_id]: 
[tags]: 
How to test whether the simulated mean differs from observed mean?

I have three log-return time series data1 , the data look like: head(data1) # r1 r2 r3 #[1,] 0.09251185 0.079745597 0.130000000 #[2,] -0.08814853 0.005890349 -0.053097345 #[3,] -0.02378687 -0.003603604 0.046728972 #[4,] 0.04902534 0.030289331 -0.005952381 #[5,] 0.03669981 0.008336990 0.017964072 #[6,] -0.03298082 -0.130113142 -0.102941176 Each log-return time series is not normal and correlated each other. r_i, i=1,2,..., are dependently and not-identically distributed but have finite four moments. The length of r_i, i=1, 2,... is above 500 items. I have applied a model and compute the mean vector model_mu2 : # [1] 0.007744681 0.013342437 0.012513445 I need to know, whether the simulated mean ( not median ) differs from observed mean (two-tailed test)? Question. What a test one can use to test whether the simulated mean differs from the observed mean? My attempt for the first column is wilcox.test(data1[,1], alternative = "two.sided", mu = model_mu2[1], conf.int = TRUE) Wilcoxon signed rank test with continuity correction data: data1[, 1] V = 53332, p-value = 4.664e-05 alternative hypothesis: true location is not equal to 0.007744681 95 percent confidence interval: -0.002419831 0.004090282 sample estimates: (pseudo)median 0.0008300681 The returned p-value = 4.664e-05 less 0.05 and we should reject the H0.
