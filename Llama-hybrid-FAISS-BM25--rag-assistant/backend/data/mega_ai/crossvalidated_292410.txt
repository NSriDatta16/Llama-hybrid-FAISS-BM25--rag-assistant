[site]: crossvalidated
[post_id]: 292410
[parent_id]: 292389
[tags]: 
There are many possible responses to your question. First, predictive accuracy is an easily fudged and manipulated metric. This is especially true when it is the only objective, goal or outcome that is under measure. Stepping out of purely statistical considerations, it is well known that otherwise objective metrics can and will become biased and unreliable in politically charged environments where monetary outcomes are dependent on their results. So, if an analyst is being evaluated and/or compensated based on the accuracy of his models, unscrupulous practitioners may elect to employ practices that boost results. These unscrupulous practices are virtually undetectable, even by his manager(s), unless they sit down side-by-side with him and, in a kind of forensic audit, rigorously groom the code driving the model(s). Next and expanding @Pere's comment, accuracy is a function of the information used. Borrowing old rules of thumb taken from applied modeling concerned with expected R-square values for predictive (or descriptive) models, in survey-based research, it used to be that R-squares ranging between 10% to 20% (or so) were expected. With financial, business and/or corporate-related information and models, somewhat higher R-squares in the 40%-60% range were the norm. At the low end of the range, such as response models developed in direct marketing applications, R-squares of 5% or less, in some cases even approaching zero, were expected. On the high end (with one exception that I'm aware of), R-squares much more than 70%-80% could be expected to be the result of overfitting or a violation of model assumption(s). In other words, extremely high R-squares were highly likely to be erroneous or, worse, fraudulent. The one exception to this generality were time series models developed for evaluating or forecasting retail products where the full set of so-called causal information is available. Here, I'm thinking of marketing mix models built, e.g., from Nielsen market-level scanner data for FMCGs (fast-moving consumer goods or products, like ketchup, potato chips or soda). With a full set of marketing instruments (price, promotion, distribution, advertising, etc.), R-squares that are 90% or more, even up to 99%, are not uncommon. However, the more disaggregate the unit of analysis in a marketing mix model becomes -- e.g., going from market-level, to store-level, to household-level information -- the lower one's expectations have to become regarding expected R-square. Of course, I'm suggesting that these hoary rules of thumb wrt R-squares are applicable by analogy to expectations concerning predictive accuracy. Note that direct marketing was one of the first applied industries where logistic regression-based response modeling saw wide use. With typically quite small response rates (frequently 2%-3% or less), finding signals in such noisy data was based on deciling the predictions from the model and sending out mailings, subscriptions, offers, whatever, to the top 2 or 3 deciles. This methodology usually ensured that those most likely to respond were targeted, resulting in profitable marketing activities. The irony in the wide use of logistic regression in DM is that, as is well known, the logistic function does not fit the tails of its distribution at all well, particularly in the presence of low incidence and/or rare events. See Gary King's papers on rare event modeling for a fuller discussion of this issue, e.g., here ... https://gking.harvard.edu/relogit . Another issue with predictive accuracy concerns the widespread use of dichotomizing or discretizing otherwise continuously scaled features and information. Many are the statisticians who have written about the fallacies and problems to be associated with these widely employed heuristics. The bottom line is the potentially massive loss of valuable information as a function of these practices that have the unsurprising and ironic effect of blunting or dulling predictive accuracy. For instance, Andrew Gelman recently discussed the truly fundamental and, for the most part, ignored issues concerned with feature measurement, reliability and validity (see here ... http://andrewgelman.com/2017/07/17/continue-not-trust-turk-another-reminder-importance-measurement/ ). One of the comments on his blog references a biostatistics paper which excoriates this practice of discretizing continuously distributed data (see here ... Senn, Stephen and Steven Julious, 2009, Measurement in clinical trials: A neglected issue for statisticians, Statistics in Medicine 28:3189-3209). This paper references a book by David Hand, Measurement Theory and Practice: The World Through Quantification , which should be required reading for all statisticians and data scientists. Echoing Gelman, Senn, et al, and Hand is Frank Harrell who has written extremely cogent critiques of the errors and fallacies inherent in blindly discretizing data, e.g., see Dichotomania here ... http://www.fharrell.com/#catg . To sum this all up, expectations regarding predictive accuracy are a function of many factors including its importance as a unit of measure, the tactical objectives of the model, the information under analysis, feature measurement, the use of discretizing heuristics, and more.
