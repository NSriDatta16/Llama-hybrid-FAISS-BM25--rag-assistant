[site]: crossvalidated
[post_id]: 329359
[parent_id]: 329144
[tags]: 
You are not optimizing any parameters in your code. The only tuning parameter considered in the caret package is the mtry value, which is specified to be 2 in your code. However, it is still important to get a good estimate of the accuracy of the random forest; model 2 shows the accuracy is around 95.3% using repeated K-fold cross-validation. This is similar to what we get using the out-of-bag (OOB) sample estimate from the random forest: randomForest(Species ~ ., data=iris, ntree=500, mtry=2) Random forest does not prune the trees. I believe the only other parameter you may want to optimize in randomForest is the nodesize. This is set to 1 for classification, but Lin and Jeon (2006) found increasing the terminal node size may yield more accurate predictions. You'll need to tune this parameter yourself though (not a tuning parameter in caret package). There are also other tree-based models you can consider (e.g., Gradient Boosting Trees, Extremely Randomized Trees). You can see a list of the tuning parameters on the github page: http://topepo.github.io/caret/train-models-by-tag.html#Random_Forest
