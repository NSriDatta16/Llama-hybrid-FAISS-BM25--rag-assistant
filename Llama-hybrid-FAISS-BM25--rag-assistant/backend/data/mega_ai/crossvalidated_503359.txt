[site]: crossvalidated
[post_id]: 503359
[parent_id]: 491630
[tags]: 
I agree with AJV's response, big idea is the linear equation is related to the log odds: $$\frac{log(y)}{log(1 - y)} = \beta_0 + \beta_1x$$ So for Interpreting the Coefficients ; SKlearn will give you the coefs and intercept as per usual after fitting a model, but the equation above is what they relate to. Thus, out of the box our logistic regressor returns log odds. From here, it isn't much work to turn odds into a probability which is what we would get solving for y. Finally, we use this probability to make the classification -- less than 0.5 we will say it is class 0, greater than 0.5 it's class 1. Sklearn gives you all of these for a point or array of points with a fit model. For your example In sklearn, you would get all these after fitting a model; for example: clf = LogisticRegression() clf.fit(X, y) clf.predict_log_proba(X) #gives log odds back clf.predict_proba(X) #converts log odds to probabilities clf.predict(X) #uses the probabilities to make predictions Now to your second point sklearn uses regularization by default. From the documentation: Note Regularization is applied by default, which is common in machine learning but not in statistics. Another advantage of regularization is that it improves numerical stability. No regularization amounts to setting C to a very high value. Thus, if you want to not worry about scaling set the C value high. With scaled features, we still won't interpret our coefficients as we would with linear regression so we aren't really in any worse situation. A bonus of scaling our features here would be that we can compare the relative importance of the features whereas we would need to do work if we hadn't scaled our data. For more on this see the example in sklearn's docs here .
