[site]: crossvalidated
[post_id]: 348515
[parent_id]: 347785
[tags]: 
To elaborate on some of the comments. Most models give a probability output--- say 0.78. If you convert this to a 1, then you have made a "decision". Accuracy, ROC, precision, recall, are all functions of decisions ; log likelihood, brier score are functions of probabilities. Often, it's best to leave the decision to the end user, but there are notable exceptions (machine translation, speech recognition (the end user is the computer), self driving cars). Accuracy Better to not. Perhaps with balanced classes. AUROC This just gives sensitivity and specificity as you vary the threshold. Applications include validation in poorly written papers on risk prediction :). Seriously, though, use it if you are interested in providing binary decisions (1 or 0) as output, not a probability estimate. This is important for example when you are guiding a self driving car that must make decisions quickly as to what is road and what is not--- and the cost of a false positive is identical to the cost of a false negative . F1 score A combination of precision and recall. Application: document retrieval. Generally document retrieval and NLP tasks like machine translation use these metrics because a decision is made (e.g., in machine translation, you want a sentence as output, not some probability distribution over sentences). Precision, in contrast to sensitivity and specificity, depends on prevalence. Therefore it is a good metric for very rare classes, like in document retrieval. logloss If you're interested in probability estimates and have a well-specified predictive model. This is the loss function when you minimize the negative log likelihood of a collection of independent Bernoulli random variables. This is related to the probability of your dataset using your estimated parameters. Application: risk score, prediction. Eg, what's the probability of disease or of defaulting on a loan. Brier score If you're interested in probability estimates, this is basically a measure of how close (using mean squared error) your estimates are numerically to the actual probabilities. This is nice because it's kind of bounded as opposed to log loss, but I have found it to be kind of insensitive.
