[site]: crossvalidated
[post_id]: 620340
[parent_id]: 
[tags]: 
Why isn't weights normalization required in SVM?

As I understand : Distance from $x_0$ to the hyperplane $a$ : $ \rho(x_0, a) = \frac{| + b|}{||w||}$ We require $ min\ | + b| = 1$ , so $min \frac{| + b|}{||w||}$ will be $\frac{1}{||w||}$ So in order to maximise distance to the closes instance we minimise $||w||$ Then we weaken and unite some restrictions to get to the hinge loss : $L(y, z) = ||w||^2 + C \sum max(0,\ 1 - y_i z)$ I implemented Linear SVM, and it works without weights normalization. Why can we still use same formula for loss and gradient, even if we can't say that $min\ \rho = \frac{1}{||w||}$ ? my implementation in python: class SVM: def __init__(self, max_iter): self.w = None self.bias_w = None self.max_iter = max_iter def fit(self, X, y): y = y.reshape(-1, 1) bias = np.ones(X.shape[0]) self.w = np.random.rand(X.shape[1], 1) self.bias_w = np.random.rand(1, 1) for i in range(self.max_iter): for j, xi in enumerate(X): #self.w /= np.min(X @ self.w + np.tile(self.bias_w, (X.shape[0],1))) #self.bias_w /= np.min(X @ self.w + np.tile(self.bias_w, (X.shape[0],1))) self.w -= 1e-4 * self._calc_grad(xi, y[j], self.w, 1e-3).reshape(-1, 1) self.bias_w -= 1e-4 * self._calc_grad_bias(xi, y[j], self.w, 1e-3) def predict(self, X): bias_w_tiles = np.tile(self.bias_w, (X.shape[0],1)) return ((X @ self.w + bias_w_tiles ) > 0 ).astype(float) def _calc_grad(self, X, y, w, C): grad = 0 distance = np.max([0, 1 - y * (X @ w + self.bias_w)]) return w if distance == 0 else w - C * y * X.reshape(-1 , 1) def _calc_grad_bias(self, X, y, w, C): grad = 0 distance = np.max([0, 1 - y * (X @ w + self.bias_w)]) return 0 if distance == 0 else self.bias_w - C * y
