[site]: crossvalidated
[post_id]: 351148
[parent_id]: 350411
[tags]: 
The basic idea behind multimodal learning techniques and especially those built on top of deep learning is that the different data modalities offer complementary information and one can profit from jointly modeling them. That is compared to using only a single modality you can obtain better results. The different modalities should be adequately different because this motivates the types of architectures shown in the tutorial. Also, the deep learning machinery offer flexible ways (convolutions or sequence models, ..) and extensible architectures to treat the modalities separately and then combine them which is promising. Your example, which is a valid one, is a more traditional approach where you perform feature extraction on a single modality (behavioral features in your case) and then feed a classifier. Had you, however, access to a different informative modality, e.g., the sounds surrounding the person who browses assuming they do provide some information from your task, you could try multimodal learning. People have been trying to take advantage of that and is also among the straight-forward baselines against a multimodal system should compare. Boosting for instance builds on similar ideas (it's not the same explicitly but just to make the point) where you iteratively train on different features and data subsets to combine weak learning into better performing learners.
