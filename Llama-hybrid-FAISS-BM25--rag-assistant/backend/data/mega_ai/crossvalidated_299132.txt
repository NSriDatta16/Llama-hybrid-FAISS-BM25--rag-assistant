[site]: crossvalidated
[post_id]: 299132
[parent_id]: 299120
[tags]: 
In multinomial regression we model odds of observing $Y=k$ for each of the $K-1$ classes relatively to the $K$-th class. So with $K=2$ the model reduces to logistic regression. Let me quote Wikipedia : One fairly simple way to arrive at the multinomial logit model is to imagine, for $K$ possible outcomes, running $K-1$ independent binary logistic regression models, in which one outcome is chosen as a "pivot" and then the other $K-1$ outcomes are separately regressed against the pivot outcome. This would proceed as follows, if outcome $K$ (the last outcome) is chosen as the pivot: $$ \begin{align} \ln \frac{\Pr(Y_i=1)}{\Pr(Y_i=K)} &= \boldsymbol\beta_1 \cdot \mathbf{X}_i \\ \ln \frac{\Pr(Y_i=2)}{\Pr(Y_i=K)} &= \boldsymbol\beta_2 \cdot \mathbf{X}_i \\ \cdots & \cdots \\ \ln \frac{\Pr(Y_i=K-1)}{\Pr(Y_i=K)} &= \boldsymbol\beta_{K-1} \cdot \mathbf{X}_i \\ \end{align} $$ (...) Using the fact that all $K$ of the probabilities must sum to one, we find: $$ \Pr(Y_i=K) = 1 - \sum_{k=1}^{K-1}{\Pr(Y_i=K)}e^{\boldsymbol\beta_k \cdot \mathbf{X}_i} \Rightarrow \Pr(Y_i=K) = \frac{1}{1 + \sum_{k=1}^{K-1} e^{\boldsymbol\beta_k \cdot \mathbf{X}_i}} $$ We can use this to find the other probabilities: $$ \begin{align} \Pr(Y_i=1) &= \frac{e^{\boldsymbol\beta_1 \cdot \mathbf{X}_i}}{1 + \sum_{k=1}^{K-1} e^{\boldsymbol\beta_k \cdot \mathbf{X}_i}} \\ \Pr(Y_i=2) &= \frac{e^{\boldsymbol\beta_2 \cdot \mathbf{X}_i}}{1 + \sum_{k=1}^{K-1} e^{\boldsymbol\beta_k \cdot \mathbf{X}_i}} \\ \cdots & \cdots \\ \Pr(Y_i=K-1) &= \frac{e^{\boldsymbol\beta_{K-1} \cdot \mathbf{X}_i}}{1 + \sum_{k=1}^{K-1} e^{\boldsymbol\beta_k \cdot \mathbf{X}_i}} \\ \end{align} $$ With logistic regression you have only $$ \ln \frac{\Pr(Y_i=1)}{\Pr(Y_i=0)} = \boldsymbol\beta \cdot \mathbf{X}_i $$ and probabilities derived from it. Logistic regression and multinomial regression with $K=2$ are the same thing . Example If you take the data from this online tutorial and run logistic regression and multinomial regression on it, you'll see exactly the same results (compare the coefficients): library(nnet) mydata
