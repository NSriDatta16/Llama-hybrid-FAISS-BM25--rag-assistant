[site]: datascience
[post_id]: 69721
[parent_id]: 69543
[tags]: 
In ranking applications of information retrieval, training data consists of queries and documents matching them together with relevance degree of each match. For example when searching in something in google, the training data may be prepared manually by human assessors (or raters, as Google calls them), who check results for some queries and determine relevance of each result. It is not feasible to check the relevance of all documents, and so typically only the top few documents, retrieved by some existing ranking models are checked. Training data is used by a learning algorithm to produce a ranking model that computes the relevance of documents for actual queries. Rank profiles can have one or two phases: Phase one should use a computationally inexpensive function to rank candidates Phase two is run on a small candidate set In short, the query selection and first phase ranking reduces the size of the computation - then machine-learned models can be used on the second-phase ranking on rerank-count documents per node. This makes the ranking scalable (see sizing): Control the second phase candidate set size Add content nodes to rank less documents per node Let's now answer your questions: 1) Let's say we have qid's in our training file. Does it mean that the optimization will be performed only on a per-query basis, all other features specified will be considered as document features and cross-query learning won't happen? It means that in order to rank all candidates it will rank only the first ones in order to be computationally less expensive. So it will only look in the first query for the extensive ranking search. 2)Let's assume that queries are represented by query features. Should we still have qid's specified in the training file or we should just list query, document and query-document features? Yes, the rank_feature query is typically used so its relevance scores are added to other scores query. Note that qid's are only used in training set https://github.com/Microsoft/LightGBM/issues/1398 Sources: - Wikipedia - Learning to rank - Vespa - Tank - Vespa - Phased Ranking - Elastic - Rank feature query
