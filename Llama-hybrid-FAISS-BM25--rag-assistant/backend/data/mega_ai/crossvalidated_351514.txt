[site]: crossvalidated
[post_id]: 351514
[parent_id]: 
[tags]: 
Usage of the term "feature vector" in Lindsay I Smith's PCA tutorial

I'm currently following Lindsay Smith's (in large parts very well written) PCA tutorial . I'm a bit confused about the usage of the term "feature vector" in this paper though. A quote from this paper is: [...] What needs to be done now is you need to form a feature vector, which is just a fancy name for a matrix of vectors. This is constructed by taking the eigenvectors that you want to keep from the list of eigenvectors, and forming a matrix with these eigenvectors in the columns. However, on Wikipedia one can read the following: In pattern recognition and machine learning, a feature vector is an n-dimensional vector of numerical features that represent some object. I'm a bit confused as I interpret Lindsay Smith's feature vector as being a matrix consisting of eigenvectors and Wikipedia's feature vector as an actual representation of each object of the input data set. My interpretation is that in the latter case one row of the feature vector could for example look as follows: Person A = {Age:30,Height=199 cm,Weight=100 kg, ...} This representation wouldn't have anything to do with eigenvectors. Is any of those two definitions wrong or do they actually mean the same thing and I just don't see it? Thanks a lot for any answers.
