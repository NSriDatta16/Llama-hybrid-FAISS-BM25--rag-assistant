[site]: datascience
[post_id]: 45312
[parent_id]: 
[tags]: 
Multioutput classification in Keras - how to get multivariate probabilities and deal with unseen classes

I'm struggling to design in Keras a deep neural network for multioutput classification model. The network works in tandem with external logic in a kind of feedback loop: in each iteration the external module generates the training set, on which the network is trained and then in next iteration the network supports the module in another round of training set generation. I'm rather at rookie+ level and I have encountered 2 problems I do not know how to deal with: how to make a network predict vectors that will not show up in training set, but still account for underlying correlations between vector components how to yield a multivariate distribution on multioutput space. To be more specific, the "ground truth" of the model is $N$ -length vector of binary labels: $y \in \{ 0, 1\}^{N} $ . This represents some yes/no $N$ -dimensional decision and is generated by the external module. The key issue is the yes/no components are highly correlated, $N$ maybe large and the ground truth is not "deterministic" - i.e. it's just some approximation that is expected to get better over the feedback learning process described above. So far I have designed a simple classification network based on Keras sequence model with $N$ output layer units. Restating my problems in more specific way: ad 1) there are $2^{N}$ possible output vectors (" metaclasses "). As $N$ is large, almost surely not all of them will appear in my training set. But I want my network to be flexible enough to be able to come up with unseen metaclasses but still take care of high correlations between single yes/no components. ad 2) I would like the network to output a few (say three) most probable vectors. In other words, I would like it to yield a (obviously some cut-off) probability distribution over vectors.
