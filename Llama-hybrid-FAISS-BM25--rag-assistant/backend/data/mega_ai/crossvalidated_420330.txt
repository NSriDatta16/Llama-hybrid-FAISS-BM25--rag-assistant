[site]: crossvalidated
[post_id]: 420330
[parent_id]: 420231
[tags]: 
Modern initialization methods are designed with strong assumptions about the scale of the input data, usually inputs have 0 mean and unit variance or that inputs are in the unit interval. If we apply scaling so that inputs are $X_{ij}\in [0,1]$ , then activations for the first layer during the first iteration are $$ X\theta^{(1)} + \beta^{(1)} $$ and at convergence are $$ X\theta^{(n)} + \beta^{(n)}, $$ where the weights are $\theta$ , the bias is $\beta$ . Network initialization draws values from some specific distribution, usually concentrated in a narrow interval around 0 . If you don't apply scaling, then activations for the first layer during the first iteration are $$ 255\cdot X\theta^{(1)} + \beta^{(1)} $$ So the effect of multiplying by the weights is obviously 255 times as large. At convergence, the model will arrive at the same loss as the scaled case; however, it will take longer to get there since the non-scaled model has initial weights that are 255 times too large. Larger weights are close to saturating the softmax function, where the gradient is not very steep, so it will take a long time for the weights to update enough to compensate for the lack of scale. The video is talking about scaling in the context of features that have different scales. That poses a different kind of conditioning problem for the optimizer. See: In Machine learning, how does normalization help in convergence of gradient descent?
