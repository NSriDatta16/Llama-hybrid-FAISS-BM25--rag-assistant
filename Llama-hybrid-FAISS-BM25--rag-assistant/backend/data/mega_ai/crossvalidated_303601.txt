[site]: crossvalidated
[post_id]: 303601
[parent_id]: 
[tags]: 
Perceptron algorithm - is this a typo or did something change in this equation?

I'm currently reading Python Machine Learning (Amazon link) by Sebastian Raschka. Here discussing Frank Rosenblatt's perceptron algorithm: More formally, we can pose this problem as a binary classification task where we refer to our two classes as $1$ (positive class) and $-1$ (negative class) for simplicity. We can then define an activation function $\phi(z)$ that takes a linear combination of certain input values $\mathbf{x}$ and a corresponding weight vector $\mathbf{w}$, where $z$ is the so-called net input ($z = w_{1}x_{1} + \dots + w_{m}x_{m}$): $$ \mathbf{w} = \begin{bmatrix} w_{1} \\ \vdots \\ w_{m} \end{bmatrix}, \ \mathbf{x} = \begin{bmatrix} x_{1} \\ \vdots \\ x_{m} \end{bmatrix} $$ Now, if the activation of a particular sample $x^{(i)}$, that is, the output of $\phi(z)$, is greater than a defined threshold $\theta$, we predict class $1$ and class $-1$, otherwise, in the perceptron algorithm, the activation function $\phi(\cdot)$ is a simple unit step function , which is sometimes called the Heaviside step function : $$ \phi(z) = \begin{cases} 1 && \text{if } z \geq \theta \\ -1 && \text{otherwise} \end{cases} $$ For simplicity, we can bring the threshold $\theta$ to the left side of the equation and define a weight-zero as $w_{0} = -\theta$ and $x_{0} = 1$, so that we write $\mathbf{z}$ in a more compact form $z = w_{0}x_{0} + w_{1}x_{1} + \dots + w_{m}x_{m} = \mathbf{w}^{T}\mathbf{x}$ and $\phi(z) = \begin{cases} 1 && \text{if } z \geq \theta \\ -1 && \text{otherwise} \end{cases}$. If we define a weight-zero as the author does and use the form $z = \mathbf{w}^{T}\mathbf{x}$, shouldn't the activation function become $$ \phi(z) = \begin{cases} 1 && \text{if } z \geq 0 \\ -1 && \text{otherwise}\end{cases}? $$ If not, I don't understand what really happened with the "more compact form", adding a term to $z$ and not doing anything with the activation function. I checked the book errata and didn't see this which is what is making me unsure if I'm completely missing something here.
