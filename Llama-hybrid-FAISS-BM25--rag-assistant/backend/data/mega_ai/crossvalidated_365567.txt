[site]: crossvalidated
[post_id]: 365567
[parent_id]: 365457
[tags]: 
As mentioned in shimao’s answer, certain metric and arithmetical properties of the vector space of the embedding reflect certain semantic relations among the words. But I think believing that all of one’s, often very subjective, perceived ‘intuitive’ properties of words will be reflected in any embedding produced by one of the commonly used word embedding algorithms reflects a misunderstanding of what the algorithm is doing. Let’s look at two common word2vec models: continuous bag-of-words (CBOW) and skip-gram. These sort of perform the reverse prediction from each other in order to generate the embeddings - CBOW takes a group of context words, where the length of the context window is a hyperparameter, and tries to generate a probability distribution to predict the center word; skip-gram does the opposite and tries to predict a neighboring word given a center word. One effect of this difference is that, compared to skip-gram, CBOW smoothes over distributional characteristics, i.e. sudden changes in the distribution get somewhat ‘leveled.’ As each training sample utilizes an unordered ‘bag’ model of the context words where only word counts matter,center words that occur with slightly modified context bag-of-words will generate similar characteristics for the distribution and be embedded close to each other. Similarly, the context ‘bags’ for each word will get ‘averaged’ in forming the representation of each word, Words that occur infrequently may get non informative embeddings. Skip-gram, on the other hand, will create multiple center-context pairs for each center word, the number of which depends on the context length hyperparameter, with farther context words given lower weight. This takes more time, but may yield better results for infrequent words as each word generates multiple sample pairs, while CBOW produces one sample point for each context bag/center word pair. Both of these are distributional representations, meaning that all the units in the model are correlated - you can’t change one embedding vector without changing all the others when the model is trained. The above points to how model performance is influenced by the choice of hyperparameters like embedding dimension and context window length, as well as the size and representational quality of the corpus of documents used for training. The word embeddings that would be produced from training on a set of advanced biology textbooks, people magazine issues from the last 10 years, and scraping YouTube comments will be vastly different. A certain corpus may demonstrate properties in the generated embedding such as sexist associations: https://arxiv.org/abs/1607.06520 . Changing the hyperparaemters or diluting the corpus with a larger collection of documents may amplify, or even reverse, such properties of the embedding. TL;DR While word embeddings, when properly tuned and trained, may display many properties conjectured to be true for language by the distributional hypothesis of J.R. Firth, this hypothesis does not capture all the nuances of language such as dialect/subgroup differences or wordplay and atypical syntactic/semantic structures
