[site]: crossvalidated
[post_id]: 399090
[parent_id]: 333099
[tags]: 
I think @stephen & @hugh have made it over-complicated, let's make it simple. A hidden unit, in general, has an operation Activation(W*X+b) . Therefore, if you think carefully, *A hidden unit in CONV layer is an operation that uses "filter_volume a.k.a volume of randomly initialized weights" in general. More loosely, you can say filter/filter volume (f *f n_c_prev) corresponds to single neuron/hidden unit in a CONV layer. Particularly, in your example, you have (3*3* 3 ) filter volume that you will convolve (element-wise multiply & add--> Bias--> Activation) over your (9*9* 3 ) input. (f* f* n_c_prev ) is a filter in general, with n_c_prev as the number of the input channel, 49 (7*7) times for 1 filter, since you have 5 of such kind, you will do the same convolve operation just 4 times more, therefore, 49*5=245 is the total number of convolution operation you are going to perform on the input using you 5 differently initialized filter volumes! Therefore, the number of the hidden unit be just 5 each of which is capacitated to use (f *f *n_c_prev) weights/vol. Thinking more abstractly, a hidden unit in layer-1, will see only a relatively small portion of the neural network. And so if you visualize, if you plot what activated unit's activation, it makes sense to plot just small image patches, because that's all of the images that particular unit sees. Example Now you pick a different hidden unit in layer-1 and do the same thing Therefore, now you have 9 different representative neurons and each of them finds the nine(3*3) image patches that maximizes the unit's activation. Now, if you deeper into the network, a hidden layer over there, a hidden unit sees a larger patch/region the image( larger receptive field! ) and able to detect many complex patterns such as More about it you can read here " visualizing and understanding convolutional networks "
