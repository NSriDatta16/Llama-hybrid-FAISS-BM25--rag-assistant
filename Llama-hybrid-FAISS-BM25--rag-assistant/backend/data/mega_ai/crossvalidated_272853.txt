[site]: crossvalidated
[post_id]: 272853
[parent_id]: 
[tags]: 
Logistic regression with censored labels

Given examples with features, each labelled 0 or 1, I can train a logistic regression model to predict the probability of the label being 1 given the features. What can I do if I have just negative and unlabelled examples? I'd be happy just to rank examples (ie determine which is more likely to be positive). This seems related to censoring , but I only found discussion of continuous variables. More formally, denote the overall population of examples as $X=X_0\sqcup X_1$, where $X_0$ and $X_1$ are negative and positive examples respectively. Suppose I can draw samples from $X_0$, and I can draw (unlabelled) samples from $X$. Let $F$ denote examples with some particular feature values. I'm interested in $$ p(X_1|F)=1-\frac{p(F|X_0)}{p(F)}p(X_0). $$ Actually I don't think I have enough information to determine $p(X_0)$, but since I only want to rank examples it's sufficient to estimate $$ \frac{p(F|X_0)}{p(F)}, $$ and both these terms could be estimated from my samples. In fact if I drew $n$ samples from $X_0$ and $m$ samples from $X$, pretended the latter were all positive, and trained a logistic regression model, its prediction on $F$ should be $$ \frac{p(F)m}{p(F)m+p(F|X_0)n}=\frac{m}{m+np(F|X_0)/p(F)}, $$ so it produces the desired ranking. However I'm not sure how robust this is (and it's not clear how the ratio $n/m$ should be chosen).
