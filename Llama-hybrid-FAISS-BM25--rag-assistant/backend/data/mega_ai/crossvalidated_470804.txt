[site]: crossvalidated
[post_id]: 470804
[parent_id]: 
[tags]: 
What is the difference between position embedding vs positional encoding in BERT?

This post about the Transformer introduced the concept of "Positional Encoding", while at the same time, the BERT paper mentioned "Position Embedding" as an input to BERT (e.g. in Figure 2). First, are they different? Do they actually co-exist? If so (I guess so), what kind of difference do they have and why is it significant?
