[site]: datascience
[post_id]: 120592
[parent_id]: 
[tags]: 
Overfitting still exists using different techniques on voice classification

I have 986 voice signals which have been collected by our team. The data set includes 745 healthy and 150 unhealthy voice signals. I split the data into 70% training and 20% validation and 10% test (unseen) data. Then, I oversampled the train and validation set. So, we have 1042 training samples (521 healthy and 521 unhealthy) and 298 validation samples (149 healthy and 149 unhealthy). I am applying augmentation on the fly on each batch with 64 batch size and on both training and validation sets. To work with these signals and CNN, I converted the signals into Melspectrograms. Now, when I apply a five-layer CNN, there is an overfitting and the model can not generalize well on validation data. What I did so far: I just applied augmentation on training set and I did not applied augmentation on validation set, the problem still exists. I used shallow CNN, but the problem still exists. I used Regularizer (l2), Dropout, Batch normalization, different optimizer (Adam, SGD,…), different learning rate, label smoothing, early stopping, scheduler, …but the problem still exists. I used different augmentation techniques/ libraries, but the problem exists. I extracted the best informative chunk (1 second) from the 5 seconds voice signals. I mean I removed background noise and silent, but the problem still exists. I applied PCA to reduce the number of mels in mel-spectrograms, but the problem still exists. I applied transfer learning like (Xception, Resnet50, Inception), but the problem still exists. I tried to get optimized hyperparameters in Melspectrograms like (n-mels, n-ffts, hop-length) by defining cost function using MSE or Euclidean distance, but it was not helpful!! I added librosa.feature.delta as dynamic feature , but it was not helpful!! 10.Even the cross validation was not helpful!! But, based on my first experiments, when I prepared the data before fitting any model, I mean when I oversample and augment the data offline(which means to add more data to train or validation set) instead of applying augmentation on the fly (which doesn’t add any data, just make more diverse data in each batch), the results are acceptable. Based on this explanation, I want to have a good fit and good performance using augmentation on the fly. I would appreciate it if anyone could help me.
