[site]: crossvalidated
[post_id]: 357346
[parent_id]: 
[tags]: 
My Neural Network gives a wrong prediction when I specify more nodes

I recently got interested in neural nets. After reading a bunch I tried to make one in python using numpy. I fed in some sample input and output data. When I train the neural net and and then ask it to predict the output from the same input that I used to train it, it gives the correct output when I specify the number of nodes as 2 or 3. But when I increase that to 10, it gives me the wrong output. It seems like an error in my code which leads to this, but I cannot figure out what it is. The full code can be found here: https://github.com/HariharanJayashankar/simple_neural_net/blob/master/simple_neural_net.py . But the relevant training code is: import numpy as np # Definitining functions def sigmoid(x): # pass a normal function here to get its sigmoid transformation return 1 / (1 + np.exp(-x)) def sigmoid_deriv(x): # pass a sigmoid function in here to get its derivative return sigmoid(x) * sigmoid(1 - x) def run(x, O, W_1, W_2, b_1, b_2, learning_rate): # x = input; W_1 = matrix of weights; Y = output # forward prop A_0 = x.T Z_1 = np.dot(W_1, A_0) + b_1 A_1 = sigmoid(Z_1) Z_2 = np.dot(W_2, A_1) + b_2 Y = (sigmoid(Z_2)).T # backward prop # Following MSE as our loss function e = sigmoid(Z_2) - O.T # overall error for the network # error signals (calculated according to MSE) d_2 = e d_1 = np.dot(W_2.T, d_2) * sigmoid_deriv(Z_1) # Update rule up_b1 = learning_rate * d_1 up_b2 = learning_rate * d_2 up_W_1 = learning_rate * np.dot(d_1, A_0.T) up_W_2 = learning_rate * np.dot(d_2, A_1.T) return Y, (up_W_1, up_W_2, up_b1, up_b2) def train(x, O, W_1, W_2, b_1, b_2, n_iter, learning_rate): for i in range(n_iter): Y, upd = run(x, O, W_1, W_2, b_1, b_2, learning_rate) W_1 = - upd[0] W_2 = - upd[1] b_1 = - upd[2] b_2 = - upd[3] if i % 10 == 0: print(i) print((1 / np.size(O, 0)) * np.dot((Y - O).T, (Y - O))) return (W_1, W_2, b_1, b_2) The number of nodes comes into play here: w_1 = np.random.rand(n_nodes, np.size(inp, 1)) w_2 = np.random.rand(np.size(out, 1), n_nodes) b1 = np.random.rand(n_nodes, np.size(inp, 0)) b2 = np.random.rand(np.size(out, 1), np.size(out, 0)) Sorry if this was too verbose but I couldn't find another person who posted a similar problem (although there were problems of finding the optimal number of nodes). My problem is that when I put in too many nodes (10 for example), the net fails to predict the output from the training example itself.
