[site]: crossvalidated
[post_id]: 453527
[parent_id]: 
[tags]: 
Performance metric for Unsupervised LSTM Anomaly Detection model

I am working on an unsupervised time-series anomaly detection project. My data consists of 10 variables with some various degrees of correlation between them and around 50 values for each variable. So the dataframe/matrix is of size: 50x10 The model I am using is a single layer univariable LSTM with an architecture I can share if required. The activation function is ReLU , optimizer adam and mae as metrics. I say univariable because at the moment, I am training each model for each variable itself, without taking into consideration the other ones. So I'm building 10 different models, one for each variable. The anomaly detection right now is made by training the model and making it reconstruct the time series, and measuring the squared distance for each point, standarizing the mse values and then simply flagging as anomaly those points where this squared distance is above a user defined threshold, or above a certain percentile of the squared distance values. The problem I am facing is I can't seem to find a proper way to analyze/evaluate the performance of my model, becuase if I try to minimize the mse it doesn't make sense to me, because this would cause the model to (in an extreme case) overfit and match exactly the true values, hence there will be no anomalies. On the other hand, if I just plot a straight line with the mean then any seasonal trend will not get correctly identified hence anomalies will be flagged incorrectly. Being the model unsupervised, there is obviously no accuracy metric to try and fit our model. Is there a metric or way of evaluating the performance of an unsupervised anomaly detection model without incurring in problems mentioned above? Here is an output of model fitted for two variables. Is there metric for performance which I can use to compare models and have an objective conclusion on whether a model is performing better than another one? *You see the x-aixs going over 50, because I replicated the original matrix to provide more data to the LSTM-model hoping it would generalize a bit better
