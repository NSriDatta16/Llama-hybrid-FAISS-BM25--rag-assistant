[site]: crossvalidated
[post_id]: 183240
[parent_id]: 183197
[tags]: 
k-means is useless for "big data" there is no "big data" suitable for k-means. The only big data you have is unstructured text and video. K-means cannot be used on such data. k-means only works on low-dimensional, continuous numeric, dense data. So if you have say 10 dimensions (at this point, k-means barely works usually) with double precision, you need 80 bytes per object. You can fit billions of objects into a single machine and it will outperform your cluster. k-means does not benefit from big data. It computes averages. You do not find significantly better averages by adding more data. Running k-means on a just-large-enough sample yields virtually the same result . k-means is an unreliable, crude heuristic that needs to be used with care. You can't just throw data at k-means, run it, and expect anything useful out of it. It is too sensitive to preprocessing, the k parameter, feature selection, feature engineering and so on. Whenever you run k-means, you need to carefully validate the outcome. You cannot automate this. So how would you validate your "big data" outcome ? You will make some error in preprocessing or using k-means and you will never know. Of course every new big data machine learning toy adds "k-means clustering" to their feature list, because it is about the easiest algorithm you can find (little more than computing a single mean). But I have never seen an example where k-means was used for "big data" with success, and I have never seen a good cluster implementation either - only crappy versions copied from the average textbook.
