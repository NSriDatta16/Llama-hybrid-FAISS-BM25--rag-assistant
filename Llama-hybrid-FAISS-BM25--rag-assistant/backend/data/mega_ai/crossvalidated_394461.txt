[site]: crossvalidated
[post_id]: 394461
[parent_id]: 152907
[tags]: 
If all you are going to do is train a model with default settings on the raw or minimally preprocessed dataset (e.g. one-hot encoding and/or removing NAs), you don't need a separate test set, you can simply train on your train set and test on your validation set, or even better, train on the entire set using cross-validation to estimate your performance. However, as soon as your knowledge about the data causes you to make any changes from your original strategy, you have now "tainted" your result. Some examples include: Model choice: You tested logistic, lasso, random forest, XGBoost, and support vector machines and choose the best model Parameter tuning: You tuned an XGBoost to find the optimal hyperparameters Feature selection: You used backward selection, genetic algorithm, boruta, etc. to choose an optimal subset of features to include in your model Missing imputation: You imputed missing variables with the mean, or with a simple model based on the other variables Feature transformation: You centered and scaled your numeric variables to replace them with a z-score (number of standard deviations from the mean) In all of the above cases, using a single holdout set, or even cross-validation, is not going to give you a realistic estimate of real-world performance because you are using information you won't have on future data in your decision. Instead, you are cherry-picking the best model, the best hyperparameters, the best feature set, etc. for your data, and you are likely to be slightly "overfitting" you strategy to your data. To get an honest estimate of real-world performance, you need to score it on data that didn't enter into the decision process at all, hence the common practice of using an independent test set separate from your training (modeling) and validation (picking a model, features, hyperparameters, etc.) set. As an alternative to holding out a test set, you can instead use a technique called nested cross-validation. This requires you to code up your entire modeling strategy (transformation, imputation, feature selection, model selection, hyperparameter tuning) as a non-parametric function and then perform cross-validation on that entire function as if it were simply a model fit function. This is difficult to do in most ML packages, but can be implemented quite easily in R with the mlr package by using wrappers to define your training strategy and then resampling your wrapped learner: https://mlr.mlr-org.com/articles/tutorial/nested_resampling.html
