[site]: crossvalidated
[post_id]: 377747
[parent_id]: 
[tags]: 
Linear discriminant analysis and logistic regression

I have found in the script of the Machine Learning lecture CS229 by Andrew Ng at Stanford University, that he claims that (at least in the case of only two classes) the posterior of the linear discriminant analysis (LDA) is also a logistic function. Given the scenario: $y$ ~ $Bernoulli(\pi)$ $x|y=0$ ~ $N(\mu_0, \Sigma)$ $x|y=1$ ~ $N(\mu_1, \Sigma)$ , he states that if we consider $p(y=1|x;\pi, \Sigma, \mu_0, \mu_1)$ to be a function of $x$ , we find: $$p(y=1|x;\pi, \Sigma, \mu_0, \mu_1) = \frac{1}{1+\exp(-\theta^Tx)}$$ where $\theta$ is some appropriate function of $\pi, \Sigma, \mu_0, \mu_1$ (that he unfortunately does not state precisely). This, of course, is the definition of the logistic regression (LR) scenario as well. However, he states that the decision boundaries of LDA and LR generally will still look differently, when being trained on the same data set. My questions now are: 1.) Does this formula for the posterior only hold in the case of two classes, or can we write the posterior of LDA as a logistic function also for more classes? 2.) How does the function $\theta$ look like? 3.) How can LDA and LR have different decision boundaries, even if they have the same posterior function? 4.) How can we derive the logistic function from the LDA equations? So can someone sketch the rough derivation? 5.) I am a bit confused about the sentence that we have to consider " $p(y=1|x;\pi, \Sigma, \mu_0, \mu_1)$ to be a function of $x$ " to derive the logistic function from LDA. Dont we always assume the posterior to be a function of $x$ anyway?
