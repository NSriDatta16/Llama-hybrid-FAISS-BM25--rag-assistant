[site]: crossvalidated
[post_id]: 452162
[parent_id]: 
[tags]: 
Finding the gradient $\nabla$ of the logistic regression cost function

I want to use vector calculus to derive the gradient $\nabla_wJ(w)$ of the logistic regression cost function $J(w) = -\textbf{y}\cdot ln\textbf{ s} - (\mathbf{1} - \textbf{y}) \cdot ln( \mathbf{1} - \textbf{s})$ , where $\textbf{s} = s(\textbf{Xw})$ is shorthand for the logistic function. I know my attempt is incorrect, but am not sure where I'm going wrong: $$\nabla_wJ(w) = -\nabla_w[\textbf{y} \cdot ln\textbf{ s}] - \nabla_w[(\mathbf{1}-\textbf{y})\cdot ln(\mathbf{1} - \textbf{s})] = -([\nabla_w\textbf{y}] ln( s(\textbf{Xw}) + [\nabla_wln(s(\textbf{Xw}))]\textbf{y}) - ([\nabla_w(\mathbf{1} - \textbf{y})]ln(\mathbf{1} - s(\textbf{Xw})) + [\nabla_wln(\mathbf{1} - s(\textbf{Xw}))](\mathbf{1} - \textbf{y})) = \\ -[\nabla_wln(s(\textbf{Xw}))]\textbf{y} - [\nabla_wln(\mathbf{1} - s(\textbf{Xw}))](\mathbf{1} - \textbf{y}).$$ Now, since $\nabla_wln(s(\textbf{Xw})) = \mathbf{1} - s(\textbf{Xw})$ , we can simplify and get: $$-(\mathbf{1} - s(\textbf{Xw}))\textbf{y} + s(\textbf{Xw})(\mathbf{1} - \textbf{y}) = s(\textbf{Xw}) - \textbf{y}.$$ I know $\textbf{X}$ should be here somewhere, though, so something is wrong. I need to compute this gradient $\textbf{only}$ in terms of matrix-vector expressions, and so sums and extracting individual terms will not help. Thanks for any assistance in advance!
