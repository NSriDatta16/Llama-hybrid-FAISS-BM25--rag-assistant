[site]: crossvalidated
[post_id]: 539953
[parent_id]: 
[tags]: 
testing for significance of a low-dimensional structure (e.g., collinearity)

I have p dependent variables $y_i (i=1,\cdots,p)$ sampled n times and a single independent variable $x$ . All my dependent variables can be modeled by a linear regression on the independent variable as $$y_i = b_i + m_i x.$$ Of course all dependent variables are directly interrelated by a linear regression. However, my hypothesis goes one step further and is interested to test if these variables are all linearly dependent (or co-linear if you will) without an intercept. In other words, there exists some constants $\alpha_i$ and $Y^\star \in \mathbb{R}^n$ satisfying $$ \alpha_i y_i = y^\star \; \forall i=1,...,p.$$ Some thoughts that I have : To recover the underlying low-dimensional (1D) structure, I'm applying PCA without centering to create the low-rank approximate matrix from the 1st principle component. This is giving me an explained variance. But how can I generate null distribution of explained variances for this? Let's say I also run a PCA with centering to see the effect of having an intercept. Is there a way to test the explained variances in both cases (with our without centering) are different or not? Maybe, with a lack of F-test by adjusting for the additional DOF of the model gained with including intercepts via centering. Along similar lines, I thought of simulating a null distribution by adding a random intercept to each of the dependent variables, thus preserving their association with the independent variable while perturbing their collinearity. However, I am not sure if that's a good idea or not. I don't wanna create a spurious null distribution by adding very large intercept. Alternatively, I can perhaps do a pairwise regression fit between variables and test for the significance of the intercept? Can I take a bootstrapping approach? Since my hypothesis can be rephrased as data matrix having rank 1, any one of my dependent variables should be written as a linear combination of others. I was wondering if I could construct a superior regression model with new dependent and independent variables as $$ Y = [y_1^T, y_2^T, ..., y_p^T]^T \text{ stacking dependent variables} \\ X = \begin{bmatrix}y_2 & y_3 & \cdots & y_p \\ y_1 & y_3 & \cdots & y_p \\ \cdots \\ y_1 & y_2 & \cdots & y_{p-1}\end{bmatrix} \text{ each row block missing one dependent variable}$$ This can be used to maybe test if the regression intercept is zero or not. Any help is appreciated greatly.
