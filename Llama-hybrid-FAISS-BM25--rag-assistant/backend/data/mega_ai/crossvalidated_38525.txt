[site]: crossvalidated
[post_id]: 38525
[parent_id]: 38471
[tags]: 
There are some nice comments already made here, but I'll throw in my 2 cents. I'll preface this all by saying that I'm assuming we're talking about a situation where using the traditional "canned" techniques will damage the substantive conclusions reached by the analysis. If that's not the case, then I think that sometimes doing an overly simplistic analysis is excusable both for brevity and for ease of understanding when the target audience are laymen. Is it really such a crime to assume independence when the intraclass correlation is .02 or to assume linearity when the truth is $\log(x); \ x \in (1,2)? \ $ I'd say no. In my career I do a lot of interdisciplinary research and has lead me to work with closely with substance abuse researchers, epidemiologists, biologists, criminologists and physicians at various times. This typically involved analysis of data where the usual "canned" approaches would fail for various reason (e.g. some combination of biased sampling and clustered, longitudinally and/or spatially indexed data). I also spent a couple years consulting part time in graduate school, where I worked with people from a large variety of fields. So, I've had to think about this a lot. My experience is that the most important thing is to explain why the usual canned approaches are inappropriate and appeal to the person's desire to do "good science". No respectable researcher wants to publish something that is blatantly misleading in its conclusions because of inappropriate statistical analysis. I've never encountered someone who says something along the lines of "I don't care whether the analysis is correct or not, I just want to get this published" although I'm sure such people exist - my response there would be to end the professional relationship if at all possible. As the statistician, it's my reputation that could be damaged if someone who actually knows what they're talking about happens to read the paper. I admit that it can be challenging to convince someone that a particular analysis is inappropriate, but I think that as statisticians we should (a) have the knowledge necessary to know exactly what can go wrong with the "canned" approach and (b) have the ability to explain it is a reasonably comprehensible way. Unless you're working as a statistics or math professor, a part of your job is going to be to work with non-statisticians (and even sometimes if you are a stat/math prof). Regarding (a) , if the statistician doesn't have this knowledge, why would they be discouraging the canned approach? If the statistician is saying "use a random effects models" but can't explain why assuming independence is a problem, then aren't they guilty of giving in to dogma in the same way the client is? Any reviewer, statistician or not, can make pedantic critiques of a statistical modeling approach because, let's face it - all models are wrong. But, it requires expertise to know exactly what could go wrong. Regarding (b) , I've found that graphical depictions of what could go wrong typically "hit home" the most. Examples: In the example given by Peter about categorizing continuous data, the best way to show why this is a bad idea is to graph the data in its continuous form and compare it with its categorical form. For example, if you're making your response variable binary then plot the continuous variable vs. $x$, and, if it doesn't look an awful lot like a step function, then you know the discretization lost valuable information. If this difference isn't drastic or resulting in any changes in the substantive conclusions, you could also see this from the plot. When the proposed "form" of the model (e.g. linear) is inappropriate. For example, if the regression function "plateaus" like $y = x$ for $x \in (0,1)$ but $y = 1$ for $x > 1$ then a linear fit's slope will be too shallow and, depending on the data, this could push the $p$-value below significance despite there being an obvious relationship between $x$ and $y$. Another common situation (also mentioned by Peter) is explaining why assuming independence is a bad idea. For example, you can show with a plot that positive autocorrelation will typically produce data that is more "clustered" and the variance will be underestimate for that reason, giving some intuition of why the naive standard errors tend to be too small. Or, you could also plot the data with the fitted curve that assumes independence and one can visually see how the clusters influence the fit (effectively lowering the sample size) in a way that is not present in independent data. There are a million other examples but I'm working with space/time constraints here :) When pictures simply won't do for whatever reason (e.g. showing why one approach is underpowered) then simulation examples are also an option that I've employed from time to time.
