[site]: datascience
[post_id]: 102123
[parent_id]: 102090
[tags]: 
I fully agree with @Erwan that this is a very bad idea. All performance estimates based on a finite sample of data will have some amount of variation caused by sampling from an underlying population, but this is just "noise". In terms of test-training splits, if one test training split gives you better results than another, it probably means that by chance most of the "easy" to classify examples happened to end up in the test set and most of the difficult to classify ones end up in the training set. So it will give an optimistic estimate of the model's true performance. If you collect a new test set (or observe the classifier in operational use), over time the proportion of easy and difficult to classify patterns will be different to those in the "best" partition and give a lower (less biased) accuracy. Here is a simple thought-experiment to demonstrate that this will produce an optimistically biased performance estimate. Consider a completely random learning task, where we are trying to predict the result of flipping a fair coin (the target variable) and we have (say) ten random input attributes, that are also generated by flipping a fair coin. Now in this case, the classifier you construct on the training partition can have no possible skill in predicting the targets in the test partition, because they are completely random and have no causal relationship to any of the inputs. Say our dataset has ten patterns where the target is a head and ten where the target is a tail. We then randomly partition the data, say 100 times and train and test a classifier each time, each containing half of the data. Now the error rate on the test partition will be on average 0.5 (as it is predicting a random coin flip), but the error rate won't be 0.5 in every partition - in some cases, by random chance it will be higher than 0.5 and in some it will be lower. It is a random process, so in some cases the output of the random classifier learned on the training partition will be better correlated with the test labels than it will be in other partitions. However this variation is just random noise, so if you choose the partition with the highest accuracy, it will suggest the classifier has some actual skill in predicting a random coin flip, but we know that cannot possibly be the case. The apparent skill is spurious, and is caused by using a biased performance estimate. I used a similar example (with code) on the stats SE here to explain why we can easily get a biased performance estimate if we perform cross-validation based feature selection and then try to use that cross-validation as an estimate of performance (it will be highly biased). It may help to read that one first. Essentially, we shouldn't make any choices about the model or it's evaluation after looking at the test data if we want to have an unbiased performance estimate. I wrote a paper about this some time ago, because it is a very common error with model selection (e.g. optimising hyper-parameters): Gavin C. Cawley and Nicola L. C. Talbot, "On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation", Journal of Machine Learning Research, 11(70):2079âˆ’2107, 2010 ( www )
