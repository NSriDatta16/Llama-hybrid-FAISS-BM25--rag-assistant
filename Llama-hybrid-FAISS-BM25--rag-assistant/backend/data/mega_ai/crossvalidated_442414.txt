[site]: crossvalidated
[post_id]: 442414
[parent_id]: 442095
[tags]: 
With orthogonal polynomial terms as predictors in linear regression (orthogonality is enforced by the poly() function in the formula), the lack of correlations between predictors means that when you choose to remove a high-order term from the model you don't face the problem with omitted-variable bias that occurs when you remove a predictor associated both with outcome and with the included predictors. So in your particular case you could start from the highest-order term and remove sequentially all those that don't pass your criterion of statistical significance without biasing the maintained coefficients. Once you get down to a "significant" term you stop: at the 5th-order term in this case. Note, however, that stepwise selection is generally not a good idea even for linear regression. That approach doesn't directly correct p -values or confidence intervals for the fact that you used the data to choose the model, and the selected model often doesn't generalize well to new samples from the same population. And in other types of regressions (e.g., logistic regression, Cox survival models) lack of correlation of a removed predictor with the included predictors doesn't even protect against omitted-variable bias . Finally, for curve fitting you almost always should use a spline fit instead of a polynomial .
