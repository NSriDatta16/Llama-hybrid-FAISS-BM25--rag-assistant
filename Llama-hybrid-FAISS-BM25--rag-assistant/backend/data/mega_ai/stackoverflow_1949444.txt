[site]: stackoverflow
[post_id]: 1949444
[parent_id]: 1941712
[tags]: 
Let me build off Roger Pate's answer . If memory is an issue, I'd suggest instead of using the strings as the keys to the dictionary, you could use a hashed value of the string. This would save the cost of the storing the extra copy of the strings as the keys (at worst, 20 times the storage of an individual "word"). import collections def hashed_slices(seq, length, hasher=None): unique = collections.defaultdict(list) for start in xrange(len(seq) - length + 1): unique[hasher(seq[start:start+length])].append(start) return unique (If you really want to get fancy, you can use a rolling hash , though you'll need to change the function.) Now, we can combine all the hashes : unique = [] # Unique words in first string # create a dictionary of hash values -> word index -> start position hashed_starts = [hashed_slices(word, 20, hashing_fcn) for word in words] all_hashed = collections.defaultdict(dict) for i, hashed in enumerate(hashed_starts) : for h, starts in hashed.iteritems() : # We only care about the first word if h in hashed_starts[0] : all_hashed[h][i]=starts # Now check all hashes for starts_by_word in all_hashed.itervalues() : if len(starts_by_word) == 1 : # if there's only one word for the hash, it's obviously valid unique.extend(words[0][i:i+20] for i in starts_by_word.values()) else : # we might have a hash collision candidates = {} for word_idx, starts in starts_by_word.iteritems() : candidates[word_idx] = set(words[word_idx][j:j+20] for j in starts) # Now go that we have the candidate slices, find the unique ones valid = candidates[0] for word_idx, candidate_set in candidates.iteritems() : if word_idx != 0 : valid -= candidate_set unique.extend(valid) (I tried extending it to do all three. It's possible, but the complications would detract from the algorithm.) Be warned, I haven't tested this. Also, there's probably a lot you can do to simplify the code, but the algorithm makes sense. The hard part is choosing the hash. Too many collisions and you'll won't gain anything. Too few and you'll hit the memory problems. If you are dealing with just DNA base codes, you can hash the 20-character string to a 40-bit number, and still have no collisions. So the slices will take up nearly a fourth of the memory. That would save roughly 250 MB of memory in Roger Pate's answer. The code is still O(N^2), but the constant should be much lower.
