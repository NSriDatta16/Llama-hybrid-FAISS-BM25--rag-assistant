[site]: stackoverflow
[post_id]: 2040653
[parent_id]: 2015947
[tags]: 
As I said in What is the fascination with code metrics? , metrics include: different populations , meaning the scope of interest is not the same for developer or for manager trends meaning any metrics in itself is meaningless without its associated trend, in order to take the decision to act upon it or to ignore it. We are using a tool able to provide: lots of micro-level metrics (interesting for developers), with trends. lots of rules with multi-level (UI, Data, Code) static analysis capabilities lots of aggregations rules (meaning those vast number of metrics are condensed in several domains of interests, adequate for higher level of populations) The result is an analysis which can be drilled-down, from high level aggregation domains (security, architecture, practices, documentation, ...) all the way down to some line of code. The current feedback is: project managers can get defensive very quickly when some rules are not respected and make their global note significantly lower. Each study has to be re-tailored to respect each project quirks. The benefit is the definition of a contract where exceptions are acknowledged but rules to be respected are defined. higher levels (IT department, stakeholder) use the global notes just as one element of their evaluation of the progress made. They will actually look more closely at other elements based on delivery cycles: how often are we able to iterate and put an application into production?, how many errors did we had to solve before that release? (in term of merges, or in term of pre-production environment not correctly setup), what immediate feedbacks are generated by a new release of an application? So: which metrics are useful to which stakeholders, and at what level of aggregation At high level: the (static analysis) metrics are actually the result of low-level metric aggregations, and organized by domains. Other metrics (more " operational-oriented ", based on the release cycle of the application, and not just on the static analysis of the code) are taken into account The actual ROI is achieved through other actions (like six-sigma studies) At lower level: the static analysis is enough (but has to encompass multi-level tiers applications, with sometimes multi-languages developments) the actions are piloted by the trends and importance the study has to be approved/supported by all levels of hierarchy to be accepted/acted upon (in particular, budget for the ensuing refactoring has to be validated)
