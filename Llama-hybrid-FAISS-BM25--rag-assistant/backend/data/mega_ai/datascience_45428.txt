[site]: datascience
[post_id]: 45428
[parent_id]: 
[tags]: 
degeneracy of a CNN having only 1 convolution kernel down to a fully connected NN

Imagine a CNN having only 1 convolution kernel C without pooling layers etc. There are 3 layers in the NN: input layer, hidden layer, output layer. The input layer is a vector X originally flattened from a matrix containing pixels of an image, which we call image matrix hereafter. If one uses only 1 kernel (n n) to do a convolution on the image matrix of the input layer, the hidden layer is just the result of convoluting the image matrix and expanding into a vector. The hidden layer vector Xc=C X, where C is a nonsquare matrix doing the convolution (different from the convolution kernel). The output layer outputs if the image is a dog or cat. The cost/loss function is taken as the sum of (W Xci+b-yi)^2 over i, i from 1 to the number of training data, where Xci and W are vectors, b is a scalar, and yi are 0 or 1 tagging if the training images are a dog or cat. (W Xci+b-yi)^2 can be rewritten as (W C Xi+b-yi)^2. Here both W and C are weights that need training and backpropping. If instead one sees W*C as a single vector, the backprop is exactly the same as in fully connect NNs. Then the calculated result of this special CNN (having only 1 convolution kernel) will be the same as that of a normal fully connected NN?
