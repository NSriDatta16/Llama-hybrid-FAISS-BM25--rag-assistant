[site]: crossvalidated
[post_id]: 114192
[parent_id]: 52773
[tags]: 
I see the question already has an accepted answer but wanted to share this paper that talks about using PCA for feature transformation before classification . The take-home message (which is visualised beautifully in @vqv's answer) is: Principal Component Analysis (PCA) is based on extracting the axes on which data shows the highest variability. Although PCA “spreads out” data in the new basis, and can be of great help in unsupervised learning, there is no guarantee that the new axes are consistent with the discriminatory features in a (supervised) classification problem. For those interested, if you look at Section 4. Experimental results , they compare the classification accuracies with 1) the original features, 2) PCA transformed features, and 3) combination of both, which was something that was new to me. My conclusion: PCA-based feature transformations allow to summarize the information from a large number of features into a limited number of components, i.e. linear combinations of the original features. However the principal components are often difficult to interpret (not intuitive), and as the empirical results in this paper indicate they usually do not improve the classification performance. P.S: I note that one of the limitations of the paper that sould have been listed was the fact that the authors limited performance assessment of the classifiers to 'accuracy' only, which can be a very biased performance indicator.
