[site]: crossvalidated
[post_id]: 71715
[parent_id]: 
[tags]: 
What are some examples of problems for which various techniques are well suited

I've learned of the existence of things like Monte Carlo Markov Chains, Neural Nets, and Random Forests. In school I learned about OLS, logit, probit, GLM, etc. While I learned those methods I learned everything in the context of establishing marginal effects with very little emphasis on predicting for the sake of predicting. This is obviously in stark contrast to rf and nn which (I believe) are specifically designed for making good predictions rather than solving for marginal effects. I'm not really sure what MCMC is best suited to solve for, I just know that it does magic by smashing a bunch of random things together to get a good real answer. Assuming your main motivation is to predict future outcomes could anyone give me a quick (or relatively so) examples of when each method is better suited than the others? (Is this even a reasonable question?) Is OLS (and by extension all the methods where modeling upfront is rigid) ever likely to be the most accurate means to predict assuming you just care about predictions? Ignoring OLS, are there any pointers for choosing between the various machine learning techniques when trying to derive a model to make predictions or is it best to run all the techniques and see how they do for a given problem/set of data?
