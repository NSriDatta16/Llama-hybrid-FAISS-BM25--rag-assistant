[site]: crossvalidated
[post_id]: 606440
[parent_id]: 
[tags]: 
How is bandwidth and roughness penalty in nonparametric regression connected?

Nonparametric regression, or smoothing, concerns finding a function that smooths the data, and appears in additive models or generalized additive models. You could say we are trying to fit the model $$y_i=f(x_i)+e_i$$ with a loosely specified $f$ . One general approach to finding $f$ is a moving average type of approach, saying that $f(x)$ is estimated via taking a weighted average of nearby $y_i$ . Often this involves picking a bandwidth parameter controlling how many nearby observations are effectively used in the average, and influences the roughness of the resulting estimated function and its prediction quality. Another general approach is the the penalized regression approach, where we minimize the sum of squared errors of the fit plus a cost of the function that is high for wiggly functions. This comes with a roughness penalty parameter that controls how much to penalize a function for being wiggly. These are often the most important parameters in getting good nonparametric estimates that serve similar functions. I suspect they are very closely related to each other despite the different mathematical phrasing, perhaps being tied together by a deeper mathematical theory. What would that theory be?
