[site]: datascience
[post_id]: 122258
[parent_id]: 
[tags]: 
"Probabilistic" One-Hot-Encoder for transformers library

I'm using the transformers library to fine-tune a LLM for classification. However, my problem is a bit particular, as the labels are hierarchical, that is, in the form A-1, A-2, B-1, B-2 etc. Sometimes, I only have the first part (e.g. "A"). I'd like to use those "incomplete" datas in my training dataset, so my idea was to use a "Probabilistic" One-Hot-Encoder as target (or, to be more correct, to give a modified softmax target). For exemple, I know that in the "A*" class I have 1/3 of samples that are A-1 and 2/3 A-2, when I have a A-?, I would have a target softmax of [1/3, 2/3, 0, ...,0]. But in transformers I didn't find a way to do that "easily", as for classification you have to give integers to the model. Do you know if it is doable ? Thank you !
