[site]: datascience
[post_id]: 24345
[parent_id]: 24000
[tags]: 
TL;DR: Yes. You can (iiuc) Longer Version: In fact, this is what many popular algorithms like Word2Vec and AutoEncoders do. (With respect to hidden layer outputs) Word2Vec: Given an input word ('chicken'), the model tries to predict the neighbouring word ('wings') In the process of trying to predict the correct neighbour, the model learns a hidden layer representation of the word which helps it achieve its task. Finally, we just remove the last layer and use the hidden layer representation of the word as its $N$ dimensional vector. So basically, we feature engineered the word vectors. For autoencoders , It takes in $X$ as the input and tries to predict $X$ again, in the process learning a latent representation of the input signal X. The input hidden representation in Layer $L2$ can be used in other tasks. (Note: here X and X hat are the same) (In fact, you can use features learned by a CNN and feed them into an SVM and get good results) Relevant to you: You can train your model on the given data and finally chop off the last prediction layer and use the output of the intermediate layers as features. I believe this should work because it works for so many tasks which I explained above. Source: Learn Word2Vec by implementing it in Word2Vec : an article by me explaining word2vec. (Shameless self-advertising here but I feel the article is good and relevant) Andrew Ng's unsupervised feature learning website
