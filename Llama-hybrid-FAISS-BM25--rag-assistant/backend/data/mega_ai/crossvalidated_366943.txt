[site]: crossvalidated
[post_id]: 366943
[parent_id]: 
[tags]: 
Why does cross entropy loss for validation dataset deteriorate far more than validation accuracy when a CNN is overfitting?

I have noticed that the cross-entropy loss for validation dataset deteriorates after a certain number of epochs when training CNN's or MLP's. This is, of course, the sign that the network is overfitting. But why don't we see a corresponding fall in the validation accuracy? Many times I have noticed that the validation accuracy keeps increasing or remains constant. Sometimes the cross-entropy loss deteriorates to a value higher than that of value at the start of training despite no significant deterioration in validation accuracy.
