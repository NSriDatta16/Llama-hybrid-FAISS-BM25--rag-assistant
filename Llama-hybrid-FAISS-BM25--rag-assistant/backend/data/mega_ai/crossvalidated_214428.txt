[site]: crossvalidated
[post_id]: 214428
[parent_id]: 214424
[tags]: 
You could try cross-validation , fitting all five models for each fold and picking the model that has the lowest MSE (or whatever other error measure you are interested in) on the holdout folds. That is, for each curve, label all dots at random with labels "1", "2", ..., "10" (for ten-fold cross validation). Fit a one-Gaussian model to all points not labeled "1", and using this fitted model, predict the $y$ component of the holdout data (i.e., the points labeled "1"). Record the Mean Squared Error. Do the same for two-Gaussian, three-Gaussian etc. models. Then repeat the process for points labeled "2". Finally, summarize the MSEs per mixture model over all labels, and you will get grand MSEs per mixture model. Pick the model with the lowest overall MSE. That said, maybe it's not all that problematic if you have "too complicated" models? The problem with too complex models is usually that they are too variable, but looking at your example with two Gaussians in the true DGP and five in the AICc-fitted model, it's not like the five-Gaussian fit is obviously problematic. This will depend on what you actually want to do with your data. In the particular example you show, it looks to me like (say) an interpolation based on five Gaussians wouldn't be overly dramatically different from one based on two Gaussians. Edit: Alternatively, you could do a Bayesian approach. Assign prior probabilities to the number of mixture components, which are higher for one or two than for five Gaussians. Next, assign sufficiently uninformative priors to the actual parameters of these $k$ Gaussians. Derive posterior distributions and pick the number of Gaussians with the highest posterior probability, which would be a kind of a maximum a posteriori (MAP) approach. (Or use the full posterior distribution, which would be a kind of a "mixture of mixtures", since it would mix one-Gaussian, two-Gaussian and so forth mixtures.) This approach has the advantage that you can fine-tune the (non-)complexity you want, by changing the prior probabilities you assign to the numbers of mixture components.
