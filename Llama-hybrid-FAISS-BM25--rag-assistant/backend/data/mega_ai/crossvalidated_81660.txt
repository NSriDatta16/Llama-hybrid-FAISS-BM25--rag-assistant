[site]: crossvalidated
[post_id]: 81660
[parent_id]: 81571
[tags]: 
Here's the argument that the average is the maximum-likelihood estimator (MLE) for the parameter $\theta$. Suppose we are given observations $x_1,\dots,x_n$ of the r.v.s $X_1,\dots,X_n$. The likelihood of $\hat{\theta}$, given $x_1,\dots,x_n$, is $$L(\hat{\theta}) = \prod_i \Pr[X_i=x_i|\hat{\theta}] = \prod_i \Pr[Y_i=x_i-\hat{\theta}] = \prod_i e^{-(x_i-\hat{\theta})^2/2\sigma^2}.$$ Thus, the log-likelihood is $$\log L(\hat{\theta}) = -\frac{1}{2\sigma^2} \sum_i (x_i-\hat{\theta})^2.$$ We want to maximize this value. When maximizing, we can ignore the constant term out front, so our goal is: given $x_1,\dots,x_n$, find $\hat{\theta}$ that minimizes the sum $$f(\hat{\theta}) = \sum_i (x_i-\hat{\theta})^2.$$ We can find the minimum by taking the first derivative and setting it to zero. Notice that $$f'(\hat{\theta}) = -2 \sum_i (x_i - \hat{\theta}).$$ Setting $f'(\hat{\theta})$ to zero yields the condition $$\sum_i (x_i - \hat{\theta}) = 0.$$ Re-arranging terms, we find $$\hat{\theta} = \frac{1}{n} \sum_i x_i.$$ In other words, the value of $\hat{\theta}$ that maximizes the likelihood score is precisely the average of the observed values of $X_1,\dots,X_n$.
