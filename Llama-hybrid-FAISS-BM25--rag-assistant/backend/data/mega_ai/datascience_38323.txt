[site]: datascience
[post_id]: 38323
[parent_id]: 
[tags]: 
Data augmentation in deep learning

I am working on a deep learning project for face recognition. I am using the pre-trained model VGG16. The dataset has around 100 classes, and each class have 80 images. I split the dataset 60% training, 20% validation, 20% testing. I used data augmentation ( ImageDataGenerator() ) to increase the training data. The model gave me different results when I change ImageDataGenerator() arguments. See the following cases: Case1: train_datagen = ImageDataGenerator( rotation_range=15, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest') validate_datagen = ImageDataGenerator() Test_datagen = ImageDataGenerator() Case1 result: High training accuracy and validation accuracy, but the training accuracy is lower. check the following image: Case2: train_datagen = ImageDataGenerator( rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True) validate_datagen = ImageDataGenerator() Test_datagen = ImageDataGenerator() Case2 result: Overfitting. check the following image: Case3: train_datagen = ImageDataGenerator( rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True) validate_datagen = ImageDataGenerator(rescale=1./255) Test_datagen = ImageDataGenerator(rescale=1./255) Case3 result: High training accuracy and validation accuracy, but the training accuracy is lower.. check the following image: 1- Why does using augmentation in validation and testing data ImageDataGenerator(rescale=1./255) in case3 give different result than case2? 2- Is adding ImageDataGenerator(rescale=1./255) to the testing and validation better than not adding it? 3- Do you think there is a problem in the result of the first case?
