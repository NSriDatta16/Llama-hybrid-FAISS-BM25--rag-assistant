[site]: datascience
[post_id]: 98195
[parent_id]: 98149
[tags]: 
I think I have figured out how to do this: The key is to use spawn not fork , and use cupy to select GPU. import multiprocessing as mp mp.set_start_method('spawn', force=True) from joblib import Parallel, delayed from itertools import cycle import cupy import spacy from thinc.api import set_gpu_allocator, require_gpu def chunker(iterable, total_length, chunksize): return (iterable[pos: pos + chunksize] for pos in range(0, total_length, chunksize)) def flatten(list_of_lists): "Flatten a list of lists to a combined list" return [item for sublist in list_of_lists for item in sublist] def process_entity(doc): super_word_ls = [] for s in doc.sents: word_ls = [] for t in s: if not t.ent_type_: if (t.text.strip()!=""): word_ls.append(t.text) else: word_ls.append(t.ent_type_) if len(word_ls)>0: super_word_ls.append(" ".join(word_ls)) return " ".join(super_word_ls) def process_chunk(texts, rank): print(rank) with cupy.cuda.Device(rank): set_gpu_allocator("pytorch") require_gpu(rank) nlp = spacy.load("en_core_web_trf") preproc_pipe = [] for doc in nlp.pipe(texts, batch_size=20): preproc_pipe.append(process_entity(doc)) rank+=1 return preproc_pipe def preprocess_parallel(texts, chunksize=100): executor = Parallel(n_jobs=2, backend='multiprocessing', prefer="processes") do = delayed(process_chunk) tasks = [] gpus = list(range(0, cupy.cuda.runtime.getDeviceCount())) rank = 0 for chunk in chunker(texts, len(texts), chunksize=chunksize): tasks.append(do(chunk, rank)) rank = (rank+1)%len(gpus) result = executor(tasks) return flatten(result) if __name__ == '__main__': print(preprocess_parallel(texts = ["His friend Nicolas J. Smith is here with Bart Simpon and Fred."]*100, chunksize=50))
