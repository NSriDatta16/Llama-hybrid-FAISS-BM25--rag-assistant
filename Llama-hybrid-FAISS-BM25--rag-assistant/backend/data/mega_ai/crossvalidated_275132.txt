[site]: crossvalidated
[post_id]: 275132
[parent_id]: 275117
[tags]: 
There are basically two possible reasons why you might want to undersample: for training or for testing. First of all, when training your classifier, if one class dominates the data, then the cost function will be dominated by how well the classifier predicts that particular class. So the parameters of the classifier will be optimized more heavily towards detecting that class, and it won't "care" so much about correctly rejecting that class because there are fewer instances in the data where that is an issue. In other words, if we consider the more prevalent class to be "positive", the classifier is getting biased towards a higher rate of hits and false alarms, which it can afford to do because the number of potential false alarms is low given the prevalence of the negative class. This problem has a alternative solution, though, which doesn't require you to throw away any training data: adjusting the cost function. The problem only occurs if you weigh all errors equally, i.e. you tell the classifier to try and get as few mistakes as possible across all the data. So instead of trying to minimize the number of errors, you can tell the classifier to minimize the frequency of errors, averaged over the two classes. This means that the total weight of errors on the rare class is equal to the total weight of errors on the common class, and so there is no longer any benefit for the classifier to always (or mostly) answer the more prevalent class. This should remove any bias in learning, i.e. the classifier should no longer learn trivial strategies based on class frequency rather than real patterns in the data. When testing the classifier, the problem is that you're unsure what the chance level of errors is, because there are trivial "cheating" classifiers that could get very low error rates simply by always (or mostly) answering the most prevalent class. The solution here is essentially the same: don't throw all errors into the same bag, but look at them separately for each class. A dumb classifier that always gives the same answer will predict the most prevalent class correctly 100% of the time, but get 0% of the other class right, which averages out to the familiar binary chance level of 50%. Any classifier that scores (significantly) higher than that must be retrieving some real information from the data. There are more sophisticated ways of looking at this, but that's essentially the intuition. So in either case, I don't think undersampling is typically the best answer because it's throwing away data, and there other ways of restoring the class balance in your analysis that use all the data.
