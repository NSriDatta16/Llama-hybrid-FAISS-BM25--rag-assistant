[site]: crossvalidated
[post_id]: 589298
[parent_id]: 
[tags]: 
For neural networks, is mini-batching done purely because of memory constraints?

I want to understand why mini-batching is used to train neural networks (rather than using the entire dataset for every update). Is the reason purely that with big datasets, it requires big computing power to use entire dataset every time, so mini-batching is the intermediate solution? If so, does that mean that it's best to use the biggest batch size your machine can handle when training neural networks? Any insight, references or info would be appreciated!
