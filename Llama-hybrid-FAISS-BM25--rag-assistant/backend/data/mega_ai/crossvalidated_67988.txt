[site]: crossvalidated
[post_id]: 67988
[parent_id]: 67986
[tags]: 
In principle, the linear transformation performed by PCA can be performed just as well by the input layer weights of the neural network, so it isn't strictly speaking necessary. However, as the number of weights in the network increases, the amount of data needed to be able to reliably determine the weights of the network also increases (often quite rapidly), and over-fitting becomes more of an issue (so using regularisation is also a good idea). The benefit of dimensionality reduction is that it reduces the size of the network, and hence the amount of data needed to train it. The disadvantage of using PCA is that the discriminative information that distinguishes one class from another might be in the low variance components, so using PCA can make performance worse. Like most things in statistical pattern recognition, there is no single recipe that works reliably for all problems, and really the best thing to do is to try both approaches and see which works best.
