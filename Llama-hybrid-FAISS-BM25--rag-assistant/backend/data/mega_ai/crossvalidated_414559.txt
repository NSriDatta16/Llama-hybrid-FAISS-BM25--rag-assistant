[site]: crossvalidated
[post_id]: 414559
[parent_id]: 321351
[tags]: 
Greene states his explanation of the "optimal linear predictor" in the quoted paragraph. He says that, "[f]or this criterion [of optimality], we will use the mean squared error rule, so we seek the minimum squared error linear predictor of $y$ ." Since the mean-squared error is equal to the variance of the estimator plus the square of its bias, you are correct that an unbiased predictor that is "optimal" in this sense, will also be a best linear unbiased estimator (BLUE). Presumably, Greene separates these concepts because there may be broader estimation problems (beyond the context of the linear model) where the estimator is not unbiased, and so the "optimal" estimator that minimises the mean-squared error is not BLUE. Additionally, even within the context of linear models, it is useful to separate these two concepts for pedagogical reasons ---i.e., because he has not yet shown that they are the same.
