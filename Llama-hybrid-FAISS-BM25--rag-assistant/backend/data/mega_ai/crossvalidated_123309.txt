[site]: crossvalidated
[post_id]: 123309
[parent_id]: 122062
[tags]: 
As others have said, the connection between the three is how you incorporate the prior information into the measurement. In case of the Stein paradox, you know that the true correlation between the input variables should be zero (and all the possible correlation measures, since you want to imply independence, not just uncorrelatedness), hence you can construct a variable better than the simple sample mean and suppress the various correlation measures. In the Bayesian framework, you can construct a prior that literally down weighs the events that lead to correlation between the sample means and up weighs the others. In case of ridge regression you want to find a good estimate for the conditional expectation value E(y|x). In principle this is a infinite-dimensional problem and ill-defined since we have only finite number of measurements. However, the prior knowledge is that we are looking for a continuos function that models the data. This is still ill-defined, since there are still infinitely many ways to model continuos functions, but the set is somewhat smaller. Ridge regression is just one simple way to sort the possible continuos functions, test them and stop at a final degree of freedom. An interpretation is the VC-dimension picture: during the ridge regression, you check that how well a f(x, p1, p2... ) model with a given degree of freedom describes the uncertainty inherent in the data. Practically, it measures how well can the f(x, p1, p2 ... ) and the empirical P(p1,p2...) can reconstruct the full P(y|x) distribution and not just E(y|x). This way the models with too many degree of freedom (which usually overfit) are weighed down, since more parameter mean after a certain degree of freedom will give larger correlations between the parameters and consequently much wider P(f(x, p1, p2... ) ) distributions. An other interpretation is that the original loss function is a measure value as well, and it the evaluation on a given sample comes with an uncertainty, so the real task is not minimizing the loss function but to find a minimum that is significantly lower than the others (practically changing from one degree of freedom to an other is a Bayesian decision, so one changes the number of parameters only if they give a significant decrease in the loss function). The ridge regression can be interpreted as an approximation to these two pictures (CV-dimension, expected loss). In some cases you want to prefer higher degrees of freedoms, for example in particle physics you study particle collision where you expect the produced number of particles to be a Poisson distribution, so you reconstruct the particle track from on an image (a photo for example) in a way that prefers a given number of tracks and suppresses models which has smaller or higher track-number-interpretation of the image. The third case also tries to implement a prior information into the measurement, namely that it is known from previous measurements that the students' height can be modeled very well by Gaussian distributions and not by a Cauchy, for example. So in short, the answer is that you can shrink the uncertainty of a measurement if you know what to expect and categorize the data with some previous data (the prior information). This previous data is what constrains your modeling function that you use to fit the measurements. In simple cases you can write down your model in the Bayesian framework, but sometimes it is impractical, like in integrating over the all the possible continuos functions to find the one that has the Bayesian Maximal A Posterior value.
