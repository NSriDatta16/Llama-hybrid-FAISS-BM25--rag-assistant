[site]: crossvalidated
[post_id]: 590685
[parent_id]: 2772
[tags]: 
It's not about Math. only. To understand why it's different, you need to know the Finance theory background of factor models. For a factor model $R = BF + \epsilon$ . It is like a multivariate regression, R has dimension the number of stocks. F, the number of factors. B is a matrix of loadings. $\epsilon$ (epsilon) has a diagonal covariance matrix. In PCA you first get the loadings as the Eigen vectors of the covariance matrix. Then you decide how many you keep, say K. In a second step you get the K factor scores (estimates of the K factor values at every time t) by cross-sectional regression of the stock returns on the loadings for every time (aka Bartlett method). Since the Bs are noisy estimates from a noisy covariance matrix, you have a measurement error in your right hand side data (the Bs) in the second step. Now why it's important: The typical equity finance application has many many stocks and not so many time period => noisy noisy cov matrix => noisy loadings B (can you say Junk in?) => massive bias to zero of the estimates of the factor scores. In the extreme, you have more stocks than periods and your cov. matrix is not full rank. In Asymptotic PC, see Connors and Korajczik, the cross-product matrix is TxT, where T is the number of observations. It is always PDS, as the number of stocks goes to infinity for a given sample size, it converges to the factor scores with arbitrary precision. Then in a second step you get the loadings by a time series regression of each return on the factors. And here is the (nice) catch: In a stock situation, you have N >> T, you could not even estimate the cov.matrix properly. But you have infinitely precise estimates of the factor scores. In the second step where you get the loadings from the scores, you have NO problem of errors in the variables. If you take the pain to read their papers, you will see the simulation that shows how effective this is for typical cross-section size vs sample size.
