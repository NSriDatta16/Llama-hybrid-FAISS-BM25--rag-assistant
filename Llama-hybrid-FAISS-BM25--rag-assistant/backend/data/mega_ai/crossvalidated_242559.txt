[site]: crossvalidated
[post_id]: 242559
[parent_id]: 
[tags]: 
R - unexpected probabilities obtained with lsmeans using a glmer (lme4) mixed effect model

This is a follow-up question to this one . I am working on survey data which contains multiple-response types of questions. More specifically, respondents were asked to "check all that applies" on 2 sections of the questionnaire. My client needs to compare the response proportions for each of the options listed . At first I was looking into pairwise tests for proportions, but as was pointed out, the dependency (each subject has a tendency to select few, or many items) should be taken into account, such as in a mixed effects model. I managed to get results that make a lot of sense. After restructuring the data, I started by just using a logistic regression, combined with lsmeans to make sure that the probabilities did reflect the raw proportions for each item. And it works as expected. library(lme4) library(lsmeans) mod.fixed These correspond exactly to the raw proportions of positive answers to each item in the survey. Going a step further, I used glmer allowing each subject to have its own intercept. mod.mixed Up to there, the difference in probabilities between the fixed and mixed models make sense (the prob's from the mixed model are not completely off the map). However, I have a second set of items that follow the exact same logic, so I followed the same steps: mod2.fixed The prob values still correspond precisely to the raw observed proportions. Now the mystery comes in in the next step, where I use glmer and lsmeans with that second set of data. mod2.mixed I can't explain why all the probabilities are near 1, whereas in the data, they range from .73 to .87. And also, why are the SE's so low? I'm linking sample, anonymous data here . (Just ignore the smok column) I've checked for anomalies in the data and I don't see anything dramatically different from mydata1 in mydata2 . If anyone can help me understand this phenomenon, I'll be more than grateful.
