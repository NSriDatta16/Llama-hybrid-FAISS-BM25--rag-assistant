[site]: crossvalidated
[post_id]: 390455
[parent_id]: 390454
[tags]: 
There are two intuitive reasons that I can think of. The first one is that the number of parameters that you have to train in a neural network can be huge. Imagine a normal, simplified neural network with only fully connected layers. Say you have 100 neurons in the first layer, 50 neurons in the second layer and 1 neuron in the output layer. The number of parameters that you need to optimize is 100*50 + 50*1 = 5050 (omitting bias parameters). For a linaer regression model with 100 features, you only need to find 100 coeffcients (again ommitting bias/intercept). For image recongnition the used networks are usually much larger, check this post to get an idea how such a network looks like and the number of parameters that you need to optimize. In addition, there are also other hyperparameters that you need to tune in each layer (e.g. regularization, dropout, activation function). The second reason is closely related to the huge possibilites neural networks have in modelling any target function . In contrast to linear regression the function you are trying to optimize can be very complicated. A very nice analogy that I have seen (but cannot find it right now) is when you compare the two functions $f(x) = x^2$ and $g(x) = sin(x)$ . It is easy to integrate $f(x)$ and find the minimum. In contrast $g(x)$ has many or better infinite minima. In reality, cost functions for neural networks do not look as predictable as the $sin$ function. There can be many local minima which your learning algorithm might get stuck at, that is you will not find the optimal solution. You will find more detailed explanations by looking up convex and non-convex functions and their optimization. Since the question was kind of broad I hope that this gives you an idea.
