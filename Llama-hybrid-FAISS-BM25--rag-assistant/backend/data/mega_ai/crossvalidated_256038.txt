[site]: crossvalidated
[post_id]: 256038
[parent_id]: 255864
[tags]: 
First of all, you may have a misconception: repeated/iterated cross validation will not lead to lower uncertainty on the cross validation results iff the models are stable. If your models are unstable, evaluating more iterations/repetitions/runs of the cross validation allows you to characterize the instability of the predictions (variance in predictions of different surrogate models that were trained on slightly different/perturbed training data) - and the average over many such surrogate models will have lower variance from this source . Personally, I consider estimates of prediction stability as their own figure of merit (though it is good to be aware of the variance contribution to the cross validation error estimate). The uncertainty (variance) due the limited number of actual cases tested which does not increase by repeating the cross validation is not affected. Some of our paper could be relevant (but they each contribute only bits and pieces to this topic): Beleites, C.; Baumgartner, R.; Bowman, C.; Somorjai, R.; Steiner, G.; Salzer, R. & Sowa, M. G. Variance reduction in estimating classification error using sparse datasets, Chemom Intell Lab Syst, 2005, 79, 91 - 100 . We did observe some reduction in variance of 40 $\times$ 5-fold CV vs. only one run of the 5-fold CV. At that time we just realized that a bit of improvement may be gotten that way. Here'd like to draw your attention to the fact that even for the situations where we observed largest reductions, $s^2 (40 \times 5$-fold$) \approx 0.8 s^2 (1 \times 5$-fold$)$ - so the observed reduction is nowhere near e.g. $\frac{1}{\sqrt{40}} \approx 0.15$ Beleites, C. & Salzer, R. Assessing and improving the stability of chemometric models in small sample size situations, Anal Bioanal Chem, 2008, 390, 1261-1271 explains how to use iterated/repeated cross validation to characterize model stability. I may say that we still hadn't realized that we're subject to multiple quite different sources of variance, so please disregard the paragraphs that try to estimate effective sample size. Note that I argue from the perspective of an applied scientist who is interested in estimating the predictive performance of a model built on the data set at hand and at the same time is typically hampered by very small sample sizes (both absolute and relative to the number of variates). The sources of error [of the CV results] I consequently focus on are quite different e.g. from the perspective of comparing training algorithms and from the perspective of having somewhat more comfortable sample sizes (in particular, absolute numbers of independent cases) at hand. E.g. Vanwinckelen, G. & Blockeel, H. On estimating model accuracy with repeated cross-validation BeneLearn 2012: Proceedings of the 21st Belgian-Dutch Conference on Machine Learning, 2012, 39-44 argue that repeated cross validation is not worth while, because they find bias more important than the variance reduction. They have about 2 orders of magnitude the sample sizes I typically deal with and do not discuss model stability at all.
