[site]: datascience
[post_id]: 56846
[parent_id]: 56844
[tags]: 
You should definitely use a sliding window. An n-gram language model represents the probabilities for all the n-grams. If it doesn't see a particular n-gram in the training data, for example "sliding cat", it will assume that this n-gram has probability zero (actually zero probabilities are usually replaced with very low probability by smoothing, in order to account for out-of-vocabulary n-grams). This would result in a zero probability for a sentence which was actually in the training case (or a very low probability with smoothing). Also it's common to use "padding" at the beginning and end of every sentence, like this: #SENT_START# The The sliding sliding cat cat is is not ... to dance dance #SENT_END# This gives the model indications about the words more likely to be at the beginning or end (it also balances the number of n-grams by word in a sentence: exactly $n$ even for the first/last word).
