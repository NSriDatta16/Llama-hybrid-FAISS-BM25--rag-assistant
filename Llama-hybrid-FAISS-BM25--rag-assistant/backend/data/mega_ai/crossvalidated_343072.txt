[site]: crossvalidated
[post_id]: 343072
[parent_id]: 343071
[tags]: 
I was able to mostly answer my own questions (I think): We aren't actually assuming that every word is equally biased. Some word $e1$ may be nearly orthogonal to our abstract axis $g$. If that is the case, the magnitude of the projected vector will be very low, and little correction will be done to $e1$. Another word $e2$ might have a direction that is very close to the abstract axis, so the magnitude of the projected vector will be nearly as large as the original vector $e2$. In that case $e2$ will be nearly zeroed out by the projections. I do think that we're assuming that gender is expressed only in proportion to this axis. $g$ could be imprecise or inaccurate, in which case our calculations would be correspondingly wrong. I'm much less sure about the following, but perhaps the embeddings could also have learned to express gender along multiple axes, which we also wouldn't adequately correct for. As for the second question about $1/2 * (e_{woman} - e_{man})$, this is wrong because we're treating $g$ as an axis, not an amount by which to correct. A simple example demonstrates this: If $e3$ is some "masculine" word, its projection onto $g$ will be of negative length. We will then subtract that negative projection, i.e. we will add it, correcting the masculine word by making it more feminine.
