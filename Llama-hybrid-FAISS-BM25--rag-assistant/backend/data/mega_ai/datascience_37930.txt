[site]: datascience
[post_id]: 37930
[parent_id]: 37921
[tags]: 
Quoting Dr. Bruce Ratner: "R-sq is a first-blush indicator of a good model. R-sq is often misused as the measure to assess which model produces better predictions...The root mean squared error (RMSE) is the measure for determining the better model. The smaller the RMSE value, the better the model is (the predictions are more precise)." I sourced this from Dr. Ratner's LinkedIn, so I can't link to it. His website is: dmstat1.com Be careful with R-sq. The more variables you use the higher your R-sq value will be no matter how good or bad your model is . If you are using more than a few predictor variables you should also look at Adjusted-R-sq. If you would rather get a measure of accuracy you should look at RMSE. If you are comparing two models also look at Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC). To answer your question, if the R-sq is 0.7, it is incorrect to say that 70% of the data has been predicted accurately by the model. R-sq is a measure of how much variance is accounted for within the predictor variables you are using . R-sq of 0.7 means that your predictor variables can explain 70% of the variance in the response variable. For example pretend that the price of a cup of coffee is influenced ONLY by the price of water and the price of coffee beans. If I only know the price of water and I predict the price of coffee using only the price of water my R-sq is 0.5, because I know only 1/2 of the things that influence the price of a cup of coffee. In reality, it is virtually impossible to know ALL predictors that influence an outcome (R-sq = 1) because we either don't know what variables influence an outcome, or the variables that influence an outcome aren't contained in the dataset.
