[site]: crossvalidated
[post_id]: 29090
[parent_id]: 29084
[tags]: 
You do dimensionality reduction if it improves results. You don't do dimensionality reduction if the results become worse. There is no one size fits all in data mining . You have to do multiple iterations of preprocessing, data mining, evaluating, retry, until your results work for you. Different data sets have different requirements. Remember how the KDD process looks like: Notice the grey arrows going back. If the result does not make you happy, try going back and e.g. try to use different preprocessing such as dimensionality reduction. But 10 dimensions is not high dimensional anyway, probably no need to be afraid of the curse of dimensionality, unless you do some grid based methods. For the behaviour of high-dimensional data, I can recommend the articles by Houle et al: Can Shared-Neighbor Distances Defeat the Curse of Dimensionality? M. E. Houle, H.-P. Kriegel, P. Kr√∂ger, E. Schubert and A. Zimek SSDBM 2010 They show that there is no direct relationship of the number of dimensions and the ability to cluster the dataset. But it is more a question of the signal-to-noise ratio . A high-dimensional dataset can be very easy and good to cluster if all the dimensions contribute signal. If most of the dimensions are noise, a much smaller dataset will already break down. So in particular, there is no rule of thumb such as "10 is good, 50 is bad", sorry.
