[site]: crossvalidated
[post_id]: 544677
[parent_id]: 544674
[tags]: 
This is actually a good CV question because it is the pivotal moment when a trained statistician becomes a professional statistician. Learning moment 1: Never judge a person by their pedigree or training By you not responding seriously to a critique of your methods because the criticism came from a non-statistician , you are selling yourself short, very short indeed. Instead you should have asked yourself whether the actual matter of the remark made sense. In other words, if John Tukey had said that $R^2$ =0.40 was underwhelming, you would have thought quite differently about this scenario. In my career I've met many non-statisticians whose level of quantitative thinking was a lot better than mine, and they've been able to understand and remark on my work with remarkable detail and clarity, and I'm grateful for their help. It's actually quite easy in the literature to expose one's self to a lot of methods when they're less concerned with implementing those methods. A statistician is more concerned with implementation, and so holistic understanding of methods and their plusses and minuses often ends up being a blindspot for us. Be gracious that someone cares enough about your work to listen. Learning moment 2: if you don't understand a person's remark, entice them to say more . Case 1: If they truly haven't mustered statistics, they'll reveal it when trying to explain it in greater detail. Broad criticism is never helpful, but everyone is capable of offering it. That criticism loses credibility when the critic can't pinpoint an idea or opportunity to turn things around in the right direction. Case 2: more likely, you just missed the point and it's a chance to respond intelligently and specifically. Suppose there's another model out there that achieves $R^2$ =0.60 you didn't know about, then you graciously thank the commenter and promise that you'll review the methods and respond about the possible impact to your presentation later. Suppose they are contrasting with another study you're knowledgeable about and you can speak to some meaningful differences, etc. In short, this all seems like a lost opportunity to just ask the person what they meant. Your answer that "human behavior is unpredictable" comes across as glib, and not hanging on to any meaningful differences; it even argues against you ever fitting any model at all. Rather, you can, for instance, feed your model's results into a cost-benefit analysis. Or you can point out that averaged over 1,000,000,000s of users, a 0.40 $R^2$ is actually impressive (did you specify benchmarks? what were the results from prior models if any? If this is the first model of it's kind, you can argue that your model establishes the benchmark). I recall a well funded air pollution model celebrated achieving $R^2$ =0.04 for predicting exposure to CO, O3, and other air pollutants, this is because the exposure is SO variable, the model's ability to detect any change was celebrated. Lastly, Learning moment 3: There's always room on the collaborator boat The saying "if you can't beat 'em, join 'em" has never been truer than today. Statisticians are lucky because the stuff of our work (data) practically falls in our lap. In the earlier parts of this answer, I mention some things (e.g. benchmarks, cost-benefit analyses). I think I'm not assuming too much in guessing that these are, in fact, gaps in understanding. And if it's mentioned for the first time at a presentation as a gap in understanding, you can be humble to point it out as such, but it doesn't need to end there. You can show commitment and dedication to both your work and the audience by saying, "This is something that we didn't consider, but there are ways we can reframe the question or provide supporting research on the possible value or room for improvement. And I'd welcome anyone here to reach me afterwards if they're interested to pursue this with me."
