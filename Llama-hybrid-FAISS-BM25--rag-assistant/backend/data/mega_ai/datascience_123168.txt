[site]: datascience
[post_id]: 123168
[parent_id]: 123117
[tags]: 
Normalization (or linear transforms in general, e.g. standardization) does not incur loss of information except the original min/max/mean/std etc.. An analog is converting between meters and centimeters - as far as you remember 1m = 100cm, then you can revert it. Usually throwing the mean etc. away is OK as most likely it does not matter how far 2 samples are apart in absolute sense, but relatively . This is evidential from the fact that tree-based models (decision tree, random forest, XGBoost etc.) generally performs well on tabular data, yet being scale-invariant - they do not care about feature scale at all. This is unlike spatial position in image recognition which is crucial to the recognize an object. By the way, occasionally a beginner would try different normalization scales e.g. 0-10, -1-+1, train a few models, gets different result and argues that something call 'contrast' is lost in normalization. This is wrong. The actual reason is usually due to 2 things - gradient descent and algorithm assumptions. Gradient descent comes with some hyperparameters e.g. learning rate. For different setup (scale, data, cost function) you have to try different parameters to find the best fit. Wrong parameters e.g. too high/low learning rate can lead to increasing loss/trap in local minima. Another thing that may go wrong is that some preprocessing steps/models have implicit assumptions on feature's distribution, e.g. PCA (assume centered features) and Gaussian Mixture model (assume Gaussian), so watch out.
