[site]: datascience
[post_id]: 52181
[parent_id]: 44091
[tags]: 
The loss function is the function used to measure the quality of the approximation $f$ . On the other hand, the empirical risk is a function that results from averaging the loss function over your data. More formally, consider that your data is drawn from a set $\Omega$ and let $\mathcal{D}$ be the set of all the possible functions $f$ that you can choose. Then the loss function is a function $L\colon\Omega\times\mathcal{D}\to\mathbb{R}_{+}$ . If $\{\omega_i\}_{i\in I}\subseteq\Omega$ is a finite family and $L$ is a loss function, then the empirical risk associated with each element $f\in\mathcal{D}$ is calculated as $$\rho(f)=\frac{1}{\vert I \vert}\sum_{i\in I}L(\omega_i,f).$$ Note that you got confused with the domains while writing the question. Let me clarify that for you: In the situation you described, considering that each of your 'data points' $(x,y)$ belongs to a set $X\times Y$ , we obtain $\Omega=X\times Y$ , and then you have that $Q\colon X\times Y\times \mathcal D\to\mathbb{R}_{+}$ is a function given by $$Q(x,y,f)=l(f(x),y).$$ Also, the empirical risk becomes $$\frac{1}{\vert I\vert}\sum_{i\in I}l(f(x_i),y_i)=\frac{1}{\vert I\vert}\sum_{i\in I}Q(x_i,y_i,f).$$ Moreover, I should warn you that dividing the sum by the size of your data set does not change the optimal function. Hope this helps!
