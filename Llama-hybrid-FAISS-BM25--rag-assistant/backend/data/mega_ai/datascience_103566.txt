[site]: datascience
[post_id]: 103566
[parent_id]: 
[tags]: 
What changes is the Neural Network back-propagation algorithm doing on the weights?

I have seen the formula for back-propagation algorithm for neural network error minimization, but I am not quite sure about what changes it is performing on the weights individually. Let us suppose a simple neural network as follows: All the weights between the connection nodes are initialized to 1. There is no activation function, so dot product of the weights and inputs from the previous level is transmitted unchanged to the next level. Here, the original inputs are 1 and 1. The final outputs thus becomes 4 in this case. Suppose we would like to change the weights such that the "corrected" output will exactly match the desired target output=1. My question is, what will the resulting weight values be (as in the above diagram) after applying changes prescribed by the back-propagation algorithm? Thank you.
