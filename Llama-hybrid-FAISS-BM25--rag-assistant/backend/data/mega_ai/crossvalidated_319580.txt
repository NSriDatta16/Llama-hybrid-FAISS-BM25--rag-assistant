[site]: crossvalidated
[post_id]: 319580
[parent_id]: 318515
[tags]: 
The rationale for fitting a fixed or random intercept is much the same. The choice ultimately boils down to power: considerably more power is required to fit the fixed effect with one level for each cluster. You have 10 clusters with 90 observations. The term called "intercept" in the fixed and mixed effects model has a different interpretation. For the fixed model the intercept is generated by contrast to estimate the predicted mean in the $p_1$ cluster with $x=0$. In the mixed model, the intercept term is the expected value for a person with $X=0$ drawn from no cluster in particular (or, similarly, averaging up over all the possible clusters to which that person may belong). You'll note in the fixed effects output that $p_1$ is not included as a term. The (intercept) is 12. The subsequent $p_2, p_3, \ldots$ terms provide the contrast as a mean difference from $p_1$. So they tend to be 2 units higher per your simulation. Your use of contrasts then marginalizes the fixed intercepts into a grand mean which is centered at 21 which you'll note is about the average of the sequence of generated means for $p$ of [12, 14, 16, 18, 20, 22, 24, 26, 28, 30]. The standard error for this marginalized coefficient is going to be more precise than the value you would get from ignoring clustering and merely looking at the intercept term in a fixed effect model which only adjusts for $x$. So this value has the same interpretation as the mixed model. Why is the inference different? The Wald based inference (obtained from summary.merMod ) is inappropriate in this case. The documentation in R points to using bootMer or confint with profile likelihood. The joint distribution of fixed and random effects is hierarchical, so as with the fixed effects model, you need to perform some kind of marginalization. The bootstrap and profile likelihood methods are akin to MCMC and numerical integration. That's why these results finally agree.
