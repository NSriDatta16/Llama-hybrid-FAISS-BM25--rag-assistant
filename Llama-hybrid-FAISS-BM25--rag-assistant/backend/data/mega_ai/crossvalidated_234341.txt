[site]: crossvalidated
[post_id]: 234341
[parent_id]: 91816
[tags]: 
Before I answer your questions, I will give some thoughts on using the linear probability model (LPM). Using the LPM ones has to live with the following three drawbacks: The effect $\Delta P(y = 1 \mid X = x_0 + \Delta x)$ is always constant The error term is by definition heteroscedastic OLS does not bound the predicted probability in the unit interval Some comments on this: For example if y is a work force participation indicator, and the x variable under study is the number of children, then the effect of 1 additional child always have the same predicted effect. Literally this means the effect on going from 0 to 1 children is the same as going from 10 to 11 children – this is clearly a strict and unrealistic assumption. You could try to loosen it, by adding interactions to your model. But it is seldom clear exactly how you should group things. This is “easy” to get free from, use robust standard errors. In fact, you know that the variance is $p(1-p)$, so technically you could use WLS to get a more efficient estimate, but this of course requires the assumptions that you can estimate $p$ with consistency (cf. above). This follows directly from that fact that OLS does not impose “range”-conditions on the response, if you increase (or decrease) the x-variables enough, then at some point you will cross the border, and get seemingly meaning less predictions (this actually happens quite often, when you use the LPM – and makes the use WLS difficult). This is why people, in the comments, suggests that you instead use a GLM, covering these models is beyond the scope here but try searching on Logit and Probit, there are excellent questions (and answers) on this site and google. Suffice it to say that GLM models directly eliminate the problems outlined above – on the other hand, they do require that you make arbitrary distributional assumptions (which you don’t have to do with OLS). That said, the LPM has some advantages in ease of interpretation, and can work quite well around the means of the independent variables, when all you want is an on average partial effect. For classification the LPM is, in my experience, extremely horrible. Now onto your question. Remember that the partial effect, in any model, is given by the derivative $\frac{\partial y}{\partial age}$, you cannot change age and hold $age^2$ fixed – it just doesn’t make sense. See (1). The same as always, it is the value of $y$ when all variables $(x)$ are set to 0 – you use a lot of dummy variables, so it will take so figuring out. Technically yes, but remember that even if you a statistically significant value – you still to have to argue whether or not the estimated effect has a significant effect in the real world. There is also much to say about p-values in general, and again I suggest a search on this site for more info. I don’t know who told you that, you can use $R^2$ the same always – it means exactly the same thing. As an alternative you could use the percentage correctly predicted measure. You form predictions for all individuals in the sample, if $p
