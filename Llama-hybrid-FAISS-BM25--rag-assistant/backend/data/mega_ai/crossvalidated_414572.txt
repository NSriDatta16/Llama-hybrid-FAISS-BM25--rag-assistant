[site]: crossvalidated
[post_id]: 414572
[parent_id]: 414570
[tags]: 
Interpolation/Imputation This all depends on the size of the missing periods relative to the size of the significant periods in your time series. I don't know enough about your EEG data, but I can give you a parallel example from my world of demand forecasting: I want to forecast weekly demand, and my data has a yearly seasonality (De,amd for swimsuits will go up in the summer, down in the winter. Demand for coats will go up in the winter, down in the summer. Pumpkin spice product demand will spike in October, etc....). For a time series like this, I will interpolate or impute data that is missing for one or two weeks. But I will not interpolate data that is missing for more than a month, since that is the level at which I start having significant seasonal patterns (i.e. my signal starts carrying important information). Interpolating or imputing at the month level will create fake patterns and/or smooth out real ones that are highly significant for my time series. And for some weeks of the year or items, even one will be too much to interpolate over without creating false patterns or smoothing out real ones. For some products the week of Christmas or Thanksgiving is very important and I can't just impute it if it is missing. Cutting out entirely The same principle applies, but in reverse. If the amount of information is over periods significantly larger than the periods that will display significant patterns, you might as well cut them out. In my previous weekly demand data example, if I have several years of sales but 2 or 3 years are missing, then I can just cut up my series into 2 or 3 series (depending on where the gaps are) and use each series separately (not that this works only if you are dealing with a large number of parallel series and using a suitable model - for example LSTM). I would be very careful though in this case to remove any trends from data and use the "chopped up" series only for my seasonal patterns (I assume that EEG data doesn't usually have long term trends in it, at least based on your graph). Another way to think of this, is that you are performing a time series boot strapping of sorts by cutting your time series into separate samples. I would not join the disjoint time series though, as you risk inserting spurious auto-correlations that don't exist in the data. Just use each resulting sub-series separately. Noise imputation. This would just boil down to being the same thing as the second case (cutting out entirely) - except that maybe you have a method which requires multiple time series of identical length? Same consideration of relative length of the missing periods to the length of significant patterns applies.
