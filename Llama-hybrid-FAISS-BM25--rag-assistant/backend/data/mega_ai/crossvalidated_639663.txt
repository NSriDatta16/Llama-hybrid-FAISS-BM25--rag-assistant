[site]: crossvalidated
[post_id]: 639663
[parent_id]: 
[tags]: 
Significant effects but very small differences between contrasts

I am puzzled by some results and I would like to ask for some advice. I have been fitting linear mixed-effects models on a rather small dataset (N = 30) and unlike I have seen before, pretty much all of my planned contrasts are significant. I have coded my factor of interest using sum coding and so the model yields results based on which I can compute estimates for each of the contrasted levels. The truth is that the differences between the contrasted levels are for many measures very small on original scales. In reality, such small differences are not meaningful and cannot even be interpreted. I am fitting models for different acoustic measures as a function of condition type. How come that I consistently detect significant differences? I have heard before that statistical significance is not practical significance. Perhaps important to mention: I have kept the model structure maximal, as theoretically justified (in my case including random intercepts), in accordance with Barr et al., 2013 and I have been decreasing the model structure using VarCorr() and summary(rePCA()) and of course likelihood-ratio tests anova() to arrive at the optimal model structure. Later edit: Based on all models, the by-participant varying random intercepts capture quite a lot of variance. I used the MuMIn::r.squaredGLMM() function. This function returns conditional and marginal coefficient of determination for Generalized mixed-effect models (I hope I have been using it appropriately for lemur models). it turned out that most of the times, the fixed effects accounted for little to no variance, whereas the model (including fixed and random effects) explained about 90% of the variance. Could the significant effects be driven by random instead of the fixed effect? Why is it still significant if it explains so little variance?
