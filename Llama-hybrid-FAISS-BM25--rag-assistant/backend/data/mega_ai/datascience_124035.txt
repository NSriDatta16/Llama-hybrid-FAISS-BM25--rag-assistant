[site]: datascience
[post_id]: 124035
[parent_id]: 124034
[tags]: 
You are overfitting A LOT. This is usual when finetuning BERT on small datasets. I suggest you take a look at the BERT article to use it as a guidance for sensible hyperparameter values and finetuning strategies. For instance: They finetuned for 3 epochs. They ran several random restarts and selected the best model on the Dev set. At each random restart, they used the same pre-trained checkpoint as starting point but performed different finetuning (e.g. different learning rates from [5e-5, 4e-5, 3e-5, 2e-5]), data shuffling and classifier layer initialization (i.e. different random seeds). Apart from that, check what optimizer you are using and how you are configuring it. In the original BERT article they used the Adam optimizer but disabling the bias compensation (aka BERTAdam), but this was counterproductive. Check this answer for advice on how to choose the optimizer and its hyperparameters.
