[site]: crossvalidated
[post_id]: 383464
[parent_id]: 382037
[tags]: 
You can do somewhat better by using logistic regression instead of linear regression, but otherwise your approach is fine. Note that with the linear model you propose, the model has no way of "knowing" that there are 1000 observations in each age band because it only "sees" the average. That means the linear model is forced to estimate variance purely from the 5 provided data points which is not very reliable. (It's also extremely likely that we will be violated the underlying assumption of normally distributed error term, although it would of course be extremely difficult to tell one way or the other with just 5 data points!) In contrast to these difficulties, a logistic regression model will be able to "see" the sample size and exploit the known relationship between n, p, and variance of the binomial distribution and we won't have to worry about violating the normality assumptions of the linear model. To adapt your approach to logistic regression, you'll need to use a categorical variable (a factor) as the response variable instead of a continuous frequency. One way to do this is simply to replicate each row of data the appropriate number of times but another faster way to do this is with observation weights. Let's say we format the data to look like this: | age | variant | n_people | |-----|---------|----------| | 20 | 0 | 750 | | 20 | 1 | 250 | | 30 | 0 | 800 | | 30 | 1 | 200 | | 40 | 0 | 800 | | 40 | 1 | 200 | | 50 | 0 | 780 | | 50 | 1 | 220 | | 60 | 0 | 750 | | 60 | 1 | 250 | Then (using R, for example) we can build models which include the quadratic age term, the main (linear) effect only, and the reduced model with only an intercept. full_model The full model looks like so (using the fake data I mocked up): > summary(full_model) Call: glm(formula = variant ~ age + I(age^2), family = binomial(), data = df, weights = n_people) Deviance Residuals: Min 1Q Median 3Q Max -20.960 -19.353 3.096 25.931 26.491 Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) -0.2617126 0.2978734 -0.879 0.379616 age -0.0576910 0.0163196 -3.535 0.000408 *** I(age^2) 0.0007348 0.0002017 3.643 0.000270 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 5319.2 on 9 degrees of freedom Residual deviance: 5305.8 on 7 degrees of freedom AIC: 5311.8 Number of Fisher Scoring iterations: 5 Note that R is helpfully performing a z-test for each coefficient, which already suggests that the quadratic age term may be statistically significant (again, for the fake data.) However, these z-tests on individual coefficients are an approximation at best, so we should use the likelihood ratio test , which is the gold standard. Here's an example in R of using the LR test to compare the full model to the reduced model. This tells us if including age (both linear and quadratic terms) is better than excluding age from the model altogether: > anova(full_model, reduced_model, test="LR") Analysis of Deviance Table Model 1: variant ~ age + I(age^2) Model 2: variant ~ 1 Resid. Df Resid. Dev Df Deviance Pr(>Chi) 1 7 5305.8 2 9 5319.2 -2 -13.472 0.001188 ** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Alternatively, we could compare the full model to the main effects model with anova(full_model, main_effects_model) if we are specifically interested whether or not the inclusion of the quadratic term to the model is statistically significant: > anova(full_model, main_effects_model, test="LR") Analysis of Deviance Table Model 1: variant ~ age + I(age^2) Model 2: variant ~ age Resid. Df Resid. Dev Df Deviance Pr(>Chi) 1 7 5305.8 2 8 5319.0 -1 -13.242 0.0002738 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Note that there are many approaches to this same problem, say a 5x2 $\chi^2$ test, but the logistic regression approach I've outlined above seems closest to your original proposal while avoiding some of the theoretical concerns of the linear model.
