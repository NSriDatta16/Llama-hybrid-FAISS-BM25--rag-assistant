[site]: datascience
[post_id]: 72028
[parent_id]: 37073
[tags]: 
The best solution is definitely using word vectors . Create your own with Keras' Embedding() layers, for example: maybe powerful, but the pehaps the slowest solution. Create your own with gensim library: very quick and simple, I'd go for that. Dowload Google's pretrained Glove embeddings and apply them directly: quick application, but the whole file of pretrained vectors is very large. Once you have trained embeddings, you can represent each ngram as a vector of word embeddings, with shape: ( number of words in ngram , embedding size ) If you are looking at some very crude but fast solution, you could then average the ngram vectors into one, and compute Euclidean distance metrics between them. That's the fastest way to deal with the problem IMHO. . - . - . - . - EDIT: Another fast solution you can do directly in gensim is to train a doc2vec from scratch. In that way, you'd immediately get an embedding vector for the whole document (i.e. ngram). I've never tried doc2vec on small pieces of text such as ngrams though, baymbe it's worth to give it a try. In gensim it's few lines of code.
