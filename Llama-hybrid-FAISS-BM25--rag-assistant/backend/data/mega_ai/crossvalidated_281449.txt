[site]: crossvalidated
[post_id]: 281449
[parent_id]: 
[tags]: 
What exactly is overfitting?

Many people (including me) is thinking or used to think that an overfitted model is the model in which the training error >> the validation error. But after reading this very interesting comment by @Firebug, I suddenly realized that it is not true. Random Forest is a perfect example of this, the training error is often closed to 0, the out-of sample is often far smaller than the latter but close to the test sample. Another example is presented below: People often refer the green curve as overfitting, and the black curve is better because the testing error of the green curve is lower than that of the training set. But it can happen that even the testing error of the green curve is lower than training error, but on the other blind test, the green curve is still better than the black curve. So my questions are: Is the black curve better than the green curve? what exactly is overfitting, and what is the proper way of identifying an overfitted model? It is not true to say that an overfitted model is worse than the non-overfitted model?
