[site]: datascience
[post_id]: 37392
[parent_id]: 37390
[tags]: 
This is not really a data science specific question, so you might want to ask elsewhere; in any case, you need to provide more information! What error message are you receiving? Are you running out of memory? What size are the images? Unless your images are really really large, I would not expect you to be running out of memory, given you have 32Gb. however, if you have built a massive network, where e.g. many ConvNet filters are then passed to an extremely wide FullyConnected layer (Dense layer), you will have a huge number of weights, which may result in a memory error. Try creating your model (or a similar one) using Keras instead of pure Tensorflow, and then using model.summary on the compiled model. This will show you a nice overview of the number of weights in each layer.
