[site]: datascience
[post_id]: 123102
[parent_id]: 
[tags]: 
Why do semantically different words produce similar embeddings?

I am comparing words in HuggingFace web UI using e5-small-v2 , one of the best vector embedding models: Assuming that the scores are in the range from 0 to 1, how come all the scores are so high? In fact, I was not able to produce any example with a score below 0.7. Is there something basic I am missing about vector embeddings?
