[site]: crossvalidated
[post_id]: 247987
[parent_id]: 
[tags]: 
Algorithm to detect periods with significant trends

I have a time series of data, which usually is pretty stable (but with noise). Sometimes, it starts to drift linearly. It could also reset back to the stable position and continue the same drift. Is there an algorithm that can detect these periods of drifts? My naive method would be: Try to find an period of more than 100 samples where a linear fit could decrease the variation significantly (say R^2 > .8 ). Make this period as big as possible. Continue with #1, but also impose a penalty on detecting too much periods. Does something like this exists?
