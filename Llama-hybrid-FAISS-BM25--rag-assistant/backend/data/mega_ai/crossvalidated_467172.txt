[site]: crossvalidated
[post_id]: 467172
[parent_id]: 466681
[tags]: 
This is not so much an "answer" as an attempt to provide a framework for the question (with the ultimate goal to give an answer). Note: I like to use the symbol $p$ to denote a probability density function (or mass function) as is common in the Bayesian literature, so I will use $\theta$ to denote the probability of a defective output. Let $y_i \in \{0,1\}$ denote the outcome of a Bernoulli trial where 1 denotes "success" and 0 denotes "failure". The probability density function (sometimes called the mass function) is $$ p(y_i|\theta) = \textsf{Bernoulli}(y_i|\theta) = \theta^{y_i}\,(1-\theta)^{1-y_i} , $$ where $\theta \in [0,1]$ is the probability of success: $$ p(y_i = 1|\theta) = \theta . $$ With this setup, a defective output from the machine counts as a "success". Let $s = \sum_{i=1}^n y_i$ denote the number of successes in $n$ independent trials. The distribution of $s$ is binomial: $$ p(s|n,\theta) = \textsf{Binomial}(s|n,\theta) = \binom{n}{s}\,\theta^s\,(1-\theta)^{n-s} . $$ To complete the model, let $p(\theta)$ denote the prior distribution for $\theta$ . The prior predictive distribution for $y_1$ is $$ p(y_1) = \int p(y_1|\theta)\,p(\theta)\,d\theta , $$ which is a Bernoulli distribution for which the probability of success equals the prior expectation of $\theta$ : $$ p(y_1 = 1) = \int p(y_1=1|\theta)\,p(\theta)\,d\theta = E[\theta] . $$ Therefore we can write $$ p(y_1) = \textsf{Bernoulli}(y_1|E[\theta]) . $$ The joint distribution for $s$ and $\theta$ conditional on $n$ is $$ p(s,\theta|n) = p(s|n,\theta)\,p(\theta) . $$ The posterior distribution for $\theta$ conditional on $s$ and $n$ is $$ p(\theta|s,n) \propto p(s,\theta|n) . $$ Finally, the posterior predictive distribution for $y_{n+1}$ is \begin{equation} p(y_{n+1}|s,n) = \int p(y_{n+1}|\theta)\,p(\theta|s,n)\,d\theta = \textsf{Bernoulli}(y_{n+1}|E[\theta|s,n]) , \end{equation} where $$ E[\theta|s,n] = \int \theta\,p(\theta|s,n)\,d\theta $$ is the posterior expectation for $\theta$ . That's the framework. As I understand it, the question is about the prior distribution $p(\theta)$ . The prior should incorporate all non-sample information available. For example, if only certain values of $\theta$ are possible because of physical considerations, then the prior should reflect that.
