[site]: crossvalidated
[post_id]: 297712
[parent_id]: 288504
[tags]: 
Generally, the aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for the purpose of dimensionality reduction. So, the target output of the autoencoder is the autoencoder input itself. It is shown in Auto-Association by Multilayer Perceptrons and Singular Value Decomposition that If there is one linear hidden layer and the mean squared error criterion is used to train the network, then the k hidden units learn to project the input in the span of the first k principal components of the data. And Nonlinear Autoassociation Is Not Equivalent to PCA shows If the hidden layer is non-linear, the autoencoder behaves differently from PCA, with the ability to capture multi-modal aspects of the input distribution.
