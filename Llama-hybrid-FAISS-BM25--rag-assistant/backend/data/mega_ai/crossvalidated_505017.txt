[site]: crossvalidated
[post_id]: 505017
[parent_id]: 504593
[tags]: 
There are several possible scenarios when one would think about calibrating probabilities: The model is misspecified or not optimally trained. That will be the case when non-linear relationships are modeled with a linear learner; or model is too rigid due to excessive regularization (model underfits); or to the contrary, the model is too flexible (overfit or data memorization). Under/over-fit may also be due to having too few/many learning epochs or bagged trees. A wrong objective function for predicting probabilities was chosen. That will be the case for distorted probabilities predicted by sklearn RandomForest, where they use "gini" or "entropy" for objective function. Classifiers with logloss as objective function are supposed to produce non-biased probability estimates given they have enough data to learn from. Onesidedness of probabilities may explain probability distortions near interval end, but that would not explain distortions in the middle. Using optimized objective function instead of exact one. This is the case with XGBoost Random Forest implementation: XGBoost uses 2nd order approximation to the objective function. This can lead to results that differ from a random forest implementation that uses the exact value of the objective function. In all three scenarios (including having too little data, where it's unclear if calibration results will generalize well when new data arrives), calibration time is better spent on (1) correct model specification (2) choosing right metric (objective function) to optimize (3) collecting more data. Case 1. Right objective, enough data (Logistic Regression from sklearn) Case 2. Wrong objective (Random Forest from sklearn) Case 3. Right objective, needs more data (Random Forest from XGBoost) Case 4. Right objective, needs more data PS There is nothing wrong with the "wrong" metric as all the classifiers, including sklearn's Random Forest, Na√Øve Bayes, or SVC, perform very well for certain tasks. The expectation these will behave well for predicting probabilities is wrong and misspecified. Calibrating well specified and well trained models, even though may show better in-sample result, most probably is fitting to test and will lead to worse generalization to new data.
