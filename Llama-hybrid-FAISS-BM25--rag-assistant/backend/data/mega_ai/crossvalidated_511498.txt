[site]: crossvalidated
[post_id]: 511498
[parent_id]: 
[tags]: 
Why is the expected value of the optimism of the training error for a linear model equal to $\frac{2}{ n}d\sigma ^{2}$?

In the book Elements of statistical learning 2 on page 229, they express the expected optimism of the training error as: $$ \omega=\frac{2}{N} \sum_{i=1}^{N} \operatorname{Cov}\left(\hat{y}_{i}, y_{i}\right) $$ This is followed by the following paragraph: This expression simplifies if $\hat{y}_{i}$ is obtained by a linear fit with $d$ inputs or basis functions. For example, $$ \sum_{i=1}^{N} \operatorname{Cov}\left(\hat{y}_{i}, y_{i}\right)=d \sigma_{\varepsilon}^{2} $$ for the additive error model $Y=f(X)+\varepsilon$ I'm struggling to see where this relation comes from. Why is the sum of the covariances between predicted and true values equal to the the number of basis functions times the variance of the errors in the linear model? What are basis functions in this context, exactly?
