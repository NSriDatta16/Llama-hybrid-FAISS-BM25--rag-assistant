[site]: crossvalidated
[post_id]: 49061
[parent_id]: 
[tags]: 
Latent variables in Bayes nets with no physical interpretation

In Pattern Recognition and Machine Learning Bishop writes about Bayes networks: For practical applications of probabilistic models, it will typically be the highernumbered variables corresponding to terminal nodes of the graph that represent the observations, with lower-numbered nodes corresponding to latent variables. The primary role of the latent variables is to allow a complicated distribution over the observed variables to be represented in terms of a model constructed from simpler (typically exponential family) conditional distributions. And after a few lines: The hidden variables in a probabilistic model need not, however, have any explicit physical interpretation but may be introduced simply to allow a more complex joint distribution to be constructed from simpler components. What do you think he means by this type of hidden variables (with no physical interpretation)? What can be a simple example of this? I thought about mixture of gaussians, but they donâ€™t correspond to a situation where the variables we are interested are highernumbered.
