[site]: crossvalidated
[post_id]: 347462
[parent_id]: 347455
[tags]: 
You don't need a standard deviation for your one point to know it it's an outlier. You just need to know its deviation from the average of the other points. By finding the mean $\mu = n^{-1} \sum_i x_i$ and variance $\sigma^2 = (n-1)^{-1} \sum_i (x_i - \mu)^2$, you are effectively fitting a model to your data saying that points are being generated by draws from a distribution with that mean and variance. If you further assume that distribution to be normal, then you can actually compute the probability $Q$ to get a value $x$ with $z = (x - \mu)/\sigma$ larger than some deviation as $Q = {\rm erfc}(z/\sqrt{2})$. Conversely, given an acceptable error rate at which you will are willing to mis-identify outliers, you can compute the threshold $z$ over which you should classify a point as an outlier. Without some assumption for the shape of the distribution, you won't be able to associate a deviation with an exact probability, but $z$ is still a useful characterization of the degree of deviation.
