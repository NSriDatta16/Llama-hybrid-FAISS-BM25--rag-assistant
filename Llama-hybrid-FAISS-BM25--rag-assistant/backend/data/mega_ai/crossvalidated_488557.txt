[site]: crossvalidated
[post_id]: 488557
[parent_id]: 
[tags]: 
How to quantify sensitivity in time series model?

X and Y are time series of length T. X is the predictor and Y is the response. A linear model is fitted as follows: $$\hat{Y_t}=\alpha+\sum_{i=1}^{N}{\beta_iX_{t-i}}$$ where $\beta$ 's and $\alpha$ are such that they minimise squared errors between $Y$ and $\hat{Y}$ . Now I want to know "How sensitive is $\hat{Y}$ to X?" In an ordinary linear regression (without the temporally lagged quantities on the right), the answer would just be $\beta$ , but here I have $N$ different $\beta$ 's. Are there ways in which I can condense the $N$ different $\beta$ 's into a scalar quantity? Or any other method to answer "How sensitive is $\hat{Y}$ to X?" Potentially relevant information but ignore if not needed: X and Y vectors are highly auto-correlated. For example, X is daily temperature, and Y is daily ice cream sales. When I say "How sensitive is $\hat{Y}$ to X?", I mean how much is Y affected for changes in X. For example, ice cream sales would likely be very sensitive to daily lagged temperature, but laptop sales would probably be insensitive to daily lagged temperature.
