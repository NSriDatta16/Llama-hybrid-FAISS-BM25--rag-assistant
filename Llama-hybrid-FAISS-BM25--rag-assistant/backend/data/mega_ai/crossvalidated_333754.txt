[site]: crossvalidated
[post_id]: 333754
[parent_id]: 
[tags]: 
Attention mechanism on sequences of state vectors of different sizes?

I am working with multivariate time series of multiple patients and want to implement this paper It proposes a bidirectional LSTM with an attention mechanism. Basically, after each new "time input" for a patient, we keep track of the state vector, represented by node h in the schema below, and then link them all (in the neural network sense) to multiple reading heads. The paper isn't too long, but I will try to add the most relevant parts as images below. Hopefully it will be enough for an experienced person to understand what I'm trying to do Even though I'm pretty new at tensorflow (which is what I have to implement this with, since it will sit on top of existing tensorflow code), I can kind of see how that would work code wise. I'm not sure there are a preexisting high level functions to do this but it should still be doable. However, I have some question on the more theoretical side of things. As far as I understand, T is variable across patients. I understand the logic, but how would I go about implementing this? Do I have to set some max_T corresponding to the longest time series seen in my dataset, create that many h nodes (state vectors) and fill every unobserved state vector for patients who have less than max_T observations with 0's? Would the extra nodes full of 0s harm the implementation? I know neural nets can typically handle input of different length, but I'm not sure whether this applies here.
