[site]: crossvalidated
[post_id]: 283730
[parent_id]: 
[tags]: 
The value of a terminal state in reinforcement learning

I am working in an environment where each transition rewards 0 except for the transitions into the terminal state, which reward 1. My question is, how should I define the value of the terminal state? Right now I just initialize all values to be 0, but since the terminal state has no transitions it remains 0 while the states around it have positive value. So if I want to improve my policy by making it greedy in respect to the neighbor states, the states next to the terminal states won't want to choose the terminal state (since there are positive non-terminal states neighboring it). Does this sort of environment just not work with state-value dynamic programming methods of reinforcement learning? I don't see how I can make this work.
