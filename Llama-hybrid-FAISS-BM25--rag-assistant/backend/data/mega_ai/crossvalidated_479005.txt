[site]: crossvalidated
[post_id]: 479005
[parent_id]: 312642
[tags]: 
Here is what I think: I agree with your explanation. I would only add the reason why each $\cfrac{\partial C_x}{\partial w}\approx0$ , which I believe is due to the saturation of the neurons mantained at the first epochs given the ''big'' standard deviation from the random weights generator ( $\sigma=1$ ). I totally agree with your explanation. Besides this is also agreed in other posts: Data Science 1 and Data Science 2 . and 4) Yes, I think you got it right. I believe so due to the fact that if we want the weights of our network to be normally disributed by $\mathcal{N}\left(0,\sigma \approx \cfrac{1}{\sqrt{n}}\right)$ (following the reasoning of the book at pages 94-96) we shoud be able to reduce its initial $\sigma = 1$ from the initial Normal distribution $\mathcal{N}(0, 1)$ (given by the random weights generator). This is something we can heuristically do by re-scaling them with a factor $\frac{1}{\sqrt{n}}$ . Note: I believe that my last argument is even roughly heuristic in the sense of successive activations of the hidden layers following a Normal distribution. For example, in the third layer of neurons, the activations of the second layer (Normally distributed following the reasoning of pages 94-96) are being multiplied to some Normally initialized weights. Thereby we are multiplying two Normal distributions, so no Normal distribution is held in this and successive layers.
