[site]: datascience
[post_id]: 30678
[parent_id]: 30676
[tags]: 
The derivative you see here is important in neural networks. It's the reason why people generally prefer something else such as rectified linear unit . Do you see the derivative drop for the two ends? What if your network is on the very left side, but it needs to move to the right side? Imagine you're on -10.0 but you want 10.0. The gradient will be too small for your network to converge quickly. We don't want to wait, we want quicker convergence. RLU doesn't have this problem. We call this problem " Neural Network Saturation ". Please see here
