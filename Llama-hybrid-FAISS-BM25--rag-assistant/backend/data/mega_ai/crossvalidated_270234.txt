[site]: crossvalidated
[post_id]: 270234
[parent_id]: 
[tags]: 
What are the most adequate parameters for assessing bias in propensity scoring systems?

I am analyzing a very large dataset featuring scores in a sport. The only information I have is the scores of many athletes in various locations, which are organized in the following way: Location X: Athlete 1: Score A | Score B , Athlete 2: Score A | Score B (5 scores: five judges giving each a score A and a score B for a performance) Location Y: Athlete 3: Score A | Score B , Athlete 4: Score A | Score B (5 scores: five judges giving each a score A and a score B for a performance) and so on (as location and athletes change) I have calculated the mean of the five scores of athletes 1 and 2 for scores A and B at the location X . And did the same for location Y and so on. I did that to see whether there is a difference in the original results. The original results trimmed the highest and the lowest scores and calculated the mean of the three remainder scores. I have also calculated the standard deviation of each set of scores. That is, the standard deviation of the five scores of athlete 1 and 2 at the location X and son on. I did that to understand how spread these scores are around the mean in each set of scores. Finally, I have calculated the residual of each score in relation to the mean and summed the absolute value of these residuals to have a sense of how large these residuals are for each judge and for each athlete in each location. My questions then are: What do you think of these parameters to assess judging bias through the analysis of the mean, standard deviation, and sum of residuals of scores? Would you suggest any other statistics I should be looking at to assess to what extent the cognitive bias of each judge (and of the five judges altogether) can be detected through their respective scores to athletes in different locations?
