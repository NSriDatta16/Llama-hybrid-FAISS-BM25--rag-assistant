[site]: datascience
[post_id]: 63832
[parent_id]: 63828
[tags]: 
Several options you can try, all of them have different preconditions you need to consider. 1. Direct Ensemble (with high number of n, should be the first choice) If n >> 103 you can directly use standard ML procedure different models and parameters with CV and find the best fit. Since most of the time its not the case, you may want to use ensemble tree models since they outperform other learners when number of observations are relatively small compare to number of features. 2. Clustering If those 100 features are sourced by a common factor, you can cluster those features with optimal K (see silhouette score ) and add those cluster labels as 3rd feature on your model. Finally your model becomes n observations, 3 features. 3. Fit Twice In case of classification, you can first fit 100 features and use their probability score ( predict_proba in sklearn ) as 3rd feature on your model. 4. PCA + Fit You can use PCA to 100 features and obtain p features. After that with p+2 features you can construct your model. Again common factor condition applies in here. (In direct approach you can apply PCA to 102 features as well.) Final Comment: All of above methods are should be tried and the best one should be picked. However, if you have n >> 102 observations you should directly fit the model with strong learners ( xgboost , lgbm , nn 's etc.) without clustering/twice fitting since those methods generates error by their nature in addition to final learner error.
