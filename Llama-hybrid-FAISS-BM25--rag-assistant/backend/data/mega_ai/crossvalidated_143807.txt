[site]: crossvalidated
[post_id]: 143807
[parent_id]: 
[tags]: 
Correlated time series at different aggregation levels

In my time series data I discovered the following behaviour. My time series are available in 500ms intervals and are about sensor data like energy consumption, motor speed, ... When correlating those time series at the shortest time interval available, I don't get any correlation, because the signals of course have some outliers, noise, maybe lag. When I calculate the mean value for each minute they are correlated, because those aspects are negligible after the aggregation. My question is now if there is some "theory" behind that "phenomenon"? Is there a scientific name or concept of that? Does it make sense to find out the aggregation level with the highest correlation and what does it tell me? It would be very helpful if someone could help me how I should classify this observation and if there it's a known and common challenge and how it is usually handled.
