[site]: datascience
[post_id]: 47666
[parent_id]: 
[tags]: 
Can feedback loops occur in machine learning that cause the model to become less precise?

In discussions about ML algorithms, in for instance crime prediction, it is often claimed by non-experts that there are problems with feedback loops causing the model to become biased and give the wrong results. Basically saying that the model's predictions give more attention to that type of data, and when retraining with the results, the predictions become skewed so even more attention is given to the same data type, and so on. Is this true? I would think that retraining the model with new data would make it more precise, regardless of how that data originated.
