[site]: crossvalidated
[post_id]: 309269
[parent_id]: 
[tags]: 
Criticizing the algorithm

I would like to ask if my algorithm can be improved or has some serious flaws. I have a time series of $n$ points (so the algorithm is offline) and $k$ well-defined complex multidimensional models (states) for each data point. Ideally, a time series stays in state $M_0$, but I know that at some points the underlying model may change and stay in this state for some time. We have a classical Hidden Markov Model case and can use Viterbi/Baum-Welch algorithms to train the model and find the optimal path between states. The goal is to decimpose the tine series to intervals of different underlying states. What I do: I calculate log likelihood of each models for each data point so I have a matrix of log likelihoods of dimension $k$ by $n$. Then I subtract log likelihood of baseline model $M_0$ from all other models' log likelihoods for each data point, having a matrix of $k-1$ by $n$. Then I run maximum subarray sum algorithm for $k-1$ alternative models $M_A$ and choose the interval with the maximum evidence of difference from model $M_0$, in other words - with the maximum sum of $\sum_{l = i}^j \Bigl( \log L(D_l|M_A) - \log L(D_l|M_0) \Bigr) $ where $l$ denotes a time point. I repeat the procedure iteratively for the regions to the left and to the right of the found segment. I stop when the log-likelihood change becomes smaller than some pre-defined threshold. 1) Is there a major flaw? Why HMM could be better? Prior probability of $M_0$ is much higher than probabilities $M_A$. 2) Can it be improved by taking in account potential covariance structure of two consecutive data points? I have some kind of answers to these questions, but I am not satisfied with them, so I am asking the community to criticize the approach.
