[site]: crossvalidated
[post_id]: 172932
[parent_id]: 172902
[tags]: 
If you want to measure overall performance, I recommend approaches like nested cross-validation instead of comparing the output of models trained on different folds. The latter isn't the way to go. I don't recommend Platt scaling in general. If you really need probabilities, don't use an SVM. You are almost always far better off with (kernel) logistic regression than with an SVM+Platt scaling. You are right to distrust discrete metrics like accuracy that are based on a single (arbitrary) cutoff on the decision values of your model. If you have probabilistic output, you should consider using proper scoring rules such as Brier score , which is closely related to MSE, or log loss. Alternatively, since you don't really get reliable probabilistic output from an SVM, you can use metrics like area under the Receiver Operator Characteristic curve (aka concordance), though be aware that these are less sensitive than proper scoring rules.
