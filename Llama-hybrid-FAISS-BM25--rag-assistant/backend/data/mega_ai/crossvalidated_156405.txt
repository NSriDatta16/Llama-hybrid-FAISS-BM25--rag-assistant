[site]: crossvalidated
[post_id]: 156405
[parent_id]: 156377
[tags]: 
There is a sub-field of analysis for dynamical systems called " Symbolic Dynamics " and you might find useful approaches drawn from this sub-field. Basically, the state space is partitioned into mutually exclusive and collectively exhaustive cells, and each cell is given an abstract label. The dynamics of the system then can be represented as strings over the labels. Drawing on this idea, you might approach as a grammar learning problem, where a grammar is a set of rewrite rules to generate strings. The tokens in your grammar are: $\{A,B,C,D,1,0\}$. The strings consist of sequences of one or more $\{A,B,C,D\}$ followed (eventually) by $\{1,0\}$. You want to learn classification of combinations of grammar rules that end either in $1$ or $0$. With this approach, you can train on full (coded) time series, without the problems associated with state compression or recoding as you described in 2). Though not about grammar learning in specific, here is a good lecture that describes "structure learning" and its benefits: https://youtu.be/97MYJ7T0xXU?t=22m43s The grammar learning approach would be appropriate if the state space of your system is richly structured. If, instead, the state space is fairly simple, you might use other approaches that are less focused on structure learning. For example, you might look into this paper: " Factorial Hidden Markov Models for Learning Representations of Natural Language ", which looks promising for your type of problem.
