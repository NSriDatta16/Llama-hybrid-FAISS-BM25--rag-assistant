[site]: crossvalidated
[post_id]: 533101
[parent_id]: 
[tags]: 
How to check for a biased/irregular number of occurrences of a given pattern in a sequence of numbers?

As a heads up, my knowledge of statistics and probability is minimal at best but I need help on something for an application I’m building. Ok I have a sequence of values either 0 or 1 that might look something like this, 0010111010 (can be 1000’s of digits in length). I currently have a program that looks at that sequence and counts the number of occurrences of any given pattern up to 6 digits. For example in the sequence; 0010111010 there are five 0’s, five 1’s, one 00, three 01, three 10, two 11, …, zero 000000, zero 000001. So it looks for how many occurrences there are for all 126 possible patterns and counts each one. So my question is, What would be the best way (if there is a way) to check for a biased/irregular number of occurrences of a given pattern? For example in the sequence mentioned above there are 9 2 digit patterns, that means on average there should be 2.25 occurrences of each possible 2 digit pattern (I’m not even sure on that because each digit effects multiple patterns results), if it turns out the sequence has a biased count of any given pattern it needs to detect it. I’ve tried applying the chi-squared test and then getting a P-value for it but I got some weird results that would suggest even a random sequence of 4000 numbers is biased towards certain patterns and not random like I would have thought. Either way could someone tell me if what I’m looking to do is possible and if it is, the easiest method as well as how to I’d do it. Summary: I need a way to know if a pattern of say 000 is more or less frequent in a sequence than it would in a random sequence. And the patterns can over lap so 0000 has 2 occurrences of 000 not 1.
