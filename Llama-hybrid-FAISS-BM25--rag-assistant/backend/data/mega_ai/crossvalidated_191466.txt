[site]: crossvalidated
[post_id]: 191466
[parent_id]: 
[tags]: 
Why isn't BFGS and LBFGS implemented in more areas where gradient descent is applied?

BFGS and LBFGS algorithms are often seen used as optimization methods for non-linear machine learning problems such as with neural networks back propagation and logistic regression. My question is why arent they implemented in everything that gradient descent is even remotely related to, LINEAR regression for example? is it simply a matter of overkill and an unnecessary piece of machinery in this case (but it would still theoretically improve training time) or is there an actual compatibility issue where BFGS and LBFGS needs non-linearity for them to work?
