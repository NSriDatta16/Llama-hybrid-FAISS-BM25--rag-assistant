[site]: crossvalidated
[post_id]: 514476
[parent_id]: 423412
[tags]: 
If you are using a probabilistic classifier, that outputs a probability of class membership for each class, then averaging the weights or choosing the weights to minimise the maximum imbalance both seem reasonable. However you would likely to end up with a classifier that grossly over-predicts some classes, so I would subsequently post-process the outputs of the trained model on a label-by-label basis to account for the difference in relative class frequencies in training set and under operational conditions (which basically requires multiplying by the ratio of operational and training set prior probabilities and then re-normalising the probabilities to sum to one). Unlike the re-sampling of the training set, that can be done on a label-by-label basis.
