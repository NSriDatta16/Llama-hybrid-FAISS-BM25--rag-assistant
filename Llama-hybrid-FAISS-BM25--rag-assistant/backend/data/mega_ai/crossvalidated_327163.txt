[site]: crossvalidated
[post_id]: 327163
[parent_id]: 
[tags]: 
Is there any research and/or guidelines on the amount of epochs depending on NN architecture?

I am interested is there any research done and if there are general guidelines for the number of epochs needed to train the neural network optimally. Does anybody have material to share on this? EDIT: I've realised that it would be good to explain a reason behind the question. I am a bit annoyed by the early stopping technique as it usually implies that the network is too powerful if the validation loss starts to diverge from the training loss. It feels that it is a better approach to make a simpler model and train it long enough until it generalises better. For example, this guy shows that generalisation may continue after reaching sufficiently low loss. But it is completely unclear when a "proper" generalisation should kick in. Are 100 epochs enough? What about 1000? 10000? How does it depend on my number of parameters? Structure? Type?
