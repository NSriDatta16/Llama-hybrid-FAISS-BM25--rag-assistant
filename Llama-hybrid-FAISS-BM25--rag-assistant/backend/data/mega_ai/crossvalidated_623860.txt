[site]: crossvalidated
[post_id]: 623860
[parent_id]: 
[tags]: 
Scikitlearn: Why are hyperplane coefficients not available if kernel is not linear

I am interested in learning the math behind support vector machines. So far, I understand that SVMs attempt to find hyperplanes that maximize the margin distance between support vectors associated with the classes that you are trying to classify. I also understand that using a kernel allows one to project the data points onto higher dimensions to help classify data that is not linearly separable. I am playing around with an SVM using the Scikitlearn Python library and I found that the .coef_ attribute returns the normal vectors associated with the hyperplanes that are calculated in the fitting procedure. I am interested in looking at these normal vectors to get a sense of how much each feature contributes to calculating the hyperplanes. However, it seems that the normal vectors are only available if you use a linear SVM. I don't see why the normal vectors wouldn't be available if a kernel is used. From what I understand, you are just finding hyperplanes in a space with more dimensions than the number of features in your original dataset. Shouldn't you be able to get the normal vectors to those hyperplanes as well? What am I missing here? I fully appreciate any insight into this question as I am still a novice when it comes to the math behind a lot of machine learning techniques. Thanks!
