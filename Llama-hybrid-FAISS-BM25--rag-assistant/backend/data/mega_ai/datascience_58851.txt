[site]: datascience
[post_id]: 58851
[parent_id]: 31499
[tags]: 
Gradient descent (on which neural networks rely for learning) is sensitive to feature scaling; you should probably normalize the x-values to start with. In your particular shift-by-500 case, I'd guess that the optimal weights lie in a small range of near-zero numbers, and so the gradient descent has a hard time finding those appropriate weights. So, perhaps not a local minimum so much as a plateau, i.e. the sigmoids are getting saturated? https://datascience.stackexchange.com/a/13221/55122
