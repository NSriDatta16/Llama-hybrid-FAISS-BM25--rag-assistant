[site]: datascience
[post_id]: 123437
[parent_id]: 123430
[tags]: 
Depending on the architecture of your RAG system, there can in some cases be efficiencies in using the same embeddings as your LLM of choice, but this is not a hard requirement. For example, if you generate embeddings, store in a vector dB, then take a prompt, embed it, and use it to query your vector dB in order to get a ptr to the original doc, you can use whatever embeddings you see fit as you'll have the original doc to feed back into the LLM. You'd definitely want to use an embedding model that includes your language of choice, or else multilingual embeddings. I'd recommend testing with multiple different embeddings and seeing what works best for your use case.
