[site]: crossvalidated
[post_id]: 341693
[parent_id]: 341683
[tags]: 
In either direction of the bias, you need to expect your confidence interval not to have the desired coverage properties. Recall that, in general, a well-calibrated $1-\alpha$-level CI has the property that the intervals calcaluted from the recipe will cover the true value in $(1-\alpha)\cdot100$% of the cases it is computed. That result however hinges on the condition that the distribution of the estimator from which the CI is constructed is "correct." That is generally not the case when the estimator is biased. In that case, the point estimates, and hence the CIs constructed around the point estimates, will tend to cluster around the expected value of the estimator, which, due to its bias, is not the true value. Through duality between tests and confidence intervals , you then need to expect your test to reject too frequently: the confidence interval collects the hypotheses you would not have rejected---if the true value is in that set too rarely, you reject too often. Here is a little simulation illustrating the issue for the case of the LS estimator of the coefficient $\phi$ of an AR(1) model $Y_t=\phi Y_{t-1}+u_t$. It is well-known (see e.g. here ) that this estimator is biased (downward), in particular for small $T$ and large $\phi$ when the regression also includes a constant. library(dynlm) ar.coeff CI[i,1] & ar.coeff The coverage hovers around values like > mean(coverage) [1] 0.873 instead of the 95% one might hope for. Here is a plot of the confidence intervals for 60 draws from the underlying AR process, showing that these sometimes lie "too far to the left." R code: plot(ar.coeff,0, cex=2, col="red", pch=19, xlim=c(0,1.2), ylab="confidence intervals", yaxt='n', xlab="") segs
