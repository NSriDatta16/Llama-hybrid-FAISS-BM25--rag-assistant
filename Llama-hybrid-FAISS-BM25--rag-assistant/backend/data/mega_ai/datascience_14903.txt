[site]: datascience
[post_id]: 14903
[parent_id]: 14871
[tags]: 
tf-idf will learn a vocabulary, idf, and some will also learn stop words (based on min_df, max_df, max_features). Read over sklearn's TfidfVectorizer and you can see the attributes that the fit method will set. When you expose a trained tf-idf to new data it will transform that data into a vector of the same size as your original data using the vocabulary to construct Term_Counts which are then converted into your tf vector. The value in this is that you can use another model to predict an outcome based on the tf_idf as each new document will have the same size tf_idf vector as the documents you used to train the model. Otherwise you couldn't use it to make a prediction! For example with a naive bayes classifying tfidf: tfidf = TfidfVectorizer() X = tfidf.fit_transform(X_train) nb = MultinomialNB() nb.fit(X, y_train) # When you receive a new document X = tfidf.transform(new_doc) prediction = nb.predict_proba(X) And I don't think you would want to refit the model. If you want some kind of continuous real-time update consider implementing a bayesian update
