[site]: crossvalidated
[post_id]: 169925
[parent_id]: 169887
[tags]: 
I think there are some choices that work with any classifier: Impute the missing values with a single value, like the mean or median from the training set or some value predicted from the observed parts of the input, or just use a random number or a constant. Use several different values for the unknowns and aggregate the results, e.g. average them Apart from that you could use tree based classifiers (e.g. random forests) and if a tree needs to evaluate a split on a missing feature, it could just pass the data down to both child nodes. A third option is to use a generative classifier that models the full joint distribution $p(x,y)$ where $x$ are your inputs and $y$ the classification label. With that, you would ideally marginalize over the unknown parts of $x$, i.e. you would try any value for the unknown parts of $x$ and average the outcomes weighted by the probability of that imputation. This could be done either in analytically in closed form for some classifiers, e.g. a Linear Discriminant Analysis model, or approximately by sampling the unknowns, e.g. for a Restricted Boltzmann Machine or the deep variants thereof (which are related to feed forward neural networks).
