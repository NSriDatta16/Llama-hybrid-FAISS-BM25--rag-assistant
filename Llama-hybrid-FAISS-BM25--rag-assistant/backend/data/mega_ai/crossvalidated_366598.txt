[site]: crossvalidated
[post_id]: 366598
[parent_id]: 
[tags]: 
understanding natural policy gradient

I'm reading this paper on Natural Policy Gradient https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf and have some questions regarding how it works. I'm coming at this from an ML background and do not have a very strong math background so please bear with me. The argument that is being made is that, the gradient that is calculated in standard gradient descent is noncovariant . The definition that I've found of noncovariant is that (from what I can tell) if you reparameterize the policy but the output probability distribution is the same, you can get different directions of steepest descent? To deal with this, the author looks to find a direction which is covariant using a particular metric tensor. When looking to choose a $d\theta$ such that the distance between $\eta(\theta)$ and $\eta(\theta + d\theta)$ is maximized, solving gives us $$d\theta = \frac{1}{2 \lambda} G^{-1} \nabla \eta({\theta}) $$ where $G$ is a metric tensor which, in euclidean space, corresponds to the identity matrix. My understanding of what a metric tensor is is very sketchy but as far as I can tell it's like a mapping from some parameter space to a distance-value according to some metric (?). To improve the optimization procedure he uses the Fisher Information Matrix $F$ instead of $G$. The author says that the average reward is technically a function on the set of distributions $\pi_\theta$ rather than on the parameters $\theta$ themselves and suggests transforming the gradient $\nabla \eta(\theta)$ by $F(\theta)^{-1}$ instead since, instead, what we should be looking for is to improve the probability distribution directly. I'm not entirely sure what exactly this transformation does ? If I think about the gradient as just a general greatest rate of change and then transforming it by the metric tensor then I guess intuitively $d\theta$ will be the update that maximizes the informational difference (which seems to try and push the policy to be almost deterministic?). While I can buy this to some degree I'm having quite a bit of trouble understanding how these are going to be different in terms of how $\eta(\theta)$ changes. Is it possible that optimizing using policy gradient and natural gradient will simply arrive at the same solution if given enough time? Hopefully someone can spot some glaring flaws in my understanding that might help me understand! Thanks
