[site]: crossvalidated
[post_id]: 205815
[parent_id]: 
[tags]: 
Is it a mistaken idea to use standardized coefficients to assess the relative importance of regression predictors?

There are various questions that speak to the relative merits of various methods of assessing the importance of regression predictors, for example this one. I noticed that in this comment @gung refers to the practice as a "mistaken idea", linking to this answer in support of this claim. The final paragraph of the answer is the relevant part. I feel this issue deserves its own question, and also am a little unsure about some aspects of the reasoning. The most important segment of the paragraph in question goes unless the true r is exactly 0, the estimated r is driven in large part by the range of covariate values that are used. Is this equivalent to saying that we shouldn't use standardized coefficients to assess importance because we might have randomly sampled a restricted range of $X1$ values and a wider range of $X2$ values? Then when we standardize this problem hasn't gone away, and we end up spuriously thinking that $X1$ is a weaker predictor than $X2$ ? Why does the problem go away if the true $r$ is exactly 0? How do other methods (e.g. looking at semipartial coefficients) do away with this problem?
