[site]: crossvalidated
[post_id]: 226926
[parent_id]: 226905
[tags]: 
If the vectors are orthogonal, you can just take the variance of the scalar projection of the data onto each vector. Say we have a data matrix $X$ ($n$ points x $d$ dimensions), and a set of orthonormal column vectors $\{v_1, ..., v_k\}$. Assume the data are centered. The variance of the data along the direction of each vector $v_i$ is given by $\text{Var}(X v_i)$. If there are as many vectors as original dimensions ($k = d$), the sum of the variances of the projections will equal the sum of the variances along the original dimensions. But, if there are fewer vectors than original dimensions ($k You may also want to calculate $R^2$ (the fraction of variance explained), which is often used to measure how well a given number of PCA dimensions represent the data. Let $S$ represent the sum of the variances along each original dimension of the data. Then: $$R^2 = \frac{1}{S}\sum_{i=1}^{k} \text{Var}(X v_i)$$ This is just the ratio of the summed variances of the projections and the summed variances along the original dimensions. Another way to think about $R^2$ is that it measures the goodness of fit if we try to reconstruct the data from the projections. It then takes the familiar form used for other models (e.g. regression). Say the $i$th data point is a row vector $x_{(i)}$. Store each of the basis vectors along the columns of matrix $V$. The projection of the $i$th data point onto all vectors in $V$ is given by $p_{(i)} = x_{(i)} V$. When there are fewer vectors than original dimensions ($k $$E = \frac{1}{n} \|x_{(i)} - \hat{x}_{(i)}\|^2$$ The goodness of fit $R^2$ is defined the same way as for other models (i.e. as one minus the fraction of unexplained variance). Given the mean squared error of the model ($\text{MSE}$) and the total variance of the modeled quantity ($\text{Var}_{\text{total}}$), $R^2 = 1 - \text{MSE} / \text{Var}_{\text{total}}$. In the context of our data reconstruction, the mean squared error is $E$ (the reconstruction error). The total variance is $S$ (the sum of variances along each dimension of the data). So: $$R^2 = 1 - \frac{E}{S}$$ $S$ is also equal to the mean squared Euclidean distance from each data point to the mean of all data points, so we can also think of $R^2$ as comparing the reconstruction error to that of the 'worst-case model' that always returns the mean as the reconstruction. The two expressions for $R^2$ are equivalent. As above, if there are as many vectors as original dimensions ($k = d$) then $R^2$ will be one. But, if $k
