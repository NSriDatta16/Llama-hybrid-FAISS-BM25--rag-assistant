[site]: crossvalidated
[post_id]: 252898
[parent_id]: 252849
[tags]: 
There are two common methods to get more-or-less continuous predictions. Don't build the trees out to purity, but instead enforce some sort of minimum node size, maximum depth, or minimum impurity to split. The effect is that some leafs (terminal nodes) will be impure, contributing some fraction to the predicted outcome. Your post doesn't mention random forests except in a tag, but if you fit $n$ trees, you have $n$ binary predictions. The $k/n$ predictions of a specific class gives you a value in $[0,1],$ representing the forest's confidence that a sample belongs to the class. (This method can also be combined with 1.)
