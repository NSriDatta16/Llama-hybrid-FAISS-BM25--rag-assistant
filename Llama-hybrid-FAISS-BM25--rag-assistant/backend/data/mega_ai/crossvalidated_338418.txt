[site]: crossvalidated
[post_id]: 338418
[parent_id]: 338341
[tags]: 
There are quite a few questions asked in this single question. I would try to answer as much as possible in a concise way. We build the network of neurons and train all of them together, otherwise what is the point in a connected network? True, we train them together. Though this is not the correct terminology to use, a network of neurons is called a neural network (again, not the exact definition, but I am trying to match your description) and we train a neural network. When we train a neural network, the weights associated with these connections between different layers of neural nets are "learnt". These weights can take up any value, it could be very small or larger than others. Now, dropout doesn't set the weight to zero completely. Remember, dropout is applied on a whole layer and has a probability associated and can keep changing from batch to batch*. So when we feed our first batch, $n_1, n_2, n_3$ might have there outputs zero, but other neurons still output values. [$n_1, n_2, n_3, n_4,.. n_{10}$ are neurons in some layer]. For next iteration, we might have $n_2, n_4, n_5$ with their outputs zero. The network is forced to learn with such perturbations and eventually adapts to learn even in absence of some data. As you pointed out, a neuron might learn to fire only when it sees a particular value. We would like to avoid neurons becoming specialised since this would lead to poor generalisation. It might not perform well against unseen data. Dropout is, thus, essentially a type of regularization technique and is used to avoid overfitting. It has been hard to explain mathematically why it works, but it is one of the successful techniques.
