[site]: crossvalidated
[post_id]: 445639
[parent_id]: 255657
[tags]: 
"Conventional" neural networks often don't go much deeper than a few hundred layers. In actuality, with the prominent use of skip-connections, the "effective" depth of the network is often one layer deep, it's just that that super layer becomes really expressive (one example of that is the U-net for image segmentation, which has several lower resolution levels, but has skip-connections at the high resolution levels that make it, in fact, quite shallow). The "layer" concept of plain feed-forward networks does not translate as well to other architectures these days. For a NeuralODE, for example ( What are the practical uses of Neural ODEs? ), we have a "layer analogue" that goes as deep as you want. For Deep Equilibrium Models , it is mathematically equivalent to infinity (see my other question here How do Deep Equilibrium Models achieve "infinite depth"? ).
