[site]: crossvalidated
[post_id]: 421978
[parent_id]: 242082
[tags]: 
I often think of it this way. In the fully Bayesian approach, we find the integral $$p(x^*|X) = \int p(x^*|\theta) p(\theta|X) \text{ d}\theta$$ as integrating over all possible models (infinitely many in fact), and we make a prediction taking all of these models "into consideration". As this is often intractable, we use the MAP estimate of the posterior $p(\theta|X)$ , which corresponds to evaluating the same integral but this time using a infinitely small part of $p(\theta|X)$ , namely at its maximum. In other words, we multiply $p(x^*|\theta)$ with a new "delta-distribution" located at the max of the posterior distribution and integrate this to obtain the prediction. The difference is therefore rather obvious: a fully Bayesian treatment corresponds to an infinite ensemble of models, where a given prediction $p(x|\textbf{x},\theta)$ is weighted by the model probability $p(\theta|\textbf{x})$ , i.e. more likely models will contribute more to the prediction. The MAP estimate of the parameters will give you the prediction from one specific model, namely the most likely one according to Bayes theorem. Ensemble theory shows us that we often obtain better generalization and more accurate predictions and therefore this will often be "better" than the MAP. Hope this helps.
