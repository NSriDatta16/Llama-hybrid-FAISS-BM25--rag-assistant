[site]: crossvalidated
[post_id]: 323431
[parent_id]: 323014
[tags]: 
This question is better asked on StackOverflow, but I'll give you a hint. First up, the tensor that the engine is trying to allocate is enourmous: $$1143\cdot44592\cdot3=152905968 \approx 150M$$ With float32 variables, it takes ~600Mb, but even with float16 is't ~300Mb, which is a lot . And this is just one layer of the network (not including the training data and intermediate tensors and internal tensorflow infrastructure). Also note that the RMSProp optimizer actually doubles all variables to perform gradient updates, so the memory is doubled as well. I've skimmed through your code and noticed that you use such a big layer for softmax output for word prediction. Not only is it inefficient in terms of memory, this approach is also hard to train. In NLP, it's common to use sampling loss function in order to classify words in a large vocabulary, most commonly negative sampling and NCE. These losses are implemented in tensorflow, but require a bit of manual work in keras (see this discussion on GitHub), but they are much more memory and computationally efficient. So I think the biggest improvement for you would be to implement NCE loss function. You can also try to train with plain sgd optimizer to save memory.
