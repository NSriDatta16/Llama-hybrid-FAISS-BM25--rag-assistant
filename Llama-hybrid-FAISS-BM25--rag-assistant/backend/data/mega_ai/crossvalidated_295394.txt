[site]: crossvalidated
[post_id]: 295394
[parent_id]: 295383
[tags]: 
Regularization (traditionally in the context of shrinkage) adds prior knowledge to a model; a prior, literally, is specified for the parameters. Augmentation is also a form of adding prior knowledge to a model; e.g. images are rotated, which you know does not change the class label. Increasing training data (as with augmentation) decreases a model's variance. Regularization also decreases a model's variance. They do so in different ways, but ultimately both decrease regularization error. Section 5.2.2 of Goodfellow et al's Deep Learning proposes a much broader definition: Regularization is any modiÔ¨Åcation we make to a learning algorithm that is intended to reduce its generalization error but not its training error. There is a tendency to asssociate regularization with shrinkage because of the term "l-p norm regularization"...perhaps "augmentation regularization" is equally valid, although it doesn't roll off the tongue.
