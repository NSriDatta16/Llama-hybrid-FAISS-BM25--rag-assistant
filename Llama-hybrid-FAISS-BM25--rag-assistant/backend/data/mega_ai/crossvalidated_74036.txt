[site]: crossvalidated
[post_id]: 74036
[parent_id]: 74000
[tags]: 
Your notes about frequentists relying on "repeated sampling" properites and Bayesians relying on "fairness" is in line with Neyman/Pearson and deFinetti's justifications of each paradigm, respectively. Bayesian and frequentist approaches are appropriate in different contexts. A controversial aspect of frequentist approaches is the relevance of the concept of "confidence" in the case where it is not clear what is the "embedding series" of experiements (there can be many, look up "relevant subsets problem" for more on this). Bayesians get critisized for applying priors when the underlying property is not random...hence there is a "calibration" problem with a, say, 95% posterior interval...95% of what, and why do we care? I'd take a look at another school/paradigm as well...the Likelihood school , as described by the accessible and useful book " In All Likelihood " by Yudi Pawitan. This approach shows how the objective and subjective aspects of probability are related via the distinction between likelihood and probability. Also, there is an interesting "meeting of the minds" when it comes to random -effects modeling . Take a look at that to see how the two, in practice, can converge in concepts.
