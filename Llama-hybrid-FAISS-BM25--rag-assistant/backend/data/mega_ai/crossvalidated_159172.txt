[site]: crossvalidated
[post_id]: 159172
[parent_id]: 159093
[tags]: 
Not much --- arguably nothing --- has so far been learnt about brain functioning from artificial neural networks. [Clarification: I wrote this answer thinking about neural networks used in machine learning; @MattKrause (+1) is right that neural network models of some biological neural phenomena might have been helpful in many cases.] However, this is perhaps partially due to the fact the research into artificial neural networks in machine learning was more or less in stagnation until around 2006, when Geoffrey Hinton almost single-handedly rekindled the whole field which by now attracts billions of dollars. In a 2012 lecture in Google called Brains, Sex, and Machine Learning (from 45:30), Hinton suggested that artificial neural networks can provide a hint into why [most] neurons communicate with spikes and not with analogue signals. Namely, he suggests to see spikes as a regularization strategy similar to dropout. Dropout is a recently developed way of preventing overfitting, when only a subset of weights is updated on any given gradient descent step (see Srivastava et al. 2014 ). Apparently it can work very well, and Hinton thinks that perhaps spikes (i.e. most neurons being silent at any given moment) serve the similar purpose. I work in a neuroscience research institute and I don't know anybody here who is convinced by the Hinton's argument. The jury is still out (and is probably be going to be out for quite some time), but at least this is an example of something that artificial neural networks could potentially teach us about brain functioning.
