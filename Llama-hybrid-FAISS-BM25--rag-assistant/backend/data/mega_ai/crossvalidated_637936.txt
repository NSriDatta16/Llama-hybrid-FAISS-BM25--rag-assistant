[site]: crossvalidated
[post_id]: 637936
[parent_id]: 637924
[tags]: 
Translation and shift are the same thing here. As far as invariance vs. equivariance, there is often a combination of both at play in CNNs. In particular, if there are max-pooling layers after every convolutional layer, then the activation pattern in the next layer will be invariant to shifting the previous layer's activation pattern around within the scope of a pooling region. E.g. if you have 2x2 pooling, and currently the max activity is in the bottom-left position of a 2x2 pooling area, then you can shift that activity upwards and/or rightwards by 1 position without changing the pooling result, and thus without changing the input to the next layer. That's invariance . More broadly, there may be a looser (not perfect) invariance simply in the sense that the activity in deeper layers does not change much if you shift the input - for instance, the class probabilities in the final layer may change very little, which is desirable. There is also equivariance in CNNs, but typically only within the convolutional layers. A typical CNN does not reflect a shift in its input with a symmetrical change along some output dimension (e.g. it doesn't have outputs that reflect the coordinates of the classified objects), though of course you can build CNNs that do try to do this.
