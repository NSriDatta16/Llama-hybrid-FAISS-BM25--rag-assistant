[site]: datascience
[post_id]: 60186
[parent_id]: 60183
[tags]: 
There are several issues with your code and your question. To start with, your architecture looks awkward; binary cross-entropy loss with accuracy metric is supposed to be used for classification problems, where the final layer needs to be a single-unit dense one with activation='sigmoid' . It is not at all clear what your network does here, with the chosen loss function and a convolutional final layer. Second, since you don't define any activation function, all your layers use the default linear one. Recall from the Keras documentation the Dense layer: keras.layers.Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) where activation : Activation function to use (see activations). If you don't specify anything, no activation is applied (ie. "linear" activation: a(x) = x). Since you don't specify explicitly any activation, you actually use a linear one for all your layers. And it is well-known that a neural network comprised simply of linear units is equivalent to a simple linear unit (check Andrew Ng's lecture Why Non-linear Activation Functions for a detailed explanation); in fact, it is only with non-linear activation functions that neural networks begin to be able to do interesting things. Third, the input_dim argument is supposed to be used only for the first layer (and not for intermediate ones, as you have done here); in intermediate layers, the input dimension is implicitly calculated as the number of outputs of the previous layer. See the SO thread Keras Sequential model input layer for more details.
