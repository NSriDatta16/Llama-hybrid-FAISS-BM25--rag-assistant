[site]: stackoverflow
[post_id]: 4507899
[parent_id]: 
[tags]: 
Recommendations for database structure with huge dataset

It seems to me this question will be without precise answer since requires too complex analysis and deep dive into details of our system. We have distributed net of sensors. Information gathered in one database and futher processed. Current DB design is to have one huge table partitioned per month. We try keep it at 1 billion (usually 600-800 million records), so fill rate is at 20-50 million records per day. DB server currently is MS SQL 2008 R2 but we started from 2005 and upgrade during project development. The table itself contains SensorId, MessageTypeId, ReceiveDate and Data field. Current solution is to preserve sensor data in Data field (binary, 16 byte fixed length) with partially decoding it's type and store it in messageTypeId. We have different kind of message type sending by sensors (current is approx 200) and it can be futher increased on demand. Main processing is done on application server which fetch records on demand (by type, sensorId and date range), decode it and carry out required processing. Current speed is enough for such amount of data. We have request to increase capacity of our system in 10-20 times and we worry is our current solution is capable of that. We have also 2 ideas to "optimise" structure which I want to discuss. 1 Sensor's data can be splitted into types, I'll use 2 primary one for simplicity: (value) level data (analog data with range of values), state data (fixed amount of values) So we can redesign our table to bunch of small ones by using following rules: for each fixed type value (state type) create it's own table with SensorId and ReceiveDate (so we avoid store type and binary blob), all depended (extended) states will be stored in own table similar Foreign Key, so if we have State with values A and B , and depended (or additional) states for it 1 and 2 we ends with tables StateA_1 , StateA_2 , StateB_1 , StateB_2 . So table name consist of fixed states it represents. for each analog data we create seperate table it will be similar first type but cantains additional field with sensor value; Pros: Store only required amount of data (currently our binary blob Data contains space to longest value) and reduced DB size; To get data of particular type we get access right table instead of filter by type; Cons: AFAIK, it violates recommended practices; Requires framework development to automate table management since it will be DBA's hell to maintain it manually; The amount of tables can be considerably large since requires full coverage of possible values; DB schema changes on introduction new sensor data or even new state value for already defined states thus can require complex change; Complex management leads to error prone; It maybe DB engine hell to insert values in such table orgranisation? DB structure is not fixed (constantly changed); Probably all cons outweight a few pros but if we get significant performance gains and / or (less preferred but valuable too) storage space maybe we follow that way. 2 Maybe just split table per sensor (it will be about 100 000 tables) or better by sensor range and/or move to different databases with dedicated servers but we want avoid hardware span if it possible. 3 Leave as it is. 4 Switch to different kind of DBMS, e.g. column oriented DBMS (HBase and similar). What do you think? Maybe you can suggest resource for futher reading? Update: The nature of system that some data from sensors can arrive even with month delay (usually 1-2 week delay), some always online, some kind of sensor has memory on-board and go online eventually. Each sensor message has associated event raised date and server received date, so we can distinguish recent data from gathered some time ago. The processing include some statistical calculation, param deviation detection, etc. We built aggregated reports for quick view, but when we get data from sensor updates old data (already processed) we have to rebuild some reports from scratch, since they depends on all available data and aggregated values can't be used. So we have usually keep 3 month data for quick access and other archived. We try hard to reduce needed to store data but decided that we need it all to keep results accurate. Update2: Here table with primary data. As I mention in comments we remove all dependencies and constrains from it during "need for speed", so it used for storage only. CREATE TABLE [Messages]( [id] [bigint] IDENTITY(1,1) NOT NULL, [sourceId] [int] NOT NULL, [messageDate] [datetime] NOT NULL, [serverDate] [datetime] NOT NULL, [messageTypeId] [smallint] NOT NULL, [data] [binary](16) NOT NULL ) Sample data from one of servers: id sourceId messageDate serverDate messageTypeId data 1591363304 54 2010-11-20 04:45:36.813 2010-11-20 04:45:39.813 257 0x00000000000000D2ED6F42DDA2F24100 1588602646 195 2010-11-19 10:07:21.247 2010-11-19 10:08:05.993 258 0x02C4ADFB080000CFD6AC00FBFBFBFB4D 1588607651 195 2010-11-19 10:09:43.150 2010-11-19 10:09:43.150 258 0x02E4AD1B280000CCD2A9001B1B1B1B77
