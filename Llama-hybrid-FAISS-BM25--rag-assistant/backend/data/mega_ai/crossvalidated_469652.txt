[site]: crossvalidated
[post_id]: 469652
[parent_id]: 
[tags]: 
How are infinite neural networks implemented in practice?

Consider for example Neural Tangents . They claim they allow to define, train, and evaluate infinite networks as easily as finite ones. If their width is infinite (and thus they have an infinite number of parameters) how exactly are these neural networks or layers represented and connected to other layers in practice? From what I read, these networks are equivalent to Gaussian Processes. To my knowledge, GPs are fully defined by their covariance matrix or function (i.e. a Kernel describing how two inputs covary), but GPs don't have an infinite number of parameters per se. Sure GPs are non-parameteric in that their ability to interpolate data grows with the data, but Kernels still have parameters governing e.g. the "range" of interaction in the covariance matrix, e.g. how smooth the process can be. Simple example exploring the relationship with GPs Let's say we use a GP in 1D as an example. In a GP, the input could just be one variable (e.g. a single real value $x$ ), so if we feed it to an "infinitely-wide neural network", how exactly is that equivalent to a layer of infinite width ? E.g. would an infinitely wide layer simply work as GP kernel $K(x,x')$ that takes (in 1D) a $\mathbf{x}$ vector as its input and it outputs a variable $\mathbf{y}$ of the same size as $\mathbf{x}$ and distributed as a GP? If so, wouldn't that be a width of 1? (one input $\rightarrow$ one output)
