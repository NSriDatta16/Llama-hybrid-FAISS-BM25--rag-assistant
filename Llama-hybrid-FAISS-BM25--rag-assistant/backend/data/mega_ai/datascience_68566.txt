[site]: datascience
[post_id]: 68566
[parent_id]: 68554
[tags]: 
Take the example of time series: $\mathbf{x}1,\mathbf{x}2,\ldots,\mathbf{x}10$ where each $\mathbf{x}i$ is let 5 dimensional. The 'timestep' here will be the window chosen such that value at time instant is dependent on previous $p$ lags. So data passed to LSTM will be of the form $\mathbf{x}1,\mathbf{x}2$ as input with $p=2$ lags. In your case as well, each word is dependent on previous words. So timestep will be the number of previous words you need to pass to LSTM in context of current words. Batchsize argument is the number of examples after which the backpropagation will happen. It is a free parameter you can control it but it should be completely divisible by number of training examples. Feature will be 839, the length of embedding vector.
