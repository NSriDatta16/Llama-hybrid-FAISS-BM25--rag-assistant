[site]: crossvalidated
[post_id]: 278922
[parent_id]: 
[tags]: 
Different sized inputs for batch training in fully CNNs

The idea of transfer learning is to use already trained networks for another purposes to the one it was initially trained for. Using fully convolutional networks, the activation maps can be used to create descriptors that later on can be used to train a classifier for image classification. I am currently using a pretrained VGG network to obtain those descriptors and train an SVM afterwards. In a fully convolutional network there is no need to use a fixed input size compared to networks with a fully connected part. My problem comes when creating batches for evaluating the convolutions on a GPU. In this case, considering how I obtain the input images, each sample of data in a batch has a different size and this becomes a problem. Has anyone dealt with this kind of evaluation of different sized inputs in a batch for a fully convolutional network using Lasagne+Theano? Any information about this issue would be highly appreciated. I will post if I find any workaraound.
