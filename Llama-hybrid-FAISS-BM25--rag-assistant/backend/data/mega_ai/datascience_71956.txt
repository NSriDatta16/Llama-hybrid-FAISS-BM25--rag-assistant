[site]: datascience
[post_id]: 71956
[parent_id]: 
[tags]: 
Which activation function of the output layer and which loss function are advised to be used for bounded regression?

I want my (deep) neural network to produce an output from a certain range, in my case between 0 and 255. I have scaled the labels from [0..255] to [0..1]. For the neural network, I have tried a couple of different architectures (number of layers, type of layers, etc.), and I've also tried with the following: sigmoid activation function for the output layer with binary crossentropy loss ReLU activation function for the output layer with MSE some other combinations that don't necessarily make sense (out of desperation) However, in every case so far, the network learns to saturate the output to the extremes of the values range, i.e. either 0 or 1 (even when I intentionally let the network try to overfit (by training for a long time on a very small dataset)). I can provide further details, but my question is more general: What is a go-to combination of activation function of the output layer and loss function to be used for getting network outputs from a full range of values that don't saturate in the extremes? A similar question was already asked, but I didn't find the one answer there helpful.
