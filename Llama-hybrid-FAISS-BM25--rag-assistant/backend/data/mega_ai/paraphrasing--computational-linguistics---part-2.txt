n end-of-sentence token. The encoder and decoder are trained to take a phrase and reproduce the one-hot distribution of a corresponding paraphrase by minimizing perplexity using simple stochastic gradient descent. New paraphrases are generated by inputting a new phrase to the encoder and passing the output to the decoder. Transformers With the introduction of Transformer models, paraphrase generation approaches improved their ability to generate text by scaling neural network parameters and heavily parallelizing training through feed-forward layers. These models are so fluent in generating text that human experts cannot identify if an example was human-authored or machine-generated. Transformer-based paraphrase generation relies on autoencoding, autoregressive, or sequence-to-sequence methods. Autoencoder models predict word replacement candidates with a one-hot distribution over the vocabulary, while autoregressive and seq2seq models generate new text based on the source predicting one word at a time. More advanced efforts also exist to make paraphrasing controllable according to predefined quality dimensions, such as semantic preservation or lexical diversity. Many Transformer-based paraphrase generation methods rely on unsupervised learning to leverage large amounts of training data and scale their methods. Paraphrase recognition Recursive autoencoders Paraphrase recognition has been attempted by Socher et al through the use of recursive autoencoders. The main concept is to produce a vector representation of a sentence and its components by recursively using an autoencoder. The vector representations of paraphrases should have similar vector representations; they are processed, then fed as input into a neural network for classification. Given a sentence W {\displaystyle W} with m {\displaystyle m} words, the autoencoder is designed to take 2 n {\displaystyle n} -dimensional word embeddings as input and produce an n {\displaystyle n} -dimensional vector as output. The same autoencoder is applied to every pair of words in S {\displaystyle S} to produce ⌊ m / 2 ⌋ {\displaystyle \lfloor m/2\rfloor } vectors. The autoencoder is then applied recursively with the new vectors as inputs until a single vector is produced. Given an odd number of inputs, the first vector is forwarded as-is to the next level of recursion. The autoencoder is trained to reproduce every vector in the full recursion tree, including the initial word embeddings. Given two sentences W 1 {\displaystyle W_{1}} and W 2 {\displaystyle W_{2}} of length 4 and 3 respectively, the autoencoders would produce 7 and 5 vector representations including the initial word embeddings. The euclidean distance is then taken between every combination of vectors in W 1 {\displaystyle W_{1}} and W 2 {\displaystyle W_{2}} to produce a similarity matrix S ∈ R 7 × 5 {\displaystyle S\in \mathbb {R} ^{7\times 5}} . S {\displaystyle S} is then subject to a dynamic min-pooling layer to produce a fixed size n p × n p {\displaystyle n_{p}\times n_{p}} matrix. Since S {\displaystyle S} are not uniform in size among all potential sentences, S {\displaystyle S} is split into n p {\displaystyle n_{p}} roughly even sections. The output is then normalized to have mean 0 and standard deviation 1 and is fed into a fully connected layer with a softmax output. The dynamic pooling to softmax model is trained using pairs of known paraphrases. Skip-thought vectors Skip-thought vectors are an attempt to create a vector representation of the semantic meaning of a sentence, similarly to the skip gram model. Skip-thought vectors are produced through the use of a skip-thought model which consists of three key components, an encoder and two decoders. Given a corpus of documents, the skip-thought model is trained to take a sentence as input and encode it into a skip-thought vector. The skip-thought vector is used as input for both decoders; one attempts to reproduce the previous sentence and the other the follo