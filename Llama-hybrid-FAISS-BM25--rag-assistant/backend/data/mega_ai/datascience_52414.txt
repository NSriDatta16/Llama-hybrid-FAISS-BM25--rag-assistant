[site]: datascience
[post_id]: 52414
[parent_id]: 52405
[tags]: 
CTC can also work with totally random text (and thus without any "word pattern"). I know that because I trained an OCR network with CTC loss on random text which was then able to read non random text (fortunately). The CTC just sums the probabilities of the all the paths which are "compatible" with the target text (random or not). The higher the sum of probas of "compatible" paths, the better the prediction. If not done yet, you should read this very good article explaining the CTC loss. Edit to clarify and answer questions asked in comments : do u mean specifically alphanumeric strings ? Yes I mean random string with any character, alpha or numeric, it does not matter (even Chinese or Arabic alphabet if you want). it needs to have seen a lot of data with patterns to generalize weights for transition from C->A->T, correct ? No, when you train on random text, the network is not using context of previous chars to predict current time step but it is still forced to predict each time step (slice of text) correctly because of the CTC loss. The CTC loss pushes the CNN to compute good features and the RNN to give good probabilities by character for all time steps. In other words, even with random text, the CTC loss still pushes the network to predict each char correctly independently of the other chars of the text. And actually a system trained on random text generalizes better on unseen words than a system trained on real words because the latter is biased toward words it has seen during training. For example a system trained on random text with Latin alphabet could read English, French and German but a system trained only on English words won't work well to read another language with same alphabet. how can we ever generalize A623BBCEF ?? Just because a network trained on random text learnt to read each char independently of other chars. the output of such a RNN is a matrix containing character probabilities for each time-step" ..how do we calculate character probabilities ? The RNN takes the feature map for the current time step and compute a "score vector" from it. Sigmoid activation is used to map the scores to probabilities (between 0 and 1). In the end it is just a fully connected layer with as many units as characters with a sigmoid activation on each unit.
