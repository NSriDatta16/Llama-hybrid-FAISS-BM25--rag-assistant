[site]: crossvalidated
[post_id]: 222162
[parent_id]: 215458
[tags]: 
I think the whole point of this paragraph is, if a gradient descent step ignores the dependence of $E[x]$ on $b$, updating the bias term b will lead to no change in the output , as claimed in the sentence before it, However, if these modifications are interspersed with the optimization steps, then the gradient descent step may attempt to update the parameters in a way that requires the normalization to be updated, which reduces the effect of the gradient step. Therefore they made the gradient descent step aware of the normalization in their method. Regarding you questions Relating to this, I guess what they call $u$ is maybe $x^{(l)}$? As claimed in their first sentence, $u$ is the input of the layer. What $u$ actually is doesn't seem to matter, as they're illustrating only the effect of $b$ in the example. I would have thought $ \Delta b \propto -\frac{\partial l}{\partial b } $ makes more sense rather than taking the derivative of with respect to the normalized activations. We know $\hat{x}=x-E[x]=u+b-E[x]$, as we are ignoring the dependence of $E[x]$ on $b$, we have $$\frac{\partial l}{\partial b}=\frac{\partial l}{\partial \hat{x}}\frac{\partial \hat{x}}{\partial b} = \frac{\partial l}{\partial \hat{x}},$$ so $\Delta b \propto -\frac{\partial l}{\partial \hat{x}}$. $u + (b + \Delta b) - E[u + (b + \Delta b)] = u + b - E[u + b]$ they don't really say what they are trying to compute in the above equation but I would infer that they are trying to compute the updated normalized activation (for the first layer?) after $b$ is updated to $b+\Delta b$? It is computing the $\hat{x}$ after $b$ is updated to $b+\Delta b$, to show that if a gradient descent step ignores the dependence of $E[x]$ on $b$, updating the bias term b will lead to no change in the output. It might be helpful to take a look at some open source implementations of batch normalization, for example in Lasagne and Keras . There's another question that might seem related, Why take the gradient of the moments (mean and variance) when using Batch Normalization in a Neural Network?
