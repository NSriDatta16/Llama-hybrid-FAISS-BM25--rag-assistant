[site]: crossvalidated
[post_id]: 474513
[parent_id]: 474505
[tags]: 
Your confusion seems to be about the notation being used here. You seem to be assuming that $x^{(i)}$ is a matrix. Whereas, in fact, $x^{(i)}$ is a column vector: $$ x^{(i)}= \begin{bmatrix} x_1^{(i)} \\ x_2^{(i)} \\ \vdots \\ x_p^{(i)} \end{bmatrix} $$ Thus, every $x^{(i)}$ contains a single observation of all the $p$ variables that you're computing the covariance of, where each row is a different variable. The index $i$ denotes observations , not variables. For instance, $x^{(5)}$ contains the values of all of our variables in the 5-th observation. And $x_3^{(5)}$ would be the value of the third variable in the fifth observation. If you wanted to do it all with matrix notation, and remove the summation operator from the formula, you could define a data matrix $X$ : $$ X= \begin{bmatrix} x^{(1)} & x^{(2)} & \cdots & x^{(n)} \end{bmatrix} $$ $$ = \begin{bmatrix} x_1^{(1)} & x_1^{(2)} & \cdots & x_1^{(n)} \\ x_2^{(1)} & x_2^{(2)} & \cdots & x_2^{(n)} \\ \vdots &\vdots & \ddots\ & \vdots \\ x_p^{(1)} & x_p^{(2)} & \cdots & x_p^{(n)} \end{bmatrix} $$ And then you can use the formula: $$ \Sigma = \frac{1}{m}{XX}^T $$ where $m$ equals either $n$ or $n-1$ . This formula is equivalent to the one used in Andrew Ng's notes. We got rid of the summation over observations by putting those observations into the columns of the matrix $X$ , and then taking a matrix product that "sums away" the observation dimension "under the hood". Importantly, though, both formulas define exactly the same sequence of operations. In Ng's version, you can think of each $x^{(i)}{x^{(i)}}^T$ -term as the "instantaneous covariance" of your variables, for the $i$ -th observation (i.e. how much they happened to co-vary in that particular instance). We then average over all these instantaneous covariances to get an estimate of the overall covariance (how much the variables co-vary on average).
