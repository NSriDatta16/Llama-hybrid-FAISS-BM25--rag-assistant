[site]: crossvalidated
[post_id]: 262060
[parent_id]: 262044
[tags]: 
A bottleneck layer is a layer that contains few nodes compared to the previous layers. It can be used to obtain a representation of the input with reduced dimensionality. An example of this is the use of autoencoders with bottleneck layers for nonlinear dimensionality reduction. My understanding of the quote is that previous approaches use a deep network to classify faces. They then take the first several layers of this network, from the input up to some intermediate layer (say, the $k$th layer, containing $n_k$ nodes). This subnetwork implements a mapping from the input space to an $n_k$-dimensional vector space. The $k$th layer is a bottleneck layer, so the vector of activations of nodes in the $k$th layer gives a lower dimensional representation of the input. The original network can't be used to classify new identities, on which it wasn't trained. But, the $k$th layer may provide a good representation of faces in general. So, to learn new identities, new classifier layers can be stacked on top of the $k$th layer and trained. Or, the new training data can be fed through the subnetwork to obtain representations from the $k$th layer, and these representations can be fed to some other classifier.
