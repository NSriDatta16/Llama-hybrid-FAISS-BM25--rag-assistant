[site]: crossvalidated
[post_id]: 289190
[parent_id]: 
[tags]: 
Theoretical motivation for using log-likelihood vs likelihood

I'm trying to understand at a deeper level the ubiquity of log-likelihood (and perhaps more generally log-probability) in statistics and probability theory. Log-probabilities show up all over the place: we usually work with the log-likelihood for analysis (e.g. for maximization), the Fisher information is defined in terms of the second derivative of the log-likelihood, entropy is an expected log-probability, Kullback-Liebler divergence involves log-probabilities, the expected diviance is an expected log-likelihood, etc. Now I appreciate the many practical and convenient reasons. Many common and useful pdfs are from exponential families, which leads to elegantly simplified terms when log-transformed. Sums are easier to work with than products (esp for differentiating). Log-probs have a great floating point advantage over straight probs. Log-transforming a pdf often converts a non-concave function into a concave function. But what is the theoretical reason/justification/motivation for log-probs? As an example of my perplexity, consider the Fisher information (FI). The usual explanation for intuiting the FI is that the second derivative of the log-likelihood tells us how "peaked" the log-likehood is: a highly peaked log-likelihood means the MLE is well-specified and we are relatively sure of its value, while a nearly flat log-likehood (low curvature) means many different parameter values are nearly as good (in terms of the log-likelihood) as the MLE, so our MLE is more uncertain. This is all well-and-good, but isn't it more natural to just find the curvature of the likelihood function itself (NOT log-transformed)? At first glance the emphasis on the log-transform seems arbitrary and wrong. Surely we are more interested in the curvature of the actual likelihood function. What was Fisher's motivation for working with the score function and the Hessian of the log-likelihood instead? Is the answer simply that, in the end, we have nice results from the log-likelihood asymptotically? E.g., Cramer-Rao and normality of the MLE/posterior. Or is there a deeper reason?
