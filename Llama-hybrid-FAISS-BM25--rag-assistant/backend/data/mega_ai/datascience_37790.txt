[site]: datascience
[post_id]: 37790
[parent_id]: 37787
[tags]: 
The neurons themselves only hold their activation values (or input values when we consider the input layer of neurons). The weights you mention actually indicate the strength of the connections between neurons in subsequent layers (and possibly the bias per layer). Using your terminology the weights would belong to the neurons in both the current and next layer. For every neuron the strength of the connection to the other neurons is saved. In other words, all weights (and possibly biases) are saved. For more information about the basics of neural networks, and more specifically multi-layer perceptrons, I can remmond this website.
