[site]: crossvalidated
[post_id]: 503303
[parent_id]: 502699
[tags]: 
I decided to do a little digging after posting the comment above (I preferred not to spend time re-crunching through the algebra again) and thought I'd post what I found here. So this is far from a comprehensive answer. All the more familiar sources from machine learning I revisited on "marginals and conditionals of multivariate Normals", such as, Section 4.3 Murphy's MLPP (2012) p113 -114, Section 2.3.1 Bishop's PRML (2006) p85-90, Jordan's PGM (draft) Ch13 , and some supplementary notes from Andrew Ng's course here and here do not seem to provide the kinds of intuition you seem to be looking for. They mostly see the derivation as a purely algebraic exercise. My preliminary thoughts on this would be that the best arena for developing intuition might be looking at the bivariate Normal case, particularly as you can see what is going on graphically. However, how you might systematically extend the development of intuition to the multivariate case with partitioned covariance matrices is currently beyond me. The best I could find which provides the kind of intuition you seem to be looking for is the bottom of this link here . However, I haven't had time to assess it myself, so assimilate at your own risk. Aside. From my limited self-study experience, I have also noticed the distinction you raised concerning the differing epistemological priorities taken by statistics vs computer science/data science/machine learning, somewhat akin to a "computational/algorithmic turn". That distinction is discussed by Leo Breiman in "Two Cultures of Statistical Modelling" here , and more recently and comprehensively, in this excellent book by Efron and Hastie called "Computer Age Statistical Inference" here .
