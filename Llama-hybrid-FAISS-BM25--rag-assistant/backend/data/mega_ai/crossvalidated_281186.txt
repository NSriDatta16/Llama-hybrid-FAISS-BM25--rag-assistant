[site]: crossvalidated
[post_id]: 281186
[parent_id]: 281183
[tags]: 
For your record, this is called normalizing/standarizing (not coding). Using binary is not the worst way to do this in my opinion. It adds a lot more non-linearity to your model, that will take more backpropagation (or more neurons) to be figured out. It's quite illogical for a neural network that 1 = 01 and 2 = 10 , but 3 = 11 . The step from 2>3 is linear, but the step from 1>2 is very complicated as it requires the outputs to be 'switched'. Even just dividing the outputs ( 0=0 , 1=0.33 , 2=0.67 , 3=1 ) is more linear. Adding binary encoding only makes the task more complicatd. Also, outputs will never be given perfectly rounded by a neural network. What if you have the output [0.34, 0.23] , will you decode this as 0 = 00 or as 2 = 10 . Both are feasable. Using one-hot encoding is the way to go. Not only is this easier for a network to learn, but it also tells you the 2nd correct answer: E.g. your output is [0.4, 0.93, 0.75, 0.1] , this tells you that the handwritten digit is most likely a 1 , but second most likely a 2 . Binary encoding does not tell you any of this information.
