[site]: crossvalidated
[post_id]: 609894
[parent_id]: 
[tags]: 
difference between the linear predictor with uncertainty and predictive distribution for a new observation

I was reading an extract from the book "regression and other stories" and at chapter 9 the author distinguish between 3 cases "After fitting a regression, $y = a + bx + error$ , we can use it to predict a new data point, or a set of new data points, with predictors $x_{new}$ . We can make three sorts of predictions, corresponding to increasing levels of uncertainty: The point prediction, $\hat{a}+\hat{b}x_{new}$ : Based on the fitted model, this is the best point estimate of the average value of y for new data points with this new value of x. We use $\hat a$ and $\hat b$ here because the point prediction ignores uncertainty. The linear predictor with uncertainty, $a + bx_{new}$ , propagating the inferential uncertainty in $(a, b)$ : This represents the distribution of uncertainty about the expected or average value of y for new data points with predictors $x_{new}$ The predictive distribution for a new observation, $a + bx_{new} + error$ : This represents uncertainty about a new observation y with predictors $x_{new}$ . and then it makes the example " For example, consider a study in which blood pressure, $y$ , is predicted from the dose, $x$ , of a drug. For any given $x_{new}$ , the point prediction is the best estimate of the average blood pressure in the population, conditional on dose $x_{new}$ ; the linear predictor is the modeled average blood pressure of people with dose $x_{new}$ in the population, with uncertainty corresponding to inferential uncertainty in the coefficients $a$ and $b$ ; and the predictive distribution represents the blood pressure of a single person drawn at random drawn from this population, under the model conditional on the specified value of $x_{new}$ . As sample size approaches infinity, the coefficients a and b are estimated more and more precisely, and the uncertainty in the linear predictor approaches zero, but the uncertainty in the predictive distribution for a new observation does not approach zero; it approaches the residual standard deviation $Ïƒ$ " but honestly i am not sure i understood the difference between 2) and 3) suppose i have a model $y= f(x,a,b,c)$ that depends on a predictor variable $x$ and 3 other coefficients $a,b,c$ and i sample the posterior distribution with EMCEE or another software and find the best fit coefficients $\hat a, \hat b, \hat c$ . if : i substitute the value of $a,b,c$ with the best fit coefficients at a point $x_{new}$ i get the point prediction case 2 should correspond to finding the uncertainty on the model expected value of $y_{pred}$ at $x_{new}$ and should correspond to this procedure (i think): fix $x= x_{new}$ and take the samples of $a,b,c$ and collect all the $f(x_{new},a,b,c)$ compute the mean and the variance to get the distribution of uncertainty about the model average value of y but what's case 3 and what is the procedure ? and what does it mean " under the model conditional on the specified value of $x_{new}$ " .
