[site]: crossvalidated
[post_id]: 413584
[parent_id]: 413502
[tags]: 
This is called transfer learning. There are many methods for doing this, including adding new layers at the end, deleting or retraining some of your DNN’s final layers, freezing or lowering learning rated on later layers, etc. I’d guess that a high percentage of Bert use involves at least some new tweaking before deployment.
