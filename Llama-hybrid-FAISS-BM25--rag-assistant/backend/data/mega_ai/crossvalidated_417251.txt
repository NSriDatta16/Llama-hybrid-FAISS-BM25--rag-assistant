[site]: crossvalidated
[post_id]: 417251
[parent_id]: 415666
[tags]: 
A VAE is a latent variable model. The encoder estimates for each input, the corresponding posterior distribution $P(z|x)$ on the latent space $z$ . The objective is typically to obtain a density model $P(x)$ of the data. In Bayes by Backprop, the setting is that you want estimate the posterior distribution over the weights $P(\theta|D)$ . This differs from a VAE because Every single data point $x$ corresponds to a different posterior $P(z|x)$ in a VAE -- and an encoder is used to estimate this posterior. On the other hand in BBB, there is only a single posterior distribution on the weights of the network, which isn't a direct function of any specific datapoint (of course it implicitly depends on the training data as a whole). The VAE in its most popular form opts for a gaussian prior and posterior, allowing easy computation of the KL term or "complexity cost". BBB opts to estimate this cost term by sampling, with the advantage of allowing more complex prior distributions (such as spike and slab). A mixture density network isn't intrinsically bayesian in any way that I'm aware of.
