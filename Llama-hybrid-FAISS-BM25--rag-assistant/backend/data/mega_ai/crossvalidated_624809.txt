[site]: crossvalidated
[post_id]: 624809
[parent_id]: 
[tags]: 
How can I compute prediction explainability (e.g. SHAP) for a classifier trained on dense embeddings (SBERT)?

I have a multiclass classification problem (intent classification). I trained an XGBoost model on a dataset that was feature extracted using SBERT embeddings. I'm trying to compute an explainability measure on a given prediction. This is much easier if I had used something like a TF-IDF vectorizer to extract features, but since I'm using pre-trained dense high-dimensional representation (SBERT from the library sentence_transormers), I'm not sure how can I have some meaningful explainability when, unlike TF-IDF, each feature column is not associated to one word/token. I tried some gradient-based feature attribution methods but that would only give sensitivities of the individual words of the test sentence but not the effect of the remaining ones in the total vocabulary. How can I go around this?
