[site]: crossvalidated
[post_id]: 364638
[parent_id]: 282425
[tags]: 
The vectors being multiplied are not embedding vectors of the words. They are The inner products are beteen of embedding vectors of words and the weight matrix/vectors of the output layer. So the objective is to minimise either the cross-entropy loss. Whether the innet products are negative or zero does not indicate anything about word similarity. This is my opinion.
