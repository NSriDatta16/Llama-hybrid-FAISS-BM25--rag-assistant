[site]: crossvalidated
[post_id]: 428749
[parent_id]: 418507
[tags]: 
There is plenty of modern work going on in density estimation. For instance, the ICLR 2020 conference submission deadline was in the last 24 hours; all the submissions are here , and I'm sure there are at least one or two that belong on this list but I just haven't seen them yet. KDE is still a workhorse in low dimensions, but its application to settings like the one in the original GAN paper was seriously flawed and it is (thankfully) no longer really done in those settings. Yet even on KDE there's still work going on, e.g. this paper from a few years ago about consistency of KDE with fixed bandwidths, or algorithmically speaking this paper that could be viewed sort of as using a "deep kernel" instead of something like a Gaussian. Some lines of recent work on density estimation include: Autoregressive models: here we factor $p(x_1, x_2, x_3) = p(x_1) p(x_2 \mid x_1) p(x_3 \mid x_1, x_2)$ , and estimate each density with some parametric model. Influential papers include RNADE , PixelRNN , PixelCNN , WaveNet , and MADE . This models often produce both good densities and good samples, but can be slow, since they need to go through e.g. a whole image pixel-by-pixel. Normalizing flows: these models start with a simple distribution, say a Gaussian, and stack a series of invertible transformations on top of that which are constrained to have simple Jacobians, so that you can compute densities with the change of variables formula. Influential papers include the original , RealNVP , GLOW , and FFJORD . ( MAF is a cool combination of MADE with flows.) These models also produce both decent likelihoods and samples, but because each transformation needs to be invertible and have a simple Jacobian-determinant, they typically need to be fairly deep and the latent factors need to be the full size of the output, so they tend to be quite memory-intensive. There's also a fair amount of work on fitting unnormalized density functions with methods other than maximum likelihood, with techniques like score matching and noise contrastive estimation ; some recent examples of score matching include a recent paper of mine and a more recent technique you should maybe use instead . There's also a huge recent literature on generative models that eschew explicit density functions entirely, like GANs and their many variants (including a few of mine). Variational autoencoders are a halfway point there, in that they use a bound on likelihoods but don't really estimate densities.
