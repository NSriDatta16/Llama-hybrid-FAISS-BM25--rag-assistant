[site]: crossvalidated
[post_id]: 415501
[parent_id]: 
[tags]: 
Polynomial Regression and Feature Transformation

According to Polynomial Regression concept, high order terms in a model such as $x_1^2, \,\, x_2^3, \,\, x_1^2x_2$ are replaced with new features. In this way, the model equation is converted to linear form. For example, I have model equation like this: $w_0 + w_1x_1^2 + w_2x_2^2$ It is a circular classification line which separates first class from second class in a binary classification problem. However, I want to solve this problem by using linear equation. To accomplish this, I decided to use polynomial regression, and I replace high order terms with new features like this: $w_0 + w_1x_3 + w_2x_4$ This operation is also mentioned in Andrew Ng machine learning course. In other words, it is correct and non-problematic approach. What I am wondering is what is the difference between polynomial regression and feature transformation ? Feature transformation performs same thing. All non-linear terms are converted to linear terms by using transformation functions.
