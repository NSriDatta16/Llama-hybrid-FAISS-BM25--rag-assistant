[site]: crossvalidated
[post_id]: 14946
[parent_id]: 
[tags]: 
What is an analogue of PCA in the regression context?

I'm writing code to approximate a function $y=f(\vec{x})$ where $y\in\mathbb{R}$ and $\vec{x}\in\mathbb{R}^N$ for medium-sized $N$ ($N$ between 20 and 50, approx.). I have a ton of examples, however, so my number of examples greatly exceeds $N$. But, the examples probably are somewhat redundant, and some of the $N$ variables probably aren't correlated much with $y$. While I can compute $\vec{x}$ in near zero time, I need to evaluate $f(\vec{x})$ often/quickly. So, I'd like to project $\vec{x}$ onto a lower-dimensional subspace before learning/evaluating $f(\vec{x})$. I know the standard technique for finding a lower-dimensional projection given a lot of examples is PCA. But in the regression context, I care more about the correlation of the projection of $\vec{x}$ onto each of the basis vectors with the value of $y$ then simply maximizing the variance of the projections. So my basic question is: how can I compute such a(n) (orthogonal) basis? I've looked into "partial least squares" and "correlated components analysis," but if I understand properly, they will only compute a single basis vector since $y\in\mathbb{R}^1$. I want fewer than $N$ vectors in my (approximate) basis but more than one. If it matters, I'll be using a kernelized regression technique, like $\nu$-SVR, LS-SVR, or kernel ridge regression.
