[site]: crossvalidated
[post_id]: 230252
[parent_id]: 
[tags]: 
Probability measures at play in Bayesian inference

This might be a purely notational, but I'm confused about the probability measures at play when using Bayesian inference. It's sufficient to focus on the numerator here. Let's assume that I have a prior over hypotheses $P(H)$ and that these hypotheses are themselves distributions about some data. When some data $D$ is witnessed, I want to update my prior in the usual way: $P(H|D) \propto P(D|H) P(H)$. This might be silly, but what I'm confused about is the distribution of the first term, $P(D|H)$. If I'm not completely off track, $P(D|H)$ is $H(E)$, i.e., the probability of the witnessed data under the hypothesis. However, $P(\cdot)$ is a distribution over $H$ and not over $D$, which is why $P(E|H)$ is not making much sense to me. Where is my thinking wrong? That is, why is $P(E|H)$ defined if $P(\cdot)$ is a distribution over $H$? I'd appreciate any clarifications about the conceptual underpinnings that allow us to go from $P(E|H)$ to $P(H)$ using the same $P(\cdot)$. -- As requested, an example that fixes the notation: Let $h \in H$ be a distribution over coin toss outcomes, and $d_{heads} \in D$ is the event of a coin landing heads. For simplicity, let's assume we only have two hypothesized distributions, $h_1$ and $h_2$, where $h_1(d_{heads}) = .5$ (fair coin), and $h_2(d_{heads}) = .25 (unfair coin). I want to update my prior over $H$, $P(h_i)$, after witnessing a coin toss that resulted in $d_{heads}$, and apply Bayes' rule. For $h_1$ this is, $P(h_1|d_{heads}) \propto P(d_{heads}|h_1) P(h_1)$ So, $P(h_1)$ is not an issue, as it is given by my prior. $P(d_{heads}|h_1)$ is what confuses me. This is the probability of the the coin landing heads given that $h_1$ is true -- but the probability measure $P(\cdot)$ is defined over distributions of coin tosses, not over coin tosses. That's what I'm having trouble with. Either $P(h_i)$ makes sense as the probability of hypothesis $i$, or it makes sense as $P(d_j|h_i)$ where it gives the probability of data $j$ given $i$'s truth. But I don't see how it can be both nor how to (conceptually) justify that $P(d_j|h_i)$ is often $h_i(d_j)$.
