[site]: datascience
[post_id]: 101978
[parent_id]: 9302
[tags]: 
Question 2 I've learned that cross-entropy is defined as... These two means either two things. The person who said you that has no idea about he/she is talking about You incorrectly learned it. In Mathematics Kullback-Leiber divergence(KL), Cross-Entropy(CE), Entropy(H) always mean only one thing, but the term Entropy unfortunately can vary from the scientific community. In any case, the good book on the subject "Information Theory is the book "Elements of Information Theory" by Thomas M. Cover, Joy A. Thomas." from 1991. But if you don't want to understand mathematical properties (they maybe are not used by you) then you can check the definition from Wikipedia: The function is defined in two discrete (or continuous distributions) In the case of discrete distributions the summation happens not for all possible $x=x(w_i)$ values of random variables but only for which the two p.m.f. has a positive value of probabilities. (In math languages the summation happens over the intersection of the support of two functions) How to make this computation numerical stable for such a situation when $\exists i:p_i\sim 0, q_i \sim0$ it's another question. And one way consider how $p,q$ are generated a-priori. For example in case of using some form of symmetric logistic transformation (name from STATs) or softmax (one of the name from Machine Learning) then it's nice to take into account log-sum-exp trick. References: https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss https://gregorygundersen.com/blog/2020/02/09/log-sum-exp/
