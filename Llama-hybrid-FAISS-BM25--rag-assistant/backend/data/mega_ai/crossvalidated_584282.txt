[site]: crossvalidated
[post_id]: 584282
[parent_id]: 
[tags]: 
What theoretical guarantees are lost in modeling reward as a function of next state?

There are a couple of threads ( 1 , 2 ) which address the dependency of reward on the next state in addition to the current state and action. Clearly, modeling the transition probability as a joint distribution over next states and rewards $p(s', r \mid s, a)$ as done in Sutton and Barto's Reinforcement Learning: An Introduction is the most general formulation. However, it seems most modern papers (or at least, theoretical papers) formulate reward as a deterministic function $R(s,a)$ of the current state-action pair. My question is this: what theoretical guarantees are lost when we generalize reward to be a function of next state in addition to the current state-action pair? Or perhaps more generally, why do papers frequently choose to model reward as a deterministic function of state-action pairs ?
