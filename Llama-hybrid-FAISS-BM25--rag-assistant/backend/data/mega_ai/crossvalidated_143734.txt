[site]: crossvalidated
[post_id]: 143734
[parent_id]: 142153
[tags]: 
A few things: gradient boosting is wrapped by caret . The gbm package implements that model Your error estimates from CV are probably not good since you are doing feature selection outside of resampling. Google ' "feature selection" "selection bias" ' to see scholarship on this subject. those (incorrect) error estimates might not be the same given the amount of experimental noise in the data. tree ensembles are not perfect. I haven't done the experiment with gbm but with random forests there can be a slight increase in the error rate as you add non-informative predictors. See Fig. 19.1 in Applied Predictive Modeling that shows this effect for a variety of different models.
