[site]: crossvalidated
[post_id]: 345351
[parent_id]: 120207
[tags]: 
In short, how do I apply machine learning tools to left-censored regression data to get consistent estimates of the relationships between my dependent and independent variables? If you can write up a likelihood and flip the sign to minus then you have your self a loss function which can be used for many machine learning models. In gradient boosting this is commonly refereed to as model boosting . See e.g., Boosting Algorithms: Regularization, Prediction and Model Fitting . As an example with the Tobit model see Gradient Tree Boosted Tobit Models for Default Prediction paper. The method should be available with the scikit-learn branch mentioned in the paper. The same idea is used for right censored data in e.g., the gbm and mboost packages in R for right censored data. The above idea can be applied with other methods (e.g., neural network). However, it is particularly easy with Gradient boosting since you just need to be able to compute the gradient of the loss function (the negative log likelihood). Then you can apply whatever method you prefer to fit the negative gradient with an $L2$ loss.
