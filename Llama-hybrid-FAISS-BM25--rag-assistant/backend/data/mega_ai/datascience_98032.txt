[site]: datascience
[post_id]: 98032
[parent_id]: 98003
[tags]: 
The thing about neural networks is that they are uninterpretable. I bet there might be some techniques that tackle this issue. You can try google scholar for that. But yeah, neural networks are basically black boxes. You can extract the convolutional filters and see the results of each layer on the image, but you will just come up with ,,When this shape is present and this shape is present next to it the probability of input being a cat raises by 30%".
