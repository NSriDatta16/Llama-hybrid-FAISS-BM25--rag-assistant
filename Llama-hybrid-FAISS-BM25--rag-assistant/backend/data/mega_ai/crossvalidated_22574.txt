[site]: crossvalidated
[post_id]: 22574
[parent_id]: 22393
[tags]: 
In chapter 5 of Data Mining with R, the author shows some ways to choose the most useful predictors. (In the context of bioinformatics, where each sample row has 12,000+ columns!) He first uses some filters based on statistical distribution. For instance, if you have half a dozen predictors all with a similar mean and s.d. then you can get away with just keeping one of them. He then shows how to use a random forest to find which ones are most useful predictors. Here is a self-contained abstract example. You can see I've got 5 good predictors, 5 bad ones. The code shows how to just keep the best 3. set.seed(99) d=data.frame( y=c(1:20), x1=log(c(1:20)), x2=sample(1:100, 20), x3=c(1:20)*c(11:30), x4=runif(20), x5=-c(1:20), x6=rnorm(20), x7=c(1:20), x8=rnorm(20,mean=100, sd=20), x9=jitter(c(1:20)), x10=jitter(rep(3.14, 20)) ) library(randomForest) rf=randomForest(y ~ ., d, importance=TRUE) print(importance(rf)) # %IncMSE IncNodePurity # x1 12.19922383 130.094641 # x2 -1.90923082 6.455262 # ... i=importance(rf) best3=rownames(i)[order(i[, "%IncMSE"], decreasing=TRUE)[1:3]] print(best3) #[1] "x1" "x5" "x9" reduced_dataset=d[, c(best3, 'y')] The author's last approach is using a hierarchical clustering algorithm to cluster similar predictors into, say, 30 groups. If you want 30 diverse predictors you then choose one from each of those 30 groups, randomly. Here is some code, using the same sample data as above, to choose 3 of the 10 columns: library(Hmisc) d_without_answer=d[,names(d)!='y'] vc=varclus(as.matrix(d_without_answer)) print(cutree(vc$hclust,3)) # x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 # 1 2 1 3 1 1 1 2 1 3 My sample data does not suit this approach at all, because I have 5 good predictors and 5 that are just noise. If all 10 predictors were slightly correlated with y , and had a good chance of being even better when used together (which is quite possible in the financial domain), then this may be a good approach.
