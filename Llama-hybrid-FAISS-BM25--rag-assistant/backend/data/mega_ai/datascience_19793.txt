[site]: datascience
[post_id]: 19793
[parent_id]: 17430
[tags]: 
Here some points on which we can focus - 1.) Averaging the words vectors lose the order of words, making it very similar to the concept of Bag of Words. That is why with less Data Bag of words approach perform better than word2vec. it has been found that training word2vec with potentially large data , outperform Bag of Words models. Google's results are based on word vectors that were learned out of more than a billion-word corpus. 2.) Published literature, distributed word vector techniques have been shown to outperform Bag of Words models. In this paper, an algorithm called Paragraph Vector is used on the IMDB dataset to produce some of the most state-of-the-art results to date. 3.) In your case Document size increase with time. So a good approach would be not to cluster each word rather than clustering documents. Word2vec algorithm has power to learn embedding for each document , which we call Doc2vec . it will cluster similar documents. and using cosine similarity you can find similar documents.
