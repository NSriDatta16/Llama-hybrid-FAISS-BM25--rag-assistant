[site]: datascience
[post_id]: 58783
[parent_id]: 22690
[tags]: 
In sequence to sequence model: The encoder network’s job is to read the input sequence to our Seq2Seq model and generate a fixed-dimensional context vector C for the sequence. To do so, the encoder will use a recurrent neural network cell – usually an LSTM – to read the input tokens one at a time. The final hidden state of the cell will then become C. However, because it’s so difficult to compress an arbitrary-length sequence in to a single fixed-size vector (especially for difficult tasks like translation), the encoder will usually consist of stacked LSTMs : a series of LSTM "layers" where each layer’s outputs are the input sequence to the next layer. The final layer’s LSTM hidden state will be used as Context vector.
