 X 1 , … , X N ) = p t ( X r ( N ) ) ∏ i = 1 N − 1 p t ( X r ( i ) | X r ( i + 1 ) ) . {\displaystyle p_{t+1}(X_{1},\dots ,X_{N})=p_{t}(X_{r(N)})\prod _{i=1}^{N-1}p_{t}(X_{r(i)}|X_{r(i+1)}).} New solutions are sampled from the leftmost to the rightmost variable, the first is generated independently and the others according to conditional probabilities. Since the estimated distribution must be recomputed each generation, MIMIC uses concrete populations in the following way P ( t + 1 ) = β μ ∘ α MIMIC ∘ S ( P ( t ) ) . {\displaystyle P(t+1)=\beta _{\mu }\circ \alpha _{\text{MIMIC}}\circ S(P(t)).} Bivariate marginal distribution algorithm (BMDA) The BMDA factorizes the joint probability distribution in bivariate distributions. First, a randomly chosen variable is added as a node in a graph, the most dependent variable to one of those in the graph is chosen among those not yet in the graph, this procedure is repeated until no remaining variable depends on any variable in the graph (verified according to a threshold value). The resulting model is a forest with multiple trees rooted at nodes Υ t {\displaystyle \Upsilon _{t}} . Considering I t {\displaystyle I_{t}} the non-root variables, BMDA estimates a factorized distribution in which the root variables can be sampled independently, whereas all the others must be conditioned to the parent variable π i {\displaystyle \pi _{i}} . p t + 1 ( X 1 , … , X N ) = ∏ X i ∈ Υ t p t ( X i ) ⋅ ∏ X i ∈ I t p t ( X i | π i ) . {\displaystyle p_{t+1}(X_{1},\dots ,X_{N})=\prod _{X_{i}\in \Upsilon _{t}}p_{t}(X_{i})\cdot \prod _{X_{i}\in I_{t}}p_{t}(X_{i}|\pi _{i}).} Each step of BMDA is defined as follows P ( t + 1 ) = β μ ∘ α BMDA ∘ S ( P ( t ) ) . {\displaystyle P(t+1)=\beta _{\mu }\circ \alpha _{\text{BMDA}}\circ S(P(t)).} Multivariate factorizations The next stage of EDAs development was the use of multivariate factorizations. In this case, the joint probability distribution is usually factorized in a number of components of limited size | π i | ≤ K , ∀ i ∈ 1 , 2 , … , N {\displaystyle |\pi _{i}|\leq K,~\forall i\in 1,2,\dots ,N} . p ( X 1 , … , X N ) = ∏ i = 1 N p ( X i | π i ) {\displaystyle p(X_{1},\dots ,X_{N})=\prod _{i=1}^{N}p(X_{i}|\pi _{i})} The learning of PGMs encoding multivariate distributions is a computationally expensive task, therefore, it is usual for EDAs to estimate multivariate statistics from bivariate statistics. Such relaxation allows PGM to be built in polynomial time in N {\displaystyle N} ; however, it also limits the generality of such EDAs. Extended compact genetic algorithm (eCGA) The ECGA was one of the first EDA to employ multivariate factorizations, in which high-order dependencies among decision variables can be modeled. Its approach factorizes the joint probability distribution in the product of multivariate marginal distributions. Assume T eCGA = { τ 1 , … , τ Ψ } {\displaystyle T_{\text{eCGA}}=\{\tau _{1},\dots ,\tau _{\Psi }\}} is a set of subsets, in which every τ ∈ T eCGA {\displaystyle \tau \in T_{\text{eCGA}}} is a linkage set, containing | τ | ≤ K {\displaystyle |\tau |\leq K} variables. The factorized joint probability distribution is represented as follows p ( X 1 , … , X N ) = ∏ τ ∈ T eCGA p ( τ ) . {\displaystyle p(X_{1},\dots ,X_{N})=\prod _{\tau \in T_{\text{eCGA}}}p(\tau ).} The ECGA popularized the term "linkage-learning" as denoting procedures that identify linkage sets. Its linkage-learning procedure relies on two measures: (1) the Model Complexity (MC) and (2) the Compressed Population Complexity (CPC). The MC quantifies the model representation size in terms of number of bits required to store all the marginal probabilities M C = log 2 ⁡ ( λ + 1 ) ∑ τ ∈ T eCGA ( 2 | τ | − 1 ) , {\displaystyle MC=\log _{2}(\lambda +1)\sum _{\tau \in T_{\text{eCGA}}}(2^{|\tau |-1}),} The CPC, on the other hand, quantifies the data compression in terms of entropy of the marginal distribution over all partitions, where λ {\displaystyle \lambda } is the selected 