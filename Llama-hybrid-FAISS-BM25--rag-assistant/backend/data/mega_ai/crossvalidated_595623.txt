[site]: crossvalidated
[post_id]: 595623
[parent_id]: 
[tags]: 
Why do we need to add confidence intervals to model predictions in cases we don't know the true data distribution?

There are cases (arguably the vast majority) where the data distribution is unknown. Confidence intervals make sense for the class of "nicely" defined theoretical pdfs ie Gaussian. In this case, we know that additional samples help to derive the characteristics (statistical moments) of the pdf. However, there are other distributions in which additional sampling is not helping to converge to estimates but on the contrary diverge, for example, Cauchy distribution sources: https://en.wikipedia.org/wiki/Cauchy_distribution#/media/File:Mean_estimator_consistency.gif https://en.wikipedia.org/wiki/Cauchy_distribution So with this context, my question is: Why do we need to add confidence intervals (or the equivalent reporting mean and variance) to model predictions in cases we don't know the true data distribution? I would like to focus the question in the context of Machine learning inference evaluation scores a common example is to add +/- in the score interval usually by rerunning the model, with some different random seed conditions. (note that this is distinct question from from k-fold method discussed here: https://datascience.stackexchange.com/questions/108792/why-is-the-k-fold-cross-validation-needed but it could be also a relevant method there if the data is violating some of Central limit theorem conditions such as being independently and identically distributed. )
