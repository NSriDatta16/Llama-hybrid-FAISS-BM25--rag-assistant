[site]: crossvalidated
[post_id]: 102647
[parent_id]: 
[tags]: 
How to define Confidence Intervals from a distribution of 1000 t-statistics?

I have a vector of length 300 containing some kind of values (say, scores of a math test). The distribution is not normal. I want to test if the average score of a small group (30 samples) is significantly different from the mean of all the remaining 470 individuals. I performed a Welch's t-test obtaining a t-statistic. In order to validate my results I performed a bootstrapping: I computed 1000 t-tests from a random creation of subgroups (again,30 vs. 470) generating a distribution of 1000 t-statistics. My question is, how can I define the Confidence Intervals for my t-statistics distribution and check if the t-statistics of the original subgroup is outside the CI?
