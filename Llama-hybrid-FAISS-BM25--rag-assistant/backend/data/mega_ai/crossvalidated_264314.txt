[site]: crossvalidated
[post_id]: 264314
[parent_id]: 215716
[tags]: 
Decision Tree is a stand alone model, while a Random Forest is an ensemble of Decision Trees. Decision Tree is a weak learner. It is prone to over fitting (high variance) and is highly dependent on the training sample. Since a RF consists of a huge number of decision trees, it adds regularization and hence becomes a strong learner. For more information about why RF are used over Decision Trees, check out my personal blog Decision Trees, Random Forests and XGBoost But the thumb rule is, if the decision tree itself is giving acceptable results for your use case, a random forest would be over kill. This is analogous to a situation where, if a linear regression model is providing acceptable results, do not build a deep neural network.
