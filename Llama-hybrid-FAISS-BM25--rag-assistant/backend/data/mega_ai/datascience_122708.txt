[site]: datascience
[post_id]: 122708
[parent_id]: 
[tags]: 
weight derivation for XGBOOST in regression trees

I am working on a variation of the XGBOOST presented on the following paper by Chen and Guestrin: https://dl.acm.org/doi/abs/10.1145/2939672.2939785 My goal is to fit a linear regression in each leaf rather than a constant value. In other words, I should estimate a vector of coefficients instead of a single weight for each leaf. What Chen and Guestrin did to calculate the weights is taking the total loss function and perform a second-order Taylor approximation that results in where are the gradients and hessians of the loss function, f(x_{i}) represents the weight for the data point x_{i} for a specific tree structure, y_hat_{i} is the estimate for a specific data point at the previous iteration of the XGBOOST, y is the target value and l(y_{i},y_hat_{i}) is the loss for a specific data point. In my case, y_hat_{i} = X_{i}*B_{j} with B_{j} being a vector of coefficient for leaf 'j'. Now my question is: when I perform the second-order approximation, should I take the derivative of l(y,y_hat) with respect to y_hat like they do on their paper (resulting in scalars g_{i} and h_{i}) or should I take it with respect to the vector of coefficients B (resulting in a vector g_{i} and a matrix h_{i})?
