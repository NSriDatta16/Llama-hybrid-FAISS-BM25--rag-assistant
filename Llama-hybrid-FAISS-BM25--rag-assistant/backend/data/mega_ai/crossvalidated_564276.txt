[site]: crossvalidated
[post_id]: 564276
[parent_id]: 564274
[tags]: 
When we are building a machine learning model, we have to take two factors into account: On one hand, we want the predictions given by the model to be as accurate as possible, this means that we want the model to have as little error as possible. On the other hand, we know that the observations in our data set are likely to have errors: either measurement errors, or they are influenced by some aspect that we do not take into account in the model. So with our model we want to achieve a compromise, we want it to be able to capture the general trend of the data, without being influenced by the specific errors of the set with which we train it. An example always helps, If our model is too general, we will be in the situation of the left image, underfitting , and not really capturing the behavior of the data. But if our model is not general enough, we will be in the situation of the middle image, overfitting , and we will be including in the model specific errors of the observations in the trainset. That is, a change in our data set will produce a very large change in the model. It is clear that the best model is the one in the image on the right, because it captures the general behavior (that curved trend) without being influenced by all the specific errors of the observations. So finally, the variance in a model tells us how variable the model is. The larger the variance, the more abrupt changes, ups and downs we will see in the model's predictions. If the variance is very small, the model is very stable and under-fitted. But if it is too large, the model is too variable and over-fitted. We look for a middle ground between the two.
