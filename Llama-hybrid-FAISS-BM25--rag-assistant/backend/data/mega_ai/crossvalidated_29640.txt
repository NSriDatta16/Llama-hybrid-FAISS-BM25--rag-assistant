[site]: crossvalidated
[post_id]: 29640
[parent_id]: 29580
[tags]: 
My current approach is to find best tuning parameters (lambda and alpha) in a grid search on 90% of the dataset with 10-fold cross-validation averaging MSE score. Then I train the model with the best tuning parameters on the whole 90% of dataset. I am able to evaluate my model using R squared on the holdout 10% of the dataset (which account to only 15 samples). How stable are the tuning parameters? Do you see large differences between goodness-of-fit (e.g. MSE of the optimal parameter's cross validation) and the 10% independent test performance? That would be a symptom of overfitting: The problem with the grid search (and many other parameter optimization strategies) is that you basically assume a rather smooth behaviour of $MSE = f (grid parameters)$. But for small test sets the variance due to the small test set size (= 135 samples total in 10 c.v.-folds) can be larger than the actual differences of $MSE = f (grid parameters)$. In that case already the parameters are rather unstable. Can I repeatedly run my procedure to create a number of models, and then average regression coefficients? Or should I use the number of occurrences of a predictor in the models as its importance score? There are several possibilities to build such aggregated models: linear models can be averaged by averaging the coefficients more generally, you can predict a sample by each of the $m$ different models, and average the $m$ predictions (you could also derive an idea of the uncertainty looking at the distribution of the predictions). Search terms would be "aggregated models", "bootstrap aggregating", "bagging". Side thought: some types of data have expected and interpretable collinearity that can cause variable selectio to "jump" between more or less equal solutions.
