[site]: crossvalidated
[post_id]: 506003
[parent_id]: 506002
[tags]: 
These spikes are common, and I think that they are of no great concern. Here is why I think they occur: At the end of each epoch, most machine learning algorithms shuffle the data. Most machine learning algorithms also perform gradient descent in small batches. Many gradient descent algorithms have momentum terms in them. The training gradient momentum that is built up over a few mini-batches may work very well for a while, until it hits a mini-batch which requires the model to learn something different. Hence the spike in the error value. Over the long run, as you can see, your errors are still decreasing.
