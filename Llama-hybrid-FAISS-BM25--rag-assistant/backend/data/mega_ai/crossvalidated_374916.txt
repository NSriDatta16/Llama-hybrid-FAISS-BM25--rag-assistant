[site]: crossvalidated
[post_id]: 374916
[parent_id]: 361485
[tags]: 
@Neil Slater, I'm not sure if I understand. Q-learning is off-policy, which means it has a target policy $\pi(A | S)$ which we want to learn about, meanwhile there is also a behavior policy $\mu(A | S)$ which we use to choose the next action. Both of these policies are allowed to improve during learning. When sampling the next action from state, we use the behavior policy $\mu(A | S)$ , which is e.g. $\epsilon$ -greedy w.r.t. $Q(S_t, a)$ . But the target policy $\pi$ is greedy w.r.t. $Q(S_{t+1}, a)$ So, I think there is both exploration and exploitation in Q-learning, which makes not directly towards optimal search. SARSA is for on-policy learning, typically always use the $\epsilon$ -greedy policy improvement strategy.
