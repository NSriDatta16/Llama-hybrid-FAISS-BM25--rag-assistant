[site]: crossvalidated
[post_id]: 230512
[parent_id]: 230193
[tags]: 
Training deeper networks becomes harder as you can incur into the Vanishing Gradient Problem , especially if the activation function you are using is the sigmoid. If you look at the derivative of the sigmoid function you will see that it's bell-shaped: This means that there is plenty of possibilities for the gradient of the sigmoid to be very low. The gradients of the parameters of earlier layers are calculated via multiplication with the gradients of further layers. If the gradients are typically less than 1, you can see how, as you add more layers, the gradient of earlier layers gets pushed towards 0. This is explained in detail from Michael Nielsen's Neural Networks and Deep Learning book (chapter 5) . One solution would be to use alternative activation functions, such as ReLu , or Exponential Linear Unit , which do not have low gradients for large inputs. Another "solution" would be to just employ a much larger number of epochs. Moreover, make sure that proper initialization of the weights (Xavier) is employed.
