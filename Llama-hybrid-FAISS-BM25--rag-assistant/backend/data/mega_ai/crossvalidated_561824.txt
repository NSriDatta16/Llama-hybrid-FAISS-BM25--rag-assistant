[site]: crossvalidated
[post_id]: 561824
[parent_id]: 
[tags]: 
Can we call "Regularization" as "Constrained Optimization"?

I have the following question on " Regularization vs. Constrained Optimization" : In the context of statistical modelling, we are often taught about "Regularization" as a method of dealing with the "Bias-Variance Tradeoff". When a L1-Norm or L2-Norm Penalty Term is added to the estimation function (corresponding to the statistical model) being optimized, some of the model parameters will either "shrink" in size towards 0 - thus producing a "sparser" model that is more likely to retain its "low bias" but possible reduce its "high variance": I have often heard of functions containing these L1-Norm and L2-Norm "Penalty Terms" being referred to as "optimization constraints" (i.e. the "feasible region" from which valid choices of model parameters can belong to has now been "altered" due to these "norm penalty constraints"): My Question: When we estimate some statistical model's parameters and the estimation equation contains some "regularization penalty term" - would it be incorrect to refer to this as an example of "constrained optimization"? Is regularized optimization in Machine Learning and Statistical Modelling fundamentally any different (with the exception of usually being more difficult and solved using approximate stochastic iterative methods) from Constrained Optimization in Linear Programming? Can someone please comment on this? Thanks! References: https://en.wikipedia.org/wiki/Regularization_(mathematics) http://ab-initio.mit.edu/wiki/index.php?title=NLopt_Tutorial
