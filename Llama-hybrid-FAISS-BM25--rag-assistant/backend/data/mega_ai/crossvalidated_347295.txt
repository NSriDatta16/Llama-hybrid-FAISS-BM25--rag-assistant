[site]: crossvalidated
[post_id]: 347295
[parent_id]: 
[tags]: 
Why semi-gradient is used instead of the true gradient in Q-learning?

I am asking a duplicated question that nobody has answered yet. In Q-learning with function approximation, the objective is to minimize MSE between the target $r + \gamma \max_{a'} Q(s',a',w)$ and the q-value function $Q(s,a,w)$, as shown below. The update of the parameter $w$ depends on "semi-gradient", i.e., we derive the gradient w.r.t to $w$ in $l$ while treating the target as a real value. My question is why we calculate "semi-gradient' rather than the true gradient. Is it for easiness of gradient calculation, since the target contains a $max$ operator? Is it a kind of simplification to prove some theories?
