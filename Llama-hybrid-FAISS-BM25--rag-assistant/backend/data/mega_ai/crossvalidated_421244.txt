[site]: crossvalidated
[post_id]: 421244
[parent_id]: 249906
[tags]: 
I think that you missed the point of boosting. Process: start with a LEAST SQUARED ERROR fit determine those items with higher error in the set add weights to them to increase how aggressively the model engages re-fit using the updated weights When you are done, if you compute weighted Least Squared Error then your fit is "Better". If you compute unweighted error, then it will be worse. If you have a super-learner, something that qualifies as a universal function approximator (NN, GBM, â€¦), then it could eventually (in an ideal sense) memorize your training data. You could get perfect representation. Boosting helps it make the most of itself. If you have a normal-learner, such as a nominal logistic regression, then boosting isn't going to make it do anything that it isn't already doing. It will not give it super-powers. Suggestion: You might consider a classification task where you care about both true positives and true negatives. Think about it in terms of a confusion matrix ( link ). Would you be willing to give up some of your accuracy in classifying the things you are good at in order to be less bad at handling the things you are bad at? Consider these: https://en.wikipedia.org/wiki/Precision_and_recall#Definition_(classification_context) Boosting A Logistic Regression Model https://projecteuclid.org/euclid.aos/1016218223
