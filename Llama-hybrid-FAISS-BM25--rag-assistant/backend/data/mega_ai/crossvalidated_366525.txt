[site]: crossvalidated
[post_id]: 366525
[parent_id]: 353617
[tags]: 
There are two algorithms used for word embeddings. Skip-grams and CBOW. To answer your question directly - both allow to set 'windows-size', which is effectively what you are looking for. Depending on characteristics of your dataset, one may be better than the other. Both algorithms are really about paris of tokens, so it is really up to you how you train it and what pairs you throw at it. Example implementation for tensorflow: https://github.com/tensorflow/tensorflow/blob/r0.8/tensorflow/examples/tutorials/word2vec/word2vec_basic.py Having said that, I am not sure what the result is going to be in your case and if it will address your requirement. You can try feeding the algorithm with pairs of words differently, like all from IB / all from BO / pairs between IB and BO.
