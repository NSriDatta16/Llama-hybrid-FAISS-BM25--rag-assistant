[site]: datascience
[post_id]: 54314
[parent_id]: 54281
[tags]: 
First, the remark from the wikipedia article deals with small (positive) values of $\lambda$ , not $\lambda=0$ . Indeed, if $\lambda=0$ , then every separating hyperplane achieves the minimum score of 0. If the data is linearly separable, then taking $\lambda$ small enough that the first term dominates ensures that minimizing the loss will require taking a separating hyperplane to zero out the first term, and subject to that minimizing the second term is equivalent to the original hard SVM.
