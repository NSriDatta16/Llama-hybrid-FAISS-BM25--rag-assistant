[site]: crossvalidated
[post_id]: 66434
[parent_id]: 66430
[tags]: 
An important distinction here is that Pearson's product-moment correlation , the linear regression of $Y$ on $X$, and the linear regression of $X$ on $Y$ (assuming $X$ is continuous) are all linear models . On the other hand, logistic regression is a nonlinear model / an instance of the generalized linear model . If you were to regress a continuous $X$ variable onto a binary $Y$ variable, that would be a t-test 1 . (The t-test, in turn, is a special case of regression / the general linear model 2 .) Using logistic regression to model a binary $Y$ is a different animal because there is a nonlinear transformation between the left hand side and the right hand side of the equation, namely the link function (specifically, the logit 3 ). ( I had wanted to address the case that @whuber discusses, when $X$ constitutes only two categories coded as $0$, $1$, earlier, but didn't have time so I had to leave off. @whuber has done a good job with that topic, but I'll go ahead and explain it again, because I'll come at it from a slightly different direction, which may help some to understand it more easily, and I'll add one more detail. ) In this situation, your data consist of four counts: $n_{00}$ (the number of observations where: $X=0,~Y=0$), $n_{01}$ (where: $X=0,~Y=1$), $n_{10}$ (where: $X=1,~Y=0$), $n_{11}$ (where: $X=1,~Y=1$). The thing to remember at this point is that logistic regression is linear in the log odds of the response, and when exponentiated, the intercept is the odds of success when $X=0$, and the slope is the odds ratio 4 associated with a one-unit change in $X$. Hence, $$ \exp(\hat\beta_{0;\text{ Y on X}})=\frac{n_{01}}{n_{00}}\quad\text{ and }\quad\exp(\hat\beta_{0;\text{ X on Y}})=\frac{n_{10}}{n_{00}} $$ Thus, the two intercepts will be equal if and only if $n_{01}=n_{10}$. In addition, $$ \exp(\hat\beta_{1;\text{ Y on X}})=\frac{\frac{n_{11}}{n_{10}}}{\frac{n_{01}}{n_{00}}}\quad\text{ and }\quad\exp(\hat\beta_{1;\text{ X on Y}})=\frac{\frac{n_{11}}{n_{01}}}{\frac{n_{10}}{n_{00}}} $$ But in both cases these equal $\frac{n_{11}}{n_{10}}\cdot\frac{n_{00}}{n_{01}}$, so the slopes must always be equal (as @whuber explained). The subscripts that @whuber and I are using to index the $n$'s are switched around. Also, in @whuber's R example, he seems to be using Y=0 as success , whereas I would call Y=1 success. For example, note that for $\hat\beta_{0;\text{ Y on X}}$, he has $\log(1/3)$, whereas using my convention, $\exp(\hat\beta_{0;\text{ Y on X}})=3/1$. I duplicate @whuber's R example below; both work. y = c(0,0,0,1,1,1,1,1,1,1) x = c(0,1,1,0,0,0,1,1,1,1) t(table(y,x)) # these data are the same as @whuber's y x 0 1 0 1 3 # using my conventions, exp(b0[YonX]) would be 3/1 = 3 1 2 4 # using my conventions, exp(b0[XonY]) would be 2/1 = 2 fit.YonX = glm(y~x, family=binomial(link="logit")) fit.XonY = glm(x~y, family=binomial(link="logit")) coef(fit.YonX) (Intercept) x 1.0986123 -0.4054651 exp(1.0986123) [1] 3 coef(fit.XonY) (Intercept) y 0.6931472 -0.4054651 exp(0.6931472) [1] 2 Footnotes: 1 Strictly speaking, running a t-test 'in the other direction' wouldn't quite be a logistic regression. A stronger analogy would be Fisher's linear discriminant analysis . That's because in logistic regression there is no assumption about the distribution of $X$, but LDA does assume $X$ is normally distributed and the t-test likewise assumes the residuals are. Nonetheless, given that we're starting from logistic regression, for a quick way to think about what you're doing if you were to switch (a continuous) $X$ and $Y$, calling it a t-test is close enough. 2 For help with understanding how the t-test is a special case of regression, see here: How are regression, the t-test, and the ANOVA all versions of the general linear model? 3 For help with understanding link functions and the logit transformation, it may help to read my answer here: Difference between logit and probit models , although it was written in a different context. 4 For more about logistic regression and how it's related to odds and odds ratios, see my answer here: interpretation of simple predictions to odds ratios in logistic regression .
