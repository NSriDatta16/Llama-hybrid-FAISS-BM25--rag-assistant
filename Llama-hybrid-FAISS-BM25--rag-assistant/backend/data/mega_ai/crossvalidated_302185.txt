[site]: crossvalidated
[post_id]: 302185
[parent_id]: 149231
[tags]: 
This is a good question which is deeper than any imprecise wording in Andrew's notes. It is true that in RL you generally do not need to learn a reward function R(S,A,S'); you need to specify it as part of the problem setup. BUT, there are algorithms (in my experience associated with the afterstate or post-decision state value function) which require the expected reward r(S,A) = E[R(S,A,S')|S,A]. Generally the texts I've seen make little comment on this and assume that r(s,a) is known just as R(S,A,S') is known. But in some cases the reward is dependent on the future state so, without a model, you need to learn this expectation. I am currently working on such a problem where the expected reward function as well as the value function need to be learned. Note that most RL algorithms do NOT require the expected reward function, but some do. See for example the discussion on pg 58 in Algorithms for Reinforcement Learning by Szepesvari. In summary you do not need to learn the reward function, but when working with post-decision state variables, you may need to learn the expected reward function. This is the only case that I am aware of where you need to learn an expected reward function, but I'd be interested to hear of other cases.
