[site]: datascience
[post_id]: 58026
[parent_id]: 
[tags]: 
Backpropagation: Relevance of the error signal of a neuron

During my quest to understand back propagation in a more rigorous approach I have come across with the definition of error signal of a neuron which is defined as follows for the $j^{\text{th}}$ neuron in the $l^{\text{th}}$ layer: \begin{eqnarray} \delta^l_j \equiv \frac{\partial C}{\partial z^l_j} \tag{1}\end{eqnarray} Basically, $\delta^l_j$ measures how much the total error changes when the input sum of the neuron is changed and is used for calculating the weights and biases of the neural network as follows: \begin{eqnarray} \frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j \tag{2}\end{eqnarray} \begin{eqnarray} \frac{\partial C}{\partial b^l_j} = \delta^l_j. \tag{3}\end{eqnarray} Besides being useful for the calculation of the weights and the biases in the sense it makes it possible to reuse its value several times, is there any other reason why this definition is always brought up when addressing back propagation?
