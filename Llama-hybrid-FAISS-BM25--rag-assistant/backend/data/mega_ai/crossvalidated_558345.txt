[site]: crossvalidated
[post_id]: 558345
[parent_id]: 
[tags]: 
Convergence issue tuning parameters for logistic regression to maximise recall

I'm currently working on a fraud detection project and I am trying to do a gridsearch with a logistic regression to find good values for parameters "C" and "class_weight". The dataset is very unbalanced (500:1) and I want to maximise "recall" (I think) to minimize the false negatives. I read that weighting the classes differently can help with this. The problem I'm having is that the grid search is just telling me to use the smallest C and the largest class_weight no matter what values I try, which leads to a useless model as it is severely overfitting, which I thought C was supposed to stop as it increases regularization. I guess weighting the classes is a bad idea? It seem's like increasing the class_weight is like the MSE issue where adding more complexity always reduces the MSE. What I want to know is how I should go about finding a good model to minimize recall since this approach doesn't seem to work.
