[site]: datascience
[post_id]: 121122
[parent_id]: 121121
[tags]: 
Yes, it's perfectly fine to fine-tune BERT on sequences comprised of more than one sentence, and the standard way of using BERT for text classification is with the ouput vector at the first position. However, take into account that the maximum length of BERT's input sequences is 512 tokens, so your documents should be short enough to fit in that.
