[site]: datascience
[post_id]: 26979
[parent_id]: 26851
[tags]: 
The key question is how we can allow the public to make useful queries to a dataset without revealing private information. The field of Differential Privacy deals with answering just that. The key concept is to track a user's queries and the information revealed and add uncertainty to the answers to guarantee privacy. For example, a broad query like "what is the rate of cancer in Europe?" could have an answer of with "1% +/- .1%", whereas "what is the rate of cancer for the population of men aged 72 living at my neighbours's address?" would give an answer of "50% +/- 50%": we do not want to reveal whether a specific individual has cancer or not. Differential privacy gives users an information budget and tracks the queries, results and information revealed. The more you ask, or the closer you get to personal information the less specific the answer. At one point the information budget is exhausted and can query no more. You end up with nothing more than "your neighbour has cancer with probability p=0.5 +/- 0.5". This also guards against constructing a clever sequence of queries to pin down the answer like in the children's game of "20 questions". For example, you might try to ask questions like "what is the rate of men of cancer for men of age 72 in this city?" , "what is the rate of cancer for men of height 182 cm at age 72?" , "what is the height distribution of men with my cancer in my street?" etc. until you can deduce whether your 182 cm tall, 72 year old, male neighbour has cancer or not. Differential Privacy guarantees that such strategies will never work (under certain assumptions). Moving Averages With this background, your example of only publishing the moving average seems quite reasonable. Differential Privacy can be used to calculate how long or short the moving average needs to be on your dataset in order to provide the best possible information while preserving privacy.
