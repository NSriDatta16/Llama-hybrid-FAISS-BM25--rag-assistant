[site]: crossvalidated
[post_id]: 574188
[parent_id]: 574181
[tags]: 
Yes, it can be a problem. A very similar example was used in the Unmasking Clever Hans Predictors and Assessing What Machines Really Learn paper by Lapuschkin et al (see below). They show an example of a neural network that learned to detect "horse" based on the fact that training set images with horses contained a textual tag. The same might apply to your problem as well: if some of the classes have pictures with watermarks and some don't, the neural network can learn to simply detect the watermark to make the classification. It can also be a problem if you have different watermarks or different placements of watermarks in different classes. But it is not only about watermarks, if you have images from different sources for different classes (or different proportions of them for different classes) the model can learn to detect features of the images. An example of this was presented in another paper , where the model learned to detect a "husky" dog based on the fact alone that the pictures of huskies were all made in winter and contained snow. Unfortunately, removing the watermarks does not necessarily need to help. In the examples used by Lapuschkin et al it did help, but keep in mind that there is no way to perfectly remove the watermark. You may be able to remove the watermark so it is not visible to a human, but in most such cases it is possible to detect the removed watermark. It can be done with neural networks , but also with much more primitive algorithms. So the watermarks may be a problem and if using this data you should pay great attention to validating if or how your model treats the watermarks on the images (removed or not) and how they influence your results. Finally, the images are watermarked most likely because you didn't pay for the commercial use license for the images. In such a case, it may be illegal for you to use the images to train the algorithm and you should first consult with a lawyer.
