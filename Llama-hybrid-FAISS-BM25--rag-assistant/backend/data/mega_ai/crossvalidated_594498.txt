[site]: crossvalidated
[post_id]: 594498
[parent_id]: 594448
[tags]: 
You should always be using a train-test split, at a minimum (cross-validation being an extra step), whenever you are building a machine learning model. Splitting your data into a training set and a test set is necessary for model validation, because you want to be able to develop and optimize the model (using the training data) before testing its performance on previously unseen data (the test data). The training set will typically be the majority of your data (a split of 80/20 or 70/30 is relatively common, but it really depends on the context of the question you are trying to answer), and you will use this data for the bulk of your workflow (EDA, feature engineering, model selection, hyperparameter tuning etc.). The test set is then held back as a means for testing the performance of your final model(s), intended to replicate out-of-sample prediction, to see if the model is generalizable and whether its performance will be sufficient in production. So in your case, if youâ€™re building a decision tree model, you would develop this model on the training set before testing the performance of the decision tree model on your test set. If the test set doesn't have the outcome in it, then I am guessing you are supposed to submit your model's predictions to someone/somewhere in order to assess performance?
