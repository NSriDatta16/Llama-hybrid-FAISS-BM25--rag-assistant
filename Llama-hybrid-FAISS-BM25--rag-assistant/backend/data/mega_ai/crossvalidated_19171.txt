[site]: crossvalidated
[post_id]: 19171
[parent_id]: 19144
[tags]: 
I first want to adress something else. Reliability. I often tell my students that reliability is a neccesary but insufficient precondition for validity. In essence, if the results are not reliable, they cannot be valid. If you have bias in your rater evaluations you should first assess the reliability by computing the interater relaibility. If it is low, then your results are not reliable and therefore drawing any conclusions from your data cannot be valid. If, on the other hand inter rater reliability is high then you have a rater bias such as leniency or severity. In this case there are some things you can do. For example, standardizing the rater evaluations by computing the mean differentiation score. The one thing I would definetly NOT DO is average the ratings across the three raters until reliability is established. A quick story. Testing a claim by a mouthwash company that used two raters to judge the breath of people who consumed pizza a garlic for several hours after using one of several brands of mouthwash showed an significant ANOVA main effect that favored the mouthwash company when the judges ratings were average. Inter rater reliabiliy was about .3 (r). Therefore, the main effect cannot be valid because the judges never agreed with one another - in other words, their judgements were unreliable and as I said at the top of this post, if it is not reliable it cannot be valid. Dr. Doug
