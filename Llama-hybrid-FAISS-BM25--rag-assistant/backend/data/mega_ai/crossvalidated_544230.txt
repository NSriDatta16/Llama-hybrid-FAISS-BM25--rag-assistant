[site]: crossvalidated
[post_id]: 544230
[parent_id]: 
[tags]: 
Measurement of regularity of events

I am dealing with events generated by different processes. My data includes time stamps when the events occur, so I am trying to differentiate the processes in regular or irregular categories or assign a numeric measure how "regular" a process is. Regular process would have very similar time deltas between each event. I have looked into this paper which seemed to deal with very similar problem. Main idea would be to find an average time delta, count the occurrences of events per average time delta and perform goodness of fit tests on 3 distributions. What I don't like in particular are the definition of irregularity and bursting hypothesis. Also it doesn't look like an approach suitable for larger datasets (efficiency wise) I have looked into approximate entropy , which seems to also have a "fast and easy" approach, but I am not sure whether I am missing something essential why it wouldn't work. Also I've seen this answer , but I am a bit lost how exactly this shall be done. Would it be "bad fit equals irregular". Also how would you select the parameters (lambda) of Poisson distribution? Capturing irregularity is straight forward: goodness of fit of the times between consecutive events with a poisson distribution. Do you have (strong) opinions on one of the approaches and / or a suggestion how to tackle this problem?
