[site]: crossvalidated
[post_id]: 467402
[parent_id]: 467393
[tags]: 
This would depend on the resolution of your coding. Essentially, you need to train a model as a channel to pass the necessary information contained in the text description to the target coding. Take your example for instance, "I flip burgers" contains more information than "McDonald's Employee" if the target coding is restaurant type, as high-end restaurants might still need someone to flip burgers nicely. Therefore, the resolution of your coding scheme would determine if the extra piece of information in "I flip burgers" would be captured by your model. That is, the two descriptions make no difference if the target coding is "catering industry", but it does make a difference for restaurant types. Now back to your question, if you indeed want to encode different restaurant types, would adding "McDonald's employee" make your model biased? It depends on the data distribution. The embedding vector for "MacDonald's employee" would clearly point to the direction of "fast food". If there are other descriptions like "I flip Wagyu burgers", it might make "I flip burgers" close to high-end restaurants. But in general, we have a good chance of being right for classifying "I flip burgers" to fast food. So, even in the high resolution case, the augmentation of adding clear-cuts might bring high quality data points for training. This is very helpful in the case of training deep neural networks, as high-quality data could bring you a good "lottery ticket" in descending on the loss surface. Hope the discussion can spark some inspirations for you.
