[site]: crossvalidated
[post_id]: 578036
[parent_id]: 
[tags]: 
How to rearrange exponent terms in a Gaussian likelihood?

I'm working out of the textbook "Bayesian Data Analysis for the Behavioral and Neural Sciences" by Todd Hudson, and on p. 105 (above) we see the preceding explanation for a Gaussian likelihood, leading to an expression proportional to $$\left(\sigma\sqrt{2\pi}\right)^{-n}\, \exp\left(-\frac{\sum_i\left(x_i - \mu\right)^2}{2\sigma^2}\right)\ \propto\ \sigma^{-n}\,\exp\left({-\frac{n}{2} \frac{\overline{x^2}-{\bar x}^2}{\sigma^2}}\right)\ \exp\left({-\frac{n}{2} \frac{(\bar x - \mu)^2}{\sigma^2}}\right).$$ I understand why we take the products of each datum (independent samples) and how that leads to the summation in the exponent of the last line. However, how do we go from summing the squared errors (on the left-hand side of the proportionality symbol) to the two exponents on the right-hand side of the proportionality symbol (i.e., $\overline{x^2}-{\bar x}^2$ and $(\bar x - \mu)^2$ )? Intuitively, I can see we have the sample variance in one and the squared distance between the sample mean and the hypothesized mean ( $\mu$ ) of the distribution in the other exponent. But what are the actual steps involved in breaking down the summation into these two separate parts? Any help explaining the mechanics of this is much appreciated.
