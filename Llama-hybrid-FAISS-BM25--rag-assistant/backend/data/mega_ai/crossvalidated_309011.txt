[site]: crossvalidated
[post_id]: 309011
[parent_id]: 
[tags]: 
mini batch matrix form of neural network

I am studying neural network. I read a lot about mini-batch gradient descent. So now, I was implementing it in TensorFlow. But I have some questions regarding the matrix formulation of the problem/vectorization. Notation First some notation $w_{ij}^l$ is the weight on the connection connecting node $j$ in layer $l$ to node $i$ in layer $l+1$. If we have $4$ nodes and $3$ features we could have: $W^{(1)} = \begin{bmatrix} w_{11}^{(1)} & w_{12}^{(1)} & w_{13}^{(1)} \\ w_{21}^{(1)} & w_{22}^{(1)} & w_{23}^{(1)} \\ w_{31}^{(1)} & w_{32}^{(1)} & w_{33}^{(1)} \\ w_{41}^{(1)} & w_{42}^{(1)} & w_{43}^{(1)} \end{bmatrix}$ At the same time, suppose we have a batch size of $5$, then the input matrix might be $X = \begin{bmatrix} x_{11} & x_{12} & x_{13} \\ x_{21} & x_{22} & x_{23} \\ x_{31} & x_{32} & x_{33} \\ x_{41} & x_{42} & x_{43} \\ x_{51} & x_{52} & x_{53} \end{bmatrix}$ where $x_{ij}$ is the input in sample $i$ of feature $j$. What I don't understand Everywhere I go I see $f(W^TX +\mathbb{b})$ as the formula. However this clearly doesn't make sense here, as we would have incompatible shapes!! $W$ has shapes $(\text{number nodes}, \text{number features})$, while $X$ has shape $($ batch size, number features $)$. Ideally we would like, for each sample, to have something like this (I only write the first two columns). But how do we get there? Is this the correct way? $A^{(1)} = \begin{bmatrix} w_{11}^{(1)}x_{11} + w_{12}^{(1)}x_{12} + w_{13}^{(1)}x_{13} & w_{11}^{(1)}x_{21} + w_{12}^{(1)}x_{22} + w_{13}^{(1)}x_{23}\\ w_{21}^{(1)} x_{11} + w_{22}^{(1)}x_{12} + w_{23}^{(1)}x_{13} & w_{11}^{(1)}x_{21} + w_{12}^{(1)}x_{22} + w_{13}^{(1)}x_{23}\\ w_{31}^{(1)}x_{11} + w_{32}^{(1)}x_{12} + w_{33}^{(1)}x_{13} & w_{31}^{(1)}x_{31} + w_{32}^{(1)}x_{32} + w_{33}^{(1)}x_{33}\\ w_{41}^{(1)}x_{11} + w_{42}^{(1)}x_{12} + w_{43}^{(1)}x_{13} & w_{41}^{(1)}x_{41} + w_{42}^{(1)}x_{42} + w_{43}^{(1)}x_{43} \end{bmatrix}$
