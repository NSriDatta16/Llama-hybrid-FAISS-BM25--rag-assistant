[site]: datascience
[post_id]: 102447
[parent_id]: 102437
[tags]: 
Your understanding is correct: in the encoder-decoder attention blocks, the Keys and Values are the output of the encoder, while the Query vectors come from the decoder layers. At inference time we have as many Query positions as the step we are in. Remember that at inference time, the decoder behaves autoregressive, meaning that at each timestep T it receives the T - 1 previous tokens and predicts the T token. Such a prediction is then concatenated to the previous step input and used as input for the following step. This way, in the first step, we only have one Query vector (per layer), which is the one belonging to the first position (the beginning of sequence token, aka or ). In the second step, we have two Query vectors, and so on.
