[site]: datascience
[post_id]: 16852
[parent_id]: 16848
[tags]: 
Sample weights change the probability of each sample to be fed into model training. For example, we have 4 samples A, B, C, D with weights $1, 2, 3, 4$, it means our model has two times the probability to fit on sample B than sample A , and 3 times the probability to fit on sample C than sample A . As you wonder what's the purpose of doing that, let's imagine what our model will become when it's fed with different weights of samples. Example: Durian World If you're born in a country full of durian, when you grow up, the word "fruit" means something smells awful, but we all know that most fruit smells tasty. So is machine learning models, when our model doesn't recognize durian, just feed more durian(increase weight as there are few durian in our life compared to apple or banana). That's the situation called "unbalanced class". When(and if) you hate durian very very much (assume you would die when you smelt durian), but you can't recognize durian well, how to keep you survived? Just feed more durian and non-durian samples, you might loss the capability to distinguish apple from banana, but you shall survive. That's the situation called "cost sensitive".
