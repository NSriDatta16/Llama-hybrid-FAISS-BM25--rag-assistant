[site]: crossvalidated
[post_id]: 625653
[parent_id]: 396986
[tags]: 
Note they also expose vaguely defined feature_importances_ : Feature importances property, return depends on importance_type parameter. When model trained with multi-class/multi-label/multi-target dataset, the feature importance is “averaged” over all targets. The “average” is defined based on the importance type. For instance, if the importance type is “total_gain”, then the score is sum of loss change for each split from all trees I don't understand what the output should be for binary classification, it's not "multi" in common sense, but it differs from all the get_score options. Also I found that training XGBoost with enable_categorical=True is a total disaster (at least version 1.7.6). Categorical factors totally unrelated to the target get the highest feature importance among all the variables, including really impactful ones. Catboost and LightGBM handle categoricals much more correctly.
