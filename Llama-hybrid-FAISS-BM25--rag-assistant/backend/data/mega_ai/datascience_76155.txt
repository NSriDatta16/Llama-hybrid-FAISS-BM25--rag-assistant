[site]: datascience
[post_id]: 76155
[parent_id]: 60475
[tags]: 
Denoising an image is typically done using Encoder-Decoder models called Denoising Autoencoders , and there's a formal reason for it. Let's take a look at these architectures: We have an Encoder that generates a compressed representation of your input data, and a Decoder that restores it to normal. The reason why this architecture is so useful is that in order to be able to reconstruct the input signal, the Network must learn efficient ways to represent the same information with less nodes at the center. This operation consists of noise reduction. Noise is by definition something that cannot be learnt, and the model must focus only on the most relevant bits of information in order to work effectively. Denoising Autoencoders are trained specifically for that. These model learn to map a noisy input with a cleaned version of the same piece of data. The model, that is made especially for noise reduction, learns to extract useful information from an image, and discards anything that seems noisy. PS: In more advanced architectures, Denoising Autoencoders are paired with a Discriminator network: while the Disciminator tries to distinguish denoised versus real images, the Denoising Autoencoder tries to fool the Discriminator generating denoised images that look more and more realistic. In that case you'd have a GAN, but that's not strictly necessary to understand image denoising.
