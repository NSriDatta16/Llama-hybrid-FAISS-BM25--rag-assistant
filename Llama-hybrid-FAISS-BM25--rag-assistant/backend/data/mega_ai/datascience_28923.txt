[site]: datascience
[post_id]: 28923
[parent_id]: 28922
[tags]: 
It depends. Actually there are research papers finding that neural network can sometimes cope very well with sparse or noisy labels. But basically when you apply any machine learning approach, you want your model to pick up patterns in your data. In order to be a pattern something needs to be repetitive and predictable. Outliers and errors in your data might not adhere to that. They rather break the pattern. It can still work but it is harder to recognize it. Making a model "learn" outliers/missing values etc. will only work, if these incidences also appear in a pattern which makes them unlikely to be outliers. As a rule of thumb it is therefore a good idea to remove this noise. Finding patterns in exceptions & errors As I mentioned some researches try to address the noise issue for example by classifying if there is random ("white") noise or structured noise. They not only predict the target value but also the type of noise and then try to make another prediction based on both pieces of information. So there are some active approaches to address noise but usually this is due to the fact that it would be to cumbersome or even impossible to remove the noise.
