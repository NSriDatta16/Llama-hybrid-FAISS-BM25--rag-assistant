[site]: crossvalidated
[post_id]: 130611
[parent_id]: 130158
[tags]: 
For simplicity, bias units are subsumed into the equation by extending the input vector adding a component which is always 1. Concretely, $$ x = (x_{1}, ..., x_{n},1) $$ so that the activation for each unit can then be rewritten as, $$ a_{i} = \sum_{j=1}^{N} w_{ij}x_{j} + w_{i0} = \sum_{j=0}^{N} w_{ij}x_{j} $$ You can see a detailed derivation of the backpropagation rule in the paper neural networks and their applications .
