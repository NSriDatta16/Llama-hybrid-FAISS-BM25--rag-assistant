[site]: crossvalidated
[post_id]: 400022
[parent_id]: 400003
[tags]: 
What you want to do is to randomly partition the original data set. That means you randomly select a subset for training and use the rest for testing. If there are the same data points in both training and test sets, then the performance of your ML algorithm will most likely be overestimated. For example, if you use random forest (RF) for classification, the data points from the training set would be perfectly 'predicted' in the test set. That would lead you to announce an inflated performance of your solution and then get disappointed once the method is tried on the new data.
