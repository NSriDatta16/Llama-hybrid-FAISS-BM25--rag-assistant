[site]: datascience
[post_id]: 53737
[parent_id]: 
[tags]: 
Neural net classifier outputting extreme probabilities

I am training a multi-label neural network text classifier (i.e. a given sample can have more than one label; most samples have exactly 1 label though): Single-layer BiLSTM producing a sequence of (16, 512) vectors with dropout of 0.2 Self attention over each output vector of LSTM to produce a single 512-dimension "sentence vector" L2 normalize sentence vector ReLU layer on the normalized vector with about 750 units Dropout of 0.3 Final sigmoid output layer to output a probability for each of the K classes (K~1000) Firstly, does L2 normalization help or make things worse? Secondly, this model is quite accurate but outputs very extreme "probabilities" for each class for the samples - very often 0.9999 probability, instead of something less extreme (like say 0.8 or 0.2 or whatever). Any idea of what might be going wrong? If this is overfitting -- I'm using quite a bit of dropout too but that didn't help either. How can I remedy this?
