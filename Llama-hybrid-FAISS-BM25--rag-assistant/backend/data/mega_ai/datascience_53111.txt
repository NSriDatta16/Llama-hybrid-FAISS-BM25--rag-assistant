[site]: datascience
[post_id]: 53111
[parent_id]: 53110
[tags]: 
The word embeddings are the weights of the first layer i.e. the embedding layer and not the softmax output of the function. The embedding values represent a vector which gives the location of the word with respect to other words in a high dimensional vector space. And yes, the embedding values change according to the training corpus. However, if you are using a given language (for example English) and have a large amount of training data the final values of the vectors will turn out to be pretty close even with training corpus of different contexts.
