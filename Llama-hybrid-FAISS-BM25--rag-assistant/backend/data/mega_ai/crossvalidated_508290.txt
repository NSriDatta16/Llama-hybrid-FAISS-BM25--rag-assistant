[site]: crossvalidated
[post_id]: 508290
[parent_id]: 
[tags]: 
What is masking in the attention if all you need paper?

I am a newbie to the NLP and specifically, the attention is all you need and I can understand the encoder part of the paper. However, I am baffled about the decoder part. In the pic below and the decoder part, where does it gets the output embeddings from? Why does it need positional encoding? What is Masked Multi-head attention? Why isn't the output of the encoder connected to the input of the decoder? Instead, it is connected to the multi-head attention? Two arrows represent the output of the encoder; what does it describe? I need a layman's understanding of the above questions.
