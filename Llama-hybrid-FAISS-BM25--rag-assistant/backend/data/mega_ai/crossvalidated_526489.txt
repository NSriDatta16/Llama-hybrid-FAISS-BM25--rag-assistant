[site]: crossvalidated
[post_id]: 526489
[parent_id]: 526485
[tags]: 
Logistic regression works just like OLS regression in this regard: nominal predictors are translated to dummy coding. Try the following code: expl sold is predicted by category . There are three levels of category so R constructs two dummy variables. You can see them in the model summary: > summary(model) Call: glm(formula = sold ~ category, family = "binomial", data = expl) Deviance Residuals: Min 1Q Median 3Q Max -1.552 -1.094 0.000 1.094 1.552 Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) 0.8473 0.6901 1.228 0.2195 categoryblue -0.8473 0.9361 -0.905 0.3654 categorypink -1.6946 0.9759 -1.736 0.0825 . --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 41.589 on 29 degrees of freedom Residual deviance: 38.298 on 27 degrees of freedom AIC: 44.298 So there is a dummy variable for blue and one for pink. None is needed for the base category as that is included in the Intercept. https://en.wikipedia.org/wiki/Dummy_variable_(statistics)
