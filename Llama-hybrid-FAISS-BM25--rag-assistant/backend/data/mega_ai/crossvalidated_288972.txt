[site]: crossvalidated
[post_id]: 288972
[parent_id]: 96225
[tags]: 
Firstly there is no reason for back propagation to 'fail' in the case of ambiguous data. Here is why. Neural nets work by producing a truly highly non-linear function by composing linear functions with a non-linear activation function. The model class of neural nets are functions of this class. Roughly speaking a neural net produces a function as follows: At each stage a decision is made as to how many variables (features) to create. Each new variable is created by composing the non-linear activation function with an arbitrary linear combination of the previous variables. That means that there are (n+1)*(m) constants that are created . Each new variable is some unknown linear combination of the n variables of the previous stage plus a constant. One wishes to minimize the difference between actual observations and predictions according to some loss function $L(\Theta ,x_i)$ where $\Theta$ is all the parameters created by the model, that is the unknown sets of coefficients of all the linear functions. Thus the loss function is a function of the parameter set ${\Theta}$ and one wishes to minimize L with respect to the "theta's" In the case discussed by bayerj, that loss function is $L_{\Theta} = \sum_i (y_i - F_{\Theta} (x_i) )^2 $ . Where i runs over all the observations. The model is (in theory only !!!!) fitted by finding the parameters $\Theta$ which minimize this highly non-linear and non-convex function. In general that is impossible to do. What one can do is find local minima of the function $L_{\Theta}$ as a function of ${\Theta} = (\theta_1, \ldots, \theta_M) $. Local minima can be calculated by various methods including gradient descent, which in the context of neural nets is called 'back propagation'. So there is nothing ambiguous about the y's being multivalued. That is because one is interested in solving the system of equations $ \frac{\partial L}{\partial w_i}=0 $ for $ i = 1 \ldots (n+1)m $ . There is no inconsistency because it treats each variable set and it's outcome as constants. I will end with a little though experiment and a bigger thought experiment. An ordinary least squares model is a trivial case of a neural net in which the activation is linear and there is only one layer and one output. Imagine a data set consisting of $x_i = i, y_i = 3x_i + 'noise'$ and i running from 0 to 10,000. If I add a new observation $(x,y) = (0, 10) $ we have the ambiguous data points (0,noise) and (0,10) . However the other 9,999 observations favor the data point (0,noise) and the model will reflect a value much closer to zero than to 5 = (0+10)/2 . Bigger thought experiment. Imagine one is trying to discover probability of loan default using a neural net with income data and loan to income ratio (LTI). Suppose one trains on 1000 people who don't default and 150 who do. Now add example 1151 a person with characteristics of the top 1% of the no-default crowd, but assign him to the default outcome. For example Mr. #1151 could have just discovered that his true love in life is gambling and not going to work every day. The model will still have no choice but to characterize him as default = 0. In essence, if one looks at the loss function for the first 150 people $L(x_i) = \mbox{nnet.train}(x_i) '=' 0 $ will be almost identical. To add $L(x_{151}$ to the mix will still have a loss function majorized by the behaviour on the first 150 examples. It cannot average.
