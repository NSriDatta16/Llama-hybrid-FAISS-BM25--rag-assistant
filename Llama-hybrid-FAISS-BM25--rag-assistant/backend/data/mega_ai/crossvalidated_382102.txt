[site]: crossvalidated
[post_id]: 382102
[parent_id]: 
[tags]: 
Highly correlated engineered features any helpful?

Take a car price predictor for an example. If you know the model and year of a car, you can extrapolate facts ("engineer features") about the car. For example: city and highway mpg, number of doors, horsepower, engine size, weight, factory recalls, popularity, etc... Of course, this assumes a non-customized car. So assuming these things stay constant for cars, is there any value in using a dozen or so features that have the same value for all cars of a certain model and year, or would simply using some representation of the model and year of the car provide the same amount of signal to an ml model? For example: all 2015 Honda Civics will have the same weight, number of doors, mpg, fuel type, etc... Concrete Example Original dataset | price | make_id | model_id | year | miles | |:-------|:--------|:---------|:-----|:-------| | 1000 | 15 | 4 | 2015 | 250000 | | 25000 | 16 | 8 | 2016 | 75000 | | 45000 | 23 | 42 | 2018 | 10000 | After feature engineering: | price | make_id | model_id | year | miles | horse_power | city_mpg | hw_mpg | doors | engine_size | |:-------|:--------|:---------|:-----|:-------|:------------|:---------|:-------|:------|:------------| | 1000 | 15 | 4 | 2015 | 250000 | 160 | 18 | 23 | 4 | 1.5 | | 800 | 15 | 4 | 2015 | 720000 | 160 | 18 | 23 | 4 | 1.5 | | 500 | 15 | 4 | 2015 | 928300 | 160 | 18 | 23 | 4 | 1.5 | | 3200 | 15 | 4 | 2015 | 268300 | 160 | 18 | 23 | 4 | 1.5 | | 2600 | 15 | 4 | 2015 | 236200 | 160 | 18 | 23 | 4 | 1.5 | | 26000 | 15 | 4 | 2015 | 1320 | 160 | 18 | 23 | 4 | 1.5 | | 40000 | 15 | 4 | 2015 | 3250 | 160 | 18 | 23 | 4 | 1.5 | Note that the example with the features expanded, for a given make/model/year, every feature is the same except for the mileage. Is this redundant?
