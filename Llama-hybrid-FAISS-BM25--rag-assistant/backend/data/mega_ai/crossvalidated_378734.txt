[site]: crossvalidated
[post_id]: 378734
[parent_id]: 
[tags]: 
How to avoid L1 regularization causing informative features to get a weight of exactly 0.0.?

L1 regularization may cause the following kinds of features to be given weights of exactly 0: Weakly informative features. Strongly informative features on different scales. Informative features strongly correlated with other similarly informative features. I can avoid 2nd point by using feature scaling using z-score. But could anyone explain how to avoid 1st and 3rd scenarios while training a model(let's say Linear model) with l1-regularization? This is my use case: consider a housing data set that covers the entire globe. Bucketing global latitude at the minute level (60 minutes per degree) gives about 10,000 dimensions in a sparse encoding; global longitude at the minute level gives about 20,000 dimensions. A feature cross of these two features would result in roughly 200,000,000 dimensions. Many of those 200,000,000 dimensions represent areas of such limited residence (for example, the middle of the ocean) that it would be difficult to use that data to generalize effectively. it would be nice to encourage the weights for the meaningless dimensions to drop to exactly 0, which would allow us to avoid paying for the storage cost of these model coefficients at inference time. In this use case, you will definitely find traces of 1st & 2nd, the question is how to avoid losing the right information being captured by the model at the training time using L1-regularization (L2- is already already out of scope as it will keep the meaningless features live). please provide your suggestion pointwise. Thanks in advance.! :)
