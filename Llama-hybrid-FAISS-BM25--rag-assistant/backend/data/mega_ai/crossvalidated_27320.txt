[site]: crossvalidated
[post_id]: 27320
[parent_id]: 27318
[tags]: 
If the die rolls are independent, then you'd no sooner bet on $6$ now than you would've on the first throw - the probability is $1/6$ now just like it is on every throw. The scenario you've described seems to be related to the common probabilistic fallacy known as the law of averages , an example of which is the belief that events that are "due" to happen are somehow more likely. There certainly could be a dependence structure in a time series that makes events that have occurred less frequently than expected somehow more likely in the future (an extreme example of this is a Self Avoiding Random Walk ), but when the rolls of the die are independent, this reasoning doesn't make sense. Note that the law of large numbers implies that the observed relative frequencies of each outcome converge to the true probabilities as the sample size goes to $\infty$, so it does "even out" in the long run. But, this does not imply that the subsequent throws have an increased probability of being '6's - if exactly $1/6$ of the throws for the rest of eternity were $6$s, then the limit implied by the law of large numbers still applies. In your example, only $100$ of the first $50100$ throws were $6$s, but $$ \lim_{n \rightarrow \infty} \frac{100 + n/6}{50100 + n} = 1/6; $$ so, you can see that although $6$s are wildly underrepresented early on in the sequence, no $6$s need to be 'made up' for things to even out in the long run .
