[site]: datascience
[post_id]: 18102
[parent_id]: 18101
[tags]: 
This is expected and well established - Vanished Gradient https://en.wikipedia.org/wiki/Vanishing_gradient_problem This has the effect of multiplying n of these small numbers to compute gradients of the "front" layers in an n-layer network, meaning that the gradient (error signal) decreases exponentially with n and the front layers train very slowly. While I'm not an expert in neural network (experts, please add an answer), I'm sure professional implementation for neural network is tricky and highly optimized . If you simply implement a vanilla text-book-styled neural network, you won't be able to use it to train a large network. Keep it small and you'll be fine.
