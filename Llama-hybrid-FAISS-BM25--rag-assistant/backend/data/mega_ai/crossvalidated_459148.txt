[site]: crossvalidated
[post_id]: 459148
[parent_id]: 459124
[tags]: 
I'm assuming this a singular and non-recurring event. At minimum you can/should test differences in time-averaged means, medians, variances, etc., for pre vs. post event. Kolmogorov-Smirnov test and others can work. This is probably the "best" way because of its simplicity. The problem is that significant differences in these tests are only a sufficient but not necessary condition, i.e. null results do not prove there's no difference pre vs. post. If there is a difference, your work is done. Stationarity tests are extensions of this approach, anyway. For instance, in the case of the Chow test, a difference in autoregressive intercepts and slopes affects the pre vs. post central tendency and variability (though not always, hence ARIMA models altogether). The problem with the Chow test is exactly the seasonality, i.e. the assumption that the pre/post autoregressive error terms are iid normally distributed. You should plot to see the extent to which normality is violated, it should look closer to uniform. The iid condition is violated by the seasonality and can be visualized by plotting the residuals over time and their autocorrelation. Differencing will not resolve the problem, either, because the data are oscillatory and sin and cos are infinitely differentiable functions. Seasonal differencing will work, in this case; subtracting the time point from its equivalent time point in the previous season. See 4. With seasonality the Chow test is not useful. You would want to use a seasonal ARIMA which is different than a non-seasonal ARIMA. Here is somewhere to start in R: https://otexts.com/fpp2/seasonal-arima.html I'd suggest using bootstrap to generate confidence intervals for the pre/post ARIMA coefficients, but this is not common practice yet. For each bootstrap sample, fit the model, shuffle the residuals, then add shuffled residuals to the predicted data, recompute the model, save coefficients. You'll get a bootstrap distribution for each of your coefficients from which to derive your confidence intervals. A cursory googling shows that this is actually already implemented for ARIMA models in R: https://rdrr.io/cran/TSA/man/arima.boot.html To test significance, it should be as simple as checking to see if the pre/post ARIMA coefficients have overlapping 95% confidence intervals. There is also a question of correlated errors, in which case block bootstrap may be necessary instead, see here: https://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29#Block_bootstrap .
