[site]: crossvalidated
[post_id]: 416776
[parent_id]: 416665
[tags]: 
Turns out the answer is simpler than expected. Consider the conditional form of the likelihood function is (still conditioning on $y_0$ ) \begin{align*} L(y) &= \frac{1}{ ( \sqrt{2 \pi \sigma^2} )^n } e^{-\frac{1}{2\sigma^2} ( \sum_{t = 1}^T (y_t - \rho y_{t-1})^2)}. \\\\ \end{align*} As in the case of linear model $$ \sum_{t = 1}^T (y_t - \rho y_{t-1})^2 = \sum_{t = 1}^T (y_t - \hat{\rho} y_{t-1})^2 + (\hat{\rho} - \rho)^2 \sum_{t = 1}^T y_{t-1}^2 $$ where $\hat{\rho} = \frac{\sum_{t = 1}^T y_{t-1} y_t}{\sum_{t = 1}^T y_{t-1}^2}$ is the OLS estimate, and $\sum_{t = 1}^T (y_t - \hat{\rho} y_{t-1})^2$ is the residual sum of squares. Evidently, $$ T_1 = (\sum_{t = 1}^T (y_t - \hat{\rho} y_{t-1})^2,\, \hat{\rho},\, \sum_{t = 1}^T y_{t-1}^2) $$ is a minimal sufficient statistic. The only difference with the linear model case is the additional term $\sum_{t = 1}^T y_{t-1}^2 = X'X$ . This is not surprising. Inspecting the calculation for the linear model, we see that taking the design matrix $X$ as fixed is unnecessary. One only needs to take $X'X$ as fixed. In the time series case, the statement "taking $\sum_{t = 1}^T y_{t-1}^2$ as fixed" doesn't make sense. Instead it appears as part of a sufficient statistic. Equivalently, \begin{align*} \sum_{t = 1}^T (y_t - \rho y_{t-1})^2 &= \sum_{t = 1}^T y_t^2 - 2 \rho \sum_{t = 1}^T y_{t-1} y_t + \rho^2 \sum_{t = 1}^T y_{t-1}^2 \\\\ &= (1 + \rho^2) \sum_{t = 1}^T y_{t-1}^2 + y_T^2 - 2 \rho \sum_{t = 1}^T y_{t-1} y_t \end{align*} means $$ T_2 = (\sum_{t = 1}^T y_{t-1}^2,\, y_T^2,\, \sum_{t = 1}^T y_{t-1} y_t) $$ is another minimal sufficient statistic. Geometrically, it is clear that $T_2$ contains the same information as $$ T_1 = \mbox{ (sum of OLS square residuals, OLS estimate, squared norm of $(y_{t-1})$). } $$
