[site]: datascience
[post_id]: 27133
[parent_id]: 25176
[tags]: 
It sounds like you are trying to classify each article based on a set of tags that are inherently generated from each of the articles. As an alternative to Christian's solution (which I think does serve the purpose that you had described), you could consider using an N-Gram model and Parts-of-Speech (POS) tagging in order to do the following (in this order): Calculate the frequency of recurring, common phrases within a document / corpus Apply POS tagging to classify nouns (proper and common), verbs, adjectives... etc. (Optional) Apply some sort of rule to only select the top Y most frequent terms Using a standard language model, you can compute the relative probabilities of those words occurring within each document to then weight which article is most likely to mention a certain tag. You can learn about creating frequency tables , POS tagging , calculating relative frequencies with the attached links (in python).
