[site]: datascience
[post_id]: 108707
[parent_id]: 106267
[tags]: 
Decision trees algorithm (whether Bagging like Random Forest or Boosting like LightGBM) before Light GBM follows level wise tree growth. In level wise tree growth when you find the best node to split you increase the level of tree. Sometimes the second split done may not lead to optimal results and unneccesary growth of tree to maintain symmetrical trees. Please refer to image below : In LightGBM, the leaf-wise tree growth finds the leaves which will reduce the loss the maximum, and split only that leaf and not bother with the rest of the leaves in the same level. This results in an asymmetrical tree where subsequent splitting can very well happen only on one side of the tree. Leaf-wise tree growth strategy tend to achieve lower loss as compared to the level-wise growth strategy, but it also tends to overfit, especially small datasets. So small datasets, the level-wise growth acts like a regularization to restrict the complexity of the tree, where as the leaf-wise growth tends to be greedy. Source : https://deep-and-shallow.com/2020/02/21/the-gradient-boosters-iii-lightgbm/
