[site]: crossvalidated
[post_id]: 250637
[parent_id]: 250627
[tags]: 
In my opinion, understanding these things by analogy might be helpful in the beginning when you're first starting, but eventually you'll get stuck if you don't begin to understand these models on their own terms. A neural network is a linear model (or a multinomial logit if classification) fitting the response as a function of derived variables: $$ \begin{array}{l l} y_{i} = \gamma_1 + \mathbf{V}^1_{i}\Gamma^1 + \epsilon_{i}\\ \mathbf{V}^1_{i} = a\left([\mathbf{1,V}^2_{i}]\mathbf{\Gamma}^2\right)\\ \mathbf{V}^2_{i} = a\left([\mathbf{1,V}^3_{i}]\mathbf{\Gamma}^3\right)\\ \hspace{1.5cm}\vdots\\ \mathbf{V}^L_{i} = a\left([\mathbf{1,Z}_{i}]\mathbf{\Gamma}^L\right)\\ \end{array} $$ The derived variables are the $V$ terms, and the $\Gamma$ are parameters. (The 1's are for the "bias" terms, which are analogous to intercepts in linear models.) Down on the bottom layer, your data $Z$ are transformed first by the parameters and then by the activation function into the first set of derived variables. The process continues upwards until you reach the top layer. So to your question about activation functions -- the activation function ($a()$) simply transforms the linear combination of derived variables. It maps the real line to a subset of it. A hyperbolic tangent maps $V\Gamma$ to the interval $[-1, 1]$, and a RELU makes negative values of $V\Gamma$ go to zero. Which one makes the most sense is probably dataset-dependent. And if $a(V\Gamma) = -1$, that simply means that one of your regressors at the top level takes a value of -1. I suppose that it is only "off" if it takes a value of zero. And to your question of how to choose between different models -- the standard answer is " cross validation "
