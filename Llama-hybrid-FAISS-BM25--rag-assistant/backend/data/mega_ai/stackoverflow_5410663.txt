[site]: stackoverflow
[post_id]: 5410663
[parent_id]: 
[tags]: 
Get fast random access to binary files, but also sequential when needed. How to layout?

I have about 1 billion datasets that have a DatasetKey and each has between 1 and 50 000 000 child entries (some objects), average is about 100, but there are many fat tails. Once the data is written, there is no update to the data, only reads. I need to read the data by DatasetKey and one of the following: Get number of child entries Get first 1000 child entries (max if less than 1000) Get first 5000 child entries (max if less than 5000) Get first 100000 child entries (max if less than 100000) Get all child entries Each child entry has a size of about 20 bytes to 2KB (450 bytes averaged). My layout I want to use would be the following: I create a file of a size of at least 5MB. Each file contains at least one DatasetKey, but if the file is still less than 5MB I add new DatasetKeys (with child entries) till I exceed the 5 MB. First I store a header that says at which file-offsets I will find what kind of data. Further I plan to store serialized packages using protocol-buffers. One package for the first 1000 entries, one for the next 4000 entries, one for the next 95000 entries, one for the next remaining entries. I store the file sizes in RAM (storing all the headers would be to much RAM needed on the machine I use). When I need to access a specific DatasetKey I look in the RAM which file I need. Then I get the file size from the RAM. When the file-size is about 5MB or less I will read the whole file to memory and process it. If it is more than 5MB I will read only the first xKB to get the header. Then I load the position I need from disk. How does this sound? Is this totaly nonsense? Or a good way to go? Using this design I had the following in mind: I want to store my data in an own binary file instead a database to have it easier to backup and process the files in future. I would have used postgresql but I figured out storing binary data would make postgresqls-toast to do more than one seek to access the data. Storing one file for each DatasetKey needs too much time for writing all the values to disk. The data is calculated in the RAM (as not the whole data is fitting simultaniously in the RAM, it is calculated block wise). The Filesize of 5MB is only a rough estimation. What do you say? Thank you for your help in advance! edit Some more background information: DatasetKey is of type ulong. A child entry (there are different types) is most of the time like the following: public struct ChildDataSet { public string Val1; public string Val2; public byte Val3; public long Val4; } I cannot tell what data exactly is accessed. Planned is that the users get access to first 1000, 5000, 100000 or all data of particular DatasetKeys. Based on their settings. I want to keep the response time as low as possible and use as less as possible disk space. @Regarding random access (Marc Gravells question): I do not need access to element no. 123456 for a specific DatasetKey. When storing more than one DatasetKey (with the child entries) in one file (the way I designed it to have not to create to much files), I need random access to to first 1000 entries of a specific DatasetKey in that file, or the first 5000 (so I would read the 1000 and the 4000 package). I only need access to the following regarding one specific DatasetKey (uint): 1000 child entries (or all child entries if less than 1000) 5000 child entries (or all child entries if less than 5000) 100000 child entries (or all child entries if less than 100000) all child entries All other things I mentioned where just a design try from me :-) EDIT, streaming for one List in a class? public class ChildDataSet { [ProtoMember(1)] public List Val1; [ProtoMember(2)] public List Val2; [ProtoMember(3)] public List Val3; } Could I stream for Val1, for example get the first 5000 entries of Val1
