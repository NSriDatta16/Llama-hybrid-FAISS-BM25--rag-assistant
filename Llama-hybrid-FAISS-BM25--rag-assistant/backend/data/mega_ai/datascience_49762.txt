[site]: datascience
[post_id]: 49762
[parent_id]: 49758
[tags]: 
Some ideas: Handling categorical features correctly: using one-hot encoding is one valid approach. Other approaches include target encoding (or mean encoding), and the hashing trick. There's no real hard and fast rule about when to choose which method. Poor performance of neural network: I don't have much experience with neural networks, but I have read that inputs into neural networks should be scaled in some way - either standardised, or to lie within some narrow and consistent interval. You could also look at other layer structures - e.g. have you tried the default values from Scikit-Learn? Considering different kind of network: Judge based on (2) above More estimators in xgboost : xgboost has many parameters to fine tune. You should also consider that xgboost uses linear regression as a default regression task, which implies that your target insurance losses are normally distributed. This is not usually the case in the real world, where we see that insurance losses usually follow a Tweedie distribution. xgboost offers Tweedie regression capability. Optimal neural network for this problem: Unsure as my experience with neural networks is limited.
