[site]: crossvalidated
[post_id]: 327554
[parent_id]: 327546
[tags]: 
Cross validation would never overcome over-fitting issue, instead, CV can only control it. The purpose of cross validation in neural network: helping choosing optimal number of epochs (controlling over training weights in neural network ) robust metrics to measure the performance under the current parameter settings It is also a good practise to train your model using full data with chosen epochs (by cross validation). Cross validation will always leaves a proportion of data aside for prediction (hence model metrics), however, training model will full data use all info without leaving any info on the table. There is an idea to avoid training on full data again: with cross validation, you will have 5 models (say you have done 5-fold cross validation). And your prediction for new data would be the average of 5 model predictions. This is so called bagging in machine learning. There is a good discussion on cross validation, and hopefully this is useful: Training with the full dataset after cross-validation?
