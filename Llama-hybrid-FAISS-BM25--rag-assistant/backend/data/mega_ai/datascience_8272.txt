[site]: datascience
[post_id]: 8272
[parent_id]: 8244
[tags]: 
It depends on what you do. Column stores have two key benefits: whole columns can be skipped run-length compression works better on columns (for certain data types; in particular with few distinct values) However they also have drawbacks: many algorithms will need all columns, and only record at a time (e.g. k-means) or may even need to compute a pairwise distance matrix compression techniques only work well on sparse data types and factors, but not well on double-valued continuous data appends on column stores are expensive, so it is not ideal for streaming / changing data Columnar storage is really popular for OLAP aka "stupid analytics" (Michael Stonebraker) and of course for preprocessing where you may indeed be interested in discarding whole columns (but you would need to have structured data first - you don't store JSONs in columnar format). Because the columnar layout is really nice for e.g. counting how many apples you have sold last week. For much of the scientific / data science use cases, array databases appear to be the way to go (plus, of course, unstructured input data). E.g. SciDB and RasDaMan. In many cases (e.g. deep learning), matrixes and arrays are the data types you need, not columns. MapReduce etc. can still be useful in preprocessing, of course. Maybe even column data (but array database usually support a column-like compression, too).
