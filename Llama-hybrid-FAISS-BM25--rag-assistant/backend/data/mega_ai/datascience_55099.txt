[site]: datascience
[post_id]: 55099
[parent_id]: 
[tags]: 
Layer notation for feed forward neural networks

Apologies in advance, for I have a fairly rudimentary question on the notations for studying Feed-Forward Neural Networks. Here is a nice schematic taken from this blog-post . Here $x_i = f_i(W_i \cdot x_{i-1})$ where $f_i$ is the activation function. Let us denote the number of nodes in the $i^{\text{th}}$ layer by $n_i$ and each example of the training set being $d-$ dimensional (i.e., having $d$ features). Which of the following do the nodes in the above graph represent? Each one of the $d$ features in every example in the training set. In this case, $n_0 = d$ and $x_0$ is $(d \times 1)$ . Each example of the training set, which is $d-$ dimensional. In this case, $n_0$ is the number of examples and $x_0$ is $(d \times n_0)$ . In both cases, the weight matrix $W_i$ is $(n_i \times n_{i-1})$ . On the one hand, most references like this blog-post claim it is (1), while on the other, I can also find few references such as this video which seem to claim it is (2). Which one of them carries the right interpretation? Although seems like the back propagation algorithm can be executed in both representations, I'm quite sure it only makes sense in one of them. Any help will be greatly appreciated.
