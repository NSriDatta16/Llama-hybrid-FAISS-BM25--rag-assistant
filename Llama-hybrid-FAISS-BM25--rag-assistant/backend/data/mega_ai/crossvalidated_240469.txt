[site]: crossvalidated
[post_id]: 240469
[parent_id]: 240468
[tags]: 
It's easy to write down the pmf for the sum of results from two fair dice with $m$ and $n$ sides, and taking $m\leq n$ (for the remainder of this answer, without losing generality) we will immediately see that if you have two such sets of dice - ($m_1,n_1$) and ($m_2,n_2$) - then they will only have the same distribution if $m_1=m_2$ and $n_1=n_2$. [But firstly $m_1+n_1=m_2+n_2$ or they won't be over the same range!] Consider the first decomposition. Let's write out the table of values on the two die-faces each with probability $\frac{1}{m_1n_1}$. We can compute the probability for the sum on the two dice by starting at the (1,1) corner (where the sum is 2) and moving up a diagonal at a time (on which the sum is constant), so the total probability of each outcome in the sum must increase linearly until we hit a diagonal at a corner (at $(m_1,1)$ for a total of $m_1+1$, at which point the probabilities no longer increase on that same line and so we also get a corner in the pmf). The second decomposition must reproduce this increase until the total hits $m_1+1$ and then reproduce the change, so it too much have a corner there, implying $m_2=m_1$, whence the second die must also be the same. Actually, the general argument is barely any harder than the two-die case. Specifically, imagine we have two decompositions in non-decreasing order of die-size $(m_i,n_i,q_i,r_i,...)$ (of dimension $k$) for $i=1,2$. Writing down (tabulating) the joint pmf for the first decomposition over a rectangular lattice, we see that the pmf for the sum must increase as a polynomial of order $k-1$ until $m_1+1$, where it hits a corner and so (necessarily) changes. To get the same pmf, the second decomposition must also increase in the same way, to the same point, whence it must also be over a rectangle with the same smallest dimension $m_1$ (i.e. $m_2=m_1$). Removing that common die from both decompositions we reduce the problem to a smaller one, to which we can apply the argument again until the list is exhausted. That seems straightforward enough. Alternatively we could look into probability generating functions (pgfs); the pgf of a sum of random variables will be the product of their pgfs ($G_{X+Y}(s)=G_X(s)G_Y(s)$). The pgf of a discrete uniform on $1, 2, ..., n$, ($X\sim \text{DU}(n)$) is $G_X(s)= \frac {s \left({1 - s^n}\right)} {n \left({1 - s}\right)}$ The pgf of the sum of two such is $\frac {s \left({1 - s^m}\right)} {m \left({1 - s}\right)} \frac {s \left({1 - s^n}\right)} {n \left({1 - s}\right)}$ Even when the $mn$ term in the denominator can be factored in several ways, the term $(1-s^m)(1-s^n)= 1 - s^m-s^n + s^{m+n}$ won't factorize into two terms of the form $(1-s^a)(1-s^b)$ any other way (if $a+b=m+n$ for $a\leq b$, but $a\neq m$, the behaviour of the $s^m+s^n=s^m(1+s^{n-m})$ terms would be different from $s^a+s^b=s^a(1+s^{b-a}$), so it seems that we must have $a=m$. It looks like almost the same argument as before -- the only way to decompose the pgf of the sum is into the pgf for two discrete uniforms you began with; if we match the highest power ($m+n=a+b$ - and we must or they're not over the same range of values) then we're left with the first possible difference being with the powers in the individual $s^k$-type terms whence we must match the lowest pair of powers and its coefficient, at which point we can pull out the common die and the remaining die must match. This should extend to more than two dice and then the "lowest power of the form $s^k$ - and hence the lowest die - must match, and when we remove the common die we can make the same argument again" approach seems to work here as well. [There are unique factorization theorems for polynomials, and we could probably get one over this particular domain, but it seems we won't need to]
