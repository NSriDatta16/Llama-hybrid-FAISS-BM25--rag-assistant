[site]: crossvalidated
[post_id]: 563196
[parent_id]: 563189
[tags]: 
To use methods like random forest, you need a fixed sized vector representation. The way you do this is up to you. You can flatten all the edge values, append any node or edge related extra features you have etc. But, all of your data vectors should have the same size. This might become trickier if you have variable size graphs, like molecules, e.g. each has different atoms, so the graphs vary in size. You don't need to use torch data types to vectorize your data. You can just read your data directly from the source based on your flattening logic. Or, you can flatten each torch tensor with torch.flatten and concatenate. But, if the graph size of each data sample is different, you'll need to pad some of them. Another approach that might be useful is graph neural networks. There are studies experimenting on molecular datasets, e.g. given a molecule, predict it is toxic or not. One example could be Adaptive GCN published in AAAI. Or, any other paper, that uses molecular datasets (e.g. SMILES, Tox21 etc.) could be useful, especially if they share code, e.g. a CNN approach .
