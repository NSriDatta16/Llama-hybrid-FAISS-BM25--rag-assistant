[site]: crossvalidated
[post_id]: 488058
[parent_id]: 
[tags]: 
Should convolutions or transposed convolutions be used in the decoder part of a Conv-based autoencoder?

I am implementing a convolutional autoencoder. For the decoder part of the model, some examples (such as this one from Francois Chollet ) use standard convolutional layers (Conv2D in keras) in the decoder part of the model (in combination with an upsampling layer) , whereas others (such as this one ) use transposed convolutional layers (Conv2DTranspose in Keras), in the example without an upsampling part. Which is the better choice, what are possible advantages or disadvantages of those two options? When using transposed convolutional layers, should I adjust the filter size to achieve upsampling, or should I include additional upsampling layers?
