[site]: crossvalidated
[post_id]: 312766
[parent_id]: 
[tags]: 
What is the cause of the sudden drop in error rate that one often sees when training a CNN?

I've noticed in different papers that after a certain number of epochs there sometimes is a sudden drop in error rate when training a CNN. This example is taken from the "Densely Connected Convolutional Networks" paper, but there are many others. I wonder what is the cause of this? Especially why it appears at the same epoch although the two architectures compared here a completely different. A similar question was asked here , but the only answer given there so far stated that it's because a saddle point in the error surface is reached and then overcome. As the two architectures are very different, they also will have a very different error surfaces (even with different dimensions), so I find highly unlikely that they both reach and overcome a saddle point at exactly the same epoch.
