[site]: crossvalidated
[post_id]: 2232
[parent_id]: 2230
[tags]: 
From my point of view the difference is important, but largely for philosophical reasons. Assume you have some device, that improves with time. So, every time you use the device the probability of it failing is less than before. Convergence in probability says that the chance of failure goes to zero as the number of usages goes to infinity. So, after using the device a large number of times, you can be very confident of it working correctly, it still might fail, it's just very unlikely. Convergence almost surely is a bit stronger. It says that the total number of failures is finite . That is, if you count the number of failures as the number of usages goes to infinity, you will get a finite number. The impact of this is as follows: As you use the device more and more, you will, after some finite number of usages, exhaust all failures. From then on the device will work perfectly . As Srikant points out, you don't actually know when you have exhausted all failures, so from a purely practical point of view, there is not much difference between the two modes of convergence. However, personally I am very glad that, for example, the strong law of large numbers exists, as opposed to just the weak law. Because now, a scientific experiment to obtain, say, the speed of light, is justified in taking averages. At least in theory, after obtaining enough data, you can get arbitrarily close to the true speed of light. There wont be any failures (however improbable) in the averaging process. Let me clarify what I mean by ''failures (however improbable) in the averaging process''. Choose some $\delta > 0$ arbitrarily small. You obtain $n$ estimates $X_1,X_2,\dots,X_n$ of the speed of light (or some other quantity) that has some `true' value, say $\mu$. You compute the average $$S_n = \frac{1}{n}\sum_{k=1}^n X_k.$$ As we obtain more data ($n$ increases) we can compute $S_n$ for each $n = 1,2,\dots$. The weak law says (under some assumptions about the $X_n$) that the probability $$P(|S_n - \mu| > \delta) \rightarrow 0$$ as $n$ goes to $\infty$. The strong law says that the number of times that $|S_n - \mu|$ is larger than $\delta$ is finite (with probability 1). That is, if we define the indicator function $I(|S_n - \mu| > \delta)$ that returns one when $|S_n - \mu| > \delta$ and zero otherwise, then $$\sum_{n=1}^{\infty}I(|S_n - \mu| > \delta)$$ converges. This gives you considerable confidence in the value of $S_n$, because it guarantees (i.e. with probability 1) the existence of some finite $n_0$ such that $|S_n - \mu| n_0$ (i.e. the average never fails for $n > n_0$). Note that the weak law gives no such guarantee.
