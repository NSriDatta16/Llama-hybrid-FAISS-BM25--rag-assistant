[site]: crossvalidated
[post_id]: 278552
[parent_id]: 
[tags]: 
Estimating Covariance Matrix of Innovations of Multivariate Random Walk

Suppose that I have a multivariate random walk: $X_{t+1} = X_t + \epsilon_t$ where $\epsilon_t \sim N(0,\Sigma)$ Estimating the covariance matrix $\Sigma$ is straightforward from first differences $X_{t+1}-X_t$ using the sample covariance. Instead if I would like to estimate the covariance directly from the levels $X_0,...,X_n$ again using the sample covariance but this time on the levels and then using the relation $Cov(X_n-X_0) = n \Sigma$ to find $\Sigma$ by simply dividing by $n$, I have a feeling that I will underestimate the variances hence overestimate the correlations. I also cannot decide whether to subtract the unconditional mean $X_0$ or sample mean of the levels during the calculation of covariance matrix. Basically does $\hat{Cov}(X_n-X_0) = \frac{1}{N} \Sigma_1^N(X_n-X_0)(X_n-X_0)^T$ or the same formula where $X_0$ replaced with sample average $\hat{\mu} = \frac{1}{N} \Sigma_1^N X_i$ makes more sense (in this scenario we divide by $N-1$ because of degrees of freedom but that's not the issue)? Would I be underestimating the variances and overestimating the correlations? Obviously my intention is not to estimate the covariance of innovations in this manner where there is already a way to estimate them using the difference series. My main question is how this is affecting OLS estimates of multiple regression when regressors have unit roots, as $\beta = \large{\frac{Cov(X,y)}{Cov(X)}}$ doesn't seem to be so well defined anymore? Yes they are super consistent in case of a cointegration, but that's a little after the fact in terms of estimation.
