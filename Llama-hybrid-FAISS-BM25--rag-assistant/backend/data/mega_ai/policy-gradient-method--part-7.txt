ut policy gradient should be on-policy. So, as θ {\displaystyle \theta } changes, the surrogate loss becomes more and more off-policy. This is why keeping θ {\displaystyle \theta } proximal to θ t {\displaystyle \theta _{t}} is necessary. If there is a reference policy π ref {\displaystyle \pi _{\text{ref}}} that the trained policy should not diverge too far from, then additional KL divergence penalty can be added: − β E s , a ∼ π θ t [ log ⁡ ( π θ ( a | s ) π ref ( a | s ) ) ] {\displaystyle -\beta \mathbb {E} _{s,a\sim \pi _{\theta _{t}}}\left[\log \left({\frac {\pi _{\theta }(a|s)}{\pi _{\text{ref}}(a|s)}}\right)\right]} where β {\displaystyle \beta } adjusts the strength of the penalty. This has been used in training reasoning language models with reinforcement learning from human feedback. The KL divergence penalty term can be estimated with lower variance using the equivalent form (see f-divergence for details): − β E s , a ∼ π θ t [ log ⁡ ( π θ ( a | s ) π ref ( a | s ) ) + π ref ( a | s ) π θ ( a | s ) − 1 ] {\displaystyle -\beta \mathbb {E} _{s,a\sim \pi _{\theta _{t}}}\left[\log \left({\frac {\pi _{\theta }(a|s)}{\pi _{\text{ref}}(a|s)}}\right)+{\frac {\pi _{\text{ref}}(a|s)}{\pi _{\theta }(a|s)}}-1\right]} Group Relative Policy Optimization (GRPO) The Group Relative Policy Optimization (GRPO) is a minor variant of PPO that omits the value function estimator V {\displaystyle V} . Instead, for each state s {\displaystyle s} , it samples multiple actions a 1 , … , a G {\displaystyle a_{1},\dots ,a_{G}} from the policy π θ t {\displaystyle \pi _{\theta _{t}}} , then calculate the group-relative advantage A π θ t ( s , a j ) = r ( s , a j ) − μ σ {\displaystyle A^{\pi _{\theta _{t}}}(s,a_{j})={\frac {r(s,a_{j})-\mu }{\sigma }}} where μ , σ {\displaystyle \mu ,\sigma } are the mean and standard deviation of r ( s , a 1 ) , … , r ( s , a G ) {\displaystyle r(s,a_{1}),\dots ,r(s,a_{G})} . That is, it is the standard score of the rewards. Then, it maximizes the PPO objective, averaged over all actions: max θ 1 G ∑ i = 1 G E ( s , a 1 , … , a G ) ∼ π θ t [ { min ( π θ ( a i | s ) π θ t ( a i | s ) , 1 + ϵ ) A π θ t ( s , a i ) if A π θ t ( s , a i ) > 0 max ( π θ ( a i | s ) π θ t ( a i | s ) , 1 − ϵ ) A π θ t ( s , a i ) if A π θ t ( s , a i ) < 0 ] {\displaystyle \max _{\theta }{\frac {1}{G}}\sum _{i=1}^{G}\mathbb {E} _{(s,a_{1},\dots ,a_{G})\sim \pi _{\theta _{t}}}\left[{\begin{cases}\min \left({\frac {\pi _{\theta }(a_{i}|s)}{\pi _{\theta _{t}}(a_{i}|s)}},1+\epsilon \right)A^{\pi _{\theta _{t}}}(s,a_{i})&{\text{ if }}A^{\pi _{\theta _{t}}}(s,a_{i})>0\\\max \left({\frac {\pi _{\theta }(a_{i}|s)}{\pi _{\theta _{t}}(a_{i}|s)}},1-\epsilon \right)A^{\pi _{\theta _{t}}}(s,a_{i})&{\text{ if }}A^{\pi _{\theta _{t}}}(s,a_{i})<0\end{cases}}\right]} Intuitively, each policy update step in GRPO makes the policy more likely to respond to each state with an action that performed relatively better than other actions tried at that state, and less likely to respond with one that performed relatively worse. As before, the KL penalty term can be applied to encourage the trained policy to stay close to a reference policy. GRPO was first proposed in the context of training reasoning language models by researchers at DeepSeek. See also Reinforcement learning Deep reinforcement learning Actor-critic method References Sutton, Richard S.; Barto, Andrew G. (2018). Reinforcement learning: an introduction. Adaptive computation and machine learning series (2 ed.). Cambridge, Massachusetts: The MIT Press. ISBN 978-0-262-03924-6. Bertsekas, Dimitri P. (2019). Reinforcement learning and optimal control (2 ed.). Belmont, Massachusetts: Athena Scientific. ISBN 978-1-886529-39-7. Grossi, Csaba (2010). Algorithms for Reinforcement Learning. Synthesis Lectures on Artificial Intelligence and Machine Learning (1 ed.). Cham: Springer International Publishing. ISBN 978-3-031-00423-0. Mohamed, Shakir; Rosca, Mihaela; Figurnov, Michael; Mnih, Andriy (2020). "Monte