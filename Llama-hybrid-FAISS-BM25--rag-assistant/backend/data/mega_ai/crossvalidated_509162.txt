[site]: crossvalidated
[post_id]: 509162
[parent_id]: 430341
[tags]: 
I think using linear regression for multi-classes classify is totally doable. First, let's recall that the backbone of logistic regression itself is linear regression. Following is the logistic function, but it is really just a linear function wrapped sigmoid function. When we train logistic regression, we are really just tweaking the parameter of the linear function inside the sogmoid function. Here I would like to demonstrate with sklearn in python: import numpy as np from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression, LinearRegression from sklearn.model_selection import train_test_split X, y = load_iris(return_X_y=True) X_train,X_test, y_train, y_test = train_test_split(X,y,test_size=0.2) ## logistic regression clf_logistic = LogisticRegression(random_state=0,max_iter=1000).fit(X_train, y_train) #print(f"score of logistic method: {clf_logistic.score(X_test, y_test)}") ### the following lines do the exact same thing y0 = np.argmax(X_test.dot(clf_logistic.coef_.T)+clf_logistic.intercept_, axis=-1) y1 = clf_logistic.predict(X_test) assert np.array_equal(y0,y1) == True acc_logistic = np.sum(y0 == y_test)/y_test.shape[0] print(f"accuracy of logistic method: {acc_logistic}") ## linear regression clf_linear = LinearRegression().fit(X_train, y_train) #print(f"score of logistic method: {clf_linear.score(X_test, y_test)}") y2 = np.round(X_test.dot(clf_linear.coef_.T)+clf_linear.intercept_) y3 = np.round(clf_linear.predict(X_test)) assert np.array_equal(y2,y3) == True acc_linear = np.sum(y2 == y_test)/y_test.shape[0] print(f"accuracy of linear method: {acc_linear}") output: accuracy of logistic method: 0.9333 accuracy of linear method: 1.0 As to which one is better, I don't know the exact answer, it may depend on the datasets. comments: What the python code above does is using logistic and linear regression to classify iris into 3 categories (0,1,2). Each iris data has 4 features and 1 label. Logistic regression used one vs rest to train the data and predict the result, you can look into the shape of clf_logistic.coef_, and clf_logistic.intercept_. However, linear regression doesn't use one vs rest (you may aslo look at the shape of clf_linear.coef_, clf_linear.intercept_), it directly predicts a numerical value, simply round float point prediction result to int, you will get the predicted label. It works well, as you can see from the printed accuracy. My understanding is that even though linear regression doesn't use one vs rest, but it works in a similar way to a single neuron like wx + b
