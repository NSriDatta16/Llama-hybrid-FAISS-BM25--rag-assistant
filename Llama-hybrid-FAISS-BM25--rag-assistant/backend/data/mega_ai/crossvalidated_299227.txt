[site]: crossvalidated
[post_id]: 299227
[parent_id]: 
[tags]: 
Polynomial approximations of nonlinearities in neural networks

Imagine, that the only operations I have are scalar addition and scalar multiplication and I want to implement different nonlinearities for neural networks with them. The only option I see here is to use some polynomial approximations. I can use Taylor series approximations, but it gives poor results. For example, for smooth ReLU $\mathrm{ln}(1 + e^x)$ its Taylor approximation with 9 terms greatly diverges outside the range $[-4,4]$ (look at the plots on WolframAlpha) . This is unacceptable because activations in neural networks can go far beyond this range. For other nonlinearities (logistic sigmoid and tanh) we have a similar picture. Of course, I can use much more terms, but it will be computationally intractable. So, I have two questions: Are there any good polynomial approximations of nonlinearities used in neural networks (ReLU, logistic, tanh)? What range should I care about? For example, if some function $f(x)$ is a good approximation of ReLU in the range [-100, 100], will it be enough for me in practice? In what range are activations of nonlinearities in AlexNet, GoogLeNet, VGG, etc?
