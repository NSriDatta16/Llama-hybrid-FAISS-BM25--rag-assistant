[site]: datascience
[post_id]: 93131
[parent_id]: 
[tags]: 
Why does word2vec try to maximize the dot product of the center word vector and context?

I am learning about the maths behind word2vec from this tutorial . u are the embeddings for the center word and v for the context. It appears that this dot product is to be maximized. Why the context words are not necessarily similar to the center. Sure they appear together, but it does not mean they are synonyms.
