[site]: crossvalidated
[post_id]: 259202
[parent_id]: 259176
[tags]: 
In practice, random forests overcome this problem with randomization. Regarding a single tree , it is possible that features which are good in combination but not individually do not get selected. In the decision trees literature, there exist non-greedy approaches called lookahead : the best possible combination of features is tried up to a certain level of depth. E.g. in Lookahead-based algorithms for anytime induction of decision trees or in MurTree: Optimal Decision Trees via Dynamic Programming and Search . Other approaches that explicitly use combination of features at each node are the oblique decision trees . In general, if you have domain knowledge and you know how some features should be combined you might want to add an additional feature to combine $f_1$ and $f_2$ . E.g. if you have weight and height you might want to try computing the BMI feature. Then, you can choose whether to keep it or not in the model if it improves accuracy or interpretability. You might also want to try to remove the original features if this simplifies the model.
