[site]: datascience
[post_id]: 112701
[parent_id]: 
[tags]: 
Classification problem with no context in numerical features

I have an extremely abstract and numeric data with equally abstract objective. I have around 3000 rows of train data ( df_train ), where I have a binary target variable target (0 or 1), where I have 50 numerical (float) features num1, num2, ..., num50 ranging from 0 and 1, and 50 integer features int1, int2, ..., int50 being either -1, 0, or 1. This adds up to my data having 3000 rows and 101 columns. My test data( df_test ) has the same format as the train data excluding the target variable, having 500 rows. My objective is to classify the target variable based on other features in the test data. Given that there are a lot of features, my instinct was to do a dimensionality reduction, and since the goal is to classify rather than cluster, I thought PCA would be more appropriate compared to other manifold methods such as t-SNE. I have a couple questions regarding the designing of the solution: Naturally, as the number of PCA components get closer to the number of features, it explains more variability. What is the good threshold to explain the data yet still reduce the dimension? After fitting the PCA by scikit, how can the result play into actually classifying the target variable in my test data? Is there a more appropriate dimensionality reduction technique, or furthermore is it actually necessary to do a dimensionality reduction? Any insights are greatly appreciated.
