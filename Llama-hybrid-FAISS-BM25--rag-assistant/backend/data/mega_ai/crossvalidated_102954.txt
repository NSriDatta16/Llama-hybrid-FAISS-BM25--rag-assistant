[site]: crossvalidated
[post_id]: 102954
[parent_id]: 102949
[tags]: 
I'll assume for the moment you're predicting X from Y and it's only the variable being predicted that's measured with error (see the warning in my comment if this is not the case). I'll interchange the roles of X and Y to be the usual way around. That is, in what follows $Y$ is the variable being predicted, and is measured with error, while $X$ is the predictor or predictors. The calculation is reasonably straightforward. The variance of a confidence interval is $\sigma^2\mathbf{x^*}(X^TX)^{-1}\mathbf{x^*}^T$ * where $\mathbf{x^*}$ is the row vector of predictors. If the rows of $X$ are random draws from some population of predictors, than as $n$ grows, $n(X^TX)^{-1}$ approaches a constant. *(while that for a prediction interval is $\sigma^2(1+\mathbf{x^*}(X^TX)^{-1}\mathbf{x^*}^T)$) That is, there's a $\sigma/\sqrt{n}$ type effect in the standard error of the mean forecast at $\mathbf{x^*}$ (just as with an average). You need 4 times the data to make the standard error halve, but you only need to halve the average error (make measurements with twice the precision) to have the same effect. If you can make the measurements have 90% of the error, that would be equivalent to having ($1/0.9^2 \approx 1.235$) 23.5% more observations. Because $\sigma$ should be proportional to the average error, while $1/\sqrt{n}$ decreases as the square of the sample size, the tradeoff is pretty easy to work with. If you're talking prediction intervals rather than confidence intervals, larger $n$ is unlikely to be worthwhile; it soon gives almost no return.
