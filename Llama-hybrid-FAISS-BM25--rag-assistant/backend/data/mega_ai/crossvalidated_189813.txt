[site]: crossvalidated
[post_id]: 189813
[parent_id]: 189687
[tags]: 
1: Yes you should either use OOB-CV with 5000 trees and/or 10-fold 20-repeated. Training on only 60% is likely to yield a pessimistic cross validation. With such few samples, any single CV will vary considerably. Repeat the 10-fold CV many times and compute the average score(+/- SD). Do not pool predictions from repeated CV and then after compute the statistics. That will yield a over optimistic CV. If using OOB-CV only, then train 5000-10000 trees to achieve a stable result. After CV, use all samples to train a single model. 2: Any decision tree model, such as random forest and gbm (gradient boosting machine, uses trees as default) handle features as non-parametric and is invariant to any monotonic transformation. Thus, no scaling etc. is needed. It's a fine start just to mix all features from both sources. You may want to perform some modest variable selection on your features, as most out of 5000 micro-array features are expected to be less useful. Don't search for a magic perfect combination of ~7 variables, but narrowing the field of potential useful variables may yield a modest improvement. Try e.g. to pretrain a RF and drop ~50-90% of variables with lowest permutation variable importance (never use gini). Variable selection needs to be cross-validated also. The direct OOB-CV error after variable selection is over optimistic. I was in similar situation training a RF model with 41 samples(very few) and 250 features(reduced to 30 features) . Variable selection seemed to yield a modest prediction accuracy improvement in outer cross-validation and in external test sets. 3: For any classifier which yields not only labeling, but also yields some probability estimate, can be used to form a ROC plot. A ROC-plot communicates any attainable combination of selectivity and sensitivity. Here is ROC example for randomForest. R package for Weighted Random Forest? classwt option? . In the slides [slide 16], the caret package is used to tune a gbm model with a ROC metric in a inner calibration loop, [edited] but not cross-validated with a outer validation loop. Thus, the tuning only selects a good set of hyper-parameters but do not provide an unbiased CV estimate. For that, wrap the tuning in a outer cross-validation manually The tuning/calibration metric does not have to match the cross-validation metric. To form a ROC plot you need the true labels(binary classification) and some cross-validated probability estimates of one of the classes. Try e.g. AUC to form the ROC plots yourself. 4 : Yes correlation filtering can also work. RF or GBM handle collinearity quite well but not perfect. Hoards of redundant or unrelated features will lower accuracy. In practice you may find that, filtering by variable importance is sufficient. As a thumb-rule, two completely redundant variables tend to share the same raw unscaled variable importance. Thus, filtering by variable importance is automatically also correlation filtering. Bonus: Found this great article on nested-CV and tuning best-practice.
