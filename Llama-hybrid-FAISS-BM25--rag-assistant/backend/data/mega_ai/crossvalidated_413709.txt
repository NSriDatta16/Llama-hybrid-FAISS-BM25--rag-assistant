[site]: crossvalidated
[post_id]: 413709
[parent_id]: 248873
[tags]: 
Now my problem is with backpropagation. After calculating the loss, how do I calculate the gradients of V(s) and A(s,a) without there being any weights? If so, what about Deltas of A(s,a) Looking at the comments from the currently accepted answer, I'd like to add mine too. This post from Data Science shows the gradient vector that should enter V and A layers respectively.
