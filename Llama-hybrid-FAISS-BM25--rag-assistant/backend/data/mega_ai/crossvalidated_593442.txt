[site]: crossvalidated
[post_id]: 593442
[parent_id]: 
[tags]: 
Continuity assumption in Machine Learning

It is clear that in the case of neural-networks-based models, we rely on continuity assumption of the underlying function $f$ that we are trying to approximate with our network given the universal approximation t. What I'm wondering about is, for other ML algorithms (e.g. Random Forest), do we still rely on the assumption that small perturbations on the inputs would lead to small perturbations on the output?
