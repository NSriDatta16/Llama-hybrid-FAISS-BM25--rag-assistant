[site]: datascience
[post_id]: 57176
[parent_id]: 
[tags]: 
Does the bias of an artificial neuron adjust or remain constant during training?

Forum contributor David Waterworth wrote: "we train $w_{0j}$ ", here $w_{0j}$ is the bias of an artificial neuron. However, this Wikipedia article https://en.wikipedia.org/wiki/Artificial_neural_network (in section "Optimization", under subtitle "Algorithm") seems to indicate that the bias does not adjust during training: (I only quoted relevant sentences.) "Let $N$ be a neural network with $e$ connections, $m$ inputs, and $n$ outputs. $w_0, w_1, w_2,$ ... denote vectors in $R^e$ . These are called weights. The output of the algorithm is then $w_p$ ." In other words, the training/optimization algorithm starts with $w_0$ , then produces $w_1$ , then produces $w_2$ , etc. Please note that $w_1, w_2, ...$ , are $e$ -dimensional vectors, and $e$ is the total number of connections. So, does this Wikipedia article say that only the weights of the connections adjust, but the bias (weight of the neuron) does not adjust? If the bias also adjusts, then the output vector of the training/optimization algorithm would have more dimensions - The number of dimensions would have to be: [the total number of connections $e$ ] plus [the total number of neurons which have biases]. Perhaps the training/optimization algorithm has evolved since this Wikipedia article was written?
