[site]: crossvalidated
[post_id]: 599564
[parent_id]: 598459
[tags]: 
We can convert this into a standard problem and you can go on from there. To begin the conversion I will slightly change the notation into a conventional one in statistics. Let $\beta = (\beta_1,\beta_2,\ldots, \beta_p)^\prime$ be the unknown vector. You have $n$ observations of the angles $\theta_i$ made between $\beta$ and $n$ known $p$ -vectors $x_1,x_2,\ldots,x_n.$ Because angles give no information about magnitudes, we can only hope to recover $\beta$ up to a positive multiple. This allows us to assume that, say, $||\beta||=1.$ In that case the inverse cosine of the angle with $x_i = (x_{i1}, x_{i2}, \ldots, x_{ip})$ multiplied by the length $||x_i||$ gives the dot product, $$y_i = \arccos(\theta_i) ||x_i|| = x_i\cdot \beta = \sum_{j=1}^p x_{ij}\beta_j .$$ Arranging the vectors as rows of a design matrix $X$ yields the relation $$X\beta = y.$$ This is a standard underdetermined (" $n \ll p$ ") regression problem, subject to the constraint $||\beta||=1.$ However, even this constraint doesn't matter, because if you find any nonzero solution $\hat\beta$ it still works: remember, we cannot hope to determine $||\beta||$ in the first place. Various methods exist to estimate $\beta,$ including Bayesian methods, Ridge Regression, and the Lasso.
