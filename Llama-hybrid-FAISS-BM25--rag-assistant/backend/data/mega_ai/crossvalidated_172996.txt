[site]: crossvalidated
[post_id]: 172996
[parent_id]: 
[tags]: 
Posterior distribution in Bayesian linear regression - why not include $p(X | \beta, \sigma^2)$?

Given parameter/s $\theta$, data $X$ and prior on the parameter/s $p(\theta)$, Bayes' theorem allows us to estimate the posterior distribution $p(\theta | X)$: $p(\theta | X) = \frac{p(\theta) p(X | \theta)}{p(X)}$ $\to p(\theta | X) \ \propto \ p(\theta) p(X | \theta)$ From the Bayesian linear regression Wiki page : $p(\beta, \sigma^2 | y, X) \ \propto \ p(y | X, \beta, \sigma^2) p(\beta | \sigma^2) p(\sigma^2)$ I was expecting something like: $p(\beta, \sigma^2 | y, X) \ \propto p(y | X, \beta, \sigma^2) \color{red}{p(X | \beta, \sigma^2)} p(\beta | \sigma^2) p(\sigma^2)$ or since I guess $p(\beta, \sigma^2, y, X) = p(y | X, \beta, \sigma^2) p(X, \beta, \sigma^2)$ Without $p(X | \beta, \sigma^2)$, I guess it's still true, but why not include it?
