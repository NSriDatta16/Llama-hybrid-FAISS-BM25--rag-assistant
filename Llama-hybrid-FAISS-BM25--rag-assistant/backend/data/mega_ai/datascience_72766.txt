[site]: datascience
[post_id]: 72766
[parent_id]: 72764
[tags]: 
No, not all of your assumptions are true 1. Missing values The sklearn implementation of RandomForest does not handle missing values internally without clear instructions/added code. So while remedies (e.g. missing value imputation, etc.) are readily available within sklearn you DO have to deal with missing values before training the model. This involves understanding how and why values are missing (MCAR VS. MAR VS. ...) and determining the best way to deal with those values (median/mean imputation, MICE, removing values, etc.). 2. Outliers and Skewness Again this isn't a done deal! You need some EDA to understand your data here and handle it accordingly. Strong biases in the distribution of the dependend variable (so-called class imbalances) can heavily influence the result. Look into ways to handle those like SMOTE. Outliers are less of a problem, granted they really ARE outliers. Check via EDA and confirm this isn't a problem to be dealt with. 3. Feature selection and transformation It is true that many ML models favor a more-is-more approach to feature selection. The main benefit of using RandomForest, XGB over classical statistical approaches is that they cope much better with irrelevant predictors. Still feature selection also means feature engineering which is still helpful and necessary. Additionally for practical purposes you should still aim to reduce predictor amount if you want to use your model again. Because no matter how irrelevant every predictor used in the training model NEEDS TO BE PRESENT in the unseen data as well. Lastly you will need to transform data! Simply example, RandomForest needs one-hot-encoding (OHE) for factor variables and string variables. You do not need the same scale for all your numerical variables but they all need to be numerical!
