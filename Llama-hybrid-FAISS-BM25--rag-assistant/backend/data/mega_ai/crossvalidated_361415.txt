[site]: crossvalidated
[post_id]: 361415
[parent_id]: 
[tags]: 
How to understand the term $\sum_{s', r}p(s', r|s,a)$ in Bellman's equation

One part from the bellman's equation in Sutton's "Reinforcement Learning: An introduction" confuses me: the third line contains the term $\sum_{s'}\sum_r{p(s', r|s, a)}$. The fact that it involves $\sum_r$ makes me believe that given a state $s'$, we need to sum up all of the rewards we might receive from $s'$, meaning that there might be more than one possible rewards we might receive when we reach $s'$. But isn't the reward we receive when we arrive at a state fixed? If there is indeed more than one possible reward to be received at $s'$, then value under policy $\pi$ at $s'$ ,$v_{\pi}(s')$, is $E_{\pi}[G_{t+1}|S_{t+1} = s']$, and $G_{t+1} $ is not dependent on the reward received at $S_{t+1}=s'$ since $G_{t+1} = \sum^{\infty}_{k=0}{\gamma}^kR_{t+k+2}$. Then shouldn't the final equation be $$\sum_a{\pi(a|s)}\sum_{s'}{p(s'|s, a)\gamma v_\pi(s')\sum_rp(r|s')r}$$ where $p(r|s')$ is the probability of receiving reward $r$ at state $s'$?
