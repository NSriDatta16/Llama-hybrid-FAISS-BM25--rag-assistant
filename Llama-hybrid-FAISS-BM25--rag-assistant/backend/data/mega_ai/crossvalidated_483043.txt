[site]: crossvalidated
[post_id]: 483043
[parent_id]: 483038
[tags]: 
Your problem comes from this: All predictors and the dependent variable sum to 0. That necessarily imposes a linear dependence, hence the near-perfect collinearity. If you "can't eliminate even a single variable," you are forced to admit a serious limit in your ability to assign unique predictor importances. Yes, each of the individual variables might have a plausible, potentially causal relationship with outcome. But with a linear dependence you can't think of all the predictors separately. In that case the effects of any one predictor can always be expressed, at least roughly, in terms of apparent effects of the others. In your sample data there isn't perfect multicollinearity, although variance inflation factors are fairly high (from 3.3 to 19). To see what's going on, take a simple example with 3 predictors $A$ , $B$ , and $C$ , constrained by $A+B+C=0$ .* The software won't let you fit all 3 with this perfect multicollinearity, so you fit a model with just $A$ and $B$ : $$Y=\beta_0 + \beta_A A + \beta_B B + \epsilon.$$ But you can just substitute the constraint to get an equally valid: $$Y=\beta_0 - \beta_A C + (\beta_B-\beta_A) B + \epsilon.$$ So is the real "importance" of $B$ represented by $\beta_B$ or by $(\beta_B -\beta_A)$ ? Is it $A$ that is "important" or is it $C$ ? The argument is similar, if not so dramatic, when the collinearity isn't perfect. Attempts to estimate individual predictor importance among correlated predictors are, at best, hard to interpret and can be downright misleading. A tree-based method like xgboost doesn't get around the problem, it just might hide it better. See this discussion for example. At any branch point the software might choose one of several correlated predictors, so that they all show up in a final estimate of variable importance. But the estimated variable importances of the correlated variables will necessarily be correlated among themselves, as you could see by comparing xgboost results with different random seeds or with resampled data. That's the same problem as the variance inflation of the linear regression coefficients. If you want to include all of a set of correlated predictors in your model, you must accept this difficulty with assigning separate importance to each of them. I would recommend seeing what others in your field have done to deal with such situations. Although this type of work is far out of my field, it seems that what you are doing bears some relationship to Leontief input-output models , for which I would assume there is a whole technology developed for time-series analysis. *Your statement suggests that it's the sum of all the predictors plus the outcome that equals 0. In that case you need to ask why all of your predictor regression coefficients aren't coming out to the obvious values of -1 each, with an intercept of 0. Even if you are getting regression coefficients other than those, as you do with the sample data you provide (intercept is 0, but coefficients range from -1 to +11), arguments similar to this will hold. My suspicion is that your model is inherently unstable, essentially trying to model the minor unaccounted-for items that prevent the exact realization of your constraint.
