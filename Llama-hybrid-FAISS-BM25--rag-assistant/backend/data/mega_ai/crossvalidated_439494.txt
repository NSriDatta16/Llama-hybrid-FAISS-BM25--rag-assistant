[site]: crossvalidated
[post_id]: 439494
[parent_id]: 439485
[tags]: 
You need to check the accuracy difference between train and test set for each fold result. If your model gives you high training accuracy but low test accuracy so your model is overfitting. If your model does not give good training accuracy you can say your model is underfitting. GridSearchCV is trying to find the best hyperparameters for your model. To do this, it splits the dataset into three-part. It uses a train set for the training part then test your data with validation set and tuning your parameters based on the validation set results. Finally, it uses test set to take the final model accuracy. from sklearn.model_selection import KFold kf = KFold(n_splits=5,random_state=42,shuffle=True) # these are you training data points: # features and targets X = .... y = .... accuracies = [] for train_index, test_index in kf.split(X): data_train = X[train_index] target_train = y[train_index] data_test = X[test_index] target_test = y[test_index] # if needed, do preprocessing here clf = LogisticRegression() clf.fit(data_train,target_train) test_preds = clf.predict(data_test) test_accuracy = accuracy_score(target_test,test_preds) train_preds = clf.predict(data_train) train_accuracy = accuracy_score(target_train, train_preds) print(train_accuracy, test_accuracy, (train_accuracy - test_accuracy) ) accuracies.append(accuracy) # this is the average accuracy over all folds average_accuracy = np.mean(accuracies)
