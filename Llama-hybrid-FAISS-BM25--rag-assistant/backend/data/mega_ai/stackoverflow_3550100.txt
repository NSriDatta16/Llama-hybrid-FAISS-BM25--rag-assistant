[site]: stackoverflow
[post_id]: 3550100
[parent_id]: 3550068
[tags]: 
I would guess "no"! And if the answer happens to be "yes", then it's almost certainly so irregular that it'll be way slower for a convolution-type operation. EDIT To qualify my guess, take an example. Let's say we store a[0][0] first. We want a[k][0] and a[0][k] to be similar distances, and proportional to k , so we might choose to interleave the storage of first row and first column (i.e. a[0][0], a[1][0], a[0][1], a[2][0], a[0][2] , etc.) But how do we now do the same for e.g. a[1][0] ? All the locations near it in memory are now taken up by stuff that's near a[0][0] . Whilst there are other possibilities than my example, I'd wager that you always end up with this kind of problem. EDIT If your data is sparse, then there may be scope to do something clever (re Cubbi's suggestion of R-trees). However, it'll still require irregular access and pointer chasing, so will be significantly slower than straightforward convolution for any given number of points.
