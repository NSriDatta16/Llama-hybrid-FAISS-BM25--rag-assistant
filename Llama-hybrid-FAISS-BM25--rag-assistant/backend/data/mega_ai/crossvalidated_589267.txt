[site]: crossvalidated
[post_id]: 589267
[parent_id]: 577846
[tags]: 
Sidenote: It depends on the situation The question is a bit of a loaded question. It presupposes that neural networks are better. But, whether neural networks are better depends on the situation. If parametric models are applicable, then often these will work better. If the data generation doesn't follow a complex pattern (so nothing specific for a deep neural network to learn), then often the other shallow machine learning methods work better. But still, the question is a fair question. The observation that over-parameterized models perform surprisingly good in particular settings, without overfitting (and whether this is only with neural networks or not is actually not relevant), that is not something unreal. Flat vs Deep The relationship between deep neural networks and other more shallow machine learning methods (shallow could be kernel methods or decision trees, but they can also be very complicated), is like the relationship between Copernicus model of the solar system and Ptolemy's model of the solar system. In some way, the kernel methods and decision trees are like glorified methods for smoothening or averaging of data. There is no strong connection with any simple underlying process or patterns that may create the observations. The methods are only superficially learning how to be able to describe the observations in a way that it can be interpolated or extrapolated with a sufficient accuracy. If you add more data the methods gain some precision for the area where the data has been added, but their learning capacity stagnates because the models never gain a 'higher level of understanding' of the patterns or some break down of the complexity of the observations into simple building blocks. On the other hand, the deep neural networks are sort of like applying Occam's razor and bring the complex gamut of observations down to a simplified underlying principle, which is captured by the organization of the network (and with every extra network layer the possibilities grow multiplicative increasing the potential complexity and rate of simplifying power). The neural network tries to learn the observation by learning a pattern behind it. Double descent phenomenon The over-parameterized networks are susceptible to fitting noise, but the simplest patterns are easier to learn and will become dominant. This is either due to some explicit regularization (obvious in ridge regression or Lasso) or due to some implicit regularization like limits on learning rates and stochastic decent. In this respect an example that more parameters work better is seen in this question: Is ridge regression useless in high dimensions ($n \ll p$)? How can OLS fail to overfit? . Another effect could be that the multiple parameters work a bit like gradient boosting and are being blended together. When we sp√®eak about gradient boosting then there are also a lot of parameters fitted, more than the number of data points, but the method uses some average and we consider it not really as increasing the flexibility. This can happen in some similar way in deep neural networks and the first layers create several parallel pattern recognition models that are blended together in other layers.
