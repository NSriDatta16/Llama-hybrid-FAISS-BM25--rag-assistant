[site]: crossvalidated
[post_id]: 50961
[parent_id]: 
[tags]: 
Fitting a surface to 3D data

I am building a program to predict the score in a game of twenty20 cricket. I have a series of 3D datapoints, calculated from a number of games I have data for. Along the x axis we have number of wickets a team has lost. Up the y axis we have number of runs they end up scoring. And in the z axis we have number of overs into the innings we are. For example, one datapoint might say, we are 10 overs into the game, the team are 2 wickets down: They end up scoring 160 runs. From this data I want to be able to plug in how many overs into a game we are and how many wickets a team has lost, and get how many runs a team are predicted to score. I have plotted the data here (where different colour lines refer to different overs in the z axis): As you may or may not be able to tell, in the middle of the data the lines correlate pretty well, this is because we have far more data for "average" games (e.g. where a team is 4 wickets down after 10 overs). On the edges however the data is based on very few games (e.g. where a team is no wickets down at the 19th over), where a data point seems obviously wrong to me (an outlier), I have added a dashed line to correct it. The question is, how do I build a smooth surface which fits these points? And how do I minimize the effect of the outliers? Is there a specific name for this kind of thing? EDIT: Now that I think about it, it's not really outliers i'm getting on the edges, just less reliable data.
