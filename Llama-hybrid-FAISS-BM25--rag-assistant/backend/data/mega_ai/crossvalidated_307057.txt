[site]: crossvalidated
[post_id]: 307057
[parent_id]: 307025
[tags]: 
It may be important to define what a model is. For example the full model of linear regression is not just $Y=\beta X$ but: $Y=\beta X+\epsilon$ $\epsilon$ has normal distribution with mean 0 and variance $\sigma$ $X$ and $\epsilon$ are independent This defines totally the distribution of $Y$ given $X$ and you can define the likelihood $p(y|x,\beta,\sigma)$ of a single line, and then the likelihood of the full training set: $L(\beta,\sigma)=\Pi_i p(y_i|x_i,\beta,\sigma)$ There is special loss function derived from the likelihood $L$. It is simply $-\log(L)$. For linear regression this loss function happens to be OLS (modulo a few constants you don't care about and where $\sigma$ disappears magically). As a consequence, maximizing the likelihood for linear regression is the same as minimizing OLS. In most models where $Y$ depends on $X$, a fully specified model is the distribution of $Y$ given $X$ (and a parameter). Thus every model has a likelihood. There is always a "canonical" way to create a loss function from a model: $-\log(L)$ (or any decreasing function instead of $-\log$ which does not change the solution of the minimization problem). A very common way to fit models is to use maximum likelihood, hence implicitly minimize this loss function. It is called maximum likelihood estimation (MLE). Logistic regression is most often fitted with MLE. More generally, generalized linear models are fitted with MLE. Same for Gaussian mixtures... But it is not the only way to do. If you think MLE is canonical, then yes, models come with a canonical loss function. But sometimes MLE is not used and another loss function can be used.
