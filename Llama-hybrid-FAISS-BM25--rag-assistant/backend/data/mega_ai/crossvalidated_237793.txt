[site]: crossvalidated
[post_id]: 237793
[parent_id]: 237790
[tags]: 
I will stick to neural networks. Note that some people swap validation and test set around. In neural nets you train until your performance on the validation set starts getting worse ('stopped training'). Therefore the weights are being influenced by the validation set (just not through backprop). Now you test your network with brand new data (the test set) which has not been used in the model building process at all. This same approach applies also when selecting hyperparameters eg dropout level/L2 regularisation parameter etc.
