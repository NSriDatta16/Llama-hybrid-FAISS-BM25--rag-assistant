[site]: crossvalidated
[post_id]: 182386
[parent_id]: 147021
[tags]: 
In highly unbalanced datasets, how do you detect over fitting? Use metrics that are robust against unbalanced datasets, like Precision, Recall or F1-score. In your example with 99% 1 's and 1% 0 's. A classifier that always predicts positive samples has an Accuracy of 0.99 ; but Precision, Recall and F1-score of 0.00 What can you do to avoid overfitting in unbalanced datasets? Cluster you positive samples into several clusters of the same size of the negative samples, i.e. move from binary classification to multi-label classification Sub-sample the positive samples in order to have a 50/50 dataset. Use other algorithm that deals naturally with unbalanced datasets, like anomaly detection methods. Do Random Forest overfit? Yes. Any classifier with high complexity (large number of parameters) respect to the training data can overfit. However, the overfitting will not increase when the number of single Decision Trees is increased. How to avoid overfitting with Random Forest? Decrease the complexity of the Decision Tree: pre- or post-pruning Randomly drop features and/or samples per node
