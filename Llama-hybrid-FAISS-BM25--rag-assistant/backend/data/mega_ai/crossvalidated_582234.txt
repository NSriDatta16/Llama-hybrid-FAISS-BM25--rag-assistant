[site]: crossvalidated
[post_id]: 582234
[parent_id]: 
[tags]: 
Power calculation to check if multiple predictive models are correlated

Say there is a classification setting so that $$ { \{(x_1,y_1),(x_2,y_2), ..., (x_n,y_n)\} } $$ is a set of $n$ observations, and then there is a set of $m$ estimators (models) that take an individual $ x $ and make predictions about $y$ , so that, for the first pair of observations $ (x_1,y_1)$ , there are $m$ predictions: $$h^{(1)}(x_1)=\hat{y}_1^{(1)}, $$ $$h^{(2)}(x_1)=\hat{y}_1^{(2)}, $$ $$..., $$ $$h^{(m)}(x_1)=\hat{y}_1^{(m)} $$ In the end, the goal is to do majority voting or averaging, so uncorrelated models are preferred. My question is: how many samples $n$ do we need to determine whether the $m$ models are (un)correlated enough? Or, similarly, how many samples $n$ do we need to properly compute the variance between models? So far, I have tried simulations of different levels of correlation between $h$ 's, but I'm guessing there is a better way, perhaps a closed form? Thanks!
