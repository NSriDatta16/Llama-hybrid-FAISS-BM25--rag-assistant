[site]: crossvalidated
[post_id]: 376767
[parent_id]: 376766
[tags]: 
In a fully connected neural network, the input can't change size because the linear transform in the first layer $Wx+b$ wouldn't work anymore -- the weight matrix $W$ wouldn't be of the correct shape. However, note that you can apply a convolution to an image of any size without needing to change the parameters in the filter. So there is nothing restricting the size of the input image. It makes sense that the network can generalize to inputs of different shape -- you are still applying the same convolutional filters to the same feature maps, so why shouldn't the result be the same as before?
