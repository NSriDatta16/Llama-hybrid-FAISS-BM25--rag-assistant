[site]: crossvalidated
[post_id]: 604127
[parent_id]: 604113
[tags]: 
Have you considered a stacking approach where you build a meta predictor using the 4 model outputs as features? In your case I would consider building a NaÃ¯ve Bayes classifier and then predicting an outlier based on the maximum a-posteriori hypothesis: \begin{align} P(Outlier|m_1,...,m_4) &\propto P(Outlier) \prod_{m=1}^4 P(M_i=m_i|Outlier) \\ P(\neg Outlier|m_1,...,m_4) &\propto P(\neg Outlier) \prod_{m=1}^4 P(M_i=m_i|\neg Outlier) \end{align} where $m_i$ is the output of the $i$ -th unsupervised outlier detection model for a given new point. You predict "Outlier" whenever $P(Outlier|m_1,...,m_4) > P(\neg Outlier|m_1,...,m_4)$ . From your labelled training set (assumed to be representative) you need to: estimate the prior probabilities $P(Outlier)$ and $P(\neg Outlier)$ estimate $P(M_i=1|Outlier)$ which is the sensitivity (true positive rate) of predictor $i$ (and hence also find $P(M_i=0|Outlier)=1-P(M_i=1|Outlier)$ ) estimate $P(M_i=0|\neg Outlier)$ which is the specificity (true negative rate) of predictor $i$ (and hence also find $P(M_i=1|\neg Outlier)=1-P(M_i=0|\neg Outlier)$ ). So, for example, if the 4 predictors predict [0,1,0,1], you would predict "outlier" if: $$P(Outlier) \cdot (1-Sens_1) \cdot Sens_2 \cdot (1-Sens_3) \cdot Sens_4 > P(\neg Outlier) \cdot Spec_1 \cdot (1-Spec_2) \cdot Spec_3 \cdot (1-Spec_4). $$ As you can see, compared to a simple model average, this approach takes into account the breakdown in terms of sensitivity and specificity rather than just the model's accuracy. If your models $m_i$ provide a score, rather than a binary value, you can use pretty much the same approach by changing the conditional probabilities into $P(M_i\ge m_i|Outlier)$ (etc), which can be estimated by counting how many of the "Outlier" examples in the labelled training set provided a score at least as high as the current sample (and same for the " $\neg$ Outliers").
