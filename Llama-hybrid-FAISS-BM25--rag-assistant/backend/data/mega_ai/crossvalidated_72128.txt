[site]: crossvalidated
[post_id]: 72128
[parent_id]: 11109
[tags]: 
This is an expansion of Scortchi and Manoel's answers, but since you seem to use R I thought I'd supply some code. :) I believe the easiest and most straightforward solution to your problem is to use a Bayesian analysis with non-informative prior assumptions as proposed by Gelman et al (2008). As Scortchi mentions, Gelman recommends to put a Cauchy prior with median 0.0 and scale 2.5 on each coefficient (normalized to have mean 0.0 and a SD of 0.5). This will regularize the coefficients and pull them just slightly towards zero. In this case it is exactly what you want. Due to having very wide tails the Cauchy still allows for large coefficients (as opposed to the short tailed Normal), from Gelman: How to run this analysis? Use the bayesglm function in arm package that implements this analysis! library(arm) set.seed(123456) # Faking some data where x1 is unrelated to y # while x2 perfectly separates y. d |z|) ## (Intercept) -18.528 75938.934 0 1 ## x1 -4.837 76469.100 0 1 ## x2 81.689 165617.221 0 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1.3863e+01 on 9 degrees of freedom ## Residual deviance: 3.3646e-10 on 7 degrees of freedom ## AIC: 6 ## ## Number of Fisher Scoring iterations: 25 Does not work that well... Now the Bayesian version: fit Super-simple, no? References Gelman et al (2008), "A weakly informative default prior distribution for logistic & other regression models", Ann. Appl. Stat., 2, 4 http://projecteuclid.org/euclid.aoas/1231424214
