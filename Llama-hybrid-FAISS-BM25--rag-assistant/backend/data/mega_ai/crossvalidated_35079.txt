[site]: crossvalidated
[post_id]: 35079
[parent_id]: 35071
[tags]: 
For the definition of the rank of a matrix, you can refer to any good textbook on linear algebra, or have a look at the Wikipedia page . A $n \times p$ matrix $X$ is said to be full rank if $n \geq p$ , and its columns are not a linear combination of each other. In that case, the $p \times p$ matrix $X^TX$ is positive definite, which implies that it has an inverse $(X^TX)^{-1}$ . If $X$ is not full rank, one of the columns is fully explained by the others, in the sense that it is a linear combination of the others. A trivial example is when a column is duplicated. This can also happen if you have a 0-1 variable and a column consists of only 0 or only 1. In that case, the rank of the matrix $X$ is less than $n$ and $X^TX$ has no inverse. Since the solution of many regression problems (including logistic regression) involves the computation intermediate $(X^TX)^{-1}$ , it is then impossible to estimate the parameters of the model. Out of curiosity, you can check here how this term is involved in the formula of multiple linear regression. That was it for absolute rank deficiency. But sometimes the problem shows up when the matrix $X$ is "almost" not full rank, as extensively detailed by @woodchips. This problem is usually referred to as multicollinearity . This issue is fairly common, you can find out more on how to deal with it on related posts here and there .
