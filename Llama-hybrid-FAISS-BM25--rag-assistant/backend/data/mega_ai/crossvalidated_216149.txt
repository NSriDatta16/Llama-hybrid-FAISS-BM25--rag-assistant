[site]: crossvalidated
[post_id]: 216149
[parent_id]: 95480
[tags]: 
Here is a good resource I use for comparison - their performance on different benchmark datasets. This is an excellent site that does that has an ordered results of most of the noteworthy papers. Take note of the "details" button on the right column, It gives a short description on their testing methodology, for example if an ensemble was used, what type of augmentation, etc. A word of advice, the best thing you can do is try it yourself . From my personal experience, significant part of the performance difference published in deep learning papers has to do with training knowhow - proper choice of hyperparameters and augmentation than the actual architecture. sad but true. UPDATE: The site I posted now seems to be less frequently updated. I would humbly still suggest it as an excellent starting point. But I suggest all avid explorers to look further into citations of these original papers to see how the research evolved.
