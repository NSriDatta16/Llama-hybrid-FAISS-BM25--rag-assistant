[site]: crossvalidated
[post_id]: 623256
[parent_id]: 479689
[tags]: 
A proper scoring rule measures the quality of estimates of the probability of class membership produced by probabilistic classifiers, such as logistic regression. The ideal decision boundary (assuming equal misclassification costs) is the contour where that probability of class membership is 0.5. So having a decision boundary is completely compatible with proper scoring rules. Even if you are only interested in the discrete "yes/no" decision, a proper scoring rule is often a good way of obtaining a good decision boundary, because if your model gives accurate estimates of the probability of class membership, it will give accurate estimates where that probability is 0.5. HOWEVER, a proper scoring rule is not necessarily going to give the optimal decision boundary. This is because optimising a proper scoring rule will try to optimise estimates of probability everywhere with high data density, not just near the decision boundary. Fitting a model often involved compromise, if the model sacrifices the accuracy of probabilities near the decision boundary in order to make greater improvements in the probabilities away from the decision boundary, then the proper scoring rule will have a better value, but the decision boundary will be worse and the classifier will make more errors in operation. For an example and illustration, see my answer to a related question. This is one reason why the Support Vector Machine can give better results than [kernel] logistic regression in some applications. Vapnik writes: "When solving a problem of interest, do not solve a more general problem as an intermediate step. Try to get the answer that you really need but not a more general one." So if you just want to estimate the decision boundary, you shouldn't estimate probabilities everywhere , just determine the decision boundary (which is working out specifically just where that probability is 0.5). The reason for this is that we use the data more efficiently if we don't spend modelling resources on aspects of the data that are irrelevant for our particular purpose. HOWEVER, HOWEVER, in practice we often do want the probabilities, perhaps because operational class frequencies or misclassification costs are variable or unknown at training time, or we need a reject option where the classifier is uncertain etc. So only use SVMs where these things are fixed and known in advance. People do use e.g. Platt scaling to get probabilities from SVMs, but I don't think that is a good idea either - if you want probabilities everywhere, use kernel logsitic regression or Gaussian Process Classification, models that are designed to do that. To misquote Vapnik: "When solving a problem of interest, do not solve a less general problem as an intermediate step and then bodge it to solve the actual problem. Try to get the answer that you really need directly." And even then, try a probabilistic model as well, so you have a baseline for comparing the performance of the SVM.
