[site]: crossvalidated
[post_id]: 462076
[parent_id]: 
[tags]: 
Shouldn't log likelihood always be normalized by data size in bayesian estimation?

This is very interesting problem. I wonder if the whole bayesian statistics is neglecting it or if I am super confused. I will illustrate it on a bayesian Maximum a Posteriori (MAP) estimation and later we can discuss how would it be when full distribution is considered (using e.g. MCMC). We are looking for MAP estimate for parameters $\theta$ : $$\theta_{MAP} = \mathrm{argmax}_\theta \ \ p(\theta|y) = \mathrm{argmax}_\theta \ \ {p(y|\theta) \cdot p(\theta) \over p(y)} = \mathrm{argmax}_\theta \ \ p(y|\theta) \cdot p(\theta) = \mathrm{argmax}_\theta \ \ \mathrm{log}(p(y|\theta) \cdot p(\theta)) = \mathrm{argmax}_\theta \ \ \mathrm{log} \ p(y|\theta) + \mathrm{log} \ p(\theta)$$ a $p(y|\theta)$ is the log likelihood, $p(\theta)$ is prior. Now, this is interesting: the magnitude of the log likelihood $p(y|\theta)$ will vary very much based on data size: I get values like -4414.6 for N=7576 and -59.4 for N=59. But the magnitude of the prior stays the same. So, just based on data size, the log prior will get different weight in the estimation compared to the log likelihood! Which looks like a different model. Question 1: Am I right in this conclusion? So (Question 2) , wouldn't it make more sense to "normalize" the log likelihood according to data size, like this? : $$\mathrm{argmax}_\theta \ \ {1 \over N} \mathrm{log} \ p(y|\theta) + \mathrm{log} \ p(\theta)$$ For example, in Yi et al (2011), they do this: I already tried this approach and found that different values of $\lambda$ will yield different cross-validation results (which doesn't yet imply that the likelihood should be normalized by data size, but it shows that the relative scale of the prior to the likelihood is important). There are some approaches like Regularization where this has been consciously taken care of. But, surprisingly, when I look at the papers and current practice in my area (in the context of Gaussian Processes), taking care of this important issue (like in the above paper) is very rare and most studies don't care about that at all and just stay with the bayesian MAP approach. Which is confusing me, because: (Question 3) Isn't the MAP approach a priori wrong, because the weight of the log prior (i.e. penalization, regularization, however you interpret the prior) relative to log likelihood will change with data size? And the interesting discussion would be whether problem does apply in the case when we don't just care about the maximum of the distribution as in MAP, but study the whole posterior distribution as e.g. in MCMC, so shouldn't we be looking at this instead? : $${p(y|\theta)^{1 \over N} \cdot p(\theta) \over p(y)}$$ Mentioned references: Yi, G., Shi, J. Q., & Choi, T. (2011). Penalized gaussian process regression and classification for high-dimensional nonlinear data. Biometrics, 67(4), 1285â€“1294. doi: 10.1111/j.1541-0420.2011.01576.x
