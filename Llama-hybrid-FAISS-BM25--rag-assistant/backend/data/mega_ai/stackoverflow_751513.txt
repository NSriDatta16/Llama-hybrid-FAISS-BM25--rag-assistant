[site]: stackoverflow
[post_id]: 751513
[parent_id]: 
[tags]: 
Chained data computation. Make and alternatives?

On UNIX, I have to produce numeric results out of previous data by means of various command-line utilities. These utilities read the starting data (in general, but not only, from csv files), perform computations, and write the data (again, in general, but not only, to csv files). Of course, I don't want to run the risk to have outdated derived data, so I need to chain the data dependency through the utilities. A trivial similarity can be seen with a spreadsheet: when you change a cell, all the other cells that are related change as well, in a cascade fashion. However, due to the more complex and automatized nature of my task, I cannot use a spreadsheet. Now, the first idea you can have is to use make, which is something I already had experience with. It is trivial, and fits the task well. You are however dependent on files, so if you have a dependency against some data that is stored in a database, you must trick the system. I also know about biomake, but as far as I remember is made in prolog, and I don't want to adventure through that path. Before I adventure the makefile path, I am interested in additional input from you. Does any of you have nice suggestions on how to do this kind of data handling, better utilities than make, and on how to organize the file layout (of the data and the makefiles)?
