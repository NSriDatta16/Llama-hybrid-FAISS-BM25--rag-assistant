[site]: crossvalidated
[post_id]: 214273
[parent_id]: 
[tags]: 
Are Residual Networks related to Gradient Boosting?

Recently, we saw the emergence of the Residual Neural Net, wherein, each layer consists of a computational module $c_i$ and a shortcut connection that preserves the input to the layer such as the output of the ith layer exhibits: $$ y_{i+1} = c_i + y_i $$ The network allows to extract residual features and allows for deeper depth while being more robust to the vanishing gradient problem, achieving state of the art performance. Having delved into gradient boosting , a very powerful ensembling technique in the machine learning world, which also seems to perform a form of gradient optimization on the residual of the loss, Its hard not to see some form of similarity. I know that they are similar but not the same - one major difference I noticed is that gradient boosting performs optimization on the additive term while the residual net, optimizes the entire network. I didn't see He et al note this as part of their motivation in their original paper . So I was wondering what are your insights on this topic and ask that you share interesting resources that you have. Thank you.
