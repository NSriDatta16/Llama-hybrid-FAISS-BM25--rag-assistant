[site]: crossvalidated
[post_id]: 312411
[parent_id]: 312397
[tags]: 
What you are describing is process where the error is not known in the beginning or there is no "real" error signal. The goal is to iteratively learn the function. An example for that is Reinforcement Learning. What he did there is Reinforcement Learning combined with a Deep Neural Net. In essence you have a set of actions $a$ (up, down, etc.), states $s$ (the screen) and a reward signal $r$. Now you want to find the optimal policy $\pi$ is found. Possibly Deep Q Learning , which works like this (brief description): Feedforward pass for the current state $s$ to get predicted Q-values for all actions. Feedforward pass for the following state $s’$ and find maximum over all outputs $\underset{a'}{max} \ Q(s’, a’)$. Set Q-value target for action to $r + \gamma \underset{a'}max \ Q(s’, a’)$ (use the max calculated in step 2). For all remaining actions set the Q-value target to the same as returned from step 1, which makes the error $0$ Update the weights using backpropagation. Where $\gamma$ is a discount factor for future rewards.
