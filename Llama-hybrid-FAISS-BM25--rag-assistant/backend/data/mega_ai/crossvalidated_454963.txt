[site]: crossvalidated
[post_id]: 454963
[parent_id]: 454885
[tags]: 
Given you have well-founded hypotheses about which variables should be included as predictors of your outcome, there is little reason to worry about variable selection. As you point out, the model comparison you are running with loo is evaluating the fit of a model with only intercepts and no predictors to a model with all the predictors plus the intercepts. Unless most of your predictors are not at all helping to explain the outcome (or predict to new observations), I would be shocked if the results indicated the random intercept only model was the superior model. Thus I am not sure it makes sense to run the model comparison using these two models. Is it the case that one or more of the predictors are less well-founded in the literature? If so, you might instead remove those two predictors, run that model, and compare it to the model with all predictors included. Or, if there is some reason to believe that the effect of one or more of the predictors on the outcome vary across groups (i.e., a random slope), you might estimate such a model and compare it to the model without random slopes. Given Bayesian estimation and loo takes some time to run, depending on sample size and model complexity, I would be more judicious about model comparison. Or, I would consider doing some of these comparisons using maximum likelihood in lmer or perhaps rstanarm because the models are pre-compiled. Perhaps also consider using variational inference with either brms or rstanarm with the (algorithm="meanfield") option just to speed things up for model testing.
