[site]: crossvalidated
[post_id]: 412886
[parent_id]: 412670
[tags]: 
You started well by writing down an expression for the likelihood. It is simpler to recognize that $Y,$ being the sum of $N$ independent Normal $(\mu,\sigma^2)$ variables, has a Normal distribution with mean $N\mu$ and variance $N\sigma^2,$ whence its likelihood is $$\mathcal{L}(y,N) = \frac{1}{\sqrt{2\pi N\sigma^2}} \exp\left(-\frac{(y-N\mu)^2}{2N\sigma^2}\right).$$ Let's work with its negative logarithm $\Lambda = -\log \mathcal{L},$ whose minima correspond to maxima of the likelihood: $$2\Lambda(N) = \log(2\pi) + \log(\sigma^2) + \log(N) + \frac{(y-N\mu)^2}{N\sigma^2}.$$ We need to find all whole numbers that minimize this expression. Pretend for a moment that $N$ could be any positive real number. As such, $2\Lambda$ is a continuously differentiable function of $N$ with derivative $$\frac{d}{dN} 2\Lambda(N) = \frac{1}{N} - \frac{(y-N\mu)^2}{\sigma^2N^2} - \frac{2\mu(y-N\mu)}{N\sigma^2}.$$ Equate this to zero to look for critical points, clear the denominators, and do a little algebra to simplify the result, giving $$\mu^2 N^2 + \sigma^2 N -y^2 = 0\tag{1}$$ with a unique positive solution (when $\mu\ne 0$ ) $$\hat N = \frac{1}{2\mu^2}\left(-\sigma^2 + \sqrt{\sigma^4 + 4\mu^2 y^2}\right).$$ It's straightforward to check that as $N$ approaches $0$ or grows large, $2\Lambda(N)$ grows large, so we know there's no global minimum near $N\approx 0$ nor near $N\approx \infty.$ That leaves just the one critical point we found, which therefore must be the global minimum. Moreover, $2\Lambda$ must decrease as $\hat N$ is approached from below or above. Thus, The global minima of $\Lambda$ must be among the two integers on either side of $\hat N.$ This gives an effective procedure to find the Maximum Likelihood estimator: it's either the floor or the ceiling of $\hat N$ (or, occasionally, both of them!), so compute $\hat N$ and simply choose which of these integers makes $2\Lambda$ smallest. Let's pause to check that this result makes sense. In two situations there is an intuitive solution: When $\mu$ is much greater than $\sigma$ , $Y$ is going to be close to $\mu,$ whence a decent estimate of $N$ would simply be $|Y/\mu|.$ In such cases we may approximate the MLE by neglecting $\sigma^2,$ giving (as expected) $$\hat N = \frac{1}{2\mu^2}\left(-\sigma^2 + \sqrt{\sigma^4 + 4\mu^2 y^2}\right) \approx \frac{1}{2\mu^2}\sqrt{4\mu^2 y^2} = \left|\frac{y}{\mu}\right|.$$ When $\sigma$ is much greater than $\mu,$ $Y$ could be spread all over the place, but on average $Y^2$ should be close to $\sigma^2,$ whence an intuitive estimate of $N$ would simply be $y^2/\sigma^2.$ Indeed, neglecting $\mu$ in equation $(1)$ gives the expected solution $$\hat N \approx \frac{y^2}{\sigma^2}.$$ In both cases, the MLE accords with intuition, indicating we have probably worked it out correctly. The interesting situations, then, occur when $\mu$ and $\sigma$ are of comparable sizes. Intuition may be of little help here. To explore this further, I simulated three situations where $\sigma/\mu$ is $1/3,$ $1,$ or $3.$ It doesn't matter what $\mu$ is (so long as it is nonzero), so I took $\mu=1.$ In each situation I generated a random $Y$ for the cases $N=2,4,8,16,$ doing this independently five thousand times. These histograms summarize the MLEs of $N$ . The vertical lines mark the true values of $N$ . On average, the MLE appears to be about right. When $\sigma$ is relatively small, the MLE tends to be accurate: that's what the narrow histograms in the top row indicate. When $\sigma \approx |\mu|,$ the MLE is rather uncertain. When $\sigma \gg |\mu|,$ the MLE can often be $\hat N=1$ and sometimes can be several times $N$ (especially when $N$ is small). These observations accord with what was predicted in the preceding intuitive analysis. The key to the simulation is to implement the MLE. It requires solving $(1)$ as well as evaluating $\Lambda$ for given values of $Y,$ $\mu,$ and $\sigma.$ The only new idea reflected here is checking the integers on either side of $\hat N.$ The last two lines of the function f carry out this calculation, with the help of lambda to evaluate the log likelihood. lambda
