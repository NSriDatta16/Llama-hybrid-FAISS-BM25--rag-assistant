[site]: datascience
[post_id]: 65191
[parent_id]: 64340
[tags]: 
Thinking in terms of optimisation, loss is convergence indicator NN can be expressed as an optimization problem . In the context of an optimization algorithm, the function used to evaluate a candidate solution (i.e. a set of weights) is referred to as the objective function. When we minimise this function we also refer to it as loss function Cross-entropy and mean squared error are the two main types of loss functions to use when training neural networks. Graph that you plotted indicates just convergance behavior on two different sets of our optimisation problem. If they diverge too much, we are overfitting. If they stop, we stopped learning.
