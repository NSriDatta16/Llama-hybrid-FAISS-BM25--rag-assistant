[site]: crossvalidated
[post_id]: 222288
[parent_id]: 93452
[tags]: 
With so many variables there's almost guaranteed multicollinearity. You're getting only $n$ variables at most as well (see If p > n, the lasso selects at most n variables ). What I'm currently doing with $p \gg n$ biomedical data is what's called Attribute Bagging [1]. It's the same idea of bagging, where you build classifiers on random subsets of your data and aggregate their predictions. You get a kind of importance measure latent to bagging heuristics by aggregating. Coincidentally, I'm using a linear model (L2-regularized L1-loss SVM to be precise) and it's giving me decent results on a regression problem I had not much hope. That way you can use LASSO and it's advantages and still get an almost completely safe feature importance metric. [1] Bryll, R., Gutierrez-Osuna, R., & Quek, F. (2003). Attribute bagging: improving accuracy of classifier ensembles by using random feature subsets. Pattern recognition, 36(6), 1291-1302.
