[site]: crossvalidated
[post_id]: 520561
[parent_id]: 520148
[tags]: 
There is no theoretical limit on the input length (ie number of tokens for a sentence in NLP) for transformers. However in practice, longer inputs will consume more memory. A slightly related question with more detailed answers: Why do attention models need to choose a maximum sentence length?
