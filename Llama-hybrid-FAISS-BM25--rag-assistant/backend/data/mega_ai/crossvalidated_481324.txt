[site]: crossvalidated
[post_id]: 481324
[parent_id]: 
[tags]: 
Meaning of the value matrix in self-attention

I'm trying to understand how the self-attention mechanism of the transformer architecture (as proposed by Vaswani et al. ) works in detail. I get that self-attention is attention from a token of a sequence to the tokens of the same sequence. The paper uses the concepts of query, key and value which is aparently derived from retrieval systems. I dont really understand the use of the value. I found this thread , but I don't really get the answer there either. So let's take an example. Let's say the input sequence is "This forum is awesome". Then to calculate the query vector, I linearly transform the current token (e.g. "This") with a matrix of weights W_Q that are learned during training. In reality, this is apparently bundled in a query matrix $Q$ for every token. I do the same with every token, just with the other matrix $W_K$ , where I get the key matrix. With the scaled dot product I calculate the similarity between my query $\mathrm{embedding}(\text{"This"})\cdot W_Q$ and keys $\mathrm{embedding}(\text{token}) \cdot W_K$ for each token and see which tokens are relevant for "This". ( Thanks in advance!
