[site]: crossvalidated
[post_id]: 435237
[parent_id]: 435230
[tags]: 
Say our data lives in $\mathbb R$ and we're using the kernel $$k(x, y) = 1 + 2 x y + x^2 y^2,$$ which corresponds to $$\phi(x) = \begin{bmatrix}1 \\ \sqrt 2 x \\ x^2\end{bmatrix}.$$ If we train a linear SVM on the one-dimensional $\phi(x)$ data, or a kernel SVM on the three-dimensional $x$ data, we'll get the same prediction rule for new data out. So, in this case, the kernel trick is "unnecessary." But say our data lives in $\mathbb R^d$ and we want to use the kernel $$k(x, y) = 1 + 2 x^T y + (x^T y)^2 = (x^T y + 1)^2.$$ Then the corresponding features end up being of dimension $\frac12 d^2 + \frac32 d + 1$ . This is suddenly a much larger model, that will take more computation to solve, more memory to store, etc. Even worse, say we want to use the kernel $$ k(x, y) = \exp\left( - \frac{1}{2 \sigma^2} \lVert x - y \rVert^2 \right) .$$ The $\phi$ features here end up being infinite -dimensional ( see here ), which means it will take an infinite amount of time and memory to compute with directly. But using the kernel trick, we can do it just fine. It can also be much easier to choose functions $k$ with certain properties than it is to design feature functions $\phi$ . (Kernels are no harder to design than feature functions – you can just write them as $\phi(x)^T \phi(y)$ directly, after all – but the opposite direction can be quite difficult.)
