[site]: crossvalidated
[post_id]: 232562
[parent_id]: 
[tags]: 
Should we account for the intercept term when kernelizing algorithms?

When a learning algorithm (e.g. classification, regression, clustering or dimension reduction) uses only the dot product between data points $\mathbf {x x^T}$ we can implicitly use a higher dimensional mapping $\phi(\mathbf x)$ through the kernel trick, exchanging every instance where the dot product occurs by the kernel $\mathbf K = \phi(\mathbf x) \phi(\mathbf x) ^ \mathbf T$. In linear models, SVMs for example, one can account for an intercept adding a constant column to data points. If we use the linear kernel $\mathbf K = \mathbf {x x^T}$ it makes a lot of sense to me to keep that constant column: you can retrieve the column coefficients $\mathbf w$ from the kernel product coefficients $\mathbf u$ through $\mathbf{w=x^T u}$ and the solutions should be identical, using the kernel or not. But what if the kernel is not linear, what if the mapping in infinite dimensional so the column coefficients are impossible to represent with $\mathbf{w=\phi(\mathbf x)^T u}$, does it still makes sense to include an intercept term?
