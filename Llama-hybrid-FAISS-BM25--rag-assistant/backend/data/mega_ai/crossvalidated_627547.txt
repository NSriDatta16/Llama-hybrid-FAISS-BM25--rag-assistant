[site]: crossvalidated
[post_id]: 627547
[parent_id]: 
[tags]: 
Neural Network Linear Activation Functions

I understand the intuition that the sum of linear functions is again linear, and that is why a neural network with linear activation functions yields a linear model. But what I'm confused about is that lets say we have two neurons outputting $w_1^{[1]}x+b_1$ and $w_2^{[1]}x+b_1$ , with just an identity activation function. Then if we feed this into the next layer which has just one neuron (and with an identity activation function), we get the function $w_{11}^{[2]}(w_1^{[1]}x+b_1) + w_{12}^{[2]}(w_2^{[1]}x+b_2).$ But then since our model looks like $w_{11}^{[2]}w_1^{[1]}x+w_{11}^{[2]}b_1 + w_{12}^{[2]}w_2^{[1]}x+w_{12}^{[2]}b_2,$ why do we not consider this a nonlinear regression, since the model is now nonlinear in the parameters $w$ ?
