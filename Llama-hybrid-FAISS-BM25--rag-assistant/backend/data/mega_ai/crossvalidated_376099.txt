[site]: crossvalidated
[post_id]: 376099
[parent_id]: 
[tags]: 
Defining "variance" of a partially defined random variable

Elsewhere within CrossValidated the following survey sampling problem was mention. To each member $i$ of a population $\{1,\ldots,N\}$ there is assigned some value $c_i$ whose average $\mu=(c_1+\cdots+c_N)/N$ is to be estimated. So $S_1,\ldots,S_N \sim \operatorname{\text{i.i.d.}} \operatorname{Bernoulli}(p),$ and if $S_i=1$ then $c_i$ is observed. The estimator $$ \widehat\mu = \frac{\sum_{i=1}^N c_i S_i}{Np} $$ was suggested. I said that I would instead use $$ \frac{\sum_{i=1}^N c_i S_i}{\sum_{i=1}^N S_i}, $$ my first thought being that it has a smaller variance. However, the probability that the numerator and denominator are both $0$ is positive (but the denominator cannot be $0$ unless the numerator is also $0,$ so there is no danger of seeing something like $6/0$ ). I had never before thought of defining such a thing as the expected value or variance of such a thing. Clearly one has a conditional variance given that the denominator is not $0.$ Can one with a clear statistical conscience define the "mean squared error" in such a context, by so conditioning? Has anyone see things like that done?
