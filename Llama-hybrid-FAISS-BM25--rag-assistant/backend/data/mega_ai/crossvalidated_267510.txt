[site]: crossvalidated
[post_id]: 267510
[parent_id]: 
[tags]: 
Clustering categorical features based on fit

I have a set of data. For our purpose lets simplify it to one independent numerical variable,$x$, and dependent numerical variable, $y$. The goal is to train on the data to determine the parameters in the model, for simplicity assume $y=mx+b$. I could then predict new $y$ values when new $x$ values are given. Pretty standard fitting. The tricky part is that I have another feature dimension in my data set which is categorical with many possible values. I could do one fit per unique categorical value or fit all values together but I have reason to believe it is best to do some clustering and do one fit per cluster. There is a clear trade-off since every cluster adds more parameters but reduces the data per fit. If we simplify to trying to find two Clusters. If I could hypothesize two clusters which would each fit to their own line. To restate, $y_1=m_1*x_1+b_1$ and $y_2=m_2*x_2+b_2$ where the data is split into two groups based on clustering the categorical variable might fit the data "better" than $y=m*x+b$ when all the data is fit together. The problem is that even if I knew the clusters I would not know what metric to use for "better". RMSE should always decrease for as the number of clusters increase. Similarly $R^2$ would always decrease because I am adding parameters so this would lead to overfitting. Should I use $\frac{\chi^2}{ndf}$? I feel like this is something that must be well understood and I am just missing something on how to balance the number of clusters/models and find them in the first place. A very similar question was asked and partially answered years ago Clustering as a means of splitting up data for logistic regression
