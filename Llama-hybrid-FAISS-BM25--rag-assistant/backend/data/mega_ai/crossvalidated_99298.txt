[site]: crossvalidated
[post_id]: 99298
[parent_id]: 99284
[tags]: 
I think the following argument shows why, unfortunately it's a messy. Much more elegant derivations are certainly out there somewhere as the linear Gaussian case is the best understood statistical model in existence. Anyway, we have that: U~N(0, sigma²) Q~N(alpha, beta^2) Y=Q+U. U and Q are independent. Because a linear function of normal random variables is itself a normal random variable and due to independence of U and Q it follows that Y | Q ~ N(Q+0,sigma²). We can now write down the probability density function of Q conditional on Y=y. By Bayes theorem that's: (pdf of Q times * pdf of Y|Q)/(pdf of Y). I won't write this out because it's very messy with all the Gaussian densities. Q|Y will be a normal random variable, which means that it's mode is its mean. Ignoring the denominator (the normalizing constant), we're left with: 1/(2pi*sigma^2*beta^2) * exp(-Something(Q)) We find the mode of the posterior distribution by choosing Q so as to maximise the density. That's going to be the conditional expected value too, because the mode is the mean for a Gaussian. To do that we can ignore everything except Something(Q) because the rest isn't a function of Q. If you do the algebra, Something(Q) = 1/2 *( (y-q)^2/sigma^2) + (q-alpha)^2/beta^2) If you differentiate wrt q, set to 0 and solve for q, you'll get: q=beta^2/(beta^2+sigma^2)*y+sigma^2/(beta^2+sigma^2)*alpha... as required!
