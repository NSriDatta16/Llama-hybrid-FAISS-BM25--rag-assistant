[site]: crossvalidated
[post_id]: 284834
[parent_id]: 284816
[tags]: 
According to the Bayesian theory, $f(\theta \mid x_1,\ldots,x_n) = \frac{f(x_1,\ldots,x_n|\theta) \cdot f(\theta)}{f(x_1,\ldots,x_n)}$ holds, that is $\text{posterior} = \frac{\text{likelihood} \cdot \text{prior}}{\textrm{evidence}}$ . Notice that the maximum likelihood estimate omits the prior beliefs(or defaults it to zero-mean Gaussian and count on it as the L2 regularization or weight decay) and treats the evidence as constant(when calculating the partial derivative with respect to $\theta$ ). It tries to maximize the likelihood by adjusting $\theta$ and just treating $f(\theta\mid x_1,\ldots ,x_n)$ equal to $f(x_1,\ldots,x_n\mid \theta)$ which we can easily get(usually the loss) and keep the likelihood as $\mathcal{L}(\theta\mid \mathbf x)$ . The true probability $\frac{f(x_1,\ldots,x_n|\theta) \ldots f(\theta)}{f(x_1,\ldots,x_n)}$ can hardly be worked out because the evidence(the denominator), $\int_{\theta} f(x_1, \ldots,x_n, \theta)d\theta$ , is intractable. Hope this helps.
