[site]: crossvalidated
[post_id]: 234316
[parent_id]: 234313
[tags]: 
Great question -- hopefully an easy solution (unless you have more complicating details to provide). Standardize your data! All you need to do is transform each column of the dataset with the formula... Zi = (xi - mu) / sigma where... xi is any individual datapoint, mu is the mean for the set, and sigma is either sample or population standard deviation (as appropriate), and Zi is the standardized point value What does this do? - Allows you to view the data in a unit-removed form where the center/mean/average/balance point of the data is 0 - Allows you to easily understand the distance from center, because the z-value denotes how many standard deviations the value is from the center - If you have multiple features/variables/columns of corresponding data, removes any confusion from unusual scales or units when looking at data (especially as data features compare to each other). For a set like this, standardizing will essentially expand that tiny range to make it much easier to differentiate values. Based on the samples you provided, I don't observe anything obviously complicating. In fact, you seem to have quite precise details. Perhaps you want to look into the sampling method and ask yourself if this many significant digits is reasonably accurate -- if not, there is an argument for rounding according to the precision of the data collection method. A couple of points: (1) Please keep in mind that the "range*" of a dataset (the difference between the max and min values) is not a good way to assess similarity. More relevant would be the distribution of values and the quantity of distinct values in your dataset. It is perfectly fine that numbers are close in numerical value, but you still have an interesting dataset with sufficient variability and the potential for some nicely statistically significant results in, say, a regression. If these were integer values, then we might have a problem -- and maybe even need to consider looking at the data as categorical or binary. (2) A large dataset is very rarely a downside. In most applications, the more the better! *Quotes to imply that I'm using the term in the strict statistical sense, rather than the colloquial, or common, usage. Best wishes with your analysis!
