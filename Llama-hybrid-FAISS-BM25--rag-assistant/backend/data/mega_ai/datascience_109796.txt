[site]: datascience
[post_id]: 109796
[parent_id]: 109790
[tags]: 
A normal transformer has two parts: encoder (non-autoregressive) and decoder (autoregressive). This allows it to generate text (i.e. sequences of tokens). Therefore the applications of the vanilla transformer are those receiving a piece of text as input and getting another piece of text as output. The main example is machine translation. BERT is a transformer encoder. Its applications are those tasks where the input is a piece of text (or N pieces of text) and the output is either: One single output (at the [CLS] token position). This includes any classification or regression task. One output per some/each of the input tokens. This mainly comprises token tagging tasks, e.g. part of speech tagging, span tagging (e.g. for question answering).
