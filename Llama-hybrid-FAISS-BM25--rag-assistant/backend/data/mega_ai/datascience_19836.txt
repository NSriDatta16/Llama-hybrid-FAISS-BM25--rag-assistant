[site]: datascience
[post_id]: 19836
[parent_id]: 
[tags]: 
When to use a Sigmoid function and when to use ReLu in a Convolutional Neural Network

Can someone please refer a good article explaining why we use a Sigmoid activation in the final layer of a neural network and a ReLu activation in the middle layers and input layer while building a Convolutional Neural Network? I am not getting how that results in the right output.
