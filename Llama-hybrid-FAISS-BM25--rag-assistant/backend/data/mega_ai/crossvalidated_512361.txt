[site]: crossvalidated
[post_id]: 512361
[parent_id]: 512306
[tags]: 
Speaking specifically to the case of the KDD99 data, I've found that the features that vary over many orders of magnitude (I remember one or three go between 0 and $10^8$ or similar, for instance) are best log-transformed and then z-scored. After the log-transformation, z-scores will give a nice scale that is amenable to gradient-based training as you'd find in a neural network. I found that without the log-transformed the data, SGD is not able to make progress, probably because the loss surface is poorly conditioned.
