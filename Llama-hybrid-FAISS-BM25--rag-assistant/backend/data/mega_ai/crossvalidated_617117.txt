[site]: crossvalidated
[post_id]: 617117
[parent_id]: 
[tags]: 
Learning rate decay in neural networks and inverse hessians in Newton's algorithm

I'm a newbie to the world of deep learning. I know that in practice what a lot of people do while training these models is to decay the learning rate as the number of iterations increases. In a similar fashion, when we are trying to minimize a function via Newtons Method we use the inverse of the second derivative as the "learning rate". Google searches on the connection between these concepts didn't yield much so I was wondering if they are related: By decaying the learning rate are we assuming (or giving?) the loss function a "stronger" curvature? Are we making our parameters more identifiable by doing this? Thanks
