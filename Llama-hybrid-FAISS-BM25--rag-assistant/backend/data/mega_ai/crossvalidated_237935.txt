[site]: crossvalidated
[post_id]: 237935
[parent_id]: 
[tags]: 
How do we apply derivatives to the vector version of canonical exponential-family log-likelihood?

Background (sorry, a bit long): The log-likelihood of observations from a (simplified) exponential-family distrib look like: $$l(y) = \sum{(y_i\theta_i - b(\theta_i))}$$ and suppose that the linear predictor is canonically linked to our model and that **we have just a single parameter $\beta$ ** - i.e. $\beta$ is a single parameter, not a vector : $$ \theta_i = x_i\beta $$ and then, since its known that $\frac{db(\theta_i)}{d\theta_i} = \mu_i$: $$ \frac{dl(y)}{d\beta} = \sum{(y_ix_i - y_i\mu_i)} $$ How do you do the same when $\beta$ is a vector of $p$ parameters? Princeton claim that then its a very elegant matrix equation: $$ \frac{dl(y)}{d\beta} = X'(y-\mu) $$ How do they do that??? (the reason its important is because that means that when the link is canonical then at $\beta_{MLE}$ the vector of means $\mu$ is the same as the vector of $y$.)
