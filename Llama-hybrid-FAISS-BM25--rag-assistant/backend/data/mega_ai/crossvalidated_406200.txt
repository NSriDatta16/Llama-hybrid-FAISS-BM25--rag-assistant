[site]: crossvalidated
[post_id]: 406200
[parent_id]: 406131
[tags]: 
question: The necessity of breaking correlation between consecutive samples comes from theory of reinforcement learning. Q-learning algorithm when used with function approximation is known to diverge quite easily, to stop that we need to break correlation between samples. Divergence of Q-learning algorithm with function approximation is not directly connected with neural networks it can happen even with linear approximator. This is quite large topic so if you want to know more you can read chapter 9 of this book (or you can read it all if you want to know more about RL). There is also part where neural networks learn better by providing samples that are not correlated. This was already ask and you can find few good answers here . question: Your understanding of how experience replay works is incorrect. Our agent plays the game "normally", that is sequentially. When you sample batch of experiences from experience replay you don't actually transfer your agent or your game state to the state that you sampled, you keep playing from the state where you were. We sample experiences from experience replay in order to use them for learning process. To make it more clear I will list the general steps that are used for learning from experience replay and playing the game. 1) We run some dummy moves in the environment to fill part of our experience replay buffer. For example you can run 1000 steps randomly in a game and store all the states you visited during that time in a buffer. We do that so that at the begging we have enough samples to sample from the buffer. 2) You start playing the game "for real", you make a single step in the environment, you store your transition in the buffer. 3) Now you sample a batch of transitions from experience replay, for example 32 of them and you use that to train the network. Your agent is still at the same state were it stopped at step 2) we don't move anywhere, we only sampled 32 previous transitions from the past to learn from them. 4) Now you start again from step 2), that is, we make a move in the environment, store the transition and again sample transitions to learn from like in step 3).
