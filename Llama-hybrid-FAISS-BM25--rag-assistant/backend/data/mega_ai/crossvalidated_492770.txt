[site]: crossvalidated
[post_id]: 492770
[parent_id]: 
[tags]: 
Local optima in high-dimensional optimization

I remember a theorem along the lines of In higher dimensional optimization problems, you are less likely to get stuck in local optima, because the more dimensions you have, the more likely you are to have saddle points rather than local optima I heard this in the context of gradient descent algorithms and training neural networks, if that helps. Now I am looking for a reference to read further on this. Any pointers for further literature/theorems/proofs would be greatly appreciated.
