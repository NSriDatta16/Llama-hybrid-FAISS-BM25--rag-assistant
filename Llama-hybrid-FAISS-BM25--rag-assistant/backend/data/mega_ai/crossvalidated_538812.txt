[site]: crossvalidated
[post_id]: 538812
[parent_id]: 
[tags]: 
What can we learn from binning before correlation?

I've seen a lot of questions about binning data and how it produces wrong results. "wrong" here refers to arbitrary increases in correlation strength, or sometimes even achieving two opposite correlation outcomes with the same data. To be clear, by binning I refer to binning along the x-axis in segments that contain the same row count. The y value of a segment then becomes the mean or median value of all y values of rows in it. My understanding is that binning eliminates the variance inside each bin. This always reduces the overall variance along the y axis, because extreme values are removed. This variance, however, is used in computing the correlation strength. I guess my question is what happens to the underlying relation between x and y axis, assuming it exists. Let's use a function f(x) = 0.01 x + rand(0,1) and sample it 10000 times in the 0-1 interval. Its scatterplot will look like a Leibniz cookie and its correlation will be very small. I measured it to be around 0.01 on average. To briefly check my assumptions, I built a little script that bins the above function. If the true correlation is 0.01, binning into 100 bins leads to an output around 0.10. With n increased to 100,000, output is about 0.30. In comparison, if correlation is 0, abs(output) is small, near 0.001. On the other hand, reducing the number of bins should lead to higher correlation. Let's say we have a great many samples of this function. Did I get it correctly that this will only help us to know the precise slope of a linear regression, and nothing else? Is there an instance where binning makes any sense here?
