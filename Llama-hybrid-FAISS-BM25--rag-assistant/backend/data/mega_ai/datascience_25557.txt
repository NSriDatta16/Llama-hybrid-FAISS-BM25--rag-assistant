[site]: datascience
[post_id]: 25557
[parent_id]: 
[tags]: 
Create the most simple/basic deep network where variable initialization does matter a lot

I'm trying to create the most basic and simple neural network to simulate a situation where the random initialization of the variables matters a lot. So if I run the network and the initialization over and over again the network will not converge well, unless the random initialization hit the sweet spot. So basically trying to come up with the most simple example of a non-convex cost function to simulate a situation of non-convergence.
