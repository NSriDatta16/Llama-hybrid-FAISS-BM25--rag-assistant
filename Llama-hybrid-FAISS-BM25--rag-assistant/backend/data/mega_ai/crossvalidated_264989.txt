[site]: crossvalidated
[post_id]: 264989
[parent_id]: 264859
[tags]: 
Since you can do only one action before the game ends, this looks like a k-armed bandit problem. The problem is that, unless you are using some kind of Q function approximator, like a neural network, the agent will not be able to understand from a certain reward how to "classify" (or, more properly here, what action to execute) similar digits from the input image/state, i.e. it doesn't learn how to generalize to different images of the same digit. So, since te reward for a (state,action) couple is deterministic, the agent should just try all possible combinations to really understand how to act. So, as I said, the agent in the end woukd just have a Q table that has as many entries as the state/actions combinations, so the number of images*10, with a single reward for each action relative to a state (so nine 0s and one 1 for each state). Since the final policy should be to choose the highest rewarding action from each state based on the Q table, I don't see how a bigger R could make a difference, but maybe it could be due to initialization factors. Try to explain in more detail your algorithm for a more accurate answer.
