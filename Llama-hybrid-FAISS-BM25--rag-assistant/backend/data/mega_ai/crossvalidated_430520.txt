[site]: crossvalidated
[post_id]: 430520
[parent_id]: 
[tags]: 
Gradient Descent: Taking the derivative of the raw error?

I'm reading this book called "Grokking Deep Learning" and there's this one part I can't help but be skeptical of: Here, the graph plots error squared against weight. The book however calculates the gradient of this graph by taking the derivative of delta (which is the raw error) with reference to the code. At first I thought since this is a beginner book that the author is to build up his point. But he uses the same way to find step_size in later parts as well. So is this just a different variant of the gradient descent that I know that differentiates the error squared? This is the gradient descent that I know of prior to reading this book: Any help on shedding some light on this will be deeply appreciated.
