[site]: crossvalidated
[post_id]: 21119
[parent_id]: 
[tags]: 
Advice on scientifically sound scale construction

I've been given a set of 20 Likert-items (ranging from 1-5, sample size n = 299) within the field of organizational research. The items are intended to measure a latent concept which is multidimensional, multifaceted and heterogenous in it's very nature. The goal is to create a scale(s) which can be nicely used to analyse different organizations and be used in logistic regression. Following the american association of psychology, a scale should be (1) unidimensional, (2) reliable and (3) valid. Therefore we decided to select four dimensions or subscales with 4/6/6/4 items each; which are hypothesized to represent the concept. The items werde constructed using the reflective approach (generating lots of possible items, and iteratively deleting items using cronbach's alpha and concept representation (validity) in three subsequent groups). Using the available data, a preliminary parallel explanatory factor analysis based on polychoric correlations and using varimax rotation revealed that items load on other factors than expected . There are at least 7 latent factors as opposed to four hypothesized ones. The average inter-item correlation is quite low (r=0.15) albeit positive. The cronbach-alpha coefficient is also very low (0.4-0.5) for each scale. I doubt that an Confirmatory factor analysis would yield a good model fit. If two dimensions were dropped, the cronbachs alpha would be acceptable (0.76,0.7 with 10 items per scale, which still could be made bigger by using the ordinal version of cronbachs alpha) but the scales themselves would still be multidimensional! As I'm new to statistics and lack the appropriate knowledge, I am at a loss on how to proceed further. As I am reluctant to discard the scale(s) completely and resign to a descriptive-only approach, I've got different questions: I) Is it wrong to use scales which are reliable, valid but not unidimensional? II) Would it be appropriate to interpret the concept afterwards as formative and use the vanishing tetrad test to assess model-specification and use partial least squares (PLS) to arrive at a possible solution? After all, the concept seems to be more a formative than a reflective one. III) Would using the item response models (Rasch, GRM etc) be of any use? As I've read, the rasch-models etc. also need the assumption unidimensionality IV) Would it be appropriate to use the 7 factors as new "subscales"? Just discard the old definition and use a new one based on factor loadings? I'd appreciate any thoughts on this one :) EDIT: Added factor loadings & correlations > fa.res$fa Factor Analysis using method = ml Call: fa.poly(x = fl.omit, nfactors = 7, rotate = "oblimin", fm = "ml") Factor loadings calculated from factor pattern matrix and factor intercorrelation matrix, only values above 0.2 are displayed
