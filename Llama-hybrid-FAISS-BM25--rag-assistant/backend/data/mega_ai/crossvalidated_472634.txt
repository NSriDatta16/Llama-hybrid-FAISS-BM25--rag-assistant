[site]: crossvalidated
[post_id]: 472634
[parent_id]: 472297
[tags]: 
Unlike ARMA and ARIMA, there are no assumptions required whatsoever when performing time series modeling with Neural Networks. This is because Neural Networks are universal approximators, and with a deep and wide enough architecture, they can approximate any arbitrary function. Moreover, they are not based on any statistical assumptions, and are instead "brute force/mechanical" non parametric function approximators, so considerations like heteroscedasticity, non-correlated errors, stationarity, etc...don't come into play the way they would for a regression model or for ARIMA. That being said, in practice, there is some evidence that performing some of the customary transformations, like stabilizing the variance with a power transform, detrending and/or deseasonalizing, improves the performance of various deep learning models on forecasting tasks. In my personal experience, just performing normalization (e.g. using the sklearn standardscaler or minmaxscaler) gives results that are just as good as a more complex preprocessing like a power transform, at least in the LSTM case. The time series in my use case were all short and had little or no trend at all to begin with, only seasonal patterns. But opinions and findings differ: Rob Hyndman's NNETAR model uses a power transform . Smyl, in the LSTM model that won the M4 competition , removed both the seasonality and the trend. Hansika Hewamalage, Christoph Bergmeir , Kasun Bandara say that it depends: Sometimes detrending is enough, sometime removing the seasonality is necessary as well. To reiterate, Neural Networks are universal function approximators that do not rely on any formally defined statistical/stochastic process, and hence none of the requirements of ARIMA are relevant in theory, but in practice, trying to enforce some of these requirements might give better results than just feeding the time series directly to the network. You mention: I presume that, if I am using the ANN model to forecast prediction intervals, statistically insignificant heteroscedasticity and statistically significant residual normality are requisites. Is this correct? No. As I said above, not at all. Moreover, one neural network forecasting model at least goes completely in the other direction and does full non-parametric density forecasts, that are based on the idea that we can't make any assumptions whatsoever on the statistical properties of our time series. In response to @Richard Harris' comment on stationarity being necessary, the below forecast for the Air Passengers time series was generated using an LSTM model, where the only preprocessing was normalizing via the variance and the mean of the data set ( $x^* = \frac{x-\mu_x}{\sigma_x})$ . Here is the code - using Tensorflow. Note that this won't run on later versions (TF 2.x) and would require you to downgrade to TF 1.12 or 1.13.
