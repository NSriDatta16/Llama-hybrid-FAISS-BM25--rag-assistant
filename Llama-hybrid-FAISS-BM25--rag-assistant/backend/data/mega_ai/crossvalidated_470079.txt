[site]: crossvalidated
[post_id]: 470079
[parent_id]: 470073
[tags]: 
You are right, if we knew the posterior probabilities, we wouldn't need the samples, since we would know the posterior distribution, and the whole point of MCMC is to learn the posterior distribution. We use MCMC to draw samples from the distributions in cases where we don't know the probabilities, or more precisely, we know them up to a constant . Bayes theorem is $$ p(\theta|X) = \frac{p(X|\theta)\,p(\theta)}{\int \, p(X|\theta)\,p(\theta) \, d\theta} $$ the problem is that computing the normalizing constant $\int \, p(X|\theta)\,p(\theta) \, d\theta$ in many cases is a hard computational problem. Hopefully, to find maximum of such distribution (maximum a posteriori estimation), or to sample from it, we just need unnormalized density $$ p(\theta|X) \propto p(X|\theta)\,p(\theta) $$ i.e. you just need to multiply likelihood by prior and do not need to take the integral.
