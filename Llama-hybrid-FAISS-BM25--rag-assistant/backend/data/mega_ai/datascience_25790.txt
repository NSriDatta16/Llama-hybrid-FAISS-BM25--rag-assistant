[site]: datascience
[post_id]: 25790
[parent_id]: 25789
[tags]: 
Someone correct me if I'm wrong, but the PCA process itself doesn't assume anything about the distribution of your data. The PCA algorithm is simple - find the direction of greatest variance in your data write down the direction of the vector pointing in that direction, and 'divide' the data along that direction by its variance in that direction, so the resulting variance in that direction is 1. This provides you with an eigenvector (direction) and associated eigenvalue (scale). repeat steps 1-2, potentially as many times as you have dimensions, but with the constraint that the next vector must be orthogonal (aka at a right angle) to all previous. The result will be an ordered list of orthogonal vectors (eigenvectors), and scales (eigenvalues). This set of vectors/values can be viewed as a summary of your data, particularly if all you care about is your data's variance. I think there is an implicit assumption that the orthogonality implies independence of the resulting vectors, and from what I understand that's true if the data is Gaussian but not necessarily true in general. So I suppose whether your data can be modeled as Gaussian may or may not matter, depending on your use case.
