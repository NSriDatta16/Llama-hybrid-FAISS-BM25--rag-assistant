[site]: crossvalidated
[post_id]: 434297
[parent_id]: 99558
[tags]: 
First, maybe you should investigate the degree of agreement between the three human annotators. Search this site for agreement-statistics . If the agreement is good, your two methods should give similar results. Otherwise, your method two should give some kind of average correlation, but you should investigate the reason for bad agreement. Also, plot your data, one plot for each human annotator. Maybe you could analyse with a linear mixed model, in R something like lme4::lmer(ann_score ~ model_score + (model_score | ann), data=your_data_frame)
