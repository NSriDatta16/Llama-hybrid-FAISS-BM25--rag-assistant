[site]: crossvalidated
[post_id]: 529908
[parent_id]: 313857
[tags]: 
You refer to two ways to formulate the anomaly detection problem in terms of a max-margin approach which were first treated in detail in the paper Estimating Support of a High-Dimensional Distribution . See also the book kernel method for pattern analysis for an accessible and detailed analysis. One motivated by measure theory, and the second by geometry. Let me start with the second (for me, the most intuitive one). You may say you want to find the smallest enclosing hypersphere $$ \min_{c, r} r^2 \\ \text{subject to } ||\phi(x_i) - c||^2 \leq r^2 $$ where $c$ is the centre of the smallest hypersphere containing the set of positive training samples. That is, the solution to the problem $$ \text{argmin}_c max || \phi(x_i) - c||^2 $$ And that gives you a valid solution. The advantage of the other formulation ( $\nu$ -SVM) is that the parameter $\nu$ has a nice interpretation which is useful in practice to find the best balance between true/false positives. Namely, it can be shown that $\nu$ places an upper bound on the fraction of outliers and a lower bound on the fraction of support vectors. The first formulation does not provide anything similar. Finally, imagine you know the probability distribution of your data $P(x)$ . Then the problem of anomaly detection would be that of finding the ( highest! ) threshold probability $\alpha$ such that all samples with $P(x) \leq \alpha$ are considered outliers. And this is precisely the sort of functions that they try to approximate, though expressed in the opposite manner: $w^T \phi(x) > \alpha$ . Proposition 3 in the aforementioned paper gives a connection of this idea to classification problems, which they exploit to prove properties on the solution and the parameter $\nu$ .
