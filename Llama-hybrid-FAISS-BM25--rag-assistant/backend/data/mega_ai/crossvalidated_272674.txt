[site]: crossvalidated
[post_id]: 272674
[parent_id]: 272482
[tags]: 
Instead of guessing the best encoding for the prediction, let the neural network do it for you! The way to do this, especially if you have a limited range of possible input characters, is start with one-hot encoding. I.e. A = 0000...0001, B = 0000...0010, etc. Next, let the first layer in your neural network be an embedding layer - this will project the one-hot encoding into a more dense vector representation that contains information about the different relationships between the characters. The next layers of your neural network will start with this dense vector representation and go from there. To understand better how this embedding process works, have a look at word2vec. This illustrates the process of creating embeddings for words in large texts. You can find more information here for example: https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/ Adding an embedding layer is particularly easy when using keras.
