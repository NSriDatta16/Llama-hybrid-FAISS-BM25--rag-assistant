[site]: crossvalidated
[post_id]: 600895
[parent_id]: 325729
[tags]: 
Sorry to revive a very old discussion but I've just come through this very interesting paper, and I also was not sure on how to interpret the loss function lp(xhw;yhw) : lp(xhw;yhw) is a cross-entropy loss function which quantifies the difference between a predicted vector Xhw and the Ground truth vector Yhw , both of dimension 65 .i.e. the number of pixels in a 8x8 grid plus the no-interest-point dustbin. The inderlying assumption in this paper is that there can NOT be more than 1 interest-point in each 8x8 grid. Ykw (Ground Truth) is therefore a 65-size vector with a unique non null element index y and all others 0's . Xhwy is therefore the y-th element (0 The cross-entropy loss function defined at h,w position between X and Y is composed of a unique non-null element log( predicted probablity of interest-point in bin y) * 1 , all the other 64 elements of Ykw and therefore the cross-entropy are 0 the Loss function Lp(X,Y) is juste the average of all cross-entropy loss functions over all 8x8 grids. This assumption seems to me an important limit.
