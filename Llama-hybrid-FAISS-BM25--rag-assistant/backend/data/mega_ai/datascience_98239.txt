[site]: datascience
[post_id]: 98239
[parent_id]: 98230
[tags]: 
Short answer: When updating the weights (or parameters) of your machine learning architecture, you move along the gradient of the loss function applied to the empirical data and the data that your model predicts. This gradient can (and hopefully will, but doesn't have to) decrease as the number of epochs increases, so training will go on just fine. Example. Consider one of the simplest "machine learning" problems: Given a set of points $$S=\{(x_1,y_1),(x_2,y_2),\dots, (x_N, y_N)\}\subset \mathbb R^2, N\in\mathbb N,$$ we want to find the best-fit line to these points, i.e. we want to find $m,b\in\mathbb R$ such that $$f_{m,b}:\mathbb R\to\mathbb R, f_{m,b}(x)=mx+b$$ minimizes the quadratic loss $$\mathcal L(m,b;S)=\sum_{k=1}^N (f_{m,b}(x_k)-y_k)^2.$$ Now, note that, for fixed $S$ , $\mathcal L$ is a convex function (actually I haven't checked this, let me know if I am mistaken here) and, as you can check as an exercise, if there exists a minimizer $(m^*,b^*)$ of $\mathcal L$ , then "gradient descent" will converge towards this minimizer (note that there is an unfortunate bug in my formulation which causes minimizers to not always exist: This bug occurs when the best fit would be a vertical line, which can't be expressed as $y=mx+b$ ). Note that the same is also true if you take for instance $g_{m,b}=\operatorname{Relu}(mx+b)$ , even though the gradient of both $\operatorname{Relu}$ and $mx+b$ does not have to converge to $0$ as we converge to the minimizer. More generally, under certain assumptions on how the loss function acts on the weights (see Theorem 2.2 here ), gradient descent will always converge to a minimizer, if it exists.
