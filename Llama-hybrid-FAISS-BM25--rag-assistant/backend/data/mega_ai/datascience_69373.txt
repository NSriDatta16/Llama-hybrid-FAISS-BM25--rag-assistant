[site]: datascience
[post_id]: 69373
[parent_id]: 69289
[tags]: 
Do I need to first convert the string column into some other data type? Yes, it's very important. Neural Networks don't take raw words and/or letters as inputs. Textual information must be processed numerically in order to be fed into the model. I thought about it, and came up with three things you could do: Use one-hot encoding , as it was suggested. I don't prefer this option: one-hot encoding generates very sparse matrices, meaning very little variation in almost everything dimension at each step. Moreover, the number of websites is absurdly high, it can be unmanageable through one-hot encoding. Additionally, it's not robust: your model have to be tested on unseen data, that by definition couldn't be operationalized with that technique. Also, some websites might be more or less similar to others, they play more or less similar roles for your model; but one-hot encodind fails at representing their differences: they will all look equally different if you compute any distance measure between them. Assuming you are working with sequences, my main suggestion is to represent web USLs with embedding vectors . Just as words can be translated into embedding vectors in NLP (it's the case of word2vec or glove), you can apply the same technique to represent the "relative meaning" of websites. In this way you can: a) keep the number of dimensions at bay, and b) have a non-sparse matrix that your models need. You would end up with a web2vec model, that fits perfectly with RNNs. Moreover, the meaning of new, unseen websites could be learned effortlessly by the model. This can be done with libraries such as gensim , or using Keras Embedding() layers. A more extreme, time consuming option is to use character embeddings . This is going to be useful only if you thing the name of the website itself contains the relevant information for your tasks. I'm not sure if it's the case. Character-level embeddings produce very sofisticated models, but they are more computationally expensive than the others. And I'm not sure they would be useful in your case. If I had to choose, I'd go for option 2. Finally, let me close with a hint: do not put Dense() layers before LSTM() layers. First, Recurrent layers process sequential data, then their output is sent to dense layers that execute the prediction. The output layer should have either one node + Sigmoid activation + Binary Crossentropy loss, or two nodes + Softmax activation + Categorical Crossentropy loss. As you prefer.
