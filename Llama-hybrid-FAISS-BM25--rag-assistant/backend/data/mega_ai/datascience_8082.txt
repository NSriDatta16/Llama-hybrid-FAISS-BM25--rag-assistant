[site]: datascience
[post_id]: 8082
[parent_id]: 
[tags]: 
Designing an (RL) agent for a graph-based music improvisation system

I am trying to implement a simple agent that creates sounds by building up signal building blocks (e.g. sound generators, filters etc.) that can generally be connected in the form of a directed acyclic graph, and each module has quasi-continuous parameter inputs (defined intervals of floating point numbers, e.g. for a frequency, an amplitude etc.). This agent should be able to operate in real-time along with me as a human agent. My idea is to use something like reinforced learning as it allows to operate in an exploratory mode where the state space is very large if not infinite, something that I understand is a disadvantage in a learning-by-example (supervised) mode. Do you consider RL applicable in this scenario? How would I go about defining the state space ? Is every module (generator or filter) a cell in that space? How about their connections and parameters? Should I introduce a partial ordering of the graph, or radically simplify the state? Perhaps the state should be a feature vector of the actually produced sound instead of the graph structure? I am thinking that I have to provide the rewards manually depending on whether the current result is musically meaningful or bad. Does that make sense? Is there any literature for this "human evaluated" RL? I know there are human fitness functions in genetic programming, so I am thinking this would be an analogous situation. I am worried about the convergence rate. In automatic learning, I can easily perform thousands of steps, but if I have to assign manual reward and I have to listen to the actual sound production, the agent can only act at a fraction of the speed.
