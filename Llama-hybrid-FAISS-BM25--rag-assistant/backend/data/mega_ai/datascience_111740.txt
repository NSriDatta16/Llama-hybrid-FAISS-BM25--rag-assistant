[site]: datascience
[post_id]: 111740
[parent_id]: 
[tags]: 
LSTM as learned positional encoding for vor variable sequence length input

I'm solving a classification task on a time-series dataset. I use a Transformer encoder with learned positional encoding in the form of a matrix of shape $\mathbb{R}^{seq \times embedding}$ . Naturally, this leads to the fact that the sequence length that the model can process becomes fixed. I had an idea to do learned positional encoding with LSTM. I.e., we project a sequence of tokens with a linear layer onto an embedding dimension, then feed the embeddings to LSTM layer and then add hidden states to the embedding. $x = MLP(x)$ $x = x + LSTM(x)$ Do you think this will have the right effect? Are there any things to consider?
