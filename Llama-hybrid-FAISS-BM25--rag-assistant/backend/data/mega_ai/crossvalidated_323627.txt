[site]: crossvalidated
[post_id]: 323627
[parent_id]: 323608
[tags]: 
I did not check the papers of Gilks and Berzuini and Doucet and Johansen but assume they are dealing with a state space (or hidden Markov) model that writes as $$f(x_{1:n},y_{1:n})=p(x_1)p(y_1|x_1)\prod_{t=2}^n p(x_t|x_{t-1})p(y_t|x_t)$$ in which case $$p(x_t'|y_{1:n},x'_{1:t-1},x_{t+1:n})=p(x_t'|y_t,x_{t-1}',x_{t+1})$$ is indeed true: the primary sequence $(x_t)$ is a Markov chain and the secondary sequence $(y_t)$ is made of independent draws given the primary sequence $(x_t)$. As for the puzzling nature of$$p(x_t'|y_t,x_{t-1}',x_{t+1})$$there may be two sources of confusion: one is a dependence on "future" values like $x_{t+1}$, but dependence and independence do not consider dynamics: if $x_{t+1}$ depends on $x_t$ then $x_{t}$ depends on $x_{t+1}$; another one is the apparent paradox of generating the value at time $t$ by conditioning on the next value at time $t+1$. The paradox is resolved by considering that the conditioning is on the value at time $t+1$ generated at the previous iteration, hence available.
