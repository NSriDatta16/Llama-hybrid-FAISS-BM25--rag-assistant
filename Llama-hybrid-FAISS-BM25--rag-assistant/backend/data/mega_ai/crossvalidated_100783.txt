[site]: crossvalidated
[post_id]: 100783
[parent_id]: 
[tags]: 
Memory consumption in NLP tasks

I am trying to apply a simple Naive Bayes or SVM (libSVM) algorithms to a large data set, which I've constructed as an .arff file. The number of features in my set is ~180k and there are ~6k examples. Also there are 8 classification classes. The data is of size ~3.2GB. I am working with Weka's Java API and Eclipse, I am increasing JVM's memory to the maximum, but I am always getting a heap space error. I am on a MacBook Pro, 2.3 GHz Intel Core i5, 4GB 1333 MHz DDR3. Do I need to find another machine to work with or is it possible that I am having an memory leaks programmatically?
