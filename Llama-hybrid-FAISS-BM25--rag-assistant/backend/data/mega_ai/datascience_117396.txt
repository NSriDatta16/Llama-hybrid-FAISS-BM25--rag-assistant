[site]: datascience
[post_id]: 117396
[parent_id]: 117395
[tags]: 
Your understanding is correct, both regarding the stochastic elements at training time (weight initialization, training data ordering, dropout) and that there are no inherent stochastic elements at inference time in the model itself . Now, assuming we are talking about either a full transformer with encoder and decoder (e.g. for machine translation, like the original formulation) or a mere transformer decoder (e.g. a causal language model like GPT-3): note that the model itself does not define the decoding strategy (i.e. how to generate tokens), so you can choose to use a deterministic strategy (e.g. greedy decoding, beam search) or to use a stochastic decoding strategy (e.g. normal sampling from the output multinomial distribution, nucleus sampling, top-k sampling). Also, there are some practical nuances to take into account, like some non-deterministic behaviors of specific implementations in CUDA (see this ).
