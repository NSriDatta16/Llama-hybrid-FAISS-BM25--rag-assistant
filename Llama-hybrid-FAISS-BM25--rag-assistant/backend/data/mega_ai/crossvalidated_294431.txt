[site]: crossvalidated
[post_id]: 294431
[parent_id]: 294323
[tags]: 
It makes perfect sense. RNNs make classifications based heavily on context. An RNN will build up an internal state that is an abstract representation of what it has seen so far, and use this to make a prediction. The hidden state is usually initialized to all 0s, and it has to build up to something meaningful, so the predictions towards the beginning of a sequence are much less accurate. This isn't a problem specific to RNNs or machine learning though. An intuitive example of this is trying to predict the next character in a text sequence. If all you are given is the letter "t", the next letter could be all kinds of things. Then you add the letter "h" so the sequence is "th" and the probability of a vowel coming next goes up. Add the letter "i" and now the probability of "s" coming next seems like a solid guess. Assuming it is, the sequence so far would be "this", and the probability of the next character being a space would by very high. This is how the accuracy builds as the sequence progresses. The accuracy increase should only be valid for the first few steps in the sequence though. It probably depends a lot on the task and data, but in my experience there was really no difference in accuracy after maybe 20 time steps. So chances are your short sequences just don't give enough context for an accurate prediction. Is it possible to improve? Probably, but I don't think there is a straightforward way to say what that is, especially without knowing what your data looks like and what your task is.
