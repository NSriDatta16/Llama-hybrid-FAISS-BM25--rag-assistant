[site]: crossvalidated
[post_id]: 587378
[parent_id]: 
[tags]: 
Interpretation/Intuition for L2 Regularization in Neural Networks

When we use L1-regularization in neural networks, it is pretty intuitive how the regularization will influence the learned weights. Namely, weights will not become needlessly large and unimportant weights will go to 0, resulting in a sparser matrix. I wonder if there is a similar intuitive intuition for L2 regularization. This could also be in terms of the linear maps that are described by the weights (maybe less extreme stretching of dimensions?). When using SVMs, we minimize the weights in the L2-norm to get the hyperplane classifier that maximizes the geometric margin. Is there a relation between L2-norm and margin in neural networks?
