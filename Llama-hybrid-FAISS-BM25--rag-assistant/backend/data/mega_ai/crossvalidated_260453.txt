[site]: crossvalidated
[post_id]: 260453
[parent_id]: 260436
[tags]: 
1) If your training loss is much lower than your test loss, then your model is likely overfitting. If training and test loss are similar, then your model is generalizing well. Accuracy on the held-out test set gives you an estimate of how well you can make predictions on unseen data. 2) Why do you believe that you're overfitting? Is it possible that this is just a very easy binary classification problem? 3) From your description, it sounds like your current neural network outputs a single unbounded continuous value when you actually want a probability $p \in [0,1]$. You could use a sigmoid function to do this.
