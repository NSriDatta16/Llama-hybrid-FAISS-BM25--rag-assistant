[site]: crossvalidated
[post_id]: 214777
[parent_id]: 214724
[tags]: 
Some additional comments on top of Bj√∂rn's answer: ABC was first introduced by Rubin (1984) as an explanation of the nature of Bayesian inference, rather than for computational purposes. In this paper he explained how the sampling distribution and the prior distribution interact to produce the posterior distribution. ABC is however primarily exploited for computational reasons. Population geneticists came up with the method on tree-based models where the likelihood of the observed sample was intractable. The MCMC (Data Augmentation) schemes that were available in such settings were awfully inefficient and so was importance sampling, even with a parameter of a single dimension... At its core, ABC is a substitute to Monte Carlo methods like MCMC or PMC when those are not available for all practical purposes. When they are available, ABC appears as a proxy that may be used to calibrate them if it runs faster. In a more modern perspective, I personally consider ABC as an approximate inference method rather than a computational technique. By building an approximate model, one can draw inference on the parameter of interest without necessarily relying on a precise model. While some degree of validation is necessary in this setting, it is not less valid than doing model averaging or non-parametrics. In fact, ABC can be seen as a special type of non-parametric Bayesian statistics. It can also be shown that (noisy) ABC is a perfectly well-defined Bayesian approach if one replaces the original model and data with a noisy one. As such it allows for all Bayesian inferences one can think of. Including testing. Our input to the debate about ABC and hypothesis testing is that the approximate model underlying ABC may end up as poorly equipped to assess the relevance of an hypothesis given the data, but not necessarily , which is just as well since most applications of ABC in population genetics are concerned with model choice. In an even more recent perspective, we can see ABC as a Bayesian version of indirect inference where the parameters of a statistical model are related with the moments of a pre-determined statistic. If this statistic is enough (or sufficient in the vernacular sense) to identify these parameters, ABC can be shown to converge to the true value of the parameters with the number of observations.
