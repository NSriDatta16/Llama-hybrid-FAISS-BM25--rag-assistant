[site]: datascience
[post_id]: 111228
[parent_id]: 111122
[tags]: 
Do you have the same results in the next epochs? If yes, your learning rate might be too high: Are you using an Adam optimizer? It could also happen with some other hyper parameters like: A too high dropout that resets too much your neural network. If it is set to 0.5 or more, you could try with a lower value like 0.1 or 0.2. A bad weigh initialization (use random or Xavier for good results for instance). In order to be more specific, I would need to read part of the code.
