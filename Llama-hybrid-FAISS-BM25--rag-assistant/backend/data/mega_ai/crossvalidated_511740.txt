[site]: crossvalidated
[post_id]: 511740
[parent_id]: 
[tags]: 
How to estimate probability of $\geq$ 1 success from a non-IID vector of probabilities, given many such vectors (now with asteroids)

I've got a deep neural net that returns sequences of probabilities. There are 25 probabilities per sequence. Many of these probabilities are zero, as a result of padding; when the input to the neural net is only 17 units long (for example), the output has 8 zeros appended to the end. These probabilities are not independent. The model is $$ f: [X_i, Z_{it}] \rightarrow \hat{Y}_{it} $$ which is to say that there are variables that vary over the sequence ( $Z$ ) but also variables that are common to the whole sequence ( $X$ ). Also, there is probably serial correlation in the $Z$ 's as well. The probability estimates themselves, the $\hat{Y}_{it}$ 's, are going to vary -- that's the whole point of the model. My main use involves those values of $\hat{Y}_{it}$ . But I also need estimates of the probability that any element of $\hat{Y}_{it}$ takes on a value of 1. The simplest way of doing this would simply be to add another output to the neural net, and directly predict it. But there are practical, prosaic reasons why this isn't ideal, so I'm looking for suggestions of statistical approaches to try. Another salient fact: I'll be generating millions of these vectors in the process of applying my fitted model to un-labeled data (and I'm interested in the properties of the estimator, rather than whatever true underlying distribution). This opens approaches to empirically estimating the nature of the dependence, and using that somehow. Here's an example to fix ideas: $i$ denotes planets; various civilizations in the galaxy $t$ denotes beings living in these civilizations $X$ are characteristics of planets $Z$ are characteristics of those beings on those planets $\hat{Y}_{it}$ denotes the probability that being $t$ on planet $i$ dies in an asteroid collision Case 1: nothing about a being $t$ can influence the probability that their planet will be hit by an asteroid (which varies by planet), and asteroids are large enough to kill everyone. They probability of death by asteroid is any element of $\hat{Y}$ . One minus the product of the complements of all the elements of $\hat{Y}$ is an overestimate. After all, the asteroid doesn't care how many beings live on the planet. Case 2: Asteroids are small and won't kill all beings on a planet. The probability of death by asteroid depends both on where you live on the planet, and what planet you're on. What is the probability that at least one being will be killed by an asteroid? It's more than the average entry of $\hat{Y}$ , and less than the one minus the product of the complements of $\hat{Y}$ . this is my actual problem Case 3: Asteroids are actually precise missiles, that generate no collateral damage. It doesn't matter what planet you live on; if they want you dead, they'll get you with probability $\hat{y}_{it}$ . This would be the case where the probability of anyone getting whacked is one minus the product of the complements. Here are some ideas that I've considered and rejected Ignore the lack of independence and just simulate draws from 25 independent binomials. This fails because some inputs are longer than others. Imagine that the $Z$ 's have no influence on the prediction, and only the $X$ 's do. Ceteris paribus, longer inputs will have higher probabilities, because there will be more binomials to try. Compare, for example, the following cases: $$ [.5, .5] $$ $$ [.5, .5, .5] $$ In the case that the $Z$ 's have no influence whatsoever on the elements, the probabilities would be $1-.5^2$ and $1-.5^3$ respectively, despite the fact that the elements of the vector are just copies of each other -- they are perfectly correlated . Poisson binomial distribution . This fails because it assumes independence, if not identical distribution. Independence is problematic for the reasons above. Also, poisson binomial is hard to compute directly. Gaussian copula . Stack all of the $\hat{Y}_{it}$ vectors into a matrix and compute the $25 \times 25$ correlation matrix $\Sigma$ . Simulate from it: $\mathcal{N}\left(\Phi^{-1}(\hat{Y}_{it}), \Sigma \right)$ , getting lots of probability vectors defined as the gaussian CDF of the output of the simulation. Simulate each of these probability vectors and average the number of times that I get one or more. This fails for the same reasons as the above: longer inputs (with fewer zeros) will still have more chances. Even though I estimate something related to the lack of independence, I still have non-independence biasing the quantity that I want to estimate. Use the determinant of $\Sigma$ somehow. Maybe multiply all of the estimated probabilities by the determinant of $\Sigma$ before simulating them? They will be smaller when the probabilities are more dependent. I imagine that the determinant would be proportional to the coefficient that I'd use to accomplish this purpose. The other problem here would be that I'd still be over-shrinking the sequences with fewer elements; they would all have the same determinant. Maybe if I multiply by the number of non-zero elements as well? This would be a weird hack, but it might point in the direction of a cromulent solution. Ultimately, the problem with all of these approaches is that non-time-varying elements worsen the dependence between the probabilities of sequence members. A good approach would somehow take into account the fact that the probabilities of each element of $\hat{Y}$ are not independent, and use that information to discount the non-independent component of the signal. I am open either to elegant math or weird hacks that have good face validity, so long as they can be implemented.
