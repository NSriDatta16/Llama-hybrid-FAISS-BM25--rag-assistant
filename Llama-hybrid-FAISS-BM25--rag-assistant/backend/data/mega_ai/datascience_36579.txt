[site]: datascience
[post_id]: 36579
[parent_id]: 
[tags]: 
How can I combine images for Matlab deep learning?

I have a data set which consist a number of instances (of two-three different classes), and for each instance I have a handful of monochrome images. This handful of images have different resolutions (e.g. one 50x200 image, one 600x10 image, and one 128x128 image), but every instance has the same number of images and the exact same resolutions (so, e.g., for every instance there is one 50x200 image, one 600x10 image and one 128x128 image). Now I would like to combine these images in some way, and use them to train a deep neural network. I know how to do this (in Matlab, using the Neural Network toolbox) on one image per instance at a time (e.g. only using the 50x200 images, ignoring the other images), but I have no idea how to proceed if I want to combine the images (which I hope will give the learning process more information, thus result in better classification performance). Of course I could simply concatenate them in some way, e.g. I could stack them horizontally and fill empty areas with black pixels (thus creating one (50+600+128)x(200+10+128) = 778x338 image per instance). But not only would this create very large images (in my example, 8 times as many pixels as the sum of the number of pixels in the original three images), but more importantly it seems to me it would make little sense to apply the standard deep network constructs of convolution and pooling over such an images. First of all, the features that need to be discovered are likely to be very different in the different original images (and thus different parts of the new, large image), and second, extracting features from the border areas, e.g. by combining some pixels from the right edge of the first image with some pixels from the left edge of the second image, seems like a weird an unnatural thing for the classifier to do. Alternatively, I could stack them in channels, thus creating one 600x200x3 image, where each channel constisted of one of the original images, padded with black pixels. This would probably make more sense in many ways, but still it seems slightly wasteful. So I wonder: Is there a better way to combine them, and if so, how can this be implemented using the Matlab Neural Network toolbox? I was thinking something like creating a sub-network for each image, then at some point, the outputs of these sub-networks could be combined by a final network (with convolution layers, or simply a fully connected one with just a couple of layers).
