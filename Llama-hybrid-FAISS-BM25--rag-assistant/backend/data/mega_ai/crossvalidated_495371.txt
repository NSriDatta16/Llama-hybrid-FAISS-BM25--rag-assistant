[site]: crossvalidated
[post_id]: 495371
[parent_id]: 493626
[tags]: 
You could try one of the smooth relaxations of ReLU, like softplus (albeit more often than not it's outperformed by ReLU, it's usually used for variance terms in VAEs, where zeros are not allowed): $$\operatorname{softplus}(x) = \log(1+\exp x)$$ See, however, What are the benefits of using ReLU over softplus as activation functions? and ReLU outperforming Softplus
