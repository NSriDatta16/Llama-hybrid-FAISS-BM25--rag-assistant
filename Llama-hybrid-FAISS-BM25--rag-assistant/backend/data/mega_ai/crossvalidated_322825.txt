[site]: crossvalidated
[post_id]: 322825
[parent_id]: 58491
[tags]: 
There is literature on Bayesian inference on over-identified models (e.g. Gelfand and Sahu, 1999. J. Amer. Statist. Assoc. 94:247-253), that is when the number of estimands in a model exceeds the number of (independent) observations. If priors are proper, the posterior is proper as well, but Bayesian learning on non-identified parameters depends on how much is learned about items that are identified. Hence, priors are influential, and this may be a serious matter with Bayesian models fitted, say, to DNA data, where the number of unknowns is in the dozens of millions. Caution should be exercised, e.g., in medical genetics. There is a concept called "effective number of parameters" or neff (see, for example, in the Deviance Information Criterion, or in regression models with shrinkage). In all cases, the neff is at most n. As in the good all times: the number of independent questions that one can ask from a data set is, at most, n. Hence if you pose n+k questions, k of the answers will b redundant with respect the first n answers. In short, statistical learning must be imperfect in overidentified models no matter how fancy you are in "regularizing" the model or how eloquent your local Bayesian resident expert is. Daniel Gianola
