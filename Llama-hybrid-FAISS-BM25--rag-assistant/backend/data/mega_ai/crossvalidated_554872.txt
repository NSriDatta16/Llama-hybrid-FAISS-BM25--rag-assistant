[site]: crossvalidated
[post_id]: 554872
[parent_id]: 554630
[tags]: 
It is not so clear to me how you want to split your data in your second approach. Maybe I can suggest the following: Uee k-fold cross validation for hyperparameter tuning. You take your data set and split 80-20 (90-10 or 80-30 would also work). The 80% you use for tuning using cross-validation. So you split your 80% in k many train-validation splits and try different parameters (Gridsearch would be one way). You take the average loss for these k validation sets for all different parameters and the setting with the lowest average loss value would be your wining set of parameters. Take your 20% (or whatever you decided on in step 1) and run your best parameters on this test set. This is the error you document as your final model loss. Because this is the closest to your expected value on how your model would generalise for a new unseen set of data. At least in the framework of empirical risk minimisation. By the way: in your final test error you hope to be somehow close to the one from your validation error. Otherwise you might have overfitted in the tuning process. If this is the case, you might need to start the tuning process again.
