[site]: crossvalidated
[post_id]: 154991
[parent_id]: 108381
[tags]: 
The recommended thing to do when using ReLUs is to clip the gradient, if the norm is above a certain threshold, during the SGD update (suggested by Mikolov, see http://arxiv.org/pdf/1211.5063.pdf ) This requires another hyperparameter, the threshold. The suggestion from the referenced paper is to sample some gradients to get an idea of the (non-exploding) norm and use the sample average. From my limited experience, it is worth playing around with this parameter a bit, even up to half the sample average. Pseudocode looks like, if norm(grad) > threshold: grad = grad * threshold/norm(grad)
