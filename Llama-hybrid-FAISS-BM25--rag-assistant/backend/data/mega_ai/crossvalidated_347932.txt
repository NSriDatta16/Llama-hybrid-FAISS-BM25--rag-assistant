[site]: crossvalidated
[post_id]: 347932
[parent_id]: 346680
[tags]: 
I think the main reason is that computer vision models tend to be much deeper than the ones commonly used in NLP. It's rare to have more than three or four layers for NLP tasks and oftentimes you can get by with just a single layer LSTM. Batch normalization helps train deeper networks but it is not as important for shallower ones.
