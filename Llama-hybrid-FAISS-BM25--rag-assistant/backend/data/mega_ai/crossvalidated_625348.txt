[site]: crossvalidated
[post_id]: 625348
[parent_id]: 
[tags]: 
Divide weight decay values by the learning rate values in a grid search?

I have come across a paper where the authors do a grid search over hyperparameters. In particular, they tested different learning rates and weight decays. One thing that caught my attention was that they mentioned in the paper that "...weight decay values are divided by the learning rate.". This seemed odd to me at first, but then I thought that one of the reasons why this could be done is that the weight decay term is multiplied by the learning rate during a gradient optimization step. So, by dividing the weight decay values by the learning rate we can make sure that the weight decay term in the optimization step will not depend on our learning rate. Does this make sense? Is this a standard practice? Thank you for your help.
