[site]: datascience
[post_id]: 33417
[parent_id]: 
[tags]: 
ConvNet exploding/vanishing loss

Problem I am using Fully Convolution Network (FCN), to classify whether an image is a random noise or not. However my FCN always predict with the means of targets. the output is not stale, but only changing a little (outputs always between 0.49 and 0.51). And after 7000 or more images the loss either suddenly vanish (FCN suddenly overfit, the outputs either 0 or 1, not in between) or explode (FCN suddenly only predict with 1) How I Train each batch consist of 8 images, each batch contains either all real or all fake. I only use 200 images for testing purpose, but the problem persist if I use all my data and increase/decrease batch size. What Works My FCN able to reach +95% accuracy to classify mnist-like problem where given images, the network try to classify it to 10 classes. What's different in botch case in my mnist-like problem, the images are controlled, same background, same brightness, same angle, meanwhile in current problem the images aren't controlled but still the same object. The input plane is different, in my previous problem 1 (gray), in current problem 3 (rgb). The output plane is different, previous problem 10, current 1. What I did I tried to throw away some layers, the vanishing/exploding loss still there but happen only after 2000 images instead of 7000 I tried to use noisy label, no vanishing/exploding loss after 14000 images. But the FCN predicts only using the means of the targets I tried to use tanh instead of logistic, the FCN predicts between -0.003 and 0.003 instead of 0.49 and 0.51 I tried using LeakyReLU and Average Pooling, same result I tried using SGD and Adadelta instead of adam, same result I tried using MSE instead Binary Cross Entropy, same result Mix real and fake images in one batch, same result
