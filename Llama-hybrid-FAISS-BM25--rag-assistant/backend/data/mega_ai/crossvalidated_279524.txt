[site]: crossvalidated
[post_id]: 279524
[parent_id]: 
[tags]: 
A mixed effect regression tool based on bayesian priors like bayesglm

I use extensively Gelman's bayesglm for the every day use due to the great stability of the estimates especially in the case of separation. Unfortunately I could not find an equivalent of empirical bayesian regularization for glm mixed effect models. These models are strongly influenced by extreme conditions (like conditional probabilities of zero and separation) and like usual logistic regression model they totally fail in these cases. Is there a way I can use bayesglm to consider clustered variance and random intercept? or is there a mixed model version of bayesglm? Thanks UPDATE: Thanks to @nate pope I found the blme package that does exactly what I need, solving the separation problem. Now the problem is to set it up in order to be work exactly as bayesglm, in order to achieve consistency in my analysis. Reading Gelman paper on bayesglm() I understood I should use a t distribution with 1 df (eg. Cauchy) and 2.5 scale, rescaling inputs: bglmer(Out ~ arm::rescale(Pred) + (1 | PatientID), family = binomial, Data.events, fixef.prior = t(df = 1, scale = 2.5)) Is it correct? My doubt is what to do with the cov.prior parameter; should I leave it as default (wishart) or should I put it to NULL? Also in Gelman paper it is said that intercept should have the same prior but with scale 10, but I don't know how to specify a different prior for it. Finally, bayesglm() provides the unscaled estimates, even if it works with rescale to compute them. How do I manually unscale them? UPDATE2: Making experiments and reading bayesglm.fit code I'm starting to think that bayesglm doesn't scale the inputs but scale the prior distributions according to them, am I right?
