[site]: crossvalidated
[post_id]: 399863
[parent_id]: 399861
[tags]: 
Variational autoencoders are unsupervised learning methods in the sense that they don't require labels in addition to the data inputs. All that is required for VAE is to define an appropriate likelihood function for your data. Commonly, a reconstruction loss is used, e.g. binary cross-entropy, squared distance, or absolute distance, to compare the actual input to the VAE with the generated output of the VAE. If you want to use VAE for dimensionality reduction or feature extraction, you would feed our data to the encoder network and use its output for downstream tasks. Keep in mind that VAE is a generative model, thus training encourages the output of the encoder to resemble an isotropic Gaussian distribution. If you don't need to generate data and just want to extract features, you might get better results with a traditional autoencoder without the Kullback-Leibler divergence regularizer that is used in VAE.
