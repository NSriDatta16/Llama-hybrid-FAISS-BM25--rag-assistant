[site]: crossvalidated
[post_id]: 471145
[parent_id]: 471069
[tags]: 
A more formal version of Dimitris' answer (+1) is as follows. The standard set-up for logistic regression is we have independent observations $Y_1,\dots,Y_n$ with $$ Y_i\sim\text{Bern}(\theta_i). $$ We then model $g(\theta_i) = x_i^T\beta$ where $x_i\in\mathbb R^p$ is our known covariate vector and $\beta\in\mathbb R^p$ is the parameter to be estimated. $g : (0,1)\to\mathbb R$ is the link function that connects the mean $\theta_i$ to the linear predictor $x_i^T\beta$ . Vectorizing our functions appropriately, we can write this as $$ g(\theta) = X\beta $$ where $X\in\mathbb R^{n\times p}$ has the $x_i$ as rows. $\beta$ can then be found by maximum likelihood and the standard way to do this is via iterative reweighted least squares (I've written more about this in my answer here ). This is a fixed effects logistic regression. A mixed-effects logistic regression would be when we have $$ g(\theta) = X\beta + Z\gamma $$ with $\gamma\sim\mathcal N_m(\mathbf 0, \Omega)$ . A simple example would be if we wanted to do a random intercept, then we could parameterize this as $\gamma\sim\mathcal N_m(\mathbf 0, \sigma^2_\gamma I)$ and $Z$ 's rows are indicators for which group each observation belongs to. The usual way to estimate this model is to first integrate $\gamma$ out and then maximize the resulting marginal likelihood over the remaining parameters. When we're using a Gaussian likelihood we have a conjugate prior so we get a closed form for the likelihood. Here that is not the case so the standard solution (e.g. what glmer in the lme4 package in R does) is to evaluate this marginal likelihood numerically; just as an example of what can be used to do this, in the documentation of glmer in lme4 the authors write "[t]he most reliable approximation for GLMMs is adaptive Gauss-Hermite quadrature." At this point some people advocate for abandoning maximum likelihood and just going fully Bayesian.
