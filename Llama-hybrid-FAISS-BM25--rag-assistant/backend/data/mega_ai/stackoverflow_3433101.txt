[site]: stackoverflow
[post_id]: 3433101
[parent_id]: 3433013
[tags]: 
Certainly the above 'test' is overkill in many respects. It is much too long and complicated, hardly readable, and asserts way too much things. I can hardly imagine how this could have been emerged from a TDD process. It is not surprising that you get tired of stuff like this... Test-driven development means: You should go in baby steps, where every step is a separate test, asserts only one thing, and contains absolutely no logic (i.e. no for , if/else or similar...). So the above code would result in about 4-6 separate test methods, which you would then implement one by one. First assert correct property initalization (with different values as required), then make sure that the methods work as expected, and so on... The code coverage metric does not tell you anything about your tests except that it can show you the production code which is not touched by any tests at all. Especially it doesn't tell you if the touched code really is tested (and not only touched...). That depends only on the quality of your tests. So don't take code coverage too serious, there are many cases where a lower coverage with better tests is much more preferrable... In sum: It is not overkill to have tests for just about everything (100% coverage), but it certainly is a problem to have tests like in your example. I recommend you reviewing your TDD/unit testing practice, The Art Of Unit Testing book might be a good resource... HTH! Thomas
