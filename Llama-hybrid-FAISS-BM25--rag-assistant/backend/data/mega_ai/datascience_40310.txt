[site]: datascience
[post_id]: 40310
[parent_id]: 
[tags]: 
How to derive the sum-of squares error function formula?

I'm attending a Machine Learning course and I'm studying linear models for classification right now. Slides present approaches to learn linear discriminants (Least squares, Fisher's linear discriminant, Perceptron and SVM), more specifically, how to compute the weight matrix $\tilde{\textbf{W}}$ to determine the discriminant function: \begin{equation}y = \tilde{\textbf{W}}^T \tilde{\textbf{x}} + w_0. \end{equation} My problem is about least squares: I don't understand how the minimization of sum-of-squares error function: \begin{equation}E(\tilde{\textbf{W}}) = \frac{1}{2} Tr\Bigl\{(\tilde{\textbf{X}}\tilde{\textbf{W}} - \textbf{T})^T(\tilde{\textbf{X}}\tilde{\textbf{W}} - \textbf{T})\Bigr\} \end{equation} (where $Tr$ is the trace). is derived and how it is possible to reach the closed formula solution: \begin{equation}\tilde{\textbf{W}} = (\tilde{\textbf{X}}^T\tilde{\textbf{X}})^{-1}\tilde{\textbf{X}}^T\textbf{T}\end{equation} Can someone explain me the main steps in the simplest and clearest possible way to make sense of these formulas? I'm a beginner. P.S. These formulas come from C.Bishop. Pattern Recognition and Machine Learning.
