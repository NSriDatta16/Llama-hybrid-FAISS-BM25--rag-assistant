[site]: crossvalidated
[post_id]: 218464
[parent_id]: 
[tags]: 
Is it possible for the forward pass computed by a wrapper to not agree with a hard coded using MatConvNet?

I was implementing a super duper simple network and I wasn't able to have the forward pass match for both the wrapper and the hardcoded version of the network: clc;clear;clc;clear; %% prepare Data M = 3; x = zeros(1,1,1,M); % (1 1 1 2) = (1 1 1 M) for m=1:M, x(:,:,:,m) = m; end Y = 5; r=Y; %% parameters L1 = 3; w1 = randn(1,1,1,L1); % (1 1 1 L1) = (1 1 1 3) b1 = ones(1,L1); w2 = randn(1,1,1,L1); % (1 1 1 L1) = (1 1 1 3) b2 = ones(1,L1); G1 = ones(1,1,1,L1); % (1 1 1 3) = (1 1 1 L1) BN scale, one per dimension B1 = zeros(1,1,1,L1); % (1 1 1 3) = (1 1 1 L1) BN shift, one per dimension EPS = 1e-20; %% Forward Pass z1 = vl_nnconv(x,w1,b1); % (1 1 3 2) = (1 1 L1 M) bn1 = vl_nnbnorm(z1,G1,B1,'EPSILON',EPS); % (1 1 3 2) = (1 1 L1 M) a1 = vl_nnrelu(bn1); % (1 1 3 2) = (1 1 L1 M) z2 = vl_nnconv(a1,w2,b2); y1 = vl_nnpdist(z2, 0, 1); loss_forward = l2LossForward(y1,Y); %% net.layers = {} ; net.layers{end+1} = struct('type', 'conv', ... 'name', 'conv1', ... 'weights', {{w1, b1}}, ... 'pad', 0) ; net.layers{end+1} = struct('type', 'bnorm', ... 'weights', {{G1, B1}}, ... 'EPSILON', EPS, ... 'learningRate', [1 1 0.05], ... 'weightDecay', [0 0]) ; net.layers{end+1} = struct('type', 'relu', ... 'name', 'relu1' ) ; net.layers{end+1} = struct('type', 'conv', ... 'name', 'conv2', ... 'weights', {{w2, b2}}, ... 'pad', 0) ; net.layers{end+1} = struct('type', 'pdist', ... 'name', 'averageing1', ... 'class', 0, ... 'p', 1) ; fwfun = @l2LossForward; bwfun = @l2LossBackward; net = addCustomLossLayer(net, fwfun, bwfun) ; net.layers{end}.class = Y; net = vl_simplenn_tidy(net) ; res = vl_simplenn(net, x); %% loss_forward = squeeze( loss_forward ) % (1 1) loss_res = squeeze( res(end).x ) % (1 1) Does someone know why this might be? It seems very mysterious to me. It seems it only happens when the Batch Normalization (BN) layer is included. Is there an additional randomization that I am not aware of that is used in the BN layer? check it out yourself: clc;clear;clc;clear; %% prepare Data M = 3; x = zeros(1,1,1,M); % (1 1 1 2) = (1 1 1 M) for m=1:M, x(:,:,:,m) = m; end Y = 5; r=Y; %% parameters L1 = 3; w1 = randn(1,1,1,L1); % (1 1 1 L1) = (1 1 1 3) b1 = ones(1,L1); w2 = randn(1,1,1,L1); % (1 1 1 L1) = (1 1 1 3) b2 = ones(1,L1); G1 = ones(1,1,1,L1); % (1 1 1 3) = (1 1 1 L1) BN scale, one per dimension B1 = zeros(1,1,1,L1); % (1 1 1 3) = (1 1 1 L1) BN shift, one per dimension EPS = 1e-20; %% Forward Pass z1 = vl_nnconv(x,w1,b1); % (1 1 3 2) = (1 1 L1 M) bn1 = z1; %bn1 = vl_nnbnorm(z1,G1,B1,'EPSILON',EPS); % (1 1 3 2) = (1 1 L1 M) a1 = vl_nnrelu(bn1); % (1 1 3 2) = (1 1 L1 M) z2 = vl_nnconv(a1,w2,b2); y1 = vl_nnpdist(z2, 0, 1); loss_forward = l2LossForward(y1,Y); %% net.layers = {} ; net.layers{end+1} = struct('type', 'conv', ... 'name', 'conv1', ... 'weights', {{w1, b1}}, ... 'pad', 0) ; % net.layers{end+1} = struct('type', 'bnorm', ... % 'weights', {{G1, B1}}, ... % 'EPSILON', EPS, ... % 'learningRate', [1 1 0.05], ... % 'weightDecay', [0 0]) ; net.layers{end+1} = struct('type', 'relu', ... 'name', 'relu1' ) ; net.layers{end+1} = struct('type', 'conv', ... 'name', 'conv2', ... 'weights', {{w2, b2}}, ... 'pad', 0) ; net.layers{end+1} = struct('type', 'pdist', ... 'name', 'averageing1', ... 'class', 0, ... 'p', 1) ; fwfun = @l2LossForward; bwfun = @l2LossBackward; net = addCustomLossLayer(net, fwfun, bwfun) ; net.layers{end}.class = Y; net = vl_simplenn_tidy(net) ; res = vl_simplenn(net, x); %% loss_forward = squeeze( loss_forward ) % (1 1) loss_res = squeeze( res(end).x ) % (1 1)
