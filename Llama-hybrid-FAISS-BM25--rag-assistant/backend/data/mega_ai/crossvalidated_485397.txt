[site]: crossvalidated
[post_id]: 485397
[parent_id]: 
[tags]: 
Variational Encoder on typical PCA task

I have just started to learn about variational autoencoders. As a first step I have tried doing a task with both PCA and VA. However, the VA results are very poor. Does anybody see any quick fixes that can be made to the VA code in order for VA to give more accurate results? Am I for example missing completely on the neuron numbers, hidden layer numbers or the activation functions? The code is heavily based on chapter 17 in Geron's notebook: https://github.com/ageron/handson-ml2 . I'va basically just changed the activation functions to reu and linear. edit1: Added examples of what I think might be wrong/bad. Modules # Python ≥3.5 is required import sys assert sys.version_info >= (3, 5) # Scikit-Learn ≥0.20 is required import sklearn assert sklearn.__version__ >= "0.20" try: # %tensorflow_version only exists in Colab. %tensorflow_version 2.x IS_COLAB = True except Exception: IS_COLAB = False # TensorFlow ≥2.0 is required import tensorflow as tf from tensorflow import keras assert tf.__version__ >= "2.0" if not tf.config.list_physical_devices('GPU'): print("No GPU was detected. LSTMs and CNNs can be very slow without a GPU.") if IS_COLAB: print("Go to Runtime > Change runtime and select a GPU hardware accelerator.") # Common imports import numpy as np import os from pathlib import Path # to make this notebook's output stable across runs np.random.seed(42) tf.random.set_seed(42) Data np.random.seed(4) def generate_3d_data(m, w1=0.1, w2=0.3, noise=0.1): angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5 data = np.empty((m, 3)) data[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2 data[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2 data[:, 2] = data[:, 0] * w1 + data[:, 1] * w2 + noise * np.random.randn(m) #data = data - data.mean(axis=0, keepdims=0) return data X_train = generate_3d_data(60) X_train = X_train - X_train.mean(axis=0, keepdims=0) PCA np.random.seed(42) tf.random.set_seed(42) encoder = keras.models.Sequential([keras.layers.Dense(2, input_shape=[3])]) decoder = keras.models.Sequential([keras.layers.Dense(3, input_shape=[2])]) autoencoder = keras.models.Sequential([encoder, decoder]) autoencoder.compile(loss="mse", optimizer=keras.optimizers.SGD(lr=1.5)) history = autoencoder.fit(X_train, X_train, epochs=20) codings = encoder.predict(X_train) X_trainDecoded = decoder.predict(codings) # Print np.set_printoptions(formatter={'float': lambda x: "{0:0.0f}".format(x)}) print('\nPercentage difference decoded and original data:') print((X_trainDecoded/X_train-1)*100) print("\nAverage percentage difference: ", np.mean((X_trainDecoded/X_train-1)*100)) PCA output Epoch 20/20 60/60 [==============================] - 0s 69us/sample - loss: 0.0054 Average percentage difference: 17.894847248764663 VA K = keras.backend class Sampling(keras.layers.Layer): def call(self, inputs): mean, log_var = inputs return K.random_normal(tf.shape(log_var)) * K.exp(log_var / 2) + mean tf.random.set_seed(42) np.random.seed(42) # The new dimension (lower than dimension of input data) codings_size = 2 # Layers before the stochastic part inputs = keras.layers.Input(shape=[X_train[0].shape[0]]) z = keras.layers.Flatten()(inputs) z = keras.layers.Dense(150, activation="relu")(z) z = keras.layers.Dense(100, activation="relu")(z) # Stochastic part codings_mean = keras.layers.Dense(codings_size)(z) codings_log_var = keras.layers.Dense(codings_size)(z) # Sampling codings = Sampling()([codings_mean, codings_log_var]) # Encoder set as an own model: Inputs and outputs specified. variational_encoder = keras.models.Model( inputs=[inputs], outputs=[codings_mean, codings_log_var, codings]) # Decoding of the stochastic outout decoder_inputs = keras.layers.Input(shape=[codings_size]) x = keras.layers.Dense(100, activation="relu")(decoder_inputs) x = keras.layers.Dense(150, activation="relu")(x) x = keras.layers.Dense(X_train[0].shape[0] * 1, activation="linear")(x) outputs = keras.layers.Reshape([X_train[0].shape[0]])(x) variational_decoder = keras.models.Model(inputs=[decoder_inputs], outputs=[outputs]) # Merge encoder and decoder models _, _, codings = variational_encoder(inputs) reconstructions = variational_decoder(codings) # Full model = encoder + reconstruction variational_ae = keras.models.Model(inputs=[inputs], outputs=[reconstructions]) # Loss function latent_loss = -0.5 * K.sum( 1 + codings_log_var - K.exp(codings_log_var) - K.square(codings_mean), axis=-1) # Add loss function to full model variational_ae.add_loss(K.mean(latent_loss) / X_train[0].shape[0]) # Loss and optimzer variational_ae.compile(loss="mean_squared_error", optimizer="rmsprop") # Fit history = variational_ae.fit(X_train, X_train, epochs=50, batch_size=128, validation_split=0.1 ) # Reconstruction tf.random.set_seed(42) reconstructions = variational_ae.predict(X_train) print((reconstructions/X_train-1)*100) print('\nAverage percentage difference: ', np.mean((reconstructions/X_train-1)*100)) VA output Average percentage difference: 97.2531197324568
