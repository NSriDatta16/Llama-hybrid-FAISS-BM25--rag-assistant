[site]: datascience
[post_id]: 25759
[parent_id]: 25754
[tags]: 
In a normal neural network, each neuron has its own weight. This is not correct. Every connection between neurons has its own weight. In a fully connected network each neuron will be associated with many different weights. If there are n0 inputs (i.e. n0 neurons in the previous layer) to a layer with n1 neurons in a fully connected network, that layer will have n0*n1 weights, not counting any bias term. You should be able to see this clearly in this diagram of a fully connected network from CS231n . Every edge you see represents a different trainable weight: Convolutional layers are different in that they have a fixed number of weights governed by the choice of filter size and number of filters, but independent of the input size. Each filter has a separate weight in each position of its shape. So if you use two 3x3x3 filters then you will have 54 weights, again not counting bias. This is illustrated in a second diagram from CS231n : The filter weights absolutely must be updated in backpropagation, since this is how they learn to recognize features of the input. If you read the section titled "Visualizing Neural Networks" here you will see how layers of a CNN learn more and more complex features of the input image as you got deeper in the network. These are all learned by adjusting the filter weights through backpropagation.
