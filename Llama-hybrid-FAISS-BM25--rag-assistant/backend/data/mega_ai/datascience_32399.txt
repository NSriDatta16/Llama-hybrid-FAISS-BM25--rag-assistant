[site]: datascience
[post_id]: 32399
[parent_id]: 
[tags]: 
Why validation loss worsens while precision/recall continue to improve?

I'm training a neural network on 'easy' dataset with ~15k examples. Network overfits pretty fast. The thing I cannot understand that after 5th epoch validation loss is starting to worsen, while precision and recall are continue to improve for 10 more epochs. (loss = binary cross-entropy) Diving deeper: I've checked if there is a lot of predictions around probability ~0.5, but it is not: Also, there is a plot of percent of correct predictions based on prediction probability. There is some pattern here, but the number of elements is quite small to make conclusions. So, my question is: why can it happen, and what to do about it?
