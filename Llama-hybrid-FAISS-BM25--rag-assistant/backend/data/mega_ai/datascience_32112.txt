[site]: datascience
[post_id]: 32112
[parent_id]: 
[tags]: 
Defining State Representation in Deep Q-Learning

So I am having difficulty difficulty figuring out exactly how I want to represent my environment state in my Deep Q-learning problem. Premise : There is a 2D grid space of which an agent needs to visit all the cells (because it is searching the space), once a cell is visited it is considered "searched" and therefore does not need to be visited again. First Take : I thought well what if the state representation of the environment to the agent was just the 8 adjacent cells and their searched/unsearched status. In this case the resulting actions would be to move to one of the adjecent cells. Here we have 2^8 different possible states and then 8 actions for our Q(s,a) function. Since I am using a DQN structure to the problem, the Q function would be approximated by a neural network with 8 inputs (status of adjacent cells) and 8 outputs (actions). Obviously the reward function would be simple as it yields a high reward for moving to a cell that has not been searched and a low reward otherwise. However, this presents an obvious problem when all the adjecent cells have been searched. The agent would move randomly and possibly away from cells that need to be searched. This is not good for my purposes. Second Take : I thought if I wanted to work around this I would need to incorporate information about the entire state-space into my Q function. Meaning that the agent knows the status of all cells and has actions that are basically "waypoints" that bring the agent to that cell. This would make my inputs and outputs to the Q function A LOT larger as I have a really big state-space. I don't have a specific size yet, but it could easily be 10,000 (100x100 grid) or larger. And obviously my reward function would need to reward the agent more for actions that involve less movement. Also, my problem can be formulated in a simulation to train the Q function, so I'm not worried about training time due to physical constraints. So basically my question is this: Can this Q function still be approximated by a neural network of reasonable size? Yes, I have a lot of processing power at my disposal (sorry I know this is all very vague), but I'd rather not blow things out of proportion. tldr: I have Deep Q-Learning problem where I think I need a large state-action space, and am wondering if it is too large to be reasonably approximated by a neural network. Edit : As @Constantinos noted, I didn't really define my problem well enough, so here are more assumptions and definitions below. State-Space: The grid itself is not exactly the state-space, each cell in the grid has a "searched" or "unsearched" status so the total number of unique states that my state-space can take on is really along the lines of 2^10,000 (assuming 100x100 grid). Additionally, the state-space also contains information about the location of the agent. Action-Space: At any given position, the agent may move to any other location on the grid. Therefor the action space is simply the 10,000 moves available. Reward Function: Positive reward for choosing action that brings agent to unsearched location. Negative reward for choosing action that brings agent to location that is far away (to constrain movement). High positive reward for choosing action that brings the agent to a goal state (i.e. where all locations have been searched). I/O of Q-Network: The input will be the a vector of elements (each representing a cell on the grid, can also be seen as an 'image feature' where the image is the grid) that take on values 0 or 1 (unsearched or searched). The input will also have to include some embedded information on the agents location (I am currently thinking of just altering the value of the input element that represents the agents location cell to be -1). Output of the network will be a vector of action utility values, each representing the utility of moving to a certain grid location. Lastly, I should note that once an agent visits a location, that location is now considered "searched" and the state of the environment is updated accordingly.
