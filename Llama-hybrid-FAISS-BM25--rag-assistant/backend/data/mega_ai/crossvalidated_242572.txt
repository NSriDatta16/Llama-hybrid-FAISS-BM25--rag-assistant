[site]: crossvalidated
[post_id]: 242572
[parent_id]: 242550
[tags]: 
As mentioned in another answer, the recent Deep Learning Review in Nature has some good discussion of this. I am not in this field, so cannot say how much this is a balanced review vs. just the authors' perspective. However one of the authors, Yann LeCun , seems to be widely acknowledged as essentially the "father of convolutional neural networks" (a.k.a. ConvNets, CNNs). So the Nature review is a reasonable source for this topic, at the very least. Given this, the reference chosen as the "origin of deep learning in ConvNets" there is essentially this 1990 NIPS paper: LeCun et al. (1990) Handwritten digit recognition with a back-propagation network . In Advances in neural information processing systems. The review mentions some earlier work on convolution-type layers, but notes that: This is the first paper on convolutional networks trained by backpropagation for the task of classifying low-resolution images of handwritten digits. The terms I have bolded lead me to conclude that this paper is a reasonable candidate for the "academic paper that introduced CNNs in deep learning" (i.e. BackProp would seem to be an essential component for the "deep" aspect). However it is worth noting that in the Nature review the authors mention that the approach was significantly inspired by Fukushima (1980) Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position . Biological cybernetics. Finally, as noted in the comments to the OP, Wikipedia has a summary of the history of ConvNets .
