[site]: crossvalidated
[post_id]: 639714
[parent_id]: 639449
[tags]: 
Since the response is the Net Operating Income (NOI), if a strictly positive continuous variable, we should consider gamma regression that allows a specific pattern of variance increasing with the mean. If the response have negative values, we can shift all values by a constant before fitting, such as I(noi + 1000) , and remove the constant from all predictions. See a tutorial at https://data.library.virginia.edu/getting-started-with-gamma-regression/ where glm(family = Gamma) estimates mean equation and a constant dispersion, despite that the dispersion estimate is inaccurate according to MASS::gamma.shape(glm()) . See a comparison between gamma regression and linear regression https://civil.colorado.edu/%7Ebalajir/CVEN6833/lectures/GammaGLM-01.pdf . I recommend glmmTMB(noi ~ 1 + (x1 + x2 + x3)^3, dispformula = ~ (x1 + x2 + x3)^3, family = Gamma(link = "log")) which reports an accurate dispersion estimate if constant and allows the dispersion parameter to vary with some predictors. I found that glmmTMB::glmmTMB() actually fits log(shape) = log(1/dispersion) = b * x in the dispersion model and that predict(type = "disp") gives sigma instead of dispersion = sigma^2, neither of which is well documented. I reported the issue at https://github.com/glmmTMB/glmmTMB/issues/990 on Feb 20, 2024. Some useful relationship in gamma regression: Mean = shape * scale. Variance = mean * scale = shape * scale^2 = dispersion * mean^2 = mean^2/shape. Shape = 1/dispersion = mean^2 / var. Scale = 1/rate = var/mean = mean/shape = mean * dispersion. Dispersion = 1/shape = sigma()^2. Nevertheless, OLS and residual plots provides useful information about data patterns. $R^2$ has nothing to do with how reasonable or reliable predictions are. To evaluate models, we need to compare different models and find better ones. $R^2$ is useful in selecting linear models. In a linear model, $R^2$ measures the proportion of variation in the response explained by all the included predictors, no matter whether the predictors are categorical or continuous. In many empirical econometric studies, $R^2 = .13$ is usually considered pretty high. For generalized linear models, the popular choice is AIC for model or variable selection. BIC prefers more parsimonious models. Comparatively, I have not found many versions of pseudo-R2 for generalized linear models any more useful. To remove incidental choice in model selection and evaluate predicting power of different models, we can use (repeated) cross validation within a training sample (e.g. 80% of cases). With a good sample size, we can consider a 10-fold cross validation repeated 5 times, so we estimate a model specification 50 times over different subsamples. We track the performance each time on the validation set, using metrics such as RMSE for a continuous response, which functions like sigma or standard error in a fitted lm() . There is also multimodel inference (see glmulti ). To assess validity of selected models on future occasions, use performance metrics from the reserved test set that is never used in model fitting or cross validation. Concerning in the residual plots are skewness (the red curve is downward), heteroscedasticity (spread of residual vary over fitted response), and outliers (a small cluster with huge response values). Even if the residual plots show undesired patterns, the linear model has the property that the point estimates of coefficients are unbiased. It states that the predicted mean income equals the expected value conditional on the predictors, assuming the model formula is correctly specified so that linearity is satisfied. Making predictions is conducting inference, however, so the residual diagnosis is relevant once p values, standard errors, and confidence intervals come into play. To test skewness or rather linearity, I recommend Ramsey's RESET test for functional form using the fitted-value approach, lmtest::resettest(lm(), power = 2:3, type = "fitted") . Other tests for linearity, such as Harvey-Collier and rainbow test, require ordering the observations properly. We should also use MacKinnon-White-Davidson PE test for comparing linear vs. log-linear specifications: lmtest::petest(lm(noi ~ .), lm(log(noi) ~ .), data = list(), vcov. = NULL, ...) . To correct skewness when predictors are all categorical, we should add interaction terms, such as lm(noi ~ x1 * x2 * x3) or lm(noi ~ (x1 + x2 + x3)^2) for two-way interaction and lm(noi ~ (x1 + x2 + x3)^3) for three-way interaction. With six categorical variables of 2, 2, 3, 4, 4, and 5 levels each, the most complex model is 2 × 2 × 3 × 4 × 4 × 5 six-way interaction. With many categorical predictors, the number of possible interactional terms can be gigantic and forward stepwise selection step(lm(), direction = "forward") is useful. To test heteroscedasticity, I recommend White’s general test, implemented as the fitted-value approach of a Breusch-Pagan test, lmtest::bptest(lm(), varformula = ~ fitted + I(fitted^2) + I(fitted^3)) after saving fitted values into the data frame as fitted . The original Breusch-Pagan test detects linear forms of heteroskedasticity by predicting squared residuals with all predictors. White's test expands it by including squares and interactions of all predictors https://en.wikipedia.org/wiki/White_test that detects heteroskedasticity, skewness and kurtosis of the error distribution but can still be implemented as bptest() (e.g. https://www.statology.org/white-test-in-r/ and White's test for heteroskedasticity in R ). Woodbridge suggests the fitted-value approach for White's test https://cran.r-project.org/web/packages/whitestrap/whitestrap.pdf . To correct heteroscedasticity, popular methods include either robust standard errors, a standard practice adopted in Stata, or generalized least squares that requires predictors of error variances. In R, the former is done in sandwich::vcovHC() and lmtest::coeftest(, vcov = ...) , which only corrects standard errors without changing coefficient estimates; the latter is done by nlme::gls(, weights = varIdent(form = ~ 1 | x1 * x2 * x3)) after which we need residual diagnosis based on standardized residuals, resid(gls.fit, type = "pearson"). With many predictors, however, the pattern of heteroskedasticity can be complex, for which we may consider auxiliary (non)linear variance models to estimate error variances in a heteroskedastic linear regression model https://cran.r-project.org/web/packages/skedastic/skedastic.pdf . With six categorical predictors, defining a few heteroscedastic groups in gls seems feasible. To test outliers, use car::outlierTest(lm()) . We can find influential cases by influence.measures(lm()) and plotting car::dfbetasPlots(lm()) , where extreme values in X (not Y), or unusual combination level of categorical variables, affect coefficients the most. The small high-income cluster may not be a problem once appropriate interaction terms are included. If the sample has natural grouping, such as geographical units, hierarchical companies, repeated measurements, or accounting suppliers, we should consider clustered standard errors through sandwich::vcovCL() and mixed-effects modeling such as lme4::lmer() and nlme::lme() for linear models and lme4::glmer() and glmmTMB::glmmTMB() for generalized linear models.
