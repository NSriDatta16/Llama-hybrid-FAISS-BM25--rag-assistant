[site]: crossvalidated
[post_id]: 640068
[parent_id]: 
[tags]: 
Isn't $f(\mathbf{x}; \mathbf{\theta}) = b + \mathbf{w}^T \mathbf{x} = b + w_1 x_1 + w_2 x_2 + \dots + w_D x_D$ the linear regression model?

Chapter 1.2.1.5 Uncertainty of Probabilistic Machine Learning: An Introduction by Kevin P. Murphy says the following: We can capture our uncertainty using the following conditional probability distribution : $$p(y = c \mid \mathbf{x}; \mathbf{\theta}) = f_c(\mathbf{x}; \mathbf{\theta}) \tag{1.7}$$ where $f: \chi \to [0, 1]^C$ maps inputs to a probability distribution over the $C$ possible output labels. Since $f_c(\mathbf{x}; \mathbf{\theta})$ returns the probability of class label $c$ , we require $0 \le f_c \le 1$ for each $c$ , and $\sum_{c = 1}^C f_c = 1$ . To avoid this restriction, it is common to instead require the model to return unnormalized log-probabilities. We can then convert these to probabilities using the softmax function , which is defined as follows $$\text{softmax}(\mathbf{a}) \triangleq \left[ \dfrac{e^{a_1}}{\sum_{c^\prime = 1}^C e^{a_{c^\prime}}}, \dots, \dfrac{e^{a_C}}{\sum_{c^\prime = 1}^C e^{a_{c^\prime}}} \right] \tag{1.8}$$ This maps $\mathbb{R}^C$ to $[0, 1]^C$ , and satisfies the constraints that $0 \le \text{softmax}(\mathbf{a})_c \le 1$ and $\sum_{c = 1}^C \text{softmax}(\mathbf{a})_c = 1$ . The inputs to the softmax, $\mathbf{a} = f(\mathbf{x}; \mathbf{\theta})$ , are called logits . See Section 2.5.2 for details. We thus define the overall model as follows: $$p(y = c \mid \mathbf{x}; \mathbf{\theta}) = \text{softmax}(\mathbf{a})_c(f(\mathbf{x}; \mathbf{\theta})) \tag{1.9}$$ A common special case of this arises when $f$ is an affine function of the form $$f(\mathbf{x}; \mathbf{\theta}) = b + \mathbf{w}^T \mathbf{x} = b + w_1 x_1 + w_2 x_2 + \dots + w_D x_D \tag{1.10}$$ where $\mathbf{\theta} = (b, \mathbf{w})$ are the parameters of the mode. This model is called logistic regression , and will be discussed in more detail in Chapter 10. Isn't $f(\mathbf{x}; \mathbf{\theta}) = b + \mathbf{w}^T \mathbf{x} = b + w_1 x_1 + w_2 x_2 + \dots + w_D x_D$ the linear regression model, not logistic regression?
