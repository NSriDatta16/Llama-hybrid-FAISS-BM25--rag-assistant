[site]: crossvalidated
[post_id]: 625861
[parent_id]: 
[tags]: 
Gaussian linear regression with no noise

In short: is there an alternative expression for posterior on weights in a linear model, that works with no observation noise? In Rasmussen & Williams "Gaussian Processes" they consider a linear model: $$\begin{aligned} y & = \mathbf{x}'\mathbf{w}+\varepsilon\\ \mathbf{w} & \sim \mathcal{N}(0,\Sigma_{p})\\ \varepsilon & \sim \mathcal{N}(0,\sigma_{n}^{2}) \end{aligned}$$ And they derive the posterior for a Gaussian linear regression: $$E[w|X,y] = (XX'+\sigma_n^2\Sigma_p^{-1})^{-1}Xy$$ This implies that when $\sigma_n^2=0$ then the estimate will be the same as the OLS/MLE case, $\hat{w}=(XX')^{-1}Xy$ . This makes sense when the number of observations is equal to the number of parameters, but if there are more parameters than observations then the matrix $XX'$ will not be invertible, and so the estimate is not well-defined. But I think the Bayesian posterior must exist in this case right? If we have priors then it doesn't matter how rich the data is (as long as it doesn't violate the linearity assumption). So I assume there's some way of rearranging the expression for $E[w|X,y]$ such that it is well-defined, even when noise is equal to zero and the number of parameters exceeds the number of observations.
