[site]: crossvalidated
[post_id]: 252272
[parent_id]: 204978
[tags]: 
The issue you describe with a linear model where coefficients for correlated features can cancel each other out resulting in large positive/negative pairs is known as multicollinearity and can be a problem, especially if the correlation structure shifts over time. I say correlation here since it requires a linear relationship between the variables. Related issues can arise with non linear relationships between features. Often the amount of noise or extraneous information in the features varies (for example height is a noisy proxy for age in children) so the best model will use only the features that contain the information relevant to the target with minimal noise (ie it's better to predict "can drive" from age then from height.) The various algorithms you describe are meant to and can deal with this to some extent. Random forests are particularly good at it as they internally repeat the simple CART feature selection algorithm over bootstrapped copies of the data set. They are often the easiest thing to get to work on highly dimensional data where this sort of issue is common/likely. Doing an explicit dimensionality reduction step like PCA or some sort of nonlinear manifold learning is also common. There are also a number of methods for doing explicit feature selection as a preprocessing step including things like the iterative process of adding and removing features to see how it changes performance. With any of these methods care must be taken to avoid overfitting. The feature selection must be considered part of the overall model generating process and done and tuned within validation. Either a separate holdout or nested cross validation system must be used or the chances of ending up with model performance that does not generalize to new data is high (and goes up with the number of features in the data set).
