[site]: datascience
[post_id]: 114324
[parent_id]: 114313
[tags]: 
One way to go when applying a propensity machine learning model on marketing emails can be: build a machine learning (ML) model to predict which customers will more likely purchase, with a binary classifier depending on the marketing budget and contacting criteria, the email should be sent to clients who are more prone to behave as intended (i.e. purchasing afterall), but leaving some of those clients without intervention as a control group (where intervention means sending the new email, and not intervention is sending the old email): With this scenario, you can: evaluate your ML model performance (once it is in production and once it has been already used for campaigns) with the selected metric (AUC, recall...) with the group of clients who did not receive any intervention (the new email in this case), it could be using B & D on the other hand, evaluate the effectiveness of the campaign comparing clients who received the new email VS who did not (A vs B), by using hypothesis testing with methods like permutation tests ; your null hypothesis is that there is no actual difference (effect) in receiving VS not receiving the cmmunication, and you want to test whether it is actually false (i.e. rejecting this hypothesis confirming that the campaign is actually effective. You can follow these steps ( source of info ): Compute the difference of the considered statistic between sample A and B (n & m sizes) Combine all measurements into a single dataset Draw a permuted dataset from all possible permutations of the dataset in 2. Divide the permuted dataset into two datasets A' and B' of sizes n and m Compute the difference of these samples and record this difference Repeat steps 3-5 until all (or sufficient) permutations are evaluated Return the p-value as the number of times the recorded differences were at least as extreme as the original difference (steep from 1) and divide this number by the total number of permutations
