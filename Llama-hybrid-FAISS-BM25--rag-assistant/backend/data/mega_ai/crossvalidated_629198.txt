[site]: crossvalidated
[post_id]: 629198
[parent_id]: 
[tags]: 
Statistical significance tests for neural networks

Horel & Giesecke 2020 developed a statistical signficance test for feature variables in a single-layer feedforward neural network. Namely, fix a probability space $(\Omega, \mathcal{F}, \mathbb{P})$ , let $Y\in\mathbb{R}$ be a dependent variable, let $X\in\mathcal{X}\subset\mathbb{R}^d$ be feature variables with distribution $P$ , and consider a regression model: \begin{equation} Y=f_0(X)+\epsilon. \end{equation} Here, $f_0:\mathcal{X}\to\mathbb{R}$ is an unknown and deterministic regressor and $\epsilon\sim (0, \sigma^2)$ is idiosyncratic error, where the regressor belongs to the function space $$\Theta=\left\{f\in\mathcal{C}^1, f:\mathcal{X}\subset \mathbb{R}^d\to \mathbb{R}, ||f||_{\lfloor \frac{d}{2}\rfloor + 2}\le B \right\}$$ for $B$ a constant. Since we are interested in the effect of variable $X_j$ on $Y$ , we observe if $\frac{\partial f_0(x)}{\partial x_j}=0$ for all $x\in \mathcal{X}$ , then the $j$ -th variable does not have an influence on the response. They propose a significance test: \begin{align*} &H_0: \lambda_j=\int_{\mathcal{X}} \left(\frac{\partial f_0(x)}{\partial x_j}\right)^2 d\mu(x)=0,\\ &H_A: \lambda_j\ne 0, \end{align*} where $\mu$ is a positive measure. When $\mu=P$ , i.e. the law of feature space $X$ , the statistic is \begin{equation} \lambda_j = \mathbb{E}\left[\Bigg(\frac{\partial f_0(x)}{\partial x_j}\Bigg)^2\right]. \end{equation} Let $f_n$ be an estimator of $f$ trained on $n$ i.i.d. samples of $(Y,X)$ . Then the test statistic is \begin{equation} \lambda_j^n =\int_{\mathcal{X}}\left(\frac{\partial f_n(x)}{\partial x_j}\right)^2 d\mu(x). \end{equation} The paper studies the asymptotic distribution of $\lambda_j^n$ as $n\to\infty$ , where it is assumed that the hidden dimension $K_n$ of the network grows with $n$ . Theorem 1 shows that $f_n$ converges weakly in the metric space $(\Theta, d)$ for $d(f,g)=\mathbb{E}[(f-g)^2]$ such that \begin{equation} r_n(f_n-f_0)\implies h^* \end{equation} for $h^*$ the argmax of the Gaussian process ${\mathbb{G}_f:f\in\Theta}$ with mean zero and covariance $Cov(\mathbb{G}_s, \mathbb{G}_t)=4\sigma^2 \mathbb{E}_X(s(X)t(X))$ and $r_n=\left(\frac{n}{\log n}\right)^{(d+1)/(2(2d+1))}$ . Under the assumptions of Theorem 1 and the null $H_0$ , they find: \begin{equation} r_n^2\lambda_j^n \implies \int_{\mathcal{X}} \left(\frac{\partial h^*(x)}{\partial x_j}\right)^2 d\mu(x). \end{equation} In section 4, they use the fact that $\Theta$ is a subspace of the Hilbert space $$L^2(P):=\left\{f: \mathcal{X} \to \mathbb{R}: ||f||_{L^2(P)} to admit an orthonormal basis and are able to obtain a series representation of the asymptotic distribution. Is there a generalized version of the convergence property in Theorem 1 for a neural network with an arbitrary number of layers? If not, is there a known/standard method for statistical significance testing of feature variables for neural networks?
