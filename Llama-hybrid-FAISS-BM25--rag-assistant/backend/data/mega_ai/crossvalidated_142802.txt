[site]: crossvalidated
[post_id]: 142802
[parent_id]: 142262
[tags]: 
Assuming you came up with this, you have invented the first half of a Naive Bayes classifier . Congrats! The traditional next step is to assume that events $X_i$ and $X_j$ are independent , so that $$\frac{P(A\mid X_i, X_j)}{P(B\mid X_i, X_j)} = \frac{P(X_i, X_j \mid A)}{P(X_i, X_j \mid B)} = \frac{P(X_i \mid A)}{P(X_i \mid B)} \cdot \frac{P(X_j \mid A)}{P(X_j \mid B)}.$$ (I'm using the more traditional math notation $X_i$ to denote your event x[,i] > 0 .) Once you have this, you know that $P(A)/P(B) = \prod_{i=1}^p P(A \mid X_i) / P(B \mid X_i)$ and, since $P(A) + P(B) = 1$, you can solve for $P(A)$. There are a couple caveats with this approach: If any entry $P(A \mid X_i)$ is zero then your algorithm will say that the instance must be of class $B$, regardless of the evidence of the other columns. If you have both $P(A \mid X_i) = 0$ and $P(B \mid X_j) = 0$ then your answer is undefined! A "hack" to get around this is to pretend you've seen one extra instance in all categories (in other words, use p(A|x[,i]>0) = (sum(x[,i]>0 & A) + 1)/(sum(x[,i]>0) + 2) in your pseudocode above). A primary strength of Naive Bayes is that it has no parameters to learn, so it works well on small datasets. If you have enough sequences of events compared to their length (say, 10 times as many strings as the length of the string), you may get better results from a different kind of algorithm like logistic regression .
