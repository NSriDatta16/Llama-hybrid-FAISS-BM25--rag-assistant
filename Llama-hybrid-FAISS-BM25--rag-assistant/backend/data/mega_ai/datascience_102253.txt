[site]: datascience
[post_id]: 102253
[parent_id]: 102239
[tags]: 
Run a PCA on or LDA your data set. Here is some sample code to start with. import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from sklearn.datasets import load_breast_cancer cancer = load_breast_cancer() df = pd.DataFrame(data=cancer.data, columns=cancer.feature_names) df.head() X = df.values X.shape from sklearn.preprocessing import StandardScaler # It is essential to perform feature scaling before running PCA if there is a significant difference in # the scale between the features of the dataset; for example, one feature ranges in values between 0 and 1 # and another between 100 and 1,000. PCA is very sensitive to the relative ranges of the original features. # We can apply z-score standardization to get all features into the same scale by using Scikit-learn # StandardScaler() class which is in the preprocessing submodule in Scikit-learn. scaler = StandardScaler() scaler.fit(X) X_scaled = scaler.transform(X) from sklearn.decomposition import PCA pca_30 = PCA(n_components=30, random_state=2020) pca_30.fit(X_scaled) X_pca_30 = pca_30.transform(X_scaled) print('variance explained by all 30 components = ', sum(pca_30.explained_variance_ratio_ * 100)) # The first component alone captures about 44.27% of the variability in the dataset and the second component alone captures about 18.97% of the variability in the dataset and so on. pca_30.explained_variance_ratio_ * 100 np.cumsum(pca_30.explained_variance_ratio_ * 100) plt.plot(np.cumsum(pca_30.explained_variance_ratio_)) plt.xlabel('number of components') plt.ylabel('explained variance') print(np.cumsum(pca_30.explained_variance_ratio_ * 100)[0]) print(np.cumsum(pca_30.explained_variance_ratio_ * 100)[1]) print(np.cumsum(pca_30.explained_variance_ratio_ * 100)[2]) # You can see that the first 10 principal components keep about 95.1% of the variability in the # dataset while reducing 20 (30–10) features in the dataset. That’s great. The remaining 20 features # only contain less than 5% of the variability in data. # two principal components pca_2 = PCA(n_components=2, random_state=2020) pca_2.fit(X_scaled) X_pca_2 = pca_2.transform(X_scaled) plt.figure(figsize=(10,10)) sns.scatterplot(x=X_pca_2[:,0], y=X_pca_2[:,1], s=70, hue=cancer.target, palette=['blue','red']) plt.title('2D Scatterplot of 63% of Variability Captured') plt.xlabel('First Principal Component') plt.ylabel('Second Principal Component') # three principal components pca_3 = PCA(n_components=3, random_state=2020) pca_3.fit(X_scaled) X_pca_3 = pca_3.transform(X_scaled) from mpl_toolkits import mplot3d fig = plt.figure(figsize= (12,9)) ax = plt.axes(projection='3d') sctt = ax.scatter3D(X_pca_3[:,0], X_pca_3[:,1], X_pca_3[:,2], c=cancer.target, s=50, alpha=0.6) plt.title('3D Scatterplot of 72% of Variability Captured') plt.xlabel('First Principal Component') plt.ylabel('Second Principal Component') pca_95 = PCA(n_components=.95, random_state=2020) pca_95.fit(X_scaled) X_pca_95 = pca_95.transform(X_scaled) # This means that the algorithm has found 10 principal components to preserve 95% of the variability in # the data. The X_pca_95 array holds the values of all 10 principal components. X_pca_95.shape df_new = pd.DataFrame(X_pca_95, columns=['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9','PC10']) df_new['label'] = cancer.target df_new Relevant link. https://towardsdatascience.com/principal-component-analysis-pca-with-scikit-learn-1e84a0c731b0
