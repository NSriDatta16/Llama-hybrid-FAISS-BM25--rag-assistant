[site]: stackoverflow
[post_id]: 3952296
[parent_id]: 3950523
[tags]: 
A digression: that the OS doesn't matter for performance is, in general , obviously false. Citing CPU utilization when idle seems a quite "peculiar" idea to me: of course one hopes that when no jobs are running the OS is not wasting energy. Otherwise one measure the speed/throughput of an OS when it is providing a service (i.e. mediating the access to hardware/resources). To avoid an annoying MS Windows vs Linux vs Mac OS X battle, I will refer to a research OS concept: exokernels . The point of exokernels is that a traditional OS is not just a mediator for resource access but it implements policies . Such policies does not always favor the performance of your application-specific access mode to a resource. With the exokernel concept, researchers proposed to " exterminate all operating system abstractions " (.pdf) retaining its multiplexer role. In this way : … The results show that common unmodified UNIX applications can enjoy the benefits of exokernels: applications either perform comparably on Xok/ExOS and the BSD UNIXes, or perform significantly better. In addition, the results show that customized applications can benefit substantially from control over their resources (e.g., a factor of eight for a Web server). … So bypassing the usual OS access policies they gained, for a customized web server, an increase of about 800% in performance. Returning to the original question: it's generally true that an application is executed with no or negligible OS overhead when: it has a compute-intensive kernel , where such kernel does not call the OS API; memory is enough or data is accessed in a way that does not cause excessive paging; all inessential services running on the same systems are switched off. There are possibly other factors, depending by hardware/OS/application. I assume that the OP is correct in its rough estimation of computing power required . The OP does not specify the nature of such intensive computation, so its difficult to give suggestions. But he wrote: The amount of calculations are enormous "Calculations" seems to allude to compute-intensive kernels , for which I think is required a compiled language or a fast interpreted language with native array operators, like APL , or modern variant such as J , A+ or K (potentially, at least: I do not know if they are taking advantage of modern hardware). Anyway, the first advice is to spend some time in researching fast algorithms for your specific problem (but when comparing algorithms remember that asymptotic notation disregards constant factors that sometimes are not negligible). For the sequential part of your program a good utilization of CPU caches is crucial for speed. Look into cache conscious algorithms and data structures. For the parallel part, if such program is amenable to parallelization (remember both Amdahl's law and Gustafson's law ), there are different kinds of parallelism to consider (they are not mutually exclusive): Instruction-level parallelism : it is taken care by the hardware/compiler; data parallelism : bit-level: sometimes the acronym SWAR (SIMD Within A Register) is used for this kind of parallelism. For problems (or some parts of them) where it can be formulated a data representation that can be mapped to bit vectors (where a value is represented by 1 or more bits); so each instruction from the instruction set is potentially a parallel instruction which operates on multiple data items (SIMD). Especially interesting on a machine with 64 bits (or larger) registers. Possible on CPUs and some GPUs . No compiler support required; fine-grain medium parallelism: ~10 operations in parallel on x86 CPUs with SIMD instruction set extensions like SSE , successors, predecessors and similar; compiler support required; fine-grain massive parallelism: hundreds of operations in parallel on GPGPUs (using common graphic cards for general-purpose computations), programmed with OpenCL (open standard), CUDA (NVIDIA), DirectCompute (Microsoft), BrookGPU (Stanford University) and Intel Array Building Blocks . Compiler support or use of a dedicated API is required. Note that some of these have back-ends for SSE instructions also; coarse-grain modest parallelism (at the level of threads, not single instructions): it's not unusual for CPUs on current desktops/laptops to have more then one core (2/4) sharing the same memory pool ( shared-memory ). The standard for shared-memory parallel programming is the OpenMP API , where, for example in C/C++, #pragma directives are used around loops. If I am not mistaken, this can be considered data parallelism emulated on top of task parallelism; task parallelism : each core in one (or multiple) CPU(s) has its independent flow of execution and possibly operates on different data. Here one can use the concept of "thread" directly or a more high-level programming model which masks threads. I will not go into details of these programming models here because apparently it is not what the OP needs. I think this is enough for the OP to evaluate by himself how various languages and their compilers/run-times / interpreters / libraries support these forms of parallelism.
