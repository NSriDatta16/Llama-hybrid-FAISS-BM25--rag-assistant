[site]: crossvalidated
[post_id]: 50341
[parent_id]: 
[tags]: 
What could be the described method for treating time-outs in algorithm benchmarks

As a computer scientist, you often face the problem to empirically analyse the improvements in run time of some algorithm. I stumbled over the following text in a paper: These run times are averages over 10 runs per benchmark, and account for time-outs using maximum likelihood estimation (MLE) [cite]. With 90% confidence level, 71% of those averages are accurate to within 25%. The cite is simply a book on statistics "Mathematical Statistics and Data Analysis, Rice" without page numbers :( To explain the setting Suppose you have a set of algorithms $A$ . Every algorithm is applicable to every benchmark problem in your problem set $P$ . This gives $|A||P|$ possible configurations. For randomized algorithms, you repeat each configuration $n$ times. This makes a total of $n|A||P|$ experiments. For each experiment, you measure the time it takes the algorithm to calculate the solution. But for practical reasons, you set a time-out $t_{max}$ . So you end up with a set of run-times with na values meaning "larger than $t_{max}$ ". The cite is from a caption plotting number of solved instances over time. So I assume they calculated some kind of "average value" for each algorithm/problem combination (over 10 different random seeds), accounting for the time-outs.
