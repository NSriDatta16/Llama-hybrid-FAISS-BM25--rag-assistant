[site]: crossvalidated
[post_id]: 352550
[parent_id]: 352430
[tags]: 
You can think of the average of the word embeddings as being a continuous space version of the traditional bag-of-words representation. Bag-of-words (BoW) represents a document with a vector the size of the vocabulary where the entries in the vector contain the count for each word. BoW treats each word independently and ignores the order of the words, but it works quite well for text classification. If you multiply the BoW vector with the word embedding matrix and divide by the total number of words in the document then you have the average word2vec representation. This contains mostly the same information as BoW but in a lower dimensional encoding. You can actually train a model to recover which words were used in the document from the average word2vec vector. So, you aren't losing very much information by compressing the representation like this.
