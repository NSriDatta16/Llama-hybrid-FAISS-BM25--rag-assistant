[site]: crossvalidated
[post_id]: 610256
[parent_id]: 609766
[tags]: 
Although you've rightly observed structural similarity between the problem formulations of the constrained TRPO and off-policy actor critic (OFF-PAC) in terms of importance sampling, their implementation algos are completely different. OFF-PAC uses a same behavior policy $b$ to generate action and observes its resultant experiences during each incremental iterative step to form your above advantage function $A_t$ , and more importantly update the parameter of the same single target policy $\pi$ thus it's off-policy learning. Implementation details can be found in Degris et al's (2012) paper "Off-Policy Actor-Critic" . On the other hand, TRPO as referenced in Schulman's original 2015 paper uses Single Path or Vine method to sample average as stochastic approximation of its maximization target $\mathbb{E}_{s \sim \rho_{\theta_{old}}, a \sim {\pi_{\theta_{old}}}}[\frac{\pi_\theta(a|s)}{\pi_{\theta_{old}}(a|s)}\cdot A_{\theta_{old}}]$ and further replace above advantage function simply by Q value estimates under current (old) policy $\pi_{\theta_{old}}$ during each current policy evaluation and improvement step until final convergence to arrive at an optimal target policy hopefully. Since TRPO learns from the experiences generated by the current policy and also improves the same current policy at every iterative step therefore it's on-policy similar to the general policy iteration method of classic dynamic programming and SARSA.
