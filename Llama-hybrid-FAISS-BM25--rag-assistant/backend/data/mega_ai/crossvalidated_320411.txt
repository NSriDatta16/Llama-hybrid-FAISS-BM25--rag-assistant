[site]: crossvalidated
[post_id]: 320411
[parent_id]: 
[tags]: 
Neural Networks Back Propagation Basic Question

When we train neural networks, we vectorize all examples and after back propagation , subtract the sum of gradients for all examples per parameter. Would not it be accurate to update parameter for one example, then reuse the calculated parameter to generate updates for the next example. When we calculate all gradients at once and sum them, we may have subtracted more out of the parameters than we would have if we had done step by step. For example if i have say 2 examples, i do -1 and then calculate again and do -.5 . We will have weights as -1.5. But if i did together, I might have done -1 , then for the next example -1.5. For a total of -2.5..
