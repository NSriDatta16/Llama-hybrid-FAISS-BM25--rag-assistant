[site]: datascience
[post_id]: 111058
[parent_id]: 111046
[tags]: 
The RNN receives as input a batch of sequences of characters. The output of the RNN is a tensor with sequences of character predictions, of just the same size of the input tensor. The number of sequences in each batch is the batch size. Every sequence in a single batch must be the same length. In this case, all sequences of all batches have the same length, defined by seq_length . Each position of the sequence is normally referred to as a "time step". When back-propagating an RNN, you collect gradients through all the time steps. This is called "back-proparation through time (BPTT)". You could have a single super long sequence, but the memory required for that would be large, so normally you must choose a maximum sequence length. To somewhat mitigate the need of cutting the sequences, people normally apply something called "truncated BPTT". That is what the code you linked uses. It consists of having the sequences in the batches arranged so that each of the sequences in the next batch are the continuation of the text from each of the sequences in the previous batch, together with reusing the last hidden state of the previous batch as the initial hidden state of the next one.
