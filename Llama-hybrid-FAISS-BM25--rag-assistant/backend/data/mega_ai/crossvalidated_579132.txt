[site]: crossvalidated
[post_id]: 579132
[parent_id]: 
[tags]: 
How to compute the start and end vector for BERT question answering?

Fine tunning BERT for the question answering task requires the training of a start vector and an end vector as it is said in the original paper : https://aclanthology.org/N19-1423/ , see below I perfectly understand, once you have those two vectors, how you extract the answer within the context. However, authors do not say much about how these vectors are trained. It looks like a classic embedding like tokens embeddings since it seems that there is a unique start token and a unique end token. But if so, how come this work? How one representation of the start vector could ever work for all pairs of (question,context)? Why those vectors do not depend on the very specific input pairs?
