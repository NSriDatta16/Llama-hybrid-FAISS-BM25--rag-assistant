[site]: crossvalidated
[post_id]: 184525
[parent_id]: 
[tags]: 
How to determine whether or not the y-axis of a graph should start at zero?

One common way to "lie with data" is to use a y-axis scale that makes it seem as if changes are more significant than they really are. When I review scientific publications, or students' lab reports, I am often frustrated by this "data visualization sin" (which I believe the authors commit unintentionally, but still results in a misleading presentation.) However, "always start the y-axis at zero" is not a hard-and-fast rule. For example, Edward Tufte points out that in a time series, the baseline is not necessarily zero: In general, in a time-series, use a baseline that shows the data not the zero point. If the zero point reasonably occurs in plotting the data, fine. But don't spend a lot of empty vertical space trying to reach down to the zero point at the cost of hiding what is going on in the data line itself. (The book, How to Lie With Statistics, is wrong on this point.) For examples, all over the place, of absent zero points in time-series, take a look at any major scientific research publication. The scientists want to show their data, not zero. The urge to contextualize the data is a good one, but context does not come from empty vertical space reaching down to zero, a number which does not even occur in a good many data sets. Instead, for context, show more data horizontally! I want to point out misleading presentation in papers I review, but I don't want to be a zero-y-axis purist. Are there any guidelines that address when to start the y-axis at zero, and when this is unnecessary and/or inappropriate? (Especially in the context of academic work.)
