[site]: crossvalidated
[post_id]: 329316
[parent_id]: 
[tags]: 
Why use fewer features to fix high variance in a machine learning algorithm

How does getting a smaller feature space result in combating high variance in a machine learning algorithm? What I currently understand is the following: With fewer features the hypothesis can easier fit the data and therefore it helps against high bias (underfitting). With more features the hypothesis can harder fit the data and therefore it helps against high variance (overfitting). Please point out to me what is wrong with my thought process.
