[site]: crossvalidated
[post_id]: 471826
[parent_id]: 240690
[tags]: 
Just stick with squares, it's safe & easy Most CNN training datasets have images that are not square. The standard method is to take a square crop out of it -- often picking a random square for training, and at test time to use multiple squares and aggregate the predictions (center + 4 corners is a classic). Cropping is preferable to padding because with padding you're wasting a lot of compute on blank image space, and you're creating this artificial edge where the real picture has a hard line against a solid color, which can confuse the classifier. So the easiest thing is to just stick with common practice and crop to square. This is general advice, which works well for typical camera images that have moderate aspect ratios like 1.5:1. In your case where the aspect ratio is extreme, cropping is obviously going to greatly limit what the network can physically see. Using different sizes That said most modern CNN's do allow you to run non-square images through them. This is somewhat advanced / unusual, so better to stick to square images unless you're confident you understand what's going on and the risks involved. Whether or not this is physically possible for a type of CNN is determined by the network layer near the end, that transitions from the convolutional processing to the fully-connected processing for classification or what-have-you. In early CNN's like AlexNet, this layer just serialized all the channel maps from the reduced-resolution processed image into a simple vector of features - so if your last conv layer outputs an 8x8x32 Tensor (HxWxChannel) this just unrolls it to a 2048 dimensional vector. This strategy encodes each position of the image into a different set of dimensions in the vector, allowing the network to reason about features at different locations of the image (can be good or bad), but also means that the resolution it can work with is fixed, because if you change the resolution, the dimensionality of the feature vector would change, and then the down-stream fully-connected layers just wouldn't work. Most modern CNN's instead use some kind of pooling layer to average or aggregate the location-based features across the spatial dimensions into a simple feature vector (sometimes accurately described as 1x1 resolution) for the down-stream FC layers. So if that same network that output 8x8x32 features reached this pooling layer, it might average all 64 (=8x8) of those 32-dim vectors into a single 32-dimensional feature vector for downstream processing. (In practice these nets have more than 32 features at the end.) With these networks, there's nothing physically stopping you from running different sized images through the network. Some good details in e.g. this answer: https://stats.stackexchange.com/a/392854/13947 But always remember that if you ask a network to do anything it wasn't trained to do, it's extremely likely to behave badly!
