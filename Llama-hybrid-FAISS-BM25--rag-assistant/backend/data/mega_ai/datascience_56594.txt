[site]: datascience
[post_id]: 56594
[parent_id]: 
[tags]: 
Blind feature engineering

I received a dataset for analysis that had ~100 numeric columns with anonymous column names( $X1$ , $X2$ , $X3$ , etc...) and asked to do a binary classification. My resulting classification algorithm using a SVM had good accuracy (> 95%), but I was unable to do much in the way of feature engineering or feature generation other than the standard scaling, null-value replacement, etc, since I had no intuition about the columns. Is there any standard logic as to how to do some sort of automated feature generation, i.e. some simple mathematical combinations of various columns to create new, useful features? Does this sort of thing have any mathematical basis for linear or tree-based models? Or is feature engineering only really meaningful when one has intuition based on the column names...
