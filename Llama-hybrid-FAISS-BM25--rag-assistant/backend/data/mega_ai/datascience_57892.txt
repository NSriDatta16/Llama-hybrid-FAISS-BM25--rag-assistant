[site]: datascience
[post_id]: 57892
[parent_id]: 57094
[tags]: 
I think that the decision tree that appears in the second article is just illustrating the xgboost model that the shap is applied on. I would like to suggest you to read Christoph Molnar tutorial book on explainability, especially the chapters about Shapley and about the shap algorithm. The first term (shapely) helps to decompose the effect of each feature on the predicted output by trying all the combinations ("coalitions") of features with the input values (vector). By that, the method allows deconstructing how the output of the model been constructed by each of the input feature values. This approach looks on the features as players that their different strategies affect the final prediction, and it originated in the game theory domain. The second term, SHAP, is basically calculating the same shapely values with some extensions for missing values that are not handled well in the first and improvements for the algorithm that estimates the shapely values themselves both by speed (the Kernel function that approximates them) and quality of the approximations (the weighting method that combines the combinations). Besides does algorithmic differences, I think that the shap method is also the approach of using the Shapley algorithm for the usage of interpreting ML (tree-based) models, mainly, by exploiting their additivity property to describe the prediction as a decomposition of the sum of features contributions.
