[site]: datascience
[post_id]: 35580
[parent_id]: 35544
[tags]: 
By oversampling . When training a CNN, you generally use a mini-batch gradient descent method. The mini-batch method consists on taking some images at random and performing a gradient step with those images by backpropagating the loss of the model on those images. This random selection doesn't need to be uniform on all the images. Instead, you can make the images in the classes with few elements appear with more likelihood (oversampling them) than the other images. With this, you can help the CNN not only to learn about the most prevalent classes. In addition, if you have very few number of images of some classes, it is recommended to do image augmentation, thus generate more images in order to avoid overfitting those classes.
