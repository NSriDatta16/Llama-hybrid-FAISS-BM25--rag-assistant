[site]: crossvalidated
[post_id]: 370923
[parent_id]: 348085
[tags]: 
The VC inequality is strictly for binary hypothesis classes ( $ \{h:X \rightarrow \{0,1\}\}$ ) . It gives an upper bound of the absolute difference between the empirical and the true risk , when they are defined with respect to the 0-1 loss .The VC dimension it's defined for binary classifiers only. However ,the inequality can be generalized to all the settings you mentioned using different complexity measures (like Rademacher complexity or pseudo-dimension). See for example Understanding Machine Learning: From Theory to Algorithms by Shai Ben-David and Shai Ben-David (it's available online for free) or Foundations of Machine Learning by Mehryar Mohri , Afshin Rostamizadeh and Ameet Talwalkar
