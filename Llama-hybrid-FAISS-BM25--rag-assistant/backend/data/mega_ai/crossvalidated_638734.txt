[site]: crossvalidated
[post_id]: 638734
[parent_id]: 
[tags]: 
How to penalize disagreement between two classification loses?

I am working with a multi-head, multi-loss neural network. Each of the two heads is associated with a multi-class classification loss. The losses are combined additively. Assume loss 1 is trained to predict labels A , B , and C . Similarly, loss 2 is trained to predict labels A , D , E . I want to enforce the condition that loss 1 always predicts A when loss 2 predicts A and vice versa, such that any deviation, i.e., L1 predicts A and L2 predicts B , is penalized during the learning process. My initial thought was maybe using an indicator function of some sort, which I know is not differentiable, so it would have to approximate an indicator. Any thoughts?
