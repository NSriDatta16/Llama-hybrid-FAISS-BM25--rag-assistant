[site]: crossvalidated
[post_id]: 570338
[parent_id]: 
[tags]: 
AB Testing: Possible to use bootstrapping to obtain the probability that each variant is optimal?

I've recently thought of an extremely simple way to interpret an AB test. The only issue is, I'm not sure it's actually a valid approach â€” I do not have formal training in statistics, so this might be complete nonsense... Let's say that I operate an ecommerce site, and I've recently completed an AB test concerning the design of my website. I'm interested in whether the average revenue per visitor is higher in variant A (my control) or variant B (my new design). I have no strong prior expectation that either variant will perform better than the other, and for simplicity's sake let's say I've exposed 50% of visitors to variant A and 50% to variant B. My results look something like this: visitID Variant Revenue Generated 535345 A 0.00 535346 B 0.00 535347 A 7.99 535348 B 0.00 535349 B 0.00 535350 B 24.49 I can think of two ways to approach this: Bootstrap a load of samples, calculate the per-variant average revenue for each sample, then compare the distribution of these mean revenue values (perhaps using some kind of frequentist test, if the distributions are suitably normal). Bootstrap a load of samples, calculate the per-variant average revenue for each sample, then count up the number of times each variant came out on top in each sample. So you would take, say, 10,000 bootstrapped samples of the data set above, check if A or B had higher average revenue per visit in each sample, and then say something like ' Variant A came out on top in 8,349 of the 10,000 samples, thus there is an ~83% chance that A is the optimal variant ' My question is: is approach 2 a valid way to interpret the test?
