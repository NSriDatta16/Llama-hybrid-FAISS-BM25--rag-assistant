re different if none of the images in one orbit coincide with any image in the other. A natural question arises: how can one compare two orbits? There are several possible approaches. One of them employs the fact that intuitively two empirical orbits are the same irrespective of the ordering of their points. Thus, one can consider a probability distribution P I {\displaystyle P_{I}} induced by the group's action on images I {\displaystyle I} ( g I {\displaystyle gI} can be seen as a realization of a random variable). This probability distribution P I {\displaystyle P_{I}} can be almost uniquely characterized by K {\displaystyle K} one-dimensional probability distributions P ⟨ I , t k ⟩ {\displaystyle P_{\langle I,t^{k}\rangle }} induced by the (one-dimensional) results of projections ⟨ I , t k ⟩ {\displaystyle \langle I,t^{k}\rangle } , where t k , k = 1 , … , K {\displaystyle t^{k},k=1,\ldots ,K} are a set of templates (randomly chosen images) (based on the Cramer–Wold theorem and concentration of measures). Consider n {\displaystyle n} images X n ∈ X {\displaystyle X_{n}\in X} . Let K ≥ 2 c ε 2 log ⁡ n δ {\displaystyle K\geq {\frac {2}{c\varepsilon ^{2}}}\log {\frac {n}{\delta }}} , where c {\displaystyle c} is a universal constant. Then | d ( P I , P I ′ ) − d K ( P I , P I ′ ) | ≤ ε , {\displaystyle |d(P_{I},P_{I}^{\prime })-dK(P_{I},P_{I}^{\prime })|\leq \varepsilon ,} with probability 1 − δ 2 {\displaystyle 1-\delta ^{2}} , for all I , I ′ {\displaystyle I,I^{\prime }} ∈ {\displaystyle \in } X n {\displaystyle X_{n}} . This result (informally) says that an approximately invariant and unique representation of an image I {\displaystyle I} can be obtained from the estimates of K {\displaystyle K} 1-D probability distributions P ⟨ I , t k ⟩ {\displaystyle P_{\langle I,t^{k}\rangle }} for k = 1 , … , K {\displaystyle k=1,\ldots ,K} . The number K {\displaystyle K} of projections needed to discriminate n {\displaystyle n} orbits, induced by n {\displaystyle n} images, up to precision ε {\displaystyle \varepsilon } (and with confidence 1 − δ 2 {\displaystyle 1-\delta ^{2}} ) is K ≥ 2 c ε 2 log ⁡ n δ {\displaystyle K\geq {\frac {2}{c\varepsilon ^{2}}}\log {\frac {n}{\delta }}} , where c {\displaystyle c} is a universal constant. To classify an image, the following "recipe" can be used: Memorize a set of images/objects called templates; Memorize observed transformations for each template; Compute dot products of its transformations with image; Compute histogram of the resulting values, called signature of the image; Compare the obtained histogram with signatures stored in memory. Estimates of such one-dimensional probability density functions (PDFs) P ⟨ I , t k ⟩ {\displaystyle P_{\langle I,t^{k}\rangle }} can be written in terms of histograms as μ n k ( I ) = 1 / | G | ∑ i = 1 | G | η n ( ⟨ I , g i t k ⟩ ) {\displaystyle \mu _{n}^{k}(I)=1/\left|G\right|\sum _{i=1}^{\left|G\right|}\eta _{n}(\langle I,g_{i}t^{k}\rangle )} , where η n , n = 1 , … , N {\displaystyle \eta _{n},n=1,\ldots ,N} is a set of nonlinear functions. These 1-D probability distributions can be characterized with N-bin histograms or set of statistical moments. For example, HMAX represents an architecture in which pooling is done with a max operation. Non-compact groups of transformations In the "recipe" for image classification, groups of transformations are approximated with finite number of transformations. Such approximation is possible only when the group is compact. Such groups as all translations and all scalings of the image are not compact, as they allow arbitrarily big transformations. However, they are locally compact. For locally compact groups, invariance is achievable within certain range of transformations. Assume that G 0 {\displaystyle G_{0}} is a subset of transformations from G {\displaystyle G} for which the transformed patterns exist in memory. For an image I {\displaystyle I} and template t k {\displaystyle t_{k}} , assume that ⟨ I , g − 1 t