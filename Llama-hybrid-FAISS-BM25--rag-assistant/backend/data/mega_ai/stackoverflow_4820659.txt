[site]: stackoverflow
[post_id]: 4820659
[parent_id]: 
[tags]: 
Maintaining Floating Point Accuracy with a Running Average

I need to calculate the mean-squared error of a 16-bit operation for an arbitrary number of data points (upwards of 100 million). I decided to go with a running average so I wouldn't have to worry about overflow from adding a large number of squared errors. At 100 million samples I had problems with floating point precision (inaccurate results) so I moved to double. Here is my code int iDifference = getIdeal() - getValue(); m_iCycles++; // calculate the running MSE as // http://en.wikipedia.org/wiki/Moving_average // MSE(i + 1) = MSE(i) + (E^2 - MSE(i))/(i + 1) m_dMSE = m_dMSE + ((pow((double)iDifference,2) - m_dMSE) / (double)m_iCycles); Is there a better way to implement this to maintain accuracy? I considered normalizing the MSE to one and simply keeping a sum with a final division on completion to calculate the average.
