[site]: crossvalidated
[post_id]: 554709
[parent_id]: 415386
[tags]: 
I'm going to add onto Brale's answer which I think is mostly right. The part that's slightly incorrect is to talk about the bias from initialization of the value neural network. I think it makes more sense to talk about the ideal value neural network (that minimizes the MSE objective). Suppose we train the value network to convergence, obtaining a local minima. This process is itself unbiased (for the value objective). However, at convergence, the value network imperfectly approximates the true value function given the policy. This error is bias in the value function. So even if we trained our policy neural net using the converged value neural network, the (policy) objective is biased. For concreteness, imagine freezing your policy net then "train your value net for infinite iterations" then make an update to the policy net. This update is biased since the value net is biased.
