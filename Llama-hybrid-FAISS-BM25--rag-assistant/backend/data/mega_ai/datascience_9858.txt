[site]: datascience
[post_id]: 9858
[parent_id]: 
[tags]: 
Convergence in Hartigan-Wong k-means method and other algorithms

I have been trying to understand the different k-means clustering algorithms mainly that are implemented in the stats package of the R language. I understand the Lloyd's algorithm and MacQueen's online algorithm. The way I understand them is as follows: Lloyd's Algorithm: Initially ‘k’ random observations are chosen that will serve as the centroids of the ‘k’ clusters. Then the following steps occur in iteration till the centroids converge. The Euclidean distance between each observation and the chosen centroids is calculated. The observations that are closest to each centroids are tagged within ‘k’ buckets. The mean of all the observations in each bucket serves as new centroids. The new centroids replace the old centroids and the iteration goes back to step 1 if the old and new centroids have not converged. The conditions to converge are the following: the old and the new centroids are exactly identical, the difference between the centroids is small (of the order of 10^-3) or the maximum number of iterations (10 or 100) are reached. MacQueen's Algorithm: This is an online version where the first 'k' instances are chosen as centroids. Then each instance is placed in buckets depending on which centroid is closest to that instance. The respective centroid is recalculated. Repeat this step till each instance is placed in the appropriate bucket. This algorithm only has one iteration and the loop goes on for 'x' instances Hartigan-Wong Algorithm: Assign all the points/instances to random buckets and calculate the respective centroid. Starting from the first instance find the nearest centroid and assing that bucket. If the bucket changed then recalculate the new centroids i.e. the centroid of the newly assigned bucket and the centroid of the old bucket assignment as those are two centroids that are affected by the change Loop through all the points and get new centroids. Do a second iteration of points 2 and 3 which performs sort of a clean-up operation and reassigns stray points to correct buckets. So this algorithm performs 2 iterations before we see the convergence result. Now, I am unsure if what I think in point 4 in the Hartigan-Wong algorithm is the correct method of the algorithm. My question is, if the following method for Hartigan-Wong is the correct method to implement k-means? Are there only two iterations for this method? if not, what is the condition for convergence (when to stop)? Another possible implementation explanation what I understand is. Assign all the points/instances to random buckets and calculate the respective centroid. Starting from the first instance find the nearest centroid and assign that bucket. If the bucket changed then recalculate the new centroids i.e. the centroid of the newly assigned bucket and the centroid of the old bucket assignment as those are two centroids that are affected by the change. Once there is a change in the bucket for any point, go back to the first instance and repeat the steps again. The iteration ends when all the instances are iterated and none of the points change buckets. This way there are a lot of iterations that start from the beginning of the dataset again and again every time when an instance changes buckets. Any explanations would be helpful and please let me know if I my understanding for any of these methods is wrong.
