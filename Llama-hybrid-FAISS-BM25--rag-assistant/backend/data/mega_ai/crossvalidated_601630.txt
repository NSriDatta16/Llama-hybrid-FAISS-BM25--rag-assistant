[site]: crossvalidated
[post_id]: 601630
[parent_id]: 
[tags]: 
Uniform ergodicity of a Gibbs sampler

We consider a classical data set from Gelfand and Smith containing the information about ten nuclear power plant pump failures. We are interested in the failure intensity of each pump and we employ the following model. Let us suppose that the number of failures of the $i$ -th pump $x_i$ observed for time $t_i$ has Poisson distribution with parameter $$ \lambda_i t_i \text{ for } i=1, \dots , 10 .$$ We also assume that $x_i$ are independent and that $\lambda_i \sim \Gamma ( \alpha, \beta )$ , where $\beta \sim \Gamma ( \gamma , \delta )$ and $\alpha , \gamma, \delta >0$ . Finally, we assume that $\lambda_i$ are conditionally independent given $\beta $ . We see that $$ p( \lambda , \beta , x) = p(x | \lambda , \beta ) \pi (\lambda | \beta ) \pi ( \beta ) ,$$ where we know all distributions in the last expression. We can deduce that $$ \lambda_i | \lambda_{-i} , \beta , x \sim \Gamma ( \alpha + x_i, \beta + t_i ), \ i = 1, \dots , 10 $$ and $$ \beta | \lambda , x \sim \Gamma \bigg( \alpha n + \gamma , \sum_{i=1}^{10} \lambda_i + \delta \bigg) .$$ We use the following Gibbs sampler to approximate the intensities. $\lambda_1^{(s+1)} \sim \pi ( \cdot | \lambda_2^{(s)}, \dots , \lambda_{10}^{(s)}, \beta^{(s)} ,x ) ,$ $ \lambda_2^{(s+1)} \sim \pi ( \cdot | \lambda_1^{(s+1)}, \lambda_3^{(s)} \dots , \lambda_{10}^{(s)}, \beta^{(s)} ,x ) ,$ $\vdots $ $\lambda_{10}^{(s+1)} \sim \pi ( \cdot | \lambda_1^{(s+1)}, \dots , \lambda_{9}^{(s+1)}, \beta^{(s)} ,x ),$ $\beta^{(s+1)} \sim \pi ( \cdot | \lambda_1^{(s+1)}, \dots , \lambda_{10}^{(s+1)} ,x ).$ I would like to prove that the resulting Markov Chain is uniformly ergodic. I would like to show that fro all $A$ in the sigma algebra on the state space and for all $x$ in the state space it holds that $$ P^2 (x, A) \geq \varepsilon \nu (A), $$ where $P$ is the transition kernel of the Markov chain, $\varepsilon >0$ and $\nu$ is a suitable probability measure. In particular, I would like to chose $\nu$ with the probability density proportional to $$ \int f_{ \beta | \lambda } ( \beta | \lambda ) \ f_{ \lambda | \beta } ( \lambda |u ) \ h(u) \ d u, $$ where $$ h ( \beta^{(1)} ) = \inf_{ \beta^{(0)} } f( \beta^{(1)} | \beta^{(0)} ), $$ where $\beta^{(0)}, \beta^{(1)}$ are the first two generated values in the sampler and $f( \beta^{(1)} | \beta^{(0)} )$ is the conditional probability density of $\beta^{(1)}$ given $\beta^{(0)}$ . If I show that $h$ is positive for all positive $\beta^{(1)}$ , then we can use such $\nu$ . We see that $$ p_{ \beta^{(1)} | \beta^{(0)} } ( w |y) = \int_{(0, \infty )^{10}} p_{ \beta^{(1)} | \lambda^{(1)} , \beta^{(0)} } ( w| z ,y ) \ p_{ \lambda^{(1)} | \beta^{(0)} } ( z | y) \ dz. $$ I would like to estimate the integral above from below using some positive function that does not depend on $y$ . We know that $$ p_{ \beta^{(1)} | \lambda^{(1)} , \beta^{(0)} } ( w| z ,y ) = \frac{ \Big( \sum_{i=1}^{10} z_i + \delta \Big)^{10 \alpha + \gamma} }{ \Gamma ( 10 \alpha + \gamma )} w^{ 10 \alpha + \gamma -1 } \exp \bigg(- \Big( \sum_{i=1}^{10} z_i + \delta \Big) w \bigg), \ w >0. $$ However, I don't know what does the second probability density in the integral look like. I know that if we integrate it over the region in the integral we get one. I also know that $\lambda$ are independent given some $\beta$ from the model, but does this mean that $\lambda^{(1)}$ are independent given $\beta^{(0)}$ . I would appreciate any help proving the property of $h$ I mentioned above or showing that the Markov chain is uniformly ergodic. Addendum Here I clarify what I mean by uniform ergodicity. Let us consider a Markov chain $\{ X_n \}$ with a stationary distribution $\pi$ and the total variation distance $\| \cdot \|_{\text{TV}}.$ We say that Markov chain $ \{ X_n \}$ is uniformly ergodic if $$ \| P^{n} ( x, \cdot ) - \pi ( \cdot ) \|_{\text{TV}} $$ converges to zero uniformly in $x$ for $n \to \infty$ . Update Using information mentioned by @Xi'an we see that $$p_{ \beta^{(1)} | \beta^{(0)} } (w|y) \geq C_1 \int_{(0, \infty )^{10}} \prod_{i=1}^{10} z_i^{\alpha + x_i -1} \ e^{-(y+t_i +w) z_i}\ dz, $$ where $$ C_1 = \frac{w^{10 \alpha + \gamma -1}}{\Gamma (10 \alpha + \gamma )} \prod_{i=1}^{10} \frac{ (y+t_i )^{\alpha + x_i} }{ \Gamma ( \alpha + x_i ) } \ e^{ - \delta w} \ \delta^{10 \alpha + \gamma} $$ and we use the fact that $$ \bigg( \sum_{i=1}^{10} z_i + \delta \bigg)^{10 \alpha + \gamma } \geq \delta^{10 \alpha + \gamma} \text{ for } z_i \geq 0. $$ Then we see that $$ p_{ \beta^{(1)} | \beta^{(0)} } (w|y) \geq C_1 \prod_{i=1}^{10} \frac{ \Gamma ( \alpha + x_i ) }{ (y+t_i + w)^{\alpha + x_i} }, $$ which is equal to $$ \frac{ w^{10 \alpha + \gamma -1} \ e^{- \delta w} \ \delta^{10 \alpha + \gamma} }{ \Gamma ( 10 \alpha + \gamma ) } \prod_{i=1}^{10} \bigg( \frac{y+t_i}{y+t_i+w} \bigg)^{\alpha + x_i}. $$ We see that $$ \forall w >0 : \ C (w) = \frac{ w^{10 \alpha + \gamma -1} \ e^{- \delta w} \ \delta^{10 \alpha + \gamma} }{ \Gamma ( 10 \alpha + \gamma ) } >0 $$ and we also see that $$ \prod_{i=1}^{10} \bigg( \frac{y+t_i}{y+t_i+w} \bigg)^{\alpha + x_i} \geq \prod_{i=1}^{10} \bigg( \frac{t_i}{t_i+w} \bigg)^{\alpha + x_i} > 0 .$$ Hence, $$ \forall w >0 : \ \inf_{ y>0} p_{ \beta^{(1)} | \beta^{(0)} } (w |y ) \geq \inf_{ y>0} C(w) \prod_{i=1}^{10} \bigg( \frac{t_i}{t_i+w} \bigg)^{\alpha + x_i} > 0 .$$
