[site]: crossvalidated
[post_id]: 159074
[parent_id]: 158839
[tags]: 
I see two ways how to deal with the situation like this. The very first approach is to use naive Bayesian classifier. In that case, you multiply the probabilities for yes and then for no and you normalize them. See this for details. Another situation assumes that you have some annotated data, say $(x_i,y_i)$ where $x_i$ are inputs to the original classifiers and $y_i$ are the known categories. In that case, it is possible to consider a meta-classification. Let $z_{i,j}$ be result of the $j$-th sub-classifier applied to $i$-th data record, using $x_i$ input data. You can consider new classification problem $(z_i,y_i)$ where $z_i=(z_{i,1},z_{i,2},\dots,z_{i,n})$ where $n$ is number of sub-classifiers. In that case we can speak about some meta-classification.
