[site]: crossvalidated
[post_id]: 328091
[parent_id]: 285745
[tags]: 
A convolutional network layer is a just a fully-connected layer where two things are true: certain connections are removed; means their weights are forced to be constant zero. So you can just ignore these connections/weights each of the non-zero weights is shared across multiple connections, ie across multiple pairs of input/output neurons When a weight is shared across connections, the gradient update for that weight is the sum of the gradients across all the connections that share the same weight. So then looking at your questions: Or are you supposed to update each weight (each value in the 2x2 filter) individually by its own gradient like in a normal artificial neural network ANN? Each weight is updated just like in a 'normal ann', by which case I interpret this to mean in a fully-connected, or 'dense', or 'linear' layer, right? However, as stated, the update applied to each of the weights will be the sum of the gradients from all the connections that share that weight. The 2x2 delta is backpropagated onto a 2x2 filter which creates a 3x3 grid of the change in weights If you are supposed to add up those 9 values and then add them to the original 2x2 filter at all positions it will just change each value in the 2x2 filter by the same amount and in the same directions. Is that the correct way to update weights in a CNN? The gradients 'flow back' to where they came. In the forward pass, each weight in the CNN 'kernel' will be used to calculate the output of multiple connections. In the backward pass, the gradients will 'flow backwards', along those exact same connections, onto the originating weights. In the worst case, rather than thinking of the CNN as some magical thing, you can, as stated, consider a CNN to be a standard fully-connected/linear layer, with many connections/weights forced to be zero, and the remaining weights shared across multiple connections. Then, you can use your existing knowledge for fully-connected / linear layers, to handle the CNN layer. For background, some conceptual aims/motivations of a CNN layer compared to a fully-connected layer are: enforces a prior on adjacency. Since pixels in images tend to have more correlation and relationships with adjacent pixels, so we keep only connections between adjacent input/output pixels, and remove the other connections partly to remove parameters, to avoid over-fitting, and partly because we want to enforce a prior on invariance to translation, we share the weights such that input pixels in one region will give identical outputs, no matter where in the image they are, but simply translated in the output image
