[site]: datascience
[post_id]: 51017
[parent_id]: 51007
[tags]: 
I have a way through which you can solve your problem. For it you will require a, A pretrained embedding generator. It can be Word2Vec or GloVe . Any of them could work. Next, we have a corpus of words which have higher frequencies. Suppose we have a set of 100 such words where the 1st word has the highest frequency. Now, we convert every word in this set to a vector using our pretrained word embedding. Hence you will have a set of vectors for the words from the corpus. Let's call it $z_i$ We have the word "data science". Get a vector for this too. Let's call it $x$ Measure the euclidean distance between the vector $x$ and $z_i$ . Or, you can measure cosine similarity betwee $x$ and $z_i$ . Both of above methods will produce a set of values which would show proximity of $x$ with values of $z_i$ . From these 100 values, we get the least 10 values and convert them to words again. These 10 words would have the highest similarity with the word "data science".
