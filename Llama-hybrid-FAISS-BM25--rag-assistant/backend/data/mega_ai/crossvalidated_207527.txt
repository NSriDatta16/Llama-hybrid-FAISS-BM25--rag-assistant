[site]: crossvalidated
[post_id]: 207527
[parent_id]: 
[tags]: 
Standard deviation vs Stardard error of sample mean

I am currently attempting to analyze some data, and it has been a few years since I have taken a statistics course, so I am a bit rusty at this stuff. I have a times series of 12 different sample types over the course of 3 years. For each year, there were 36 measurements for each sample type. There is a pool of 4000 independant samples, where each sample is one of the 12 types. For each measurement, I have the number of samples each type has, the mean value for the sample type, and the standard deviation of each sample type. The person who attempted to do this project before me did all of the calculations, and then at the end did the SD of each month over the 3 years. (He had monthly averages and then did the SD of Jan 2000, Jan 2001, and Jan 2002). As far as I remember this is not the correct way to do this. I am attempting to find the final SD.If I take the variance of each of the measurements, can I do the square root of the weighted average of variances to find the SD? I am just trying to add error bars to my final value, which is an overall average of all 3 years. In this case would SD be more appropriate, or would the SEM be better? Any help with this would be greatly appreciated!
