[site]: crossvalidated
[post_id]: 343178
[parent_id]: 343177
[tags]: 
First I should point out that: In your significance-testing approach, you followed up a negative result with a different model that gave you another chance to get a positive result. Such a strategy increases your project-wise type-I error rate. Significance-testing requires choosing your analytic strategy in advance for the $p$-values to be correct. You're putting a lot of faith in the results of Study 1 by translating your findings from that sample so directly into priors. Remember, a prior is not just a reflection of past findings. It needs to encode the entirety of your preexisting beliefs, including your beliefs before the earlier findings. If you admit that Study 1 involved sampling error as well as other kinds of less tractiable uncertainty, such as model uncertainty, you should be using a more conservative prior. The second of these points to an important departure you have made from Bayesian convention. You didn't set a prior first and then fit both models in Bayesian fashion. You fit one model in a non-Bayesian fashion and then used that for priors for the other model. If you used the conventional approach, you wouldn't see the dependence on order that you saw here.
