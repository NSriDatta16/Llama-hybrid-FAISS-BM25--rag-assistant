[site]: datascience
[post_id]: 67065
[parent_id]: 67061
[tags]: 
By performing GridSearch I understand you want to say searching for the best hyperparameters. For sake of simplicity, let say that you want to fit a linear regression with a penalty (lasso/ridge) with 1 feature and with 100 features. The hyperparameter that you are looking for is the $\lambda$ penalty. It is easy to see that with 1 feature your model might need some parameter, it could be that the feature has a perfect distribution and it doesnÂ´t need any penalty at all. But when we go for 100 features there is some noise and you might need penalties to make sure that your model is generalizing well. So $\lambda$ will be different With this example, my idea is to show that hyper-parametrization is a specific problem for each task. For Random Forest, it would be the same. Different features will require different parameters so yes, you have to do a GridSearchCV with every subset of features if you want to achieve optimality
