[site]: crossvalidated
[post_id]: 190276
[parent_id]: 190270
[tags]: 
Paradoxically, a large spread in the independent variables makes a regression slope easier to estimate. Here's why. Ordinary least squares regression (OLS) assumes constant error variability, so I'll assume that in my explanation. Given the squared errors are on average $\sigma^2$, it will be very difficult to detect a difference of $0.01\sigma$ units between two different conditions. It will be very easy to detect a difference of $100\sigma$ units. Imagine the true underlying slope is 1/10. Then, x-values separated by a distance of $0.1\sigma$ will differ in their responses by $1/10 \times 0.1\sigma = 0.01\sigma$. You could easily mistake that by a factor of 2 or even end up with a negative number. When you went to estimate the slope, it would be off by the same factor (or sign flip). If the x values had been placed farther apart, at an interval of length $1000\sigma$, then the gap in responses will be about $1/10 \times 1000\sigma = 100\sigma$. You might estimate it at 99.5 or 100.5 if you only have a few samples. The slope estimate ends up erring only by half a percent of its true value. The second setup, with lots of variability in the independent variable, makes it much easier to peg the coefficient.
