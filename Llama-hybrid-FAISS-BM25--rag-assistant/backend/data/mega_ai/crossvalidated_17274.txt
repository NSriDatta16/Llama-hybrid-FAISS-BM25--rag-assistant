[site]: crossvalidated
[post_id]: 17274
[parent_id]: 
[tags]: 
Is sequential Bayesian updating an option when using MCMC?

I have an implementation of the Griddy Gibbs sampler, but my observations on which I'm conditioning model parameters are too many in number, thus the likelihood underflows quickly, even with a log transformation. Sequential updating holds if one is using conjugate priors, since the posterior can be found analytically, but would this be the same with MCMC, and in particular my sampler? What I'm not sure about is the loss that comes from the candidate distribution, or approximating the full conditional, which is my case. Every chunk of observation is bound to have some error, and I'm not sure if chaining chunks of observations can be trusted. In general, how can one implement a MCMC algorithm of this sort (non conjugate full conditionals for Gibbs) for say, tens of millions of observations from a model?
