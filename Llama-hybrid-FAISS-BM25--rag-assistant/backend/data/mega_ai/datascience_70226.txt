[site]: datascience
[post_id]: 70226
[parent_id]: 70222
[tags]: 
The past token internal states are reused both in GPT-2 and any other Transformer decoder. For example, in fairseq's implementation of the transformer, these previous states are received in TransformerDecoder.forward in parameter incremental_state (see the source code ). Remember that there is a mask in the self-attention blocks in the decoder that prevents the predictions and intermediate states to attend to positions equal to or greater than the current one, which means that the internal state won't change even if you recomputed them at every decoding step. Update: of course it is technically possible to recompute past tokens attending future tokens, but then, what do you do with the future tokens after you re-compute the past ones? Do you recompute them? This is a totally different beast, which has been studied to some degree and is referred to as "iterative refinement". An example can be found in article "Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement" . AFAIK, this kind of approach has not studied in autoregressive models, only in non-autoregressive ones.
