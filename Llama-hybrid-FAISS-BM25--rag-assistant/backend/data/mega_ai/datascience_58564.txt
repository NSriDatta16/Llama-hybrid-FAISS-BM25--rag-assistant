[site]: datascience
[post_id]: 58564
[parent_id]: 58560
[tags]: 
There exists multiple ways, each consisting on summarizing the embeddings of each word from a document. The most common ones when using linear models like Logistic Regression are: max of word vectors mean of word vectors concatenation of word vectors (but implies a fixed length) However, it is important that using such approaches often results in : either equal or worse performance than using bag-of-words less interpretability (if applicable) The reason is the loss of information (especially for long documents). When using word embeddings (and so called word2vec), the best choice is often to use models that can handle a second dimension in your input. For that, recurrent and convolutional neural networks have proven to be effective. The former operates at the word level by recurrently feeding the word embeddings, the latter operates at the whole document level (represented by a matrix of $n_{words}\times d_{embedding}$ as you mentioned) by locally convoluting filters with windows of the document.
