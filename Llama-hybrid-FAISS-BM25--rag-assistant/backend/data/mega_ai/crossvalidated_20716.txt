[site]: crossvalidated
[post_id]: 20716
[parent_id]: 20714
[tags]: 
First of all, let's make sure what you mean by overfitting. I assume you mean that the algorithm has learned too many of the nuances of the training data and will not perform well when you apply it to new data it hasn't seen before (from a similar population). This would also be known as poor generalization. All machine learning algorithms, boosting included, can overfit. Of course, standard multivariate linear regression is guaranteed to overfit due to Stein's phenomena. If you care about overfitting and want to combat this, you need to make sure and "regularize" any algorithm that you apply. For regular regression, the simplest and often best method of regularization would be ridging. For boosting specifically: to combat overfitting is usually as simple as using cross validation to determine how many boosting steps to take. On a more subtle level you probably want to make sure and use a small enough learning rate. Really small learning rates can take forever to overfit (take a ton of steps) so it's harder to screw them up. For pure accuracy though, you want to use as small of learning rate as you can and push the boosting steps right up until it does start to overfit, so if you really care you need to find the smallest learning rate that you can feasibly "bottom out". I believe gbm in R also bags a sample for each step, although I'm not sure that actually combats overfitting as much as it does spread the learning across the training data. So, to your specific question, we can't really know if 400 ensembles is too many. In fact, the only way you really can is via Cross Validation or a hold out set (Or a kind of OOB estimate if your boosting algorithm does do the bagging at each step). If your base learner in each step is too strong or the learning rate is too high, then those 400 ensembles could easily be a drastic overfit. With no other data than a 21% to 70% gain, I would lean towards it overfitting.
