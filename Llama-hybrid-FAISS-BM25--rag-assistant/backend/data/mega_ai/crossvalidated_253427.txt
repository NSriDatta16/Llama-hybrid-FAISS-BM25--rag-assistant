[site]: crossvalidated
[post_id]: 253427
[parent_id]: 
[tags]: 
One-vs-All Logistic Regression Probability Scores either close to 0 or 1

I am trying to implement one-vs-all logistic regression by myself on a small dataset that I have already divided into 8 folds (I do not want to use nnet::multinom ). I have 14 classes and as many as about 90 features. I have create 14 different logistic regression models for each of the classes. The probabilities I am getting in the end are either so close to 0 or so close to 1, therefore, making it impossible to specify a 'threshold' for classification. For example, in a normal binary logistic regression task where you only have 0 or 1 in the vector you're trying to predict, after using the predict function on your test-set, you can then write something like this: fit.res 0.5, 1, 0) So here you choose between 0 or 1 by a threshold of 0.5. But in the case of my task, I have 14 different classifiers where each classifier gives the probability of for example "1" or "NOT.1", another classifier gives the probability of being "4" or "NOT.5". Naturally I would use the code above to set a threshold and based on the given probabilities choose between the classes for each classifier. However, probabilities I am getting are either something like 0.99999999 or 0.0000002, therefore, no threshold for classification can be chosen. Am I doing something wrong? At the end my misclassification error and accuracy is really really bad.. Training dataset; here Test dataset; here predictions At the end I also get a lot of warnings. glm.fit: algorithm did not converge glm.fit: fitted probabilities numerically 0 or 1 occurred Probabilities of NOT-being-in-a-certain-class as returned by the classifiers for the 1st fold (column names correspond to class names) :
