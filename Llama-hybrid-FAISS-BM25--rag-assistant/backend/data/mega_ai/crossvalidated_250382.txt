[site]: crossvalidated
[post_id]: 250382
[parent_id]: 
[tags]: 
Elaboration on weight change for output layer in neural network

I have trouble understanding the section regarding backpropagation in Murphy's Machine Learning book. He derives the weight change for the output layer (16.68) as follows: $\nabla_{\mathbf{w}_k} J_n = \frac{\partial J_n}{\partial b_{nk}} \nabla_{\mathbf{w}_k} b_{nk} = \frac{\partial J_n}{\partial b_{nk}} \mathbf{z}_n$ $b_{nk} = \sum_j w_{kj} g(a_{nj})$ (16.73) $b_{nk}$ normally is given as the sum of all weighted outputs from the prior layer. Here, however it is given as follows: $b_{nk} = \mathbf{w}_k^T \mathbf{z}_n$ I don't quite understand where that $\mathbf{z}_n$ comes from and how it fits in there. What am I missing?
