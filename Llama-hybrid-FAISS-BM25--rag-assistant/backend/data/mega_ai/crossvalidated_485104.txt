[site]: crossvalidated
[post_id]: 485104
[parent_id]: 
[tags]: 
How to retrieve regularisation term from xgboost

To evaluate the bias-variance tradeoff, I prefer to plot the prediction error vs. the complexity of the trained estimator . In case of xgboost the documentation states that the complexity is taken as the regularisation term of the objective function . So in principle it should be easy to generate such a plot. However, I can't figure out how to retrieve the calculated regularisation term for the trained model. I am using the Python scikit-learn API. Is this value available in the library, or should it be recomputed after training?
