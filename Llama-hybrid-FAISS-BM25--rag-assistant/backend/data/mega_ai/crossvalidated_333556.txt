[site]: crossvalidated
[post_id]: 333556
[parent_id]: 
[tags]: 
How to reduce type I and II error for determining bias of loaded dice for reduced legal costs?

Suppose you are running a casino and that you are responsible for ensuring that all the dice are fair to avoid lawsuits. In order to do this, you might take a mean of 1000 throws of each die and perform a hypothesis test [using the central limit theorem, CLT] to see whether they are likely biased. The average cost of a lawsuit is $£240,000$, whilst the cost of a die is $£3$, so in order to minimise costs would you aim to have $240000 \,\beta = 3\,\alpha$, where $\beta$ is Type II error, A.K.A., false negative rate, and $\alpha$ is the significance level of the hypothesis test (and also the probability of a Type I error, A.K.A., false positive rate)? Now, in order to find the optimal $\alpha$ value, one must know the value of $\beta$, something that can only be calculated from die outcomes (which is what we are testing for in the first place), so the optimal solution cannot be found. That being said, however, I'm sure scenarios like this arise rather often, so how are they usually dealt with? tldr: How would you find a threshold value for the mean of a die above (or below) which it should be considered biased whilst also keeping $240000\,\beta \text{ error} \approx 3\,\alpha \text{ error}$? Edit: It seems my choice of example is rather poor, as a die shouldn't even be tested for fairness with a location test. That being said, however, my question more concerns the tradeoff between Type I and Type II error than the fact that a die is being used.
