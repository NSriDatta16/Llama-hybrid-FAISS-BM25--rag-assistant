[site]: datascience
[post_id]: 10997
[parent_id]: 
[tags]: 
Need help understanding xgboost's approximate split points proposal

background: in xgboost the $t$ iteration tries to fit a tree $f_t$ over all $n$ examples which minimizes the following objective: $$\sum_{i=1}^n[g_if_t(x_i) + \frac{1}{2}h_if_t^2(x_i)]$$ where $g_i, h_i$ are first order and second order derivatives over our previous best estimation $\hat{y}$ (from iteration $t-1$): $g_i=d_{\hat{y}}l(y_i, \hat{y}) $ $h_i=d^2_{\hat{y}}l(y_i, \hat{y}) $ and $l$ is our loss function. The question (finally): When building $f_t$ and considering a specific feature $k$ in a specific split, they use the following heuristic to assess only some split candidates: They sort all examples by their $x_k$, pass over the sorted list and sum their second derivative $h_i$. They consider a split candidate only when the sum changes more than $\epsilon$. Why is that??? The explanation they give eludes me: They claim we can rewrite the previous equation like so: $$\sum_{i=1}^n\frac{1}{2}h_i[f_t(x_i) - g_i/h_i]^2 + constant$$ and I fail to follow the algebra - can you show why is it equal? And then they claim that "this is exactly weighted squared loss with labels $gi/hi$ and weights $h_i$" - a statement I agree with, but I don't understand how does it relate to the split candidate algorithm they are using... Thanks and sorry if this is too long for this forum.
