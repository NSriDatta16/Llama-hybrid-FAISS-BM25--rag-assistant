[site]: datascience
[post_id]: 113878
[parent_id]: 63325
[tags]: 
I think the answers you got are technically correct, but don't address the big picture. In the data science world, cosine similarity is mainly used for documents which have been encoded by an embedding. Documents could be anything from a single sentence or a tweet, to a paper with dozens of pages of text. Embeddings include things like doc2vec, BERT, and similar, and they try to reflect some level of semantic knowledge into their encoding. That is, words that tend to have similar meanings will end up close together in the high-dimensional embedding space. And documents with similar contexts will also end up close together in this space. So, to answer your question: Levenshtein distance has no knowledge of semantics, it's simply an edit distance and nothing more. And in general you only use it if you have no other choice. For example, compare: 3000 N Main Street 3000 N Maan Street 3001 N Main Street 9000 N Main Street 3000 S Main Street All of the lines after the first are an edit distance of 1 from the first line. That is, one character is changed. As you can see, which character is changed -- in this context -- can make a HUGE difference so using Levenshtein on the entire street address is useless. So if "similar" means "would type nearly the same text" it might be useful, though it's hard to apply well and if you're looking for things like "fat finger" mistakes (typos) you might want to use a different edit distance that accounts for swaps of two adjacent characters. Cosine similarity (where "similarity" is the inverse of "distance") is in general used on embeddings. The Bag of Words approach that the accepted answer uses for pedagogical purposes is clever but I've never seen or heard of it before now. It does allow you to use Cosine distance as an approximation of editing distance but it's really not used that way in practice. In practice, you're using Cosine distance on an embedding that encodes semantic information: you're looking for words or documents (collections of words) that are using similar words in similar contexts. This would not be useful for the street address example, above, but you could imagine it being very useful on news articles or scientific papers, etc. You could use Euclidean distance in the embedding space -- comparing the vector for each document directly -- but there can be issues with magnitude. And cosine similarity measures only the relative directions of the documents, not their magnitude, which is in general more useful and more what you expect when you want to compare two documents in terms of their "topic" or "meaning", etc. So if "similar" means "talking about something similar or in a similar way" than you'll probably end up using a Cosine similarity measure with an embedding. The accepted answer is technically creating an embedding, but in general I think the term "embedding" in data science is referring to something like doc2vec, BERT, GLoVE, etc, which reflect co-occurrences and other factors from which a semantic-like quality emerges.
