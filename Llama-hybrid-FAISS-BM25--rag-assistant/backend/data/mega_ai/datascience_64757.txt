[site]: datascience
[post_id]: 64757
[parent_id]: 64756
[tags]: 
I'd start here . Most basic idea is to run statistical tests to see how target variable depends on each feature. These include tests like chi-square or ANOVA . Tree-based models can also output feature importance. Check this post . There's plenty of posts on kaggle with code. Might be worth checking those: https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering https://www.kaggle.com/rejasupotaro/effective-feature-engineering https://www.kaggle.com/willkoehrsen/automated-feature-engineering-tutorial As your data set isn't so drastically large, you could push grid search and check how your model behaves for different factors of PCA . It's hard to tell a priori whether you should drop some features. I guess trying each combination of 30 features is completely out of scope, though you might try dropping most redundant ones. As your data contains categorical features, it might be good idea to give catboost a try. They claim it handles categorical features better than other gradient boosters. Just keep in mind, that default number of estimators is 10 times of that in xgboost. You might lower it for experiments. First, I'd create base model with all the features. Now comes the question: which method to choose? Gradient boosters poses ability of learning the feature importance, those redundant ones will get little weight and you might not see much of an improvement, when dropping features. You might get more insight using more vanilla methods, but in the end you'll be certainly deploying gradient boosting to production, so I don't see much sense in it. I'd stick with xgboost or catboost and perform experiments using same parameters. Please keep in mind: though some features might be highly redundant, they may still contribute some knowledge to your model.
