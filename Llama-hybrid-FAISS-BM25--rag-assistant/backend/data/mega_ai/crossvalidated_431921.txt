[site]: crossvalidated
[post_id]: 431921
[parent_id]: 
[tags]: 
What does this proof mean?

I am self-studying Shai-Shawrts and Shai Ben-David's Understanding Machine Learning book. In chapter 4(Learning via uniform convergence) I have encountered this proof. Can someone explain me the proof? As I understand the first term of the proof is the erm for the learner that was chosen from the training sample and tested on the distribution D. It is smaller than the erm that was chosen from the training sample and tested on the sample size S plus the epsilon over 2. And then I am just getting confused by the notation.
