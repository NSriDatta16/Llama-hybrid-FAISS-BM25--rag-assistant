[site]: crossvalidated
[post_id]: 564289
[parent_id]: 563203
[tags]: 
This is more easily done using your raw data: the number of correct and incorrect classifications, for each version of the test set, for each classifier. A simple approach would be to run four chi-squared tests on the contingency tables for each classifier, e.g. | | Test | Obf | | N Correct | a | b | | N Incorrect| c | d | which would give you a p-value for each classifier. A more complete approach would be to use multilevel logistic regression, with estimates allowed to vary between classifiers are random effects, e.g., (using the lme4 package for R): model = glmer(accuracy ~ 1 + set + (1 + set | classifier), data = your_data, family = binomial)` where your data has one row per classification case, and set is either 'test' or 'obf' .
