[site]: crossvalidated
[post_id]: 207872
[parent_id]: 32318
[tags]: 
(note that I'm focusing on standard error of the mean, which I believe the questioner was as well, but you can generate a standard error for any sample statistic) The standard error is related to the standard deviation but they are not the same thing and increasing sample size does not make them closer together. Rather, it makes them farther apart. The standard deviation of the sample becomes closer to the population standard deviation as sample size increases but not the standard error. Sometimes the terminology around this is a bit thick to get through. When you gather a sample and calculate the standard deviation of that sample, as the sample grows in size the estimate of the standard deviation gets more and more accurate. It seems from your question that was what you were thinking about. But also consider that the mean of the sample tends to be closer to the population mean on average. That's critical for understanding the standard error. The standard error is about what would happen if you got multiple samples of a given size. If you take a sample of 10 you can get some estimate of the mean. Then you take another sample of 10 and new mean estimate, and so on. The standard deviation of the means of those samples is the standard error. Given that you posed your question you can probably see now that if the N is high then the standard error is smaller because the means of samples will be less likely to deviate much from the true value. To some that sounds kind of miraculous given that you've calculated this from one sample. So, what you could do is bootstrap a standard error through simulation to demonstrate the relationship. In R that would look like: # the size of a sample n You'll find that those last two commands generate the same number (approximately). You can vary the n, m, and s values and they'll always come out pretty close to each other.
