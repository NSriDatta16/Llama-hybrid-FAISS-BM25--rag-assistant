[site]: crossvalidated
[post_id]: 330856
[parent_id]: 328550
[tags]: 
@Satwik, @gung and @Tim have already provided a lot of detail, but I'll try and add a small example of how the case of underlying factors may cause such an effect. A Key Principle: Bias Sensitivity/Specificity and ALL statistical tests share the same caveat: it applies only to repeating the same sampling procedure as before in an unbiased manner. Hospitals are functioning organisations designed to perform biased sampling, using admissions policies to filter the general population down into those requiring admission and treatment. This is very antithesis of scientific procedure. If you want to know how a test performs in different populations it needs to be tested in different populations. The latent effect: Correlation It is rare (or impossible in the real world if you want to be strict) for a diagnostic to be independent/orthogonal to all other risk factors for a disease, so there is some degree of correlation. If the screen for admission to hospital is positively correlated with the diagnostic then what you will find is that people who pass the admissions test are favourably predisposed for positive results by the diagnostic, proportional to the correlation. Thus true positives are enriched and false negatives are reduced by amounts proportional to the correlation. This then makes sensitivity appear larger. The explanation of the phenomenon An observation that sensitivity may be higher in a hospital based context is therefore not unrealistic. In fact if the admissions policy is well thought out and fit for purpose one would expect this to occur. It is not evidence of a breakdown in the assumption that sensitivity and specificity are prevalence independent, rather it is evidence of a biased sampling based on the hospital admission policy. Which, given a hospital is there to treat people and not to do scientific experiments, is definitely a good thing. But it does give scientists a headache.
