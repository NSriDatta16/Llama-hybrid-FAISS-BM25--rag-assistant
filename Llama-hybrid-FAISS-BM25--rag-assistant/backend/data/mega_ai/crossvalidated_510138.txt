[site]: crossvalidated
[post_id]: 510138
[parent_id]: 510052
[tags]: 
SVM is interesting if you have a kernel in mind that you know is appropriate, or a domain-specific kernel that would be difficult to express in a differentiable way (a common example might be a string-similarity space for DNA sequences). But what if you have no idea what kind of kernel you should use? What if your data is a wide collection of values and you're not even sure in advance which ones have relevance? You could spend human researcher time doing feature engineering, or you could try automatic kernel search methods, which are pretty expensive, but might even come up with something that could be considered interpretable, on a good day. Or you could dump the whole thing into a DNN and train. What a neural net does through backprop and gradient descent could very well be considered to be learning a kernel , only instead of having a nice functional form, it's composed (literally) of a large number of applications of a basic nonlinearity, with some additions and multiplications thrown in. The next-to-last layer of a typical classification network is the result of this — it's a projection into a space with one dimension per neuron in that layer, where the categories are well-separated, and then the final result (ignoring the softmax, which is really just a kind of normalization) is an affine map of that space into one where the categories are axis-aligned, so the surfaces of separation come for free with the geometry (but we could send them backwards onto that second-to-last layer if we wanted). The DNN classifier accomplishes something very similar to an SVM classifier, only it does it in a "dumb" way using gradient descent and repetition of simple differentiable units. But sometimes in computation, "dumb" has its advantages. Ease of application to GPUs (which love applying the same simple operation to a large number of data points in parallel) is one. The ability of SGD and minibatch gradient descent to scale up to very large numbers of examples with minimal loss of efficiency is another. Of course, it comes with its own downsides. If you make the wrong choices of NN architecture, initial weights, optimization method, learning rate, batch size, etc. then the stochastic training process might completely fail to converge, or take a million years to do so — whereas SVM training is basically deterministic. (Forgive an amateur blundering around, oversimplifying, and abusing terminology; these are my personal experiences after 15 years or so of playing with this stuff on an occasional hobby level).
