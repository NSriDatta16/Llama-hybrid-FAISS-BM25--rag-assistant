[site]: crossvalidated
[post_id]: 245737
[parent_id]: 
[tags]: 
Mathematical justification for using recurrent neural networks over feed-forward networks

I was wondering and trying to understand if there exists any mathematical reason behind the superiority of RNNs over Feed-forward networks when dealing with sequential data. For example when modeling time series, HMMs, etc. Assume that the length of the sequence is fixed, but very large. Intuitively it's clear that RNNs are able to remember the relevant information till the current time instant $t$ in the state variable $h_{t-1}$ and use that to update their state to $h_t$ based on the current input $x_t$. For example, if we want to model $Y_t=X_t+X_{t-1}+\ldots+X_0$ using a Feed-forward network, we should have an input node for each $X_i$ and sum them to obtain $Y_t$ whereas with RNN we just need a single input node for $X_t$ and the state remembers the sum $X_{t-1}+\ldots+X_0$. Though the above example is pretty elementary, it highlights that the complexity of RNN (# of nodes, depth) is much less compared to that of a Feed-forward one. Can anyone provide an example of families of functions which can't be captured by Feed-forward but can be well approximated by RNNs? Any references directing to the same is also appreciated.
