[site]: crossvalidated
[post_id]: 565907
[parent_id]: 565888
[tags]: 
This question is a very good device for demonstrating the emptiness of the Neyman–Pearsonian all or none decision approach when applied in most real-life circumstances. It also illustrates the importance of maintaining a clear distinction between the observed p-value and the designed $\alpha$ of the test. (Expressing the $\alpha$ as a threshold p-value is a lazy and potentially confusing convention that has done a lot of harm.) 1. How many type I errors have I made over the years? That sounds like a very straightforward question. Particularly so given the simple error rate accounting implied by the approach. But not so simple in reality. Your rate of long term errors is unknowable even if we were to assume (unrealistically) that all of your hypothesis tests are well matched with the assumptions of the statistical models and that all of your samples are truly random or randomised, and that you stupidly used a fixed $\alpha$ for all of your tests, and perhaps some other assumptions along those lines. The simple fact is that we do not know what fraction of the tested null hypotheses were true and therefore we cannot even give a ball-park answer to the question. Allowing all of those unrealistic assumptions we can say what fraction of the true null hypotheses would have been erroneously discarded ‘in the long run’, but that does not answer the question. 2. I've run a lot of A/B tests in my career. I was just wondering: how many Type I errors have I made thus far, where I erroneously rejected the null hypothesis? The trivial answer is that you’ll never know, but there is a lot of intellectual goodness in why and how you’ll never know. And even more goodness in considerations of why it is not such a helpful question. 3. One perspective says: none; no null hypothesis is ever "true" so no rejection is ever wrong. Okay fine, then how many times have I got the wrong direction on effect size, choosing the wrong winner? If I reject at p That particular perspective is nothing more than a polemical device for casting shade on the utility of significance testing and hypothesis testing approaches. It’s not true in general. ESP trials are a striking example where the null hypothesis is true, and there are many real-world situations where the intervention does nothing to the parameter of interest in the statistical model. We need to be more open about it being the null hypothesis within the statistical model that is being tested, not the scientific hypothesis of interest. Even where it might be true that a particular null hypothesis cannot be exactly true, for example because nothing is exactly zero, that complaint is not an important consideration, for reasons that I hope will be made clear below. 4. The tests where I didn't reject the null can't possibly be Type I errors, so they are irrelevant. They might be irrelevant within the commonplace statistical framework where type I errors are vastly more important than other considerations, but they are rarely irrelevant in the real world. They often mean that the theory, hypotheses and experimental designs need revision or some sort of reconsideration. Often they mean that another study should be done and (or) more data gathered. 5. If the other tests all miraculously had the same p-value, p, and there are N such tests, then I expect Np errors. E.g. p=0.05. It is often misleading to bring up magical conditions like this, and this is a perfect example of that misleadingness. This question is just a variant of the original question and suffers from all of the same unanswerability, but it is additionally complicated by increased specificity that comes with the magical sameness and specification of the familiar p=0.05. That sameness and p=0.05 do nothing to make the question easier. 6. If I had N1 tests where my p-value was p1, and N2 with p2, where N1+N2=N, then I'd expect N1p1+N2p2 errors, and so forth. Same as above, but even worse because the N1 p1, N2 p2 serve to distract and confuse. What are the thresholds used for rejection in those cases? Are they p1 and p2, are they all the same, or are they variable? 7. So the lower my p-values are in all these tests the better my record. In a sense that might be true. The reject/don’t reject decision of a Neyman–Pearsonian hypothesis test makes no distinction between results with a p-value just below the pre-assigned threshold for rejection of the null and results where the p-value is far lower. Nonetheless, if you regularly obtain very small p-values then you regularly find strong evidence against the null within the statistical model, and so you are regularly dealing with cases where the null is probably not true. You will then have fewer false positive inferences (type I errors) because you have fewer cases where the null is true. 8. P.S. this is a sloppy proof but I have new parent brain. I feel like a more elegant proof would use the U(0, 1) distribution of the p values under H0, but I can't quite see it. (I can say that new parent brain is a thing, and I will reassure you that it passes after a couple of decades.) In the cases where the null is true (and the assumptions I mentioned in response to q1 are all true) then the rate of false positive errors from a Neyman–Pearsonian hypothesis test method is equal to the alpha because of that uniform distribution. If you routinely use a lower alpha (which may be what you mean by saying “lower p-values”) then you routinely have a lower long run type I error rate. This seems like a decent thing for me to track so I know how worried I should be about some past decision coming back to haunt me. Sort of like a "statistical debt" metric, analogous to tech debt. The best way to reduce the number of errors is to make decisions on the basis of all of the information available. It will require withholding of decisions when the information is insufficient or ambiguous, the revisiting of past decisions in light of new information, and above all, a response to evidence that is graded on the basis of its strength and reliability. I cannot say which statistical methods that are best adapted to your own use-case, but it is unlikely that the all or none decisions from p $\alpha$ . If you are not clear on why I keep writing "Neyman–Pearsonian" then you probably need to improve your understanding of the distinction between significance tests that yield p-values (as indices of the strength of evidence agains the null according to the statistical model) and hypothesis tests that yield a decision to reject the null hypothesis (or not) on the basis of whether the observed test statistic falls within a pre-defined critical range. There are many questions on this site about that, but I recommend a more extensive review of the topic, including its history, via a couple of my papers. Bad statistical practice in pharmacology (and other basic biomedical disciplines): you probably don't know P https://bpspubs.onlinelibrary.wiley.com/doi/10.1111/j.1476-5381.2012.01931.x A reckless guide to P-values: local evidence and global errors https://link.springer.com/chapter/10.1007/164_2019_286#enumeration
