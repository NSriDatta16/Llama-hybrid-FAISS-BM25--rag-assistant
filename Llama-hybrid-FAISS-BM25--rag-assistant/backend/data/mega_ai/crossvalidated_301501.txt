[site]: crossvalidated
[post_id]: 301501
[parent_id]: 267027
[tags]: 
have a look at this: A tutorial on Stochastic Approximation Algorithms for Training RBM/Deep Belief Nets (DBN) It gives a very nice explanation of PCD vs. CD as well as the actual algorithm (so you can compare). Furthermore, it tells you how PCD is related to the Rao Blackwellisation process and Robbins Monro stochastic update. You can also check the original paper on PCD training of RBM In a nutshell, when you sample from the full RBM model (joint visible+hidden), you can either start from a new data point and perform CD-1 to update your weights/parameters or you can persist the previous state of your chain and use that in the next update. This in turn means you'll have n markov chains, where n is the number of data points in your dataset, or minibatch (depending on how you train it). Then you can average over your chain. Remember that the learning rate has to be smaller for PCD because you don't want to move too much by using only one point in the dataset.
