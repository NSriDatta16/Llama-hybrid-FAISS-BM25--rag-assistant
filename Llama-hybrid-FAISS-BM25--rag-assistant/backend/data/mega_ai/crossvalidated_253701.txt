[site]: crossvalidated
[post_id]: 253701
[parent_id]: 141462
[tags]: 
I think the question while somewhat trivial and "programmatic" at first read touches upon two main issues that very important in modern Statistics: reproducibility of results and non-deterministic algorithms. The reason for the different results is that the two procedure are trained using different random seeds. Random forests uses a random subset from the full-dataset's variables as candidates at each split (that's the mtry argument and relates to the random subspace method ) as well as bags (bootstrap aggregates) the original dataset to decrease the variance of the model. These two internal random sampling procedures thought are not deterministic between different runs of the algorithm. The random order which the sampling is done is controlled by the random seeds used. If the same seeds were used, one would get the exact same results in both cases where the randomForest routine is called; both internally in caret::train as well as externally when fitting a random forest manually. I attach a simple code snippet to show-case this. Please note that I use a very small number of trees (argument: ntree ) to keep training fast, it should be generally much larger. library(caret) set.seed(321) trainData At this point both the caret.train object fitRFcaret as well as the manually defined randomForest object fitRFmanual have been trained using the same data but importantly using the same random seeds when fitting their final model. As such when we will try to predict using these objects and because we do no preprocessing of our data we will get the same exact answers. all.equal(current = as.vector(predict(fitRFcaret, testData)), target = as.vector(predict(fitRFmanual, testData))) # TRUE Just to clarify this later point a bit further: predict(xx$finalModel, testData) and predict(xx, testData) will be different if one sets the preProcess option when using train . On the other hand, when using the finalModel directly it is equivalent using the predict function from the model fitted ( predict.randomForest here) instead of predict.train ; no pre-proessing takes place. Obviously in the scenario outlined in the original question where no pre-processing is done the results will be the same when using the finalModel , the manually fitted randomForest object or the caret.train object. all.equal(current = as.vector(predict(fitRFcaret$finalModel, testData)), target = as.vector(predict(fitRFmanual, testData))) # TRUE all.equal(current = as.vector(predict(fitRFcaret$finalModel, testData)), target = as.vector(predict(fitRFcaret, testData))) # TRUE I would strongly suggest that you always set the random seed used by R, MATLAB or any other program used. Otherwise, you cannot check the reproducibility of results (which OK, it might not be the end of the world) nor exclude a bug or external factor affecting the performance of a modelling procedure (which yeah, it kind of sucks). A lot of the leading ML algorithms (eg. gradient boosting, random forests, extreme neural networks) do employ certain internal resampling procedures during their training phases, setting the random seed states prior (or sometimes even within) their training phase can be important.
