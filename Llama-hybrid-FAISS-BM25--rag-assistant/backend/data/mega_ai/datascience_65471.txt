[site]: datascience
[post_id]: 65471
[parent_id]: 
[tags]: 
Validation loss much higher than training loss

I am training a CNN on some text data. The sentences are padded and embedded and fed to a CNN. The model architecture is: model = Sequential() model.add(Embedding(max_features, embedding_dims, input_length=maxlen)) model.add(Conv1D(128, 5, activation='relu')) model.add(GlobalMaxPooling1D()) model.add(Dense(50, activation = 'relu')) model.add(BatchNormalization()) model.add(Dense(50, activation = 'relu')) model.add(BatchNormalization()) model.add(Dense(25, activation = 'relu')) #model.add(Dropout(0.2)) model.add(BatchNormalization()) model.add(Dense(1, activation='sigmoid')) model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) Any help would be appreciated.
