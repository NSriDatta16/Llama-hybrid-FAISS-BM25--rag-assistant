[site]: crossvalidated
[post_id]: 207248
[parent_id]: 137917
[tags]: 
Although the two measures have quite different logic, the F1 score is actually a special case of the specific agreement coefficient . Specifically, F1 is equivalent to specific agreement for the positive class when there are two raters, two categories, and no missing data. The "opposing F1 score" that you propose is equivalent to specific agreement for the negative class in this same scenario. I encourage you to investigate the specific agreement formulation, as it is more flexible than the F1 score (in that it can handle multiple raters, multiple categories, and missing data) and has an nice interpretation in addition to the harmonic mean of precision and recall: specific agreement is equal to the probability that a randomly selected rater will assign a given item to a category given that another randomly selected rater has also assigned that item to that category.
