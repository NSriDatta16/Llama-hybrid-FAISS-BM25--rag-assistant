[site]: crossvalidated
[post_id]: 247114
[parent_id]: 203833
[tags]: 
I think you might find your answer in one of the two following videos. The first video shows the derivation of the free energy. The second video shows the derivation of the gradient. I believe both videos assume binary units. Restricted Boltzmann machine - free energy https://youtu.be/e0Ts_7Y6hZU?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH Restricted Boltzmann machine - contrastive divergence (parameter update) https://youtu.be/wMb7cads0go?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH EDIT: To answer your second question about where does the $exp()$ term come from, after conferring with some colleagues, I believe [2] actually contains a typo and Eqn. 27.114 should read : $$F(v)=\sum_{h}\exp(E(v,h))=\sum_{h}\exp(\sum_{r=1}^{R}\sum_{k=1}^{K}v_{r}h_{k}W_{rk})$$ The exp() term doesn't just appear, it seems to have accidentally been left out. That correction makes the derivation consistent with [1] Eqn. 18 $$FreeEnergy(x)=-log\sum_{h}\exp(-Energy(x,h))$$ The $-log()$ term shown above is accounted for in [2] in Eqn. 27.118. Also [1] uses $x$ instead of $v$. Note sure what you mean in your first question. The FreeEnergy term is actually inspired by the Ising model from physics, which models statistical mechanics of neuron dynamics (the extent of my knowledge of where the FreeEnergy definition comes from ends there). To minimize the cross-entropy of an RBM requires taking the gradient of the FreeEnergy to determine how to update the system parameters. By minimizing the system energy we increase the probability to observe a given visible layer and it's associated hidden layer (i.e. learn the distribution of the data). Hope that in some part steers you in the right direction? [1] Bengio, Yoshua. "Learning deep architectures for AI." Foundations and trendsÂ® in Machine Learning 2.1 (2009): 1-127. [2] Kevin P. Murphy. 2012. Machine Learning: A Probabilistic Perspective. The MIT Press.
