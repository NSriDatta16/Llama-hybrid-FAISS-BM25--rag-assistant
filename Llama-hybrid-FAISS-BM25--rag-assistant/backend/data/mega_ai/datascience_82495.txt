[site]: datascience
[post_id]: 82495
[parent_id]: 81515
[tags]: 
It's probably hard to get exactly a uniform distribution on the weights. One heuristic approximation is to repeat the following many times: Randomly choose initial weights. Train the neural network until you get 100% accuracy on the training set. Save the resulting neural network. Each neural network is a sample of weights that are consistent with the training set. Are they uniformly distributed among the set of all such weights? That seems unlikely. But they might give an approximation to such a sample. This might fail, if training never gives you 100% accuracy. However, research has demonstrated empirically that if you choose a deep neural network architecture with sufficient capacity and you train for long enough, neural nets can memorize the training set and achieve 100% accuracy on the training set [1]. So, if it fails, I'd recommend increasing the size of the network and trying again. Of course, there are no guarantees -- it can still fail. It's a heuristic. [1] Understanding deep learning requires rethinking generalization . Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, Oriol Vinyals. arXiv:1611.03530
