[site]: crossvalidated
[post_id]: 238078
[parent_id]: 
[tags]: 
Chow-Liu trees and Kullback Leibler divergence

I'm reading David Barber's book on Bayesian Reasoning and Machine Learning. At Section 9.5.4 he covers Chow-Liu trees, and I am having difficulties understanding the flow of the equations after he introduces KL divergence as a way to find the best approximating distribution. I have typed out the problem below. Consider a multivariate distribution $p(x)$ that we wish to approximate with a distribution $q(x)$. Furthermore, we constrain the approximation $q(x)$ to be a BN in which each node has at most 1 parent. First we assume that we have chosen a particular labeling of D variables so that children have higher parent indices than their parents. The DAG single parent constraint then means: $q(x) = \prod ^D_{i=1}q(x_i|x_{pa(i)})$ $pa(i) To find the best approximating distribution of this constrained class, we may minimise the KL divergence: $KL(p|q) = \left \langle log\:p(x)\right \rangle_{p(x)} - \sum^D_{i=1}\left \langle log\:q(x_i|x_{pa(i)})\right \rangle_{p(x_i, x_{pa(i)})}$ My question is if we are defining the KL divergence as $KL(q|p) = \left \langle log\:q(x) - log\:p(x)\right \rangle_{q(x)}$ Then how did the subscript turn out to become ${p(x_i, x_{pa(i)})}$ and not $p(x_i)$ in the equation before this?
