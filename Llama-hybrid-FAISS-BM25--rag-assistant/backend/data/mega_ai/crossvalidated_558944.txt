[site]: crossvalidated
[post_id]: 558944
[parent_id]: 
[tags]: 
Studies on where to apply L2?

Are there any studies on where (and maybe how much) L2 to apply per parameter? E.g. in a more complex neural network, e.g. some encoder-decoder, with different components, from my own experience, just using L2 everywhere is not so effective, but it is effective on some selected layers. But I figured this out just by trial and error. You can easily expand the possible search space of hyper parameters much more, by also considering different L2 scales per parameter. And maybe also other things, like optimizer, other regularization such as dropout, etc. So I wonder if there are any studies on this, or if trial and error (or maybe sth like population based training) is really my best option?
