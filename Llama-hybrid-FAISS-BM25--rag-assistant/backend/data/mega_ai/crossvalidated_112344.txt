[site]: crossvalidated
[post_id]: 112344
[parent_id]: 112339
[tags]: 
From my experience, feature scaling is not always required. It's mainly to bring your features down to a comparable range so that the feature space is not skewed along any one particular feature. This will speed up convergence. Whether you do it or not before training neural networks would depend on the data you're dealing with. For example, I won't do it when my feature vectors are binary one-hot or refer to indices of words (in language modelling). Likewise, also not if they are already in a comparable range, such as feature 1 in (0, 1) and feature 2 in (-1, 3) (arbitrary choice of ranges). On the other hand, if one of my features is in the range (0, 200) and another in the range (0, 1) , I would definitely do it. You'll find a very nice summary on this subject from Prof. Andrew Ng in this video about feature scaling for gradient descent which is very pertinent to your question. And this topic seems to have been already discussed on Cross-validated where the chosen answer states: It's simply a case of getting all your data on the same scale: if the scales for different features are wildly different, this can have a knock-on effect on your ability to learn (depending on what methods you're using to do it). Ensuring standardised feature values implicitly weights all features equally in their representation. You can find more about that question here: How and why do normalization and feature scaling work?
