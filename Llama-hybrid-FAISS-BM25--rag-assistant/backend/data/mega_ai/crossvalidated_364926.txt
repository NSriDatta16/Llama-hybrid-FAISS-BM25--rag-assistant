[site]: crossvalidated
[post_id]: 364926
[parent_id]: 364917
[tags]: 
Wikipedia provides a synopsis of the universal approximation theorem : In the mathematical theory of artificial neural networks, the universal approximation theorem states that a feed-forward network with a single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of $\mathbb{R}^n$ , under mild assumptions on the activation function. This theorem is the core justification for attempting to model complex, nonlinear phenomena using neural networks. Even though it is very flexible, it doesn't cover everything -- in this case, you've defined a discontinuous function, and the universal approximation theorem only extends to continuous functions. I am not aware of a theorem which allows a neural network to approximate arbitrary, discontinuous functions. Despite the impressive name, the universal approximation theorems (there are many) are less practical than one might like. Showing that there exists some approximating function with the desired qualities is very different from using some observed data to determine the weights and biases of that approximation. But the universal approximation theorem is actually way more general than is needed for the specific function in OP's question: $$ f(x) = \begin{cases} 50 & x \ge 100 \\ 25 & \text{otherwise}. \end{cases}$$ This function is just two horizontal lines with 1 discontinuity. As long as our approximation function is constrained to only predict values between 25 and 50 (the smallest and largest values of the function), we know that the largest absolute error will at most 25 (the largest possible difference between approximation and the true value). Bounding the maximum error is crucial because this bound is the precise sense in which a neural network approximates a function. In practical terms, we need to identify a family of functions that can be parameterized such that specific parameter choices reduces the maximum error to a desired level. For this specific problem, we just need a function that "looks like" two horizontal lines with some kind of "ramp" in the middle. We know that we want a "ramp" because we need to make predictions that are between 25 and 50, and because our approximation is continuous. There are many such functions, but a common choice is a (scaled and shifted) sigmoid function. Just by reading off the statement of the problem, we can fill in nearly all of the free parameters: $$\begin{align} \hat f(x) &= a + b \sigma(w(x-c)) \\ &= 25 + 25 \sigma(w(x-100)) \end{align}$$ This one-layer, one-neuron neural network seems like a decent approximation to the desired function. For instance, for $w=3$ , we have $\hat f(99)= 26.18565\dots$ $\hat f(101)= 48.81435\dots$ However, minimizing the loss by backpropagation will never reach a minimum because we can always improve this approximation by making $w$ larger. In the limit of $w$ tending towards infinity, the sigmoid approximation becomes the desired step function. Suppose that instead of $$ f(x) = \begin{cases} 50 & x \ge 100 \\ 25 & \text{otherwise}, \end{cases}$$ we have the function $$ h(x) = \begin{cases} 1 & x \ge 100 \\ 0 & \text{otherwise}. \end{cases}$$ All we've done is shifted & rescale the function values. Restating the function in this way might be illuminating because $h$ is recognizable immediately as classification (using the conventional binary coding for the outputs). Moreover, we can see from its definition that this classification problem exhibits perfect separation , because there is some fixed constant above which we have only 1s and below which we have only 0s. So we know that the weight(s) $w$ will "drift away" towards infinity.
