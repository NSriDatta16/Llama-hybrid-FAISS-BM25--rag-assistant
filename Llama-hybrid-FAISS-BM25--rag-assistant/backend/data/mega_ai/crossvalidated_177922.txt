[site]: crossvalidated
[post_id]: 177922
[parent_id]: 177532
[tags]: 
If I interpret things correctly, you mean that the first principle component (eigenvalue) explains most of the variance in the data. This can happen when your compression method is linear. However, there might be non-linear dependencies in your feature space. TL/DR: PCA is a linear method. Use Autoencoders (non-linear pca) for dimensionality reduction. If the machine learning part is supervised learning then simply monitor your loss function while adjusting the (hyper)parameters for the autoencoder. In this way you will end up with a far better compressed version of your original data. Here's a scikit example where they do grid search to find the optimal number of principal components to keep (hyper-parameter) using PCA. Finally they apply Logistic Regression on the lower dimensional space: http://scikit-learn.org/stable/auto_examples/plot_digits_pipe.html#example-plot-digits-pipe-py Protip: Autoencoders do not have a closed form solution (afaik) so if your context is streaming data, this means you can continuously update your autoencoder (compressed representation) and can thus compensate for things such as concept drift. With pca you have to re-train batch mode from time to time as new data comes in. As to giving some features more "weight", see regularization ( I'd start from norms https://en.wikipedia.org/wiki/Norm_(mathematics) ). You might also be surprised how similar logistic regression is to the perceptron.
