[site]: crossvalidated
[post_id]: 521402
[parent_id]: 521388
[tags]: 
The optimization algorithms we usually use for training neural networks are deterministic. Gradient descent , the most basic algorithm, that is a base for the more complicated ones, is defined in terms of partial derivatives $$ \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\Theta) $$ Partial derivative tells you how does the change of the optimized function is affected by the $\theta_j$ parameter. If all the parameters are the same, they all have the same impact on the result, so will change by the same quantity. If you change all the parameters by the same value, they will keep being the same. In such a case, each neuron will be doing the same thing, they will be redundant and there would be no point in having multiple neurons. There is no point in wasting your compute repeating exactly the same operations multiple times. When you initialize the neurons randomly, each of them will hopefully be evolving during the optimization in a different "direction", they will be learning to detect different features from the data. You can think of early layers as of doing automatic feature engineering for you, by transforming the data, that are used by the final layer of the network. If all the learned features are the same, it would be a wasted effort. You may also be interested in reading the The Lottery Ticket Hypothesis: Training Pruned Neural Networks paper by Jonathan Frankle and Michael Carbin, who explore the hypothesis that the big neural networks are so effective because randomly initializing multiple parameters helps our luck by drawing the lucky "lottery ticket" parameters that work well for the problem.
