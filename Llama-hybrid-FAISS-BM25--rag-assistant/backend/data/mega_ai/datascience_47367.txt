[site]: datascience
[post_id]: 47367
[parent_id]: 
[tags]: 
Why do we share parameters between two different inputs in the embeddings layer?

I noticed in some deep learning networks that have two inputs to the network, they use one embeddings layer to share the parameters between these two different inputs. As an example, in Keras: input_target = Input((1,)) input_context = Input((1,)) embedding = Embedding(vocab_size, embed_size, input_length=1, name='embedding') target = embedding(input_target) context = embedding(input_context) Why do they use this way? To make everything clear, the other case is: for each input we have different embeddings layer before moving to the RNN or CNN layers.
