[site]: datascience
[post_id]: 92395
[parent_id]: 91147
[tags]: 
You should look at the data with Features intact for each step.Featues can't be flattened since each point of time is defined by all the Features. Let's see this snap, The upper table is the data Let's assume we want to predict 2 steps using 3 input steps. So, one instance of our input will have 3 sequential steps having 6 Features part of each sequential steps. So, input becomes [Batch, 3, 6]. In you case [Batch, 168, 6] Output need to have 2 sequential steps(per requirement). Since we are predicting one feature, so it will have just one feature. So, output shape [Batch, 2]. In your case [Batch, 24]. But this would have been the case when we only want the backprop after the last step i.e. return_sequences=False for the last RNN. Since we are returning sequence every time, so output must have 24 values for each sequential step. So output becomes [Batch, 168, 24] Features Features don't pass sequentially, all 6 Features will pass together for each time step. Check this depiction below. Each feature will go into each Neuron and each neuron will add a recurrent weight to each Neuron. If you check your model's parameter count for the first layer, it will be 540. 6*20(Input weights) + 20*20(Recurrent) + 20(Bias) = 540 $\hspace{5cm}$ Image credit - SO Answer
