[site]: datascience
[post_id]: 57532
[parent_id]: 
[tags]: 
Relation between an underlying function and the underlying probability distribition function of data

I heard and read a lot of times the following statements and got a lot of confusion over time. Statement 1 : The goal of machine learning is to get a function from the given data Statement 2 : The goal of machine learning is to find the underlying distribution function of the given data From the above two statements, I generally interpret that the underlying function is a probability distribution function of the given data. But I did not understand the relation between the probability distribution function and the function we want to get for a particular task. Let us consider the following example. A random experiment $E$ has a sample space $\Omega$ and I defined two random vectors $X_1, X_2$ on $\Omega$ . I am using a neural network for my task. The domain of the neural network is the range of $X_1$ and I am expecting the range of the neural network to be the range of $X_2$ with correct mapping that satisfies the data. Let $f$ be the actual function that we want the neural network to become that maps from the range of $X_1$ to the range of $X_2$ . Assume that I have the joint probability distribution $P(X_1, X_2)$ for my dataset. Now, what is the relation between the actual function $f$ we are approximating using the neural network that transforms the range of $X_1$ into a range of $X_2$ and the joint distribution $P$ ?
