[site]: crossvalidated
[post_id]: 307340
[parent_id]: 
[tags]: 
Role of delays in LSTM networks

LSTM network is assumed to be about memory, keeping the important information for predictions. If it is the case, why do we need to consider delayed inputs as well? My assumption would be that the LSTM - if the model is sufficiently complex - shall somehow remember the very last inputs if relevant. (Similar trick as if we transform a Markov Chain or higher order to a first order Markov Chain.) However, my experiments indicate that the delayed terms matter, regardless to the complexity/simplicity of the LSTM model. How to explain that? Edit : By delayed inputs, I mean the following: $X_{t}$ is my time series. I want to predict $X_{t+1}$. I know that $X_{t+1}$ depends on $X_t$ and also on $X_{t-1}$. I would assume LSTM to be able to work even just on $X_t$. Some code: from keras.models import Sequential from keras.layers import Dense, SimpleRNN data = [0,1,2,3,2,1]*20 import numpy as np def shape_it(X): return np.expand_dims(X.reshape((-1,1)),2) n_data = len(data) data = np.matrix(data) n_train = int(0.8*n_data) X_train = shape_it(data[:,:n_train]) Y_train = shape_it(data[:,1:(n_train+1)]) X_test = shape_it(data[:,n_train:-1]) Y_test = shape_it(data[:,(n_train+1):]) model = Sequential() model.add(SimpleRNN(units=2,activation='relu',input_shape=(None,1))) model.add(Dense(units=5,activation='relu')) model.add(Dense(units=1,activation='relu')) model.compile(optimizer='adam',loss='mean_squared_error') model.fit(X_train,Y_train.reshape(-1,1),epochs=5000,batch_size=n_train) import matplotlib.pyplot as plt plt.plot(model.predict(X_test).reshape(-1,1)) plt.plot(Y_test.reshape(-1,1)) Which results in the following picture: Note that this shall be completely accurate which is obviously not that case. An ideal answer will contain configuration of an RNN that will be capable to learn accurately, without involving $X_{t-1}$.
