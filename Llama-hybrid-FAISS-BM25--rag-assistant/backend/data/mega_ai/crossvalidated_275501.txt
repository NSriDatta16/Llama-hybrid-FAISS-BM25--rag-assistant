[site]: crossvalidated
[post_id]: 275501
[parent_id]: 274815
[tags]: 
Here are a few other ways of justifying Bayesian inference in misspecified models. You can construct a confidence interval on the posterior mean, using the sandwich formula (in the same way that you would do with the MLE). Thus, even though the credible sets don't have coverage, you can still produce valid confidence intervals on point estimators, if that's what you're interested in. You can rescale the posterior distribution to ensure that credible sets have coverage, which is the approach taken in: MÃ¼ller, Ulrich K. "Risk of Bayesian inference in misspecified models, and the sandwich covariance matrix." Econometrica 81.5 (2013): 1805-1849. There's a non-asymptotic justification for Bayes rule: omitting the technical conditions, if the prior is $p(\theta)$, and the log-likelihood is $\ell_n(\theta)$, then the posterior is the distribution that minimizes $-\int \ell_n(\theta) d\nu(\theta) + \int \log\!\Big(\frac{\nu(\theta)}{p(\theta)}\Big)d\nu(\theta)$ over all distributions $\nu(\theta)$. The first term is like an expected utility: you want to put mass on parameters that yield a high likelihood. The second term regularizes: you want a small KL divergence to the prior. This formula explicitly says what the posterior is optimizing. It is used a lot in the context of quasi-likelihood, where people replace the log-likelihood by another utility function.
