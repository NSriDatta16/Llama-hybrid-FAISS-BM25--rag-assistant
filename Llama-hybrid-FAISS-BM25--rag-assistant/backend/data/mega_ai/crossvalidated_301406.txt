[site]: crossvalidated
[post_id]: 301406
[parent_id]: 298983
[tags]: 
The short answer is: No. Now you already have the historical action and performance, this is a classical supervised learning problem that maps your (customer profile, action) tuple to a performance score. The reasons below are why reinforcement learning will be a bad choice for your task: Reinforcement learning makes very INEFFICIENT use of data, so usually it requires kind of infinite amount of supplied data either from simulator or real experience. I would think neither of these cases apply to you, since you will not want your untrained model to send random notifications to your customers at the beginning state of training, and your problem will be considered solved if you already have a simulator. Reinforcement learning is usually used to deal with long sequences of actions, and the early action could have drastic influence on the final outcome, such as in chess. In that case, there is no clear partition of the final reward received at the end to each step of your actions, hence the Bellman equation is used explicitly or implicitly in reinforcement learning to solve this reward attribution problem. On the other hand, your problem does not seem to have this sequential nature (unless I misunderstood or your system is emailing back and forth with a customer), and each sample from your data is a single-step IID.
