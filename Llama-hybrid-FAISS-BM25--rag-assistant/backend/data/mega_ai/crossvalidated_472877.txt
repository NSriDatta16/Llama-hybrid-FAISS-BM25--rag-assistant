[site]: crossvalidated
[post_id]: 472877
[parent_id]: 
[tags]: 
Why is this an example of a noninformative prior?

From Bayesian Data Analysis 3rd Edition [Gelman et. al], they give this as an example when introducing non-informative priors: "We return to the problem of estimating the mean θ of a normal model with known variance $σ^2$ , with a $N(μ_0 , τ_0^2 )$ prior distribution on $θ$ . If the prior precision, $1/τ_0^2$ , is small relative to the data precision, $n/σ^2$ , then the posterior distribution is approximately as if $τ_0^2 = ∞$ : $$p(θ|y) ≈ N(θ|y, σ^2 /n)$$ Putting this another way, the posterior distribution is approximately that which would result from assuming $p(θ)$ is proportional to a constant for $θ ∈ (−∞, ∞)$ . Such a distribution is not strictly possible, since the integral of the assumed $p(θ)$ is infinity, which violates the assumption that probabilities sum to 1. In general, we call a prior density $p(θ)$ proper if it does not depend on data and integrates to 1. (If $p(θ)$ integrates to any positive finite value, it is called an unnormalized density and can be renormalized—multiplied by a constant—to integrate to 1.) The prior distribution is improper in this example, but the posterior distribution is proper, given at least one data point." In particular, I don't really understand the bold part. It doesn't look to me like the marginal $p(θ)$ is proportional to a constant. Is my understanding correct that the integral is infinity because if the variance of the posterior is approximately $∞$ , then $θ$ is equally likely to be found anywhere, and the pdf is uniform on $(-∞, ∞)$ . Also, why is the posterior distribution proper given at least one data point? I don't understand the intuition here
