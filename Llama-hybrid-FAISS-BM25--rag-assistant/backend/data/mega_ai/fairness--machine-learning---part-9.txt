ed to leverage causal models to assess bias in machine learning models. This approach is usually justified by the fact that the same observational distribution of data may hide different causal relationships among the variables at play, possibly with different interpretations of whether the outcome are affected by some form of bias or not. Kusner et al. propose to employ counterfactuals, and define a decision-making process counterfactually fair if, for any individual, the outcome does not change in the counterfactual scenario where the sensitive attributes are changed. The mathematical formulation reads: P ( R A ← a = 1 ∣ A = a , X = x ) = P ( R A ← b = 1 ∣ A = a , X = x ) , ∀ a , b ; {\displaystyle P(R_{A\leftarrow a}=1\mid A=a,X=x)=P(R_{A\leftarrow b}=1\mid A=a,X=x),\quad \forall a,b;} that is: taken a random individual with sensitive attribute A = a {\displaystyle A=a} and other features X = x {\displaystyle X=x} and the same individual if she had A = b {\displaystyle A=b} , they should have same chance of being accepted. The symbol R ^ A ← a {\displaystyle {\hat {R}}_{A\leftarrow a}} represents the counterfactual random variable R {\displaystyle R} in the scenario where the sensitive attribute A {\displaystyle A} is fixed to A = a {\displaystyle A=a} . The conditioning on A = a , X = x {\displaystyle A=a,X=x} means that this requirement is at the individual level, in that we are conditioning on all the variables identifying a single observation. Machine learning models are often trained upon data where the outcome depended on the decision made at that time. For example, if a machine learning model has to determine whether an inmate will recidivate and will determine whether the inmate should be released early, the outcome could be dependent on whether the inmate was released early or not. Mishler et al. propose a formula for counterfactual equalized odds: P ( R = 1 ∣ Y 0 = 0 , A = a ) = P ( R = 1 ∣ Y 0 = 0 , A = b ) ∧ P ( R = 0 ∣ Y 1 = 1 , A = a ) = P ( R = 0 ∣ Y 1 = 1 , A = b ) , ∀ a , b ; {\displaystyle P(R=1\mid Y^{0}=0,A=a)=P(R=1\mid Y^{0}=0,A=b)\wedge P(R=0\mid Y^{1}=1,A=a)=P(R=0\mid Y^{1}=1,A=b),\quad \forall a,b;} where R {\displaystyle R} is a random variable, Y x {\displaystyle Y^{x}} denotes the outcome given that the decision x {\displaystyle x} was taken, and A {\displaystyle A} is a sensitive feature. Plecko and Bareinboim propose a unified framework to deal with causal analysis of fairness. They suggest the use of a Standard Fairness Model, consisting of a causal graph with 4 types of variables: sensitive attributes ( A {\displaystyle A} ), target variable ( Y {\displaystyle Y} ), mediators ( W {\displaystyle W} ) between A {\displaystyle A} and Y {\displaystyle Y} , representing possible indirect effects of sensitive attributes on the outcome, variables possibly sharing a common cause with A {\displaystyle A} ( Z {\displaystyle Z} ), representing possible spurious (i.e., non causal) effects of the sensitive attributes on the outcome. Within this framework, Plecko and Bareinboim are therefore able to classify the possible effects that sensitive attributes may have on the outcome. Moreover, the granularity at which these effects are measured—namely, the conditioning variables used to average the effect—is directly connected to the "individual vs. group" aspect of fairness assessment. Bias mitigation strategies Fairness can be applied to machine learning algorithms in three different ways: data preprocessing, optimization during software training, or post-processing results of the algorithm. Preprocessing Usually, the classifier is not the only problem; the dataset is also biased. The discrimination of a dataset D {\textstyle D} with respect to the group A = a {\textstyle A=a} can be defined as follows: d i s c A = a ( D ) = | { X ∈ D | X ( A ) ≠ a , X ( Y ) = + } | | { X ∈ D | X ( A ) ≠ a } | − | { X ∈ D | X ( A ) = a , X ( Y ) = + } | | { X ∈ D | X ( A ) = a } | {\displaystyle disc_{A=a}(D)={\frac {|\{X\in D