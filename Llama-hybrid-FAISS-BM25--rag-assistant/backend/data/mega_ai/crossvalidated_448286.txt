[site]: crossvalidated
[post_id]: 448286
[parent_id]: 
[tags]: 
maximizing KL divergence as the objective function

As far as I know, the most common approach to train neural networks is to minimize the KL divergence between the data distribution and the output of the model distribution which results in minimizing the cross-entropy. Now assume we have a binary classification task, our goal can be optimizing a network such that the KL divergence between the two class distributions is maximized. I want to know is there any basic difference in these two different approaches? and can we optimize the latter approach explicitly with regards to the parameters of a neural network?
