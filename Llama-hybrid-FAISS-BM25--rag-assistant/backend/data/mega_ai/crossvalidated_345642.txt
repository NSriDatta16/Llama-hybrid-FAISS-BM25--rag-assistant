[site]: crossvalidated
[post_id]: 345642
[parent_id]: 315586
[tags]: 
You ask: How can $P(X|z;θ)$ be chosen to be normal? and what is it with mean $f(z;θ)$? For the first question , I think we can answer this question by asking: Why we make an autoencoder variational in the first place? It is explained in this post that we do this because we want to make an autoencoder generative. By using a classical autoencoder the latent distribution is discrete. So when the decoder samples from a gap between clusters in the latent space, it will have no idea how to deal with it. So, instead of a deterministic mapping $f(z; \theta)$, we want to add some noise to it. So we can write $f(z; \theta) + \epsilon$, and this $\epsilon$ can be sampled from a normal distribution, and that's why $P(x|z)$ can be chosen to be normal. Of course, there are other choices of this $P(x|z)$, as is pointed out in this paper on page 11, $P(x|z)$ can also be chosen to be a multivariate Bernoulli. For the second question: $f(z; \theta)$ is only a mapping. Say the latent space is $p$ dimensional, and the data space is $d$ dimensional, then this $f(z;\theta)$ will just eat a $p$ dimensional vector and returns a $d$ dimensional vector. But note that this mapping can be non-linear since we are using a neural network to implement this, which is unlike what we have in factor analysis, in which the mapping is linear.
