[site]: datascience
[post_id]: 13362
[parent_id]: 13061
[tags]: 
The normal vs uniform init seem to be rather unclear in fact. If we refer solely on the Glorot 's and He 's initializations papers, they both use a similar theoritical analysis: they find a good variance for the distribution from which the initial parameters are drawn. This variance is adapted to the activation function used and is derived without explicitly considering the type of the distribution. As such, their theorical conclusions hold for any type of distribution of the determined variance. In fact, in the Glorot paper, a uniform distribution is used whereas in the He paper it is a gaussian one that is chosen. The only "explaination" given for this choice in the He paper is: Recent deep CNNs are mostly initialized by random weights drawn from Gaussian distributions with a reference to AlexNet paper . It was indeed released a little later than Glorot's initialization but however there is no justificaion in it of the use of a normal distribution. In fact, in a discussion on Keras issues tracker , they also seem to be a little confused and basically it could only be a matter of preference... (i.e. hypotetically Bengio would prefer uniform distribution whereas Hinton would prefer normal ones...) One the discussion, there is a small benchmark comparing Glorot initialization using a uniform and a gaussian distribution. In the end, it seems that the uniform wins but it is not really clear. In the original ResNet paper , it only says they used a gaussian He init for all the layers, I was not able to find where it is written that they used a uniform He init for the first layer. (maybe you could share a reference to this?) As for the use of gaussian init with Batch Normalization, well, with BN the optimization process is less sensitive to initialization thus it is just a convention I would say.
