[site]: stackoverflow
[post_id]: 2857656
[parent_id]: 2819081
[tags]: 
I hope this is okay. It's my first answer on StackOverflow. Yes absolutely if your chunksize is over 85000 bytes then the array will get allocated on the large object heap. You will probably not run out of memory very quickly as you are allocating and deallocating contiguous areas of memory that are all the same size so when memory fills up the runtime can fit a new chunk into an old, reclaimed memory area. I would be a little worried about the Array.Resize call as that will create another array (see http://msdn.microsoft.com/en-us/library/1ffy6686(VS.80).aspx ). This is an unecessary step if actualLength==Chunksize as it will be for all but the last chunk. So I would as a minimum suggest: if (actualLength != chunkSize) Array.Resize(ref buffer, actualLength); This should remove a lot of allocations. If the actualSize is not the same as the chunkSize but is still > 85000 then the new array will also be allocated on the Large object heap potentially causing it to fragment and possibly causing apparent memory leaks. It would I believe still take a long time to actually run out of memory as the leak would be quite slow. I think a better implementation would be to use some kind of Buffer Pool to provide the arrays. You could roll your own (it would be too complicated) but WCF does provide one for you. I have rewritten your code slightly to take advatage of that: BufferManager bm = BufferManager.CreateBufferManager(chunkSize * 10, chunkSize); for (int i = resumeChunk; i this assumes that the implementation of UploadFile Can be rewritten to take an int for the no. of bytes to write. I hope this helps joe
