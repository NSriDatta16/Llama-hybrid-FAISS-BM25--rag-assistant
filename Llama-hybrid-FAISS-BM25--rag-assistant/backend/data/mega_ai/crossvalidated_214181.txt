[site]: crossvalidated
[post_id]: 214181
[parent_id]: 214175
[tags]: 
Boosting to minimize binomial loss indeed does use decision trees as weak learners (though it doesn't have to, it's just the most popular choice). Logistic regression looks like this $$ E[y \mid x] = \text{logit} \left( \sum_j \beta_j x_j \right) $$ where $\text{logit}$ is the usual logistic transformation. For a booster the model form is $$ E[y \mid x] = \text{logit} \left( \sum_j t_j(x) \right) $$ where each $t_j$ is a decision tree. This model is fit by writing the binomial deviance as a function of the linear predictor instead of the predicted probability (which is replaced by a sum of decision trees in the boosting framework), and then boosting to minimize that loss function. Only at the end, when making predictions, is $\text{logit}$ reapplied to get probabilities.
