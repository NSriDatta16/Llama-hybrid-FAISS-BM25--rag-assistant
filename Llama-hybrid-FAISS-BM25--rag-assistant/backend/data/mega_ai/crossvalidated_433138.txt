[site]: crossvalidated
[post_id]: 433138
[parent_id]: 433099
[tags]: 
Given some hyperparameters, a random forest is a random model that has a overall random performance. We can divide our uncertainity about this performance into two components: bias and variance. The way it works is this one: every tree has a large amount of bias, and a little, unavoidable, amount of variance; stacking many trees togheter you get rid of the bias, because that bias itself is variable, and averaging it among trees, you cancel it. More biased trees need bigger forest to get away with their bias, more flexible trees need need more little ensembles, but in any case, adding more trees will never worsen expected results (as the model is random, in practice performance can actually get worse, but its expected value won't, this is just because of random fluctuations). You choose some finite number of trees because more of them just aren't helping anymore. So, coming to your case: With a lower mtry , your trees are more biased, so it makes sense that the forest needs more trees to give good results. If your forests with mtry = 10 perform better with less trees, that's just because of case. You could have grown one lucky forest, or maybe it's just because estimation of R squared is variable itself. Actually, the more trees you have, the more it is reliable. As for what I wrote until now, best model seems to be the one with many trees and mtry = 1. Because of all these sources of variability, you just can never be sure about how your model will really generalize, so these tiny differences in measured performance actually are not that important. At this point, your question should be completely answered, apart maybe from some curiosity you may still have about why a low value of mtry could be good for the algorithm. This is the trickyier part of the question, because noone knows all about why these algorithms work how they work, so the best we can do is to make some hypothesis. It is possible that some feature that has very little empirical association with the outcome, actually brings some information that can't be found in the other ones. So when mtry is large, that feature never gets to enter the model, when mtry is 1 instead, sometimes it is randomly chosen. But since it is just one (or maybe few) variable, you need a big number of trees to make it work. This is just an imaginatve hypothesis, the way mtry affects the trained forests is actually very difficult to catch, you may look into importance scores of the two different forests to make your own idea about it.
