[site]: crossvalidated
[post_id]: 173470
[parent_id]: 
[tags]: 
State of art streaming learning

I have been working with large data sets lately and found a lot of papers of streaming methods. To name a few: Follow-the-Regularized-Leader and Mirror Descent: Equivalence Theorems and L1 Regularization ( http://jmlr.org/proceedings/papers/v15/mcmahan11b/mcmahan11b.pdf ) Streamed Learning: One-Pass SVMs ( http://www.umiacs.umd.edu/~hal/docs/daume09onepass.pdf ) Pegasos: Primal Estimated sub-GrAdient SOlver for SVM http://ttic.uchicago.edu/~nati/Publications/PegasosMPB.pdf or here : Can SVM do stream learning one example at a time? Streaming Random Forests ( http://research.cs.queensu.ca/home/cords2/ideas07.pdf ) However, I have been unable to find any documentation regarding how they compare to each other. Every article I read seem to run experiments on different data set. I know about sofia-ml, vowpal wabbit, but they seem to implement very few methods, compared to the huge amount of existing methods! Are the less common algorithms not performant enough? Is there any paper trying to review as many methods as possible?
