[site]: crossvalidated
[post_id]: 291368
[parent_id]: 
[tags]: 
How to split dataset for model selection and tuning

I have read as many questions as I could on model selection, cross validation, and hyperparameter tuning and I am still confused on how to partition a dataset for the full training/tuning process. The scenario: I have 100,000 training instances and I need to pick between 3 competing models (say random forest, ridge, and SVR). I also need to tune the hyperparameters of the selected model. Here is how I think the process should look. Step 1: Split the data into 80,000 training and 20,000 test sets. Step 2: Using cross validation, train and evaluate the performance of each model on the 80,000 training set (e.g. using 10 fold cv I would be training on 72,000 and testing against 8,000 10 times). Step 3: Use the 20,000 test set to see how well the models generalize to unseen data, and pick a winner (say ridge). Step 4: Go back to the 80,000 training data and use cross validation to re-train the model and tune the ridge alpha level. Step 5: Test the tuned model on the 20,000 test set. Step 6: Train tuned model on full dataset before putting into production. Is this approach generally correct? I know that this example skimps on technical details, but I am wondering specifically about the partitioning of the dataset for selecting and tuning. If this is not correct, please provide the steps and numeric splits that you would use in this scenario.
