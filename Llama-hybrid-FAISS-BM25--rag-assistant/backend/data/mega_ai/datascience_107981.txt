[site]: datascience
[post_id]: 107981
[parent_id]: 
[tags]: 
BERT each Word Embedding in Keras

How to use BERT to extract the embeddings of every word in a sentence. Suppose I pass my corpus of sentences with different lengths to a BERT model , I want to be able to extract the embeddings of each word in every sentence. And what is the best way to utilize every word embeddings? should I calculate their average? when I follow the basic usage in this BERT model , the sequence length is always 128 regardless of the actual sentence length, I know I can change the 128 but it will be the same for all sentences. Is there a way to get an embedding for every word and when the sentence length is less than 128 then remaining embeddings appear as zeros, for example, if my sentence is 7 words, I want bert to return an embeddings of (128,128) but the useful ones are the first 7 and the remaining are zeros.
