[site]: crossvalidated
[post_id]: 297376
[parent_id]: 297371
[tags]: 
Most of the time training your machine learning model on simulated data is a risky proposition, since any deviation that your generated data has from the real data distribution you're trying to model will result in unwanted bias (and hence lower accuracy) in your final trained model. However here it actually sounds like a reasonable thing to do, since the post-classification deblurring step implicitly assumes that the image patches can be split up into discrete blur types, so therefore generating your data with these different blur types explicitly baked in isn't such a crazy idea. One potential pitfall however would be, say, generating your sample data such that it contains an equal number of each of the blur types, while the actual test image patches have a highly-skewed distribution of blur types (e.g. 80% Gaussian blur, 20% other).
