[site]: datascience
[post_id]: 90314
[parent_id]: 90287
[tags]: 
When carrying machine learning, having a (even quick) look at distribution within the data set helps in getting a little understanding of how data has been generated. Most of the time, and for a broad number of professional projects, since you have to carry on with the data you've got, you cannot make a lot of assumptions from this kind of observations. When it comes to distribution between training, validation and test sets, it's kinda slightly different though. Indeed, it's very recommended to at least have a quick glance at how data is distributed within the three sets, and even ensure that this distribution is the same. Put simply, if you build a model using an algorithm and associated parameters using your training set, it seems reasonable to argue that such model won't be able to describe what's going on within a data set sourced from other processes. I usually think of model parameters as characteristics of the data generating process that gave birth to the data set I have to work with. Not willing to reinvent the wheel here, so I'd point to this answer which explains it quite well, using a toy example. Concerning oversampling and undersampling as mentioned above, these are pretty useful methods of ensuring that an imbalanced distribution between target labels in your data set would be properly extended to the process of splitting it into training, validation and test sets, while avoiding bias propagation due to the difference in occurrences between labels.
