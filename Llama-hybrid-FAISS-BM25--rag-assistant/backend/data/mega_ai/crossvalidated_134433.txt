[site]: crossvalidated
[post_id]: 134433
[parent_id]: 134418
[tags]: 
If you already have established your cut score, and cannot set it again, then the only thing I can think of doing is mapping the range of $\theta$ values that match the observed scores corresponding to your cut score. You will then have to figure out which specific $\theta$ value will be the official cut score, it may make sense to choose the lowest value in the range, or the median perhaps. You could see how many respondents would fail at each one, looking carefully at the respondents on the border. You can also incorporate the standard errors of the $\theta$'s to figure it out. I can't think of an objectively best way to do this - I would think you need to dig into your test specification, client relationships and expectations, resources, etc. However, given it looks like you have changed your item analysis procedures and possibly test validation procedures, I would consider setting your standards again. This never hurts of course if you have the resources to do it well. You may already be aware of the multitude of standard setting methods, but if not, from my experience, this book by Cizek is the best, most balanced overview: Cizek, G. J., & Bunch, M. B. (2007). Standard setting: A guide to establishing and evaluating performance standards on tests (Vol. xv). Thousand Oaks, CA: Sage Publications Ltd. While there are many methods for setting cut scores, there are a few that very easily incorporate the information provided by IRT methods. That is, many methods such as Angoff and Hofstee typically use observed scores for determining the cut score, and inherently ignore the measurement error involved in that. The $\theta$ values obtained via IRT provide a unique opportunity to get a more reliable estimate of respondent scores, and you are wise to look for a cut score method that can incorporate that info. In what I have seen (I am experienced, but certainly not a seasoned expert), the Bookmark method is one of the best to incorporate IRT-based $\theta$ values. A quick and dirty summary: Assemble your group of expert judges and provide them a training on the method. Guide the judges through defining minimal competence (or if there are multiple levels, have them define all of them). Take all the items on your assessment, obtain their IRT difficulty scores, and order them from easiest to hardest. Have your judges go through each item and place a "bookmark" at the item where they think minimally competent respondents have a 50/50 chance of getting it right. Get the bookmarks from all your judges, identify the difficulty level of each item they chose and average them. That is the cut score from round 1. If you have the luxury (because this can be costly to get judges), repeat steps 4 and 5, after giving the judges some feedback on the cut score obtained from step 5 and the percentage of respondents that would fail and pass. There are variations on this, and if you read the Cizek book and do some internet searches you will likely find one that works for you.
