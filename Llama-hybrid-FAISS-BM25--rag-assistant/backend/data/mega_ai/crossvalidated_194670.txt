[site]: crossvalidated
[post_id]: 194670
[parent_id]: 
[tags]: 
Why do we estimate mean using MLE when we already know that mean is average of the data?

I have come across a problem in textbook to estimate mean. The textbook problem is as follows: Assume that $N$ data points, $x_1$, $x_2$, . . . , $x_N$ , have been generated by a one-dimensional Gaussian pdf of unknown mean, but of known variance. Derive the ML estimate of the mean. My question is, Why do we need to estimate mean using MLE when we already know that mean is average of the data? The solution also says that MLE estimate is the average of the data. Do I need to do all the tiring maximizing MLE steps to find out that mean is nothing but average of the data i.e. $(x_1+x_2+\cdots+x_N)/N$ ?
