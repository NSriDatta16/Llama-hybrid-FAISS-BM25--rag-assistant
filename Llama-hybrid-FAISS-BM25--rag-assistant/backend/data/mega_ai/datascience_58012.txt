[site]: datascience
[post_id]: 58012
[parent_id]: 58000
[tags]: 
When trying to find the optimum number of iterations it's normally quite useful to visualise how the increasing iteration effect the accuracy (can identify over-fitting and when you should stop fitting). # Import libraries used import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.metrics import accuracy_score from sklearn.svm import SVC # Create a template lit to store accuracies acc = [] # Iterate along a logarithmically spaced ranged for i in np.logspace(0,5, num = 6): # Print out the number of iterations to use for the current loop print('Training model with iterations: ', i) # Create an SVC algorithm with the number of iterations for the current loop svc = SVC(solver = 'lbfgs', multi_class = 'auto', max_iter = i, class_weight='balanced') # Fit the algorithm to the data svc.fit(X_train, Y_train) # Append the current accuracy score to the template list acc.append(accuracy_score(Y_test, logreg.predict(X_test)) * 100) # Convert the accuracy list to a series acc = pd.Series(acc, index = np.logspace(0,5, num = 6)) # Set the plot size plt.figure(figsize = (15,10)) # Set the plot title title = 'Graph to show the accuracy of the SVC model as number of iterations increases\nfinal accuracy: ' + str(acc.iloc[-1]) plt.title(title) # Set the xlabel and ylabel plt.xlabel('Number of iterations') plt.ylabel('Accuracy score') # Plot the graph acc.plot.line() plt.show() This will produce a graph where the number of iterations has been logarithmic increased (note that it might take a bit of time experimenting with np.logspace to create whole number iteration steps). If the accuracy increases then keep following the trend, if it plateus stop theres probably no point wasting your time, if it drops go back to the highest value (you've over-fitted to the training data).
