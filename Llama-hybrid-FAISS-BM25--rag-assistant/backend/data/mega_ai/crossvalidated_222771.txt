[site]: crossvalidated
[post_id]: 222771
[parent_id]: 222279
[tags]: 
Random Forest converges with growing number of trees, see the Breiman 2001 paper. So if you would set the number of trees (ntree) to infinity, you would always get the same accuracy (or some other measure like logloss). It only varies a lot because your number of trees is too small (or your resampling strategy (10-fold-CV) is to unstable, can be reduced by more repetitions). In normal data situations (especially if the data is big enough) your accuracy should grow with growing trees. So instead of training with 100 different seeds I would train one randomForest with actual_ntree * 100 or even more. In some packages you can also see the development of the accuracy with growing number of trees. For getting a faster evaluation and possibly tuning you can use out-of-bag estimations, that are usually implemented in standard packages (like randomForest in R). They are normally as good as 10 fold-CVs (and more stable) if the number of trees is big enough.
