[site]: datascience
[post_id]: 98236
[parent_id]: 98200
[tags]: 
LSTM (and GRUs, and Recurrent Neural Networks more generally) predict the next item $x_n$ in a sequence X of items $[x_1, ..., x_{n-1}]$ , as you mentionned. However, you can combine them with a feed forward network (as simple as a perceptron), either as a separate layer or a new network, to take the prediction at each set (the $x_i$ ) and from this $x_i$ predict your two variables/features ( $y_{i1}$ and ${y_i2}$ ). You would train both layers/networks jointly, one on predicting $x_i$ from $[x_1, ..., x_{i-1}]$ and the other on predicting $y_{i1}$ and $y_{i2}$ from $x_i$ .
