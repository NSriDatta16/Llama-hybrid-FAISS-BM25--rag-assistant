[site]: crossvalidated
[post_id]: 454106
[parent_id]: 452842
[tags]: 
Does the neural network recognize some feature of 'cat-ness' from the images - say the general shape of a cat being two points ears and then a rounded face? [...] We would hope neural networks to do that. However this does not have to be the case. Neural network learn to assign such weights to the data that give them the highest "prize" in terms of the smallest loss. Often they cheat, because don't do much about prohibiting them to cheat. Famous example was given by Ribeiro et al , of a neural network that has learned that pictures of wolves in the data had snow in background, so it can tell the picture shows a wolf if it has big white areas in it. As you can see, this has nothing to do with "wolfness" of the photo. How good, or how bad, they can get depends on many factors like the data you have, the neural network architecture, hyperparameters etc. If I trained the net entirely on images where a cat was in the left half of the photo, would it be able to recognize a cat when presented with a photo that has the cat on the right side? Theoretically, this should be solved by a convolutional neural network that has a sliding window that searches for the relevant features in different parts of the image. But again, you cannot take it for granted that it will always work, with any hyperparmeters and any data. The activation function is based on the input value of pixels in the image, that is rgb(x, y, z) , does that mean that the neural network would potentially struggle with pictures containing a black cat - since these values would be lower? If the network would somehow consider color of the cat when classifying, e.g. you trained it only on the pictures of light-coated cats, then yes, it can have problems wit black cats. Also on pictures of black cats, the details can be harder to recognize, what may make problem even harder. This can possibly happen for exactly the same reasons why human facial recognition has problems with recognizing faces of Afro-Americans . My current understanding is essentially that the neural network process would be analagous to taking all of the training pictures and laying them on top of each other, and then finding the general distribution of rgb values (i.e. the pixel-wise average rgb value). Then when presented with a new image, we would take this flattened 'map' and overlay it with the new image, and see if it lines up relatively well. If yes, we determine it to be a cat image. Is this correct (for a linear activation function? at all?) How does changing the number of layers affect this analogy? What about changing the activation function? This is not a good analogy. First of all, neural networks have weights, so they do not treat each pixel the same. Second, neural network, on each layer, has multiple neurons, so if "stacking all pictures" could be a rough analogy (again, remember about the weights!) of logistic regression , then each layer of a neural network is multiple such regressions stacked , and you have multiple such layers learning on the results of the previous layers as features, so this gets more complicated. Moreover, activation functions, pooling layers, convolutional layers etc. make non-linear transformations to the outputs, so the "take the average" analogy is also bad because the result is not linear. There is no simple and meaningful analogy for what they do. More then this, we still do not really understand that well why do they actually work , for example, they show quite strange behavior in terms of bias-variance tradeoff (lack of it) .
