[site]: crossvalidated
[post_id]: 143947
[parent_id]: 143932
[tags]: 
If your goal is to accurately estimate the parameters in your model then how close you are to the true model is how you should select your model. Predictive validity via cross-validation is one way to do this and is the preferred$^*$ way for selecting $\lambda$ in LASSO regression. Now, to answer the question as to which parameter estimate is the "real estimate" one should look at which parameter is "closest" to the real parameter value. Does "closest" mean the parameter estimates that minimize bias? If so, then the least square estimator is unbiased in linear regression. Does closest mean the parameter estimate that minimizes mean square error (MSE)? Then it can be shown that there is a specification of ridge regression that will give you estimates that minimize MSE (similar to LASSO, ridge regression shrinks parameter estimates toward zero but, different from LASSO, parameter estimates do not reach zero). Similarly, there are several specifications of the tuning paramater $\lambda$ in LASSO that will result in smaller MSE than linear regression (see here ). As the statistician, you have to determine what is the "best" estimate and report it (preferably with some indication of the confidence of the estimate) to those who are not well versed in statistics. What is "best" may or may not be a biased estimate. The glmnet function in R does a pretty good job of selecting good values of $\lambda$ and, in summary, selecting $\lambda$ through cross-validation and reporting the parameter estimates is a perfectly reasonable way to estimate the "real" value of the parameters. $^*$A Bayesian LASSO model that selects $\lambda$ by marginal likelihood is preferred by some but I'm, perhaps incorrectly, assuming you are doing a frequentist LASSO model.
