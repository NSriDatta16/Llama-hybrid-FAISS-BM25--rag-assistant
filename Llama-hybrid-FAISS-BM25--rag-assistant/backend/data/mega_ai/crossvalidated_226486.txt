[site]: crossvalidated
[post_id]: 226486
[parent_id]: 226477
[tags]: 
Of course you understand that effect sizes are the preferred metric relative to p-values with large amounts of data in an analysis. The obvious reason for this being that significance is very sensitive to sample size -- with large data, everything is "significant." Effect sizes help decide whether something matters. That said, there is a cult of significance among the technically semi-literate. This is one of the biggest problems with peer-review in publishing. A key question should be the choice of effect size measure. There are many as the term "effect size" does not have a single meaning and overlaps strongly with measures of feature relative importance. Ulrike Groemping's papers are one of the best sources for a review of this literature. Pairwise correlations are not an appropriate metric since they are not conditional. The many machine learning workarounds for modeling large numbers of features (e.g., random forests, bags of little jacknifes, etc.), once summarized, would generate the kind of information you seek to create such a plot. One barrier to this graphic will be the fact that most packages report significance only out to several decimal places, preferring to roll up smaller values with a " What would be interesting would be to see how the relationship between p-values and effect size changes as a function of the metric used.
