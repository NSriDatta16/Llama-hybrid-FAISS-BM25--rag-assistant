[site]: crossvalidated
[post_id]: 631367
[parent_id]: 
[tags]: 
Ordinal log-loss in a multiclass classification in XGBoost?

I have a multi-class problem that which classes are simultaneously mutually exclusive and have ordering. You can think of the classes as being some score: 0 (Low), 1 (Medium), 2 (High). What I would like to do is make a custom loss function that will capture this characteristic. Following from Section 7.2.1 . XGBoost's documentation gives the loss function $L(\theta)$ for a binary classification, which is simply the log-likelihood as given by: $$ L(\theta) = \sum_{i}[y_{i}ln(1+e^{-\hat{y}}) + (1-y_{i})ln(1-e^{-\hat{y}})].\\ $$ Although not documented, I would assume that for multiclass with softmax: $$ L(\theta) = \sum_{i}[y_{i,1}ln(\frac{e^{\hat{y_{i,1}}}}{\sum_j^{k}e^{\hat{y_{i,j}}}}) + y_{i,2}ln(\frac{e^{\hat{y_{i,2}}}}{\sum_j^{k}e^{\hat{y_{i,j}}}}) + ... y_{i,m}ln(\frac{e^{\hat{y_{i,m}}}}{\sum_j^{k}e^{\hat{y_{i,j}}}})] $$ From here, two questions follow: Since XGBoost does not provide a ordinal loss, I can only adjust the loss function after the softmax transformation. This is problematic because in the proportional log-odds (ordinal) problem we have: $$ P(y=1) = P(y^* which in XGBoost's case will be: $$ \frac{1}{1+e^{(\tau_{1} - \hat{y})}} $$ Since I cannot invert the softmax function without a constant to get \hat{y}, this is problematic .. If you could do this then I would assume that you can then simply just sum the log-probabilities to get the log-likelihood as your desired loss. In XGBoost, when you are coding the custom loss (see, code snippet ), predt and dtrain are provided to you; if you input a multiclass vector suitable for ordinal classification, dtrain will be, e.g., [0,0,1,3,2] , and predt will be a matrix of probabilities, which is the softmax-transformed $\hat{y_{i,j}}$ for each observation $i$ and class $j$ . However, the ordinal problem, following from above, seems to be one-dimensional (it resembles more a binary classification than multinominal), so I am not fully understanding how can you work with this. I have checked out the following post which explains some differences in encoding, but this affects the dtrain if anything and not the probabilities. I have also looked at the statsmodels.miscmodels.ordinal_model.OrderedModel implementation and they do accept a vector of integers (e.g., [0, 0, 1, 3, 0, 2] ) as their exog variable, so I am assuming this is possible to implement. I've cross posted this question here as well: here
