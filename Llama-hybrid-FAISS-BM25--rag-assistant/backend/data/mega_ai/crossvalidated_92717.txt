[site]: crossvalidated
[post_id]: 92717
[parent_id]: 92716
[tags]: 
Prediction complexity is proportional to the amount of SVs and the complexity of kernel evaluation. Typically, the complexity of kernel evaluation is linear in terms of the number of input dimensions. The SVM decision function for a test instance $\mathbf{z}$ is as follows: $$f(\mathbf{z}) = \sum_{i\in\mathcal{S}} \alpha_i y_i \kappa(\mathbf{x}_i,\mathbf{z}) + b,$$ with $\alpha$ the dual weights, $\mathbf{y}$ the training labels, $\kappa(\cdot,\cdot)$ the kernel and $b$ a bias term. The prediction complexity scales linearly with the amount of SVs due to the sum of kernel evaluations (a consequence of the representer theorem). Even though typical SV models are sparse (e.g. not all training instances become SVs), this can become problematic. For large data sets, the set of SVs tends to become large too, despite SVM's sparsity. Pruning SVs based on their corresponding weights is generally a bad idea, as is already mentioned in the OP. This can be avoided for some kernels to make prediction complexity independent to the number of SVs, but not in general. Some examples: models using the linear kernel can be summarized as $f(\mathbf{z}) = \mathbf{w}^T\mathbf{z}+b$ with $\mathbf{w} = \sum_{i\in\mathcal{S}} \alpha_i y_i \mathbf{x}_i$. $\mathbf{w}$ needs to be computed only once. This is one of the reasons linear models are so practical. models using an RBF kernel can be approximated using a second-order Maclaurin series of the exponential function . Using such an approximation, prediction complexity becomes quadratic in terms of the input dimensionality. The approximated decision function assumes a quadratic form $\tilde{f}(\mathbf{z}) = \mathbf{z}^T\mathbf{M}\mathbf{z} + \mathbf{v}^T\mathbf{z}+c$ in which $\mathbf{M}$, $\mathbf{v}$ and $c$ need to be computed once. models using additive kernels .
