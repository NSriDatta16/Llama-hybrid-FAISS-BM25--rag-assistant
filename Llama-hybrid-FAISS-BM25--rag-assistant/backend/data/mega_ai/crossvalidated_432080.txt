[site]: crossvalidated
[post_id]: 432080
[parent_id]: 
[tags]: 
Different reconstruction errors using different PCA algorithms

In Matlab, I perform PCA on a centered and scaled (std-scaled) data set X_cs in four ways: builtin pca using the builtin pca function with default svd algorithm cov-eig computing the covariance matrix of X_cs and then doing the eigendecomposition of the covariance matrix cov-svd computing the covariance matrix of X_cs and then doing the SVD of the covariance matrix svd doing the SVD of the X_cs' Finally, I also do PCA in Python: python using the PCA function from sklearn.decomposition I then reconstruct the data using all found PCs, uncenter and unscale it. Let the reconstructed data be called X_app , for each of the four ways I compute the RMS error and also the normalized (by mean) RMS error. There is a discrepancy in the errors between all five methods and I wonder what could be the source of that. Specifically, the default builtin pca function should use the svd agorithm: Default. Singular value decomposition (SVD) of X. So why is it different from the SVD done 'by hand'? Another interesting thing that I check is when I change the 'Algorithm' of the Matlab builtin pca function to 'eig' it also does not correspond to the errors obtained from 2. (however, the errors in that case are the lowest). Why do different PCA algorithms result in different reconstruction errors? In order to also judge where the differences are, as suggested by the comment of ttnphns , I checked the eigenvalues and eigenvectors for different algorithms. The distribution of the eigenvalues look pretty similar for builtin pca , cov-eig and cov-svd , however the values are not exactly the same. For instance, the first eigenvalues of builtin pca start to differ at at a second decimal place compared to cov-eig or cov-svd . On the other hand, the eigenvalues of cov-eig vs. cov-svd differ at a much further decimal place, for instance the second eigenvalue at 13th place. Similar thing can be seen for the eigenvectors. When plotted, the weights on eigenvectors look pretty much the same for three algorithms: builtin pca , cov-svd and svd ( instead of cov-eig , like was the case for the eigenvalues ). Typically, the differences can be seen starting from the 14th decimal point. Taking into account the comment of ttnphns , another question is: should I even bother to investigate this? One motivation to investigate those differences might be to find out if certain algorithm is simply finding a better basis from the quality of reconstruction point of view. I look forward to your suggestions. Code: 1. [A_builtin, Z_builtin, eig_builtin] = pca(X_cs, 'Centered', false, 'economy', false); X_tilde_builtin = Z_builtin * A_builtin'; 2. C = cov(X_cs); [A_cov_eig, eig_cov_eig] = eig(C); eig_cov_eig = sort(diag(eig_cov_eig), 'descend'); Z_cov_eig = X_cs * A_cov_eig; X_tilde_cov_eig = Z_cov_eig * A_cov_eig'; 3. C = cov(X_cs); [U, S, ~] = svd(C); A_cov_svd = U; eig_cov_svd = diag(S); Z_cov_svd = X_cs * A_cov_svd; X_tilde_cov_svd = Z_cov_svd * A_cov_svd'; 4. [U, S, V] = svd(X_cs'); A_svd = U; eig_svd = diag(S); Z_svd = X_cs * A_svd; X_tilde_svd = Z_svd * A_svd'; 5. pca = PCA() pca.fit(X_cs) scores = pca.transform(X_cs) PCs = pca.components_ eigvals = pca.explained_variance_ratio_ X_tilde_python = np.dot(scores, PCs)
