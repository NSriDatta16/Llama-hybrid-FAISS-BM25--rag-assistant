[site]: datascience
[post_id]: 112424
[parent_id]: 
[tags]: 
Formal conditions on mappings that can NOT be learned from data

I am new to machine learning and would appreciate some help on the following question. I have observed the literature is focused on algorithms, how one learning does better compared to others for a given data set and remarkable progress have been made on that front. However, I have not been able to find references that discuss the underlying structure of the data space as related to limits of what "can" and "can NOT" be learned from a specific data set/type. My specific question is as follows: are there formal conditions on a mapping (will settle for examples) f:X→Y , with y∈Y⊂Rn and x∈X⊂Rm, to indicate that f can NOT be "learned" from a finite set of training data : X^={x1,x2,...,xT}⊂X and Y^={y1,y2,...,yT}⊂Y, regardless of the size, T, or choice of the training data?
