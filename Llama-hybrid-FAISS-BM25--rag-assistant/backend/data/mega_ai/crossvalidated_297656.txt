[site]: crossvalidated
[post_id]: 297656
[parent_id]: 217802
[tags]: 
The parameters of the matrix factorization algorithm you link to above are the user and item latent vectors. The dot product of any user vector and the item latent vector matrix yields recommendation scores for all items for that particular user; when you treat those as defining a preference ranking over items, you can use standard ranking evaluation metrics to measure the quality of your recommender. The procedure would go roughly as follows: Fit the model. Iterate over all users in your test set; for every user compute their recommendations by multiplying their user vector with the item vectors. Using this predicted ranking, and the known positive items for that user from the hold-out test set, compute your choice of ranking metric . Precision@K and mean reciprocal rank are two metrics that are particularly easy to interpret and common in the literature. Reciprocal rank is the inverse of the metric defined in the section you quote when you set the weights $r_{ui}$ to 1. To evaluate the model as a whole, average your metric over all the users in your test set. During evaluation, you may want to exclude the known positives from the training set. This would correspond to a setting where known positive items are excluded from further recommendations (because the user has already bought or seen them).
