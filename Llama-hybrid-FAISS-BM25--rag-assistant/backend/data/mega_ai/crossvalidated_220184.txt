[site]: crossvalidated
[post_id]: 220184
[parent_id]: 
[tags]: 
Normalise X/Y coordinates to stop jitter

Disclaimer: I am a programmer by trade, not a statistician, so please cater to my ignorance and I apologize now if I make any incorrect assumptions Please consider the following problem: I am using machine learning to predict an x/y coordinate of a point, on an frame, in a video with fairly accurate results. Each time a new frame is loaded from the video, I am re-predicting the position. Often the position has not moved very far (if at all) between the two frames, however due to the algorithm not predicting perfectly there is a small amount (sometimes quite large jumps aswell) of jitter when overlaying the points on the video. Now I had considered comparing the points x/y with the x/y of the previous frame, and calculating the absolute distance between them, and then if this is over a certain threshold, do not move the point, or perhaps move the point part of the way towards the next x/y, however this may cause issues with genuine quick movements, as well as I cannot think of an intelligent way to choose what the threshold would be. I hope I have explained myself well enough, and I am open to any advise you may have, if you refer to a known statistical algorithm or method, please either explain it in plain english, or link me to somewhere where I can read about it.
