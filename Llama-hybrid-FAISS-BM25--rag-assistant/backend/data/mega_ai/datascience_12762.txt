[site]: datascience
[post_id]: 12762
[parent_id]: 12759
[tags]: 
If you look deeper in LSTMs or GRUs, we observe that the gates(input, output, cell or forget based on the RNN) are calculated using an equation like you specified. Example, according to deep learning tutorial of lstm , i t =sigma(W i x t + U i h t-1 + b i ) In this, h is the hidden state vector and x is the input state vector as specified and W and U are the corresponding weights for the input gate i t . SImilarly, there are gates for output and forget. So in the paper, they recall a gist of RNNs and sum it up as a general equation. It is a common computational block in RNNs despite their minor differences. Refer Colah's blog or wildml , I think they are one of the best to understand RNNs.
