[site]: crossvalidated
[post_id]: 207150
[parent_id]: 
[tags]: 
MCMC for Maximum Entropy?

Is there a way to sample from a discrete probability distribution, whose distribution itself is the solution to a Maximum Entropy problem with known linear constraints, without needing to solve for the distribution itself? Illustrative Example: If we knew the constraints of values $\{1,\dots,6\}$ with two linear constraints 1. Sum is 100% (probability distribution) 2. Expected Value is 4. We could either solve the Maximum Entropy distribution directly, and obtain $\{ 0.1031 ,0.1227, 0.1461,0.1740,0.2072 ,0.2468\}$, and do a random sampling according to those distribution. Alternatively, is there a way to sample from that distribution, which would have those limiting probabilities, without solving the Maximum Entropy problem directly? (maybe with some burn-in time, etc.) In other words, using the constraints directly the output is simulation of $\{1,\dots,6\}$ which would have the limiting distribution of $\{ 0.1031 ,0.1227, 0.1461,0.1740,0.2072 ,0.2468\}$. This sort of reminds me of MCMC, except the distribution itself is unknown. Here we have a finite domain, of integers 1-6, but it could be an infinite discrete domain or large number of constraints. It a simulation of the distribution I'm interested in (i.e. rolling a biased dice), and not necessary what the theoretical limiting exact probabilities are.
