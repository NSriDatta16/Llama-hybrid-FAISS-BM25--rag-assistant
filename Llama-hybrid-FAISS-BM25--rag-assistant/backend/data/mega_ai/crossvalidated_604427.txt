[site]: crossvalidated
[post_id]: 604427
[parent_id]: 
[tags]: 
Neural Networks Miscalibration Measure

I have read these two papers related to the neural network miscalibration problem: " On Calibration of Modern Neural Networks " and " Multivariate Confidence Calibration for Object Detection ". In the second paper, the authors propose to use various estimations for the probability of neural network predictions instead its confidence. But after calibration, they use confidence (not the calculated estimate of probability) to calculate miscalibration measure D-ECE. I have trouble understanding the miscalibration measure calculation in the first paper. What values (confidences or temperature-scaled confidences), are used here to calculate miscalibration measure ECE after calibration? I can't find an explanation of this point in the paper or in a code at a related GitHub repository. At first, I thought, that temperature-scaled confidences were used for this. But now I am unsure because in the second paper, original confidences are used to calculate miscalibration. REFERENCES Guo, Chuan, et al. "On calibration of modern neural networks." International conference on machine learning. PMLR, 2017. Kuppers, Fabian, et al. "Multivariate confidence calibration for object detection." Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops. 2020.
