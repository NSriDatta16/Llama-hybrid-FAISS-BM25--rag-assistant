[site]: datascience
[post_id]: 12384
[parent_id]: 
[tags]: 
Estimating data set size for grammar extraction

I have a dependency treebank including 100 sentences, which I divide into a training set and a test set. I extract some rules ((DS,PS) pairs) to convert the treebank to phrase structures. When I extract such rules from the training set, I can measure the percentage of rules (DS patterns) that cover the test set, suppose (10, 24%), (20, 34%), (30,40%), (40,44%), (50, 55%),(60, 58%), (70, 61%)... As you see as I increase the size of the training set, the coverage of extracted patterns increases! however its not linear!, I want to see how many data I need to reach 100% coverage? I guess I can use a regression, but which regression? logarithmic? Is this related to 'learning curve'? if yes how can I use regression for a learning curve?
