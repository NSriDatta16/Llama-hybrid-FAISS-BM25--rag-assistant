an the dimensionality of the images, the principal components can be computed more easily as follows. Let T be the matrix of preprocessed training examples, where each column contains one mean-subtracted image. The covariance matrix can then be computed as S = TTT and the eigenvector decomposition of S is given by S v i = T T T v i = λ i v i {\displaystyle \mathbf {Sv} _{i}=\mathbf {T} \mathbf {T} ^{T}\mathbf {v} _{i}=\lambda _{i}\mathbf {v} _{i}} However TTT is a large matrix, and if instead we take the eigenvalue decomposition of T T T u i = λ i u i {\displaystyle \mathbf {T} ^{T}\mathbf {T} \mathbf {u} _{i}=\lambda _{i}\mathbf {u} _{i}} then we notice that by pre-multiplying both sides of the equation with T, we obtain T T T T u i = λ i T u i {\displaystyle \mathbf {T} \mathbf {T} ^{T}\mathbf {T} \mathbf {u} _{i}=\lambda _{i}\mathbf {T} \mathbf {u} _{i}} Meaning that, if ui is an eigenvector of TTT, then vi = Tui is an eigenvector of S. If we have a training set of 300 images of 100 × 100 pixels, the matrix TTT is a 300 × 300 matrix, which is much more manageable than the 10,000 × 10,000 covariance matrix. Notice however that the resulting vectors vi are not normalised; if normalisation is required it should be applied as an extra step. Connection with SVD Let X denote the ⁠ d × n {\displaystyle d\times n} ⁠ data matrix with column ⁠ x i {\displaystyle x_{i}} ⁠ as the image vector with mean subtracted. Then, c o v a r i a n c e ( X ) = X X T n {\displaystyle \mathrm {covariance} (X)={\frac {XX^{T}}{n}}} Let the singular value decomposition (SVD) of X be: X = U Σ V T {\displaystyle X=U{\Sigma }V^{T}} Then the eigenvalue decomposition for X X T {\displaystyle XX^{T}} is: X X T = U Σ Σ T U T = U Λ U T {\displaystyle XX^{T}=U{\Sigma }{{\Sigma }^{T}}U^{T}=U{\Lambda }U^{T}} , where Λ=diag (eigenvalues of X X T {\displaystyle XX^{T}} ) Thus we can see easily that: The eigenfaces = the first k {\displaystyle k} ( k ≤ n {\displaystyle k\leq n} ) columns of U {\displaystyle U} associated with the nonzero singular values. The ith eigenvalue of X X T = 1 n ( {\displaystyle XX^{T}={\frac {1}{n}}(} ith singular value of X ) 2 {\displaystyle X)^{2}} Using SVD on data matrix X, it is unnecessary to calculate the actual covariance matrix to get eigenfaces. Use in facial recognition Facial recognition was the motivation for the creation of eigenfaces. For this use, eigenfaces have advantages over other techniques available, such as the system's speed and efficiency. As eigenface is primarily a dimension reduction method, a system can represent many subjects with a relatively small set of data. As a face-recognition system it is also fairly invariant to large reductions in image sizing; however, it begins to fail considerably when the variation between the seen images and probe image is large. To recognise faces, gallery images – those seen by the system – are saved as collections of weights describing the contribution each eigenface has to that image. When a new face is presented to the system for classification, its own weights are found by projecting the image onto the collection of eigenfaces. This provides a set of weights describing the probe face. These weights are then classified against all weights in the gallery set to find the closest match. A nearest-neighbour method is a simple approach for finding the Euclidean distance between two vectors, where the minimum can be classified as the closest subject. Intuitively, the recognition process with the eigenface method is to project query images into the face-space spanned by eigenfaces calculated, and to find the closest match to a face class in that face-space. Pseudo code Given input image vector U ∈ ℜ n {\displaystyle U\in \Re ^{n}} , the mean image vector from the database M {\displaystyle M} , calculate the weight of the k-th eigenface as: w k = V k T ( U − M ) {\displaystyle w_{k}=V_{k}^{T}(U-M)} Then form a weight vector W = [ w 1 , w 2 , . . . , w k , . . . , w n ] {\displaysty