[site]: crossvalidated
[post_id]: 615176
[parent_id]: 
[tags]: 
Does generalization methods in ANN increase variance in results?

I am new to generalization techniques in machine learning and I have been experimenting with them recently. I have observed that when I apply them to my model, the performance of the model improves, but the results also become more variable when I try to rebuild the model. For example: Regular model has an r squared of: 0.15-0.16 Models with different generalization techniques have an r squared of: 0.14-0.18 The generalization techniques that I have used are: dropout and weight regularization. My question is whether this increase in variance is related to the randomness associated with these generalization techniques, and if it is normal or if it can be avoided somehow.
