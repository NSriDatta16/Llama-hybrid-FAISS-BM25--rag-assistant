[site]: crossvalidated
[post_id]: 208554
[parent_id]: 207642
[tags]: 
I hope it is not too late yet. I am not sure to understand what test you are talking about but if you want to know how to handle PCA dimensions' inertia here is how I do it. If you are not familiar with how PCA is performed, it is, roughly speaking, a four steps problem : compute $X'X$ the covariance matrix extract the eigenvalues (and eigenvectors) of the covar matrix sort your eigevalues (by descending order) concatenate you eigenvectors to create a matrix $U$ in which columns correspond to ordered eigenvalues' eigenvectors. (simply put, first column is eigenvector associated with the highest eigenvalues) That said, inertia of a dimension is simply the eigenvalues associated to that dimension. You can see it as a quantity of information held by the dimension, the greater the value the greater the amount of information. As eigenvalues are not absolute values, a dimension of inertia 5 for a specific PCA may not be as informative as another dimension of inertia 5 in another PCA. That's why inertia percentages are often computed. A dimension with a high percentage of inertia is very informative and is, by itself, a good substitue for a large part of your dataset. Now try to see PCA as a modeling problem. If you keep all your dimensions then you perfectly describe your initial dataset, you just have performed a change of viewpoint (basis). What's different from your initial basis however is that each dimension is a linear combination of all your initial variables, that means you can keep less variables while still, at a certain degree, describe the same dataset. That's when inertia comes in handy, if your two first dimensions describe a plan with say 70% of inertia then that means that 70% of the initial 'variance' of your dataset is summed up by just those two dimensions (implying that there were a lot of correlation between your initial variables). You can use inertia percentage and 'raw' inertia value to chose the best number of dimensions so that you can describe your dataset on lower dimensionality (often 2-3 so that it's easy for humans to understand) while still being a relevant description of the initial dataset. I hope it helps and was clear
