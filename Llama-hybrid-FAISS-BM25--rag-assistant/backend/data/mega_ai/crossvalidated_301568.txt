[site]: crossvalidated
[post_id]: 301568
[parent_id]: 
[tags]: 
Is that possible to distill the knowledge of a stacked ensemble model?

There is a famous paper "distilling the knowledge in a neural network" from Hinton about training a small NN to represent a large deep NN. Is that possible to do the same thing for a stacked Ensemble model? For example, if I have a three-layer stacking model. The first layer contains a combination of XGBoost, Random Forest, glmnet, SVM Radial Kernel, and KNN; The second layer contains XGBoost and glmnet; The third layer contains one Random Forest model. Is that possible to represent this model with a much smaller model?
