[site]: crossvalidated
[post_id]: 215547
[parent_id]: 
[tags]: 
Employing cross_validation to to develop a reasonable linear regression model using scikit learn

Situation: I have fifteen years worth of monthly observations of price of a chemical, roughly 190 data-points. I want to develop a simple linear model of this price based on the price of other chemicals. (eg. price of milkshakes = f(price of milk, icecream, and syrup)) Over fifteen years the price-setting economics of chemicals change (eg. during the early days, milkshakes were runnier, so milk price was relatively more important. Now milkshakes with syrup are the rage, so syrup price has increased relative importance to milkshake price) Because the underlying price-setting mechanism is dynamic, I believe this makes developing a good model a bit trickier than, say, modelling the K constant of an unknown spring based on pairs of (weight,stretch) observations. In a spring experiment, I would include as many observations as possible, and I would also not care which subset of measurements I'd use to build my model because the K constant is same during every test) In the milkshake example, if I build my model from observations mostly in the 'runny milkshake' era, then if the future regime changes to preference for thicker milkshakes, my model will lose some predictive accuracy. Additionally, I've only got 190 precious observations to work with. So I think I don't dare do things like, "well, just build the model based on the last 24 points of monthly data, when the price setting regime would have been similar to what it will be next year". How I suspect I should build the model Training Set: train different models that use unique input variables model1: chemical_predict = f(chemA, chemB, chemC) model2: chemical_predict = f(chemX, chemY, factor1, factor2) model3: chemical_predict = f(chemJ, chemG, factor3) Validation Set: assess which of the three models works best Test Set: Get a fair assessment of the performance of my 'winning' model Confusion over scikit learn's cross_validation system Example code: scores = sk.cross_validation.cross_val_score(lm, X, y, cv=5) print scores print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2)) My code gives me the test results for linear models built on five different 'folds' of data. I understand this as it builds a unique linear_model for each training set and then runs a test set on that same model. [ 0.99838503 0.99534679 0.99873148 0.99387044 0.96632694] Accuracy: 0.99 (+/- 0.02) But I'm confused -- how would I retrieve the linear_model parameters that yielded, for example, the 3rd result in the scores array (ostensibly the best parameter set to use)). The cross_val_score just returns an nd.array; there is no way to see what parameter set created any one of those results. That this is not obvious scares me that maybe I'm using the cross_validation process incorrectly in the first place. So my questions are (in order of increasing broadness) How do I use the cross_validation to return the "best" fitting model based on the scores of the test set? The alternative is for me to explicitly do something where I run the folds and test-fits myself: kf = sk.cross_validation.KFold(len(X), n_folds=5, shuffle=True, random_state=123) for train, test in kf: X_train, X_test, y_train, y_test = X.ix[train,:], X.ix[test,:], y.ix[train], y.ix[test] lm.fit(X_train, y_train) Is there a scikit learn model that will give me a Degree 1 polynomial result, but as part of its optimization, penalize excessive parameterization, so that I avoid overfitting Any thoughts on my overall approach to this problem?
