[site]: crossvalidated
[post_id]: 445867
[parent_id]: 
[tags]: 
Can someone explain how an action reward function is calculated in markov decision process

In his second lecture on Reinforcement Learning, David Silver, writes the expression for reward function(for MDP) as: Why do we need to calculate the expected value? Because if we are in state s1 and take action a1 we will end up in only one particular state(say s2). And the reward of reaching state s2 is a fixed number
