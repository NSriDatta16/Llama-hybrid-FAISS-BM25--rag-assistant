[site]: crossvalidated
[post_id]: 499141
[parent_id]: 
[tags]: 
Confidence vs credible interval for binomial probability

I have two related questions regarding the calculation of confidence intervals for a binomial probability and how they relate to credible intervals. (This must have appeared a thousand times- apologies). While looking at R code for binom.test I noticed that the 95% confidence interval for the probability of success given the number of success and failures is: $$ CI_{0.025} = BetaCdf^{-1}(p= 0.025 | \alpha= \text{n_succ}, \beta= \text{n_fail + 1}) \\ CI_{0.975} = BetaCdf^{-1}(p= 0.975 | \alpha= \text{n_succ + 1}, \beta= \text{n_fail}) $$ Or in R code: ci.low First question: Why do not we add 1 to $n\_succ$ for calculating the lower bound of CI? (Likewise, why don't we add 1 to $n\_fail$ for the upper bound?) I.e. my thought was that we should calculate CIs as: $$ ci_{p} = BetaCdf^{-1}(p | \alpha= \text{n_succ + 1}, \beta= \text{n_fail + 1}) $$ In R: qbeta(c(0.025, 0.975), n_succ + 1, n_fail + 1) My reasoning being that $\alpha = 1$ and $\beta = 1$ give a uniform prior that we update with the observed data. This procedure should give Bayesian credible intervals, right? And since we use a uniform prior credible and confidence intervals should coincide. Where is the fault in this reasoning? Second question: Consider this ridiculously small experiment with 1 observation only: n_succ The confidence intervals would be: ci.low The range [0.025 1] is very wide and it seems reasonable given only one observation. However, the credible interval would be (if I'm correct): qbeta(c(0.025, 0.975), n_succ + 1, n_fail + 1) # [1] 0.158 0.987 which seems relatively narrow to me (you are fairly convinced to exclude $p with 95% probability!). How can I reconcile credible and confidence intervals? Can you explain the discrepancy?
