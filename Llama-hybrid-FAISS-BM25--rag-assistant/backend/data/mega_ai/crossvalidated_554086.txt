[site]: crossvalidated
[post_id]: 554086
[parent_id]: 554041
[tags]: 
This seems to be mostly matters of convention. Let me say mine, and clarify the actual important properties besides the nomenclature. $$ \text{Likelihood} = L(\theta) = P\left(\text{Observed data} \, \middle| \, \theta\right) $$ Note well, that here we consider the observed data. This is neither a distribution in terms of data (since data was fixed to the observed data) or parameters, $\theta$ . $$ \text{Sampling distribution} = P\left(\text{data} \, \middle| \, \theta\right) $$ This is a distribution of the data and could be used e.g., to draw pseudo-data. This would usually only be used frequentist inference (which violates the likelihood principle) though may be used in Bayesian inference in: approximate Bayesian computation, simulation based calibration, for computing e.g., prior predictive distributions, and lastly objective Bayesian priors. Broadly speaking though, as we condition only on the observed data in Bayesian analysis, it obeys the likelihood principle and requires only the likelihood.
