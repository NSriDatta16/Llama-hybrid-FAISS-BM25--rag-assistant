[site]: datascience
[post_id]: 124613
[parent_id]: 124612
[tags]: 
There are many problems with that code: It uses a lot of LSTM layers. Instead, start with a single LSTM. As you are working with text: Use token indices as input and have an initial embedding layer Use softmax (but without ReLU) as final activation. Use token indices as expected output. Use a categorical cross-entropy loss instead of MSE. I suggest you use another tutorial. For instance, this notebook has the kind of setup that I was describing: model1 = Sequential() model1.add(Embedding(num_classes, embedding_size, input_length = maxlen)) model1.add(LSTM(lstm_size)) model1.add(Dense(num_classes, activation = 'softmax')) model1.compile(loss = 'categorical_crossentropy', optimizer = 'adam')
