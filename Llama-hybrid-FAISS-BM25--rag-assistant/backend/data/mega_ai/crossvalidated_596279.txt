[site]: crossvalidated
[post_id]: 596279
[parent_id]: 
[tags]: 
Derive posterior density function with Jeffrey's prior for theta

I need a guide on how to derive the posterior distribution for $\theta$ and checking whether it is proper. I have been given that the likelihood function is $$L(\theta; x) = \theta \exp(−\theta x).$$ You decide to perform Bayesian inference on $\theta$ given a realisation $x = \{x_i\}_{i=1}^N$ of an i.i.d. sample $X_1,\ldots,X_N$ of size $N = 100.$ You do not wish to take into consideration any prior knowledge about $\theta,$ so you choose to use the Jeffrey’s prior for $\theta,$ given by $$\pi J (\theta) \propto 1/\theta.$$ If I multiply the likelihood and the prior I would only get $\exp(-\theta x)$ which does not resemble any distributions I know but I found a similar question here (Question 2b) . Which says that it is the improper $\rm Gamma(0,0)$ distribution and the posterior is $\textrm{Gamma}(n,n\bar x).$ How does it become $\rm Gamma(0,0)?$ I found a relating question which from my understanding is that it is actually not $0$ but rather a number approaching very close to $0$ thus I can write the posterior as $\textrm{Gamma}(n,n\bar x)?$
