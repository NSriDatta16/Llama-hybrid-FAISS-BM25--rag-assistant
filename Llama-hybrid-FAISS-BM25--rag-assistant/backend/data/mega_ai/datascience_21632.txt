[site]: datascience
[post_id]: 21632
[parent_id]: 
[tags]: 
What are the advantages of the rectified linear (relu) over the sigmoid activation function in networks with many layers?

The state of the art of nonlinearity is to use rectified linear units (ReLU) instead of a sigmoid function in deep neural networks. What are the advantages?
