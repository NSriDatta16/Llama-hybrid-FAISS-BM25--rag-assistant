[site]: datascience
[post_id]: 62471
[parent_id]: 62409
[tags]: 
Transfer learning A different perspective on this is to take a look at what exactly has (in some fields) replaced feature engineering, and that is usually not simply having neural networks learn to extract features from your task-specific data, but it usually involves transfer learning from large quantities of somewhat similar data, which can be labeled for a different task or unlabeled at all. In general, all the models do need good features; however, in some domains of data - for example, photos of objects and faces, and natural language text analysis - the features that you can get from large sources of non-task-specific data (using e.g. a good ImageNet model for photos, or good contextual word embeddings for text) are quite good, and contain most of the things that you might include in manual feature extraction. So the argument is that if for your particular problem it is possible to apply such a transfer learning approach, then that is mandatory (since the knowledge that you can "import" outweighs anything you can realistically do to augment your small task-specific dataset) but manual feature engineering is useful but optional. It's likely to be helpful and bring some improvement, but not that much. If in "pre-NN" methods the results before feature engineering were horrible and you needed to do significant feature work to get good results, then nowadays simply picking the first reasonable, best practice data representation will get you most of the way to the best results possible. You need to ensure that you don't make major mistakes that either throw away important data or include an unrealistic signal factor (e.g. "detect skin cancer from images" where all the positive images have a ruler next to the lesion), but putting a lot of effort in feature engineering in most domains will give just marginal improvements over a small but skilled effort to ensure that the data representation is reasonable.
