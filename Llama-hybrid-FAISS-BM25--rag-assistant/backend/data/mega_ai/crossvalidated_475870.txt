[site]: crossvalidated
[post_id]: 475870
[parent_id]: 
[tags]: 
The goal of triplet loss is to find an embedding such that $$ \left\lVert f(x^a_i) - f(x^p_i) \right\rVert_2^2+\alpha where $\mathcal{T}$ is the set of all possible triplets. A triplet is composed of an anchor point, a positive point (same class as the anchor), and a negative point (distinct class from the anchor). Clearly, iterating over all possible triplets becomes enormously expensive when the data set is even moderately sized. Therefore, it's common to carefully choose which triplets to use when computing the loss. This means that instead of $\mathcal{T}$ , training proceeds on some well-chosen $\mathcal{S} \subset \mathcal{T}$ The loss is zero when the inequality $(*)$ holds, and becomes larger the more that this inequality is violated, giving us the loss function $$L = \sum_{i\in \mathcal{S}} \max \left \{ 0, \lVert f(x^a_i) - f(x^p_i) \rVert_2^2 - \lVert f(x^a_i) - f(x^n_i) \rVert_2^2 +\alpha \right\} $$
