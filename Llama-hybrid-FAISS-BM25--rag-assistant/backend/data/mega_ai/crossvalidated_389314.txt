[site]: crossvalidated
[post_id]: 389314
[parent_id]: 
[tags]: 
Writing PCA as a special kind of auto-encoder

I would like to know if it's possible to view PCA as a particular type of neural network, however, there's one major stumbling block that I haven't yet been able to get past. Define the following "single-layer neural network with linear transfer/activation function": $$\mathrm{nn}(x) = Wx + b.$$ What loss function or training scheme would you use in order to get $\mathrm{nn}$ to behave as PCA? Specifically, is there a gradient descent scheme that seems to guarantee, at least in practice, the orthonormality of the rows of $W$ ? PCA gives the transformation $$ X \mapsto U\Sigma $$ defined by $$ f(\tilde X) = \tilde X V $$ where $X = U\Sigma V^*$ is the singular value decomposition of the data matrix $X$ . Note: The way I've written it, $x$ is a column vector, and so $x^T$ is the $j$ th row ("observation") of the matrix $X$ . So, it must be that $W = V^T$ and either $b \equiv 0$ [if there is no whitening], or $b = V^T \mu$ where $\mu$ is the vector of column means of $X$ [if there is whitening].
