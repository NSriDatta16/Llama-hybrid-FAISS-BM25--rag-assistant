[site]: crossvalidated
[post_id]: 601655
[parent_id]: 601184
[tags]: 
No, it has been demonstrated how CNNs with Relus fail to generalize outside of the training domain, more specifically, it fails to classify samples with high confidence. See Why relu networks yield high-confidence predictions far away from the training data and how to mitigate the problem . The idea is that the output space of a CNN with relus is piecewise (roughly speaking a bunch of hyperplanes that define the regions associated with each class in the case of classification). The authors show that it is always possible to find for (almost) all directions an input z far away from the training data which is assigned some label with high confidence. Also, as a consequence of the theorem, they show that techniques like introducing a rejection option, or manipulating the logits in some form cannot help solve this issue. They propose a adversarial training technique which aims at enforcing the network to produce outputs with lower confidence for out-of-distribution inputs. I do not suggest that this is necessarily the solution to your problem. It might also be possible to improve your results with appropriate data augmentation or preprocessing.
