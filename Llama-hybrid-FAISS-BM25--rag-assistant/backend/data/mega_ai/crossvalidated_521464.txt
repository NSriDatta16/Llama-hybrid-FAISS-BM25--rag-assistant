[site]: crossvalidated
[post_id]: 521464
[parent_id]: 341863
[tags]: 
1. Computational tractability. number of weights between CNN and FC such as input image of shape 500 x 500 x 3 will be in FC layer with 100 hidden unit (basis = 0) FC layer = Wx = 100 x ( 500 x 500 x 3 ) = 100 x 750000 = 75M on other hand: input image of shape 500 x 500 x 3 will be after convolving a 5 * 5 kernel with zero padding, the stride of 1. and 2 filters the new CNN layer = ((Hn + 2p - k )/s)+1,((Wn + 2p - k )/s)+1, Cn * filters num) = 496 x 496 x 6 the number of parameters in a CONV layer is : ((shape of width of the filter * shape of height of the filter * number of filters in the previous layer+1)*number of filters) 1 for bias number of parameters = (Fw * Fh * D + 1 ) * F = (5 * 5 * 3 + 1 )*2 = 152 2. Explicit hierarchical representation of features. the best thing in CNN architecture is no need for feature extraction. 3. Reduces overfitting. If the model is massively overfitting you can start adding dropout in small pieces. also, max-pooling reduce the overfitting too 4. Translation invariant. Invariance refers to the ability to remember an object as an object even though its object place changes. This is usually a positive thing because it maintains the object's identity, category, "Note that translation here has a specific meaning in vision, borrowed from geometry." The same object in different locations.
