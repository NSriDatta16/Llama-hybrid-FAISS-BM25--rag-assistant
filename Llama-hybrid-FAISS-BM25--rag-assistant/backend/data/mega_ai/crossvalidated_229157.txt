[site]: crossvalidated
[post_id]: 229157
[parent_id]: 
[tags]: 
Logistic Tensor Regression

Say there are users and they view articles and click (or not click) on articles. I represent the $i$-th user as $x_i$, a $D \times1 $ vector and $j$-th article as $z_j$, a $C \times 1$vector. The reward (click) of user $i$ on article $j$ is $r_{ij}$. Now say there is a tensor indicator $s_{ij} = \Sigma_{a=1}^C\Sigma_{b=1}^D x_{i,b}z_{j,a}w_{a,b}$. In matrix terms $s_{ij} = x_i^TWz_j$. $z_{j,a}$ is the $a$-th feature of $z_j$.$x_{i,b}$ is the $b$-th feature of $x_i$. And $w_{a,b}$ represents the affinity of these two features. I want to use logistic regression to determine W. So lets say, $p(r_{ij}|s_{ij})=\frac{1}{1+exp^{-r_{ij}s_{ij}}}$ Is it correct to say that the gradient of the cost function is $w_{a,b}-\Sigma_{i,j}r_{i,j}x_{i,b}z_{j,a}(1-p(r_{ij}|s_{ij}))$ If the above it correct, is it correct to say the gradient descent would be $$ w_{a,b} = w_{a,b}-\alpha[w_{a.b}-\Sigma_{i,j}r_{i,j}x_{i,b}z_{j,a}(1-p(r_{ij}|s_{ij}))] $$ I have written the following python code to generate this. Is it correct or is there a better way to do this? if __name__ == '__main__': data_matrix =np.random.rand(8,6) print gradient_decent((3,2)) def gradient_decent(dimension, iterations=10): weights = np.ones(dimension) alpha = 0.001 for k in range(iterations): gradient_error = weights_by_user_article_features(weights) estimated_weights = np.subtract(weights,gradient_error) weights = np.subtract(weights,estimated_weights) return weights def weights_by_user_article_features(original_weights): weights = np.zeros(original_weights.shape) for j in range(0, 3): # number of user features for k in range(3, 5): # number of article features for i in range(0, 8): # number of records sigmoid_tensor = tensor_indicator(original_weights,np_data_matrix[i][:]) * np_data_matrix[i][5] predicted_reward = sigmoid(sigmoid_tensor) reward_error = 1 - predicted_reward error = np_data_matrix[i][j] * np_data_matrix[i][k] * np_data_matrix[i][5] * reward_error weights[j][k - 3] = weights[j][k - 3] + error return weights This question is related to this paper http://www.gatsby.ucl.ac.uk/~chuwei/paper/isp781-chu.pdf .
