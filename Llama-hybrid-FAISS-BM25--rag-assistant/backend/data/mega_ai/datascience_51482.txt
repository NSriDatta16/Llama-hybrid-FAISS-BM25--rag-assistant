[site]: datascience
[post_id]: 51482
[parent_id]: 
[tags]: 
How Tensorflow text prediction predicts without softmax activation

In the Colab notebook here: RNN text generation in def generate_text() , there is predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy() I looked into tf.random.categorical here: stackoverflow answer and sort of understand how it works. I tried to debug/figure out what it does with print statements: for i in range(num_generate): predictions = model(input_eval) # remove the batch dimension predictions = tf.squeeze(predictions, 0) # using a categorical distribution to predict the word returned by the model predictions = predictions / temperature predicted_id1 = tf.random.categorical(predictions, num_samples=1) predicted_id2 = tf.random.categorical(predictions, num_samples=1)[-1,0] predicted_id3 = tf.random.categorical(predictions, num_samples=1)[0,0] predicted_id4 = tf.random.categorical(predictions, num_samples=1)[-2,0] predicted_id5 = tf.random.categorical(predictions, num_samples=1)[1,0] predicted_id6 = tf.random.categorical(predictions, num_samples=1)[2,0] predicted_id7 = tf.random.categorical(predictions, num_samples=1)[3,0] #predicted_id8 = tf.random.categorical(predictions, num_samples=1)[4,0] predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy() #index 0 for print("predicted_id1", predicted_id1) print("predicted_id2", predicted_id2) print("predicted_id3", predicted_id3) print("predicted_id4", predicted_id4) print("predicted_id5", predicted_id5) print("predicted_id6", predicted_id6) print("predicted_id7", predicted_id7) print("predicted_id", predicted_id) This was the output: predicted_id1 tf.Tensor( [[19] [33] [ 3] [35] [ 4] [64] [22]], shape=(7, 1), dtype=int64) predicted_id2 tf.Tensor(35, shape=(), dtype=int64) predicted_id3 tf.Tensor(19, shape=(), dtype=int64) predicted_id4 tf.Tensor(3, shape=(), dtype=int64) predicted_id5 tf.Tensor(38, shape=(), dtype=int64) predicted_id6 tf.Tensor(26, shape=(), dtype=int64) predicted_id7 tf.Tensor(36, shape=(), dtype=int64) predicted_id 29 So it looks like there is some kind of distribution and that some of the indices pick from that distribution but the actual prediction in this case of 29 does not appear in the distribution so I am confused. Are the elements in the distribution not integer ID's of characters in the text? The one way I learned in the Udacity DLND was to assign probabilities to the predicted next character and choose the argmax so please feel free to enlighten me.
