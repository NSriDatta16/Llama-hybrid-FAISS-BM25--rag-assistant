[site]: crossvalidated
[post_id]: 598069
[parent_id]: 552531
[tags]: 
Regarding a point you raise in A : the sufficient statistic for the parameters $\beta$ of a logistic regression - that's a good question. There is no analytical solution to Logistic regression, only numerical. So, calling the final solution a function of the data $T(X,y)$ is a bit of a stretch to the definition of a function. It's true that you use the data in the fitting process - and technically you need only $X$ and $X^Ty$ - but there are other considerations: like the fitting algorithm (do you use Fisher-Scoring? Gradient Ascent? What threshold do you use? What learning step for the gradient?) Maybe others can answer if there is such a thing as a sufficient statistic for the logistic regression model. Regarding point D : one of the main algorithms for Variational Inference (VI) / Variational Bayes - is called CAVI - Coordinate Ascent VI. It uses the "mean field" approximation (what John Madden called "Independent") $q(z)=\prod_j q_j(z_j)$ . The CAVI update rule says that for each coordinate of the parameter, you update the variational distribution to be $q_j(z_j)\propto e^{\mathbb E_{-j}[\log p(z_j|z_{-j},x)]}$ . This can be hard and require quite a lot of analytical work to find out what is this distribution, and what are it's parameters (take expectation on all other variational distributions, etc.). If you further assume that each "complete conditional" $p(z_j | x, z_{-j})$ is in the expo. family, the algorithm simplifies: the variational distributions have the same exponential family form as their complete-conditionals counterparts, and the update rule becomes simply updating the natural parameters of that family, $\theta_j$ , by setting $\theta_j=\mathbb E_{-j}[\theta_{-j}]$ (where $\theta_{-j}$ will be a function of the conditioning set $x,z_{-j}$ ). For more information check sections 4.1 and 4.2 in "Variational Inference: A Review for Statisticians" here .
