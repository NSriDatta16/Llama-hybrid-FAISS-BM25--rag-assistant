[site]: crossvalidated
[post_id]: 299041
[parent_id]: 299018
[tags]: 
No, this is not a good idea. Consider a NN with no hidden layers and one output neuron. This is exactly logistic regression. Consider running logistic regression with two points: -1, and 1, where the label for -1 is 0 and the label for 1 is 1. If logistic regression is performed with some weight penalty, then it will predict positive for all inputs greater than 0 and negative for all values less than 0 -- but not with perfect certainty. Suppose the test set contains points -0.1 and 0.1. The LR model will classify these as negative and positive respectively, and then if you add these to the training set, the network will get much more confident about classifying samples near 0, even through it shouldn't. In fact, there could be a data point 0.2 which is negative. This point would be mistakenly classified as positive by the network, but with the augmented with the additional classified test data, the network gets much more confident about the wrong decision.
