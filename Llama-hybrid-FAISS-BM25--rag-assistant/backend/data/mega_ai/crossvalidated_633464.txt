[site]: crossvalidated
[post_id]: 633464
[parent_id]: 
[tags]: 
Showing that the estimator of the log posterior used in stochastic gradient MCMC is unbiased

In the SGLD paper as well as in this paper it is claimed (paraphrasing) that the following estimator: $$\widetilde{U}(\theta) = -\dfrac{|\mathcal{S}|}{|\widetilde{\mathcal{S}}|} \sum_{{x}\in \widetilde{\mathcal{S}}} \log p({x}|\theta) - \log p(\theta); \quad \widetilde{\mathcal{S}} \subset \mathcal{S}. $$ where $\widetilde{\mathcal{S}}$ is a random sample of the dataset $\mathcal{S}$ , is an unbiased estimator of $$ U(\theta) = - \sum_{x\in \mathcal{S}} \log p(x|\theta) - \log p(\theta). $$ How can I show that the estimator $\widetilde{U}(\theta)$ is unbiased? And as a follow up what if we instead use this estimator (no scaling by $\dfrac{|\mathcal{S}|}{|\widetilde{\mathcal{S}}|}$ ): $$\widetilde{U}(\theta) = - \sum_{{x}\in \widetilde{\mathcal{S}}} \log p({x}|\theta) - \log p(\theta); \quad \widetilde{\mathcal{S}} \subset \mathcal{S}. $$ Is this estimator biased? And how does the variance compare to the original?
