[site]: datascience
[post_id]: 32709
[parent_id]: 32699
[tags]: 
In the subsection of the figure, they compute the average loss across 100 iterations, which is why the loss is monotonically decreasing because on average the loss does decrease with the training. You are correct in inferring that if this was reported on an iteration to iteration basis, the loss would be a zig zag curve, which is less nice to look at than a smooth curve. As in the comments, the loss can monotonically decrease to 0, if it is the Wasserstein distance. The Wasserstein distance is computed as $\sup_{f \in L_{k}} E_{x \sim P_X}[f] - E_{x \sim P_{\theta}} [f]$ where the first term is the expectation of the function, $f$ (discriminator) over the batch and the second term is the generated data from the generator computed on the discriminator. Note that by this formulation, it returns the $k$ -Wasserstein distance.
