[site]: crossvalidated
[post_id]: 336389
[parent_id]: 
[tags]: 
Is REINFORCE equivalent to MLE in degenerate cases?

Consider a simple regression or classification problem using a typical squared loss or cross-entropy loss respectively. The supervised setup uses the gradient as estimated by: $$\nabla_W J \approx \nabla_W l(f(x), y)$$ where $x$ is the input (or batch of inputs), sampled from the dataset $X$, $f$ is the network, $y$ is the target, and $l$ is the loss. From the MLE perspective, $f(x)$ parameterizes a probability distribution $\pi_f$ and we are actually computing $$\nabla_W J \approx -\nabla_W \log \pi_{f(x)}(y)$$ where $\pi$ is either a categorial distribution in the case of classification or a gaussian in the case of regression. On the other hand, we can frame this problem as a one-step MDP where the state is the input data $x$, and the action space is either the real line (regression) or a fixed set (classification). We take a single action $\hat y$, and then the episode ends. The REINFORCE gradient estimator says $$\nabla_W J \approx E_{\hat{y} \sim \pi_{f(x)}} \left[R(\hat y) \nabla_W \log \pi_{f(x)}(\hat y) \right]$$ The obvious reward would just be the negative loss: $$\nabla_W J \approx E_{\hat{y} \sim \pi_{f(x)}} \left[\log \pi_{f(x)}(\hat y)\cdot \nabla_W \log \pi_{f(x)}(\hat y) \right]$$ Where in practice the expectation is estimated by sampling. However, since we are only taking a single step, it would be silly not to directly evaluate the expectation if it is possible to do so. This expression looks very similar to our supervised setup, but it is not quite identical. It is possible to manipulate one expression the other to show that they are equivalent or somehow related? Or are the gradient estimates fundamentally different? Clarifications: In both cases, the gradient estimate is computed by sampling $x \sim X$, so the exact gradients should be: $$\nabla_W J = E_{x\sim X}\left[-\nabla_W \log \pi_{f(x)}(y) \right]$$ and $$\nabla_W J = E_{x\sim X}\left[ E_{\hat{y} \sim \pi_{f(x)}} \left[\log \pi_{f(x)}(\hat y)\cdot \nabla_W \log \pi_{f(x)}(\hat y) \right]\right]$$
