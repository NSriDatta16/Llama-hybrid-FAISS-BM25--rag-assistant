[site]: crossvalidated
[post_id]: 576463
[parent_id]: 
[tags]: 
Why not perform weight decay on layernorm/embedding?

I am learning the code of minGPT. In the function , the author excluded layernorm and embedding layer from experiencing weight decay and I want to know the reasons. Besides, what about batchnorm?
