[site]: crossvalidated
[post_id]: 162885
[parent_id]: 162869
[tags]: 
Valid covariance matrices are symmetric positive semi-definite (SPSD). Multiplying by a scalar factor shouldn't change anything. A covariance matrix can be factored as $\Sigma=\tau\Omega\tau,$ where $\tau$ is a diagonal matrix of standard deviations, and $\Omega$ is the correlation matrix. Denote the (positive) rescaling factor as $\lambda,$ we wish to compute $\lambda\Sigma$. This is the same as rescaling the matrix $\tau$ by a factor of $\sqrt{\lambda}$, i.e. computing the product $\lambda\Sigma=\sqrt {\lambda} \tau\Omega(\tau \sqrt {\lambda})$. If we accept that $\Sigma=\tau\Omega\tau$ is SPSD, clearly this rescaling must also be SPSD, since we are only rescaling the standard deviations, which may change independently of correlations. Per whubr's suggestion, I'll include a simpler demonstration. The definition of PSD is that for a vector $a$, it holds that $a^T\Sigma a\ge0.$ Applying scaling, we can write $a^T\lambda\Sigma a\ge0,$ and rearrange to have $\lambda a \Sigma a\ge0.$ Even under positive scaling $\lambda$, the PSD property is retained. Surely the resulting product $a^T\lambda\Sigma a$ will differ, but all we care about is whether we satisfy the inequality. I think the problem is numerical, or user error. Things to check: Is $\Sigma$ symmetric? Some Cholesky factorization routines check for symmetry; small, nonzero differences between $\Sigma_{ij}$ and $\Sigma_{ji}$ can occasionally creep into $\Sigma$, and cause these checks to fail. Two cheap fixes are to average $\Sigma_{ij}$ and $\Sigma_{ji}$, or just set $\Sigma_{ij}$ = $\Sigma_{ji}$. Is $\Sigma$ PSD? If $\Sigma$ isn't PSD, there's no reason to believe $\lambda\Sigma$ is. Look at the eigenvalue decomposition $\Sigma=Q\Lambda Q^T.$ If you have some slightly negative eigenvalues, this can be the culprit (again, because of accumulated error). One quick fix is to do the following: set each $\Lambda_{ii}=\max(\Lambda_{ii}, \epsilon\times\max(\Lambda))$ for some small $\epsilon$ such as $\epsilon=10^{-6}.$ (That is, you're fixing the value of the smallest eigenvalue to be at least $\epsilon$ times the size of the largest eigenvalue.) Then recompute the factorization using the modified diagonal matrix of eigenvalues: $\tilde{\Sigma}=Q\tilde{\Lambda}Q^T.$ Since we're talking about factorizations of covariance matrices, it's worth noting that the Cholesky factorization $\Sigma=LL^T$ can be related to the correlation-standard deviation factorization. Denote the Cholesky factor of $\Omega$ as $A$, so we have $\Omega=AA^T$. Now we can write $\Sigma=\tau AA^T \tau$, which implies $L=\tau A$.
