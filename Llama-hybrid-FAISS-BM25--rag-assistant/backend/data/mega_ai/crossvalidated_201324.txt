[site]: crossvalidated
[post_id]: 201324
[parent_id]: 201305
[tags]: 
I (and I guess many others) would agree that P(Parameter∣Data) is the most interesting inferential quantity, because it truly quantifies our knowledge about the parameter we are interested in. However, as @Glen_b points out, calculating P(Parameter∣Data) requires to specify a prior p(Parameter), which introduces a certain level of subjectivity. At the beginning of the 20th century, Fisher was looking for a way to get rid of the prior, and thus to get a unique solution to the question about the "best" parameter. He showed various useful properties of the MLE = looking for the parameter that P(Data|Parameter). See also my answer to this question . If you are swayed by Fishers argument, use MLE. If you prefer to estimate P(Parameter∣Data), you are doing Bayesian Inference, which requires specifying a prior.
