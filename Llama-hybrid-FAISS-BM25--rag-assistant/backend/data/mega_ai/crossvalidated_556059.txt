[site]: crossvalidated
[post_id]: 556059
[parent_id]: 556051
[tags]: 
Stepwise feature selection is a "greedy" algorithm for finding a subset of features that optimizes some arbitrary criterion. Forward, backward, or bidirectional selection are just variants of the same idea to add/remove just one feature per step that changes the criterion most (thus "greedy"). Greedy algorithms are often employed where finding a global maximum is computationally infeasible, in the hope that the result is good enough. Stepwise selection can be applied to any prediction algorithm (so-called "wrapper methods") and even without a prediction algorithm at all (so-called "filter methods"). All you need is a function that computes the criterion for a given data set and feature subset selection. Concerning your last two questions: Statistical significance only makes sence in the context of statistical models, which is only a (minor) subset of machine learning algorithms. Even if you use a model-based approach, p-values after variable selection are biased and thereby of only limited use. To detect overfit, you would need an outer additional cross-validation loop, because most criteria for feature slection already minimize the cross-vaidated error rate (e.g. minimizing AIC is asymptotically equivalent to minimizing the leave-one-out error rate). It should be noted that Monte Carlo methods like Boruta or Genetic Algorithms also address the feasibility problem and do not suffer from getting stuck in a local optimum. And if you use neural nets, feature selection is built into the training process ("deep features"). There are methods specifically tailored for linear regression (e.g. LASSO), but stepwise selection is not restricted to linear regression.
