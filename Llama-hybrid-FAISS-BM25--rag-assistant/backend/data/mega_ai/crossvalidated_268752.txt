[site]: crossvalidated
[post_id]: 268752
[parent_id]: 268671
[tags]: 
I think one needs to take into account that changing the class proportions in the training sample will substantially change the final model that is going to be learned through training. This might not be something we necessarily want. The cost of misclassifying majority class samples might be not negligible (invasive medical treatments to rare diseases being a standard example). When down-sampling a dataset we intrinsically hypothesis that the cost of misclassification of their class is similar but this might not be the case; cost-sensitive learning (Elkan,2001) is potentially important concept in this setting. In addition, before randomly under-sampling a dataset it could be beneficial to look at a number of resampling algorithms (eg. NearMiss - (Zhang & Mani (2003)) or One-sided selection - (Kubat & Matwin (1997) ) which can help us do an informed downsampling of a dataset. Majority downsampling algorithms while far less famous than their minority oversampling cousins (eg. SMOTE ) still exist! In that way when downsampling the data we are able to discard points that (in principle at least) do not greatly benefit our training process while at the same time retaining useful exemplars. Finally, examining carefully the "large dataset" it might be possible to construct a better training set through it. By that I mean to recognise latent features or patterns in the data that can assist our training while at the same time lower the size of our training dataset in absolute terms. Feature engineering is an extremely crucial part of any real life Machine Learning application. It allows domain expertise to enter the modelling task; in addition it effectively changes the representation of our original problem (hopefully to a smaller but more informative set in this case). To that extend we might want to use a dimensionality reduction technique like PCA or ICA to reduce the size of our feature space in absolute terms and possibly avoid hardware limitations altogether.
