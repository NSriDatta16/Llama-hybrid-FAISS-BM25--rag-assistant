[site]: crossvalidated
[post_id]: 446872
[parent_id]: 446715
[tags]: 
Firstly, it's not clear exactly what the subscript on the expectation is actually denoting in this case. We can alternatively write $\int q(z|x) \log p(x|z) dz$ (dropping sub/super scripts). Secondly, if our model is producing real values, then how do we determine $p(x|z)$ for even a single sample. Intuitively, the chance of predicting a specific sample is infinitesimally small, even if we output a probability distribution. For continuous distributions $p(x)$ is used to refer to the probability density at some point $x$ . This is different from the probability that $x$ will be drawn from that distribution (which is 0). See more info here . I see that it's just calculated as the Negative Log Likelihood/Cross Entropy, or MSE. In the real case, $p_\theta(x|z)$ is typically $\mathcal{N}(\mu, \sigma^2I)$ where $\mu$ and $\sigma$ are deterministic functions of $z$ (computed via a neural network). In the discrete case, $p_\theta(x|z)$ is categorical, again with parameters determined by neural network. It just happens that the log of a gaussian density evaluated at $x$ comes out to be the MSE, and the log of the categorical distribution evaluated at $x$ comes out to be cross-entropy. A model is very different to a probability; how can I reconcile these two descriptions? A model is a function which maps parameters $\theta$ to some probability measure over the sample space. You can think of $p_\theta$ as an instantiation of the model with parameters $\theta$ , and $p_\theta(x|z)$ as the (ratio of the) probability of some event(s) in the sample space according to that model.
