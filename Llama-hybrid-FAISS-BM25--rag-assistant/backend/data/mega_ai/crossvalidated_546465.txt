[site]: crossvalidated
[post_id]: 546465
[parent_id]: 546451
[tags]: 
You are correct, choosing between two algorithms based on their test set evaluations causes contamination of your test set. What you should do is select the best performing algorithm during the CV stage and evaluate only that algorithm on the test set. Think about it this way, when you are choosing between a random forest with tree depth x vs tree depth y during CV this is no different than choosing between random forest with tree depth x and a SVR. Consider the algorithm selection as part of the hyper parameter tuning. It is important to note also that the test set is never wasted data. After the generalization error is estimated using the test set, the appropriate action is to recombine your data again and perform CV on the whole dataset road select again your hyper parameters/algorithm. In this second iteration ignore the error rates you find as you already determined that in the previous step
