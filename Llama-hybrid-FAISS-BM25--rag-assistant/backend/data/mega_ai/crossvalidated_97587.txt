[site]: crossvalidated
[post_id]: 97587
[parent_id]: 97586
[tags]: 
Your $\delta$ is known in the forecasting community as the MAPE, the Mean Absolute Percentage Error. It is of course only defined if all $t_i\neq 0$, and it really only makes sense if all $t_i>0$. It has the disadvantage of penalizing overestimates much more than underestimates: the error is bounded by 100% for underforecasts, but unbounded for overforecasts. Thus, a MAPE-"optimal" prediction will be biased low . Forecast accuracy is a frequent topic in forecasting. This link gives an overview of possible error measures. Unfortunately, there is no "universally accepted" relative error measure. The MAPE (your $\delta$) is frequently used among practitioners that do time series forecasting for sales, because it has a simple interpretation, but see the weaknesses above. A variant where we don't take the simple average of absolute percentage errors is the "weighted MAPE", where we take a weighted average, weighting each absolute percentage error by the actual : $$\text{wMAPE} := \frac{\sum_{i=1}^n|p_i-t_i|}{\sum_{i=1}^n t_i}$$ This is still a percentage error ( total absolute error as a percentage of total actuals), but it can deal with some zero actuals, and it doesn't bias the prediction as much. The Mean Absolute Scaled Error (see the link above ) is gaining traction in the academic forecasting community. It again is a relative error, giving the sum of the absolute forecast error divided by the sum of the in-sample error from some suitable benchmark method. In time series forecasting, you will usually use the naive forecast (the forecast is the last observation) as the in-sample benchmark, and occasionally use the seasonal naive forecast (the forecast is the observation from one year back). Depending on your particular prediction setting, you may be able to find a similar benchmark. The MASE then basically tells you whether your measured method out-of-sample beats the benchmark method in-sample. The MASE is still relative, but really harder to interpret, and I haven't seen any practitioners adopt it. The debate about error measures has been raging for decades in the forecasting community, and it hasn't really died out because we found a solution, but because there doesn't seem to be a good one. Serious forecasters will usually assess their forecasts using multiple error measures. If forecasting method A beats method B on error measure X, but vice versa on measure Y, then there is probably not all that much to choose between the two methods.
