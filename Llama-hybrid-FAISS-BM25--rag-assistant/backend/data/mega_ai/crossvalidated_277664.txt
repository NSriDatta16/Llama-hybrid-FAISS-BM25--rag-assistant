[site]: crossvalidated
[post_id]: 277664
[parent_id]: 
[tags]: 
Measuring goodness of fit when making a localized average of a function with uncertainty in $x$ and $y$

I have a function $y=f(x)$ that is defined on the interval $[0, 1]$. I would like to estimate $y_0=f(x_0)$ at some location $x_0$. I have a procedure with multiple free parameters $\vec \alpha$ that estimates $y_0 = f(x_0; \vec \alpha)$. The procedure terminates with estimates $x_0'$ and $y_0'$ as well as $1\sigma$ standard deviations in these quantities, call them $\sigma_y$ and $\sigma_x$. I want to search for the optimal parameters $\hat \alpha$ that give me the best fit at that location in my function. My difficulty is in formulating the objective function. Is a good objective function simply $$ \chi^2(\vec \alpha) = \left( \dfrac{x_0' - x_0}{\sigma_x} \right)^2 + \left( \dfrac{y_0' - y_0}{\sigma_y} \right)^2 $$ or can I do better? My worry with this kind of objective function is that it ignores the fact that I am estimating a function rather than a single value, and so the behavior of $x_0$ at $x_0'+\sigma_x$ may be drastically different than the behavior at $x_0'$. Another worry is the fact that the $\sigma$s depend on $\vec \alpha$, and so this optimization procedure may choose to inflate $\sigma_x$ and $\sigma_y$ instead of getting good estimates $x_0'$ and $y_0'$. Any advice?
