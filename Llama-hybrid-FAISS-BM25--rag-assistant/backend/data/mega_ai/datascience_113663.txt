[site]: datascience
[post_id]: 113663
[parent_id]: 113618
[tags]: 
There are various cases where a problem works better with a simpler representation of text than word embeddings: Data size: if it's too small, the model may overfit because the embeddings give too much precision. Generally embeddings are more subtle so they require more data diversity. The selected embeddings are not suitable for the data, e.g. general text embeddings may not work well with scientific texts, social media data, etc. Embeddings are trained on some data, so if this training data is too different from the data for the application then it won't give good results. Generally one should never assume that a method is always better than another, as per the No Free Lunch theorem.
