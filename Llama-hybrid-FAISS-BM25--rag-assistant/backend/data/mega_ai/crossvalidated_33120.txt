[site]: crossvalidated
[post_id]: 33120
[parent_id]: 32991
[tags]: 
I doubt you can come up with a model with analytic solution, but the inference can still be made tractable using right tools as the dependency structure of your model is simple. As a machine learning researcher, I would prefer using the following model as the inference can be made pretty efficient using the technique of Expectation Propagation: Let $X(t)$ be the outcome of $t$-th trial. Let us define the time-varying parameter $\eta(t+1) \sim \mathcal{N}(\eta(t), \tau^2)$ for $t \geq 0$. To link $\eta(t)$ with $X(t)$, introduce latent variables $Y(t) \sim \mathcal{N}(\eta(t), \beta^2)$, and model $X(t)$ to be $X(t) = 1$ if $Y(t) \geq 0$, and $X(t) = 0$ otherwise. You can actually ignore $Y(t)$'s and marginalize them out to just say $\mathbb{P}[X(t)=1] = \Phi(\eta(t)/\beta)$, (with $\Phi$ cdf of standard normal) but the introduction of latent variables makes inference easy. Also, note that in your original parametrization $\theta(t) = \eta(t)/\beta$. If you are interested in implementing the inference algorithm, take a look at this paper . They use a very similar model so you can easily adapt the algorithm. To understand EP the following page may found useful. If you are interested in pursuing this approach let me know; I can provide more detailed advice on how to implement the inference algorithm.
