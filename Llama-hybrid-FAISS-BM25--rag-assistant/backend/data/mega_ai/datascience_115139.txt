[site]: datascience
[post_id]: 115139
[parent_id]: 
[tags]: 
How long is the generator pre-trained in SeqGAN?

I am reading up about SeqGAN and I am trying to understand the pretraining step better. The authors claim they want to maximize the Maximum Likelihood Estimation on the dataset S by pretraining the generator on it (see pseudocode below). This is achieved by minimizing the negative log likelihood over the sequences. However, both from the paper and the code, it is unclear to me what stopping criterion they chose for training. Sure, they have a pre-set number of episodes the models run, but I would like to understand the idea behind it. What makes more sense here: pre-train the generator until its loss is absolutely minimal on the training data , or using early stopping as soon as the loss on the validation data increases? Normally I would choose the latter, but maybe there is a good argument to be made for overfitting on the training data in the pretraining step.
