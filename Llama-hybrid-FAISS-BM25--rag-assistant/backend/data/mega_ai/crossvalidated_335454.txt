[site]: crossvalidated
[post_id]: 335454
[parent_id]: 
[tags]: 
Word2Vec - Why do we take input-hidden layer weights as word embeddings

I am currently trying to understand how the Word2Vec neural network works, but do not understand why we choose to take the weight vectors between the input and hidden layer as our word embedding vectors. If we are using both sets of weights(input-hidden and hidden-output) to predict the context vectors from a word vector, why do we only the hidden-output weight vectors represent our word embeddings?
