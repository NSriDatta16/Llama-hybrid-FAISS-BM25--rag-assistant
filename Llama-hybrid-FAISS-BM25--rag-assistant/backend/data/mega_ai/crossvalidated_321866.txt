[site]: crossvalidated
[post_id]: 321866
[parent_id]: 321851
[tags]: 
I wouldn't call it a main theorem, but I think the following (sometimes referred to as the Universal approximation theorem) is an interesting (and at least for me surprising) one as it states the approximative power of feed-forward neural networks. Theorem: Let $\sigma$ be a nonconstant and monotinically-increasing continous function. For any continuos function $f:[0,1]^m\to\mathbb{R}$ and any $\epsilon>0$, there exist an integern $N$ and an multilayer perceptron $F$ with one hidden layer having $N$ Neurons that has $\sigma$ as activation function so that $$|F(x)-f(x)|\le\epsilon$$ for all $x\in[0,1]^m$. Of course, as this is a statement on existence , its impact for practitioners is negligible. A proof can be found in Hornik, Approximation Capabilities of Muitilayer Feedforward Networks, Neural Networks 4 (2), 1991,
