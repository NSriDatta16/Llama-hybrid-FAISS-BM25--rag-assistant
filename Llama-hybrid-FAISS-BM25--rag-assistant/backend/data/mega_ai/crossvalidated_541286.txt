[site]: crossvalidated
[post_id]: 541286
[parent_id]: 
[tags]: 
Has Fundamental Theorem of Statistical Learning been proven for infinite classes of functions?

I am reading the book "Understanding Machine Learning" by Shai Shalev-Shwartz and Shai Ben-David. The theorem 6.7 has several equivalent statements for a class of functions $H$ . The first three are: $H$ has the uniform convergence property. Any ERM rule is a successful agnostic PAC learner for $H$ . $H$ is agnostic PAC learnable. For the proof of inference 1 $\rightarrow$ 2 the book refers to the chapter 4, where the results are proven only for finite classes. It says, inference 2 $\rightarrow$ 3 is trivial. If the ERM rule exists, and it is a successful agnostic PAC learner, then $H$ is agnostic PAC learnable, of course. But it is based on an assumption that ERM rule exists. ERM rule is the rule which, given a sample $S$ , finds hypothesis with minimal empiric risk among all functions in $H$ . Whole proof of the theorem depends on the 2 $\rightarrow$ 3 inference. Is there a proof that ERM rule exists always, or is there a way to see that the theorem is true regardless?
