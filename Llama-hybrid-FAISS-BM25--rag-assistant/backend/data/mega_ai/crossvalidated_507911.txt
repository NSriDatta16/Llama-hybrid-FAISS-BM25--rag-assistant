[site]: crossvalidated
[post_id]: 507911
[parent_id]: 499696
[tags]: 
There are multiple concepts in this question. In statistics, you will find there are theorems for certain models about what methods produces the "best" parameter estimates. In linear regression with normally distributed errors, you can prove that the normal statistical parameter estimates are the BLUE (Best linear unbiased estimates) of the regression parameters. In other contexts, you find MLE (maximum likelihood estimates) and MVUE (minimum variance unbiased estimates) are the best we can do statistically. When you cross into machine learning and start to use a loss function for an optimization, you are stepping away from the statistical concept of what is "best" and instead thinking about the parameters that produce the minimum cost/loss (in training, validation, or test). Further, what makes a good loss function in machine learning is also related to guarding against overfitting. The MLE doesn't have any such considerations. Once the likelihood function is specified, there isn't any consideration for overfitting, you are only attempting to find the parameters that maximize the likelihood.
