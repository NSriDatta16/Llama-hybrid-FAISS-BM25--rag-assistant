[site]: crossvalidated
[post_id]: 223918
[parent_id]: 
[tags]: 
Multilevel metaregression in R, "redundant predictors dropped" (metafor)

First time doing a meta-analysis, and I'd prefer not to botch this. Out of a set of studies, I have six studies that each have used a pair of measures (on the same participants) that are commonly averaged to one value. However, there are theoretical reasons that it might be better to use these measures separately. I am testing this by comparing whether the effect sizes from measure 1 are different from the effect sizes from measure 2. So I have 12 correlations from 6 studies, which introduces dependency between correlations from the same study. Using the metafor package, I use rma.mv and the random argument to account for the dependency: pd where the source denotes the study, with six levels, and the compare denotes the measure, with two levels. The results I get are: Multivariate Meta-Analysis Model (k = 12; method: REML) Variance Components: outer factor: source (nlvls = 6) inner factor: compare (nlvls = 2) estim sqrt fixed tau^2 0.0040 0.0634 no rho 1.0000 no Test for Residual Heterogeneity: QE(df = 10) = 22.9376, p-val = 0.0110 Test of Moderators (coefficient(s) 2): QM(df = 1) = 69.7315, p-val I'm interpreting that there is a difference between the two measures, as the model results row for compareConsEc is much smaller than the intercept (it read somewhere that the use of Q statistic is not recommended). However, the warning indicates that a predictor has been dropped, and since the compare is there but the estimate for the study-level is not, I assume the study-level was dropped. Yet, when I run the same analysis in rma without the random argument, the results are not (exactly) the same, which suggests that it is not simply dropping the random argument but doing something else differently as well. My questions are: Is the analysis and syntax correct in principle? Reading this , I interpret that I have too few datapoints to estimate a two-level model. Does that mean that it is simply not possible to take the study-level dependency into account in the analysis? If so, does that mean I should run a rma without the random argument instead, acknowledging in the report that this dependency might confuse the results? (and if so, to which direction?) Or is there a better option, such as an alternative analysis? Why are the results from rma and rma.mv different? Is the tau^2 useful in that table, with a predictor being dropped and the dependency being unaccounted for? Why is the rho 1? I used REML method, as a default. What would be a good source to the respective pros and cons of different methods?
