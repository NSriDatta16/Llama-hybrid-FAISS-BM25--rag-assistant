[site]: datascience
[post_id]: 102939
[parent_id]: 102930
[tags]: 
It is great you are thinking like this. Often when we build a model, we do not think how we will get the data for the next model build or the biases in this building model from previous models. In many models, like fraud or credit, the current model and business policy biases future data. I have built fraud models with event rates I think the random process should be turned into a test - check out design of experiments. Then you test specifically from areas of the population to get good coverage. It is hard to test in fraud for business reasons, so like you said, your test data will be thin. You may also not want to test in certain areas that are too risky - like if your model is about financial fraud and you do not want to test where the loss may be > $10,000. In your train/validation/holdout data sets, include some of the test data. Run specific metrics - whatever makes sense for your business problem - on the test and non-test data. Now you have some unbiased metrics and a potentially better view how the model will perform on newer data. If the test data is very thin, put it all into the holdout set.
