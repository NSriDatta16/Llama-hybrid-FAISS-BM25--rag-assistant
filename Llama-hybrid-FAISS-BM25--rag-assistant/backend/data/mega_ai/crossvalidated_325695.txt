[site]: crossvalidated
[post_id]: 325695
[parent_id]: 325676
[tags]: 
It would help if you were to provide information regarding the magnitude of the noise or overdispersion in these features. This can be as simple as providing means, medians and std deviations in a tabular format. My experience is that K-means, as with many traditional methods, is fairly robust to violations of normality when the magnitude of the noise is not large . In this sense, K-means differs significantly from, e.g., OLS regression assumptions that the errors be normally distributed (not the inputs). K-means does not have similar assumptions. In fact and for the most part, assumptions wrt K-means are rules of thumb, conventions and heuristics that vary from discipline to discipline as well as varying as a function of highly subjective decisions made by the analyst. For you this means that there is no 'cookbook of rules' in existence wrt K-means which stipulates what you, the analyst, should or should not do in driving to a solution. This suggests additional K-means limitations: It's not recommended for use with longitudinal information (time series) Solutions are sensitive to dependence among features The number of clusters has to be pre-specified and, given that number, K-means always finds a solution. This solution may or may not accurately represent the behavior of the data Solutions are always spherical around the cluster centroids. If the 'real' clusters in the data are differently shaped, K-means will not capture patterning other than spherical It does not return reasonable results with noncontinuous features or mixtures of continuous and discrete information If the seeds used in forming the clusters are automatically created from random data draws, extreme values in the features can cause sparsity and nonrepresentative distortion in the results As aksakal notes, k-medians is one workaround to the issue you've raised. Assuming the ready availability of that routine, its results should be compared with solutions using k-means. Wrt rules of thumb, a common prescription in marketing is first, standardize the features to a mean of 0 and std dev of 1 then, second, reduce the dimensionality of the data using PCA (or similar methods). In part, this helps mitigate feature dependence. This also smoothes 'lumpiness' that can result from using the full or unreduced matrix of features. Finally and for your purposes dimension reduction could also help blunt the impact of extreme or overdispersed information. I've found such pre-clustering, dimension reduction methods helpful, that is when they work -- which is most of the time. However, what does one do in the case of large magnitude noise? To facilitate a deeper understanding of the issues that raises, Xie and Xing's paper Cauchy Principal Component Analysis (ungated here ... http://www.cs.cmu.edu/~pengtaox/papers/cpca.pdf ) presents a typology of data matrices as a function of sparsity and noise. Despite its focus on PCA, their discussion is informative as it can be generalized to K-means. If Xie and Xing's discussion is too technical, there are many simpler transformations that can be applied to the extreme features. For positive valued features, the natural log transformation compresses heavy tails. Other possibilities include the Box-Cox transformation, the inverse hyperbolic sine, Lambert's W , and so on. Formulas for all of these are readily obtainable online. The bottom line is that there is no 'one size fits all' answer to your question.
