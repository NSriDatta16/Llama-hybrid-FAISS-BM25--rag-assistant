[site]: crossvalidated
[post_id]: 303794
[parent_id]: 
[tags]: 
Early stopping and bounds

Say we are training neural networks using a train set and set aside a validation set V. We obtain models h's after each epoch along with the validation losses(0-1 loss) $\hat{L}(h_1,V)$, $\hat{L}(h_2,V)$ ... if we use the early stopping rule suggested here (top answer). Is the resulting $\hat{L}(h_*,V)$ an unbiased estimate of the true loss? How can I bound the true loss using $\hat{L}(h_*,V)$ ? I'm guessing no for the first one since the stopping rule depends on the partition of our data set. Afaik the bounds that can be applied depends on the size of our hypothesis set and I'm not entirely sure if it's finite, countable or uncountable in this case.
