[site]: crossvalidated
[post_id]: 212724
[parent_id]: 181603
[tags]: 
Answer to your question 1 Appending $\bf 1$ to matrix $\bf X$ is adding the "intercept" term. Suppose you have $p$ features in data, without adding $\bf 1$ term, you are actually fitting $$y=\theta_1x_1+\theta_2x_2+\cdots+\theta_px_p$$ With appending $\bf 1$ , you are fitting. $$y=\theta_0+\theta_1x_1+\theta_2x_2+\cdots+\theta_px_p$$ . Similarly, you can try following two models in R to see difference lm(mpg~wt,data=mtcars) vs. lm(mpg~wt-1,data=mtcars) where first formula gives a fit of $mpg=\theta_0+\theta_1*wt$ and second formula gives a fit of $mpg=\theta_1*wt$ Answer to your question 2 In the Coursera course Andrew Ng spent a lot of time on iterative methods, instead of "analytical solution" / "normal equations". Which means he is teaching you how to get those weights by some algorithms that runs in many iterations. Rather than deriving the answers from algebra and calculate the weights from the formula. In linear regression case, using iterative method may not be necessary, (in fact R is not using it, R is using QR decomposition and solve it directly instead of gradient decent), but in many other complicated models, say neural network, iterative methods are extremely useful. PS. just found that more information can be found in this related post
