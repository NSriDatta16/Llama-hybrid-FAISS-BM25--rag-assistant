[site]: datascience
[post_id]: 69742
[parent_id]: 69700
[tags]: 
Indeed, convolution and cross-correlation are closely related. The former is a bit more natural in some areas of mathematics; most notably, in the convolution theorem for the Fourier transform, which states that the Fourier transform of the convolution of two functions is equal, under certain conditions, to the product of their Fourier transforms: $$ \mathcal{F}\{f * g\} = \mathcal{F}\{f\} \mathcal{F}\{g\}, $$ thus providing an efficient way to compute the convolution. In the context of CNN, there difference between the convolution and the cross-correlation is irrelevant. In the discrete two-dimensional case $$ (f {**} g)[n_1, n_2] = \sum_{m_1 = -\infty}^{+\infty}\sum_{m_2 = -\infty}^{+\infty} f[m_1, m_2] \, g[n_1 - m_1, n_2 - m_2], $$ $$ (f' {\star\!\star} g)[n_1, n_2] = \sum_{m_1 = -\infty}^{+\infty}\sum_{m_2 = -\infty}^{+\infty} \overline{f'[m_1, m_2]} \, g[n_1 + m_1, n_2 + m_2]. $$ If $f$ is the filter and $g$ is the output of the previous layer of the CNN, you can see that the convolution is equivalent to cross-correlation with a filter $$ f'[m_1, m_2] = \overline{f[-m_1, -m_2]}, $$ which is just a reflection around the secondary diagonal since all values are real. In short, there are no benefits to using the true convolution in CNN. The only difference would be that the learned filters would be mirrored about the secondary diagonal.
