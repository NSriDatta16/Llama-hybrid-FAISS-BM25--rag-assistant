[site]: crossvalidated
[post_id]: 340656
[parent_id]: 71782
[tags]: 
I too was seduced by both bootstrapping and Bayes' theorem, but I couldn't make much sense of the justifications of bootstrapping until I looked at it from a Bayesian perspective. Then - as I explain below - the bootstrap distribution can be seen as a Bayesian posterior distribution, which makes the (a?) rationale behind bootstrapping obvious, and also had the advantage of clarifying the assumptions made. There is more detail of the argument below, and the assumptions made, in https://arxiv.org/abs/1803.06214 (pages 22-26). As an example, which is set up on the spreadsheet at http://woodm.myweb.port.ac.uk/SL/resample.xlsx (click on the bootstrap tab at the bottom of the screen), suppose we've got a sample of 9 measurements with a mean of 60. When I used the spreadsheet to produce 1000 resamples with replacement from this sample and rounded the means off to the nearest even number, 82 of these means were 54. The idea of bootstrapping is that we use the sample as a "pretend" population to see how variable the means of samples of 9 are likely to be, so this suggests that the probability of a sample mean being 6 below the population mean (in this case the pretend population based on the sample with a mean of 60) is 8.2%. And we can come to a similar conclusion about the other bars in the resampling histogram. Now let's imagine that the truth is that the mean of the real population is 66. If this is so our estimate of the probability of the sample mean being the 60 (i.e. the Data) is 8.2% (using the conclusion in the paragraph above remembering that 60 is 6 below the hypothesised population mean of 66). Let's write this as P(Data given Mean=66) = 8.2% and this probability corresponds to an x value of 54 on the resampling distribution. The same sort of argument applies to each possible population mean from 0, 2, 4 ... 100. In each case the probability comes from the resampling distribution - but this distribution is reflected about the mean of 60. Now let's apply Bayes' theorem. The measurement in question can only take values between 0 and 100, so rounding off to the nearest even number the possibilities for the population mean are 0, 2, 4, 6, ....100. If we assume that the prior distribution is flat, each of these has a prior probability of 2% (to 1 dp), and Bayes' theorem tells us that P(PopMean=66 given Data)= 8.2%*2%/P(Data) where P(Data) = P(PopMean=0 given Data)*2%+ P(PopMean=2 given Data)*2% + ... + P(PopMean=100 given Data)*2% We can now cancel the 2% and remember that sum of the probabilities must be 1 since the probabilities are simply those from the resampling distribution. Which leaves us with the conclusion that P(PopMean=66)=8.2% Remembering that 8.2% is the probability from the resampling distribution corresponding to 54 (instead of 66), the posterior distribution is simply the resampling distribution reflected about the sample mean (60). Further, if the resampling distribution is symmetrical in the sense that asymmetries are random - as it is in this and many other cases, we can take the resample distribution as being identical to the posterior probability distribution. This argument makes various assumptions, the main one being that the prior distribution is uniform. These are spelled out in more detail in the article cited above.
