[site]: crossvalidated
[post_id]: 261619
[parent_id]: 260649
[tags]: 
Question 1 I am not aware of any canonical definition, and your questions suggests that this term is used with different meanings. Let's start with simple examples (which will answer question 2). Question 2 The ridge regression may be a good starting point. It is a regularization method that circumvent the issue raised by a singular matrix . However, the "regularization parameter" defined in gradient boosting methods (per example) is here to ensure a low complexity for the model. Question 3 Normalization as regularization has another meaning (and this terminology is quite misleading). It turns a complex problem "from the gradient descent point of view" into something simpler. Though it is not needed to calibrate a neural network, it really helps during the calibration. (However, note that if we could find the global extrema of arbitrary functions, normalization would not be needed) Question 4 Regularization (as a way to reduce the complexity of a model) is used to reduce overfit. The less complex a model is, the less likely it is to overfit. Aside S. Watanabe makes a rigorous usage on this terminology in his research.
