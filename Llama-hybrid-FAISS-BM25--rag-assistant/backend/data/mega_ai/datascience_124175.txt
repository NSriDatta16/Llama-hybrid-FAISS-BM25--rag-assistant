[site]: datascience
[post_id]: 124175
[parent_id]: 123229
[tags]: 
I also wanted to understand why is A initialized randomly and B to zero. Would it make a difference if it would be the other way around (A zero, B random?). I think they just want initial step of fine tuning to be like we fine tune with only pretrained weight (no additional weight affect the result). One of A or B have to be all zero, so (A zero, B random?) should work too. Also, what would go wrong if both would be set to zero? Initializing both of them to zero might cause some issue, I think it is the same reason why we don't initialize deep learning model with zero. The gradient signal sent from A to B will be the same (all zero) and each node in A will look like the same to B. Related link Initialize perceptron weights with zero
