[site]: datascience
[post_id]: 45574
[parent_id]: 45566
[tags]: 
There is a situation called "exploding gradients" where very large error gradients accumulate and result in very large updates to neural network model weights during training. This can result in an unstable model that cannot learn from the training data or even result in NaN weight values that can no longer be updated.
