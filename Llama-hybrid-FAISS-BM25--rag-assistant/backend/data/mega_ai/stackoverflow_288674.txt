[site]: stackoverflow
[post_id]: 288674
[parent_id]: 87304
[tags]: 
Good answers here. Just how you implement it is dependent on what you need it for. I prefer the running average one myself "time = time * 0.9 + last_frame * 0.1" by the guy above. however I personally like to weight my average more heavily towards newer data because in a game it is SPIKES that are the hardest to squash and thus of most interest to me. So I would use something more like a .7 \ .3 split will make a spike show up much faster (though it's effect will drop off-screen faster as well.. see below) If your focus is on RENDERING time, then the .9.1 split works pretty nicely b/c it tend to be more smooth. THough for gameplay/AI/physics spikes are much more of a concern as THAT will usually what makes your game look choppy (which is often worse than a low frame rate assuming we're not dipping below 20 fps) So, what I would do is also add something like this: #define ONE_OVER_FPS (1.0f/60.0f) static float g_SpikeGuardBreakpoint = 3.0f * ONE_OVER_FPS; if(time > g_SpikeGuardBreakpoint) DoInternalBreakpoint() (fill in 3.0f with whatever magnitude you find to be an unacceptable spike) This will let you find and thus solve FPS issues the end of the frame they happen.
