[site]: crossvalidated
[post_id]: 337179
[parent_id]: 337151
[tags]: 
I'm assuming this is due to some sort of underfitting, or we need to better diagnose what's happening with the model? I would agree that it is under-fitting for the Logistic regression (first case) and over-fitting for the DecisionTreeClassifier (second case). With regards to your original question: Validation scores above training scores This can happen because of the randomness of the data within your training/validation folds. For example, say you have a ZeroR classification model that only predicts one class, e.g. $(1, 1, 1, \ldots)$. Well, if your training dataset contains 60% of $1$s and your validation dataset contains 70% of $1$s, your training accuracy = 60%, but your validation accuracy = 70%. As you train/validate over different folds, the variations in the sampling of labels can cause the train/validation accuracies to "invert". I also think your observation that the validation scores are higher than the training scores are accentuated because the scales on the two graphs are different (the range in the first graph is very small compared to the second). If you re-plot the first graph with the same range as the second, you might not notice that behaviour as easily. You may find your Logit model is actually just taking the most-dominant class (c.f. ZeroR) as a proxy for its prediction; hence your under-fitting ("high-bias") problem.
