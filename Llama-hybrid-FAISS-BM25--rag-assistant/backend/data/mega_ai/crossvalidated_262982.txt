[site]: crossvalidated
[post_id]: 262982
[parent_id]: 261553
[tags]: 
I'm afraid you are mixing two different things. Naive Bayes algorithm uses Bayes theorem for classifying $Y$ given some data $X$ using marginal $p(Y=k)$ and conditional $p(x_i \mid Y = k)$ probabilities: $$ \DeclareMathOperator*{\argmax}{arg\,max} \hat y = \argmax_{k\in \{1,\dots,K\}} \Big\{ p(Y=k) \prod_i p(x_i \mid Y = k) \Big\} $$ On another hand, in your beta-binomial example, you refer to Bayesian model that assigns prior to unknown parameter value $\theta$ to estimate it. This is a well-known beta-binomial model , where Bayes theorem is applied to obtain posterior distribution of $\theta$: $$ p(\theta|X) \propto p(\theta) \, p(X|\theta) $$ and if you are interested in point estimate, you simply take $$ \hat\theta= \argmax_{\theta\in\Theta} \Big\{ p(\theta) \, p(X|\theta) \Big\} $$ where $p(\theta)$ is a beta prior distribution for possible values of $\theta$ and $p(X|\theta)$ is the likelihood , that in this case is a binomial distribution of $X$ parametrized by $\theta$. So in the first case you do not use any priors (what makes it a non-Bayesian approach) and use only empirical probabilities for classification . In the second case, you assume some prior distribution for the unknown parameter, to estimate it. To use the second approach for classification you would need to introduce some decision rule to translate what you have learned about your model and it's parameters to actual predictions. So you are right, $Y$ is not a parameter and we do not assign priors to it in the case of naive Bayes algorithm. Those are two different things, yet both use Bayes theorem. As about the question in your title, Bayesian approach can be used and is used in machine learning, but this is a very broad topic. To learn more you can refer, for example, to the following book: Murphy, K.P. (2012). Machine learning: a probabilistic perspective . MIT press.
