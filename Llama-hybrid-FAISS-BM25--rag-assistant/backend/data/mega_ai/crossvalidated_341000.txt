[site]: crossvalidated
[post_id]: 341000
[parent_id]: 340842
[tags]: 
The question is somewhat out of focus as it mixes the issue of inference (to wit, Bayesian inference in this case) and computation (i.e., Metropolis-Hastings approximation to the posterior distribution). The prior selection influences inference and estimates, as it should. I suggest reading some introductory material to Bayesian statistics to this effect, not in the least suggesting my own books! In case one is incapable of setting a prior, there are reference solutions (e.g., Jeffreys priors), and algorithms that provide an assessment of the choice being made. To test an algorithm implementation like MH it is completely acceptable to start with a prior distribution that strongly agrees with one's (simulated) data. An easy solution in this regard is to generate a parameter from the prior (provided it is proper) and a dataset conditional on this generated parameter. The posterior distribution (as approximated by MH) should then "cover" the value generated unless the model is ill-defined, e.g., unidentifiable.
