[site]: datascience
[post_id]: 28978
[parent_id]: 
[tags]: 
Word2Vec benefit in LSTM

if Word2Vec is nothing but a transformation of one-hot into a dense vector, why can't I just feed one-hot into LSTM (or for that matter sacrifice first dense layer, in any network that will end up using the embedding) and call it a day? Why would I actually spend time pre-computing Word2Vec embeddings? Yes, the resulting embedings are vectors that are clustered together if the words have similar meaning. But I presume a feed-forward classifier would figure out a good internal model anyway?
