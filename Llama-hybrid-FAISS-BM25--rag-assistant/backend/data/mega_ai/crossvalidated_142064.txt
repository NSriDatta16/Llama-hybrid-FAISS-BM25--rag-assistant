[site]: crossvalidated
[post_id]: 142064
[parent_id]: 142053
[tags]: 
To be honest, in the absence of a clear theory, I'd be inclined to frame the model in terms of orthogonal polynomials, and then put priors on the higher order terms that indicate that I expect them to be small -- which would tend to pull the posterior toward zero, rather than be uninformative. If I wanted an uninformative prior, I'd still be tempted to do it with orthogonal polynomials. Edit: Elvis' question about why orthogonal polynomials is important -- One big advantage of orthogonal polynomials is that (because of the orthogonality) lower order coefficients are not affected if the higher order coefficients are shrunk or even set to 0. It makes it more sensible to try to do things like order selection with averaging of parameter estimates across models. (It's less important if interest is only on the function as a whole, but sometimes the values of particular coefficients, or functions of them, can be important.) (There may also be some advantages in accuracy and computational effort; in particular if you're doing all possible degrees of polynomial, there can be an advantage in effort.) The polynomials are orthogonal in the following sense - $\sum_i p_j(x_i)\,p_k(x_i)=0$ (when $j\neq k$).
