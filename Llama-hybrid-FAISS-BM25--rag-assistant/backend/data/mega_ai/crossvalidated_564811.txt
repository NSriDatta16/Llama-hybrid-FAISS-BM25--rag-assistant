[site]: crossvalidated
[post_id]: 564811
[parent_id]: 564809
[tags]: 
How do I learn the variance of my Gaussian model using a neural network? The problems you outline arise because you're not estimating the variance of the target distribution , you're estimating the variance of the predictions . You need a network that has two outputs, $\hat y$ and $ \hat \sigma$ . Then use $$ \sum_{i=1}^{m} \left[ \frac{\left(\hat{y}_i - y_i \right)^2}{2 \hat \sigma_i^2} + \log(\hat \sigma_i) \right] $$ as the loss function, taking three arguments: $\hat \sigma_i, \hat y_i, y_i$ . (If each $y_i$ is a vector, then you'll generalize to the multivariate Gausisan case, which is straightforward to derive from the multivariate normal pdf.) You'll either need to enforce that $\sigma_i$ is positive or parameterize your loss (and model) in terms of $\gamma_i = \log \sigma_i$ (which can be either positive or negative). I just pick a weight and say "this is the variance" and off the MSE optimizer goes to optimize things? Not a weight, an output of the model. But yeah, one of the outputs is $\hat \sigma_i$ and one of the outputs is $\hat y_i$ . This is supervised learning, how do I pick a target for $\sigma$ ? You don't need targets for $\sigma$ because the negative log-likelihood does it all. If $\hat \sigma_i$ gets too large, then $\log \hat \sigma_i$ gets larger. If $\hat \sigma_i$ gets too small, then the MSE term gets too large. You want to find a compromise value that minimizes the loss function, so you optimize the loss. See also: Improvement in NN regressor by Negative Log Liklihood loss vs MSE loss
