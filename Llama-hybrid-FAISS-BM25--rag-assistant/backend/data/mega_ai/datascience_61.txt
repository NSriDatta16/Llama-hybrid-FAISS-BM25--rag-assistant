[site]: datascience
[post_id]: 61
[parent_id]: 
[tags]: 
Why Is Overfitting Bad in Machine Learning?

Logic often states that by overfitting a model, its capacity to generalize is limited, though this might only mean that overfitting stops a model from improving after a certain complexity. Does overfitting cause models to become worse regardless of the complexity of data, and if so, why is this the case? Related: Followup to the question above, " When is a Model Underfitted? "
