[site]: datascience
[post_id]: 65684
[parent_id]: 
[tags]: 
How do tech-companies employ Random Forest on large data sets?

The algorithm takes quite a long time to train on large data sets with a moderate number of parameters: https://stats.stackexchange.com/questions/37370/random-forest-computing-time-in-r https://stackoverflow.com/questions/34997134/random-forest-tuning-tree-depth-and-number-of-trees https://stackoverflow.com/questions/31278688/how-can-you-reduce-the-default-ntree-500-parameter-passed-to-rf-from-caret I've been trying to run it on a ~25,000 row data set with 36 predictors and it has been using 6GB of RAM for over 2 hours. Are there instances where this algorithm is used in production or is being run daily? If so, how does one approach re-training it or optimising it for large data sets?
