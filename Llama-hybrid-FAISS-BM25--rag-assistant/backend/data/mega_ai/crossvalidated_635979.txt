[site]: crossvalidated
[post_id]: 635979
[parent_id]: 
[tags]: 
Why is the maximum path length for convolutional layer $O(n/k)$ in attention is all you need paper?

In the table-1 third row it is being mentioned. Why is it $O(n/k)$ ? Take for example 1d convolution of 2 over 9 tokens with stride $1$ . It won't be $n/k$ or $9/2=4.5$ rather it would be roughly $n-1$ or $8$ . The paper did not mention about stride, so I assumed it would work for any stride. From paper: ( link ) A single convolutional layer with kernel width k Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(logk(n)) in the case of dilated convolutions [18],
