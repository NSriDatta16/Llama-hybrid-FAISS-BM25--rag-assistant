[site]: crossvalidated
[post_id]: 531831
[parent_id]: 
[tags]: 
How to intepret a test statistic in terms of the equation outcome = model + error

I am reading Andy Field's An Adventure in Statistics . In the book, he mentions that all statistical models are basically a variation of the equation: $outcome_i = model + error_i$ . For the sake of simplicity, let's just assume it's $outcome_i = b_0 + error_i$ (equation 1). There are no variables, just a single parameter. Next, he mentions that $test\:statistic = \frac{signal}{noise} = \frac{parameter\:estimate \:(b)}{standard \:error \:of\:b}$ (equation 2) and that the way we use this in NHST is that each test statistic has an associated probability distribution, and we essentially want the probability of obtaining the test statistic under the null. Now, I'm trying to relate equation 1 to equation 2 in the simple case where we are just doing a t-test to see if the population mean is really as it is claimed to be e.g let's say we want to test the (probably unrealistic) hypothesis that the number of car accidents per year is 1 per person on average. So now, we collect a sample of people, and we fit a model, which really just means we compute the mean number of car accidents per person in that sample. That's our $b_0$ . Say it's 0.5. Equation 2 seems to suggest that the test statistic is $t = \frac{parameter\:estimate \:(b)}{standard \:error \:of\:b} = \frac{0.5}{standard\:error\:of\:b}$ which doesn't make a lot of sense to me since it seems that we'd want $0.5 - 1$ in the numerator, where 1 here is what the null is. I'm wondering what I'm misunderstanding here.
