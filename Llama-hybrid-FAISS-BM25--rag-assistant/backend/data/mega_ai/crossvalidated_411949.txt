[site]: crossvalidated
[post_id]: 411949
[parent_id]: 411935
[tags]: 
MLE is a general framework, the same approach could be applied to other distributions. It's not that difficult to find function that you need to minimize with gradient descent. For example, MLE for poisson: https://en.wikipedia.org/wiki/Poisson_regression#Maximum_likelihood-based_parameter_estimation It's a bit more difficult for the negative binomial distribution because it has two parameters. In the gaussian distribution, there are also two parameters, but one of them is fixed (variance). For the negative binomial, you also need to fix one of the parameters. If you're interested in integer predictions then you need to fix parameter that corresponds to the probability of success. You can use similar approach in order to find log likelihood for the function that you need to optimize: https://en.wikipedia.org/wiki/Negative_binomial_distribution#Maximum_likelihood_estimation In general, both of these approaches follow the same procedure. First, you find log-likelihood for your estimations and then you apply gradient descent on the negative log-likelihood with respect to the parameter. Basically, you just create different loss functions for the neural network training. Of course, you need to use all your training samples in order to obtain the solution for the MLE.
