[site]: crossvalidated
[post_id]: 386422
[parent_id]: 386265
[tags]: 
For starters, let us show that indeed the underlying relationship of this dataset of size $M$ is linear. We are given that the data is generated by the relationship \begin{align} S_i &= S_0 + 100 i \boldsymbol{1}^T \tag*{$\forall i \in \lbrace 0, 1, \cdots, M-2, M-1 \rbrace$} \end{align} where $\boldsymbol{1} \in \lbrace 1 \rbrace^{100}$ and $S_0 = [0, 1, \cdots, 98, 99]$ . We are also told the label $l_i$ for $S_i$ is $l_i = i$ . With the definitions above, it is clear that the analytical solution to this model is indeed $l_i = S_i w$ , where $w = [w_0, w_1, \cdots, w_{99}]^T = [10^{-2}, 0, \cdots, 0]^T$ . We see this because \begin{align} l_i &= S_i w \\ &= S_0 w + 100 i \boldsymbol{1}^T w \\ &= 100 i w_0 \\ &= i \end{align} Thus, the data can indeed be captured by a linear model and consequentially one can model it exactly using a single layer neural network with linear activations. Now the question is, why are you struggling to converge to the unique global minimum of your corresponding optimization? While this data described above looks quite simple, it is actually interesting to see that the data produces a poorly conditioned optimization problem. We first note that your single layer neural network with linear activation is effectively defined as \begin{align} f(S) = Sw + b \end{align} where $w$ and $b$ need to be estimated using the data. For simplicity, I will define $\hat{S} = [S, 1]$ and $\hat{w}^T = [w^T, b]$ , making the neural net $f(\hat{S}) = \hat{S} \hat{w}$ . Since you are training with a Mean Squared Error (MSE) cost function, we know the optimization problem to solve is \begin{align} \min_{\hat{w}} \frac{1}{2N} \left(X \hat{w} - L\right)^T \left(X \hat{w} - L\right) \end{align} where $X^T = [S_0^T, S_1^T, \cdots, S_{M-1}^T]$ is a stacked set of the inputs and $L^T = [l_0, l_1, \cdots, l_{M-1}]$ is a stacked vector of all the output labels. This cost function is convex since the hessian is $H = X^T X$ , which is clearly positive semi-definite. Since this function is a paraboloid, we can investigate the condition number of the hessian $H$ to gain insight into how easy it will be to traverse the error surface with first order optimization methods. Performing a simple computation, one can find for $M = 1500$ that $\text{cond}(H) = O\left(10^{21}\right)$ , which is very large. This is telling us that the ellipsoid isosurfaces for the error surface have extremely skewed shapes. An example of this and the optimization progress can be seen in the visual below: As one can imagine based on the visual, the extremely skewed shapes greatly impact convergence to the actual local (global) minimum and can require many more iterations. In our actual example, since the condition number is so large, the skew is much, much worse than what's shown in the visual. Anyway, the situation affecting convergence in the above example is largely due to a poorly conditioned optimization problem. The poor convergence rates in the first image make me skeptical about the optimization code being used because a simple momentum-based gradient descent with line searches, which I wrote in a few minutes to investigate this problem, converges to $O(10^{-2})$ error in less than 10 iterations. However, even with a properly coded optimization algorithm, first order optimization methods should struggle to converge quickly for this problem.
