[site]: crossvalidated
[post_id]: 269687
[parent_id]: 226555
[tags]: 
It is a valid approach: this is a model ensemble, averaging being the basic ensemble method. If the base model prediction results are not highly correlated, and their prediction is not very bad, then you can combine the base models' predictions, combining the benefit of each model; the final result is always better than each base model's. Why is averaging so effective? I will cite the word from http://mlwave.com/kaggle-ensembling-guide/ : One may be mystified as to why averaging helps so much, but there is a simple reason for the effectiveness of averaging. Suppose that two classifiers have an error rate of 70%. Then, when they agree they are right. But when they disagree, one of them is often right, so now the average prediction will place much more weight on the correct answer.
