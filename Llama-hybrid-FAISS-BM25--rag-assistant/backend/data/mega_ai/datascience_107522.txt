[site]: datascience
[post_id]: 107522
[parent_id]: 107521
[tags]: 
Distillation (sometimes also called knowledge distillation or teacher-student training) is the technique of 'distilling' knowledge from a large neural network into a smaller neural network, generally allowing you achieve a similar performance (i.e. accuracy wise) with a smaller network. An example of such a model would be the DistilBERT model, which uses distilled knowledge from the BERT model. The way this technique works is that you have two models, one being the large model (also called teacher model) and a smaller model (also called the student model). During the training phase the student model tries to predict values and changes its weights based on a loss which combines both the error of the prediction of the student model and the error between the prediction of the teacher model and student model. The idea is that in this way the teacher model helps guide the student model to make the correct predictions. Some additional links with additional info on knowledge distillation are this documentation page from the Distiller package and this practical example from Weights & Biases .
