[site]: crossvalidated
[post_id]: 378781
[parent_id]: 378771
[tags]: 
Your approaches are unclear. So, here is my simple explanation of cross validation. Cross-validation is done to tune the hyperparamaters such that the model trained generalizes well (by validating it on validation data). So here is a basic version of held-out cross-validation: Train test(actually validation) split the data to obtain XTrain, yTrain, XVal, yVal Select a set of hyperparameter grid you want to search on. For i th hyperparameter combination: a. Train(fit) model on XTrain, yTrain b. Evaluate the model c. Evaluate the model on XVal, yVal i.e., compute the performance metric (accuracy, auc, f1, etc). After 3, select the hyperparameter combination which provides best performance metric. There are other flavors of cross-validation like k-fold cross validation and iterated cross-validation which work better. EDIT: For doing k-fold cross-validation, you don't need to split the data into training and validation set, it is done by splitting the training data into k-folds, each one of which will be used as a validation set in training the other (k-1) folds together as training set. The evaluation metric will then be the average of the evaluation metrics in the k iterations.
