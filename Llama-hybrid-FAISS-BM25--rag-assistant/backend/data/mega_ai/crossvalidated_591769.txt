[site]: crossvalidated
[post_id]: 591769
[parent_id]: 
[tags]: 
VQ-VAE uniform prior over the latent variables

VQ-VAE propose to make the posterior categorical distribution $q(z|x)$ one-hot = defined as $1$ for the closest embedding $e$ in the codebook to the output of the encoder $z_e(x)$ ; define a simple uniform prior over $z$ (the latent variables). The authors claim that this obtains a KL divergence which is discrete and equal to $\log K$ , where $K$ is the number of discrete latent embeddings. If I understand correctly, that means that the range of the uniform prior over $z$ should be so that every embedding has a probability equal to $1/K$ to be the closest to the encoder output. Am I correct? If that's so, how should we initialize the embedding vectors? Should their weights be initialized to a uniform probability - $U[\min(z_e(x)), \max(z_e(x))] ; x\in X$ ?
