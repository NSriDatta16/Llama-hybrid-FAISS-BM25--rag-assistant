[site]: stackoverflow
[post_id]: 1743688
[parent_id]: 1743496
[tags]: 
You have plenty of tools/techniques to get more performance out of this type of work load. If appropriate Bulk Load anything you can. Somethings you can't. Need to run validated against the records, destination table has nullable columns... Consider moving complex Data Warehousing/ETL operations to a staging database with no transaction logging (aka simple mode). This will improved performance greatly. Then batch/bulk the data to the destination system. Batch non-bulk load insert operations. Commit every n records start with 1,000 and performance tune from there. Improve the speed of your disk storage. Smaller faster disk are much better than bigger and slower. The last db performance tuning project I worked on we moved from local disk 10,000 RPM to SAN then back to solid state disk on the server for some operations. Solid State most definitely rocks! But is expensive. Use the force, um performance tuning tools for Sql Server to find less obvious bottle necks. Sometimes the best course of action might be to drop and rebuilt indexes based on what % of records are being inserted/deleted compared to the table size; disable triggers during certain operations; and modifying the sparseness of records in data blocks.
