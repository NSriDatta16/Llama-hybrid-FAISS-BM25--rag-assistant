[site]: crossvalidated
[post_id]: 262803
[parent_id]: 262788
[tags]: 
By "bad prediction" do you mean e.g. $y^{(i)} = 1$ and $h(x^{(i)}) = 0$? That would indeed lead to an infinite loss â€“ but luckily, for any finite weight vector, $h$ is never going to actually achieve values 0 or 1, and $\log h$ will always be finite. So, you can initialize logistic regression wherever you want. Because the problem is convex, you'll always get an equivalent answer. The only way that $J$ can actually become infinite is if the problem is linearly separable, in which case driving all the predictions to exactly 0 and 1 becomes the optimal solution. This is usually avoided via regularization.
