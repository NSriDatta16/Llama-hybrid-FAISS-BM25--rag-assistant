[site]: datascience
[post_id]: 111824
[parent_id]: 110740
[tags]: 
Depending on the language of the tweets you collected, and the availability of pre-trained sentiment analysis models for this language. You should aim for models trained on the most similar domains, typically models that were trained with social media text, as it is quite different than other domains (e.g. news, or articles). However, you will always have a problem with validation, since you do not have ground truth data. To get around this problem, you could encode the tweets using a pre-trained encoder (e.g. universal sentence encoder or BERTweet ). Then cluster the encoded tweets. Optionally, you could project the tweets first using UMAP (faster) or t-SNE (less efficient with large datasets). You could then label few tweets from each resulting cluster and propagate the labels for most similar tweets. This approach would work since the encoders used are trained for semantic similarity tasks, so they are encoding tweets similar in meaning into similar vectors, and then the UMAP projections further brings similar vectors closer (due to the reduced dimensionality). You could then use the propagated labels as predictions, and the few labeled tweets from each cluster as ground truth. This approach has been validated and used multiple times in NLP and computational social science literature. An example here .
