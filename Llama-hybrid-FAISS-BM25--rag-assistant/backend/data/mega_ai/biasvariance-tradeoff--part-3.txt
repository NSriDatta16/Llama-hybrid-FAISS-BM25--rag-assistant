displaystyle P(x,y)} which can for example be done via bootstrapping. The three terms represent: the square of the bias of the learning method, which can be thought of as the error caused by the simplifying assumptions built into the method. E.g., when approximating a non-linear function f ( x ) {\displaystyle f(x)} using a learning method for linear models, there will be error in the estimates f ^ ( x ) {\displaystyle {\hat {f}}\!(x)} due to this assumption; the variance of the learning method, or, intuitively, how much the learning method f ^ ( x ) {\displaystyle {\hat {f}}\!(x)} will move around its mean; the irreducible error σ 2 {\displaystyle \sigma ^{2}} . Since all three terms are non-negative, the irreducible error forms a lower bound on the expected error on unseen samples. The more complex the model f ^ ( x ) {\displaystyle {\hat {f}}\!(x)} is, the more data points it will capture, and the lower the bias will be. However, complexity will make the model "move" more to capture the data points, and hence its variance will be larger. Derivation The derivation of the bias–variance decomposition for squared error proceeds as follows. For convenience, we drop the D {\displaystyle D} subscript in the following lines, such that f ^ ( x ; D ) = f ^ ( x ) {\displaystyle {\hat {f}}\!(x;D)={\hat {f}}\!(x)} . Let us write the mean-squared error of our model: MSE ≜ E [ ( y − f ^ ( x ) ) 2 ] = E [ ( f ( x ) + ε − f ^ ( x ) ) 2 ] since y ≜ f ( x ) + ε = E [ ( f ( x ) − f ^ ( x ) ) 2 ] + 2 E [ ( f ( x ) − f ^ ( x ) ) ε ] + E [ ε 2 ] {\displaystyle {\begin{aligned}{\text{MSE}}&\triangleq \mathbb {E} {\Big [}{\big (}y-{\hat {f}}\!(x){\big )}^{2}{\Big ]}\\&=\mathbb {E} {\Big [}{\big (}f(x)+\varepsilon -{\hat {f}}\!(x){\big )}^{2}{\Big ]}&&{\text{since }}y\triangleq f(x)+\varepsilon \\&=\mathbb {E} {\Big [}{\big (}f(x)-{\hat {f}}\!(x){\big )}^{2}{\Big ]}\,+\,2\ \mathbb {E} {\Big [}{\big (}f(x)-{\hat {f}}\!(x){\big )}\varepsilon {\Big ]}\,+\,\mathbb {E} [\varepsilon ^{2}]\end{aligned}}} We can show that the second term of this equation is null: E [ ( f ( x ) − f ^ ( x ) ) ε ] = E [ f ( x ) − f ^ ( x ) ] E [ ε ] since ε is independent from x = 0 since E [ ε ] = 0 {\displaystyle {\begin{aligned}\mathbb {E} {\Big [}{\big (}f(x)-{\hat {f}}\!(x){\big )}\varepsilon {\Big ]}&=\mathbb {E} {\big [}f(x)-{\hat {f}}\!(x){\big ]}\ \mathbb {E} {\big [}\varepsilon {\big ]}&&{\text{since }}\varepsilon {\text{ is independent from }}x\\&=0&&{\text{since }}\mathbb {E} {\big [}\varepsilon {\big ]}=0\end{aligned}}} Moreover, the third term of this equation is nothing but σ 2 {\displaystyle \sigma ^{2}} , the variance of ε {\displaystyle \varepsilon } . Let us now expand the remaining term: E ⁡ [ ( f ( x ) − f ^ ( x ) ) 2 ] = E ⁡ [ ( f ( x ) − E ⁡ [ f ^ ( x ) ] + E ⁡ [ f ^ ( x ) ] − f ^ ( x ) ) 2 ] = E ⁡ [ ( f ( x ) − E ⁡ [ f ^ ( x ) ] ) 2 ] + E ⁡ [ ( E ⁡ [ f ^ ( x ) ] − f ^ ( x ) ) 2 ] + 2 E ⁡ [ ( f ( x ) − E ⁡ [ f ^ ( x ) ] ) ( E ⁡ [ f ^ ( x ) ] − f ^ ( x ) ) ] {\displaystyle {\begin{aligned}&\operatorname {\mathbb {E} } \left[\left(f(x)-{\hat {f}}\!(x)\right)^{2}\right]\\[1ex]&=\operatorname {\mathbb {E} } \left[\left(f(x)-\operatorname {\mathbb {E} } [{\hat {f}}\!(x)]+\operatorname {\mathbb {E} } [{\hat {f}}\!(x)]-{\hat {f}}\!(x)\right)^{2}\right]\\[1ex]&={\color {Blue}\operatorname {\mathbb {E} } \left[\left(f(x)-\operatorname {\mathbb {E} } [{\hat {f}}\!(x)]\right)^{2}\right]}\,+\,\operatorname {\mathbb {E} } \left[\left(\operatorname {\mathbb {E} } [{\hat {f}}\!(x)]-{\hat {f}}\!(x)\right)^{2}\right]\\&\quad \,+\,2\ {\color {PineGreen}\operatorname {\mathbb {E} } \left[\left(f(x)-\operatorname {\mathbb {E} } [{\hat {f}}\!(x)]\right)\left(\operatorname {\mathbb {E} } [{\hat {f}}\!(x)]-{\hat {f}}\!(x)\right)\right]}\end{aligned}}} We show that: E [ ( f ( x ) − E [ f ^ ( x ) ] ) 2 ] = E [ f ( x ) 2 ] − 2 E [ f ( x ) E [ f ^ ( x ) ] ] + E [ E [ f ^ ( x ) ] 2 ] = f ( x ) 2 − 2 f ( x ) E [ f ^ ( x ) ] + E [ f ^ ( x ) ] 2 = ( f ( x ) − E [ f ^ ( x ) ] ) 2 {\dis