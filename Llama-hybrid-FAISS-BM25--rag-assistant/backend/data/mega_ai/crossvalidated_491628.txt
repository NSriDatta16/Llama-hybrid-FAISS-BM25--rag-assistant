[site]: crossvalidated
[post_id]: 491628
[parent_id]: 
[tags]: 
Effect of weight decay on loss

I am new to deep learning and the terminology "L2 regularization" and "weight decay" seems to be used almost interchangeably... L2 regularization however modifies the loss function by adding a term, while weight decay does not modify the loss and instead decays the weights during the weight update. Is this correct?
