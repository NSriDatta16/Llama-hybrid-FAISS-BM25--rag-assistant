[site]: crossvalidated
[post_id]: 397022
[parent_id]: 
[tags]: 
Should I use pretrained word2vec or train word2vec on my own dataset?

I am trying to perfrom fake news detection using machine learning naive bayes classifier. So far I have used BOW and TFIDF as my feature vectors. From research I have found that word embeddings plays a crucial part in text classification and I want to use word2vec to generate word vectors and then take its average to represent the document. I am having a great confusion on deciding the best option: either use pretained word2vec model or train word2vec on my own dataset. Which option would be good for my project and also why? I have a labeled dataset of fake and real news. I have preprocessed the dataset by removing punctuation, by stemming and also its lemmatized. So If I used a pre trained word2vec model, would that be a good option to use in my preprocessed dataset? I think the dataset used in pretrained model (say google-news dataset word2vec model) is preprocessed in one way and mine is preprocessed in another way so it's not the best idea to use pretrained model.Is that so?
