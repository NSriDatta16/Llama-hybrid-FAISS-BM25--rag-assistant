[site]: crossvalidated
[post_id]: 394798
[parent_id]: 394763
[tags]: 
Having many predictors is not per s√© a bad thing (although in some cases one must pay attention to the bias-variance trade off, especially when using parametric models containing many variables to fit the data); this said: Visualisations In general it is beneficial to perform descriptive statistics before starting any analysis: in particular one can: calculate pairwise correlations among numerical variables; this gives an indication of whether or not some pairs of variables have a linear dependence between each other, which in turn can help sub-selecting the ones you want to use as predictors for the eventual model (to make a long story short if two variables are correlated you can replace the one with the other) boxplot the response variable against categorical variables in your predictors, if any; this helps understanding how the responses are distributed within your classes and can help identify outliers In python there a very straightforward way to visualise the above, namely using the seaborn library (specifically pairplot if you want to do it all at once). However, as mentioned, the limitation of the pairplots is that you will have no information about possible interactions among sets of variables themselves. Model There is really no general rule to guess what model may fit the data the best. If you have good reasons to believe that the decision boundaries are linear then one may try more or less complicated polynomial fits (by including possible interactions or higher order terms), eventually performing backward/forward variables selections (that is, adding/removing one variable at a time, usually the least significant one, to see whether the goodness of the fit changes - notice that in this case the coefficients of your variables will of course change). If not, one must move towards non-linear models like support vector machines, nearest neighbours, neural networks or ensemble methods, like the whole category of random forests and the like.
