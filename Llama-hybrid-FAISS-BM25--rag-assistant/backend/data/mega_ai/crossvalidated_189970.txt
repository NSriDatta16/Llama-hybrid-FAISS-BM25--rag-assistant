[site]: crossvalidated
[post_id]: 189970
[parent_id]: 
[tags]: 
In Bayesian linear regression, why do we assume parameter prior has zero mean?

Bayesian linear regression $$f(x)=x^Tw$$ $$y=f(x)+\epsilon$$ $w$: is a vector of the parameters $(w_1, w_2,.., w_p)$ $f$: the function value $y$: observed value $\epsilon \sim N(0, \sigma^2)$ Prior distribution is $w \sim N(0, \Sigma_p)$. Why is it ok to have zero mean here? I think it's wrong! This means $x^Tw$ has mean zero, so $f(x)$ is zero. Why would you want to have a model with mean zero? You want to predict $y$! Unless we are assuming the data $y$ has mean zero, otherwise, this doesn't make sense to me. Specific examples Here 1: http://www.gaussianprocess.org/gpml/chapters/RW2.pdf See on page 9 Here 2: https://www.youtube.com/watch?v=dtkGq9tdYcI Could someone please explain what is going on here?
