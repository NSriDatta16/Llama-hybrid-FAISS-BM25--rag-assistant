[site]: datascience
[post_id]: 76832
[parent_id]: 76816
[tags]: 
There are generic methods to avoid overfitting, but I'd like to address your specific problem. Like you said, your dataset doesn't have a lot of examples compared to the number of features. This, on its own, increases the risk of overfitting, especially if you use a more complex model such as GradientBoost or RandomForest (I'm not sure I'd use either when my number of samples is only 4x times the number of features). So, the first thing to do would be to try and reduce the number of features. Any model with regularization can help you with that, preferably L1 regularization and not L2. In the sklearn implementation of Logistic Regression ( see the docs ), you just set penalty='l1' , and try you can make the regularization stronger by reducing the parameter C (or you can select C automatically by cross-validation, which I would do; see LogisticRegressionCV ) After fitting such a model (don't forget to scale your features!), you can check which features have the smallest coefficients (some would be zero, hopefully) and remove them. This step would help any model, including those more complex than logistic regression... though, again, a simple model carries a smaller risk of overfitting than a complex one, and with your errors being what they are (if I understand your post correctly), I see no incentive to go with something more complex than logistic regression... until you get yourself more data to train a more complex model!
