[site]: crossvalidated
[post_id]: 367545
[parent_id]: 365419
[tags]: 
Okay I will take a crack at the revised questions: "What are the scenarios that deem necessity of reporting a p-value. Where it is mandatory to report p-value. Why is it necessary to report p-value. Why are they important. Why do we care?" Short answers: No empirical scenario requires it. In practice, certain journals and fields of research mandate the reporting of p-values for publication/credibility. It is not necessary to report a p-value. I do not think we really should care about them at all. Now for some longer answers. There is not a methodology in which p-values are required for results to be important. It doesn't matter whether you are running experiments with subatomic particles, using survey data, or running a field experiment. None of these scenarios require p-values for you to estimate effects. As scientists, we care about estimation and prediction no matter the methodology, and p-values do not help with either (more on this below). As was stated in the comments, p-values are terribly misunderstood, even by experts. They are used inappropriately as a simple heuristic to gauge whether or not your results are meaningful. While this is quite short-sighted, it certainly helps coordinate expectations and reduces the number of papers viable for publication (making the lives of the editors easier). This is why many academic journals and fields effectively mandate them to be present. The last three questions are all closely related. Let's start with the definition of a p-value in the context of a simple linear regression. The p-value tells the researcher how likely it is that he observed some data under the assumption that the null hypothesis is true. So what does having statistical significance actually mean? It means that it is unlikely that we observed the given data if the null hypothesis is true. Note, this does NOT tell us that the null model is more or less likely than the alternative model! This is a very common mistake. P-values are not valid metrics for model comparisons and can only be used to interpret the likelihood of observing specific data sets. With this information in mind, it follows that p-values are also unable to make claims about the predictive capabilities of the null relative to the alternative. Finally, for completeness, just note the obvious fact that p-values do not help in obtaining estimates of the magnitude or spread of the magnitude. In summary, the p-value doesn't help us with estimation or model selection. More often than not, researchers misuse p-values, believing them to be some indication of the null hypothesis being more or less likely. In my experience, people do not have any great argument for their use other than it is simply the procedure of a certain field. So it seems to me that the answer to your last three questions is this: we should not care about p-values because they really do not tell us anything. Let me drive home this point with one last p-value fact that is often overlooked. If you have enough data, you will obtain significance. This is due to the (somewhat silly) nature of the null hypothesis. That is, the null assumes that the effect size is exactly zero. In almost all cases, this is certainly untrue. There will be some effect, just a small one. So what a p-value does in practice is set up a straw-man requirement that can always be met provided you have enough data. This fact should give any researcher who uses p-values pause. Intuitively, it suggests that p-values are not a useful metric for anything because they are simply a function of the size of the data. Scientists are interested in estimation and prediction, and p-values do not help with either. If you are interested in model comparison, you should use something like AIC or BIC. You could also leave the frequentist framework all together and use Bayesian methods to compute Bayes Factors (which are approximated by BIC).
