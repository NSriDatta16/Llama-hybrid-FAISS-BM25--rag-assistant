[site]: datascience
[post_id]: 112947
[parent_id]: 93570
[tags]: 
First of all, adding features to your model can easily cause overfitting. Even adding a column of random values can cause overfitting. Take for example your series with exactly one feature - the price. The price can be the same value X several times - first when the series is rising, and then when it's decreasing. if you only use the price, you are forced to try to learn the average of the future value at that price. Now add a moving average. Now when the price is X and the series is rising, the MA will be less than X, while when the series is decreasing the MA will be more than X. your model can now on one hand learn to predict better (if the MA is actually a good feature), but it can also hack the data batter because it is now better able to memorize the training set. Now add another 50 moving averages to the features. you may now have a unique value of features for every output, so your model can fit the data extremely well, but obviously, that noise will not predict anything on the test set. How many features of (basically) the same thing to use is a good question, but you need to remember you're not actually adding a lot of information with those features. And frankly, LSTMs can calculate moving averages pretty well themselves.
