[site]: crossvalidated
[post_id]: 442790
[parent_id]: 336820
[tags]: 
In multi-label classification, a misclassification is no longer a hard wrong or right. A prediction containing a subset of the actual classes should be considered better than a prediction that contains none of them. source So accuracy counts no of correctly classified data instance, Hamming Loss calculates loss generated in the bit string of class labels during prediction, It does that by exclusive or (XOR) between the actual and predicted labels and then average across the dataset. source Number of Instances = 2 Number of Labels = 2 Case 1: Actual Same as Predicted Actual = [[0 1] Predicted= [[0 1] [1 1]] [1 1]] Actual XOR Predicted = [[0 0 0 0]] from sklearn.metrics import hamming_loss import numpy as np print(hamming_loss(np.array([[0,1], [1,1]]), np.array([[0,1], [1,1]]))) HL= 0.0 Case 2: Actual completely different from Predicted Actual = [[0 1] Predicted= [[1 0] [1 1]] [0 0]] Actual XOR Predicted = [[1 1 1 1]] from sklearn.metrics import hamming_loss import numpy as np print('HL=',hamming_loss(np.array([[0,1], [1,1]]), np.array([[1,0], [0,0]]))) HL = 4/(2*2) = 1 Case 3: Actual partially different from Predicted Actual = [[0 1] Predicted= [[0 0] [1 1]] [0 1]] Actual XOR Predicted = [[0 1 1 0]] from sklearn.metrics import hamming_loss import numpy as np print(hamming_loss(np.array([[0,1], [1,1]]), np.array([[0,0], [0,1]]))) HL = (1+1)/(2*2) = 0.5 hamming loss value ranges from 0 to 1. Lesser value of hamming loss indicates a better classifier.
