[site]: crossvalidated
[post_id]: 390219
[parent_id]: 
[tags]: 
Cover Theorem and hidden layer of MLP for classification

I know that if the input space is not linearly separable, we can map it to a higher dimensional space with a hidden layer. I am learning about handwritten digit classification, but I am seeing that most of the neural networks have an input space like 400 (20x20 pixel), but hidden layer has lower units, less than 400. Why? Is not 400 the dimension of the input space? What am I missing?
