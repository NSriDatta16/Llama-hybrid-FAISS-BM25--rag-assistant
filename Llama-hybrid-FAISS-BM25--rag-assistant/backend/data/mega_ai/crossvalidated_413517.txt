[site]: crossvalidated
[post_id]: 413517
[parent_id]: 246512
[tags]: 
I'll try to tell from the regard of information that when is it okay to pad and when it is not. Let's for base case take the example of tensorflow padding functionality. It provides two scenarios, either "Valid" or "same". Same will preserve the size of the output and will keep it the same as that of the input by adding suitable padding, while valid won't do that and some people claim that it'll lead to loss of information, but, here's the catch. This information loss depends on the size of the kernel or the filter you're using. For example, let's say you have a 28x28 image and the filter size is 15x15(let's say). The output should have dimension 16x16, but if you pad using "same" in tensorflow it will be 28x28. Now the 12 rows and 12 columns in themselves don't carry any meaningful information but are still there as a form of noise. And we all know how much susceptible deep learning models are towards the noise. This can degrade the training a lot. So if you're using big filters, better not go with padding.
