[site]: datascience
[post_id]: 101780
[parent_id]: 62862
[tags]: 
You don't need to make preprocessing as I understand, and the reason for this is that the Transformer makes an internal "dynamic" embedding of words that are not the same for every word; instead, the coordinates change depending on the sentence being tokenized due to the positional encoding it makes. Note the difference with Word2Vec, GloVe or Fastext approaches, where the Embedding Matrix is fixed along the whole dictionary. This being said, an example: This living room is boring. This bear in the room is boring. When builing the embedding, $$\text{Embedding(This, first sentence)} \ne \text{Embedding(This, second sentence)}$$ In my opinion, that's the transformer's real power. In any case, it won't hurt to make both experiments and decide after based on results, now that Transfer Learning is around!
