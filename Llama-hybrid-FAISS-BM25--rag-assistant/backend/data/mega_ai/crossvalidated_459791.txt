[site]: crossvalidated
[post_id]: 459791
[parent_id]: 459780
[tags]: 
The tuning parameters are ones that cannot be estimated from fitting and it kind of control how your model behaves. Every model would have its own set of tuning paratmers and usually packages would provide a list of these, for example scikit-learn and caret . For example, let's take k nearest neighbour . When you train a knn model, you need to define k, and you predict the value of an observation based on either the label or mean of its k nearest neighbors. The question is which k to use to make this effective? This is a tuning parameter. But if we go to another model, for example ridge regression, then the question becomes how to penalize large / many coefficients and for this we set lambda (or shrinkage penalty, see for a guided example ). From the two examples above, you can see it is not the k in k-fold cross validation. The k in fold cross validation refers to how many folds of cross validation to perform, and this is something you decide when training the model, to find the optimal parameters, as briefly discussed above. For regression machine learning RMSE is commonly used, but you can also use R square, Mean square error (MSE), Mean absolute error (MAE)
