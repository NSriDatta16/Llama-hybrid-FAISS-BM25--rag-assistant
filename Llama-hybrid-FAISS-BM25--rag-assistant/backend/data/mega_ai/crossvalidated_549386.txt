[site]: crossvalidated
[post_id]: 549386
[parent_id]: 
[tags]: 
How to train a custom embedding?

I have data with a lot of categorical features. The cardinality of some of these features is quite big (>100), so I want to avoid using one-hot encoding. The idea is to use an embedding. The problem here is that I do not quite understand how I can learn an embedding for my problem domain. There are these pre-trained embeddings like Word2Vec, but suppose the goal is to create a totally new embedding. What I have done so far is to encode my features as numeric values. For example, if one of the features is a type of vehicle, then the conversion looks like this: car --> 1 train --> 2 boat --> 3 aircraft --> 4 ... rocket --> 100 Each of these integers i corresponds to a 100-dimensional one-hot vector, where the i-th entry is a 1 and the rest is zero. To reduce the dimensionality of the input to, let's say, 5, an embedding is a matrix of size 100 x 5, which is randomly initialized at first. So when I put a car (1) into the embedding layer, it returns a 5-dimensional (random) vector. To give meaning to these embeddings, I have to adjust the the weights inside the matrix with respect to something like a target variable. But where do I get it from, so that the embedding represents something meaningful? A lot of models seem to include the embeddings as part of their model, but as far as I can tell, this does not work for me. If it helps, I try to use the embedded variable in combination with other numeric variables as input for an autoencoder, but this model minimizes the loss between the output of the embedding layer and their respective reconstruction. I would be grateful for any help! EDIT: Model: "model_1" __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) [(None, 2)] 0 __________________________________________________________________________________________________ embedding (Embedding) (None, 2, 5) 500 input_1[0][0] __________________________________________________________________________________________________ input_2 (InputLayer) [(None, 2, 2)] 0 __________________________________________________________________________________________________ concatenate (Concatenate) (None, 2, 7) 0 embedding[0][0] input_2[0][0] __________________________________________________________________________________________________ lstm (LSTM) (None, 10) 720 concatenate[0][0] __________________________________________________________________________________________________ repeat_vector (RepeatVector) (None, 2, 10) 0 lstm[0][0] __________________________________________________________________________________________________ lstm_1 (LSTM) (None, 2, 10) 840 repeat_vector[0][0] __________________________________________________________________________________________________ time_distributed (TimeDistribut (None, 2, 7) 77 lstm_1[0][0] ================================================================================================== Total params: 2,137 Trainable params: 2,137 Non-trainable params: 0
