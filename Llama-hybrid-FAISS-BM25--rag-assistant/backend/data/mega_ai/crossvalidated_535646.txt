[site]: crossvalidated
[post_id]: 535646
[parent_id]: 534538
[tags]: 
The result on the test set is noisy due to the relatively small test set, so I'd not worry too much about small differences. However, you are right the drop is quite notable. It's worth double-checking how the training-validation-test split was done? E.g. are they truly from the same distribution, is there an information leak from training to validation set, but not to the test set? You have a huge feature space and relative to that very little data. There's always the worry that your hyperparameter choice might in a sense overfit the validation set. The drop in performance as you go train -> val -> test is noticable, so it could be a case of the model being able to substantially overfit the training set, less so the validation set and not the test set. L2-regularized logistic regression is a problem with a convex loss function, so you'll get the true global loss minimum for your model (conceivably badly overfit). That may not truly be what one wants here. In constrast overparameterized neural networks trained with regularized SGD tend to not end up finding the sharpest global optima and finding wide and flat local optima seems to lead to better generalization. I assume you are using scikit learn and your regularization is L2-regularization. You are picking the hyperparameter on a pretty small validation set, which might effectively be even smaller, if only a small amount of the data is anywhere near the decision boundary. You might be better off optimizing it with repeated-K-fold cross-validation instead of a single training-validation split. You also did not say how you pick the parameter, usually the + 1 SE rule helps. I don't think that's the main problem though. You did not say what type of text you are working with, but a sentence encoder might not create a very good feature space here. The less good (and high-dimensional) the feature space, the more risk of overfitting to meaningless aspects. A neural network (such as BERT or GPT-2 see e.g. here , but you could try with something simpler like ULMFiT ) is usually a sensible thing to try for this problem. There's a bunch of ways of regularizing neural networks/improve their performance. Some typical ideas include: a) fine-tuning the language model to the training data before training the classifcation model, b) data augmentation (e.g. roundtrip translation like English->Spanish->English - huggingface has pre-trained translation models you could use for this, unless you have the budget to e.g. use the Google API), c) traditional regularization parameters (e.g. weight decay, drop-out), d) some sensible kind of learning rate schedule (e.g. the one-cycle policy or the flat-cosine schedule; either after picking a sensible learning rate using the learning rate finder) and/or early stopping. It's really worth looking at what highly placed solutions in Kaggle competitions used, people there have to worry a lot about their test set performance.
