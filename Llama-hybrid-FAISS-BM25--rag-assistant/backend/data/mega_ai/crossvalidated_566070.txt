[site]: crossvalidated
[post_id]: 566070
[parent_id]: 
[tags]: 
Learning-Agnostic Evaluation of SVM Models

I am at a point where I want to evaluate existing SVM models. For this task I assume I have: SVM model (to make it easier let's say it's a scikit-learn one) Training Dataset that was used to learn (1) What measures can I possibly compute to evaluate the SVM model? The way I see it, my possibilities are limited: I could compute accuracy etc. on the given dataset (2). This is of course not really what anyone wants, because the data for testing should be different from the data for training I could train my own SVM models and compare them to (1) in some way. And that's about the end of what I can think of. I would love to have any other ways to evaluate the given model in whatever metric (Accuracy, Robustness, Compliance with some proven bound, ...). Are there any more possible ways to evaluate? If more information about the training-process of the model is needed, what information would be helpful?
