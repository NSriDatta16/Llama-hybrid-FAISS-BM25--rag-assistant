[site]: crossvalidated
[post_id]: 193086
[parent_id]: 119959
[tags]: 
Actually, if you pre-train all the layers to learn the activations of the previous one, I found it may perform sub-optimally during the subsequent fine-tuning. I get a much better performance when I set the last layer during pre-training to try to reconstruct the original input (the one fed to the first layer) instead of the activations of the previous hidden layer. This way the resulted multi-layer autoencoder during fine-tuning will really reconstruct the original image in the final output. See my post here: How to train and fine-tune fully unsupervised deep neural networks?
