[site]: crossvalidated
[post_id]: 241755
[parent_id]: 
[tags]: 
feature scaling in online learning algorithms

Let's take Logistic Regression as an example, typically we should do feature scaling before applying l1 or l2 regularizations. When do batch learning, feature scaling could be easily done, but how could we do this in online learning scenario, since data points come in a stream fashion and no way to know the mean and std of the data. By feature scaling , I mean all the features should be transformed such that they have roughly the same scale, and this is typically done by mean-centering and divided by the standard deviation of the feature. Questions 1) AFAIK, we could buffer a few of recent data points, say data received within an hour, and compute the mean and std , then do feature scaling for later data. Could this approach work well? 2) FTRL algorithm, I didn't see how it does feature scaling in the paper, but it applies per-coordinate (i.e. per feature) learning rate, is this the way it deals with features with different scales?
