[site]: crossvalidated
[post_id]: 327376
[parent_id]: 
[tags]: 
Binary cross entropy vs mse loss function when asymmetric payoffs

I'm building a binary classifyer that has an unequal payoff given the following cass: $Y_{pred}=Y_{actual}= True$: payoff is +x*100 $Y_{pred}$$\ne$$Y_{actual}= True$: payoff is -x*100 $Y_{pred}$$\ne$$Y_{actual}= False$: payoff is -1 $Y_{pred}=Y_{actual}= False$: payoff is +1 In other words, there are two possible courses of action: True for action 1 and False for action 2. I'm looking at two approaches to implement it: I could mse as loss function with two output neurons, assign the payoffs directly to $Y_{actual}$, with the neural network predicting the payoffs for each of the actions. I assume I would then just look for which of the two neurons predicts a higher payoff and use liner function in the last activation layer. Correct? I could create a custom loss function that will calculate the respective payoffs as described above and then use binary cross entropy on that? Which of those two approaches would be preferred? Should they lead to the same result, giving a recommendation for each sample which action is preferable?
