[site]: datascience
[post_id]: 86971
[parent_id]: 86966
[tags]: 
A couple of pictures of the documents with your target data highlighted might help. Also your approach seems sound enough. I agree you wouldn't get perfect results each time, however there is a concept of Human In the Loop. Where the data extracted by your program would have to be verified by a human once before certifying. This is basically a digital signature of the person verifying the data to have an audit trail. From what I understand, you're trying to extract just a few entities from the documents. So glancing over the data to verify whether things indeed make sense shouldn't be too difficult. This approach would certainly work faster than editing the template and making changes to the regular expressions. Alternatively, your problem could also be construed as contextual summarization where the summary would be an extract of your NERs. I'm not sure how effective this would be (maybe someone more experienced could chime in) but you could go the deep learning route and train an attention based model like BERT on your custom dataset containing the extracted NERs as your target summarization.
