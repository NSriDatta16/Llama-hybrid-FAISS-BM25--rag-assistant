[site]: crossvalidated
[post_id]: 472871
[parent_id]: 
[tags]: 
Why the batch normalization is not applied on the last layer of a neural network

As I found in some tutorials, they didn't perform BN on last layer. It seems like a best practice, but I didn't find any detailed explanation of why this helps training. Can anyone kindly help me brief the reason for why doing this?
