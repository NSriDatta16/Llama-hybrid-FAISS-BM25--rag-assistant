[site]: crossvalidated
[post_id]: 281071
[parent_id]: 281069
[tags]: 
There are a few approaches you could take. First, you could project-out the fixed effects, then run ridge or lasso: library(lfe) library(glmnet) "%ni%" You'd then go back and calculate the fixed effects using getfe or something, to make the prediction. If your dataset is small like in the example, you could make model matrices of all squares and cross-products, etc. Also, if your data is small, you could simply put your cross-sectional unit into the design matrix of the ridge regression as a factor -- (penalized) least squares dummy variables. It turns out that L2-penalized LSDV is equivalent to random effects. If you don't care about unbiased parameter estimates, you should always prefer random effects to fixed effects. You could also simply ignore the cross-sectional unit: library(randomForest) rf Here you'd want to take out year, because RF can't extrapolate. I assume you're making predictions for the next period. You could consider detrending your data before putting it into the random forest. Finally, there is an experimental package here that projects-out fixed effects from the top level of a neural network. It might not yet be very reliable, however.
