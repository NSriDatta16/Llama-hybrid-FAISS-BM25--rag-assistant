[site]: crossvalidated
[post_id]: 346208
[parent_id]: 
[tags]: 
Accounting for sensitivity of model to small changes in assumptions

I am running a Monte Carlo simulation of correlated asset returns, based on this Matlab function . The model's inputs are a vector of expected means and standard deviations for 9 assets, and their covariance matrix. It takes these inputs and simulates correlated returns for each asset over a fixed horizon for many trials, and then calculates the average likelihood that a portfolio of these assets will exceed a certain return threshold at the end of the simulation period. The model works fine, but I am worried about estimation error as the probability output is very sensitive to small changes in the assumptions (ie the mean and standard deviation for the individual assets). Ideally I would want to resample the data based on the observations, but I don't have the underlying raw data ; all I have are the initial assumptions for the assets. For simplicity, I can assume the assets all follow a normal distribution with an unknown true mean/st dev. To account for the sensitivity of the inputs, I had the following approach in mind. Instead of giving a fixed probability as the output, I think it might be a better idea to give an interval to account for the estimation error (i.e instead of saying the probability is 60%, I'll instead say I am 95% sure the probability of exceeding the threshold is between 50% and 75%). I will construct this interval using this approach: Using the original input mean and covariance matrix, I generate N sets of 500 random drawings for each of the assets For each of these sets, I use the simulated drawings to estimate the sample mean, st.dev and covariance, and then run my model using these new inputs to calculate the probability. This process is repeated N times. Now that I have N test statistics based on the simulated data, I can rank order them and create a distribution. The 5th percentile is the lower bound for my interval, and the 95th is the upper bound. My question is: Is this a good solution, considering these bounds will still be sensitive to my starting assumptions? Given the limitations of my dataset, is there a better way to account for the estimation error?
