[site]: crossvalidated
[post_id]: 621261
[parent_id]: 271516
[tags]: 
It has been previously stated that the k-means objective is equivalent to the log-likelihood with spherical clusters under hard assignment, which gives rise to a valid AIC, eg here http://www.cs.cmu.edu/~aarti/Class/10701_Spring14/slides/EM_annotatedonclass.pdf with corresponding AIC here: https://nlp.stanford.edu/IR-book/html/htmledition/cluster-cardinality-in-k-means-1.html Given Christian Hennig's comments I was trying to figure out where/when this argument breaks down. Let $\Theta = \text{vec}(\{ \vec \mu_j, \sigma \})$ , and $\mu_{C(i)}$ be the function that assigns $\vec x_i$ to the centroid of cluster $j$ , i.e. the nearest $\vec \mu_j$ $$ \begin{aligned} \mathcal{\ell}(\Theta;\vec{x}) & = \prod_{i=1}^{n} \sum_{j=1}^{k} \Pr( \vec x_0 = \vec x_i | \vec x_0 \in C_j ) \cdot \Pr(\vec x_0 \in C_j) \\ & = \prod_{i=1}^{n} \sum_{j=1}^{k} (2\pi\sigma^2)^{-d/2} \exp \left( -||\vec{x_i}-\vec{\mu_j}||^2 / 2\sigma^2 \right) \cdot \pi_j( \sigma, \{ \vec \mu_{\ell} \}_{\ell=1}^k) \\ & \ne \prod_{i=1}^{n} \sum_{j=1}^{k} (2\pi\sigma^2)^{-d/2} \exp \left( -||\vec{x_i}-\vec{\mu_j}||^2 / 2\sigma^2 \right) \cdot 1(\vec{x_i} \in C_j| \{ \vec \mu_j \}, \sigma) \\ & \text{which is equivalent in arg max to the k-means objective function:} \\ & \space \arg\min_\Theta \sum_{i=1}^{n} \frac{||\vec{x_i}-\vec{\mu_{C(i)}}||^2}{2 \sigma^2} \end{aligned} $$ The problem is that even under the hard-assignment rule, you can't just replace the probability of being in cluster $C_i$ with an indicator function. Even though assignment of $x_i$ to $C_j$ is deterministic, given $\mu$ and $\sigma$ , to retain a legitimate likelihood, you still need to re-normalize the now truncated Normal distributions, which only have density integrated over $C_j$ , while the 'catchment area' for $C_j$ is now a polygonal/polyhedral region (tesselation) determined by the distances to all the $\vec \mu_j$ 's. So even under hard assignment, the k-means objective is equivalent to maximizing a likelihood only if the probability weights $\pi_j$ can be replaced with indicator functions without affecting the validity of the normal densities. So basically this is to say the AIC applies to k-means when the data-generating mechanism is from a tesselation of the space by spherical multivariate normals truncated by the decision boundaries, but which don't require renormalization due to the truncated density, then k-means centroids are the same as the $\mu_j$ 's and the AIC based on this would be a valid likelihood approach to comparing k-means across different $k$ . Otherwise your mileage may vary. The one case I can think of where the conditions are satisfied is if your clusters are spherical with equal variance and widely separated. This is exactly what is suggested by the $\sigma \rightarrow 0$ results here: K-means as a limit case of EM algorithm for Gaussian mixtures with covariances $\epsilon^2 I$ going to $0$ and here: https://yuhangzhou88.github.io/ESL_Solution/ESL-Solution/_13-Prototypes-and-Nearest-Neighbors/ex13-1/ . However, the AIC will not be especially helpful in determining the number of clusters in that case, as it is unlikely that you are in doubt about $k$ when the clusters are obvious and distinct.
