[site]: datascience
[post_id]: 30506
[parent_id]: 17146
[tags]: 
As you said, you cannot prove mathematically that esembling increases performance, but it generally does. That's reason why gradient boosting and random forests are so popular in kaggle competitions, because they outperform what a decision tree can learn in many ways. As a curiosity, even Neural Networks can be used as "weak" learners, as can be seen in https://arxiv.org/abs/1704.00109 . So, ensembling is a very powerful technique that can be applied in many areas of machine learning. The main problem is that ensembles are not easily interpretable, being way more black-boxy than its weak learners.
