[site]: datascience
[post_id]: 90699
[parent_id]: 90536
[tags]: 
appears that the implementation of the self-attention mechanism has no effect on the model so I think my implementations have some problem. The first thing I noticed is your base model is doing badly, with low F1 scores, especially for the classes 1..7. Could it be there is just not that much signal in the training data, and the 2-layer LSTM has already sucked it all out? My second thought is that self-attention is good at finding connections between items however far apart they are in the sequence. But an LSTM is also fairly good at that. So maybe more of the same is not what you need. Self-attention is an essential part of a transformer, because it is the only component that works across the sequence; the only other component is the FFN, which operates on each item in isolation. Having got those two things off my chest, one thing I've noticed in experiments (and, again, these are in the context of transformers) is the importance of the residual connection. If I take out the residual connection that sits between self-attention and the FFN I get a significantly worse model. (Even though I still have the residual connection across the whole of each layer.) So you could try adding a residual connection across the self-attention. The other idea is to move self-attention earlier. Put it before the LSTMs, rather than after them. (Again with a residual connection across it.) Here is the idea, in (untested) code. I've moved the self attention module to before the first LSTM layer, and then take the sum of the input and the output - that is what makes the residual connection across it. ... x = Masking(mask_value=0.0, input_shape=(X_train.shape[1], X_train.shape[2]))) x2 = SeqSelfAttention(attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL, attention_activation='sigmoid')(x) x = x + x2 x = Bidirectional(LSTM(lstm_unit, dropout=dropout,return_sequences=True))(x) ... Note that you can't use the Keras sequential class (because the data flow is no longer sequential). Ref: https://keras.io/examples/nlp/text_classification_with_transformer/ The residual part is hard to spot at first, as it is done as the input to the two layer norm calls.
