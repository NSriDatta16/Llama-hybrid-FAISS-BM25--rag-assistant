[site]: datascience
[post_id]: 126294
[parent_id]: 87961
[tags]: 
First, I give a simple proof why neural networks are non-convex. Then, I talk about the symmetry argument. For simplicity, consider a two-layer MLP without bias, which can be written as $f(x) = W_2\sigma(W_1x)$ . Assuming ReLU activation functions, $\sigma(\cdot)=\max\{\cdot, 0\}$ and thus, $\sigma$ is convex in $W_1$ , as the $\max$ preserves convexity. Now, if $W_2$ is constant, then $f(x)$ is convex in $W_1$ . Same, if $\sigma(W_1x)$ is constant, $f(x)$ is convex in $W_2$ . However, $f(x)$ is not convex in both, $W_1 $ and $W_2$ . To see this, we simplify the problem even further and assume a one-dimensional input $x \in \mathbb{R}$ , just one hidden-layer neuron with weight $w_1 \in \mathbb{R}$ , and one output neuron with weight $w_2 \in \mathbb{R}$ . W.l.g., we also assume $x > 0$ , but the $x case can be constructed similarly. First, we rewrite $f(x)=f(w_1, w_2) = f(w)$ , with $w=(w_1, w_2)$ as we are interested in the fact if $f$ is convex w.r.t. $w$ given fixed $x$ . By definition, $f$ is convex w.r.t. $w$ , if $$f(tw+(1-t)w') \le tf(w) + (1-t)f(w')$$ for any two $w, w' \in \mathbb{R}^2$ and $t \in [0,1]$ . We construct a case where this inequality is violated. Consider the choice $w=(0, 1)$ , then $f(w)=0$ and $w'=(1,0)$ , then again $f(w')=0$ . Thus, any point $w''(t)=tw+(1-t)w'$ on the line segment joining $w$ and $w'$ , is not allowed to have a function value larger than $0$ for convexity to hold, as $tf(w) + (1-t)f(w')=0$ for any $t$ . Now consider the point $w''(t=0.5)=(0.5, 0.5)$ on the line segment. It has a function value $f(w'') = 0.5\cdot 0.5x = 0.25x > 0$ and thus, $f$ is not convex w.r.t. both $w_1$ and $w_2$ . Regarding the symmetry argument: It states that if I have multiple hidden layers and found a local minimum, I can "swap" neurons (weights) without effecting the function value, and thus create a different local minimal solution conflicting with convexity. However, this argument is incomplete as convex functions can have multiple local minima, which are then all global minima. Only strictly convex functions have a unique global minimum. Thus, this argument only rules out that our function is strictly convex. So one would need to show that on the line segment (in parameter space) between the two found minima, our function value increases again, as we did in the above proof.
