[site]: crossvalidated
[post_id]: 476374
[parent_id]: 476308
[tags]: 
I would recommend some changes to your approach. First, with only 10 effective predictors ( species with 4 levels counts as 3) there should be no need for predictor selection provided that you have on the order of 100-200 infected trees in your data sample. The usual rule of thumb for logistic regression is about 15 of the minority class per predictor evaluated. If you don't have that many infected trees you will probably be much better off using a logistic ridge regression, which will keep all predictors but penalize their regression coefficients to minimize overfitting. Second, predictor selection of this type is particularly risky (a) in general and (b) in logistic regression, as omitting any predictor associated with outcome can lead to a bias in coefficient estimates of the retained predictors, often toward lower magnitudes. Third, don't use accuracy as your criterion for model performance. Particularly, don't use a probability cutoff of 0.5 to gauge accuracy, as that implicitly assumes that false-positive and false-negative classifications are equally costly. That's probably not the case for you, as you want to find those rare infected trees. The logistic regression model provides probabilities. When applying your model, you can choose a probability cutoff that works for your intended purpose. In your case you would probably want to use a much lower cutoff than 0.5. Fourth, please read this discussion about issues with unbalanced data like yours and this discussion about SMOTE. If you still decide that SMOTE is the way to go and you decide to omit predictors despite my recommendation, SMOTE certainly should be done at the first step before you have removed any predictors. SMOTE generates synthetic data by a type of interpolation among minority-class cases, so you want to provide the algorithm as much information as possible to start. Finally, a logistic regression as you have written it might not work well if interactions among predictors are important. If you don't already know which interactions are likely to be important, you might consider another approach like boosted trees , which can be implemented to look for interactions while minimizing overfitting. If you go that route, however, make sure to use an approach that provides probability estimates rather than all-or-none classifications (see "Third" above).
