[site]: datascience
[post_id]: 31312
[parent_id]: 31291
[tags]: 
When you do k-fold cross-validation, you train k models, each one of them leaving the proportion $1/k$ of the data out. For each of the models, you can compute its train error and validation error. The train error will be the error on the data selected to train the model, and the validation error will be the data left out of the training. For this reason, you have k training errors and k validation/test errors, and computing their averages will give you the quantities you are talking about.
