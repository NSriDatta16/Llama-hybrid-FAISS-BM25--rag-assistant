[site]: crossvalidated
[post_id]: 379767
[parent_id]: 379259
[tags]: 
A first general comment is that RJMCMC is a simulation method attempting to produce simulations from a given posterior. As such it does not over- or under-fit. If anything, it converges to the posterior or it does not [in practice]. The over-fitting issue is concerning the posterior itself. The robustness attribute on the posterior is the same as with the Bayes factor: larger parameter spaces get penalised [with a penalty in $n\log(p)$ ] in a natural manner [the Ockham's razor argument] when integrating the likelihood over a prior on that parameter space. A quick explanation when looking at the approximation produced by a RJMCMC algorithm is that the associated jumps between spaces [and thus models] are somewhat blind and unlikely to reach good overfitting values in larger dimensions. Much more likely to hit a poor fit value that will be rejected. Posteriors tend to get more concentrated when the dimension increases.
