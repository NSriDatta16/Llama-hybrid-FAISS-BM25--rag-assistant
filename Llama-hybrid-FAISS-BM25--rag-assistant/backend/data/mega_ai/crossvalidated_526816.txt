[site]: crossvalidated
[post_id]: 526816
[parent_id]: 
[tags]: 
Calculating perplexity with smoothing techniques (NLP)

This question is about smoothed n-gram language models. When we use additive smoothing on the train set to determine the conditional probabilities, and calculate the perplexity of train data, where exactly is this useful when it comes to the test set? Which of these two things do we do? apply the conditional probabilities calculated using smoothing in the train set to the same n-grams that we might see in the test set, then calculate the perplexity for the test set separately? apply smoothing on the test set as well? If that's the case, what's the point of having a train set and a test set?
