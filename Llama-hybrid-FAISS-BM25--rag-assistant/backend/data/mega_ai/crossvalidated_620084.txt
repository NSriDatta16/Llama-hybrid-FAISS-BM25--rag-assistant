[site]: crossvalidated
[post_id]: 620084
[parent_id]: 
[tags]: 
Trying to understand equation 3.6 of "Reinforcement Learning: An Introduction (2nd edition)"

I'm going over the book (2nd edition) and I noticed something (IMHO) odd with equation 3.6 (pp 49). I was expecting the equation chain to end just as $$ \sum_{r \in R} r \cdot p(s', r | s, a) $$ because I don't see any point in going further. Am I missing something? Why not just leave it like this? To try to answer this, I tried to expand $p(s', r | s, a)$ in multiple ways. Since we are looking into the formula for $r(s, a, s')$ , we can turn $p(s', r | s, a)$ into $p(r | s', s, a)$ because $r(s', a, s)$ assumes that we know s' , s and a (we are literally providing them as parameters to the function). Let's decompose $p(r | s', s, a)$ : $p(r | s', s, a) = \frac{p(r, s', s, a)}{p(s', s, a)}$ For $p(r | s', s, a)$ , we can decompose/expand it in multiple ways (depending on the assumptions on what is independent of what). One decomposition is $p(r, s', s, a) = p(r, s' | s, a) \cdot p(s) \cdot p(a)$ and another is $p(r, s', s, a) = p(r | s') \cdot p(s' | s, a) \cdot p(s) \cdot p(a)$ . I chose these two because they fit the idea of MDPs (i.e. the reward and new state is dependent on the old state and what the agent did while in that state). For $p(s', s, a)$ , a reasonable decomposition (that fits the idea of MDPs) is $p(s', s, a) = p(s' | s, a) \cdot p(s) \cdot p(a)$ . If we put this all together, we get two "paths" for $r(s', a, s)$ : $r(s, a, s') = \sum_{r \in R} r \cdot p(s', r | s, a) = \sum_{r \in R} r \cdot \frac{p(r, s', s, a)}{p(s', s, a)} = \sum_{r \in R} r \cdot \frac{p(r, s' | s, a) \cdot p(s) \cdot p(a)}{p(s' | s, a) \cdot p(s) \cdot p(a)} = \sum_{r \in R} r \cdot \frac{p(r, s' | s, a)}{p(s' | s, a)}$ , which is what we have in the book. $r(s, a, s') = \sum_{r \in R} r \cdot p(s', r | s, a) = \sum_{r \in R} r \cdot \frac{p(r, s', s, a)}{p(s', s, a)} = \sum_{r \in R} r \cdot \frac{p(r | s') \cdot p(s' | s, a) \cdot p(s) \cdot p(a)}{p(s' | s, a) \cdot p(s) \cdot p(a)} = \sum_{r \in R} r \cdot p(r | s')$ Why did the authors opt for (1) instead of (2)? In my honest opinion, (2) is simpler and requires only the knowledge of s'. The only argument I can think against (2) is that r isn't directly conditioned by s (equation refers to $r(s', a, s)$ , not $r(s', a)$ or $r(s')$ . But s directly conditions s' , and s' directly conditions r , which (unless I'm missing something big) makes me conclude that r still depends on s .
