[site]: crossvalidated
[post_id]: 156523
[parent_id]: 155457
[tags]: 
On behalf of the initiative by @PawelP, I a writing a summary of the paper by Dem≈°ar , which covers $N \times M$ comparisons and is applicable to the original question. Firstly, let me explain what $N \times M$ comparisons actually are. Imagine having $N$ subjects (e.g. datasets), which one wants to asses using $M$ methods (e.g. machine learning algorithms). This is usually done by assessing every subject with every method, yielding a data table with $N$ rows and $M$ columns. Each individual column therefore represents some performance measurement (e.g. error rate, precision, recall etc.) across all subjects. In analogy, each individual row represents a subject, assessed by multiple methods. Since each subject is assessed under the same conditions, this means that the data table is actually a vector of dependent variables. Therefore, usage of statistical tests for dependent variables is the way to go. Now that we understand what kind of data we are dealing with, we can continue with analysing our data using various statistical tests. In the aforementioned paper, the authors have compared various parametric and non-parametric tests, such as paired t-test, ANOVA, Wilcoxon signed-rank test, Sign test, and Friedman's test. The $N \times M$ test set-up consists of two phases: Testing across all subjects and all methods using either ANOVA or Friedman's test. Whether the previous test yields significant results, one can continue to pairwise comparisons between all pairs of methods, yielding a $M(M-1)/2$ comparisons. The pairwise comparisons can be done by either paired t-test, Wilcoxon signed-rank test or the Sign test. One can notice that multiple comparisons in step #2 are prone to the familywise error rate (FWER). Therefore a Type I correction method needs to be used. The authors of the aforementioned papers have also tested various methods, such as: Dunnet, Holm, Bonferroni, Hochberg, and Hommel. To sum up , the authors' conclusions were the following: Use non-parametric tests. Namely Friedman test for overall comparison and Wilcoxon signed-rank test for pairwise comparisons (or Sign test if Wilcoxon signed-rank test is not applicable). Overall, the non-parametric tests, namely the Wilcoxon and Friedman test are suitable for our problems. They are appropriate since they assume some, but limited commensurability. They are safer than parametric tests since they do not assume normal distributions or homogeneity of variance. As such, they can be applied to classification accuracies, error ratios or any other measure for evaluation of classifiers, including even model sizes and computation times. Empirical results suggest that they are also stronger than the other tests studied. The latter is particularly true when comparing a pair of classifiers. Use Holm-Bonferroni FWER method The corresponding non-parametric post-hoc tests give similar results, so it is upon the researcher to decide whether the slightly more powerful Hommel test is worth the complexity of its calculation as compared to the much simpler Holm test. For further reading, if one needs more statistical power, he/she may look into the follow-up paper by Garcia & Herrera , which explains more powerful methods for rejecting hypotheses based on $p$-values obtained by pairwise tests. TL;DR First test all subjects across all methods using the Friedman test. Then proceed with pairwise tests using Wilcoxon signed-rank test along with the Holm-Bonferroni FWER method.
