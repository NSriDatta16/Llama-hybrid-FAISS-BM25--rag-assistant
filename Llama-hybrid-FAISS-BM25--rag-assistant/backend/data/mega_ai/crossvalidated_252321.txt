[site]: crossvalidated
[post_id]: 252321
[parent_id]: 
[tags]: 
When to use predictive power versus when to use model fitting metrics?

I built a binary classifier using logistic regression. But I can't seem to rationalize this in my mind. After cross validation, the model's AUC is 0.9003. But, as a sanity check, I ran a GOF (goodness of fit) test using Hosmer-Lemeshow test. What I found was that the model did not fit the data well at all!! (the p-value on the $\chi^2$-square test was very small). I'm confused as to whether or not I should use a GOF to assess the quality of my model. If it predicts really well - do I really care if the GOF is bad? Is the GOF measurement only relevant if I care about interpreting the results of the model, like the parameters, etc.? This article tries to highlight the differences, but doesn't go into how to use them in practice. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3575184/
