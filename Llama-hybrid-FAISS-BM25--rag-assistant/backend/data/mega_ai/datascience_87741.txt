[site]: datascience
[post_id]: 87741
[parent_id]: 28328
[tags]: 
I just came across a research paper that answers this question. In case this helps anyone in the future, the paper Multicollinearity: A tale of two nonparametric regressions mentions that neural networks generally do not suffer from multicollinearity because they tend to be overparameterized. The extra learned weights create redundancies that make things that affect any small subset of features (such as multicollinearity) unimportant. Due to its overparameterization, the coefficients or weights of a neural network are inherently difficult to interpret. However, it is this very redundancy that makes the individual weights unimportant. That is, at each level of the network, the inputs are linear combinations of the inputs of the previous level. The final output is a functions of very many combinations of sigmoidal functions involving high order interactions of the original predictors. Thus neural networks guard against the problems of multicollinearity at the expense of interpretability.
