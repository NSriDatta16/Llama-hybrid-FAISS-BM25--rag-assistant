[site]: crossvalidated
[post_id]: 519126
[parent_id]: 
[tags]: 
How can i understand this inequality related to the ReLU activation function?

I am not sure why the following inequality is true: $$ P\left(\text{ReLU} \circ f\right) \leq 2 \cdot P(f) $$ Where $P(f)$ is the number of linear pieces in the function $f$ . For example, if $f$ is a piecewise function with 2 pieces, then $P(f) = 2$ . Also, how can I understand it for a neural network with 1 input and 1 output?
