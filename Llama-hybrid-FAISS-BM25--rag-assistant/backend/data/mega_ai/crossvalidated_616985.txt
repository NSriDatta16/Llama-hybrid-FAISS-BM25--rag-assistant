[site]: crossvalidated
[post_id]: 616985
[parent_id]: 
[tags]: 
Why complete data log likelihood in M-step of EM algorithm

In Bishop's Pattern Recognition and Machine Learning book(page440), it talks about the M-step in EM algorithm of Gaussian Mixture Model. I am confused about the likelihood function of M-step. By likelihood, it is usually $p(X|\theta)$ , right? But when involving this latent variable in the maximum likelihood function, it becomes a bit hard to understand. In the last sentence of the first paragraph, it says: "we shall suppose that maximization of this complete-data log likelihood function is traightforward", then in the second paragraph, it writes:" Because we cannot use the complete-data log likelihood, we consider...". Why cannot we use the complete-data log likelihood if it is straightforward to maximize? Why taking the expected value under the posterior distribution of the latent variable makes sense? What is the issue if we just maximize $p(X,Z|\theta)=p(Z)p(X|Z,\theta)=\prod_{n=1}^{N}\prod_{k=1}^K{\pi_k^{z^{nk}}}{N(x_n|\mu_k,\Sigma_k})^{z_{nk}}$
