[site]: crossvalidated
[post_id]: 354026
[parent_id]: 354019
[tags]: 
there are examples out there, like from machinelearningmastery , from a kaggle kernel , another kaggle example . The things you should do before going for LSTMs in keras is you should pad the input sequences, you can see that your inputs have varying sequence length 50,56,120 etc. So that you would get uniform length, let's say you are going to fix on sequence length 120. the sequence with less than 120 get's filled with 0s (default) and greater than 120 get stripped off. And LSTM accepts a 3D tensor as input, meaning you need an extra dimension called timestep , which handles on how long you are giving importance. In case of stocks based details, you'd have observations in relevance to a minute. So your input tensor should be of dimensions: (batch_size, timestep, sequence_length) . So you have to convert your padded (batch_size, sequence_length) . You can use an Embedding Layer for that, which takes 2D sparse vector and converts into a 3D tensor, but I have used them only on text based time series classification. Code snippet: github gist I guess that's it. Hope it helps.
