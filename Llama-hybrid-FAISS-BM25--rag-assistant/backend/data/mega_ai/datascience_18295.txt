[site]: datascience
[post_id]: 18295
[parent_id]: 
[tags]: 
Improving classifier performances in R for imbalanced dataset

I have used an "adabag"(boosting + bagging) model on an imbalanced dataset (6% positive), I have tried to maximized the sensitivity while keeping the accuracy above 70% and the best results I got were: ROC= 0.711 SENS=0.94 SPEC=0.21 The results aren't Inhofe, especially the bad specificity. Any suggestion on how to improve the result? Can the optimization be improved, or would the addition of a penalty term help? This is the code: ctrl plot from classifierplots package: I tried xgboost as well. Here is the code: gbmGrid plot from classifierplots package: Update: I tried asymmetric adaboost, this is the code: model_weights but the specificity is zero, what am I doing wrong?
