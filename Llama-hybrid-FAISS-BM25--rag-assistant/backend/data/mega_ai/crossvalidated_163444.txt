[site]: crossvalidated
[post_id]: 163444
[parent_id]: 163399
[tags]: 
First to explain the MH algorithm, it's used to approximate numerically a target distribution , in this case $p(\theta|D)$. At each stage of the algorithm: A value $\theta_{proposed}$ is proposed using the jumping or proposal distribution . An acceptance ratio is calculated, equal to $\frac{p(\theta_{proposed}|D)}{p(\theta_{current}|D)}$. Because we cannot calculate $p(\theta|D)$ directly, we leverage the proportional expression of Bayes rule and calculate this quotient using the products of likelihood and prior corresponding to $\theta_{proposed}, \theta_{current}$. That is, the acceptance ratio is: $$\frac{p(D|\theta_{proposed})p(\theta_{proposed})}{p(D|\theta_{current})p(\theta_{current})}$$ If this ratio exceeds one—intuitively, if the proposed parameter value is more likely given data and prior—we accept this proposal. If not, we accept it with probability equal to the ratio. As more and more values are sampled in this way, the trace of $\theta$ values more and more closely approximates the true distribution of $\theta$. To your specific questions: Your intuition on the jumping distribution is correct. There are a few more formal requirements, see wiki , but it's primary purpose is to propose a candidate parameter value at each step of the algorithm. The prior expresses an analyst's prior beliefs about the parameter values. It is distinct from the proposal distribution. You are correct that the application of Bayes rule happens in the acceptance ratio (see step 2 above). For resources, a very intuitive explanation is given in Kruschke's Doing Bayesian Data Analysis , and that example is loosely summarized here .
