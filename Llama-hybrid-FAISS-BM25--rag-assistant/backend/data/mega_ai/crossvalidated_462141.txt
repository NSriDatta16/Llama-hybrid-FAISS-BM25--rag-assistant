[site]: crossvalidated
[post_id]: 462141
[parent_id]: 233627
[tags]: 
This is my understanding about how kNN works: given a new observation, we will calculate the distance between this new observation and all the other observations in the training dataset. Then you get the neighbours (the ones that are the closest to the new observation). If $k=5$ , then we look at the 5 closest observations. "a locally constant function" means that after choosing these 5 observations, we don't care about the distances. They are the same, they have the same importance now for the prediction. The average that we calculate is the average value of y (which is either 0 or 1, to signify "blue" or "orange"). Say it in another way, it will be the proportion of the positive class (the class you try to predict, it can be arbitrary here, either "blue" or "orange"). Now, if we try to find a function that is not "locally constant", it would be a normal distribution. In this case, you will get an algorithm call Linear Discriminant Analysis or Naive Bayes (depending on some other assumptions). PS: it is the same logic for histogram vs parametric distribution.
