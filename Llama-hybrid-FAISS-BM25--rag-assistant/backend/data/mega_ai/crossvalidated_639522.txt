[site]: crossvalidated
[post_id]: 639522
[parent_id]: 27813
[tags]: 
I'll try to give some background for Xi'an's answer . In the abstract of Prior Probabilities (1968) , E. T. Jaynes says, In decision theory, mathematical analysis shows that once the sampling distributions, loss function, and sample are specified, the only remaining basis for a choice among different admissible decisions lies in the prior probabilities. Therefore, the logical foundations of decision theory cannot be put in fully satisfactory form until the old problem of arbitrariness (sometimes called "subjectiveness") in assigning prior probabilities is resolved. Objective Bayesians like Jaynes see statistical inference as an extension of logic, so they believe that two competent statisticians, faced with the same problem and the same information, should arrive at the same conclusions. But on the standard account of Bayesian statistical inference, there's no principled way to choose a prior in the absence of any information at all. So Jaynes wanted to find a collection of principles that would be enough to uniquely determine an appropriate prior probability distribution for given statistical inference problem in the absence of any data. In the late 18th century, Laplace had proposed the "principle of insufficient reason": you should assign equal probabilities to events which are distinguished only by their labels. Jaynes generalized and formalized that principle, and he also argued for another, the "principle of maximum entropy". The idea of both of these principles is that you should assign prior probabilities in a way that introduces the least possible additional information to the problem. Here's a toy example related to your question about whether the uniform distribution is non-informative for a parameter constrained to the interval [0, 1]: Suppose you're presented with a coin of unknown bias θ, and you don't know anything else about it. You haven't observed any flips yet. What should you believe about the value of θ? Well, you could always just call heads "tails" and vice versa, so by Laplace's principle we know the prior distribution on θ must be symmetric about 1/2. Jaynes gets us further: it's not hard to prove that the uniform distribution is the one that maximizes entropy for a continuous parameter constrained to an interval. So it is indeed non-informative in that specific sense. Unfortunately for Jaynes, skeptics of the objective Bayesian framework have pointed out a number of issues with it. One well-known one that Stéphane alludes to in his comment is van Fraasen's "cube factory paradox" from Laws and Symmetry (1989): A precision tool factory produces iron cubes with edge length ≤ 2 cm. What is the probability that a cube has length ≤ 1 cm given that it was produced by that factory? The problem here is that the uniform prior on edge length is not uniform anymore if we parameterize this problem in terms of area or volume instead. Jefferey's priors are an attempt to resolve this sort of issue, which brings us to the beginning of Xi'an's answer.
