[site]: datascience
[post_id]: 31192
[parent_id]: 31186
[tags]: 
Just a couple of ideas: Batch size : 8 is quite a small batch, meaning the average loss that is computed might have high volatility. If you have enough memory, you could increase this. Diversity of input : try adding batch normalisation layers in the encoder part, to try smoother the input for the conv layers. You said there are quite a few features, so perhaps this makes for noisy input, which would benefit from being normalised. You could trying running the same experiment for 15 epochs, then plotting the training and validation losses (as they evolve perhaps, using the TensorBoard callback alongside your others). Do they follow any patterns or converge after some time? You could try using different initilisation methods, or even gradient clipping, in order to make training a little smoother - constraining the size of the updates to weights during backpropagation. Finally, another (brand-new!) result from research into GAN models, shows that progressively increasing the size of the inputs to your models might help to smooth learning and also extract a more robust set of features, which generalise better. Have a read of this section of an article from FastAI on their experience. EDIT: information regarding the ELU activation Using this activation my help learning, as it has a little more flexibility than e.g. the ELU, because it may also assume negative values. Here is an image from the original paper : Here is the official definition (might slightly differ in the implementation of your framework): The authors mentions that this activation assists in pushing the mean activation closer to zero, just as batch normalisation does. In your case this might mean simply getting past the bumpy initial epochs and converging a little more quickly. The final major point that the authors highlight is that the models they trained also generalised better - so you might enjoy an improved performance with your model using ELUs versus a model using ReLUs (assuming both are trained for similar time).
