[site]: crossvalidated
[post_id]: 396373
[parent_id]: 396369
[tags]: 
It may seem logical to you but your suggested change of the criterion goes against the idea of statistical testing. The threshold of 0.05 is an a priori determined rate of type I error (rejecting the null hypothesis when it should not be rejected). It's set before the test and it's to stay. Once the test is performed, we conclude with either reject the null or not. Two related issues: If the sample size you have right now was based on a certain computation process, that process would have specified the rates of type I and type II error, by toying with the threshold after the test, you may render the results useless. For example, if the original sample size calculation had used type I error rate of 0.10, you might have a lower sample size, and all these ANOVA results could have been different. The p-value is just one of many facets of quantitative comparison. Do not put too much undeserved attention to it as it's not a be-all end-all answer. Also, evaluate the actual mean difference across groups and their corresponding variability (e.g. group SD or 95%CI) as well. Forcing your p-value from not significant to significant would not make your findings more/less useful.
