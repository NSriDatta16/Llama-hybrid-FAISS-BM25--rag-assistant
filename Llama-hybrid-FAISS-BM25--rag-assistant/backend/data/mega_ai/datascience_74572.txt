[site]: datascience
[post_id]: 74572
[parent_id]: 74567
[tags]: 
In fact, there is no maximum or reasonable amount of features for LSTM layers in general. Everything is extremely task specific. Before training an LSTM model with that amount of variables you'd encounter a number of practical problems, for example: Prohibitive computational costs. I don't think any machine could handle that without crashing. Its need of massive amounts of data. Such a model would require a dataset of unimaginable size, in order to be trained properly. But that said, practical problems apart, there is no theoretical threshold to the number of features you can feed into RNNs.
