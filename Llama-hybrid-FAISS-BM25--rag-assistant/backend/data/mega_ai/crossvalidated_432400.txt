[site]: crossvalidated
[post_id]: 432400
[parent_id]: 
[tags]: 
Regularization via model averaging?

Say you have the model $$ \Phi^{-1} \left(y\right)=\beta_0 + \beta_1 x$$ I am interested in adding some regularization, specifically concerning the parameter $\beta_1$ , to introduce some "skepticism" toward large values. I know I could use penalized likelihood methods (e.g. LASSO or Ridge), but I came up with this other approach, which also would avoid the need of selecting the value of the regularization coefficient. The alternative approach would proceed in this way: Estimate a reduced model (equivalent to fixing $\beta_1=0$ ) $$ \Phi^{-1} \left(y\right)=\beta_0 $$ Calculate the AIC: $$\text{AIC}_1 = 4 - 2 \ln \mathcal{L}_1$$ $$\text{AIC}_0 = 2 - 2 \ln \mathcal{L}_0$$ where $\mathcal{L}_1$ and $\mathcal{L}_0$ are the maximized likelihoods of the full and reduced models, respectively. Transform them in differences with respect to the best model ${\Delta _m} = {\rm{AI}}{{\rm{C}}_m} - \min {\rm{AIC}}$ and calculate the Akaike weights $${a_m} = \frac{{\exp \left( { - \frac{1}{2}{\Delta_m}} \right)}}{{\mathop \sum \limits_{j } \exp \left( { - \frac{1}{2}{\Delta_j}}\right)}}$$ Calculate the model-averaged estimate $$\bar \beta_1 = a_0 \beta_{1,0} + a_1 \beta_{1,1} = a_1 \beta_{1,1}$$ (since $\beta_{1,0}=0$ by construction in the reduced model). Following Buckland et al. (1997) the standard error of the model averaged parameter would be $$ \begin{align} \hat \sigma_{\bar \beta_1} & = \sum\limits_{j} {a_j} \sqrt{ \sigma_{\beta_j}^2 + \left(\beta_j - \bar \beta \right)^2 } \\ & = a_0 \beta_{1,0} + a_1 \sqrt{ \sigma_{\beta_{1,1}}^2 + \left(\beta_{1,1} - \bar \beta_1 \right)^2 } \end{align} $$ (again because in the reduced model $\beta_{1,0}=0$ and $\sigma_{\beta_{1,0}}=0$ ) This procedure thus should result in a estimate of the coefficient $\beta_1$ that is shrunk toward zero by an amount that depends on the difference in AIC between the full and reduced model: the less useful is $x$ for predicting $y$ , the more the estimate of $\beta_1$ will shrink toward zero. It seems correct to me to say that AIC could be interpreted as a type of regularization (e.g. this question ) but I was wondering if this approach of using AIC averaging for the specific purpose of regularizing the estimated value of a single parameter has been described before? Is there any relevant reference? In which cases LASSO or Ridge penalty should be preferred, and in which cases instead this approach could be useful?
