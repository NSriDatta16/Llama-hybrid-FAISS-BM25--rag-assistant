[site]: crossvalidated
[post_id]: 441698
[parent_id]: 441687
[tags]: 
The other large class of models that can do that are Variational Autoencoders . Basically, they explicitly try to parameterize some specified probability distribution (up to you what it is) with one half of your neural network so that the parameters match your specification AND that the samples from that distribution can be decoded by the other half to reconstruct your inputs. In a successfully trained model, you can throw out the encoder and feed it some arbitrary latent encodings; unlike a standard Autoencoder, the latent space is contiguous. Note that this method is not mutually exclusive with a GAN, and indeed such a hybrid often generates nicer-looking synthetic decodings.
