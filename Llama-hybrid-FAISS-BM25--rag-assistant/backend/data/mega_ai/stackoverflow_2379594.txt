[site]: stackoverflow
[post_id]: 2379594
[parent_id]: 2379240
[tags]: 
I know you're asking about pycurl, but I find it too hard and unpythonic to use. The API is weird. Here's a twisted example: from twisted.web.client import Agent from twisted.internet import reactor, defer def get_headers(response, url): '''Extract a dict of headers from the response''' return url, dict(response.headers.getAllRawHeaders()) def got_everything(all_headers): '''print results and end program''' print dict(all_headers) reactor.stop() agent = Agent(reactor) urls = (line.strip() for line in open('urls.txt')) reqs = [agent.request('HEAD', url).addCallback(get_headers, url) for url in urls if url] defer.gatherResults(reqs).addCallback(got_everything) reactor.run() This example starts all requests asynchronously, and gather all results. Here's the output for a file with 3 urls: {'http://debian.org': {'Content-Type': ['text/html; charset=iso-8859-1'], 'Date': ['Thu, 04 Mar 2010 13:27:25 GMT'], 'Location': ['http://www.debian.org/'], 'Server': ['Apache'], 'Vary': ['Accept-Encoding']}, 'http://google.com': {'Cache-Control': ['public, max-age=2592000'], 'Content-Type': ['text/html; charset=UTF-8'], 'Date': ['Thu, 04 Mar 2010 13:27:25 GMT'], 'Expires': ['Sat, 03 Apr 2010 13:27:25 GMT'], 'Location': ['http://www.google.com/'], 'Server': ['gws'], 'X-Xss-Protection': ['0']}, 'http://stackoverflow.com': {'Cache-Control': ['private'], 'Content-Type': ['text/html; charset=utf-8'], 'Date': ['Thu, 04 Mar 2010 13:27:24 GMT'], 'Expires': ['Thu, 04 Mar 2010 13:27:25 GMT'], 'Server': ['Microsoft-IIS/7.5']}}
