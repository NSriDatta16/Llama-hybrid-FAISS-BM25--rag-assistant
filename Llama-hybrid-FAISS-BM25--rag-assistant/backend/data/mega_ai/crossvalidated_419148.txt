[site]: crossvalidated
[post_id]: 419148
[parent_id]: 288504
[tags]: 
If you are only trying to compress the images, you should follow @aginensky. But if you are using the compressed vector obtained from the autoencoder in downstream machine learning models, your PCA post-processing step can be useful, especially as some machine learning models either assume uncorrelated input data or perform better when the input data is uncorrelated. Whether or not you should retain only the top k principal components is a different matter again (if you are doing downstream machine learning), and depends a lot on how much loss of information you are willing to accept. You could also have a look at variational autoencoders as an alternative. Here the bottleneck is much narrower, as your latent representation is not a "latent image", but a number of gaussian distributions., i.e. for each gaussian you are learning a mean and a variance. So if you'd be limiting your bottleneck to 20 distributions, you would end up with two vectors of length 20. These two vectors can be concatenated and used as input values in a non-linear machine learning model in your downstream processing.
