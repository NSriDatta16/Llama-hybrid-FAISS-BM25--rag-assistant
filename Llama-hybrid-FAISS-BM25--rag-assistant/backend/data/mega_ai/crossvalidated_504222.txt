[site]: crossvalidated
[post_id]: 504222
[parent_id]: 503764
[tags]: 
Think very carefully about whether you really want to do completely separate models. All regression models run a risk of omitted-variable bias when you omit predictors associated with outcome. With Cox regression, as for logistic regression , this can happen even if the omitted predictor isn't correlated with the included predictors. If your interest is in prediction, then you should include all predictors associated with outcome (provided that you don't over-fit the data). As part of modeling you might then choose to re-evaluate individual predictors among the complete set of predictors, by comparing nested models. That is easily done by standard anova() comparison of the models. There are ways to compare non-nested Cox models formally, some of which are discussed in this thread . I'm struck, however, that an R package devoted to non-nested model comparison doesn't seem to provide support for Cox models, and the package that provides partial-likelihood comparisons of Cox models doesn't seem to have an official release yet. If your interest is in prediction, one possibility would be to evaluate the predictive performance. In your example case, the concordance (fraction of pairs of cases in which the predicted and actual event order in time agrees) of the second model is substantially greater than that of the first, indicating superiority of the second model. In general, however, concordance isn't a very sensitive measure for comparing different models. A better general approach is to repeat the modeling on multiple bootstrapped samples of the data and test performance of each of the model types on the full data set. That's a good way to estimate how well your models might perform on new samples from the population.
