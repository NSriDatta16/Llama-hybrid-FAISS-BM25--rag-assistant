[site]: crossvalidated
[post_id]: 398687
[parent_id]: 398548
[tags]: 
The most straightforward approach to your problem consists of finding document features and then finding the most similar document to features extracted from queries (using kNN for example). You can obtain these features using: Matrix decomposition There is a simple method that does exactly what you described called Latent Semantic Analysis . It factorizes term-document matrix using SVD, so it effectively finds directions in the space defined by words/terms which correspond to groups of words, which usually correspond to concepts. Another similar method is Nonnegative Matrix Factorization (it could be interpreted as soft clustering). You can find these methods in Python in scikit-learn library. LSA can be performed with Truncated SVD and NMF is just NMF. Word embeddings Another approach would be to use neural network-based methods, but these methods are more arcane, and there are lots of them. Also many of them require complex pipelines to load data and are language-specific (note that decomposition using methods are not). For examples that you can just simply run Universal Sentence Encoder comes to mind. This Tensorflow version seems to have very simple interface (you just put strings you want to encode, it also doesn't seem to require any Tensorflow skills). Other You can also use topic models, which in some cases work like matrix factorization (for example NMF can be used for topic modelling), but I'll skip these methods since user3554004 covered them.
