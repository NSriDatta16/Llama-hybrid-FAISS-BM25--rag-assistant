[site]: datascience
[post_id]: 51798
[parent_id]: 51785
[tags]: 
At each decoding time step, the decoder receives 2 inputs: the encoder output: this is computed once and is fed to all layers of the decoder at each decoding time step as key ( $K_{endec}$ ) and value ( $V_{endec}$ ) for the encoder-decoder attention blocks. the target tokens decoded up to the current decoding step: for the first step, the matrix contains in its first position a special token, normally . After each decoding step $k$ , the result of the decoder at position $k$ is written to the target tokens matrix at position $k+1$ , and then the next decoding step takes place. For instance, in the fairseq implementation of the decoding, you can see how they create the target tokens matrix and fill it with padding here and then how they place an EOS token ( ) at the first position here . As you have tagged your question with the bert tag, you should know that what I described before only applies to the sequence-to-sequence transduction task way of using the Transformer (i.e. when used for machine translation), and this is not how BERT works. BERT is trained on a masked language model loss which makes its use at inference time much different than the NMT Transformer.
