[site]: crossvalidated
[post_id]: 345648
[parent_id]: 345566
[tags]: 
Here are some points that I get from Thomas Dietrich's article Ensemble Methods in Machine Learning that might help you Statistical The statistical problem can be found when the amount of training data is too small than the size of search space that causes the learning algorithm give the same accuracy on the training data although it is different hypotheses. By using ensemble methods, the algorithm can average the accurate hypotheses, reduce the risk of choosing the wrong classifier and also can find a good approximation to the true hypothesis. Computational The computational problem arise when training data is enough but learning algorithm is still stuck in local optima. So it's still very difficult computationally for the learning algorithm to find the best hypothesis. An ensemble constructed by running the local search from many different starting points may provide a better approximation to the true unknown function than any of the individual classifiers. Representational In most applications of machine learning, the true function can be represented by forming weighted sums of hypotheses. This may be possible to expand the space of representable functions. So, we must consider the chosen search space to be the effective space of hypotheses searched by the learning algorithm for a given training dataset.
