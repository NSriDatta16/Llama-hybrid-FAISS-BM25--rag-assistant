[site]: crossvalidated
[post_id]: 461396
[parent_id]: 
[tags]: 
How does duplicate data get more weight in a decision tree algorithm?

In many cases duplicate data is used in machine learning such as in boosting when sampling with replacement is used in order to increase the weight of certain examples. However, it is not clear to me how exactly duplicate data effects decision tree algorithm and how exactly this biases the algorithm to focus more on these duplicated data. To my surprise I could not find any detailed explanation of this. I can think of one way: in a leaf of a decision tree the class value is chosen to be the majority class. Hence if you duplicate an example, that example's class will have a higher chance of being the majority class at the leaf that it appears. Is this correct? Are there other ways?
