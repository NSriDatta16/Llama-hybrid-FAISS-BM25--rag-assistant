[site]: crossvalidated
[post_id]: 430771
[parent_id]: 430749
[tags]: 
You actually lay out most of the important points in your question. I assume we're restricting attention to strictly monotonic transformations. Monotonic transformations preserve order so quantiles are "preserved" (more precisely, quantiles are equivariant to monotonic transformation) but they don't preserve relative lengths/distances so moments and moment ratios are not "preserved". So for example, if $Y$ has mean $\mu_Y$ , median $\delta_Y$ and standard deviation $\sigma_Y$ , and $Z=t(Y)$ for some monotonic increasing transformation $t$ (with corresponding population parameters $\mu_Z$ , median $\delta_Z$ and $\sigma_Z$ ), then backtransforming we have that in general $\mu_Y\neq t^{-1}(\mu_Z)$ , $\sigma_Y\neq t^{-1}(\sigma_Z)$ but it is the case that $\delta_Y= t^{-1}(\delta_Z)$ . Similarly, modes are not preserved. It's important to note that because relative distances are not preserved distances between quantiles are not equivariant. The backtransformed interquartile range is not the interquartile range of the original data, even though that works for the quantiles individually. In many cases, the key is to be precise about what it is you want to know on the scale of the original response variable. In hypothesis testing, if your null + assumptions corresponds to having the distributions be the same, then on the transformed scale the distributions should still be the same. However, the relationship under the alternative is different. An example would be transforming, doing a two-sample t-test and transforming back. If the usual assumptions are satisfied on the transformed scale, then you're dealing with a location-shift alternative (though this is not absolutely necessary, somewhat more general alternatives can be dealt with). When you backtransform the alternative will not be a location shift. (For an example, if your transformation was the log, once you transform back you're looking at a change of scale, not location.) A typical case where transformation crops up is when someone transforms a response in order to fit a linear regression model, producing an estimate of the conditional mean response. However, when you transform back you don't end up with the conditional mean. You can find an approximation (e.g. via the delta method) to try to get a reasonable estimate of $E(Y|x)$ from estimates of $E(Z|x)$ and $\text{Var}(Z|x)$ where $x$ represents the vector of predictors. Alternatively if you're prepared to assume you have a normal distribution on the transformed scale you may be able to get more precise estimate of the conditional mean after backtransforming. One common error with regression I've seen is people backtransform a coefficient estimate and treat the result like they would a coefficient in a regression. This usually doesn't make sense. It certainly doesn't describe a linear relationship between $Y$ and some predictor $x$ . Interpretation of backtransformed relationships is nontrivial. Even additivity is not preserved. That is, if you had a main-effects model on the transformed scale you can't write a backtransformed conditional mean as a sum of nonlinear effects in the variables. A simple example would be taking a log transform and transforming back by exponentiation. If you fit $\log Y = \beta_0 + \beta_1 x_1 +\beta_2 x_2+\epsilon$ then you cannot write $E(Y|x_1,x_2) = f(x_1) + g(x_2)$ ; you can't even assume symmetry on the log scale and have it hold for the backtransformed median (the connection would be multiplicative in that case). It's important to think about how the backtransformation acts on the whole relationship. If conditional means are an important consideration, an alternative is generalized linear models. By writing the linear predictor as a model for the transformed mean (via the link function) rather than transforming the data, you end up with a model for the conditional mean on the original scale, and no similar issues result from backtransforming there (well, aside from the kind of error described in the previous two paragraphs). There are times when transformation makes sense -- but most often this is when the transformed variable is easily interpretable in its own right.
