[site]: crossvalidated
[post_id]: 430217
[parent_id]: 430184
[tags]: 
Sparse representations have two main purposes: They are form of regularization , that pushes parameters to exact zeros. It works as any other form of regularization, so basically, it leads to simpler model by learning what parameters can be dropped, lowering their total number. Another reason for using them, is to produce models that are smaller in terms of memory usage, for example, when you need to send them over network, or store on mobile devices. That's the same reason as for using sparse data formats. Think of a recommender system, where you have thousands of users and thousands of products, so the matrix of all the possible interactions is huge, by using sparse data format you can save a lot of memory. Same with sparse model representation. Deep learning models have a lot of redundancies, and in many cases you can get rid of a lot of weights by preserving the quality of results, as discussed for example by James Kwork in this talk on Compressed Deep Neural Networks . You can remove those redundant weights and use sparser solution, that needs less memory to be stored. Notice that sparse models are defined in terms of sparse weights, they are not different story.
