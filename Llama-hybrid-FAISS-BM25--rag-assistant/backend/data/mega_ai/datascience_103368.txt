[site]: datascience
[post_id]: 103368
[parent_id]: 
[tags]: 
Long term mathematical side effects of overfitting a model to reinforce a behavior

I have a neural network I designed, and previously I got this network working with some considerable simplification to its design which I was not fond of making. However I noted that during training if I just let the training engine respond to its horrible initial inferenced moves there would be points where it would simply stop moving because of initial and sometimes side effects relating to actions that conflicted and did opposite things being outputted. I placed some code which I never really used that responded to 'critical events', and would overfit until the most important outputs were resolve. Eg if the output said 'move left and move right' or 'rotate up and rotate down' or 'out of range or fire when the target is perfectly aligned' i'd fixate on training and training with the correct outputs until the model state shifted to eliminate these stalemates. Would the side effects of this eventually work themselves out as they were gradually eliminated ? As to work them out what I was doing was essentially overfitting to resolve issues with an inferenced value. I don't think further information is really necessary to understand what I'm asking. The network in question controls a simulated targeting system, its outputs are simple, its inputs are simple. It works well enough if you train it to an optimal path. But the idea here is to let it mess up, until it hits a snag. @NikosM. the model in question is linear, it uses the sigmoid function as its activation function, the process interprets values from 0.5 to less than 0.74 as False and >0.75 as yes the model takes the current turret rotation, and the polar coordinate value of the target, it also takes a muzzle velocity and a target distance which indicates the direct line distance between the target and the current landing area. outputs are Up, Down (Elevation of muzzle), Left, Right (Horizontal Rotation), Out of Range , and Fire Now. This model does train with some success but this question is more non specific. In one of the training modes I try, I take whatever garbage output the model gives me, let the turret do it and train it with a calculated correction value. In some of the cases the model spits out information which is opposing. It tells the model to go up and down, left and right or oor and fire. I consider these 'critical' outputs and begin training with the correct value till one of them changes as it should trying to simulate practicing away a very very bad failure in performance, this of course is overfitting when I hit the model with 200 or so training events on the same data. However because its correcting a pretty serious deadlock, I was wondering if eventually the effects of overfitting in this case would be mitigated by continued training on more data points. The training program generates data, random targets, and calculates the values to feed the program which are based off the available calculatable data and constant values related to the turrets operation. So I don't understand why this is considered helpful because the question seems like a general one, but okay. The neural networks smallest working goddamn part lol Is defined as follows in pytorch. self.linear_relu_stack = nn.Sequential( nn.Linear(8, 22), nn.Linear(22,22), nn.Linear(22,64), nn.Linear(64,22), nn.Linear(22,8), nn.Sigmoid() ) It uses the MSELoss function and SGD optimizer.
