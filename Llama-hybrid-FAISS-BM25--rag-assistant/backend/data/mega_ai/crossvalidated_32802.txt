[site]: crossvalidated
[post_id]: 32802
[parent_id]: 
[tags]: 
Regression to obtain autocorrelation measure (AR(1))

This is not homework. I am a frequent user on math.stackexchange, but I am learning a bit about time series models and came across this example. Any ideas would be greatly appreciated. A linear regression model ￼was fit to some time-series data by ordinary least squares. The residuals￼ from the fit were then used to create two new variables, namely $E$ with values $\hat{e}_2,...,\hat{e}_n$￼ and $E_1$ with values￼ $\hat{e}_1,...,\hat{e}_{n-1}$. A linear “regression through the origin” was then run with E as the dependent variable and $E_1$ as the predictor. The slope estimate was 0.412 with a standard error of 0.133. Assume that the $e_t$ follow a standard AR(1) model. Estimate the first order autocorrelation $\rho$ of the AR model. Can the output be used to obtain a valid standard error for the estimate in 1? Is the answer to #1 above just the slope of the regression E~E_1 ?
