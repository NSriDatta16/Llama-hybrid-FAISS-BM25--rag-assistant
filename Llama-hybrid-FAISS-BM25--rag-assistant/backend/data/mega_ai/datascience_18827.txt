[site]: datascience
[post_id]: 18827
[parent_id]: 18782
[tags]: 
Just to amplify @Dal's excellent answer, you certainly can fit models with small sample sizes. That's exactly what classical statistics attempts to do, often with success. But there is a price in terms of quality of the data , simplicity of the model and experimental design. For example, to estimate interaction terms efficiently and credibly, you want to randomly apply the treatment effects. Machine learning typically occurs in observational data where none of these assumptions are correct. That said, 102 samples is way to small in a problem with 11 variables and a binary outcome. Classifications typically take more data than regression problems (continuous outcome). That's why you often hear pollsters using 1000 respondents to predict a categorical outcome on the basis of one or two features (questions). There is an optimistic rule of thumb that one needs 10 variables for each parameter that one wants to estimate. I have always thought this was a bit thin, but even on that measure, your sample is too small.
