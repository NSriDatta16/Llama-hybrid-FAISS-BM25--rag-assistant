[site]: datascience
[post_id]: 10103
[parent_id]: 
[tags]: 
Improve the speed of t-sne implementation in python for huge data

I would like to do dimensionality reduction on nearly 1 million vectors each with 200 dimensions( doc2vec ). I am using TSNE implementation from sklearn.manifold module for it and the major problem is time complexity. Even with method = barnes_hut , the speed of computation is still low. Some time even it runs out of Memory. I am running it on a 48 core processor with 130G RAM. Is there a method to run it parallely or make use of the plentiful resource to speed up the process.
