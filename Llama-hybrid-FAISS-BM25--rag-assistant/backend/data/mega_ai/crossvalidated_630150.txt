[site]: crossvalidated
[post_id]: 630150
[parent_id]: 
[tags]: 
The relationship between Bellman Equation and EM(Expectation Maximisation) Algorithm

Today I came across Value Evaluation Algorithm under the topic of Bellman Equation when learning reinforcement learning, which is stated as: While not converged: Policy Update : $\pi_{k+1} =\arg\max_{\pi}(\gamma_{\pi} + \gamma P_{\pi}v_k)$ Value Update : $v_{k+1} = r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}}v_k$ This algorithm really reminds me of EM. Because we first initialize our $v$ randomly, then we find the best policy that will yield the maximum state value. After that, we calculate state value $v$ again based on the policy we have determined earlier. Does it mean that Value Evaluation is a special case of EM algorithm? What's the intuition or math behind it?
