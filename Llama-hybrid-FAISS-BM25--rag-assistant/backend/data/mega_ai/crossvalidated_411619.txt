[site]: crossvalidated
[post_id]: 411619
[parent_id]: 411441
[tags]: 
Are there "topological" style theorems about machine learning/computational learning theory, that don't make use of $\epsilon$ 's and $\delta$ 's? I think it's illustrative to mention why most ML uses " $\epsilon$ 's and $\delta$ 's" - that's because it's based on statistics, and if you actually use statistics for anything, you need guarantees for finite samples , not for how stuff behaves in the limit. I recall single theorem that may actually look like what you've described: On Fiber Diameters of Continuous Maps uses Borsuk-Ulam theorem to establish that every dimensionality reduction method necessarily 'glues together' arbitrarily distant points. If on the other hand you're asking about interesting topological ideas in data science/ML, then you could check these ones out: Topological Data Analysis UMAP algorithm I think that you should also find Topological and Geometric Data Reduction and Visualization course interesting.
