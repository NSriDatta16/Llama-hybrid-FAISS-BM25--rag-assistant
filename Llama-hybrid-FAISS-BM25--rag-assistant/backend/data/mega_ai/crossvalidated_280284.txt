[site]: crossvalidated
[post_id]: 280284
[parent_id]: 131491
[tags]: 
I know it is less elegant, but I had to simulate it. Not only did I build a pretty simple simulation, but it is inelegant and slow to run. It is good enough, though. One advantage is that, as long as some of the basics are right, it is going to tell me when the elegant approach falls down. The sample size is going to vary as a function of the hard-coded value. So here is the code: #main code #want 95% CI to be no more than 3% from # prevalence #expect prevalence around 15% to 30% #think sample size is ~1000 my_prev 1){ idx_3 $\pm$ 3% without going over it. [![sample size vs prevalence][1]][1] Away from 50%, "somewhat less observations" seem to be required, as kjetil suggested. I think that you can get a decent estimate of prevalence before 400 samples, and adjust your sampling strategy as you go. I don't think there should be a jog in the middle, and so you might bump N_loops up to 10e3, and bump the "by" in "my_prev" down to 0.001. [1]: https://i.stack.imgur.com/vNYcH.png
