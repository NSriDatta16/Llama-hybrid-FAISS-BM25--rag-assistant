[site]: datascience
[post_id]: 25030
[parent_id]: 11134
[tags]: 
You could use region embeddings. Rather than converting individual "tokens" to vectors you could use a strategy to convert regions of text to vectors. This approach is used here . If you're not limited to CNN, you could use a hierarchical attention models such as this one where you have a pipeline of this kind: word vectors (combined into) sentence vectors (combined into) final document vector Note that, with this method, you will still have to convert all the word vectors to embeddings, but not all at once. To handle documents of different lengths, padding/cutting is the only solution so far. Finally, to increase speed, you could try to reduce the dimension of the text by only including important sections (maybe only the beginning of the document is sufficient to have good classification accuracy)
