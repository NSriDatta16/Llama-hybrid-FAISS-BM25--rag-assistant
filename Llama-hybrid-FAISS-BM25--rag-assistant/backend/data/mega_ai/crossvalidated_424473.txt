[site]: crossvalidated
[post_id]: 424473
[parent_id]: 424468
[tags]: 
It's a bit nuanced. You could pull out the big guns and use a poisson regression # example sample of counts (n = 50, 'true' mean = 5) set.seed(25) d >> 2.5 % 97.5 % 4.143126 5.348048 Or you could use the normal approximation since, as $\lambda$ gets big enough, the poisson starts to look a lot like a normal (though this only really happens when $\lambda$ is really really big). Be warned, if your sample is small and your $\lambda$ is close to zero, this may result in the CI covering values less than 0 mu = mean(d) se = sd(d)/sqrt(length(d)) c(mu-1.96*se, mu+1.96*se) >>>[1] 4.124606 5.315394 That the two estimates differ is perfectly acceptable since they make different assumptions. I would always prefer the former since the CI is computed using profile likelihood estimates. Lastly, there is the procedure defined here though I think it adds little value over using glm . Here are some simulations to explore the coverage properties for both methods given your data generating process: set.seed(0) simGlmCi >>0.951 simNormalCi >>0.964 You might be better off with the profile likelihood approach since it seems the normal approach is a bit permissive.
