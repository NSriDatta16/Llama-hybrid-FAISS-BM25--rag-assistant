[site]: crossvalidated
[post_id]: 635166
[parent_id]: 345765
[tags]: 
Especially if the teams were of fixed size, then one could use just about any model and teach them about the invariance to order by feeding them training data consisting of permutations of all inputs in all possible orders (plus having some overall team information like number of members, mean/variance/other things about each feature etc.). That may feel a bit inefficient and you need to be careful to e.g. do cross-validation splits before doing this data augmentation. For many models this ignores that (if I understand correctly) items/team members share features that in a way mean the same thing for each one of them. If you put that information into the separate variables, it gets treated like it has nothing to do with each other. With neural networks, there's the option of using a common encoder to produce identical representations in a new feature space. That could be one tempting approach (in combination with the data augmentation above), but still does not deal well with the variable number of items. One potential model architecture that combines all of the above (and does not need permuting of the items) would be a transformer neural network (of language model fame), which actually had to originally be modified with positional encodings to reflect that text is not just a bag of words but rather have a distinct order (positions for each word in the text).
