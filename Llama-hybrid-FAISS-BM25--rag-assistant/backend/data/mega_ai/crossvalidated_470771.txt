[site]: crossvalidated
[post_id]: 470771
[parent_id]: 470753
[tags]: 
1) Is differentiability of G and D wrt. their parameters only required to train them by gradient descent or are there other reasons? I believe it's also assumed in a lot of GAN convergence proofs. example . Granted, this is mostly of theoretical interest rather than practical. 2) Why do we need differentiability wrt. the inputs of the generator? Yeah, you're right, wrt. the parameters should be good enough. are they talking about discrete or continuous probability distributions? Usually continuous. However there is work which explores discrete domain GANs (which have interesting applications in language or molecule generation). The discrete case usually requires some sort of trick like gumbel-softmax or Reinforce. Also, the existence of a continuous $p_g$ is non-trivial by just assuming a continuous $p_z$ . I'm not familiar with theory here, but the paper I linked above does analyze convergence on absolutely continuous generator distributions versus the general case.
