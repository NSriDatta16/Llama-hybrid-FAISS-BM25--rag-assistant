[site]: crossvalidated
[post_id]: 358476
[parent_id]: 199221
[tags]: 
Density estimation in the standard case: A standard technique for estimating distributions from observations is the kernel density estimator (KDE) , which reduces down to the empirical distribution if your kernel is a Dirac delta function . A broader continuous kernel centred at the observation value is usually used to give a continuous density over the support. For the standard sampling problem you observe data $\mathbf{x} = (x_1,...,x_n)$ and you estimate the density as: $$\hat{p}(x | \mathbf{x}) = \frac{1}{n \delta} \sum_{i=1}^n K \Big( \frac{x-x_i}{\delta} \Big),$$ where $K$ is a density centred at zero and $\delta$ is a bandwidth parameter that is usually estimated from the data (e.g., by MLE). The KDE is equivalent to estimating the density function using an empirical estimate of the characteristic function, with a damping function to prevent divergence. (The kernel in the KDE is the Fourier transform of the divergence function.) Extension to data with errors-in-variables: If you observe densities $\mathbf{p} = (p_1,...,p_n)$ in lieu of exact observations, the natural generalisation of the KDE is to replace the kernel at the observed value with its expectation under the observed densities. This generalisation gives: $$\hat{p}(x | \mathbf{p}) = \frac{1}{n \delta} \sum_{i=1}^n \mathbb{E} \Big[ K \Big( \frac{x-X_i}{\delta} \Big) \Big] = \frac{1}{n \delta} \sum_{i=1}^n \int \limits_\mathcal{X} K \Big( \frac{x-x_i}{\delta} \Big) p_i(x_i) dx_i.$$ Since expectation is a linear operator, we have $\hat{p}(x | \mathbf{p}) = \mathbb{E} (\hat{p}(x | \mathbf{X})) $, which is the expected KDE under the random observations $\mathbf{X} \sim \text{IID }\mathbf{p}$. So all we are really doing in this extension is finding the expected KDE given a set of densities for the data. Convergence: The standard KDE is used in a context where you have an underlying sequence of exchangeable random variables, so you have IID data with a common distribution. In this context, the KDE is known to be uniformly convergent to the true distribution under mild assumptions (see e.g., Jiang 2017 ). When you extend this to replace the observations with individual densities, you no longer have an exchangeable sequence of observations, and so you will need to specify what you mean by the underlying "true" distribution, and you will also need to specify the relationships of the "true" distribution with the observed densities for the observations ( @dsaxton points out this limitation in your question in the comments). Taking the "true" distribution to be an average of the sequence of observable densities would be a natural extension, and it should allow you to get convergence results similar to the standard case. This would be a larger analysis, and it is beyond the scope of what I can do in a short answer like this.
