[site]: crossvalidated
[post_id]: 204111
[parent_id]: 
[tags]: 
random forest - summarize two features in one without losing information

I am training a random forest on a dataset including both categorical and numerical features. In particular I have a binary feature, call it $x_1$, which has $0$ or $1$ as possible outcomes. I also have a feature, call it $x_2$, which has integer values $1, 2, 3, 4, 5$ as possible outcomes. The training set is not very large and when training I have overfitting (perfect score on training set, 75% accuracy on test set), thus I want to reduce the number of features. My question is: does it make sense to replace $x_1$ and $x_2$ by a new feature, call it $x_3$, defined by $$ x_3 = x_1 + 2^{-x_2} \quad ? $$ In this way I seem to have the best of both worlds: I have reduced the number of features by one (thus, in principle, reducing overfitting) and at the same time I didn't lose any information, because: if a training sample has $x_1=0$, it will have $x_3$ between $2^{-5}$ and $1/2$; if on the other hand $x_1=1$, it will have $x_3$ between $1+2^{-5}$ and $3/2$. The two subsets stay well detached one from the other and the random forest in principle should be able to discriminate between them. This seems to me too good to be true. Does this procedure can really reduce overfitting without reducing the "power" of the model?
