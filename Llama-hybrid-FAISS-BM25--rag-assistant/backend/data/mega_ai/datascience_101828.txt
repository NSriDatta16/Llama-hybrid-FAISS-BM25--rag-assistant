[site]: datascience
[post_id]: 101828
[parent_id]: 101783
[tags]: 
Either at training time or at inference time, both an LSTM and a Transformer decoder act exactly the same in terms of inputs and outputs: At training time, you provide the whole sequence as input , and you obtain the next token predictions. In LSTMs, this training regime is called "teacher forcing"; we use this fancy name because LSTMs (RNNs in general) have another way of being trained: by using at training time their own predictions as input for the following token. This training regime, however, is unstable and almost nobody uses it in practice; the standard for training LSTMs is using teacher forcing. At inference time, you predict one token at a time, and use such a prediction as the input for the next step. Note that I am referring to the Transformer decoder, not the whole encoder-decoder Transformer, which was originally proposed by Vaswani et al., 2017 : In an encoder-decoder Transformer (e.g. source sentence in a neural machine translation setup), the encoder part ingests the context at once, processing it in parallel, and the decoder receives the output of the encoder and acts autoregressively, as I described previously. You can't compare a single LSTM with an encoder-decoder Transformer architecture. But you can indeed compare the text generation process of an LSTM an a Transformer decoder. Both are normally used as language models.
