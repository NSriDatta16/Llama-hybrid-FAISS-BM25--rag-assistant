[site]: crossvalidated
[post_id]: 181202
[parent_id]: 
[tags]: 
Sum over Posterior Predictive Distribution

I am confused about the posterior predictive distribution. This is from Murphy's Machine Learning: A Probabilistic Perspective. According to the example set in Chapter 3, the posterior predictive distribution is: $$p(\tilde{x} \in C \mid D) = \displaystyle\sum_{h} p(y = 1 \mid \tilde{x}, h) p(h \mid D) $$ I believe $y=f(x\in C) = \begin{cases} 1 & \quad \text{if } x \in C\\ 0 & \quad \text{if } x \notin C\\ \end{cases} $ although it's not mentioned in the text. Should the posterior predictive distribution sum to 1? $$ \displaystyle\sum_{\tilde{x}} p(\tilde{x} \in C \mid D) = 1$$ or is the interpretation that $$ p(\tilde{x} \in C \mid D) + p(\tilde{x} \notin C \mid D) = 1$$
