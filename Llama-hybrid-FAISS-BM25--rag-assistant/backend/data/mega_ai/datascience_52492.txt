[site]: datascience
[post_id]: 52492
[parent_id]: 52445
[tags]: 
Summary: the question probably* isn't whether one false negative is worse than one false positive, it's probably* more like whether 500 false positives are acceptable to get down to one false negative. * depends on the application Let me expand a bit on @Dragon's answer: Screening means that we're looking for disease among a seemingly healthy population. As @Dragon explained, for these we need extremely low FPR (or high Sensitivity), otherwise we'll end up with many more false positives than true positives. I.e., the Positive Predictive Value (# truly diseased among all diagnosed positive) would be inacceptably low. Sensitivity (TPR) and Specificity (TNR) are easy to measure for a diagnostic system: take a number of truly (non)diseased cases and measure the fraction of correctly detected ones. OTOH, both from doctors' and patients' point of view, the predicitive values are more to the point. They are the "inverse" to Sensitivity and specificity and tell you among all positive (negative) predictions, what fraction is correct. In other words, after the test said "disease" what is the probability that the patient actually does have the disease. As @Dragon showed you, the incidence (or prevalence, depending on what test we're talking about) plays a crucial role here. Incidence is low in all kinds of screening/early cancer diagnosis applications. To illustrate this, ovarian cancer screening for post-menopausal women has a prevalence of 0.04 % in the general population and 0.5 % in high-risk women with family history and/or known mutations of tumor suppressor genes BRCA1 and 2 [Buchen, L. Cancer: Missing the mark. Nature, 2011, 471, 428-432] So the question is typically not whether one false negative is worse than one false positive, but even 99 % specificity (1 % FPR) and 95 % sensitivity (numbers taken from the paper linked above) then means roughly 500 false positives for each false negative. As a side note, also keep in mind that early cancer diagnosis in itself is no magic cure for cancer. E.g. for breast cancer screening mammography, only 3 - 13 % of the true positive patients actually benefit from the screening . So we also need to keep an eye on the number of false positives for each benefitting patient. E.g. for mammography, together with these numbers , a rough guesstimate it that we have somewhere in the range of 400 - 1800 false positives per benefitting true positive (39 - 49 year old group). With hundreds of false positives per false negative (and also maybe hundreds or even thousands of false positives per patient benefitting from the screening) the situation isn't as clear as "is one missed cancer worse than one false positive cancer diagnosis": false positives do have an impact, ranging from psychological and psycho-somatic (worrying that you have cancer in itself isn't healthy) to physical risks of follow-up diagnoses such as biopsy (which is a small surgery, and as such comes with its own risks). Even if the impact of one false positive is small, the corresponding risks may add up substantially if hundreds of false positives have to be considered. Suggested reading: Gerd Gigerenzer: Risk Savvy: How to Make Good Decisions (2014). Still, what PPV and NPV are needed to make a diagnostic test useful is highly dependend on the application. As explained, in screening for early cancer detection the focus is usually on PPV, i.e. making sure you do not cause too much harm by false negatives: finding a sizeable fraction (even if not all) of the early cancer patients is already an improvement over the status quo without screening. OTOH, HIV test in blood donations focuses first on NPV (i.e. making sure the blood is HIV-free). Still, in a 2nd (and 3rd) step, false positives are then reduced by applying further tests before worrying people with (false) positive HIV test results. Last but not least, there are also medical testing applications where the incidences or prevalences aren't as extreme as they usually are in screening of not-particularly-high-risk populations, e.g. some differential diagnoses.
