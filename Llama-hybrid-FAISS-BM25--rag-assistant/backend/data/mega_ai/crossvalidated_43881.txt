[site]: crossvalidated
[post_id]: 43881
[parent_id]: 
[tags]: 
With a small sample from a normal distribution, do you simulate using a t distribution?

I want to simulate temperature data for some "what-if" calculations. The problem is, I only have a time series of 10 actual temperature data values. I want to use temperature as an input to the simulation, so I need a way to generate a large number of temperature values that are consistent with the original 10 values. It's probably ok to assume that they came from a normal distribution, but I don't know the mean or the variance. I have no way to prove it, but I doubt the 10 values do a good job of representing the full temperature range. If I use the sample function for the simulation, as shown below, I only get the original values back. That just doesn't look right. If I use the rnorm function, I know that I don't know the variance, so I don't think that is right either. So, I'm left with the rt function (t-distribution). Below is a mock up of the problem. ori My questions are, When I only have data (mean and variance unknown), and it is reasonable to assume that the data came from a normal distribution, is it ok to generate data for a simulation using a t-distribution? For this type of situation, the only time I would use rnorm for the simulation is if I knew the variance (not a variance estimated from the data), right? If a t-distribution simulation is ok for these conditions, are there any conditions where it is better to just sample the data (for example 100 original data points, 200, etc)? Edit: Since I used the original data to estimate the mean and variance, should the degrees of freedom in the third line of the code (for rt(...) ) be reduced from 10 to 9? Or 8?
