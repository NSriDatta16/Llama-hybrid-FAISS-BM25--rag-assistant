[site]: crossvalidated
[post_id]: 520704
[parent_id]: 520684
[tags]: 
The goal of a hard-margin SVM (you didn't mention slack variables, $\xi$ , for soft-margin SVMs) is to minimize the Euclidean norm $||\mathbf{w}||$ to impose the inequality $y_i D(\mathbf{x}_i)/||\mathbf{w}||>1$ (Boser et al, 1992). This is accomplished by using both the margin value $M$ and the weight vector $\mathbf{w}$ and enforcing the constraint $M ||\mathbf{w}||=1$ , which when solving for the optimal margin gives $M=1/||\mathbf{w}||$ . For hard-margin SVMs, the unconstrained Lagrangian function is also \begin{equation} L({\bf w},b,\boldsymbol{\alpha})=\frac{1}{2}{\bf w}^T{\bf w} - \sum_{i=1}^n \alpha_i [y_i ({\bf w}^T{\bf x}_i + b) - 1 ]\\ \end{equation} $ s.t.\quad \quad \alpha_i \geq 0, \quad y_i ({\bf w}^T{\bf x}_i + b) - 1=0$ Reference: B.E. Boser, I.M. Guyon, V.N. Vapnik. A training algorithm for optimal margin classifiers. $\textit{Proc. 5th Annual Work. Comp. Learning Theory (COLT'95),}$ pp. 144-152. New York (NY), ACM Press, 1992.
