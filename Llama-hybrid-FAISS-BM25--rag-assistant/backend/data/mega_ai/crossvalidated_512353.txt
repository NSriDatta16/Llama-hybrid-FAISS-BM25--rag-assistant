[site]: crossvalidated
[post_id]: 512353
[parent_id]: 512350
[tags]: 
Let's assume that the test set is perfect representation of the population. This is a really bad assumption because it's hardly ever true. What's happened before isn't necessarily informative about what will happen in the future. Past performance is not indicative of future results. If I will evaluate the machine learning predictive model on the test set, can we call the good results the absolute proof that the model will work in real word? No. Even if the test set is drawn from the same distribution as the data that you'll encounter in the future, that data is still a sample. Repeated samples from the same distribution exhibit random variation -- a pattern or measurement you find on the test data might be a statistical artifact. Of more immediate relevance is the question of overfitting the test data. If I keep training and tuning my models until I get a good result on the test data, there's a good chance that I've obtained a completely bogus result because I've done a lot of work to find a model with a good score on the test data.
