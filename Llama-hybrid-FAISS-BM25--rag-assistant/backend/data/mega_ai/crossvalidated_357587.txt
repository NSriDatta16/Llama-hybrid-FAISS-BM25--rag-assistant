[site]: crossvalidated
[post_id]: 357587
[parent_id]: 90938
[tags]: 
I would like to use multivariate normal as an example. Recall that the likelihood is given by $$ P(y_1,y_2,...,y_n|\mu,\Sigma) = (2\pi)^{-\frac{ND}{2}}\det(\Sigma)^{-\frac{N}{2}}\exp(\frac{1}{2}\sum_{i=1}^N(x_i-\mu)^T\Sigma^{-1}(x_i-\mu)) $$ In order to find a prior to this likelihood, we may choose $$ P(\mu,\Sigma)=\text{Normal}(\mu;\mu_0,\Lambda_0)\text{InverseWishart}(\Sigma;\nu_0,S_0) $$ I assure you NOT to worry about $\mu_0,\Lambda_0,\nu_0,S_0$ for now; they are simply parameters of the prior distribution. What is important is, however, that this is not conjugate to the likelihood. To see why, I would like to quote a reference I found online. note that $\mu$ and $\Sigma$ appear together in a non-factorized way in the likelihood; hence they will also be coupled together in the posterior The reference is "Machine Learning: A Probabilistic Perspective" by Kevin P. Murphy. Here is the link . You may find the quote in Section 4.6 (Inferring the parameters of an MVN) at the top of page 135. To continue the quote, The above prior is sometimes called semi-conjugate or conditionally conjugate , since both conditionals, $p(\mu|\Sigma)$ and $p(\Sigma|\mu)$, are individually conjugate. To create a full conjugate prior , we need to use a prior where $\mu$ and $\Sigma$ are dependent on each other. We will use a joint distribution of the form $$ p(\mu, \Sigma) = p(\Sigma)p(\mu|\Sigma) $$ The idea here is that the first prior distribution $$ P(\mu,\Sigma)=\text{Normal}(\mu;\mu_0,\Lambda_0)\text{InverseWishart}(\Sigma;\nu_0,S_0) $$ assumes that $\mu$ and $\Sigma$ are separable (or independent in a sense). Nevertheless, we observe that in the likelihood function, $\mu$ and $\Sigma$ cannot be factorized out separately, which implies that they will not be separable in the posterior (Recall, $(\text{Posterior}) \sim (\text{Prior})(\text{Likelihood})$). This shows that the "un-separable" posterior and "separable" prior at the beginning are not conjugate. On the other hand, by rewriting $$ p(\mu, \Sigma) = p(\Sigma)p(\mu|\Sigma) $$ such that $\mu$ and $\Sigma$ depend on each other (through $p(\mu|\Sigma)$), you will obtain a conjugate prior, which is named as semi-conjugate prior . This hopefully answers your question. p.s. : Another really helpful reference I have used is "A First Course in Bayesian Statistical Methods" by Peter D. Hoff. Here is a link to the book. You may find relevant content in Section 7 starting from page 105, and he has a very good explanation (and intuition) about single-variate normal distribution in Section 5 starting from page 67, which will be reinforced again in Section 7 when he deals with MVN.
