[site]: crossvalidated
[post_id]: 576732
[parent_id]: 576253
[tags]: 
I look at this question as one of trying to assess the reliability or agreement of the two models in terms of how they rate model features. A simple method for assessing agreement would be to compute the correlation (Pearson, Spearman, Kendall, etc.) between the two vectors of ranks. The correlation coefficient between the two could provide a single value that would provide an assessment of their concordance. Other agreement-oriented statistics such as Krippendorf's $\alpha$ could also be useful in this case as well but penalizes deviations across methods differently than would any of the correlation coefficients. Give the linked Wikipedia page a look with attention to the difference function for ordinal data (most applicable to rank vectors like discussed here).
