[site]: crossvalidated
[post_id]: 326714
[parent_id]: 326688
[tags]: 
The confusion here stems from conflating a random variable with its distribution. To be clear about the issue, a random variable is not a function of the model parameters, but its distribution is. Taking things back to their foundations, you have some probability space that consists of a sample space $\Omega$, a class of subsets on that space, and a class of probability measures $\mathbb{P}_\theta$ indexed by a model parameter $\theta$. Now, the random variable $X: \Omega \rightarrow \mathbb{R}$ is just a mapping defined on the domain $\Omega$. The random variable itself does not depend in any way on the parameter $\theta$, and so it is wrong to write it as a function $X(\theta)$. It is of course true that the probability distribution of $X$ depends on $\theta$, since the latter affects the probability measure over the sample space. However, it does not affect the sample space itself.$\dagger$ Consequently, when you are dealing with a statistic , which is just a function of the observed random variables, this also does not depend on $\theta$, but its distribution usually does. (If not, it is an ancillary statistic,) $\dagger$ This treatment has taken $\theta$ as an index for the probability measure, but the same result occurs under a Bayesian treatment where $\theta$ is regarded as a random variable on $\Omega$ and the behaviour of $X$ is treated conditionally on the parameter.
