[site]: datascience
[post_id]: 12433
[parent_id]: 12422
[tags]: 
I don't write a lot of Python code but it appears you are hard coding alpha, which would explain your results. I suggest reading up on the math and intuition behind logistic regression, along with how model parameters are estimated in general.( https://en.wikipedia.org/wiki/Maximum_likelihood_estimation ). For guidance on implementation this article does a good job of going through the steps and has the benefit of being in Python: http://aimotion.blogspot.com/2011/11/machine-learning-with-python-logistic.html . While I highly suggest you learn how to write it from scratch first, you may want to check out R which has a very easy to use base implementation of logistic regression. df = read.csv("~/data.csv", header = T, stringsAsFactors = F) glm_train = glm(classifier ~ value, data = df, family = "binomial") library(ggplot2) binomial_smooth = function(...) { geom_smooth(method = "glm", method.args = list(family = "binomial"), ...) } ggplot(df, aes(x = value, y = classifier)) + geom_point() + binomial_smooth() summary(glm_train) # Deviance Residuals: # Min 1Q Median 3Q Max # -1.70557 -0.57357 -0.04654 0.45470 1.82008 # # Coefficients: # Estimate Std. Error z value Pr(>|z|) # (Intercept) -4.0777 1.7610 -2.316 0.0206 * # x 1.5046 0.6287 2.393 0.0167 * # --- # Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
