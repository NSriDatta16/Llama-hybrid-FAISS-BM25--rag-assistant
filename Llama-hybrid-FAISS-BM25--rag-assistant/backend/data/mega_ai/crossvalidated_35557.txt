[site]: crossvalidated
[post_id]: 35557
[parent_id]: 
[tags]: 
On Elo++ updating rule

I am not sure if this is the correct place to ask this kind of question, I hope it is. I am studying this paper on an improvement of Elo rating system called Elo++. On page 4 the author states that he want minimize a function called total loss that is $$L=\sum_{i,j\in T}w_{i j}\left( \hat o_{i j}-o_{i j}\right)^2+\lambda\sum_{i\in D}\left(r_i-a_i\right)^2$$ where $r_i$ is the rank of the player that as to be estimated, $ \hat o_{i j}$ is the outcome of a game where player $r_i$ plays against player $r_j$ , $a_i$ is the arithmetic average of the set $D$ of the player $r_j$ (the players $r_j$ which have played against $r_i$ ), $o_{i j}$ is the predicted outcome that is a function of $r_i, r_j$ . The problem for me comes at page 5 where the author says that in order to minimize the total loss, he had used a stochastic gradient descent. The updating formula in the stochastic gradient descent is $$ r_i \leftarrow r_i-\eta\left(w_{i j}\left(\hat o_{i j}-o_{i j}\right)\hat o_{i j}\left(1-\hat o_{i j}\right)+\frac{\lambda}{N_i} \left(r_i-a_i\right)\right) $$ How is this updating formula is related to the problem of minimizing the first condition?
