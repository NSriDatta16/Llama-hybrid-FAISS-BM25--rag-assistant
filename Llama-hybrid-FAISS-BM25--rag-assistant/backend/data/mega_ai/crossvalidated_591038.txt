[site]: crossvalidated
[post_id]: 591038
[parent_id]: 
[tags]: 
How to find optimum column values when they affect another column value in time series data?

Original data frame spans a million rows and has an year's data. Here's top ten rows: Code: import pandas as pd df = pd.DataFrame({'time': {0: '2020-12-08', 1: '2020-12-08', 2: '2020-12-08', 3: '2020-12-08', 4: '2020-12-08', 5: '2020-12-08', 6: '2020-12-08', 7: '2020-12-08', 8: '2020-12-08', 9: '2020-12-08'}, 'points': {0: 200, 1: 50, 2: 0, 3: 0, 4: 150, 5: 200, 6: 200, 7: 50, 8: 100, 9: 200}, 'count': {0: 19, 1: 9, 2: 32, 3: 28, 4: 22, 5: 17, 6: 22, 7: 17, 8: 28, 9: 24}}) If you'd observe the actual whole dataset, you'd see that the 'count' decreases when 'points' is either too high or too low. How to find the optimum value of points? Optimum means the value of points that gives maximum number of counts. Answer might be a high school level mathematics solution. One possible solution I can think of is to train an ML model using just points as X and count as y, (maybe no train test split), then make a data frame with points 1 to 500 and predict count for each point. Maybe the number of point with most count could be considered as optimum statistically?
