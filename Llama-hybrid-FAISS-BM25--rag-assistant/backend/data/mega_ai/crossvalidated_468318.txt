[site]: crossvalidated
[post_id]: 468318
[parent_id]: 467621
[tags]: 
If your data is distributed in a well-defined way, Westgard rules are probably the simplest solution. For your problem, I'd propose something like this: Compute some lower bound on "good" periods in your time series, e.g., the 10% percentile. Work through previous failure-cases and establish a sequential-deviation rule that discriminates failures from usual noise. For example: three out of the last four numbers are below the 10% percentile. You could also use something like mean and SD but that rarely represents proportions well. Apply this rule for each new data point as it enters. This is very fast, easy to understand, and does not model when the change point occurred. However, it only works well if the system is non-noisy, i.e., it should be easy to discriminate true change from business-as-usual. If not, you'd be better off going for a probabilistic approach using a changepoint package that can detect intercept changes in time series ( mcp , EnvCpt , or others )
