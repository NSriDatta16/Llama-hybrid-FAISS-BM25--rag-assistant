[site]: crossvalidated
[post_id]: 573554
[parent_id]: 573475
[tags]: 
I infer that your question is what can be done about this. You need to take advantage, carefully, of the numbers of relevant observations going into each "mean of means" and of the experimental design structure. You example data actually are significantly different. I use R instead of GraphPad. t.test(c(2,5,4,3),c(6,7,7,9)) # Welch Two Sample t-test # # data: c(2, 5, 4, 3) and c(6, 7, 7, 9) # t = -4.1603, df = 5.9961, p-value = 0.005951 For illustration in a harder case, start with "ideal" values each 2 units lower for the second set. Say that the two sets of values represent two treatments evaluated in 4 separate experiments. Each experiment involved 4 animals for each treatment. That's 32 animals total. Say that experiments are expected to have systematic differences from each other, something captured here by putting both sets of "ideal" values into increasing order. setA Now there is no "significant" difference between the "means of means" with an unpaired t-test under the usual p t.test(setA,setB) # Welch Two Sample t-test # # data: setA and setB # t = -1.9415, df = 5.9961, p-value = 0.1003 sample estimates: mean of x mean of y 3.50 5.25 Use these "ideal" values as a guide, but add some randomness. First simulate mean values for each of the 32 individual animals with random variation--normal distributions, standard deviation (SD) of 2--around the above "ideal" mean values. set.seed(101) indivA Then simulate 3 technical replicates for each animal around those animal-mean values, with an SD of 3. techA Collect these final "measurements" into a dataframe, with val the measurement values, id representing animals, experiment the 4 experiments, and treatment of A or B. Use factor() to have the numeric labels interpreted correctly as identifiers rather than numbers. df1 Verify that the simple means of means of means aren't "significantly" different after this added randomness. First get the overall means (each a mean of 3 observations on each of 4 animals) for each experiment/treatment combination. byExpTx An unpaired t-test on those mean values, comparing the 4 experiments under treatment A against the 4 under treatment B, isn't "significant." t.test(byExpTx[1:4,"val"],byExpTx[5:8,"val"]) # Welch Two Sample t-test # # data: byExpTx[1:4, "val"] and byExpTx[5:8, "val"] # t = -1.7533, df = 5.6455, p-value = 0.1332 # # sample estimates: # mean of x mean of y # 3.277114 4.971085 If, however, you take advantage of the structure of the experiments, pairing of treatments within experiments, you see a "significant" difference. t.test(byExpTx[1:4,"val"],byExpTx[5:8,"val"],paired=TRUE) # # Paired t-test # # data: byExpTx[1:4, "val"] and byExpTx[5:8, "val"] # t = -4.7523, df = 3, p-value = 0.01768 You can also do better than comparing the means of means of means if you use the information about the number of cases underlying the observations. You might use the average of technical replicates to represent each animal:* byExpTxId Then even without considering the pairing within experiments: lm1 |t|) # (Intercept) 3.2771 0.5546 5.909 1.8e-06 # treatmentB 1.6940 0.7843 2.160 0.0389 # Residual standard error: 2.218 on 30 degrees of freedom # Multiple R-squared: 0.1346, Adjusted R-squared: 0.1057 # F-statistic: 4.665 on 1 and 30 DF, p-value: 0.0389 The "30 degrees of freedom" represent the 32 animals that the calculations now incorporate, less the 2 estimated coefficients. Add in the pairing within experiments to see how that also matters: lm2 F) # treatment 1 22.956 22.9563 5.9832 0.02124 # experiment 3 44.029 14.6765 3.8252 0.02098 # Residuals 27 103.593 3.8368 More generally, you might use a mixed model with animals ( id ) (or even experiment ) treated as random effects, but in this case with a balanced design and underlying normal distributions it doesn't make any difference. *You have to be careful to distinguish the technical replicates within animals from the biological replicates among animals. To evaluate the treatments you need to compare the treatment differences against the differences among animals, not against the differences of technical replicates within animals. Taking the mean of technical replicates is one way to avoid that problem. A mixed model using all of the measurements and treating animals as random effects is another, better particularly if you have different numbers of replicates for each animal. In GraphPad that's what's done in "nested" t-tests and ANOVA.
