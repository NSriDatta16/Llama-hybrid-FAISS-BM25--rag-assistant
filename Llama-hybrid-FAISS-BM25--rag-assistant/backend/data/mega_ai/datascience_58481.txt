[site]: datascience
[post_id]: 58481
[parent_id]: 
[tags]: 
Soft Margin SVM kernels

Kernels are used to map datasets into higher dimensions so that they could be linearly separable. However, if we introduce the slack variable in the soft margin SVM, we are allowing some mistakes, and in that case, do we still bother using kernels to try to make the dataset separable in higher dimensions knowing that we are allowing mistakes already? If so, what are the advantages of that compared to not using kernels? Thanks!
