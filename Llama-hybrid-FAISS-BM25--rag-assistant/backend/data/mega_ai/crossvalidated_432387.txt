[site]: crossvalidated
[post_id]: 432387
[parent_id]: 
[tags]: 
Use of Random Forest in a paper

I am currently reading a paper from a Geophysics journal in which the authors apply a random forest to data sets from shear laboratory experiments. I am new to machine learning, and I'm confused about what part of their method makes it a random forest, and not just a Gradient Boosted Regression Tree. Basically, the idea is to predict the time until failure using windows of acoustic time series data. In the paper, they state, The RF model is an average over a set of decision trees. Each Decision tree predicts the time remaining before the next failure using a sequence of decisions based on statistical features from the time windows. However, there is a supplementary information detailing the algorithm used. They explicitly state that they used a Gradient Boosted Regression Tree model: For building our model we use the “fitensemble” Matlab function, method “LSBoost”. Fitensemble is based on the gradient boosting strategy applied for least squares. We set the learning rate equal to 0.01 and a minimum number of leaf node equal to 5 observations. The model we grow is made of 400 trees. To train the model, the used a shifting window consisting of all samples from $N$ seismic cycles and predict time to failure for all samples in the subsequent cycle. Each training window is split into 2 parts and used separately and recursively to identify relevant features for training: We first create 25 random permutations of $J$ features, where $J$ is the minimum between $L$ (i.e., 94 in our case) and the number of samples in half of the training set. This is done to avoid over-training by using a number of features which exceeds the number of samples used to train the algorithm. Then, for every permutation, we build up a training feature sub-matrix $Z$ consisting of all the objects in the whole training set, but described only by the $J$ features in $X$ , as indexed in the permutation, and the corresponding time to failure vector. We start by considering only the first one of these $J$ features, we build a small model of 5 trees on the first half of the training data in $Z$ and we systematically compute, and keep record of, R and RMSE between predicted and observed time to failure in the second half of the training data in $Z$ . We then proceed by progressively adding one feature (of the $J$ indexed) per iteration so that, per every permutation, we keep track of RMSE and R for every given combination of features. Finally, we select the features combination that ensures the minimum RMSE and $R>0$ , and we re-fit the model on the whole training set using only the selected features. Am I missing something, or is this just gradient boosting and not random forest? At no point in the supplement do they mention random forest.
