[site]: crossvalidated
[post_id]: 138064
[parent_id]: 
[tags]: 
Does the number of rows really matter beyond a point?

While working with any machine learning algorithm, does the number of rows really matter beyond a certain point? I have kept some algorithms(decision tree in this instance) running for days, and the accuracy I get is similar to those I get immediately for 5000 rows. I'm kind of feeling that running it on the entire data set is more like a 'check' to verify the results you get with 5000 rows. Is there any study on how many rows are actually needed and how much results vary when increasing the number of rows?
