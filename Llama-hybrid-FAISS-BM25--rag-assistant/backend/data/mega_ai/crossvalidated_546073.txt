[site]: crossvalidated
[post_id]: 546073
[parent_id]: 
[tags]: 
Differential entropy of a kernel density estimator

I have a set of (~500k) observations that I use to fit a parametric model (univariate three-parameter lognormal). To have an idea of the goodness of fit, I want to compute the Jensen-Shannon distance to a kernel density estimator (held for the "true", or at least, a truer approximation of the sample distribution). My kernel density estimator has the form $$ \hat{f}_{\text{KDE}}(x) = \frac{1}{x h \sqrt{2\pi}}\sum_i w_i \exp\left(-\frac{\ln^2\left(x/x_i\right)}{2h^2}\right) $$ (using a lognormal kernel to account for the shape of my sample distribution, with constant window width $h$ , observations $x_i$ and weights $w_i$ summing to 1). Now there is a closed-form of the entropy of the lognormal distribution. I compute the KDE entropy (and the average entropy of the two estimators) numerically. But as I have a lot of different samples on which I want to perform this computation, having a closed-form for the entropy $-\int_0^{\infty} \hat{f}_{\text{KDE}}(x)\ln\left(\hat{f}_{\text{KDE}}(x)\right)\mathrm{d}x$ as well would really speed things up. I did not manage to get one, the log of the sum seeming rather intractable to me; I'd be happy to know if a closed-form for the differential entropy of a linear combination of distributions is indeed impossible to get!
