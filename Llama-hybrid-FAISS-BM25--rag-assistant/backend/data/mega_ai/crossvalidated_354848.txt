[site]: crossvalidated
[post_id]: 354848
[parent_id]: 258045
[tags]: 
No one has mentioned likelihood, which is foundational to Bayesian statistics. An argument in favor of teaching Bayes first is that the flow from probability, to likelihood, to Bayes, is pretty seamless. Bayes can be motivated from likelihood by noting that (i) the likelihood function looks (and acts) like a probability distribution function, but is not because the area under the curve is not 1.0, and (ii) the crude, commonly-used Wald intervals assume a likelihood function that is proportional to a normal distribution, but Bayesian methods easily overcome this limitation. Another argument favoring Bayes first is that the P(A|B) versus P(B|A) concern about p-values can be more easily explained, as mentioned by others. Yet another argument favoring "Bayes first" is that it forces students to think more carefully about conditional probability models, which is useful elsewhere, e.g., in regression analysis. Sorry for the self-promotion, but since it is entirely on-topic, I do not mind stating that this is precisely the approach that Keven Henning and I took in our book "Understanding Advanced Statistical Methods," ( https://peterwestfall.wixsite.com/book-1 ) whose intended audience is non-statisticians.
