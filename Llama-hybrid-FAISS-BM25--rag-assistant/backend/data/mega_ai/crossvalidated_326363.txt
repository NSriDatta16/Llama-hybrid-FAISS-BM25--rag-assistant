[site]: crossvalidated
[post_id]: 326363
[parent_id]: 12492
[tags]: 
I think an interesting point is, that when the data is separable there should be infinite solutions. But if u use GD you converge to the max-margin solution (intuition I have is that GD for linear regression when its overparametrized converges to pseudo-inverse which is min-norm which is max margin since margin is sometimes related to 1/w). So in a way, its like having a unique minimizer. Even though we never truly get there, we do converge to it. You can check this out here: [1710.10345] The Implicit Bias of Gradient Descent on Separable Data ( https://arxiv.org/abs/1710.10345 ) intuitively if you look at the gradient: $$ \nabla_w l(w) = \frac{1}{N} \sum^N_{n=1} \frac{y^{(n)} x^{(n)}}{ 1 + e^{y^{(n)} w^\top x^{(n)}} }$$ since the weights increase so does the score and thus the denominator of the above. But the weights increase “sort of linearly” while the decrease in the size of the of the gradient is exponential (as seen above). So GD stops updating “pretty soon”. Or at least thats the ay I sort of understand it at a high level. For real answers, refer to the paper of course. Therefore if the data is separable and you use GD you converge (approach) to the max-margin solution for unregularized logistic regression, which is unique .
