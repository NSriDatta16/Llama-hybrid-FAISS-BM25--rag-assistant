[site]: crossvalidated
[post_id]: 312200
[parent_id]: 
[tags]: 
In training my classifier, average precision stays constant regardless of the recall

I'm using a neural network to do binary classification. My architecture is 500->128->128 followed by a sigmoid cross entropy output layer. Regardless of the number of hidden layers/neurons I use, the average precision over the validation stays fairly constant, at roughly 44%, even though the recall can vary between 5% and 30%. I have tried various alternative loss functions, regularization methods, etc, but the precision remains in this narrow range around 44%. What could be an explanation for this?
