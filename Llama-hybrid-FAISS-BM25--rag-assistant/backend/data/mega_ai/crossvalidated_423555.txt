[site]: crossvalidated
[post_id]: 423555
[parent_id]: 
[tags]: 
Do these two Bellman equations express the same idea?

I am currently trying to wrap my head around the Bellman equation and found the following two definitions. The first one is taken from cs229 Stanford - link to the sources is avaiable below. The second equation is taken from own lecture notes and I find them more difficult to understand. $V_\pi (s) = R(s) + \lambda \sum\limits_{s'âˆˆS } P_{s\pi(s)}(s')V_\pi (s')$ $V \pi (s) =\sum\limits_a \pi(a|s)\sum\limits_{s_{t+1},r}p(s_{t+1}, r |s, a) [r + \lambda V_\pi(s_{t+1})]$ I would like to know: Do these equations actually describe the same thing the same way? Are they equivalent? In the 2nd equation: Why do we have the term $\sum\limits_a p(a|s)$ ? What does it do? In the 2nd equation: Why do we also sum over all $r$ in this $\sum\limits_{s_{t+1},r}\dots$ ? Links I found useful: http://cs229.stanford.edu/notes/cs229-notes12.pdf Why is the optimal policy in Markov Decision Process (MDP), independent of the initial state? Are these three different ways of expressing the optimal value function $V^*$ the same? (reinforcement learning)
