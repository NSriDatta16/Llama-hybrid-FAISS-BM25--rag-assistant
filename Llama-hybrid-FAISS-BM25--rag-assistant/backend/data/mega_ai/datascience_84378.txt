[site]: datascience
[post_id]: 84378
[parent_id]: 
[tags]: 
how weighted log loss works

I have seen this in a kaggle notebook. I understand we add some weight to classes. what I don't understand is how those weights are generated. below is the code. Can you explain why it's useful and how weights are produced? p = 0.369197853026293 pos_public = (0.55410 + np.log(1 - p)) / np.log((1 - p) / p) pos_private = (0.55525 + np.log(1 - p)) / np.log((1 - p) / p) average = (pos_public + pos_private) / 2 print (pos_public, pos_private, average) w1 = average / p w2 = (1 - average) / (1 - p) dtrain = xgb.DMatrix(X, label = y) def weighted_log_loss(preds, dtrain): label = dtrain.get_label() return "weighted_logloss", -np.mean(w1 * label * np.log(preds) + w2 * (1 - label) * np.log(1 - preds))
