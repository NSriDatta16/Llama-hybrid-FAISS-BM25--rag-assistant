[site]: crossvalidated
[post_id]: 612966
[parent_id]: 438709
[tags]: 
See this post . To summarize the post above, the goal of word2vec is to compute word embeddings such that semantically similar words are embedded closer to each other, and conversely, dissimilar words are embedded farther from each other. In the neural network formulation, that means we minimize $$u_c^T v_c$$ because a word c is rarely in its own context. This minimization is not feasible if we constrain u to be same as v.
