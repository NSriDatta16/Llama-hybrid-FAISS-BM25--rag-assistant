[site]: crossvalidated
[post_id]: 280655
[parent_id]: 280654
[tags]: 
You don't need to "improve your results." That sounds like p-hacking, i.e. nudging the p-value below 0.05. Don't focus on the p-value. There's nothing magical about p = 0.05. What you should focus on is communicating your data. In this case the data says that your method is slightly better than Method 1. However there's probably more here, and some of it might be interesting. One thing to look at is whether classification accuracy is a valid measure. Are the classes imbalanced? If so, accuracy might not be useful, since a dumb classifier could simply guess the more abundant class for >50% accuracy. What about the data points that Method 1 fails on? Does your classifier do better than average on these? If so, that could be interesting. Or does your classifier tend to fail in the same way that Method 1 fails? What data set are you using to compare the classifiers? Can you collect or simulate more data? How do the classifiers compare for those data sets? If nothing interesting comes up, then you might just want to conclude with the information you gave in your answer: your classifier does slightly better than Method 1.
