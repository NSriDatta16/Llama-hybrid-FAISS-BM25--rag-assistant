[site]: crossvalidated
[post_id]: 348154
[parent_id]: 
[tags]: 
Why is Bayesian inference necessary for generative models?

I'm trying to understand and summarise the equations for Bayesian inference and generative models in order to motivate the use of deep generative models (such as VAE). A generative model is given by the joint probability distribution $$p(x,z) = p(x|z)p(z)$$ with the latent variable $z$ being a unit gaussian process. The likelihood is given by $$ p(x|z) = \dfrac{p(z|x)p(x)}{p(z)} $$ We know that the marginal likelihood (evidence) is intractable due to the integral given by $$ p(x) = \int p(x|z)p(z)dz $$ Re-arranging everything yields $$p(x|z) = \dfrac{p(z|x) \int p(x|z)p(z)dz }{p(z)}$$ So the problem we are facing is that we can't find the evidence. However, in literature everyone is concerned about finding the true posterior $p(z|x)$ by re-arranging the second equation. Then the posterior becomes intractable due to the intractable evidence. Apparently finding the likelihood is not the problem. But how does that relate to the generative model in the first equation since that contains the likelihood? Basically, I'm confused about how these equations exactly relate to one another, where the problem really lies and how these deep generative models (VAE in particular) solve them. Why do we perform inference to create a generative model when a generative model is defined by the likelihood and prior? Note: I do know very well how a VAE works, I am just confused by the fundamental theory behind what problem they solve.
