[site]: crossvalidated
[post_id]: 127012
[parent_id]: 126921
[tags]: 
As a Bayesian, it does not make much of a difference to me to think of $\mathcal{P}(\lambda)$ as a given distribution, indexed by a parameter $\lambda$, or as a conditional distribution, conditional on the realisation of a random variable $\lambda$. Indeed, in either case, when I observe $$ y_1,\ldots,y_n\stackrel{\text{iid}}{\sim}\mathcal{P}(\lambda) $$ the data is indexed by a given if unknown value of $\lambda$. Whether or not this $\lambda$ is the realisation of a random variable does not change the behaviour of the data. Remember, there is only one realisation of $\lambda$ for a given dataset, no matter its size. So, even when making the assumption that $\lambda\sim\pi(\lambda)$ (a certain prior distribution chosen by me), I do not get observations from the marginal $$ m(y_i) = \int_0^\infty f(y_i|\lambda)\pi(\lambda)\,\text{d}\lambda $$ but observations from the conditional. Hence, for the likelihood function and related inference, conditioning or not does not make a difference as the parameter is assumed fixed for the data at hand. Conditioning only makes a difference when running a Bayesian analysis, since the posterior $$ \pi(\lambda|y_1,\ldots,y_n) \propto f(y_1|\lambda)\cdots f(y_n|\lambda)\pi(\lambda) $$ only makes sense as a conditional distribution from the joint distribution $$ \pi(\lambda,y_1,\ldots,y_n) = f(y_1|\lambda)\cdots f(y_n|\lambda)\pi(\lambda) $$ (meaning that $\lambda$ has to be a random variable for this derivation to make sense). At a probabilistic level, assuming the $\sigma$-algebra on $\mathcal{Y}\times\Theta$ is induced by the products of the measurable sets on $\mathcal{Y}$ and on $\Theta$, writing $p_\lambda(y)$ or $p(y|\lambda)$ does not make a difference either.
