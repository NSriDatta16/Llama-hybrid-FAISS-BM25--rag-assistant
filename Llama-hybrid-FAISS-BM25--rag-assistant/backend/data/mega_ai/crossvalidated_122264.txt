[site]: crossvalidated
[post_id]: 122264
[parent_id]: 121916
[tags]: 
A problem with unorder categorical values is that if you dummy encode them you force an ordering and thus a new meaning to the variables. E.g if you encode blue as 1 and orange as 2 and green 3 then you imply that a data pattern with orange value is closer to a pattern with green value than the one with the blue value. One way to handle this is to make them new features (columns). For each distinct value you create a new binary feature and set it to true/false (in other words binary encode the values and make each bit a column). For each data pattern from this new set of features, only one feature will have the value 1 and all the others 0. But this usually doesn't stop the training algorithm to assign centroid values close to 1 to more than one features. This ofcourse might cause interpretation issues cause this doesn't make sense in the data domain. You don't have the same problem with "capacity classes" namely ordered categories since in that case the numerical values assignment makes sence. And ofcourse is you use features of different nature or measurement unit or different range of values then you should always normalize the values. https://stackoverflow.com/questions/19507928/growing-self-organizing-map-for-mixed-type-data/19511894#19511894 https://stackoverflow.com/questions/13687256/is-it-right-to-normalize-data-and-or-weight-vectors-in-a-som/13693409#13693409
