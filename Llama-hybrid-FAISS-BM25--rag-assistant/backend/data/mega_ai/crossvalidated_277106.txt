[site]: crossvalidated
[post_id]: 277106
[parent_id]: 277103
[tags]: 
If you pad with zeros before you regress, then in general this could bias your results (typically time series don't do us the courtesy of being conveniently near 0). This would not usually be regarded as suitable -- but when it is, it would be a form of AR, but not any of the usual ones. If instead you omitted the first two rows (the ones you padded) so that there's no extra zero's brought in, then: if you regress $w_t$ on $w_{t-1}$ and $w_{t-2}$ then that is effectively an autoregressive model, but the estimation differs from the usual way the AR(2) model is estimated in the way that the first few observations are treated. Specifically the regression method effectively conditions on the first two observations (using what's called conditional least squares estimation ), while the more usual approaches to estimating AR(2) doesn't condition on those first few observations -- it incorporates them into the estimation as well (there are several different estimation methods that don't do this conditioning, of which maximum likelihood estimation would be the most widely used nowadays). Consider for example, an AR(1) model. The conditional regression is equivalent to writing the Gaussian likelihood for the last n-1 observations (conditional on the first one) and maximizing that. Maximum likelihood estimation would multiply that likelihood by the likelihood of the first observation (which we can write unconditionally - using a regression-style parameterization - it's $w_1 \sim N(\beta_0/(1-\beta_1),\sigma^2/(1-\beta_1^2))$). Similarly likelihood for the first few terms of higher order AR models can be written by only conditioning on the prior observations that are available -- more generally you write the likelihood for $w_1$, then $w_1|w_2$ then $w_3|w_1,w_2$ and so forth until you hit the order of the AR and you're back to the usual terms as you have with errors from the conditional least squares. In a long series the conditioning in the conditional least squares approach will typically make only a little difference. In a short series it can be more substantial. There is a form of estimation that does "fill in" data before the first observation (to do something similar to what you attempted but in a more sophisticated way) -- it effectively uses the AR model to "backcast" the pre-sample values and them proceeds with least squares again.
