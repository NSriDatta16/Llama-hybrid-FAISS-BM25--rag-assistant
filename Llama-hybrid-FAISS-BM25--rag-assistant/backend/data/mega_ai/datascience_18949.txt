[site]: datascience
[post_id]: 18949
[parent_id]: 5178
[tags]: 
This is a pretty common problem. I had this pain when I did research projects for a university and now - in industrial data science projects. I've created and recently released an open source tool to solve this problem - DVC . It basically combines your code in Git and data in your local disk or clouds (S3 and GCP storage). DVC tracks dependency between data and code and builds the dependency graph (DAG). It helps you to make your project reproducible. DVC project could be easily shared - sync your data to a cloud (dvc sync command), share your Git repository and provide access to your data bucket in the cloud. "learnable in a few hours" - is a good point. You should not have any issues with DVC if you are familiar with Git. You really need to learn only three commands: dvc init - like git init . Should be done in an existing Git repository. dvc import - import your data files (sources). Local file or URL. dvc run - steps of your workflow like dvc run python mycode.py data/input.jpg data/output.csv . DVC derives the dependency between your steps automatically, builds DAG and keeps it in Git. dvc repro - reproduce your data file. Example: vi mycode.py - change code, and then dvc repro data/output.csv will reproduce the file (and all the dependencies. You need to learn a couple more DVC commands to share data through the cloud and basic S3 or GCP skills. DVC tutorials is the best starting point.
