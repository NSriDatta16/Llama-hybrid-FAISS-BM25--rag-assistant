[site]: crossvalidated
[post_id]: 409510
[parent_id]: 408119
[tags]: 
Neither of the two methods is "wrong"; they measure different aspects of the data-modelling aspects. That being said, I would have a strong preference on the Leathwick et al. approach as it tries to explicitly account for sampling variability; ultimately it is a repeated cross-validation report of our performance metric. To comment on this a bit further: On the one hand, the Nieto & MÃ©lin's approach aims to directly generalise the concept of coefficient of determination $R^2$ , using the deviance residuals instead of the actual ones. It does not resample the data or anything similar, it directly reports the choose metric (here $D^2$ ) overall the whole data. This can potentially lead to unreasonably optimistic results regarding the generalisation of our model's performance. On the other hand, the Leathwick et al. approach aims to incorporate the sampling variability directly through the repeated cross-validation step. We estimate values of our performance metric using "unseen" data that were excluded during training. Note that the "optimistic bias" can be immediately seen in the Table 3 of the L. et al. paper, if we use the model residual deviance instead of the CV residual deviance. In that case for example the CV-generated $D^2$ would move from $0.600$ to $0.663$ for the case of a Boosted Regression Tree with tree size 5 . The sample size of your particular application is not explicitly stated. Nevertheless given it is not gigantic, reading through the methodology presented in Beleites et al. (2013) Sample size planning for classification models is a good starting point to get idea of how to assess sample size consideration in a (multiple) CV procedure.
