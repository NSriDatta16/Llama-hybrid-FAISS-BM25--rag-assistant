[site]: crossvalidated
[post_id]: 395977
[parent_id]: 
[tags]: 
How can I lower the resources taken by an ANN while limiting data loss?

I designed a visual quality assurance system with a goal to determine whether a factory part during manufacturing is flawed or not. The need arose because it is rather difficult to tell if a part is valid in a quick glance, and there are many different types of flaws. The system takes a high-resolution snapshot during assembly in grayscale, scales the image to a much, much lower resolution , and feeds the pixel values to a deep feedforward neural network. This works, but the problem is that it basically makes the high-end camera pointless. I would like to utilise the camera, but having $4500 \cdot 6000$ inputs is out of the question. I had another idea where I could split entire images into smaller parts, then feed those parts to a smaller ANN. This maintains the resolution and keeps the ANN simple, but the problem now is that I would have to somehow label all the mini-images as either good or bad examples, whereas before I only had to label the entire full image. This seems like too much work. Here's a crude demonstration of what I mean: Let's say that the holes in the part are flaws: right now, I train the network to recognise parts with holes, and feed it the entire image. With the new method, I train it to recognise holes using the red mini-images, then feed the segments to the network & calculate whether or not the sample is valid based on how many "bad" segments there were. The issue here is the categorisation of mini-images instead of whole images. Can anyone recommend some ideas or reading material on this particular problem?
