[site]: datascience
[post_id]: 66636
[parent_id]: 66627
[tags]: 
It does not support at the time (it will come just as xgboost did not have to have it) Given thats its a boosting method in the first place one can ask whats the history of xbgoost and subsequent cat and lgboost. XGBoost implementation of gradientboosting did not handle categorical features because it did not have to, it was sufficient enough as it was. What made xgboost special was the use of Hessian information. When other implementations (e.g. gbm in sklearn in Python) used just gradients, XGBoost used Hessian information when boosting. Which in turn was super faster. TL;DR handling categorical features is not just a matter of convinience, it is also important for speed. This feature will come subsequently (if we contribute!) for time its all about representing uncertainty .
