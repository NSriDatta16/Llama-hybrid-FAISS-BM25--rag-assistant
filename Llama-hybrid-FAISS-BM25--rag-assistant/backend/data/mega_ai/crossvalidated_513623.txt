[site]: crossvalidated
[post_id]: 513623
[parent_id]: 
[tags]: 
How to stabilize model performance?

I am performing a regression classification to predict genes that are likely to cause disease. I have 600 rows of training genes by 8 features. Although only 50 genes have a score >0.9 (on a scale of 0 to 1, where 1 is definitely causing disease and 0 is a gene not causing the disease) and unfortunately cannot increase this, whilst I also have ~400 genes I benchmark a few models using nested cross-validation and Bayesian hyperparameter tuning (models: gradient boosting, extreme gradient boosting, random forest, support vector machine, and k-nearest neighbors). I assess model performance with all the metrics scikit-learn has to offer for regression classification (MSE, MAE, max error, r2, etc.). However when I re-run the same code I get small flucuations in these measures (e.g. r2 changes from 0.70 to 0.71. Differences in performance have only been +/- 0.01 on my re-runs, but this concerns me as I thought I've done everything to make sure the performance is stable. Is there anything I can do further check/ensure model performance stability? I thought about removing some of the genes that have 0.9 but I'm not sure if this is good practice? Also wouldn't go for synthetically increasing high scored genes as I'm not sure that's trustworthy either. I have also: Set the random_state seed globally (to 0) I select the 8 features using the Boruta algorithm and check model performance further with shap Checked that the imputation on the data before machine learning also outputs a stable dataset (so the data going into the models is the same every time). I use missforest random forest imputation for this. For the tree-based models I don't scale or normalize data but for the other non-tree-based I do scaling. Gradient boosting is my top model so in theory if its performance was stable I'd go for using that model, and so as an example the parameter tuning I do looks like: seed=0 gbr = GradientBoostingRegressor(random_state=seed) gbr_params = { 'learning_rate': (0.01, 0.5), 'max_depth': (1, 4), "max_features":["log2","sqrt", "auto"], "criterion": ["friedman_mse", "mse", "mae"], 'n_estimators': (10, 50) } inner_cv = KFold(n_splits=5, shuffle=True, random_state=seed) outer_cv = KFold(n_splits=5, shuffle=True, random_state=seed) I also use sklearn's train_test_split() for which I also set the same seed of 0.
