[site]: crossvalidated
[post_id]: 525015
[parent_id]: 525009
[tags]: 
We can think of logistic regression as being a two-part process: we specify the likelihood for each data point as $y_i\mid x_i \stackrel\perp\sim \text{Bern}(\theta_i)$ , and then we model the probabilities by $\theta_i = \sigma(x_i^Tw)$ for some parameter vector $w\in\mathbb R^p$ . This can be generalized by instead modeling the probabilities as $\theta_i = \sigma(f(x_i))$ for some function $f$ . We could keep this a logistic regression by taking $f(x)=x^Tw$ for a parameter vector $w$ , or we could be more flexible by doing something like taking $f$ to be a spline. $f$ itself could be parameterized and we could put a prior on these, like if $f(x)=x^Tw$ we could put a Gaussian prior on $w$ . By saying $f(x)=w^Tx$ I'm really saying that out of all possible $f$ s that I could use, I'm restricting myself to the finite-dimension space of linear functions. But we could also directly place a GP prior on $f$ and spare ourselves from needing to parameterize it. It wouldn't be $w$ that's drawn from $\mathcal N(\mathbf 0, K)$ ; it would be $(f(x_1), \dots, f(x_n))^T$ which are the values of the processes at the indices in question (and for GP regression we take the covariate space to be the index set). Depending on our choice of kernel we may now be sampling from an infinite dimension function space. If we use a kernel like $k(x,x') = x^Tx'$ then we return to something equivalent to a Bayesian linear model, but in general there won't be a fixed- and finite-dimension parameter $w$ indexing all of the possible functions under consideration.
