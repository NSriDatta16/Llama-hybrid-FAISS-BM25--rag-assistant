[site]: crossvalidated
[post_id]: 637746
[parent_id]: 235118
[tags]: 
Motivation Suppose we have a (machine learning) model trying to classify a data point between two classes, the model assigns to a data point a score (usually this score is a number between 0 and 1). The score is a measure of how confident the model is that the data point belongs to a certain class (sometimes the score is a calibrated probability, although not always). For example, suppose you are trying to classify whether an email is spam or not spam. The model scores the first email as 0.9 and the next email as 0.4. The model is in some sense more confident that the first email is spam than the second. Having the raw scores is good, but sometimes in the real world we need to make a hard classification so we can take action off it, in this example will we block the email. We can set a threshold (or cutoff), and say that any score above this value will be marked as one class, and below it as the other. For example we might say anything above 0.7 we will mark as "spam", and anything below it as "not spam". Please take note that the threshold does not have to be 0.5, even though this is often the default value in some popular packages. Definitions When we set our threshold and classify anything above it as spam and below it as not spam, we will (probably) get some classifications correct and some incorrect. There are different things that can happen, we will call spam emails the positive class: We correctly mark a spam email as spam (TP - True Positive) We correctly mark a not spam email as not spam (TN - True Negative) We incorrectly mark a not spam email as spam (FP - False Positive) We incorrectly mark a spam email as not spam (FN - False Negative) Using these we can define the following two quantities: True Positive Rate (Also known as sensitivity or recall) $$\text{TPR} = \frac{\text{TP}}{\text{TP} + \text{FN}}$$ False Positive Rate (Also known as fall-out) $$\text{FPR} = \frac{\text{FP}}{\text{FP} + \text{TN}}$$ You could set your threshold in different places, doing this would give you different values for TP, TN, FP, FN and consequently different values for the TPR - true positive rate and FPR - false positive rate. For example if you set the threshold at 0 and so marked every email as spam, then you would have correctly marked every positive as a positive, so there are no false negatives, and so the TPR = 1. On the other hand if you set the threshold at 1 and so marked every email as not spam, then you would have no false positives, and FPR=0. So depending where you set your threshold, you will have different values for the true positive rate and false positive rate. Now, imagine if you try every possible threshold, and for each threshold value you calculate the false positive rate and true positive rate, and plot this curve with the x-axis as the false positive rate and the y-axis as the true positive rate - this is the ROC - curve which you have in the diagram in the original post. Why is it useful? This was not part of the original question, but it is worth briefly mentioning. The a rea u nder the ROC c urve, (often ROC-AUC or AUCROC) tells us something about how 'good' the classifier is - in particular it tells us something about the discriminative ability of the classifier. A ROC-AUC of 1 means the model has perfect discriminative ability. A ROC-AUC of 0.5 means the model is performing the same as random guessing between the two classes. The performance of this classifier would be a diagonal line from bottom left to top right of the ROC space, you can see this on the diagram in the original post. A ROC-AUC below 0.5 indicates the model is performing worse than random chance, consistently making incorrect predictions. In this case it may be worth checking if the class labels are being flipped. There is a probabilistic interpretation for the ROC-AUC, if you randomly select an observation from both classes (spam - positive class, not spam - negative class), then the ROC-AUC is the probability that the model scores the positive observation higher than the score for the negative observation. Given thresholds were mentioned in the original post, it is worth noting that the ROC-AUC gives us a measure of the discriminative ability of a model which is independent of the threshold that is chosen . Further Reading Google ML notes on ROC
