[site]: crossvalidated
[post_id]: 48808
[parent_id]: 48752
[tags]: 
With such small numbers of "sets" within each group, it is possible to generate the full permutation distribution. The following account focuses on the statistical ideas and techniques, using R to illustrate with simulated data like those described in the question. The number of samples of size $k$ (such as $k=5$ or $k=6$) with replacement from a set of $n$ items (such as $n=11$) equals $\binom{n+k-1}{k}$. It is useful to know why that is, because that knowledge helps with the coding. It is based on a one-to-one correspondence between $k$-subsets of $n+k-1$ things and $k$-multisubsets of $n$ things. (A "multisubset" can have repeated elements.) By indexing these things $1, 2, \ldots, n+k-1$, we can represent a subset by an ordered list of $k$ distinct indexes $$1 \le i_1 \lt i_2 \lt \cdots \lt i_k \le n+k-1.$$ Now subtract $0$ from the first index, $1$ from the second, etc., producing the sequence $$1 \le j_1 = i_1-0 \le j_2 = i_2-1 \le \cdots \le j_k = i_k - (k-1) \le n.$$ Because the $i_s$ can be recovered as $i_s = j_s + s - 1$, this is a one-to-one correspondence. But the second set of indexes, which range only from $1$ to $n$, allows replicates and therefore enumerates all samples of $n$ things with replacement, QED . From this we can compute that there are $\binom{11+6-1}{6} = 8008$ ways to simulate the first group and $\binom{11+5-1}{5} = 3003$ ways to simulate the second group. Depending on the test statistic, there are two ways to proceed. The most efficient is where the statistic depends separately on the values in the first group and the values in the second. A simple example would be the difference between the group means. In such cases we can obtain the full permutation (actually, bootstrap) distributions $F_1$ and $F_2$ of means for groups $1$ and $2$, respectively, by doing just $N=3003+8008$ calculations. By computing the convolution of $F_1(x)$ and $F_2(-x)$--which can be done in just $O(N)$ operations--we can obtain directly the permutation distribution of the difference of group means. So, for a computational cost equivalent to a few tens of thousands of random samples, we can obtain the distribution exactly with no sampling error at all. Less efficient is when the statistic depends intimately on the data in both groups. Then we might have to perform $3003 \times 8008$ calculations. However, because these can be very fast, the prospect is still not daunting. The following R code illustrates this method. To illustrate, let's begin by simulating data like those in the question. To make it interesting, I will draw medians in each "set" from a standard normal distribution, so that in reality there is no intrinsic difference between the two groups in this regard. However, I will make the four replicates within each set lognormally distributed and will make the variability greater in the first group. (This actually makes the expected values in the first group greater than those in the second.) These conditions of highly skewed distributions (within sets) and heteroscedasticity call into question the validity of conventional parametric analyses, such as ANOVA: that's why a permutation test can be useful. # Simulation # n1 The data x are arranged by rows, one row per set. The first n1 rows contain data for group 1 and the next n2 rows contain data for group 2. Note that these boxplots are on a logarithmic scale. The groups are distinguished by color: blue for group 1, red for group 2. The heteroscedasticity is evident in the consistently shorter red boxes. The red boxes tend to be higher than the blue ones, but this is an accident due to chance alone. To proceed, we need to choose a test statistic. A simple one that should work well is a robust non-parametric analog of group grand means: obtain the median within each set, then return the median of those medians to characterize the general tendency of a group. To compare the two groups, we will subtract the group 2 median from the group 1 median. If that is close to $0$, the groups look similar. statistic (Note that this is inefficient: in this particular example, we would be better off computing all $11$ set medians once and for all and applying a permutation test to them. In order to illustrate the general procedure, I don't do that.) For these data, the test statistic turns out to be $-1.76$, reflecting the evident lowering of the blue boxes compared to the red ones. We will subject this to a significance test. Here is the code: permute This could be written more compactly without multisubsets , but it might be useful to see the code for obtaining all $k$-multisubsets of $n$ things, so I set it apart. As previously explained, it obtains all $k$-subsets of $n+k-1$ things (via combn ) and then adjusts the indexes. The result is an array whose columns contain the indexes of rows of x . The apply line then iterates over those columns, using each in turn to extract a sample of the rows of x and compute its test statistic (median of medians). Because apply and the sub-indexing that go on are not inherently fast, this is a fairly slow calculation: the next two lines take four seconds on my machine to simulate the full permutation distributions (having $8008$ and $3003$ elements, respectively). system.time(group1.sim Before we proceed, let's look at these results separately. I use histograms to show the two distributions. Remember, these are exact : they are computed without error. par(mfrow=c(1,2)) hist(group1.sim, probability=TRUE, breaks=25, col=col1) abline(v = group1.stat, lwd=2) hist(group2.sim, probability=TRUE, breaks=25, col=col2) abline(v = group2.stat, lwd=2) The vertical black lines mark the actual (observed) statistics: that of group 1 is fairly low and that of group 2 is fairly high. We proceed to compute the exact distribution of the bootstrap statistic--the difference between the group medians--using the brute force approach rather than the efficient $O(N)$ method. This is very efficiently carried out using a generalized outer product: par(mfrow=c(1,1)) system.time(simulation.all This goes quickly, taking only a quarter second. The vertical red line shows the observed test statistic of $-1.76$ previously noted. It looks a little low. Let's compute its p-value by averaging the upper and lower p-values (this accounts for ties in the data): p = sum(simulation.all = test.stat) / length(simulation.all) (p + 1 - q)/2 The value is $0.0452$: mild evidence of a "significant" difference between these groups. (It should be doubled, though, for a two-sided alternative hypothesis.) Because we simulated these data, we know there is no underlying difference in medians.
