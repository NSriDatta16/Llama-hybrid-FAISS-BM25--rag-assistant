[site]: datascience
[post_id]: 17300
[parent_id]: 17294
[tags]: 
The tokenization process shouldn't be changed even when you are interested in multi words. After all, the words are still the basic tokens. What you should do it to find a way to combine the proper words into term. A simple way to do so is to look for term in which the probability of the term is higher than that of the independent tokens. For example P("White house") > P("White")*P("House") Choosing the proper values of need lift, number of occurrences and term classification can be deduce if you have a dataset of terms form the domain. If you don't have such a domain then requirement at least 10 occurrences and and a lift of at least 2 (usually it is much higher since each token probability is low) will work quite well. In your case can can also extract terms by combining contexts relevant to your domain (e.g., "studied X", "practiced Y"). Again, you can build complex and elegant models for that but usually, looking for the few next words after the context indicators will be very beneficial.
