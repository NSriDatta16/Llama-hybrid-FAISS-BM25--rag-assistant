[site]: crossvalidated
[post_id]: 327611
[parent_id]: 
[tags]: 
How does out of bag error represent the overall performance of a random forest?

From my understanding, OOB error is computed as a weighted average of the errors from each individual learner. Each decision tree is trained on 2/3 of the data and the OOB error for each learner is computed as just the error on the remaining 1/3. But the overall OOB error for the random forest is the average of OOB errors from each individual learner. So if that is how OOB error is obtained, then how does it represent the overall ensemble and not just the individual learners if the random forest then combines those individual learners? I'm using OOBPredict and oobError in Matlab. I wish to create a random forest for classification but I need a metric to evaluate it, but I do not know how to interpret OOB error. Would it be better to evaluate the model by doing 10-fold cross validation while only feeding 9/10 of the data into the random forest, getting a prediction accuracy on the 1/10, and repeating? Or is OOB representative?
