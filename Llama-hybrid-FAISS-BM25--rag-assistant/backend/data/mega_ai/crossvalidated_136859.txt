[site]: crossvalidated
[post_id]: 136859
[parent_id]: 
[tags]: 
Average hazard rate with Cox proportional hazard model always larger than the hazard rate without any explanatory variables?

I am estimating a Cox proportional hazard model with and without explanatory variables. Without explanatory variables, the hazard rate is just the proportion of all individuals that failed at time $t$ out of all individual that lasted at least time $t$. After estimating the model with covariates, I calculate the predicted hazard for each observation in the sample used to fit the model by multiplying the baseline hazard by $\exp(z_i'\beta)$. I then average the predicted baseline hazard for each unit of time across all individuals that have not yet failed at that time. My problem is that the averaged hazard rate is always larger than the hazard rate from the model without any explanatory variables. The explanatory variables do not vary over time and the sample used to estimate both models is the exact same. See the figure below as an example: My questions are: 1) Is this possible or am I doing something wrong? I do not understand how the average hazard rate across all individuals is larger than the hazard rate from the no variables model at each point in time. 2) What should the relationship between the averages of the two hazard estimates be? Should it be: $\frac{\sum_t \sum_i \text{Hazard Rate w/ explanatory variables for observation i at time t}}{\sum_t \text{Number of Observations at time t}} = \frac{\sum_t \text{Hazard rate without explanatory variables at time t}\times \text{Number of observations at time t}}{\sum_t\text{Number of observations at time t}}$
