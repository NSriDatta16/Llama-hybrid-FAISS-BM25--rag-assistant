[site]: crossvalidated
[post_id]: 111517
[parent_id]: 79122
[tags]: 
Without further context, such a claim seems a bit too extreme to me. Standard SVMs are no deep architectures, they are flat architectures which perform a clever pattern comparison. What I mean is that the decision function for SVMs is, $$ f(x) = \text{sign}\left[\sum_{n}\alpha_{n}K(x_{n},x)-b\right] $$ so you can see it as a voting, where $K(x_{i},x)$ is a measure of the similarity, and $\alpha_{i}$ is the weight of each vote. Still, these "simple" algorithm has achieved great performance in many tasks in machine vision and natural language processing. What is the difference? Convolutional neural networks, for example, can be thought as built of two blocks: a first one which learns a good representation of the input data in the form of features which are invariant to scaling, rotations and so on, and a second layer which learns to classify the objects based on those features. The SVMs do not learn any features. They compare samples. Following this idea there is the paper " Large-scale Learning with SVM and Convolutional Nets for Generic Object Categorization ", where the features learned by a convolutional network are used to train a SVM, achieving great results. Better than the convolutional network on that task, which speaks for the ability of the SVM as a discriminative classifier. Another issue are structured SVMs (in the setting of structured learning) where they are competitive which deep networks (which are also able to solve such tasks).
