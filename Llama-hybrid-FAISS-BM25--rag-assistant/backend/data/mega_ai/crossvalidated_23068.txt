[site]: crossvalidated
[post_id]: 23068
[parent_id]: 
[tags]: 
Steps to figure out a posterior distribution when it might be simple enough to have an analytic form?

This was also asked at Computational Science. I am trying to compute a Bayesian estimate of some coefficients for an autoregression, with 11 data samples: $$ Y_{i} = \mu + \alpha\cdot{}Y_{i-1} + \epsilon_{i} $$ where $\epsilon_{i}$ is Gaussian with mean 0 and variance $\sigma_{e}^{2}$ The prior distribution on the vector $(\mu, \alpha)^{t}$ is Gaussian with mean $(0,0)$ and a diagonal covariance matrix with diagonal entries equal to $\sigma_{p}^{2}$. Based on the autoregression formula, this means that the distribution of the data points (the $Y_{i}$) are normal with mean $\mu + \alpha\cdot{}Y_{i-1}$ and variance $\sigma_{e}^{2}$. Thus, the density for all of the data points $(Y)$ jointly (assuming independence, which is fine for the program I am writing), would be: $$ p(Y \quad | (\mu, \alpha)^{t}) = \prod_{i=2}^{11}\frac{1}{\sqrt{2\pi\sigma_{e}^{2}}}\exp{\frac{-(Y_{i} - \mu - \alpha\cdot{}Y_{i-1})^{2}}{2\sigma_{e}^{2}}}.$$ By Bayes' theorem, we can take the product of the above density with the prior density, and then we'll just need the normalizing constant. My hunch is that this should work out to be a Gaussian distribution, so we can worry about the normalizing constant at the end rather than explicitly calculating it with integrals over $\mu$ and $\alpha$. This is the part I am having trouble with. How do I compute the multiplication of the prior density (which is multivariate) and this product of univariate data densities? The posterior needs to be purely a density of $\mu$ and $\alpha$, but I cannot see how you'll get that out of such a product. Any pointers are really helpful, even if you just point me in the right direction and then I need to go and do the messy algebra (which is what I've already attempted several times). As a starting point, here is the form of the numerator from Bayes' rule: $$ \frac{1}{(2\pi\sigma_{e}^{2})^{5}\cdot{}2\pi\sigma_{p}^{2}} \exp{\biggl [ \frac{1}{2\sigma_{e}^{2}}\sum_{i=2}^{11}(Y_{i} - \mu - \alpha\cdot{}Y_{i-1})^{2} - \frac{\mu^{2}}{2\sigma_{p}^{2}} - \frac{\alpha^{2}}{2\sigma_{p}^{2}} \biggr ] }.$$ The issue is how to see that this reduces down to a Gaussian density of $(\mu, \alpha)^{t}$. Added Ultimately, this boils down to the following general problem. If you are given some quadratic expression such as $$A\mu^{2} + B\mu\alpha + C\alpha^{2} + J\mu + K\alpha + L$$ how do you put that into a quadratic form $(\mu-\hat{\mu},\alpha-\hat{\alpha})Q(\mu-\hat{\mu},\alpha-\hat{\alpha})^{t}$ for some 2x2 matrix $Q$? It's simple enough in easy cases, but what process do you use to obtain the mean estimates, $\hat{\mu}$ and $\hat{\alpha}$? Note, I tried the straightforward option of expanding the matrix formula and then trying to equate coefficients as above. The problem, in my case, is that the constant $L$ is zero, and then I end up getting three equations in two unknowns, so it is underdetermined to just match coefficients (even if I assume a symmetric quadratic form matrix).
