[site]: crossvalidated
[post_id]: 444096
[parent_id]: 
[tags]: 
Neural networks: cross validation, how to accurately do experiments

I am writing a paper using neural networks for binary segmentation. I have made my own dataset for this, consisting of about 350 images. I want to run and report results on different experiments, but unsure of the best way to approach it. In many papers I have read, authors simply state their metrics without ever mentioning if they were calculated based on folds or not. As several popular datasets feature a test set, these papers usually report their metrics on this test set. As far as I'm aware, most seem to imply they just trained on the given training set in some unmentioned way (either train/validation, k-folding or some other way) and then probably retrained on full training set, or kept best fold, or made an average and calculate metrics on the test set. Why is not k-fold average preferred? Is it just because it is an easy way of being able to compare yourself against other papers? As for my experiments, I want to: Compare different loss functions, leaving the rest of the parameters the default ones Compare different set of data augmentations, with loss function chosen in the first step Compare training on just synthetic data vs my real data, with loss function chosen in first step Compare different batch size, which will lead to different image resizing, with loss function chosen in first step Finally choose the best set of data augmentation, batch size and train my model. Then compare it against 2 other segmentation models from other people (which I have the trained model but not their dataset or training code). Should I run k-fold to decide best thing on each experiment and report the average? Or should I only choose best according to that average but additionally report metrics based on a small test set of about 40 images which weren't used in the k-fold? Is there any case where I should do nested k-fold instead? Or is this only necessary when you try to optimize multiple parameters on a single experiment?
