[site]: datascience
[post_id]: 126575
[parent_id]: 
[tags]: 
Random Forest Classifier Removing Features using Top-N Features Method

I am a new-comer to data science and machine learning techniques and processes. I'm working on a personal project that predicts the winner of an NBA game using a random forest classifier. I have sought to remove and modify my list of features so that I can increase accuracy and decrease noise. I implemented the solution found here: Decision Trees Should We Discard Low Importance Features? , where I would loop through the top N most important features and plot out the resulting accuracy. After all my features have gone through that loop, I'm left with a plot that looks like this: As you can see, the resulting graph is kind of all over the place. Do I remove the features that have a negative slope? Or what's the threshold to removing features? Is there a better way to calculate noise? How would I get the most accurate model given that I have so many features with such a variable impact on my model accuracy on training data?
