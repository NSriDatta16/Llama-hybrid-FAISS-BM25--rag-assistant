[site]: datascience
[post_id]: 26845
[parent_id]: 
[tags]: 
What's the proper Word2vec model to get pre-trained word embedding for a classification task?

I have to use a neural network to classify whether some reviews of hotels are deceptive or truthful. I also have to use pre-trained word embeddings to fed the neural networks. So I can use Word2vec to get the word vectors from a way larger dataset of hotels reviews. However, Word2vec gives the possibility to use continuous bag-of-words and continuous skip-gram models for this task. Which one would be generally better for this specific task?
