[site]: crossvalidated
[post_id]: 113864
[parent_id]: 46780
[tags]: 
Finding an optimal features set can be quite computationally expensive. The main categories of available solutions can be grouped in two sets: either bind to a specific classifier (Wrappers) or simple ranking of features based on some criterion (Filter methods). Based on your requirements (quick/non-parametric/non-linear) probably you need candidates from the Filter methods. There are quite a few examples of those described in literature . For example Information Gain - that evaluates the worth of an attribute by measuring the information gain with respect to the class; or Correlation that evaluates the worth of an attribute based on the correlation between the attribute and the class. The wrapper methods are bind to a classifier and may end up to a better set of features for the classifier of interest. Due to their nature (full training/testing in each iteration) they can not considered quick or non-parametric, however they can deal with non-linear relations of features (your 3rd requirement). An example would be Recursive Feature Elimination that is based on SVMs, thus targets on maximising the margin between the classes and can deal with non-linear relations of features (using a non-linear kernel).
