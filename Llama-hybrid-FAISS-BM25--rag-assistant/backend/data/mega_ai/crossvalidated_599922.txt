[site]: crossvalidated
[post_id]: 599922
[parent_id]: 
[tags]: 
Prove that the leverage converges to 0. Do the residuals in a linear regression really approximate the errors?

In linear regression, $y_i = x_i^T \beta + \epsilon_i$ , $i=1,\dots,n$ , and $Var(\epsilon_i)=\sigma^2$ . It is well known that the residuals $e_i$ have variance $Var(e_i) = \sigma^2 (1-h_{ii})$ , where $h_{ii}$ is the leverage of observation $i$ , and that is not equal to $\sigma^2$ . It is commonly believed that the leverage goes to 0, and the residuals $e_i$ approximate the errors $\epsilon_i$ . But how can this be justified? Clearly, it is necessary that the leverage converges to 0 in order for the residuals to approximate the errors, so that they have the same variance asymptotically. But I can find no reference that proves: for all $i=1,\dots,n$ , $h_{ii}\to0$ as $n\to\infty$ . It is known that the average of the leverage is $p/n$ , where $p$ is the number of parameters, and $h_{ii}>0$ . But the mean of a sequence of positive numbers converging to 0, does not imply the elements of the sequence converge to 0. In fact, $h_{ii}$ depends on $x_i$ , so it would seem any proof would need to place some assumption on how $x_i$ , $i=1,\dots,n$ , grows for large $n$ . How does one prove that leverage converges to 0? Is there a reference?
