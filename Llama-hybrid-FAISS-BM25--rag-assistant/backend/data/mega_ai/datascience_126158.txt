[site]: datascience
[post_id]: 126158
[parent_id]: 
[tags]: 
How to think when designing deep learning architectures?

My question is about people studying deep learning. There is a point that bothers me and prevents me from working and understanding the articles. Especially in articles containing CNN-based deep learning architecture, I am very confused about how they design each layer, the functions of the layers and why they are preferred. Let's consider a simple image as input. I'm reading statements like this. "First, we downsample by applying 1-D convolution. Then we apply an LSTM in between. Then we apply attention. Of course, there are also skip connections." I know what each term is, but how do you figure out which is needed where when creating architecture? The articles say, we are using X here. And then the results are amazing! How that happens? Are they doing a million experiments? How does such a mathematics-based mechanism proceed so empirically? I can never write a CNN when I'm alone because I can't visualize what I should use on which layer. Is this normal? Is the field of deep learning really so purely empirical? What are the methods/ideas you use when designing deep learning architecture?
