[site]: crossvalidated
[post_id]: 260394
[parent_id]: 260391
[tags]: 
The question may be too broad to answer. It is hard to guess another person's perspective. But I think the question is interesting, and I would like to try to answer. The term "machine learning" can have many definitions. I believe The popular ones are Convex optimization (there are tons of papers on NIPS for this topic) "Statistics minus any checking of models and assumptions" by Brian D. Ripley From optimization perspective, the ultimate goal is minimizing the "empirical loss" and try to win it on testing data set. Where we do not emphasize too much on the "statistical model" of the data. Some big black box discriminative model would be perfect examples, such as Gradient Boosting, Random Forest, and Neural Network. These types of work got popular because the way we collect data and process data has been changed. Where we can think we have infinite data and will never over-fit (for example number of images in Internet). All the computational model we can afford would under-fit super complicated data. The goal would be have an effective way to build the model faster and more complex (For example using GPU for deep learning) On the other hand, from statistical points (probabilistic approach) of view, we may emphasize more on generative models. For example, mixture of Gaussian Model, Bayesian Network, etc. The book by Murphy "machine learning a probabilistic perspective" may give you a better idea on this branch.
