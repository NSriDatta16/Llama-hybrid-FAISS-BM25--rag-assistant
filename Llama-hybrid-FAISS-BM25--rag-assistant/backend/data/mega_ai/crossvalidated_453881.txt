[site]: crossvalidated
[post_id]: 453881
[parent_id]: 378875
[tags]: 
It is a partial ordering, if you check back in the reference, just above your quote: ﬁnding a policy that achieves a lot of reward over the long run. For ﬁnite MDPs, we can precisely deﬁne an optimal policy in the following way. Value functions deﬁne a partial ordering over policies. So your two policy can't be ordered with respect to each other. so neither is better than the other, that does not prove that there can't be an optimal one. I think that the partial ordering may be all they needed to infer existence. There are threads here, I think, asking for details on that proof. I have not yet been able to read them. It just occurred to me, that you could construct a better policy from patching the two non-ordered ones. Axiom of choice (something like that): partition the action-state space according to what ails, and choose the best of the two policies in every such subset, adding quantifiers to cover A X S defined support for such mapping. Maybe then we are both more inclined to accept that optimal policy... Would that patched new mapping be a policy? I don't recall a policy definition, but if mapping from A X S to space of "conditional" probabilities is enough qualification, then you just have to worry about your partition sub-sets be measurable, I would think. The quotes on conditional were a remark that when probability notation becomes frequent, I have experienced that set notation tends to get fuzzy, makes the Bayesian juggling faster... (that was a joke).
