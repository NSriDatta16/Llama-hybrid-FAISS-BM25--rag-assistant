[site]: crossvalidated
[post_id]: 390218
[parent_id]: 390181
[tags]: 
The complete quote from the paper (which should have been included in the text of the question to make it self-contained) is Since entropy measures information and a lack of randomness (Shannon, 1948), the authors propose minimizing the entropy of the approximate posterior, $p_\text{ABC}(Î¸|s^\text{obs})$ , over subsets of the summary statistics, $s$ , as a proxy for determining maximal information about a parameter of interest. High entropy results from a diffuse posterior sample, whereas low entropy is obtained from a posterior which is more precise in nature. which does not mention the posterior being more diffuse than the prior. Now, as discussed in the comments, the posterior variance is on average smaller than the prior variance $$\text{var}(\theta)=\mathbb{E}[\text{var}(\theta|X)]+\text{var}(\mathbb{E}[\theta|X])$$ but it does not mean that $$\text{var}(\theta)\ge\text{var}(\theta|X=x)$$ for all realisations $x$ of $X$ , as exemplified in Robin's answer.
