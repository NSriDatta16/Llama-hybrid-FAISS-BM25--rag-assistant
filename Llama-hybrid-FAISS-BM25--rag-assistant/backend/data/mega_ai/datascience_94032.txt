[site]: datascience
[post_id]: 94032
[parent_id]: 94028
[tags]: 
If one permutes the connections of the hidden layer ( $d!$ ways to do that), and move and rename connections appropriately, then one effectively has the same MLP with the exact same minima, yet the configuration has changed (in a trivial sense). Thus there are (at least) $d!$ configurations only trivialy different with the exact same minima. To see it in your notation, effectively the hidden layer output is the following sum: $$O_{\text{hidden}} = \sum_{i=1}^d w_{\pi_i} \cdot x + b_{\pi_i}$$ Where $\pi_i$ is some order of the connections. For example $\pi_i = i$ But for $d$ items there are $d!$ permutations thus $d!$ order functions $\pi(i) = \pi_i$ . Yet the difference is only in re-ordering the configuration and is trivial. The rest follow from that. This is one reason why Neural Networks are non-convex models. See also: Explanation of why Neural Networks are non convex
