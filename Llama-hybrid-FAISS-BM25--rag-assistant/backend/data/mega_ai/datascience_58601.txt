[site]: datascience
[post_id]: 58601
[parent_id]: 36862
[tags]: 
I would like to suggest another dependency. Sometimes, predicting the large class is relatively easy. Meaning, every classifier you will try(with reasonable predicting power and one that matches your problem), will get high f1-score on the large class, but it is doing a poor job to predict the small class (f1-score). So when it is important for you to predict well the small class and predicting the big class is relatively easy, I'm suggesting to use only f1-score of the small class as main metric, or using Precision-Recall AUC(PR-AUC) as main metric. Here is an example from my research: This is a classification report I got in one of my classifiers. In my case, class 0 is 4 times larger than class 1. All the classifiers I played with gave me high f1-score on class 0 (above 0.9) but around 0.7 of F1-score on class 1. I'm interested in predicting well class 1, and I'm fine with suffering a small loss in predicting class 0. So, in my case, the main difference between the classifiers was reflected on how well they perform on f1-score of class 1, hence I considered f1-score of class 1 as my main evaluation metric. My secondary metric was PR-AUC, again, on class 1 predictions (as long as my classifiers keep performing pretty well on class 0, and they all did). Optimizing these metrics better reflected my needs than the averaged versions of f1 metric. I could consider macro avg F1, and it is a reliable metric in imbalanced case. However, that would suggest that my predicting power is 82% when I know that in my case, it's not the best metric that separate good classifiers and bad ones. So, just be real with your data and your task in hand. Understand that an evaluation metric is a tool for you to pick the best classifier that reflects your needs, it does not have to be the one with the best numbers.
