[site]: crossvalidated
[post_id]: 256374
[parent_id]: 256308
[tags]: 
The key thing to keep in mind about any neural net model (including all the deep learning varieties) is that the solution you get out of gradient descent is merely a local minimum. The loss function is non-convex. By running it multiple times and taking the average (instead of the best) you hope to get something better than a single solution. Some researchers in deep learning (e.g. Yann LeCun) have noted that this local minimum issue is probably not so problematic; they observed that typically a local minimum found in a single descent trajectory turns out to be pretty good.
