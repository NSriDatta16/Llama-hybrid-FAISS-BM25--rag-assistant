[site]: crossvalidated
[post_id]: 323896
[parent_id]: 
[tags]: 
Explanation for MSE formula for vector comparison with "Euclidean distance"?

I study the loss functions for regression tasks with artificial neural networks. In case of evaluating loss with Mean Squared Error for multidimensional outputs I read the following usual formula which is straightforward for me (N is the number of samples, M is the output dimensionality): However, I also confronted with a slightly different format: The accompanying explanation says: " Youâ€™ll recognize the inner sigma in the preceding equation as the square of the Euclidean distance. In fact, the MSE is sometimes referred to by these terms. Note that N and M are constants. So, consider these as simple scaling factors that you can account for in other ways (like by scaling the learning rate). In a lot of use cases M is dropped and a division by two is added for mathematical convenience (which will become clearer in the context of its gradient in backpropagation). " I cannot follow this explanation (and neither the second formula). What is the connection between the formula and the Euclidean distance? How does the fact that "N and M are constants" explain anything? How scaling factors come into the picture? What is the "mathematical convenience" here and how does it become clearer in backpropagation? (I'm more or less familiar with backprop).
