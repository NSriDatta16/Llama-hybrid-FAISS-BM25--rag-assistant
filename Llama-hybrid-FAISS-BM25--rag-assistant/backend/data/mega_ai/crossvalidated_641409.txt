[site]: crossvalidated
[post_id]: 641409
[parent_id]: 641290
[tags]: 
Requested from comments: Cross-validation is the step for selecting the model including setting its hyperparameters: it is not about training the final model. So it could include the decision tree rather than some other method, and then for example deciding the maximum depth of the tree (as too shallow a tree could cause underfitting and too deep a tree cause overfitting). Once you have decided the depth using cross-validation, you can train a tree to that depth using the whole training data, and that is your final model. The cross-validation aims to tell you how good that choice of hyperparameters is on what aims to be unseen data (averaged over the folds) and you want to optimise that. It is not perfect, partly because this optimisation can contaminate the process, which is why you may have held out a test set away from your training/cross-validation data to perform a final test on your final model (too late to make any changes).
