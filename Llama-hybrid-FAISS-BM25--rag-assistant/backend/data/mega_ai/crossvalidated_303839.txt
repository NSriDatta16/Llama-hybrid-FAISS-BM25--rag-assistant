[site]: crossvalidated
[post_id]: 303839
[parent_id]: 303836
[tags]: 
I usually hear it being referred to as "splitting your data up into, a training, test, and validation set". Google seems to be quite receptive to the phrase, "splitting data machine learning validation" which suggest that a common term for it is just splitting the data, here into three categories, as you suggested. There is, at least, a quite wide range of sources using this term. A related technique, with an actual name, is k-fold cross validation where the test and training set are initially combined, but later split up into k folds where one of them functions as the test set. After testing this, the test set is shifted to the next fold repeat k times. When you have k test scores they are usually averaged to get the final test score. Here it's less important to have a validation set as it's harder for the model to overfit.
