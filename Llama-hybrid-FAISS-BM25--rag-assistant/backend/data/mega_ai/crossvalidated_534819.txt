[site]: crossvalidated
[post_id]: 534819
[parent_id]: 534746
[tags]: 
1-3 : Many things are possible, from unsupervised machine learning to stats : you can make really simple things (put thresholds by your own, calculate point distance from neighbours once you normalized them, ...) or a bit more complicated (clustering and isolate less populated clusters for example, Isolation Forest algorithm). In unsupervised learning, chosing variables is way more subjective : you don't use metrics to show if your model is good / bad as you'd do in supervised (actually you can, but that's harder, I won't go into details). So that's a bit arbitrary : select variables you want to find the anomalies about, and try showing results and adapt since you like the result. 4 : Cross-Validation is a process (mainly used for Supervised) to evaluate your model. You have to understand one thing : when you create a model, you often separate 3 sets : Train, Test, Validation : Train is the set you use to train your model, and then you test on your Test. To make your model better, you adapt parameters and variables (which variables and how you present them) on train and test, to see how your metric (applied on how your model predicts your test) evolves. Once you have a correct model, you finally apply it on Validation. Validation is like a 2nd test set, that you didn't use to find the "best" model, so you're not biased. It gives you an overlook on how the model will perform on new, unknown data. Cross validation is used when you don't want to split data : if you allow 60% for train, 20% for test and 20% for val, you actually are only training your model on 60%, which can be a problem (in case you don't have a lot of data for example). So you use Cross Validation. Let's say you use 10 folds : Split your data in 2 sets : Train (80%) and Validation (20%). Then, to tune your model and find the best for you, you'll only use your Train set like that : Cross Validation will cut it in 10, and use 9/10 as train and 1/10 as test. It then does that 10 times, changing each time the 1/10 used for test. After that, make the mean of your 10 results for your metric. Like that, you Trained and Tested your model on all your 80%, without splitting it again. Once you made the perfect model for you, test on your unused Validation to have an idea how it performs in "real world"
