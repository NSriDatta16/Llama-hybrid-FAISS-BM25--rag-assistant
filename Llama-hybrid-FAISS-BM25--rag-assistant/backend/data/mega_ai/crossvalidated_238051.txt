[site]: crossvalidated
[post_id]: 238051
[parent_id]: 238045
[tags]: 
First, you might try filtering the 100 features down to a lower number as many of them may not be predictive of outcome(0,1). So maybe employ a chi-squared test of two proportions ($p_1$ for the proportion of ones in the output and $p_2$ for the proportion of ones in each feature). Thus, you will have 100 chi-squared tests. Then, only use features whose p-values are not significant, since you want $p_2$ to be similar to $p_1$, not significantly different. In spite of using dummy indicator variables in regression to acquire mean change of $y$ for a one-unit change in $x$, artificial neural networks (ANNs) don't always work well with purely binary or Boolean data, since there are a lot of partial derivatives of network error w.r.t to weight training from hidden layer outputs (output-side) and between network error and input-side coefficients. Depending on the output-side transformation being used (softmax, linear) and activation functions (tanh, logistic, linear, RBF) many ANNs expect input features with values in the range [-1,1]. So maybe try to rescale the input feature values of [0,1] to [-1,1], and see how the results compare. Certainly, don't simply throw all 100 features into an ANN, since there may be features that are not predictive of class. Such features will be useless and degrade the learning rate of the ANN. Also, try using the softmax function on the output-side, and either the linear or logistic activation function on the hidden layer (input-side). An ANN is like an engine: if the right combination of gas-air-spark is not used, it won't run. Filtering out bad features as a first step (whose values don't predict outcome singly, i.e. univariately), will be like increasing the octane of the fuel used.
