[site]: crossvalidated
[post_id]: 396918
[parent_id]: 
[tags]: 
Encoding Layers in the Transformer

In the transformer architecture for NLP, at each layer there are multiple self-attention filters. My question is about the encoded content within these filters. An example can be found here : My understanding is that this is an analogue of convolutional neural nets, where we are extracting higher-and-higher level features as we pass through each encoding layer. Initially we're looking for simple things like verbs, nouns, etc. Then we're looking for subject-object relations. Then relations between subjects, and if the phrases are longer, relations between phrases. Looking that these images however, I'm a bit lost about the actual need for so many attention filters at each layer. The nearly-diagonal attention maps represent dead filters (e.g. features that weren't detected). But, the last layer (6) is most telling: there is a vertical collapse of nearly all filters. Question: Why is this vertical collapse happening in the final encoder layer 6? Is it again some form of non-detected features? Why isn't the collapse diagonal?
