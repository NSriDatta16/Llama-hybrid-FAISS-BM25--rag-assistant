[site]: crossvalidated
[post_id]: 459357
[parent_id]: 459023
[tags]: 
The Wikipedia article you cite does not seem good or clear to me, so find some other source. Let us look first at the basics of Bayesian experimental design. We want to choose a design (maybe choose which observations to take from some experimental region, with regression-like models.) Denote the design by $\xi$ , the response variable by $y$ , with a distribution (density) governed by some unknown parameter $\theta$ , density is $p(y \mid \theta)=p_\xi(y \mid \theta)$ . The model function have subscript $\xi$ because choosing the design is choosing a distribution for $y$ . The prior distribution is $p(\theta)$ , without subscript because the prior information do not depend on the design. If the goal of the experiment is inference on $\theta$ , we need a criterion function reflecting that. If, after doing the experiment, the posterior distribution is equal to (or close to) the prior, we didn't learn much! So we want an experiment $\xi$ that we can expect to give data that surprises us and so changes our opinion on $\theta$ , see Statistical interpretation of Maximum Entropy Distribution . So a natural measure is the Kullback-Leibler divergence $$ \tag{*}\label{*} \newcommand{\KL}[2]{KL\left(#1 || #2\right)} \newcommand{\KLint}[3]{\int #1 \log\left(\frac{#1}{#2}\right)\;d #3} \KL{p_\xi(\theta | y)}{p(\theta)} = \KLint{p_\xi(\theta | y)}{p(\theta)}{\theta} $$ This can never be negative, and will be zero only if the posterior equals the prior, in which case we have learnt nothing, out state of knowledge is exactly as before doing the experiment. But, this cannot be used directly, it depends on $y$ , which is unknown when we are planning the experiment. So, we need its expectation, relative to the marginal distribution of $y$ , $m(y)$ . From Bayes theorem $p(\theta | y)= p(y | \theta)p(\theta) / m(y)$ we can write $$ m(y)=\frac{p(y | \theta)p(\theta)}{p(\theta | y)} $$ Calculating the expectation of $\eqref{*}$ over $m(y)$ : (leaving out subscript $\xi$ ) $$ \int m(y) \cdot \KLint{p_\xi(\theta | y)}{p(\theta)}{\theta} \; dy = \\ \int \int \frac{p(y | \theta) p(\theta)}{p(\theta | y)} \cdot p(\theta | y) \log\left( \frac{p(\theta | y)}{p(\theta)} \right) \; d\theta \; dy = \\ \int\int p(y,\theta) \log\left( \frac{p(\theta | y)}{p(\theta)} \right) \; d\theta \; dy = \\ \int\int p(y,\theta) \log p(\theta | y) \;d\theta\; dy - \int\int p(y,\theta) \log p(\theta)\;d\theta\;dy $$ where we can see that the last term do not depend on the design, so we can omit it and use the first term to define the utility function we want to maximize: $$ U(\xi) = \int\int p(y,\theta) \log p(\theta | y) \;d\theta\; dy $$ which we can see is the expected Shannon (or differential) entropy of the posterior. As this is calculated before observing $y$ , it is sometimes called a preposterior expectation. But for understanding it must be better to look at the earlier expressions. What is maximized here is emphatically not sharpness of my posterior ,it is the surprise value of the experiment!
