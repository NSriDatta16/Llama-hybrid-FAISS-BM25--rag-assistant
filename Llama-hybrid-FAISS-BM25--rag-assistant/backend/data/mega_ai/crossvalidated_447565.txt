[site]: crossvalidated
[post_id]: 447565
[parent_id]: 446975
[tags]: 
Thanks tddevlin for your answer, now these things became a bit clearer for me. But I want to add some thoughts about this topic, which are too long for comment. So I decided to write them here. I should note, that instead of using empirical risk we can also simply take the aforementioned "ideal" results ( Kevin Murphy in his book, section 5.7, calls them "Bayes estimators" because they minimize Bayesian posterior expected loss; and there is one subtle thing: Murphy writes that the bayesian estimation is done over some "action space" $\mathcal{A}$ , i.e. our hypothesis $f$ is a map $f: \mathcal{X} \to \mathcal{A}$ , but as tddevlin pointed out in the post above, we shoold choose $\mathcal{A} \equiv \mathcal{Y}$ , i.e. consider the entire function space ) and then just plug-in some estimation of conditional density into them. I mean algorithms such as Naive Bayes classifier. As one more example, Murphy in his book wrote about MMSE gaussian linear regression model: $\hat f(x) = \mathrm{E}[Y|X=x, \mathcal{D}] = x^T \mathrm{E}[w|D], ~ \forall x \in \mathcal{X}$ , where we plug-in the posterior mean parameter estimate for $w$ .
