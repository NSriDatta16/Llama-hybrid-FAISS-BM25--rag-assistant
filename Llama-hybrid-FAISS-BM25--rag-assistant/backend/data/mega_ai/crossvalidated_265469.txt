[site]: crossvalidated
[post_id]: 265469
[parent_id]: 95660
[tags]: 
In the case of high dimensional problems, linear SVMs tend to perform very well, like in the case of text classification (see for example the classic paper Text Categorization with Support Vector Machines: Learning with Many Relevant Features ). It is shown how in the case of a high dimensional, sparse problem with few irrelevant features, linear SVMs achieve great performance. Also, Geometrical and Statistical Properties of Systems of Linear Inequalities with Applications in Pattern Recognition shows how the higher the dimensionality, the more likely it is to find a separating hyperplane. Non-linear kernel machines tend to dominate when the number of dimensions is smaller. In general, non-linear SVMs will achieve better performance, but in the circumstances referred above, that difference might not be significant, and linear SVMs are much faster to train. Another interesting point to consider is correlation. Both, linear and non-linear are affected by highly correlated features (see this answer ).
