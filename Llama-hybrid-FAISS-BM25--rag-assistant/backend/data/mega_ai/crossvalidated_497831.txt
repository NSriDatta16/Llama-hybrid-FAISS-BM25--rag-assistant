[site]: crossvalidated
[post_id]: 497831
[parent_id]: 496756
[tags]: 
The question concerns how to prove this result. The necessary assumptions will become clear over the course of the analysis. Let's start with some intuition afforded by the simplest (non-trivial) case of regression, where the true values of $x$ take on just two possible values $\xi_0$ and $\xi_1$ and corresponding to those are the values $\eta_0$ and $\eta_1,$ respectively. The observed values of the response $y$ depart randomly from the $\eta_i.$ This situation is depicted by the points in the left hand scatterplot: The scatterplot depicts observations made with no error in $x$ (so all points line up above $x=\xi_0$ and $x=\xi_1$ ) and all the variation is the vertical ( $y$ ) direction. This is the usual linear regression model. The line shows the ordinary least squares (OLS) fit. It is unbiased: this means that on average the OLS line passes through the points $(\xi_0,\eta_0)$ and $(\xi_1,\eta_1)$ (shown as large solid dots). Measurement errors in $x$ displace those points randomly and horizontally, as shown in the middle picture. That results in the scatterplot at right. Randomly moving the $x$ values has changed the least squares fit. Why? Mainly because the slope is most strongly influenced by the points at the upper right (coming from positive errors in measuring $\xi_1$ ) and the points at the lower left (coming from negative errors in measuring $\xi_0$ ). The other points pretty much cancel each other out, since they are scattered randomly through the middle of the scatterplot. On average, the ordinary least squares estimate will have a slope that is smaller in magnitude: it is therefore is biased. A similar explanation applies more generally when there are more than two values underlying the $x_i.$ Proving this result is now straightforward: we ought to be able to see the same phenomenon in the algebra. Rather than going through this formally, let me sketch the underlying idea. Recall that the slope estimate is the average product of the standardized variables. When you include measurement errors in the first variable, that Increases the variance of the $x_i.$ Thus, on the whole, expect the standardized values of the $x_i$ to become smaller in size (because standardization divides all values by the square root of the variance). Does not change the variance of the $y_i$ (obviously). Does not introduce any more or less correlation, because the errors in the $x_i$ are assumed to be independent of those in the $y_i.$ (Apart from the usual linear regression assumptions, this is the only additional assumption.) Consequently, the average product of the standardized variables has a tendency to decrease (due to #1), QED.
