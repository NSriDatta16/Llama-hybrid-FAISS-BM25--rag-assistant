[site]: crossvalidated
[post_id]: 358138
[parent_id]: 61783
[tags]: 
Before discussing about bias and variance, the first question is: What is estimated by cross-validation? In our 2004 JMLR paper , we argue that, without any further assumption, $K$-fold cross-validation estimates the expected generalization error of a training algorithm producing models out of samples of size $n(K-1)/K$. Here, the expectation is with respect to training samples. With this view, changing $K$ means changing the estimated quantity: the comparison of bias and variance for different values of $K$ should then be treated with caution. That being said, we provide experimental results that show that variance may monotonically decreases with $K$, or that it may be minimal for an intermediate value. We conjecture that the first scenario should be encountered for stable algorithms (for the current data distribution), and the second one for unstable algorithms. my intuition tells me that in leave-one-out CV one should see relatively lower variance between models than in the $K$-fold CV, since we are only shifting one data point across folds and therefore the training sets between folds overlap substantially. This intuition would be correct if cross-validation was averaging independent estimates, but they can be highly correlated, and this correlation may increase with $K$. This increase is responsible for the overall increase of variance in the second scenario mentioned above. Intuitively, in that situation, leave-one-out CV may be blind to instabilities that exist, but may not be triggered by changing a siongle point in the training data, which makes it highly variable to the realization of the training set.
