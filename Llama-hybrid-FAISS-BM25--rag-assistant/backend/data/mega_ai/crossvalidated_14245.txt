[site]: crossvalidated
[post_id]: 14245
[parent_id]: 14219
[tags]: 
First, welcome! Text processing is lots of fun, and doing it in R is getting easier all the time. The short answer: yes - the tools in R are now quite good for dealing with this kind of data. In fact, there's nothing special about R, C++, Groovy, Scala, or any other language when it comes to data storage in RAM: every language stores an 8 byte double float in...wait for it...wait for it... 8 bytes! The algorithms and their implementation do matter, especially if implemented very poorly with regard to data structures and computational complexity. If you are implementing your own algorithms, just take care. If using other code, caveat emptor applies, as it does in any environment. For R, you will need to consider: Your data representation (look at sparse matrices, especially in the Matrix package) Data storage (perhaps memory mapped, using bigmemory or ff ; or distributed, using Hadoop) Your partitioning of data (how much can you fit in RAM is dependent on how much RAM you have) The last point is really under your control. When it comes to this dimensionality, it's not particularly big anymore. The # of observations will be more of an impact, but you can partition your data to adjust for RAM usage, so there's not really much to get too worried about.
