[site]: datascience
[post_id]: 87890
[parent_id]: 87882
[tags]: 
Is this approach a correct approach, or logical with respect to machine learning principles? It will affect the performance of the model in the sense that your algorithm learned to separate the clusters based upon distance according to all the features. I have read discussions about how to calculate feature importance on unsupervised problems like yours, so you could do some research on that and figure out a way to measure the importance of your features so that you have an idea of how influential is a feature on your model and therefore the impact on removing one. In this case removing will imply filling the features you are not using with NaNs so your model must be prepared for such scenario (sklearn pipelines are the best way of doing this) Will it affect the model accuracy and if yes then how? First of all, you are referring to an unsupervised model (K-means) so the metrics you mention does not apply, rather there must be metrics on the separateness of the clusters you formed (silhouette score, etc) and according to my first answers, you may adapt a version of Permutation importance using a metric according to your problem to see how it impacts to remove a feature the general performance. If I have to provide features B and C, then can I populate them with zeros and then provide it to trained model for making predictions Remember you are using an algorithm that is based on euclidian distance so imputing with zeros may have an undesired result Will the action taken in step 3) affect the model accuracy, if yes and then why and how? Sure it will, imputing with zeros will take the features not present to the origin on the euclidean space, so be careful with that
