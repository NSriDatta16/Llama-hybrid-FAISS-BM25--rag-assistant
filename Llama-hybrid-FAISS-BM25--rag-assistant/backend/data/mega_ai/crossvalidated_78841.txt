[site]: crossvalidated
[post_id]: 78841
[parent_id]: 78827
[tags]: 
Pragmatically speaking, if you have a choice over how you partition the data (i.e. there are no time constraints or such), I would say you would be better off shuffling your dataset so as to distribute the classes more or less evenly over all partitions. If this is not an option, I would simply drop any classes that you don't have any training data on: if the model has never seen a class, it cannot by any reasonable means make a valid prediction over this class. This is more of a data quality issue than a modelling issue. As to your second question, I have seen it being done in the literature for unbalanced datasets where some class is heavily underrepresented (causing the same problem of not having information in some cuts of your dataset). Uniformly redistributing instances per class over the full dataset being the solution.
