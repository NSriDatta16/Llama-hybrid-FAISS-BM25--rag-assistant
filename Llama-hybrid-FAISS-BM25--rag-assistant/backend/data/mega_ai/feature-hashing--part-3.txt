h finite support: ∀ ( x t ) t ∈ T ∈ R T {\displaystyle \forall (x_{t})_{t\in T}\in \mathbb {R} ^{T}} , only finitely many entries of ( x t ) t ∈ T {\displaystyle (x_{t})_{t\in T}} are nonzero. Define an inner product on R T {\displaystyle \mathbb {R} ^{T}} in the obvious way: ⟨ e t , e t ′ ⟩ = { 1 , if t = t ′ , 0 , else. ⟨ x , x ′ ⟩ = ∑ t , t ′ ∈ T x t x t ′ ⟨ e t , e t ′ ⟩ {\displaystyle \langle e_{t},e_{t'}\rangle ={\begin{cases}1,{\text{ if }}t=t',\\0,{\text{ else.}}\end{cases}}\quad \langle x,x'\rangle =\sum _{t,t'\in T}x_{t}x_{t'}\langle e_{t},e_{t'}\rangle } As a side note, if T {\displaystyle T} is infinite, then the inner product space R T {\displaystyle \mathbb {R} ^{T}} is not complete. Taking its completion would get us to a Hilbert space, which allows well-behaved infinite sums. Now we have an inner product space, with enough structure to describe the geometry of the feature hashing function ϕ : R T → R n {\displaystyle \phi :\mathbb {R} ^{T}\to \mathbb {R} ^{n}} . First, we can see why h {\displaystyle h} is called a "kernel hash": it allows us to define a kernel K : T × T → R {\displaystyle K:T\times T\to \mathbb {R} } by K ( t , t ′ ) = ⟨ e h ( t ) , e h ( t ′ ) ⟩ {\displaystyle K(t,t')=\langle e_{h(t)},e_{h(t')}\rangle } In the language of the "kernel trick", K {\displaystyle K} is the kernel generated by the "feature map" φ : T → R n , φ ( t ) = e h ( t ) {\displaystyle \varphi :T\to \mathbb {R} ^{n},\quad \varphi (t)=e_{h(t)}} Note that this is not the feature map we were using, which is ϕ ( t ) = ζ ( t ) e h ( t ) {\displaystyle \phi (t)=\zeta (t)e_{h(t)}} . In fact, we have been using another kernel K ζ : T × T → R {\displaystyle K_{\zeta }:T\times T\to \mathbb {R} } , defined by K ζ ( t , t ′ ) = ⟨ ζ ( t ) e h ( t ) , ζ ( t ′ ) e h ( t ′ ) ⟩ {\displaystyle K_{\zeta }(t,t')=\langle \zeta (t)e_{h(t)},\zeta (t')e_{h(t')}\rangle } The benefit of augmenting the kernel hash h {\displaystyle h} with the binary hash ζ {\displaystyle \zeta } is the following theorem, which states that ϕ {\displaystyle \phi } is an isometry "on average". The above statement and proof interprets the binary hash function ζ {\displaystyle \zeta } not as a deterministic function of type T → { − 1 , + 1 } {\displaystyle T\to \{-1,+1\}} , but as a random binary vector { − 1 , + 1 } T {\displaystyle \{-1,+1\}^{T}} with unbiased entries, meaning that P r ( ζ ( t ) = + 1 ) = P r ( ζ ( t ) = − 1 ) = 1 2 {\displaystyle Pr(\zeta (t)=+1)=Pr(\zeta (t)=-1)={\frac {1}{2}}} for any t ∈ T {\displaystyle t\in T} . This is a good intuitive picture, though not rigorous. For a rigorous statement and proof, see Pseudocode implementation Instead of maintaining a dictionary, a feature vectorizer that uses the hashing trick can build a vector of a pre-defined length by applying a hash function h to the features (e.g., words), then using the hash values directly as feature indices and updating the resulting vector at those indices. Here, we assume that feature actually means feature vector. Thus, if our feature vector is ["cat","dog","cat"] and hash function is h ( x f ) = 1 {\displaystyle h(x_{f})=1} if x f {\displaystyle x_{f}} is "cat" and 2 {\displaystyle 2} if x f {\displaystyle x_{f}} is "dog". Let us take the output feature vector dimension (N) to be 4. Then output x will be [0,2,1,0]. It has been suggested that a second, single-bit output hash function ξ be used to determine the sign of the update value, to counter the effect of hash collisions. If such a hash function is used, the algorithm becomes The above pseudocode actually converts each sample into a vector. An optimized version would instead only generate a stream of ( h , ζ ) {\displaystyle (h,\zeta )} pairs and let the learning and prediction algorithms consume such streams; a linear model can then be implemented as a single hash table representing the coefficient vector. Extensions and variations Learned feature hashing Feature hashing generally suffers from hash collision, which means that