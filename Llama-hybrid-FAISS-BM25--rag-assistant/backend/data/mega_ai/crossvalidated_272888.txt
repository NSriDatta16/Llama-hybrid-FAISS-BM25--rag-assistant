[site]: crossvalidated
[post_id]: 272888
[parent_id]: 272881
[tags]: 
For constraint 3: dummy code $x_2$ so that you can model all categories within one regression function. EDIT: I can think of two ways to enforce similarity between coefficients of those dummy variables but they are rather cumbersome and I do not know available implementations. If you want to go down that road, you could of course always program it yourself: a) You could penalize the difference between the coefficients in the objective: $\min \sum_{i}(y_{i} - \hat{y}_{i})^2 + \lambda |\beta_{2}-\beta_{3}| + \lambda |\beta_{3}-\beta_{4}| + ... $ where $\hat{y}_{i}$ is your estimate of $y_{i}$ and and $\beta_{2}$, $\beta_{3}$, ... are the coefficients of the dummies. This would require tuning of $\lambda$. b) You could put extra constraints such as $|\beta_{j}-\beta_{j+1}| \leq c$ where $c$ is your limit on the dissimilarity. You would have to determine $c$ for that. For constraint 2: use a logistic regression. For constraint 1: if there is truly a positive relationship between $x_1$ and $y$, then the regression should find a positive coefficient. I would not know the benefit of forcing negative regression coefficients to become positive if the data says the opposite.
