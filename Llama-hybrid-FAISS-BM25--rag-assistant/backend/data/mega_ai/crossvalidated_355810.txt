[site]: crossvalidated
[post_id]: 355810
[parent_id]: 355257
[tags]: 
But beside the normal tricks - is there something fundamentally wrong with my problem. Yes, I think there is something fundamentally wrong with your problem statement. From your description of the training data and the loss function I infer that you train the network to predict $k$. However, at the same time you somehow expect that the network generates $\lambda$ as output. Obviously, the network cannot do it. Also, note that according to your description the true $\lambda$ does not depend at all on $x$, thus no model in the world would be able to predict $\lambda$ observing only $x$. On the other hand, the $k$'s depend on $\lambda$ (and vice versa) and thus if one extends the training data set and includes $k$'s one could predict $\lambda$. However, in this situation NN would an overkill, because estimating $\lambda$ given $x$ and $k$'s is straightforward. Can the neural network learn the average sum squared of the input? Yes. Update Hence, my assumption is that $\lambda$ is the derivative value and a function of $x$. Whatever this sentence means, the second derivative of quadratic function is constant and doesn’t depend on $x$. Also, I am not trying to predict $k$. I have a model for it which is $\bar{k}$ Your loss function suggest that you do. Also note that it’s straightforward to estimate $\lambda$ if you have $x$ and $\bar{k}$.
