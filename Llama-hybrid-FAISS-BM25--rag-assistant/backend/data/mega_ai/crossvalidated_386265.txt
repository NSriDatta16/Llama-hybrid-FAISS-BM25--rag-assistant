[site]: crossvalidated
[post_id]: 386265
[parent_id]: 
[tags]: 
Why is my multi-layer (with identity activation) neural net converging petter than single layer perceptron?

Before I ask my actual question, I just want to verify that the following is correct: A multi-layer neural network with an identity activation function is equivalent to a single layer (also with identity activation function) neural network? This is mathematically obvious unless I am missing something. Moving on to my actual question: I generated some synthetic data for training. The independent variable is a 100 element array for each sample. The first sample is $$ S_0 = [0 \ \ 1\ \ 2\ \ 3\ \ \ldots\ \ 99] $$ There are 1500 training samples. Each training sample follows this relation $$ S_i = S_0 + i*100 \ \ \ \forall i = 1,\ldots,1499 $$ The training labels are simply: $$ \ell_i = i $$ I am using pytorch, with MSE as the objective function, and batch gradient descent as the optimization method. The 4 layer network contains 100, 500, 250, 1 node(s), respectively. The 1 layer network contains 1 node, the output node. The first plot below shows the MSE error vs. epoch # for 1 layer network. The second plot shows the MSE error vs. epoch # for the the 4 layer network. The y-axis is on a log-scale, and you can see that the 1 layer network converges to a value that is about 3-4 orders of magnitude larger than the 4 layer network. Shouldn't they be converging to the same value because of what is in italics at the top of this post? This result is very puzzling to me and I do not understand why this is the case.
