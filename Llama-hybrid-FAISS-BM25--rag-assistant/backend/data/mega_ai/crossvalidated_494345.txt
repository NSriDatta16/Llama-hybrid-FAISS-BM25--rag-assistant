[site]: crossvalidated
[post_id]: 494345
[parent_id]: 494322
[tags]: 
This is a great question and one for which there is no single answer, so I won't attempt to give one to be comprehensive. I'll mention a few topics that might satisfy some of your curiosity and point you to some interesting studies seeking to address the question you asked. The method you described of training a random forest and then producing predictions under the treatment and under control is a well-established and somewhat popular method called g-computation. The bootstrap is often used to estimate confidence intervals for effects estimated with g-computation. A recently popular method of g-computation uses Bayesian additive regression trees (BART) as the model; it has proven to be very successful and straightforward to use because it does not require parameter tuning. Inference is straightforward because it produces a Bayesian posterior from which credible intervals can be computed and interpreted as confidence intervals. There is a class of methods known as "doubly-robust" methods that involve estimating both an outcome model and a propensity score model and combining them. A benefit of these methods is that the estimate is consistent (i.e., unbiased in large samples) if either the propensity score model or outcome model is correct, and often inference is straightforward with these methods. Examples of doubly-robust methods include augmented inverse probability weighting (AIPW), targeted minimum loss-based estimation (TMLE), g-computation in propensity score-matched samples, and BART with the propensity score as an additional covariate. These methods are gaining popularity and are widely discussed in the statistics literature. They combine the best of both outcome modeling and treatment modeling. That said, many researchers prefer to only use matching and other treatment model-focused methods like weighting. I'll provide a short list of some of the primary motivations I've seen: Matching methods can be more robust to model misspecification, making their estimates more trustworthy Matching and weighting involve the assessment and reporting of covariate balance, which provides evidence to the reader that the method has plausibly reduced all the bias due to the measured covariates (this cannot be done with outcome regression) With matching and weighting, one can try many different methods without estimating a treatment effect to find the one that is going to be the most trustworthy. With outcome modeling, you only get one chance, or else you succumb to capitalization on chance and the potential to try many models until the desired effect is found Matching methods are easier to understand and explain to lay audiences Matching and weighting are agnostic to the outcome type, so they can be used in larger models or for outcome types for which g-computation is less straightforward, like survival outcomes Matching and weighting methods are sometimes found to be less biased than g-computation in simulations Matching and weighting are more transparent and customizable; it is easier to incorporate substantive expertise into the way certain variables are prioritized Matching and weighting do not involve extrapolation beyond the region of common support Hopefully that list gets you started in trying to understand this choice. Unfortunately the question of "should I use matching or g-computation for my data?" is basically equivalent to "what is the correct model for my data?" which is an eternal mystery. The "correct" answer for any given dataset is unknown, and some methods may be better suited for different kinds of datasets based on qualities that are unobservable. To specifically address your hypotheses: Yes, sometimes, though combinations of both tend to do best. Yes-ish; bootstrapping is often used, but not necessarily always valid. For some methods, we can use Bayesian to help. G-computation is not too hard to implement nonparametrically but it often has to be manually programmed. Same as 2). Absolutely yes. Just because a method is flexible doesn't mean it will always get the answer right. There is an inherent bias-variance tradeoff that must be managed with all methods. BART tends to do better than other machine learning methods because of how it balances flexibility with precision. Not really; we know a lot about how to use them, but we know a lot about how they can be improved and using doubly robust methods in many cases dramatically improves their performance.
