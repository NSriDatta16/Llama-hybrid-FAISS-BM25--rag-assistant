[site]: datascience
[post_id]: 15415
[parent_id]: 
[tags]: 
What is the reward function in the 10 armed test bed?

The Sutton & Barto book on reinforcement learning mentions the 10 armed test bed in chapter 2, Bandit Problems: To roughly assess the relative effectiveness of the greedy and ε-greedy methods, we compared them numerically on a suite of test problems. This was a set of 2000 randomly generated n-armed bandit tasks with n = 10. For each bandit, the action values, $q_∗(a), a = 1, . . . , 10,$ were selected according to a normal (Gaussian) distribution with mean 0 and variance 1. On t th time step with a given bandit, the actual reward $R_t$ was the $q_∗(A_t)$ for the bandit (where $A_t$ was the action selected) plus a normally distributed noise term that was mean 0 and variance 1 [. . . .] We call this suite of test tasks the 10-armed testbed. What is the reward function in the 10 armed test bed ? I interpreted it as something like q*(a) + some normally random value. where q*(a) is the true value of action a. Why is the reward function chosen this way in the test bed and how does the reward function affect the value estimations and the graphs ? I guess the reason i'm asking this question is because i'm not completely clear how a reward looks like in the real world where i don't know anything about q*(a).
