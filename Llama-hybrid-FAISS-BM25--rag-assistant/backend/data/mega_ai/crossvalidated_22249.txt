[site]: crossvalidated
[post_id]: 22249
[parent_id]: 15449
[tags]: 
There are two basic components, technical analysis (grammar, sentence length, etc) and machine learning/statistical analysis. The best (IMHO) papers on this are those involving automated essay grading. It's a little dated, but this paper covers the techniques employed by all of the major vendors. ETS (the people behind the TOEFL and the GRE) have put a LOT of work into this area, and they are surprisingly open about their research. The wall they run up against is that of length and specificity. They can only grade papers (with a high degree of accuracy) that are short and have specific prompts. The biggest problem I see is that ALL of these grading facilities are based on machine learning. Even ETS uses their graders as backups for human graders; they pay 1 human to grade and if their grade doesn't match the ML grade, the essay is handed to another human grader. It essentially cuts their costs in half but it doesn't replace human graders. If you really want to try analysis on ad-hoc text you will probably need to use the API of someone that has access to a lot of unstructured text, like OpenCalais or AlchemyAPI or even Zamenta for the categorizing and then roll your own quality analysis. However, that's a bit out of my pay range. update Jonny's answer is really not getting enough credit. If you are really concerned about content quality, analyzing network connections and other measurements of human behavior are REALLY valuable. Again, I am no expert in this field! It would help if you could tell us what our accuracy/validity requirements are.
