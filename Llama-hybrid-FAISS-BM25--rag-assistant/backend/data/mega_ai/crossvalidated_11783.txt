[site]: crossvalidated
[post_id]: 11783
[parent_id]: 11609
[tags]: 
I'll throw in my two cents (maybe redigesting some of the former answers). To a frequentist, the confidence interval itself is in essence a two-dimensional random variable: if you would redo the experiment a gazillion times, the confidence interval you would estimate (i.e.: calculate from your newly found data each time) would differ each time. As such, the two boundaries of the interval are random variables. A 95% CI, then, means nothing more than the assurance (given all your assumptions leading to this CI are correct) that this set of random variables will contain the true value (a very frequentist expression) in 95% of the cases. You can easily calculate the confidence interval for the mean of 100 draws from a standard normal distribution. Then, if you draw 10000 times 100 values from that standard normal distribution, and each time calculate the confidence interval for the mean, you will indeed see that 0 is in there about 9500 times. The fact that you have created a confidence interval just once (from your actual data) does indeed reduce the probability of the true value being in that interval to either 0 or 1, but it doesn't change the probability of the confidence interval as a random variable to contain the true value. So, bottom line: the probability of any (i.e. on average) 95% confidence interval containing the true value (95%) doesn't change, and neither does the probability of a particular interval (CI or whatever) for containing the true value (0 or 1). The probability of the interval the computer knows but you don't is actually 0 or 1 (because it is a particular interval), but since you don't know it (and, in a frequentist fashion, are unable to recalculate this same interval infinitely many times again from the same data), all you have to go for is the probability of any interval.
