[site]: crossvalidated
[post_id]: 575812
[parent_id]: 575642
[tags]: 
The only thing wrong with this is your conclusion "we would have to predict an outcome of 0 for all cases". Logistic regression is not a classifier; it is a regression model that produces predicted probabilities. The predicted probabilities for several values of your predictor are close to .5, and that is borne out in the fact that at those levels of the predictor, there are approximately equal 0s and 1s in the outcome. It is the case that, if you create a classifier by adding a decision rule that any probability less than .5 leads you to classify the case as a 0, you should predict all 0s for this dataset, but that's exactly the problem with using such a strict decision rule with no regard for balancing false positives and false negatives. Also, there is nothing inherently wrong with the shape of the curve. A nearly flat curve in the domain of the variable does indeed mean the predictor is not a great predictor of the outcome, but that doesn't mean the logistic curve is not the right model to use. You can always try other models and compare them on some metric; if you think there is a pattern that the logistic model is missing, you can try a machine learning model that more flexibly adapts to nonlinear relationships. That said, nothing about this problem has to do with the restriction of the range of the predictor. The range simply is what it is; it is not restricted in any sense.
