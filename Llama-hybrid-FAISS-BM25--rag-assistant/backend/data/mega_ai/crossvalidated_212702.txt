[site]: crossvalidated
[post_id]: 212702
[parent_id]: 212198
[tags]: 
Performance usually drops with batch_size=1 if you supply your batch with python which usually 100+ slower than c code and preparing you batch usually done with batch operations (numpy) 2.From result standpoint the more batch size it is the closer it to real gradient (rather than stochastic) which can be desirable and faster coverage, but in neral networks usually a lot of local minimums so you want that randomness in gradient to get away from them. Thats why Limited-memory BFGS usually not work for NN.
