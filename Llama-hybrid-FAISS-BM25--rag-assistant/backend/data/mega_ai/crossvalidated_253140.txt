[site]: crossvalidated
[post_id]: 253140
[parent_id]: 253139
[tags]: 
I know the feeling you have. Just a couple of weeks ago I had problems to grasp the idea of FWL. What helped me a lot was using $Z$ (instead of $X_2$) and $X$ (instead of $X_1$), so I'll try to explain it in that fashion. Starting with step 1, regressing $X$ on $Z$ means actually regressing each column of $X$ on the whole matrix $Z$, i.e. one considers $$ X = Z\Gamma + V. $$ Note that $\Gamma$ and $V$ are matrices (hence the capital letters) since the dependent variable is a matrix; cp. https://en.wikipedia.org/wiki/General_matrix_notation_of_a_VAR(p) (eventhough this is related to time series analysis but the notation is the same). The OLSE of $\Gamma$, $\hat\Gamma$, is given by $$\hat\Gamma = (Z'Z)^{-1}Z'X.$$ Why? Well, you can either check the source given in the link above (LÃ¼tkepohl) or try it on your own by treating $X$, $\Gamma$ and $V$ as vectors (eventhough they are matrices!) and perform OLS. You will get $\hat\Gamma$ as stated in the latter equation. Now calculate the residuals (which works again just as in the case with a single vector as dependent variable): $$\hat V = X - Z\hat\Gamma \\\quad\qquad\qquad= X - Z(Z'Z)^{-1}Z'X\\\quad\quad=(I-P_Z)X\\=M_ZX,$$ where $P_Z = Z(Z'Z)^{-1}Z'$ and $M_Z = I-P_Z$. The second step tells you to regress the actual dependent variable, $y$, on the residuals of the model calculated in step 1, i.e. $$y = \hat V\beta + u\\\qquad=M_ZX\beta + u.$$ Now you can already tell what the OLSE of this model is. It is $$\hat\beta = (\hat V'\hat V)^{-1}\hat V'y\\\qquad\qquad=(X'M_Z'M_ZX)^{-1}X'M_Z'y\\\quad\qquad=(X'M_ZX)^{-1}X'M_Zy$$ since $M_Z=M_Z'$ and $M_ZM_Z=M_Z$. And finished :-) Regarding your second question: You may know that the estimator $\hat\sigma^2=\hat u'\hat u = SSR$ is biased since $E(\hat\sigma^2) = \sigma^2(n-k)$. If you replace $\hat\sigma^2$ by $s^2 = SSR/(n-k)$ you will get $E(s^2) = \frac{\sigma^2}{n-k}(n-k) = \sigma^2$ (unbiasedness).
