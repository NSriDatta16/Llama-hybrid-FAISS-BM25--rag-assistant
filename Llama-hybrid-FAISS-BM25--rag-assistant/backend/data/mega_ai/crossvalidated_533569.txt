[site]: crossvalidated
[post_id]: 533569
[parent_id]: 533518
[tags]: 
Well, dealing with unbalanced data requires a lot of attention. Despite the fact that many algorithms rely on the premise of balanced data in the training phase, they can easily make you misinterpret your results. To validate on unbalanced datasets the most practical way is to downsample it exactly how you did for the training. In your example, you can randomly choose 50 samples of each class for it. The other non-obvious way is to give different weights for each class when calculating your eval metric, like accuracy. I believe your preferred framework will have functions like balanced accuracy on scikit-learn to handle it for you. But again, be careful to not forget they are unbalanced when looking for some results, and analyze precision and recall for each class individually.
