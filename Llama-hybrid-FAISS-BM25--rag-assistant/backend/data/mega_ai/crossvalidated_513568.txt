[site]: crossvalidated
[post_id]: 513568
[parent_id]: 513562
[tags]: 
Mini-batch learning is a middle ground between gradient descent (compute and collect all gradients, then do a single step of weight changes) and stochastic gradient descent (SGD; for every data point, compute the gradient, and update the weights). Mini-batch (we average gradients over smaller batches and then update) trades off statistical and computational efficiency. In both SGD and mini-batch, we typically sample without replacement, that is, repeated passes through the dataset traverse it in a different random order.
