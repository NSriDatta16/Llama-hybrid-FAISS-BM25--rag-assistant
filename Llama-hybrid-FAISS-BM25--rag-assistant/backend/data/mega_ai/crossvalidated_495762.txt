[site]: crossvalidated
[post_id]: 495762
[parent_id]: 
[tags]: 
Precision-Recall Curve and Area under Precision-Recall Curve (AUC)

I created model ( logistic regression ) and now trying to create Precision-Recall plot and calculate area under Precision-Recall Plot . I'd like to note that this model is defective : glm.fit: fitted probabilities numerically 0 or 1 occurred Data can be downloaded here: download The problem is that I am getting different results from different libraries. Manual calculations of AUC and library ROCR : data = read.csv("lr.csv", stringsAsFactors = F) predictions = data $predictions actual = data$ actual library(ROCR) predict_obj = prediction(predictions, actual) prc = performance(predict_obj, "prec", "rec") x = prc@x.values[[1]] y = prc@y.values[[1]] nans = is.nan(y) if (sum(nans) > 0) { y = y[!nans] x = x[!nans] } plot(prc, main = "ROCR library") auc_prc = sum(diff(x) * (head(y, -1) + tail(y, -1)) / 2) auc_prc gives me this plot and AUC = 0.3630107 : with cutoffs: library PRROC : library(PRROC) pr_curve = pr.curve(scores.class0 = predictions[actual == 1], scores.class1 = predictions[actual == 0], curve = T) plot(pr_curve, main = "PRROC library") gives me this plot and AUC = 0.7160321 : library PerfMeas : library(PerfMeas) obj = precision.at.all.recall.levels(predictions, actual) plot(obj $recall, obj$ precision, type = "l", main = "PerfMeas library") AUPRC(list(precision.at.all.recall.levels(predictions, actual)), comp.precision = TRUE) gives me this plot and AUC = 0.6938177 : library DMwR (as wrapper of ROCR): library(DMwR) PRcurve(predictions, actual, main = "DMwR library") gives me this plot (without AUC): My question is: why plots are so different? Which graph should I choose and what is my AUC? This question is connected with my another question: What is actually AUC (area under the curve)?
