[site]: crossvalidated
[post_id]: 642278
[parent_id]: 620657
[tags]: 
I assume that the notation and context is taken from the An Introduction to Variational Autoencoders . Following the same notation, You don't need another log det of jacobian term for $p_{\theta}(Z)$ because $p_{\theta}(Z)$ is a prior over $Z$ with no learnable parameters. Think of it as $p(Z_0)$ . It is assumed to be a Gaussian distribution in case of above paper. Throughout the learning process of VAE, this prior distribution doesn't change. However, In $q_{\phi}(Z|X)$ , $Z$ is sampled through $\epsilon$ and smooth invertible transformation $g(x, \epsilon, \phi)$ . Hence, a reparametrization with respect to $\epsilon$ was necessary to compute $\nabla_{\phi}(q_{\phi}(Z|X))$ . Ideally in standard Bayes methods, We use this sample along with prior $p(Z_0)$ above to update posterior.(But not in the case of VAE)
