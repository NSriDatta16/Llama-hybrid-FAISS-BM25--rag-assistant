[site]: crossvalidated
[post_id]: 32316
[parent_id]: 32313
[tags]: 
The best way to test out of sample prediction is to do a pseudo out of sample forecasting experiment. Use about 75 percent of the data to train the models, make the prediction, record the forecast error, update the information set, and repeat. At the end, you can use all of the forecast errors to get the mean-squared forecast error (MSFE). The model with the lowest MSFE is the best predictor. In general the out-of-sample (OOS) r-squared can help you compare a model to some benchmark. $$\text{OOS } R^2 = 1 - \frac{MSFE_{model}}{MSFE_{benchmark}}$$ If the OOS r-squared is greater than zero, the model beats the benchmark. A typical choice for the benchmark is the historical mean. The higher the OOS r-squared, the better. A couple of other thoughts: If you're not sure which model to choose, just average the forecasts from both models. Average forecasts usually perform better than individual forecasts. Also, since your time series looks like a sine wave you may want to check for seasonality and use seasonal time series models.
