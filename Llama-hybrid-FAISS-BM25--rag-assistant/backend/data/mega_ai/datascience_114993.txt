[site]: datascience
[post_id]: 114993
[parent_id]: 114701
[tags]: 
Background Although it is a prerequisite to Transformer, attention mechanism was introduced four years earlier 1 as part of an improved LSTM architecture. Attention Pooling Layers Attention improves performance of recurrent architectures by allowing the model to reason, not only from hidden state of the final position (Figure - top), but also from any position in the sequence input independently of how far back it may be (Figure - bottom). In other words, we could call this an extension of the local LSTM attention to an attention mechanism that allows for much much longer sequences to be modelled. The fixed Attention in Transformers In transformers the self-attention mechanism is leveraged (Wang et al., also proposed prior to the transformer architecture) so that the recurrence in layers is completely abandoned with a positional encoding taking its role for ensuring/encoding the position/order of each input token. But imo, the key feature of the transformer is the parallel computation exactly because you don't anymore have to loop through each event of the sequential input to impose the chronological/stepwise ordering of tokens. Source: Lmk if you identify the source of this figure, as I don't recall it [1] Neural Machine Translation by Jointly Learning to Align and Translate. https://arxiv.org/pdf/1409.0473.pdf
