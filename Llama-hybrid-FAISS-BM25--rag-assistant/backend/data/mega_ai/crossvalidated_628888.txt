[site]: crossvalidated
[post_id]: 628888
[parent_id]: 
[tags]: 
Sample size to achieve an error E on mean estimation, for a normal distribution with unknown variance

Disclaimer : This is actually related to a previous question . I decided to post this as a new question as I feel it specifies the problem enough to be considered a separate question. If mods believe this is not the case, I can erase this and edit the first question. Background : I am investigating best practices in performing a given mechanical experiment that measures a material property (lets call it Property A ). The test works by producing as many consecutive measurements as we want on the same sample of material, and in the end the result is usually the mean of these measurements. Measurements seem to be normally distributed, consecutive measurements seem to be uncorrelated (example of data here: question ). More specifically, some testing conditions may be varied to study the material behaviour. Lets say, for example, that we vary a test condition called Test Pressure , so after an experiment is conducted on a single sample, the following results on Property A at 11 different Test Pressure levels would be obtained: As can be visually observed, at different Test Pressure levels the variance of measurements is different. Higher pressures lead to higher variances. My question : I want to suggest a minimum number of consecutive measurements to be taken before the mean is calculated, by defining a maximum error $E$ between the sample mean $\overline{x}$ and the true mean $\mu$ . How can I do it for the data above? Attempts made so far : AFAIK, this problem can be related to finding a confidence interval on the mean of a normal distribution with unknown variance. For such a case, I know that if I take $N$ measurements that produce a sample mean $\overline{x}$ and a sample variance $s^{2}$ , and the true mean is $\mu$ , a $100(1-\alpha)$ % confidence interval is: $$\overline{x}-\frac{t_{\alpha/2,n-1}\cdot s}{\sqrt{n}} \leq \mu \leq \overline{x}+\frac{t_{\alpha/2,n-1}\cdot s}{\sqrt{n}}$$ Where $t_{\alpha/2,n-1}$ is the point at upper $100\alpha/2$ % of a t distribution with $n-1$ degrees of freedom. Now we may define the error of the sample mean as $E=\lvert \overline{x}-\mu \rvert$ . If we specify that an acceptable value for $E$ is $E_{accep}$ , then we may find the minimum sampling size $n_{min}$ such that the $100(1-\alpha)$ % confidence interval is met by simply considering the definition of the confidence interval above: $$E_{accep} = \frac{t_{\alpha/2,n-1}\cdot s}{\sqrt{n_{min}}}$$ Now the problem is that we can't directly find $n_{min}$ because $t_{\alpha/2,n-1}$ and $s$ depend on sample size, right? Here I find my first doubt: the dependency of $t_{\alpha/2,n-1}$ is clear to me; the dependency of $s$ seems to be intuitive, but I am not 100% sure about it, as perhaps I did not get some nuance behind the concept of using a t-distribution when variance is unknown. In the case both dependencies exist, the problem seems to require some sort of iterative process for solving for $n_{min}$ : I set increasing values for $n$ and verify the smallest which leads to $E_{accep}$ . For each iteration, retrieval of $t_{\alpha/2,n-1}$ seems easy. Retrieval of $s$ causes me doubts. I have only the result of one experiment with 30 measurements (shown above as example). As I am confortable with Python, I thought of performing a bootstrap sampling on this set of measurements for each value of $n$ and use the average $s$ obtained from the bootstraping procedure. Not sure if this is statistically sound or plain rubbish, and would appreciate any leads on that.
