[site]: crossvalidated
[post_id]: 492343
[parent_id]: 492331
[tags]: 
Nearly. Vanilla GBMs work pretty much like this. Each tree is built to approximate the gradient of the loss function, but then the tree construction is just like any ordinary regression tree: split using some impurity criterion, and assign the average value at the leaves. One of XGBoost's additions to the algorithm is the second derivative. The exact answer to your question is equation 5 of the paper: https://arxiv.org/pdf/1603.02754.pdf $$ w^*_j = âˆ’ \frac{ \sum_{i\in I_j} g_i }{ \sum_{i\in I_j} h_i + \lambda } $$ To compare to the above, think about the case of regression with MSE loss, where $h_i$ is constant, and without regularization, so $\lambda=0$ .
