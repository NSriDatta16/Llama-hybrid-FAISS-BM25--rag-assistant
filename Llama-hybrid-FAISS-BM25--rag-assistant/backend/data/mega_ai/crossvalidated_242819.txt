[site]: crossvalidated
[post_id]: 242819
[parent_id]: 
[tags]: 
Why is the regularization penality equal to $\langle c , Kc \rangle_{R^n}$ when using the kernel trick in Tikhonov regularization?

This might be a stupid question but why is it that when we try to solve: $$ \min_{f \in \mathcal H} \frac{1}{n} \sum^n_{i=1} L(f(x_i),y_i) + \lambda \|f \|^2_{\mathcal H}$$ in a RKHS (Reproducing Kernel Hilbert space) that the penalty is equivalently expressed as: $$ \|f \|^2_{\mathcal H} = \langle c , Kc \rangle_{R^n} $$ in particular when using the linear function $f(x) = \langle w_i, x_i \rangle_{R^d}$ its clear that the norm $\|f \|^2_{ \mathcal H} = \| w \|^2_2$. However, why does it become $\|f \|^2_{\mathcal H} = \sum^n_{i,j = 1} c_i c_j K(x_i, x_j)$? Note that I accept the Representer theorem as true, i.e. that $\mathcal H $ is effectively equal to $\hat{\mathcal H} = \{ f \mid f( \cdot) = \sum^n_{i=1} c_i K(x_i, \cdot) \}$ (i.e. we only use function that use the training set points from the RKHS). However, I don't understand how $\langle c , Kc \rangle_{R^n}$ is derived, or from which inner product it came from. Or essentially, my intuition from the linear case to the kernel case seems confused, because in the linear case we don't have the penalty term something involving x. In other words, analogously I would have expected the penalty when $f(x) = \langle w_i, x_i \rangle_{R^d}$ (linear case) to be something like: $$ \|f \|^2_{\mathcal H} = \langle c , Xc \rangle_{R^n} = \sum^n_{i,j = 1} c_i c_j \langle x_i, x_j \rangle_{R^d} $$ but clearly its not and this product $\langle x_i, x_j \rangle_{R^d}$ seems to have gone missing for some reason. i.e. why in the linear case is the norm not $ \langle c, XX^T c \rangle$. What happened to $ XX^T$?
