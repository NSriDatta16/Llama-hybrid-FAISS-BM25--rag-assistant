[site]: crossvalidated
[post_id]: 193414
[parent_id]: 193264
[tags]: 
Let us define a round of tosses as two tosses; the first toss by A who tosses a coin with $P(\text{Heads}) = p_1$ and the second toss by B who tosses a coin with $P(\text{Heads}) = p_2$. The outcomes of a round and the corresponding probabilities are $$\begin{matrix} HH & &p_1p_2\\ HT & &p_1(1-p_2)\\ TH & &(1-p_1)p_2\\ TT & &(1-p_1)(1-p_2) \end{matrix}$$ where, if either of the first two outcomes occurs, A wins (with probability $p_1$); in fact, it does not matter in the least whether B tosses a coin at all once A has tossed a Head. Call this event $\mathcal A$ If the third outcome $TH$ occurs, B wins (with probability $(1-p_1)p_2 = p_2 - p_1p_2$; we call this $\mathcal B$. If the fourth outcome $TT$ occurs, nobody wins, and a new round is initiated. This is event $\mathcal C$. Thus, one of the three (disjoint) events $\mathcal{A, B, C}$ can occur on a round. The game ends on a round with probability $p^* = p_1 + p_2 - p_1p_2$ and continues for the next round with probability $1-p^*$. A wins the game if the outcomes of the rounds are any of the following disjoint sequences of events: \begin{align} &\mathcal{A}\\&\mathcal{CA}\\&\mathcal{CCA}\\&\mathcal{CCCA}\\ &\quad\vdots\\&\mathcal{C}^{k-1}\mathcal{A}\\&\quad\vdots \end{align} Thus, $$P(\mathcal A) = p_1 + (1-p^*)p_1 + (1-p^*)^2p_1 + \cdots = \frac{p_1}{p^*}$$ and similarly, \begin{align}P(\mathcal B) &= (1-p_1)p_2 + (1-p^*)(1-p_1)p_2 + (1-p^*)^2(1-p_1)p_2 + \cdots \\ &= \frac{(1-p_1)p_2}{p^*} \\ &= \frac{p^*-p_1}{p^*}\\ &= 1-P(\mathcal A).\end{align} Note that conditioned on the event that the game ended on the round under consideration, the probabilities of the players winning are $$P(\text{A wins}) = \frac{p_1}{p_1 + p_2 - p_1p_2} = \frac{p_1}{p^*}, \quad P(\text{B wins}) = \frac{p_2 - p_1p_2}{p_1 + p_2 - p_1p_2} = \frac{p^*-p_1}{p^*}.$$ These are the same as the unconditional probabilities of $\mathcal A$ and $\mathcal B$ Let $X$ denote the number of rounds till the game ends. $X$ also denotes the number of times that the winner of the game tosses the coin. Then, $X$ is a geometric random variable with parameter $p^* = p_1 + p_2 - p_1p_2$. But, the previous paragraph shows that conditioned on the occurrence of the event $\{X = k\}$, the conditional probability of $\mathcal A$ is the same as the unconditional probability, that is, _for each $k, k =1,2,3,\ldots$, the event $\{X = k\}$ and the event $\mathcal A$ are mutually independent events. Indeed, the random variable $X$ is independent of the event $\mathcal A$. Now, the expected value of $X$ is \begin{align} E[X] &= 1\cdot p^* + 2\cdot (1-p^*)p^* + 3\cdot (1-p^*)^2p^* + \cdots\\ &= p^*\left[1 + 2\cdot (1-p^*) + 3\cdot (1-p^*)^2 + \cdots \right]\\ &= p^* \cdot \frac{1}{(1-(1-p^*))^2}\\ &= p^* \cdot \frac{1}{(p^*)^2}\\ &= \frac{1}{p^*} = \frac{1}{p_1 + p_2 - p_1p_2} \end{align} and this expected value is supremely indifferent to the occurrence or the non-occurrence of $\mathcal A$. In short, the expected number of tosses when A wins the game is $\frac{1}{p^*}$ which is the same as the expected number of tosses when B wins the game.
