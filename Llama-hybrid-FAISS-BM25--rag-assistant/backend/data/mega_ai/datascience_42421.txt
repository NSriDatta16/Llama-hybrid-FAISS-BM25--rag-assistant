[site]: datascience
[post_id]: 42421
[parent_id]: 16729
[tags]: 
The "universal approximation" of a standard neural network is not universal. The small letters tell that the target function must be bounded, and that you may need a really wide hidden layer to get a big value range, when the output of each unit is bounded to e.g. (-1,1). Then comes the issue of efficient learning, because ability to represent does not mean ability to learn efficiently. Exploring other types of neurons can be useful. A concrete application of multiplicative neurons could be to have an initial layer that generates an arbitrary number of compounded features from the inputs, generalizing beyond a fixed set of polynomial terms with non-negative integer exponents below a given value. The next layers could consist of standard additive neurons, giving linear combinations of the features. Multiplicative neurons have been explored. Also neurons that interpolate (at least approximately) between addition and multiplication: https://arxiv.org/abs/1503.05724 https://arxiv.org/abs/1808.00508 The last one, NALU from Deepmind, apparently is a great success, and exists in community implementations for several NN frameworks. It can be plugged in for a linear layer, although costs more to compute, and has more parameters to train. The logarithm of a product is the sum of logarithms of the factors. So one way to make a multiplicative neuron layer for positive inputs, is to apply a logarithm to the inputs, then run them through a linear layer, then exponentiate the output. The weights will correspond to exponents. Update: See this answer .
