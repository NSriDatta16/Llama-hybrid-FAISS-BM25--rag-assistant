[site]: datascience
[post_id]: 100641
[parent_id]: 100598
[tags]: 
To my understanding words unknown to the tokenizer will be masked with [UNKNOWN]. Your understanding is not correct. BERT's vocabulary is defined not at word level, but at subword level. This means that words may be represented as multiple subwords. The way subword vocabularies work mostly avoids having out-of-vocabulary words, because words can be divided up to the character level, and characters from the training data are assured to be present in the subword vocabulary. Therefore, as long as the alphabet used in your fine-tuning data is the same as the training data, there should be no out-of-vocabulary words: your out-of-domain terms will simply be divided into smaller subwords. So you should not need to include new entries for them in the embedding table. There are some answers in this site that may give you more examples, like this and this .
