[site]: crossvalidated
[post_id]: 154864
[parent_id]: 
[tags]: 
Imposing singular value constraints on recurrent neural networks

Apologies for beginning with what are surely very old results, to make clearer my thinking. First, note that any multi-layer perceptron (MLP) may be represented as a recurrent neural network (RNN). In particular, if $z^{(k)}$ is the output of the $k$'th layer of the MLP, $u$ is the input, including a constant, and we allow all neurons (not just the first layer) a direct connection to the input (a weak generalisation), then with $z^{(0)}:=0$ and $W^{(0)}:=0$, an MLP can be defined by: $$ \forall k\in{1,\dots,N}, \hskip20pt z^{(k)}=\phi(W^{(k-1)}z^{(k-1)}+V^{(k)}u). $$ This is equivalent to the RNN defined by: $$ W:=\left[ \begin{matrix} 0 & 0 & 0 & \cdots & 0\\ W^{(1)} & 0 & 0 & \cdots & 0\\ 0 & W^{(2)} & 0 & \cdots & 0\\ \vdots & \vdots & \ddots & \ddots & \vdots\\ 0 & \cdots & 0 & W^{(N-1)} & 0 \end{matrix} \right], \hskip10pt z:=\left[ \begin{matrix} z^{(1)}\\ z^{(2)}\\ z^{(3)}\\ \vdots\\ z^{(N)} \end{matrix} \right], \hskip10pt V:=\left[ \begin{matrix} V^{(1)}\\ V^{(2)}\\ V^{(3)}\\ \vdots\\ V^{(N)} \end{matrix} \right], $$ $$ z(t)=\phi(Wz(t-1)+Vu), $$ where the input is "held on constantly". In particular, at the end of iteration $t$, $z(t)$ will contain the correct values of $z^{(1)},\dots,z^{(t)}$, so after $N$ iterations, the RNN will have converged to the complete output of the MLP. Clearly, this is a very special structure of RNN, since convergence occurs in finite time. However, if we generalise to an arbitrary RNN, we will not even get guaranteed convergence, let along guaranteed convergence to the same point, in finite time. For a smaller step away from the MLP benchmark, we might like an RNN that is guaranteed to asymptotically converge to the same point, independent of the initialization of $z(0)$. By Banach's fixed point theorem, providing $\sup_{y\in\mathbb{R}}{\phi'(y)}\le1$, we just require that the largest singular value of $W$ is strictly less than $1$. Now, the largest singular value of $W$ is a twice differentiable function of $W$ (though it is not thrice differentiable), so imposing the singular value constraint via a penalty function approach ought to work OK with full training set gradient descent optimisation (or 2nd order methods). Side question: The representation of an MLP as an RNN above actually fails this condition! How can we weaken it in a tractable way so that at least all MLP's pass? One weakening is the following: define $T(z)=\phi(Wz+Vu)$, then we require that the supremum over $z$ of the maximum singular value of $\frac{\partial T^M(z)}{\partial z}$ is less than $1$, for some $M\in\mathbb{N}$. However, this does not appear to be equivalent to a condition on the maximum singular value of $W^M$, which would be far easier to check (by the generalisation of the spectral radius formula to singular values, it is equivalent to the maximum absolute eigenvalue of $W$ being less than $1$, which is also the local stability condition for this model). Perhaps this was discussed in Matsuoka (1992a) or (1992b), but I do not appear to have access to either of these papers at my university. Main questions: Is a penalty function approach likely to work acceptably in stochastic gradient descent, which is necessary with larger training sets? Does the simultaneous tuning of the learning rate and the penalty function strength raise additional difficulties? Is there any way to escape the cost of repeated finite difference approximations to the gradient of the maximum singular value? Has anyone tried something like this previously? (I know that Perfettia and Massarellib (1997) handled the special case of spatially homogenous networks.) Any advice would be much appreciated.
