[site]: crossvalidated
[post_id]: 550474
[parent_id]: 
[tags]: 
How to prove that, in finite sample, Bayesian posterior is more informative than the prior?

Suppose there is a space of possible models $\theta \in \Theta$ , and that we can generate i.i.d. data $\{x_1, x_2,...\}$ from the true model. Asymptotically, the Schwartz Theorem shows that Bayesian inference is consistent: the posterior will cluster around the true model. However, can the same be shown in finite sample? That is, can we show that, in some appropriate sense, the posterior after one data observation $P(\theta | x)$ is (on average) more informative than the prior $P(\theta)$ ? When the number of models is finite, a more concrete version of this question is: can we show that $E(P(\theta^\text{true} | x)) > P(\theta^\text{true})$ ?
