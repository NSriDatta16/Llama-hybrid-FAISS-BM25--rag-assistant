[site]: crossvalidated
[post_id]: 314079
[parent_id]: 313480
[tags]: 
The idea is to use the Kalman filter. An easy to read (but quite complete) introduction is available here: How a Kalman filter works, in pictures . It is essentially Gaussian Bayesian inference. Assume $\mu_t$ is just a random walk with Gaussian step of known variance $1$. I explain how to implement the Kalman filter in this simple case. At each time $t$ the filter saves its knowledge about $\mu_t$ as a Gaussian distribution for $\mu_t$ with mean $m$ and variance $s^2$. When you move forward into the future, you loose information because $\mu_t$ varies randomly with variance $1$. The filter is updated as such: $$m\leftarrow m \\ s^2\leftarrow s^2+1\\$$ When you observe $x$ a realization of $X_t$, the filter does a Bayesian inference resulting in: $$m\leftarrow \frac{\frac{1}{\sigma^2}x+\frac{1}{s^2}m}{\frac{1}{\sigma^2}+\frac{1}{s^2}} \\ s^2\leftarrow \frac{1}{\frac{1}{\sigma^2}+\frac{1}{s^2}}\\$$ You can manage $n$ observations as a single update replacing $x$ by the average value $a_t$ and $\sigma^2$ by $\sigma^2/n_t$. In other words, when you move forward into the future, the uncertainty $s^2$ increases. After each observation the uncertainty $s^2$ decreases and the forecast $m$ is updated with weights depending on the present uncertainty and the number of observations. Red: observations. Blue: $m$ after observations
