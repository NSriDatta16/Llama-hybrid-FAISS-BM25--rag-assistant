[site]: crossvalidated
[post_id]: 343420
[parent_id]: 
[tags]: 
Bayesian thinking about overfitting

I've devoted much time to development of methods and software for validating predictive models in the traditional frequentist statistical domain. In putting more Bayesian ideas into practice and teaching I see some key differences to embrace. First, Bayesian predictive modeling asks the analyst to think hard about prior distributions that may be customized to the candidate features, and these priors will pull the model towards them (i.e., achieve shrinkage/penalization/regularization with different amounts of penalization for different predictive features). Second, the "real" Bayesian way does not result in a single model but one gets an entire posterior distribution for a prediction. With those Bayesian features in mind, what does overfitting mean? Should we assess it? If so, how? How do we know when a Bayesian model is reliable for field use? Or is that a moot point since the posterior will carry along all of the caution-giving uncertainties when we use the model we developed for prediction? How would the thinking change if we forced the Bayesian model to be distilled to a single number, e.g., posterior mean/mode/median risk? I see some related thinking here . A parallel discussion may be found here . Follow-up question : : If we are fully Bayesian and spend some time thinking about the priors before seeing the data, and we fit a model where the data likelihood was appropriately specified, are we compelled to be satisfied with our model with regard to overfitting? Or do we need to do what we do in the frequentist world where a randomly chosen subject may be predicted well on the average, but if we choose a subject who has a very low prediction or one having a very high predicted value there will be regression to the mean?
