[site]: crossvalidated
[post_id]: 494322
[parent_id]: 
[tags]: 
Propensity score matching vs non-parametric regression

I am trying to understand the benefit of propensity matching over non-parametric regression for causal inference from non-experimental data. As background: the way I understand it, parametric regressions are generally a poor choice for causal inference when selection bias exists. One can try to create a model that takes into account the different baselines between treatment and control groups, but she/he will be be extremely vulnerable to model misspecification.* Matching solves this issue by making the two datasets look "alike." The exact meaning of alike depends on the matching algorithm used, but all the algorithms strive to eradicate model dependence, with the most common types being stratified and weighted propensity matching. The mathematics for these algorithms are well-developed, and in particular, we know how to extract error bars and statistical significance. What I'm struggling with is why this is superior to just using a non-parametric regression like a decision tree or random forest, which are also designed to prevent model misspecification. After creating the forest, one could run individuals through it assuming treatment or no treatment, and call the difference the estimated treatment effect for that individual. My first guess is that extracting significance, which is critical to causal inference, from trees is difficult, but it seems that statisticians have made strides in that regard over the last decade or so. To be clear, I am not asking about using a tree to develop the propensity scores, but to use one instead of propensity matching. To help kick off the conversation, I've developed five hypotheses for why matching is preferred to non-parametric regression, but haven't been able to find anything proving or disproving any: Empirical research demonstrates that stratified or weighted propensity matching (the most common types) yield results closer to causal experiments than non-parametric regressions like trees. Though it is possible to extract significance from non-parametric regressions like random forests, the math isn't settled, or the notion of "significance" for a decision tree variable doesn't map precisely to the notion of "one minus the odds of a type I error." Though it is possible to extract significance from non-parametric regressions, the code is difficult to write. Model misspecification actually is an issue for decision trees due to the tuning required to run them. Empirical observation has demonstrated this is more of an issue for decision trees than it is for matching algorithms. We don't actually know a lot about whether we can use non-parametric regressions for causal inference, but we do know that matching works, so it there's no reason to reinvent the wheel. *As detailed in the first ten minutes of this wonderfully intuitive Youtube: https://www.youtube.com/watch?v=rBv39pK1iEs
