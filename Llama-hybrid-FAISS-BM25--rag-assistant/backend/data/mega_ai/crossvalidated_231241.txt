[site]: crossvalidated
[post_id]: 231241
[parent_id]: 231238
[tags]: 
For continuous models, the Bayes factor is defined as the ratio of marginal likelihoods (marginal density functions of the data): $$P(M_i\vert x) = \int p(x\vert\theta_i, M_i)\pi(\theta_i)d\theta_i \neq 0,$$ where $p$ denotes the likelihood function (joint density of the data given the parameters of model $M_i$). See: Kass, Robert E., and Adrian E. Raftery. " Bayes factors ." Journal of the american statistical association 90.430 (1995): 773-795. EDIT: Regarding the derivation of the formula. Recall that discrete and continuous variables cannot be treated the same way. Thus, the conditional probability is given by $$P(M_i\vert x) = \dfrac{P(M_i)f(x\vert M_i)}{f(x)}.$$ Using the Law of total probability: $$f(x\vert M_i) = \int f(x\vert M_i,\theta_i)\pi(\theta_i)d\theta_i.$$ Thus: $$P(M_i\vert x) = \dfrac{P(M_i)\int f(x\vert M_i,\theta_i)\pi(\theta_i)d\theta_i}{f(x)}.$$ Finally: $$\frac{P(M_1\vert x)}{P(M_2\vert x)} = \dfrac{P(M_1)\int f(x\vert M_1,\theta_1)\pi(\theta_1)d\theta_1}{P(M_2)\int f(x\vert M_2,\theta_2)\pi(\theta_2)d\theta_2}.$$ $\theta_i$ represent the parameters associated to model $M_i$.
