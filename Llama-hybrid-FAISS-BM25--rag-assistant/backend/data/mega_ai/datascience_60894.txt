[site]: datascience
[post_id]: 60894
[parent_id]: 60764
[tags]: 
As you mentioned in a comment, you are upsampling before splitting the test set, which leads to data leakage; your scores are not to be trusted. The problem is that a given positive sample may be duplicated and then put into both the training and the test set. Especially with tree models, this is very likely to correctly predict that sample in the test set. The story with SMOTE is similar, but as you pointed out, not quite so severe. In SMOTE you're interpolating between positive samples (see image from imb-learn docs ), so if some of those points are in the training set and some in the testing set you're still more likely to correctly identify those points. Instead, you should split first, upsample the training set second. Alternatively, set class weights (this has the benefit of being independent of the split). Now your test set has a different distribution that the training set, so you'll need to adjust the class prediction threshold, or adjust the probability predictions. See e.g. "Convert predicted probabilities after downsampling to actual probabilities in classification?" . Part of the question here is whether you want actual estimates of the probabilities, or just care about the class predictions. There's a serious question about whether resampling techniques are helpful at all. See e.g. "What is the root cause of the class imbalance problem?" "When is unbalanced data really a problem in Machine Learning?" As a first attempt, I would stick with the original data, fit the random forest, and have a look at different thresholds. In your case, I would worry that 88 positive samples may just not be enough to see a meaningful pattern. (It might be; it depends on how separated the classes are.)
