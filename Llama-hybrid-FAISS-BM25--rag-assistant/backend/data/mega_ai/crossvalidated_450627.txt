[site]: crossvalidated
[post_id]: 450627
[parent_id]: 450416
[tags]: 
I think that the confusion stems from a somewhat ambiguous use of the terminology. In my view the term frequentism should be used for an interpretation of probabilities, namely that they refer to data generating processes in reality and correspond to limits of relative frequencies under idealised infinite repetition. In frequentism often parametric models are used in which the parameter is fixed. However, it is also possible to have a process in which a parameter value itself is the result of a potentially repeatable experiment, in which case there can be a frequentist distribution over the parameter, and Bayes theorem can be applied. Otherwise, though, most frequentists would not accept a probability distribution over their parameters, because they are fixed and not random in reality, or rather in what they think of as reality. What is today called "Bayesian" is an approach in which a prior distribution over the parameter of parametric model is assumed, which normally is interpreted as encoding epistemic probabilities, referring to prior knowledge of the parameter, but not to a real physical process generating it. This approach always requires Bayes theorem. However this can still be connected with a frequentist idea of probabilities, see e.g. the section on "falsificationist Bayes" in Gelman and Hennig "Beyond subjective and objective in statistics". Furthermore, Fisher favoured so-called "fiducial probabilities", which are epistemic probabilities over the parameters of frequentist models. These are widely rejected these days though. Anyway, in this setting the $\theta$ was treated as random variable, but without prior, not requiring Bayes theorem. What I find problematic and potentially confusing about the terminology is that seemingly today many would no longer call the use of Bayesian computation with frequentist probabilities Bayesian, despite the fact that Bayes himself did such a thing.
