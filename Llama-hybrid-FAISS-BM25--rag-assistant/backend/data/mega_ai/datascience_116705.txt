[site]: datascience
[post_id]: 116705
[parent_id]: 
[tags]: 
Why do neural networks with more layers perform better?

Why do neural networks with more layers perform better than a single layer MLP with a number of neurons that leads to the same number of parameters? I read this post: https://www.quora.com/Why-do-neural-networks-with-more-layers-perform-better-than-a-single-layer-MLP-with-a-number-of-neurons-that-leads-to-the-same-number-of-parameters and still I'm not sure it always true. For example: Assume with have model with 2 linear layers (for simplicity with no bias) and RELU as the final layer, the model look: RELU (w1x + w2x) and the number of parameters to optimize is (w1 + w2) We can see that: RELU (w1x + w2x) = RELU ((x(w1 + w2)) = RELU (w3x) so w3 = w1+w2 i.e second model with 1 linear layers has less parameters to optimize. In my example is it still better to use one layer or 2 layers ? Am I right that the second model (with w3 ) has less parameters? Is it easier to optimize the second model ( w3 ) ?
