[site]: crossvalidated
[post_id]: 45915
[parent_id]: 26497
[tags]: 
If your question is whether it is possible to separate without errors a linearly separable set of points by using polynomial kernel $k(x, z) = (\langle x, z \rangle + 1)^d$, $d > 1$, then the answer is yes , it is possible to do that. One of the feature spaces $H$ for the polynomial kernel $k(x, z) = (\langle x, z \rangle + 1)^d$ defined for $x, z \in R^n$ contains all monomials of variables $x_1, x_2, ..., x_n$ of degree not higher than $d$. Therefore it contains a subspace of variables $x_1, ..., x_n$. If your dataset is linearly separable in the space of $x_1, ..., x_n$ then it is linearly separable in $H$, which means that there exist an SVM with kernel $k(x, z)$ that separates your dataset in $R^n$ (without errors). It does not mean, however, that your SVM training algorithm of choice will find that hyperplane for a given $d$.
