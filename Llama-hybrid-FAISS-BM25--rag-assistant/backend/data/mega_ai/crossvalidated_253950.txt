[site]: crossvalidated
[post_id]: 253950
[parent_id]: 253105
[tags]: 
No, the gradient is still calculated from the whole mini-batch. If you want to read about this: This goes back to fitted Q iteration [1] and possibly further. DQN is an iteration of neurally fitted Q-iteration (the main author of that one was also on the DQN paper) which builds on this one. The fact that it's done this way, is also in the DQN paper. [1] Ernst, Damien, Pierre Geurts, and Louis Wehenkel. "Tree-based batch mode reinforcement learning." Journal of Machine Learning Research 6.Apr (2005): 503-556.
