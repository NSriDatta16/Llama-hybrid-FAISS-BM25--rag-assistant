[site]: crossvalidated
[post_id]: 614582
[parent_id]: 
[tags]: 
Finding inequality relationships in stochastic data

Consider random variables $X_i$ such that the following holds: $$ a_1 X_1 + \dots + a_n X_n -k \geq 0 + \epsilon $$ where the $a_i$ are constants and $\epsilon$ is random noise. How is it possible to uncover a relationship like this given realisations from the $X_i$ ? Initial Thoughts PCA If the inequality were an equality, one way to do this would be via PCA and looking at the components with small eigenvalues. Of course performance would be dependent (as above) on the signal to noise ratio determined by the $\epsilon$ . Optimisation Instead we could look to perform an optimisation to determine the optimal $a_1, \dots, a_n, k$ parameters. For a given realisation $x_i$ , we could use a loss function of the form: $$ \exp(-h(a_1, \dots, a_n, k)) $$ where $h(a_1, \dots, a_n, k) = a_1 x_1 + \dots + a_n x_n - k$ . This would penalise small values while remaining fairly indifferent to larger values. Non-Uniqueness This is all well and good but then runs into the issue that with a finite dataset there are going to be infinite hyperplanes surrounding the data in all orientations. Even with an infinite amount of data, if the inequality holds then there will be an inifinite number of looser inequalites holding with the same $a_i$ parameters but smaller $k$ . Reframing Hence we want to find a 'tight' inequality i.e. a lot of the points are close to the hyperplane itself. Note this would not be generally true given the initial inequality e.g. imagine points distributed on a unit circle. The 'tight' hyperplanes would be tangents to these where there would not be a lot of close points. However I will focus on the cases where this assumption is valid. This motivated a loss function of the form: $$ \exp(-h) - \lambda \exp(-h^2) $$ Therefore rewarding values of $h$ close to 0, where $\lambda$ is a hyperparameter. Tuning Now the problem is the above is very sensitive to the $\lambda$ value and I can't think of an obvious way to tune it. It essentially gives the 3 Goldilocks scenarios seen in plots of $h$ below. Note to generate these plots I used the following: Sample from $X_1, X_2, Z \sim MVN(0, I)$ . Defining $X_3 = X_1 - X_2 + 5 - Z^2$ , calculate the realisation for $X_3$ using above values. Hence $X_1 - X_2 - X_3 + 5 = Z^2$ and there is a 'tight' hyperplane to discover. Too small (we care too much about not breaking the constraint and get a very loose hyperplane): Too large (we care too much about getting values close to 0: Just right (we uncover a tight hyperplane): I'd be interested in any improvements for example hyper-parameter tuning in the above scenario, reframing of the objective function or a different approach to the problem.
