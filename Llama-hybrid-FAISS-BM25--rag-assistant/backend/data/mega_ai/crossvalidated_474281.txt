[site]: crossvalidated
[post_id]: 474281
[parent_id]: 474192
[tags]: 
In addition to the points raised in the other answers, a pruned network may not be faster . Common machine learning frameworks have very efficient optimizations for computing dense matrix multiplications (i.e. normal, unpruned layers), but those algorithms can't take any additional advantage of the fact that some weights are set to 0 (because they are pruned). So the result of pruning is often a neural network that is smaller, but no faster and has worse performance. In many cases, better performance is more important than a smaller model size, so pruning is not useful in those cases. Note that pruned networks could be faster if 1. an overwhelmingly large fraction of weights were pruned away, in which case sparse matrix multiplication algorithms might start being faster; or 2. (in CNNs; I'm not sure off the top of my head if this is applicable to other architectures) if pruning was not weight-level but rather channel-level (so either an entire channel is pruned all at once or the whole channel is left as is), which does work with the optimizations; or 3. given specialized hardware or ML frameworks.
