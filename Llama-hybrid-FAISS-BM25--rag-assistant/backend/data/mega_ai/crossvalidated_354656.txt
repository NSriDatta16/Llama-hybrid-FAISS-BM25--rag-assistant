[site]: crossvalidated
[post_id]: 354656
[parent_id]: 225748
[tags]: 
I am wondering if it is due to that I did not find the best parameters or simply because ReLU is only good for deep networks? I believe I can safely assume that you mean hyperparameters instead of parameters. A neural network with 5 hidden layers is not shallow. You can consider it deep. The hyperparameter space search for 'best' hyperparameters is a never ending task. By best I mean the hyperparameters that lets the network attain the global minima. I agree with Sycorax that once you change the activation function you need to tune the network again. Usually, one can achieve comparable performance across many different configurations of hyperparams for the same task.
