[site]: datascience
[post_id]: 88703
[parent_id]: 88552
[tags]: 
As per the reference, Layer Normalization is applied 2 times per block (or layer). Once for the hidden states from the output of the attention layer, and once for the hidden states for the output from the feed-forward layer. However, it is (For hugging-face implementation, you can check out class Block here ) For your example, normalization necessarily doesn't really does not change the order of spikes of various tokens, but changes the magnitude of the spikes. This has shown to increase the model training time, and improve the performance. ( paper )
