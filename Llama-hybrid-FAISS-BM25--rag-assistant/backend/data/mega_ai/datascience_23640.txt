[site]: datascience
[post_id]: 23640
[parent_id]: 23638
[tags]: 
I will answer your second question first, doc2vec and word2vec both are primarily good representations of text data that capture the semantics of words and documents. So whenever you are working with text data, you need a representation for it and that is what word2vec and doc2vec provides. Now think of any real world task on text data, like document similarity, using doc2vec you can find cosine similarity between two documents easily, now think of real world applications for it, finding duplicate question on a site like stack overflow, ranking candidate answers for a question answering model, features for text cladsification like sentiment analysis ( word2vec doesnt work well here, the context of good and bad are quite similar so it struggles to differentiate between positive and negative reviews). So these are just representations, you can apply them to any NLP and a lot of IR task. To answer your first question, a model is not just task dependent but also data dependent. So you can read Mikolov's paper to find out how each model works for the baseline tasks but a good idea is to try both models on your data and extrinsically evaluate which algorithm performs better.
