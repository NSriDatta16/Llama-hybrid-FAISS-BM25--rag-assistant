[site]: crossvalidated
[post_id]: 520651
[parent_id]: 447991
[tags]: 
by minimizing squared reconstruction error, you are basically looking for a latent representation that explains the most variance in the data. If the hidden neuron has a linear activation function, this will be the same as calculating the first principal component of your dataset. If you want to get more intuition into this, look for an explanation of PCA and the relationship between autoencoder and PCA. If the hidden neuron is not linear, then the result will be something weird, since with one hidden layer and neuron we cannot learn a meaningful non-linear mapping between the hidden layer and the data. If you have multiple hidden layers with non-linear activation functions, then the non-linear function of these components will explain the most variance in the data, in contrast to principal components that are only related to the data linearly.
