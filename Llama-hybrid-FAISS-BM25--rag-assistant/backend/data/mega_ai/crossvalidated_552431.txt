[site]: crossvalidated
[post_id]: 552431
[parent_id]: 552375
[tags]: 
At least for deep learning purposes, one shouldn't overthink the importance of the exact terms in the softmax function, nor that it maps onto the probability simplex. What matters is that it's a function $s_\lambda : \mathbb{R}^n \to [0,1]^n$ with the properties $$\begin{align} \frac{\partial (s_\lambda(\mathbf{z}))_i}{\partial z_i} >&\: 0 & \text{for all $i$} \\ \frac{\partial (s_\lambda(\mathbf{z}))_i}{\partial z_j} and $$ s_\lambda(\mathbf{z}) \approx \{0,0,...,\underbrace{1}_{i},0,...\} $$ if $z_i \gg z_j$ for all $j\neq i$ . As a result, tweaking the inputs sufficiently far in some direction will eventually give you a nearly categorical output, and the gradients of mismatching results are always propagated in a direction that's suitable. So why is this exponential form the softmax function that's used by everyone? It's mostly just that $\exp$ happens to be both monotone and strictly convex, and from that the above properties already follow. In addition, it's very fast-growing, which means the convergence to almost-black&white result does not take too long time.
