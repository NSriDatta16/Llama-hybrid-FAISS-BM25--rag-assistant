[site]: stackoverflow
[post_id]: 491543
[parent_id]: 100235
[tags]: 
I think your issue is likely to be that last point: When setting or clearing a value it should not re-write the entire underlying file, instead it should seek to the position in the file and update the value. This is exactly what a DB does - you're basically describing a simple file based table structure. We can illustrate the problem by looking at strings. Strings in memory are flexible things - you don't need to know the length of a string in C# when you declare its type. In data storage strings and everything else are fixed sizes. Your saved dictionary on disk is just a collection of bytes, in order. If you replace a value in the middle it either has to be exactly the same size or you will have to rewrite every byte that comes after it . This is why most databases restrict text and blob fields to fixed sizes. New features like varchar(max) / varbinary(max) in Sql 2005+ are actually clever simplifications to the row only actually storing a pointer to the real data. You can't use the fixed sizes with your example because it's generic - you don't know what type you're going to be storing so you can't pad the values out to a maximum size. You could do: class PersistantDictionary : Dictionary where V:struct ...as value types don't vary in storage size, although you would have to be careful with your implementation to save the right amount of storage for each type. However your model wouldn't be very performant - if you look at how SQL server and Oracle deal with table changes they don't change the values like this. Instead they flag the old record as a ghost, and add a new record with the new value. Old ghosted records are cleaned up later when the DB is less busy. I think you're trying to reinvent the wheel: If you're dealing with large amounts of data then you really need to check out using a full-blown DB. MySql or SqlLite are both good, but you're not going to find a good, simple, open-source and lite implementation. If you aren't dealing with loads of data then I'd go for whole file serialisation, and there are already plenty of good suggestions here on how to do that.
