[site]: crossvalidated
[post_id]: 274657
[parent_id]: 274650
[tags]: 
This is nicely described by Christian P. Robert in The Bayesian Choice book (pp. 77-78): Proposed by Legendre (1805) and Gauss (1810), this loss is undoubtedly the most common evaluation criterion. Founding its validity on the ambiguity of the notion of error in statistical settings (i.e., measurement error versus random variation), it also gave rise to many criticisms, commonly dealing with the fact that the squared error loss $$ L(\theta, d) = (\theta - d)^2 \tag{2.5.1} $$ penalizes large deviations too heavily. However, convex loss functions like (2.5.1) have the incomparable advantage of avoiding the paradox of risk lovers and to exclude randomized estimators. Another usual justification for the quadratic loss is that it provides a Taylor expansion approximation to more complex symmetric losses (see Exercise 4.14 for a counterexample). In his 1810 paper, Gauss already acknowledged the arbitrariness of the quadratic loss and was defending it on grounds of simplicity. (...) In fact, the Bayes estimators associated with the quadratic loss are the posterior means. (...) So it avoids the risk lovers paradox, since risk lovers "prefer a random gain to the expectation of this gain" (p. 59) and squared loss settles at the mean. It is connected to the notion of error and normal distribution , and it is optimization-friendly. Nonetheless, it is somehow arbitrary and certainly it is not always preferable, or ultimately the best.
