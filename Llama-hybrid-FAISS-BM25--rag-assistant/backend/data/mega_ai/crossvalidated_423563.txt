[site]: crossvalidated
[post_id]: 423563
[parent_id]: 423548
[tags]: 
Your interpretation is correct. In my opinion that particular passage in the Wikipedia article obfuscates a simple concept with opaque technical language. The initial passage is much clearer: "is an interval within which an unobserved parameter value falls with a particular subjective probability". The technical term "random variable" is misleading, especially from a Bayesian point of view. It's still used just out of tradition; take a look at Shafer's intriguing historical study When to call a variable random about its origins. From a Bayesian point of view, "random" simply means "unknown" or "uncertain" (for whatever reason), and "variable" is a misnomer for "quantity" or "value". For example, when we try to assess our uncertainty about the speed of light $c$ from a measurement or experiment, we speak of $c$ as a "random variable"; but it's obviously not "random" (and what does "random" mean?), nor is it "variable" – in fact, it's a constant. It's just a physical constant whose exact value we're uncertain about. See § 16.4 (and other places) in Jaynes's book for an illuminating discussion of this topic. The question "what does a Bayesian interval for a 'parameter' mean?" comes from the even more important question "what does this parameter mean?". There are two main points of view – not mutually exclusive – about the meaning of "parameters" in Bayesian theory. Both use de Finetti's theorem . Chapter 4 of Bernardo & Smith's Bayesian Theory has a beautifully deep discussion of the theorem; see also Dawid's summary Exchangeability and its ramifications . The first point of view is that the parameter and its distribution are just mathematical objects that completely summarize an infinite set of joint belief distributions about the actually observable quantities $x_1, x_2, \dotsc$ (say, the outcomes of the tosses of a coin, or the presences of a genetic allele in individuals having a particular disease). So, in the binomial case, when we say "we have a 95% belief that the parameter value $p$ is within interval $I$ ", we mean "we have a belief between $b_1$ % and $b_1'$ % that $x_1=1$ ", "we have a belief between $b_2$ % and $b_2'$ % that $x_1=1$ and $x_2=1$ ", and all possible similar statements. The exact numerical relation between the $b_i$ s and the interval $I$ is given by de Finetti's integral formula. The second point of view is that such "parameters" are long-run observable quantities, so it does make sense to speak about our belief in their values. For example, the binomial parameter $p$ is the long-run frequency of observations of "successes" (tails for a coin, minor allele for the genetic case, and so on). So when we say "we have a 95% belief that the parameter value $p$ is within interval $I$ " we mean "we have a 95% belief that the long-run relative frequency of successes is within interval $I$ ". The context here is that, if an oracle or jinn told us that the long-run relative frequency were, say, 0.643, then our belief that the next observation is a success would be, from symmetry reasons, 64.3%; for the next two observations, 41.3449%, and so on. ("From symmetry reasons" because we believe equally in all possible time sequences of successes and failures – this is the context of the theorem.) These long-run observations need not be infinite, but just large enough: in this case de Finetti's infinite-exchangeability formula can be considered as an approximation of a finite-exchangeability one (for example, the binomial distribution is an approximation for a hypergeometric one: "drawing without replacement"); see Diaconis & Freedman about such approximation. Often such parameters are related to long-run statistics (see again the cited chapter in Bernardo & Smith). In short, the "parameter" is a long-run frequency or other observable, empirical statistics. I personally like the second point of view – which tries to find the empirical meaning of the parameter as a physical quantity – also because it helps me to assess my pre-data belief distribution about that specific physical quantity, in its specific context. See for example Diaconis & al's paper Dynamical bias in the coin toss for a beautiful study of the relation between long-run parameters and physical principles. Today, unfortunately, many "models" and parameters come just as black boxes: people use them just because other people use them. In Diaconis's words : de Finetti's alarm at statisticians introducing reams of unobservable parameters has been repeatedly justified in the modern curve fitting exercises of today's big models. These seem to lose all contact with scientific reality focusing attention on details of large programs and fitting instead of observation and understanding of basic mechanism. In frequentist theory the term "random variable" may have a different meaning though. I'm not an expert in this theory, so I won't try to define it there. I think there's some literature around that shows that frequentist confidence intervals and Bayesian intervals can be quite different; see for example Confidence intervals vs Bayesian intervals or https://www.ncbi.nlm.nih.gov/pubmed/6830080 .
