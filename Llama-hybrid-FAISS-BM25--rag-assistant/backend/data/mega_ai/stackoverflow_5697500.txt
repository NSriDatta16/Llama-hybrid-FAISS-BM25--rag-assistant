[site]: stackoverflow
[post_id]: 5697500
[parent_id]: 
[tags]: 
Take the average of two signed numbers in C

Let us say we have x and y and both are signed integers in C, how do we find the most accurate mean value between the two? I would prefer a solution that does not take advantage of any machine/compiler/toolchain specific workings. The best I have come up with is: (a / 2) + (b / 2) + !!(a % 2) * !!(b %2) Is there a solution that is more accurate? Faster? Simpler? What if we know if one is larger than the other a priori? Thanks. D Editor's Note : Please note that the OP expects answers that are not subject to integer overflow when input values are close to the maximum absolute bounds of the C int type. This was not stated in the original question, but is important when giving an answer.
