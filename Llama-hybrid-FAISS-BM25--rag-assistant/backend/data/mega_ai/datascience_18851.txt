[site]: datascience
[post_id]: 18851
[parent_id]: 18839
[tags]: 
One-hot encoding is the normal approach, and yes you would end up with 500 features for your group alone. Depending on how much training data you have this does not need to be a problem. If you have a lot of other features without direct interactions between the individual products you could use an Embedding layer before adding them to the rest, which maps your sparse categorical one-hot encoded features into a dense space via backpropagation. This will reduce the amount of parameters significantly. If you don't have enough training data to do this, you could look at gathering statistics about your product, that say something about other features for the group, or about the target for other rows (be sure not to use the current row, which would introduce target leakage) which would allow you to drop the category all together. Turning them into IDs and then using this as a numerical feature is a bad idea, since there is no inherent structure in the numbers, which means there is no signal in that and it would need a very high complexity to learn how to distinguish 1 and 5 being very similar but 2 and 4 very different.
