[site]: crossvalidated
[post_id]: 497167
[parent_id]: 497161
[tags]: 
It makes a difference whether you want to maximize the predictive accuracy of future data or infer parameters from at-hand data. Predicting future data If your goal is out-of-sample prediction, then it makes sense to evaluate the model on out-of-sample prediction (e.g., cross-validation). Simple as that. The R^2 on a "regular" regression model is in-sample. Cross-validation is a nice out-of-sample method because you make the most of the data: every data point is used both for training and validation. You would use cross-validation, both for parameter selection and for hyperparameter tuning, e.g., if you chose a more flexible multiple regression model like elastic net. Once you find a good model/hyperparameters, you can train it on all your available data and use that model for prediction. The new data would then be just like your hold-out data in the CV, so your CV above helped you learn about the performance of your model in this scenario. Making inferences from existing data While frequentist methods are popular (p-values, confidence intervals, likelihood-ratio tests), I'd recommend going Bayesian for better interpretability of the resulting parameter values (posterior distributions). Model selection for Bayesian regression models is a field in heavy development, but take a look at the rstanarm or brms packages for regression. They both support k-fold and leave-one-out cross-validation for model comparison (via the loo package), which makes a lot more sense for inference in a Bayesian setting, because the posteriors represent possible worlds rather than just the single most likely world as frequentist does. Notes on cross-validation CV need not be 10-fold. It can be 11-fold or 5-fold or something else. At the other end of the spectrum, you have leave-one-out (LOO) CV. While RMSE is a common index for fit, you could easily have situations where another metric better represents your goal. I often work with problems where the cost of error is linear, not quadratic, and so I use mean absolute error - both as the cost function in the algorithm (regression etc.) and at the model selection level (aggregate of out-of-sample performance). CV is only valid if you assume all folds to have essentially the same properties (you can help this along with stratification), and that your training set is representative of the yet-to-be-seen data you want to predict. This is an issue with time series where a leave-future-out (e.g., "rolling window" CV or "walk-forward" CV) method is often more appropriate.
