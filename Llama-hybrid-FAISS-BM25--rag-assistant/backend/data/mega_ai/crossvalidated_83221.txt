[site]: crossvalidated
[post_id]: 83221
[parent_id]: 83216
[tags]: 
overfitting is always bad as it means you have done something to your model that means that it generalisation performance has become worse. This is less likely to happen when you have lots of data, and in such circumstances regularisation tends to be less helpful, but over-fitting is still something you don't want. This diagram (from Wikimedia) shows an over-fitted regression model In order for the regression line to pass through each of the data points, the regression has high curvature at many points. At these points, the output of the model is very sensitive to changes in the value of the input variable. This generally requires model parameters of large magnitude, so that small changes in the input are magnified into large changes in the output. No, regularisation is not always needed, particularly if you have so much data that the model isn't flexible enough to exploit the noise. I would recommend putting regularisation in and use cross-validation to set the regularisation parameter(s). If regularisation is unhelpful, cross-validation will tend to make the regularisation parameter small enough that it has no real effect. I tend to use leave-one-out cross-validaition as it can be computed very cheaply for many interesting models (linear regression, SVMs, kernel machines, Gaussian processes etc.), even though its high variance is less attractive.
