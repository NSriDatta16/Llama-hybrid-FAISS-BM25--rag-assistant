[site]: datascience
[post_id]: 16652
[parent_id]: 16616
[tags]: 
I agree with Anony-Mousse that if you can't easily visualize the resulting clusters and evaluate them manually then this technique is probably not going to be very useful. That being said... Choosing Number of dimensions to reduce to: typically you decide on an acceptable level of information loss in terms of how much total variance of the original data is lost once you execute your dimensionality reduction. Then you make the number of dimensions as small as possible without dropping below this limit. Choosing Number of clusters: You can take a look at wikipedia for some info on how people assess this. If you take average distance of points from their assigned cluster centers as a metric, then one common approach is to graph this "error" as a function of number of clusters. Obviously it will always get smaller with more clusters but sometimes there is an "elbow" where you stop getting much more "bang for your buck" as you add more clusters. It looks like you're using sklearn, so you could take a closer look at the documentation and User Guide for those tools.
