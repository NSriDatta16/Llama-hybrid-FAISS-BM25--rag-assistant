[site]: crossvalidated
[post_id]: 481497
[parent_id]: 92679
[tags]: 
Is this reasoning valid? Yes it is. See below. Is this a well known rule of thumb, e.g., is it stated as a recommendation in any standard books? I think it should be but I don't think it is, at least judging by the number of postgraduate students (and beyond) that haven't really considered it. This question made me think about a section in the brilliant paper by W N Venables "Exegeses on Linear Models" (which is required reading for all my students) and I encourage anyone who has not read it, or hasn't read it recently, to do so. I will provide the full reference and link at the end of this answer. Most of what follows is taken from the paper, almost verbatim. Let's start with a model that, on the face of it, is not very interesting: $$ Y = f(x,Z)$$ where $x$ is a matrix of continuous explanatory variables and $Z$ is a random variable which we can think of as normally distributed around zero, but it does not have to be. If we take a first-order Taylor series approximation around $x_0$ , then we have: $$Y \approx f(x_0, 0) + \sum_{i = 1}^{p} f^{(i)}(x_0,0)(x_i-x_{i0}) + f^{(p+i)}(x_0,0)Z $$ or equivalently $$Y \approx \beta_0 + \sum_{i = 1}^{p} \beta_{i}(x_i-x_{i0}) + \sigma Z $$ Note that it is common practice to subsume all the $x_{i0}$ into the intercept and then the model takes on a very familiar form. At this point we could naturally discuss whether a global intercept is a good idea, and whether centring the data could be of value. If we continue with the Taylor series, the next approximation will be: $$Y \approx \beta_0 + \sum_{i = 1}^{p} \beta_{i}(x_i-x_{i0}) + \sum_{i = 1}^{p}\sum_{j = 1}^{p} \beta_{ij}(x_i-x_{i0})(x_j-x_{j0}) + \left( \sigma + \sum_{i = 1}^{p} \gamma_i(x_i-x_{i0}) \right) Z + \sigma Z^2 $$ and so now we find: nonlinearty in the main effect (quadratic terms) a linear x linear interaction (cross product of two linear terms) heteroskedasticity (the $(x_i-x_{i0}) Z$ terms) skewness (the term in $Z^2$ ) So this brings us back to the question about whether we should include quadratic terms when we include an interaction, and this approach tells us that we should. In general I think it is always a good idea to consider quadratic terms when fitting an interaction. Perhaps a good question to ask is why should we not do so ? I always encourage students and colleagues to step back and look at what we are doing from a wider viewpoint. Statistical models are models , an abstraction of reality, which as George Box famously said, are all wrong, but some are useful. It is our job to make them as useful as possible, whether that be for prediction or inference. It might very well be the case that in a particular context (eg a small region of the domain of $x$ ) that the nonlinear (and/or) interaction terms will not be needed, but at the very least it is a good idea to think about this, and the same goes for heteroskedasticity and skewness. Source for the above: Venables, W.N., 1998, October. Exegeses on linear models . In S-Plus Userâ€™s Conference, Washington DC. http://www.stats.ox.ac.uk/pub/MASS3/Exegeses.pdf
