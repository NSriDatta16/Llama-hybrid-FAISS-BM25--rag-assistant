[site]: datascience
[post_id]: 96671
[parent_id]: 96665
[tags]: 
For this, you may consult paperswithcode in the NER category , where you will see that in most datasets multiple top performers are BERT-based solutions. This approach normally consists of taking a pre-trained neural network model that is some variation of the BERT architecture (Transformer encoder trained on a masked language model loss) and fine-tuning it on your data. You can have a look at the Huggingface Transformers python library , which is well suited for this.
