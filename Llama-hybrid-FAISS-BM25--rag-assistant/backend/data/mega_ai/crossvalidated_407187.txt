[site]: crossvalidated
[post_id]: 407187
[parent_id]: 407185
[tags]: 
One example of the quote you give is the use of standardization in PCA (and thus also PCR): Variables measured on a different scale will by definition have a higher variance, causing PCA to attribute greater importance to such variables. In this case, whether you standardize or not says something about whether variables with greater variance are indeed more 'important', or simply measured on a different scale. A similar argument can be made for normalization in other methods. Another reason is numerical stability. Very small numbers may be hindered in their precision by the machine precision. Very high numbers may hinder the speed of computation. As to why algorithms can't take this into consideration by themselves: How else would an algorithm do so other than by normalizing or standardizing?
