[site]: stackoverflow
[post_id]: 3033608
[parent_id]: 3031582
[tags]: 
Resolving all of those subdirectories takes time. You're over-taxing the file-system. Maybe instead of using the directory tree, you could instead encode the path information into the file name, so instead of creating a file with a path like this: /parent/00/01/02/03/04/05/06/07 /08/09/0A/0B/0C/0D/0E/0F /10/11/12/13/14/15/16/17 /18/19/1A/1B/1C/1D/1E/1F.txt ...you could create a file with a path like this: /parent/00_01_02_03_04_05_06_07_ 08_09_0A_0B_0C_0D_0E_0F_ 10_11_12_13_14_15_16_17_ 18_19_1A_1B_1C_1D_1E_1F.txt ...of course, you'll still have a problem, because now all of your ten million files will be in a single directory, and in my experience (NTFS), a directory with more than a few thousand files in it still over-taxes the file-system. You could come up with a hybrid approach: /parent/00_01_02_03/04_05_06_07 /08_09_0A_0B/0C_0D_0E_0F /10_11_12_13/14_15_16_17 /18_19_1A_1B/1C_1D_1E_1F.txt But that will still give you problems if you exhaustively create all those directories. Even though most of those directories are "empty" (in that they don't contain any files ), the operating system still has to create an INODE record for each directory, and that takes space on disk. Instead, you should only create a directory when you have a file to put into it. Also, if you delete all the files in any given directory, then delete the empty directory. How many levels deep should you create the directory hierarchy? In my little example, I transformed your 32-level hierarchy into an 8-level hierarchy, but after doing some testing, you might decide on a slightly different mapping. It really depends on your data, and how evenly those paths are distributed through the combinatorial solution space. You need to optimize a solution with two constraints: 1) Minimize the number of directories you create, knowing that each directory becomes an INODE in the underlying file-system, and creating too many of them will overwhelm the file system. 2) Minimize the number of files in each directory, knowing that having too many files per directory (in my experience, more than 1000) overwhelms the file-system. There's one other consideration to keep in mind: Storage space on disks is addressed and allocated using "blocks". If you create a file smaller than the minimum block size, it nevertheless consumes the whole block, wasting disk space. In NTFS, those blocks are defined by their "cluster size" (which is partially determined by the overall size of the volume), and usually defaults to 4kB: http://support.microsoft.com/kb/140365 So if you create a file with only one byte of data, it will still consume 4kB worth of disk space, wasting 4095 bytes. In your example, you said you had about 10 million files, with about 1gB of data. If that's true, then each of your files is only about 100 bytes long. With a cluster size of 4096, you have about a 98% space-wasted ratio. If at all possible, try to consolidate some of those files. I don't know what kind of data they contain, but if it's a text format, you might try doing something like this: [id:01_23_45_67_89_AB_CD_EF] lorem ipsum dolor sit amet consectetur adipiscing elit [id:fe_dc_ba_98_76_54_32_10] ut non lorem quis quam malesuada lacinia [id:02_46_81_35_79_AC_DF_BE] nulla semper nunc id ligula eleifend pulvinar ...and so on and so forth. It might look like you're wasting space with all those verbose headers, but as far as the disk is concerned, this is a much more space-efficient strategy than having separate files for all those little snippets. This little example used exactly 230 bytes (including newlines) for three records, so you might try to put about sixteen records into each file (remembering that it's much better to have slightly less than 4096 bytes-per-file than to have slightly more than 4096, wasting a whole extra disk block). Anyhow, good luck!
