[site]: crossvalidated
[post_id]: 363157
[parent_id]: 
[tags]: 
Bayesian inference and regression

We have data D = (x1, x2,...., y) and try to understand what are the impacts of the features giving to y . From Baye's rule, we can simply have: P(y|x1, x2,...) = P(x1, x2, ....|y)P(y)/p(x1)p(x2).. here we assume x1 , x2 ,.. are all independent each other. The likelihood P(x1,x2,...|y) is simply obtained from data, and we can describe it by a distribution. Meanwhile, we use Jeffrey's prior P(x1) , P(x2) , ... Once we have the posterior P(y|x1,x2,..) we can find expectation value of y . I didn't see anything wrong here, but my question is, where the model is? Is this Naive Bayes regression? Or there is terminology for this?
