ta, statutory definitions for algorithmic discrimination, automation bias, and equity being cancelled, cuts to NIST and 19% of FDA workforce eliminated. Europe Other countries have implemented data protection regulations, more specifically with company privacy invasions. In Denmark, the Danish Expert Group on data ethics has adopted recommendations on "Data for the Benefit of the People". These recommendations are intended to encourage the responsible use of data in the business sector, with a focus on data processing. The recommendations include a focus on equality and non-discrimination with regard to bias in AI, as well as human dignity which is to outweigh profit and must be respected in all data processes. The European Union has implemented the General Data Protection Regulation (GDPR) to protect citizens' personal data, which applies to the use of AI in healthcare. In addition, the European Commission has established guidelines to ensure the ethical development of AI, including the use of algorithms to ensure fairness and transparency. With GDPR, the European Union was the first to regulate AI through data protection legislation. The Union finds privacy as a fundamental human right, it wants to prevent unconsented and secondary uses of data by private or public health facilities. By streamlining access to personal data for health research and findings, they are able to instate the right and importance of patient privacy. In March 2024, the European Union approved the pivotal Artificial Intelligence Act (AI Act). The regulation applies to European companies and organizations, and foreign providers of AI systems in the EU market. The EU AI Act has a risk-based structure, where AI enabled medical devices are in the "high-risk" category, the highest risk category of permitted uses for AI. In the United States, the Health Insurance Portability and Accountability Act (HIPAA) requires organizations to protect the privacy and security of patient information. The Centers for Medicare and Medicaid Services have also released guidelines for the development of AI-based medical applications. In 2025, Europe was leading the USA on AI regulation, while lagging in innovation and at least one California-based biotech company was "engaging the European Medicines Agency earlier in development than previously anticipated to mitigate concerns about the FDA's ability to meet development timelines." Ethical concerns While research on the use of AI in healthcare aims to validate its efficacy in improving patient outcomes before its broader adoption, its use may introduce several new types of risk to patients and healthcare providers, such as algorithmic bias, Do not resuscitate implications, and other machine morality issues. AI may also compromise the protection of patients' rights, such as the right to informed consent and the right to medical data protection. Privacy and data collection In order to effectively train machine learning systems and use them in healthcare, massive amounts of data must be gathered. Acquiring this data, however, comes at the cost of patient privacy, which can be controversial. For example, a survey conducted in the UK estimated that 63% of the population is uncomfortable with sharing their personal data in order to improve AI technology. The scarcity of real, accessible patient data is a hindrance that deters the progress of developing and deploying more AI in healthcare. The lack of regulations surrounding AI in the United States has generated concerns about mismanagement of patient data, such as with corporations utilizing patient data for financial gain. For example, as of 2020, the Swiss healthcare company Roche reportedly purchased healthcare data for approximately 2 million cancer patients at an estimated total cost of $1.9 billion. This generated ethical concerns about whether it was fair to sell patients' data, even considering the benefits. Ultimately, the current potential of AI in healthcare is additiona