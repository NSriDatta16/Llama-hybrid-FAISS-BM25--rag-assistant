[site]: datascience
[post_id]: 87088
[parent_id]: 87085
[tags]: 
A reproducible example with no data leakage: In there I'm scaling the data only with the train data on the k-fold stage import numpy as np from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split, cross_val_score from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer X, y = load_iris(return_X_y= True) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state = 42) model = Pipeline([("imputing", SimpleImputer(strategy= "mean")),("scaling", StandardScaler()), ("modeling", LogisticRegression(random_state= 42, class_weight= "balanced"))]).fit(X_train, y_train) cv_scores = cross_val_score(estimator= model, X = X_train, y = y_train, scoring= "accuracy") print(f"Mean accuracy cv: {np.mean(cv_scores)}") model.score(X_test, y_test) Note that in this case it is easy to apply all the pipeline because all the feature are the same data type, but imagine you have categorical and continuous features, so you need to apply different preprocessing and imputing to each. In that case a combination of ColumnTransformer and Pipeline would do the job. For reference check: https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html
