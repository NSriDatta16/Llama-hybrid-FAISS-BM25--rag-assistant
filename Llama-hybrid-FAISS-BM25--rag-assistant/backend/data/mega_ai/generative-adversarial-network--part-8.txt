pace Ω Z {\displaystyle \Omega _{Z}} to the image space Ω X {\displaystyle \Omega _{X}} . This can be understood as a "decoding" process, whereby every latent vector z ∈ Ω Z {\displaystyle z\in \Omega _{Z}} is a code for an image x ∈ Ω X {\displaystyle x\in \Omega _{X}} , and the generator performs the decoding. This naturally leads to the idea of training another network that performs "encoding", creating an autoencoder out of the encoder-generator pair. Already in the original paper, the authors noted that "Learned approximate inference can be performed by training an auxiliary network to predict z {\displaystyle z} given x {\displaystyle x} ". The bidirectional GAN architecture performs exactly this. The BiGAN is defined as follows: Two probability spaces define a BiGAN game: ( Ω X , μ X ) {\displaystyle (\Omega _{X},\mu _{X})} , the space of reference images. ( Ω Z , μ Z ) {\displaystyle (\Omega _{Z},\mu _{Z})} , the latent space. There are 3 players in 2 teams: generator, encoder, and discriminator. The generator and encoder are on one team, and the discriminator on the other team. The generator's strategies are functions G : Ω Z → Ω X {\displaystyle G:\Omega _{Z}\to \Omega _{X}} , and the encoder's strategies are functions E : Ω X → Ω Z {\displaystyle E:\Omega _{X}\to \Omega _{Z}} . The discriminator's strategies are functions D : Ω X → [ 0 , 1 ] {\displaystyle D:\Omega _{X}\to [0,1]} . The objective function is L ( G , E , D ) = E x ∼ μ X [ ln ⁡ D ( x , E ( x ) ) ] + E z ∼ μ Z [ ln ⁡ ( 1 − D ( G ( z ) , z ) ) ] {\displaystyle L(G,E,D)=\mathbb {E} _{x\sim \mu _{X}}[\ln D(x,E(x))]+\mathbb {E} _{z\sim \mu _{Z}}[\ln(1-D(G(z),z))]} Generator-encoder team aims to minimize the objective, and discriminator aims to maximize it: min G , E max D L ( G , E , D ) {\displaystyle \min _{G,E}\max _{D}L(G,E,D)} In the paper, they gave a more abstract definition of the objective as: L ( G , E , D ) = E ( x , z ) ∼ μ E , X [ ln ⁡ D ( x , z ) ] + E ( x , z ) ∼ μ G , Z [ ln ⁡ ( 1 − D ( x , z ) ) ] {\displaystyle L(G,E,D)=\mathbb {E} _{(x,z)\sim \mu _{E,X}}[\ln D(x,z)]+\mathbb {E} _{(x,z)\sim \mu _{G,Z}}[\ln(1-D(x,z))]} where μ E , X ( d x , d z ) = μ X ( d x ) ⋅ δ E ( x ) ( d z ) {\displaystyle \mu _{E,X}(dx,dz)=\mu _{X}(dx)\cdot \delta _{E(x)}(dz)} is the probability distribution on Ω X × Ω Z {\displaystyle \Omega _{X}\times \Omega _{Z}} obtained by pushing μ X {\displaystyle \mu _{X}} forward via x ↦ ( x , E ( x ) ) {\displaystyle x\mapsto (x,E(x))} , and μ G , Z ( d x , d z ) = δ G ( z ) ( d x ) ⋅ μ Z ( d z ) {\displaystyle \mu _{G,Z}(dx,dz)=\delta _{G(z)}(dx)\cdot \mu _{Z}(dz)} is the probability distribution on Ω X × Ω Z {\displaystyle \Omega _{X}\times \Omega _{Z}} obtained by pushing μ Z {\displaystyle \mu _{Z}} forward via z ↦ ( G ( x ) , z ) {\displaystyle z\mapsto (G(x),z)} . Applications of bidirectional models include semi-supervised learning, interpretable machine learning, and neural machine translation. CycleGAN CycleGAN is an architecture for performing translations between two domains, such as between photos of horses and photos of zebras, or photos of night cities and photos of day cities. The CycleGAN game is defined as follows:There are two probability spaces ( Ω X , μ X ) , ( Ω Y , μ Y ) {\displaystyle (\Omega _{X},\mu _{X}),(\Omega _{Y},\mu _{Y})} , corresponding to the two domains needed for translations fore-and-back. There are 4 players in 2 teams: generators G X : Ω X → Ω Y , G Y : Ω Y → Ω X {\displaystyle G_{X}:\Omega _{X}\to \Omega _{Y},G_{Y}:\Omega _{Y}\to \Omega _{X}} , and discriminators D X : Ω X → [ 0 , 1 ] , D Y : Ω Y → [ 0 , 1 ] {\displaystyle D_{X}:\Omega _{X}\to [0,1],D_{Y}:\Omega _{Y}\to [0,1]} . The objective function is L ( G X , G Y , D X , D Y ) = L G A N ( G X , D X ) + L G A N ( G Y , D Y ) + λ L c y c l e ( G X , G Y ) {\displaystyle L(G_{X},G_{Y},D_{X},D_{Y})=L_{GAN}(G_{X},D_{X})+L_{GAN}(G_{Y},D_{Y})+\lambda L_{cycle}(G_{X},G_{Y})} where λ {\displaystyle \lambda } is a positive adjustable parame