[site]: crossvalidated
[post_id]: 459766
[parent_id]: 
[tags]: 
Optimizing neural networks for maximum utilization of data instead of faster training

A number of methods, some of which I'm familiar with, some not, are used to optimize neural networks for faster convergence. Tricks like batching, different backpropagation algorithms, and so on speed training, often at the cost of extracting less information from each new training example. In other words, convergence will be faster, but the converged to results might be worse off. If I have more than enough computational power at my disposal for any given training task, but I'm working in a very data-constrained environment, how do I optimize my network for accuracy given less data and an overabundance of compute, rather than speed given less compute and an overabundance of data?
