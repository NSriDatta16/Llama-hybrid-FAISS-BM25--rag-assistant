[site]: crossvalidated
[post_id]: 473688
[parent_id]: 473667
[tags]: 
Before deep NNs, people used to engineer features manually. For example, to learn an image classifier, you could preprocess your images using Gabor filters or use SIFT, etc. These algorithms do not learn features, they are invented by people as general purpose tools to extract features from raw data. By features, we mean data that is lower-dimensional than the raw data itself (that is why CNNs use pooling, to downsample the data) and that encode higher-level characteristics of the datapoint, for example the color or the edges of an object in an image. Deep learning is an approach to machine learning that does away with these fixed preprocessing step and learn features. The idea is that by using feature extractors that are learned specifically for a task, the features suit the task better and the overall performance can be improved. Convolutions in themselves are just a building block. If you remove convolutions and use fully-connected layers in a DNN, you still have a feature extraction step. You could consider that if you have a $n$ -layers DNN classifier, the $n-1$ layers constitute a feature extractor. The last layer is a linear classifier that operate on these complex, task-specific, learned features.
