[site]: crossvalidated
[post_id]: 66800
[parent_id]: 66799
[tags]: 
The current go to tool for performing ridge regression in R (in my estimation) is the glmnet package. Default behavior is lasso regression: for ridge regression, set alpha=0 when you call the glmnet or cv.glmnet functions for your regression. Setting alpha to any other value between 0 and 1 gives an elastic net penalty. Additionally, base R has the lm.ridge function. UPDATE: If you want to homebrew your own ridge regression, you should probably start by picking which of the 16 parameter estimation methods you've come across you want to work with. Beyond that advice, your question is extremely broad and I don't really know what you want to hear. A very simple way you could implement ridge regression would be to do a bayesian linear regression with normal priors centered at 0. Otherwise you will probably need to apply an optimization technique to the RSS loss function augmented with regularization penalty. I'm guessing the parameter estimation techniques you have read about largely pertain to this optimization problem.
