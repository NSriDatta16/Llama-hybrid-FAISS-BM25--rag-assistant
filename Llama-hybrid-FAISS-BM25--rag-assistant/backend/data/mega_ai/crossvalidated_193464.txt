[site]: crossvalidated
[post_id]: 193464
[parent_id]: 193313
[tags]: 
Note: This might not serve as an answer, yet I want to point out some points that should be recognized for such a problem. The first thing I recognized is that this problem seems like a classical problem of model selection. So the easiest way is to screen using something like AIC and BIC. A more advanced way of doing this is to use methods from model selections. In modern texts, by modern I mean after E.P.Box, we usually do not separate some effect but talk about the regression function as a whole. A naive example is that we use some k-cutoff Fourier basis consisting of only k base functions instead of all $sin(nx),cos(nx)$ to model a regression function. We can give some interpretation of each term after fitting the regression model. Say for $k=4$, the term $a_0 + a_1\cdot sin(x) + b_1\cdot cos(x)$ is considered as "main effect" while the higer order term "$a_2\cdot sin(2x)+b_2\cdot cos(2x)$" as "interaction". However true, what we care is the regression function itself instead of the meaning of each term. The very basics can be found in Rigollet's Notes[3] which talked a lot about some penalized methods and the bound imposed on them. Now, what I am talking about is that we can penalize the form of regression function for its smoothness like [2]. A readable introduction can be found at [1]. What I meant in the comment to your question in the previous comment is [2], or we can even use the Bayesian model selection mentioned in [1]. If you have already determine that you must select from those 3 forms of interactions, that is find since you can run a model selection procedure directly and see how well each model fits; if you have not yet determine that, you would probably want to write a model in the form $Y=\alpha X_1 +\beta X_2 +f +\epsilon$ where the regression function is actually $\alpha X_1 +\beta X_2 +f$ in which $\alpha X_1,\beta X_2$ are the main-effect indicators. Then you can regard $f$ as some "interaction" in $\mathcal{C}^2$ or $L^{2}$ depending on how much freedom you want. The methods I referred to above will work neatly to give you the "best" $f\in\mathcal{C}^2$ or $L^{2}$ under MSE risk. As you indicated, logistic model is also treated quite well in this framework. But I think such a selection with a risk other than MSE is still under research. However, @Frank Harrell pointed out something worth noticing. If you decided to interpolate that, close attetion to the choice of cuttings is needed. Reference [1] http://www.stat.ufl.edu/archived/casella/Papers/BL-Final.pdf [2] https://cran.r-project.org/web/packages/penalized/vignettes/penalized.pdf [3] http://www-math.mit.edu/~rigollet/PDFs/RigNotes15.pdf
