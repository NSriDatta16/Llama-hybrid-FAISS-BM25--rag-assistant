[site]: crossvalidated
[post_id]: 621761
[parent_id]: 
[tags]: 
Why do we expect the eigenvalues of the Gramian matrix found by Maximum Variance Unfolding to capture the number of degrees of freedom in the data?

Maximum Variance Unfolding (MVU) is a manifold learning method which, like other forms of dimensionality reduction, makes the assumption that whatever (high-dimensional) data we're dealing with "lives" on a much lower-dimensional manifold. Formally, we'd like to map the inputs $\{x_{i}\}_{i=1}^{K}\in\mathbb{R}^D$ to outputs $\{y_{i}\}_{i=1}^{K}\in\mathbb{R}^d$ , with $d\ll D$ . The formulation of the MVU problem is as such: $$ \begin{align*} \max_{y_{1},\dotsc,y_{K}} \quad & \sum_{k=1}^{K}||y_{k}||_{2}^{2}\\ \textrm{s.t.} \quad & ||y_{k}-y_{l}||_{2}^{2}=||x_{k}-x_{l}||_{2}^{2},\quad k\sim l\\ \quad & \frac{1}{K}\sum_{k=1}^{K}y_k=0 \end{align*} $$ Intuitively, the objective function tries to "unfold" the manifold in the target space. At the same time, for each $k$ -th data point, we wish to preserve the (euclidean) distance to its $l$ -nearest neighbors. Finally, we zero-center the data in the target space for numerical stability. This non-convex problem can be approximated by the following semi-definite program [1]: $$ \begin{align*} \max_{G} \quad & \text{tr}(G)\\ \textrm{s.t.} \quad & e_{k}^{\top}Ge_{k}-2e_{k}^{\top}Ge_{l}+e_{l}^{\top}Ge_{l}=||x_{k}-x_{l}||_{2}^{2},\quad k\sim l\\ \quad & \mathbf{1}^{\top}G\mathbf{1}=0\\ \quad & G\succeq 0 \end{align*} $$ Here, $G$ is the inner product matrix (or Gramian) of the data. After we have solved this problem, we can eigendecompose $G=Q\Lambda Q^\top$ , pick the $d$ most significant eigenvalues from $\Lambda$ (e.g., the eigenvalues that account for 95% of its trace) and the corresponding eigenvectors, and obtain the low-dimensional embeddings $Y=Q_{d}\Lambda_{d}^{1/2}$ . Here, $Q_d$ contains the first $d$ columns of $Q$ , which correspond to the $d$ greatest eigenvalues; likewise for $\Lambda_d$ . Furthermore, the number of significant eigenvalues of the inner product matrix $G$ is the number of degrees of freedom in the data as discovered by MVU [2]. Question : Why should we expect that the number of significant eigenvalues of $G$ reveal the number of underlying degrees of freedom in the data? Thinking of matrices as linear transformations between vector spaces, I'm used to look at eigenvectors as "special" vectors in the original space that don't get rotated by the transformation, and eigenvalues as the factors by which they are scaled. I am failing to understand the geometric intuition behind the eigendecomposition of the inner product matrix $G$ discovered by MVU. [1] K. Q. Weinberger and L. K. Saul, "Unsupervised learning of image manifolds by semidefinite programming," Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004., Washington, DC, USA, 2004, pp. II-II, doi: 10.1109/CVPR.2004.1315272. [2] K Q. Weinberger and L. K. Saul. 2006. An introduction to nonlinear dimensionality reduction by maximum variance unfolding. In Proceedings of the 21st national conference on Artificial intelligence - Volume 2 (AAAI'06). AAAI Press, 1683â€“1686.
