[site]: crossvalidated
[post_id]: 448766
[parent_id]: 448757
[tags]: 
Feature engineering: working with the available data in order to create/transform good predictors (your $X$ ). This can usually be done by transforming, averaging, combining, etc. the available columns of your database, in order to obtain the predictors that are the most meaningful (or just work better) for the problem at hand. This can also include feature selection - where we try to reduce the number of predictors to simplify our model. Hyperparameter Optimization: the choice of the correct hyperparameters for your model. Hyperparameters are parameters that are specific to a statistical/ML model and that need to be set up before the learning process begins . These generally will dictate the behavior of your model, such as convergence speed, complexity etc. Examples are regularization coefficients (Lasso, Ridge), structural parameters (Number of layers of a Neural Net, number of neurons in each layer, Depth of a decision tree, etc.), loss/metrics (Optimizing L2/L1 loss, or Accuracy/LogLoss/AUC), and many more depending on the model you're using. Hyperparameters are opposed to normal parameters, such as the $w$ of your $y=wx + b$ equation, that are instead learned during the training process, by looking at the data. The two things are separate - the first one focus on the data and the variables you have, while the second one on the setup of your algorithm. However, some feature engineering, and in particular feature selection, can be included in the Optimization of the Hyperparameters . Indeed, the number of features to be included in the model can often be treated as a hyperparameter (in the sense that it often needs to be set up before the training), and it can be optimized within the same optimization procedure (Random Search + Cross Validation, most often) jointly with the intrinsic hyperparameters of the algorithm.
