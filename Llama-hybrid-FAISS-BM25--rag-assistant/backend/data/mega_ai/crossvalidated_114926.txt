[site]: crossvalidated
[post_id]: 114926
[parent_id]: 
[tags]: 
Generating even-sized clusters in scikit-learn

I'm attempting to generate approximately even-sized clusters of a PCA'd feature set in Scikit-learn, but I'm not having any luck. I'm only familiar with KMeans clustering, and with that algorithm the largest cluster contains the majority of the examples (in the case of K=2 it's 80%, in K=4 it's 65%, etc...). I've done some basic experiments with default MeanShift and AffinityPropagation, but I've had no luck there. This is where a graduate degree would have come in handy, but in the meantime can anyone point me in the direction of some good resources on what types of clustering algorithms can control cluster size (specifically any that are implemented in sklearn!) I realize this question is super vague, but I'm not sure what information is relevant to the problem. My data set begins as a combination of normalized continuous variables and one-hot encoding for categorical variables. 36 original features are reduced with PCA to 20 features that describe 99%+ of the variance. Attempts to modify my pre-PCA data set don't really effect how the clustering divides up the examples. Thanks for any suggestions/input!
