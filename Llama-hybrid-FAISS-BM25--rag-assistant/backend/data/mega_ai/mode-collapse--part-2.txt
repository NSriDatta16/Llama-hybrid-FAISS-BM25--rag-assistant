complishes the specific task, but loses ability to generate other forms of text. It may also be able to generate a smaller subset of texts that accomplish the specific task. It is hypothesized that there is a tradeoff between quality and diversity. Given a single pretrained model, one may finetune it to perform a specific task. More finetuning would result in higher average task performance, but less diverse outputs. Less finetuning would result in lower average performance, but more diverse outputs. A similar tradeoff has been observed in image generation models and GAN-based text generators. Similarly, mode collapse may occur during RLHF, via reward hacking the reward model or other mechanisms. See also Variational autoencoder Generative model Generative artificial intelligence Generative pre-trained transformer Overfitting == References ==