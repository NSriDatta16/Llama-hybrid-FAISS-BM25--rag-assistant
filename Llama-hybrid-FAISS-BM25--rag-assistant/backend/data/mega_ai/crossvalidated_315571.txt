[site]: crossvalidated
[post_id]: 315571
[parent_id]: 315565
[tags]: 
There are two ways in which gradient descent may be inefficient. Interestingly, they each lead to their own method for fixing up, which are nearly opposite solutions. The two problems are: (1) Too many gradient descent updates are required. (2) Each gradient descent step is too expensive. In regards to (1), comparing gradient descent with methods that take into account information about the second order derivatives, gradient descent tends to be highly inefficient in regards to improving the loss at each iteration. A very standard method, Newton's Method , generally takes much fewer iterations to converge, i.e. for logistic regression, 10 iterations of Newton's Method will often have lower loss than the solution provided by 5,000 iterations of gradient descent. For linear regression, this is even more extreme; there's a closed form solution! However, as the number of predictors gets very large (i.e. 500+), Newton's Method/directly solving for linear regression can become too expensive per iteration due to the amount of matrix operations required, while gradient descent will have considerably less cost per iteration. In regards to (2), it is possible to have such a large dataset that each iteration of gradient descent is too expensive to calculate. Computing the gradient will require $O(nk)$ operations ($n $ = sample size, $k$ = number of covariates). While $n = 10^{6}$ is not a problem at all on modern computers for values of $k stochastic gradient descent . I say that these fixes are nearly opposite, in that something like Newton's method is more costly but more efficient (in terms in of change in loss) per update, while stochastic gradient descent is actually less efficient but much computationally cheaper per update.
