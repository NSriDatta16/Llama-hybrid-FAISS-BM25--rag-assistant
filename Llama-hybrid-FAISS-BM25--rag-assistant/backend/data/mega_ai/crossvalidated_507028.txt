[site]: crossvalidated
[post_id]: 507028
[parent_id]: 
[tags]: 
Drawing the Line between a Model and a parameters in Bayesian Model selection

In Bayesian Model Selection, we first compute the evidence: $$ p(D|m) = \int_{\theta} p(D|\theta) p(\theta|m) d\theta $$ Then, we select the model that maximizes, which is a MAP estimator of the model. $$ p(m|D) = \frac{p(D|m) p(m)}{\sum_m p(m,D)} $$ If we assume the prior of the models $p(m)$ is uniform, this equates to selecting the model that maximizes $p(D|m)$ . This is all fine. But here is my question: how do we draw the line between parameters $\theta$ and model $m$ . A more complex model can include simpler models. For example, a polynomial with 10 degrees of freedom clearly includes a polynomial with 3 degrees of freedom as we can just set the appropriate coefficients to 0 to get to the latter. Conversely, we can restrict parameters to a small set and treat each set as a model. At an extreme, we can say that every unique parameter $\theta$ value is a new model. In this sense, $$ p(D|m) = p(D|\theta) $$ But here is the problem. We know that complex models tend to have larger likelihoods. So if we think of each unique parameter is a model, then the model that maximizes $p(D|m) = p(D|\theta)$ clearly overfits, defeating the purpose of model selection. Of course, this is an extreme case to illustrate my point. But I feel that the line between model and parameters is blurred. We can have a lot of models with restrictive parameters, or fewer models with parameters taking wider ranges. And how we draw the line seems to affect the result of Bayesian model selection. In particular, if we tilt the line towards more models with restrictive parameters, it seems more prone to overfitting. So in practice, how do people draw the line between models and parameters and go about Bayesian model selection? A side question: In both of the sources I cited below, evidence is computed as: $$ p(D|m) = \int_{\theta} p(D|\theta) p(\theta|m) d\theta $$ However, I wonder if the likelihood with regard to $\theta$ should be $p(D|\theta, m)$ instead of $p(D|\theta)$ . I am not sure why it is considered not dependent on $m$ . References: http://alumni.media.mit.edu/~tpminka/statlearn/demo/ MURPHY, KEVIN P. MACHINE LEARNING: a Probabilistic Perspective. MIT Press, 2012.
