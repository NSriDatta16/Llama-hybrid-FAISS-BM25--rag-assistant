[site]: crossvalidated
[post_id]: 258010
[parent_id]: 
[tags]: 
Understanding how the prior affects the estimate slope in Bayesian regression

I was reading this article on characterizing the posterior for a simple Bayesian linear regression (Gaussian posterior) when I saw the posterior mean and variance described thusly: \begin{split} \mu_n &= (X^TX+\sigma^2\Lambda)^{-1}X^Ty,\\ \Sigma_n &= \sigma^2(X^TX+\sigma^2\Lambda)^{-1}. \end{split} where $\Lambda$ is the precision matrix. Here is my question: does the term $\sigma^2 \Lambda$ only make the slope smaller? What exactly is going on here?
