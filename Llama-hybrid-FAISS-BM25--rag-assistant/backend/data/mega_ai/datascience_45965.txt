[site]: datascience
[post_id]: 45965
[parent_id]: 
[tags]: 
Neural Networks - Back Propogation

The following code is my implementation of neural network (1 hidden layer) trying to predict some number based on input data. Number of input node: 11 Number of nodes in hidden layer: 11 Number of nodes in output layer: 1 m : number of training examples, here = 4527 X : [11, m] matrix y : [1, m] matrix w1 : weights associated from input layer to hidden layer b1 : bias vector associated from input layer to hidden layer w2 : weights associated from hidden layer to output layer b2 : bias vector associated from hidden layer to output layer alpha : learning rate ite : number of iteration, here = 10000 Since I'm trying to predict a continuous value output, I'm using sigmoid function in input layers and identity function in output layer def propagate(X,y,w1,b1,w2,b2,alpha,ite): assert(X.shape[0] == 11) assert(y.shape[0] == 1) assert(X.shape[1] == y.shape[1]) m = X.shape[1] J = np.zeros(shape=(ite,1)) iteNo = np.zeros(shape=(ite,1)) for i in range(1,ite+1): z1 = np.dot(w1,X) + b1 a1 = sigmoid(z1) z2 = np.dot(w2,a1) + b2 dz2 = (z2-y)/m dw2 = np.dot(dz2,a1.T) db2 = np.sum(dz2, axis=1, keepdims=True) dz1 = np.dot(w2.T,dz2)*derivative_of_sigmoid(z1) dw1 = np.dot(dz1,X.T) db1 = np.sum(dz1, axis=1, keepdims=True) w2 = w2 - (alpha*dw2) b2 = b2 - (alpha*db2) w1 = w1 - (alpha*dw1) b1 = b1 - (alpha*db1) iteNo[i-1] = i J[i-1] = np.dot((z2-y),(z2-y).T)/(2*m) print(z2) return w1,b1,w2,b2,iteNo,J I have tried both the ways (With feature normalization and scaling & without) but my cost function varies as follows with respect number of iterations (Plotted J). On $x$ -axis: Number of iteration, On $y$ -axis: Error $\times 10^{12}$ . Please help!
