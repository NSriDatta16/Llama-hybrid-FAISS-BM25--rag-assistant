[site]: datascience
[post_id]: 62319
[parent_id]: 62314
[tags]: 
I don't use Python so I can't tell you exactly what is going on but I had a quick look at your data: A few remarks: it looks like the vast majority of the points are created artificially by interpolation. Why not, but that's unlikely to reflect the reality of the price changes: I would expect much more variation/noise in a real dataset about car prices. there's no need to add so many points to the dataset anyway. With all this data there's not even a need to train a model, since virtually every possible instance is already in the data. It seems to me that there's something weird with the prediction year: normally the higher the prediction year the more the price decreases right? here year 0 has no decrease at all, year 1 has the highest decrease,..., and year 4 has little to no decrease. That could confuse the model. because it's mostly an artificial dataset the relation is very simple: a basic regression future_price = past_price * a + b would already give quite good results, and the relation can be learned perfectly when adding the prediction_year feature. At least MLP and random forest should give near perfect results. From a quick look at the code, I suspect that the problem has to do with the scaling. I'm not sure what is supposed to happen there since I'm not familiar with these functions: it might be that the predicted values need to be "un-scaled" at the end maybe? Anyway I don't think scaling it's needed at all here. For the record the graph was done with R like this: library(ggplot2) d
