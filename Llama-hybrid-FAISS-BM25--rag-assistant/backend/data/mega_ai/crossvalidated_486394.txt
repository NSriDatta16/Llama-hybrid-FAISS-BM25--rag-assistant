[site]: crossvalidated
[post_id]: 486394
[parent_id]: 485488
[tags]: 
To go directly to the answer, the loss does have a precise derivation (but that doesn't mean you can't necessarily change it). It's important to remember that Variational Auto-encoders are at their core a method for doing variational inference over some latent variables we assume to be generating the data. In this framework we aim to minimise the KL-divergence between some approximate posterior over the latent variables and the true posterior, which we can alternatively do my maximising the evidence lower bound (ELBO), details in the VAE paper . This gives us the objective in VAEs: $$ \mathcal{L}(\theta,\phi) = \underbrace{\mathbb{E}_{q_\phi}[\log p_\theta(x|z)]}_{\text{Reconstruction Loss}} - \underbrace{D_{KL}(q_\phi(z)||p(z))}_{\text{KL Regulariser}} $$ Now the reconstruction loss is the expected log-likelihood of the data given the latent variables. For an image which is made up of a number of pixels the total log-likelihood will be the sum of the log-likelihood of all of the pixels (assuming independence), not the average log-likelihood of each individual pixel which is why it's the case in the example. The question of whether you can add an extra parameter is an interesting one. DeepMind for example have introduced the $\beta$ -VAE , which does exactly this, albeit for a slightly different purpose - they show that this extra parameter can lead to a more disentangled latent-space that allows for more interpretable variables. How principled this change in objective is is up for debate, but it does work. That being said it is very easy to change the KL regulariser term in a principled way by simply changing your prior ( $p(z)$ ) on the latent variables, the original prior is a very boring standard normal distribution so just swapping in something else will change the loss function. You might even be able, though I haven't checked this myself, to specify a new prior ( $p'(z)$ ) such that: $$ D_{KL}(q_\phi(z)||p'(z)) = \lambda * D_{KL}(q_\phi(z)||p(z)), $$ which will do exactly what you want. So basically the answer is yes - feel free to change the loss function if it helps you do the task you want just be aware of how what you're doing is different to the original case so you don't make any claims you shouldn't.
