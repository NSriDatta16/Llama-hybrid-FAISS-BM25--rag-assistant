[site]: crossvalidated
[post_id]: 156758
[parent_id]: 156740
[tags]: 
The purpose of cross validation is to simulate how your model would perform on unseen data, including data that contains words or n-grams that were not contained in any of your training data. This motivates the following steps: Assign data to CV folds Create features based on training fold and train model Extract identical features from the testing dataset and apply the model Repeat for all k folds. The average error is an approximation of the generalized error rate. Incorporating outside data is not needed. Consider these possibilities: The word is in the training dataset: These words will already show up in your model without using outside data. The word is not in the training or the testing dataset: These words unnecessarily add more features to the dataset. If you remove any because they don't occur in the training or testing dataset, then you are using your testing dataset for feature selection which is not good practice and can bias your model. The word is only in the testing dataset: Even if this feature is in your training dataset because you added it from an outside corpus, all the values of feature will be set to zero for every observation in your training data. This means that there is no information that any possible model could extract from the feature being included in the training dataset. For all these possibilities, collecting an outside corpus adds additional work, without any benefit above standard cross validation. I would recommend setting the number of cv folds to be as high as your computing resources allow if you really want to minimize the impact of excluded features. Consider if you use 20 fold cv. Then for a feature to not appear in one of your folds, at worst it would occur in only 5% of your observations. But, the feature would be guaranteed to occur in the other 19 folds. In practice, the issue of leaving a few rare words out of a single cv does very little to the predictive accuracy of the model.
