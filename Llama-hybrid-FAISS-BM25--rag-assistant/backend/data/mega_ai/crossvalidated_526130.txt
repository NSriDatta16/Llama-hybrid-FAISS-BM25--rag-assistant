[site]: crossvalidated
[post_id]: 526130
[parent_id]: 
[tags]: 
How is the Q value translated to action in DRL framework presented in Gu, Shixiang et al (2016)

I was reading the paper Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Off-Policy Updates and I kind of understand everything. The results also seem very interesting, enabling fast learning in real-time for complex tasks in a real environment. In any case, I've been struggling to understand how the Q-value calculated is translated in this case to an action(this is, the angular speed of the joints, or desired joint position, ...) , especially since they are working in a continuous space, and not a discrete space, which is what I usually see when I read about or work in RL Q-Learning.
