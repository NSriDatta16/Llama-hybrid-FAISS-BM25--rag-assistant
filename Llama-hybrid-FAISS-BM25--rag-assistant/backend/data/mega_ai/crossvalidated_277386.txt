[site]: crossvalidated
[post_id]: 277386
[parent_id]: 
[tags]: 
Comparing methods of aggregating standard deviations

Background Problem One of the economic/finance software systems that I am using is adopting the following approach to aggregate standard deviations: Let $R_t$ be the time series of interest. The software estimates the standard deviation, $\sigma$, of the series in two steps. Step One: Compute a rolling standard deviation of $R_t$. More specifically, let $\hat{W}_j$ be the standard deviation estimator of $R_t$ for a time window of length $T_1$ (e.g., 3 months) ending in time $j$: \begin{equation} \hat{W}_j^2:=\frac{1}{T_1}\sum_{t=j-T_1+1}^{j}R_t^2 \end{equation} The software apparently assumes that $\mathbb{E}(R_t)=0 \text{ }\forall t$. Step Two: Take the numeric average of $\hat{W}_j$ as the final standard deviation estimator. More specifically, let $\hat{\sigma}_S$ be the standard deviation estimator calculated using $T_2$ (e.g., 5 years worth of) $\hat{W}_j$'s: \begin{equation} \hat{\sigma}_S := \frac{1}{T_2}\sum_{j=1}^{T_2}\hat{W}_j \end{equation} The final standard deviation estimator is $\hat{\sigma}_S$. I Disagree I disagree with this approach because $\hat{\sigma}_S$ is a biased estimator, assuming that $R_t$ are I.I.D. Proof: \begin{split} \mathbb{E}(\hat{\sigma}_S)=\mathbb{E}(\hat{W}_j) &= \mathbb{E}\Bigg(\sqrt{\frac{1}{T_1}\sum_{t=j-T_1+1}^{j}R_t^2}\Bigg) \\ & My Proposal I straightforwardly would like to use \begin{equation} \hat{\sigma}_M^2 := \frac{1}{T_2}\sum_{j=1}^{T_2}\hat{W}_j^2 \end{equation} Despite the fact that $\hat{\sigma}_M^2$ is an unbiased estimator of $\sigma^2$, $\hat{\sigma}_M$ is still a biased estimator of $\sigma$ (Jensen's, proof omitted). However, I am able to prove that $\hat{\sigma}_M$ has a smaller bias than $\hat{\sigma}_S$: Proof: \begin{split} \hat{\sigma}_S^2 - \hat{\sigma}_M^2 &= \Big( \frac{1}{T_2}\sum_{j=1}^{T_2}\hat{W}_j \Big)^2 - \frac{1}{T_2}\sum_{j=1}^{T_2}\hat{W}_j^2 \\ & = \frac{1}{T_2^2} \Big( -(T_2-1)\sum_{j=1}^{T_2}\hat{W}_j^2 + 2\sum_{i This trivially leads to the following three consequences: 1) $0 2) $0 3) $0 This isn't good enough My problem is that I don't know how the Mean Square Errors (MSEs) of the two estimators compare with each other. I tried comparing them using different ways, but each time I hit the same dead end. Attempt I: Direct Comparison \begin{split} \mathbb{E}\big( (\hat{\sigma}_S-\sigma)^2 - (\hat{\sigma}_M-\sigma)^2 \big) = \Big(\mathbb{E}(\hat{\sigma}_S^2)-\mathbb{E}(\hat{\sigma}_M^2)\Big) - 2\sigma\big(\mathbb{E}(\hat{\sigma}_S)-\mathbb{E}(\hat{\sigma}_M) \big) \end{split} This leads to nowhere because I can't decide the sign. Attempt II: Changing the Form \begin{split} \mathbb{E}\big( (\hat{\sigma}_S-\sigma)^2 - (\hat{\sigma}_M-\sigma)^2 \big) & = \mathbb{E}\big( (\hat{\sigma}_S - \hat{\sigma}_M) (\hat{\sigma}_S + \hat{\sigma}_M - 2\sigma) \big) \\ & = \text{Cov}(\hat{\sigma}_S - \hat{\sigma}_M, \hat{\sigma}_S + \hat{\sigma}_M) + \mathbb{E}(\hat{\sigma}_S - \hat{\sigma}_M) \mathbb{E}(\hat{\sigma}_S + \hat{\sigma}_M - 2\sigma) \\ & = \text{Var}(\hat{\sigma}_S) - \text{Var}(\hat{\sigma}_M) + \mathbb{E}(\hat{\sigma}_S - \hat{\sigma}_M) \mathbb{E}(\hat{\sigma}_S + \hat{\sigma}_M - 2\sigma) \end{split} This still leads to nowhere because I still can't decide the sign. Attempt III: Bias-variance Decomposition \begin{split} \mathbb{E}(\hat{\sigma}-\sigma)^2 = \text{Var}(\hat{\sigma}) + \mathbb{E}\big( \mathbb{E}(\hat{\sigma})-\sigma \big)^2 \end{split} This ends up comparing the variances of the two estimators again. Challenge 1) Are there other metrics to evaluate the two estimators? 2) How to deal with the variance of the square root of a random variable? Thank you!
