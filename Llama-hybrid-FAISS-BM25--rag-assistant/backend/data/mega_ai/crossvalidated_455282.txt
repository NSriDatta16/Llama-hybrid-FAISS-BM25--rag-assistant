[site]: crossvalidated
[post_id]: 455282
[parent_id]: 455279
[tags]: 
If you are training deep (>3 hidden layers) neural nets, I would advise you to use stochastic gradient descent, even if you can fit your whole dataset into a single batch. Rough SGD loop: Calculate gradient for each example in a batch (I recommend 32 examples). Average (mean) the gradient across examples in the batch. Change the weights in the direction of the gradient a little (learning rate), usually between 1e-3 and 1e-2. Pick a new batch and go to step 1. After you repeat this enough times (at least 1 epoch, probably more than 10) your loss will stabilise. There are some tricks to make it work better: You should also randomise the order in which you pick examples for good results. Add exponential averaging of gradient across batches (momentum). The reason for using SGD as opposed to plain Gradient Descent, is that if you gradient descent on the whole dataset you are likely to end up in a minimum which is specific to your dataset, but is not true in population (over-fitting). EDIT: Another important thing to remember: you need to initialize your weights randomly or your network might train poorly. In case of fully connected layers, if they have the same weights, then the gradients will be equal and all nodes be the same.
