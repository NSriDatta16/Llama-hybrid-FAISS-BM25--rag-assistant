[site]: datascience
[post_id]: 97106
[parent_id]: 
[tags]: 
What is the disadvantage of using a completely normalized training set for Deep learning?

Batch normalization is generally preferred in deep learning, which normalizes the output of the activation function in each layer (as an output from the cost function differs depends on the input). Instead, if the training set is normalized before passing through layers, does this solve the same problem?
