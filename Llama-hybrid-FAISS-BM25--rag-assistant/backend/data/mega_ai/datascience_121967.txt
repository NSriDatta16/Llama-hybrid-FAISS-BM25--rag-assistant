[site]: datascience
[post_id]: 121967
[parent_id]: 121841
[tags]: 
One option is to find the average of the word embeddings of each token in each descriptor. A pretrained word embedding model would be a useful way to start. Here is a rough start in Python: import gensim import gensim.downloader from gensim.utils import tokenize # Pretrained word embedding model model = gensim.downloader.load('glove-wiki-gigaword-300') # Sample descriptor from CUB 200 2011 dataset descriptor = "this bird has a short orange bill, white breast" # Split into tokens tokens = list(tokenize(descriptor)) # Create mean vector embedding of tokens vector = model.get_mean_vector(tokens)
