tic numerical methods model this uncertainty explicitly and allow for automated decisions and parameter tuning. Linear algebra Probabilistic numerical methods for linear algebra have primarily focused on solving systems of linear equations of the form A x = b {\displaystyle Ax=b} and the computation of determinants | A | {\displaystyle |A|} . A large class of methods are iterative in nature and collect information about the linear system to be solved via repeated matrix-vector multiplication v ↦ A v {\displaystyle v\mapsto Av} with the system matrix A {\displaystyle A} with different vectors v {\displaystyle v} . Such methods can be roughly split into a solution- and a matrix-based perspective, depending on whether belief is expressed over the solution x {\displaystyle x} of the linear system or the (pseudo-)inverse of the matrix H = A † {\displaystyle H=A^{\dagger }} . The belief update uses that the inferred object is linked to matrix multiplications y = A v {\displaystyle y=Av} or z = A ⊺ v {\displaystyle z=A^{\intercal }v} via b ⊺ z = x ⊺ v {\displaystyle b^{\intercal }z=x^{\intercal }v} and v = A − 1 y {\displaystyle v=A^{-1}y} . Methods typically assume a Gaussian distribution, due to its closedness under linear observations of the problem. While conceptually different, these two views are computationally equivalent and inherently connected via the right-hand-side through x = A − 1 b {\displaystyle x=A^{-1}b} . Probabilistic numerical linear algebra routines have been successfully applied to scale Gaussian processes to large datasets. In particular, they enable exact propagation of the approximation error to a combined Gaussian process posterior, which quantifies the uncertainty arising from both the finite number of data observed and the finite amount of computation expended. Ordinary differential equations Probabilistic numerical methods for ordinary differential equations y ˙ ( t ) = f ( t , y ( t ) ) {\displaystyle {\dot {y}}(t)=f(t,y(t))} , have been developed for initial and boundary value problems. Many different probabilistic numerical methods designed for ordinary differential equations have been proposed, and these can broadly be grouped into the two following categories: Randomisation-based methods are defined through random perturbations of standard deterministic numerical methods for ordinary differential equations. For example, this has been achieved by adding Gaussian perturbations on the solution of one-step integrators or by perturbing randomly their time-step. This defines a probability measure on the solution of the differential equation that can be sampled. Gaussian process regression methods are based on posing the problem of solving the differential equation at hand as a Gaussian process regression problem, interpreting evaluations of the right-hand side as data on the derivative. These techniques resemble to Bayesian cubature, but employ different and often non-linear observation models. In its infancy, this class of methods was based on naive Gaussian process regression. This was later improved (in terms of efficient computation) in favor of Gauss–Markov priors modeled by the stochastic differential equation d x ( t ) = A x ( t ) d t + B d v ( t ) {\displaystyle \mathrm {d} x(t)=Ax(t)\,\mathrm {d} t+B\,\mathrm {d} v(t)} , where x ( t ) {\displaystyle x(t)} is a ν {\displaystyle \nu } -dimensional vector modeling the first ν {\displaystyle \nu } derivatives of y ( t ) {\displaystyle y(t)} , and where v ( t ) {\displaystyle v(t)} is a ν {\displaystyle \nu } -dimensional Brownian motion. Inference can thus be implemented efficiently with Kalman filtering based methods. The boundary between these two categories is not sharp, indeed a Gaussian process regression approach based on randomised data was developed as well. These methods have been applied to problems in computational Riemannian geometry, inverse problems, latent force models, and to differential equations with a geometric structure such as s