[site]: stackoverflow
[post_id]: 957466
[parent_id]: 957320
[tags]: 
Sometimes the process of decreasing the learning rate over time is called "annealing" the learning rate. There are many possible "annealing schedules", like having the learning rate be a linear function of time: u(t) = c / t ...where c is some constant. Or there is the "search-then-converge" schedule: u(t) = A * (1 + (c/A)*(t/T)) / (1 + (c/A)*(t/T) + T*(t^2)/(T^2)) ...which keeps the learning rate around A when t is small compared to T (the "search" phase) and then decreases the learning rate when t is large compared to T (the "converge" phase). Of course, for both of these approaches you have to tune parameters (e.g. c , A , or T ) but hopefully introducing them will help more than it will hurt. :) Some references: Learning Rate Schedules for Faster Stochastic Gradient Search , Christian Darken, Joseph Chang and John Moody, Neural Networks for Signal Processing 2 --- Proceedings of the 1992 IEEE Workshop, IEEE Press, Piscataway, NJ, 1992. A Stochastic Approximation Method , Herbert Robbins and Sutton Monro, Annals of Mathematical Statistics 22, #3 (September 1951), pp. 400â€“407. Neural Networks and Learning Machines (section 3.13 in particular), Simon S. Haykin, 3rd edition (2008), ISBN 0131471392, 9780131471399 Here is a page that briefly discusses learning rate adaptation .
