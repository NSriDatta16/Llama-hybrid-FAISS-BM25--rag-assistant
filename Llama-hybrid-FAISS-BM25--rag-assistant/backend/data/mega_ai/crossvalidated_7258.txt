[site]: crossvalidated
[post_id]: 7258
[parent_id]: 
[tags]: 
Information content of examples and undersampling

As I have written in my question " How much undersampling should be done? ", I want to predict defaults, where a default is per se really unlikely (average ~ 0.3 percent). My models are not affected by the unequal distribution: It's all about saving computing time. Undersampling the majority class to a ratio [defaulting/non-defaulting examples] of 1:1 is the same as expressing the believe that I think examples are equally important in increasing the prediction quality. Does anyone know a reason why/when equal importance could not be the case? Is there literature on this specific topic (I could not find sampling-literature that modeling/computation-oriented)? Thanks a lot for your help!
