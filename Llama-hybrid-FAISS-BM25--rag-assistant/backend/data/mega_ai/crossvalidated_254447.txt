[site]: crossvalidated
[post_id]: 254447
[parent_id]: 254035
[tags]: 
Theoretically RBMs are undirected models with no connections between any observed variables or any latent variables, VAEs are directed models with continuous latent variables. The Deep Learning book (Chapter 20 Deep Generative Models) provides a very good summary of pros and cons of VAEs and all types of RBMs. To quote some The VAE framework is very straightforward to extend to a wide range of model architectures. This is a key advantage over Boltzmann machines, which require extremely careful model design to maintain tractability. The variational autoencoderis deﬁned for arbitrary computational graphs, which makes it applicable to a wider range of probabilistic model families because there is no need to restrict the choice of models to those with tractable mean ﬁeld ﬁxed point equations. One very nice property of the variational autoencoder is that simultaneously training a parametric encoder in combination with the generator network forces the model to learn a predictable coordinate system that the encoder can capture. This makes it an excellent manifold learning algorithm.
