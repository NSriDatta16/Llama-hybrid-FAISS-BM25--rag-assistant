[site]: crossvalidated
[post_id]: 18815
[parent_id]: 
[tags]: 
Increasing number of features results in accuracy drop but prec/recall increase

I am new to Machine Learning. At the moment I am using a Naive Bayes (NB) classifier to classify small texts in 3 classes as positive, negative or neutral, using NLTK and python. After conducting some tests, with a dataset composed of 300,000 instances (16,924 positives 7,477 negatives and 275,599 neutrals) I found that when I increase the number of features, the accuracy goes down but the precision/recall for positive and negative classes goes up. is this a normal behavior for a NB classifier? Can we say that it would be better to use more features? Some data: Features: 50 Accuracy: 0.88199 F_Measure Class Neutral 0.938299 F_Measure Class Positive 0.195742 F_Measure Class Negative 0.065596 Features: 500 Accuracy: 0.822573 F_Measure Class Neutral 0.904684 F_Measure Class Positive 0.223353 F_Measure Class Negative 0.134942 Thanks in advance... Edit 2011/11/26 I have tested 3 different feature selection strategies (MAXFREQ, FREQENT, MAXINFOGAIN) with the Naive Bayes classifier. First here are the Accuracy, and F1 Measures per class: Then I have plotted the train error and test error with an incremental training set, when using MAXINFOGAIN with the top 100 and the top 1000 features: So, it seems to me that although the highest accuracy is achieved with FREQENT, the best classifier is the one using MAXINFOGAIN, is this right ? When using the top 100 features we have bias (test error is close to train error) and adding more training examples will not help. To improve this we will need more features. With 1000 features, the bias gets reduced but the error increases...Is this ok ? Should I need to add more features ? I don't really know how to interpret this... Thanks again...
