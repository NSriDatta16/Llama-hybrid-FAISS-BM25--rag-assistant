[site]: crossvalidated
[post_id]: 148460
[parent_id]: 
[tags]: 
t-statistic vs two-sample t-tests

When and in which context is the t-statistic used? $ t = \frac{\hat{\beta} - \beta_0}{se(\hat{\beta})} $ where $\beta_0$ is a non-random, known constant, which defines the null-hypothesis in the form $H_0: \beta = \beta_0 $; and $se(\hat\beta)$ is the standard error of the $\beta$ I used to think that the above formulation should only be used to assess whether a parameter in a statistical model is statistically significant, but I have increasingly seen it used in other contexts. For example, assessing whether the results of an a/b test are statistically significant. Unfortunately I can not link to the documents where I have seen this use, but in a nutshell, I have seen this methodology being used: Take the daily sales of version a and b calculate the difference of the average of daily sales for the a and b variant (the time series don't necessarily have the same length) calculate the std deviation of the difference of the averages: $\sigma_{a-b}$ apply the above where $\hat{\beta}$ is set equal to the difference in averages and $se(\hat\beta)$ is replaced by $\sigma_{a-b}$ Using the above methodology: yes, we can build a linear model $y = a$ for the difference in average daily sales where $a$ is just the intercept yes, working with averages allows to use the central limit theorem to infer convergence to normality ...but I suspect something is lost along the way.. Is there a loophole in the methodology described above? What about the sample size and the tests for the difference in mean that apply to two samples ?
