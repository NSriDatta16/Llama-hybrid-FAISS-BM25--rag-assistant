[site]: crossvalidated
[post_id]: 142827
[parent_id]: 142810
[tags]: 
What you're seeing is a phenomenon called shrinkage , which is a fundamental property of mixed models; individual group estimates are "shrunk" toward the overall mean as a function of the relative variance of each estimate. (While shrinkage is discussed in various answers on CrossValidated, most refer to techniques such as lasso or ridge regression; answers to this question provide connections between mixed models and other views of shrinkage.) Shrinkage is arguably desirable; it is sometimes referred to as borrowing strength . Especially when we have few samples per group, the separate estimates for each group are going to be less precise than estimates that take advantage of some pooling from each population. In a Bayesian or empirical Bayesian framework, we can think of the population-level distribution as acting as a prior for the group-level estimates. Shrinkage estimates are especially useful/powerful when (as is not the case in this example) the amount of information per group (sample size/precision) varies widely, e.g. in a spatial epidemiological model where there are regions with very small and very large populations. The shrinkage property should apply to both Bayesian and frequentist fitting approaches -- the real differences between the approaches lie at the top level (the frequentist's "penalized weighted residual sum of squares" is the Bayesian's log-posterior deviance at the group level ...) The main difference in the picture below, which shows lme4 and MCMCglmm results, is that because MCMCglmm uses a stochastic algorithm the estimates for different groups with the same observed proportions differ slightly. With a little more work, I think we could figure out the precise degree of shrinkage expected by comparing the binomial variances for the groups and the overall data set, but in the meantime here's a demonstration (the fact that the J=10 case looks less shrunk than J=20 is just sampling variation, I think). (I accidentally changed the simulation parameters to mean=0.5, RE standard deviation=0.7 (on logit scale) ...) library("lme4") library("MCMCglmm") ##' @param I number of groups ##' @param J number of Bernoulli trials within each group ##' @param theta random effects standard deviation (logit scale) ##' @param beta intercept (logit scale) simfun
