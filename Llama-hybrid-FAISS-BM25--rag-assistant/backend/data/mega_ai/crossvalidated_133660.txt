[site]: crossvalidated
[post_id]: 133660
[parent_id]: 48434
[tags]: 
The failure mode you'll encounter is that, with enough random features, there will exist features that relate to the target within the bagged samples used for each tree but not within the larger dataset. A similar issue to that seen in multiple testing. Rules of thumb for this are hard to develop since the exact point at which this happens depends on the amount of noise and strength of the signal in the data. There also exist methods that address this by using multiple test corrected p-values as splitting criteria, doing a feature selection step based on variable importance and/or comparison of feature importances to artificial contrast features produced by randomly permutating the actual feature, use of out of bag cases to validate split selection and other methods. These can be extremely effective. I've used random forests (including some of the above methodological tweaks) on data sets with ~1000 cases and 30,000-1,000,000 features. (Data sets in human genetics with varying level of feature selection or engineering). They can certainly be effective in recovering a strong signal (or batch effect) in such data but don't do well piecing together something like a disease with heterogenous causes as the amount random variation overcomes each signal
