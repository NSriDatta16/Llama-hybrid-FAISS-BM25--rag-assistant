[site]: crossvalidated
[post_id]: 588754
[parent_id]: 
[tags]: 
Is it wrong to use sufficent statistc estimated from the data as a prior in Bayesian data analysis?

First I want to state that I got unexpected feedback from a reviewer in regard to my question and I am simply interested in others' views in this regard (I have already sent in my rebuttal). Suppose that we have $N$ observations of the random variable $Y$ and we know that they are iid. In addition, we believe them to be normally distributed. As such we have the following model: $$Y \sim N(\mu,\sigma)$$ Now, let's assume that $\sigma$ is known and for simplicity equal to 1. We then have a sample of $Y$ , $y$ . Now, if we assume the following model: $$y \sim N(\mu,1)$$ $$\mu \sim N(\mu_o+\eta\sigma_\mu,\sigma_\mu)$$ $$\eta\sim N(0,1)$$ and suppose that $\sigma_\mu$ is known. My question is: would it be circular to assume that $\mu_0=\bar{y}$ ? That is, we assume that mean of our prior on the mean is the observed sample mean. To me, it would seem like the sufficient statistic of $\mu$ would maximize the likelihood, but I am not sure. What are others' opinions? Edit: also, in my actual model, the prior on the variance inhibits a closed form solution. But I tried to simplify things.
