[site]: crossvalidated
[post_id]: 544861
[parent_id]: 513738
[tags]: 
In short, the n_samples in OP's weight_decay_term equation should be the size of the whole training set instead of mini batch. It was the same error I made. Below are the explanation of my mistakes and corrections: I had the same problem, restudy the equations, and found that in my coding, I mixed up the n , size of the whole training set, with the m , size of the mini batch. Based on my understanding, for the weight decay term calculation, n , size of the whole training set should be used. Whereas, m , size of the mini batch is used in the approximation of gradient of cost function w.r.t weights or biases using mini batch, which are $\frac{1}{m}\sum_x\frac{\partial C_x}{\partial w}$ and $\frac{1}{m}\sum_x\frac{\partial C_x}{\partial b}$ , and I assumed that you have covered in your self.dw and self.db calculations as it's not shown in your codes. I am following Michael Nielsen's convention so the terms might be different from yours but I think they are the same concept. With some algebra, my equation derivations are as follows: For weights: $$ v\rightarrow v'=\mu v-\eta\nabla C \\\therefore v\rightarrow v'=\mu v-\frac{\eta}{m}\sum_x\frac{\partial C_x}{\partial w}-\frac{\eta\lambda}{n}w \\w\rightarrow w'=w+v' $$ For biases: $$ v\rightarrow v'=\mu v-\eta\nabla C \\\therefore v\rightarrow v'=\mu v-\frac{\eta}{m}\sum_x\frac{\partial C_x}{\partial b} \\b\rightarrow b'=b+v' $$ where, $\nabla C$ = gradient of cost function w.r.t weights or biases, $\mu$ = momentum coefficient, $\eta$ = learning rate, $\lambda$ = regularization parameter in weight decay term, $m$ = size of mini batch, $n$ = size of the whole training set After the correction, with $\mu=0.5$ , $\eta=0.1$ , $\lambda=5$ , $m=10$ , the model managed to get around 97% of final evaluation accuracy with 100 epochs in the MNIST problem. $n$ will be size of the MNIST training set, which is $50000$ . Please let me know if I made mistakes in the explanation above, coz I am new to neural networks.
