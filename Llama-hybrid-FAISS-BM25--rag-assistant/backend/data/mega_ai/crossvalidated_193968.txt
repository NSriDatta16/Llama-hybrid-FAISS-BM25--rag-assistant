[site]: crossvalidated
[post_id]: 193968
[parent_id]: 193887
[tags]: 
An important point to make here is that classifying all of the patterns as $c_1$ might be the correct answer, from a statistical decision theory perspective. If misclassification costs are equal, we want to classify the pattern according to the class with the highest posterior probability, i.e. $p(C = c_i|\vec{x})$, which depends on the prior probabilities. So it may be that the prior probabilities of the minority classes are sufficiently low, and the distribution of patterns for each class sufficiently broad, that the true posterior probability is always highest for the majority class. If this is not acceptable for your application, then that probably means that your misclassification costs are not equal, so you need to think about what the costs of each kind of misclassification actually are, and build that into your cost function used to train the network. Note if your network outputs estimates of probability of class membership, then it is offten possible to post-process the output of the trained network, rather than pre-process and retrain - see the excellent boook by Chris Bishop ("Neural Networks for Pattern Recognition"). In my experience, it isn't so much a "class imbalance problem" per se, but that there are just too few patterns from the minority class to accurately estimate their distribution, and if you increase the size of the dataset (but keep the ratios the same) the problem often goes away.
