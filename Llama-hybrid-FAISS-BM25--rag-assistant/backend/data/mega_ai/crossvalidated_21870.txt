[site]: crossvalidated
[post_id]: 21870
[parent_id]: 21833
[tags]: 
Random forest is a machine learning algorithm proposed by Breiman in this paper (there is also a webpage about it). Its significant property is that it can calculate an importance measure for attributes showing more-less how they were useful for the model -- it is usually better than correlation with a decision or linear model coefficient significance since it can handle some nonlinearity and multiattribute interactions without blowing the roof with overfitting or combinatorical explosion, but it is obviously far from perfectly recreating underlying Bayes net. Now, this measure works quite well as a ranking of features, but it is not a complete answer for neither of feature selection problems -- one needs some cutoff to select minimal optimal (i.e. set of attributes on which model works best) and all relevant sets (i.e. set of attributes which are objectively connected to the decision). Minimal optimal problem is usually quite easy and can be done with recursive feature elimination or (even better) with some regularization-supporting algorithm. On the other hand, all relevant problem is very pesky and usually requires some explicit or implicit contrast attributes to obtain the importance threshold and some stabilization and "robustization" of importance measure -- Boruta is one of the RF wrapper algorithms trying to do this by extending the data set with artificial random attributes and iterating RF training progressively purging attributes claimed unimportant. Note: there are of course non-RF based methods to deal with both feature selection problems, either using other importance sources, adding feature selection to internal optimization of the model or simply performing some more or less complex correlation tests between attributes and decision. For some more ramblings about this topic, you can skim this preprint . For this two-problems-Bayes-net vision of feature selection, see this paper .
