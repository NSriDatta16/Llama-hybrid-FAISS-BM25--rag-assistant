[site]: crossvalidated
[post_id]: 531686
[parent_id]: 531683
[tags]: 
A batch size of 1 is extreme. Something like 32 or so is much more typical. Unless you run into memory issues, I typically set 32 as my initial batch size (when you try to go larger problems also arise, there's a whole literature on training with large batch sizes to avoid those). Much smaller (certainly once we get into single digits, but sometimes even before that) tends to cause problems. Some, you can avoid by gradient accumulation, but some not. One of the best known issues with small batch sizes arises when you have BatchNorm layers in your neural network (I'm guessing you have them). The batch size is the first thing, I'd investigate. Also, how did you pick your learning rate (the popular learning rate finder approach?)? Making sure it is set sensibly is important. The other typical idea to avoid overfitting is data augmentation: a sensible extent of data augmentation (and making sure the augmentation deals properly with the masks - as e.g. the albumentations python library and many other libraries do - by always transforming the mask, too, so it remains in the right location on the image no matter whether you rotate, zoom in or whatever). Did you try to follow the approach of a good standard example such as the one given in the first chapter of the "Deep Learning for Coders with fastai & PyTorch" book (the example there using a convenient library that does a lot of sensible default things, you can then look into their code for more details; see also the matching course video )?
