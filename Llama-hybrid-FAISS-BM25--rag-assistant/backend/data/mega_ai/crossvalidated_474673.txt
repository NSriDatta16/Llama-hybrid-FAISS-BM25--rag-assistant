[site]: crossvalidated
[post_id]: 474673
[parent_id]: 
[tags]: 
Topic modelling when the "documents" are not static and new tokens are generated regularly

This question is related to this one and this one . As background, I'm working with Magic: the Gathering decklist data; I'm trying to automatically classify decks into different archetypes based on the cards in the decks. Based on advice from the second question referenced above, I've spent the last six months playing around with and learning about latent dirichlet allocation and general data science and machine learning topics. Specifically, I've been learning about the gensim topic modelling library, with mixed success. However, I've recently realized that there are two things that may or may not affect the efficacy of LDA in correctly identifying topics: The database that the decklists are drawn from is updated regularly. I know that LDA can handle new documents, but this also means that old decks (documents) may or may not have changed since the last time that data was gathered. I suppose that this could be solved with rerunning LDA from scratch, but that doesn't seem like the best way to go about it. Wizards of the Coast releases several new sets a year, which means that roughly 1000 brand-new cards, as well as a significant number of previously printed cards (tokens), are introduced into the game (the dictionary, I suppose) every year, spaced out every couple of months. This means that while, eventually, cards that become associated with specific archetypes (topics) would become associated with said archetypes, initially they might not be present in enough decks (documents) to get recognized. So is there a way to construct the LDA model to account for these two realities? Alternatively, is there another topic extraction algorithm that might be better-geared for this application?
