 A N α + B D β + L 0 {\displaystyle L={\frac {A}{N^{\alpha }}}+{\frac {B}{D^{\beta }}}+L_{0}} . One can also directly fit a statistical law for D o p t ( C ) , N o p t ( C ) {\displaystyle D_{opt}(C),N_{opt}(C)} without going through the detour, for which one obtains: { N o p t ( C ) = 0.1 C 0.5 D o p t ( C ) = 1.7 C 0.5 {\displaystyle {\begin{cases}N_{opt}(C)=0.1\;C^{0.5}\\D_{opt}(C)=1.7\;C^{0.5}\end{cases}}} or as tabulated: Discrepancy The Chinchilla scaling law analysis for training transformer language models suggests that for a given training compute budget ( C {\displaystyle C} ), to achieve the minimal pretraining loss for that budget, the number of model parameters ( N {\displaystyle N} ) and the number of training tokens ( D {\displaystyle D} ) should be scaled in equal proportions, N o p t ( C ) ∝ C 0.5 , D o p t ( C ) ∝ C 0.5 {\displaystyle N_{opt}(C)\propto C^{0.5},D_{opt}(C)\propto C^{0.5}} . This conclusion differs from analysis conducted by Kaplan et al., which found that N {\displaystyle N} should be increased more quickly than D {\displaystyle D} , N o p t ( C ) ∝ C 0.73 , D o p t ( C ) ∝ C 0.27 {\displaystyle N_{opt}(C)\propto C^{0.73},D_{opt}(C)\propto C^{0.27}} . This discrepancy can primarily be attributed to the two studies using different methods for measuring model size. Kaplan et al.: did not count the parameters in the token embedding layer, which when analyzed at smaller model sizes leads to biased coefficients; studied smaller models than the Chinchilla group, magnifying the effect; assumed that L ∞ = 0 {\displaystyle L_{\infty }=0} . Secondary effects also arise due to differences in hyperparameter tuning and learning rate schedules. Kaplan et al.: used a warmup schedule that was too long for smaller models, making them appear less efficient; did not fully tuning optimization hyperparameters. Beyond Chinchilla scaling As Chinchilla scaling has been the reference point for many large-scaling training runs, there had been a concurrent effort to go "beyond Chinchilla scaling", meaning to modify some of the training pipeline in order to obtain the same loss with less effort, or deliberately train for longer than what is "Chinchilla optimal". Usually, the goal is to make the scaling law exponent larger, which means the same loss can be trained for much less compute. For instance, filtering data can make the scaling law exponent larger. Another strand of research studies how to deal with limited data, as according to Chinchilla scaling laws, the training dataset size for the largest language models already approaches what is available on the internet. found that augmenting the dataset with a mix of "denoising objectives" constructed from the dataset improves performance. studies optimal scaling when all available data is already exhausted (such as in rare languages), so one must train multiple epoches over the same dataset (whereas Chinchilla scaling requires only one epoch). The Phi series of small language models were trained on textbook-like data generated by large language models, for which data is only limited by amount of compute available. Chinchilla optimality was defined as "optimal for training compute", whereas in actual production-quality models, there will be a lot of inference after training is complete. "Overtraining" during training means better performance during inference. LLaMA models were overtrained for this reason. Subsequent studies discovered scaling laws in the overtraining regime, for dataset sizes up to 32x more than Chinchilla-optimal. Broken neural scaling laws (BNSL) A 2022 analysis found that many scaling behaviors of artificial neural networks follow a smoothly broken power law functional form: y = a + ( b x − c 0 ) ∏ i = 1 n ( 1 + ( x d i ) 1 / f i ) − c i ∗ f i {\displaystyle y=a+{\bigg (}bx^{-c_{0}}{\bigg )}\prod _{i=1}^{n}\left(1+\left({\frac {x}{d_{i}}}\right)^{1/f_{i}}\right)^{-c_{i}*f_{i}}} in which x {\displaystyle x} refers to the quantity being scaled (i.e. C {\