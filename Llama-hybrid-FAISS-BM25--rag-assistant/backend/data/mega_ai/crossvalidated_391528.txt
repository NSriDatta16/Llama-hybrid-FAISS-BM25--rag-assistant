[site]: crossvalidated
[post_id]: 391528
[parent_id]: 386520
[tags]: 
This isn't specific to Bayesian networks but it is general to causal inference. I hope it is helpful to your question. The predictive validation tools you describe work by way of having a ground truth with which to compare the predictions against. Causal methods, however, are challenging because there is no ground truth with which to compare them to, as you alluded too. Yet there is hope. One way of validating your causal results is through something called sensitivity analysis. Sensitivity analysis is a set of methods that explore whether or not your causal assumptions hold. For example, a nearly universal causal assumption is unconfoundedness between treatment and the outcome. To validate that causal assumption you will need to explore whether or not unobserved confounding exists. And, if it exists how strong it would need to be to bias your observed effect in a major way. There's a problem, however, in that it is unobserved. Classically, economists would spend pages and pages arguing that any unobserved confounders that exist either do not bias their results in any significant way or said unobserved confounders simply don't exist, all from a conceptual standpoint. There are, however, more data-oriented ways of approaching this problem as discussed in Cinelli et al, 2020 . The paper is cited below and in the introduction cites several other papers on the topic of sensitivity analysis. Cinelli, C., & Hazlett, C. (2020). Making Sense of Sensitivity: Extending Omitted Variable Bias. Journal of the Royal Statistical Society,Series B (Statistical Methodology)
