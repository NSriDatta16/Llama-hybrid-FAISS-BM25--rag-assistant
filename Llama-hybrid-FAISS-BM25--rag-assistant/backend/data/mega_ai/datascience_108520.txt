[site]: datascience
[post_id]: 108520
[parent_id]: 108515
[tags]: 
Your mistake is that you make adjustments based on the test data performance and then retest on the same test data when you think you’ve made an improvement. In “regular” machine learning, say a linear regression, you fiddle with the regression parameters until you find a minimal loss value. That’s essentially what you’re doing here. You fiddle with the model hyperparameters on the training data and the test them out on the test data. This risks overfitting the hyperparameters to the test data in the same way that parameters fit to the in-sample data. In other words, you risk tuning your hyperparameters to fit the test data, rather than giving good ability to generalize.
