[site]: crossvalidated
[post_id]: 12957
[parent_id]: 12955
[tags]: 
In my opinion you want to focus on time series analysis which deals directly with the subject you raise. When dealing with time series data one can use memory models (ARIMA/Autoprojective Structure) to capture the importance of previous values in predicting future values. Google Box-jenkins or ARIMA for more. An equally interesting approach is to use a fixed-effects (X) approach which might incorporate day-of-the-week effects, weekly effects, Holiday/Event effects, Particular days-of-the-month effects. What is even more powerful is to incorporate both the ARIMA component and the X structure into an ARMAX model or a Transfer Function Model. Care should also be taken to identify unusual data via Intervention Detection to accomodate Pulses, Level Shifts, Seasonal Pulses and even Local Time Trends. It also might be important to validate a Gaussian Error Process and to take steps to ensure same. I have made a number of comments on this board about such things. You might review my posts and other posts that you may find equally informative. Modified my answer to deal with the need for models to detect anomalies that if untreated inflate the variance of the errors causing incorrect acceptance of the hypothesis of randomness. Prof.J.K.Ord has referred to this as "the Alice in wonderland effect". The problem is that you can't catch an outlier without a model (at least a mild one) for your data. Else how would you know that a point violated that model? In fact, the process of growing understanding and finding and examining outliers must be iterative. This isn't a new thought. Bacon, writing in Novum Organum about 400 years ago said: "Errors of Nature, Sports and Monsters correct the understanding in regard to ordinary things, and reveal general forms. For whoever knows the ways of Nature will more easily notice her deviations; and, on the other hand, whoever knows her deviations will more accurately describe her ways."
