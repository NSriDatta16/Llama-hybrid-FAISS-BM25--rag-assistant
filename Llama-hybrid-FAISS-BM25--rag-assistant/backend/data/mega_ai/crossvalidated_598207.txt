[site]: crossvalidated
[post_id]: 598207
[parent_id]: 294190
[tags]: 
Compositional data has the quality that the sum of the columns is 1. In a regression setting (e.g. OLS, logistic regression), this can cause problems because when the design matrix includes an intercept, it is not full-rank. As a result, the model is not identified and there are arbitrarily many solutions of equal quality. Dropping the intercept or one of the proportion columns, or employing regularization such as ridge regression, are some solutions. None of the listed methods (decision trees, random forest, SVM) require a full-rank design matrix to be estimated. SVMs act on the kernel matrix, while decision trees simply seek out good splits. For tree-based models, dropping a redundant column can cause side-effects, such as causing the model to seek out deeper trees. This answer discusses an example in the context of gradient-boosted trees One hot encoding of a binary feature when using XGBoost It can be desirable to transform the composition data to a new representation. Here's a discussion. How to perform isometric log-ratio transformation
