[site]: crossvalidated
[post_id]: 621464
[parent_id]: 
[tags]: 
Question on the philosophy and functioning of hypothesis testing with parametric bootstrapping

In the book of Bradley Efron and Robert Tibshirani (1993) "An Introduction to the Bootstrap" chapter 16, a bootstrap method for hypothesis testing is presented. It is precised that the two quantities that have to be specified when carrying out a bootstrap hypothesis test are : A test statistic $t(X)$ , A null distribution $\hat{F}_0$ for the data under the null hypothesis $H_0$ . $X$ represents the observed data and an estimate of the p-value is given by $\hat{p} = \# \{t(X^b) \geq t(X) \}/B$ , where $X^b$ is the $b$ -bootstrap resampling and $B$ is the number of bootstraps, i.e. $b \in \{1,2,\cdots,B\}$ . In some cases, $t(X)$ can be used as a parameter to define $\hat{F}_0$ . Let us then write the 'bootstrap' distribution of $X$ under the null hypothesis $\hat{F}_0(\cdot, t(X))$ . So each $t(X^b)$ is obtained from a sample $X^b$ drawn from $\hat{F}_0(\cdot, t(X))$ . Can someone explain why $\hat{F}_0(\cdot, t(X))$ is a good choice for the distribution under the null hypothesis? For me it looks like we would never be able to reject the null hypothesis because the probability to observe $t(X^b)$ close to $t(X)$ under $\hat{F}_0(\cdot, t(X))$ is really high. Is there something I am missing?
