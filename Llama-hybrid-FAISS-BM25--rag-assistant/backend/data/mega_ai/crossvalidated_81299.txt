[site]: crossvalidated
[post_id]: 81299
[parent_id]: 81262
[tags]: 
I think you are describing nested cross validation and you can use it to select your best hyperparameters. R already has some packages implementing this, for example for support vector machines you could use the package e1071 and do something like this assuming you have two independent variables: svmTuning If you had 1000 observations the previous would perform leave-one-out cross validation, sweeping through the possible combinations of selected gammas and costs (but only one kernel in this case). You can see the best parameters by doing: svmTuning$best.parameters I'm pretty sure the optimal is chosen using the mean squared error calculated based on the cross validation you chose (in the case of regression) and average classification error. Here's another example with kernel k-nearest neighbours knnTuning Which sweeps through all combinations of neighbors up to 40 and the different kernels but using the euclidean distance (distance=2). You may plot all these results and again obtain the best parameters: plot(knnTuning) knnTuning$best.parameters You could do the same for random forest: rfTuning Where you just sweep through possible values for the amount of variables in the candidates for each split. This is known to overfit if not done carefully. And so on and so forth. Since you appear to have a small sample size maybe leave-one-out is the way to go. Maybe you can also look into the caret package which has good capabilities for model building and the actual documentation is very solid (theoretical descriptions and all).
