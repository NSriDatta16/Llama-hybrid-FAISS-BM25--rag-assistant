[site]: crossvalidated
[post_id]: 490932
[parent_id]: 
[tags]: 
When to stop the chain of priors in Bayesian hierarchical models?

From Wkipedia's article on hyperprior : In Bayesian statistics, a hyperprior is a prior distribution on a hyperparameter, that is, on a parameter of a prior distribution. There will be some parameters for the hyperprior and there is nothing stopping us from defining prior distribution on these parameters too. Practical considerations stop us from doing this inductive step too many times. Q1. Is there any theoretical result showing that the effect of these hyperpriors diminishes as we keep on defining them? Q2. Is there a more principled way (than computational and practical considerations) to decide when to stop this chain of priors?
