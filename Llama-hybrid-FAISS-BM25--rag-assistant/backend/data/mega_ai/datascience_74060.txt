[site]: datascience
[post_id]: 74060
[parent_id]: 74034
[tags]: 
I find the easiest way for people to understand this is to think of the confusion matrix . Accuracy score is just one measure of a confusion matrix, namely all the correct classifications over all the prediction data at large: $$\frac{True Positives + True Negatives}{True Positives + True Negatives + False Positives + False Negatives}$$ Your False Negative Rate is calculated by: $$\frac{False Negatives}{False Negatives + True Positives}$$ One model may turn out to have a worse accuracy, but a better False Negative Rate. For example, your model with worse accuracy may in fact have many False Positives but few False Negatives, leading to a lower False Negative Rate. You need to choose the model which produces the most value for your specific use case. Why do some classifier perform poorly? While an experience practitioner might surmise what could be a good modeling approach for a dataset, the truth is that for all datasets, there is no free lunch... also known as "The Lack of A Priori Distinctions Between Learning Algorithms" You don't know ahead of time if the best approach will be deep learning, gradient boosting, linear approaches, or any other number of models you could build.
