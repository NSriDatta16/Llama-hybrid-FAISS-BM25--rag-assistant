[site]: crossvalidated
[post_id]: 483343
[parent_id]: 482890
[tags]: 
The embeddings are trained jointly with the rest of the network. In the beginning, the embeddings are initialized randomly and the error gets back-propagated through the entire network down to the embeddings. When you train the embeddings jointly with the rest of the model, the problem often is that embeddings of the rare words only get updated once in a while a get sort of out of the sync with the rest of the model. Transformer tries to avoid this problem by: Using not really large sub-word-based vocabulary where infrequent words split into smaller units that appear frequently enough. Sharing the embedding between the encoder and the decoder. Resuing the embeddings as parameters of the final output layer (i.e., the classification can be then interpreted as a sort of measuring dot-product similarity between the output state and the embeddings).
