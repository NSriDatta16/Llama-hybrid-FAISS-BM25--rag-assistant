[site]: crossvalidated
[post_id]: 190470
[parent_id]: 96026
[tags]: 
I think the correct answer to this question is provided by a document of sklearn here: http://scikit-learn.org/stable/modules/cross_validation.html Basically by doing cross-validation(CV), compared with hold-out validation, we can reduce the amount of data taken by the validation set, thus increase the amount of data used by training set. This solves the problem where the amount of training data is not enough while we still want to have training, validation and test set. As written in the document: "The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data (as it is the case when fixing an arbitrary test set), which is a major advantage in problem such as inverse inference where the number of samples is very small."
