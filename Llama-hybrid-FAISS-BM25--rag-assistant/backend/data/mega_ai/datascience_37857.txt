[site]: datascience
[post_id]: 37857
[parent_id]: 37856
[tags]: 
The learning rate regulates the amount by which the weights will change at every step $t$ of the gradient descent algorithm. It is not true that it can be set to any arbitrary amount, this is furthest from the truth. A learning rate which is too small will never allow your machine learning model to converge to a minimum, and a learning rate which is too large will cause your model parameters to oscillate around a possible minimum. I answered in more details here how the basic gradient descent algorithm is affected by the choice of the learning rate and then i propose some better alternative methods: Do adaptive learning optimizers follow the steepest decent? . More details More accurately the update rule for gradient descent is $w^{t+1} = w^t + m\nabla J(w)$ where $m$ is the learning rate and $J(w) = \frac{1}{2}\sum(y-\hat{y})^2$ is the cost function. Thus, it does not scale the weights in a multiplicative sense, however it does scale the correction the weights will experience at each step of the algorithm.
