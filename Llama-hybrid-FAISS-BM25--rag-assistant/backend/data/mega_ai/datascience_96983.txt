[site]: datascience
[post_id]: 96983
[parent_id]: 96982
[tags]: 
The output of the decision tree is the corresponding value on the leaf of the tree which is determined from the splits in the trees. The splits as well as the leaf values are parameters which are learnt. I recommend to watch this video which explains how a regression tree is trained and what it learns https://www.youtube.com/watch?v=g9c66TUylZ4 In summary, this means that a decision tree for regression (like the current XGBoost employs it) cannot perform well on data which follows a different distribution than the one seen in the training data set. If you want to have some generalization beyond the training data you provided, consider trying out a linear tree, which is a tree where each leaf stands for a linear model. Checkout the python package https://github.com/cerlymarco/linear-tree which you can install via pip like this "pip install linear-tree".
