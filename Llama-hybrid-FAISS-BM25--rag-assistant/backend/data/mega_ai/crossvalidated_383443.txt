[site]: crossvalidated
[post_id]: 383443
[parent_id]: 381991
[tags]: 
What is the definition of a projection in this strict (linear algebraic) sense (of the word) https://en.wikipedia.org/wiki/Projection_(linear_algebra) In linear algebra and functional analysis, a projection is a linear transformation $P$ from a vector space to itself such that $P^2 = P$ . That is, whenever $P$ is applied twice to any value, it gives the same result as if it were applied once (idempotent). For orthogonal projection or vector projection you have that https://en.wikipedia.org/wiki/Projection_(linear_algebra) An orthogonal projection is a projection for which the range U and the null space V are orthogonal subspaces. Why isn't RP a projection under this definition? Michael Mahoney writes in your lecture notes that it depends on how the RP is constructed , whether or not the RP is a projection in the traditional linear algebraic sense. This he does in the third and fourth points: Third, if the random vectors were exactly orthogonal (as they actually were in the original JL constructions), then we would have that the JL projection was an orthogonal projection ... but although this is false for Gaussians, $\lbrace \pm \rbrace $ random variables, and most other constructions, one can prove that the resulting vectors are approximately unit length and approximately orthogonal ... this is “good enough.” So you could do, in principal, the random projection with a different construction that is limited to orthogonal matrices (although it is not needed). See for instance the original work: Johnson, William B., and Joram Lindenstrauss. "Extensions of Lipschitz mappings into a Hilbert space." Contemporary mathematics 26.189-206 (1984): 1. ...if one chooses at random a rank $k$ orthogonal projection on $l_2^n$ ... To make this precise, we let $Q$ be the projection onto the first $k$ coordinates of $l_2^n$ and let $\sigma$ be normalized Haar measure on $O(n)$ , the orthogonal group on $l_2^n$ . Then the random variable $$f: (O(n), \sigma) \to L(l_2^n)$$ defined by $$f(u) = U^\star Q U$$ determines the notion of a "random rank $k$ projection." The wikipedia entry describes random projection in this way (the same is mentioned in the lecture notes on pages 10 and 11) https://en.wikipedia.org/wiki/Random_projection#Gaussian_random_projection The first row is a random unit vector uniformly chosen from $S^{d − 1}$ . The second row is a random unit vector from the space orthogonal to the first row, the third row is a random unit vector from the space orthogonal to the first two rows, and so on. But you do not generally get this orthogonality when you take all the matrix-entries in the matrix random and independent variables with a normal distribution (as Whuber mentioned in his comment with a very simple consequence "if the columns were always orthogonal, their entries could not be independent"). The matrix $R$ and the product in the case of orthonormal columns, can be seen as a projection because it relates to a projection matrix $P = R^TR$ . This is a bit the same as seeing ordinary least squares regression as a projection. The product $b = R^T x$ is not the projection but it gives you a coordinate in a different basis vector. The 'real' projection is $x' = Rb = R^TRx$ , and the projection matrix is $R^TR$ . The projection matrix $P=R^TR$ needs to be the identity operator on the subspace $U$ that is the range of the projection (see the properties mentioned on the wikipedia page). Or differently said it needs to have eigenvalues 1 and 0, such that the subspace for which it is the identity matrix is the span of the eigenvectors associated to the eigenvalues 1. With random matrix-entries you are not going to get this property. This is the second point in the lecture notes ... it “looks like” an orthogonal matrix in many ways ... the $range(P^T P)$ is a uniformly distributed subspace ... but the eigenvalues are not in $\lbrace 0, 1 \rbrace$ . note that in this quote the matrix $P$ relates to the matrix $R$ in the question and not to the projection matrix $P = R^TR$ that is implied by the matrix $R$ So random projection by different constructions, such as using random entries in the matrix, is not exactly equal to an orthogonal projection. But it is computationally simpler and, according to Michael Mahoney, it is “good enough.”
