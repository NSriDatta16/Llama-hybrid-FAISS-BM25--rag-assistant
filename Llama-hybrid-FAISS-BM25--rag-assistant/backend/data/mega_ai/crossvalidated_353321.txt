[site]: crossvalidated
[post_id]: 353321
[parent_id]: 
[tags]: 
machine learning - Derivative of log-likelihood function in softmax regression

I'm trying to find the derivative of the log-likelihood function in softmax regression. I have (with $\Theta$ being the parameters, and $x^{(i)}$ being the $i$th training example, and $s_j$ representing the softmax function), $$\ell(\Theta) = \sum_{i=1}^m \sum_{j=1}^k \log \left( \frac{e^{\Theta_j^T x^{(i)}}}{\sum_{l=1}^k e^{\Theta_l^T x^{(i)}}} \right)^{I\{y^{(i)}=j\}}$$ I got the derivative of the softmax function itself as $$\frac{\partial}{\partial \Theta_p} \left( \frac{e^{\Theta^T_j x^{(i)}}}{\sum_{l=1}^k e^{\Theta^T_l x^{(i)}}} \right)=s_j(\delta_{pj}-s_p)x^{(i)}$$ On using this to find the derivative of the log-likelihood, I get $$\begin{aligned}\frac{\partial}{\partial \Theta_p}\ell(\Theta) &= \sum_{i=1}^m \sum_{j=1}^k I\{y^{(i)}=j\} \cdot \frac{s_j(\delta_{pj}-s_p)x^{(i)}}{s_j} \\ &= \sum_{i=1}^m \sum_{j=1}^k I\{y^{(i)}=j\} \cdot (\delta_{pj}-s_p)x^{(i)} \\ &= \sum_{i=1}^m \sum_{j=1}^k I\{y^{(i)}=p\}x^{(i)}-I\{y^{(i)}=j\}s_p x^{(i)} \end{aligned} $$ I'm not sure where to go from here. From what I've seen online, I shouldn't have the second summation at all, and the second term should just be $s_p$. I'm sure I'm just missing a step or two, but I'd love if someone could help me move ahead.
