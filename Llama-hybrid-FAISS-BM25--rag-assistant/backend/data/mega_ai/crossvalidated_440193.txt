[site]: crossvalidated
[post_id]: 440193
[parent_id]: 
[tags]: 
Why is the denominator of z-score for linear regression coefficients called standard error?

In machine learning and linear regression applications, the z-value is given by the following formula: $$z_j = \frac{\beta_j}{\hat{\sigma}\sqrt{v_j}}$$ where $$\hat{\sigma}^2 = \frac{1}{N-p-1}\sum_{i=1}^{N}(y_i-\hat{y}_i)^2$$ and $\beta_j$ are the regression coefficients and $\hat{\sigma}^2$ is the variance of the coefficients; $v_j$ are the diagonal elements of $(X^TX)^{-1}$ . The denominator $\hat{\sigma}\sqrt{v_j}$ is generally called standard error : why is that? Why is it considered an error and why standard ? What does it tell/represent?
