[site]: crossvalidated
[post_id]: 421302
[parent_id]: 
[tags]: 
Increasing sample size increases no of trainable parameters

I was working with keras and tensorflow as backend on an NLP problem when I observed that increasing my training data size caused an increase in the number of trainable parameters even when batch size remained the same. From what I understand, trainable parameters are the weights which are learnt for each layer. If that is the case then it should not change irrespective of whether I increase or decrease my input data size. So what is exactly happening here? The reason why this is important is because I perform normalization upon my data once it is fully loaded. This normalization would not work properly if I used a generator function.
