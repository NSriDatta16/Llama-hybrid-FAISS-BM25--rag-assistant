[site]: crossvalidated
[post_id]: 314780
[parent_id]: 
[tags]: 
Determine periods of minimum standard deviation in time series

I have followed many of the posted answers but I have not found anything that works, what works best so far is visual inspection of the time series, but it would be nice to have something quantitative. I am not interested in detecting change points or seasonality, perhaps these might be called steps? What methodology can provide a list of periods when SD is at its lowest throughout the time series and remains within the lowest standard deviation for a period over n time steps (with n defined previously by the user), I would also like to know the beginning and ending time steps of each low SD period. The low SD could be defined using a rolling SD function. This is to find when the time series is more stable for longer. I am providing a few examples, as you can see some are easier to detect than others. So far using the rolling standard deviation seems to help. PS something in R is preferred.
