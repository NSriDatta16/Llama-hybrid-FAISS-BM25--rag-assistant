[site]: crossvalidated
[post_id]: 600320
[parent_id]: 600310
[tags]: 
This is a fun little problem. @jbowman notes "As is often the case, the issue is one of defining your features". One way to engineer a feature for this problem is to look at the last digit of the number. Here is a simple little script in sklearn import numpy as np from sklearn.linear_model import LogisticRegression from sklearn.pipeline import make_pipeline from sklearn.preprocessing import OneHotEncoder, FunctionTransformer @np.vectorize def get_last_digit_of_float(x): str_x = str(x) if len(str_x) == 1: return int(str_x) else: return int(str_x[-1]) last_digit_transformer = FunctionTransformer(get_last_digit_of_float) def main(): train_set = np.arange(100).reshape(-1, 1) y_train = np.arange(100) % 2 test_set = np.arange(100, 200).reshape(-1, 1) y_test = np.arange(100, 200) % 2 model = make_pipeline( last_digit_transformer, OneHotEncoder(categories='auto', sparse=False, handle_unknown='error', drop='first'), LogisticRegression(solver='lbfgs', multi_class='multinomial') ) model.fit(train_set, y_train) print('Test Set Accuracy:', model.score(test_set, y_test)) if __name__ == '__main__': main() This returns a test set accuracy of 1.0. The trick here is to turn the last digit of the number into a feature and then one hot encode the number. This works because any number which ends in an even number is also even. The result is a design matrix with 9 columns (numbers 1 through 9, 0 is not needed in this case since it is absorbed by the intercept in Logistic regression). As always, a clever feature engineering approach is all that is needed.
