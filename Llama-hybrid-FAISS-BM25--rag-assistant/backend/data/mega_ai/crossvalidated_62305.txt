[site]: crossvalidated
[post_id]: 62305
[parent_id]: 
[tags]: 
(Why) should bootstrap sampling distribution for logistic regression slope be conditional on $S=\sum Y_j$?

I am working my way through Chapter 4 (Tests) in Davison & Hinkley's (D&H's) book "Bootstrap Methods and their Applications", and have a question about one of their examples. Davison & Hinkley's proposed test Example 4.1 proposes a permutation test for the (alternative) hypothesis of non-zero slope in a simple logistic regression. Quoting from page 141: Example 4.1 (Logistic regression) Suppose that $y_1, \ldots , y_n$ are independent binary outcomes, with corresponding scalar covariate values $x_1, \ldots , x_n$ and that we wish to test whether or not $x$ influences $y$. If our chosen model is the logistic regression model $$ \log \frac{\Pr(Y_j = 1 | x_j)}{\Pr(Y_j = 0 | x_j} = \lambda + \psi x_j, \quad j=1,\ldots,n, $$ then the null hypothesis is $H_0 : \psi=0.$ Under $H_0$ the sufficient statistic for $\lambda$ is $S=\sum Y_j$ and $T=\sum x_j Y_j$ is the natural test statistic [...snipped text...] The null distribution of $Y_1,\ldots,Y_n$ given $S = s$ is uniform over $n \choose s$ permutations of $y_1, \ldots, y_n$. Rather than compute all of those permutations to get the exact distribution of $T$, they propose leaving the $x_i$ fixed and generating $R$ permutations of the $y_i$ to approximate $T$'s exact distribution. My na√Øve test That all seems reasonable, but my thought would have been to simulate a slightly different null distribution. Given that $\psi = 0$ under the null hypothesis, $$ \Pr(Y_j = 1| x_j) = \frac{e^{\lambda}}{e^{\lambda} + 1}, \quad j=1,\ldots,n, $$ Leaving $x_1, \ldots , x_n$ fixed, I would produce $R$ sets of $y^*_1, \ldots , y^*_n$ with each $y^*_i \sim \textrm{Bernoulli}(\frac{e^{\lambda}}{e^{\lambda} + 1})$. Instead of (but probably equivalently to) D&H's $T=\sum x_j Y_j$, I would take as my test statistic the estimate of $\psi$ from a fitted logistic regression of the bootstrap sample. Questions The two tests above depend on different distributions of bootstrap samples. Mine, for instance, may produce some samples in which all of the $y^*_i$ equal 1, whereas all of D&H's samples contain the same number of 1's. The salient difference seems to be that D&H's sampling distribution is conditional on $S=\sum Y_j$ whereas mine is not. Is one of these approaches preferable to the other in all situations? They seem to assume/model different data-generation processes in the originally sampled populations. Is that correct? If so, when should I choose one approach over the other?
