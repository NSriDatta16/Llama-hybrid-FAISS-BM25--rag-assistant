[site]: crossvalidated
[post_id]: 365269
[parent_id]: 
[tags]: 
How to have different source and target vocabularies?

I am a bit confused as to how to use different sets of source and target vocabularies in deep learning for NLP tasks. What are the implications of using separate source and target vocabularies (for example, in machine translation, or in general)? Should the indices for corresponding tokens match in the word embeddings? How should their sizes be relative to each other? Do you have to modify the architecture in any way to accommodate this, or could you simply change the path to vocabulary? Thanks!
