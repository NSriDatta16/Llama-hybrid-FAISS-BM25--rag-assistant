[site]: crossvalidated
[post_id]: 254155
[parent_id]: 
[tags]: 
Why is multilinear PCA rarely used for nonlinear dimensionality reduction, compared to, say, t-SNE?

I'm only starting to understand some of the subtleties that arise when choosing one dimensionality reduction technique over another. Now I'm wondering why it seems that in recent years, one of the more popular methods for nonlinear dimensionality reduction was/is t-SNE, while multilinear Principal Component Analysis seems to be rarely used -- at least in the literature I look at, machine learning, and usually for the purpose of visualization of high-dimensional data. Could someone comment on what might be the reason for this (unless of course my assumption is wrong to begin with)? In particular, it seems that (linear) PCA has a number of advantages over t-SNE (e.g. learning a function that can be applied to new data, 'preservation' of global structure), so I am wondering whether these advantages are also present in multilinear PCA. If so, I wonder why MPCA doesn't seem to be used that commonly. Related to the question above: I recently read about Isomap , which by my (very incomplete understanding) looks like it should be another useful nonlinear method for interpreting high-dimensional data. In that case, what would be the distinguishing features between t-SNE, MPCA and Isomap?
