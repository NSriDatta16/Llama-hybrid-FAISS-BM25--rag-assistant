[site]: crossvalidated
[post_id]: 592505
[parent_id]: 
[tags]: 
Is likelihood a conditional probability?

If we have a set of observations $\mathcal{D} = \{x_i\}_{i=1}^n$ then the likelihood $\mathcal{L}$ is: $$ \mathcal{L}(\theta \mid \mathcal{D}) = P_\theta(\mathcal{D})$$ and if the observations are independent then: $$\mathcal{L}(\theta \mid \mathcal{D}) = \prod_{i=1}^{n}P_\theta(x_i)$$ where $P$ is the probability mass function for a given parameter $\theta$ . In maximum likelihood estimation we treat $\theta$ as a parameter for estimation. Furthermore, in MAP: $$P(\theta \mid \mathcal{D}) \propto P(\mathcal{D} \mid \theta) P(\theta)$$ If we assume uniform prior then: $$P(\theta \mid \mathcal{D}) \propto P(\mathcal{D} \mid \theta)$$ and it is said that MLE and MAP give the same point estimate. What troubles me is that in Bayesian context the likelihood is $P(\mathcal{D} \mid \theta)$ where we treat $\theta$ as a random variable so conditional probabilities make sense. But in a Frequentist approach we don't treat the parameters as random variables. Should we view likelihood as a conditional probability or it depends on the approach (Bayesian vs Frequentist)?
