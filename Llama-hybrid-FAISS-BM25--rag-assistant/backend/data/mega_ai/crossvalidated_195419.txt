[site]: crossvalidated
[post_id]: 195419
[parent_id]: 
[tags]: 
Batch gradient descent in Perceptron linear classifier

I'm learning about batch gradient descent for the Perceptron linear classifier and I'm confused about the update rule. On Wikipedia, it says that the update rule for batch gradient descent is $w := w - \alpha \sum_{i = 1}^{n} \nabla Q_i(w)$ where $\alpha$ is the learning rate. Why is the gradient a sum and not an average of the gradients of each misclassified sample? I tried implementing the batch gradient descent update rule above and it seemed to make the error worse since the weights would update by a huge amount at each iteration. Instead, I tried $w := w - \alpha \frac{1}{n} \sum_{i = 1}^{n} \nabla Q_i(w)$ and the results were much better. Am I understanding the update rule incorrectly?
