[site]: datascience
[post_id]: 103716
[parent_id]: 103706
[tags]: 
so sorry, but you are confusing peeps with "text" vs "string" ..looking at your examples, they are all "strings" and in this case using any pre trained embedding (glove, word2vec ) wont work very well since most of your strings will come up with 0 while looking up the global embeddings. One hot will have to be done at character level (meaning at least a 26 dimension vector with 1 non zero) BUT this will be super sparse so, u ll need to take care of that as well ..the best bet is , if you had enough samples, train a local glove embedding (there are scripts available on their git page)
