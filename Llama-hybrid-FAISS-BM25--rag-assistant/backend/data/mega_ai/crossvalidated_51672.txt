[site]: crossvalidated
[post_id]: 51672
[parent_id]: 51670
[tags]: 
A common approach would be logistic regression, where you model the probability of a bad outcome in terms of the levels of a,b,... , e, possibly allowing for interaction effects : A simple main effects model would look like this: $$ \Pr(Y=1|\underline{x}) = \frac{e^{(\beta_0 + \beta_1 x_1 + \beta_2 x_2 +...+ \beta_k x_k)}} {e^{(\beta_0 + \beta_1 x_1 + \beta_2 x_2 +...+ \beta_k x_k)} + 1} = \frac {1} {e^{-(\beta_0 + \beta_1 x_1 + \beta_2 x_2 +...+ \beta_k x_k)} + 1} $$ where $Y$ is 1 if the outcome is bad and $x_1,...,x_k$ are the values of your predictors (the levels of $a$, $b$, ... etc). It helps to think of this model in terms of the log-odds , which is the scale of the linear predictor: $$ \log{\frac{\Pr(Y=1|\underline{x})}{\Pr(Y=0|\underline{x})}} = \eta(\underline{x}) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 +...+ \beta_k x_k $$ The size of the coefficients tells you about how a unit increase in a predictor impacts the log of the odds for a bad outcome. If you wanted to allow for interactions, there'd be additional terms in this model. This sort of approach can in some circumstances give very good predictions; it depends on how noisy the data is and the suitability of the form of the model. These are relatively easy to fit, being available in many stats packages. Another approach would be to try to find a classifier (possibly based on some clustering algorithm) - basically some function of those variables that best separates the good and bad outcomes. There are still other things that might be done.
