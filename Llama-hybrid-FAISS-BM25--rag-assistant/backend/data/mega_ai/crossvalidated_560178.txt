[site]: crossvalidated
[post_id]: 560178
[parent_id]: 539175
[tags]: 
As shown, I had a similar situation with a TD3 agent, of an exoskeleton, where the Actor's did not converge to the Critic estimates. The Critic expected more but the Actor reached its maximum. I improved it by opening up the limits of the Action range as follows: actInfo = rlNumericSpec([numAct, 1],'LowerLimit',[a1;b1;c1],'UpperLimit',[a2;b2;c2]); The analogy is that I was trying to train a dog not to leave the house but kept the doors closed. It was therefore a "good dog" with a bone reward every time it reached the doors. When I opened up the doors it sometimes left the house, got a "bad dog" critic without a bone and learnt to stay home with doors wide open.
