[site]: datascience
[post_id]: 80631
[parent_id]: 
[tags]: 
weight sharing among neurons at same depth

I'm trying to understand some visual illustrations about the wight sharing in the Convolutional Neural Network as following: In this picture we see that for different outputs different inputs share the same weights, as the weights are associated with the color (same color - same weight). So in the Stanford example we can see it eather: As for example, for the o(0,0,0)=5 and for o(0,1,0)=7 both we used the w0[0],w0 1 ,w 2 weights, and it fits the picture above. Now there is this picture: And I'm not fully understand the 3 estimation. It says that the same color means the same weight, but as I can tell in the picture we see a filter, each neuron, let's mark it as $f$ refers to the multiplication of $f(x)=wx(+b?)$ , and as much as I can tell from the Stanford exmaple, the weight can be all different and not the same color, and the weight sharing isn't representable on the filter, but only in any view that includes the input, as in the first image, so isn't it a bit misleading ? Or maybe I just didn't understand it and anyone could explain it more briefly ?
