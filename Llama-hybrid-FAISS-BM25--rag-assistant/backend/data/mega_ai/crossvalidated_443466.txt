[site]: crossvalidated
[post_id]: 443466
[parent_id]: 234207
[tags]: 
An autoencoder might be what you are after. The idea is to define two functions, an encoder $f_{\theta}: \mathbb{R}^N\to \mathbb{R}^n$ and a decoder $g_{\phi}: \mathbb{R}^n\to\mathbb{R}^N$ , where $N$ is the dimensionality of your data and $n . It is common to parametrize $f$ and $g$ by neural networks. Then each datum $X$ can be approximately reconstructed by $g_{\phi}(f_{\theta}(X))$ , and the parameters $\phi,\theta$ are trained so as to minimize the average reconstruction loss. Given any point in the latent space $\mathbb{R}^n$ , you can simply run it throug the decoder $g_{\phi}$ to get back a point in the original space.
