[site]: crossvalidated
[post_id]: 555164
[parent_id]: 477446
[tags]: 
I realise this is a little late to the party, but I just had a similar issue with a logistic regression model of my own and wanted to understand why it wasn't at least orienting the decision plane to include more data-points and/or basing the minimum misclassification boundary more favourably. I was using iterated reweighted least squares fitting (Rubin, 1983) but where I had implemented this myself. For me, I was getting very similar results to Raz (with a different arrangement of data-points). Namely, the decision plane didn't seem best oriented and the minimum misclassification boundary (at posterior probability 0.5) wasn't in the best position (I could change the threshold at which I drew the boundary and improve the accuracy). These strange characteristics seemed to arise because I had forgotten to include a bias term in my model (or a constant bias column in my data). Adding this in I began to see the anticipated results. If anyone sees this behaviour themselves, I suggest you check to see that your bias term is properly included. References D. B. Rubin, Iteratively Reweighted Least Squares , Encycl. Stat. Sci. 33 (1983), 7â€“17.
