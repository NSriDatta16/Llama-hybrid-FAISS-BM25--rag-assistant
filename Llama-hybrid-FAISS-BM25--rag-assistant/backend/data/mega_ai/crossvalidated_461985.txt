[site]: crossvalidated
[post_id]: 461985
[parent_id]: 151263
[tags]: 
I have another viewpoint, based on the preference of weighting the regression when two factors are 'turned on'. In essence, you have a preferential weighting of the regression line in the presence of selected factors. Now, I quoted a passage from an introduction to Bayesian regression to quote: When we want show the linear fit from a Bayesian model, instead of showing only estimate, we can draw a range of lines, with each one representing a different estimate of the model parameters. As the number of datapoints increases, the lines begin to overlap because there is less uncertainty in the model parameters. Sounds familiar? Also, with respect to the interpretation of parameters to quote: The result of performing Bayesian Linear Regression is a distribution of possible model parameters based on the data and the prior. This allows us to quantify our uncertainty about the model: if we have fewer data points, the posterior distribution will be more spread out. As the amount of data points increases, the likelihood washes out the prior, and in the case of infinite data, the outputs for the parameters converge to the values obtained from OLS. Bottom line, my suggestion adopt a more formalistic Bayesian linear regression approach.
