[site]: crossvalidated
[post_id]: 385912
[parent_id]: 
[tags]: 
Why are there differences in recall values when I use GridSearchCV vs classification_report (scikit-learn)?

I'm currently working on a clasification problem through random forest. When I use GridSearchCV, using the parameter scoring="recall" , the best_estimator_ is: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini', max_depth=5, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=5, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1, oob_score=False, random_state=700, verbose=0, warm_start=False) and the grid.best_score_ = 0.63860 but when I save this randomforest in a variable called "m" and use: from sklearn.metrics import classification_report m_predictions = m.predict(X_train) print (classification_report(y_train,m_predictions, digits = 5)) the results I get are : precision recall f1-score support 0 0.85268 0.95728 0.90196 1826 1 0.86644 0.62624 0.72701 808 avg/total 0.85690 0.85573 0.84829 2634 Why in the first case I get a recall of 0.63860 and in the second one of 0.85573? Shouldn't the same value be, or at least, a close value?
