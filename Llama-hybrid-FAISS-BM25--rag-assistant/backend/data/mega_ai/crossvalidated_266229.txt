[site]: crossvalidated
[post_id]: 266229
[parent_id]: 266225
[tags]: 
Here's the "default" nested cross-validation procedure to compare between a fixed set of models (e.g. grid search): Randomly split the dataset into $K$ folds. For $i$ from 1 to $K$: Let test be fold $i$. Let trainval be all the data except that which is in test . Randomly split trainval into $L$ subfolds $(i, 1), (i, 2), \dots, (i, L)$ . So, each subfold $(i, j)$ has some elements from outer fold 1, some from outer fold 2, ..., but none of them has any from outer fold $i$. For $j$ from 1 to $L$: Let val be fold $(i, j)$. Let train be all the data which is not in either test or val . Train each proposed parameter set on train , and evaluate it on val , keeping track of classification accuracy, RMSE, whatever criterion you want to use. Look at the average score for each set of parameters over the $L$ folds, and choose the best one. Train a model with that best set of parameters on trainval . Evaluate it on test , and save the accuracy / RMSE / whatever. Report the mean / mean + std / boxplot / whatever of the $K$ accuracies.
