[site]: crossvalidated
[post_id]: 193318
[parent_id]: 
[tags]: 
Backpropagation: Is there a general weight update rule for both output and hidden layers?

I'm looking for a general weight update rule for both hidden and output layers, no matter the number of layers, the connections or the transfer function. Does anything like this exist? I'm quite new to backpropagation and neural networks in general, so if someone has an answer, I would be really grateful if it didn't include too many assumptions about my knowledge in the field.
