[site]: crossvalidated
[post_id]: 615253
[parent_id]: 
[tags]: 
What would be the best way to find N clusters equally separated, ignoring outliers?

I am working on a computer vision problem (chessboard recognition), and my goal is to find the lines that correspond to the chessboard lines. I want to do this via Hough transform , since said lines usually appear as a bunch of clustered points along a sinusoidal curve (although I approximate the curve with a straight line since the vanishing point is usually very far away). I am able to almost always correctly identifying the two set of lines corresponding to the chessboard, and isolate the straight lines like so: Now I am facing the problem of correctly identifying the center of the clusters that correspond to the actual chessboard lines: there should be 9 clusters approximately equally spaced along the red line, but some other noise make this clustering task non trivial: for instance, near the edges the cluster appear larger because it detects the outer border of the chessboard that is not actually part of the pattern. I tried K-Medoids, and sometime it works but sometimes it doesn't, and doesn't take advantage of the fact that I know these clusters should be equally separated. I tried a gaussian mixture model, specifically the scikit-learn one, but I modified the class to include a term in the log likelihood that is proportional to the entropy of the distances of the gaussians means (to encourage equally distributed distances). I even tried to come up with a model of 9 gaussians parametrized like $$P(X|\mu_0,l) \sim \sum_{i=1}^9 \mathcal{N}(\mu_0 + il,\sigma)$$ and tried to maximize the likelihood for $\mu_0$ and $l$ but still the outliers influence the result. I thought about using some sort of frequency domain approach but I wouldn't really know where to start. So any suggestion appreciated. Have a great day!
