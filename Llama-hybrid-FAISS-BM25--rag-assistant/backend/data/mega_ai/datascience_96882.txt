[site]: datascience
[post_id]: 96882
[parent_id]: 
[tags]: 
How to do batch inference on Hugging face pretrained models?

I want to do batch inference on MarianMT model. Here's the code: from transformers import MarianTokenizer tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de') src_texts = [ "I am a small frog.", "Tom asked his teacher for advice."] tgt_texts = ["Ich bin ein kleiner Frosch.", "Tom bat seinen Lehrer um Rat."] # optional inputs = tokenizer(src_texts, return_tensors="pt", padding=True) with tokenizer.as_target_tokenizer(): labels = tokenizer(tgt_texts, return_tensors="pt", padding=True) inputs["labels"] = labels["input_ids"] outputs = model(**inputs) How do I do batch inference?
