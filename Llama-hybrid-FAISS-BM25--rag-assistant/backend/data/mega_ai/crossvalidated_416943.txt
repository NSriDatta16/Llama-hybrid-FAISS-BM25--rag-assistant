[site]: crossvalidated
[post_id]: 416943
[parent_id]: 
[tags]: 
Minimizing expected brier score and Brier score interpretation

For a probabilistic binary forecast, the BS (Brier score) is given by $$ \text{BS}= \begin{cases} (1-f_i)^2\\ f_i^2\\ \end{cases} $$ Where $f$ is the forecast. If the event occurs with probability $p_i$ then the Expected Brier score is $$E[\text{BS}] = p_i(1-f_i)^2 + (1-p_i)f_i^2$$ which is minimized by setting $f = p$ . This means that if one where to make accurate forecast $f$ of the true probability the expected Brier score reaches a minimum. If we instead had many probabilistic forecasts, $\text{BS}=\sum \text{BS}_i$ , then its expectation would be minimized when every forecast equals the true probability for the outcome. If the random variable $\text{BS}$ materializes the sample mean is: $n^{-1} \sum (f_i-O_i)^2$ . Where $O_i$ is the observed event: 1 or 0. But the sample mean is minimized by letting $f_i$ equal the true outcome: 1 or 0 which may not be the true probability of the outcome. Something is wrong with my reasoning but I can't understand what? Could someone explain? From the reasoning about minimizing the expected Brier score above, should I interpret the Brier score such that if I minimized the expected Brier score then I am making more accurate predictions? ** EDITED** I Want to emphasize that each event has a different probability of occurring. ** EDITED** @kjetil b halvorsen suppose we fitted a logistic regression in millions of observations then we fit the model $logit( f_i) = \hat{\alpha} + \hat{\beta}_1 x $ What is the difference when we use logistic regression model? what more restrictions are there than less parameters than observations? In this setting we probably could not minimize the sample mean so that it equals zero ?
