[site]: crossvalidated
[post_id]: 625667
[parent_id]: 
[tags]: 
Detecting biased coins

EDIT: By detecting I mean to statistically confirm, with a certain confidence, if the coin is actually biased or not, with a particular bias of $p = 0.57$ . I'd like to know what is the best way of framing this as a statistical test. I guess that the answer depends on the prior confidence that we have on the bias actually being 0.57 or not. I'm interested in developing a statistical test to detect coins with a given bias, and in particular I want to specify how many coin flips are needed to detect such a biased coin. More precisely, say we are told that a certain coin has a probability of heads $p$ equal to $0.57$ , and we are asked to determine the minimum amount of coin flips such that we can confidently detect the bias. Here, confidently means with a particular significance level. There are several ways we can tackle this problem. Using hypothesis testing, and using either the Z-test or the binomial test, we could do the following: first, one would think of a test where the null hypothesis is that the coin is fair, and the alternative is $p = 0.57$ or $p > 0.5$ . The latter, $p > 0.5$ , would help us in detecting if the coin is more biased towards heads, but we wouldn't be able to detect the particular bias, so putting $p = 0.57$ makes more sense. The rejection zones would be the same, but we would have a better control of the type $2$ error if we set $H_1: p = 0.57$ . If $H_1: p > 0.50$ , the required minimum sample size would be larger. Since we are explicitly told that the coin has this particular bias of $p = 0.57$ , it also makes sense to set $H_0: p = 0.57$ , $H_1: p \neq 0.57$ , set a particular significance level $\alpha$ to define our rejection zone, and then aim for a certain reduction in the probability of type $2$ error, $\beta$ , and find the minimum sample size such that this reduction in $\beta$ is achieved. Finally, another method that we could use is the following: we know that the best estimator of $p$ is the average of heads, $\widehat{p}$ , that we see in a particular independent sample. Thus, we could fix a significance level $\alpha$ , build a $(1 - \alpha)$ -confidence interval for $p$ and determine the minimum sample size such that $\left| p - \widehat{p} \right| , for any given maximum error $\delta$ that we are happy with. We could combine the minimum sample size we obtain from the previous test $H_0: p = 0.57$ , $H_1: p \neq 0.57$ together with this new threshold. My questions are: Did I make any mistakes in my reasoning? Among the methods that I suggested, which one is the "most appropriate" here? Are there any "better" or similar methods?
