[site]: crossvalidated
[post_id]: 447798
[parent_id]: 447775
[tags]: 
You seemed to have found the answer by yourself, so just for context, let's add some formality for you to understand it better. The distribution for coin flips is Bernoulli , so the likelihood is $$ f(x|p) = p^x \, (1-p)^{1-x} $$ where $x$ is tails encoded as $0$ , or heads encoded as $1$ , and $p \in [0, 1]$ is probability of observing heads. In frequentist setting you rely only on what you saw in the data, without making any out-of-data assumptions. In frequentist setting you would be maximizing the likelihood of seeing your result $$ \hat p_\text{ML} = \underset{p}{\operatorname{arg\,max}} \; p^x \, (1-p)^{1-x} $$ For example, if you observed $X=1$ , then you can easily check by yourself (e.g. you can plot it) that this function has the highest value for $p=1$ . This is how, given the data alone , you "assign" $\hat p_\text{ML} = 1$ , since you observed $x/n = 1/1$ heads in the sample of size $n=1$ . In Bayesian setting, you additionally are allowed (even required!) to use a prior to describe your a priori assumptions about $p$ , and you would be maximizing the posterior probability $$ \hat p_\text{MAP} = \underset{p}{\operatorname{arg\,max}} \; f(x|p)\, f(p) $$ This function does not have to have the maximum at $p=1$ , as it would depend also on the prior $f(p)$ . With flat prior $f(p) \propto 1$ it would have same maximum, as with maximum likelihood, but with other prior the results may be different (check for example how does it behave with different "uninformative" priors ). Here we take into consideration also the prior beliefs about the possible distribution of probabilities $p$ , and given the data, we update the belief.
