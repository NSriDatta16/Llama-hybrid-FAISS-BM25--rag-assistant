[site]: datascience
[post_id]: 96538
[parent_id]: 41871
[tags]: 
I have not seen the youtube video you post, but if you read "Understanding deep learning requires rethinking generalization" (Zhang, Bengio et al, 2016), you will see a very clear analysis of why this Vapnik's claim is overstating. In Statistical Learning the complexity-generalisation trade-off lead us to a pseudo-paradox: Models with more parameters than there are input data should not be able to generalise. In total disregard to the theory, in fact, they do. To sum up, VC will give you vacuous bounds for DNNs. Recently, Dziugaite and Roy "Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks" have been able to give nonvacuous bounds for Deep Learning, but they relied on the characteristics of solutions found by SGD. I recommend this video ( https://www.youtube.com/watch?v=dHUH0hmKvs8 ) with Karoline Dziugaite explaining their effort, she explains very well.
