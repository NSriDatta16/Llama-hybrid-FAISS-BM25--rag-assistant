[site]: datascience
[post_id]: 90060
[parent_id]: 89945
[tags]: 
If I understand correctly, it sounds somewhat like an NLP problem. For each row, and each column, you will compute the similarity between entries across dataframes e.g. Your two dataframes: Row182, ColumnA, DataFrame1 = XYZ_ABC_123 Row182, ColumnA, DataFrame2 = XYZ_ABC_152 New data frame: Row182, ColumnA = 0.8 Repeat for all rows and columns. You would then set some threshold at an acceptable similarity rate / accuracy of your similarity metric matrix. To highlight the most severe cross-overs. You could have a look at https://www.nltk.org/ , but also depending on the data entries you could just do a pure pythonic solution e.g. 80% match between string entries. I'm guessing at what your data looks like, so some examples there could help
