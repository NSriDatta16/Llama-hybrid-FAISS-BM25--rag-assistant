[site]: datascience
[post_id]: 121686
[parent_id]: 
[tags]: 
Distilling a Random Forest to a single DecisionTree, does it make sense?

I stumbled into this blog which shows how a decision tree trained to overfit the predictions of a properly trained random forest model is able to generalize in pretty much the same way as the original random forest. I'm interested in this as I'm implementing ML in embedded settings where a 1000 decisors RF is simply not feasible but a simpler tree with 10s of branches may be doable. My first question: is this too good to be true? The only downside I can see is that the resulting decision tree will be very large due to the overfitting process but in any instance I assume it to be simpler than the full random forest. Secondary question: is there anything in literature that discusses this process in more detail?
