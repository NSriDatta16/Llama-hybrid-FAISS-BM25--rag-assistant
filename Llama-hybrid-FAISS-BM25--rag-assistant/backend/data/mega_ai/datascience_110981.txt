[site]: datascience
[post_id]: 110981
[parent_id]: 110968
[tags]: 
Yes, absolutely. First it's important to understand that word embeddings accurately represent the semantics of the word because they are trained on the context of the word, i.e the words close to the target word. This is just another application of the old principle of distributional semantics . Characters embeddings are usually trained the same way, which means that the embedding vectors also represent the "usual neighbours" of a character. This can have various applications in string similarity, word tokenization, stylometry (representing an author's writing style), and probably more. For example, in languages with accentuated characters the embedding for Ã© would be closely similar to the one for e ; m and n would be closer than x and f .
