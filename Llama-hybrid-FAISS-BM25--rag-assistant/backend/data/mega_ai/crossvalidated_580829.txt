[site]: crossvalidated
[post_id]: 580829
[parent_id]: 574483
[tags]: 
For GPT models (or autoregressive in general) only last embedding is predicted based on the entire sequence, so it makes sense why last token is selected instead of any other. So the question seems to be does last embedding have information about whole sentence/text. I think that your intuition is accurate, but the question is if only linear layer is trained during fine tuning or is it trained jointly with GPT. If its trained jointly then the problem dissapears as the network learns to predict embeddings that describe more of entire sentence instead of describing single token.
