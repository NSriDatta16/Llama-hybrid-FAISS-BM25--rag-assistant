[site]: crossvalidated
[post_id]: 568513
[parent_id]: 567104
[tags]: 
I would naively use $\lambda^{-1}$ (or some more sophisticated similar calculation) as the number of data points With $\lambda^{-1} = 1/0.95 \approx 1.05$ you are using approximately 1.05 datapoints? That is extremely low. If you compute $$\left(1+\sqrt{\frac{45}{1.05}} \right)^2 \approx 56.8$$ then you are extremely underestimating the number of datapoints. You could determine a cut-off $\lambda$ by simulations. Repeatedly generate $45$ columns with data. Perform the EWMA smoothening and then compute the correlation matrix and eigenvalues. Then see the distribution of those eigenvalues and whether the smoothening has a large effect on te effective number of datapoints. Below is an example where we have 45 columns of 200 points and we changed the $\lambda$ variable The red curve that we added follows the relation $$\lambda_+ = \left(1 + \sqrt{\frac{m}{n(1-\lambda)}} \right)^2$$ So it seems like the moving average reduces the effective number of points approximately by a factor $1-\lambda$ . sim = function(n = 100, m = 45, q = 0.95) { ### generate data x = matrix(rnorm(m*n),n) y = x*0 ### compute moving average filt = q^c(0:(n-1)) for (i in 1:m) { for (j in 1:n) { y[j,i] = sum(filt[j:1]*x[1:j,i])/sum(filt[j:1]) } } ### standardize mu = rep(1,n) %*% t(apply(y,2,mean)) sig = rep(1,n) %*% t(apply(y,2,var)^0.5) y = (y - mu)/sig mu = rep(1,n) %*% t(apply(x,2,mean)) sig = rep(1,n) %*% t(apply(x,2,var)^0.5) x = (x - mu)/sig ### compute eigenvalues ev_x = eigen(t(x) %*% x/n) ev_y = eigen(t(y) %*% y/n) return(list(x=max(ev_x $values),y = max(ev_y$ values))) } set.seed(1) m = 45 n = 200 q = seq(0,1,0.005) values = sapply(q, FUN = function(qs) sim(n=n, q = qs)) boundary = (1+sqrt(m/n))^2 plot(c(0,1),boundary*c(1,1), type = "l", lty = 2, ylim = c(0,max(as.numeric(values))), xlab = expression(lambda), ylab = "sampled eigenvalue maximum") points(q,as.numeric(values[2,]), pch = 21, col = 1, bg = 0, cex = 0.7) lines(q, (1+sqrt( m/(n*(1-q)) ))^2, col = 2)
