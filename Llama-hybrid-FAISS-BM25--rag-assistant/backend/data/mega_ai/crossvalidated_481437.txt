[site]: crossvalidated
[post_id]: 481437
[parent_id]: 66027
[tags]: 
In time series problems, the performance often drops rapidly when the predicted phenomenon is distant in time from the train data. I am describing a related issue in this post: why performance drops so fast when test set is distant from train? : This is generally logical: in the problem described here, we try to predict human behavior (a decision to cancel the mobile telephone account). Such a decision is very possibly influenced by some very recent events (such as an irritating technical problems, or a frustrating helpline call) rather than the events more distant in time. My hypothesis: the main cause for the cancellations in days 35-45 could be the receipt of the first invoice, on day 31. There may be few day's gap between the causal event (such as frustration upon receiving the invoice) and the target action (cancellation). This short period, characterized by an increased user frustration, could indeed be reflected in change of behavior (between days 31 and day 35), and that could be detected by classifier. But since the train set contains data up to day 21 only, it misses those most vital signals. In other words, the train has no causal relationship with the target variable. Solution: get hold of the train data directly preceding the target action. Then try to predict the target action 1-2 day in advance. Chances to predict the same 15 days in advance are slim. For this reason, the literature recommends to retrain the time series models optimally every time before prediction, on the freshest data. In this particular case, it means the we need the call data from the days immediately preceding the cancellation event. Here is an article on this: ML time series forecasting the right way
