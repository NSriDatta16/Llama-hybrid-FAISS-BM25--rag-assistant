[site]: datascience
[post_id]: 69609
[parent_id]: 69505
[tags]: 
Learning with GANs is known to be unstable due to the game theoretic nature of training. Plus, loss is not always a good indicator for GANs. Therefore, sometimes it requires a trial and error approach. Here is what I would try: I. If you have not done so: Check intermediate image results of G and see if there is any development in terms of image quality. II. Check the literature for GANs which have been applied to a similar task like yours, i.e. similar images, and try them. III. Implement the WGAN exactly as described in one of the WGAN papers, e.g. the ResNet-like net in "Improved Training of Wasserstein GANs" on page 14/15 (here for 32x32 images): Also use the other hyperparameters from the paper incl. loss function, optimizer, optimizer parameters, skipped epochs for G (like your 5/1 scheme) and do not forget batch normalization (part of the Residual blocks in G in above network). Moreover, implement a simple DCGAN to obtain a baseline. IV. If your output is truly just noise it could also be an implementation issue. What I mean by "just noise" is that your GAN does not even learn very basic patters like this GAN does where the left hand side is a real example and the right hand side a fake example (output of G): However, even if your GAN is not able to learn these basic patterns and G outputs purely noise it does not necessarily imply any implementation errors.
