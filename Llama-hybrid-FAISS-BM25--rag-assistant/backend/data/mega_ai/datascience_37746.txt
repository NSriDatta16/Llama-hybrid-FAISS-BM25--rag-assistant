[site]: datascience
[post_id]: 37746
[parent_id]: 37744
[tags]: 
Decision Trees are definitely easier to overfit than Random Forests. The averaging effect (see bagging ) is meant to combat overfitting . Other than that I think the default parameters will overfit. Example: from sklearn.tree import DecisionTreeRegressor # Create a dataset x = np.linspace(0, 10 * np.pi, 50).reshape(-1,1) y = x + 3 * np.sin(x) noise = np.random.random(50).reshape(-1,1) noise -= noise.mean() # center noise at 0 noisy = y + noise * 2 # Define a Decision Tree (with default parameters) dtr = DecisionTreeRegressor() dtr.fit(x, noisy) y_dtr = dtr.predict(x) # Draw the two plots plt.figure(figsize=(14, 4)) ax1 = plt.subplot(121) ax1.plot(np.linspace(0, 10 * np.pi, 100), np.linspace(0, 10 * np.pi, 100) + 3 * np.sin(np.linspace(0, 10 * np.pi, 100)), color='gray', label='desired fit', zorder=-1, alpha=0.5) ax1.plot(x, y_dtr, color='#ff7f0e', label='decision tree', zorder=-1) ax1.scatter(x, noisy, label='data') ax1.set_xlabel('x') ax1.set_ylabel('y') ax1.set_title('Model Overfit') ax1.spines['right'].set_visible(False) ax1.spines['top'].set_visible(False) ax1.yaxis.set_ticks_position('left') ax1.xaxis.set_ticks_position('bottom') ax1.legend() ax2 = plt.subplot(122) ax2.plot(np.linspace(0, 10 * np.pi, 100), np.linspace(0, 10 * np.pi, 100) + 3 * np.sin(np.linspace(0, 10 * np.pi, 100)), color='gray', label='desired fit', zorder=-1, alpha=0.5) ax2.plot(x, y_dtr, color='#ff7f0e', label='decision tree', zorder=-1) ax2.set_xlabel('x') ax2.set_ylabel('y') ax2.set_title('Same graph') ax2.spines['right'].set_visible(False) ax2.spines['top'].set_visible(False) ax2.yaxis.set_ticks_position('left') ax2.xaxis.set_ticks_position('bottom') ax2.legend() Running the code below will produce the following figure:
