[site]: crossvalidated
[post_id]: 27181
[parent_id]: 27175
[tags]: 
Hierarchical clustering may be a good solution because it provides flexibility, a quantitative assessment of cluster similarity and grouping, and--by means of dendrograms--a natural graphical way to analyze the data. Let's illustrate with some R commands. First, the data, for those who might wish to follow along: x An "average" linkage method appears to get the results you have derived: x.cluster In this plot (produced by the second R command), the "height" is the Euclidean distances between the columns of the matrix x (thought of as 10-vectors). The clusters (1), (3(4(69))), (0(78)), and (25) are evident among the leaves near the bottom of this plot. What you consider to be a "cluster" depends on the cutoff height. E.g., cutting the tree at height 52 reproduces your four clusters: > split(0:9, cutree(x.cluster, h=52)) $`1` [1] 0 7 8 $`2` [1] 1 $`3` [1] 2 5 $`4` [1] 3 4 6 9 whereas cutting it slightly lower (at 40) will split column 3 away from (4(69)). For links to the documentation for hclust , plclust , and cutree , as well as links to alternative clustering solutions in R , please see the Cluster Analysis Task View .
