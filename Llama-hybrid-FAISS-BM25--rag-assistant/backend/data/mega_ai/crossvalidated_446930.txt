[site]: crossvalidated
[post_id]: 446930
[parent_id]: 
[tags]: 
Derivation of the Objective Function for Expectation Propagation

I was reading Expectation Propagation As A Way Of Life and the original paper by Minka Expectation Propagation for Approximate Bayesian Inference and they both say that a fixed point of the EP algorithm is equivalent to a stationary point of the following objective function \begin{alignat}{2} &\!\min \limits_{\boldsymbol{\lambda}} \max \limits_{{\boldsymbol{\lambda}}_{\backslash k}} &\qquad& (K-1)\log \int p(\theta) \exp({\bf{s}}^\top \boldsymbol{\lambda}) d\theta - \sum_{k=1}^K \log \int p(\theta) p(y_k\mid \theta) \exp({\bf{s}}^\top \boldsymbol{\lambda})d\theta\\ &\text{s.t.} & & (K-1){\boldsymbol{\lambda}} = \sum_{k=1}^K \boldsymbol{\lambda}_{\backslash k} \end{alignat} However they don't show how it's derived. I looked everywhere and I couldn't find an answer. I tried to do the proof myself but failed. I think the proof involves two steps: showing that stationary points of teh objective function above are also fixed points of the EP algorithm, and that viceversa fixed points of the EP algorithm are stationary points of this objective function. Can someone help?
