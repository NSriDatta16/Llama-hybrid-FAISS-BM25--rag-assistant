[site]: crossvalidated
[post_id]: 27234
[parent_id]: 6252
[tags]: 
A method such as that used in unsupervised random forest could be used. Random Forest algorithms treat unsupervised classification as a two class problem, were a whole different artificial and random data set is created from the first data set by removing the dependency structure in the data (randomization). You could then create such a artificial and random data set, apply your clustering model and compare you metric of choice (eg. SSE) in your true data and your random data. Mixing in randomization, permutation, bootstrapping,bagging and/or jacknifing could give you a measure similar to a P value by measuring the number of times a given clustering model gives you a smaller value for you true data than your random data using a metric of choice (eg. SSE, or out of bag error prediction). Your metric is thus difference (probability, size difference,...) in any metric of choice between true and random data. Iterating this for many models would allow you to distinguish between models. This can be implemented in R. randomforest is available in R
