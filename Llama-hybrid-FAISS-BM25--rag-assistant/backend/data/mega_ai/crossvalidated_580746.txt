[site]: crossvalidated
[post_id]: 580746
[parent_id]: 579320
[tags]: 
Correct, the method is randomized, so you can get different results on each run. For reproducibility, you can fix the random seed. If there is single global minimum, you would reach it if you try long enough. How long it is depends on how big the search space is. With continuous parameters the space is infinite, what would make it infinitely long. There were experiments showing that random search can give equally good results as “smarter” algorithms if you use twice as many iterations. With hyperparameter tuning we usually don’t aim at finding the global minimum, but at finding better hyperparameters. The machine learning algorithms are usually pretty robust to the choice of hyperparameters, so you don’t really need to find the “best” ones. Tuning is designed for being efficient in exploring the space, not for being the most precise.
