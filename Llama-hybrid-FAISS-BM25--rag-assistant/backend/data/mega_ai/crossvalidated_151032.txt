[site]: crossvalidated
[post_id]: 151032
[parent_id]: 150936
[tags]: 
@RichardHardy's observation that you're modelling a dichotomous response using ordinary least squares regression is at the heart of the matter: the distribution of errors is far from normal, & with a single observation at $x=1$, the distribution under the null of the standardized coefficient estimate for $x$ will show a gross departure from a t-distribution. Your description of the situation & the predictor distribution are enough to raise a red flag, but the usual regression diagnostics will highlight the issue: a clearly non-normal distribution of the residuals & the hugely disproportionate leverage of a single point. In general tables of coefficient estimates, standard errors, t-statistics, &c. don't help you to check whether the model's badly mis-specified (that's why we have diagnostics)â€”in this case, knowing that $x$ is dichotomous & that reference-level coding was used, & without other predictors to muddy the waters, the large difference in standard errors between the estimates for the intercept & for the $x$ coefficient reflects the discrepancy in sample size between the two groups. # make data & fit model x lmod # leverage plot plot(hatvalues(lmod), type="h") # normal quantile-quantile plot qqnorm(residuals(lmod)) # simulate y values under the null of common 10% probability for Y=1 t.stat mod.sim summary(mod.sim)$coef["x", "t value"] -> t.stat[i] } # plot kernel-smoothed density estimate of t-statistic distribution # for the x coefficient plot(density(t.stat)) # plot empirical distribution function of t-statistic plot(ecdf(t.stat)) # compare to theoretical distribution t.cumve
