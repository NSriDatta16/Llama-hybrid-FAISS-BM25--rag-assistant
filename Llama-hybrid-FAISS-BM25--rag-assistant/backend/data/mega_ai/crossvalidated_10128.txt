[site]: crossvalidated
[post_id]: 10128
[parent_id]: 6698
[tags]: 
Although this question has already been answered a useful way to remember for more general situations is the law of iterated expectations. Note that the independence for prediction does not hold even if the "true process" is independent. This is because the estimates are not independent, unless you have $Z^{T}Z$ and $Z_{new}Z_{new}^{T}$ both to be diagonal ("new" for the predictions) So if you let $\hat{Y}_{ti}$ denote the estimated monthly values in year $t$ for month $i$, and $\hat{X}_{t}$ denote the estimated annual value, you have: $$\hat{X}_{t}=\sum_{i=1}^{12}\hat{Y}_{ti}$$ $$Var(\hat{X}_{t})=E[Var(\hat{X}_{t}|\hat{Y}_{t,1},\dots,\hat{Y}_{t,12})]+Var[E(\hat{X}_{t}|\hat{Y}_{t,1},\dots,\hat{Y}_{t,12})]$$ (note sure if it should be an average or total, if average, then divide my final result for the standard error by $12$ and divide variance by $144$) Plugging one into the other we get: $$Var(\hat{X}_{t})=E[Var(\sum_{i=1}^{12}\hat{Y}_{ti}|\hat{Y}_{t,1},\dots,\hat{Y}_{t,12})]+Var[E(\sum_{i=1}^{12}\hat{Y}_{ti}|\hat{Y}_{t,1},\dots,\hat{Y}_{t,12})]$$ $$=Var[\sum_{i=1}^{12}\hat{Y}_{ti}]=\sum_{i=1}^{12}\sum_{j=1}^{12}Cov(\hat{Y}_{tj},\hat{Y}_{ti})$$ Now when you condition on something, it is a constant, so that's why the "inner" variance term disappears. Now you have a regression model for $Y_{ti}$ so we know that $$\begin{array}{l l} \hat{Y}_{ti}=Z_{ti,new}^{T}\hat{\beta} & Cov(\hat{Y}_{ti},\hat{Y}_{sj})=s^{2}Z_{ti,new}^{T}(Z^{T}Z)^{-1}Z_{sj,new} \\ \hat{\beta}=(Z^{T}Z)^{-1}Z^{T}Y & s^{2}=\frac{1}{n-dim(\hat{\beta})}(Y-Z\hat{\beta})^{T}(Y-Z\hat{\beta}) \end{array}$$ Where $Z$ and $Y$ are the matrix and vector that you used to actually fit the regression (I am assuming OLS regression here), $dim(\hat{\beta})$ is the number of betas that you have fitted (including the intercept). $Z_{ti,new}$ is a new set of regression co-efficients to be used in the prediction. Note that for prediction, your estimates of $Y$ are not independent , even if the "true values" are. So the square root of $N$ rule doesn't apply, unless your $Z$ variables are orthogonal so that $(Z^{T}Z)^{-1}=I$ and $Z_{ti}^{T}Z_{sj}=0$ when $s\neq t$ or $i\neq j$. Plugging this into the variance formula for $\hat{X}_{t}$ we get: $$Var(\hat{X}_{t})=\sum_{i=1}^{12}\sum_{j=1}^{12}s^{2}Z_{ti,new}^{T}(Z^{T}Z)^{-1}Z_{tj,new}=s^{2}J^{T}Z_{t,new}(Z^{T}Z)^{-1}Z_{t,new}^{T}J$$ Where $J$ is a column of 12 ones, and $Z_{t,new}$ is the twelve $Z_{ti}^{T}$ rows for prediction stacked on top of each other, of dimension $12\times dim(\hat{\beta})$. But note that we also have the "true" process $X_{t}$, assumed to be governed by the regression model, so we apply the law of iterated expectations again, but conditioning on $\hat{X}_{t}$ this time: $$Var(X_{t})=E[Var(X_{t}|\hat{X}_{t})]+Var[E(X_{t}|\hat{X}_{t})]=E[Var(\sum_{i=1}^{12}Y_{t}|\hat{Y}_{ti})]+Var[\hat{X}_{t}]$$ $$=E[\sum_{i=1}^{12}Var(Y_{ti})]+Var[\hat{X}_{t}]=12s^{2}+s^{2}J^{T}Z_{t,new}(Z^{T}Z)^{-1}Z_{t,new}^{T}J$$ I probably should put approx because this is a "plug-in" of $s^{2}$ for the "true variance" $\sigma^{2}$ - however I don't know of many people who don't just do this. It is also justified on Bayesian grounds as the proper way to account for uncertainty in estimating $\sigma^{2}$ for the normal model, plus it is an unbiased estimator on frequentist grounds. So the annual standard error should really be $$s\sqrt{12+J^{T}Z_{t,new}(Z^{T}Z)^{-1}Z_{t,new}^{T}J}$$ So what the $\sqrt{12}$ rule is essentially doing here is ignoring the uncertainty in estimating the betas. If you already estimate the betas pretty well, then this will make little difference to the $\sqrt{12}$ rule - probably something like $\sqrt{13}$. If the betas are not estimated well, or you are close to multi-collinearity, then the extra term may be important.hb** text **
