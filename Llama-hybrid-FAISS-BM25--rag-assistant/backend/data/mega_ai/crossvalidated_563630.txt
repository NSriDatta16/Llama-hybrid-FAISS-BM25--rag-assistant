[site]: crossvalidated
[post_id]: 563630
[parent_id]: 
[tags]: 
Applying machine learning on a point cloud dataset

I have multiple 3d models in ply format. These models are archeological imprints. And I have to make a model to classify the dataset according to their imprint type. I have 4 different classes. I use this code to parse the data. This code samples the dataset and extract the labels in a separate list. def parse_dataset(num_points=2048): all_points = [] all_labels = [] class_map = {} folders = [os.path.normpath(i) for i in glob.glob("File Path")] print(folders) for i, folder in enumerate(folders): print("processing class: {}".format(os.path.basename(folder))) # store folder name with ID so we can retrieve later class_map[i] = folder.split("/")[-1] # gather all files train_files = [os.path.normpath(i) for i in glob.glob(os.path.join(folder,"*"))] #print(train_files) for f in train_files: all_points.append(trimesh.load(f).sample(num_points)) all_labels.append(i) return ( np.array(all_points), np.array(all_labels), class_map, ) NUM_POINTS = 10000 all_points, all_labels , CLASS_MAP = parse_dataset( NUM_POINTS ) Then I split the dataset and pass them to a SVM. The problem is that the maximum accuracy I can get is 42%. As I am new to these types of files I don't know how to preprocess these files to increase my accuracy. I have a total of only 520 ply files. So i guess a deep learning model won't work well on a small dataset. Can anybody recommend any data augmentation or pre-preprocessing techniques to increase the accuracy of my model?
