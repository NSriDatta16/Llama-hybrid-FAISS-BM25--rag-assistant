[site]: crossvalidated
[post_id]: 79551
[parent_id]: 79542
[tags]: 
You're confusing the dimensionality of your data set with its size. They get swapped all the time informally, but the difference matters here: Your data set has a dimensionality of 32000 cases x 135 features. The size of your data set is 8500 KB Note that the units aren't compatible. PCA reduced the dimensionality of your data from 32000 x 135 -> 32000 x 54 (2.5x, not too shabby). PCA is pretty agnostic to how the data is stored, and unfortunately, switching from boolean values (a smart program can fit 8 to a byte) to doubles (8 bytes/double) more than counteracts whatever savings you get from projecting your data onto a lower dimensional subspace. If reducing the memory footprint of your data is the only reason you care, I'd just skip it. (It's only 8 Mb anyway!). However, if you're interested in feature selection, you could do a factor analysis and manually look for variables that are either poorly correlated with the target variable OR are highly correlated with many other variables. These could then be eliminated from your original data set. This might give you a modest reduction in its dimensionality, while keeping it as a collection of binary features.
