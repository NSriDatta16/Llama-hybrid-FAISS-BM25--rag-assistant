[site]: crossvalidated
[post_id]: 317428
[parent_id]: 317420
[tags]: 
I know pretty well that the parameters search (e.g. weights of your neural net) needs to be done in your training set alone, but is it the same for your hyperparameters? Yes. Models with many hyperparameters are more prone to overfit even if you select the hyperparameters inside the inner CV (that's using train data only). Given that you have chosen the CV design perfectly, you still have a chance of selecting the hyperparameter set that is overfit to your validation folds. A good example is random forest or GBM, where you don't optimize weights but solely make a choice of the complexity of the model and its learning rate, which are hyperparameters. You need outer crossvalidation in order to estimate how badly your inner-CV overfitted your model. A canonical k-fold CV is known to fail to meet the assumptions of error independence thus creating optimistically biased error (loss-function value). However good the CV is, the more hyperparameters you search through the more chances that your model will behave as though you used the whole dataset for training. Then the outer-CV may come in handy. Reading that may help get it: Is cross-validation enough to prevent overfitting?
