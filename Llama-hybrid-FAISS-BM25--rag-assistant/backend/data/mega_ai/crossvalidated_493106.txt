[site]: crossvalidated
[post_id]: 493106
[parent_id]: 
[tags]: 
Is there a better way to describe a model's generalization performance than "under" and "overfitting"?

To me, under and overfitting are the two of the most vague concepts in machine learning. From Google's first link when you look up these definitions. A model is said to be underfitted if it "performs badly" on the training as well as test set. And A model is said to be overfitted if it "performs well" on the training set but "performs poorly" on the test set. And it is usually follow by either a graph of the training/validation error plot or some curve associated a particular model (model is never specified, hence curve not reproducible). I don't need to go into the details why "performs badly, well, good", etc. is subjective and leaves a lot of room for guessing. I also don't want to go into detail why deep network tend not to overfit even when you train for a very high number of epochs. Why is this concept so central to machine learning when it is so vague at the same time? Is there a better metric or descriptor of generalization of a model as of 2020 than "over/underfitting"? A more radical idea: should we completely abandon this notion because it is vague?
