[site]: datascience
[post_id]: 82184
[parent_id]: 
[tags]: 
Confusion with Notation in the Book on Deep Learning by Ian Goodfellow et al

In chapter 6.1 on ' Example: Learning XOR ', the bottom of page 168 mentions: The activation function $g$ is typically chosen to be a function that is applied element-wise, with $h_i = g(x^TW_{:,i}+c_i).$ Then we see equation 6.3 is defined as (assuming g as ReLU): We can now specify our complete network as $f(x; W,c,w,b) = w^T$ max $\{0, W^Tx + c\} + b$ Wondering why the book uses $W^Tx$ in equation 6.3, while I expect it to be $x^TW$ . Unlike XOR example in the book where $W$ is a $2\times2$ square matrix, we may have non-square $W$ as well, and in such cases, $x^TW$ is not same as $W^Tx$ . Please help me understand, if I'm missing something here.
