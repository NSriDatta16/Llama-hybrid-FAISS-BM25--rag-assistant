[site]: datascience
[post_id]: 128276
[parent_id]: 128274
[tags]: 
LLMs are usually fine-tuned on instruction-following datasets so that they can answer user requests. For instance, ChatGPT comes from InstructGPT , which was trained from GPT-3 with reinforcement learning with human feedback (RLHF) on an instructions dataset (you can learn more about the evolution of ChatGPT/GPT-4 from the original GPT in this answer ). As for the prompts that request the LLM to impersonate a role giving better results, you can check the article In-Context Impersonation Reveals Large Language Models' Strengths and Biases , which verifies the claim. The reason for that is difficult to figure out, given the black-box nature of LLMs and deep learning in general.
