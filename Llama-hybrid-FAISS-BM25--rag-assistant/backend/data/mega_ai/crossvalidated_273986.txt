[site]: crossvalidated
[post_id]: 273986
[parent_id]: 
[tags]: 
Why does Multinomial naive bayes work better than SVM and Logistic Regression on small amount of data

I want to do aspect-based sentiment classification on user text reviews for mobile phones. For a mobile, aspect could be the camera, battery, processor, screen, memory etc. A text review is first broken into sentences and then the aspect for each one of them is determined on the basis of a pre-maintained keyword list. One sentence could fall into more than one aspect category. I have to manually annotate each sentence for positive and negative sentiment. With a small number of sentences(say 100 sentences for positive and negative each) I trained classifier- Multinomial Naive Bayes , Logistic Regression and SVM out of which multinomial have best prediction power. I do not have a testing set right now but I could see the prediction and the probability estimate given by multinomial are more satisfactory than the rest. If anyone could help me to explain this behavior since I am asked by my Director to explain it to him.
