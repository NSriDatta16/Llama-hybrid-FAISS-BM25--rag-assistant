[site]: crossvalidated
[post_id]: 429503
[parent_id]: 429115
[tags]: 
Disclaimer/ Important Note: The example below assumes that the ratio of probabilities to select certain bars is completely independent from the specific selection available. This is likely not the case. For instance the decoy effect is an obvious example that shows that relative probabilities to choose a specific bar may drastically change (even inverse) depending on the options that are available to the participants/consumer. If you wish to capture these effects then you will need to make the model much more flexible. But then the question is more about how to model preference (the deterministic part of the underlying probabilities) rather than modelling the random behaviour. The example below is an example how you could proceed once you have a model . It shows how you can use the data to estimate the parameters by finding the parameters that maximize the likelihood function. But note that to obtain a reasonable idea about the model to use you should first start by exploring/plotting your data, see what it is and how it behaves, before you apply a statistic model (such as assuming a Dirichlet distribution). Logistic regression You might model your system in terms of the Gibbs Measure (that means a multinomial distributed variable, and not a Dirichlet distributed variable). Let $P(Y_i = j)$ be the probability that a person $i$ selects candybar $j$ with properties $X_{j}$ : $$P(Y_i = j) = \frac{e^{-\beta X_j}}{\sum_{\forall j} e^{-\beta X_j}}$$ Here the term $e^{-\beta X_j}$ relates to logistic regression (you might wish to model it differently, more complex, as mentioned before) and the term $\sum_{\forall j} e^{-\beta X_j}$ is a normalization constant. Example computation, maximizing log likelihood This normalization constant is problematic in your situation because the available bars $j$ are not constant every time. I do not know of a package that allows you compute this (it is also not a typical situation). The example below shows how you might do it (manually) by finding the parameters that optimize the likelihood function: set.seed(1) # sample size n which in this case has model parameters $1,1$ and will return as result $0.87, 0.91$ . I imagine that in a more typical situation you would not use a linear model as above but instead some sort of neural network that allows to capture more various features than the fixed presupposed (limited) linear model. Dirichlet regression Dirichlet regression models your variables as a Dirichlet distributed variable. But a multinomial distribution feels more natural to me (probabilities for counts). Your raw data is categorical, and has values 0 or 1. This is not what a Dirichlet distribution describes (it describes a continuous distribution). Possibly, you wish to model the distribution like a Dirichlet-multinomial distribution . If you insist on using a Dirichlet distribution then you would need to have outcome variables that describe fractions. E.g. you have several times a group of 100 people and you observe the fractions of bars in those groups. Here you will assume that for a single group the distribution will be multinomial, but the parameters that describe that multinomial distribution are themselves variable (not every group of 100 people is the same) and distributed according to a Dirichlet distribution. In any case, because you are dealing with a result vector that is not continuous you may not be able to use standard R packages like DirichletReg . Such packages model the outcome variable $Y_i$ for each sample $i$ as a ratio of different fractions ( $Y_i$ is a vector) as a function $\beta X_i$ , but here $X_i$ are properties of the sample $i$ (describing the group, e.g. age, gender, etc.). Hack example In those standard packages (which model $Y_i$ as a single function of properties $X_i$ that does not allow different $Y_i$ ) there is no way to make the vector $Y_i$ with different sizes for different samples/observations $i$ . You might hack it by adding some variables in the matrix $X_i$ that describes whether or not the particular option was present or not, but it is not natural/exact/realistic/precise. (I show this to illustrate the idea, I am not suggesting that you should do this) Example with multinom package # make predictor variable 'choicematrix' that is defined by whether or not the # particular bar was a choice or not. sel This models the probability for each bar to be selected as a function of whether or not certain bars are in the sample. (so the properties $X_i$ refer to the test conditions defining which bars where not in the sample). The intercept will refer to the solution giving a probability to each bar (but not as some function of the properties of that bar).
