[site]: datascience
[post_id]: 117468
[parent_id]: 
[tags]: 
Implementation of PCA algorithm for reconstruction of data(images)

I'm learning the theory and implementation of PCA algorithm in the book 'Mathematics For Machine Learning' and finishing the official tutorial notebook in https://colab.research.google.com/github/mml-book/mml-book.github.io/blob/master/tutorials/tutorial_pca.ipynb . Taking an encoder-decoder perspective, as the figure shows, the algorithm could be formulated as a linear, lossy auto-encoder. Specifically, the centered data matrix $X\in \mathbb{R}^{D\times N}$ , where $N, D$ are the number of samples and the sample dimension respectively. The encoder part is: $$ Z=B^TX\in \mathbb{R}^{M\times N} $$ where $B\in \mathbb{R}^{D\times M}$ is constructed by the $M$ eigenvectors as columns corresponding to the $M$ largest eigenvalues of covariance matrix $S=\frac{1}{N}XX^T$ . And the reconstruction data by the decoder are: $$ \hat{X}=BZ=BB^TX\in \mathbb{R}^{D \times N} $$ So the projection matrix is $P=BB^T$ . But in the official implementation/solution of the tutorial in https://colab.research.google.com/github/mml-book/mml-book.github.io/blob/master/tutorials/tutorial_pca.solution.ipynb , it calculates the projection matrix as follows: projection_matrix = (B @ np.linalg.inv(B.T @ B) @ B.T) and the reconstruction matrix as: reconst = (projection_matrix @ X.T) And this corresponds to calculating the projection matrix as follows: $$ P=B(B^TB)^{-1}B^T $$ instead of $BB^T$ . The only difference of the implementation here is that the data matrix is transposed as opposed to the formulation in the book, so $X\in \mathbb{R}^{N\times D}$ . So I'm confused by the way the tutorial calculates the projection matrix. Can anyone give some explanations on this?
