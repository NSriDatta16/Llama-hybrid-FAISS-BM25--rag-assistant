[site]: datascience
[post_id]: 20394
[parent_id]: 20367
[tags]: 
Cleaning data and bringing it into a proper format is very often necessary but I would call this step data preparation rather than feature engineering . It may totally make sense to apply normalization techniques as Felipe Cruz already explained. However, neural networks basically can approximate any function (see this post ) and thus this is also not really necessary. It may still help especially in terms of training time, though. A strength of neural networks comes from them learning the relevant features themselves. You might develop a feature based on the combination of two or more properties of your input but a neural net with proper architecture would also be able to "come up" with this feature on its own if it sees enough samples for this during training. I will go with the example of text here but analogous steps could basically be taken for any kind of input. Lets suppose we're training a sentiment classifier for tweets. The data could be cleaned by applying for example a spelling correction algorithm. Another preparation step then would be the vectorization of the input. We could tokenize the tweets and based on a vocabulary transform each word into a one-hot vector . As for feature engineering, one important feature might be negation, i.e. the difference between "happy" and "not happy". It would take great effort to "manually" solve this problem as there are many ways to express negation (and then there's also irony...). Suppose now we use a recurrent neural network that reads in the tweets word by word to predict the class in the end. In the optimal case the network would learn this feature (and others) itself based on examples in the training data where negation influences the outcome. I.e. in a simple case if there was a negating word like not in the sequence before reading the word happy the output is altered accordingly. Then again, applying some domain knowledge to extract features could help the model to identify important patterns more easily and thus speed up learning (if the features you develop are really relevant and sufficiently represented in your training data). But rather than focusing on feature engineering, it is the architecture of your net that you normally focus on to make the net learn the features itself. In the end you only will know what works best by trying it out.
