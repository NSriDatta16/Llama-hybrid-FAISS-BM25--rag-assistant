[site]: crossvalidated
[post_id]: 367205
[parent_id]: 
[tags]: 
Computing average precision metric and cost function for object detection task using scikitLearn and Tensorflow

I have a Data set that contains 5 thousand pictures of my object of interest and 5 thousand pictures with out it. I trained a Convolutional Neural Network using Tensor Flow to detect the position of that object. The output vector of my network is [p, x, y, w, h] where p is the probability of the object within the image and x , y , w , h values define the bounding box The cost function that I am using looks as follows: cost_pos = tf.multiply( y[:,0],tf.reduce_sum(tf.squared_difference(predictions,y),1)) cost_neg = tf.multiply(tf.subtract(1.0,y[:,0]),tf.squared_difference(predictions[:,0],y[:,0])) cost = tf.reduce_mean(tf.add(cost_pos,cost_neg)) where predictions is the output of the network and y is the ground truth label. In order to measure the performance of the model I need to compute the mean average precision. So ,I implemented this helper function: def AP(y,y_hat): y_scores = y_hat[:,0] y_hat_bbox = y_hat[:,1:] y_bbox = y[:,1:] y_true = np.zeros(y_scores.shape[0]) for i in range(y_true.shape[0]): y_true[i] = iou(get_corners(y_hat_bbox[i]), get_corners(y_bbox[i]), True) AP = average_precision_score(y_true = y_true, y_score = y_scores) return AP Note: average_precision_score corresponds to: The scikit Learn implementation of Average Precision But I am not sure if this is a correct implementation of it, and if this value is really representative of the performance of my model, and also it my cost function is correct or not, or if I should simply use a mean squared error for this task.
