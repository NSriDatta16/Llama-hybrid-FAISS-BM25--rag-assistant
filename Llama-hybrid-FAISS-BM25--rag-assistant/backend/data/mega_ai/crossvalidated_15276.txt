[site]: crossvalidated
[post_id]: 15276
[parent_id]: 15227
[tags]: 
The term "feature" is typically used generally within the context of machine learning. Given some raw data matrix $X$ comprised of vectors $x_i$, and outputs $y_i$, you transform each $x_i$ into some $x_i^{'}$. The point of this is to have feature values that are as telling as possible with respect to the output; for example, you would want them to be correlated in some manner, or at least have like outputs near like inputs in Euclidean space. Then you train your model (linear regression, CART tree, whatever) on the new $x_i^{'}$. In the context of NLP, given a sentence as your raw input data, perhaps (for example) your feature function maps each $x_i$ to a new vector containing counts of nouns, verbs and adjectives, respectively. You might take this approach if you thought the different parts of speech would impact the output variable significantly. In your case, it sounds like the data gets parsed and then the parsed data is somehow converted by the feature function into a real-valued vector suitable for running machine learning algos on. If your prof said that the vector was obtained by means of a classification algorithm, that simply means that a classification algorithm (of any type, be it SVM, LDA, Naive Bayes, etc.) was used to create the features. It seems he's doing some sort of nested learning. Perhaps he's classifying whether a particular parsed sentence has feature Q, and then putting the certainty of having feature Q into the feature vectors, for example.
