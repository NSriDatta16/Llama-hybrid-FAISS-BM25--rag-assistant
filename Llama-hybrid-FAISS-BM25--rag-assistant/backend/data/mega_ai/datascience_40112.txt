[site]: datascience
[post_id]: 40112
[parent_id]: 40107
[tags]: 
Yes, XGBoost is famous for having been demonstrated to attain very good results using small datasets often with less than 1000 instances. Of course when choosing a machine learning model to fit your data, the number of instances is important and is related to the number of model parameters you will need to fit. The greater the number of parameters in the model the more data you will need to reduce the bias of your final model. If you do get good results using a complex model on very few instances then there is a high probability that you are overfitting. For example 1000 instances is hardly enough to fit a deep neural network. That being said, the distribution of your classes and the noise in the data is always going to be a limiting factor to how well any model you select will fit your data.
