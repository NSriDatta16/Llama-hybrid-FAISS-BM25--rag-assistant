[site]: datascience
[post_id]: 38184
[parent_id]: 38143
[tags]: 
My view on your question, is that tiny networks seldom work. The above method uses a Neural Network to learn the function $y=\sin(x)$ . Although this problems seems simple, it cannot be expected to be solved by a really tiny network (the above model uses a 5-layer MLP with hidden size [5,6,7], which is small). Even if back-propagation is implemented correctly, would the model learn anything? No. I suppose Tensorflow implemented back-propagation correctly, here is the result using Tensorflow: You see, it learns almost nothing . In fact, the MSE loss is very close to 0.5 as stated above. My suggestion is to try a 3 layer MLP with hidden size 256. Here is the result: You can see it's much better. MSE ------------------code--------------------- x_ =np.atleast_2d(np.arange(0,360,1)).T y_ = np.atleast_2d(np.sin(x_/180*np.pi)) g = tf.Graph() with g.as_default(): with tf.variable_scope("mlp"): input_x = tf.placeholder(shape=[None, 1], dtype=tf.float32) input_y = tf.placeholder(shape=[None,1], dtype=tf.float32) layer1 = tf.layers.dense(inputs=input_x, units=256, activation=tf.nn.sigmoid) #layer2 = tf.layers.dense(inputs=input_x, units=6, activation=tf.nn.sigmoid) #layer3 = tf.layers.dense(inputs=input_x, units=7, activation=tf.nn.sigmoid) output_y = tf.layers.dense(inputs=layer1, units=1) # inputs=layer1 loss = tf.losses.mean_squared_error(input_y, output_y) train_op = tf.train.AdagradOptimizer(0.01).minimize(loss) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for epoch in range(300): _, loss_ = sess.run((train_op, loss), feed_dict={input_x:x_, input_y:y_}) y_hat_ = sess.run(output_y, feed_dict={input_x:x_, input_y:y_}) print(loss_, end='\t') plt.plot(x_,y_, 'g', x_,y_hat_,'b') plt.legend(['ground truth', 'predicted'])
