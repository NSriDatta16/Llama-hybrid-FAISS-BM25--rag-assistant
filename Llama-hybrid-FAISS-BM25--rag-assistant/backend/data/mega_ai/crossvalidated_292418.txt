[site]: crossvalidated
[post_id]: 292418
[parent_id]: 
[tags]: 
What is a good reason why reinforcement learning relies on Q value instead of the reward r?

I am new to reinforcement learning and I am reading off of some tutorial materials , but I have noticed a seldom discussed assumption that we should calculate our action based on Q-value instead of the reward. Is there a simple reason why this is the case? Consider the bandit problem. What happens if we did not use Q-value, but simply used the reward to select our strategy? It seems if we see our action yields the highest reward, we should simply repeat our action, instead of calculating some estimate on the reward and base off our decision on the estimate. Further, wouldn't greedy action selection (or Boltzmann action selection, etc.) make much more sense because we are selecting our action based on highest reward instead of the highest estimate, which may not even be a good estimate? Lastly, it seems that this would model real life scenario much more closely. I haven't noticed anybody calculating a Q-value in their head when making decisions in reinforcement learning scenarios, almost all of our decisions are based on the immediate (and most frequently, deterministic) reward, especially in game scenarios. I apologize in advance if this seems to be a really novice observation.
