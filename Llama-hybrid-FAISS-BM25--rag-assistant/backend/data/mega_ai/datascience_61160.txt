[site]: datascience
[post_id]: 61160
[parent_id]: 61140
[tags]: 
Just to add some general thoughts to the other answers. Gradient boosting is fairly robust to overfitting through increasing the number of trees. Increasing the number of trees is expected to increase the performance if the learning rate is small. It is therefore generally considered best to set the number of trees through early stopping instead of treating them like other hyperparameters. You would set a small learning rate (something $\eta ) and the number of trees to a large value, and stop adding trees once you don't see any more improvements on a separate validation set. As a last optional step, since adding more and more trees results in smaller and smaller gains in performance, once you have found a satisfactory model and want to put it in production, you can analyze how much you can reduce the number of trees without significantly reducing the performance. You can then trim the number of trees to speed up computation in production if speed is an issue. This idea is often also used for Random Forests because here adding more trees should never decrease performance.
