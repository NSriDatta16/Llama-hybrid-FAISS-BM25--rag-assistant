[site]: crossvalidated
[post_id]: 486588
[parent_id]: 
[tags]: 
Bayesian update on two possibilities when test is unreliable

Imagine I have an integer variable A that can be 1 or 2 . And I have a test that returns the value of A , but this test fails (returns wrong value) with probability $\beta$ . I start out with a uniform prior: $P(A=1) = 0.5, P(A=2) = 0.5 $ . I perform a single test, and it returns A=1 . According to standard Bayes formula I can update my beliefs as follows: $$ P(B|A=1) = (1-\beta) \\ P(B|A=2) = \beta \\ P(A=1) = \frac{P(B|A=1)P(A=1)}{P(B)} = \frac{(1-\beta)0.5}{0.5} = 1-\beta \\ P(A=2) = \frac{P(B|A=2)P(A=2)}{P(B)} = \frac{\beta0.5}{0.5} = \beta $$ If I plug in some value for $\beta$ , for example 20%, then I get $P(A=1) = 0.8, P(A=2) = 0.2$ . And that makes sense - if I performed a single test with 20% failure rate I would be about 80% sure in the outcome. But now imagine I perform three tests in a row (starting with the same uniform prior), and they return A=1 , A=2 , A=1 . I calculated the update: $$ P(B|A=1) = (1-\beta)\beta(1-\beta) \\ P(B|A=2) = \beta(1-\beta)\beta \\ P(A=1) = \frac{P(B|A=1)P(A=1)}{P(B)} = \frac{P(B|A=1)P(A=1)}{avg(P(B|A=1),P(B|A=2))} = \frac{(1-\beta)\beta(1-\beta)0.5}{((1-\beta)\beta(1-\beta) + \beta(1-\beta)\beta)0.5} = \frac{(1-\beta)^2\beta}{(1-\beta)\beta(1 - \beta + \beta)} = 1-\beta \\ P(A=2) = \frac{P(B|A=2)P(A=2)}{P(B)} = \frac{P(B|A=2)P(A=2)}{avg(P(B|A=1),P(B|A=2))} = \frac{\beta(1-\beta)\beta0.5}{((1-\beta)\beta(1-\beta) + \beta(1-\beta)\beta)0.5} = \frac{(1-\beta)\beta^2}{(1-\beta)\beta(1 - \beta + \beta)} = \beta $$ I get the same result! And I will get the same result even if I have 1001 tests for A=1 and 1000 for A=2 - these calculations show that I should still trust the A=1 with 80% ( $1-\beta$ ) confidence, even though I had mostly the same number of evidence both ways. This seems incorrect. What am I missing?
