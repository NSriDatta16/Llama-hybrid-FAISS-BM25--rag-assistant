[site]: crossvalidated
[post_id]: 566932
[parent_id]: 566930
[tags]: 
I would not like to take it, it looks more like an outlier Several algorithms for outlier detection and removal could work. In the image it seems that if you use a simple smoothening of the curve then the weight of small peaks becomes less and this would already do the job. How to smoothen or detect outliers for best performance in you application, for instance how to determine by which degree you smoothen, depends on what you exactly want to achieve and what this maximum is supposed to do. What do the peaks/outliers mean, how do they arrise, and why do you not wish to take them into account? Possibly using some more advanced filter might be better than just a simple moving average. The green curve that you got there in the picture also seems to do the job already. From the comments it seems like this problem is about a situation where the points on the surface are not already sampled for the entire space (like in a grid search or parameter sweep). So this relates to some optimization problem that follows a path. But in this case you can still make use of outlier detection and smoothening. Namely, you can incorporate the expressions that do the smoothening inside your cost function. For instance, instead of sampling the cost function in a single point you could also sample points in the neighbourhood and take an average. In this case you probably want to use some smoothening that doesn't use too much data points and uses local information. I imagine you could use some average or possibly a small kernel to blur the values. Several optimisers are already doing this sampling in multiple points, in order to find the derivative of the surface (the method SANN* in the package optim searches for an optimum by sampling several points and applying a Gauss-Markov kernel filter). Possibly you could tune them to obtain a bit more extra points and smoothen more strongly at the same time. You can do this also yourself by providing functions for the value and derivative of the cost function that apply the smoothening and use those with an optimiser. Some packages seem to exist that are dealing with this problem. For instance tgp has some version of optim that works with noisy functions. *Bélisle, Claude JP. "Convergence theorems for a class of simulated annealing algorithms on ℝ d." Journal of Applied Probability 29.4 (1992): 885-895.
