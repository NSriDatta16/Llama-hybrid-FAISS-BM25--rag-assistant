[site]: crossvalidated
[post_id]: 638786
[parent_id]: 107597
[tags]: 
The answer above is complete and great. The following aims to complement the answer with a different perspective. I also want to bring attention to the fact that using covariances to estimate coefficients you are implicitly assuming there exists a constant factor on you regression, even if you don't calculate it. Consider the model: $$ Y = 1\alpha + X^T\beta + e $$ Where $1$ is a vector of ones and $X^T$ is a matrix of observable variables. There are $N$ rows. We can use Frisch-Waugh-Lovell theorem. Consider the projection matrix: $$ M = I - 1(1^T1)^{-1}1^T $$ Then, by FWL, $\beta$ vector can then be retrieved by estimating with the orthogonal components: $$ MY = MX^T \beta + Me $$ $$ (I - 1(1^T1)^{-1}1^T)Y = (I - 1(1^T1)^{-1}1^T)X^T \beta + Me $$ Now you need to realize that $(1^T 1)^{-1} = 1/N$ and $1^T Y$ just sums all values, for example: $$ (1^T 1)^{-1} 1^T Y = \frac{1}{N} \sum_i^N y_i = E(Y) $$ The above expression can be rewritten as: $$ (Y - 1E(Y)) = (X^T - 1 E(X^T)) \beta + Me $$ That is, this projection matrix $M$ is demeaning $Y$ and $X$ . Consider writing the above expression as: $$ \tilde{Y} = \tilde{X}^T \beta + \tilde{e} $$ And we know that OLS estimates are calculated with: $$ \beta = (\tilde{X}'\tilde{X})^{-1} \tilde{X}' \tilde{Y} $$ $$ \beta = [(X^T - 1 E(X^T))' (X^T - 1 E(X^T)]^{-1} (X^T - 1 E(X^T))' (Y - 1E(Y)) $$ $$ \beta = Cov(X)^{-1} Cov(X,Y) $$
