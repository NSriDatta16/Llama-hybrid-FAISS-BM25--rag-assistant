[site]: crossvalidated
[post_id]: 444545
[parent_id]: 
[tags]: 
PCA; variance, interpretability, and scaling

I've been going through threads about PCA and the predictive power of various axes, but since I cannot comment, I am opening a new question. There is a lot of discussion whether PCA components with small variance can be highly predictive and thus we should not 'drop them'? In general the answer is 'yes', since it all depends on the data, but the examples that are typically given are a bit confusing to me. A typical example goes like this: we have a feature x_1 (e.g. height) that fluctuates a lot and feature x_2 (e.g. temperature) that varies less, but on the other hand somehow effects our label most. Thus, low variance axes can be important. I have a few questions in regards to the above: 1) the examples refer to features in the non-transformed system (prior to applying PCA) and since these features need to be/can be standarised, the whole idea of their absolute variance looses the point, no? 2) after the PCA transformation there can be a heterogeneity of principal axes variance, but these axes are now linear combinations of the original axes, so when we 'drop' one or more to reduce the dimension, we do preserve partial information from all the original axes (i.e. we do not drop any original axes) Just to summarise my problem - it would seem that all the examples refer to the degree of variance in the original data and not variance after the transformation (maximisation of which is the objective of PCA) so I am wondering what would be a correct example of a case where low-variance PCA axes can be important? (the issue being also that after PCA the data looses its interpretability).
