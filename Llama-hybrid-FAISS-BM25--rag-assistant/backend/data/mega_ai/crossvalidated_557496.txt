[site]: crossvalidated
[post_id]: 557496
[parent_id]: 
[tags]: 
Please clarify Bayesian calibration of the posterior mean

In the book Bayesian Data Analysis , the authors state on page 128: The concept of calibration of the posterior mean [is] the Bayesian analogue to the classical notion of bias. They define the miscalibration of the posterior mean of some parameter $\theta$ as $$ \text{miscalibration} = E(\theta|\hat{\theta}) - \hat{\theta} $$ where $$ \hat{\theta} = E(\theta|y) $$ and $y$ is the observed data. Can someone clarify what this means? How does this $E(\theta|\hat{\theta})$ term work? It's a conditional expectation that's conditional on something that is itself a conditional expectation. How does that work? Can you show that the miscalibration is 0 if the prior distribution is true, i.e. if the data are constructed by first drawing $\theta$ from $p(\theta)$ and then drawing $y$ from $p(y|\theta)$ ?
