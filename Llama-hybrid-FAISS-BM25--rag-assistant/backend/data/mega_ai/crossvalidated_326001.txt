[site]: crossvalidated
[post_id]: 326001
[parent_id]: 325451
[tags]: 
Possible reasons: Gradient blow up Your input contains nan (or unexpected values) Loss function not implemented properly Numerical instability in the Deep learning framework You can check whether it always becomes nan when fed with a particular input or is it completely random. Usual practice is to reduce the learning rate in step manner after every few iterations.
