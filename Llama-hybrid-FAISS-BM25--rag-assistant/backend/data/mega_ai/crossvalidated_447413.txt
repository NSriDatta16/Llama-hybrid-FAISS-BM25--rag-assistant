[site]: crossvalidated
[post_id]: 447413
[parent_id]: 
[tags]: 
two basic questions about Bayesian learning

I'm trying to understand Bayesian learning. I've read some book and articles, and am still a bit confused. I'd appreciate it if someone would clarify it, correct me, and/or give me some pointers. 1) My understanding is: Unlike the frequentist approach, where the learning algorithm takes a training set $\mathcal T$ and a hypothesis class $\mathcal H$ and outputs a hypothesis $h \in \mathcal H$ , a Bayesian learning algorithm is additionally given a prior distribution $P(h)$ over $\mathcal H$ and outputs a posterior distribution $P(h\mid \mathcal T)$ over $\mathcal H$ according to Bayes Theorem $$P(h\mid \mathcal T) =\frac{ P(\mathcal T\mid h)P(h)}{P(\mathcal T)} \propto P(\mathcal T\mid h)P(h)$$ Does this mean that the learning/training process in Bayesian learning is primarily in computing $P(\mathcal T\mid h)$ for each $h\in \mathcal H$ , i.e. the so-called likelihood function? 2) How is the likelihood function $P(\mathcal T\mid h)$ usually determined? A simple example, or a reference to one, would be much appreciated. For example, can linear regression $Y=wX+b$ be learned with Bayesian approach? If so, what would be a possible or typical way to compute $P(\mathcal T\mid h)$ , where I suppose $h=(w, b)$ in this case?
