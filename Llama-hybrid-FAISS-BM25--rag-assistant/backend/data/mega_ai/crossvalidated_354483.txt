[site]: crossvalidated
[post_id]: 354483
[parent_id]: 354470
[tags]: 
It sounds like you want to reduce the dimension of your problem. This can be done using the singular value decomposition if you have gradient information. If you don't have gradients, we can still reduce the dimension of your problem. Say you have a vector of observations $Y$ and a matrix of associated inputs $X$. Let $Xa=Y$ and solve for $a$. This linear model will produce a set of weights of how "important" each input is. We can now construct a new reduced dimension variable, $z_j=\frac{\sum a_i x_{ij}}{\sum a_i}$. This is a linear active subspace model. A quadratic active subspace can give you multiple reduced variables and a metric describing their relative contribution to the system. Here is a tutorial with more information in case you are interested: https://github.com/paulcon/active_subspaces/blob/master/tutorials/basic.ipynb If you really do want to identify which already-defined input contributes the most to your signal, you have to define what sensitivity means to you. There are several metrics available, and problem-specific information is what gives them meaning. Common sensitivity metrics are the average gradient estimated using finite differences across the function space, the contribution of an input's variability to the variance of the output, and the $a$ weights I described earlier. One common approach to estimating the average gradient in each input dimension is called Elementary Effects. The analysis of variance is usually called ANOVA or sobol indexes.
