[site]: crossvalidated
[post_id]: 122518
[parent_id]: 122421
[tags]: 
It sounds like you want to reduce computation by summarizing your data and then running inference on the small summary. This is known as data squashing and there are many variants. The main principle is that you want the summary to capture as much information in the data as possible, in other words, you want it to overfit . Thus a Bayesian GMM is the wrong thing to use as a summary. A greedy algorithm like k-means or density tree would be a better choice. Similarly, there is no reason to constrain $G_d$ to have a smaller number of clusters than $G_m$. If you do, then you will have to solve an expensive disaggregation problem when fitting $G_m$. Since the whole point of the method is to save computation, it is better to use a cheap over-clustering of the data as the summary. Then while fitting $G_m$ you can constrain the data in each region to have the same cluster assignment, giving a large speedup. For details, see the paper Scalable model-based clustering for large databases based on data summarization . If instead $G_d$ is part of your sensor model, then the appropriate formula is: $$ p(D|G_m) = \int_{G_d} p(D|G_d) p(G_d|G_m) dG_d $$ where $p(G_d|G_m)$ is a physical model of the "effective" scatterers given the atomic positions. The method in the question corresponds to approximating this integral with a single point. A more accurate result, with similar computational complexity, could be obtained by Laplace's method or variational approximation.
