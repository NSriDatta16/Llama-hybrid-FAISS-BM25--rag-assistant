[site]: crossvalidated
[post_id]: 442628
[parent_id]: 
[tags]: 
In feature selection, what the size of the data set is considered as too small? Is this an appropriate use of machine learning?

I am in a non-computer science field, and machine learning is being blatantly misused in my field. I recently got a journal paper to review, where the researchers used machine learning to develop a predictive model of an experimental result. I think that the dataset they have utilized in their work is too little for successful feature selection and for determination of the important features. I would like to get an insight into whether the researchers have done a sound study or not? In the figure, S1...S8 are 8 different samples. According to the researchers, Machine learning can be used to predict the values of A,B,C... for an unknown sample. For training, they have used 6 samples, and 2 for testing. Now they have claimed in the paper that they have used 28 cases for feature analysis, which they obtained by using the 6 data and implementing extra trees, correlation analysis and SelectKbest analysis. I don't understand, how they produced the 28 datasets. And they have later validated the model using these 28 cases. I don't know how to review this work. I feel that it is not well-researched work, but I wanted some expert opinion before putting forward my thoughts. I might be absolutely wrong with this. The researchers have claimed 94% predictive accuracy using the ANN model.
