[site]: crossvalidated
[post_id]: 350465
[parent_id]: 350451
[tags]: 
the agent would learn 23/24 times that whatever action it takes, the reward is 0 The Q-function maps a state-action pair to the expected discounted cumulative future reward , not the immediate reward of taking an action. Therefore it's perfectly fine to run Q-learning with sparse rewards. (It can be problematic for other reasons though).
