[site]: crossvalidated
[post_id]: 324382
[parent_id]: 324370
[tags]: 
According to Elements of Statistical Learning (section 15.3): Recommended default values are $m = p/3$ for regression problems and $m = \sqrt{p}$ for classification problems (attributed to Breiman) The best value of $m$ depends on the problem, so $m$ should be treated as a tuning parameter. As discussed by Breiman (2001), the generalization error of a random forest decreases with the generalization error of the individual trees, and with the correlation between trees. Subsampling features is a way to decorrelate the trees. Increasing $m$ makes individual trees more powerful, but also increases their correlation. The optimal value of $m$ achieves a tradeoff between these two opposing effects, and typically lies somewhere in the middle of the range. He states that, in regression problems (compared to classification problems), the generalization error of individual trees decreases more slowly with $m$, and the correlation between trees increases more slowly. So, a larger value of $m$ is needed. This would explain why the commonly recommended default values for $m$ scale faster with $p$ for regression problems than classification problems. However, I didn't see an explicit recommendation for $m = p/3$ in this paper. Regarding scikit-learn's default setting of $m=p$: As mentioned above, subsampling features is one of the essential properties of random forests, and improves their performance by decorrelating the trees. Setting $m=p$ would remove this benefit, and would make the model equivalent to simple bagged trees. Breiman (2001) showed that this gives inferior performance on regression problems. Someone on the scikit-learn github page you linked claimed that $m=p$ is indeed recommended. However, the paper they cite is about 'extremely randomized trees', not standard random forests. Extremely randomized trees choose completely random split points, whereas random forests optimize the split point. As above, good performance requires a balance between strengthening the individual trees and injecting randomness to decorrelate them. Because extremely randomized trees inject a higher degree of randomness in the split points, it makes sense that they would benefit by compensating with searching over more features to split on. Conversely, random forests fully optimize the split points, so it makes sense that they would benefit by subsampling features to increase randomness. References: Breiman (2001) . Random Forests. Hastie, Tibshirani, Friedman (2009) . The Elements of Statistical Learning.
