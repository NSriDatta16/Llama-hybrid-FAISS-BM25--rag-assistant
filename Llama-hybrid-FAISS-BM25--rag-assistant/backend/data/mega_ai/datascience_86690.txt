[site]: datascience
[post_id]: 86690
[parent_id]: 
[tags]: 
How to get non-normalized feature importances with random forest in scikit-learn

I'm using the feature_importances_ attribute in the random forest classifier of scikit-learn to plot the importances of each feature. However, I'd like to plot these importances non-normalized. I have searched around how to do this, but there doesn't seem to be an easy method how to do this. I tried manually: temp = [t.tree_.compute_feature_importances(normalize=True) for t in clf.estimators_] arr = np.array(temp) arr2=[] for i in range(19): arr2.append(sum(arr[:,i])) arr3 = np.array(arr2) indices = np.argsort(arr3)[::-1] indices.reshape(1,-1) plt.figure() plt.title("Feature importances") plt.bar(range(X_train.shape[1]), arr3[indices], color="r", align="center") plt.xticks(range(X_train.shape[1]), indices) plt.xlim([-1, X_train.shape[1]]) plt.show() Which gives these results for normalize=True and normalize=False respectively. (the normalize parameter seems to be inverted somehow...?) For reference, this is the result from using feature_importances_ (the error-bars are not relevant to the question):
