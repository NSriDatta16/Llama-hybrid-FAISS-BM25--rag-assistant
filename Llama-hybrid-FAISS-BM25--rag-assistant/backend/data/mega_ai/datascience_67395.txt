[site]: datascience
[post_id]: 67395
[parent_id]: 
[tags]: 
Fitting a model as a significance testing alternative

My colleagues use somewhat unusual approach to estimate how far away performance of some of our company related processes diverge from historical levels. Their nature isn't really relevant for the purpose of this discussion. We may just assume here, that the final result can be represented with some real number which fluctuates over time and has some seasonality components. They fit a regression model (with XGBoost) on historical data excluding the most recent period (say, the last month) and than compare the most recent values with model's forecast. I argue, that this is basically an overkill and some sort of a crooked statistical significance test. And rather than fitting a model and making a forecast, we should actually calculate some normalised historical indicators (stratified by relevant groups and taking into account seasonality, where appropriate) and then do a standard significance test using a significance level, that would make sense from a business perspective. Am I right or not? Are there any better ways to approach a problem of estimating performance and comparing it to historical data? Does approach with modelling/forecasting has any legs? How it can be justified from a statistical standpoint?
