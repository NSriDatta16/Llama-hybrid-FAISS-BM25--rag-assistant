[site]: crossvalidated
[post_id]: 441320
[parent_id]: 
[tags]: 
Class imbalance, Random Over/Undersampling and the use of Class Weights and bias

I was looking at the Tensorflow tutorial on "Classification on Imbalanced Data" . Now they first show how to apply some approaches to deal with imbalance using model weights and a pre-initialized bias that conforms to the level of imbalance in the dataset. The tutorial uses logistic regression. The second approach that they show involves Random Oversampling of the data to include more of the minority class. This approach seems very promising since I have a very very imbalanced dataset. My question was, if I am training using Random Over/Undersampling, should I keep the class weights in the loss function? Should I keep the bias initializer, or not? I was not sure whether having class weights and biases would impair the over/undersampling? Seems like this would just make the system learn the weights faster, but it might also promote overfitting if the classes are more balanced.?
