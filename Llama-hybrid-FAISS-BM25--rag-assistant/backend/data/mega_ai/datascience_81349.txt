[site]: datascience
[post_id]: 81349
[parent_id]: 
[tags]: 
Trying to figure best machine learning technique to evaluate schedules for a telescope

I work for an astronomical observatory with two geographically separated telescopes. Users submit proposals in the form of programs consisting of multiple observations , and we are trying to automize our observation scheduling system. For many years, we have had "queue coordinators" (QCs) spend several hours per day coming up with multiple schedules of observations for the night depending on weather conditions. Right now, the science staff is having a real issue formalizing what makes for a good schedule as a mathematical equation, and I don't think it can be boiled down to a simple equation given the number of variables. My thought was that if we assume that the schedules created by the QCs are "close enough" to optimal, we can train some assisted machine learning technique (which one, I am not sure) to, being fed the data, to produce a score indicating the fitness of a proposed schedule. (We do have many years of daily historical data of schedules that we could consider close enough to optimal to use for the training and testing.) Here are examples of some of the variables, although I don't know all of policies yet: We divide scientific programs into bands 1, 2, 3, and 4, with band 1 programs being of the highest importance: essentially, we must complete all band 1 programs, and if the conditions are not sufficient for a band 1 program, we work on a band 2 program, etc. The higher the number, the less programs we expect to complete in that band. A program generally has enough data to be useful for scientific purposes when it hits 80% completion; before then, it is essentially uselesss. Completeness over 80% is much less valuable than hitting the 80% mark. We have thesis vs. non-thesis programs within each of the bands, with thesis programs being worth slightly more than non-thesis programs. Completing a long program should be worth considerably more than completing a short program. Observations are best scheduled near zenith (when the target is right overhead), offset by a few degrees due to the way the mirrors are arranged on the telescope. The target of an observation obviously has to be visible at the time it is scheduled, and have a certain distance from the moon to avoid lunar interference. We keep running into problems with proposed scoring functions by science, who seem to think that this can be modeled as a function of two inputs, i.e. band and completion rate. For example: One proposed function motivated the scheduler to start as many band 1 programs as possible, but had much less impetus to complete them to the point of scientific use. One proposed function made completing programs overly valuable, so the scheduler was motivated to complete as many short programs as it could, which left long programs with not enough nights to be completed. They have just been throwing functions at each other and multiplying functions by their slopes, etc, to try to find something that comes close to capturing the observatory policy, which is not working. I'm a combinatorial design theorist and while I have taken courses in ML years ago, I can't think of the best assisted machine learning technique to train for this kind of problem. Does anyone have any ideas? If this is posted to the wrong stack exchange, I apologize. Please direct me to the correct one. I considered mathematics but thought it was more appropriate here. Thanks very much for any suggestions, as we are really unsure of how to proceed.
