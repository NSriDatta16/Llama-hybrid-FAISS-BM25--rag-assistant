NP algorithm thus slightly modifies the batch normalization step for the ease of mathematical analysis. It can be shown that in GDNP, the partial derivative of f L H {\displaystyle f_{LH}} against the length component converges to zero at a linear rate, such that ( ∂ γ f L H ( w t , a t ( T s ) ) 2 ≤ 2 − T s ζ | b t ( 0 ) − a t ( 0 ) | μ 2 {\displaystyle (\partial _{\gamma }f_{LH}(w_{t},a_{t}^{(T_{s})})^{2}\leq {\frac {2^{-T_{s}}\zeta |b_{t}^{(0)}-a_{t}^{(0)}|}{\mu ^{2}}}} , where a t ( 0 ) {\displaystyle a_{t}^{(0)}} and b t 0 {\displaystyle b_{t}^{0}} are the two starting points of the bisection algorithm on the left and on the right, correspondingly. Further, for each iteration, the norm of the gradient of f L H {\displaystyle f_{LH}} with respect to w {\displaystyle w} converges linearly, such that | | w t | | S 2 | | ▽ f L H ( w t , g t ) | | S − 1 2 ≤ ( 1 − μ L ) 2 t Φ 2 γ t 2 ( ρ ( w 0 ) − ρ ∗ ) {\displaystyle ||w_{t}||_{S}^{2}||\triangledown f_{LH}(w_{t},g_{t})||_{S^{-1}}^{2}\leq {\bigg (}1-{\frac {\mu }{L}}{\bigg )}^{2t}\Phi ^{2}\gamma _{t}^{2}(\rho (w_{0})-\rho ^{*})} . Combining these two inequalities, a bound could thus be obtained for the gradient with respect to w ~ T d {\displaystyle {\tilde {w}}_{T_{d}}} : | | ▽ w ~ f ( w ~ T d ) | | 2 ≤ ( 1 − μ L ) 2 T d Φ 2 ( ρ ( w 0 ) − ρ ∗ ) + 2 − T s ζ | b t ( 0 ) − a t ( 0 ) | μ 2 {\displaystyle ||\triangledown _{\tilde {w}}f({\tilde {w}}_{T_{d}})||^{2}\leq {\bigg (}1-{\frac {\mu }{L}}{\bigg )}^{2T_{d}}\Phi ^{2}(\rho (w_{0})-\rho ^{*})+{\frac {2^{-T_{s}}\zeta |b_{t}^{(0)}-a_{t}^{(0)}|}{\mu ^{2}}}} , such that the algorithm is guaranteed to converge linearly. Although the proof stands on the assumption of Gaussian input, it is also shown in experiments that GDNP could accelerate optimization without this constraint. Neural networks Consider a multilayer perceptron (MLP) with one hidden layer and m {\displaystyle m} hidden units with mapping from input x ∈ R d {\displaystyle x\in R^{d}} to a scalar output described as F x ( W ~ , Θ ) = ∑ i = 1 m θ i ϕ ( x T w ~ ( i ) ) {\displaystyle F_{x}({\tilde {W}},\Theta )=\sum _{i=1}^{m}\theta _{i}\phi (x^{T}{\tilde {w}}^{(i)})} , where w ~ ( i ) {\displaystyle {\tilde {w}}^{(i)}} and θ i {\displaystyle \theta _{i}} are the input and output weights of unit i {\displaystyle i} correspondingly, and ϕ {\displaystyle \phi } is the activation function and is assumed to be a tanh function. The input and output weights could then be optimized with min W ~ , Θ ( f N N ( W ~ , Θ ) = E y , x [ l ( − y F x ( W ~ , Θ ) ) ] ) {\displaystyle \min _{{\tilde {W}},\Theta }(f_{NN}({\tilde {W}},\Theta )=E_{y,x}[l(-yF_{x}({\tilde {W}},\Theta ))])} , where l {\displaystyle l} is a loss function, W ~ = { w ~ ( 1 ) , . . . , w ~ ( m ) } {\displaystyle {\tilde {W}}=\{{\tilde {w}}^{(1)},...,{\tilde {w}}^{(m)}\}} , and Θ = { θ ( 1 ) , . . . , θ ( m ) } {\displaystyle \Theta =\{\theta ^{(1)},...,\theta ^{(m)}\}} . Consider fixed Θ {\displaystyle \Theta } and optimizing only W ~ {\displaystyle {\tilde {W}}} , it can be shown that the critical points of f N N ( W ~ ) {\displaystyle f_{NN}({\tilde {W}})} of a particular hidden unit i {\displaystyle i} , w ^ ( i ) {\displaystyle {\hat {w}}^{(i)}} , all align along one line depending on incoming information into the hidden layer, such that w ^ ( i ) = c ^ ( i ) S − 1 u {\displaystyle {\hat {w}}^{(i)}={\hat {c}}^{(i)}S^{-1}u} , where c ^ ( i ) ∈ R {\displaystyle {\hat {c}}^{(i)}\in R} is a scalar, i = 1 , . . . , m {\displaystyle i=1,...,m} . This result could be proved by setting the gradient of f N N {\displaystyle f_{NN}} to zero and solving the system of equations. Apply the GDNP algorithm to this optimization problem by alternating optimization over the different hidden units. Specifically, for each hidden unit, run GDNP to find the optimal W {\displaystyle W} and γ {\displaystyle \gamma } . With the same choice of stopping criterion and stepsize, it follows that | | ▽ w ~ ( i ) f ( w ~ t ( i ) ) | | S − 1 2 ≤ (