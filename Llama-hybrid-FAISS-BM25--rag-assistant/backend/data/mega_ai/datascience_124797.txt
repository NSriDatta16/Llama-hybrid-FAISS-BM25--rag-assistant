[site]: datascience
[post_id]: 124797
[parent_id]: 118260
[tags]: 
TLDR (simplified): encoder sees into future, decoder predicts transformer sees into future and then predicts, encodes, then decodes gpt doesnt see into future, it only predicts - thats why it's decoder-only you can check karpathy's explanation: https://youtu.be/kCc8FmEb1nY?t=6159 (~2m short, at the linked time, or for full understanding watch the whole 2h video, or even whole ~10h series) basically, gpt decoder doesnt look into future, it is only predicting, so this is the decoding part.. and it is specific in that we dont know the full context of sentence.. however, in case of translating from one language to another, we know the full context from the original sentence, so basically before running the decoder to predict the next part, we first encode the context from the original language to simplify, you could say, in this usecase, that encoding is more about encoding what the data means into symbolic representation, while decoding is more about predicting the output - the original transformer did both of these things, while gpt only does the second part (to be clear, there is always some "encoding" in any code, but we are talking about transformer-style encoding when deciding whether to call gpt an encoder, and such encoding is not being done. but there are word/token embeddings being done in decoder, which you could call encoding, but that is just simple data processing. but in case of transformer, there is huge neural network that does encoding)
