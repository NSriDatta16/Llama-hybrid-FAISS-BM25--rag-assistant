[site]: crossvalidated
[post_id]: 448970
[parent_id]: 448953
[tags]: 
Since Haitao gave an explanation of a frequentist approach, I will supply a Bayesian one. In a Bayesian setting, we still generally believe that there is a "true" $p$ . We want to understand how probable different values of $p$ are given the data we have observed. In the coin flip example, $p$ is the probability of observing heads. Say we have a fair coin, and we flip it 100 times, and get 40 heads. p We can then use the binomial distribution to tell us how likely we would be to observe these results for different values of $p$ . s In this case, a maximum likelihood estimator would give us a result of $p=0.4$ . However, imagine we have prior knowledge about how "fair" coins are in general. We could say that the distribution of $p$ (chance of heads) across all coins we've ever seen is described by a Beta distribution , say $\text{Beta}(50, 50)$ prior_alpha We would like to combine this prior knowledge with our likelihood distribution. Formally, this is allowed by Bayes theorem: $$p(a|b) = \frac{p(b|a) p(a)}{\int p(b|a)p(a) da}$$ I'll skip the math here. Suffice to say that when we combine out beta and binomial distribution, we get a Beta distribution with updated parameters. This is because beta is a conjugate prior for the binomial distribution. In this case we take the $\alpha$ of our prior and add heads , and take the $\beta$ of our prior and add flips - heads . plot( s, dbeta( s, shape1 = prior_alpha + heads, shape2 = prior_beta + flips - heads ), type = "l", xlab = "p (probability of heads)", ylab = "Posterior density" ) By using prior information like this, you can see that we've shrunk our guesses towards what we believe about coins in general.
