[site]: crossvalidated
[post_id]: 442287
[parent_id]: 362425
[tags]: 
I might try to postulate some things that help to define a Neural Network. A computation (directed) graph with adjustable parameters. Said parameters can be adjusted to conform to data (real or simulated). An objective function to be optimized is involved implicitly or explicitly. It can be global or local on parameters. I'm pretty sure this covers all neural networks in common use today and also some esoteric ones. It's agnostic to the optimization (if we imposed gradient-based optimization, then evolved networks wouldn't be neural networks). It doesn't mention neurons/nodes or layers (some neural networks today are hardly described by these terms), but I guess we could incorporate that and be a bit more restrictive.
