[site]: crossvalidated
[post_id]: 128939
[parent_id]: 
[tags]: 
Calculating the first time a particle hits a state

Let $(X_{n})$ be a Markov chain with state space $D=(a,b,c)$ and transition matrix $$P= \pmatrix{ 0.4 & 0.6 & 0 \\ 0.5 & 0 & 0.5 \\1 & 0 & 0 \\}$$ A) Find the lim$_{n-> \infty }$ $P[X_n = j |X_0 =a ]$, for $j \in D$. I found the answer for this. $\pi_{1} = \frac{10}{19}$ $\pi_{2} = \frac{6}{19}$ $\pi_{3} = \frac{3}{19}$ B) Let $T$ be the first time that the particle hits state $c$, that is, T = $Min[n >=1 : X_{n}=c ]$. Compute $E[T|X_{0} = a]$ , $E[T|X_{0}=b]$ , $E[T|X_{0}=c]$ . Defining $m_{i}$ to be the expected number of steps needed to arrive to state c given you start out in state i. The expectation for $E[T|X_{0}=c]$ is given $m_{c} = E[T|X_{0}=c] = \frac{1}{\pi_{3}}$ I understand this. But I don't understand why $m_{c} = 1 + m_{a}$ So what this is saying is that the expected number of steps needed to get to state c given you start in state c is equal to the expected number of steps needed to get to state c given you start in state a is 1. But I don't understand how they are equal. and $m_{a} = 1+ 0.4m_{a} + 0.6m_{b}$ I understand the $0.4m_{a} + 0.6m_{b}$ part but why do you have to add the one. Managed to figure it out. If you condition it, you end up wasting one step which is why you add the 1.
