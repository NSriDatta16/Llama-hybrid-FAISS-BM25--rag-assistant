[site]: crossvalidated
[post_id]: 415498
[parent_id]: 415416
[tags]: 
In random forests, stochasticity is mainly caused by the following two factors: For each tree, only a subset of features is selected ( randomly ), and the decision tree is trained using only those features For each tree, a bootstrap sample of the training data set is used, i.e. dataset sampled with replacement . In sklearn, this can be controlled via bootstrap parameter. In a Decision Tree, we have none of them. There may also be other sources of randomness due to implementation differences. For example, DecisionTree in sklearn uses a splitter argument which is best by default, however it can be set to random . RandomForest uses this class with default attributes, i.e. splitter=best , where the threshold to choose is the arithmetic average of boundary values. There seems to be no random-rule here. For Decision Tree , (and therefore Random Forest), the following explanation is given in the documentation (which you've also found in the RF's explanation page): The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data and max_features=n_features, if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, random_state has to be fixed. This means, if max_features=None , splitter = 'best' , it's less likely to be random, yes; but, since the features are randomly shuffled at each node, if two of them gives the same gain, we end up with different trees at different runs. Note that, this property is implementation specific .
