[site]: datascience
[post_id]: 128382
[parent_id]: 128379
[tags]: 
Start with a sequence of embeddings. In the standard attention computation, each embedding in the sequence attends to every other embedding in the sequence. This can be considered "bidirectional" as a given embedding is attending every other embedding, considering the full sequence. Masked language models (MLMs) like BERT use this form of attention.
