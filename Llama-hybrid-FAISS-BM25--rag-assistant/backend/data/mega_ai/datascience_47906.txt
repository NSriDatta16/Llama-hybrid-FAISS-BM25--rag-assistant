[site]: datascience
[post_id]: 47906
[parent_id]: 
[tags]: 
Understanding ELBO Learning Dynamics for VAE?

As I understand it, I'm basically minimizing the KL Divergence between the Prior and Encoder latent distribution, and the log probability of the decoder distribution. I have a model that does generate some useful latent distribution for classification using this loss function, however, it has weird learning dynamics. My decoder distribution is a 300 dimension Gaussian and my latent is a 40 dimension Gaussian, so in all instances, both terms of my ELBO should vary wildly in order of magnitude. It seems from looking around on the internet, most use cases of VAEs look similar - high dimension decoders and low dimensional encoders, so I would imagine some people see a similar a problem, but I haven't mentioned any note of this in any literature or tutorial. Is there any way to correct for this?
