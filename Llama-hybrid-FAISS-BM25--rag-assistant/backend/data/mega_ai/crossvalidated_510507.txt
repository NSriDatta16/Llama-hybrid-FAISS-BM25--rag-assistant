[site]: crossvalidated
[post_id]: 510507
[parent_id]: 510496
[tags]: 
Exponential discounting is "time-consistent" in a way that other forms of discounting are not. For example, with $\gamma = 0.9$ , you would prefer 1 reward today to 1 reward tomorrow, and 1 reward in 10 days to 1 reward in 11 days. You would also prefer 2 reward tomorrow over 1 reward today, and 2 reward in 11 days over 1 reward in 10 days. Under your scheme, it seems like you'd prefer 1 reward tomorrow over 1 reward today, but you'd prefer 1 reward in 10 days over 1 reward in 11 days. You might prefer 1 reward today over 2 tomorrow, but 2 in 11 days rather than 1 in 10 days. So you answer differently to the same questions depending on how far away something is, which is a bit strange. If taking these rewards required longer term planning and preperation, you might find yourself spending a few days to prepare to do X, only to later change your mind and throw it all away to do Y. Another popular alternative to exponential discounting is hyperbolic discounting, which is supposedly what humans use. However this is also not time-consistent. Practically speaking, it's a bit nontrivial to use alternate discount functions because the Bellman equation, the basis of many reinforcement learning algorithms, assumes exponential discounting. Fedus et al show you can tweak some things to make hyperbolic discounting work with Q-learning. Another practical reason for exponential discounting is that it converges, whereas a hyperbolic sum of rewards might diverge to infinity. So it makes things nice for theoretical analysis.
