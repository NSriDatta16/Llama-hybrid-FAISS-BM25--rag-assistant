[site]: crossvalidated
[post_id]: 418810
[parent_id]: 418766
[tags]: 
I would suggest checking the same concept in logistic regression, which is the limiting case of a classification neural net a) logistic regression essentially is fitting a best fit sigmoid to the probability distribution of your class given your input features. If the log odds are actually a linear function, then the probability output will be correct. b) if the log odds are not given by a linear function, then the probability will not be 'calibrated', and grouping the probability outputs will not match the real world probability. ( just as if you plot (y against y_hat) as linear regression output for true model y=x^2, actual model y_hat=ax (for x>0), you will not get a straight line through origin. c) a hack to match the probabilites is to create a mapping function ( using the real world data to output probability bins) AFAIK, the probability estimates of neural networks are maximum likelihood estimates so they are true probabilities. This assuumes you use log loss and all your tweaks are incorporated into the backprop calculation) - is dropout included in backprop?
