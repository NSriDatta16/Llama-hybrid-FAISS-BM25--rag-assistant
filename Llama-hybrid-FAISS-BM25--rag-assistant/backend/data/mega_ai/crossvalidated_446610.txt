[site]: crossvalidated
[post_id]: 446610
[parent_id]: 373858
[tags]: 
I co-wrote a paper on this exact problem: https://papers.nips.cc/paper/7642-leveraging-the-exact-likelihood-of-deep-latent-variable-models We show that, as you thought, maximum-likelihood is ill-posed for Gaussian output VAEs. Things go pretty much like for GMMs. A solution is to constrain the eigenvalues of the covariance network to be bigger than some threshold. An interesting remark is that, for discrete data, the problem is well-posed. This possibly explains why VAEs are usually benchmarked on discrete data sets (like binary MNIST). We show all these results in Section 2.1 of our paper. Similar investigations were also conducted in this paper: http://www.jmlr.org/papers/volume19/17-704/17-704.pdf they show (Theorem 5) that the VAE objective is unbounded. This means that, in general, even having the KL term does not make the objective well-posed.
