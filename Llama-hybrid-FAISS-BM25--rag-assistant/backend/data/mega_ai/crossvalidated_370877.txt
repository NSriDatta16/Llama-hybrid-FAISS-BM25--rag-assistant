[site]: crossvalidated
[post_id]: 370877
[parent_id]: 
[tags]: 
Standard error of mean vs error propagation

I'm confused on how to correctly calculate the final uncertainty from averaging measurements that each have their own internal errors. Say I have 3 voltage measurements: (1.232 ± 0.001) V, (1.197 ± 0.001) V, and (1.292 ± 0.001) V. The uncertainties here are due to the precision limit on the voltmeter. I want to plot the mean of these three measurements as a point on a graph, with its associated error bar for the voltage V. I've read things that have said the final error is just (max value - min value) / N , but this ignores the uncertainties within the original measurements. I've also seen something that suggested the final uncertainty is simply the uncertainty of 1 measurement divided by the square root of N, so in my case 0.001/sqrt(3) . But this value seems like too small of an error. Is one of these methods correct, or do I need to somehow combine these uncertainties? If so, how?
