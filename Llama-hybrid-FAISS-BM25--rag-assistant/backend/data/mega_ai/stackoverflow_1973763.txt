[site]: stackoverflow
[post_id]: 1973763
[parent_id]: 1972863
[tags]: 
People don't just use BigDecimal / BigInteger for money. Rather, they use them in applications that need more precision than is available using double or long . Of course, using BigDecimal and BigInteger comes at the cost of much slower arithmetical operations. For example, big number addition is O(N) where N is the number of significant digits in the number, and multiplication is O(N**2) . So the way to decide whether to use long / double or their "big" analogs is to look at how much precision your application really needs. Money applications really do need to be able to represent values without losing a single cent. Other applications are equally sensitive to precision. But frankly, I don't think that a neural network application needs 13 decimal digits of precision. The reason your network is not behaving as it should is probably nothing to do with precision. IMO, it is more likely related to the fact that "real" neural networks don't always behave the way that they should.
