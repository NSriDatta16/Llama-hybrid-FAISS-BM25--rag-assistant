[site]: crossvalidated
[post_id]: 330461
[parent_id]: 
[tags]: 
On not wasting data when taking lags of one variable

I just have a basic question about what 'best practice' generally is, in such a situation: Suppose I have two finite time series of equal length $\{x(t)\}_{t \in I}$ and $\{y(t)\}_{t \in I}$ and say I want to regress $(x(t)-x(t-i))$ against $y(t)$, where $i$ is a fixed constant positive integer. This results in a bunch of unused data points, namely, the first $i$ data points, since there are no further values back in the $x$ time series to take differences of. Similarly, if we take differences with greater lags, we will see more unused data points. In such cases, what is generally a good way to make use of that data without totally wasting it? Backfilling the last difference, i.e. using constant $x(i+1)- x(1)$ for the first $i$ entries? Or using $x(j) - \mu$ for $j \le i$, where $\mu$ is some local average of the $x$ values? Or what? Any help would be much appreciated. Thanks!
