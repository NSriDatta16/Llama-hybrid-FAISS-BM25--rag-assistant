[site]: crossvalidated
[post_id]: 551216
[parent_id]: 
[tags]: 
Why is the covariance matrix inverted in the definition of the Mahalanobis distance?

I'm on my first course on data science, and I encountered the Mahalanobis distance for the first time. It was mentioned that intuitively, what it does is that it corrects for the fact that some attributes in a dataset might be correlated (so in effect, correlated attributes might "count twice" if we were to use the Euclidean distance). From this, it makes sense why it would use the covariance matrix, as it contains the information on which columns correlate with each other and how much, but I am not sure why one would need to invert it in this case. So why is it inverted in the definition? Would a "non-inverted" version of the Mahalanobis distance not be a useful distance metric? If so, why?
