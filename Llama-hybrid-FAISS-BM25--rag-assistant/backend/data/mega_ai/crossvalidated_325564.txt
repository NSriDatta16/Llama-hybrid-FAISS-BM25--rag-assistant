[site]: crossvalidated
[post_id]: 325564
[parent_id]: 323788
[tags]: 
For machine learning and statistical prediction problems, it is unlikely that there will be any value in trying to coerce symmetric matrices of different sizes to a uniform size. This will generally either involve extending small matrices to large ones (thus having many useless entries), or coercing larger matrices to smaller ones (which is messy and unlikely to illuminate your problem). Technically it is possible to map real matrices of any (finite) size down to real numbers, and vice versa . Any $n \times n$ real matrix (for finite $n$) is cardinally equivalent to a single real number, so you can map either way. There are innumerable ways that you could coerce these to equivalent size, either by extending the smaller ones (e.g., zero-padding each matrix to the size of the largest matrix), or by doing a nasty conversion down to smaller size (e.g., collapsing multiple real numbers to a single real number using annoying strings of alternating digits from different numbers). Without knowing the context of your problem, it is difficult to say what is best here, but it is very unlikely that any of these methods is going to be helpful. Extending smaller matrices to bigger ones without any additional information is essentially just a waste of extra matrix entries, and coercing larger matrices down to smaller ones is messy and does not save information. Perhaps a better idea is to take a step back from your problem and think about the formulation of the underlying probabilistic mechanism generating these observable matrices. If your statistical problem involves observing symmetric matrices then your model ought to be written in a way that takes a vector of these matrices, allowing for different sizes.
