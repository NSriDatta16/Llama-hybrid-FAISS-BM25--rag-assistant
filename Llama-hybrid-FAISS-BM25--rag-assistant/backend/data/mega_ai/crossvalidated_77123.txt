[site]: crossvalidated
[post_id]: 77123
[parent_id]: 77018
[tags]: 
It is an extension of bagging. The procedure is as follows, you take a bootstrap sample of your data and then use this to grow a classification or regression tree (CART). This is done a predefined number of times and the prediction is then the aggregation of the individual trees predictions, it could be a majority vote (for classification) or an average (for regression). This approach is called bagging (Breiman 1994). Furthermore the candidate variable for each split of each tree is taken from a random sample of all the available independent variables. This introduces even more variability and makes the trees more diverse. This is called the random subspace method (Ho, 1998). As mentioned, this produces trees which are very diverse which translates into trees which are highly independent of each other. Because of the Jensen's inequality we know that the average of the errors of these trees predictions will be smaller or equal to the error of the average tree grown from that data set. Another way to look at it is to look at the Mean Squared Error and notice how it can be decomposed in bias and variance parts (this is related to an issue in supervised learning called the bias-variance tradeoff ). Random forest achieves better accuracy by reducing variance through the averaging of the prediction of orthogonal trees. It should be noted that it inherits the bias of its trees, which is quite a discussed problem, check for example this question.
