[site]: crossvalidated
[post_id]: 433940
[parent_id]: 
[tags]: 
Neural Network Architecture Search

I'm applying NN for regression purpose. The model has 30 Input nodes, 1 hidden layer and 1 output. In order to find the optimal architecture of the hidden layer, I've constructed a loop that: tests all possible hidden layer neurons (e.g. from 1 to 30) Calculates cross validation error for each loop iteration Filters the iteration that has the minimum error (e.g. 12 neurons) Applies the 12 neurons to the full data set Can I consider this a form of regularization? Reason for asking is because the package I deploy doesn't support L2 or dropout.
