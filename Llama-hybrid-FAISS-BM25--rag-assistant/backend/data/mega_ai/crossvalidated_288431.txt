[site]: crossvalidated
[post_id]: 288431
[parent_id]: 66939
[tags]: 
First, make sure that you understand what an embedding is. It's borrowed from mathematics . Roughly speaking, it is a mapping of the data into another space (often called embedding space or feature space ), preserving some structure or properties of the data. Note that its dimensionality can be bigger or smaller than the input space. In practice, the mapping is complex and highly non-linear. A few examples: A real-valued "word vector" to represent a word, such as word2vec The activations of a layer of a convnet, such as the FC7 layer AlexNet (FC7 is the 7th fully-conected layer) To illustrate, I'll take an example of this paper from Josh Tenenbaum: Fig. 1 illustrates the feature discovery problem with an example from visual perception. The set of views of a face from all possible viewpoints is an extremely high-dimensional data set when represented as image arrays in a computer or on a retina; for example, 32 x 32 pixel grey-scale images can be thought of as points in a 1,024-dimensional observation space [input space] . The perceptually meaningful structure of these images [feature space] , however, is of much lower dimensionality; all of the images in Fig. 1 lie on a two-dimensional manifold parameterized by viewing angle Josh Tenenbaum then discusses the difficulties of learning such a mapping from input to feature space. But let's go back to the question: we are interested in how the input and feature spaces are related. The 32*32 array of grey pixel values is the input space The [x1=elevation, x2=azimuth] space is the feature space (although simplistic, it can be thought as a valid embedding space). Re-stating the manifold hypothesis (quoting from this great article ): The manifold hypothesis is that natural data forms lower-dimensional manifolds in its embedding space With this example, it is clear that the dimensionality of the embedding space is way less that the input space: 2 vs 1024. (This distinction will hold even for choices higher dimensional, less simplistic embedding spaces). To convince yourself that the embedding forms a manifold, I invite you to read the rest of the Tenenbaum paper paper or the Colah article . Note: this is just an illustration of what the manifold hypothesis means, not an argument of why it happens . Related: Explanation of word vectors , word2vec paper
