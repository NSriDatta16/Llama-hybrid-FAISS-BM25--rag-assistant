[site]: crossvalidated
[post_id]: 504542
[parent_id]: 
[tags]: 
Dealing with quasi-complete separation in General Additive Model?

I am modelling influence of fire on occurrence of certain bird species (count response variable) in Before/after control/impact experiment design. I've got intact and burned sites and count data both from before and after the burning. I want to see if the fire has an effect on species occurrence. I've got three random variables: site (18 lvls), year of count (3 lvls) and point count spot within site (10 counts per 18 sites = 180 lvls). I use family=nb based on my data distribution (histogram, slight overdispersion). dataset fragment as csv file I create a GAM model in R for the species: mydata When I run the model (which seem to work well for other species), I get enormous standard error for the interaction parameter. This is the exact result: I investigated my data and I think I've found the root of the problem. There was not a single observation of the species on sites struck by fire after the fire (what I believe is a case of "quasi-complete separation"). The summary table for my data looks like this: impact period sum control before 117 control after 39 fire before 74 fire after 0 When I experimentally changed one point count observation to value "1" to make impact/after total sum="1" instead of "0", the model results changed drastically and the fire turned out to have a visible impact on the species, which is the expected result. I've looked up for some advice how to deal with the problem, but mostly found solutions relevant for situations where this perfect prediction is undesired. In my case it's different - this is my most important result. Is there any way to deal with this problem? If it requires switching to different model than GAM, I'm fine with that (my explanatory variables are super simple). However, GAM-based solution would be the most desired one (for consistency in future publication). Any advice would be highly appreciated. EDIT: Tried fitting the data (response as binomial) into model fitted by Penalized ML (following advices here: How to deal with perfect separation in logistic regression? ): library(logistf) gfirth 0 ~ before.after*ctrl.impact, data=mydata) This actually worked quite great, but unfortunately it doesn't take into consideration the random effect. It would be amazing to have a solution which includes random effects. Other methods I've tried so far: 1. library(lme4) confint(glmer.nb(bird.numbers~before.after*ctrl.impact + (1|point.count.id) + (1|year) + (1|site), data=mydata)) CI still enormous 2. library(lme4) confint(glm(bird.numbers~before.after*ctrl.impact, data=mydata, family = negative.binomial(theta = 48417.29, link = "log"))) CI still enormous. [Theta value derived from previous glmer.nb model.] 3. library(mgcv) g1 Getting this error: "in if (u[i] Tried experimenting with parameter values (I do not understand them quite well), but nothing worked. Changing the species doesn't result in an error. 4. library(mgcv) g2 Getting this error: "in na.fail.default(data.frame(x = as.numeric(obj1), y = as.numeric(obj2))) : missing values in object In addition: Warning message: In min(dens0, na.rm = TRUE) : no non-missing arguments to min; returning Inf" Again tried experimenting with parameter values (again, I do not understand what they mean too well), but nothing worked. Changing the species doesn't result in an error. Right now trying to figure out how bootstrapping methods works, but couldn't work up to a code yet - it's quite complicated to me so far. Perhaps I messed something up with ginla or gam.mh parameters? Could anyone suggest how to estimate parameter using bootstrapping methods? Maybe anyone else has some other suggestions to try out, which would include random effects?
