[site]: crossvalidated
[post_id]: 461078
[parent_id]: 461030
[tags]: 
In neural network training, the sources of variations are plenty. Because the dynamics are like that of a stochastic differential equation, having same data as in option 2 does not guarantee that given the same weights for initialization your model would deterministically converge to same point after 100 epochs or so. Both option 1 and option 2 are used for model selection, but given that neural network training is expensive, option 2 is more popular. I don't know how large your dataset is, but if a reasonable argument can be made that separating a validation set ( say 1â€“10% of data ) does not cause your model to underfit, then I would go with option 2. This is really a concern in learning from very low data (e.g. fitting VGG-16 from scratch using let's say 10% of CIFAR10 data). Taking option 2, you can try two things: Select different random seed for weight initialization, repeat neural network training under a consistent model selection strategy (e.g. best performance on val set in E epochs, under assumption that E epochs are more than enough to get the model to converge), and measure a response variable (e.g. f1 score as you proposed). Fix a random seed and repeat training, under a similar model selection strategy to measure response variable as in 1) above. This time, you're after variance caused by the dynamics of neural network training. Why is this necessary you ask? Because the gradient flows are different as the order of mini-batches drawn is different. This is a fascinating direction of on-going research. Further, batch-norm will converge to a slightly different mean and sigma for normalizing activations given that order of minibatches will be different, just FYI if your model is using batch-norm. Lastly, don't forget to turn-off dropout. I would report variance in response variable under both of the above described scenarios.
