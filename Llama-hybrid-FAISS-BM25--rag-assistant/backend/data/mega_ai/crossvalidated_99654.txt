[site]: crossvalidated
[post_id]: 99654
[parent_id]: 99651
[tags]: 
Some form gradient descent algorithm will be fastest on your problem. There are a number of variants of gradient descent algorithms and and a number of different algorithms. The most basic variant is Stochastic Gradient Descent (SGD). Most of the major machine learning libraries/systems will have a version of this. For massive datasets that do not fit into memory Vovpal Wabbit is probably the go, but for your data sets most of the major implementations should be fine. Enhancements to SGD include Pegasos, PassiveAggressive algorithms, Confidence Wighted Learning, Adagrad, AROW, and Natural Gradient Descent. If you use Python then the scikit library has SGD and Passive Agrresive Preceptrons. I have used them very succesfully for datasets like you describe. Training should take only seconds or possibly minutes depending on the particulars of your data and the settings you use. If your inputs are sparse, make sure the algorithm your algorithm is using sparse data structures to represent them. This could be what made glmnet slow.
