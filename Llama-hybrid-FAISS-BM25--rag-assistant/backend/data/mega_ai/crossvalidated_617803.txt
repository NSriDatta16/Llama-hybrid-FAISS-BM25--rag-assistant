[site]: crossvalidated
[post_id]: 617803
[parent_id]: 
[tags]: 
When comparing PCA and FA Models, is it possible to have a model with lower explained variance, but a higher Log Likelihood?

Having a model with a higher log-likelihood and lower explained variance doesn't make sense to me. The Bing AI thing says it's impossible... I'm new to these analyses and the log-likelihood statistic. The difference is so tiny (see below) I'm tempted to call it a rounding error, but I'd like to better understand what is going on if I can. Is there something I'm missing or should know? I'm running a five-fold cross-validation of a PCA and a Factor Analysis model. data= pe_growth_earn[bad_PE_PEG_TY_Final_filter] cross_val_dict = dict() fa = FactorAnalysis(n_components=1) pca = PCA(n_components = 1) cross_val_dict['rel_letg_ty_pca'] = cross_val_score(pca, data[['Rel_LTEG', 'Rel_TY_Final']]) cross_val_dict['fa_rel_letg_ty'] = cross_val_score(fa, data[['Rel_LTEG', 'Rel_TY_Final']]) cross_val_dict_sorted = dict(sorted(cross_val_dict.items(), key=lambda item: np.mean(item[1]), reverse=True)) for k,v in cross_val_dict_sorted.items(): print(k, np.mean(v), np.std(v)) You can see they have the same variables. I get: fa_rel_letg_ty -3.0702495295898276 1.0239627390665273 rel_letg_ty_pca -3.0723270850577697 1.02128110579763 fa_rel_letg_ty (the Factor Analysis model) has a slightly higher average log-likelihood (by about 2 thousandths); however, its explained variance (calculated elsewhere via explained_variance_score(data[['Rel_LTEG', 'Rel_TY_Final']], np.dot(df, model.components_) ) is much worse (about 10% vs 50%). I've done some simple sample testing and the explained variance difference is fairly constant.
