[site]: crossvalidated
[post_id]: 477629
[parent_id]: 477627
[tags]: 
Basically, you reduce the input (dimension) by reducing the neurons per layer. This is the encoder part. Afterwards, based on that very low dimensional representation, the entire data is broken down to only important features. This is what you finally wanna' have: A meaningful, low dimensional data representation. To train and validate this encoder function, you decode this data by mostly using the same topology in a reversed manner. Now there are two options: Either you use the encoder + decoder: So you wanna' be able to restore all your data again only based on these few features. This would be something like a nonlinear PCA. Or you detach the encoder part and use it for new data. A good example, e.g., is the use for mobile communication which breaks down/encodes the data so that only a significant part of your speech remains, then this data is transfered and at the receiver side it is decoded. Despite from that concept the handling and so on is quite the same like for every neural network (train, validate, minimize error function and so on..). Image source
