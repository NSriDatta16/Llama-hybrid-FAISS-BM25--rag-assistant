[site]: datascience
[post_id]: 45047
[parent_id]: 45039
[tags]: 
The reason, IMHO, that there is a similar pattern is more due to the convolutional layers than anything else. You can have an imbalance in the number of dense layers. Let's take even the case of an autoencoder. If you take something simple like the SwissRoll, you will see that you don't need the same amount of layers on the encoder and the decoder side (basically your discriminator and the generator). But if you have an image, having only dense layers makes it hard for the generator to generate smooth images. It will take far more time to train it, and meanwhile, the discriminator can use this unsmoothness to detect them. Also, you have to remember that the C layers are preprocessors, extractors, the same can be said for the generator outputs, they are creators, they need to match. So yes, it's about balancing, but only the features. The rest doesn't have to be symmetric, it's all about the complexity of the function you have to approximate.
