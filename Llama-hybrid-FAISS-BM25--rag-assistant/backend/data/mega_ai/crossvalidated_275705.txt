[site]: crossvalidated
[post_id]: 275705
[parent_id]: 271276
[tags]: 
There are a few ways to optimize trade-offs between precision and recall : As Mattew said, you can modify the classification threshold: lower threshold (eg 0.1 on logistic regression) yields to high recall and lower precision. Viceversa, higher threshold (eg 0.9) yields to high precision and lower recall. I guess that this is what you are doing at the moment; You fix the classification threshold to the default value: ie 0.5 for logistic regression. Then you modify the cost of a positive instance in the loss function. You might want to use something like the weighted cross entropy : $-[weight \cdot y \cdot \log(p) + (1 - y) \log(1 - p)]$. Higher $weight$ for the positive class yields to higher precision and lower recall; You fix the threshold and the loss and you change the training data set. If you oversample the positive class you will classify with higher precision and lower recall.
