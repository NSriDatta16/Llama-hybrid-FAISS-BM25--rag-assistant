[site]: crossvalidated
[post_id]: 601098
[parent_id]: 
[tags]: 
What does the PCA similarity factor tell us?

I found the following formula for comparing time series $$ S_{\mathrm{PCA}}^{\lambda}=\frac{\sum \limits_{i=1}^{k} \sum \limits_{j=1}^{k}\left(\lambda_{i}^{(1)} \lambda_{j}^{(2)}\right) \cos ^{2} \theta_{i j}}{\sum \limits_{i=1}^{k} \lambda_{i}^{(1)} \lambda_{i}^{(2)}} $$ where PCA is applied to two data sets (or time series) $X_1$ and $X_2$ so that $\lambda_{i}^{(1)}$ is the eigenvalue of the $ith$ eigenvector (principal component) of the first data set $X_1$ , respectively for $\lambda_{j}^{(2)}$ and $\theta_{i j}$ is the angle between the $ith$ eigenvector of $X_1$ and $jth$ eigenvector of $X_2$ . I use this formula to compare the similarity of the time series of trajectories and the similarity factor is around $0.8$ for each pair of 100s of time series but I don't quite get what the score means. Could anyone explain to me what exactly is calculated here and how we can conclude how similar the data sets are? The Python code: def pca_similarity_factor(ts1: np.array, ts2: np.array, components = 3) -> float: pca1 = PCA(n_components=components) pca2 = PCA(n_components=components) pca1.fit(ts1) pca2.fit(ts2) len_ev = len(pca1.explained_variance_) data1 = zip(pca1.explained_variance_, pca1.components_) data2 = zip(pca2.explained_variance_, pca2.components_) numerator = sum([ev1*ev2*(np.cos(angle_between(v1,v2))**2) for ev1,v1 in data1 for ev2,v2 in data2]) denominator = sum([pca1.explained_variance_[i] * pca2.explained_variance_[i] for i in range(len_ev)]) similarity_factor = numerator/denominator return similarity_factor def unit_vector(vector): return vector / np.linalg.norm(vector) def angle_between(v1, v2): v1_u = unit_vector(v1) v2_u = unit_vector(v2) return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0)) Here is the source: Singhal, A. and Seborg, D.E. (2005), Clustering multivariate time-series data . J. Chemometrics, 19: 427-438.
