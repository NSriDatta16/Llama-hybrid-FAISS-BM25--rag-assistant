[site]: crossvalidated
[post_id]: 270704
[parent_id]: 
[tags]: 
How to intuitively understand difference between algorithmic adaptation vs problem transformation in multi-label classification?

I am trying to understand how multi-label classification is done and rather got confused with what wikipedia has to say, I am directly quoting it: "Several problem transformation methods exist for multi-label classification; the baseline approach, called the binary relevance method,[1][2][3] amounts to independently training one binary classifier for each label. Given an unseen sample, the combined model then predicts all labels for this sample for which the respective classifiers predict a positive result. This method of dividing the task into multiple binary tasks has something in common with the one-vs.-all (OvA, or one-vs.-rest, OvR) method for multiclass classification. Note though that it is not the same method: in binary relevance we train one classifier for each label, not one classifier for each possible value for the label. Various other transformations exist. Of these, the label powerset (LP) transformation creates one binary classifier for every label combination attested in the training set.[1] The random k-labelsets (RAKEL) algorithm uses multiple LP classifiers, each trained on a random subset of the actual labels; prediction using this ensemble method proceeds by a voting scheme.[4]" Can someone provide a intuitive example, given a some samples with multi-labels how the classification is done? More precisely how is algorithmic adaptation method is different from problem transformation? Please compare Binary Relevance (as a representative of problem transformation) with Multi class SVM/any classifier trained on single label (as a representative of algorithmic adaptation) Given some training samples having multi-labels, how can that be transformed to single labels? For example, according to my understanding, if you know that the labels for each of the training sample is arranged in a descending order of their occurrence probability then you can just consider the first label as the most possible single label for that sample. The second way is to make copies of that sample for each corresponding labels. Third way is to consider each set of labels as a single class. Which one is a preferred way? For algorithmic adaptation method, I guess you don't need n-single class classifiers just the way you need it for problem transformation method. So, its a single classifier capable of providing multiple outputs. Am I making sense?
