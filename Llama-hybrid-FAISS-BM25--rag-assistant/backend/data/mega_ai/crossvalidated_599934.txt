[site]: crossvalidated
[post_id]: 599934
[parent_id]: 199426
[tags]: 
There are many methods available, especially if you start by examining the individual accuracy of each of the two methods. See for example my val.prob function in the R rms package (documentation here ). This will draw a smooth nonparametric calibration curve to assess the absolute forecast accuracy and will give you the powerful Spiegelhalter test for testing for calibration accuracy. For an over-simplified calibration model you can use binary logistic regression to play the logit of one of the model predicted probabilities against observed 0/1 outcomes and test the slopes against 1.0 and the intercept against 0.0. If the slope is near 1.0 but the intercept is much different from 0.0 that would be a "miscalibration in the large". Things are clear if one of the methods is well calibrated and the other isn't. Otherwise you can easily use logistic regression to test whether one method adds predictive information to the other. Material related to that is at my blog article https://fharrell.com/post/addvalue . This can be as simple as fitting a binary logistic model with two predictors, which are the logit of the probability forecast for each of the two methods.
