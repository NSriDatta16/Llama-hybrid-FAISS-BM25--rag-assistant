[site]: crossvalidated
[post_id]: 619008
[parent_id]: 618722
[tags]: 
In order for myself to understand this better, and hopefully for others as well, I have made the graphical representation below. bottom panel What the algorithm effectively does is creating a sample according to the distribution below that represents the transitions in the Markov Chain. In the middle panel we see the transition probabilities for a specific value between 0.78 and 0.8 and about 11% of the transitions are a repetition of the same value (the high peak in the histogram) and 89% of the transitions are to a new proposed value. So for some 'transitions' we actually 'remain in place' and we could alternatively view the process as Deciding how long to stay in place or how long to repeat the same value. Let's call this a waiting time $t_i$ Deciding which new value to transition to. In the upper panel we see that this process generates a sample with a marginal distribution that resembles the density $f(x) \propto max(0,(1-x)^2)$ So given some current value $x_i$ the transitions in the Markov chain are according to a mixture distribution with a part $1-p$ that is a degenerate distribution centered at $x_i$ and a part $p$ that is distribution of the probability density of the next point. The Metropolis Hastings MCMC will do both of these two processes (1. sample waiting time $t_i$ 2. sample new value) together by using uniform variables $u_i$ and proposal variables $y_i$ . By doing it in that way, by making comparisons with the uniform values, the algorithm will give only discrete values for the waiting time. This is nice when we want a sample (which requires a discrete number of values), but not neccesary for computing a derived value (where we could also use a weight with the average waiting time). E.g. if a sample has 10% probability to stay in place, and 90% probability to transition to some other value, then we get as waiting times only discrete values of 1, 2, 3, etc. with respective frequencies of 0.9, 0.09, 0.009, etc. whereas we would like to know the average waiting time 0.9+0.09 2+0.009 3+... â‰ˆ 1.11 average. The idea behind the Rao-Blackwellization is to use a more precise estimate of the waiting time instead of the discrete values. Estimating the average stopping time. When we consider the average waiting time for making a transition to a different value, then the observed waiting time as estimate might not be the most efficient estimate. Let's consider a more abstract view of the part 1 of the process where the waiting time is sampled. That part 1, deciding the $n_i$ how long we 'stay in place' can also be considered as being (independently) decided with a follow process: We repeatedly draw a sample of transition/acceptance probability values $p_i \sim f(p)$ and a sample $u_i \sim U(0,1)$ of cutoff values. We stop the drawing at some time $t$ when $u_t We choose the observed $t$ , the stopping time of the process, as the estimate for the average stopping time. So we have a sample $y_1, y_2, \dots, y_t$ and $u_1, u_2, \dots, u_t$ and with that sample wish to estimate the average stopping time. This Metropolis Hastings method uses the observed stopping time, which is dependent of the values $u_i$ , is that an efficient estimator of the average 'stay in place' time? Intuitively it seems not. Consider for example the following: we could have large values for $p_i$ (large probability to transition and stop), but by chance we also have very large values $u_i$ which makes us stop very late. Would it be good to estimate a large stopping time? Should we maybe ignore the values of $u_i$ and use the values of $p_i$ only? ... to be continued ... (I have to figure out the sufficient statistic and the distribution of the stopping time given that sufficient statistic)
