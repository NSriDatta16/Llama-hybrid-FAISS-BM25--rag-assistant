[site]: crossvalidated
[post_id]: 595226
[parent_id]: 
[tags]: 
XGBoost - worse accuracy when increasing sample size? Survival analysis on a large dataset

I've encountered the above problem and struggle to find any info about online. I'm messing around with a large dataset that I unfortunately cannot share. It has ~600 features of which not all are informative and there is some collinearity. There is upward of 150k samples with imbalanced, right-censored survival data where the target endpoint is met in around 2% of the samples. I'm using an XGBoost regressor with 'survival:cox' as objective and I'm running K-fold cross-validation with an inner loop for hyperparameter tuning and an outer loop for performance evaluation. When starting out with a small subsample of the data predictive accuracy is ok, but when increasing the sample size over a certain threshold (around 20-30k) the evaluation performance starts to deteriorate. Training accuracy also goes down so it seems to be an underfitting problem. I'm thinking of trying to reduce the number of features, and/or reducing the class imbalance by subsampling. Happy for any insights to the nature of the problem, which I fail to understand, and possible solutions!
