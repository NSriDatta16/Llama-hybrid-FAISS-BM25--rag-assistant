[site]: crossvalidated
[post_id]: 468460
[parent_id]: 468454
[tags]: 
Residual Networks are designed to avoid the vanishing gradient problem in deep neural networks. LSTM cells, which are commonly used in NLP, have a 'natural' way to avoid these, see e.g. ( How does LSTM prevent the vanishing gradient problem? ). However there is research using residual connections combined with LSTMs for NLP, see Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation .
