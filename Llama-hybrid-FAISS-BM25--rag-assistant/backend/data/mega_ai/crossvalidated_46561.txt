[site]: crossvalidated
[post_id]: 46561
[parent_id]: 
[tags]: 
What are some well known improvements over textbook MCMC algorithms that people use for bayesian inference?

When I'm coding a Monte Carlo simulation for some problem, and the model is simple enough, I use a very basic textbook Gibbs sampling. When it's not possible to use Gibbs sampling, I code the textbook Metropolis-Hastings I've learned years ago. The only thought I give to it is choosing the jumping distribution or its parameters. I know there are hundreds and hundreds of specialized methods that improve over those textbook options, but I usually never think about using/learning them. It usually feels like it's too much effort to improve a little bit what is already working very well. But recently I've been thinking if maybe there aren't new general methods that can improve over what I've been doing. It's been many decades since those methods were discovered. Maybe I'm really outdated! Are there any well known alternatives to Metropolis-Hastings that are: reasonably easy to implement, as universally appliable as MH, and always improves over MH's results in some sense (computational performance, accuracy, etc...)? I know about some very specialized improvements for very specialized models, but are there some general stuff everybody uses that I don't know?
