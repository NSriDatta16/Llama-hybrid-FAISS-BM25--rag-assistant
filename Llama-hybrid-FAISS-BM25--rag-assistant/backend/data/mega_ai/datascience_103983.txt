[site]: datascience
[post_id]: 103983
[parent_id]: 
[tags]: 
Why are deep learning models unstable compare to machine learning models?

I would like to understand why deep learning models are so unstable. Suppose I use the same dataset to train a machine learning model multiple times (for example logistic regression) and a deep learning model multiple times as well (for example LSTM). After that, I compute the average of each model and its standard deviation. The standard deviation of the deep learning model will be much more higher than that of the machine learning model. why is this so? Does this have anything to do with the weight initialization in deep learning approaches. If this is the case, why does the model not always converge at the same point?
