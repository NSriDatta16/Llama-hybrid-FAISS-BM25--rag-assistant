[site]: crossvalidated
[post_id]: 271308
[parent_id]: 
[tags]: 
Correct use of a development (a.k.a. validation) set with PredefinedSplit and GridSearchCV

I'm dealing with a classical machine learning classification task. I'm using the sklearn library. My question is about the correct use of the test, development (can be intended as validation?) and train data with some API function of the library in the following example: # given X_train, X_dev, X_test as data and y_train, y_dev, y_test as labels validation_set_indexes = [-1]*len(X_train) + [0]*len(X_dev) ps = sklearn.model_selection.PredefinedSplit(test_fold=validation_set_indexes) search = ms.GridSearchCV( cv=ps, estimator=..., param_grid=..., scoring=... ) search.fit(X_train+X_dev,y_train+y_dev) y_test_pred = search.predict(X_test) In particular: It is correct to use development set as validation set? Are they the same or not? Does the PredefinedSplit do what I intended? i.e. Use X_dev as validation set in the grid search? Here it is explained the use of zeros and ones; It is correct to predict y_test_pred directly with the object return by GridSearchCV ( search in the example)? Or do I need to do something like the following: # pick the best parameters found by the grid search best_param_1 = search.best_params_["param_1"] best_param_2 = search.best_params_["param_2"] ... new_model = model(param_1=best_param_1, param_2=best_param_2,...) new_model.fit(X_train, y_train) y_test_pred = model.predict(X_test) In other words, use the grid search only for finding the best parameters for the scoring function and then run again the fitting of the model only on the training set?
