[site]: datascience
[post_id]: 104303
[parent_id]: 
[tags]: 
Train a parametrized model to sample from a known target distribution

I wonder if there is a way to train a parametrized model to sample from a known distribution such as Gaussian. We usually don't need a model to sample from a known distribution (if we know the CDF for instance), but this is necessary in some task I am dealing with. In addition, I want to avoid using GANs if possible. In GANs we don't know the target distribution, so knowing it should alleviate the problem. Would it be correct to "switch" the direction in Maximum Likelihood Estimation? We usually have samples from an unknown distribution $p_{X}$ and a way to compute $p_{\theta}$ , so we can update $\theta$ by maximizing the log-likelihood: $$max_{\theta}\mathbb{E}_{X}[\log{p_{\theta}(X)}]$$ Which is theoretically equivalent to minimizing the KL divergence. Can we maximize, for instance, $$max_{\theta}\mathbb{E}_{X_{\theta}}[\log{p_{X}(X_{\theta})}]?$$
