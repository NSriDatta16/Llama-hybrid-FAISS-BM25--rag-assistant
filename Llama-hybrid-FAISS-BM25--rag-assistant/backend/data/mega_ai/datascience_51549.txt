[site]: datascience
[post_id]: 51549
[parent_id]: 51404
[tags]: 
I have checked four well-cited papers related to word embedding: 2013 Word2Vec , 2014 GloVe , 2018 BERT , and 2018 ELMo . Only GloVe has experimented on the embedding dimension for the analogy task (answering "a" is to "b" as "c" is to ? ). Other papers did not report an experiment on embedding dimension size. They are all using an arbitrary dimension on the order of hundreds ( 100 and 300 are used more frequently). The lack of experiments for embedding size implies that the performance is not very sensitive to this parameter and only the order of magnitude matters, and also other aspects of the model architecture are more important to investigate. You can carry out a similar experiment using values from three orders of magnitude 10, 100, and 1000. The required dimension size will depend on the task too. I speculate that a classification (discriminative) task would require fewer dimensions than a sentence generation task. Generally, for hyper parameter optimization, methods like Bayesian Optimization can be used to find the best hyper parameter (here, embedding dimension) with as few (costly) training-evaluations as possible. These techniques are more useful in competitive settings where every improvement counts.
