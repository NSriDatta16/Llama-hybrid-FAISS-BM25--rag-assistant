[site]: crossvalidated
[post_id]: 548853
[parent_id]: 548837
[tags]: 
There are the following 5 standard assumptions: A0 (Linearity): The data generating process is given by $y_i = x_i\cdot\beta + u$ . Here $y$ is the dependent variable and $x$ is a vector of $k$ explanatory variables. The parameter $\beta$ determines the effect of the explanatory variables on the dependent variable. Deviations from this relationship are captured by the error term $u$ . A1 (No perfect multikolinearity): None of the $k$ explanatory variable is a perfect linear combination of the other. This assumption ensures that the OLSE exists. If one variable was a perfect linear combination of the other (i.e. we can write one variable $a$ as $a = \lambda b + \delta c$ , where $\lambda$ and $\delta$ are real numbers and $b$ and $c$ are other explanatory variables), it would be impossible for a linear estimator to distinguish between the three variables. Heuristically, the variables $a,b$ and $c$ would have to be estimated but since the variables are a perfect linear combination, there are only two independent equations. That is, we would have to solve for three variables with only two equations. A2 (Mean independence): The conditional mean of $u$ given the explanatory variables $x_1, x_2, \dots, x_n$ in the sample is zero, i.e. $\mathbb E[u\mid x_1, x_2, \dots, x_n] = 0$ . This assumption ensures that the (conditional and unconditional!) mean of the OLSE is given by $\beta$ , i.e. the OLSE is (conditionally and unconditionally) unbiased. Consequently, the average effect of the explanatory variables is given by $\hat\beta$ . A3 (Constant variance): The conditional mean of $u^2$ given the explanatory variables $x_1, x_2, \dots, x_n$ in the sample is a non-zero finite constant, i.e. $\mathbb E[u^2\mid x_1, x_2, \dots, x_n] = \sigma^2$ for some finite $\sigma > 0$ . This assumption ensures that the variance of the OLSE does not vary with the explanatory variables and that $\sigma^2$ can be easily estimated with the usual variance estimator $(n-k)^{-1}\sum_{i=1}^n(y_i - x_i\cdot\hat\beta)^2$ , provided that assumption A2 holds. Furthermore, the (conditional and unconditional!) variance of the OLSE is then given by $\sigma^2(\sum_{i=1}^n x_ix_i')^{-1}$ A4 (Normality): The conditional distribution of $u$ given the explanatory variables $x_1, x_2, \dots, x_n$ in the sample is normal with mean $0$ and variance $\sigma^2$ . This assumption allows statistical inference in finite samples since the OLSE $\hat\beta$ will be also normal with mean $\beta$ and variance-covariance matrix $\sigma^2(\sum_{i=1}^n x_ix_i')^{-1}$ . Assuming $\sigma$ is known, one could test whether the $\ell$ th component of $\beta$ is equal to zero by checking whether $$\frac{e_\ell\cdot\beta}{\sqrt{\sigma^2e_\ell\cdot(\sum_{i=1}^n x_ix_i')^{-1}e_\ell}}$$ is larger than 1.96. Here $e_\ell$ denotes the $\ell$ th basis canonical basis vector of the Euclidean space. Another important consequence of assumption A4 is that $\hat\beta$ is not only BLUE (best linear unbiased estimator) but BUE (best unbiased estimator). Note that we don't need the assumption that $u_1, u_2, \dots, u_n$ are independent and identically distributed (assumption A4 implies it in the above setting though). If we added the assumption, it would suffice to condition on $x_i$ instead of the full sample, as the full sample would be, by assumption independent, i.e. we would have, for example, $\mathbb E[u\mid x_1, x_2,\dots,x_n] = \mathbb E[u\mid x_i]$ . The sampling assumption is also very specific to the use-case. For example, independent errors works well for most cross section applications, but it is a very strong assumption for time series or panel data analysis.
