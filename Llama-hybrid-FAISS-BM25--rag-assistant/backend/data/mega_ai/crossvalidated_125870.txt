[site]: crossvalidated
[post_id]: 125870
[parent_id]: 41888
[tags]: 
Let $p(\alpha,\beta|M_1)=\mathrm{Ga}(\alpha,\beta)$ be your first prior, and $p(\alpha,\beta|M_2)=\mathrm{Ga}(\alpha+1,\beta)$ be your second prior. Furthermore, let $D$ be your observed data. Your posterior distribution can then be expressed as: $$p(\alpha,\beta|D,M_i) = \frac{L(D|\alpha,\beta)p(\alpha,\beta|M_i)}{p(D|M_i)}$$ where $p(D|M_i)=\int\int L(D|\alpha,\beta)p(\alpha,\beta|M_i) \,\mathrm{d}\alpha\mathrm{d}\beta$ and $i$ is either 1 or 2. How would a Bayes factor for the two prior distributions be formed? The Bayes factor is defined as $$K=\frac{p(D|M_1)}{p(D|M_2)}$$ Meaning, that the Bayes factor is not formed for the two prior distributions alone, but always in combination with the respective likelihood. Typically, in Bayesian analysis we compare different likelihood functions. However, comparing different prior assumptions also makes perfect sense in some cases. A typical application would be to make robust predictions: $$p(\alpha,\beta|D) = p(\alpha,\beta|D,M_1)p(M_1|D) + p(\alpha,\beta|D,M_2)p(M_2|D) $$ with $$ p(M_i|D) = \frac{p(D|M_i) p(M_i)}{p(D|M_1) p(M_1)+p(D|M_2) p(M_2)} $$ Assuming that you are indifferent between $M_1$ and $M_2$ a priori, we have $p(M_i)=0.5$ How can an estimate of the observed data be combined with the model probabilities to achieve a posterior ratio of model probabilities? I am not sure if I understand your question correctly, but in my opinion this is the Bayes factor. A value of $K > 1$ means that $M_1$ is more strongly supported by the data under consideration than $M_2$.
