[site]: crossvalidated
[post_id]: 410620
[parent_id]: 
[tags]: 
Backpropagation gradient of the average

In the Pytorch Udacity course , the following is said at one point: To calculate the gradients, you need to run the .backward method on a Variable, z for example. This will calculate the gradient for z with respect to x Following that, it is said in the course material that $$ \frac{\partial z}{\partial x} = \frac{\partial}{\partial x}\left[\frac{1}{n}\sum_i^n x_i^2\right] = \frac{x}{2} $$ I would have assumed that one could only take the derivative with respect to an individual $x_i$ since each of them is a separate random variable. However, then the solution would not make sense. I'd be very thankful for some clarity on how to arrive at $x/2$ in this case.
