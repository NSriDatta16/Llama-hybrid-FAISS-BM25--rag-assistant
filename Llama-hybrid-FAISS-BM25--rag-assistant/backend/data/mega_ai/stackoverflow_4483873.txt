[site]: stackoverflow
[post_id]: 4483873
[parent_id]: 
[tags]: 
Semantics of comparison of char objects

While I was reading through some old code today, I noticed the following assert line: assert(('0' The purpose is to assert that hexChar is a hexadecimal digit ([0-9A-Fa-f]). It does this by relying on an ASCII-like ordering of char objects representing 'A' , 'B' , ..., 'F' and 'a' , 'b' , ..., 'f' . I began wondering whether this always does what I intended, given that the execution character set is implementation-defined. The C++ standard in Section 2.3, Character sets, mentions: The basic execution character set and the basic execution wide-character set shall each contain all the members of the basic source character set, plus control characters representing alert, backspace, and carriage return, plus a null character (respectively, null wide character ), whose representation has all zero bits. For each basic execution character set, the values of the members shall be non-negative and distinct from one another. In both the source and execution basic character sets, the value of each character after 0 in the above list of decimal digits shall be one greater than the value of the previous. The execution character set and the execution wide-character set are implementation-defined supersets of the basic execution character set and the basic execution wide-character set, respectively. The values of the members of the execution character sets and the sets of additional members are locale-specific. I interpret this to mean that ('0' is okay because '0' , '1' , ..., '9' are digits and each has a value one greater than the previous. However, the order of other basic source characters with respect to one another is still implementation-defined. Is this a correct statement? Knowing nothing about the C++ compiler (so not knowing the implementation details), do I need to rewrite the assert as the following? assert(('0'
