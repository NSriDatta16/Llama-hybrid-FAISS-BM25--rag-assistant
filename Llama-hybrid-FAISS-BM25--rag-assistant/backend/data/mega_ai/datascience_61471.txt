[site]: datascience
[post_id]: 61471
[parent_id]: 37488
[tags]: 
hssay answer is well-written but I thought I add my own explanation to simplify it even more and shows how it works for PV-DM also Doc2Vec, unlike Word2Vec, is built in a unique way that makes the prediction (or inference as they call it) of the same document slightly different each time. The reason for this slightly un-stable prediction is that the prediction operation actually tries to adjust some weights of the neurons inside the network each time to be able to “correctly” predict the outcome. You might find it odd first but this is why the algorithm is able to predict embedding for documents not only words. Explanation Distributed memory model architecture Before the training starts, we assign a Document ID to each document in our training dataset. So each data point has 2 components: 1) Document ID 2) A Document (list of words.) First, we transform each word in the corpus to a vector using the traditional Word2Vec algorithm. Second, we train a model by feeding the network 2 things: 1) Document ID and 2) Context from that Document. The objective of the training phase is to adjust the weights so that for any Document ID and a Context (fixed-length and sampled from a sliding window over the Document), the model can predict the next word with high probability. To do this, we form 2 layers, one to transform the Document ID into vector (Layer A), and another hidden layer (Layer B) with softmax at the output that takes as an input the average (or concatenation) of : 1- The output of Layer A (Vector of Document ID) 2- The embedding of the words in the Contexts done by Word2Vec Softmax layer outputs the vector representation of the Document. The model trains until all weights are setup in a way to achieves the highest prediction probabilities (or as close it can get). Now comes the prediction phase. The data point for prediction is only the Document we want to predict its vector (we don’t have Document ID as before). We need Document ID as input for Layer A otherwise we won't’ be able to use the network, so what do we do? We try to predict the Document ID and here is why it’s not deterministic :) To be able to predict the Document ID: 1- we freeze the hidden layer weights we learned from training (Layer B) 2- We give the network a Context from that Document we want to predict its vector. 3- We use our Word2Vec model to get the embeddings for each word in context 4- We come up a random vector 5- We take the average (or concatenation) of vectors in step 3 and 4 above and use it as input to the hidden layer (which has weights frozen) 4- We check if the random vector we chose did maximize softmax probability for the predicted next word in Context. 5- We repeat this process by stochastically gradient descending on Document ID vector until we find the Document ID vector represents that maximizes the probability for the selected word 6- We use the predicted Document ID along with Document words vectors to predict Document vector representation (which will be as good as the predicted Document ID) This is why we can't always get the exact same value for the Document vector. This is how a distributed memory model architecture works, Distributed bag of words also rely on predicting the Document ID to predict the Documents vector but has a different algorithm (explained below if you are interested) Distributed bag of words architecture: At the training phase, we train the model by feeding the network 2 things: 1) Document ID and 2) randomly selected words from that Document. The objective of the training phase is to adjust the weights so that for any Document ID, the model can predict the randomly selected words from that document with high probability. To do this, we form 2 layers, one to transform the Document ID into vector (Layer A), and hidden layer (Layer B) with softmax at the output that takes as an input the output of Layer A (Vector of Document ID). Softmax layer outputs the vector representation of the Document. The model trains until all weights are setup in a way to achieves that (or as close it gets). For prediction: 1- we freeze the hidden layer weights (Layer B) we learned from training 2- We give the network randomly selected words from that Document we want to predict its vector. 3- We come up a random vector as an input to the hidden layer (Layer B) 4- We check if the random vector we chose did maximize softmax probability for the selected words. 5- We repeat this process by stochastically gradient descending on Document ID vector until we find the Document ID vector represents that maximizes the probability for the selected words 6- We use the predicted Document ID as in input to the hidden layer to predict Document vector representation (which will be as good as the predicted Document ID)
