[site]: crossvalidated
[post_id]: 390076
[parent_id]: 
[tags]: 
Why does the "window-based" model fail to take advantage of the repetition?

In Glove paper https://nlp.stanford.edu/pubs/glove.pdf , the author says "Unlike the matrix factorization methods, the shallow window-based methods suffer from the disadvantage that they do not operate directly on the co-occurrence statistics of the corpus. Instead, these models scan context windows across the entire corpus, which fails to take advantage of the vast amount of repetition in the data." However, doesn't the window-based model (i.e. word2vec) take into account the repetition, by reading the whole corpus? Throughout the whole corpus, even if the word appears multiple times, the loss will be repeatedly calculated and update the params during the training. What am I missing here? These authors are smart people. Why do they say this?
