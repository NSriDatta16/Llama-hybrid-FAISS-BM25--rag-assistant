[site]: datascience
[post_id]: 87653
[parent_id]: 87648
[tags]: 
$|d| \gg 0$ means there is a very strong correlation between $x_1$ and $x_2$ . This means one can be expressed (almost completely) in terms of the other , thus one of two is almost redundant . A simple example: Consider that $x_2$ is simply a copy of $x_1$ , ie $x_2=x_1$ . Does $x_2$ offer any new information about the the label $y$ apart from the information $x_1$ provides? No , it is clear. Now consider that $x_2$ is simply a shifted version of $x_1$ , ie $x_2=c_0+x_1$ . Does it now offer any new information apart from what $x_1$ provides? Again no , by simply changing the reference point $c_0$ , which is equivalent to changing the baseline of the measuring system, does not by itself offer new information. Thirdly consider that $x_2$ is a re-scaled version of $x_1$ , ie $x_2 = s \cdot x_1$ . Does it offer now different information than what $x_1$ provides? Again, no since scaling is equivalent to changing the measuring unit of $x_1$ , but this is simply a re-naming of units, no new information is added. One can see that any transformation of the form $x_2 = c_0 + s \cdot x_1$ does not offer any new information from what $x_1$ already provides, by the arguments above. If one wants to be more realistic one can add some noise, ie $x_2 = c_0 + s \cdot x_1 + \epsilon$ , where $\epsilon$ is some random variable with zero mean and very small variance. A correlation value $|d|$ which is quite high actualy means that $x_2$ can be expressed as such a transformation of $x_1$ , ie $x_2 = c_0 + s \cdot x_1 + \epsilon$ (or vice-versa) (there are some non-trivial exceptions to this ). So $x_2$ offers no new information apart from what $x_1$ offers and is redundant. So if such a high correlation exists between variables/features , one of them can be always eliminated without diminishing performance as being redundant (in the sense that, as far as second-order statistics are concerned, the two variables are identical ). This is one of the main rationales of feature selection and dimensionality reduction in machine learning.
