[site]: datascience
[post_id]: 114988
[parent_id]: 
[tags]: 
Why is this an incorrect update of the parameters in the gradient descent algorithm? (Bishop, Pattern Recognition and Machine Learning)

Let's say we are performing a linear regression, with general model $y(x,w) = w_0 + w_1x$ . The error function is $E(w) = \frac{1}{2N}\sum_n ((y(x_n,w)-t_n)^2$ , for $N$ datapoints ${(x_n,t_n)}$ (training dataset). What's the difference between this correct update: $\text{temp0} := w_0-\alpha {\partial \over {\partial w_0} }E(w_0,w_1)$ $\text{temp1} := w_1-\alpha {\partial \over {\partial w_0} }E(w_0,w_1)$ $w_0 := \text{temp0}$ $w_1 := \text{temp1}$ and this incorrect one? $\text{temp0} := w_0-\alpha {\partial \over {\partial w_0} }E(w_0,w_1)$ $w_0 := \text{temp0}$ $\text{temp1} := w_1-\alpha {\partial \over {\partial w_0} }E(w_0,w_1)$ $w_1 := \text{temp1}$ The values of temp0 and temp1 are still the same ones. The only thing we change is the order of update. I'm struggling to understand why does this matter. Also, now that I'm here, I gotta ask: why does this error function has a $\frac{1}{2N}$ term? Wouldn't have the error function given by the negative log-likelihood this expression?: $$ E(w) = \beta \frac{1}{2}\sum_n ((y(x_n,w)-t_n)^2 $$ for $\beta^{-1}=$ std dev of the datapoints. This reasonings are taken from the Bishop book "Pattern Recognition and Machine Learning".
