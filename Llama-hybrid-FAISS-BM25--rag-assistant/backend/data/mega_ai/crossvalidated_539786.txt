[site]: crossvalidated
[post_id]: 539786
[parent_id]: 539766
[tags]: 
ML models define their own conditional distributions ( $p(y|x)$ for discriminative modeling, $p(x)$ for generative modeling etc.), likelihood of which is maximized with the training data (or for deriving the posterior in bayesian analysis). I see ML as a generalization of probabilistic modeling where the distribution itself is inferred from the data, often without any assumptions. Here the fact data comes from an unknown distribution, which can not be easily modeled, acts as an impetus for using more expressive neural networks as distributions. In this case common statistical distributions often act as strong priors for parameters (such as in bayesian neural networks) and in that way, the parameter space essentially becomes a multivariate statistical distribution. On the other hand, I have sometimes seen examples where parameters of statistical distributions (eg. $\mu$ , $\sigma$ in case of normal distribution) are framed as the outputs of a neural network and computed by training it. In this case, we make assumptions that the data comes from a certain, fixed distribution and learn its parameters via flexible ML modeling. This can often take the form complicated hierarchical statistical modeling as well.
