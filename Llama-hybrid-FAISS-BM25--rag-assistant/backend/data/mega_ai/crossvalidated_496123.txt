[site]: crossvalidated
[post_id]: 496123
[parent_id]: 
[tags]: 
Does it make sense to use attention mechanism for seq-2-seq autoencoder for anomaly detection?

So I want to train LSTM sequence to sequence model, autoencoder, for anomaly detection. The idea is to train it on normal samples and when anomaly comes into model it will not be able to reconstruct it correctly and will have high reconstruction error. I am thinking about how to make the model better, does it make sense to use attention mechanism after encoder network ?
