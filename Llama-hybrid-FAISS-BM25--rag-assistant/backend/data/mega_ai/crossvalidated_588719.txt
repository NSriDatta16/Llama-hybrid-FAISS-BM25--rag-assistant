[site]: crossvalidated
[post_id]: 588719
[parent_id]: 587969
[tags]: 
There are lots of ways you could model this type of data, but I will confine my attention here to models that are roughly in accordance with what you are proposing with your idea of having the engagements given an exponentially decaying effect on the target variable. Your proposed method of assuming exponential decay in relevance might be a reasonable way to proceed, but your present form assumes a fixed rate of decay rather than estimating this from the data. To be effective, your model would need to estimate the rate of decay from the data, which would yield a nonlinear count model. Suppose we let $\{ A_t \}$ , $\{ B_t \}$ , $\{ S_t \}$ and $\{ Y_t \}$ denote the time-series of engagement values for A and B, sites (categorical variable), and the target values respectively. Then you would want to use some kind of GLM equation like: $$\mathbb{E}(g(Y_t)) \equiv \mu_t = \beta_A \sum_{k=0}^\infty \exp(-\theta_A A_{t-k}) + \beta_B \sum_{k=0}^\infty \exp(-\theta_B B_{t-k}) + \sum_{s=1}^{K-1} \beta_{k} \mathbb{I} (S_t = s),$$ where in practice we would cut off the sums at some sufficiently large value $N$ . (The function $g$ is the link function in the GLM.) You are correct that there is some arbitrariness in cutting this off at a finite value $N$ , but if this value is sufficiently large then the effect of engagements further back in time should be minimal. (You can check the upper bound on earlier effects as a post-hoc inference once you've estimated your coefficients, so you should be able to satisfy yourself that you have enough model terms included.) There is also a drawback in assuming that unobserved earlier values are no-engagement, but again, this effect can be limited by noting the upper bounds on the exponential decay. In any case, if you decide to go with something like this, it is a nonlinear GLM equation where the rate parameters in the exponentials cannot be "linearised". Consequently, you would need to use a function like the gnlr function in the gnlm package to fit the model. You should use a negative binomial count model rather than a Poisson count model, because the negative binomial is a two-parameter model that allows you to correctly estimate the dispersion. If you were to successfully implement a model like this, it would allow you to estimate the effects $\beta_A$ and $\beta_B$ from the two types of engagements to the target variable and the rates of decay $\theta_A$ and $\theta_B$ that determine the degree to which older engagements lose relevance to the target variable. You would also be able to estimate the effect of the site (categorical variable) on the target variable. You would need to augment your analysis with appropriate diagnostic plots (e.g., added variable plots) to see if your posited model looks reasonable from the data. Ultimately, you would be looking to get a model where the fit is reasonable and the diagnostic plots do not falsify your model assumptions.
