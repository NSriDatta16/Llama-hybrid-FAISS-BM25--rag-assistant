[site]: datascience
[post_id]: 87485
[parent_id]: 87482
[tags]: 
From a practical point view, yes you could use both options seamlesly with in general similar results (check this scikit learn functionality ), but: SGD (Stochastic Gradient Descent) is an optimization algorithm among others log loss/hinge loss ... are the loss functions used in the selected optimization strategy (SGD in your case) to find the optimal weights to fit such linear models, but another optimization solvers could be used with those algorithms. See this post for logistic regression with sklearn , where you have the solver parameter, which can be, among others, 'sag' (stochastic average gradient descent), but others like 'lfbs' are also accepted.
