[site]: crossvalidated
[post_id]: 510469
[parent_id]: 510052
[tags]: 
I am not an expert, so consider this as an opinion from someone who is still learning about these two fields. Short Answer Theoretically, No. DNNs can perform all the functions of SVMs and more. Practically, mostly no. For most modern problems DNNs are a better choice. If your input data size is small and you are successful in finding a suitable kernel, however, an SVM may be a more efficient solution. But, if you can't determine a suitable kernel, NNs are then a better choice. Long Answer If we gloss over some details about the constraints of different implementations, deep neural networks are essentially universal function approximators. Given enough training time and a sufficiently robust architecture and training set (what we might call operating at the theoretical limit) they will essentially learn an approximation of an optimum solution. It might be an SVM-style kernel transformation or it might be some other function, it all depends on what approach actually works best for the task at hand. In this sense, I think DNNs can be thought of as a super set that contains SVMs as one option among many. If an SVM is the most appropriate, the DNN will automatically learn the best kernel transformation for the task (assuming we are operating at the limit). So in a purely theoretical sense, I would say no, there is nothing an SVM can do that a DNN cannot (given enough layers of sufficient size, training, ect.) From a practical perspective, however, which one is better comes down to the particular problem at hand and the constraints you are working with. In general though, I think DNNs are still often the better solution in practice, especially for most modern problems. There are many ways of looking at this, but I think one of the more informative is thinking about the kernel. In some sense, SVMs are not "learning" a solution to your problem, they are just applying the kernel you pre-specified to your data. If that kernel results in your data being linearly separable in kernel space, great, it will work, if not, it will "fail" (to varying degrees). The key point being that you are not learning the appropriate kernel from your data, you are essentially guessing that your predetermined kernel will work. DNNs, on the other hand, can be thought of as simultaneously learning the appropriate kernel and applying it to your data. I think this is a major advantage of DNNs, especially in more "exploratory" cases. If, however, you already "know the answer" to your classification problem (in the sense you know the input data has certain properties and have a kernel that is known to exploit those properties to make the classes linearly separable in kernel space) then an SVM may be a better solution. Given that the computational load scales quadratically with the data set size for SVMs, however, your data also needs to be small enough. SVMs may also be preferred in that they are more "predictable"/theoretically founded. You know they are applying X kernel transformation to the data and attempting to linearly separate the classes by drawing a hyperplane equidistant between the two closest points from each class. That's a solution with defined behavior. What's going on inside of a very deep neural network, however, is much more difficult to ascertain. In conclusion, from a theoretical perspective, I think DNNs provide power and flexibility beyond what SVMs can provide, and from a practical perspective, given that many modern problems involve fairly large input data sets and often no obviously appropriate kernel, I think NNs are often the more appropriate choice. The only benefits of SVMs over DNNs that I can see are that SVMs are more interpretable, easier to train, and and can be more computationally efficient for small datasets.
