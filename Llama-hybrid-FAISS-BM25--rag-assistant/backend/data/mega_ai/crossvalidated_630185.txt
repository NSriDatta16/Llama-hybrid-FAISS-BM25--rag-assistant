[site]: crossvalidated
[post_id]: 630185
[parent_id]: 
[tags]: 
KL divergence as minimum patch size for data differencing?

The Wikipedia article on KL divergence mentions a link with data differencing . Directly quoting Wikipedia (as of 2023/11/01): Just as absolute entropy serves as theoretical background for data compression, relative entropy serves as theoretical background for data differencing â€“ the absolute entropy of a set of data in this sense being the data required to reconstruct it (minimum compressed size), while the relative entropy of a target set of data, given a source set of data, is the data required to reconstruct the target given the source (minimum size of a patch). I find this quote a bit confusing. In my understanding, given the value of "source" data $S=s$ , the average number of bits required to encode a random target $T$ (in a uniquely decodable way) is theoretically bounded by the conditional entropy : $$ H(T\vert S=s)=\sum _t\text{Pr}(T = t \vert S = s) \log \text{Pr}(T = t \vert S = s), $$ on average: $$ H(T\vert S)=\sum _s \text{Pr}(S = s)\sum _t\text{Pr}(T = t \vert S = s) \log \text{Pr}(T = t \vert S = s). $$ I fail to see the link with KL divergence here. I guess my confusion may arise from the specific meanings of the wordings "reconstructing target data [...] given source data [...]" in the above quote. Can someone clarify in what sense KL divergence "serves as theoretical background for data differencing", as in the Wikpedia quote? PS: I'm aware about the usual interpretation of KL divegence $D(P\vert \vert Q)$ as the average number of "extra" bits required to encode $T\sim P$ , when using an optimal code for a wrong probability distribution $Q$ . But that seems to be another matter... isn't it?
