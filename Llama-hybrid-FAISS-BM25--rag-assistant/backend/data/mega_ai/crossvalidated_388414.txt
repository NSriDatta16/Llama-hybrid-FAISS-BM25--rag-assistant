[site]: crossvalidated
[post_id]: 388414
[parent_id]: 
[tags]: 
Hypothesis testing for a known parameter value vs. for an estimated parameter value

I will motivate this question with 2 examples: Cointegration coefficient known vs. unknown The expectations theory of the term structure of interest rates establishes that the cointegration coefficient between the 10-year interest rate (Y t ) and the 3-month interest rate (X t ) must be 1. Therefore, to test whether these two interest rates are cointegrated using the Engle-Granger method, we can test whether the series Z t is stationary, where Z t =Y t -1*X t . In so doing, we will use one set of critical values to conduct the hypothesis test (the Dickey Fuller critical values). If, however, we did not know the cointegration coefficient and had to determine it first by regressing Y t on X t (i.e. first step in the Engle-Granger cointegration method), we would have to use a different set of critical values (the Engle-Granger Dickey Fuller critical values) in conducting the hypothesis test that the two series are cointegrated, with the cointegration coefficient equal to the value we estimated. Why? What exactly is different about hypothesis testing on a known/assumed parameter value vs. hypothesis testing on estimated parameter value? To be more specific, suppose that we've estimated the cointegration coefficient and it is equal to 1. In testing the hypothesis that the two series are cointegrated with a cointegration coefficient equal to 1, we would use different critical values depending on whether the cointegration coefficient is known/assumed to be equal to 1 (Dickey Fuller critical values) or estimated to be equal to 1 (Engle-Granger Dickey Fuller critical values). Why would these two tests differ? How exactly is the additional uncertainty arising from the fact that the cointegration coefficient was estimated (vs. known) reflected in the Engle-Granger Dickey Fuller critical values (vs. Dickey Fuller critical values)? Date of break in time series known vs. unknown To determine whether a series is produced by a non-stationary process due to a break in the process, we can either know the break date or not know it and need to determine it. In the first case, if we know the break date, we can use the Chow test to test whether the F-statistic corresponding to the hypothesis that the break date dummy variable, as well as the break date dummy interacted with all relevant regressors are all zero. In conducting this test, a particular set of critical values will be used. If, however, we did not know the break date, we would have to determine it first. We could estimate it by testing (Chow test) for breaks at every possible date between the first date and the last date in our dataset. We could select the date with the largest F-statistic as the break date. I've just described the QLR test. It is identical to the Chow test, except that the break date is determined rather than known and hence as Stock and Watson write "Because the QLR statistic is the largest of many F-statistics, its distribution is not the same as an individual F-statistic". Why? Why is the distribution of the QLR statistic different from that of the Chow statistic? How is the additional uncertainty arising from the fact that the break date was determined/estimated (vs. known) reflected in the QLR statistic (vs. Chow statistic)? The only difference is that in the QLR test we determined the break vs knew/hypothesized it in the Chow test. I would be happy with a technical answer and thrilled with a technical + intuitive explanation.
