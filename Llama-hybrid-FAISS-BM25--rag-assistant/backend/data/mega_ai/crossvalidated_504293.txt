[site]: crossvalidated
[post_id]: 504293
[parent_id]: 504281
[tags]: 
@gunes has already given the answer, that random forests do work well with interactions. It may help to give a practical example, though. For that, we will need some toy data set.seed(2021) n = 5000 x1 Basically we have a y that is a linear function of x1 , x2 , x3 and the interaction x1*x2 . In the dataset there is also a permutation of the interaction term and two more random variables, all of which bear no real information. We split this into a training and an equally sized testing dataset like so: train As the toy data have been modelled in a linear model, linear regression should be able to solve that puzzle easily: > linear1 summary(linear1) Call: lm(formula = y ~ . - pseudointeraction, data = example) Residuals: Min 1Q Median 3Q Max -1.071e-12 -3.400e-16 5.000e-17 4.700e-16 1.524e-12 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) -1.641e-14 9.004e-16 -1.823e+01 The linear modell is wrong in declaring x4 a significant predictor but as it was to be expected, $R^2$ is about 100%. If we take the interaction term away or exchange it for the permuted interaction term, things get far worse as was to be expected: > linear2 summary(linear2) Call: lm(formula = y ~ . - interaction, data = example) Residuals: Min 1Q Median 3Q Max -17.4533 -1.2764 0.0621 1.2977 14.8616 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) -0.1791910 0.1214622 -1.475 0.140 x1 6.0262464 0.0287402 209.680 $R^2$ dropped to 82%. So the interaction term is important in this model. Let's grow a randomForest with the interaction term and one without it: library(randomForest) rf1 This will take some time to compute and the print results will claim that the random forest with an interaction term given explained 99.67% of the variance while the random forest with the interaction term explained 99.93% of the variance. It is hard to compare within-sample $R^2$ to Out-of-bag-performance, so we predict our test data set with all four of our models we have so far: linear1.on.test In a next step we compute the absolute residuals and plot them as boxplots: boxplot(linear1.on.test - test $y, linear2.on.test - test$ y, rf1.on.test - test $y, rf2.on.test - test$ y, ylim=c(-7, 7), xaxt="n", ylab = "residuals") axis(1, at = 1:4, labels = c("linear with I", "linear w/o I", "rf w/o I", "rf with I")) So the linear model with interaction term fits best to how the data was constructed and thus predicts best. The linear model without interaction term has no flexibility to adapt and has the worst predictive value of all models. For the random forests we see quite similar results with and without the interaction term. Stating the interaction term explicitely appears to be helpfull but the model without it is flexible enough to give good predictions without that. Real world interactions will rarely follow the rules of exact multiplication so the small advantage of computing the interaction term for the model may be even smaller for real world interactions. All of this will be influenced by sample size, number of features, tuning parameters and so on but the short take away message is that whilst feature building is a good thing, random forest can detect and make use of interactions even if they are not stated in the call to randomForest . Feel free to run this code with different set.seed values, compute RMSEs etc.
