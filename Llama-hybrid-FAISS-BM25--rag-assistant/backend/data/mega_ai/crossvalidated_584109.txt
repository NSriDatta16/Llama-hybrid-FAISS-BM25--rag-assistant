[site]: crossvalidated
[post_id]: 584109
[parent_id]: 
[tags]: 
Can a neural network fail to replicate the training data if trained on a very small dataset?

I created a neural network and I have trouble getting it to train. I followed all advice in this post: What should I do when my neural network doesn't learn? but have not had success yet. Then I decided to create a very simple dataset to check if the model is able to learn at all. I expected to see the model learning the dataset perfectly and overfitting. Instead it failed to learn. The loss stops going down quite early on and the model is unable to replicate even this tiny dataset. The complexity of the model was increased iteratively with no visible effect. Assume that the complexity is large enough to learn the patterns, but not too large to make optimization hard. So maybe I am making the wrong assumption. If you have a neural network and you train it on say, 30 examples. Should you not expect it to learn these 30 examples well enough to be able to replicate them? Or are there situations where the 30 examples might be too different from each other for the model to find a pattern? Even assuming that the 30 examples are generated using the same process/function?
