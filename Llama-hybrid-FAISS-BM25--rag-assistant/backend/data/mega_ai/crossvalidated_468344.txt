[site]: crossvalidated
[post_id]: 468344
[parent_id]: 
[tags]: 
Why the approximation of $\log \pi_{\theta}(a|s)$ improves numerical stability?

In Maxim Lapan's book Deep Reinforcement Learning Hands-on , section Continuous A2C, it says By definition, the probability density function of the Gaussian Distribution is $$f(x | \mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{(x-\mu)^2}{2 \sigma^2}}$$ We could directly use this formula to get the probabilities, but to improve numerical stability, it is worth doing some math and simplifying the expression for $\log \pi_{\theta}(a|s)$ . The final result will be this: $\log \pi_{\theta}(a|s) = \frac{-(x-u)^2}{2 \sigma^2} - \log \sqrt{2 \pi \sigma^2}$ Can you explain to me how to get the approximation of $\log \pi_{\theta}(a|s)$ ? How does it improve the numerical stability?
