[site]: crossvalidated
[post_id]: 281292
[parent_id]: 279385
[tags]: 
What is shown in that plot is a cumulative gains curve. These gains are based on the notion that a classifier estimates a probability $\hat{p}_{A}$ quantifying the belief that a sample instance is a member of class $A$. Based on the estimated probabilities $\hat{p_A}$ we can define segments of our sample for a particular cut-off point $C$, $0 C}$, instances associated with values greater that $C$. Given that we can define segments $S_{p_A\leq C}$ and $S_{p_A>C}$ we can naturally ask what percentage of overall sample each contains (for example if $C$ is close to $0$ we would generally expect $S_{p_A >C}$ to be large). Similarly we can ask: what percentage of the whole sample's class $A$ instances each these segments contain. Iterating through different cut-off values $C$ and recording their answers produces the gain curve. The percentage of whole sample within the segment tested serves as the $X$-axis and the percentage of the whole sample's class $A$ instances within the segment tested serves as the $Y$-axis. They are called cumulative lift/gain charts because they determine the cumulative class occurrences as more and more sample instances are tested. Let's see all these with the following R code: We have a random classifier that just estimates random probabilities ( randRes ), a perfect classifier that assigns probabilities ( perfRes ) such that BB instances always have lower probabilities $\hat{p}_A$ than AA examples and an "average Joe" classifier that assign probabilities ( averRes ) that are mostly OK but do mistakenly assign some high $\hat{p}_A$'s to instances of BB too. I append code using caret 's lift to plot the gains curve to ensure we get the same. set.seed(123) N = 5000; myRes min(estProbs)] percOfSampTested = sapply( relevantC, function(x) sum(estProbs > x) ) / MAll perfOfPosFound = sapply( relevantC, function(x) sum('AA' == realLab[ x So the interpretation of a curve is relatively straight-forward: If we test at random an $X\%$ of our sample we should get approximately $X\%$ of the class $A$ instances. If we get a higher percentage that will a gain. How much that gain will be can be seen from the gain curve. For example, based the plot shown, using the random classifier and testing $40\%$ of our data captures $40\%$ of our class $A$ samples ($Y$ value of the red curve at $X = 0.4$). If we used the perfect classifier (blue line) we would get $80\%$ and using the average classifier would get us a hint below $70\%$. Note that like other "curve-metrics" ( ROC curves , PR curves , Calibration curves , etc.) the utility of gains curves is often dependent to a particular area. For example we might be able to examine only $20\%$ of our sample, in that case we care only for a particular segment of the graph rather than the whole graph. A short one-page description of gains (and lift) curve can be found in Kuhn's Applied Predictive Modeling book (Chapter 11). A final note: I tried to steer away from notions of explicitly "sorting" or "reordering" the raw data. These procedures were probably helpful when people did not have computers but now such definitions are mostly legacy ones; they probably remain due to existing business literature. Clearly we use the estimated probabilities to infer which class-assignment is more plausible based on a particular cut-off but no direct data reordering is required (as seen in the code provided).
