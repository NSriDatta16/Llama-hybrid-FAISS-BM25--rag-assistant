[site]: crossvalidated
[post_id]: 187591
[parent_id]: 
[tags]: 
When the data set size is not a multiple of the mini-batch size, should the last mini-batch be smaller, or contain samples from other batches?

When training a artificial neural network using stochastic gradient descent with mini-batches, if the data set size is not a multiple of mini-batches, should the last mini-batch contains fewer samples? Or instead is it preferable to have the last mini-batch contain the same number of samples as the other batches, by randomly adding samples from other batches (which is the strategy used here and here )?
