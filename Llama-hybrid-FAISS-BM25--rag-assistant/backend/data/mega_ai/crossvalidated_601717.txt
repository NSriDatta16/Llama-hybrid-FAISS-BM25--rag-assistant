[site]: crossvalidated
[post_id]: 601717
[parent_id]: 
[tags]: 
To achieve consistency, why do we only make regularizer invariant?

The question comes from section 5.5.1 of "Pattern Recognition and Machine Learning" by Christopher M. Bishop. After giving linear transformation equations needed for network weights, the book says: If we train one network using the original data and one network using data for which the input and/or target variables are transformed by one of the above linear transformations, then consistency requires that we should obtain equivalent networks that differ only by the linear transformation of the weights as given. Any regularizer should be consistent with this property, otherwise it arbitrarily favours one solution over another, equivalent one. Clearly, simple weight decay (5.112), that treats all weights and biases on an equal footing, does not satisfy this property. Training is minimizing a regularized error function. For the simple weight decay regularization, the total regularized error function is of the form $$\widetilde E({\bf w})=E({\bf w})+\frac{\lambda}{2}{\bf w}^T{\bf w}.\tag{1}$$ I can see that the regularizer in $(1)$ is indeed not consistent. For example, assume in the linear transformation of weights for input $$w_{ji}\to\tilde w_{ji}=\frac{1}{a}w_{ji},\tag{2}$$ the factor $\frac{1}{a}>1$ and denote the weight vector after linear transformation $(2)$ as $\frac{1}{a}{\bf w}^*$ (a bit saggy on the notation since only weights of the first layer are scaled) where ${\bf w}^*$ is training solution for original data. If the optimization algorithm happens to arrive at $\frac{1}{a}{\bf w}^*$ under transformed input data, the value of regularizer $\frac{\lambda}{2}\bigl(\frac{1}{a}{\bf w^*}\bigr)^T\bigl(\frac{1}{a}{\bf w^*}\bigr)$ would be greater than $\frac{\lambda}{2}{\bf w}^{*T}{\bf w}^*$ when the original data was used. As a consequence, the optimization algorithm would likely continue to search for a minimum, leading to a different solution $\tilde {\bf w}^*$ which is in general not equal to the transformed $\frac{1}{a}{\bf w}^*$ . So, we failed to get an equivalent network as required, violating the consistency property. If my understanding is incorrect, you can stop the reading outright here and point out my mistake. To solve this issue, the book presents a regularizer that is invariant to re-scaling of weights and shift of bias in the following paragraph. By replacing regularization parameters $\lambda_1\to a^2\lambda_1$ , the value of the new regularizer under transformed weights $(2)$ would be $$\frac{a^2\lambda_1}{2}\sum\limits_{w\in\mathcal W_1}\left(\frac{1}{a}w\right)^2=\frac{\lambda_1}{2}\sum\limits_{w\in\mathcal W_1}w^2,$$ which results in the same value of the original invariant regularizer presented no matter what. Here and below I considered only the linear transformations for input variables for simplicity. So, if we only consider regularizer term in the error function, the optimizer algorithm stops at the new solution $\tilde{\bf w}^*=\frac{1}{a}{\bf w}^*$ , and consistency (or an equivalent network) is ensured. Perfect. However, the error function $(1)$ contains not only the regularizer, but also the basic error term $E({\bf w})$ , which is $$\frac{1}{2}\sum\limits_{n=1}^N\{t_n-y_n({\bf w})\}^2$$ in the 2-layer MLP settings of the section. When the optimization algorithm happens to arrive at $(2)$ , the value of regularizer remains unchanged as designed, but the value of the basic error term $E(\frac{1}{a}{\bf w^*})$ does not necessarily remains the same. Nor do I know if it grows larger or smaller. If it is greater than $E({\bf w^*})$ , the optimization algorithm would continue to search for a minimum which is in general different from the solution $\tilde{\bf w}^*=\frac{1}{a}{\bf w}^*$ needed for an equivalent network, leading to violation of consistency just like simple weight decay case I described above. So my question is: To achieve consistency, why do we only make regularizer invariant while assuming that the unregularized basic error term $E({\bf w})$ is magically invariant to re-scaling of the weights and to shifts of the biases, leading to a nonequivalent network and violation of consistency property required? It makes no sense to only make regularizer invariant but let the basic error go freely while still hoping for consistency. Did I miss anything? Thank you.
