[site]: crossvalidated
[post_id]: 99380
[parent_id]: 97014
[tags]: 
Gradient descent is an optimization algorithm . There are many optimization algorithms that operate on a fixed number of real values that are correlated ( non-separable ). We can divide them roughly in 2 categories: gradient-based optimizers and derivative-free optimizers. Usually you want to use the gradient to optimize neural networks in a supervised setting because that is significantly faster than derivative-free optimization. There are numerous gradient-based optimization algorithms that have been used to optimize neural networks: Stochastic Gradient Descent (SGD) , minibatch SGD, ...: You don't have to evaluate the gradient for the whole training set but only for one sample or a minibatch of samples, this is usually much faster than batch gradient descent. Minibatches have been used to smooth the gradient and parallelize the forward and backpropagation. The advantage over many other algorithms is that each iteration is in O(n) (n is the number of weights in your NN). SGD usually does not get stuck in local minima (!) because it is stochastic. Nonlinear Conjugate Gradient : seems to be very successful in regression, O(n), requires the batch gradient (hence, might not be the best choice for huge datasets) L-BFGS : seems to be very successful in classification, uses Hessian approximation, requires the batch gradient Levenberg-Marquardt Algorithm (LMA) : This is actually the best optimization algorithm that I know. It has the disadvantage that its complexity is roughly O(n^3). Don't use it for large networks! And there have been many other algorithms proposed for optimization of neural networks, you could google for Hessian-free optimization or v-SGD (there are many types of SGD with adaptive learning rates, see e.g. here ). Optimization for NNs is not a solved problem! In my experiences the biggest challenge is not to find a good local minimum. However, the challenges are to get out of very flat regions, deal with ill-conditioned error functions etc. That is the reason why LMA and other algorithms that use approximations of the Hessian usually work so well in practice and people try to develop stochastic versions that use second order information with low complexity. However, often a very well tuned parameter set for minibatch SGD is better than any complex optimization algorithm. Usually you don't want to find a global optimum. Because that usually requires overfitting the training data.
