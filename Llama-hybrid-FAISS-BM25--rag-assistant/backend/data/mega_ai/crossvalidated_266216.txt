[site]: crossvalidated
[post_id]: 266216
[parent_id]: 265940
[tags]: 
XGBoost does not produce a decision tree/trees with leaf node values of 0 or 1. Instead, it uses multiple regression trees with continuous value "weights" on its leaf nodes e.g. in the range 0 ~ 1. Hence, regularization is applied in the same manner as for similar regression based loss functions. The tree boosting algorithm applies the trees additively i.e. sums the weights for all trees to arrive at the final value for the given input. This continuous valued "score" needs to be interpreted by you as a class label by applying a cutoff, e.g. predicted class = 1 if $\hat y > 0.5 $, if cutoff = 0.5.
