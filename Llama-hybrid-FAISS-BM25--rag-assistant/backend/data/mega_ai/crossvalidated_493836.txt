[site]: crossvalidated
[post_id]: 493836
[parent_id]: 
[tags]: 
Uplift modeling for Train, validation, test data sets

I am wondering when I should tune hyper parameter when we build uplift model. In a normal machine learning context, data will be split into train, validation, test. And, we train the model with train data and decide hyperparameter with validation data. And we finally can evaluate the model with test data. Estimation method, such as two model methods, build each prediction model and evalute with AUUC or Qini. For example, one random forest for the treatment group and another for the control group. How should I decide hyperparameter for each model? 1. when we build each model? it is tuned for estimating classification/continuous prediction.but it does not maximize the uplift result such as AUUC. 2. when we build the uplift model and calculate AUUC? it is tuned for maximizing uplift results such as AUUC. but it is not tuned for estimating classification/continuous prediction. If you can provide any source of research paper, I would be really appreciated.
