[site]: datascience
[post_id]: 42737
[parent_id]: 42715
[tags]: 
You may use the technique explained in the article "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning" Very briefely the technique consists of applying dropout for training and predictions . In Keras this can be easily applied passing the training argument in the call of the Dropout layer. import keras input = ... x = keras.layer.Dense(100)(input) dp = keras.layer.Dropout(0.5)(x, training=True) output = keras.layer.Activation('relu') model = keral.Model(input, output) During prediction you get the mean and standard deviations: T = 1000 # Do 1000 predictions to estimate uncertainty predictions = np.array([model.predict(X_test)] for _ in range(T)]) pred_mean = results.mean(axis=0) pre_std = results.std(axis=0) You can increase (or descrease) T if you need more (or less) precision.
