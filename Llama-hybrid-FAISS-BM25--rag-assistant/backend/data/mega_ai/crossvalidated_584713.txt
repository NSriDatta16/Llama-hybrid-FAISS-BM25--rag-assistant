[site]: crossvalidated
[post_id]: 584713
[parent_id]: 
[tags]: 
If $p \sim \Pi$, then $p$ is a random variable and $\Pi$ is a distribution but then how is $X_1,\ldots,X_n \,|\, p \stackrel{i.i.d.}{\sim} p$?

I am trying to understand the following from https://www.dianacai.com/blog/2021/02/14/schwartz-theorem-posterior-consistency/ We consider a model class given by a space of densities $\mathcal{P}$ with respect to a $\sigma$ -finite measure $\mu$ , and we denote the distribution of a density $p \in \mathcal{P}$ as $P$ , i.e., $p = \frac{dP}{d\mu}$ . Denote the joint distribution of $n \in \mathbb{N} \cup \{\infty\} $ samples by $P^{(n)}$ . Let $\Pi$ be a prior distribution on our space of models $\mathcal{P}$ , consider the following Bayesian model: \begin{align} p \sim \Pi \\ X_1,\ldots,X_n \,|\, p \stackrel{i.i.d.}{\sim} p, \end{align} If $p \sim \Pi$ , then $p$ is a random variable and $\Pi$ is a distribution but then how is $X_1,\ldots,X_n \,|\, p \stackrel{i.i.d.}{\sim} p$ ? I mean is it possible to write $x\sim y,y\sim z$ . How does this make sense? Also, is $P$ the cumulative distribution here?
