[site]: crossvalidated
[post_id]: 347534
[parent_id]: 347530
[tags]: 
It has quite a nice intuition in the Bayesian framework. Consider that the regularized cost function $J$ has a similar role as the probability of a parameter configuration $\theta$ given the observations $X, y$. Applying the Bayes theorem, we get: $$P(\theta|X,y) = \frac{P(X,y|\theta)P(\theta)}{P(X,y)}.$$ Taking the log of the expression gives us: $$\log P(\theta|X,y) = \log P(X,y|\theta) + \log P(\theta) - \log P(X,y).$$ Now, let's say $J(\theta)$ is the negative 1 log-posterior, $-\log P(\theta|X,y)$. Since the last term does not depend on $\theta$, we can omit it without changing the minimum. You are left with two terms: 1) the likelihood term $\log P(X,y|\theta)$ depending on $X$ and $y$, and 2) the prior term $ \log P(\theta)$ depending on $\theta$ only. These two terms correspond exactly to the data term and the regularization term in your formula. You can go even further and show that the loss function which you posted corresponds exactly to the following model: $$P(X,y|\theta) = \mathcal{N}(y|\theta X, \sigma_1^2),$$ $$P(\theta) = \mathcal{N}(\theta | 0, \sigma_2^2),$$ where parameters $\theta$ come from a zero-mean Gaussian distribution and the observations $y$ have zero-mean Gaussian noise. For more details see this answer . 1 Negative since you want to maximize the probability but minimize the cost.
