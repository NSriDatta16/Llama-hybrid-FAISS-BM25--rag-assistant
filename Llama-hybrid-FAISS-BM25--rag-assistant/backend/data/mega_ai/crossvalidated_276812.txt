[site]: crossvalidated
[post_id]: 276812
[parent_id]: 
[tags]: 
Evaluation of a model on Deep Neural Networks

Suppose that we train some Deep Neural Network and during the training (forward-backward passes) phase, we use Leaky ReLU as our activation function. During the evaluation, when we show the network the test data and basically do a forward pass, do we need again to have exactly the same activations functions as in training phase? Would it make any sense to use ReLU instead of leaky ReLU which used during the training?
