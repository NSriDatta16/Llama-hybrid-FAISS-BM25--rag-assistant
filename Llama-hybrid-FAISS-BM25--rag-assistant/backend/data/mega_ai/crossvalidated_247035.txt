[site]: crossvalidated
[post_id]: 247035
[parent_id]: 246873
[tags]: 
To see the difference in interpretations we will do some simulations in R. But first: interactions in linear models are usually interpreted via an additive model, so no interactions means a purely additive model. But, logistic regression is a multiplicative model, so interactions modeled via product terms do have a different meaning there. We set up a model to show this via simulations. First, we have two binary predictors coded as 0/1, and define probabilities via an additive probability model $p=\beta_0 +\beta_1 x_1 + \beta_2 x_2$. So, on this additive scale there is no interaction: set.seed(1234) beta_0 We use a big sample size so randomness is unimportant: x_1 Then we can estimate a logistic regression model with interaction: mod |z|) (Intercept) -2.17084 0.06596 -32.909 Note the interaction term! Can we recover the probabilities $p$ from this model output? newdata Yes, we can. Then, what happens if we fit a logistic model without interactions? mod.wi |z|) (Intercept) -2.01884 0.04838 -41.73 Chi) 1 9997 11030 2 9996 11016 1 13.611 0.0002248 *** --- Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 pred.wi Note how the estimated parameters changes a lot, while the fitted probabilities do not change that much (about 0.02). But anyhow, for most people the fitted probabilities are easier to understand than the fitted log-oddsratios, so this teqnique of showing fitted probabilities is preferred for logistic regression, and should be more used. (with probabilities closer to zero or one the difference would be more pronounced. Try that yourself by editing the code above).
