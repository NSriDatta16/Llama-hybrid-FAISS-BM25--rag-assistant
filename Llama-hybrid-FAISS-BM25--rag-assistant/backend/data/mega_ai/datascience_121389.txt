[site]: datascience
[post_id]: 121389
[parent_id]: 
[tags]: 
How to do batch inference in pytorch on a local GPU?

I'm working on a version of SAM model which predicts the segmentation map using text prompts provided by the user. As of now, running a single image takes about 5 seconds. I want to run about 100 images in a folder. This is taking too much time on my GPU. The GPU is RTX 3070 with 16GB RAM. Is it possible to batch multiple images and run them as one so that the processes are run on multiple cores of the GPU resulting in better performance? Please do not give ChatGPT answers. Thanks.
