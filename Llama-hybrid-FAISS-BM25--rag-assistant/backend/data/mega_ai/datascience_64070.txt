[site]: datascience
[post_id]: 64070
[parent_id]: 
[tags]: 
Why would batch normalization allows us to use higher learning rate in the neural network?

I am doing some study about the BatchNormalization: https://towardsdatascience.com/batch-normalization-8a2e585775c9 In the article, it says: Using batch normalization allows us to use much higher learning rates, which further increases the speed at which networks train. Could anyone please share their thoughts on why would batch normalization allow higher learning rate? Thanks!
