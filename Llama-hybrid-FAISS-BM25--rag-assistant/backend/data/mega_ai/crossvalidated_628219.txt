[site]: crossvalidated
[post_id]: 628219
[parent_id]: 628216
[tags]: 
The log likelihood is a relative information or error measure that doesn’t tell you by itself whether a model fits adequately. And you’re right about non-likelihood fixes such as sandwich covariate matrix estimators that don’t fix any problems with the point estimates. There are robustifications of the log-likelihood to make it more resistant to lack of fit but I’ve not used that. To my mind a better philosophy is to formulate a model that is as flexible as the sample size allows, e.g., by adding parameters for non-normality or non-equal variance as in the Bayesian t-test. You put priors on those extra parameters so that the model has more effective parameters as the sample size increases, when you can afford to be more flexible.
