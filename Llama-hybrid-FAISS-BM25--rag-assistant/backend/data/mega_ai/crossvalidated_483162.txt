[site]: crossvalidated
[post_id]: 483162
[parent_id]: 
[tags]: 
In PCA, how to deal with features being a convenience sample from an infinite universe of possible features?

Principal Components Analysis (PCA) of the covariance matrix is often used to provide a reasonable approximation to the covariance matrix of some data by retaining the leading $k$ principal components and discarding the rest. Because the principal components are ordered by decreasing variance this means that the the leading $k$ components must account for more data variance than any other subset of components. It is commonly pointed out that this depends on the scaling of the variables. For example if you multiplied some variable by a million that would pretty much guarantee the first principal component corresponded to only that variable. Absent some belief that all the variables are appropriately scaled, it is common to standardise all variables before PCA. This is equivalent to analysing the correlation matrix rather than the covariance matrix. It is equivalent to claiming that all the variables are equally important with respect to variance. There is a related issue that I can’t recall ever having seen discussed, and I would appreciate pointers to any prior discussion of it. It seems to me that the “use the $k$ leading PCA components” heuristic implicitly assumes that the analysed set of variables is THE correct set of variables to use. Obviously, nobody would knowingly include variables they think are irrelevant. However, there is a less obvious case that is analogous to the issue of the scaling of the variables. In applied data science it is standard to construct large numbers of features (derived variables) from the base measurements. In principle there are infinitely many features that could be derived and many of them will be correlated because they are parametric variations of a function applied to the same base variable. PCA would presumably have these closely related features loading highly on the same component. The number of closely related features (parametric variants of the same function) then plays a role analogous to the scaling of the variables. If there are many closely related variants, that increases the variance of the corresponding component, and is equivalent to having increased the scaling factor of a single variable. The set of features used in some applied data science analysis is generally a convenience sample from the infinite universe of possible features. (They are the ones the analyst thought of. It will also be a systematically biased sample of features in that it will be biased to features that are easy to compute and that are parametric variations on a small number of themes.) Given that the features may be parametric variations on a theme the number of closely related features may vary widely (by orders of magnitude). This is analogous to applying a random scaling to the variables prior to PCA. That being so, the fact that some PCA component has a high variance might be due to the (essentially arbitrary) fact that a large number of closely related features were generated from the same base variable. This will render useless the heuristic to approximate the (theoretically interesting) data with the $k$ leading components because the identity of the top $k$ components depends on the arbitrary choice of the convenience sample of variables. I would greatly appreciate any pointers to dealing with this issue. TIA
