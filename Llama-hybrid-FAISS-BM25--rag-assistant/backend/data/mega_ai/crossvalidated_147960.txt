[site]: crossvalidated
[post_id]: 147960
[parent_id]: 
[tags]: 
Measuring dispersion of data points

I have a number of data points (which can be small) with measurement error bars. I assume the intrinsic values are drawn from some sort of normal distribution. What is the most robust method for calculating the width of this distribution (i.e. the dispersion) and its uncertainty, removing the additional spread from the error bars? Including the additional dispersion in a $\chi^2$ minimisation (or MCMC) doesn't appear to work as the dispersion just goes to infinity to reduce the $\chi^2$. Perhaps there is some approach where I could examine the cumulative probability for a $\chi^2$ distribution against the measured $\chi^2$, increasing the error bars, but I'm not convinced this is valid.
