[site]: datascience
[post_id]: 93638
[parent_id]: 
[tags]: 
Is positional encoding (in transformers) an estimation of the relative positions of words in the training corpus texts?

Is this some kind of estimation of the relative positions of words in the training texts? are they creating some kind of statistical "distribution" of words? is "cat" usually 2 or 3 words away from "milk" in English language? things have to have a meaning, havent they? Is BERT just adding some aditional dimensions to the vector space to include info on the relative positions of words?
