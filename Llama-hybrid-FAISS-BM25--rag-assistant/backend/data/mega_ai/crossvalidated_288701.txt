[site]: crossvalidated
[post_id]: 288701
[parent_id]: 288699
[tags]: 
In a cross-sectional data set (no time series or panel data), the OOB estimate of true performance of a random forest is usually very accurate and in my opinion can even replace (cross-)validation. Put differently, you can trust the OOB accuracy in such cases. This is in constrast to the insample (training set) accuracy: By construction, random forests tend to extremely overfit on the training data because the individual trees are usually very deep and unstable. So don't get lured by an insample accuracy/R-squared of 97%. One warning: According to your description, you have used the test set to optimize hyperparameters of the model. This is inappropriate. The role of a test set is to get an impression of the performance of the final model. So you basically use it just once. There is no point of using the hold-out test sample in model optimization. It is very easy to overfit on the test data! That would be the role of a separate validation set, cross-validation or the OOB info.
