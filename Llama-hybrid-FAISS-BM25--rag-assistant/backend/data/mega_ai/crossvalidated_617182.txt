[site]: crossvalidated
[post_id]: 617182
[parent_id]: 333697
[tags]: 
No, they are not well-calibrated. The predicted probabilities are likely not outright horrible as we would expect from an SVM classifier but they are not usually very well-calibrated. For that matter the estimated probability deciles are not even guaranteed to be monotonic. In Caruana et al. (2004) " Ensemble Selection from Libraries of Models " boosted trees have some of the "worst" calibration performance scores. Similarly in Niculescu-Mizil & Caruana (2005) " Predicting good probabilities with supervised learning " , boosted trees have " the predicted values massed in the center of the histograms, causing a sigmoidal shape in the reliability plots ". An important caveat is that these results are most likely referring to AdaBoost behaviour and are not validated again XGBoost (which is published about a decade later) (thank you to @seanv507 for raising this point). That said, empirically newer GBM implementations (XGBoost, LightGBM, etc.) are still prone to a "sigmoidal shape" in their results as model training rewards "over-confident" predictions. (and that's why there is interest in well-calibrated probabilities still) Finally, it's worth pointing out that these findings don't even touch upon the scenarios of up-sampling, down-sampling or re-weighting our data; in those cases, it is very unlikely that our predicted probabilities have a direct interpretation at all. Do note that "badly" calibrated probabilities are not synonymous with a useless model but I would urge one doing an extra calibration step (i.e. Platt scaling , isotonic regression or beta calibration ) if using the raw probabilities is of importance. Similarly, looking at Guo et al. (2017) " On Calibration of Modern Neural Networks " can be helpful as it provides a range of different metrics (Expected Calibration Error (ECE), Maximum Calibration Error (MCE), etc.) that can be used to quantify calibration discrepancies. This paper also touches upon the issue of "over-confidence" too (and also propose their "own" calibration step, temperature scaling ).
