[site]: crossvalidated
[post_id]: 295560
[parent_id]: 
[tags]: 
Stochastic gradient descent for neural networks with tied weights

For the neural network depicted below, I want to calculate the Error with respect to $w_{tied}$, which we get if we tie the weights $w_1$ and $w_4$ together. Tying the weights together would help to reduce overfitting, since we reduce the number of parameters. The hidden units $h_1,h_2$ are logistic, the output neuron f is a linear unit, and we are using the squared error cost function $E = (y âˆ’ f)^2$. I know that the solutions is (1) $\frac{\partial E}{\partial w_{tied}} = \frac{\partial E}{\partial f}(\frac{\partial f}{\partial h_1}\frac{\partial h_1}{\partial w_{1}}+\frac{\partial f}{\partial h_2}\frac{\partial h_2}{\partial w_{4}})$ From that it follows that $\frac{\partial E}{\partial w_{tied}} = -2(y-f)(u_1*h_1*(1-h_1)*(-x_1)+ u_2*h_2*(1-h_2)*(-x_2))$ Question: How do I write the stochastic gradient descent algorithm for $w_{tied}$ now? In stochastic gradient descent, we randomly pick one datasample to optimize - Here, do we randomly pick a weight we want to optimize? So, we aren't we optimizing for the entire weight vector $\vec{w}$? But for random i we optimize $w_i$, by feeding all possible datasamples x into the gradient? Also, I realize that in our solutions, it is "$x_1$" or "$x_2$" in the gradient above, and not "$-x_1$" or "$-x_2$" - why is that? Thanks
