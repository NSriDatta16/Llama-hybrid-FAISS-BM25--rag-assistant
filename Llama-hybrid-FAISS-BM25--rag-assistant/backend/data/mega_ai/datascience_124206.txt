[site]: datascience
[post_id]: 124206
[parent_id]: 
[tags]: 
Minimize MAE loss for a target that is sum of two other targets

Working on a regression modelling task where my dataset have some feature columns, two more columns A, B and a target column T. The goal is to predict T, and minimize MAE, that is mean(|T_actual - T_predict)| . My question is about how to reformulate my modelling/learning task. I have a target variable T, that is not intuitive and hard to model. However, I know that T = A + B , where both A and B and intuitive and easier to model (I know how to create meaningful features for them). Thus instead of modelling T, I want to create 2 models - one for A and one for B, and in the end sum them up hoping to minimize mean(|T_actual - T_predict|) = mean(|A_actual + B_actual - A_predict - B_predict|) The simplest thing I could do was: when training a model for A: I chose again the MAE objective and minimized mean(|A_actual - A_predict|) when training a model for B: I chose again the MAE objective and minimized mean(|B_actual - B_predict|) Finally, my predictions for target T are T_predict = A_predict + B_predict . However, I get very bad predictions and MAE for target is very large. This is probably because MAE is not additive and mean(|A_actual+B_actual - A_predict - B_predict|) != mean(|A_actual - A_predict|) + mean(|B_actual - B_predict|) . My question, is what objectives can I use when training models for A and B, so that MAE for target T is optimized? Not sure if it matters, but the model I use is a gradient boosting model (gbm) xgboost.XGBRegressor and it has built in MAE objective. I was thinking if somehow I could train the two gbms for A and B in parallel and update the loss function mean(|A_actual+B_actual - A_predict - B_predict|) but am not sure how would that even be implemented.
