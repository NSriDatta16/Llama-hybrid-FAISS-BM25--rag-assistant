[site]: crossvalidated
[post_id]: 581516
[parent_id]: 
[tags]: 
How can it come to have a negative r-squared value and a relative absolute error (RAE) below 1?

i am training for my masterthesis a feed forward neural network regression model to predict the annual sales of accounts. The training data is highly skewed, which means that there are a lot of low annual sales accounts compared to only a few high annual sales accounts. To remove the skewness of data i log transformed the data and as a comparison model i did not transform the data. Before the data was inputed into the model, all data was minMax Normalized to a number space from 0 to 1. I did a 10 folded Cross validation over the data. To select the best model for each fold i used the Mean Squared error. The result of the predicted data compared to the actual data produced some error rates and coefficients, i can't get behind. Based on the actual and predicted data, i calculated the r-squared (R2) value and the Relative Absolute Error (RAE). Some models have produced a negative R2 value and a positive RAE, which i do not understand. To my understanding, the R2 value is normally between 0 and 1. Only when the model predicts worse than a model always guessing the mean of data, it can become negative. The relative absolute error, is in the range from 0 to infinity. The value 0 indicates a model which predicts all values correctly, a value of 1 indicates a model which is as good as a naive model (always predicting the mean) and a value above 1 indicates a model worse than the naive model. How can it come that a model is, based on the r2 value, worse than a mean predicting model and based on the RAE a better model than this naive model? Thank you for your help
