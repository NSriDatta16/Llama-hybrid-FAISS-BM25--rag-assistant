[site]: crossvalidated
[post_id]: 309790
[parent_id]: 309632
[tags]: 
While I am not sure how much sampling theory is actually accessible to you (based on your language, you are a data scientists/machine learner rather than a statistician... no offence, it's just your expertise and background is very different), the question has obviously been addressed in the literature, although with a focus on estimation rather than prediction. Late Alastair Scott (who was the founding Stat department chair at U of Auckland at the time R was first written there in early 1990s) was the ultimate expert on the topic, see JRSS C and Survey Methodology summary papers. His work concerned the following issue: in epidemiological population-based studies of rare diseases (say with population incidence of less than 1%), you may have to take all cases ( outcome==1 ), because there are so few of them, but sample controls ( outcome==0 ) at some low rate, because you cannot study all of them -- it's just too expensive. So when you are running a logistic regression trying to identify the factors contributing to the incidence of the outcome , how concerned should you be about your sampling scheme? You have two options: you can ignore the issue completely; or you can weight the cases with inverse probabilities of selection, as is done for most of survey samples. The latter, while apparently technically correct, blows up the variances of the estimates. The answer that Scott arrived at is quite simple: it does not matter for slopes, which are still consistently estimated if you ignore the sampling design, but the intercept is obviously biased (by a factor of log ratio of the sampling rates between cases and controls, or something like that). So when you run a logistic regression and score the probability of whether a new observation is likely to have the disease, you can use unweighted logistic regression pretending you have 1000 cases and 1000 controls rather than 1000 cases and 100K controls, but you would need to scale your probability way down so that it produces an average across the population of 1% rather than 50%. I don't know if this result generalizes well to other classification methods, say SVMs or random forests. If the above result is any indication, you'd be responsible for undoing the attempts to rebalance the classes somewhere in your machinery.
