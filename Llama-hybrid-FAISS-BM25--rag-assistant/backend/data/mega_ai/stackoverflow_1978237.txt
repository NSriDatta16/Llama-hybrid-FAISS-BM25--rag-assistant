[site]: stackoverflow
[post_id]: 1978237
[parent_id]: 1978035
[tags]: 
you should pull your items in a batch so you don't clog the network with requests. grabbing a list of IDs, then looping them and pulling the full item each time is N extra database calls. you can use a webservice to handle calling the database, if you think you will benefit from the abstraction. otherwise you'll just create unnecessary complexity. update the databse as you finish each item. the finished items can be used further down the line as soon as they are ready, instead of having to wait for batches of 5000 to finish. this assumes you will be saving data for each item you need to make N calls (to save each item) no matter what, so you don't gain much by waiting and then updating at the end of each batch. if it crashes, you'll lose all the unsaved data. if you don't need to store per-item results from the black box then you'd have a good reason to consider updating everything as a batch. I've written a bunch of apps like this for a bank. My usual approach is as follows-- It's simple, fault-tolerant, and efficient. (assuming you need to process sets of items and save data for each one) the database has a table representing the status of processing an item, in addition to the items table. for a little extra work upfront, this will make debugging and auditing the process a breeze: table ItemsProcessStatus -- feel free to improve upon the name int orderID (auto increment) int itemID (fk to items) datetime pulledForProcessing null datetime finishedProcessing null ..etc windows service runs on a timer, say once every X minutes and pulls limit(Y) items to process. this marks the pulledForProcessing flag with a timestamp in the ItemsProcessStatus table. You want to pull items where the pulled date is null [and also those that have been pulled, but not completed, and are older than Z minutes (I usually pick 15 to 30 minutes)] Be careful with the procedure that pulls these. you need to use locks You can refine this further: On the first iteration, grab Y items, where Y is a decent guess at how much you can process in that time span. The next iteration, you calculate the rate that it is processesing (as a sliding average) and adjust the the number of items to pull. this way it will continuously adjust itself to process at full capacity. the windows service processes these one by one (well, usually it's multithreaded, so many at once) by sending them to the black box. I put them in a threadsafe Queue<> (not to be confused with msmq). Worker threads loop, pulling from the queue, processing the item in the black box, and then updating the database. you can use any of the typical multithreading techniques here (wait/pulse, reader/writer lock slim, wait handles), or just have the worker thread sleep for a few seconds if the queue is empty after each item finishes, call the updates proc for that item, which also updates the ItemsProcessStatus table (signifying that it has finished processing) When your service is stopped, finish processing any items that are being processed and update them in the db. For all the items that haven't been sent to the black box, you unmark them in the process table by setting pulledForProcessing to null. if your service crashes, you don't 'lose' a lot of data. items that didn't get unmarked will get pulled again when they are over a certain age (process table) This works with multiple instances of the windows service installed on an array of servers (though you'll want to add ComputerName to the process table to identify which computer each service is running on). this works because each service just grabs the 'next set of items' to process--there's no need for any kind of routing or for the processes to communicate with each other.
