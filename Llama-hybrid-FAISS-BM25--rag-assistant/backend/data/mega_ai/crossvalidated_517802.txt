[site]: crossvalidated
[post_id]: 517802
[parent_id]: 
[tags]: 
How pre-trained weights in the BERT can help the fine tuning task?

I have been using the BERT architecture implemented by the Huggingface library for my sentence classification task. Although, I read the paper (and related papers) and the result of my experiments is promising, However, I can not truly understand how the weights in the pre-trained model are actually helpful for the fine-tuned model. I understand the intuition behind the pre-training and fine-tuning. My sentences are divided into tokens and the Masked Language Modeling (MLM) method tries to predict the masked tokens. The model learns some weights in the pre-training. What do these weights learn? I know the general idea is that the model learns a representation of the language. What are these representations? Are the weights learned for each token? Suppose, the fine-tuning task is about sentence classification. To classify a sentence (using the same tokenizer, we will have the same tokens) how the pre-trained weights are helping?
