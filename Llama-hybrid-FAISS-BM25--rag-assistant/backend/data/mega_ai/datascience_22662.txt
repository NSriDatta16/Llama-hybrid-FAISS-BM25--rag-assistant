[site]: datascience
[post_id]: 22662
[parent_id]: 
[tags]: 
Handling Larger number of category label in text classification

In the current problem I am working on I am running text classification against ~16 possible variables. If we assume there's equal number of every label our baseline would be 1/16 = 6.25% . Using scikit learn, TF-IDF, ngrams and running text normalization the best results I am yielding so far are (without running gridsearch on the parameters): 52% for SGDClassifier 49% for RandomForestClassifier 49% for DecisionTreeClassifier 38% for MultinomialNaiveBayes Not the best results for the final product but already a huge improvement on the baseline. What are better approaches to improving the results? One idea is to add a label and a sub-label, break the 16 into 4 groups and then run 2 models, one against the 4 major groups, and a second against the 4 minor groups. Other approaches can include SVMClassifier, KNN, OnevsRestClassifier. What is better approach to dealing with an array of labels for text classification to try and improve upon a 52% accuracy?
