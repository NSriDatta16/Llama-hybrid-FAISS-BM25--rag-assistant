[site]: crossvalidated
[post_id]: 202366
[parent_id]: 201907
[tags]: 
First of all, if you want to compare the training algorithms as opposed to the predictive ability of the models you got for the particular dataset you used (i.e. you want to extrapolate the training algorithms' performance to unknown training data of the same kind), you need to be aware that there are at least 2 different sources of uncertainty in the values you measure: Uncertainty on measuring a given surrogate model's performance: the variance here is due to the finite number of test cases you evaluate for each of the surrogate models The actual models you train are subject to variation as well: model instability wrt. to the actual training data. For cross validation (resampling validation), you can further divide this into one part that you can measure (e.g. via repeated/iterated cross validation) and a part you cannot measure because your surrogate models share most of their training data. See Bengio, Y. and Grandvalet, Y.: No Unbiased Estimator of the Variance of K-Fold Cross-Validation Journal of Machine Learning Research, 2004, 5, 1089-1105 for this. This latter part is of course important if you want to compare the training algorithms in general, but it is of no concern if you want to compare them only for the task at hand, i.e. the data set you actually have. For regression, you may get away with pooling the two variances you can actually measure into a single pooled variance. However, you should be aware (and state) that for comparison of the algorithms, you can only measure part of the variance uncertainty this way and that this does limit the conclusions you can draw. I'd probably conclude that this is such a big uncertainty that testing (as in calculate some kind of number that quantitates the conclusion) doesn't make sense and rather report on observed differences. That being said, I'd go for a paired evaluation as you can test all splits with each algorithm.
