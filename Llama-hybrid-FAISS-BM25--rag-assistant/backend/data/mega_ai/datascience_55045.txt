[site]: datascience
[post_id]: 55045
[parent_id]: 55023
[tags]: 
To the best of my knowledge, if the performance is higher on the training set than on the validation set, you should probably be worrying about over-fitting. I am assuming that your validation set, for the moment, is just a fraction of the data of the training set. Notice that if the autoencoder is not performing well on your validation set, it is unlikely that you will reach good performance on new data "since new data will be very similar to current data", as already on current data you don't have good performance. If you have to retrain the autoencoder every time you want to encode something, it may be not worth using the autoencoder in the first place.
