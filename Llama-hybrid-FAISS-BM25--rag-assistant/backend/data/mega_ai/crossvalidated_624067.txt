[site]: crossvalidated
[post_id]: 624067
[parent_id]: 
[tags]: 
Unusual results from XGBoost learning curves

I'm working on training an XGBoost classification model on time series data. Currently, I have a lot of data and it is hard to fit it all in memory, so I am trying to better understand if more data leads to better performance for my model through plotting learning curves. However, i'm getting some very unusual results that I don't know how to interpret. For generating data for the learning curves, I'm using 5-fold walk-forward validation where, for each fold, the test set is comprised of data that happened after time period i and the training set is comprised of data that happened prior to time period i. Given that I am interested in the probabilities predicted by the model, I'm using log loss as my scoring metric - this is the loss that I am plotting to see if it decreases as samples increase. The results that I'm seeing show training set log loss increasing as the number of samples increases and test set log loss barely decreasing as samples increase. Is this indicative of underfitting or is my model just not learning anything at all? How can I better diagnose what is causing this and potentially fix it? Is this perhaps happening due to the time series nature of my data? FWIW I know that tree based models don't produce probabilities which are directly representative of probability of X event occurring, but I am waiting to perform probability calibration until after I have decided how many samples I need.
