[site]: crossvalidated
[post_id]: 432753
[parent_id]: 308424
[tags]: 
Yes, batch size affects Adam optimizer . Common batch sizes 16, 32, and 64 can be used. Results show that there is a sweet spot for batch size, where a model performs best. For example, on MNIST data, three different batch sizes gave different accuracy as shown in the table below: |Batch Size | Test Accuracy | -----------------------------------| |1024 | 96% with 30 epochs | |64 | 98% with 30 epochs | |2 | 99% with 30 epochs | |__________________________________| Therefore, it can be concluded that decreasing batch size increases test accuracy. However, do not generalize these findings, as it depends on the complexity of on hand data. Here is a detailed blog ( Effect of batch size on training dynamics ) that discusses impact of batch size. In addition, following research paper throw detailed overview and analysis how batch size impacts model accuracy (generalization). Smith, Samuel L., et al. "Don't decay the learning rate, increase the batch size." arXiv preprint arXiv:1711.00489 (2017). Hoffer, Elad, Itay Hubara, and Daniel Soudry. "Train longer, generalize better: closing the generalization gap in large batch training of neural networks." Advances in Neural Information Processing Systems. 2017.
