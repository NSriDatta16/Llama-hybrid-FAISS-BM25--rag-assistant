[site]: datascience
[post_id]: 69332
[parent_id]: 
[tags]: 
Would averaging two vectors in word embeddings make sense?

I'm currently using the GloVe embedding matrix which is pre-trained on a large corpus. For my purpose it works fine, however, there are a few words which it does not know (for example, the word 'eSignature'). This spoils my results a bit. I do not have the time or data to retrain on a different (more domain-specific) corpus, so I wondered if I could add vectors based on existing vectors. By E(word) I denote the embedding of a word. Would the following work? E(eSignature) = 1/2 * ( E(electronic) + E(signature) ) If not, what are other ideas that I could use to add just a few words in a word embedding?
