[site]: crossvalidated
[post_id]: 366131
[parent_id]: 366096
[tags]: 
False: If there are no hidden layers, then your neural network will only be able to approximate linear functions, not any continuous function. In fact, you need at least one hidden layer for a solution to the simple xor problem (see this post and this one ). When you only have an input and an output layer, and no hidden layer, the output layer is just a linear function of its weights since the activation function only acts on the inner product of the input with the weights, hence you can only produce linearly separable solutions. N.B. It does not matter what your activation function are, the point is that no neural net with no hidden layer can solve the xor problem, since its solutions are non-linearly separable.
