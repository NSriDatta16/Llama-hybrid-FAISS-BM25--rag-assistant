[site]: crossvalidated
[post_id]: 456172
[parent_id]: 456167
[tags]: 
You don't accumulate those losses unless you are reporting training loss, which is usually not part of the training where mini-batching matters. The mini-batch is assumed to approximate the full-batch loss function and we update the weight and bias under that assumption, in the hope that full batch also approximates the average loss for the population. In the extreme case, there is online learning, where only one training sample is thrown into the neural net and the weights are updated according to that sample only. When data is extremely abundant, we sometimes don't even use a single sample twice, so again no aggregation of losses.
