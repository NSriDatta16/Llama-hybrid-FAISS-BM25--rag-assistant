[site]: datascience
[post_id]: 17674
[parent_id]: 17673
[tags]: 
The "low hanging fruit" approach is "bag of words". Lemmatize the sentence and use the lemma frequencies as features. Build two models: Sentiment (or reuse StanfordCoreNLP ) Topic (for this you need a training sample - a set of sentences labeled with the topic). Now you can map every sentence to topic and sentiment and this will tell you whether the customer liked or disliked (sentiment) the softness (topic). Multi-topic sentences Usually a sentence has a single topic, but, if you do require multiple topics per sentence, you can use predict_proba or similar and use all classes with probability over, say, 0.3. Class proliferation The above approach fails on sentences like I liked the softness but hated the design. because the total (average) sentiment of the sentence is probably neutral and we miss out softness-good and design-bad . It is thus tempting to triple the number of classes ( topic-sentiment ) to accommodate reviews like The table combines pleasant design, flimsy build and neutral color. The problem is that building models for very many classes requires a lot of training data. IOW, you will need to manually annotate a lot of reviews - and even then the model quality if unlikely to meet your expectations. Real NLP If you really need to handle multi-topic sentences, you might want to consider a "deeper" approach. Specifically, take a look at the parse tree of the sentence. split it into constituent phrases use the topic model to identify the unique topic of the each phrase use the sentiment model on the phrase
