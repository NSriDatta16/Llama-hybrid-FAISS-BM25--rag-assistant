[site]: crossvalidated
[post_id]: 560109
[parent_id]: 
[tags]: 
Bootstrapped regression model for quasi-posterior distributions?

I'm very much of the Bayesian mind and do enjoy the ability to literally integrate my prior beliefs about a system into my parameter estimation. However, the aspect I enjoy most about Bayesian analysis is the output as a distribution, not a point estimate. It crossed my mind that there might be mechanism to create distributions over P(Y|X) with linear regression models given a bagging strategy. I'm sure most folks are familiar, but for those who are not, decision trees can be "bagged" together by training trees on different bootstrap samples of the training data. The concept I'm proposing is similar in that say 100 linear regression models receive MLE fits on bootstrapped samples of the same training data; then at inference time, each model makes its own prediction. Following, the outputs are aggregated, returning the mean and variance around the P(Y|X=x). The idea of a distribution, rather than a point estimate of P(Y|X=x) can't be accomplished by a typical MLE fit of a linear regression model because the final output contains confidence intervals over both the slope and intercept, however, this analysis does not give us the joint distribution over slope and intercept values. Bayesian posterior inference directly samples from the joint distribution of all parameter values and so inferring a distribution over P(Y|X=x) is possible. I believe that this bagging MLE fit strategy I'm proposing will enable to Frequentist tools to be used to estimate the distribution over P(Y|X=x). However, when I've searched these ideas before on Google, Stack Exchange, etc. I haven't found anything. Any thoughts on this strategy? Edit: I believe Bayesian posteriors would be biased by the specific priors provided; the LR MLE bagging strategy would be biased by the hyperparameters supplied, namely the bootstrap sample size. So no getting around bias...
