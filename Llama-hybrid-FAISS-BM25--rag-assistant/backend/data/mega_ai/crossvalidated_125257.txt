[site]: crossvalidated
[post_id]: 125257
[parent_id]: 
[tags]: 
The linearity of perceptrons

I am having quite a dilemma whether multi-layer perceptrons are linear in nature or not. In this wikipedia article , it is said that: If a multilayer perceptron has a linear activation function in all neurons, that is, a linear function that maps the weighted inputs to the output of each neuron, then it is easily proved with linear algebra that any number of layers can be reduced to the standard two-layer input-output model However, from different sources we can get the information that XOR function can be implemented successfully using multi-layer perceptrons ( one of them , page 6) If the cited paragraph in wikipedia is correct, multi-layer perceptron should not be able to successfully classify XOR, yet it does. Why is that?
