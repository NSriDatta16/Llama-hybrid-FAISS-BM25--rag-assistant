[site]: crossvalidated
[post_id]: 526371
[parent_id]: 526361
[tags]: 
I might understand your question and I'll keep it very hand-wavey. You are correct for how random forests predict but for gradient boosting (although they have similarities) it is an iterative ensemble which means that we do have several models, however, each model is essentially just updating the previous model's predictions so it is nothing like the random forest in that respect. A MLP is not like the others in that the nodes are working together concurrently to combine your inputs for the prediction. So: Random Forest : Ensemble where each tree is a separate model predicting for the same thing. The bootstrapping and variable subset can be applied to basically any other model. Gradient Boosted Tree : Ensemble where each tree is a separate model which is dependent on the last tree and is trying to adjust for the last tree's error. The boosting algorithm which takes each round's residuals and trains the next model on these 'psuedo' residuals can be applied to basically any other model. MLP : A single model where the nodes are working together to create the prediction. Unlike the other two this is not an ensemble method, i.e. we can't typically can't take a single node of a trained mlp and do much with it. To your other point, the boosting and mlp methods follow the same routine for classification or regression we just do transformations to allow for classification. Unlike random forests which do voting for classification and then use the mean for prediction.
