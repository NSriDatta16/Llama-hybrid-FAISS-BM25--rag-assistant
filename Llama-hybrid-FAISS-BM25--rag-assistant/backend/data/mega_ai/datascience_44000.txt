[site]: datascience
[post_id]: 44000
[parent_id]: 43933
[tags]: 
This tutorial assumes the text will be represented as a fixed length feature vector. There are several ways to do that. The good old one is using n-grams, indexing and several tricks around that. First, N-grams can be seen as kind of words (or rather, phrases). The extreme version of it is unigram, or 1-gram, which is just a single word. Bi-grams, tri-grams and so on encode certain word ordering, of course, but the ordering of the n-grams in the text is usually ignored. Let's assume you use unigrams and defined the vocabulary: Index assigned for every token: {'the': 0, 'mouse': 1, 'ran': 2, 'up': 3, 'clock': 4 } By using the count encoding, we come up with the following vector representing the text: 'The mouse ran up the clock' = [2, 1, 1, 1, 1] But if you simply shuffle the words, you will end up with the same vector. 2-grams, 3-grams and so on capture the word ordering better and it will be more difficult to get similar vectors for different texts. So, this is not completely true that they cannot leverage any information about text ordering. However, just by swapping sentences in a text, you can get very close representations. Especially, if some rare 3-grams are dropped from vocabulary (which is widely used to keep the vocabulary of reasonable size). Recurrent neural networks and word embeddings are more modern approaches in NLP. In case of RNNs, words (in form of embeddings) are fed to the model sequentially (not independently like in case of n-grams), so the model is much more sensitive to the word ordering.
