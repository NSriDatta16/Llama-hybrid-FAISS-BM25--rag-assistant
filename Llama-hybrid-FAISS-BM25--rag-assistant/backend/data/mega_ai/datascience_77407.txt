[site]: datascience
[post_id]: 77407
[parent_id]: 77327
[tags]: 
For an ROC curve, you need the model to produce an ordering of the test points (ordered by likelihood of being in the positive class). A few approaches: A hard voting classifier usually just outputs the final class prediction. This is an ordering, just with nearly all ties. That will produce an ROC curve of two straight line segments, from the square's corners to a single point representing the model's specificity and sensitivity. Not the most informative, but there it is. While you probably are using a majority vote to produce the final class prediction, you can also interpret it slightly differently: produce a probability score by averaging the votes (as binary) from the base models. The final class prediction is the same as thresholding this score at 0.5. So now you have at least fewer ties, and you can produce an ROC curve that looks a little smoother. This will work best if you have lots of base models, and in fact is pretty much exactly what you do when building random forests. If your base models come with probability scores, then for each threshold you can get the classifications from all of the base models, and have those vote to produce classes. Varying that threshold then produces an ROC curve that's just as smooth as any of the base models' ROC curves. The catch is whether one threshold represents the same thing for all the base models: you need all the base models to be well-calibrated for this to really make sense. You also lose insight into the potential power of setting different thresholds for each of your base models.
