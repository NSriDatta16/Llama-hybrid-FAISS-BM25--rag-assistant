[site]: crossvalidated
[post_id]: 365679
[parent_id]: 
[tags]: 
How does Dempster-Shafer relate to Machine Learning?

I read Dempster-Schafer can be thought of as a generalization of Bayesian theory. Say I have data from disparate sources that indicate the class of some object. If I have some prior beliefs about the world, or by examination of some corpus of training data, I can relate features to classes through conditional probabilities and do some reasoning. (Or is it the marginals I should be caring about?) Alternatively I can collect indicators together and pass them to a classifier, which in training will come to have its own prior beliefs. Can Dempster-Schafer be thought of as a kind of learner, then, just as Bayes Nets are? Is there a context where I should prefer a Dempster-Schafer model or Bayes Net to a model not based on belief propagation? Is there a context where I should prefer the opposite? I have never been extremely comfortable with belief propagation ("message passing"). I gather it is merely an efficient method to calculate marginal distributions, much as backpropagation just happens to be an efficient way to calculate gradients in a neural net. But what, in your words, do these marginal distributions mean? What are they for?
