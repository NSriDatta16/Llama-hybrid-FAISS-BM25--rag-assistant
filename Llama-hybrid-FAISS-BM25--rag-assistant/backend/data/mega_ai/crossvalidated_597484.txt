[site]: crossvalidated
[post_id]: 597484
[parent_id]: 597446
[tags]: 
All feature importance measures come down to assessing how much the classification degrades if a particular predictor is either completely removed from the model, or randomly permuted. If a model that removes (or randomly permutes) a particular predictor yields predictions that are as good as a model that retains that predictor, then the predictor can't be that important. The difference in predictive performance is then typically normalized in some way. Thus, a measure of feature importance really comes down to choosing an evaluation metric (if you predict numerical observations, this could be the Mean Squared Error, or in other cases, it could be Gini impurity or similar), deciding whether to assess this in-sample, out-of-bag or with a holdout sample, and then running multiple models, removing or permuting predictors. Some of these approaches are already provided in standard tools, e.g., in Random Forests (as in the eponymous R package), where this is a very simple and natural extension of the core functionality. In other cases (e.g., OLS), you can do the same thing, but you may need to code it yourself. In any case, there is really nothing particular about categorical predictors here, beyond having a model that can deal with them.
