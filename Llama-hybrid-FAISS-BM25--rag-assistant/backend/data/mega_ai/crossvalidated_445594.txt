[site]: crossvalidated
[post_id]: 445594
[parent_id]: 358047
[tags]: 
short Lets say you recorded the following episode: $$s_1, a_1^o, r_1,\quad s_2, a_2^o, r_2,\quad s_3, a_3^o, r_3,\quad s_4, a_4^p, r_4,\quad s_5, a_5^p, r_5,\quad s_6, a_6^p, r_6 $$ $a^o$ is an off-policy or non-greedy action and $a^p$ is the best action according to the current policy (greedy action). If you learn from $a^p_4$ , you improve your model for $Q(s, a^p)$ concerning actions according to the current policy. If you learn from $a^o_3$ you improve you model for $Q(s, a^o)$ . This is the understanding of what happens if you choose an off-policy action. Most likely the rest score $r_3 + r_4 + r_5 + r_6$ will be lower. But in a few cases not - this is when you learn a new tactics. If you learn from $a^o_1$ , you will most likely have a very low rest-score, as $r_1$ , $r_2$ and $r_3$ are on average lower then if acting according to the current policy. For this reason, your learning will just confirm the current policy. Even if $a_1^o$ was a brilliant action, probably $a_2^o$ and $a_3^o$ were not and $\sum\limits_{t \geq 1} r_t$ is smaller compared to a purely greedy episode. Hence, the algorithm can not see the brilliance of what he tried at $t=1$ . detailed explanation Your algorithm relies on two ingredients to find the optimal $Q(s,a)$ , i.e. $Q^*(s,a)$ : A good estimate of the final score based on the current policy $\pi$ A good estimate of what would happen in a state $s$ , when the next action would be off-policy and all / most further steps would be on-policy. Ingredient 2 ensures, that new actions are tried, which eventually improves the current policy. This is the reason you use non-greedy actions. Otherwise, $Q(s, a)$ would only give you great predictions if $a$ is an action according to the current policy ( $a=\pi(s)$ ) and horrible predictions for all other possible actions ( $a\in A \backslash \{\pi(s)\}$ ). This corresponds to learning from the tail of an episode. But this is not to be confused with learning from sequences of random actions! Your algorithm learns backwards, i.e. starting with the last rewards and propagating this information step-wise forward to earlier times. If you try to learn from a state $s_{t=7}$ and a lot of off-policy actions follow later at $t \gt 7$ , then the corresponding Q-value will be very low, as (most likely) you did a lot of unfavorable actions. Hence, the learning of your algorithms will be: I stick to my (non-optimal) policy for all early stages of the game. For the reason you do not (or just slowly) learn from early portions of the game if non-greedy actions are common.
