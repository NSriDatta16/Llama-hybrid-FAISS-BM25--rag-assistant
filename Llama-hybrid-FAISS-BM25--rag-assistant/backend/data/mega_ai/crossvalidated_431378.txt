[site]: crossvalidated
[post_id]: 431378
[parent_id]: 
[tags]: 
Square root of an almost diagonal matrix

Is there an efficient way to compute square root of an almost diagonal symmetric Hessian matrix, which is diagonal with the exception of the last two columns and last two rows? Could the efficient solution be somehow using the matrix diagonalization , that the diagonalization could be effective for this type of matrix? PS: Very likely it's not important for the answer at all, but I will include the exact definition of the matrix. The matrix is a modification of an originally diagonal Hessian from Rasmussen & Williams 2006 (Gaussian Processes in Machine Learning) , defined as (orig. definition see pg. 42-43, formula 3.15): $$ \begin{eqnarray} W_{ii} &=& -{\partial^2 \over {\partial{f_i^2}}}\log p(\boldsymbol{y}|\boldsymbol{f},g,h) = \pi_i(1-\pi_i); \text{ for } _{i = 1 \dots n-2} \\ W_{i,n-1} &=& -{\partial^2 \over {\partial{f_i}\partial{g}}}\log p(\boldsymbol{y}|\boldsymbol{f},g,h) = x_{1,i}\pi_i(1-\pi_i); \text{ for } _{i = 1 \dots n-2} \\ W_{i,n} &=& -{\partial^2 \over {\partial{f_i}\partial{h}}}\log p(\boldsymbol{y}|\boldsymbol{f},g,h) = x_{2,i}\pi_i(1-\pi_i); \text{ for } _{i = 1 \dots n-2} \\ W_{n-1,n-2} &=& -{\partial^2 \over {\partial{g}\partial{h}}}\log p(\boldsymbol{y}|\boldsymbol{f},g,h) = \sum_{i=1}^{n-2} x_{1,i} x_{2,i}\pi_i(1-\pi_i) \\ W_{n-1,n-1} &=& -{\partial^2 \over {\partial{g^2}}}\log p(\boldsymbol{y}|\boldsymbol{f},g,h) = \sum_{i=1}^{n-2} x_{1,i}^2 \pi_i(1-\pi_i) \\ W_{n,n} &=& -{\partial^2 \over {\partial{h^2}}}\log p(\boldsymbol{y}|\boldsymbol{f},g,h) = \sum_{i=1}^{n-2} x_{2,i}^2 \pi_i(1-\pi_i) \end{eqnarray} $$ where $\pi_i = \mbox{logit}^{-1}(f_i+g x_{1,i} + h x_{2,i})$ , and it's symmetric, i.e. $W_{i,j} = W_{j,i}$ . The original likelihood function is: $$p(\boldsymbol{y}|\boldsymbol{f},g,h) = \prod_{i=1}^{n-2} \mbox{logit}^{-1}(y_i (f_i+g x_{1,i} + h x_{2,i}))$$ for $y_i \in \{-1,1\}$ . I need to compute $W^{1 \over 2}$ for the formula 3.26 (pg 45) in the original paper. EDIT: PROBLEM SOLVED. I found a great paper - Eisenfeld 1976 - Block Diagonalization and Eigenvalues . The Corollary 3.1 is awesome, but it only works for upper triangular matrices. So I implemented the general iterative computation of root matrices in Theorem 4.1, but even with very beautiful matrices with all eigenvalues positive and not small and full rank, the iterations strongly diverged, leading to exponentially higher and higher differences (see the code below). So in the end, I was saved by great @Yves' comment "Can't you use a Cholesky square root rather than a symmetric one? I believe that the matrix inversion lemma would work as well in (3.27) if we write $W=LL^T$ ". Excellent idea!!! I did exactly that! The Cholesky decomposition of my matrix can actually be done much simpler and very fast!! I used the Block LU decomposition and this completely solved my problem :) I don't post this as an answer, since it did not answer the original question of the square root; but it solved my problem, Cholesky is perfect fit in this context as well. Not sure why the Eisenfeld 1976 iterations didn't work - I post my code here: # Implementation of Eisenfeld 1976 - Block Diagonalization and Eigenvalues # Generate matrix tol || Y2diff > tol) && it
