[site]: stackoverflow
[post_id]: 1658556
[parent_id]: 229472
[tags]: 
Test-Driven-Development: Nope. At best, in very tiny portions. We're all talking about, but don't do it. Domain-Driven-Design: Nope. Hard to know what a "domain" is if you're developing a technical framework. Have not much experience in DDD to know how to do it. Model-Driven-Design/Architecture: Nope. Do you test?: Yes, but not enough. With every release (we're trying to push out minor releases every 8 weeks) there're always more than 2 service releases. We're in the first year of product development, so i think this is pretty okay. Unit Testing: Yes. At approx. 30% coverage. Most of the developers know now that they should write unit tests for themselves. Every time they have to fix a critical bug in their own code, they can see the benefit if they would have written a simple test up front to prevent the bug in the first place. Integration Testing: Yes, using Selenium and WebDriver. Performance Testing: Yes, beginning with now. Goal is to archive long-term performance measurements and compare them against releases. Using JMeter and a dedicated performance test server for that. Acceptance Testing: Not really, but our product is used internally too and we're getting feedback pretty fast before it's being released. I count that as acceptance testing. Code Reviews: Nope. Sometimes, someone else looks at it for 30 minutes, but that's it. Innovative Technologies (Spring, Hibernate, Wicket, JSF, WS, REST, ...): From my POV, those technologies are not "innovative" any more. They're pretty much old-school now. Except JSF, which died a couple of years ago. Did Spring+Hibernate for the last couple of years. Now doing Wicket + WS for 2 years. Replaced Hibernate with iBatis SqlMaps. Agile: Nope. Pair Programming: Nope. UML: A little bit, mainly for deployment diagrams. Class diagrams too fine-granular and often are not in sync with reality. Developers do what they think is best, not what an UML diagram tells them to do. Domain-specific languages: Yes, we're using home-brewn business rules technology. It's a visual DSL which is suitable for endusers. Kind of like using Visio to model decisions. Requirement Specification (How?): Nope. Continuous Integration: Yes. Based on Hudson and Maven. Unit tests are run on each build. Additional nightly builds with more detailed reporting enabled. Whole team is notified about failed builds (yeah, they complain about getting too many mails sometimes if something breaks the chain and all 30 submodules get build failures, e.g. when the Maven Repository is unreachable) Code-Coverage Tools: Yes. Maven/Cobertura and Sonar. Aenemic Domain Model: No idea what this is supposed to be. Communication (Wiki, Mail, IM, Mailinglists, other documents): Wiki, Mail, IM, Daily standup meetings, Enduser and developer manuals written by a professional/non-developer. Effort estimates: Trying hard to do good estimates. But without reqs, they are just rough estimations. Good enough for resource planing though. Team size: 12 Meetings: Daily standup, retro every 8 weeks after each minor release. Code metrics: Sonar. Looking to comply with most of the rules. Did not have time to reconfigure the rules to suit our needs though. Static code analysis: Sonar. Bug tracking: JIRA Notes: Sonar is a code quality server. It combines tools like PMD, Findbugs, Checkstyle etc.
