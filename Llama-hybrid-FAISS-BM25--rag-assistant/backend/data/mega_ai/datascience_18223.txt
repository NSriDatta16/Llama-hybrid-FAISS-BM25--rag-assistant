[site]: datascience
[post_id]: 18223
[parent_id]: 17254
[tags]: 
With CNNs, a common strategy is to visualize the weights. They are usually most interpretable on the first CONV layer which is looking directly at the raw pixel data, but it is possible to also show the filter weights deeper in the network. If you want to have a look at the activation maps, you will have to display them 1 by 1 and they will be grayscale (e.g. you would display 15 AxB grayscale filters for your layer N). Check this site for more ideas: http://cs231n.github.io/understanding-cnn/ Having said that, it is unclear what you can expect to "see" on layers that are looking at a 200*200*10 cube and what action you can take afterwards. EDIT - Additional link This article describe a new technique that tries to show the "meaning" included in the neural network layers: https://distill.pub/2018/building-blocks/ "For instance, by combining feature visualization (what is a neuron looking for?) with attribution (how does it affect the output?), we can explore how the network decides between labels like Labrador retriever and tiger cat."
