[site]: crossvalidated
[post_id]: 642881
[parent_id]: 
[tags]: 
When is variational inference really powerful

I'm learning VI, and specifically Coordinate Ascent Variational Inference, and I'm wondering when we actually get a strong benefit for this method. In particular, the objective is: $$ \min D_{KL}(q(\theta)||p(\theta||D)) $$ And the whole point is that we can minimize this distance without knowing $p(\theta||D)$ However, in most the examples I see around, the prior $p(\theta)$ is usually set to something very loose, such as $N(0,100)$ (or anyway, with a big $\sigma^2$ ) Now, from a non-theorist-person, a prior $N(0,100)$ corresponds to a uniform prior pretty much, and we also know that: $$ p(\theta|D) \approx p(D|\theta), \text{ given } p(\theta) = Uniform $$ So, at that point, we can say that our posterior is pretty much our likelihood that we know how to calculate Now, this point of view comes from a person with little experience from the world of Bayesian/variational inference, so I might definitely not be seeing something utterly wrong in what I'm seeing, so please, correct me if I'm wrong The final question is: the whole point of VI was to approximate $p(\theta|D)$ without knowing it, though we usually use such a loose prior $p(\theta)$ that the posterior is almost equal to the likelihood, which we know how to calculate, and thus we can solve $\min D_{KL}(q(\theta)||p(D||\theta))$ ... however I have a sense that I'm missing something....
