[site]: crossvalidated
[post_id]: 337451
[parent_id]: 337262
[tags]: 
The graph does show that there are issues with this model, but the question is whether or not it shows deviations from the hypothesis of homoscedasticity, and that is not quite clear from the graph. I think it does show some deviation: sigma seems smaller at low values of the predicted variable, but it is hard to tell. The graph does show that the mean of the residuals is going down from right to left (although, as pointed out by whuber, there is some level of optical illusion in this, because of the larger amount of points in the bottom left). So, the distribution of the residuals (specifically, the mean of the distribution) is probably not the same at high and low values of the predictions (can fit a model of the form a*x+b to the residuals and show it in the same plot) . I would not call this a violation of linearity (residuals in non-linear regression models are also expected to have the same mean throughout the application domain), but it does seem to violate the assumption that all residuals are coming from the same distribution. The simplest way to test whether the mean of the residuals has a trend is to look at at ANOVA table of the regression model (this is a bit different from, although related to, doing an ANOVA analysis of the residuals). In Mathematica, the property "ANOVATable" is available for regression models obtained with LinearModelFit and NonlinearModelFit. In fairness, using the standard ANOVA is not quite justified, because the distribution of the residuals does not look normal. There are non-parametric ANOVA methods that could be used (Kruskal-Wallis). Honestly, I think a test is hardly necessary, the trend seems clear to me from the graph. The plot I suggest above will give you a value for the slope. A test like ANOVA (or similar) would give you a p-value to judge whether that slope is significantly different from zero (one cannot expect it to be exactly zero). Back to homoscedasticity, you would need to check that the standard deviation (sigma, for short), not the mean, is the same at different values of the predicted variable. One simple way to do this is to bin the data in a few bins and compute sigma in each bin. Of course, there is some freedom as to how to bin the data (so this simple approach relies on judgement). Instead of computing the sigmas for each bin, you can just do a test on the groups of residuals in each bin. The Bartlett test, for example, takes several samples as its input and decides whether the samples came from distributions with the same sigma (not whether they came from the same distribution). In your case, it looks like there are serious deviations from normality, so the Bartlett test may not be the best, Levene is more robust. In Mathematica, the function VarianceEquivalentTest takes in several samples (which would just be your residuals in different bins) and returns the results of several tests (Bartlett, Levene, Conover, etc.), on whether the samples have the same sigma. I think it will refrain from reporting on a test that does not seem applicable to the data. From your graph, I would be inclined to say that at low values of the predicted variable your sigma looks smaller (same level of spread with more data points usually means lower sigma). But it is not as clear to me as is the trend in the mean, for example (again, even for the trend in the means, it has been suggested that the graph could be deceiving because of the variations in population density). A clear violation of homoscedasticity is a graph with very narrow spread in some parts and very wide in others, and yours does not display that very clearly. But is does not clearly show the opposite either. Another way to test for homoscedasticity directly on the residuals (without binning the data) is to run the Breusch-Pagan test or the White test on the residuals (you will need the residuals AND the predicted values, because these tests check whether sigma seems to be a function of the predicted values). The residuals do show a clear deviation from normality, it even seems clear that they are bimodal. For example, at large values of the predicted variable, the residuals are clearly concentrated around two values, one high and one low. You could run a normality test on the residuals (seems hardly necessary), like Anderson-Darling, Smirnov, etc., but since the residuals seem to be coming from different distributions, these tests are not very meaningful. A test of normality makes sense in a sample that was all taken from the same distribution. In this sense, a test of normality should usually come last, after you have established that all the residuals come from the same distribution. Some people just do a test of normality and nothing else, assuming that if the residuals came from different distributions, most likely a test of normality would fail. There is some truth to that, but it is very shaky statistics. It is like saying that if the body temperature is normal, then the patient is healthy. In general, you want to see evidence that all your residuals are coming from the same distribution. The first thing to check is the mean and sigma. In your case, the mean does not not seem constant to me, but you should check, and for sigma it is just harder to say from your graph (so the short answer to your original question, which this certainly isn't, is that it is hard to tell from your graph). Of course, a distribution is more than just its mean and its sigma, but if those two look good (meaning, if they constant throughout the application domain), then there is reason to celebrate. If the residuals all come from the same distribution, then one would also want for that distribution to be centered around zero, to be unimodal, preferably symmetric, and, ideally, normal. But normality is like the last nice-to-have, not a requirement for a good model. Lastly, remember that the scatter plot of the residuals for a good model can look a bit messy. To a large degree this depends on how uniformly populated the different regions of the model domain are. If one has significant amount of data in one region and very little in others, then there are issues related to leverage of different data points. This is in part why one looks at standardized residuals and studentized residuals. In my mind, a plot of residuals that looks too ideal (a perfectly horizontal, evenly populated rectangle), suggests a fudged model, rather than a good model.
