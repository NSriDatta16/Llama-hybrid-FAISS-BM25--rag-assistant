[site]: crossvalidated
[post_id]: 166525
[parent_id]: 
[tags]: 
Is Mahalanobis distance equivalent to the Euclidean one on the PCA-rotated data?

I've been led to believe (see here and here ) that Mahalanobis distance is the same as the Euclidean distance on the PCA-rotated data. In other words, taking multivariate normal data $X$, the Mahalanobis distance of all of the $x$'s from any given point (say $\mathbf{0}$) should be the same as the Euclidean distance of the entries of $X^{rot}$ from $\mathbf{0}$, where $X^{rot}$ is the product of the data and the PCA rotation matrix. 1. Is this true? My code below is suggesting to me that it is not. In particular, it looks like the variance of the Mahalanobis distance around the PCA-Euclidean distance is increasing in the magnitude of the PCA-Euclidean distance. Is this a coding error, or a feature of the universe? Does it have to do with imprecision in an estimate of something? Something that gets squared? N=1000 cr = runif(1,min=-1,max=1) A = matrix(c(1,cr,cr,1),2) e 2. If the answer to the above is true, can one use the PCA-rotated Euclidean distance as a stand-in for the Mahalanobis distance when $p>n$? If not, is there a similar metric that captures multivariate distance, scaled by correlation, and for which distributional results exist to allow the calculation of the probability of an observation? EDIT I've run a few simulations to investigate the equivalence of MD and SED on scaled/rotated data over a gradient of n and p. As I mentioned previously, I'm interested in the probability of an observation. I am hoping to find a good way to get the probability of an observation being part of a multivariate normal distribution, but for which I've got $n slightly biased estimator of the MD, with a fair amount of variance that seems to stop increasing when $p=N$. f = function(N=1000,n,p){ a = runif(p^2,-1,1) a = matrix(a,p) S = t(a)%*%a x = mvrnorm(N,rep(0,p),S) mx = apply(x, 2, mean) sx = apply(x, 2, sd) x = t(apply(x,1,function(X){(X-mx)/sx})) Ss = solve(cov(x)) x = x[sample(1:N,n,replace=F),] md = mahalanobis(x,rep(0,p),Ss,inverted=T) prMD Two questions: 1. Any criticism of what I'm finding in these simulations? 2. Can anyone formalize what I'm finding with an analytical expression for the bias and the variance as functions of n and p? I'd accept an answer that does this.
