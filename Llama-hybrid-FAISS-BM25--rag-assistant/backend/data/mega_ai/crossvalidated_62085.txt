[site]: crossvalidated
[post_id]: 62085
[parent_id]: 62071
[tags]: 
Ensemble methods arise when there is uncertainty about a particular model structure. The goal is to draw inference from a set, or ensemble, of candidate models. In a general sense, ensemble methods are a form of model averaging. The model may be a classification tree (random forests) or a regression model, or whatever. Several of the terms you've listed are simply alternative ways to take the average across models. Boosting is a term where models are sampled randomly, whereas other methods such as bagging and bumping more strongly emphasize training where the averaging is adaptive based on how well a particular model is doing. So I believe the more general class of methods you're interested in is 'model averaging' where the ensemble represents the set of candidate models to be averaged over.
