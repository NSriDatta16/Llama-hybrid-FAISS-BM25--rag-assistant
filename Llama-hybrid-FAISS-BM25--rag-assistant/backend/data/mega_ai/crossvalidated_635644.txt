[site]: crossvalidated
[post_id]: 635644
[parent_id]: 635642
[tags]: 
I share your observation that on well-behaved tabular data (quite many rows, not too many columns), a well-built boosted trees model usually beats a random forest, and a random forest usually beats a single tree. One of the main deficits of a tree-based model over a linear model is the wigglyness/non-smoothness of the prediction function. Another one, of course, is the ease of interpretability. One thing that I particularly like about Boosted Trees models is that you can easily modulate their complexity via tree depth (1 = additive, 2 = pairwise interactions), and interaction constraints. Combining tree-depth 2 with interaction constraints, e.g., allows to build models with some features modeled additively for maximal interpretability, while others bear pairwise interactions. Additionally, carefully set monotonicity constraints help to reduce wiggliness of the prediction function. You won't find these options on Kaggle, but they greatly improve interpretability behavior of the model. References on constrained Boosted Trees Michael Mayer and Christian Lorentzen, Lecture notes "Responsible ML with Insurance Applications" (ETH Zurich), Chapter 3 Python notebook with explanations in https://github.com/samathizer/boosted-trees (work in progress) Edit Another advantage of linear models: direct availability of inferential methods, even if they require additional assumptions to be sufficiently met.
