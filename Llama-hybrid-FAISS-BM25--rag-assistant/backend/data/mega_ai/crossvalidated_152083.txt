[site]: crossvalidated
[post_id]: 152083
[parent_id]: 152078
[tags]: 
Some machine learning algorithms require the input data to be normalized in order to converge to a good result. Let's take for example a data set where samples represent apartments and the features are the number of rooms and the surface area. The number of rooms would be in the range 1-10, and the surface area 200 - 2000 square feet. I generated some bogus data to work with, both features are uniformly distributed and independent. Before scaling, the data could look like this (note that the axes are proportional): You can see that there is basically just one dimension to the data, because of the two orders of magnitude difference between the features. After standard scaling, the data would look like this (note that the axes are proportional): Now the data is nicely distributed and a logistic regression or SVM could do a much better job classifying the samples.
