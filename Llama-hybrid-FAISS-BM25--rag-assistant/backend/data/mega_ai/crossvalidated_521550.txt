[site]: crossvalidated
[post_id]: 521550
[parent_id]: 521503
[tags]: 
You have two different models. Let $X$ be the data and $f$ be some function (logistic regression, neural network, whatever). Model 1 $\text{probability} = f(X) \in [0, 1]$ Model 2 $\text{probability} = \text{round}(f(X)) \in \{0,1\}$ The second model has a lower ROCAUC. The way I might explain this is that a number of cases are just on the wrong side of the threshold (probably $0.5$ ), such as being true $0$ cases with a predicted probability of $0.51$ that gets rounded to $1$ in Model 2 . Model 1 considers this to be an error but not a gigantic error. Model 2 considers this to be a gigantic error: not only do you lean in the wrong direction, but you confidently assert the wrong class.
