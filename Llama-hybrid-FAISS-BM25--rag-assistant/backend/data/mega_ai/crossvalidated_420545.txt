[site]: crossvalidated
[post_id]: 420545
[parent_id]: 
[tags]: 
Why is there a reconstruction loss in PCA with orthonormal eigenvectors?

I've already read How to reverse PCA and reconstruct original variables from several principal components? and I understand conceptually and visually why there has to be a reconstruction loss. However, if we have a data matrix $\mathbf X$ and its orthonormal eigenvectors $\mathbf {V}$ and then take the first $k$ eigenvectors and make a low-rank approximation: $\mathbf Z=\mathbf {XV_k}$ With $\mathbf {V_k}$ being orthonormal, shouldn't it be $\mathbf X=\mathbf {XV_k V^{T}_{k}}$ because $I = \mathbf {V_k V^{T}_{k}}$ ? Maybe someone can provide an example why exactly this is wrong? A perfect answer would just provide a simple numerical example and/or an explanation of what I'm missing here.
