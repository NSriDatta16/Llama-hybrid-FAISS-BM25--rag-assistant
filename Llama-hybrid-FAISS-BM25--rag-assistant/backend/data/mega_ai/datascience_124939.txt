[site]: datascience
[post_id]: 124939
[parent_id]: 124914
[tags]: 
To find the necessary function using error back propagation, you only need to design the architecture of the neural network , which will represent the desired function (for example, several layers, activation functions, etc.). I do not see any problems when using the function you specified as a loss function. Because cosine similarity in this case is the product vector embeddings, then the gradient is calculated obviously. I recommend that you study how face recognition models are trained (for example, Facenet ). Very similar approaches are used there. I'm pretty sure you're aware of the use of Kullbackâ€“Leibler divergence to solve this problem. I just wanted to draw attention to the possible use of SGD for its optimization, which may give a hint in your case.
