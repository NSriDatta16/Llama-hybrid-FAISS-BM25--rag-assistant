[site]: stackoverflow
[post_id]: 4925381
[parent_id]: 4924978
[tags]: 
It has been my experience that "cache design" is a mixture of black art and hard science. While hard science tends to be extremely predictable, this would make you think there's a formula, or at least a good rule of thumb, that you can apply to get useful results. The black art part means that this is true, but is completely falsified while still managing to remain true. One thing that remains invariant, however, is the need for comprehensive metrics. You must, unconditionally, have extensive numbers based on profiling your application using Real Worldâ„¢ data. Without this, you are simply guessing. Decades of practical experience has shown time and time again that if you, as a programmer, are guessing about the nature of "where the performance problem is", you are 100% guaranteed to get it wrong. Hence the need for hard, empirical data. If you decide to pursue this, this first thing you must do, before you even start to "work on the problem", is find a way to gather empirical metrics. Since you don't mention what language or tools you're using, I can't make specific recommendations, but virtually every tool chain has some profiling tools specifically designed to help you understand where your program is spending its time. Next, your intuition in this case is probably correct. You've already identified that your access patterns are likely to be "write biased". A very common property of writes is that "they must happen before you can do anything else". If this involves writing the data out to disk, you usually get bottlenecked on waiting for the disk i/o operation to complete, which is generally a real performance killer. In this case, caching is unlikely to help at all as its not like you can "cache the write" because it has to happen. There are some cases where "write caching" can help. If your design and requirements allows the in memory version of data to be temporarily inconsistent with the on disk version of the data, it is often possible to "write combining". This essentially involves delaying committing the data to disk based on the fact that for some access patterns, some non-consecutive writes will "update" the same "block" within a "flush to disk" window. Another thing you must do when designing a caching system is take all your metrics, and your understanding of how your cache works, and then write performance tests that are maximally orthogonal to your design choices. Ideally, your cache system should not noticeably degrade performance even in a worst case scenario, and there's always a worst case scenario. EDIT After re-reading your question, it's not clear if this is a performance problem you are experiencing right now, or one that you think you "might" experience. If it's the later, re-read, at least three times, the second paragraph in my answer. The only time you should be contemplating building a cache system is when you have identified, with hard empirical data, that you have a performance problem.
