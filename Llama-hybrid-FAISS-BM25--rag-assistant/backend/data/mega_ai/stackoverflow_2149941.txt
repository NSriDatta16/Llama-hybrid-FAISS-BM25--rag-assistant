[site]: stackoverflow
[post_id]: 2149941
[parent_id]: 638030
[tags]: 
I use these incremental/recursive mean and median estimators, which both use constant storage: mean += eta * (sample - mean) median += eta * sgn(sample - median) where eta is a small learning rate parameter (e.g. 0.001), and sgn () is the signum function which returns one of {-1, 0, 1}. (Use a constant eta if the data is non-stationary and you want to track changes over time; otherwise, for stationary sources you can use something like eta =1/n for the mean estimator, where n is the number of samples seen so far... unfortunately, this does not appear to work for the median estimator.) This type of incremental mean estimator seems to be used all over the place, e.g. in unsupervised neural network learning rules, but the median version seems much less common, despite its benefits (robustness to outliers). It seems that the median version could be used as a replacement for the mean estimator in many applications. Also, I modified the incremental median estimator to estimate arbitrary quantiles. In general, a quantile function tells you the value that divides the data into two fractions: p and 1-p. The following estimates this value incrementally: quantile += eta * (sgn(sample - quantile) + 2.0 * p - 1.0) The value p should be within [0,1]. This essentially shifts the sgn () function's symmetrical output {-1,0,1} to lean toward one side, partitioning the data samples into two unequally-sized bins (fractions p and 1-p of the data are less than/greater than the quantile estimate, respectively). Note that for p=0.5, this reduces to the median estimator. I would love to see an incremental mode estimator of a similar form... (Note: I also posted this to a similar topic here: "On-line" (iterator) algorithms for estimating statistical median, mode, skewness, kurtosis? )
