[site]: crossvalidated
[post_id]: 552518
[parent_id]: 552375
[tags]: 
One way to think about the softmax function is that it gives you an output that can be interpreted as a probability distribution (i.e., all numbers are in the range [0,1], and they sum to 1). This is useful, because then the output of the softmax can be interpreted as a "probability" of each class/category (conditioned on the features). Why does its output always have this property? Well, the softmax is essentially the composition of two steps: Apply the exp function to each value. This makes all values non-negative. Normalize the values so they sum to 1 (by dividing by the sum). This makes all values sum to 1. After both of these steps, you are guaranteed that all values are non-negative and they sum to 1, which means they can be interpreted as a probability distribution. The generalized softmax with scaling $\lambda$ just amounts to multiplying all values by $\lambda$ , then applying the softmax, so it is not very different from the normal softmax. Another way to think about the softmax is that it is a natural generalization of the standard logistic function $f(x) = e^x/(1+e^x)$ , used in logistic regression . Logistic regression is used when you want to do two-class classification. When you want to do multi-class classification, you replace the standard logistic function with the softmax function. If you apply the softmax function with two classes, the result reduces to the standard logistic function that you're used to in (two-class) logistic regression.
