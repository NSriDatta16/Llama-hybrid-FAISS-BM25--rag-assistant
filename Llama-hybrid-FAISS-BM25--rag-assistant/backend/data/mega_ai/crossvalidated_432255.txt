[site]: crossvalidated
[post_id]: 432255
[parent_id]: 87182
[tags]: 
I guess your question is more about the "meaning" of that logarithm and why each component contributes to the overall meaning of the formula, rather than the mere formalism showing the coherence of the definition to certain requirements. The idea in the Shannon entropy is to evaluate the information of a message by looking at its FREQUENCY (i.e. $p(x)$ ) and at its GENERALITY (i.e. $-log(p(x))$ ): $p(x)$ : the more "frequent" a message is the less information will carry (i.e. easier to be predicted). $-log(p(x))$ : The more "general" a message is the more information will carry. The first term $p(x)$ is about the frequency, the $-log(p(x))$ is about its generality. From now on, I will discuss how the GENERALITY affects the final entropy formula. So, we can define how general (e.g. rain/not rain) or specific (e.g. ligth/avg/heavy/veryHeavy rain) is a message based on the number of bits needed to encode it: $$ log_2(x) = number\_of\_bits\_to\_encode\_the\_messages $$ Now, sit, relax and look at how beautifully Shannon's Entropy does the trick: it is based on the (reasonable) assumption that messages which are more GENERAL are, consequently, more FREQUENT. E.g. I will say that is raining either if it is an average, heavy or veryHeavy rain. Thus, he proposed to encode the GENERALITY of messages based on how FREQUENT they are... and there you go: $$ log_2 N = -log_2 1/N = -log_2 P $$ with $N$ the frequency of a message $x$ . The equation can be interpreted as: rare messages will have longer encoding because they are less general, so they need more bits to be encoded and are less informative. Therefore, having more specific and rare messages will contribute more to the entropy than having many general and frequent messages. In the final formulation, we want to consider two aspects. The first, $p(x)$ , is that frequent messages are easier to be predicted, and from this perspective less informative (i.e. longer encoding means higher entropy). The second one, $-log(p(x))$ , is that frequent messages are also general, and from this perspective more informative (i.e. shorter encoding means lower entropy). The highest entropy is when we have a system with many rare and specific messages. The lowest entropy with frequent and general messages. In between, we have a spectrum of entropy-equivalent systems which might have both rare and general messages or frequent but specific messages.
