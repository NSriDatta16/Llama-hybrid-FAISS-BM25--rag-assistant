[site]: crossvalidated
[post_id]: 423230
[parent_id]: 
[tags]: 
Problems with SMOTE optimizing function

I am new to machine learning or R and tried to code a function "smotevalue" in R in order to fine-tune the parameters of SMOTE for binary classification/prediction in imbalanced data. The idea is to vary the two parameters of SMOTE (K and dup_size) in order to optimize the AUC score of predictions for testdata. Something seems to be wrong here, as the optimization function reports perfect AUC scores at every iteration. I tested the code within the "smotevalue" function in isolation, and it also gives me unreasonably high AUC scores when predicting the test data. (For example I changed the data-split to 5%/95% training/test, I still got an AUC of about 0.99 for the predictions on test) Is the testdata somehow used to train the model here? Where is my mistake? Edit: In the end the models are trained within k-fold cross validation through the "caret" package. When I use the k-fold cross validation within the "smotevalue" function the results get feasible, but the computation takes way too long as you might imagine. But it seems as if the problem would be connected to my use of the XGBoost function here..in the end my aim is to imitate the "xgbLinear" method I use within "caret". # Libraries library(smotefamily) #SMOTE library(caret) #Data Splitting library(pROC) #AUC Metric library(DEoptim) #Differential Evolution Optimization # Set Seed set.seed(123) # Read Training Data Into R bankruptcy.train integer values } # Differential Evolution Optimization of function smotevalue, # varying K and dup_size for highest AUROC value smote_de_obj $optim$ bestmem The following is the code that seems to work (it produces more reasonable results for AUC, here the best scores are somewhere in the 0.96 range). I would be thankful for any comments whether the approach is reasonable (besides a lot of basic concerns like whether Oversampling should generally be performed when working with XGBoost for example, which I'm also trying to figure out). smotevalue K and dup_size will be varied for optimization data_train $class class) # in order to avoid error in train function levels(data_train $class) class))) # xgbLinear (Extreme Gradient Boosting) xgbLinear_tune
