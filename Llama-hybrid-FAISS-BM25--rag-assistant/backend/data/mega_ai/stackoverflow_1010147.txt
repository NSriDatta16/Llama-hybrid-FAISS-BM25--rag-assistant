[site]: stackoverflow
[post_id]: 1010147
[parent_id]: 1010040
[tags]: 
I think you might have a conceptual problem more than anything else. "Calculate potential memory usage" is at odds with "efficiently calculate the size of each chunk". The only way to really get at your memory usage to the degree of accuracy where you can predict adequate chunk size is to actually make your conversion. It sounds like the best way to come at this efficiently might be to tackle it progressively--essentially what those who are suggesting streaming objects are saying. If you can't leverage actual streaming, you'll probably want to structure your serialization so that you progress one conceptual unit at a time (i.e. one item in your list with it's attendant dictionary children).
