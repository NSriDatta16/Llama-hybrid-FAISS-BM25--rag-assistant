[site]: crossvalidated
[post_id]: 551309
[parent_id]: 152897
[tags]: 
In the top answer to this question, there's a link to the lecture of Pr. Yaser Abu-Mostafa from CalTech and it gives a very nice intuition of it... so I'll try to explain what i understood, without equation : a kernel is a function (relatively simple to compute) taking two vectors (living in the X space) and returning a scalar that scalar happens in fact to be exactly the dot-product of our two vectors taken to a higher dimension space Z so, the kernel tells you how close two vectors are in that Z space, without paying the (possibly enormous) price of computing their coordinates there that's all you need to fit an SVM model! in a regular SVM model, you would have used the dot-product in the X space... using the kernel instead is as if you were doing the same thing in the Z space Here's some words from the lecture: Think of it this way. I am a guardian of the Z space. I'm closing the door. Nobody has access to the Z space. You come to me with requests... If you give me an x and ask me, what is the transformation, that's a big demand. I have to hand you a big z. And I may not allow that. But let's say that all I'm willing to give you are inner products . You give me x and x dash, I close the door, do my thing, and come back with a number, which is the inner product between z and z dash, without actually telling you what z and z dash were . That would be a simple operation. And if you can get away with it, then that's a pretty good thing. Selected parts of the lecture: Watch "What do we need from the Z space?" Watch "The kernel in action"
