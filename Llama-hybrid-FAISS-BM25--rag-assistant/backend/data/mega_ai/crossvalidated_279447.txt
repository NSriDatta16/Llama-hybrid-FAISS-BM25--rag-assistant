[site]: crossvalidated
[post_id]: 279447
[parent_id]: 279429
[tags]: 
You could take the difference between the two time series, $z_t:=y_t-x_t$ (where $x_t$ and $y_t$ are the two original series), and test whether $z_t$ has zero mean. You could implement this by running a regression of $z_t$ on a constant and either (1) allowing the error to follow and ARMA process or (2) using heteroskedasticity and autocorrelation robust standard errors. You would look at the statistical significance of the intercept of the model to see whether $z_t$ is statistically above or below zero, which corresponds to $y_t$ being above or below $x_t$. This is simpler than randomization, and the problem of autocorrelation is handled in a more proper way (whereas I think it is not handled properly in randomization as you describe it).
