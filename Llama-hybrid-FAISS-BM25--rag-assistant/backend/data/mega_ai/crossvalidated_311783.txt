[site]: crossvalidated
[post_id]: 311783
[parent_id]: 311756
[tags]: 
Couple of things that can be done with these tools Checking whether your model's weights make sense when you suspect a bug Validating that your model doesn't just fit to random noise Extracting useful features for different models Compare different models when their performance is similar I'm by no means an expert on the topic, but it seems like machine learning models interpretability is an active topic - for example you can check out eli5 , or techniques mentioned in this repository .
