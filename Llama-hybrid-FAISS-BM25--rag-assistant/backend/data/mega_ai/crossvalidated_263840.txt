[site]: crossvalidated
[post_id]: 263840
[parent_id]: 263066
[tags]: 
Besides $â„¦(h) = ||h||_1 = \sum_i |h_i|$ there are also other forms of penalties like Student-t, KL divergence, average activation restrictions. Completely different approach is orthogonal matching pursuit which encodes an input x with the representation h that solves the constrained optimization problem: $$\arg\limits_{h,||h||_0 where $||h||_0$ is the number of non-zero entries of h. Basically it gives you kind of (but not exactly) dimensionality reduction (see PCA, auto-encoders), so you can try to just compress your input as well (you'll need less neurons to process it then). Anyway, you have to learn some "useful representation of the input". Sometimes, even deep-networks with ReLU (I assume your perceptron is one-layered) can help as the more layers you have the less connections between neurons you need ("exponential gain from depth"), less connections for output (sigmoids) neurons too. You can read more in "7.10 Sparse Representations" of deep learning book . Another approach is parameter sharing (like in CNN), which can reduce the size of your $W$ matrix. Goal is not regularization, but to drop the feature completely and optimize the model for the production use. If you want to drop a feature, this feature must be some noise (redundant), getting rid of noise is basically what regularization (in general sense) does anyway :). Otherwise, you could just drop $w$ values less than some $\epsilon$ - it might actually work sometimes (you can check error on your validation set to estimate the effect of such approach).
