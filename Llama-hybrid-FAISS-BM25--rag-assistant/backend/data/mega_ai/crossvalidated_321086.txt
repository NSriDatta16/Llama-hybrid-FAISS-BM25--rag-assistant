[site]: crossvalidated
[post_id]: 321086
[parent_id]: 321009
[tags]: 
Yes they’re sufficient, in both senses of that word. This was mentioned in the comments. Here are a couple comments on some of things being mentioned in this thread. Comment 1: you probably wouldn’t want a Kalman filter. Following up on @BenOgorek's comment, you can use a Kalman filter if you write your model as $$ x_t = \Phi x_{t-1} + \Upsilon u_t + w_t $$ and $$ y_t = x_t + v_t = x_t $$ where $\Phi = 0$ and $\text{Var}(v_t) = 0$, $u_t = 1$ and $\Upsilon = \mu$. Also, $\text{Var}(w_t) = Q$, and you're assuming Normal errors. Sorry for the extra notation, I am just following "Time Series And Its Applications with R Examples". The Kalman recursions give you recursive formulas for $E[X_t \mid y_{1:t}]$, $\text{Var}[X_t \mid y_{1:t}]$, $E[X_{t+1} \mid y_{1:t}]$, $\text{Var}[X_{t+1} \mid y_{1:t}]$. Specifically they are $E[X_{t} \mid y_{1:t-1}] = \Upsilon = \mu$ $\text{Var}[X_{t} \mid y_{1:t-1}] = Q$ $E[X_t \mid y_{1:t} ] = \mu + (y_t - \mu) = y_t$ $\text{Var}[x_t \mid y_{1:t}] = 0$. This is the "recursive" formula that doesn’t have much to do with what you’re looking for. It assumes you know the parameters, and gives you the common sense answer for the next data point, but it doesn’t say much about the average one step ahead sample variance. Comment 2: the posterior predictive distribution Another thing, @Zhanxiong's answer looks good (+1), but it might not work in a Bayesian setting, where you're using the posterior predictive distribution instead of assuming the parameters are known. In a Bayesian setting, you would have (by the law of total expectation) $$ E[S_{t+1}^2 \mid y_{1:t}] = E[E(S_{t+1}^2 \mid \mu, \sigma^2) \mid y_{1:t}] = E[\sigma^2 \mid y_{1:t}], $$ which is the posterior mean of $\sigma^2$. Whether or not this has a nice recursive formula depends on your choice of priors for $\mu$ and $\sigma^2$.
