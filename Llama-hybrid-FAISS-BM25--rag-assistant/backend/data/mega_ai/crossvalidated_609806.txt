[site]: crossvalidated
[post_id]: 609806
[parent_id]: 609685
[tags]: 
I'm not sure why the intervals shrank when unconditional = TRUE was used - I'd expect them to get somewhat wider if anything as this option is trying to correct for the fact that the model estimated (selected is perhaps better) the values of the smoothing parameters. As to the question in your title: When to use 'unconditional = FALSE' in plot.gam()? or the similarly phrased one in the body of the Q When would we treat the smoothing parameters as fixed? When we fit a GAM with penalised splines in {mgcv}, we need to estimated coefficients for all basis functions involved in smooth, plus coefficients for any parametric terms, plus other parameters (such as dispersion parameters). To do this, {mgcv} minimises the peanliased log-likelihood $$ \mathcal{L}_p(\boldsymbol{\beta}) = \mathcal{L}(\boldsymbol{\beta}) - \frac{1}{2\phi} \sum_{j} \lambda_j \boldsymbol{\beta}^{\mathsf{T}}\mathbf{S}_j\boldsymbol{\beta} $$ Note the $\lambda_j$ in the equation, which are the $j$ smoothing parameters that control how much penalty we pay for the wiggliness of the smooths in the model. The values of the $\lambda_j$ are not known before we fit the model; we want the model to find optimal values for the $\lambda_j$ . gam() does this by setting the $\lambda_j$ to some value, and then updating values for $\boldsymbol{\beta}$ given these initial values of $\lambda_j$ . These steps are iterated in such a way that each outer iteration (where we update the $\lambda_j$ ) moves towards more optimal values of the smoothing parameters, while the inner part of the iteration finds estimates of $\boldsymbol{\beta}$ conditional upon the current values of $\lambda_j$ . Eventually the $\lambda_j$ and $\boldsymbol{\beta}$ converge on some values and don't change much if further iterations are performed; the model has converged. So far, so good. The problems begin however, when we want to do inference on the estimated smooth functions. The general theory works if we treat the values of $\lambda_j$ as if they were known before fitting and were fixed at their ML or REML estimates. But we didn't know them; we estimated (selected) values for $\lambda_j$ using the data. This means that inferences (statistical tests) we make by treating the $\lambda_j$ as known and fixed are anti-conservative because they do not reflect the true state of our uncertainty about the values of the smoothing parameters. This is why you may have read that p values for smooths are more approximate than in a LM or GLM setting. Statisticians, including Simon Wood, have tried to provide corrections that account for the uncertainty in the smoothing parameters. Unsurprisingly, Simon uses the theoretical developments he is responsible for in his {mgcv} package. This correction isn't applied to the p values in the output from summary() but it can be used to produce credible intervals that better reflect the uncertainty in the estimated smooths that arises from us treating the smoothing parameters as known and fixed. This is what unconditional = TRUE does; if available (you have to use method = "REML" or "ML" for this to work), the Bayesian covariance matrix corrected for smoothing parameter uncertainty is used to form credible intervals for the smooths. Simon's approach works well in general, but it breaks down (doesn't work as well) when the true function is close to the penalty null space of the smooth; e.g. when the true function is close to a linear function. When to use it? All the time (if available) â€” the intervals on your smooths will better reflect the actual uncertainty about the entire fitted model. If you use unconditional = FALSE , you are doing a bit of hand waving misdirection by proceeding as if you knew the values of the smoothing parameters ahead of time, before you fitted the model. I believe it is not turned on by default because it isn't available if the model is fitted using GCV (which is also the {mgcv} default - but you shouldn't really use that for most things) or if you used gamm() for example. This is why I don't set unconditional = TRUE by default in my {gratia} package; if I did I'd have to throw a message all the time anyone used draw() on a GAM fitted with GCV to tell them that we didn't actually use the corrected covariance matrix, despite the documentation saying unconditional = TRUE was the default. It's easier to just let users turn this on if they want it and know what they are doing. That way, if they ask for it on a model estimated with GCV, say, then they deserve to get a loud warning (and this is what {gratia} does if you inappropriately ask for the smoothing parameter uncertainty-corrected covariance matrix.)
