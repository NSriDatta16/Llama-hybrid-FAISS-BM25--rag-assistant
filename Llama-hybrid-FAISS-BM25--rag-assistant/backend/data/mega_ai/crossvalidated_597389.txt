[site]: crossvalidated
[post_id]: 597389
[parent_id]: 597105
[tags]: 
In your model, the effect of $x_1$ is a linear function of $x_1$ itself: $$\Delta(x_1) = \frac{\partial E[y \vert x_1,x_2]}{\partial x_1}= \beta_1 + \beta_3 \cdot (2 \cdot x_1)$$ This is just a weighted sum of two regression coefficients so that you can easily calculate the standard error and perform the usual hypothesis test. But you still have to decide what value(s) of $x_1$ to plug in. There are three common options. The first option is to plot the function and its confidence interval for various values of $x_1$ (say from min to max in your sample), along with the distribution of $x_1$ and reference line (like $y=0$ ). This will partition the range of $x_1$ into 1-3 pieces: a positive SS effect range, a negative SS effect range, and an interval consistent with a nil effect. The second option is to calculate the average in your estimation sample: $$\frac{1}{N}\sum_i^{N}(\beta_1 + \beta_3 \cdot 2 \cdot x_{1i})=\beta_1 + \beta_3 \cdot 2 \cdot \frac{1}{N}\sum_i^{N}x_{1i}=\beta_1 + \beta_3 \cdot 2 \cdot \bar x_{1}.$$ This reduces to testing a single null hypothesis. The third option is something in between these: you can also evaluate at the mean/median/mode/percentile/interesting value of $x_{1}$ and perform a joint hypothesis test that all such effects are zero. The choice depends on the steepness of the slope relative to the intercept and your decision or research question. It's not solely a statistical question. As a general approach, if there is a good reason to include an interaction (previous work, theory, etc), I would plot the function $\Delta(x_1)$ for myself and then decide how to report the effect. If the sign or the magnitude of the effect changes substantially, I would report the function and its CI. If not, I would report the average and stick the function in an appendix or robustness section. All of these methods treat $x_1$ as if it was given or fixed. If you want to make inferences about the underlying population, you need to account for the variation in the covariates that would arise in repeated sampling. This adjustment will usually widen your standard errors. Example in Stata : Here I fit a regression model of expected car price with a 2nd-degree polynomial in miles per gallon ( $\beta_1$ and $\beta_3$ ) and 1st-degree polynomial (aka linear) weight ( $\beta_2$ ). Then we calculate the derivative wrt mpg for mpg at 15, 30, and 45. This is a baby version of approach (1), with only three values. Then we do a Wald test that all three effects are equal to each other and zero (approach 3). Stata expresses that as 3 separate equality hypotheses. . sysuse auto, clear (1978 automobile data) . regress price c.mpg##c.mpg c.weight Source | SS df MS Number of obs = 74 -------------+---------------------------------- F(3, 70) = 12.70 Model | 223815416 3 74605138.6 Prob > F = 0.0000 Residual | 411249980 70 5874999.72 R-squared = 0.3524 -------------+---------------------------------- Adj R-squared = 0.3247 Total | 635065396 73 8699525.97 Root MSE = 2423.8 ------------------------------------------------------------------------------ price | Coefficient Std. err. t P>|t| [95% conf. interval] -------------+---------------------------------------------------------------- mpg | -981.0308 377.9748 -2.60 0.011 -1734.878 -227.1838 | c.mpg#c.mpg | 17.32961 6.859794 2.53 0.014 3.648184 31.01104 | weight | .8344929 .7160289 1.17 0.248 -.5935817 2.262567 _cons | 16106.35 6591.341 2.44 0.017 2960.333 29252.36 ------------------------------------------------------------------------------ . margins, dydx(mpg) at(mpg = (15(15)45)) post Average marginal effects Number of obs = 74 Model VCE: OLS Expression: Linear prediction, predict() dy/dx wrt: mpg 1._at: mpg = 15 2._at: mpg = 30 3._at: mpg = 45 ------------------------------------------------------------------------------ | Delta-method | dy/dx std. err. t P>|t| [95% conf. interval] -------------+---------------------------------------------------------------- mpg | _at | 1 | -461.1425 182.8921 -2.52 0.014 -825.9092 -96.37576 2 | 58.74589 93.4678 0.63 0.532 -127.6698 245.1615 3 | 578.6342 262.1548 2.21 0.031 55.78293 1101.486 ------------------------------------------------------------------------------ . test _b[1._at] == _b[2._at] == _b[3._at] == 0 ( 1) [mpg]1bn._at - [mpg]2._at = 0 ( 2) [mpg]1bn._at - [mpg]3._at = 0 ( 3) [mpg]1bn._at = 0 Constraint 1 dropped F( 2, 70) = 3.37 Prob > F = 0.0401 Here the joint p-value is 0.0401, so we would reject the two-sided null that the three effects on price of mpg (evaluated at mpg of 15, 30, and 45) are all zero at conventional levels of significance. For completeness, I can also perform approach (1): . quietly regress price c.mpg##c.mpg c.weight . margins, dydx(mpg) post Average marginal effects Number of obs = 74 Model VCE: OLS Expression: Linear prediction, predict() dy/dx wrt: mpg ------------------------------------------------------------------------------ | Delta-method | dy/dx std. err. t P>|t| [95% conf. interval] -------------+---------------------------------------------------------------- mpg | -242.883 112.9552 -2.15 0.035 -468.1651 -17.601 ------------------------------------------------------------------------------ Here the effect of improving mpg by 1 above current values of mpg has a negative effect on price that is also statistically significant. This is consistent with what we saw above since most cars are low mpg (the median is 20). I used the terms effects above, though these are certainly not causal ones, merely descriptive.
