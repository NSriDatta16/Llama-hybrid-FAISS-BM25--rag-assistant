[site]: crossvalidated
[post_id]: 100713
[parent_id]: 
[tags]: 
Financial exposures modelling with graph theory tools

I was wondering how finance folks go about storing and modelling portfolio exposure relationships with the aim to later aggregate or slice & dice the exposures by different factor sets. For example, a portfolio invests into changing set of instruments (stocks, bonds, other portfolios ...) with time-varying value. The instruments in turn are sensitive to factors such as sectors (e.g. GICS classification) , regions/countries, size etc. Tabular Approach One straightforward approach would be to express this mix of network and/or hierarchical relationships in a tabular format such as below, which is amenable to use with SQL design patterns ( adjacency-list , path-enumeration , nested-sets , closure-table ) : > HOLDINGS Portfolio Holding Instrument Date BALANCE.USD 1 ABC Stock 1 Share Class A Stock 1 2013-12-31 25360291 2 ABC Stock 1 Share Class A Stock 1 2014-01-31 25302011 3 ABC Stock 1 Share Class B Stock 1 2013-12-31 12264011 4 ABC Stock 1 Share Class B Stock 1 2014-01-31 12893201 5 DEF Fund 1 Share Class EUR Series 1 Fund 1 2013-12-31 21012222 6 DEF Fund 1 Share Class EUR Series 1 Fund 1 2014-01-31 21632101 7 DEF Fund 1 Share Class EUR Series 2 Fund 1 2013-12-31 8214325 8 DEF Fund 1 Share Class EUR Series 2 Fund 1 2014-01-31 8292630 9 DEF Portfolio ABC Account Portfolio ABC 2013-12-31 155364592 10 DEF Portfolio ABC Account Portfolio ABC 2014-01-31 156202162 > FACTORS Instrument Factor ExposureStrength 1 Stock 1 North America: US 1.00 2 Stock 1 Industrials 1.00 3 Fund 1 Liquidity: Low 0.05 4 Fund 1 North America 0.70 5 Fund 1 Europe: Eurozone: Germany 0.20 6 Fund 1 Industrials : Capital Goods: Building Products 0.25 Network Approach Does anyone have any experience with modelling this particular domain as network objects rather than tables/matrices? In R for example using packages such as igraph , or statnet umbrella of packages ( network , networkDynamic as exposures are time-varying attributes) If so, could you kindly share short snippets of code demonstrating that it makes sense to use graph theory for these kinds of problems. My web research was not fruitful in this regard; I found mostly social network models. If not, does it make any sense at all to persist this as network objects rather than plain old tables, in terms of complexity (code overhead), computational and storage efficiency? To the support of the latter, vignette for the network package in R states: For example, a network with 100,000 vertices and 100,000 edges currently consumes approximately 74MB of RAM (R 2.6.1), versus approximately 40GB for a full sociomatrix (a savings of approximately 99.8%). When dealing with extremely large, sparse graphs it therefore follows that network objects are substantially more eï¬ƒcient than simpler representations such as adjacency matrices.
