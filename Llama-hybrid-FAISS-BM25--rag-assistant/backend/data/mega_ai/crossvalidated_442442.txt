[site]: crossvalidated
[post_id]: 442442
[parent_id]: 
[tags]: 
Why does training performance suffer when scaling a feature in logistic regression without regularization?

I am training a model using scikit-learn via logistic regression with no regularization applied. Scaling one of the features by factor 10^6 negatively impacts training performance. To my understanding, logistic regression is invariant under feature scaling when no regularization is applied. Why does the training performance of my model suffer from the scaling? Python code: import numpy as np import pandas as pd from sklearn.linear_model import LogisticRegression from sklearn import metrics np.random.seed(1) # create a DataFrame with 1000 samples of three independent random variables "x1","x2","noise", # which are uniformly distributed between -1 and 1 df = pd.DataFrame(np.random.rand(1000,3) * 2 - 1, columns = ["x1","x2","noise"]) # calculate binary target "y" as # y = 1 when x1+x2+noise > 0 # y = 0 when x1+x2+noise 0) * 1 # add a calculated field "x1_scaled" as x1_scaled = x1*10^6 df["x1_scaled"] = df.x1.apply(lambda x: x*1000000) # create a training data set "X" with x1 and x2 X = df[["x1","x2"]] # create a training data set "X_scaled" with x1_scaled and x2 X_scaled = df[["x1_scaled","x2"]] # train two models from the two training data sets via logistic regression with no regularization y = df.y model = LogisticRegression(random_state = 1, max_iter = 1000, penalty = "none", solver = 'lbfgs').fit(X,y) model_scaled = LogisticRegression(random_state = 1, max_iter = 1000, penalty = "none", solver = 'lbfgs').fit(X_scaled,y) # calculate and output training performance for the two models pred = [x[1] for x in model.predict_proba(X)] pred_scaled = [x[1] for x in model_scaled.predict_proba(X_scaled)] print("ROC AUC") print(metrics.roc_auc_score(y, pred)) print(metrics.roc_auc_score(y, pred_scaled)) print("LOG LOSS") print(metrics.log_loss(y, pred)) print(metrics.log_loss(y, pred_scaled)) Output - ROC AUC decreases and LOG LOSS increases: ROC AUC 0.8909082907938743 0.7828527307932343 LOG LOSS 0.40560311881940897 0.5629675489573317
