[site]: crossvalidated
[post_id]: 367146
[parent_id]: 366862
[tags]: 
It is not necessarily overfitting, but it also runs an unnecessary risk of overfitting , and you deprive yourself of the possibility to detect overfitting. Overfitting happens when your model is too complex/has too many degrees of freedom for the available training data. This includes degrees of freedom for the hyperparameter space you search. So if your data set is still large enough, you don't overfit (say, you have thousands of cases, just two variates to regress on and a single continuous hyperparameter to tune - that would likely still be OK. On the other hand, if you a handful of cases, hundreds or thousands of variates and a large hyperparameter search space, you run a huge risk of overfitting). But as all your data entered the training phase (during the hyperparameter optimization), you lost the chance to measure generalization error and thus cannot check/show that you do not overfit. Which is as bad as overfitting, unless you can give other evidence that you are not in a situation where overfitting can occur. Moreover, you traded in your ability to measure generalization error for at most a minute improvement in training: You could (and should) have done the whole training on the training set - that's what it is for. And training includes fixing the hyperparameters. From that point of view, the decision is really whether you need to have an error estimate based on unknown data or not (again based on the overall risk of overfitting - and in machine learning the decision would pretty much always be that unknown data performance is needed), and then either do the whole training on your data, or do the whole training on the training set and test with the test set. (Or possibly on multiple such train/test splits as in cross validation).
