[site]: crossvalidated
[post_id]: 97400
[parent_id]: 96738
[tags]: 
Any nonlinear statistic (a non-linear combination of linear statistics such as sample means) has a small sample bias. Cohen's $d$ is obviously no exception: it is essentially $$ d=\frac{m_1 - m_2}{\sqrt{m_3-m_4^2}} $$ which is fairly non-linear, at least as far as the terms in the denominator go. Each of the moments can be considered an unbiased estimator of what it's supposed to estimate: $$ \begin{array}{ll} m_1 & = \frac1{n_1} \sum_{i\in\mbox{group }1} y_i , \\ m_2 & = \frac1{n_2} \sum_{i\in\mbox{group }2} y_i , \\ m_3 & = \frac1{n_1+n_2} \sum_{i} y_i^2 , \\ m_4 & = \frac1{n_1+n_2} \sum_{i} y_i , \\ \end{array} $$ However, by Jensen's inequality there is no way on Earth you get an unbiased estimator of the population quantity out of a nonlinear combination. Thus ${\mathbb{E}}[ d]\neq$ population $d$ in finite samples, although the bias is typically of the order of $O(1/n)$. Wikipedia article on effect sizes mentions the small sample biases in discussion of Hedges' $g$. I imagine that Cohen's $d$ has a limited range (in the extreme case, if there is no variability within groups, then $d$ must equal $\pm 2$, right?), hence its sampling distribution must be skewed, which contributes to the finite sample biases (some function of the skewness of the sampling distribution is typically the multiplier in front of $1/n$ that I mentioned above). The closer you are to the limits of the allowed range, the more pronounced the skewness is. What bootstrap does, rather miraculously considering that it is such a simple method, is it gets you the ability to estimate this finite sample bias through comparison of the bootstrap mean and the estimate from the original sample. (Keep in mind though that unless you make special adjustments to how the bootstrap sampling is set up, the former will be subject to Monte Carlo variability.) I provided more detailed and more technical explanations in another bootstrap question which may be worth reading anyway. Now if there's a positive bias, i.e., the estimate based on the original sample is biased upward relative to the population $d$, then the bootstrap will mock that and produce estimates that are, on average, even higher than the sample estimate. It is not actually as bad as it sounds, as then you can quantify the bias and subtract it from the original estimate. If the original estimate of a quantity was $\hat\theta_n$, and the mean bootstrap of the bootstrap replicates is $\bar\theta^*_n$, then the bias estimate is $\hat b_n=\bar\theta^*_n-\hat\theta_n$, and a bias-corrected estimate is $\hat\theta_n - \hat b_n=2\hat\theta_n - \bar\theta^*_n$.
