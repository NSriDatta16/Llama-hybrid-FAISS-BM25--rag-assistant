[site]: crossvalidated
[post_id]: 444102
[parent_id]: 221715
[tags]: 
I would suggest to use window-size approach. Given window-size=1024 (token) and you pre-define says 10 windows, then concatenating all vectors of the windows. This is similar to your solution 2, but rather than using word vectors, using window vectors. With this approach, you can try with other embedding such as BERT or similar as these have limited size of token length. If using Word2Vec, or word vector, would you consider to use a linear combination with the word weighting such as TFIDF and the word vectors. I found it's outperformed compared with word vectors without weightings.
