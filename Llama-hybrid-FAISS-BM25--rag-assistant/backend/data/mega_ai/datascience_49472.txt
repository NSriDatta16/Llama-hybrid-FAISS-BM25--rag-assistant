[site]: datascience
[post_id]: 49472
[parent_id]: 49355
[tags]: 
Thanks for providing the sample data. I do not really see any severe problems to pin something down as the definite cause of your problem, but I can give you some advice for improvement that could help. Standardizing and scaling Some of your features have larger values and some have smaller values. If you don't standardize and scale your features and targets, it will result in "unbalanced" weights inside your NN which can lead to an unstable model. Therefore, use something like the StandardScaler to standardize and scale your data after splitting it up into a train and a test dataset. Activation function It is always worth a try to play around with different activation functions. ReLu is quite simple and computationally inexpensive compared to other activation functions, but in a bad setup, it can lead to many dead neurons. So I would suggest trying out other activation functions like Tanh or Leaky Relu. Note: That does not mean that ReLu is a bad activation function. For many reasons, it is actually a very popular one. Learning rate Especially if you stick with ReLu, check what difference it makes if you reduce the learning rate and/or set a learning rate decay. Neural Network Architecture Since you are working with a time-series, it would make sense to use a Recurrent Neural Network which was designed for time-series data like GRU or LSTM . Other One side note to prevent you from falling into the same trap I did: If you work with TA-Lib, scale your values before you calculate any features. There is an open issue on Github of TA-Lib calculating wrong features if the input-values are too small. I see that your targets also have quite small values, so maybe keep an eye on that.
