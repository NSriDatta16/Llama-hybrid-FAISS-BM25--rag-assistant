[site]: crossvalidated
[post_id]: 641937
[parent_id]: 641934
[tags]: 
This is not uncommon in randomized studies, especially with smaller sample sizes where you are more likely to overestimate a difference in means or underestimate variance. Under perfect randomization the null hypothesis is true per definition, so any statistical test can only make a type I assertion with probability $\alpha$ . Add to that the number of baseline characteristics you could measure (multiplicity is through the roof), and you see why testing for baseline differences in a randomized study becomes a meaningless exercise. What you should be concerned with is that the randomization was set up and executed correctly (or no evidence to suggest otherwise); any prior difference that results from that is by definition a chance finding. The suggestion to re-randomize a sample where you observe such difference is therefore nonsensical, and often not even feasible unless all subjects are enrolled at once. There's another argument to be made against making frequentist decisions based on accumulating data unless your study was very specifically designed for it (e.g. Bayesian or adaptive elements). You did well to adjust your inference for the outcome's baseline. Even if there were no imbalance across groups that is almost always the way to go because the baseline might still be prognostic on the subject level. A baseline imbalance may be confounded with the intervention, but you can only address that by introducing additional assumptions which are just as likely to lead to less reliable results. It's quite possible that removing the baseline adjustment from your model or looking at otherwise unadjusted results leads to different conclusions, but it should be clear which of these is to be preferred if the baseline has any plausibly prognostic value. In summary, if randomization went as planned (and was planned well) that is the only argument you need. A baseline adjusted model is an extra layer of defense against such imbalance, although that doesn't solve the issue entirely (if the effects are confounded you lose power, but that's still preferred to a misspecified model). The only 'real' solution to prevent this is to randomize enough subjects so that any chance differences are almost certainly irrelevant.
