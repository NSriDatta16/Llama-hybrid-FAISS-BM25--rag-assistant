[site]: crossvalidated
[post_id]: 571374
[parent_id]: 417821
[tags]: 
The link below explains (thoroughly enough) how the vanishing gradient issue in LSTM may occur and also provides two sources where specifically in one of them (Bayer,2015) the formulation is discussed. It's the same formulation given in the link. It may be a little difficult to find since the source is a thesis. Basically it argues that the vanishing gradient issue can still occur in LSTM but, not as easily and as fast as in Vanilla RNN since there exists at least one path to a set of weights that can help prevent the vanishing gradient issue. My opinion is that the argument is a little far-fetched (no one can guarantee that there is such a path in all types of tasks without some serious analysis or mathematical proof). How does LSTM prevent the vanishing gradient problem?
