[site]: crossvalidated
[post_id]: 484452
[parent_id]: 484397
[tags]: 
Quoting from ISLR , pages 33 to 34, on the bias-variance tradeoff: ... the expected test MSE, for a given value $x_0$ , can always be decomposed into the sum of three fundamental quantities: the variance of $\hat f(x_0)$ , the squared bias of $\hat f(x_0)$ and the variance of the error terms $\epsilon$ . That is, $$ E\left( y_0 − \hat f(x_0)\right)^2 = \text{Var}\left( \hat f(x_0) \right) + \left[ \text{Bias} \left( \hat f(x_0) \right) \right]^2 + \text{Var}(\epsilon)$$ Here the notation $E\left( y_0 − \hat f(x_0)\right)^2$ defines the expected test MSE, and refers to the average test MSE that we would obtain if we repeatedly estimated $f$ using a large number of training sets, and tested each at $x_0$ . The overall expected test MSE can be computed by averaging $E\left( y_0 − \hat f(x_0)\right)$ over all possible values of $x_0$ in the test set. So the random variable in this context is related to the predicted fitted values at a series of given values of $x_0$ over a series of training sets. If you are willing to apply the bootstrap principle --the population is to your data set as your data set is to bootstrapped samples from it--Dave's initial sense of how to proceed was correct. You repeat the modeling process on a set of bootstrapped resamples from your data set, representing multiple training sets. You evaluate bias and variance and error with respect to the full data set, representing the population. You do that over the range of $x_0$ values of interest, and average. This is only an estimate of the true bias and variance of your modeling process, but it might be the closest that you can get without having access to the full population for testing and multiple samples from the population for training.
