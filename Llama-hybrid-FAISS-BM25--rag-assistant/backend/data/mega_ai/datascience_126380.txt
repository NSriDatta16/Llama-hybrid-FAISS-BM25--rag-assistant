[site]: datascience
[post_id]: 126380
[parent_id]: 
[tags]: 
Should you care about truncation and padding in an LLM even if it has a very large tokenizer.max_length so that truncation will never happen?

I want to find out the role of truncation and padding in Huggingface Transformers pretrained models and/or any fine-tuning models on top. Taking a large language model like the German GPT2 shows that the max_length is very large so that truncation should not play a role in the code - if I am not mistaken: tokenizer = AutoTokenizer.from_pretrained("dbmdz/german-gpt2") tokenizer.truncate_sequences Out: > PreTrainedTokenizerFast(name_or_path='dbmdz/german-gpt2', > vocab_size=50265, model_max_len=1000000000000000019884624838656, > is_fast=True, padding_side='right', truncation_side='right', > special_tokens={'bos_token': ' ', 'eos_token': > ' ', 'unk_token': ' '})> Checking only the max_length : tokenizer.model_max_length Out: 1000000000000000019884624838656 We can see that the max_length is so utterly large that I doubt any full document will ever reach it - and this is just the length of each example, row by row, in the dataset. I guess that truncation does not play a role at all anymore if such a large max_length is set. "Set the truncation parameter to True to truncate a sequence to the maximum length accepted by the model" is what the Huggingface guide says about truncation, see Truncation . That does not sound as if I ever can truncate the text or ever wanted to since that would lead to a worse understanding of the text. And that seems right since Padding and truncation shows that: False or 'do_not_truncate': no truncation is applied. This is the default behavior. ... False or 'do_not_pad': no padding is applied. This is the default behavior. The default for truncation is False anyway, why should I care about truncation? Should I care at all? If the tokenizer.max_length is so very very large, is truncation not just never happening anyway? Will the change to True change the output of the model or not?
