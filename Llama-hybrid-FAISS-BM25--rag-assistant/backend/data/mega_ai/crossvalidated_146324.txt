[site]: crossvalidated
[post_id]: 146324
[parent_id]: 146323
[tags]: 
Linear regression model is $$ y_i = X_i \beta + \varepsilon_i , \ \ \varepsilon_i \sim \mathcal{N}(0, \sigma^2)$$ so as you can see, $\sigma^2$ is error variance. On another hand, you can look at the model as $$ y_i \sim \mathcal{N}(X_i \beta,\ \sigma^2) $$ in this case, $\sigma^2$ is a variance of distribution of $y_i$'s. So if you assume Bayesian point of view, where both you variables and parameters are random variables with a certain distributions, then $\sigma^2$ becomes an additional parameter in your model and not just "error variance" like in traditional OLS regression. In Bayesian approach you get a whole distribution of $y_i$'s, that is described by its mean and variance. So replying to your comment, I don't know what you mean by "ignoring" this value, but what can be said is that it is a parameter of model defined like this that describes distribution of $y_i$'s. You can read more on linear regression in Bayesian approach e.g. on Kruschke's page . See also the JSS paper on MCMCpack ( Martin, Quinn, & Park, 2011 ) for learning more on this library.
