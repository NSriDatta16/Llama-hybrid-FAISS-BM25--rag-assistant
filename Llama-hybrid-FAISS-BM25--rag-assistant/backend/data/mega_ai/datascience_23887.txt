[site]: datascience
[post_id]: 23887
[parent_id]: 
[tags]: 
Outlier detection: Should the metric used in kNN take into account variance explained by each coordinate?

After applying PCA and working with the reduced dataset, I want to delete the outliers. To do this my idea is to compute the kNN-graph and delete those vertices (points) that have an inner degree of 0. That is that they are not in the neighborhood of any other point besides itself. My question is if it is a good idea to instead of working with the $l_2$ distance, I should add weights that take into account the variance explained by each coordinate. The idea is that if the variance explained by coord 1 is 0.93, and by coordinate 2 only 0.05, then the points are expected to be "close together" on the projection over coord 2. It seems natural that the distance on the second coordinate should be much more important than the first one for outlier detection. First approach was to simply use the inverse of the variance explained as weights. Does that idea make sense? If so, what is the proper way to implement it?
