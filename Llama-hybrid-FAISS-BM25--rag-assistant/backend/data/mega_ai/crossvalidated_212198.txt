[site]: crossvalidated
[post_id]: 212198
[parent_id]: 
[tags]: 
What is the significance of a batch size of greater than one?

Why do we use batch sizes in neural networks and deep learning methods such as ConvNets? I mean rather than taking average of several steps and going in that direction which causes smoother movement in the parameter space, what are the benefits that we gain by doing this? Does using batch-size=1 affect the performance badly? does using more batch-size translate to faster convergence?
