[site]: crossvalidated
[post_id]: 451469
[parent_id]: 450634
[tags]: 
I would like to add something to the previous answer - though it's already good (+1). Decision trees implementations normally use Gini index or Entropy for finding splits. These are functions that are maximized when the classes in a node are perfectly balanced - and therefore reward splits that move away from this balance. This means that the splits are always done assuming that the classes distribution is $1/K$ (or 50-50 in the binary case), and this is particularly clear when classes are VERY unbalanced. In those cases, since classification is done by majority voting, most of the leaf nodes will contain majority class elements and the performance will be sub-optimal. There are a number of papers that discuss this issue, but I really suggest reading Using Random Forest to learn Imbalanced Data , that proposes the use of Weighted Gini (or Entropy) to take into account the class distribution, or using a mixture of Under and Over sampling of the classes when bagging decision trees. Indeed, standard trees are mathematically not constructed to deal particularly well with unbalanced data, and some adjustments are needed (to the voting, splitting, or sampling) - which is also why many implementations allow the use of class weights.
