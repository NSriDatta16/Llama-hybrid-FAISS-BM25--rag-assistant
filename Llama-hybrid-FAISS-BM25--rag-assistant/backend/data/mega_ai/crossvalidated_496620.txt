[site]: crossvalidated
[post_id]: 496620
[parent_id]: 496547
[tags]: 
Training word embeddings on sentence and document level typically does not make a huge difference. During training, both the skip-gram and CBOW algorithm makes prediction only in a small sliding window of text (typically 5 words), so most of the prediction will be within sentences anyway. Also, you cannot really hope to learn some document-level features becase the discourse phenomena (such as coreference, stylicistcal cohesion, etc.) occur in much longer distances than the small sliding window.
