[site]: crossvalidated
[post_id]: 185938
[parent_id]: 184272
[tags]: 
Why bias is important? The bias term $b$ is, indeed, a special parameter in SVM. Without it, the classifier will always go through the origin. So, SVM does not give you the separating hyperplane with the maximum margin if it does not happen to pass through the origin, unless you have a bias term. Below is a visualization of the bias issue. An SVM trained with (without) a bias term is shown on the left (right). Even though both SVMs are trained on the same data , however, they look very different. Why should the bias be treated separately? As user logistic pointed out, the bias term $b$ should be treated separately because of regularization. SVM maximizes the margin size, which is $\frac{1}{||w||^2}$ (or $\frac{2}{||w||^2}$ depending on how you define it). Maximizing the margin is the same as minimizing $||w||^2$ . This is also called the regularization term and can be interpreted as a measure of the complexity of the classifier. However, you do not want to regularize the bias term because, the bias shifts the classification scores up or down by the same amount for all data points . In particular, the bias does not change the shape of the classifier or its margin size. Therefore, ... the bias term in SVM should NOT be regularized. In practice, however, it is easier to just push the bias into the feature vector instead of having to deal with as a special case. Note: when pushing the bias to the feature function, it is best to fix that dimension of the feature vector to a large number, e.g. $\phi_0(x) = 10$ , so as to minimize the side-effects of regularization of the bias.
