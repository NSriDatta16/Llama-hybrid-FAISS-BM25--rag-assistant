[site]: crossvalidated
[post_id]: 575645
[parent_id]: 571023
[tags]: 
Cross-validation typically checks how well a model trained on some real data makes predictions on other real data. But that's not happening here. Instead of "external cross-validation," I would call this a "posterior predictive check." That might be a better term to search for online if you'd like to understand how these kinds of checks work. See for instance this answer . The terminology "posterior predictive checks" has been popularized in a Bayesian context by folks like Andrew Gelman, but you don't have to be Bayesian to use the general idea. It seems to be what's described in the paper you are reading: After you fit a model, you generate new data from that model and see if it broadly resembles your original data. These are very useful as a sanity-check on your model-fitting process. For example, if it's important to account for the fact that your real data are discrete counts, but your initial model generates new data that can be fractional or negative, then this kind of check can help you notice that fact and correct it. Think of your posterior predictive checks as a minimum bar that your models should pass. One benefit is that it's a "free" check: you're not "using up" or overfitting to your holdout/validation data when you make these checks. It's debugging, not inference. But that means they are not so useful for making comparisons between well-developed models. Once all your "serious candidates" for models pass this minimum bar, you'll have to use held-out data (or other approaches) to actually compare models or to estimate their predictive performance on future data.
