[site]: crossvalidated
[post_id]: 561299
[parent_id]: 561164
[tags]: 
There's a mathematical result in optimisation, less interesting than it first sounds, called the "No Free Lunch Theorem" . It says that for a discrete problem (like @JonnyLomond's answer), no algorithm can beat random search when its performance is averaged over all possible functions to be optimised. That is, you have a function $f:\Omega\to L$ where $\Omega$ is a finite discrete space (like the space of 1000-character strings) and $L$ is a discrete space of numerical values (like 1:10 or 1:1000000000). There are only finitely many such $f$ . You can define any algorithm that evaluates $f(\omega_1)$ , $f(\omega_2)$ , and so on for $n$ attempts, choosing $\omega_{i+1}$ in terms of earlier results, and then take $\max_i f(\omega_i)$ as your best result. No algorithm will outperform random search averaged over all $f$ . One proof idea is to consider $f$ as randomly chosen from the possible functions with equal probability. Because $f$ could be anything, with equal probability, evaluations at $\omega_1,\ldots,\omega_i$ are independent of $f(\omega_{i+1})$ ; you can't learn anything. This result isn't that interesting because we usually aren't interested in the average performance over all possible objective functions. But it does imply that the reason random search isn't actually a good competitor is because the objective functions we care about have structure. Some have smooth (or smooth-ish) structure -- parameter values near the optimum give better outputs than those far from the optimum. Sometimes the structure is more complicated. But there is (typically) structure. [The no-free-lunch theorem is also perhaps less interesting than it seems because it doesn't seem to have an analogue for continuous parameter spaces]
