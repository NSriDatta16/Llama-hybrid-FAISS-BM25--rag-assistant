[site]: crossvalidated
[post_id]: 357145
[parent_id]: 
[tags]: 
Stochastic gradient descent update

Equation 93 of Chapter 3 of Michael Nielsen's neural networks book describes the stochastic gradient descent update rule as the following: $w \leftarrow (1-\frac{\eta\lambda}{n})w - \frac{\eta}{m}\sum_x \frac{\partial C_x}{\partial w}$ for a mini-batch of size $m$ and individual example cost $C_x$. His cost function is of the form $C = C_0 + \frac{\lambda}{2n}\sum_w w^2$ where $C_0$ is the original unregularized cost function. My question is, why is there a $n$ in the update rule for stochastic gradient descent? Why does the update rule not have $(1-\frac{\eta\lambda}{m})$ instead since we only have $m$ examples? Or is this not really a big deal because that's just the way we defined $C$ and the $\lambda$ term can take care of the difference?
