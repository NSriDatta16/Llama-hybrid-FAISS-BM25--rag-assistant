[site]: datascience
[post_id]: 89243
[parent_id]: 
[tags]: 
ResourceExhaustedError when building Sequential model

i have a big problem when trying to build my model, input shape: (1447, 224, 224, 3) output shape: (1447, 154457) model = Sequential([ Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='valid', input_shape=(224,224,3)), BatchNormalization(axis=-1), Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='valid'), BatchNormalization(axis=-1), Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='valid'), BatchNormalization(axis=-1), MaxPool2D(pool_size=(2, 2), strides=(2, 1)), Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='valid'), BatchNormalization(axis=-1), MaxPool2D(pool_size=(2, 2), strides=(2, 1)), Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='valid'), BatchNormalization(axis=-1), MaxPool2D(pool_size=(2, 2), strides=(2, 1)), Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='valid'), BatchNormalization(axis=-1), MaxPool2D(pool_size=(2, 2), strides=(2, 1)), Conv2D(filters=512, kernel_size=(3, 3), activation='relu', padding='valid'), BatchNormalization(axis=-1), Conv2D(filters=512, kernel_size=(3, 3), activation='relu', padding='valid', strides=3), BatchNormalization(axis=-1), Conv2D(filters=512, kernel_size=(3, 3), activation='relu', padding='valid', strides=3), AveragePooling2D(pool_size=(6, 1), strides=1, padding='same'), ReLU(), BatchNormalization(axis=-1), Flatten(), Dense(154457, activation='relu'), Dense(154457) ]) i got below error, ResourceExhaustedError: OOM when allocating tensor with shape[11264,154457] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:RandomUniform] what is the problem here as i am new to neural network thing.
