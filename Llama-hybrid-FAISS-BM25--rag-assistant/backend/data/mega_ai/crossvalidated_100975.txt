[site]: crossvalidated
[post_id]: 100975
[parent_id]: 
[tags]: 
Choice of test set for classification

I have 50 measurements of 10 descriptors and 1 binary output variable. I want to use a classification procedure to be able to predict the output, so I split the data into a training and a test set and I can then generate my classifier (I am using a decision tree) and test it on the test set. Now, obviously the choice of test set is absolutely arbitrary and, whatever the result of my procedure, I cannot be certain that the result I get for that specific test set is similar to what I will get from any other test set. So, would I have a point in repeating my classification several times, each time with a randomly chosen test/training set, then report the distribution of misclassification errors for my classifier? I understand that this is a bit similar to what Random Forests are doing, but I am wondering if this procedure makes sense also when applied to other type of classifiers, not necessarily decision trees.
