[site]: crossvalidated
[post_id]: 573644
[parent_id]: 573636
[tags]: 
Now for my understanding and also according to this notation used, the parameters for the decoder and the prior have to be the same, since overall we want to compute $p_\theta(x,z)$ with a neural network right? Prior is the distribution for the parameters. Parameters come “from” the prior. Then, how is it possible to choose $p_\theta(z)=\mathcal{N}(0,I)$ while at the same time optimising the parameters $\theta$ computed by the decoder network? What you are optimizing is the posterior probability, so likelihood times prior. You choose the prior a priori , plug-in to the Bayes theorem and optimize. When optimizing you consider the prior. As a third question: if not restricting the prior to be a standard normal distribution, how do we learn the parameters of the prior $p_\theta(z)$ ? You assume them based on your prior knowledge, on what you can expect the distributions of the parameters to be before seeing the data. There's nothing special about Gaussians here. Moreover, the above prior choice is dictated rather by convenience than any deep mathematical argument. It may be easier to understand on practical example, Keras’s blog has a nice tutorial on autoencoders .
