[site]: crossvalidated
[post_id]: 362461
[parent_id]: 
[tags]: 
Is it better to avoid ReLu as activation function if input data has plenty of negative values?

ReLu is probably the most popular activation function in machine learning today. Yet, ReLu function outputs 0 when input data values are negative. ReLu totally disregards negative data. This may result in information loss. Is it better to avoid ReLu as activation function if input data has plenty of negative values?
