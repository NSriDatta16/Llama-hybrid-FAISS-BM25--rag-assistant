[site]: crossvalidated
[post_id]: 133218
[parent_id]: 130867
[tags]: 
In the context of machine learning, inference refers to an act of discovering settings of latent (hidden) variables given your observations. This also includes determining the posterior distribution of your latent variables. Estimation seems to be associated with "point estimation", which is to determine your model parameters. Examples include maximum likelihood estimation. In expectation maximization (EM), in the E step, you do inference. In the M step, you do parameter estimation. I think I hear people saying "infer the posterior distribution" more than "estimate the posterior distribution". The latter one is not used in the usual exact inference. It is used, for example, in expectation propagation or variational Bayes, where inferring an exact posterior is intractable and additional assumptions on the posterior have to be made. In this case, the inferred posterior is approximate. People may say "approximate the posterior" or "estimate the posterior". All this is just my opinion. It is not a rule.
