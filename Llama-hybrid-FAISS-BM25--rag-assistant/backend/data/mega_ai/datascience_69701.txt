[site]: datascience
[post_id]: 69701
[parent_id]: 
[tags]: 
How do you think about neural networks and ways to design new models?

I'm currently learning about neural networks, and it seems to me that there usually is no good theoretical explanation given for why certain architectures work; there is most of the times, no formal argument as to why certain architectures work. The explanation give by professors seems to be more that we devise architectures based on what we think might work; not on some formalized notion. As an example, we think GRUs work because they allow us to remember information from the past, and allows us to forget and remember as we chose. Well, who's to say simple skip level connections don't allow that? basically, is it the case that ANNs will remain blackboxes, and new techniques are basically architectures coming from gut feeling as to what might work?
