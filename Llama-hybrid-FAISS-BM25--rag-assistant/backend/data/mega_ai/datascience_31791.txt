[site]: datascience
[post_id]: 31791
[parent_id]: 9832
[tags]: 
$V^\pi(s)$ is the "state" value function of an MDP (Markov Decision Process). It's the expected return starting from state $s$ following policy $\pi$ : $$V^\pi(s) = E_{\pi} \{G_t \vert s_t = s\} $$ $G_t$ is the total DISCOUNTED reward from time step $t$ , as opposed to $R_t$ which is an immediate return. Here you are taking the expectation over ALL actions according to the policy $\pi$ . $Q^\pi(s, a)$ is the "state action" value function, also known as the quality function. It is the expected return starting from state $s$ , taking action $a$ , then following policy $\pi$ . It's focusing on the particular action at the particular state. $$Q^\pi(s, a) = E_\pi \{G_t | s_t = s, a_t = a\}$$ The relationship between $Q^\pi$ and $V^\pi$ (the value of being in that state) is: $$V^\pi(s) = \sum_{a âˆˆ A} \pi (a|s) * Q^\pi(s,a)$$ You sum every state action-value multiplied by the probability of taking that action (given by the policy $\pi(a|s)$ ). If you think of the grid world example, you multiply the probability of (up/down/right/left) with the one step ahead of the state value of (up/down/right/left).
