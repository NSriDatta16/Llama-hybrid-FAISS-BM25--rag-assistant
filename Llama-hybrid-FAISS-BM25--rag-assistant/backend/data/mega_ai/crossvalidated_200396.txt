[site]: crossvalidated
[post_id]: 200396
[parent_id]: 
[tags]: 
dropout regularization in gbm

Background: Dropout regularization reduces overfitting in Neural networks, especially deep belief networks ( srivastava14a ). It also has the opportunity to accelerate learning because individual learning iterations are on a reduced set of the model. The gradient boosted tree (like those xgboost or gbm) is known for being an excellent ensemble learner, but one that suffers from over-fitting. Question: Is there an analog to dropout regression that is used in GBM learning? Does "subsampling" or "stochastic gradient learning" relate to this - does it count as a version of dropout regression? I think that the stochastic gradient learning is about "randomly disabling inputs" and not "randomly disabling individual learners".
