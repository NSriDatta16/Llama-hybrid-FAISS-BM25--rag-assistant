[site]: crossvalidated
[post_id]: 400948
[parent_id]: 
[tags]: 
Neural nets with custom loss function depending on inputs?

I'm trying to make a custom loss function for my neural network. Each of my data point $i$ has a set of input parameters and an output coefficient $p_i$ . My goal is to have a neural net which minimizes $J=-\prod p_i^{a_i}$ where $a_i$ is $1$ or $0$ , the output of the sigmoid function at the last layer of my network ( $a_i=1$ iff the NN output is $>0.5$ , else $a_i=0$ ). Note that if yout take the log of the function above, I'm trying to opimize $J=-\sum log(p_i)a_i$ where $log(p_i)$ are constants. Currently, I've simplified my problem where $p'_i=1$ iff $p_i\geq1$ , else $p'_i=0$ , which allows me to use the standard loss funcion $J'=-\frac{1}{m}\sum p'_ilog(a'_i)+(1-p'_i)log(1-a'_i)$ where $a'_i$ is the output of the NN. However using a custom loss function should yield better results: my real end goal is to minimize the $J$ described above, so for example it's okay to include data points near $p_i=0.99$ if it includes a lot points $p_i>1$ thus making $-\prod p_i^{a_i}$ lower (so this is binary classification parametrized by the value of $p_i$ ) I really can't figure out how to have custom loss function which are not derivable functions. All the ressources I could find use well defined functions to compute $\dfrac{\partial J}{\partial W_i}$ and $\dfrac{\partial J}{\partial b}$ (where the linear part of the neurons computes $WX+b$ ) (e.g. http://www.cs.cornell.edu/courses/cs5740/2016sp/resources/backprop.pdf ) Could anyone points me toward the right ressources to do so please? Thanks. Edit : after reading the comments, it seems I should use a surrogate function and according to http://www.deeplearningbook.org/contents/optimization.html (p 273), I should use the negative log likelihood. However, doing so in the equation above gives me : $ J = \sum log(p_i)log(a_i) $ and this doesn't make sense to me: I'm OK for the cases when $a_1\approx 0$ , BUT if $a_i\approx1$ then $log(p_i)log(a_i)\approx0$ , and in that case if $log(p_i) , I expected my loss to increase? Using the negative log likelihood seems to make $J$ ignores cases where $a_i=1$ ? Shouldn't the function $$J=\sum log(p_i)log(1-a_i)$$ be more appropriate? Edit 2 : herrrr thats actually the same function, OK, case closed.
