[site]: crossvalidated
[post_id]: 416501
[parent_id]: 
[tags]: 
Proof that auto-correlation function of stationary series depends on constant, if we use linear prediction

Consider question 1.10c of Shumway's book: https://www.stat.pitt.edu/stoffer/tsa4/tsa4.pdf Below the actual statement (I just fixed the greek letter used for auto-correlation function, to be consistent with his own notation; and added a note about constant A): 1.10 Suppose we would like to predict a single stationary series $x_t$ , with zero mean and auto-correlation function $\rho(h)$ at some time in future, say, $t + l$ for $l > 0$ . c) Show that if $x_{t+l} = Ax_{t}$ (for some scalar multiplier $A$ ), then $\rho(l) = 1$ if $A > 0$ , and $\rho(l) = -1$ if $A . My lame attempt to prove it: The auto-correlation function is defined as: $\rho(l) = \dfrac{\gamma(l)}{\gamma(0)}$ where the function $\gamma(h)$ is the auto-covariance function of the stationary time-series defined as: $\gamma(l) = \gamma(l, 0) = \gamma(t+l, t) = cov(x_{t+l}, x_t)$ Now, by definition the covariance of those two random variables $x_{t+l}$ and $x_t$ is: $cov(x_{t+l}, x_t) = E[(x_{t+l} - \mu_{t+l})(x_t - \mu_t)]$ Since we said $x_t$ has zero mean we can simplify to: $E[(x_{t+l} - \mu_{t+l})(x_t - \mu_t)] = E[x_{t+l}\,x_t]$ Now, since we are saying that time $t+l$ is to be predicted with $Ax_t$ , then we can rewrite as: $E[x_{t+l}\,x_t] = E[A x_t\,x_t]$ By linear properties of the expected-value operation we can take the constant $A$ out: $E[A\,x_t\,x_t] = A\,E[x_t\,x_t]$ and then we realize that $E[x_t\,x_t]$ is the (assumed finite) variance of the time-series itself, which per author notation can be written as $\gamma(0)$ ... at least for [weakly] stationary time-series: $A\,E[x_t\,x_t] = A\,cov(x_t, x_t) = A\,var(x_t) = A\,\gamma(0)$ Putting everything together, we have that: $\rho(l) = \dfrac{\gamma(l)}{\gamma(0)} = \dfrac{A\,\gamma(0)}{\gamma(0)} = A$ and above is clearly not what we want, we should be getting something like $sign(A)$ . Where did I screw it up? Thanks. Edit1: Thanks to folks who provided comments. Following the advise I solved it using non-abbreviated definition of auto-correlation function $\rho(l)$ (left side of definition 1.9/equation (1.24) from the book at page 21). That is: $\rho(l) = \dfrac{\gamma(t + l, t)}{\sqrt{\gamma(t+l, t+l)\,\gamma(t,t)}}$ where $\gamma(t,t)$ is still the auto-covariance function; though without using the abbreviation that applies to stationary series (which we should be able to use, but let us hold on that for a bit). With definition above in mind, let us develop a couple of terms in numerator and denominator; leveraging the forecasting technique of $x_{t+l} = Ax_t$ : \begin{align} \gamma(t+l, t) &= cov(x_{t+l}, x_t) \\ &= cov(A\,x_t, x_t) \\ &= A\,cov(x_t,x_t) \\ &= A\, \gamma(t,t) \end{align} and \begin{align} \gamma(t+l, t+l) &= cov(x_{t+l}, x_{t+l}) \\ &= cov(A\,x_t, A\,x_t) \\ &= A^2\,cov(x_t, x_t) \\ &= A^2\,\gamma(t,t) \end{align} Replacing the two above in the definition for auto-correlation function $\rho(l)$ yields: \begin{align} \rho(l) &= \dfrac{\gamma(t + l, t)}{\sqrt{\gamma(t+l, t+l)\,\gamma(t,t)}} \\ &= \dfrac{A\, \gamma(t,t)}{\sqrt{A^2\,\gamma(t,t)\,\gamma(t,t)}} \\ &= \dfrac{A\, \gamma(t,t)}{\sqrt{A^2\,\gamma^2(t,t)}} \\ &= \dfrac{A\, \gamma(t,t)}{\left|A\,\gamma(t,t)\right|} \\ &= \dfrac{A\, \gamma(t,t)}{|A|\,|\gamma(t,t)|} \\ &= \dfrac{A\, \gamma(t,t)}{|A|\,\gamma(t,t)} \\ &= \dfrac{A}{|A|} \\ &= sign(A) \\ \end{align} Which is what the exercise wanted. So the whole problem with my previous reasoning was using right side of definition 1.9/equation (1.24) from the book at page 21; which is: $\rho(l) = \dfrac{\gamma(t + l, t)}{\sqrt{\gamma(t+l, t+l)\,\gamma(t,t)}} = \dfrac{\gamma(l)}{\gamma(0)}$ The numerator equality is fine, as it complies with book conventions for stationary series: $\gamma(t + l, t) = \gamma(l, 0) = \gamma(l)$ As probably suggested by A.M. in his comment, it is the denominator that causes trouble; cause for it to hold we would need that: $\gamma(t+l, t+l) = \gamma(t, t)$ as $\gamma(t,t)$ does follow the pattern of $\gamma(t+h, t) = \gamma(h, 0) = \gamma(h)$ ; hence could be abbreviated as $\gamma(t+0, t) = \gamma(0, 0) = \gamma(0)$ , for our stationary series. But $\gamma(t+l, t+l)$ does not follow same pattern, hence not sure we can assume above equality to hold. Now, I am having trouble calling this a typo in book cause there may be exercises which seem to depend on it (eg see related exercise 1.10b). I still struggle then, what I am missing here?
