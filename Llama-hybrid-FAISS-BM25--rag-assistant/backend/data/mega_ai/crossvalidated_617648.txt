[site]: crossvalidated
[post_id]: 617648
[parent_id]: 
[tags]: 
In Bayesian linear regression Advantages of predictive posterior compared to posterior of model coefficients

In Bayesian linear regression, if we want to get confidence intervals for predictions of a new observation. I was thinking of the following two options. Use the quantiles from samples sampled from the coefficents in the posterior $\beta | \mathbf{y}; \mathbf{X}$ Use the predictive posterior distribution, $y^* \, | \beta, \mathbf{y}; \mathbf{x}^*, \mathbf{X}$ Where $\mathbf{x}^*$ is a new observation. What are the advantages of 2. over 1. If we have the setup: \begin{gather} \mathbf{y} \, | \beta ; \mathbf{X} \sim \mathcal{N}( \mathbf{X} \beta, \mathbf{I} \sigma^2) \\ \beta \sim \mathcal{N}(0, \mathbf{I}\sigma_{\beta}^2) \nonumber \end{gather} Then we can get a closed form solution for the posterior of $\beta$ \begin{equation} \beta \, | \mathbf{y}; \mathbf{X} \sim \mathcal{N}\big((\mathbf{X}^T \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X} \mathbf{y}^T , \; (\mathbf{X}^T \mathbf{X} + \lambda \mathbf{I})^{-1} \sigma^2\big) \end{equation} where $\lambda := \frac{\sigma^2}{\sigma_{\beta}^2}$ . Therefore can see that for a new point $\mathbf{x}^*$ the variance of $(\mathbf{x}^*)^T\beta$ will be $$ (\mathbf{x}^*)^T((\mathbf{X}^T \mathbf{X} + \lambda \mathbf{I})^{-1} \sigma^2)\mathbf{x}^* $$ But, if we compute the predictive posterior: \begin{align*} p(y^* \, | \beta, \mathbf{y}; \mathbf{x}^*, \mathbf{X}) = \int p(y^* \, | \beta; \mathbf{x}^*, \sigma^2) p(\beta \, | \mathbf{y}; \mathbf{X}, \sigma^2, \sigma_{\beta}^2) \; d\beta \end{align*} we can compute this integral and get that the variance is: $$ \sigma^2 + (\mathbf{x}^*)^T((\mathbf{X}^T \mathbf{X} + \lambda \mathbf{I})^{-1} \sigma^2)\mathbf{x}^* $$
