[site]: datascience
[post_id]: 96970
[parent_id]: 
[tags]: 
Counter-intuitive weight in logistic regression

I have a question about logistic regression. One of the features I built is a binary feature which produces either 0 or 1. The number of the positive instances with the feature activated is ~110K while the number of the negative instances is only 57. So, I expected a high positive weight for the feature. However, the learned weight is a high negative. One possible reason could be a high learning-rate which prevented hitting the global minima. Other than that, could there be any other reason? I'm using sklearn. The total # of training instances is 1.3M.
