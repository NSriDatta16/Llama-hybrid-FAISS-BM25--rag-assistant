[site]: crossvalidated
[post_id]: 571666
[parent_id]: 
[tags]: 
Confining the function approximation of NN

I was recently studying about LMSE estimates and came across an idea. Is there a technique to reduce the hypothesis space of a Neural Network by introducing regularisation techniques of some sort similar to how we reduce function spaces in estimates? One method that immediately comes to my mind is to change the activation functions and manipulate them somehow but it isnt so obvious to me how it might work. EDIT: I think my question has been misunderstood. Iterating over learnable activation functions is probably one solution to what I am thinking of. Specifically, I think my question is " Similar to how the LMSE loss searches over an affine subspace of estimators, is there a similar logic in NNs to make them search for an approximation of any arbitrary function using a certain "type" of estimators i.e say only piecewise constant approximations as done by the Haar scaling function in wavelets.
