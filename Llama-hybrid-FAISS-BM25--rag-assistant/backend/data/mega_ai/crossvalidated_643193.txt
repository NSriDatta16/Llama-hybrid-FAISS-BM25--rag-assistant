[site]: crossvalidated
[post_id]: 643193
[parent_id]: 643190
[tags]: 
Per the original paper by Hyndman & Koehler (2006) , you would only use the in-sample one-step-ahead naive MAEs as the MASE denominator. So if you use the first $N-M$ data points to fit your models, that is what you would use. However, different people use other methods to standardize the MAE, most commonly not using the in-sample errors, but the ones in the forecast period. Either way is reasonable, though the "original" way gives you a more stable denominator, since you typically have more training than testing data. In-sample fit is a notoriously poor indicator of true performance in production. This is why we use holdout data. And therefore we would only use the holdout data MAE from the last $M$ periods for the MASE numerator. As with point 2 above, if you use data both for model fitting and evaluation, you will overfit. The result is that you will be too sure of yourself. Your forecasts will be worse than expected in production, where you forecast truly new data. What you can do is to fit your models, evaluate them using a holdout period, select the most accurate one, and refit that one using all data for actual production forecasting. (Even better, fit all models and average forecasts. Averages are often very hard to beat.) There is a plethora of error measures. MASE is not bad at your level of aggregation. If you look at finer granularities, e.g., daily sales per location and SKU, which are typically intermittent, the MASE can lead you astray badly . You may find our resources on forecasting helpful: Resources/books for project on forecasting models
