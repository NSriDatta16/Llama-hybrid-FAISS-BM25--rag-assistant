[site]: crossvalidated
[post_id]: 526410
[parent_id]: 526130
[tags]: 
In the discrete case where there are $n$ actions, the typical setup is to have a neural network $f: S \rightarrow \mathbb{R}^n$ which computes the Q-values of all actions at once. In the continuous case, it isn't possible to enumerate all actions, so $f : S\times A \mapsto \mathbb{R}$ only computes the value of a single state-action pair. It's often desireable to compute a "greedy policy" corresponding to a Q function estimator, that is, $\pi(s) = \text{argmax}_{a \in A} \hat{Q}(s, a)$ . This is difficult in the continuous case because we'd probably have to make many queries to our neural network $f$ in order to find a good approximation of this maximum. Instead, actor critic algorithms get around this by using a separate policy network for rollouts, to avoid having to compute greedy policies. You can read more about DDPG here . (DDPG is a specific instance of actor-critic).
