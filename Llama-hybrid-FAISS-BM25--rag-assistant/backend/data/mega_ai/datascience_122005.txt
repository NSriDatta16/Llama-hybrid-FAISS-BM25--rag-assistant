[site]: datascience
[post_id]: 122005
[parent_id]: 122004
[tags]: 
Language models predict the next token. At training time, we have an input sequence and we want to train the model to output the next token for each possible sequence prefix at the same time , that is, we train the model to generate token 1 based on nothing, token 2 based on token 1, token 3 based on tokens 1 and 2, etc, everything at once. For that, the target output is the same as the input but shifted one position. Note that, due to the self-attention mask of the Transformer model, the prediction at each position only depends on the previous tokens.
