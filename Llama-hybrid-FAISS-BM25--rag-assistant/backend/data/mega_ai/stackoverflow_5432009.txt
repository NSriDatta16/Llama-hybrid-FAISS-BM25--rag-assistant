[site]: stackoverflow
[post_id]: 5432009
[parent_id]: 5431905
[tags]: 
You can use the robots.txt file to prevent complying bots. Next thing you can do, once robots.txt is configured is to check your server logs. Find any useragents that seem suspicious. Let's say you find evil_webspider_crawling_everywhere as a useragent, you can check for it in the headers of the request (sorry, no example, haven't used php in a long time) and deny access to the webspider.
