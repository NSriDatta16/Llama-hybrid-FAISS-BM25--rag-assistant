[site]: crossvalidated
[post_id]: 583166
[parent_id]: 345353
[tags]: 
Adding to Joel's excellent answer, your MLops people will thank you for using a hash instead of a random seed. In many real world applications, you want to continue monitoring and retraining a model that is in production. Or you might want to try using a different programming language or data pipeline in the future. By using a hash function, you know that every observation (past, present, and future) will be reliably and reproducibly categorized, even as new data comes in and you move to a new system. Here's a minimum reproducible example. Imagine I want to see if the mean of an important value has changed since we last updated our model. I don't want to use the holdout data in my analysis, which I'll define as an MD5 hash that starts with a, b, c, d, e, or f. I also don't feel like exporting all the data to my workstation just to look at some averages. I can write a quick SQL query to check the means over time without any holdout contamination. SELECT date, AVG(value) FROM tbl WHERE LEFT(MD5(id),1) IN(0,1,2,3,4,5,6,7,8,9) GROUP BY 1 Now imagine I've downloaded some data into R. I can run the same analysis without having to worry about seeds being consistent between SQL and R. tbl %>% mutate(group = str_sub(md5(id), 1, 1)) %>% group_by(date) %>% summarise(mean(value)) In addition to SQL and R, you could do the same thing with Python , Julia , Beam , BigQuery , etc. My workplace is currently moving from one database system to another. Instead of having to worry about maintaining the same random seed across systems, I just have to Google "generate md5 in new system ."
