[site]: datascience
[post_id]: 31572
[parent_id]: 
[tags]: 
GAN - why doesn't the generator nullify the noise input?

In GAN architecture, during training, what keeps the generator's output dependant on the input noise? Why don't the weights of the noise input become zero (plus a bias)? I would expect the generator to converge to outputting a single picture which is extremely real and non-distinguishable from a real picture, and ignore the noise all together, since this is a "cheaper" way (in convergence time and in number of parameters used) to decrease the generator's loss.
