[site]: crossvalidated
[post_id]: 587258
[parent_id]: 
[tags]: 
Why does the test loss decrease even when the training loss and the validation loss increase

I was trying out different regression models to fit a time series. Models include a multiple linear regression model, ReLU regression models (with varying numbers of ReLU functions) and sigmoid regression models (with varying numbers of sigmoid functions). For example, the sigmoid regression models can be expressed as follows: $$y=b+\sum_i^n c_i \cdot \sigma(b_i+\sum_j^m w_{ij} x_j)=b+{\bf c}^T \sigma({\bf b} + W {\bf x})$$ , with $m$ being the number of features and $n$ being the number of sigmoid functions. All models considered 28 features and used the same set of hyperparameters except that varying numbers of ReLU/sigmoid functions were used in the ReLU/sigmoid regressions. Finally, I got the following result: In the figure, 10 ReLU means that 10 ReLU functions were used ( $m=10$ ). Regarding this result, I have the following questions about interpreting the data: Comparing 10 ReLU and 100 ReLU, the latter has slightly lower training loss and validation loss, so I was expecting that its test loss should be lower as well. However, it turned out to be higher and I'm wondering the reason for this. 1000 ReLU has a lower training loss than 100 ReLU but had a higher validation loss. To my understanding, this seems to be a sign of 1000 ReLU overfitting the data. However, why does 1000 ReLU had a lower test loss than 100 ReLU? I don't understand why 1000 Sigmoid had a much higher training and validation loss than 100 Sigmoid, since I thought that 1000 Sigmoid should have had larger flexibility and might have been overfitting (i.e. lower training loss). To me, it seems weird that in 1000 Sigmoid, the test loss was lower than the training loss and the validation loss. 1000 Sigmoid also had a lower test loss than 100 Sigmoid. I'm wondering the reason for this.
