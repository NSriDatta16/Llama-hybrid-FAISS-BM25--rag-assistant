[site]: crossvalidated
[post_id]: 138948
[parent_id]: 
[tags]: 
Support Vector Machines and the curse of dimensionality

I am reading this paper: "Automated MR image classification in temporal lobe epilepsy", by Focke et al. NeuroImage, 2012 . The authors use support vector machines to classify subjects between healthy controls and patients with epilepsy using magnetic resonance images using leave-one-out cross validation. Every image provides features in the order of hundreds of thousands. However there are only 38 patients and 22 controls (observations). In your opinion, is there any problem/disadvantage with this design? In a situation like this, I would use feature selection and/or dimensionality reduction before classification. It seems intuitive to get rid of uninformative features first. However, I understand that in the boundary optimization problem associated with SVMs, non informative dimensions are be assigned a low weight, even a zero coefficient. Then, is there then any argument for feature selection prior to SVM training other than facilitate convergence of the optimization problem?
