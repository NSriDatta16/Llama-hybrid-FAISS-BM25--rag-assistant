[site]: datascience
[post_id]: 15641
[parent_id]: 15638
[tags]: 
When you are unsure about how something (e.g. RBMs) should really be implemented, it is often useful to look at the code of others. G. Hinton himself has published a MATLAB-script ( here ) which demonstrates the training of an RBM. There, you can see that for each mini-batch, he does the positive phase, then the negative phase, and finally updates the weights - and that's it. So he doesn't iterate between the visible and hidden states. However, this is not the full truth: for the weight updates we need to know the probability $p(v,h)$. This is very complicated to calculate, as it would contain a sum over all possible states of the RBM. There is a "mathematical trick" called Gibbs sampling: it allows us to iterate back and forth between visible and hidden units to calculate this probability $p(v,h)$. But: for the result to be correct, we have to iterate forever , which is not really practical. So what Hinton proposed is to iterate for only 1 step instead (this is $CD_1$), so he only goes back-and-forth once. But, you can also iterate any number of times $k$, which is denoted by $CD_k$. While for Hinton's $CD_1$, you would do visible --> hidden --> visible For a $CD_3$, you would iterate from visible to hidden and back, three times: visible --> hidden --> visible --> hidden --> visible --> hidden --> visible And just to be clear: you run this iteration for every data point in the training set. Actually, you usually make so-called mini-batches of maybe 10 data points, which you run at the same time. Then you calculate the average weight update from this batch. But then, you do this iteration again, and again, and again, until you have finished your training.
