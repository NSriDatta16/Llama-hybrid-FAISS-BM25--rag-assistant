[site]: datascience
[post_id]: 97739
[parent_id]: 97725
[tags]: 
The core principle in supervised machine learning is that the training data is a representative sample of the true distribution (i.e. the possibly infinite full set of instances that could happen). Under this assumption, the intuition and the numerical information gain (or other statistical measure) are expected to be more or less in agreement, because if they disagree it can only mean that: Either the intuition was wrong, because if a feature is intuitively important then there should be evidence of that in the data. Or the data is not a proper representative subset of the true distribution (insufficient or noisy training data). But it's important to keep in mind that a dataset is never perfect and that an intuition is, well, just an intuition. So for example it would be common that the top 3 features A,B,C according to intuition are not exactly the top 3 features according to IG, it might be something reasonably close like B,D,E,A,C for instance. If the data doesn't match the intuition at all, it's worth investigating why. However in general it would be a bad idea to overrule the computed IG value and force the use of "intuitively strong" features, because this is clearly not optimal according to the data.
