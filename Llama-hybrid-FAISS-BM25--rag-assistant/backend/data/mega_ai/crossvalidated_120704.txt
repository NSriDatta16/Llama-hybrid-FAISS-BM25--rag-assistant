[site]: crossvalidated
[post_id]: 120704
[parent_id]: 
[tags]: 
Appropriate correction for tests for determining best of family

I have a sample of $N$ instances of a problem (assumed to be independent samples of an infinite population of instances). To each of these instances I apply $M$ methods, and record how quickly they "solve" the instance. The questions I want to answer, in general, is which is the best method (shortest time)? My initial approach was to determine, for each instance, the best (smallest) of the times across the methods. This was used to normalize the solve times, so I have a dimensionless ratio $\ge 1$ for each method-instance pair. I then log-tranform this ratio, and take the mean across instances for each method (so the geometric mean, essentially). This captures not only the ranking in mean, but ideally, the degree to which a method is best. The problem is, the differences might not be statistically significant. I can do, for each pair of methods (i.e. $O(M^2/2)$), a paired t-test to determine if the their means are different (two-tailed). I interpreted this as enabling me to say things like "at this confidence level, Method B is best, and there is no significant difference between Methods A,C, and D". But then I realized that as I was doing multiple tests, I should be doing something like the Bonferroni correction . I believe that I'd have to then adjust my signifance level for the individual tests to $\alpha/(number\ of\ pairs)$ for this to be a correct statement. My question is: Is the methodology I outlined sound? I only really need to be able to say that one of the methods is better than all the rest. The Bonferroni correction seems very conservative, and I'm almost certain it will find that all my methods are the same. Should I rather than doing all-pairs paired t-tests with two tails be doing $M^2$ one-tailed t-tests? Would be changing the problem to one where I rank the methods, for each instance, be "better" at distinguishing between the methods? If so, what then? I could imagine "averaging" the ranks somehow, then my null hypothesis being that "the rank is this" and the alternative being "it is not this rank" I could imagine a different hypothesis being that "Method X is the best" and doing that $M$ times - but then again I'd surely need to correct for doing $M$ tests. If its any help, I'd rather falsely say the methods are the same (Type 2) than say the methods are different when they are not (Type 1). EDIT: It has just struck me, on further reflection, that my (normalized) data is not at all normal, thus using a t-test is probably a bad idea to begin with and I should probably be using a Wilcoxon signed-rank test instead. But this still begs the question of how to adjust for doing multiple of these.
