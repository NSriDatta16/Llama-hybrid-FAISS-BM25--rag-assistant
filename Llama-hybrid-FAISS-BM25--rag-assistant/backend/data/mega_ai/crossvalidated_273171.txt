[site]: crossvalidated
[post_id]: 273171
[parent_id]: 273161
[tags]: 
As Aksakal mentioned in his answer, the video Ken T linked describes properties of trends , not of models directly, presumably as part of teaching about the related topic of trend- and difference-stationarity in econometrics. Since in your question, you asked about models, here it is in the context of models : A model or process is stochastic if it has randomness. For example, if given the same inputs (independent variables, weights/parameters, hyperparameters, etc.), the model might produce different outputs. In deterministic models, the output is fully specified by the inputs to the model (independent variables, weights/parameters, hyperparameters, etc.), such that given the same inputs to the model, the outputs are identical. The origin of the term "stochastic" comes from stochastic processes . As a general rule of thumb, if a model has a random variable, it is stochastic. Stochastic models can even be simple independent random variables. Let's unpack some more terminology that will help you understand the literature around statistical models (deterministic, stochastic, or otherwise...): Stochastic models do not need to be time-dependent or even Markov processes (dependent on past states, for example $AR(1)$ is first-order Markov since it depends on the state at $t-1$). The linear model you posed above is stochastic (has a random variable) but not Markov (does not depend on past states). In the linear model posed in the question, the error term is a random variable that we assume is uncorrelated (some people go further to state that error is i.i.d.), symmetrically distributed about the mean (some people go further to state that error is normally distributed), and mean zero ($\mu_{\epsilon_t}=0$), etc. We make these assumptions in order to make the linear model useful to estimate the dependent variable(s) by minimizing some norm of that error term. These assumptions allow us to derive useful properties of estimators and prove that certain estimators are the best under those assumptions; for example, that the OLS estimator is BLUE . A simpler example of a stochastic model is flipping a fair coin (heads or tails), which can be modeled stochastically as an i.i.d. uniformly distributed binary random variable, or a Bernoulli process . You can also consider the coin flip as a physical system and come up with a deterministic model (in an idealized setting) if you take into account the shape of the coin, angle and force of impact, distance to the surface, etc. If the latter (physical) model of the coin flip has no random variables in it (e.g. it doesn't consider measurement error of any of the inputs to the model), then it is deterministic. In teaching statistics, there is a common point of confusion between stochasticity and heteroscedasticity . For example, Ken T has confused stochasticity for heteroscedasticity (or variability in variance). A random (stochastic) variable, such as the output variable $X_t$ of an $AR(1)$ process or $\epsilon_t$ in a linear model $y_t = ax_t+\epsilon_t$, is heteroscedastic iff its variance changes over some input, such as time ($t$) in this case, such that different groups within the population have different variances. In the video that Ken T linked (by Ben Lambert), if you pause it at 4:00 (4 minutes), you can see that $Var[X_t]$ in the model on the right side changes with $t$ (heteroscedastic) while $Var[X_t]$ in the linear model is constant (homoscedastic). Furthermore, there is sometimes confusion between stationary stochastic processes and non-stationary stochastic processes. Stationarity implies that statistics such as mean or variance do not change over time in the model. Both are still considered stochastic models/processes as long as there is randomness involved. As fellow Maroon, Matthew Gunn, mentions in his answer, Wold's decomposition states that any stationary stochastic process can be written as the sum of a deterministic and a stochastic process.
