[site]: crossvalidated
[post_id]: 87715
[parent_id]: 
[tags]: 
Computing Standard Errors in EM algorithm

I'm applying the EM to a hidden markov chain (the $\mathbf{Z}=\{Z_1,...,Z_n\}$ variable), with observations(the $\mathbf{Y}=\{Y_0,...,Y_n\}$ variable) dependent not only on the hidden markov chain, but also on the immediate past observation (the $Y_{j-1}$). $$Log(P(\mathbf{Z},\mathbf{Y}|\mathbf{\Theta)})=\log(\pi_{0z_1})+\sum^n_{j=2}\log(\pi_{z_{j-1}z_j})+\sum^n_{j=1}\log(P(y_j|y_{j-1},z_j;\mathbf{\Theta)})$$ In "McLachlan's Finite Mixtures" , page 63 and 64, it talks about how to obtain the Observed Information Matrix, in terms of the complete data Log-Likelihood. My question is how, from the Observed Information Matrix, do we get the standard errors? (Why can one estimate the inverse of the covariance matrix of the MLE, by the Observed Information Matrix?) And could I obtain the std errors in a similar way for a hidden Markov model?
