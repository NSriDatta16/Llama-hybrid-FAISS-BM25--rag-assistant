on of heavy-tailed distributions is relatively difficult. A rule-of-thumb bandwidth estimator If Gaussian basis functions are used to approximate univariate data, and the underlying density being estimated is Gaussian, the optimal choice for h (that is, the bandwidth that minimises the mean integrated squared error) is: h = ( 4 σ ^ 5 3 n ) 1 / 5 ≈ 1.06 σ ^ n − 1 / 5 , {\displaystyle h={\left({\frac {4{\hat {\sigma }}^{5}}{3n}}\right)}^{1/5}\approx 1.06\,{\hat {\sigma }}\,n^{-1/5},} An h {\displaystyle h} value is considered more robust when it improves the fit for long-tailed and skewed distributions or for bimodal mixture distributions. This is often done empirically by replacing the standard deviation σ ^ {\displaystyle {\hat {\sigma }}} by the parameter A {\displaystyle A} below: A = min ( σ ^ , I Q R 1.34 ) {\displaystyle A=\min \left({\hat {\sigma }},{\frac {\mathrm {IQR} }{1.34}}\right)} where IQR is the interquartile range. Another modification that will improve the model is to reduce the factor from 1.06 to 0.9. Then the final formula would be: h = 0.9 min ( σ ^ , I Q R 1.34 ) n − 1 / 5 {\displaystyle h=0.9\,\min \left({\hat {\sigma }},{\frac {\mathrm {IQR} }{1.34}}\right)\,n^{-1/5}} where n {\displaystyle n} is the sample size. This approximation is termed the normal distribution approximation, Gaussian approximation, or Silverman's rule of thumb. While this rule of thumb is easy to compute, it should be used with caution as it can yield widely inaccurate estimates when the density is not close to being normal. For example, when estimating the bimodal Gaussian mixture model 1 2 2 π e − 1 2 ( x − 10 ) 2 + 1 2 2 π e − 1 2 ( x + 10 ) 2 {\displaystyle {\frac {1}{2{\sqrt {2\pi }}}}e^{-{\frac {1}{2}}(x-10)^{2}}+{\frac {1}{2{\sqrt {2\pi }}}}e^{-{\frac {1}{2}}(x+10)^{2}}} from a sample of 200 points, the figure on the right shows the true density and two kernel density estimates — one using the rule-of-thumb bandwidth, and the other using a solve-the-equation bandwidth. The estimate based on the rule-of-thumb bandwidth is significantly oversmoothed. Relation to the characteristic function density estimator Given the sample (x1, x2, ..., xn), it is natural to estimate the characteristic function φ(t) = E[eitX] as φ ^ ( t ) = 1 n ∑ j = 1 n e i t x j {\displaystyle {\hat {\varphi }}(t)={\frac {1}{n}}\sum _{j=1}^{n}e^{itx_{j}}} Knowing the characteristic function, it is possible to find the corresponding probability density function through the Fourier transform formula. One difficulty with applying this inversion formula is that it leads to a diverging integral, since the estimate φ ^ ( t ) {\displaystyle {\hat {\varphi }}(t)} is unreliable for large t's. To circumvent this problem, the estimator φ ^ ( t ) {\displaystyle {\hat {\varphi }}(t)} is multiplied by a damping function ψh(t) = ψ(ht), which is equal to 1 at the origin and then falls to 0 at infinity. The "bandwidth parameter" h controls how fast we try to dampen the function φ ^ ( t ) {\displaystyle {\hat {\varphi }}(t)} . In particular when h is small, then ψh(t) will be approximately one for a large range of t's, which means that φ ^ ( t ) {\displaystyle {\hat {\varphi }}(t)} remains practically unaltered in the most important region of t's. The most common choice for function ψ is either the uniform function ψ(t) = 1{−1 ≤ t ≤ 1}, which effectively means truncating the interval of integration in the inversion formula to [−1/h, 1/h], or the Gaussian function ψ(t) = e−πt2. Once the function ψ has been chosen, the inversion formula may be applied, and the density estimator will be f ^ ( x ) = 1 2 π ∫ − ∞ + ∞ φ ^ ( t ) ψ h ( t ) e − i t x d t = 1 2 π ∫ − ∞ + ∞ 1 n ∑ j = 1 n e i t ( x j − x ) ψ ( h t ) d t = 1 n h ∑ j = 1 n 1 2 π ∫ − ∞ + ∞ e − i ( h t ) x − x j h ψ ( h t ) d ( h t ) = 1 n h ∑ j = 1 n K ( x − x j h ) , {\displaystyle {\begin{aligned}{\hat {f}}(x)&={\frac {1}{2\pi }}\int _{-\infty }^{+\infty }{\hat {\varphi }}(t)\psi _{h}(t)e^{-itx}\,dt\\[1ex]&={\frac {1}{2\p