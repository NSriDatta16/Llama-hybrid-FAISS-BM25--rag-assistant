[site]: crossvalidated
[post_id]: 585509
[parent_id]: 566746
[tags]: 
I had the same doubt as you since I was introduced to time series analysis by the same book. Although it is useful to have an autocorrelation coefficient defined in that way, it may be a good idea to give some clarification in the book itself. So, yes, as you said, the autocorrelation function decreases over the lag axis due to the fact that the global sample variance in the denominator remains the same and that - given a trended series - the actual sample variances of both the series used for the autocorrelation decrease w.r.t. the original one, since a bigger and bigger chunk of the variability at the extreme points of the series is not considered. My idea is that this is useful mainly to distinguish a trended series from one that hovers around a constant value: without this artificial behaviour of the autocorrelation calculation we would not be able to distinguish the two in the ACF, since they would both have a constant ACF of about 1. This is not the case with the formulation using global sample variance instead of the actual sample variances of the lagged and cut versions. Obviously, a non-linear trend induces a decreasing ACF either way.
