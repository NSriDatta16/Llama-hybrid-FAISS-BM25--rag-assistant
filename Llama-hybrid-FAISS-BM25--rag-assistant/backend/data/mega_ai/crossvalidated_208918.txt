[site]: crossvalidated
[post_id]: 208918
[parent_id]: 
[tags]: 
Neuron saturation occurs only in last layer or all layers?

In Chapter 3 of the Neural Networks and Deep Learning book, the text repeatedly states that neuron saturation depends only on the activation function of the output layer and the cost function, such as: "When should we use the cross-entropy instead of the quadratic cost? In fact, the cross-entropy is nearly always the better choice, provided the output neurons are sigmoid neurons." and, "This shows that if the output neurons are linear neurons then the quadratic cost will not give rise to any problems with a learning slowdown. In this case the quadratic cost is, in fact, an appropriate cost function to use." However, it's unclear to me why saturation is only a problem for the output layer. If there are previous hidden layers with sigmoid activations and a quadratic cost function, wouldn't the gradient for those previous layers also have a problem with saturation?
