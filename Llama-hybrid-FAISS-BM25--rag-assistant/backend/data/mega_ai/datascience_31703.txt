[site]: datascience
[post_id]: 31703
[parent_id]: 
[tags]: 
Very Deep Convolutional Networks for Text Classification: Clarifying skip connections

Question RE this research paper if anyone has experience with CNN's, pooling & skip connections: https://arxiv.org/pdf/1606.01781.pdf In figure 1, the input to the first convolutional block has shape (batch_size, 64, s) The output from block 1 must be (batch_size, 64, s) The output from block 2 must be (batch_size, 64, s) However the output from the pooling step has shape (batch_size, 128, s/2). How can a pooling step increase the number of parameters on axis 1 from 64 to 128?! My guess is that the input to the pooling layer is actually a concatenation of 2 of the previous layer's outputs. In this case it would have input shape (batch_size, 128, s). However, the paper does not appear to clearly specify at what outputs are concatenated... Can anyone clarify how this is the case?
