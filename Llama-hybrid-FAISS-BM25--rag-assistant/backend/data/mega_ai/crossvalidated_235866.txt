[site]: crossvalidated
[post_id]: 235866
[parent_id]: 235862
[tags]: 
There are all sorts of local search algorithms you could use, backpropagation has just proved to be the most efficient for more complex tasks in general ; there are circumstances where other local searches are better. You could use random-start hill climbing on a neural network to find an ok solution quickly, but it wouldn't be feasible to find a near optimal solution. Wikipedia (I know, not the greatest source, but still) says For problems where finding the precise global optimum is less important than finding an acceptable local optimum in a fixed amount of time, simulated annealing may be preferable to alternatives such as gradient descent. source As for genetic algorithms, I would see Backpropagation vs Genetic Algorithm for Neural Network training The main case I would make for backprop is that it is very widely used and has had a lot of great improvements . These images really show some of the incredible advancements to vanilla backpropagation. I wouldn't think of backprop as one algorithm, but a class of algorithms. I'd also like to add that for neural networks, 10k parameters is small beans. Another search would work great, but on a deep network with millions of parameters, it's hardly practical.
