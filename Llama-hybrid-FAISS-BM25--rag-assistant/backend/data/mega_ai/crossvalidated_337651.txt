[site]: crossvalidated
[post_id]: 337651
[parent_id]: 
[tags]: 
Quantify robustness of ML estimate from likelihood function

Let's say that for some observation I build a likelihood function. Then the point of maximum of that function will give the ML estimate of the parameters. Is there a way to quantify the "confidence" (I am not sure if this is the right term here) of the estimate from the likelihood function? Indeed, I would expect that a more "impulse-like" likelihood function will give an estimate with higher confidence (more robust), while an almost flat one will be less robust. I could normalise the likelihood function such that its area is one, and then take the entropy. But that doesn't seem like a very elegant approach. Alternatively, I could use some type of measure of flatness of the likelihood function. Is there a more elegant way of doing this?
