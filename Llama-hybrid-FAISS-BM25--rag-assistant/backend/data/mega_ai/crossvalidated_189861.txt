[site]: crossvalidated
[post_id]: 189861
[parent_id]: 
[tags]: 
Fitting a non-linear model where observations at each time are random variables drawn from a different (non-Gaussian) distribution

I have a non-linear (and not clearly linearizable) function of a few parameters that models a response over an independent variable (time): $$ f(t;\lambda_1,\lambda_2,\lambda_3). $$ The function $f$ is the exact solution to a biological model that predicts an organism's response (over time) to series of predetermined stimuli. My research collaborators have run an experiment on each of $N$ organisms where observations are made at fixed time increments, so I'm presented with a data-set $$ Y \in \mathbb{R}^{N\times T} $$ where $N$ is the number experiments run and $T$ is the number of observations made in each experiment. Observations in $y \in Y$ are greater than or equal to zero, and I have good reason to believe (because of how the observations at each time $t$ are computed, and based on some QQ plots I've put together) that the observations at each time are drawn from a gamma distribution. I.E. for each time $t_r \in [1,T]$, we have $N$ observations $\lbrace y_{i r} \rbrace_{i=1}^N$ of an independent variable: $$ X_r \sim \Gamma(k_r,\theta_r) $$ where $\Gamma$ is the pdf of the Gamma distribution and $k_r$ and $\theta_r$ are the shape and scale parameters of the $r$-th distribution. I am trying to find values of $\lambda_i$ that minimize some loss function so that $$ f(t_r;\lambda_1,\lambda_2,\lambda_3) \approx y_{i r} \quad \forall i \in [1,N] \mbox{ and } \forall r \in [1,T]. $$ Question: Is there a standard approach to fitting the parameters $\lambda_i$ in this case? My ultimate goal is to use the parameters from multiple experiments for classification and/or some sort of concentration series analysis. More specifically, are there any glaring errors in the approach I describe below? I am a mathematician entering into the realm of biostatistics, so I could definitely use the advice of a good statistician! Everything I've found so far in the literature seems to assume either a linearizable model and/or Gaussian distributed noise. My approach so far: My first approach was to try and minimize a loss function based on the sum of the log-likelihoods of the gamma distribution associated with observations at each time. The optimization problem was not stable for me, so I tried to break the analysis up a bit as follows. For each time $t_r$ I have around 50 observations, enough I think to estimate the parameters $k_r$ and $\theta_r$ of the gamma distribution at that time. I do so by maximum likelihood estimation, so I have a set of parameters: $$ \lbrace k_r,\theta_r \rbrace_{r=1}^T $$ that specify the assumed gamma distribution at each time $t_k$ based on the observations. Now I want to fit the parameters $\lambda_i$ to somehow minimize a loss function so that $f(t_r;\lambda_i)$ is close to the expected value of $\Gamma(k_r,\theta_r)$ at each $r\in[1,T]$. Recall that the expected value of a gamma distribution is specified by its parameters as $\mu = k\theta$. I've tried three approaches. a. I minimized the mean squared error of the difference between the fitted value of the model and the expected value of the distribution at each time point: $$ \min_{\lambda_i} \sum_r \left(f(t_r;\lambda_i) - k_r\theta_r \right)^2. $$ b. I thought that the above might not be consistent since the error is not normally distributed so I tried do something with the cumulative distribution function of the gamma distribution (call it $F(x;k,\theta)$). My thought is that I can use the CDF to infer the area under the gamma PDF between the expected value on the modeled value. I first tried minimizing the following: $$ \min_{\lambda_i} \sqrt{\sum_r \left( F\left( f(t_r;\lambda_i);\theta_r,\lambda_r\right)-F\left(k_r\theta_r;k_r,\theta_r\right)\right)^2 }. $$ c. I also tried what I did in (b), but in more of an $L_1$ fashion: $$ \min_{\lambda_i} \sum_r \left\lvert F\left( f(t_r;\lambda_i);\theta_r,\lambda_r\right)-F\left(k_r\theta_r;k_r,\theta_r\right) \right\rvert. $$ The optimization in all three approaches above seemed stable... I'm leaning towards (b) based on what I think I know about the distribution of error. A few extra questions: Is this a reasonable approach? Is there a name for this? Am I missing something glaringly obvious to an experienced practitioner? UPDATE I've done a lot of digging on this, and my current approach is actually to find parameters that minimize the total deviance (the difference between the log-likelihood of the fitted model and the log-likelihood of a hypothetical saturated model). I'll write an answer up when I get a chance in the next week if there are no responses, but for now I'll hold off on answering my own question.
