[site]: crossvalidated
[post_id]: 109976
[parent_id]: 
[tags]: 
In convolutional neural networks, how to prevent the overfitting?

Given certain amount of labeled data, we define the net structure, such as number of layers, types of layers, the number of convolutional layers, the number of pooling layers, etc. And train the parameters using back propagation, meanwhile we show the loss in training procedure and view the testing accuracy in validating data set. But, the loss in training set is nearly zero, and the testing accuracy is kept unchanged no matter how to decrease the learning rate. In this circumstance, is it overfitting? Should we change the net structure? More layers for more parameters? Could you please recommend some suggestions or references?
