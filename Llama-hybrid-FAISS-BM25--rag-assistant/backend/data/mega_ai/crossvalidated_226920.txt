[site]: crossvalidated
[post_id]: 226920
[parent_id]: 
[tags]: 
Selecting the best performing model for time series analysis

Right now I am using some machine learning techniques (in particular SVMs) to fit to some time series data. The time series version of k-fold cross validation is walk-forward analysis with a large enough window. This solves the problem of model verification, but what about model parameter selection? Suppose we have a nonstationary process that we want to model. For parameter selection we can use a grid search. However, if we train on the data at time $t_0,\dotsc,t_{n-1}$ and then test on $t_n,\dotsc,t_{m-1}$ and repeat this process over and over again fitting on a bunch of different subsets of the data in sequence, does this really tell us what the best parameters are? It's possible the data at $t_0,\dotsc,t_{n-1}$ is old and no longer fully reflects the behavior of future data. So if a model that did extremely well too far in the past exists, it could accidentally beat out a model that has worse past performance, but better overall recent performance. My solution to this is that we choose a set of parameters, and then cross-validate the parameters (either with walk-forward analysis or another technique) and then use the score coming from that to determine the best parameters. We repeat this process for all of the parameters we are grid searching on and then choose the best. Is this the best way to handle this problem, or is there a better way? Any links to papers/websites discussing this would be awesome. Rob J. Hyndman had an excellent blog post "Why every statistician should know about cross-validation" that made me think about this.
