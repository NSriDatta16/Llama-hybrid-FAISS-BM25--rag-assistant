[site]: crossvalidated
[post_id]: 175227
[parent_id]: 
[tags]: 
Equivalence between predictive distribution of Bayesian regression and Gaussian process with linear kernel

I am attempting to understand the algebraic relationship between a Gaussian process with linear kernel covariance and a Bayesian regression. I know they are equivalent formulations but I seem to be missing something. Here's what I got so far: $Y^{*} \:| \:X, X^{*} \sim Normal (\mu{*}, \Sigma^{*})$ Where $\mu^{*} = K(x,x^{*})(K(x^{*},x^{*}) + \sigma^{2}I)^{-1}Y$ $\:\:$ (for data with 0 mean) and $\Sigma^{*} = (K(x,x)+\sigma^{2}I) - K(x,x^{*})(K(x^{*},x^{*}) + \sigma^{2}I)^{-1}(K(x^{*},x))$ Now, for a linear kernel, $K(x,x^{*}) = x^{T}x^{*}$ I can replace all my $K$'s above with this expression to get: $\mu^{*} = x^{T}x^{*}(x^{*T}x^{*} + \sigma^{2}I)^{-1}Y$ And I can do the same for $\Sigma^{*}$. It's at this point where I am stuck. What am I missing to see that this mean and covariance are the same as a mean and covariance for a predictive Bayesian linear regression? I've looked at a number of references which discuss this relationship and I understand it conceptually, but I cannot get the algebra to work out. Am I formulating the problem incorrectly? Edit I think I answered my own question - I suspect that for linear kernels, the covariance term $x^{*T}x$ is 0 because the new data, $x^{*}$ and observed data $x$ are independent. Therefore your $\mu^{*}$ reduces to 0, which makes intuitive sense since we're assuming that all the observations have mean 0. Though I suppose that can just insert $\mu_{observed}$ if they were non-zero. And your $\Sigma^{*}$ reduces to $x^{*T}x^{*} + \sigma^{2}$ which makes sense - just the sum of the sample variance and the variance of the predicted term. I suppose with a non-linear kernel this wouldn't work out as nicely. If anyone can opine on my conclusion, I'd greatly appreciate it.
