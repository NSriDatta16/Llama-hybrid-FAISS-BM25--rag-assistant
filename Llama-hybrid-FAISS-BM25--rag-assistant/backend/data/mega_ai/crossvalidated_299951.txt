[site]: crossvalidated
[post_id]: 299951
[parent_id]: 299666
[tags]: 
He's referring to how ReLU isn't bounded from above, which allows the training process to handle all (positive) activation of a neuron according to how activated it is, as opposed to tanh for example which provides 0 gradient when "saturated" by large values. ReLU is good for convolutional neural networks for example because the process looks for the presence of features as it scans across an image, which manifests as large outputs of the matrix convolution operation. If we used a bounded activation function, we'd be discarding all the information that tells us how "large" of a presence some feature had vs another arbitrary presence.
