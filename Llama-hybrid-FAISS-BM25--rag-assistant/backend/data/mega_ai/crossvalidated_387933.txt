[site]: crossvalidated
[post_id]: 387933
[parent_id]: 387922
[tags]: 
First of all, let's remember that model interpretation is always contextual (depends on your data and what are you trying to achieve). However, I'll try to explain some general points. You're in a tricky scenario. The low R squared means that your model only explains 7% of the dependent's variation . Nonetheless, the low p-value means that your model fits very well the data , and therefore the probability of your coefficient actually being zero is very low. If we plot the regression line and your data points, it would look something like this: Notice how the regression line (in red) predicts accurately the general trend of the data points , i.e., a positive tendency: the higher the independent variable, the higher the dependent value (this is the low p-value). But the data points are not very close from the regression line, they are heavily scattered around it (this is the low R-squared). Given this information, let's try to interpret your model (since I don't know what your independent variables actually mean, I'll be as general as possible). Take the variable Friday as an example. If your other variables remain constant, a change in Friday (0 to 1) means that there's an average change in Inflow of about 804.4 units. This would be the same if your model has an R squared of 0.99 or a R squared of 0.01 . However, if you were to use this model to predict Inflow with new values for your independent variables, your prediction interval will be fairly high. This is the only point where R squared matters in retrieving useful info from your model. Now, my friend. A low R-squared is not necessarily bad (and they are fairly common in Social Science research). Yes, a model with a higher R squared would be better, but your model fits the data very well and can give you very useful information already on the relationship between your independent variables and your dependent variable. Edit: High R squared values are not Inherently good. , your model can account for a high proportion of the dependent's variation, but have coefficients that underestimate or overestimate the data distribution. Take a look a this exemple: Although there's a R square of 0.985, you can clearly see that the regression is biased (at some points it overestimates, while at others it underestimates). I'd recommend that to better assess your model, you should plot the fitted values vs the observed values. You can also plot the standardized residuals, to better picture what you're dealing with. Also, to improve your model (not necessarily your R squared), keep in mind that your dependent variable may be affected by other factors that are not currently in your. Also you can try some form of factorial ANOVA.
