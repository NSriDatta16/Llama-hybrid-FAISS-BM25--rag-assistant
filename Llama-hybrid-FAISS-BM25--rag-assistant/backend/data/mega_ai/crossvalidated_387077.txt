[site]: crossvalidated
[post_id]: 387077
[parent_id]: 386982
[tags]: 
There was, historically, disagreement about whether an alternative hypothesis was necessary. Let me explain this point of disagreement by considering the opinions of Fisher and Neyman, within the context of frequentist statistics, and a Bayesian answer. Fisher - We do not need an alternative hypothesis; we can simply test a null hypothesis using a goodness-of-fit test. The outcome is a $p$ -value, providing a measure of evidence for the null hypothesis. Neyman - We must perform a hypothesis test between a null and an alternative. The test is such that it would result in type-1 errors at a fixed, pre-specified rate, $\alpha$ . The outcome is a decision - to reject or not reject the null hypothesis at the level $\alpha$ . We need an alternative from a decision theoretic perspective - we are making a choice between two courses of action - and because we should report the power of the test $$ 1 - p\left(\textrm{Accept $H_0$} \, \middle|\, H_1\right) $$ We should seek the most powerful tests possible to have the best chance of rejecting $H_0$ when the alternative is true. To satisfy both these points, the alternative hypothesis cannot be the vague 'not $H_0$ ' one. Bayesian - We must consider at least two models and update their relative plausibility with data. With only a single model, we simple have $$ p(H_0) = 1 $$ no matter what data we collect. To make calculations in this framework, the alternative hypothesis (or model as it would be known in this context) cannot be the ill-defined 'not $H_0$ ' one. I call it ill-defined since we cannot write the model $p(\text{data}|\text{not }H_0)$ .
