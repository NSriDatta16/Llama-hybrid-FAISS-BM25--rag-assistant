[site]: crossvalidated
[post_id]: 64202
[parent_id]: 
[tags]: 
Best method of calculating line of best fit / extrapolate to compensate for delays

Let's suppose there is a project which is expected to take a certain amount of time to complete. As certain jobs are done, we can quantifiably measure how much of the project has been completed at any time. Consider a progress bar - it starts at 0%, and is updated at some discrete time values as parts of the project are completed, until it reaches 100%. It is expected that this completion percentage will increase linearly over time. While the project has yet to be completed, it seems logical that the best way of estimating the final completion date based on this data is to plot the (time,%) values, calculate a line of best fit, and find where that intersects the 100% value. As time goes on, these approximations should get closer and closer to the true value, oscillating around it on both sides. However, let's say some small part of the project (for example, a 5% range) gets delayed (or perhaps gets completed faster than expected). This period is going to drag down (or up) the line of best fit every single time it is calculated - meaning that our estimated completion date is going to be permanently too low, and will increase each time we calculate it, rather than oscillating about the true value like you'd expect. Is there a better method of estimating the completion time that can take into account local periods of faster or slower increase? Some sort of moving average may smooth the data, but doesn't seem logical with the variation not being cyclic. It is more like you'd want a piece-wise model - but how could you calculate the actual pieces and use this information appropriately without just looking at the graph and eyeballing where it 'seems' different?
