[site]: crossvalidated
[post_id]: 498968
[parent_id]: 498858
[tags]: 
No, the decoder cannot generate the outputs in parallel. It generates the output autoregressively, i.e., the probability distribution of the $i+1$ -th token is conditioned on the $i$ -th output token. At the training time, this is simulated by the triangle maks in the self-attention that prevents the decoder from attending to the tokens that will be generated in the future (this is discussed in the Decoder paragraph in Section 3.1 of the paper). At the inference time, the triangle maks is not needed because the future words simply are not there. Methods for generating all outputs parallel exist. This is called non-autoregressive machine translation, but in this case, you need to deal with the fact output tokens become conditionally independent. This can be solved e.g. using the CTC loss or by iterative masking and improving the output .
