[site]: crossvalidated
[post_id]: 142732
[parent_id]: 142263
[tags]: 
In a typical LSTM block forget gate computes its value based on its inputs (from layer below) and weights associated with each input. Weights are trained, usually by gradient descent using gradients that are computed by backpropagation. You can think of forget gate as simple logistic regression classifier that is trained to classify inputs into two classes (forget/not forget). Hovewer, in LSTM case one don't expicitly assign classes to inputs because output error is backpropagated from above units. Thus, one can (informally) say that forget gate learns optimal time to forget by adjusting weights on its inputs in a way that minimizes overall network output error.
