[site]: datascience
[post_id]: 100160
[parent_id]: 
[tags]: 
How is attention different from linear MLPs?

Each output for both the attention layer (as in transformers) and MLPs or feedforward layer(linear-activation) are weighted sums of previous layer. So how they are different?
