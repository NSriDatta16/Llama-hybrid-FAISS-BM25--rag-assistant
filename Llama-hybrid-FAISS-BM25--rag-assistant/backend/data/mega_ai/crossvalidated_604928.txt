[site]: crossvalidated
[post_id]: 604928
[parent_id]: 
[tags]: 
How to Prove that Multicollinearity is "Bad"?

Multicollinearity ( https://en.wikipedia.org/wiki/Multicollinearity ) is when two or more predictor variables are heavily correlated with each other. As an example, if you have "weight" and "height" as predictor variables in a regression model - if you were to include the "ratio of weight and height" as a new predictor variable, you might now have multicollinearity in your model. When multicollinearity is present, it is said that this can result in problems. For example: If there is a high degree of multicollinearity, the OLS matrix can become non-invertible which leads to not being able to estimate the regression coefficients (I am not sure as to why multicollinearity makes the matrix non-invertible). I think this is what is referred to as "non-identifiability" If there is a high degree of multicollinearity, the estimates of the regression coefficients are said to become "unstable" and significantly change when re-estimated using new data (but again, I am not sure why this happens) My Question: Can someone please suggest how we can use demonstrate (e.g. using linear algebra) that multicollinearity results in these two adverse phenomena? Thanks! Note: Apparently techniques such as (LASSO) Regularization and models such as Random Forest are believed to be less affected by multicollinearity. I can understand that in the case of Random Forest, sets of predictor variables with high multicollinearity might be "ignored over time" - but I am not sure why Regularization (e.g. pushing some of the regression coefficients to 0) can reduce multicollinearity (e.g. how do we know that Regularization will specifically concentrate on the multicollinear predictors?) References: https://www.stat.cmu.edu/~larry/=stat401/lecture-17.pdf https://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/lecture-17.pdf
