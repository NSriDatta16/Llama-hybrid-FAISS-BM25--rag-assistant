[site]: datascience
[post_id]: 80091
[parent_id]: 80072
[tags]: 
Although the first answer has explained the difference, I will add a few other points. If the model is very deep( i.e. a lot of Pooling ) then the map size will become very small e.g. from 300x300 to 5x5 . Then it is more likely that the information is dispersed across different Feature maps and the different elements of one feature map don't hold much information. So you are reducing the dimension which will eventually reduce the number of parameters when joined with the Dense layer . Excerpt from Hands-On Machine Learning by Aurélien Géron the global average pooling layer outputs the mean of each feature map: this drops any remaining spatial information, which is fine because there was not much spatial information left at that point. Indeed, GoogLeNet input images are typically expected to be 224 × 224 pixels, so after 5 max pooling layers, each dividing the height and width by 2, the feature maps are down to 7 × 7. Moreover, it is a classification task, not localization, so it does not matter where the object is. Thanks to the dimensionality reduction brought by this layer, there is no need to have several fully connected layers at the top of the CNN (like in AlexNet), and this considerably reduces the number of parameters in the network and limits the risk of overfitting. Is this specific to transfer learning? You can apply this concept to your own model too and test the result/parm count for different cases i.e. small/large Model . Anyway, Transfer learning is just a special case of Neural Network i.e. you are continuing to use an already trained model. Adding a depiction for the difference in approach
