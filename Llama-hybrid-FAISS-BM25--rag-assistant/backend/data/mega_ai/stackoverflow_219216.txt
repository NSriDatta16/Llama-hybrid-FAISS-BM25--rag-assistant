[site]: stackoverflow
[post_id]: 219216
[parent_id]: 
[tags]: 
How would you approach this data processing task?

I have a file containing 250 million website URLs, each with an IP address, page title, country name, server banner (e.g. "Apache"), response time (in ms), number of images and so on. At the moment, these records are in a 25gb flat file. I'm interested in generating various statistics from this file, such as: number of IP addresses represented per country average response time per country number of images v response time etc etc. My question is, how would you achieve this type and scale of processing, and what platform and tools wuld you use(in a reasonable time)? I am open to all suggestions, from MS SQL on Windows to Ruby on Solaris, all suggestions :-) Bonus points for DRY (don't repeat yourself), I'd prefer not to write a new program each time a different cut is required. Any comments on what works, and what's to be avoided would greatly be appreciated.
