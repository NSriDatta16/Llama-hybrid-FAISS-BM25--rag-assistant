[site]: datascience
[post_id]: 92269
[parent_id]: 
[tags]: 
BERT Masked Language Model question

I have been reading about BERT from the internet, and from what I understand the point of masked language modelling for BERT pretraining is so that BERT will learn to guess a "masked" word from the context given. The loss function will be the lowest for output embeddings which are closest to the original masked word embedding. Wouldn't it be that using this loss funtion does not guarantee that BERT will output word embeddings with context and instead could just output an embedding closest to the original masked word embedding without the relevant context? Thanks.
