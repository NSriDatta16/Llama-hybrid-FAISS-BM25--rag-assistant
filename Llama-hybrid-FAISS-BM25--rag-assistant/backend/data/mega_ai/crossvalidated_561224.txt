[site]: crossvalidated
[post_id]: 561224
[parent_id]: 561151
[tags]: 
First of all, unless you're rather confident that the three time series exhibit the exact same patterns/dynamics, then I would advise against training a single model on all 3. You should do separate ones - since there are presumably different patterns to learn. To your main question, though - how to compare prediction errors? Unless there's some larger objective you're trying to achieve by which to measure success (such as some kind of trading based on predictions), I don't think there's a truly objective way to compare prediction errors, as you're (forgive me) literally comparing apples to oranges if the prediction problems are different - the patterns to be learned are different, the scales are different, the variance is different etc. It'll be some kind of judgement call (because the distribution you're predicting is just different from one case to another). If you scaled by normalizing, then I don't see a problem with comparing the RMSE of the scaled predictions of the 3 time series. I think the proposed scaled error is certainly worth a try! It seems that conceptually, using the scaled error as described in the chapter is like scaling not only for the units, but also for the "difficulty" of the problem, since you're scaling by the error of a basic model (so if a basic model does well, it means it's a relatively easy/predictable prediction problem and vice versa).
