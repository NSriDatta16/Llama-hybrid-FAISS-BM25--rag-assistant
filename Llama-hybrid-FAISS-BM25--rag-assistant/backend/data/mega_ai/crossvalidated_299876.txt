[site]: crossvalidated
[post_id]: 299876
[parent_id]: 
[tags]: 
softmax+cross entropy compared with square regularized hinge loss for CNNs

SVM is actually a single layer neural network, with identity activation and squared regularized hinge loss, and can be optimized with gradients. In addition, squared regularized hinge loss can be transformed into dual form to induce kernel and find the support vector. Compared with softmax+cross entropy, squared regularized hinge loss has better convergence and better sparsity. Why softmax+cross entropy is more dominant in neural network? Why not use squared regularized hinge loss for the CNN?
