[site]: crossvalidated
[post_id]: 224169
[parent_id]: 223962
[tags]: 
Yes, your reasoning is correct, and you could make both mentioned variants work. Some things I'd like to point out in short: Think about normalizing the data you obtain from whichever encoding you use. This will have different impact on different models (e.g. more impact on logistic regression than tree based models). For the standard approach: there are multiple possibilities used out there. As you pointed out, e.g. tree based models can naturally deal with such data, and both categorial and one-hot encoded variables are actually used with it (see e.g. Applied Predictive Modeling by Max Kuhn and Kjell Johnson ). But if you are dealing with other models, one-hot encoding ore similar might become necessary, as the model might infer different/wrong information from such features otherwise. If you happen to have many features, think about employing features selection first (e.g. feature filters, feature wrappers). You could also use such after one-hot encoding variables.
