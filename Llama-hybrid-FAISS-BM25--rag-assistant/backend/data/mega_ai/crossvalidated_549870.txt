[site]: crossvalidated
[post_id]: 549870
[parent_id]: 549741
[tags]: 
Maximum likelihood is about maximizing the likelihood function to find the point estimate of the parameters $$ \hat \theta_\text{ML} = \operatorname{arg\,max}_\theta \; p(X | \theta) $$ In the case of Bayesian inference, we are looking for posterior distribution of the parameters, i.e. we consider also the prior $p(\theta)$ $$ p(\theta | X) = \frac{ p(X | \theta) \, p(\theta) }{ \int \, p(X | \theta) \, p(\theta) \,d\theta } $$ the problematic part is usually calculating the integral in the denominator . Hopefully, we can approximate the posterior distribution by using Markov Chain Monte Carlo to sample from it and treat the empirical distribution of the samples as an approximation of the posterior. There are also other approximations possible. Maybe you don't need to know the full posterior distribution and knowing the mode of it would be enough? In such a case, you can use maximum a posteriori estimation $$ \hat \theta_\text{MAP} = \operatorname{arg\,max}_\theta \; p(X | \theta) \, p(\theta) $$ (We don't need the normalizing constant for optimization.) Another approximation is Laplace approximation , where we approximate the posterior distribution with Gaussian distribution. In such a case, we get something in-between using MAP and full Bayesian inference, because we are left with distribution, but not an exact one, while it is just a little bit more complicated to do than using MAP alone.
