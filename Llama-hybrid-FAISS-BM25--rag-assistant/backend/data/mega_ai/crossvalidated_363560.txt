[site]: crossvalidated
[post_id]: 363560
[parent_id]: 317412
[tags]: 
Random forests (RFs) and gradient boosting machines (GBMs, also called boosted regression trees) can do this. Both are methods based on ensembles of regression trees. I should note that there are certainly other methods that can be used for problems such as this, but I'll focus on these since I am somewhat familiar with them. In the case of RFs, there are multiple approaches used that depend on the implementation: NAs may either be excluded or handled by imputation (based on the mean/median value, or value of similar points). GBMs (perhaps not universally, but in the implementations I've seen) handle NAs in a different way: through surrogate splits. This is explained in Tierney et al. (2015) as: [Surrogate split] means that when a value for a variable is missing, and that variable needs to be used to determine a split, an alternative variable that is highly correlated with the missing variable is used to determine the direction of the split. My impression is that the GBMs do a better job of handling missing data, but I don't have a lot of evidence for that claim. Tierney, N. J., Harden, F. A., Harden, M. J., & Mengersen, K. L. (2015). Using decision trees to understand structure in missing data. BMJ Open, 5(6), e007450. http://doi.org/10.1136/bmjopen-2014-007450
