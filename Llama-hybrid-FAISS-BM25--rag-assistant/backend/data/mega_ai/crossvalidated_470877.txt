[site]: crossvalidated
[post_id]: 470877
[parent_id]: 470246
[tags]: 
The repeated 5x means you do the cross validation process 5 times. With one CV, you split the data into 10 parts, train on 9 parts test on 1 part. This is based on 1 partitioning of the data. You can now resample your data and partition again into a different 10 parts. This is the "repeated". To answer your question is how does the "repeated 5x" part come into play? Each repeat has its own coefficient estimates. So did it just literally average the 5 to give you final "best model"? Instead of having 10 datapoints to estimate the metric (for example accuracy), you now have 5x10 datapoints because you repeated the CV 5 times. It calculates the mean accuracy across these 50 data points for each combination and chooses, the combination with the best metric, for example, highest accuracy. We can do an example: library(caret) data = iris data $Species = factor(ifelse(data$ Species=="versicolor",1,0)) mdl1 = train(Species ~ .,data=data,method="glmnet", trControl=trainControl(method="cv",number=10),tuneLength=3) mdl1$resample Accuracy Kappa Resample 1 0.7333333 0.3333333 Fold08 2 0.6000000 0.0000000 Fold04 3 0.7333333 0.4000000 Fold07 4 0.6666667 0.2857143 Fold10 5 0.6000000 0.1000000 Fold03 6 0.7333333 0.3333333 Fold06 7 0.6666667 0.1176471 Fold09 8 0.8000000 0.4705882 Fold01 9 0.8000000 0.5263158 Fold02 10 0.9333333 0.8421053 Fold05 The above shows you the accuracy across each fold, for the final model. We can calculate the average and you can see it corresponds to the final result: mean(mdl1 $resample$ Accuracy) [1] 0.7266667 mdl glmnet 150 samples 4 predictor 2 classes: '0', '1' No pre-processing Resampling: Cross-Validated (10 fold, repeated 5 times) Summary of sample sizes: 135, 135, 135, 135, 135, 135, ... Resampling results across tuning parameters: alpha lambda Accuracy Kappa 0.10 0.0004409547 0.7293333 0.3464136 0.10 0.0044095469 0.7293333 0.3483365 0.10 0.0440954694 0.7080000 0.2604608 0.55 0.0004409547 0.7280000 0.3443084 0.55 0.0044095469 0.7280000 0.3463114 0.55 0.0440954694 0.7133333 0.2674701 1.00 0.0004409547 0.7266667 0.3426221 1.00 0.0044095469 0.7266667 0.3441009 1.00 0.0440954694 0.7173333 0.2750243 Now we do repeated, you can see that the data has now 50 entries,compared to 10 previously: mdl2 = train(Species ~ .,data=data,method="glmnet", trControl=trainControl(method="repeatedcv",number=10,repeats=5),tuneLength=3) head(mdl2$resample) Accuracy Kappa Resample 1 0.8666667 0.6666667 Fold04.Rep5 2 0.8666667 0.6666667 Fold05.Rep5 3 0.7333333 0.4000000 Fold01.Rep4 4 0.8000000 0.5263158 Fold02.Rep4 5 0.6666667 0.2105263 Fold02.Rep5 6 0.7333333 0.4000000 Fold09.Rep2 dim(mdl2$resample) [1] 50 3 mean(mdl2 $resample$ Accuracy) [1] 0.7306667 mdl2 glmnet 150 samples 4 predictor 2 classes: '0', '1' No pre-processing Resampling: Cross-Validated (10 fold, repeated 5 times) Summary of sample sizes: 135, 135, 135, 135, 135, 135, ... Resampling results across tuning parameters: alpha lambda Accuracy Kappa 0.10 0.0004409547 0.7226667 0.3428366 0.10 0.0044095469 0.7306667 0.3564700 0.10 0.0440954694 0.7146667 0.2772609 0.55 0.0004409547 0.7213333 0.3402191 0.55 0.0044095469 0.7280000 0.3505853 0.55 0.0440954694 0.7120000 0.2589946 1.00 0.0004409547 0.7213333 0.3402191 1.00 0.0044095469 0.7293333 0.3530414 1.00 0.0440954694 0.7146667 0.2639771 Accuracy was used to select the optimal model using the largest value. The final values used for the model were alpha = 0.1 and lambda = 0.004409547.
