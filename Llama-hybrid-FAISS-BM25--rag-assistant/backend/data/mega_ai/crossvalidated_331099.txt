[site]: crossvalidated
[post_id]: 331099
[parent_id]: 
[tags]: 
K-fold cross validation: confusion over which model is actually chosen and validated

I am watching the online lectures of Machine Learning by Hastie and Tibshirani, which are quite good. But they discussed $k$-fold cross-validation and there is something confusing me. As they describe it, the analyst will divide the dataset into $k$ equal partitions and then use $k-1$ partitions to train the model. Once the model is trained, the system will capture the mean squared error for the model applied to the $k$th partition that was used as a testing set. The analyst repeats this process for each of the $k$ folks, whether that be 5 or 10, etc. Now, in one of their quiz questions--image below--they suggest that in each folk of the cross-validation, the model is retrained or refit to the data. So you may actually have different predictors in the model from one fold to another. Different predictors means a different model, right. But even if the predictors are the same from fold to fold, you are almost certain to get different coefficients on the predictors from fold to folks, so again the models will be different. So my question is what are we doing when we cross-validate? If the model keeps changing from one fold to the next--which may not happen often but can--then will cross-validation tell us how a particular model performs against test data? No, since we are testing multiple different models. Are we just supposed to pick the model that performs the best out of all the k-folds and test it against all folds again? Again, just wanted to clarify how is k-fold cross validation supposed to work if the models change from fold to fold.
