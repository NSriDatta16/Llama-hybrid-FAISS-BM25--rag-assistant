[site]: stackoverflow
[post_id]: 2228712
[parent_id]: 2228621
[tags]: 
Your example will block all all the files in root. There isn't a "standard" way to easily do what you want without specifying each folder explicitly. Some crawlers however do support extensions that will allow you to do pattern matching. You could disallow all bots that don't support the pattern matching, but allow those that do. For example # disallow all robots User-agent: * Disallow: / # let google read html and files User-agent: Googlebot Allow: /*.html Allow: /*.pdf Disallow: /
