[site]: datascience
[post_id]: 115320
[parent_id]: 
[tags]: 
Why is gradient boosting better than random forest for unbalanced data?

I've searched everywhere and still couldn't figure this one out. This post mentioned that Gradient Boosting is better than Random Forest for unbalanced data. Why is that? Is Random Forest worse because of bootstrapping (perhaps this wouldn't get a stratified sample during training, idk)? Any thoughts? Thanks in advance
