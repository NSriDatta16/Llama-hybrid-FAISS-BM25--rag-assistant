[site]: crossvalidated
[post_id]: 35287
[parent_id]: 35276
[tags]: 
I will start with the second and last questions. The problem of generalization is obviously important, because if the results of machine learning cannot be generalized, then they are completely useless. The methods of ensuring generalization come from statistics. We usually assume, that data is generated from some probability distribution that originates in reality. For example if you're a male born in year 2000, then there is a probability distribution of what is your weight / height / eye colour when you reach 10, which results from available gene pool at year 2000, possible environmental factors etc. If we have lots of data, we can say something about those underlying distributions, for example that with high probability they are gaussian or multinomial. If we have accurate picture of distributions, then given height , weight and eye colour of a 10 year old kid in 2010, we can get a good approximation of the probability of the kid being male. And if the probability is close to 0 or 1 we can get a good shot at what the kids sex really is. The most important part of this whole thing is: we assume there is some probability distribution that generates both training data, test data, and the real world data we would like to use our algorithm on. More formally, we usually try to say that if the training error is $k$ then with high probability ($\delta$) the error on some data generated from the same distribution will be less than $k + \epsilon$. There are known relations between size of the training set, epsilon and the probability of test error exceeding $k+ \epsilon$. The approach I introduced here is known as Probably Approximately Correct Learning, and is an important part of computational learning theory which deals with the problem of generalization of learning algorithms. There are also number of other factors that can lower epsilon and increase delta in those bounds, ie. complexity of hypothesis space. Now back to SVM. If you don't use kernels, or use kernels that map into finite dimensional spaces, the so called Vapnik-Chervonenkis dimension which is a measure of hypothesis space complexity, is finite, and with that and enough training examples you can get that with high probability the error on the test set won't be much bigger than the error on training set. If you use kernels that map into infinite-dimensional feature spaces, then the Vapnik-Chervonenkis dimension is infinite as well, and what's worse the training samples alone cannot guarantee good generalization, no matter the number of them. Fortunately, the size of the margin of an SVM turn out to be a good parameter for ensuring generalization. With big margin and training set, you can guarantee that the test error won't be much bigger than training error as well.
