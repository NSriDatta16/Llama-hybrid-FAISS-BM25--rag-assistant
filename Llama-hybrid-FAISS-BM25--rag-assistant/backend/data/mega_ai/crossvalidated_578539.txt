[site]: crossvalidated
[post_id]: 578539
[parent_id]: 573599
[tags]: 
Dropout: I agree with comments saying that dropout has mostly been dropped (ha) in favor of other regularization techniques, especially as architectures have gone more fully convolutional (and dropout doesn't really work with conv layers). Also note that dropout and batch norm can have bad interactions with each other. I don't think anyone really understands why batch norm helps - some have argued against "covariate shift" for example - How Does Batch Normalization Help Optimization? . So I don't this is a strike against using it in GANs. Some GAN varieties like WGAN assume independence between samples in a batch, which is a good reason to avoid batch norm.
