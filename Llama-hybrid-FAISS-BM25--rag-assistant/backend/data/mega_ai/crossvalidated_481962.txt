[site]: crossvalidated
[post_id]: 481962
[parent_id]: 
[tags]: 
Semi-supervised training of multilayer perceptron

I'll explain my problem through a simplified example. Let's say I have a matrix $X$ of a single variable with the following observations x = np.zeros(10).reshape(-1,1) x[0] = 1 x[-1] = 0.5 x >>> array([[1. ], [0. ], [0. ], [0. ], [0. ], [0. ], [0. ], [0. ], [0. ], [-1.]]) and a target variable $y$ with 3 classes one hot encoded as ytest = np.zeros((10,3)) ytest[:5,0] = 1 ytest[5:,2] = 1 ytest >>> array([[1., 0., 0.], [1., 0., 0.], [1., 0., 0.], [1., 0., 0.], [1., 0., 0.], [0., 0., 1.], [0., 0., 1.], [0., 0., 1.], [0., 0., 1.], [0., 0., 1.]]) Let's call the classes A, B and C. As you can see I have zero examples of the class B by design. I've also constructed the $X$ matrix to make it impossible to correctly predict the middle $y$ values (index 2 - 9) I would like to train a neural network to predict class B for all of these values. ie when the network is "unsure" about which class to predict, just predict B. I'd expect my prediction to look something like this, where the middle samples are predicted as class B: array([[1., 0. , 0.], [0., 1. , 0.], [0., 1. , 0.], [0., 1. , 0.], [0., 1. , 0.], [0., 1. , 0.], [0., 1. , 0.], [0., 1. , 0.], [0., 1. , 0.], [0., 0. , 1.]]) This to me seems like I should have different misclassification penalties. I've tried using weighted crossentropy loss functions to implement misclassification penalties yet it doesn't seem to produce the desired effect. What is the appropriate way to accomplish such a task?
