[site]: crossvalidated
[post_id]: 43127
[parent_id]: 
[tags]: 
What type of regression model do I use?

$y = \mathbf{X} \beta$ + $\epsilon$ + $m$ Where y, $\epsilon$, and $m$ are $n \times 1$ column vectors, $\beta$ is $p \times 1$ and $\mathbf{X}$ is $n \times p$. $y$ is a noisy time-series signal modeled by linear combinations of a basis set $\mathbf{X}$ discribed by $\beta$. $\epsilon$ is additive Gaussian noise. $m$ is an estimate of the mismatch inherent in the reconstruction. The columns of $\textbf{X}$ can't perfectly reconstruct $y$, even if it were noiseless. Am I correct in my understanding that this can be called an errors-in-variables model? So given y and $\textbf{X}$, what is $\beta$? Here's the twist: The true value of $\beta$ should either be 0 or 1, while all other variables are comprised of continuous data. What I'd like to be able to do is find $\beta$ to be a value between 0 or 1 which corresponds to a probability the corresponding column of $\textbf{X}$ is present in the reconstruction. A value in $\beta$ of 1 indicates that its corresponding column of x is present, 0 not. Since the columns of $\textbf{X}$ can't perfectly reconstruct $y$ even if it were noiseless, basic methods like OLS give answers where elements of $\beta$ are above 1 and below 0, which for me are unphysical. In other words, is there a way to constrain the results of a linear regression ($y = \mathbf{X} \beta + \epsilon + m$) to only admit $\beta$ values between zero and 1 in a way which they can be interpreted as a probability that $\beta$ equals one or not? Perhaps through the use of a prior distribution? If anyone has any keywords I can look into, or references to point out, that would be very appreciated! P.S. If anyone can give a Bayesian interpretation.. Bonus points! The people this is for love that stuff. Thanks in advance!
