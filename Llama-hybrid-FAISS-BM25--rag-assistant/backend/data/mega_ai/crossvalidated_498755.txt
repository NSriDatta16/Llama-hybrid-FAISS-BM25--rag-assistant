[site]: crossvalidated
[post_id]: 498755
[parent_id]: 
[tags]: 
Sequential updating of a Bayesian regression model

I'd like to perform Bayesian regression of $Y$ on $X$ . i.e. estimate a posterior $p\left(\theta\mid\mathcal{D}\right)$ of some regression parameter $\theta$ given a dataset $\mathcal{D}$ of pairs of points $(x_i, y_i)$ . Suppose I have a dataset, $\mathcal{D}_1$ , I estimate $p\left(\theta\mid\mathcal{D}_1\right)$ , and then I get another dataset $\mathcal{D}_2$ , and I want $p\left(\theta\mid\mathcal{D}_1, \mathcal{D}_2\right)$ . Intuitively, I want to use the posterior after the first update as the prior for the second. e.g. in linear regression with a Gaussian prior $\mu_{\theta},\Sigma_{\theta}$ . I have a formula to which I insert $\mu_{\theta},\Sigma_{\theta}$ and $\mathcal{D}_1$ and it gives me $\mu_{\theta\mid\mathcal{D}_{1}},\Sigma_{\theta\mid\mathcal{D}_{1}}$ . So when I subsequently input $\mu_{\theta\mid\mathcal{D}_{1}},\Sigma_{\theta\mid\mathcal{D}_{1}}$ and $\mathcal{D}_2$ it should give $\mu_{\theta\mid\mathcal{D}_{1}, \mathcal{D}_{2}},\Sigma_{\theta\mid\mathcal{D}_{1}, \mathcal{D}_{2}}$ I know how to justify this for sequential binary hypothesis testing, and I could probably show it for the specific example I gave above. But I know (maybe "feel" is a better word) that this is a general principle, just not sure how to prove it.
