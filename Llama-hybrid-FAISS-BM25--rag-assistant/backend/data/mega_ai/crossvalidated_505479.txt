[site]: crossvalidated
[post_id]: 505479
[parent_id]: 505023
[tags]: 
In a way, any model will be as good as another. But models aren't as good as each other even when "kept the same". Counter-examples: Neural networks and some other learners give a different result each time with training. Choice of network structure and activation functions can have a mathematically proven limitation on what functions the network is capable of learning. Often you try an arbitrary bunch of models and just pick the best one. And the best one could have been the best one just because of blind luck. But the model still "matters" since it got the best result, unless you would say it doesn't matter whether you won lotto or not because every ticket, winning or not, just contains lines of numbers.
