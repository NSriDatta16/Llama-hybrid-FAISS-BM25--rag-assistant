[site]: crossvalidated
[post_id]: 439277
[parent_id]: 
[tags]: 
Deep generative models learning a Bayesian-network distribution

Say I have a generative model for some distribution $p$ over a small number of RVs which allows me to easily sample from said distribution. For example, let's say it's a parameterized Bayesian network $\mathcal{B}$ over $n$ random variables $X_1, \ldots, X_n$ , which constrains the joint distribution $p_\mathcal{B}(X_1, \ldots, X_n)$ with the various conditional independencies which the BN structure implies. Given such a network I can easily generate as many samples $(x_1,\ldots,x_n)\sim p_\mathcal{B}$ as I want by forward sampling. Question: Is there any work on training a generative deep model to generate samples from such a distribution? (That is, to imitate the original generator), ideally with a meaningful latent space. Are there any generative models which are expected to do better/worse on such a task? I'm especially interested in the case where there are relatively few variables (no more than a few dozen) and the joint distribution is highly constrained (so probably can be mapped to from a simple latent space)
