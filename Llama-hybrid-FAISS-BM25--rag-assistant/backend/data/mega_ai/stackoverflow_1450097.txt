[site]: stackoverflow
[post_id]: 1450097
[parent_id]: 1450085
[tags]: 
grep may do it, but I'm guessing you'll have a much easier time with awk (aka gawk, on some systems). The effective chain / script to be used for your need depends on a few extra bits of info. For example, is the input file readily sorted, how big is the input (or rather is it huge or a stream)... Assuming sorted input (either originally or from piping through sort), the awk script would look something like that: (attention untested) Check the solution provided by Jonathan Leffler or Hai Vu, for a way to achieve the same without the pre-sort requirement. #!/usr/bin/awk # *** Simple AWK script to output duplicate lines found in input *** # Assume input is sorted on fields BEGIN { FS = ";"; #delimiter dupCtr = 0; # number of duplicate _instances_ dupLinesCtr = 0; # total number of duplicate lines firstInSeries = 1; #used to detect if this is first in series prevLine = ""; prevCol2 = ""; # use another string in case empty field is valid } { if ($2 == prevCol2) { if (firstInSeries == 1) { firstInSeries = 0; dupCtr++; dupLinesCtr++; print prevLine } dupLinesCtr++; print $0 } else firstInSeries = 1 prevCol2 = $2 prevLine = $0 } END { #optional display of counts etc. print "*********" print "Total duplicate instances = " iHits " Total lines = " NR; }
