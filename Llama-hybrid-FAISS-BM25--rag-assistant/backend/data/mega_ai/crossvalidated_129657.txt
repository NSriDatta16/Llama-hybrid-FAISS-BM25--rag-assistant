[site]: crossvalidated
[post_id]: 129657
[parent_id]: 
[tags]: 
Recommendation: Fast algorithm for logistic random effects?

What is the fastest algorithm for fitting a simple logistic 'random effects' type model, with only one level of categorical predictors? Another way of putting it might be a logistic regression with a Gaussian prior on the coefficients, or "with shrinkage". I'm looking for a very fast and reliable implementation to use in a production environment. This means that the algorithm would need to have a low risk of 'hanging', and a not-drastically-variable time to converge. There would be between 1 and 5000 data points per 'cell', and 5-100 groups/categories. It would need to exploit sufficient statistics (take counts of group data). Second-level nesting a bonus, but not essential. This could be done via lme4 in R . However, is there a library (e.g. stand-alone C++) which is more efficient for this narrowly-defined type of model? EDIT: Goal is inference over prediction - specifically, comparison of group estimates (with standard errors), construction of confidence intervals etc. EDIT: Just to make it clear, I wouldn't be fitting a 'mixed model' so to speak - there would be no fixed effect. The data would be a very long two-column ('successes', 'failures') contingency table, with highly variable n counts. EDIT: I need the degree of 'shrinkage' in the individual estimates to be informed by the group level variance (as opposed to banging a Jeffery's prior on each individual estimate, or using an Agresti-Coull (1998) type interval).
