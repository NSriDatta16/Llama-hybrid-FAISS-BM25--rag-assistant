[site]: crossvalidated
[post_id]: 613585
[parent_id]: 
[tags]: 
100% training and test accuracy in binary classification task

I'm currently reproducing a method on my data set. In the literature, training and test accuracies are generally high, mostly between 90% to 99%. I get a training accuracy of 100% and a test accuracy of 100%, too. I used SVM and k-NN with same results. However, I believe that something is not right ;) The type of data is illustrated in the picture below. It's a time series consisting of basically rectangular non-stochastic signals that vary in amplitude depending on the class. In the literature and the method I want to reproduce, features are not extracted from this signal but the whole time series of a sample of a class is used as an input for training and classification. Due to the nature of the data and how it is acquired, the test set is usually a subset of the training data. Well, one reason the models accuracy is so high is probably because I used a subset of the training data for testing. However, when I use the same data for a multiclass problem, the accuracy drops significantly. On the other hand I also wonder if it could be because the input is a complete time series. What possible explanations are there for the fact that the classification rate in the binary task is significantly higher than in the multiclass problem, although I am already overfitting by the method?
