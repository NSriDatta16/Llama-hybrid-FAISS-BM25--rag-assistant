[site]: crossvalidated
[post_id]: 144245
[parent_id]: 
[tags]: 
How can increasing the dimension increase the variance without increasing the bias in kNN?

My question is about understanding Figure 2.8 in The Elements of Statistical Learning (2nd edition) . The topic of the section is how increasing dimension influence the bias/variance. I can roughly understand Figure 2.7 in ESL, but have no idea about 2.8. Any explanation on the roughly unchanged bias, or the dominating variance? I cannot imagine how they change when the dimension is increasing. Following is the detail: Suppose we have 1000 training examples $x_i$ generated uniformly on $[-1,1]^p$. Assume that the true relationship between $X$ and $Y$ (capital letters for variables) is $$ Y=F(X)=\frac12(X_1 + 1)^3 $$ where $X_1$ denotes the first component of $X$ ($X$ has totally $p$ components, in other words, features). We use the 1-nearest-neighbor rule to predict $y_0$ at the test-point $x_0 = 0$. Denote the training set by $\mathcal{T}$. We can compute the expected prediction error at $x_0$ for our procedure, averaging over all such samples of size 1000. This is the mean squared error (MSE) for estimating $f(0)$: \begin{align} \operatorname{MSE}(x_0) &=E_{\mathcal{T}}[f(x_0)-\hat{y}_0]^2 \\ &= E_{\mathcal{T}}[\hat{y}_0-E_{\mathcal{T}}(\hat{y}_0)]^2 + [E_{\mathcal{T}}(\hat{y}_0)-f(x_0)]^2 \\ &= \operatorname{Var}_{\mathcal{T}}(\hat{y}_0) + \operatorname{Bias}^2(\hat{y}_0) \end{align} The figure is below. The right plot is the case with increasing $p$ (dimension).
