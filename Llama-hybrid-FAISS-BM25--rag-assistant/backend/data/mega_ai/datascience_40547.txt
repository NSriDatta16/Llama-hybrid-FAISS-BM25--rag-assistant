[site]: datascience
[post_id]: 40547
[parent_id]: 40451
[tags]: 
Neural networks are function approximators, thus the answer is yes. Neural networks go from a set of inputs and map them to a set of outputs. That is precisely what a function does as well. In theory a sufficiently deep enough network and sufficient data coverage can approximate any function within a finite range. Let's look at an example import numpy as np import matplotlib.pyplot as plt We will approximate some arbitrary function $y = x_1^2 + x_2^2 + x_1x_2^2 + \text{tan}(x_1)$ def func(x1, x2): return x1**2 + x2**2 + x1*x2**2 + np.tan(x1) We will bound our inputs from $[-1, 1]$ . n = 10000 x_train = (np.random.rand(n,2) - 0.5) * 2 y_train = np.zeros((n,)) for i in range(n): y_train[i] = func(x_train[i,0], x_train[i,1]) n = 1000 x_test = (np.random.rand(n,2) - 0.5) * 2 y_test = np.zeros((n,)) for i in range(n): y_test[i] = func(x_test[i,0], x_test[i,1]) plt.scatter(x_train[:,0], x_train[:,1], c=y_train) plt.show() Let's now build a model. from __future__ import print_function import keras from keras.datasets import mnist from keras.models import Sequential from keras.layers import Dense, Dropout, Flatten from keras.layers import Conv1D, MaxPooling1D, Reshape from keras.callbacks import ModelCheckpoint from keras.models import model_from_json from keras import backend as K from keras import optimizers input_shape = (2,) model = Sequential() model.add(Dense(32, activation='linear', input_shape=input_shape)) model.add(Dense(64, activation='tanh')) model.add(Dense(32, activation='tanh')) model.add(Dense(1, activation='linear')) sgd = keras.optimizers.RMSprop(lr=0.001) model.compile(loss=keras.losses.mean_squared_error, optimizer=sgd, metrics=['mae']) We will train the model epochs = 50 batch_size = 128 # Fit the model weights. history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test)) Let's see how well we did. We will compare the test set that we have to the predicted values. plt.scatter(x_test[:,0], x_test[:,1], c=y_test) plt.show() plt.scatter(x_test[:,0], x_test[:,1], c=model.predict(x_test).reshape(x_test.shape[0],)) plt.show() Other functions def func(x1, x2): if x1 >= 0 and x2 >= 0: return x1**2 + x2**2 else: return x1*2 + x2*2 Test set Prediction def func(x1, x2): return np.sinc(x1 + x2) Test set Prediction Try your own function with this network.
