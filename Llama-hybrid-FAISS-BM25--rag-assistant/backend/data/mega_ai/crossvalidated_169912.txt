[site]: crossvalidated
[post_id]: 169912
[parent_id]: 
[tags]: 
Linear regression to minimize the Cost Function

I am new started machine learning program. I find it difficult to understand gradient descent algorithm. I am going through machine learning from coursera by Andrew Ng.All of his lecturer in second week course is around gradient descent algorithm. Since I don't understand this formula , I am unable to move further on this course. Kindly help. Linear regression to minimize the Cost Function: $$J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^m \left(h_\theta(x_i) - y_i \right)^2 $$ Hypothesis of linear model is $$h_\theta(x) = \theta \cdot x = \theta_0 + \theta_1 x_1$$ How to read this formula? What are $h_\theta$, $x_i$ and $y_i$? How to make a decision which data has to go with this parameter? Why 1/2m?
