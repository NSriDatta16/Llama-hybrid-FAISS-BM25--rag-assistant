[site]: crossvalidated
[post_id]: 172982
[parent_id]: 
[tags]: 
Maximum Likelihood estimation of a parametric density of a univariate response, given multidimensional data?

I have some data represented by vectors $\mathbf{x}_1,\mathbf{x}_2,\ldots,\mathbf{x}_n \in \mathbb{R}^m$ , that is, each sample is a collection of $m$ features. In addition, I have a response corresponding to each data point: $y_1,\ldots,y_n \in \mathbb{R}$ . I would like to train a parametric model on this data, in order to be able to predict the probability density of the response $y(\mathbf{x})$ corresponding to a new sample $\mathbf{x} \in \mathbb{R}^m$ . (I really need to have a probability density for $y(\mathbf{x})$ , not just a punctual estimation.) So my idea would be to select a family of parametric densities defined through a parameter $\boldsymbol{\theta}\in\mathbb{R}^k$ , $f(y;\boldsymbol{\theta})$ . Then, I would like to run a maximum-likelihood procedure to find the best possible function $\theta$ from a class of functions $\Theta$ mapping $\mathbb{R}^m\mapsto\mathbb{R}^k$ : $$ \max_{\theta \in \Theta} \sum_{i=1}^n \log f(y_i,\theta(\mathbf{x}_i)). $$ Question 1 Is this a well-studied problem, and if yes, how is it called in the literature? Or is there an alternative approach that is known to work better? Question 2 My first, naive idea, is to restrict myself to a class $\Theta$ of linear functions, so the above problem reduces to determining $km$ coefficients. In this case, any reference to some algorithm performing this task would be appreciated. Question 3 If we consider a family of gaussians whose parameters $\mu$ and $\sigma$ are linear functions of $\mathbf{x}$ , i.e. $y \sim \mathcal{N}(\boldsymbol{\mu}^T\mathbf{x}, (\boldsymbol{\sigma}^T\mathbf{x})^2)$ for some vectors $\boldsymbol{\mu},\boldsymbol{\sigma}\in \mathbb{R}^m$ , then the optimization problem to solve is $$ \min_{\boldsymbol{\mu},\boldsymbol{\sigma}\in \mathbb{R}^m}\quad \sum_{i=1}^n \frac{1}{2}\left(\frac{y_i-\boldsymbol{\mu}^T\mathbf{x}_i}{ \boldsymbol{\sigma}^T\mathbf{x}_i} \right)^2 + \log(\boldsymbol{\sigma}^T\mathbf{x}_i). $$ What is the state-of-the-art to solve this problem ? Or maybe, because we want small predicted variances, we could add a penalization term: $+ \lambda \sum_i (\boldsymbol{\sigma}^T\mathbf{x}_i )^2$ in the function to minimize? Question 4 If we assume that $f$ is a mixture of laws $f_1,\ldots,f_N$ defined by a vector of probabilities $\boldsymbol{\pi}$ , is there a way to construct a function $\pi$ that maps $\mathbf{x}$ to the probability vector $\boldsymbol{\pi}=\pi(\mathbf{x})$ , so that the density of the response $y(\mathbf{x})$ is given by $ \sum_{j=1}^N \pi_j(\mathbf{x}) f_j(y). $ I have the feeling that using a neural network might be appropriate to do construct the function $\pi$ , but again, I would be grateful if someone could point out a good reference for this specific problem. Question 5 Here is a tentative approach to answer Question 4. How appropriate do you think this is? Is this a standard approach (or a stupid one)? Do you know some references? We could use the EM algorithm to model the augmented vectors $(\mathbf{x}_i,y_i)\in\mathbb{R}^{m+1}$ as coming from a mixture of multivariate gaussian variables: $$\left(\left[\begin{array}{c}\mathbf{x}_i\\y_i\end{array}\right] | Z=k\right) \sim \mathcal{N}\left( \left[\begin{array}{c}\boldsymbol{\mu}_k\\ m_k\end{array}\right], \left[\begin{array}{cc}\Sigma_k & \boldsymbol{v}_k\\ \boldsymbol{v}_k^T & \sigma_k^2\end{array}\right] \right), \quad\textrm{with}\ P(Z=k) = w_k. $$ Then, the distribution of the response $y$ given the value of a new sample $\mathbf{x}$ and the value of the latent variable $Z$ would be: $$(y|\mathbf{x},Z=k) \sim \mathcal{N}\left( m_k + \boldsymbol{v}_k^T \Sigma_k^{-1} (\mathbf{x}-\boldsymbol{\mu}_k),\ \sigma_k^2-\boldsymbol{v}_k^T \Sigma_k^{-1}\boldsymbol{v}_k \right). $$ Finally, we obtain the distribution of $y|\mathbf{x}$ as a mixture of gaussians, with weights $P(Z=k|\mathbf{x})$ that can be obtained from the Bayes formula.
