[site]: crossvalidated
[post_id]: 11750
[parent_id]: 11544
[tags]: 
I was thinking more about the question and thought I would give a slight enhancement of the naive approach as an answer in hopes that people know further ideas in the direction. It also allows us to eliminate the need to know the size of the fluctuations. The easiest way to implement it is with two parameters $(T,\alpha)$. Let $y_t = x_{t + 1} - x_{t}$ be the change in the time series between timestep $t$ and $t + 1$. When the series is stable around $x^*$, $y$ will fluctuate around zero with some standard error. Here we will assume that this error is normal. Take the last $T$, $y_t$'s and fit a Gaussian with confidence $\alpha$ using a function like Matlab's normfit . The fit will give us a mean $\mu$ with $\alpha$ confidence error on the mean $E_\mu$ and a standard deviation $\sigma$ with corresponding error $E_\sigma$. If $0 \in (\mu - E_\mu, \mu + E_\mu)$, then you can accept. If you want to be extra sure, then you can also renormalize the $y_t$s by the $\sigma$ you found (so that you now have standard deviation $1$) and test with the Kolmogorov-Smirnov test at the $\alpha$ confidence level. The advantage of this method is that unlike the naive approach, you no longer need to know anything about the magnitude of the thermal fluctuations around the mean. The limitation is that you still have an arbitrary $T$ parameter, and we had to assume a normal distribution on the noise (which is not unreasonable). I am not sure if this can be modified by some weighted mean with discounting. If a different distribution is expected to model the noise, then normfit and the Kolmogorov-Smirnov test should be replaced by their equivalents for that distribution.
