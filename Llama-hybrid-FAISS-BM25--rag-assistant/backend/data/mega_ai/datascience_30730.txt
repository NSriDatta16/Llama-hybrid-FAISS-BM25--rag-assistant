[site]: datascience
[post_id]: 30730
[parent_id]: 30227
[tags]: 
From what I have seen dropout for LSTMs should not be so high as 0.5 Recommendations are 0.1 or less. This paper - Where to Apply Dropout in Recurrent Neural Networks for Handwriting Recognition has a detailed study on how to use Dropout with RNNs, and one of the recommendations is that 'it is almost always better before the lstm layer than inside or after it' (pg 3), so you can try that. However, I believe this depends on kind of data you have. I remember reading a paper that had dropout for LSTM only being useful for a large LSTM (like 4096 unit x 4 layers). I cannot find it now, but in this one , the authors suggest something similar - showing dropout having better results on a 1500 unit x 2 layer lstm than a 650 unit by 2 layer, while the smaller network would just overfit. The lesson I guess depends on how big your lstm is and what you are trying to achieve - the use case being preventing overfitting while increasing model size.
