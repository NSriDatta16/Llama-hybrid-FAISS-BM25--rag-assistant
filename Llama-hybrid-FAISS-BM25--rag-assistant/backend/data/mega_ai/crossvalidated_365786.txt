[site]: crossvalidated
[post_id]: 365786
[parent_id]: 
[tags]: 
Explanation of overshooting accumulated reward in reinforcement learning

We are curious about the trend of the total accumulated reward in RL applications. Especially when it comes to an overshooting in the signal at the beginning of the training as shown in the plots from the dopamine framework from Google DeepMind : Even DQN and other networks, dependent on the task, show this behavior. Can some explain or give a hint to literature which describes the causes of this? Thanks
