[site]: crossvalidated
[post_id]: 464337
[parent_id]: 
[tags]: 
Difference between loss functions in neural networks

Supose I would like to build a neural network with a sigmoidal function in the output layer, why is there a preference in the use of cross-entropy as a loss function in regard square loss for example? I can understand that in a simple logistic regression the cross-entropy would make the loss a convex function but in neural networks the domain is always non convex, so why the preference?
