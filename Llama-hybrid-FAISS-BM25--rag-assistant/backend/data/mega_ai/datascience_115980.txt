[site]: datascience
[post_id]: 115980
[parent_id]: 60229
[tags]: 
I don't think you will find details of this resource hanging around - it is trade secret. My bet is Google first trains a huge language model as base, and let users fine-tune it with their own data, just like any usual transfer learning. As for how they train the base model, the closest thing I find is the Pathways Language Model PaLM , developed around 2021, which is described as ' a 540-billion parameter, densely activated, Transformer language model ', trained on ' 6144 TPU v4 chips '. Though Google may be keeping more dark magics, PaLM is already the state of the art.
