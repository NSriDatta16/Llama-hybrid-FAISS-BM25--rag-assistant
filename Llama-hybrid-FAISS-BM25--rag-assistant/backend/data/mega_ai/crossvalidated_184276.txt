[site]: crossvalidated
[post_id]: 184276
[parent_id]: 182734
[tags]: 
The difference between a "Deep" NN and a standard NN is purely qualitative: there is no definition of what that "Deep" means. "Deep" can mean anything from the extremely sophisticated architectures that are used by Google, Facebook and co which have 50-80 or even more layers, to 2 hidden layers (4 layers total) architectures. I wouldn't be surprised if you could even find articles claiming to do deep learning with a single hidden layer, because "deep" doesn't mean much. "Neural network" is also a word that doesn't have a very precise meaning. It covers an extremely large ensemble of models, from random boltzman machines (which are undirected graphs) to feedforward architectures with various activation functions. Most NNs will be trained using backprop, but it doesn't have to be the case so even the training algorithms aren't very homogenous. Overall, deep learning, deep NNs and NNs have all become catch-all words which capture a multitude of approaches. For good introductory references into "what changed": Deep Learning of Representations: Looking Forward , Bengio, 2013 is a good review + perspective for the future. Also see Do Deep Nets Really Need to be Deep? Ba & Caruana, 2013 which illustrate that being deep might not be useful for representation but for learning.
