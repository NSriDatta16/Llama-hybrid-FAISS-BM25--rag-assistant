[site]: crossvalidated
[post_id]: 609309
[parent_id]: 493762
[tags]: 
When you collect data, there is always some chance that, just due to rotten luck, you observe something in the data that is not really there. This is why, for instance, we can observe two groups with unequal empirical means of $\bar x_1$ and $\bar x_2$ , yet not immediately conclude that $\mu_1\ne\mu_2$ . It could be that, just due to some bad luck, we happen to observe unequal sample means, despite equal population means. In fact, it can be that $\bar x_1 , yet $\mu_1>\mu_2$ , as the simulation below shows. set.seed(2023) N How this relates to regression and an $R^2$ better than chance is that, even if you add a totally unrelated variable to your regression model, it might be that, just due to the luck of what you happen to observe, there is a slight empirical relationship. Therefore, the $R^2$ will increase slightly upon adding the variable, despite the lack of a relationship. set.seed(2023) N Despite the fact that the simulation above has x and y totally unrelated to each other, just due to the luck of the values that happen to be observed/simulated, there is a fairly high $R^2 = 0.784$ . If you do this kind of simulation many times, you see that high $R^2$ values can occur fairly often. set.seed(2023) N From the graph, we see that about $20\%$ of observed $R^2$ values exceed $0.5$ , despite there being absolutely no relationship between x and y . Therefore, if you run a regression and get $R^2 = 0.5$ , despite that looking like a solid score (at least in many cases), it is so easy to get such a value just by chance that that you cannot really consider your performance to be above the chance level. Fortunately, as the sample size increases, it becomes highly unlikely to observe high $R^2$ values just by chance. For instance, when I bump up the sample size to $50$ (so still not very high), not one $R^2$ value in $10,000$ iterations exceeds $0.28$ . set.seed(2023) N Increasing the sample size to $500$ results in a highest observed $R^2$ a bit above $0.03$ . What seems to be happening in that sklearn documentation is that feature importance is tested by permuting the values. By doing so, you break any relationship between the feature and the outcome, except for the occasional bad break. When the $R^2$ with the original feature is higher than the vast majority of $R^2$ values the result from permuting the feature, you conclude that there is a true relationship between the feature and your outcome ( $y$ ). I will demonstrate below. set.seed(2023) N Not one of the $10000$ permutations gave an $R^2$ greater than the observed $R^2$ with the original $x$ . That seems like evidence of the $x$ variable being predictive of $y$ , which we can see from the simulation is true. Compare this with a stuation where $x$ has no relationship with $y$ . set.seed(2023) N Many $R^2$ values with permuted $x$ exceed the observed $R^2$ using the original $x$ , suggesting that $x$ is not contributing to predicting $y$ any better than it would be expected to by chance alone. Where this can be quite useful in a machine learning context is if you have a complicated model. Perhaps you have lots of interaction terms or nonlinear functions of the original features, such as $y = x_1 + x_2 + x_1^2 + x_2^2 + x_1x_2$ , and you just want to know if $x_1$ is a contributor of any kind to $y$ . If you permute $x_1$ , you can figure that out. I will simulate it below. set.seed(2023) N About $15\%$ of models with permuted $x_1$ result in $R^2$ exceeding the $R^2$ of the original model. Does that make it sound like $x_1$ plays a role in determining $y?$ I reveal the answer below, but think about it! You can see from the code that the x1 variable plays no role in determining y, consistent with the permutation test. After all, when y is determined in the y
