[site]: crossvalidated
[post_id]: 502754
[parent_id]: 502714
[tags]: 
There isn't any hard requirement here, but some considerations that may lead to equally sized sets in the end. The largest difference is between training vs. validation/test/optimization/verification: for training, as a rule of thumb the ratio training sample size : model complexity/variates/degrees of freedom counts. In other words, we can adapt to smaller training sample sizes (to some extent). In contrast, the precision of any kind of testing (whether the results will be used for optimization or as final estimate of generalization error) depends on the absolute number of tested cases. As a side note: any general recommendation of a split ratio without looking at the acutal sample size (and expected model complexity) therefore doesn't make too much sense. The "absolute number" consideration thus applies to both validation (aka optimization) and test (aka verification) sets, and this may be a first reason why it can make sense to have them both the same size. One may argue that optimization requires more precise performance estimates since we typically compare multiple (many) models. Which would boil down to larger validation/optimization set than test/verification set. OTOH, few people will be willing to accept a low precision final estimate of generalization error. And the optimization "wrongly" picking only an almost-optimal rather than the optimal model won't hurt much in practice: if the chosen model does not have sufficient performance, we'll see that in the test/verification and typically need to reconsider our modeling in a more thorough manner. If test/verification (or rather, an actual validation) shows that the optimized model is fit for purpose, we don't care whether it is only almost optimal. It is sufficient, and that's all we really need. So we may get away with somewhat lower precision in the optimization. Last but not least, if you do a cross validation for any of the two testing steps, its sample size will be the whole data set available at that stage since the test results are pooled across the folds. From a precision point of view, that's the best you can get wrt. the given size of your data set. In my experience and with the data I see (I'm typically in small to very small sample size situations), the important point is to make sure there are sufficient cases to derive meaningful interpretations from the test results. After that, I'm fine with somewhat handwavy pragmatic decisions. Here's a paper we wrote about these thoughts: Beleites, C. et al. : Sample size planning for classification models. Anal Chim Acta, 2013, 760, 25-33. DOI: 10.1016/j.aca.2012.11.007 accepted manuscript on arXiv: 1211.1323
