[site]: crossvalidated
[post_id]: 473557
[parent_id]: 473538
[tags]: 
You also mention merging classes, this might have impact on the results as well. If you had under-represented classes in your data, you might not have enough samples to learn them, so after collapsing it might have simplified the problem. This seems to be the case if you had separate class for hand and forearm, and arm, and in second case the whole arm. I can imagine how you could have many photos of people where hands are not visible, because they are smaller then other parts of the body, or could have been covered with something (body part, clothing, object), so the effective sample size containing hands is smaller then the set where arms are visible. Recall that you start optimization with randomly initialized parameters, if you didn't use fixed random seed, this might lead to different results between different trainings, even if you used exactly the same data and code. There are known examples in deep learning literature where people failed when training the models with different random seeds then the ones used by original authors. There is resent paper by Engstorm, Ilyas, et al. (2020) showing how different implementation of the same algorithm have lead to different results. Surely, many things have changed between TensorFlow 1.x and 2.x, some of those changes apparently had some impact on the algorithm you used. From technical point of view, if I were you, I'd start with checking if any of the default parameters in any of the functions didn't change. Also make sure same random seeds are used, or that you didn't shuffle the data differently.
