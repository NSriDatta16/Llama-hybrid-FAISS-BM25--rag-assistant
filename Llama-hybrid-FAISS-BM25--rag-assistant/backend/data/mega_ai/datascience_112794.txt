[site]: datascience
[post_id]: 112794
[parent_id]: 112791
[tags]: 
Your lowercase setting is different for CountVectorizer and HashingVectorizer . It might have an impact. Otherwise, they do very similar job in this case, the accuracy difference varies with the exact task but is not that huge. Disparate training speeds you observe are not related to the method itself (the feature matrix size is comparable), it's just HashingVectorizer normalizing the results by default, which is usually beneficial for SVC, resulting in much fewer iterations (check clf_count.n_iter_ ). Applying sklearn.preprocessing.Normalizer() to CountVectorizer results will likely make it fit equally fast. HashingVectorizer is still faster and more memory efficient when doing the initial transform, which is nice for huge datasets. The main limitation is its transform not being invertible, which limits the interpretability of your model drastically (and even straight up unfitting for many other NLP tasks).
