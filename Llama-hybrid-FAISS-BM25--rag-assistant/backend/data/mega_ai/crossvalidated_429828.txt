[site]: crossvalidated
[post_id]: 429828
[parent_id]: 429819
[tags]: 
First, to have a posterior distribution for $\theta$ , $\theta$ must be (modeled as) a random variable. For the likelihood function that is not necessary. So this is deeper than the comment (by @gazza89) saying The likelihood is a pdf, it's just normalised w.r.t all possible data outcomes, and the posterior is a pdf, but it's normalised w.r.t all possible parameter values Even if the likelihood was (or could be normalized too, it is not always possible) to integrate to one, that is not enough to make it into a pdf (probability density function). A pdf must be the pdf of some random variable and if $\theta$ is not modeled as a random variable, then it cannot have a pdf, period. So conceptually it is quite clear-cut: If $\theta$ is a random variable (probably in some Bayesian model) then it can have a posterior, and it can have a likelihood function. Even if those two functions should be numerically equal (as for instance if the prior is uniform), they are distict mathematical entities. If $\theta$ is not (modeled as) a random variable the problem does not arise, only the likelihood function can be defined. You say And the likelihood is "the likelihood of Î¸ having generated D" but that is not a good way of thinking about it. Likelihood tells you the probability (under the model ...) of the data if data was generated according to that value of $\theta$ . There is no "probability of $\theta$ " involved . To get that you need some extra assumptions, a Bayesian probability model. To understand likelihood better see all the answers to Maximum Likelihood Estimation (MLE) in layman terms .
