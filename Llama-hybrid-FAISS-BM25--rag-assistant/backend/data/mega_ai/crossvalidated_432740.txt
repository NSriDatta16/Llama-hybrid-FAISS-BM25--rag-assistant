[site]: crossvalidated
[post_id]: 432740
[parent_id]: 428902
[tags]: 
In many cases the data we observe depends on some hidden variables, that were not observed, or could not be observed. Knowing those variables would simplify our model, and in many cases we can get away from not knowing their values by assuming a latent variable model , that can "recover" the unobserved variables from the data. Among popular examples of such models are finite mixture models , which assume that the data is clustered, while the cluster assignment is unknown and to be learned from the data. Those models can be used to learning distribution of the data, or more complicate cases like regression . In each case, the model learns to distinguish among several groups in the data, that share common characteristics, and fits the sub-models per each group, no matter that the group assignment was not known a priori . In plain English, instead of needing to build a complicated one-size-fitts-all model, you are building a model that consists of several, problem-specific, simpler models. Another popular example is principle components analysis (see e.g. chapter 12 from Pattern Recognition and Machine Learning book by Bishop, 2006), or basically any other dimensionality reduction model, that are used to "compress" the observed data to smaller number of dimensions without much loss of information. Here the latent variables are the unobserved "features" of the data, that almost fully explain it. We are aiming at finding those features. You can find very different example in my recent question , where we observed an aggregated data, while wanting to learn about the individual-level variability. As pointed in the answer, this can be thought as latent variable model, where we treat the predictions for the individuals as latent variables, that get aggregated, so that we can predict the aggregated responses to train the model. So contrary to previous examples where we used latent variables to find some higher-level features, here we use them to make lower-level, de-aggregated predictions. Again, here the individual-level values were not observed, so we replaced them with latent variables placeholders and made the model to predict them from the data. Those are just few examples that illustrate latent variable models. You can find some more in the books by Bishop (2006) , or Machine Learning: A Probabilistic Perspective by Kevin P. Murphy, who give many more examples and detailed explanations. As a sidenote, it is worth mentioning, that those models can be in many cases hard to identify and often need some problem-specific computational tweaks and algorithms, so "guessing" the data that was not observed comes at some cost.
