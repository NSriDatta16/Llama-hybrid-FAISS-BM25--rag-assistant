[site]: crossvalidated
[post_id]: 56558
[parent_id]: 56543
[tags]: 
I think there are a few approaches here. First Approach As far as I know, there is no way to use @deterministic or @stochastic (without the likelihood). An alternative way is to use the potential s class, which is like multiplying your likelihood by a factor. In this case, we should multiply by the pdf of a lognormal given $Z$ and $X$. import pymc as mc z = -1. #instead of 0 and 1, unknowns can be put here. For example: # mc.Normal( "x", unknown_mu, unknown_std ). X = mc.Normal( "x", 0, 1, value = -2. ) @mc.potential def Y( x =X, z = z): #similar to my comment above, you can place unknowns here in place of 1, 0.2. return mc.lognormal_like( z-x, 1, 0.2, ) mcmc = mc.MCMC( [X] ) mcmc.sample(20000,5000) Notice $Z$ is negative, so this must make $X$ negative too. And we observe this: By symmetry (since $Y = Z-X$) the posterior of $Y$ is similar: Z is a vector of observations If $Z$ is a vector of observations, then the potential function can be modified to look like: z = [2,3,4] #... X = mc.Normal( "x", 0, 1, value = -2., size = 3 ) @mc.potential def Y( x =X, Z = Z): return mc.lognormal_like( Z, 1, 0.2, ) To extend to more than two linear combinations, eg $Z = X_1 + X_2 + ... +X_N$, well to be continued. Second Approach A more specific approach is to notice that as $X$ is normal, we can think of this task as $Z = Y + \text{noise}$: import pymc as mc Z = -1 Y = mc.Lognormal( "y", 1, 0.2 ) obs = mc.Normal( "obs", 0, 1, value = Z, observed = True ) mcmc = mc.MCMC( [Y, obs] ) mcmc.sample( 20000,5000 ) Running this second version did give me some unstable results (was returned a handful incredibly large values )
