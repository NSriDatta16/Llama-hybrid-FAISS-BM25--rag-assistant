[site]: crossvalidated
[post_id]: 451119
[parent_id]: 
[tags]: 
Proving MLE's undefined for logistic regression with separable classes in p dimensions

According to section 4.4.5 of "The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Second Edition" by Trevor Hastie & Robert Tibshirani & Jerome Friedman: ... if the data in a two-class logistic regression model can be perfectly separated by a hyperplane, the maximum likelihood estimates are undefined (i.e. infinite) I can see how this is the case in $\mathbb{R}$ as: $$l(\beta) = \sum_{x_i x_0}\{\beta_0 + \beta_1x_i -log(1+\exp{(\beta_0 + \beta_1x_i)}\}$$ where the two classes are separated by point $x_0$ and coding classes as $y_1=1$ and $y_2=0$ . Now: $$l(\beta) = \sum_{x_i x_0}\{\beta_0 + \beta_1x_0 + \beta_1(x_i - x_0) -log(1+\exp{(\beta_0 + \beta_1x_0 + \beta_1(x_i - x_0))}\}$$ Noticing $x_1 - x_0 > 0$ when $x_i>0$ , $x_1 - x_0 when $x_i , and setting $\beta_0 = -\beta_1x_0$ , this reduces to: $$l(\beta) = \sum_{x_i x_0}\{\beta_1(x_i - x_0) -log(1+\exp{(\beta_1(x_i - x_0))}\}$$ which is maximised as $\beta_1 \to \infty$ Giving the following (undefined) MLE's: $\beta_0 = -sign(x_0)\infty$ and $\beta_1 = \infty$ How can this be proven in $p$ dimensional space ( $\mathbb{R}^p$ ) and thus a separating hyperplane?
