[site]: crossvalidated
[post_id]: 326162
[parent_id]: 325689
[tags]: 
I think that your question hinges on two concepts. 1. Convexity It can be simpler to consider logistic regression model as the composition of two steps. Suppose that $y\in\{0,1\}$ are the outcomes we'd like to model as a multilinear function of some features $X$ . \begin{align} z &= X\beta \\ p &= \text{logit}^{-1}(z) \\ y &\sim \text{Bernoulli}(p) \end{align} From the definition of nullspace, if the nullspace of $X$ is nonempty, then one can choose some pair $\beta^\prime \neq \beta$ for which $X\beta = X\beta^\prime$ . Since the product is the same, so is $z=X\beta=X\beta^\prime$ also the same for either choice of coefficients. Since $z$ is the same, either choice of coefficients has the same success probability $p$ . Since the success probability is the same, the likelihood (or cross-entropy loss, if you prefer) is the same for either choice of coefficients. This implies that the optimization problem is not uniquely optimized when the nullspace of $X$ is nonempty. 2. Separability Consider the binary cross-entropy loss. \begin{align} \mathcal{L}(\theta) &= -\frac{1}{n}\sum_{i=1}^n \left[y_i \log(p_i) + (1-y_i) \log(1-p_i)\right] \end{align} For a single observation, for $y_j = 1$ , the loss will get smaller as $p$ gets closer to $1$ , so that means $z_j$ will have to get larger, implying that $\beta$ is moving in some direction to improve model fit. For $y_k = 0$ , the loss will get smaller as $p$ gets closer to $0$ , so $z_k$ will have to get smaller. (The logistic function has horizontal asymptotes at 0 and 1, so $\log(0)$ is never attained for $||\beta|| .) However, when separability is present, there is a set of features for which you can do both of these things at the same time. These are some examples. The simplest form is that a feature takes values $\pm 1$ , and all positive values correspond to the positive class and all negative values to the negative class. Making its corresponding $\beta_i$ larger will always move $z$ in the "right" direction for all examples. We can generalize the simple binary example to the case of a real-valued feature, if $x > 0$ corresponds to $y = 1$ and $x to $y = 0$ . It should be clear now that the same pathological behavior is present, since making the coefficient larger will always move $z$ in the "right" direction for both classes. Finally, we can generalize to linear combinations of features. Suppose you're fitting a model with an intercept and a real-valued feature. If $x > c$ has $y=1$ and $x has $y=0$ , choose the coefficient for the intercept as $-c$ . In this case, we're back in the same situation as (2). When separability is not present, this is not possible. Suppose that instead of $y$ always "matching" the feature, the label and feature "match" in all but 1 instance. In that case, making $\beta$ larger will not always improve the loss. In this case, $\beta$ is finite. Naturally, placing a constraint on $\| \beta \|$ implies that it cannot grow without bound. Ridge, lasso and elastic net are common choices for penalizing $\beta$ and preventing it from growing arbitrarily large. 3. Convexity & Separability It's possible that one, both or neither of these effects is present in any particular problem. Moreover, strongly convex, non-separable logistic regression has a unique, finite minimizer.
