[site]: datascience
[post_id]: 50852
[parent_id]: 
[tags]: 
Feature importance after PCA (or other dimensionality reduction methods)

I have text data which I one hot encoded and then used PCA on it (although I'm experimenting with other methods as well, LDA, NMF..). I am using the result of the dimensionality reduction as an input for a supervised classification task. Now I can use the random forest feature importance or other methods to get feature importance of the input to the supervised cls task. However, naturally these features are meaningless. I would like to know which words are most the most important for this classification. In other words, somehow propagate the feature importance score back through the PCA. Is there any known method to do it?
