[site]: crossvalidated
[post_id]: 617654
[parent_id]: 
[tags]: 
Calculation of Neurons in GPT-2: Understanding the Methodology

In a recent update from OpenAI, they mentioned the discovery of N neurons in their GPT-2 model. This finding raises the question: how did they arrive at this calculation? In their publication, they did not explicitly mention the methodology behind determining the number of neurons. Upon further investigation, I came across a set of formulas that are commonly used to estimate the number of neurons and parameters in transformer-based models like GPT-2. These formulas, although not specifically mentioned by OpenAI, provide a good starting point for understanding the calculation. The formulas are as follows: $$\text{Neurons}= H * A * L$$ $$\text{Parameters} = A * (H^2 / A) * L$$ Here, "H" represents the hidden size, "A" refers to the number of attention heads in the model, and "L" denotes the number of layers in the model. It is important to note that these formulas make certain assumptions about the model architecture. They assume that the number of neurons in each attention head and layer is constant, which might not be the case in practice. Additionally, they do not account for other architectural elements specific to transformer models, such as positional encodings and feed-forward networks. While the provided formulas offer a general approach for estimating the number of neurons, it is essential to consider the specific model architecture and consult the actual implementation details for more accurate calculations. Considering this, can anyone shed more light on how OpenAI arrived at the calculation of N neurons in their GPT-2 model? Has OpenAI explicitly mentioned their methodology or provided additional insights into the process? Any further clarification on this topic would be greatly appreciated. Thank you in advance for your input and expertise!
