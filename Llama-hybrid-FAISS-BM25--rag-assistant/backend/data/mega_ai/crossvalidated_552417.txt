[site]: crossvalidated
[post_id]: 552417
[parent_id]: 552397
[tags]: 
Basically, all 1-1 transformations (e.g. log-, sqrt-, exp-, square-transformation of positive numbers, standardization, adding constants, multiplying by a constant etc.) have no effect for random forest, XGBoost, LightGBM and similar algorithms that pick cut-points in variables. Non-1-1-transformations like taking the square of numbers that could be negative (in that case suddenly -1 and +1 get both mapped to 1 and treated as the same) on the other hand of course make a big difference. That's very different from regression models, where things are easiest for you if you transform your feature so that it's relationship with the outcome is linear (after accounting for the other features). E.g. when you do a Poisson regression with a log-link-function for number of phone calls received in a day in a call center, then I'd probably first try log(phone calls last week) as a predictor rather than the untransformed number. Standardization tends to be useful for two reasons: 1) with extremely different scales between some features you can get numerical issues with regression, and 2) if you want regularization techniques to treat different variables similarly, it's easiest if they are on the same scale. This doesn't mean, at all, that variable transformations are unimportant for XGBoost, RF and friends, it's just that it's usually more (possibly non-linear) transformations across multiple variables (and maybe records) that you may want. E.g. ratios such as when you have miles driven and fuel used for a car, then L per 100 km (or miles per gallon) could be a useful feature, which a human can understand as potentially important and provide so the model does not need to figure this out.
