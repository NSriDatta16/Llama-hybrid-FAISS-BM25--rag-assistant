[site]: stackoverflow
[post_id]: 5664412
[parent_id]: 5662261
[tags]: 
If you think about an automobile assembly line, you hear things like X number of cars come off the line in a day. That doesnt mean the raw materials started at the beginning of the line and X number completed the whole run in a day. Who knows it probably doesnt but could take a number of days per car beginning to end, thats the point of the assembly line. Imagine though if for some reason you had a manufacturing change and you basically had to flush all the cars in the line and scrap them or salvage their parts to be put on another car at some other time. It would take a while to fill that assembly line up and get back to X number of cars per day. The instruction pipeline in a processor works exactly the same way, there arent hundreds of steps in the pipeline, but the concept is the same, to maintain that one or more instructions per clock cycle execution rate (X number of cars per day) you need to keep that pipeline running smoothly. So you prefetch, that burns a memory cycle, which is usually slow but layers of caching helps. Decode, takes another clock, execute, can take many clocks esp on a CISC like an x86. When you perform a branch, on most processors, you have to throw away the instruction in the execute and prefetch, basically 2/3rds of your pipeline if you think in terms of a general, simplified pipeline. Then you have to wait those clocks for the fetch, and decode before you get back into smooth execution. On top of that the fetch, kinda by definition, not being the next instruction, some percentage of the time is more than a cacheline away and some percentage of the time that means a fetch from memory or a higher layer cache which is even more clock cycles than if you were executing linearly. The other common solution is that some processors state that no matter what whatever instruction is after a branch instruction or sometimes two instruction after the branch instruction are always executed. This way you execute as you flush the pipe, a good compiler will arrange the instructions so that something other a nop is after each branch. The lazy way though is to just put a nop or two after every branch, creating another performance hit, but for that platform most folks will be used that. A third way is what ARM does, having conditional execution. For short, forward branches, which are not all that uncommon, instead of saying branch if condition, you mark the few instructions you are trying to branch over with execute if not condition, they go into decode and execute and execute as nops and the pipe keeps moving. ARM relies on the traditional flush and refill for longer or backward branches. Older x86 (the 8088/86) manuals as well as other equally old processor manuals for other processors as well as microcontroller manuals (new and old) will still publish the clock cycles for execution per instruction. And for the branch instructions it will say add x number of clocks if the branch happens. Your modern x86 and even ARM and other processors intended to run windows or linux or other (bulky and slow) operating systems dont bother, they often just say it runs one instruction per clock or talk about mips to megahertz or things like that and dont necessarily have a table of clocks per instruction. You just assume one, and remember that is like one car per day, its that last execution clock not the other clocks getting there. Microcontroller folks in particular deal with not one clock per instruction, and have to be more aware of execution times than the average desktop application. Look at the specs for a few of them the Microchip PIC (not the PIC32, which is mips), the msp430, definitely the 8051, although those are or were made by so many different companies their timing specs vary wildly. Bottom line, for desktop applications or even kernel drivers on an operating system, the compiler is not that efficient and the operating system adds that much more overhead that you will hardly notice the clock savings. Switch to a microcontroller and put too many branches in and your code is up to 2 or 3 times slower. Even with a compiler and not assembler. Granted using a compiler (not writing in assembler) can/will make your code 2 to 3 times slower as well, you have to balance development, maintenance, and portability with performance.
