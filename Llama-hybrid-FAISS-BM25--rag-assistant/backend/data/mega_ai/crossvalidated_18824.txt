[site]: crossvalidated
[post_id]: 18824
[parent_id]: 18815
[tags]: 
In order to know if it is useful to use more features I would plot learning curves. I think this is clearly explained in the 10th Unit of Stanford's Machine Learning class, named "Advise for applying machine learning", that you can find here: http://www.ml-class.org/course/video/preview_list . Plotting learning curves you can understand if your problem is either the high bias or the high variance. As long as you increase the number of training example you should plot the training error and the test error (ie 1-accuracy), the latter is the error of your classifier estimated on a different data set. If these curves are close to each other you have an high bias problem and it would probably be beneficial to insert more features. On the other hand, if your curves are quite separated as long as you increase the number of training examples you have a high variance problem. In this case you should decrease the number of features you are using. Edit I'm going to add some examples of learning curves. These are learning curves obtained with a regularized logistic regression. Different plots are related to different $\lambda$ to tune the power of regularization. With a small $\lambda$ we have overfitting, thus high variance . With a large $\lambda$ with have underfitting, thus high bias . A good result is obtained setting $\lambda=1$ as trade-off.
