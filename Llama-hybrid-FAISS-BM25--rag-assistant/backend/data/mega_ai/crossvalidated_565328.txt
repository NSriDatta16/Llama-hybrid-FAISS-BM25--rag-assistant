[site]: crossvalidated
[post_id]: 565328
[parent_id]: 564063
[tags]: 
It really all boils down to two rules of thumb: When splitting your data, leave out what you want to predict. If you want to generalize to new hospitals, rather than new patients at the same hospital, leave out one hospital at a time when doing CV — do not leave out one patient at a time, as this only tests your ability to generalize to patients at the same hospital. When doing cross-validation, split your test data into folds that can be considered approximately independent. For example, with time series data, you want to leave out a single run/“chunk” of observations at a time. If you have a time series running from 1900 to 2000 and want to use 10 folds, the first fold should be the first 10 years, the second the next 10, and so on. The idea here is that even if a time series isn’t independent, we can think of 10 years as enough for most of the correlation to disappear, especially if we’re comparing models that are already OK at dealing with the time series structure of our data. If we assign each year to a random fold, a model can easily “Cheat” by assuming that 2020 will look just the same as 2021 and 2019, but it’s hard to predict 2020 from 2010. A correlogram can help you identify how long a lag is enough that you can consider each block “Basically independent” of the others. Some relevant papers: https://onlinelibrary.wiley.com/doi/abs/10.1111/ecog.02881 https://www.sciencedirect.com/science/article/pii/S0020025511006773 https://www.tandfonline.com/doi/full/10.1080/00949655.2020.1783262 You can also check out the Sperrorest R package for this.
