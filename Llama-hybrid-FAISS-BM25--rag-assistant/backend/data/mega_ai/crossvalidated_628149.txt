[site]: crossvalidated
[post_id]: 628149
[parent_id]: 
[tags]: 
MAP estimation for a Gaussian mixture using EM. Concerns with the covariance update formula

I am implementing the EM algorithm for a Gaussian mixture model with prior ; that is, I am using the EM algorithm to find the MAP estimate, rather than the ML estimate. As briefly discussed in section 9.4 of Bishop's Pattern Recognition and Machine Learning this can be done by modifying the M-step to optimize $$R(\theta, \theta^\text{old}) = Q(\theta, \theta^\text{old}) + \ln p(\theta) \tag 1$$ instead of $Q(\mathbf \theta, \mathbf \theta^\text{old})$ , where $Q(\theta, \theta^\text{old})$ is familiar from the regular EM algorithm . Now, I think I have correctly worked out the parameter values that maximize (1), but I am a bit concerned about the result for the component covariance matrices. Recall that a Gaussian mixture distribution has the form $$p(x | \pi, \mu, \Lambda) = \sum_{k=1}^K \pi_k \mathcal N(x | \mu_k, \Lambda_k^{-1}), \tag 2$$ with the mixing coefficients $\pi = \{\pi_k\}$ , component means $\mu = \{\mu_k\}$ , and component precision matrices $\Lambda = \{\Lambda_k\}$ being the parameters of the distribution. We treat these parameters as latent variables, and try to determine them through EM on a data set $X = \{x_n\}$ . The conjugate prior for the mixing components is a Dirichlet distribution, $$p(\pi) = \text{Dir}(\pi|\alpha) = C(\alpha) \prod_{k=1}^K \pi_k^{\alpha_k - 1}, \tag 3$$ and the conjugate prior for each mean-precision pair is a Gaussian-Wishart distribution, giving $$p(\mu, \Lambda) = \prod_{k=1}^K p(\mu_k, \Lambda_k) = \prod_{k=1}^K \mathcal N(\mu_k | \mu_0, (\beta_0 \Lambda_k)^{-1}) \mathcal W(\Lambda_k | W_0, \nu_0). \tag 4$$ Optimizing (1) with respect to $\mu$ or $\Lambda$ is simply done by setting derivatives to zero and solving. To optimize with respect to $\pi$ one must employ a Lagrange multiplier to enforce the condition $\sum_{k=1}^K \pi_k = 1$ . Assuming I have not made any mistakes, the result is as follows: $$\pi_k^* = \frac{N_k + \alpha_k - 1}{N + \sum_{j=1}^K \alpha_j - K} \tag 5$$ $$\mu_k^* = \frac{\sum_{n=1}^N \gamma_{nk} x_n + \beta_0 \mu_0}{N_k + \beta_0} \tag 6$$ $$\Sigma_k^* = \frac{1}{N_k + \nu_0 - D} \left[ \sum_{n=1}^N \gamma_{nk} (x_n - \mu_k^*)(x_n - \mu_k^*)^T \\+ \beta_0 (\mu_k^* - \mu_0)(\mu_k^* - \mu_0)^T + W_0^{-1} \right] \tag 7$$ Here $\{\gamma_{nk}\}$ are the so called responsibilities $$\gamma_{nk} = \frac{\pi_k \mathcal N(x_n|\mu_k,\Lambda_k^{-1})}{\sum_{j=1}^K \pi_j \mathcal N(x_n|\mu_j,\Lambda_j^{-1})} \tag 8$$ which were evaluated in the E step (with respect to the old parameter values), and $N_k = \sum_{n=1}^N \gamma_{nk}$ (sometimes viewed as an effective count of points assigned to component $k$ ). $D$ is the dimensionality of the data points $x_n$ . So, finally to my question . It seems to me that the denominator $N_k + \nu_0 - D$ in (7) may in some cases become negative, which would imply that $\Sigma_k^*$ fails to be positive definite, as is required for a covariance matrix. Say for example that we initialize the $k$ th component such that $\mu_k$ is very far away from all of the data. Then $\gamma_{nk}$ would be vanishingly small for all $n$ , and hence $N_k \ll 1$ . From the definition of the Wishart distribution we have $\nu_0 > D-1$ , but this is not enough to guarantee $N_k + \nu_0 - D > 0$ , since $\nu_0$ is a continuous parameter that can be arbitrarily close to $D-1$ . How should I interpret this result? Does a maximum of (1) not exist in this sort of case? Have I simply made a mistake in my calculation? Is there perhaps a written source somewhere that I can check my results against?
