[site]: datascience
[post_id]: 70101
[parent_id]: 68998
[tags]: 
If your target values are just 0 and 1, you should probably be treating it as classification, and use e.g. XGBClassifier instead of XGBRegressor . You brought up (originally in the comments on your SO post, now edited into your questions) a scenario when your true values might be limited to 0.5, 1.5, 2.5. That's unusual enough that I suspect the best answer depends on context, but: Consider ordinal regression , which treats the target values as ordered but non-numeric. If the values really are to be treated numerically, then you're left with (I think) doing it manually, which is closest to your original question. (I'd like to stress one more time that this is odd, and optimizing your loss function may do something unintuitive.) Since the predictions out of sklearn will be numpy arrays, maybe try numpy.digitize . Notes: This is useful for post-processing, but won't be easy to incorporate into the model fitting nor a pipeline (since sklearn's predict doesn't behave the same way as transform ). If you really need to process the continuous predictions before scoring inside a model fit , I think you're left with hacking the model's code a bit; and with xgboost, that'd be quite a task.
