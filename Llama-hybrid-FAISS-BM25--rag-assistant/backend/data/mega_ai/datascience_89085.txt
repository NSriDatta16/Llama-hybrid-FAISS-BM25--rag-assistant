[site]: datascience
[post_id]: 89085
[parent_id]: 89076
[tags]: 
BERT is a fairly large model that requires many data and lots of training time to achieve its state-of-the-art performance. More often than not, there isn't enough data nor resources to completely train BERT from scratch. That's where these pretrained models are useful. The weights learned from prior training serve as a useful starting point for training your dataset -- a concept refereed to as transfer learning . In a silly example, to properly generate movie tag recommendations, it first needs to learn how to "read" the tags. Or with image classification, it first needs to "see" the image. Training these models from scratch forces them to "learn" how to read or see before learning how to classify. With pretraining, the model already knows how to see/read and can better utilize training time/resources to optimize performance. Many people freeze most layers during transfer learning and focus on training the tail-end of the model as a way to reduce the training time needed. How many layers you freeze -- if you freeze any at all -- depends on how much time you're willing to put into training the model. Play around, and see what happens with BERT. Good luck!
