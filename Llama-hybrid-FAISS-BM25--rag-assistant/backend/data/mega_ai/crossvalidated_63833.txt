[site]: crossvalidated
[post_id]: 63833
[parent_id]: 63826
[tags]: 
From the formulation of a general conditional model (lets omit the bias $\lambda_y$ for simplicity ), $$ P(y|x) = \frac{1}{Z(x)} \exp( \sum\limits_{k=1}^K \lambda_{y,j}x_j ) $$ Where $Z(x) = \sum\limits_y \exp(\sum\limits_{k=1}^K \lambda_{y,j}x_j ) $ Call $w_y = [\lambda_{y,1},...\lambda_{y,k}]$ $$ \Rightarrow P(y|x) = \frac{1}{Z(x)} \exp(w_y^\top x) $$ If $y$ can take only binary values, i.e. $y \in \{+1,-1\}$, $$ P(y = 1|x) = \frac{\exp(w_{+1}^\top x)}{ \exp(w_{+1}^\top x) + \exp(w_{-1}^\top x) } $$ $$ P(y = 1|x) = \frac{1}{ 1 + \exp( (w_{-1}-w_{+1})^\top x ) } $$ Call $w' = w_{+1}-w_{-1}$ $$ P(y = 1|x) = \frac{1}{ 1 + \exp( -w'^\top x ) } $$ Similarly, $$ P(y = -1|x) = \frac{1}{ 1 + \exp( +w'^\top x ) } $$ This corresponds to binary logistic regression with $t = w'^\top x$. So by restricting the conditional model to 2 outcomes you get the binary logistic regression model.
