[site]: stackoverflow
[post_id]: 984763
[parent_id]: 984707
[tags]: 
There is no real limit -- everything is named with a 160-bit name. The size of the file must be representable in a 64 bit number so no real limit there either. There is a practical limit, though. I have a repository that's ~8GB with >880,000 files and git gc takes a while. The working tree is rather large so operations that inspect the entire working directory take quite a while. This repo is only used for data storage, though, so it's just a bunch of automated tools that handle it. Pulling changes from the repo is much, much faster than rsyncing the same data. %find . -type f | wc -l 791887 %time git add . git add . 6.48s user 13.53s system 55% cpu 36.121 total %time git status # On branch master nothing to commit (working directory clean) git status 0.00s user 0.01s system 0% cpu 47.169 total %du -sh . 29G . %cd .git %du -sh . 7.9G .
