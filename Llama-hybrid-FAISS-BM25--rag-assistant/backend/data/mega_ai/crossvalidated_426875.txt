[site]: crossvalidated
[post_id]: 426875
[parent_id]: 
[tags]: 
Understanding the trade-off between bias and variance in machine learning prediction using the math formula

About machine learning prediction, I would like to understand the trade-off between the bias and variance but using the mathematical formula. We have some train data X and y target variable. To be more exact, I have a training matrix X1 and a test matrix X2. Similarly, for our target variable: y1 and y2, respectively. One objective is to find a predict function or model - f with a hat. We use the X1 matrix and y1 variable to adjust our model f and we use the test matrix X2 and the y2 variable to measure the score of my model. We know that the concepts of overfit and underfit are related with the bias and variance concepts. However, I do not understand this relationship very well especially when looking at the mathematical formula: For example in a regression problem, if we use a straight line, we that the straight-line model will never be able to describe this dataset well. Such a model is said to underfit the data: that is, it does not have enough model flexibility to suitably account for all the features in the data; another way of saying this is that the model has high bias: But, how can I relate a high bias formula with the fact that the model pays very little attention to the training data and oversimplifies the model? The other case attempts to fit a high-order polynomial through the data. Here the model fit has enough flexibility to nearly perfectly account for the fine features in the data, but even though it very accurately describes the training data, its precise form seems to be more reflective of the particular noise properties of the data rather than the intrinsic properties of whatever process generated that data. Such a model is said to overfit the data: that is, it has so much model flexibility that the model ends up accounting for random errors as well as the underlying data distribution; another way of saying this is that the model has high variance: But how can I associate the variance formula with the fact that the model adjusts training data too much while neglecting generalization for new data? I cannot have a good intuition that relates these concepts to these formulas. The following picture can help: set.seed(123) x
