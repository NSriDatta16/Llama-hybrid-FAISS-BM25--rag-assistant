[site]: crossvalidated
[post_id]: 631538
[parent_id]: 
[tags]: 
Conditional Average Treatment Effects: What is the logic underlying using the covariates to predict the residualized scores?

I've consulted a few references, including this online book , and I've got a good grasp on using double machine learning to estimate ATE after debiasing/denoising the treatment variable ( T ) and the target variable of interest ( y ). In a nutshell, some ML algorithm uses a set of nuisance covariates to predict both T and y , and the prediction error is used to compute the residualized scores, T_res and y_res . With the variance among these scores attributable to the covariates removed, we can get an unbiased estimate of the parameter describing the causal relationship between T and y . Applications of double-ML to estimate conditional average treatment effects (CATE) puzzle me, however. Here, we construct a model in which we use the interaction between T_res and the covariates to predict y_res , as in the following code snippet: cate_model = smf.ols(formula='Y_res ~ T_res * (cov_1 + cov_2)', data = train_df).fit() CATE = cate_model.predict(test_df.assign(T_res=1)) - cate_model.predict(test_df.assign(T_res=0))) By definition, cov_1 and cov_2 do not predict Y_res , and if a linear regression was performed after normalization, their associated coefficients should theoretically be zero (they won't be exactly zero, but they will be non-significant, and the CI includes zero). Is there an explanation for why the covariates have any utility in predicting y_res in the CATE model? I understand that, without these covariates, the prediction for all individuals would just be the ATE, but I don't understand why the variability among predictions introduced by the interaction with the covariates isn't just attributed to error.
