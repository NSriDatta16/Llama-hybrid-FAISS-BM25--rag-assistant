[site]: crossvalidated
[post_id]: 428965
[parent_id]: 
[tags]: 
Comparing variance in random effects: crossed ranom effects or different models?

I am interested in making sure that a predictor I include into a regression actually explains the type of variance it should. To be more specific: in an experiment in which a number of people sees a number of images each (so every image is seen by every participant once), I expect the picture category to explain variance between individual pictures but not participants. Thus I define a null model: Null Model dv ~ 1 + (1 | person) + (1| image) and compare it to a model with the predictor Meaningful Model dv ~ 1 + image_category + (1 | person) + (1| image) in brms . Now, both models provide an estimate of the standard deviation for each "random intercept". The standard deviation of the random intercept for images should be much smaller in the second model since much of the differences between the individual images is actually due to categorical differences between them. So here is the question: Is the way I am calculating these standard deviations correct as specified above? Or should I rather have two models in each step that look like this: Null Models dv ~ 1 + person + (1| image) for the sd(random intercept between images) dv ~ 1 + image + (1 | person) for the sd(random intercept between persons) Meaningful Models dv ~ 1 + image_category + person + (1| image) for the sd(random intercept between images) dv ~ 1 + image_category + image + (1 | person) for the sd(random intercept between persons) When I calculate the models with both "random effects" in one model, the models run fine. However in the lower case with a seperate model for each "random effect", the calculation in lme4 throws singularity-warnings (I suspect this has to do with the fact that I have one observation for each participant/image combination). Bayesian models don't have this problem. But their chains either don't converge or they take forever to run when their sampling parameters are set to make them converge. Any thoughts are appreciated!
