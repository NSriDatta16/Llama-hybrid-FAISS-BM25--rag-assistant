[site]: crossvalidated
[post_id]: 615573
[parent_id]: 
[tags]: 
Effect of dK (key vector dimension) in transformers

Intuitively speaking, what's the impact of changing the $d_k$ (and $d_{kv}$ ) for transformers? My understanding is that each attention head is effectively a lower-dimensional projection of a higher-dimensional representation of the sequence, and each head effectively looks at different subspaces. Hence, more heads lead to a lower $d_{kv}$ . Presumably this means that for some tokens the model has to learn how to compress it to such a low dimension that it might not capture all the nuances of that token with respect to others within the larger sentence? Is this the best way to think about this?
