[site]: datascience
[post_id]: 120840
[parent_id]: 
[tags]: 
Modelling an anticommutative function with a neural network

Are there any tricks or best practices for modelling an anticommutative function with a neural network? That is, you're performing supervised learning trying to learn $F(Q,R)$ , and you know a priori that the function being represented has the symmetry $F(Q,R) = -F(R, Q)$ . (This implies, among other things, that $F(Q, Q) = 0$ .) Several approaches I know of offhand: Ignore it. Just model ignoring this symmetry. Issue: model will likely not be anticommutative. Issue: model will likely be inefficient, weight-wise. (This is most obvious on the first layer - the model will have to independently learn 'feature X on first input' and 'feature X on second input', even though the two are trivially redundant with each other.) Issue: input selection biases may result in output being not even approximately anticommutative. Shuffle data (either ahead of time or during training) - that is half the time when you have a training datapoint $(q, r) \rightarrow s$ , instead use $(r, q) \rightarrow -s$ . Issue: model will likely not be anticommutative (though it'll likely be closer than #1). Issue: model will likely be inefficient, weight-wise. Duplicate data including the symmetry (either ahead of time or during training). That is, when you have a training datapoint $(q, r) \rightarrow s$ , also include $(r, q) \rightarrow -s$ Issue: model will likely not be anticommutative (though it'll likely be closer than #2). Issue: model will likely be inefficient, weight-wise. Instead of training $f(Q,R)$ , instead train $g(Q, R) = f(Q,R) - f(R,Q)$ . This is guaranteed to be anticommutative (up to floating-point inaccuracy), as $g(R, Q) = f(R,Q) - f(Q,R) = -(f(Q,R) - f(R,Q)) = -g(Q, R)$ Issue: model will likely still be inefficient, evaluation-time wise. Issue: model is still somewhat inefficient, weight-wise. As an example, if a component of the first layer requires seeing feature X in both Q and R, it's not enough for the layer to learn feature X on one input. It needs to learn it on both. ...but these are all essentially blackbox approaches that apply to all supervised learning models, not just neural networks. Are there e.g. neural network architectures, techniques for structuring layers, etc that guarantee or encourage anticommutativity, either approximate or 'exact' (in quotes because floating-point)?
