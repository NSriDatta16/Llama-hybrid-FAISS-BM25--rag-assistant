[site]: datascience
[post_id]: 120817
[parent_id]: 
[tags]: 
Appropriate sample size for prediction algorithm

Our study aims to develop a Random Forest algorithm to predict the incidence of suicidal thoughts, after one year, based on the responses given to four surveys at baseline (time-1). Each survey focuses on one known risk factor for suicidal thoughts. These four risk factors have been established (filtered) after several prior studies, on a much larger number of risk factors. After one year (time-2), the follow-up survey will only ask a single question on whether the participant experienced any suicidal thoughts in the year that went by (yes/no question). The participants are 18-24-year-olds and based on existing prevalence data, we expect a modest 5-7% of people in this age-range to have suicidal thoughts. Now, at baseline (time-1), the four surveys administered include: a) presence of depression (score of >=10); b) presence of low self-esteem (score >=15); c) presence of suicidal thoughts in the last one year (the year before), yes/no answer; and d) presence of anxiety (score of >=38). Our question is - how many people should be administered this survey at baseline to effectively train the algorithm? We can assume a non-response rate of 20% after 1 year, which can be added to the estimated sample size. We plan to determine algorithm accuracy by using out of bag error, ROC, sensitivity and specificity.
