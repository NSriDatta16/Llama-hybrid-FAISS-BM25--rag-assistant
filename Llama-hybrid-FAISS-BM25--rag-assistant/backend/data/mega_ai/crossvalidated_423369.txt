[site]: crossvalidated
[post_id]: 423369
[parent_id]: 
[tags]: 
To what extend do the pitfalls of linear / logistic regression apply to other machine learning methods?

During my university days, they took great care to go through everything we could do wrong when using simple regression models. Reverse causality, omitted variable bias, heteroskedasticity, normality assumptions, non-exogenous independent variables, etc. I'm wondering about the extent to which we should worry about these things when using other families of models, particularly tree-based methods (xgboost / random forest), and neural networks. Are there any pitfalls of simple regression models that don't matter much when using trees / neural nets?
