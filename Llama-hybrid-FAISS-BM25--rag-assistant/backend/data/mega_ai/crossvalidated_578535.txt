[site]: crossvalidated
[post_id]: 578535
[parent_id]: 
[tags]: 
Probabilistic interpretation for log-loss

Suppose I am modelling a binary response variable $Y \sim B(p)$ as a function of $p$ features $X_1, \dots, X_p$ by means of an equation of the form $$ p(Y = 1 \, | \, X = x) = f(x,\theta), $$ where $\theta$ are parameters appropriate for the model under consideration. The sources I've consulted take one of two approaches to estimating $\theta$ : either they maximize the log-likelihood of the joint density, or they minimise the cross-entropy (log-loss) of the model. Both amount to the same, namely $$ \hat{\theta} = \underset{\theta}{\mathrm{argmax}}\sum_{i = 1}^n \left\lbrace y_i\log(f(x_i, \theta)) + (1 - y_i)\log(1 - f(x_i, \theta)) \right\rbrace \,. $$ Now, I understand how this formula is derived from the maximum likelihood principle, but I have a hard time grasping the conceptual explanation in terms of cross-entropy. From what I've read, the idea seems to be to take the average of the cross-entropy between the model distribution $(1 - f(x_i, \theta), f(x_i, \theta))$ and the one-hot encoded distribution $$ p((1,0)) = 1 - y_i \qquad p((0,1)) = y_i \, $$ which supposedly represents the "empirical distribution" of the i -th data point. This makes little sense to me. According to our model, $Y_i \sim B(p(Y = 1 | X_i = x_i))$ , so surely the empirical distribution of $Y_i$ should be obtained by estimating this probability, giving us the criterion $$ \hat{\theta} = \underset{\theta}{\mathrm{argmax}} \sum_{i = 1}^n \left\lbrace \hat{p}(Y = 1 | X = x_i) \log(f(x_i, \theta)) + (1 - \hat{p}(Y = 1 | X = x_i)) \log(1 - f(x_i, \theta)) \right\rbrace $$ for estimating $\theta$ . What am I not seeing here?
