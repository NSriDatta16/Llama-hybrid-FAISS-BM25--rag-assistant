[site]: crossvalidated
[post_id]: 388219
[parent_id]: 387918
[tags]: 
The choice of your figure of merit (aka loss function or error) depends on a few things. Interestingly, that you want to perform $k$ -fold cross validation isn't one of those! The idea is that your figure of merit needs to be suitable for your task at hand. I.e. it needs to answer the spelled-out questions you want to answer about your model's performance. If you want to compare several models/optimize your model (logistic regression vs. other models, or hyperparameter tuning), it is important to use a so-called (strictly) proper scoring rule . These are figures of merit that yield their optimum iff the predicted probability (of the logistic regression) equals the actual probability. Note that sensitivity, specificity, accuracy and the predictive values are no proper scoring rules. However, if you need to talk, say, to medical doctors about your model, you may need to calcualate figures of merit like sensitivity, specificity, predictive values etc. (even though they are mathematically not as well behaved as the proper scoring rules). Cross validation does not limit you to calculation of a single figure of merit: it primarily provides you with predictions for a set of test cases, and you are free to calculate as many figures of merit as you need. You say that type-I-error, which I assume to translate to a false positive, is more serious [than false negative]. Have a look at the concept of misclassification costs. However, avoiding literally at all cost will get you a trivial model that never predicts positive.
