[site]: crossvalidated
[post_id]: 245659
[parent_id]: 
[tags]: 
TPR TNR problem with logistic regression

I have implemented a modified logistic regression with update equation of weights as where $ h_{\theta}(x) = \frac{1}{1+e^{-\theta^{T}x}} $, sigmoid function. $x^{(i)}$ being $i$ th data sample, $y^{i}$, $i$ th output. I learnt about this from an online course on Machine learning by Andrew N G. Accuracy was so low, near 50%. But one thing I observed was, TPR (True Positive Rate)(sensitivity) and TNR(True Negative Rate)(specificity) are always opposite to each other. Like negatively correlated. I have checked my code several times, whether it's because of computation error, but it isn't. Ideally, TPR (True Positive Rate)(sensitivity) and TNR (True Negative rate)(specificity) must not be dependent. Here's how they are: I used 10 fold cross validation to generate 10 clsssifiers, 10 times. Overall 100 classifiers. X axis is the classifier number out of 100 classifiers found. What this correlation actually says ? How to improve the model to remove this dependency. What's the main reason for this.
