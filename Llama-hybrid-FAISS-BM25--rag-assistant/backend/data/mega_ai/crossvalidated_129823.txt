[site]: crossvalidated
[post_id]: 129823
[parent_id]: 119944
[tags]: 
Ok first off what you are doing is akin to the backbone of bayesian optimisation and quadrature. That is trying to learn a true noise free function from observations. However you seem quite confused and I think it's worth pointing you to a good reference rather than writing a massive post. Implement the GP: Look at sudo code page 19 of this Optimise Hyperparameters: Equation 5.8 and 5.9 of this chapter shows you the log marginal likelihood and its derivatives with respect to parameters. You need to maximise this (or minimise the negative of it). Depending on the kernel you wish to use the derivatives vary in their difficulty. However I show how to optimise the RBF kernel in this answer. To be honest I would not use MCMC to optimise hyper parameters, especially if this is your first GP. Instead use a convex optimisation algorithm as there a many off the shelf blackboxes that work fine for simple GPs. I can't be much help re: R. My advice though is to look at GPy, GPML or even pyGPs. Read through there code for the basic samples and read through the code of the library to see the flow of execution and how everything fits together. It's a massive help!
