[site]: crossvalidated
[post_id]: 282937
[parent_id]: 279630
[tags]: 
You seem to fine-tune the wrong things. On your feature selection: I don't think that this is done properly: You remove the good feature and all linearly correlated features. That's nice, but higher order correlated features are still there. On the other hand, strong correlation does not always mean that the feature is useless. So you should keep the good feature in the set and remove all the features that are useless. The goal is to still have a high score in the end. This way you make sure you don't remove good features as you would notice it because the score decreases. You should train a good model (at least once in a while) in order to know which features are helpful. For the hyperparameter optimization: you should fix some variables in the beginning (all except n_estimators), optimize (roughly) that parameter with a more fine grained grid (from 10 to 500 in steps of 20 for example). My general suspicion: Way to many estimators, to low learning_rate (at this stage) and to shallow (set depth to 6). Try maybe the following: eta = 0.2 n_estimators = [50...400] subsample = [0.8] depth = 6 and leave the rest as is. Of course, those depend strongly on the data. A nice guide for XGBoost hyperparameter optimization can be found here . So I'd propose you to redo the feature selection keeping the good features in the set and sometimes use a good XGBoost configuration by optimizing it. Do not forget to maybe create a small holdout set which you do not use in the feature selection. This can be used in the end to know the real performance.
