[site]: crossvalidated
[post_id]: 204961
[parent_id]: 204930
[tags]: 
Your understanding is close, but needs some extension: Simple linear regression is trying to find the formula that once you give X to it, would provide you with the closest estimation of Y based on a linear relation between X and Y . Your example of house prices, when extended a bit, shows why you end up with scatter plots and the like. First, simply dividing the price by the area doesn't work in other cases, like land prices in my home town, where regulations on construction mean that simply owning a parcel of land upon which you can build a house has a high value. So land prices aren't simply proportional to areas. Each increase of parcel area might give the same increase in parcel value, but if you went all the way down to a (mythical) parcel of 0 area there would still be an associated apparent price that represents the value of just owning a parcel of land that's approved for building. That's still a linear relation between area and value, but there is an intercept in the relation, representing the value of just owning a parcel. What makes this nevertheless a linear relation is that the change in value per unit change in area, the slope or the regression coefficient, is always the same regardless of the magnitudes of area or value. So say that you already know somehow both the intercept and the slope that relate parcel areas to value, and you compare the values from that linear relation to the actual values represented by recent sales. You will find that the predicted and actual values seldom if ever coincide. These discrepancies represent the errors in your model, and result in a scatter of values around the predicted relation. You get a scatter plot of points clustered around your predicted straight-line relation between area and value. In most practical examples you don't already know the intercept and the slope, so you have to try to estimate them from the data. That's what linear regression tries to do. You may be better off thinking about linear regression and related modeling from the perspective of maximum-likelihood estimation , which is a search for the particular parameter values in your model that make the data the most probable. It's similar to the "brute-force" approach you propose in your question, but with a somewhat different measure of what you are trying to optimize. With modern computing methods and intelligent design of the search pattern, it can be done quite quickly. Maximum-likelihood estimation can be conceptualized in ways that don't require a graphical plot and is similar to the way you already seem to be thinking. In the case of linear regression, both standard least-squares regression and maximum likelihood provide the same estimates of intercept and slope. Thinking in terms of maximum likelihood has the additional advantage that it extends better to other situations where there aren't strictly linear relations. A good example is logistic regression in which you try to estimate the probability of an event occurring based on predictor variables. That can be accomplished by maximum likelihood, but unlike standard linear regression there is no simple equation that produces the intercept and slopes in logistic regression.
