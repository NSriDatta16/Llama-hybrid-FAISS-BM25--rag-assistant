[site]: crossvalidated
[post_id]: 374466
[parent_id]: 164085
[tags]: 
I made some numerical experiments using random samples from a logistic distribution. Next I fit e.g. a normal model, a logistic model and a student-4 model. Depending on the observed random samples it can easily happen that the logistic fit is best according KS, but e.g. the normal model is better on likelihood L. Repeating such analysis many times we would observe that the logistic model is in average best according both L and KS. For large sample counts both L and KS become very stable and decisive, but L starts earlier to give stable decisions. For models with more differences in the distribution center the advantage of L is often smaller, e.g. if you want to differentiate a bimodal normal mix model and a triangular model. If your model application is more in the tail regions (e.g. high percentile estimation), then better trust L, because KS is not very sensitive to model deviations in that region (but AD is). If your data has outliers, then trusting blindly L is no good idea, whereas KS is quite a robust criteria. If you data is from a Quasi-MC analysis (e.g. LDS or LHS), then it can be smoother then pure rnd data. In such cases, GoF criteria like KS can become now more sensitive than L.
