[site]: datascience
[post_id]: 29537
[parent_id]: 29529
[tags]: 
To clarify, an n-gram usually refers to a sequence of characters, i.e. the word "clear" is comprised of the trigrams {cle, lea, ear}. I think the term you are looking for is "multi-word phrases". Embedding collections of words is referred to a couple different ways, including "sent2vec", "doc2vec" or "thought vectors". These terms generally refer to embedding a "complete" set of words, either a sentence, paragraph, collection of paragraphs, or a document. A common -- if somewhat inelegant -- approach to using a pre-trained word2vec model to embed multiple words is to embed each word separately and then take the average as the embedding for their combination. I think you'll find this article relevant: Representation learning for very short texts using weighted word embedding aggregation
