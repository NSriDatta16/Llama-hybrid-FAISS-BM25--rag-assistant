[site]: stackoverflow
[post_id]: 5625987
[parent_id]: 
[tags]: 
Map and Reduce with large datasets = how does it work in practice?

i would be thankfull for advice: http://en.wikipedia.org/wiki/MapReduce states: "...a large server farm can use MapReduce to sort a petabyte of data in only a few hours..." and "...The master node takes the input, partitions it up into smaller sub-problems, and distributes those to worker nodes..." I completely do NOT understand how this will work in Practice. Given I have a SAN(storage) with 1 Petabyte of Data. How can I distrubute that amout of data efficiently through the "Master" to the slaves? Thats something I can not understand. Given I have a 10Gibt connection from SAN to the Master, and from the Masters to the slave 1 Gbit, I can at maximum "spread" 10Gbit at a time. How can I process Petabytes withing several hours,as I first have to transfer the data to the "reducer/worker nodes"? Thanks very much! Jens
