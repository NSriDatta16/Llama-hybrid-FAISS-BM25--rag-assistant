[site]: crossvalidated
[post_id]: 451107
[parent_id]: 421712
[tags]: 
Disclaimer, I haven't read the article you posted. 1) Yes, most certainly the term prior refers to the prior probability density function (PDF) in the Bayes formula. 2.1) The prior over parameters $\theta_{i}$ would represent the PDF $p(\theta_{i}), \forall i$ . A very common assumption would be to consider $\theta_{i}\sim\mathcal{N}(\mu, \sigma^{2}), \forall i$ , where $\mu=0$ and $\sigma^{2} = 1$ . (i.e. answers your quesiton on how to use $p(\theta)$ to generate $\theta_{i}, \forall i$ ) 2.2) The likelihood $p(d|\theta)$ can be pretty much any model, (e.g.) suppose $p(d|\theta) = f(d; \theta)\sim\mathcal{N}(\mu, \sigma^2) = \frac{1}{\sqrt{ 2\pi\sigma}}e^{-\frac{1}{2}\frac{(x-\mu)^{2}}{\sigma}}$ (i.e. should tell you that the posterior amounts to averaging samples from the product of likelihood x prior) To get the curves form the paper you'll have to know what the authors used as $p(\theta)$ and $p(d|\theta)$ . The dashed line represents the true posterior a.k.a the data distribution $d$ and the coloured curves are the different posteriors if slightly change $p(\theta)$ . 3) The only thing I can think of is if you can find a correspondence such that you could argue that if a parameter stays within a given range then it induces X PDF or vice versa. (maybe?)
