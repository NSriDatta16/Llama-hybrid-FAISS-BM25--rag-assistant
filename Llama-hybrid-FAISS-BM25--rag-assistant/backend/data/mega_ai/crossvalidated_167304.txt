[site]: crossvalidated
[post_id]: 167304
[parent_id]: 
[tags]: 
Number of lags for a unit root test

I have some panel data and am suspicious of presence of unit root. As the dataset is unbalanced, I would like to use the xtunitroot fisher command in Stata. Is there any way to find out the optimal number of lags? The dfgls command for time series data provides multiple criteria for optimal lag selection, is there any similar approach also for panel data? When only one lag is included, the null is rejected on a low significance level but when the number of lags increases, the result is diametrically different.
