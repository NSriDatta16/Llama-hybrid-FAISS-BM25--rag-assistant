[site]: crossvalidated
[post_id]: 171247
[parent_id]: 150995
[tags]: 
As you might surmise from the diversity of comments, combining weak learners into strong ones isn't a task with a single, "correct" approach, but a field of approaches with varying levels of known strengths and drawbacks. For a brief introduction, the wikipedia entry for ensemble learning is a reasonable place to start. There you'll find reference to many of the methods discussed in comments, such as averaging predictions or combining them using logistic regression. In particular, here's an excerpt from the section on stacking: Stacking (sometimes called stacked generalization) involves training a learning algorithm to combine the predictions of several other learning algorithms. First, all of the other algorithms are trained using the available data, then a combiner algorithm is trained to make a final prediction using all the predictions of the other algorithms as additional inputs. If an arbitrary combiner algorithm is used, then stacking can theoretically represent any of the ensemble techniques described in this article, although in practice, a single-layer logistic regression model is often used as the combiner. The of questions in your edit seem to me best answered in individual cases using cross-validation. Whether @Tim's suggestion of inverse-error-weighted mean performs better than @yasin.yazici's logistic regression will, I suspect, vary depending on the algorithms and dataset. Will a single-layer neural net using the classifications features produce good results? Could be, as neural nets can learn non-linear functions. Maybe it will learn that when two of the probabilities agree, they're nearly always right, and produce a hidden layer node that weights these two very highly. But until you try those and compare them, who knows? Thankfully, cross-validation provides a means of comparing each of those approaches for a given, practical problem.
