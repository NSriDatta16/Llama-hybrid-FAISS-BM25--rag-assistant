[site]: crossvalidated
[post_id]: 221703
[parent_id]: 
[tags]: 
How to tune hyperparameters for LLE?

I'm running LLE using Scikit-Learn (with the LocallyLinearEmbedding class), but there are a few hyperparameters and I would like to use grid search with cross-validation to tune them. Unfortunately, I don't know what performance measure to use for that. I was thinking of using the reconstruction error, but the LocallyLinearEmbedding class has no inverse_transform() method. What's a good way to tune LLE (using Scikit-Learn)? Here's the code I'm using to perform LLE on the swiss roll dataset. from sklearn.datasets import make_swiss_roll from sklearn.manifold import LocallyLinearEmbedding X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=0) lle = LocallyLinearEmbedding(n_neighbors=10, n_components=2, random_state=0) Z = lle.fit_transform(X) I then plot the result using matplotlib: import matplotlib.pyplot as plt plt.scatter(Z[:, 0], Z[:, 1], c=t) plt.show() Looking at the plot, it seems that n_neighbors=10 is a reasonably good setting. With a lower value, I get more or less line segments, and with higher values the different layers of the swiss roll start to overlap. I'd like to find this without having to visualize the dataset.
