[site]: crossvalidated
[post_id]: 630306
[parent_id]: 
[tags]: 
Nonparametric test (akin to a sign test?) for multiple interacting effects across multiple conditions

Problem I have $K$ sets of conditions (index by $k\in\{1,..,K\}$ ), and each set has $N_k$ conditions. For example, if $K=2$ and $N_1=N_2=2$ , we could call the conditions (A,B) and (C,D). I have paired data from every possible combination, in the form of a $N_1\otimes N_2\otimes\dots\otimes N_K$ tensor. For the $K=2$ , $N_1=N_2=2$ case, this would mean a 2×2 matrix with data from each possible combination (AC,AD,BC,BD). Nothing is linear, nothing is Gaussian. A general method would be welcome—although answers restricted to the 2×2 case may suffice. I'm interested in a non-Gaussian, non-linear test for differences between these conditions. I believe there is some (nonlinear) effect for each condition (A,B) and (C,D). I would like to test whether there is a smaller effect per-pair (AC,AD,BC,BD) that cannot be explained away as a the product of marginal effects from (A,B) and (C,D). Question I think what I'm looking for is some sort of generalization of the Wilcoxon test—or really, and and all bullet-proof ways of dissociating effects under this structure. What topics/methods/tests should I consider? I would also strongly prefer something out-of-the-box in Python/scipy/statsmodels, as devising and defending a novel bespoke algorithm would be overkill. Particulars Some reader may prefer more concrete specifics. I suspect you do not need this information to fully answer the question, but in interest of thoroughness: The quantities being tested are various measures of model-fit quality from an extremely flexible generalized linear model, which you can assume is some sort of general function approximation trained with a Poisson loss The residuals are not actually Poisson, but perhaps vaguely quasi-Poisson with undetermined dispersion; I.e the data don't meet the required assumptions in order for quantities like the log-likelihood or posterior dispersion to be interpretable or compared directly across subjects without further normalization. I am able to calculate a normalized-fraction-explained-deviance based on held-out test data, R² = (log-likelihood - null-log-likelihood)/(saturated-log-likelihood - null-log-likelihood); This normalization removes some of the issues with unknown scaling/shifting of the log-likelihood values. A more bullet-proof approach to side-stepping the interpretational caveats is to analyze the cross-validated Area Under the Receiver-Operator Characteristic Curve (AUC); Or rather, 2×AUC-1, to re-scale this quantity to the range $[0,1]$ . I have four conditions with $N_1=N_2=N_3=N_4=2$ , so, lots of pairwise tests mixed together. I have hierarchical data in five subject-groups, each with 4–5 longitudinally-sampled data groups (different days), each with 5–100 subjects samples on each day. Results are paired across all conditions per subject and per day, but subjects are not matched across days (assume these are sampled with replacement from a fixed population per subject group) . While analyzing the full hierarchical structure is possible, I would be very happy for a test as to whether a particular trend is significant in aggregate per subject-group , with false-discovery rate and effect size/direct comparisons used to identify consistent trends across all groups. Related questions and prior research (I welcome corrections on my terminology, provided either as comment or edits to this question) I do not want a linear mixed-effects/ANOVA model You can find many similar questions on the CrossValidated stack exchange, but most of the answers involve some sort of linear ANOVA/mixed-effects/repeated measures. I'm very likely confused, but I don't think these are quite what I want. If I try to estimate per-condition means, say $(\mu_A,\mu_B,\mu_C,\mu_D)$ , I find that the combined effects $\{ \mu_A+\mu_C, \mu_A+\mu_D, \mu_B+\mu_C, \mu_B+\mu_D \}$ don't quite explain-away the per-condition variations in model performance. Under some variable transformations I can get a plausible looking fit under multiplicative combination, i.e. log-liner, $\{ \mu_A\mu_C, \mu_A\mu_D, \mu_B\mu_C, \mu_B\mu_D \}$ , for capturing the per-paired-condition effect as a function of separate (marginal) per-condition effects. However, this is entirely ad-hoc; As mentioned above, any procedure that relies on divining the nonlinear interaction here will not be able to dissociate model mismatch from a significant per-pair effect. This is not good. I (think) I do not want a parametric nonlinear mixed-effects model or similar I can choose a more flexible parameterization, maybe something like $x_{ij} = \phi(i,j;\theta) + \epsilon{ij}$ where $x_{ij}$ are the data (a 4×4 "paired" matrix for each sample), $\phi(i,j;\theta)$ is a nonlinear model of the mixed effects that is in some sense "coordinate-wise monotonic", and parameterized by $\theta$ , and $\epsilon{ij}$ are the per-group effects which I would like to extract and test for significance. Relatedly, perhaps a copula model could learn a condition-wise map from the $x_{ij}$ to some transformed variables $\xi_{ij}$ where the linear+Gaussian mixed-effect model is reasonable? The problem with this general strategy is that $\phi(i,j;\theta)$ is arbitrary, and all-but guaranteed to be biased in some way. Model mismatch will be subsumed into the residuals $\epsilon{ij}$ , which are the quantities that I want to test. Unfortunately, I can't see a rigorous way of dissociating the scenarios of "your estimator is biased" from the scenario of "there are per-paired-group effects". I don't think its any of the usual paired two-sample sign (rank) tesks For a simple paired-two-sample test, I usually fall back on the Wilcoxon rank-sum test , which side-steps concerns about finding the right model to capture the underlying effect. Unfortunately, this test is not valid for considering e.g. the pairwise conditions (AC,AD), (BC,BD), (AC,BC), and (AD,BD). The per-condition effect means that we cannot assume that each condition within the pair have the same distribution under the null hypothesis. Kruskal–Wallis one-way analysis of variance The Kruskal–Wallis one-way analysis of variance appears relevant, but won't tell me about particular comparisons like "Is AC>AD and BC>BD above chance". The Wiki refers the reader for to Dunn's and Conover–Iman tests. It also suggests pairwise Wilcoxon tests, but (and please correct me if I'm mistaken here), I believe the conditions for this are not met in my scenario. Friedman test This question on multi-way generalizations of sign tests may be related. It has no answers. However, it does link to the Friedman test , which may be what I want? I would need to better understand the test to verify that its appropriate for my use case. The Wiki on the Friedman test refers to the Skillings–Mack and Wittkowski tests but I have found no succinct summary of these variations. It also refers to Cochran's Q and Durban tests, which don't appear to be what I want. I've downloaded the linked original literature for the Dunn's and Conover–Iman tests, but have not yet had time to review them. My hesitation with the Friedman test is that it seems to be designed for matrix -like block designs, whereas the structure I have is really a 2×2×2×2 tensor. It's not obvious to me that this procedure for normalizing ranks remains valid when the "blocks" arise from flattening of this 2×2×2×2 into e.g. a 2×8 or 4×4 grouping to examine a particular effect in isolation. The structure of the test also doesn't seem to have an obvious generalization to the rank-D>2 case. If these concerns regarding higher-rank block structures can be allayed, then (according to the Wiki), one should apply the Eisinga c.s. exact test after the Friedman test to isolate specific effects. This is all new to me, so reference to bring me up-to-speed quickly would be welcome (if this is indeed the correct approach). Something else? Finally, I think there is a test that I should know the name of, but can't quite dredge it up from memory. I know this is wrong, but it goes something like this: I have data from a set of 4 binary conditions Ξ := {E,F,G,H} (for a total of 2^4=16 conditions) I have 16 measurements "x" from each of K subjects (indexed by k∈{1…K}) For each binary condition P ∈ Ξ: For all possible combination f,g,h ∈ {0,1}³ of the remaining Ξ\P conditions: For each subject `i`: Get the sign of x(E=1|k=i,F=f,G=g,H=h) - x(E=0|k=i,F=f,G=g,H=h) Average these signs over all combinations and subjects to get the marginal sign-rank for condition P Given the marginal sign-ranks {q₁,q₂,q₃,q₄} for each of the four conditions Construct a null distribution under the assumption that the joint-condition-wise comparisons can be explained as a product over the marginals (somehow?) Test whether the proportion of sign-rankings for a specific effect lies outside this null model. For example, I may want to test whether x(E=1|F=0,G=0,H=0) > x(E=0|F=0,G=0,H=0), above what one might be expected from the product-of-marginals models I may want to get p-values for all i∈{1…K} subject for x(E=1|k=i,F=0,G=0,H=0) > x(E=0|k=i,F=0,G=0,H=0), then FDR correct these p-values to report the estimated number of subjects with a significant deviation Does this structure remind you of any tests not mentioned so far? This pseudocode might be a half-remembered hallucinatory corruption of the significance tests for contingency tables , such as Fisher's exact test McNemar's exact test , and Cochran-Mantel-Haenszel test . Instead of having a binary variable for inclusion in each (joint) contingency for each subject, it does something like treat the "Is condition E=1 > condition E=0", conditioned on a specific subject $i\in\{1\dots K\}$ , and specific values for the other conditions $(f,g,h)\in\{\;0,1\;\}^3$ . If I narrow down the questions I want to ask, and commit to one/two-tail alternatives in advance, I can then reduce my 2×2×2×2 testing conundrum into a collection of tests on conditional contingency tables? I'm very hesitant to do this without further expert consultation; It seems too avant garde for me to trust myself to address all caveats/assumptions rigorously. Even if valid, I'd still need to generalize one of the contingency table tests to a 3-dimensional 2×2×2 structure.
