[site]: datascience
[post_id]: 126084
[parent_id]: 
[tags]: 
Correct way to take a subset of a dataset?

I am attempting a binary classification problem (using Weka). My dataset has 100,000 rows, 14 attributes (1 output variable). It takes already too long just to open the dataset in excel so I just know my laptop won't be able to run all that data through supervised learning algorithms in a reasonable time. What is the correct way to take a subset of a dataset? Say I want to try build a Logistic Regression model on 1000 rows only? Do I need to carefully preserve the "information" contained in the 100,000 rows or can I cherry pick whatever 1000 rows I want? For instance in the full dataset the ratio in the output variable is: Y=20%, N=80%. When I tried just taking the first 1000 rows from my excel file the ratio in the output variable became: Y=18%, N=82%. Either way that is unbalanced which is bad apparently. Can I choose 1000 rows such that I have an even split of data in the output variable or will that end up making a less accurate model and maybe bad practice? Likewise I have a "Gender" attribute (male vs female). Is it ok to engineer a subset of the 100,000 dataset (for training) that has an equal split of gender even if though the original 100,000 dataset is 65% female, 35% male? As you can see I don't really understand the basics. I want to take a subset of a large dataset but I'm not sure what you are allowed to do because maybe you cherry pick things then your training dataset is not longer representative of the original dataset.
