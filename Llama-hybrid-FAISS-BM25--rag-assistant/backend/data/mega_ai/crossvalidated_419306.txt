[site]: crossvalidated
[post_id]: 419306
[parent_id]: 376275
[tags]: 
I am not sure if a single hidden layer is sufficient, but it can be shown that if your input is in $\mathbb{R}^k$ , you will need at most $k$ hidden layers. See Theorem 3.1 in https://ieeexplore.ieee.org/document/5443743 Theorem 3.1 : For any continuous monotone nondecreasing function $f : K \rightarrow \mathbb{R}$ , where $K$ is a compact subset of $\mathbb{R}^k$ , there exists a feedforward neural network with at most $k$ hidden layers, positive weights, and output $O$ such that $|f(\mathbf{x}) - O_{\mathbf{x}}| , for any $\mathbf{x} \in K$ and $\varepsilon > 0$ .
