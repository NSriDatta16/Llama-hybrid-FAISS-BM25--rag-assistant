[site]: crossvalidated
[post_id]: 367510
[parent_id]: 367500
[tags]: 
While I can't guarantee this is well-suited to your data, the following approach is pretty standard for when you want to cluster high-dimensional data. You basically want to dimensionally reduce your data and then apply a clustering algorithm which would perform poorly in high dimensions but works well in low-dimensions. PCA, TSNE, UMAP and AutoEncoders are all dimensionality reduction algorithms which you might consider using. All of these algorithms have hyperparameters whose performance greatly affects the output. There is, in all cases, a hyper parameter related to the dimension you want to project into, so one thing you could try is to project into 2- or 3-d and plot what your data looks like. For PCA, you can see what fraction of the variance is retained as a function of dimension and for AutoEncoders, you could plot your divergence as a function of dimension (although this will be computationally costly). Once your data is dimensionally reduced, you can try the usual suspects (e.g. KMeans), although most likely you'll want to use DBscan or HDBScan. I'm intrigued as to how you know you have 55 clusters. If, as you say, there are 55 classes, does this mean your data is labelled? If so, why do you want to cluster? Regardless of this, if you know you have 55 clusters, this is something you can harness very nicely in this approach. I'm not an expert on (H)Dbscan, but I don't believe you set the cluster number, you set a density parameter and it will find the clusters and tell you the cluster number. Consequently, if for some reasonable combination of dimensionality reduction hyperparameters and (H)Dbscan hyperparameters, you are getting 55 clusters (or close to), you're probably on the right track. Documentation for your dimensionality reduction implementation and your clustering algorithm should give you an idea for what constitutes reasonable ranges of parameter values.
