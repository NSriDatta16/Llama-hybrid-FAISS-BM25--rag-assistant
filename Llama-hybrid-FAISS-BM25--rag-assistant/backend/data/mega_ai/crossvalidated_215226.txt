[site]: crossvalidated
[post_id]: 215226
[parent_id]: 215154
[tags]: 
As part of an algorithm for learning a purely predictive model, variable selection is not necessarily bad from a performance viewpoint nor is it automatically dangerous. However, there are some issues that one should be aware of. To make the question a little more concrete, let's consider the linear regression problem with $$E(Y_i \mid X_i) = X_i^T \beta$$ for $i = 1, \ldots, N$, and $X_i$ and $\beta$ being $p$-dimensional vectors of variables and parameters, respectively. The objective is to find a good approximation of the function $$x \mapsto E(Y \mid X = x) = X^T \beta,$$ which is the prediction of $Y$ given $X = x$. This can be achieved by estimating $\beta$ using combinations of variable selection and minimisation of a loss function with or without penalisation. Model averaging or Bayesian methods may also be used, but let's focus on single model predictions. Stepwise selection algorithms like forward and backward variable selection can be seen as approximate attempts to solve a best subset selection problem, which is computationally hard (so hard that the improvements of computational power matters little). The interest is in finding for each $k = 1, \ldots, \min(N, p)$ the best (or at least a good) model with $k$ variables. Subsequently, we may optimise over $k$. The danger with such a variable selection procedure is that many standard distributional results are invalid conditionally on the variable selection. This holds for standard tests and confidence intervals, and is one of the problems that Harrell [2] is warning about. Breiman also warned about model selection based on e.g. Mallows' $C_p$ in The Little Bootstrap ... . Mallows' $C_p$, or AIC for that matter, do not account for the model selection, and they will give overly optimistic prediction errors. However, cross-validation can be used for estimating the prediction error and for selecting $k$, and variable selection can achieve a good balance between bias and variance. This is particularly true if $\beta$ has a few large coordinates with the rest close to zero $-$ as @probabilityislogic mentions. Shrinkage methods such as ridge regression and lasso can achieve a good tradeoff between bias and variance without explicit variable selection. However, as the OP mentions, lasso does implicit variable selection. It's not really the model but rather the method for fitting the model that does variable selection. From that perspective, variable selection (implicit or explicit) is simply part of the method for fitting the model to data, and it should be regarded as such. Algorithms for computing the lasso estimator can benefit from variable selection (or screening). In Statistical Learning with Sparsity: The Lasso and Generalizations , Section 5.10, it it described how screening, as implemented in glmnet , is useful. It can lead to substantially faster computation of the lasso estimator. One personal experience is from an example where variable selection made it possible to fit a more complicated model (a generalised additive model) using the selected variables. Cross-validation results indicated that this model was superior to a number of alternatives $-$ though not to a random forest. If gamsel had been around $-$ which integrates generalised additive models with variable selection $-$ I might have considered trying it out as well. Edit: Since I wrote this answer there is a paper out on the particular application I had in mind. R-code for reproducing the results in the paper is available. In summary I will say that variable selection (in one form or the other) is and will remain to be useful $-$ even for purely predictive purposes $-$ as a way to control the bias-variance tradeoff. If not for other reasons, then at least because more complicated models may not be able to handle very large numbers of variables out-of-the-box. However, as time goes we will naturally see developments like gamsel that integrate variable selection into the estimation methodology. It is, of course, always essential that we regard variable selection as part of the estimation method. The danger is to believe that variable selection performs like an oracle and identifies the correct set of variables. If we believe that and proceed as if variables were not selected based on the data, then we are at risk of making errors.
