[site]: crossvalidated
[post_id]: 89093
[parent_id]: 88809
[tags]: 
This is not really an answer to my question, but I would like to provide an explicit and very simple simulation demonstrating the symptoms I described, and I don't want to clutter the question too much. Let's consider the absolutely simplest case possible. I will take 400 one-dimensional samples: 200 are equal to -1 and 200 are equal to +1. All samples located at -1 belong to class A, and all samples located at +1 belong to class B. The classifier will measure the mean (centroid) of each class and assign every test sample to the class whose centroid is closer. Nothing can be simpler than that. Here is an illustration ("A" means 200 points from class A, "B" means 200 points from class B): ---------A-----------B---------> I can do exactly the same Monte Carlo cross-validation as I described above: I randomly select $K=4$ test cases, 2 from each class, train the classifier on the remaining training set, and classify these four; this is repeated 100 times. Obviously the number of correct classification is 400, i.e. 100% accuracy. Especially for @cbeleites I can also run a usual 100-fold (stratified) CV, the accuracy is also 100%. Note that it does not make sense to iterate this CV, because nothing will change. And now we do the shuffles. I randomly shuffle the labels, repeat exactly the same procedure and get the number of correct classifications $B_j$. Then shuffle the labels again, and repeat it 100 times. Results: mean of $B_j$ is around 200 (very close, like 198-202 on different runs), so chance level. But variance of $B_j$ is in the range of 400-900 (on different runs). This is true for both Monte Carlo CV and standard 100-fold CV. The variance is always MUCH larger than the expected binomial variance that should be equal to 400*0.5*(1-0.5) = 100. Now either I am overlooking a completely stupid mistake (which is absolutely possible!), or we have a big problem with all the reasoning from Confidence interval for cross-validated classification accuracy , because the binomial intervals don't make any sense. For example, if I spoil my ideal class separation by relabeling 80 points from class A to B and vice versa, then my actual number of decoded samples becomes 240. The stability over iterations of CV is perfect. The binomial confidence interval binofit(240,400) is [0.55, 0.65] which excludes 0.5 so we would conclude that the decoding is significant. But the variance of shuffled correct decoders is still on average around 500-600, so standard deviation is around let's say 22, so 95% interval for the null hypothesis of random decoding is around 200$\pm$45, which includes 240, which means not significant . As far as I can see this problem has nothing to do with different CV folds not being independent, it's entirely different problem that has to do with finite sample size. The larger the sample size, the smaller the variance of $B_j$ (now I am back to Monte Carlo cross-validation, where I can still classify 400 cases even if the sample size is much larger). But I have to go to sample sizes above 10000 to get variance close to 100. Such sample sizes are way beyond realistic. Update: In comments above @julieth quoted a paper by Jiang et al. Calculating Confidence Intervals for Prediction Error in Microarray Classification Using Resampling : "... the test [I think they mean "training" -- amoeba] set on which prediction of the $i$-th case has $n-2$ specimens in common with the training set on which prediction of the $j$-th is based, hence, the number of prediction errors is not binomial" . In other words, they claim that the reason for non-binomiality is that training sets are not mutually exclusive. Turns out, Nadeau and Bengio have a mammoth 49-page long paper about it called Inference for the Generalization Error where they discuss exactly this issue in great detail. I did not believe it at first, so I used the simulation above to check this claim. If I increase the total number of samples to 4000, I can use the Monte Carlo CV procedure with 100 folds (each time classifying 4 test cases) to get 400 predictions. On the shuffled data (I increased the number of shuffles to 1000) the mean number of correct classification is 199 and the variance is 346: still a whole lot more than 100 even though the sample size is now as large as 4000. But now I can also do the following: split my 4000 samples in 100 stratified parts of 40, and in each part use 36 samples to predict 4. I will also get 400 predictions, but this time all training sets are mutually exclusive. The outcome (also after 1000 shuffles): mean 199, variance 98. Wow! Nadeau, Bengio, and @julieth seem to be right. And binomial assumption seems to be dead wrong. I wonder how many papers there are out there using binomial confidence intervals and tests...
