[site]: datascience
[post_id]: 126852
[parent_id]: 
[tags]: 
Why do object detection model adversarial masks look different from those of image classifiers?

I was messing around to observe the behavior for adversarial attacks on image classifiers, and decided to try it with an object detector as well. I realize that inference time attacks are more complex on object detectors. I noticed that an L1 Loss based un-targeted adversarial attack on these models yielded some interested masks. The image classifier generated the usually expected noise mask that covers the entire size of the image that is popular in literature, but the object detector, under the same conditions, generated a mask that closely resembles the objects in the image. It kind of looks like it is applying the adversary to the important features of the objects. What is the reasoning behind this? Thank you for your help! 1st image: Adversarial mask for object detector (FRCNN), 2nd image: Original Image
