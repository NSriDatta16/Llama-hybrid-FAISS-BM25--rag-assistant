[site]: crossvalidated
[post_id]: 189965
[parent_id]: 177564
[tags]: 
I think you are on to something by noting the influence of what the machine learning currently touts as the 'best' algorithms for dimensionality reduction. While t-SNE has shown its efficacy in competitions, such as the Merck Viz Challenge , I personally have had success implementing SOM for both feature extraction and binary classification. While there are certainly some who dismiss SOMs without justification besides the algorithm's age (check out this discussion , there are also a number of articles that have been published within the last few years that implemented SOMs and achieved positive results (see Mortazavi et al., 2013 ; Frenkel et al., 2013 for instance). A Google Scholar search will reveal that SOMs are still utilized within a number of application domains. As a general rule, however, the best algorithm for a particular task is exactly that - the best algorithm for a particular task. Where a random forest may have worked well for a particular binary classification task, it may perform horribly on another. The same applies to clustering, regression, and optimization tasks. This phenomenon is tied to the No Free Lunch Theorem , but that is a topic for another discussion. In sum, if SOM works best for you on a particular task, that is the algorithm you should use for that task, regardless of what's popular.
