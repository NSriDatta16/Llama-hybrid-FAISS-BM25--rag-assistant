[site]: crossvalidated
[post_id]: 331271
[parent_id]: 331258
[tags]: 
Tl;dr : the standard procedure is to use a minibatch it for both forward and backward pass. What you're trying to do is to run a stochastic gradient descent on your neural network. The step are similar to what you've written, but differ in an important way: Sample a small number of data points. Run forward pass on a minibatch . Run backward pass on a minibatch. I'm not sure if I correctly understood the part about "parameters get updated, but the working residual doesn't". However, I'll still try to answer that part :) Even though it may seem weird, there is a solid reasoning behind that. Namely, even though you never get the true gradient vector, you get a vector that is on average equal to the gradient. The hope is that you'll go in the "right" way most of the time and (because you can run a lot of steps) you'll eventually get in the optimal point. Hope that helps :)
