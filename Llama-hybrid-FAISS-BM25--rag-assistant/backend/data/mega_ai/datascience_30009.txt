[site]: datascience
[post_id]: 30009
[parent_id]: 30008
[tags]: 
In parametric models such as linear regression, logistic regression and multi-layers perceptrons, weights are updated with regards to the " difference " between the output of your model and the real label. More precisely, weights are updated using the gradient descent / backpropagation procedure. It is composed of two parts : the forward pass and the backward pass . For a given observation (or a set of observations), the forward pass is about feeding the model with observations and output a result a . This output "a" is then compared with the real value, the label y . Using some cost function metrics (such as Absolute Error or Square Error for regression purposes, cross-entropy for classification purposes...), we can then compute j(y,a) which is the error between the output and the real value. We can now run the backward pass which is about computing the derivative of the cost function with regards to any weight / bias coefficient in the logistic regression / neural network. We can the update coefficients such as : Where alpha is the learning rate. So to answer your question, weights are not updated until they reach the expected value. We are just trying to reach the minimum in cost function surface by running gradient descent procedure.
