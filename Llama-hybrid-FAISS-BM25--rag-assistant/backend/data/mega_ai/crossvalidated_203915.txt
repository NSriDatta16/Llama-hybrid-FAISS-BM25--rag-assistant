[site]: crossvalidated
[post_id]: 203915
[parent_id]: 203816
[tags]: 
sklearn's logistic regression doesn't standardize the inputs by default, which changes the meaning of the $L_2$ regularization term; probably glmnet does. Especially since your gre term is on such a larger scale than the other variables, this will change the relative costs of using the different variables for weights. Note also that by including an explicit intercept term in the features, you're regularizing the intercept of the model. This is generally not done, since it means that your model is no longer covariant to shifting all the labels by a constant.
