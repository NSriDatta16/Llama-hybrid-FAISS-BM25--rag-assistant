[site]: crossvalidated
[post_id]: 511281
[parent_id]: 
[tags]: 
Why is the expected reward of this $\epsilon = 0$ greedy algorithm equal to 1?

I am learning Reinforcement Learning and the concept of $\epsilon$ -greedy algorithms. In an example on page 28 of Richard Sutton's book Reinforcement Learning: An Introduction, second edition , there was an example of a multi-armed bandit problem ran for empirical solutions called the ten-armed testbed. The example is defined as follows: This was a set of 2000 randomly generated k-armed bandit problems with $k = 10$ . For each bandit problem, the action values, $q_{*}(a), a = 1, . . . , 10$ , were selected according to a normal (Gaussian) distribution with mean 0 and variance 1. Then, when a learning method applied to that problem selected action $A_{t}$ at time step $t$ , the actual reward, $R_{t}$ , was selected from a normal distribution with mean $q_{*}(A_{t})$ and variance 1 Varying levels of $\epsilon$ were tried, and it produced the average reward over time for each $\epsilon$ . The figure is seen below. My confusion comes from when $\epsilon$ is equal to 0. In this case, we take action which is the highest value of our estimates $Q_{t}(a)$ for each $t$ . As we are effecively sampling from a "gaussian of a gaussian", then wouldn't the average reward over time always be 0? Why is that as the time steps increase, $\epsilon = 0$ always yields average reward equal to 1? Why is it, firstly, non-zero, and why is it 1?
