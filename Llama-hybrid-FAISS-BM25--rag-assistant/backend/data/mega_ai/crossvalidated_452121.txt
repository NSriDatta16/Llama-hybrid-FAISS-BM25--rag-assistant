[site]: crossvalidated
[post_id]: 452121
[parent_id]: 106003
[tags]: 
Here is an alternate solution for neural network users: You can approximate the CDF with 1 / (1 + 2*exp(-sqrt(2*pi)*x)) (extending this approximation). And exploit the fact that most deep learning framework have a numerically stable implementation of log(1 + exp(x)) as the softplus activation function . The resulting formula can be written in two lines of pytorch: def log_standard_normal_cdf(x): return -F.softplus(np.log(2) - x*np.sqrt(2*np.pi))
