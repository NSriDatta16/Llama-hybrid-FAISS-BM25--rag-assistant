[site]: crossvalidated
[post_id]: 614154
[parent_id]: 614151
[tags]: 
Random forest inherently has this ability. Contrary to popular belief, a random forest does no classification. A random forest outputs values on a continuum. Those values can be compared to a threshold in order to make classifications (below threshold is negative, above is positive), but that is separate from the random forest (even if software typically bundles the two together). For instance, the randomForest::predict.randomForest in R has an argument type that returns values on a continuum if you set type = prob . Importantly, these can be interpreted as probabilities, and those give you your confidence scores. Not only can you claim a classification, but you can claim the probability of that classification. Random forests have this pesky attribute, however, that they tend to lack calibration. That is, when they predict an event to happen with probability $p$ , the event does not happen in $p$ proportion of the times. The sklearn documentation has some nice material to introduce you to the idea behind calibrating probabilities. Two terms worth knowing for your research are "Platt scaling" and "isotonic regression".
