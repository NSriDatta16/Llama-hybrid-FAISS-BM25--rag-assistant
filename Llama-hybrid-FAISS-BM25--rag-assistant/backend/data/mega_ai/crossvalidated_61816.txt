[site]: crossvalidated
[post_id]: 61816
[parent_id]: 61779
[tags]: 
You can probably look at your data in another way. Start with a t-test of the difference in average response time between the groups (that would be equivalent to a test that the Pearson – not Spearman – correlation between group membership and average time is 0). From there, you can easily add other variables in the model (i.e. turn it into an ANOVA or linear regression), consider transformations, rank-based statistics or generalized linear model if needed, etc. Because of the specifics of this type of experiments, it's also standard practice to analyze only successful trials and to exclude large response times before computing the means (there are better and more principled ways to deal with this problem but you definitely need to do something about it). It could also be more appropriate to analyze the response time to individual trials directly, using a multilevel model (see the literature on the “language-as-fixed-effect fallacy” in psycholinguistics). @AdamO is right, thought, doing all this after the fact is at best suggestive. If you fiddle with the model until you get something you like, p -values become meaningless. Also, you might have heard of the mounting debate on reproducibility within psychology. I personally think that the ease with which we explain away unexpected results through ancillary variables or details of the procedure is part of the problem. The effect might not be what you expected but a second look at the literature might reveal that it was not as strong as it first seems. If that's the case, the “disappearance” of the effect really does not need any explanation.
