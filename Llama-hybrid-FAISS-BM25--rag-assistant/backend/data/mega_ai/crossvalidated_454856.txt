[site]: crossvalidated
[post_id]: 454856
[parent_id]: 
[tags]: 
Why is temporal difference learning biased in reinforcement learning?

When I learn reinforcement learning from David Silver's online video, I saw "the objective of TD learning, $r_t + \gamma V(s_{t+1})$ is a biased target for learning value function. " I know the definition of "biased", but I'm not sure why TD learning is biased. The followings are my questions: Under the framework of functional approximation for value function, is it because now the parameters of $V(s_{t+1})$ are not correct value during training, hence the current value of $V(s_{t+1})$ could be totally meaningless which causes it to be biased? In that case, suppose the parameters will converge at the end of training procedure. In that case, is $r + V(s_{t+1})$ still biased or not? For tabular cases, is $r + V(s_{t+1})$ still biased? I think it is now an unbiased estimator of value function. Suppose the process is episodic with total number of $T$ steps. The value function at the last time step $V(s_T)$ is clearly unbiased. This causes the estimation of $V(s_{T-1})$ to be unbiased. We can repeat the process to prove for any $t$ , $V(s_t)$ is unbiased. Is this correct?
