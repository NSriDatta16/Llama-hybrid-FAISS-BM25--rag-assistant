[site]: crossvalidated
[post_id]: 177545
[parent_id]: 177486
[tags]: 
Adding a label to each distinguishable edited sentence in set 'c' might provide a way to evaluate intra-judge variability in scoring, based on each judge's different ratings of the quality of the same sentence. Note that this labeling of distinguishable sentences doesn't add anything to evaluating the upstream issues of the input sentence-editor-output sentence process, your primary interest. You could incorporate this intra-judge variability in your model, but you don't have much evidence for such intra-judge variability, and it doesn't seem to be of much concern to you. In that case there really is no need to add this extra complexity to what seems otherwise to be a nicely balanced crossed design, and would probably cost you unnecessary degrees of freedom. If you want the best estimate of inter-judge variability, you will have to code things in a way that you compare responses of all judges to each combination of input sentence and editor. If the inter-judge variability is not of concern, and you simply want to average scores among the judges and perhaps correct for differences in mean scores among judges, then you don't have to code the individual output sentences. So the answer to your question depends on how much you want to know about inter-judge variability.
