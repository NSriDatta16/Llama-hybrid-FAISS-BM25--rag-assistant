[site]: crossvalidated
[post_id]: 620243
[parent_id]: 620224
[tags]: 
Many, many researchers in many fields do regression and all sorts of other statistics on non-random samples. For some such methods, there are ways to adjust things. But often the sampling is a pure convenience sample. Random sampling is often impossible (e.g. when you don't have a list of the population), or extremely expensive, or unethical or otherwise difficult or impossible. What to do? The regression itself is fine. The math works. What's hard is the inference from the results (including interpreting the p values, CIs and so on). So, you can still use the regression as a representation of this sample, and you can assume that it is a random sample from some population. You can try to add control variables to deal with some of this. Is such research "valid and appropriate"? It can be. For instance, all the early information on smoking causing cancer was from non-random samples and non-randomized treatment (not exactly OK to randomly assign people to the smoking and non-smoking groups!) One of my favorite statistics books is Statistics as Principled Argument by Robert Abelson. He argues that statistics and data analysis are not about what is absolutely right and wrong, but about what you can justify. It is part (but only part) of a principled argument. In the case of smoking, other parts of the argument were discovered over time (such as the way that the chemicals in cigarettes cause cancer).
