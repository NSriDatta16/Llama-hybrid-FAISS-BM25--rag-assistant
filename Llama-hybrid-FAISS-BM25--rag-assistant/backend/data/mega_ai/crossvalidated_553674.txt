[site]: crossvalidated
[post_id]: 553674
[parent_id]: 553658
[tags]: 
... I use train and test sets for building and testing my model (this includes all the preprocessing steps and nested cv) and use the valid set to test my final model. Test set is typically used as final evaluation and validation set for tuning. So, I'll be using the general convention below. Do we use Nested Cross validation to tune the hyperparameters during the cross validation process or do we first select the best performing algorithm via cross validation and then tune the hyperparameter for only that algorithm? You shouldn't compare models without tuning them. One way to do is nested cross validation where we have two levels of validation sets, i.e. train_inner + validation_inner + validation_outer . Each algorithm's hyperparameters (HP) are tuned on validation_inner . Then, in the outer loop, each algorithm with its best HP set is trained on train_inner + validation_inner , which is train_outer , and tested on validation_outer . If this is CV, the sets of best HPs change in each outer loop evaluation, but in the end the two algorithms are compared. The winner algorithm will be tuned on train_outer and tuned validation_outer . Finally, the best model with its best HP is trained on train_outer + validation_outer , which we can call train , and it's tested on a test set for last performance report. One other way would be linearizing everything to a model list, e.g. models = [RF(n_est=10), RF(n_est=100), SVM(C=1), SVM(C=10)] and selecting the best among them using a single validation level, without nesting. This may be desirable when compute time is a concern since nested CV takes more time and resources. About your Method 1 , you shouldn't tune your HP and test your success on the same set: clf = GridSearchCV(model, ...) clf.fit(X_train, y_train) score = cross_val_score(clf, X_train, y_train, ...) This is an optimistic view on the success of the tuned model since it's being measured on the test it was tuned. About your Method 2 , the nested_score should be calculated solely on the test set, not X_iris , which the full dataset. Because, it also contains the training set. None of the implementations you shared conforms with the methods I explained above. A paper that explains both methods I proposed and favours the second one (calls it as flat cv): https://arxiv.org/pdf/1809.09446.pdf
