[site]: crossvalidated
[post_id]: 618925
[parent_id]: 
[tags]: 
What is sigma for a time series and how to calculate it

Trying to apply changepoint detection. Many algorithms such as Binary segmentation and PELT use sigma ( std deviation ) as a parameter if the number of changepoints are unknown. Confused if sigma is std deviation of noise or std deviation of time series data itself. Also is there a formula to calculate it as std deviation f the noise ?
