[site]: crossvalidated
[post_id]: 580577
[parent_id]: 580562
[tags]: 
The Kingma et al. paper is very readable, and a good place to start understanding how and why VAEs work. Kingma, Diederik P., and Max Welling. " Auto-encoding variational Bayes. " arXiv preprint arXiv:1312.6114 (2013). "Another example used MSE loss (as follow), is MSE loss a valid ELBO loss to measure p(x|z)?" Yes, MSE is a valid ELBO loss; it's one of the examples used in the paper. the authors write We let $p_\theta(\bf{x}|\bf{z})$ be a multivariate Gaussian (in case of real-valued data) or Bernoulli (in case of binary data) whose distribution parameters are computed from $\bf{z}$ with a MLP (a fully-connected neural network with a single hidden layer, see appendix C). In other words, we can use any $p_\theta(x|z)$ we like; we just need to implement a network to decode $z$ into $x$ and then measure the loss according to $p_\theta$ . Simple manipulations shows that minimizing MSE loss is the same as maximizing the joint probability of the gaussian density wrt the mean parameter. See: How do we get to the MSE in the loss function for a variational autoencoder? "Can we use other loss functions as a reconstruction loss in VAE, such as Huber loss?" Yes, choosing the Huber loss corresponds to replacing $p_\theta(\bf{x}|\bf{z})$ with another density, specifically the Huber density. For the Huber loss given by $$ H_\alpha(x) = \begin{cases} \frac{1}{2} x^2 & | x | \le \alpha \\ \alpha \left(|x| - \frac{1}{2}\alpha \right) & | x | > \alpha \end{cases} $$ we can work backwards from the corresponding likelihood to show that the probability density implied by the Huber loss is given by $$ p_\theta(y) \propto \exp \left(-H_\alpha(y)\right). $$ But knowing this fact isn't strictly necessary from a practical standpoint -- you can simply replace MSE with the Huber loss.
