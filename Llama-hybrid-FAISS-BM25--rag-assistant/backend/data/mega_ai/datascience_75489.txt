[site]: datascience
[post_id]: 75489
[parent_id]: 75483
[tags]: 
The "total" reward is calculated with a lot of different models, which one are we measuring? The total reward in principle should be calculated by freezing a policy (equivalent to freezing the parameters for a parametric policy) and then computing the average of multiple rollouts on the environment, the usual Monte Carlo. If it works well - it trains a lot, does loop a lot until done and moves its weights away from the good policy and can get unstable. One of the reasons of having a replay buffer in off-policy learning is in principle to prevent precisely this catastrophic forgetting in the neural network parametrized for the policy under dataset shift (the shift in distribution of states, actions and rewards observed). If we save a model - which model are we really saving? You would probably want to compute the rewards like mentioned earlier (becomes sort of a validation set) and pick the model which gives you the best mean reward (and probably variance). This is often expensive and a cheap proxy is your first version of the algorithm which is biased in reward computation but the bias reduces to zero as the policy converges in theory. The changes you've made to the data collection and learning loop is a valid one. However, this is usually not done because in principle this is sample inefficient . You have to wait to complete a full trajectory before incorporating that information into the model. A more attractive approach usually followed in literature is where you'd ideally do it every few steps.
