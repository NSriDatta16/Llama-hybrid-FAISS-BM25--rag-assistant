[site]: datascience
[post_id]: 48670
[parent_id]: 
[tags]: 
Knowing when a GAN is overfitting (sequence classification study)

I have sequences of long, sparse 1_D vectors (3000 digits, made of of 0s and 1s) that I am trying to classify. I have previously implemented a simple CNN to classify them with relative success (with keras). data: label, sequence 0,0000000000010000000...... .... 1,00000000000000000001........ I am trying to create a GAN that returns a sequence that is emblematic of the sequences that have a '1' label. I have attempted to re-implement typical analysis of 2D tensors (i.e. images) on my 1-D data. Thus far I have managed to get functioning code that creates 'fake' vectors that are similar looking to the original ones.... however my problem is that I don't know how to assess what is a sufficient number of epochs... I was advised that assessing the performance on test data might be a good indicator of this. Hence as a separate task to the normal procedure of the GAN training I also assess the generator's performance on test data in each epoch. See above in the upper-left panel I have plotted the loss profiles of the generator and discriminator, in the upper right I have plotted the accuracy achieved at each epoch when validating on the test data and the bottom three figures are exemplary sequences output at different stages of the training (indicated by arrows). The code that I use is pretty similar to that at the following link: https://github.com/osh/KerasGAN/blob/master/MNIST_CNN_GAN.ipynb (If the actual code is needed please comment below and I will include it). See below another exemplary output (from a different task): Here the generator seems to be overfitting and generating all 0 sequences- which is undesirable. As an attempt to mitigate this I trained the discriminator to reject all 0 sequences before freezing its' weights and initiating the GAN: discriminator=load_model('..discriminator..') X_filler,y_filler=np.zeros((100,3000, 1)),np.zeros(100) discriminator.fit(X_filler,y_filler,epochs=3,batch_size=100) discriminator.trainable = False ......initiate GAN training.... Still after doing this the above shown problem persists.. I cannot find a good recommendation as to when to stop training.. The following source recommends: Stop criteria: the number of Generatorâ€™s failures (failed attempts to fool the Discriminator) is [almost] equal to the errors of Discriminator to distinguish artificially generated samples from real samples. https://www.quora.com/What-is-the-stop-criteria-of-generative-adversarial-nets-Can-I-treat-it-as-a-multi-objective-problem But I am not sure how to assess when this is happening based on the test accuracy graph displayed above. Overall my questions are: 1. When can I know based on test validation, done in each epoch, when to stop training the GAN? 2. How do I know if my generator is overfitting to the and what can I do to mitigate this?
