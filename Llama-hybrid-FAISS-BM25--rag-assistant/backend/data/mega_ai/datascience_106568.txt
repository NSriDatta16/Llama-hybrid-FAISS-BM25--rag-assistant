[site]: datascience
[post_id]: 106568
[parent_id]: 
[tags]: 
How to combine preprocessor/estimator selection with hyperparameter tuning using sklearn pipelines?

I'm aware of how to use sklearn.pipeline.Pipeline() for simple and slightly more complicated use cases alike. I know how to set up pipelines for homogeneous as well as heterogeneous data, in the latter case making use of sklearn.compose.ColumnTransformer() . Yet, in practical ML one must oftentimes not only experiment with a large set of model hyperparameters, but also with a large set of potential preprocessor classes and different estimators/models. My question is a dual one: What would be the preferred way to set up a pipeline where the selection of text vectorizers is treated as an additional hyperparameter for grid or randomized search? Additionally, what would be the preferred way to set up a pipeline where the selection of multiple models can also be treated as an additional hyperparameter? What about optimizing the model-specific hyperparameters in this case? In the first case a common use case is text vectorization: treating the choice of CountVectorizer() or TfidfVectorizer() a hyperparameter to be optimized. In the second case a practical use case could be selecting between various algorithms or in the case of multiclass classifications, whether to use OneVsOneClassifier() or OneVsRestClassifier() . I understand that this might exactly be what AutoML solutions have been developed for. I heard of out-of-the-box AutoML solutions in the past that can do automatic model selection with hyperparameter tuning but I have no experience in any of them, thus I don't know if they indeed provide an answer for the general topics I described in this post.
