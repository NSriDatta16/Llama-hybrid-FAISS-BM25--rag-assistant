[site]: stackoverflow
[post_id]: 463615
[parent_id]: 463569
[tags]: 
The reason why they are ignored is that you have the fully qualified URL in the robots.txt file for Disallow entries while the specification doesn't allow it. (You should only specify relative paths, or absolute paths using /). Try the following: Sitemap: /sitemap_index.xml User-agent: Mediapartners-Google Disallow: /scripts User-agent: * Disallow: /scripts # list of articles given by the Content group Disallow: /Living/books/book-review-not-stupid.aspx Disallow: /Living/books/book-review-running-through-roadblocks-inspirational-stories-of-twenty-courageous-athletic-warriors.aspx Disallow: /Living/sportsandrecreation/book-review-running-through-roadblocks-inspirational-stories-of-twenty-courageous-athletic-warriors.aspx As for caching, google tries to get a copy of the robots.txt file every 24 hours in average.
