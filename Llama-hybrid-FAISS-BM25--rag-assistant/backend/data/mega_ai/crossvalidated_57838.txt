[site]: crossvalidated
[post_id]: 57838
[parent_id]: 57836
[tags]: 
You could make the (very naive) assumption that the best features for SVM will also be the best ones for other classifiers as NB. Therefore you can (as you said) select the best features for NB. You can also include the similarity ones in this set. Other (better) option is to apply feature selection weighting to see which features separate the classes better. In Text Classification, chi^2 is commonly applied and it will provide an idea (and a ranking) of how good the features are. One question though, why do you want to select the features in advance in this fashion? I mean, apart from scalability issues, SVM is known for dealing well with very large number of features. I think I have only partially answer your question, I hope it helps...
