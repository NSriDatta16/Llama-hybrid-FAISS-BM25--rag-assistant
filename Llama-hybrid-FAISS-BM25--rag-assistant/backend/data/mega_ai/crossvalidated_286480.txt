[site]: crossvalidated
[post_id]: 286480
[parent_id]: 138319
[tags]: 
When dealing with streaming data, you might not want/need to embed all the points in history in a single t-SNE map. As an alternative, you can perform an online embedding by following these simple steps: choose a time-window of duration T, long enough so that each pattern of interest appears at least a couple of times in the window duration. scroll the window as the data streams in, with a time-step dt much smaller than T. For each position of the window, compute a t-SNE embedding of the data points in the time window. seed each embedding with the outcome of the previous one. In t-SNE, one needs to choose the initial coordinates of the data points in the low-dimensional space. In our case, because we choose dt much smaller than T, two successive embeddings share most of their data points. For all the shared data points, match their initial coordinates in the present embedding to their final coordinates in the previous embedding . This step will ensure that similar patterns have a consistent representation across successive embeddings. (in the sklearn implementation in python, the seed parameter is "init". By default, the sklearn implementation sets the initial position of the points randomly) Note 1: It is important that the patterns of interest appear at least once in any given time window, so that the memory of the representation does not get lost as the window slides through the dataset. Indeed, t-SNE typically does not converge to a unique solution but only to a local minimum, so if the memory is lost, a similar pattern might be represented in very different ways in two instanciations of an embedding. Note 2: This method is particularly relevant when dealing with non-stationary time series, where one wishes to track patterns that evolve slowly through time. Indeed, each embedding is here taylored specifically to the small time window on which it is computed, ensuring that it captures temporally local structure in the best way (contrarily to a full embedding of the whole non-stationary dataset). Note 3: In this method the successive embeddings cannot be parallelized, because one needs the outcome of the previous embedding in order to seed the next one. However, because the seed (i.e. initial coordinates of the points) is well chosen for most points (all shared points between succesive embeddings), an embedding typically converges very fast, in a few iterations only. For an example of application of this method to non-stationary time series, see this article ( ICLR 2016, Learning stable representations in a changing world with on-line t-SNE: proof of concept in the songbird ), where it was successfully applied to track the emergence of syllables across development in the songbird.
