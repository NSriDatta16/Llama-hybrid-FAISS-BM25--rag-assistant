[site]: crossvalidated
[post_id]: 602195
[parent_id]: 119970
[tags]: 
I suggest approaching this problem from a slightly different, more pragmatic angle. Rather than considering a general rule of thumb, you could take a strictly empirical approach to consider if your forecasting model is good enough for your specific short-time-series dataset . "Good enough" should be understood in the same sense of what is a "good" or "useful" time series model. In principle, a good or useful model is one that does a better job forecasting than a naive (default) forecast model. (This is a universal principle for forecasting evaluation, not just for short time series.) Common naive forecast models for non-seasonal data are the last outcome in the series or the mean of the last two to four outcomes. Common naive forecast models for seasonal data are the mean of outcomes for the last season, or slightly more complex, the outcome for the same period from the last season. These naive forecast models tend to provide surprisingly robust forecasts that can be hard to beat by simple forecast models. So, if any forecast model that you can create on your short series can do better than an appropriate naive forecast model, then it should be considered good enough to go by --it has demonstrated that it is better than the naive model and so you are justified to prefer your forecast model. The only caveat is that with a short time series, it is very difficult to demonstrate that your forecast model is not better just by chance or that it is not overfitting the past data. For such assurance, you need to validate the performance of your model, and sliding window validation will normally cut the already-small dataset size in at least half, so its results might not be reliable. In summary: even for a short time series, you can argue that your forecasting model is good or useful by demonstrating its superiority to a naive default forecast, but it might be difficult to demonstrate that such superiority is reliable.
