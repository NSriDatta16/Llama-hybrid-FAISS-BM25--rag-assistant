[site]: crossvalidated
[post_id]: 615106
[parent_id]: 559009
[tags]: 
A less trivial explanation can be that converting gray-scale to RGB is effectively adding a layer of ReLU neurons with fixed parameters. For example converting an image to RGB using the viridis colour-map is using something similar to three piecewise linear functions that can be composed out of ReLU functions. This addition has the effect of increasing the depth (extra layer) and width (potential extra neurons in subsequent layers) of the neural network. Both effects can potentially improve the performance of the model (if it's current depth and/or width was not sufficient). Width A simple example is converting a single grayscale channel to three rgb channels by simply copying the image three times. This can be effectively like performing some ensemble learning . Your neural network or decision tree may converge to different patterns on the different channels which can be later on merged in an average with a final layer or classification boundary. You could also see it alternatively as effectively making several of the hidden layers three times wider (but not fully connecting them, and adding only three times more connections). This can create some potential for different training and convergence which is potentially better. Depth The additional color mapping layer may allow to create patterns that are not possible with less connections. The flexibility is increased. The simplest example is an image of a single pixel that passes through a layer with a single neuron with a step function (so this is an example where even the number of neurons remain the same and the width of the subsequent network is not changed). For BW, this is a two parameter function (weight $w_1$ and bias $b$ ) that effectively makes a classification based on whether or not the input is above or below some level. For RGB, then we get two additional parameters, $w_2$ and $w_3$ , for the extra channels, and this makes it possible to create more patterns. For example we can make a classification when the grayscale pixel has either a high or either low value. Obviously one can achieve the same when not converting to rgb, and instead add more neurons or an additional layer. But possibly the cases where the rgb performed better did not test this out. Also the conversion to rgb, using some useful scale, is making a hardcoded seperation into shadows, middle tones and highlights, which a NN needs training and extra neurons for. (So in a way it is adding an extra layer which is regularised. And also it is adding pre-trained information because the human decision to choose a particular colour map instead of another; ie the human chooses the trigger points of the ReLU layer and the conversion to rgb is additional information). Anyway, this simple example is a case where it is possible to prove that rgb can perform better (if we compare with a limited model, like only a fixed number of neurons and layers).
