[site]: crossvalidated
[post_id]: 351045
[parent_id]: 350220
[tags]: 
It is true that some of the machine learning models have the ability to handle the non-linearity and interaction between variables, however, depends on the situation, I see three reasons it becomes necessary. Some models like linear regression don't handle non-linearity automatically, in that case, you need to create extra features to help. For example below: if you have the following dataset that all the $Y = 1$ of the target variable clustered at the center of a circle like area. If you are given only two features, $x_1$ and $x_2$. A simple linear model of $y = x_0 + c_1x_1 + c_2x_2$ will not find any way to classify the target variable. So, instead, you need new quartic features to capture the non-linearity: $y = x_0 + c_1x_1^2 + c_2x_2^2$. If you know in advance that some features (from business knowledge or experience), it may help create them to speed up the runtime of the model and make it easy for your model. For example, in your example of the Titanic data and if you are using a decision tree classification model. If you know that old ladies (age & gender) are more likely to survive, by creating a single feature that captures the information, your tree can make one split on the new variable instead of making two split on the two variables. It may speed up the computation time if you know in advance that the feature is significant. In the real world, you won't get a single dataset like Kaggle provides. Instead, you get information from all over the place. For example, if you want to predict customer attrition for an online retail company like Amazon, you have customer demography info, purchase transaction info. You need to generate a lot of feature from different sources, in this case, You will find a lot of useful features can be obtained/aggregated from the transaction level. As Andrew Ng puts it: Often times, the ability to do feature-engineering defines the success or failure of a machine learning project.
