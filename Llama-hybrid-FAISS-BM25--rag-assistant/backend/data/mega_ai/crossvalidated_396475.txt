[site]: crossvalidated
[post_id]: 396475
[parent_id]: 
[tags]: 
Are there "typical" mean and variance patterns that are learned in batch normalization in CNNs?

When we apply batch normalization to the output of a convolutional neural network layer we learn $\gamma$ and $\beta$ and scale the layers output based on a running average and those learned values. Assuming we're using standard relu activations, we know that negative values become dead neurons, zero gradient. I want to know if it's typical for relu activated conv nets to learn positive values of $\beta$ which would shift the outputs into the positive realm, or whether that is not necessarily beneficial and typical values of $\beta$ commonly vary across positive and negative ranges. Ideally I'm interested in knowing what the distribution of $\beta$ values is across many trainings and datasets. Or to gain intuition as close as I can come to this.
