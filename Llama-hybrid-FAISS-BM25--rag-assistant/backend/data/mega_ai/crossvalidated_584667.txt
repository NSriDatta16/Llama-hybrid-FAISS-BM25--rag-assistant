[site]: crossvalidated
[post_id]: 584667
[parent_id]: 584640
[tags]: 
Linear regression or logistic regression are linear models, scaling is a linear transformation. Generalized linear models take the form $$\begin{align} \eta &= \beta_0 + \beta_1 X_1 + \dots + \beta_k X_k \\ E[y|\mathbf{X}] &= g(\eta) \end{align}$$ After scaling, we want to get the same, optimal, result as before, so we want $g(\eta)$ to be the same, so $\eta$ needs to be the same. That means, we can focus only on the linear predictor $\eta$ and the reasoning would be the same regardless of the link function and family of the GLM. We use scaling of the data for two reasons: to improve interpretability of the parameters, and in some models (e.g. regularized) to prevent numerical problems in optimization. Notice that when you scale the data, the result would be the same as with non-scaled data, but the parameters would adapt to the scaling. So if you have linear function $\alpha + \beta X$ then after scaling $X' = (X-\bar x)/s_X$ you have to also scale the parameters accordingly. So you have $$ \require{cancel} \begin{align} \alpha + \beta X &= \alpha' + \beta' X'\\ &= (\alpha + \beta' \tfrac{\bar X}{s_X}) + \beta' \tfrac{X - \bar X}{s_X} \\ &= (\alpha + (\beta s_X) \tfrac{\bar X}{s_X}) + (\beta s_X) \tfrac{X - \bar X}{s_X} \\ &= \alpha + \beta \cancel{s_X} \tfrac{\bar X}{\cancel{s_X}} + \beta \cancel{s_X} \tfrac{X}{\cancel{s_X}} - \beta \cancel{s_X} \tfrac{\bar X}{\cancel{s_X}} \\ &= \alpha + \cancel{\beta \bar X} + \beta X - \cancel{\beta \bar X} \end{align}$$ You can see the same with a data example: > set.seed(42) > n x1 x2 y lm(y ~ x1 + x2) Call: lm(formula = y ~ x1 + x2) Coefficients: (Intercept) x1 x2 3.180 2.922 2.050 > lm(y ~ I((x1-1)/20) + I((x2-2)/40)) Call: lm(formula = y ~ I((x1 - 1)/20) + I((x2 - 2)/40)) Coefficients: (Intercept) I((x1 - 1)/20) I((x2 - 2)/40) 10.20 58.44 82.02 > 3.17952 + 58.4442*(1/20) + 82.015*(2/40) [1] 10.20248 > 2.92221 * 20 [1] 58.4442 > 2.05038 * 40 [1] 82.0152 So as you can see: After the scaling, the parameters can be higher than three, or lower than negative three. The values of the parameters would depend on the values of the parameters before scaling and on the standard deviations of the raw data. There is really no reason for assuming such bounds.
