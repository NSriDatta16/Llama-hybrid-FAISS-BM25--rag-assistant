[site]: crossvalidated
[post_id]: 605711
[parent_id]: 605709
[tags]: 
Itâ€™s the usual logistic regression maximum likelihood estimation. Your model matrix has a column of all $1$ s (for the intercept), a column of your variable, a column of your variable squared, and a column of your variable cubed. Once you have that, you get let the maximum likelihood numerical optimization go. That you created the model matrix by calculating exponents of your variable does not matter to the optimization. This is a key point in generalized linear models: the interpretation of models and their coefficients may depend on how you designed the features, but the math of estimating those parameters does not know, let alone care, how you got to those numbers. As far as how the calculation goes, there is no closed-form solution like there is in an OLS linear model that has $\hat\beta_{ols}=(X^TX)^{-1}X^Ty$ . The typical way to approximate the maximum likelihood estimate is to have a computer numerically optimize a multivariable function. $$ \underset{\hat\beta_0,\hat\beta_1,\hat\beta_2,\hat\beta_3}{\arg\max}\left\{ \sum_{i=1}^n\left[ y_i\log(\hat y_i)+(1-y_i)\log(1-\hat y_i) \right] \right\}\\ \hat y_i =\dfrac{1}{ 1+\exp\left(-\left( \hat\beta_0+\hat\beta_1x_i+ \hat\beta_2x_i^2+ \hat\beta_3x_i^3 \right) \right) }\\ y_i\in\left\{0,1\right\} $$ This equation comes from the binomial log likelihood, since the conditional distribution in a logistic regression is binomial. The typical syntax in R would be to use the glm function with family = binomial , which defaults to the logistic regression link function. Parameter estimation with this implementation will be via maximum likelihood estimation, as is desired in the OP.
