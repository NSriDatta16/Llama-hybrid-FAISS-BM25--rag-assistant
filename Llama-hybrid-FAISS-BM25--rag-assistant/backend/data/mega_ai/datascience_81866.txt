[site]: datascience
[post_id]: 81866
[parent_id]: 
[tags]: 
Why does the overfitting decreases if we choose K to be large in K-nearest neighbors?

I am studying machine learning and I am focusing on K-nearest neighbors . I have understood the algorithm, but I have still a doubt, which is on how to choose the K for the number of neighbors . I have seen that choosing $K=1$ lads to a frafmented decision boundary, while if I choose a larger value of K, I obtain a smoother boundary: I think that I have lear the reason of why this happens. But I have also studied that if $K=1$ we do overfitting, whiile if we increase $K$ the overfitting decreases, but why? So, why does the overfitting decreases if we choose K to be large in K-nearest neighbors?
