[site]: datascience
[post_id]: 113431
[parent_id]: 
[tags]: 
Trimming "unused" neurons from the bottleneck of an autoencoder

I'm working with autoencoding data in segments, and working with the latent space afterwards (I am also working on VAEs, but this segment of the project concerns deterministic AEs). I've noticed that within the latent space, there are some neurons that don't seem to be particularly relevant to reconstruction. Here I've plotted the size of the latent space that's allocated for each segment of data against the real number of neurons it uses. I consider a neuron unused when its standard deviant is (strictly) equal to zero. Is there any way to remove these completely from the network? I know that tensorflow has a "pruning" library, but after reading the docs a bit I'm not convinced that it's what I need, or at least I haven't found the section that tells me what I need to do to implement this.
