[site]: crossvalidated
[post_id]: 428372
[parent_id]: 
[tags]: 
How are higher level posteriors modeled in a hierarchical Bayesian model?

Hope the question isn't too naive. I've been playing around with examples from Doing Bayesian Data Analysis by Kruschke, and in the Therapeutic Touch data section there's this multi-level model example which follows this structure. I do understand the fact, that the higher level priors are based of off the data, but I do not understand how it's done. I understand how MCMC algorithm works in sampling from the posterior based off the prior and likelihood, especially in "flat" structures, but I'm having a hard time wrapping my head around hierarchical model process. The model above has uninformative priors. That is the omega param is initialized as beta distribution $\alpha=1$ and $\beta=1$ , and kappa is also quite vague (mean=1 and sd=1), and the theta one is based of off both omega and kappa . So yeah, how is a model, such as the one above, interpreted by JAGS or pymc3 in a way that it can create posterior distribution for each parameter ( omega , kappa and theta ) while their all dependent on each other? I'm just having difficulty understanding how the omega (and kappa ) parameter, which was initialized as an uninformative prior, changes it's shape depending on the result of theta , which is dependent on omega (and kappa ) itself.
