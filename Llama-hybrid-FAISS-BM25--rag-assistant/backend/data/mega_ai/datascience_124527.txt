[site]: datascience
[post_id]: 124527
[parent_id]: 53045
[tags]: 
Don't think about each layer performing a dimension transformation on your data. If your input is 20-dimensional and your network structure is composed of a input layer, a 100-dimensional hidden layer and a 1-dimension output layer like this: from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense # Define the neural network using Sequential model = Sequential([ Dense(100, input_dim=20, activation='relu'), # Input layer with 20 inputs and hidden layer with 100 neurons Dense(1, activation='sigmoid') # Output layer with 1 neuron and sigmoid activation function ]) The first layer just receives the input and the second layer is of dimension 100. But note that this dimension transform is actually not made "in a layer" but in between the layers. This is performed by a simple matrix (of dimension 20x100 in this case) multiplication. The activation function is only applied after the matrix multiplication. So we could write this neural net mathematically: $x \in \mathbb{R}^{20}$ $y_1 = Relu(W_1.x) \quad ; \quad W_1 \in \mathbb{R}^{100 \times 20}$ $y_2 = Simoid(W_2.y_2) \quad ; \quad W_2 \in \mathbb{R}^{1 \times 100}$ Where $W_1, W_2$ matrices represend the layers steps and $y_2$ would be the final output.
