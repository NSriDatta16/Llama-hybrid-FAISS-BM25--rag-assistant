[site]: crossvalidated
[post_id]: 388471
[parent_id]: 77527
[tags]: 
I believe the comments are pointing at the answer, but not stating it bluntly. So I'll be blunt. The V formula cited here is specific to linear ridge regression. They don't say it is the same as PRESS, they say it is a rotation-invariant version of PRESS. The "rotation-invariant" part is what makes this generalized. Efron's paper is about logistic regression, customized to that context. If you want to see the math translation between the two contexts, the right book to read is Elements of Statistical Learning, 2ed, by Hastie, Tibshirani, and Freedman. They offer that book free, online: https://web.stanford.edu/~hastie/Papers/ESLII.pdf . Another helpful read on GCV is Generalized Additive Models by Simon Wood. His treatment integrates GCV in general with applications in regression and logistic regression. If you look at the ESL book, p 244, you see basically the same symbology. They refer to that large matrix product you have as the Smoother matrix (I'd say its a Hat matrix, or a near cousin). They describe the Smoother $S$ as the mapping from $y$ to $\hat{y}$ $$ \hat{y}=S y $$ $S$ can be used to calculate leave one out CV values, one for each row in the data. For linear models , the $S$ matrix plays the role of the Hat matrix in regression diagnostics. However, they say it may be computationally challenging or unnecessary to work that out, and the GCV approach is a slightly more general version of same idea. They offer a formula for the approximation of GCV: $$ GCV(\hat{f})=\frac{1}{N}\sum_{i=1}^{N}\left[\frac{y_i - \hat{f}(x_i)}{1-trace(S)/N}\right]^2 $$ This is quite similar in behavior to the AIC in many models. The $trace{S}$ is the effective number of parameters. The $n\lambda$ piece you quote is more generally a trace of $S$ . As far as I can understand, in the abstract GCV is an approximate version of leave one out crossvalidation, but in some cases, (I believe ridge regression), it is exact. That's a main point in the Golub paper. Good luck, write back if you learn more.
