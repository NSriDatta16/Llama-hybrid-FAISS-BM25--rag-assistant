[site]: crossvalidated
[post_id]: 232604
[parent_id]: 
[tags]: 
What is bad about maximizing $\sum_{i}{p(x_{i}|\theta)}$ instead of maximizing $\prod_{i}{p(x_{i}|\theta)}$

When learning a model $\theta$, what is bad about maximize $\sum_{i}{p(x_{i}|\theta)}$ instead of maximizing $\prod_{i}{p(x_{i}|\theta)}$? In a typical learning setting, a model is trained to maximize likelihood. Likelihood is probability of data given a model, and with IID assumption, it can be written as $\prod_{i}{p(x_{i}|\theta)}$. This is what we usually do in statistics and machine learning. Here is an instructive question: Why shouldn't it be a summation of probabilities? I think chaining questions can be raised and it's worth contemplating upon. How bad if we perform learning with such a criterion? Is there any pathological or illustrative examples? Is ML always superior than the not-well-formulated sum-of-probabilities?
