[site]: crossvalidated
[post_id]: 530259
[parent_id]: 530233
[tags]: 
A marginal distribution on $Y$ associated with a joint distribution on $(U,Y)$ is by definition the distribution of $Y$ on its own, that is, ignoring the realisation of $U$ . As an example, consider the joint density $$f(u,y)=\varphi(u;0,1)\times\varphi(y;u,1)$$ for which $Y|U=u$ is conditionally distributed as a $\mathcal N(u,1)$ $Y$ is marginally distributed as a $\mathcal N(0,2)$ variate. and both statements are simultaneously correct. This implies that if one jointly simulates $(U,Y)$ from the joint Normal distribution, the resulting $Y$ will be distributed from $\mathcal N(0,2)$ . As an illustration, here are 100 $\mathcal N(u_i,1)$ densities when $$U_1,\ldots,U_{100}\stackrel{\text{iid}}{\sim} \mathcal N(0,1)$$ and their average density (in red): which is very close to a $\mathcal N(0,2)$ density. A mistake in considering that since $Y\sim \mathcal N(U,1)$ it cannot be $\mathcal N(0,2)$ at the same time is to forget that $U$ is a random variable. (There is some similarity with accept-reject in that, while all simulations are from the proposal distribution $g$ , those that are accepted are also simulations from the target distribution $f$ .) Another illustration is provided by finite mixtures of distributions : if $$Y\sim f(y)=\sum_{i=1}^k\omega_i f_i(y)\qquad\sum_{i=1}^k\omega_i=1$$ this distribution can be represented as the marginal distribution (in $Y$ ) of a joint distribution on $(U,Y)$ such that $U$ is marginally distributed as a Multinomial variable: $$\mathbb P(U=i)=\omega_i \qquad i=1,\ldots,k$$ and the conditional distribution of $Y$ given $U$ is $$Y|U=i \sim f_i(y)$$ Generating an index $i$ with probability $\omega_i$ and then a realisation from $f_i(\cdot)$ is equivalent to generating from $f(\cdot)$ .
