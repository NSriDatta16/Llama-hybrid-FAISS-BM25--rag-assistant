 dictionary is present in the text. These N + 1 {\displaystyle N+1} binary variables sum up all the information about an e-mail. Decomposition Starting from the joint distribution and applying recursively Bayes' theorem we obtain: P ( Spam ∧ W 0 ∧ ⋯ ∧ W N − 1 ) = P ( Spam ) × P ( W 0 ∣ Spam ) × P ( W 1 ∣ Spam ∧ W 0 ) × ⋯ × P ( W N − 1 ∣ Spam ∧ W 0 ∧ ⋯ ∧ W N − 2 ) {\displaystyle {\begin{aligned}&P({\text{Spam}}\wedge W_{0}\wedge \cdots \wedge W_{N-1})\\={}&P({\text{Spam}})\times P(W_{0}\mid {\text{Spam}})\times P(W_{1}\mid {\text{Spam}}\wedge W_{0})\\&\times \cdots \\&\times P\left(W_{N-1}\mid {\text{Spam}}\wedge W_{0}\wedge \cdots \wedge W_{N-2}\right)\end{aligned}}} This is an exact mathematical expression. It can be drastically simplified by assuming that the probability of appearance of a word knowing the nature of the text (spam or not) is independent of the appearance of the other words. This is the naive Bayes assumption and this makes this spam filter a naive Bayes model. For instance, the programmer can assume that: P ( W 1 ∣ Spam ∧ W 0 ) = P ( W 1 ∣ Spam ) {\displaystyle P(W_{1}\mid {\text{Spam}}\land W_{0})=P(W_{1}\mid {\text{Spam}})} to finally obtain: P ( Spam ∧ W 0 ∧ … ∧ W N − 1 ) = P ( Spam ) ∏ n = 0 N − 1 [ P ( W n ∣ Spam ) ] {\displaystyle P({\text{Spam}}\land W_{0}\land \ldots \land W_{N-1})=P({\text{Spam}})\prod _{n=0}^{N-1}[P(W_{n}\mid {\text{Spam}})]} This kind of assumption is known as the naive Bayes' assumption. It is "naive" in the sense that the independence between words is clearly not completely true. For instance, it completely neglects that the appearance of pairs of words may be more significant than isolated appearances. However, the programmer may assume this hypothesis and may develop the model and the associated inferences to test how reliable and efficient it is. Parametric forms To be able to compute the joint distribution, the programmer must now specify the N + 1 {\displaystyle N+1} distributions appearing in the decomposition: P ( Spam ) {\displaystyle P({\text{Spam}})} is a prior defined, for instance, by P ( [ Spam = 1 ] ) = 0.75 {\displaystyle P([{\text{Spam}}=1])=0.75} Each of the N {\displaystyle N} forms P ( W n ∣ Spam ) {\displaystyle P(W_{n}\mid {\text{Spam}})} may be specified using Laplace rule of succession (this is a pseudocounts-based smoothing technique to counter the zero-frequency problem of words never-seen-before): P ( W n ∣ [ Spam = false ] ) = 1 + a f n 2 + a f {\displaystyle P(W_{n}\mid [{\text{Spam}}={\text{false}}])={\frac {1+a_{f}^{n}}{2+a_{f}}}} P ( W n ∣ [ Spam = true ] ) = 1 + a t n 2 + a t {\displaystyle P(W_{n}\mid [{\text{Spam}}={\text{true}}])={\frac {1+a_{t}^{n}}{2+a_{t}}}} where a f n {\displaystyle a_{f}^{n}} stands for the number of appearances of the n t h {\displaystyle n^{th}} word in non-spam e-mails and a f {\displaystyle a_{f}} stands for the total number of non-spam e-mails. Similarly, a t n {\displaystyle a_{t}^{n}} stands for the number of appearances of the n t h {\displaystyle n^{th}} word in spam e-mails and a t {\displaystyle a_{t}} stands for the total number of spam e-mails. Identification The N {\displaystyle N} forms P ( W n ∣ Spam ) {\displaystyle P(W_{n}\mid {\text{Spam}})} are not yet completely specified because the 2 N + 2 {\displaystyle 2N+2} parameters a f n = 0 , … , N − 1 {\displaystyle a_{f}^{n=0,\ldots ,N-1}} , a t n = 0 , … , N − 1 {\displaystyle a_{t}^{n=0,\ldots ,N-1}} , a f {\displaystyle a_{f}} and a t {\displaystyle a_{t}} have no values yet. The identification of these parameters could be done either by batch processing a series of classified e-mails or by an incremental updating of the parameters using the user's classifications of the e-mails as they arrive. Both methods could be combined: the system could start with initial standard values of these parameters issued from a generic database, then some incremental learning customizes the classifier to each individual user. Question The question asked to the program is: