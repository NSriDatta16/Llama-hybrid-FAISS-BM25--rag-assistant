[site]: crossvalidated
[post_id]: 268726
[parent_id]: 268722
[tags]: 
I don't think there is any general answer to that question. It depends on many factors such as the size of your specific corpus, to what extent the specific corpus differs from the wide generic corpus, what you plan to do with the word embeddings, etc. Personally, if time permits I try both, and my experience word embedding that was trained on a wide generic corpus tends to works best. e.g. https://arxiv.org/abs/1606.03475 : Regarding the choice of token embeddings, we tried pre-training them on the i2b2 2014 dataset and the MIMIC dataset\footnote{For MIMIC, we used the entire dataset containing 2 million notes and 800 million tokens.} using word2vec and GloVe. We also experimented with the publicly available token embeddings such as GloVe trained on Wikipedia articles. The results were quite robust to the choice of the pre-trained token embeddings. The GloVe embeddings trained on Wikipedia articles yielded slightly better results, and we chose them for the rest of this work. We believe the GloVe embeddings' superiority is mostly due to the much larger size of the Wikipedia corpus than the others on which we trained word2vec. Note that you could train the word embeddings on the wide generic corpus + the specific corpus.
