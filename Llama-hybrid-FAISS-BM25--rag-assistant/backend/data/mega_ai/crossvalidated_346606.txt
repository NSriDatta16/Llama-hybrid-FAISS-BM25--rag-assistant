[site]: crossvalidated
[post_id]: 346606
[parent_id]: 346583
[tags]: 
Likely no. To reiterate the reasons why you can't do this in a linear model: The p-values in other coefficients will change when another variable is added or removed from a model. (this is addressed by proper stepwise regression). The p-value is not a valid measure of predictive accuracy in models. (this is not addressed by proper stepwise regression). Stepwise models are not smart enough to know what order variables should be entered/retreated from the model. For instance, if two variables $x_1$ and $x_2$ are numerically identical, the choice of first entry for one versus the other is completely arbitrary and determined by pragmatic reasons only. The differences in interpretation and meaning can be staggering. Consider, for instance, two highly collinear variables that have a world of difference in interpretation for the few cases where they differ: number of pregnancies and number of children , hours spent watching any TV programming and hours spent watching educational TV programming , et cetera. The further complications of mixed models: There are many types of p-values you could consider for mixed models. The legitimate ones obtained from bootstrapping or profile likelihood are usually too computationally expensive to calculate at each iteration. A mixed model is more likely to omit between-cluster confounders even though they explain a large portion of intracluster variance: the random effect is never considered or treated like an effect. Given the general lack of enthusiasm by the statistical community for stepwise modeling, you should just abandon the idea outright. Instead, select, by hand, the variables that matter as far as a causal model for the outcome.
