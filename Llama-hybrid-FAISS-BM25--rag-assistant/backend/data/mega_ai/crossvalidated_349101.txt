[site]: crossvalidated
[post_id]: 349101
[parent_id]: 
[tags]: 
Bayesian estimation by sampling the prior?

Given a quadratic loss function, the Bayes estimator is given by \begin{equation}\hat{\theta} = E[\theta|y] = \frac{\int_\Theta\theta p(y|\theta) p(\theta) d\theta}{\int_\Theta p(y|\theta) p(\theta) d\theta}\end{equation} where $\theta\in\Theta$ is the parameter vector, $y$ is the data, $p(\theta)$ is the prior, $p(\theta|y)$ is the posterior, and $p(y|\theta)$ is the likelihood. A variety of numerical techniques (Rejection Sampling, MCMC methods, etc.) have been developed to draw from the posterior distribution when direct sampling from the posterior is unavailable. These techniques allow us to estimate $E[\theta|y]$ with \begin{equation}E[\theta|y]\approx\frac{1}{n}\sum_{i=1}^n\theta_i\end{equation} where $\theta_i$ is the $i^{th}$ draw from the posterior. My question: If we are able to draw from the prior distribution, couldn't we just sample the prior to numerically approximate the integrals on the right hand side of the first equation? In other words, approximate $E[\theta|y]$ with\begin{equation}E[\theta|y]\approx \frac{\frac{1}{n}\sum_{i=1}^n\theta_i p(y|\theta_i)}{\frac{1}{n}\sum_{i=1}^np(y|\theta_i)} = \frac{\sum_{i=1}^n\theta_i p(y|\theta_i)}{\sum_{i=1}^np(y|\theta_i)} \end{equation} where $\theta_i$ is the $i^{th}$ draw from the prior. This would seem to be a more convenient and computationally efficient alternative to Rejection Sampling and MCMC techniques. I understand that in some cases we may not be able to draw from the prior, but in those cases why not just pick different priors? I haven't seen the method I'm proposing discussed in any textbooks or online, so I'm hoping someone can explain to me the folly of this approach.
