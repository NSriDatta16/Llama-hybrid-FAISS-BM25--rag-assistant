[site]: crossvalidated
[post_id]: 631724
[parent_id]: 
[tags]: 
Sklearn feature selection performs strangely with 2 groups (and with SVC)

Previously I've successfully performed support vector classification with recursive feature elimination in R using the e1071 package, but I'm now hoping to move over to SciKit Learn given that Python has much better support for machine learning than R does. I am fairly new to Python and very new to sklearn. My classification problem will involve identifying features that are predictive of differences between two groups, and I would like to use SVC. To check that things will work as epxected with real data, I began by replicating the example recursive feature selection using the code provided in this example . Doing so generated the expected feature selection curve, with accuracy increasing right-to-left as uninformative features are removed, then dropping precipitously once truly informative features begin being lost: I then switch from logistic regression to SVC, by changing one line: clf = LogisticRegression() becomes clf = SVC(kernel='linear') This still works, finding 3 as the optimal number of features, but the accuracy curve on the right side is much shallower, seemingly indicating that the benefits of removing uninformative alleles is less dramatic in SVC than logistic regression: My first question is why this curve is so much flatter than that for the logistic regression classifier? That's a little concerning to me because the accuracy at 8 features is almost as high as for 3 features- it wouldn't take much for us to find 8 as the 'optimal' number of features. But okay, there's presumably a workaround: I guess in the case of such a plateau we could say that we take as optimal the lowest number of features for which the accuracy remains very close to the highest observed value, in which case we would indeed always find 3 to be optimal. The really confusing result for me is the next one. In my real data I will only have two treatments to distinguish, so I change from 8 groups to 2 groups without changing any other parameters: X, y = make_classification( n_samples=500, n_features=15, n_informative=3, n_redundant=2, n_repeated=0, n_classes=2, n_clusters_per_class=1, class_sep=0.8, random_state=0, ) I then re-run everything as before, but now the feature selection curve is a complete mess, regardless of whether I use SVC or logistic regression: Our error bars are now huge, and there's no hope from these plots that we'd be able to ID 3 as the correct number of informative variables. I thought perhaps increasing the cross-validation K from 5 to 50 might reduce the error: cv = StratifiedKFold(50) But doing so doesn't help much at all: What's going on here? Why, in this case, does feature selection work as expected with 8 groups but seem to fail completely with 2?
