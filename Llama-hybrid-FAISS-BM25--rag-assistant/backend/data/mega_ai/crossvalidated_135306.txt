[site]: crossvalidated
[post_id]: 135306
[parent_id]: 135297
[tags]: 
Just to add to what @Barmaley.exe said (+1), if you want probabilities, then use an algorithm that was designed for that purpose, such as the Relevance Vector Machine, Gaussian Process classifier or Kernel Logistic Regression (I like KLR and use it a lot, e.g. here which gives some benchmark results compared to SVM and Gaussian process classification etc.). Platt scaling will give probability estimates, but they are not likely to be as good as those from a model designed to give a probabilistic output. One of the ideas behind the SVM is that it is better to solve the problem directly, rather than solving a more general problem and simplifying the answer. For instance if you are only interested in discrete yes/no classification, then it is better to try and infer the decision boundary directly, rather than estimate the posterior probability of class membership (a more general problem) and then threshold at 0.5 (the simplification step). The reason for this is that model resources are not allocated to improve the performance of the model away from the decision boundary and the problem of estimating the parameters is simplified, and less data are likely to be required. The SVM then is likely to do a good job of estimating the decision boundary (or p = 0.5 contour of the probability), but the raw output of the SVM will not necessarily be a good basis for estimating the probability away from the decision boundary.
