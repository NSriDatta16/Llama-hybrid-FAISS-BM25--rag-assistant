[site]: crossvalidated
[post_id]: 299915
[parent_id]: 
[tags]: 
How does the Rectified Linear Unit (ReLU) activation function produce non-linear interaction of its inputs?

When used as an activation function in deep neural networks The ReLU function outperforms other non-linear functions like tanh or sigmoid . In my understanding the whole purpose of an activation function is to let the weighted inputs to a neuron interact non-linearly. For example, when using $sin(z)$ as the activation, the output of a two input neuron would be: $$ sin(w_0+w_1*x_1+w_2*x_2) $$ which would approximate the function $$ (w_0+w_1*x_1+w_2*x_2) - {(w_0+w_1*x_1+w_2*x_2)^3 \over 6} + {(w_0+w_1*x_1+w_2*x_2)^5 \over 120} $$ and contain all kinds of combinations of different powers of the features $x_1$ and $x_2$. Although the ReLU is also technically a non-linear function, I don't see how it can produce non-linear terms like the $sin(), tanh()$ and other activations do. Edit: Although my question is similar to this question , I'd like to know how even a cascade of ReLUs are able to approximate such non-linear terms.
