[site]: datascience
[post_id]: 27909
[parent_id]: 
[tags]: 
Learning rate in the Perceptron Proof and Convergence

Every perceptron convergence proof i've looked at implicitly uses a learning rate = 1. However, the book I'm using ("Machine learning with Python") suggests to use a small learning rate for convergence reason, without giving a proof. Can someone explain how the learning rate influences the perceptron convergence and what value of learning rate should be used in practice?
