[site]: datascience
[post_id]: 84590
[parent_id]: 84587
[tags]: 
First, congratulations for thinking to do a qualitative analysis of the results :) I know it should be obvious, but so many people just assume that the system works and don't bother checking their output. Now, strictly speaking what you're seeing is not a bug. These are errors made by a statistical system. A statistical system is not meant to get everything right, it's only meant to label the input "to the best of its knowledge" , and its knowledge is limited primarily by (1) the data it was trained with and (2) the assumptions made in the design of the model itself. I don't know the exact characteristics of the systems that you used, but I can make an educated guess about the errors that you mention: "fewer people are dying every day" is likely to be predicted as negative because it contains the word "dying". Probably there were no (or very few) examples in the training data which contain the word "die" and are labelled positive. As a consequence the system assumes that any sentence containing "die" is likely negative. One may notice that the positive semantics of "fewer people dying" is completely lost on the system, because it focuses on simple clues (individual words), it's not able to parse more complex phrases. "The audience here in the hall has promised to remain silent." would be a similar case: the word "silent" or perhaps the two words "remain silent" likely were found only in negative examples during training, so the system just generalizes wrongly that a sentence containing these words is negative. The sarcastic "Oh really?!" is an even more complex concept for the system to properly identify. The task of sarcasm detection is studied on its own because it's such a difficult task for a machine. I don't follow this field closely so I could be wrong, but I don't think the task has reached any satisfying level of maturity yet, let alone been integrated with standard sentiment analysis systems. Nonetheless these errors don't mean that the results are useless. If you annotate manually a random sample and evaluate the performance of the system on this sample, hopefully you'll see that overall the system performs decently. That's what is expected of a statistical system: it's not reliable on an individual basis, but normally it's doing a good job in average. More generally, all these errors show that the problem of Natural Language Understanding is far from being solved yet... and it might never be. The good news is that there's still a lot of interesting problems to solve for NLP scientists ;)
