[site]: crossvalidated
[post_id]: 532241
[parent_id]: 532124
[tags]: 
Out of all of the references I dug through, the clearest exposition of this derivation is found in Bishop's Pattern Recognition and Machine Learning (2006). I roughly follow the exposition of sections 1.5.5 (p. 46-7) and section 3.2 (pp. 147-9). Bishop assumes $\mathbf{X}$ , $Y$ are continuous random vectors/variables. Let $\mathbf{X} \in \mathbb{R}^k$ be the new input vector. Squared-error loss between a value $Y := Y(\mathbf{X})$ and a proposed estimator $\hat{Y}(\mathbf{X})$ is given by $$\int_{\mathbb{R}}\int_{\mathbb{R}^k}\left[\hat{Y}(\mathbf{x}) - y \right]^2f_{\mathbf{X}, Y}(\mathbf{x}, y)\text{ d}\mathbf{x}\text{ d}y = \mathbb{E}\left[\left(\hat{Y}(\mathbf{X})-Y\right)^2 \right]$$ where $f_{\mathbf{X}, Y}(\mathbf{x}, y)$ denotes the joint density of $(\mathbf{X}, Y)$ . We know from standard results that the above quantity leads to the decomposition $$\begin{align} \mathbb{E}\left[\left(\hat{Y}(\mathbf{X})-Y\right)^2 \right] &= \mathbb{E}\left\{\left[\hat{Y}(\mathbf{X}) - \mathbb{E}\left[Y \mid \mathbf{X} \right]\right]^2\right\} + \mathbb{E}\left\{\left[\mathbb{E}\left[Y \mid \mathbf{X} \right] - Y\right]^2\right\} \tag{1} \end{align}$$ and thus we obtain, letting $g(\mathbf{x}) = \mathbb{E}[Y \mid \mathbf{X} = \mathbf{x}]$ , $$ \mathbb{E}\left[\left(\hat{Y}(\mathbf{X})-Y\right)^2 \right] = \int_{\mathbb{R}^k}\left[\hat{Y}(\mathbf{x}) - g(\mathbf{x}) \right]^2f_{\mathbf{X}}(\mathbf{x}) \text{ d}\mathbf{x}+ \int_{\mathbb{R}}\int_{\mathbb{R}^k}\left[g(\mathbf{x}) - y \right]^2f_{\mathbf{X}, Y}(\mathbf{x}, y)\text{ d}\mathbf{x}\text{ d}y\text{.}$$ We next observe that $$\left(\mathbb{E}\left[Y \mid \mathbf{X} \right] - Y\right)^2 = \left[f(\mathbf{X})-f(\mathbf{X}) -\epsilon\right]^2 = \epsilon^2$$ with the assumption that $\mathbb{E}[\epsilon \mid \mathbf{X}] = 0$ . Thus, in $(1)$ , the term $$\mathbb{E}\left\{\left[\mathbb{E}\left[Y \mid \mathbf{X} \right] - Y\right]^2\right\} = \mathbb{E}[\epsilon^2] = \sigma^2$$ if we assume either one of the following are true: either $\mathbb{E}[\epsilon^2] = \sigma^2$ or $\mathbb{E}[\epsilon^2 \mid \mathbf{X}] = \sigma^2$ . What we haven't taken into account here at all yet is the use of the training data. Let $\mathcal{T} = \{(\mathbf{X}_i, Y_i)\}_{i=1}^{N} = \{\mathbf{P}_i\}_{i=1}^{N}$ denote the training data, which we consider continuous random vectors/variables. In this case, our estimator $\hat{Y}(\mathbf{X})$ will not only depend on $\mathbf{X}$ , but the given training data $\mathcal{T}$ as well, so we denote $\hat{Y}(\mathbf{X} \mid \mathcal{T}) := \hat{Y}(\mathbf{X})$ . Following notation similar to Bishop's, let $$\begin{align} &\mathbb{E}_{\mathcal{T}}\left[\hat{Y}(\mathbf{X} \mid \mathcal{T})\right] =\\ &\int_{\mathbb{R}^{k+1}} \cdots \int_{\mathbb{R}^{k+1}} \hat{Y}(\mathbf{X} \mid \mathbf{P}_1 = \mathbf{p}_1, \dots, \mathbf{P}_N = \mathbf{p}_N)f_{\mathbf{P}_1, \dots, \mathbf{P}_N}(\mathbf{p}_1, \dots, \mathbf{p}_N)\text{ d}\mathbf{p}_1 \cdots \text{ d}\mathbf{p}_N \end{align}$$ denote the expectation of $\hat{Y}(\mathbf{X} \mid \mathcal{T})$ over all possible training sets. With some algebra laid out in (3.39) and (3.40) of Bishop, one can build from $(1)$ and apply double expectation to obtain $$\mathbb{E}\left\{\left[\hat{Y}(\mathbf{X} \mid \mathcal{T}) - \mathbb{E}\left[Y \mid \mathbf{X} \right]\right]^2\right\} =\mathbb{E}\left[\mathbb{E}\left(\mathbb{E}\left\{\left[\hat{Y}(\mathbf{X} \mid \mathcal{T}) - \mathbb{E}\left[Y \mid \mathbf{X} \right]\right]^2 \mid \mathbf{X}, \mathcal{T}\right\} \mid \mathbf{X}\right)\right] $$ add and subtract $\mathbb{E}_{\mathcal{T}}\left[\hat{Y}(\mathbf{X} \mid \mathcal{T})\right]$ inside the squared term (similar to what is done in the link above) and use the work laid out in Bishop to show that the above may be decomposed into, if $h(\mathbf{x}) = \mathbb{E}_{\mathcal{T}}\left[\hat{Y}(\mathbf{X} \mid \mathcal{T})\right](\mathbf{x})$ , $$\int_{\mathbb{R}^k}\left\{ h(\mathbf{x}) - \mathbb{E}[Y \mid \mathbf{X} = \mathbf{x} ]\right\}^2f_{\mathbf{X}}(\mathbf{x})\text{ d}\mathbf{x} + \int_{\mathbb{R}^k}\mathbb{E}_{\mathcal{T}}\left[\left\{\hat{Y}(\mathbf{x}) - h(\mathbf{x}) \right\}^2 \right]f_{\mathbf{X}}(\mathbf{x})\text{ d}\mathbf{x}$$ The first term above is the squared bias term, and the second term above is the variance term. If $\mathbf{X}$ is deterministic, the PDFs can be ignored, and the integrals are just sums over one point.
