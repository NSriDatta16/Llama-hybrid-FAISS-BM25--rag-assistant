[site]: crossvalidated
[post_id]: 159678
[parent_id]: 
[tags]: 
explanation of MCMC and bayesian estimation

I have some questions about bayesian methods. First of all I have a set of iid observations, calling $y_1,\ldots,y_n$ that come from a certain distribution $f$ with unknown parameters(for example Normal distribution). Then I can form likelihood by $P(D|\vec{\theta})=\prod_i^n f(y_i)$. Now I am going to use bayesian inference to estimate unknown parameters of posterior. My questions are, How to choose prior? Given a prior (for example a gamma distribution) how can I estimate parameters? Let denote parameters and data by $\theta=(\theta_1,\ldots,\theta_p)$, $D=(y_1,\ldots,y_n)$ respectively. As far as I realize the posterior is $$ Pos(\theta|D)\propto P(\theta)P(D|\theta) $$ Because $\theta$ is unknown, how can I compute $P(\theta)$ and $P(D|\theta)$? I would be very thankful If somebody give me an explanation of MCMC and connection to above paragraphs. I want to appreciate all of you for your help: I also found a nice tutorial for bayesian and MCMC algorithm here : https://theoreticalecology.wordpress.com/2010/09/17/metropolis-hastings-mcmc-in-r/
