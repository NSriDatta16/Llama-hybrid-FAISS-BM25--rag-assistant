[site]: datascience
[post_id]: 74147
[parent_id]: 60674
[tags]: 
Word embedding techniques can be adapted to obtain a representation of each document as a single vector, i.e. "Doc2Vec". A naive approach to Doc2Vec is to simply sum the word embedding vectors in each document, and divide each element in this vector by its length. For a better approach to Doc2Vec, see Mikolov et al's paper: Distributed Representations of Sentences and Documents . Once you obtain the Doc2Vec representations of each document, you can then apply traditional classification algorithms to this. Alternatively, you can use the matrices of word embeddings directly as the first layer in a convolutional neural network: see here .
