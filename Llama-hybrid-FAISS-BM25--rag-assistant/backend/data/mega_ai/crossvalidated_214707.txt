[site]: crossvalidated
[post_id]: 214707
[parent_id]: 213687
[tags]: 
The slack variable $\xi$ is defined as follows (picture from Pattern Recognition and Machine Learning). $\xi_i = 1- y_i(\omega x_i+b)$ if $x_i$ is on the wrong side of the margin (i.e., $1- y_i(\omega x_i+b)>0$ ), $\xi_i=0$ otherwise. Thus $\xi_i=max(0, 1- y_i(\omega x_i+b))\qquad(1)$ . So minimizing the first definition subject to constraint (1) $$ f(\mathbf{w}) = \frac{\left \| \mathbf{w} \right \|^2}{2} + C(\sum_{i=1}^{N} \xi)^k $$ is equivalent to minimizing the second definition (regularizer+hinge loss) $$R(w)+C\sum max(0, 1- y_i(\omega x_i+b)).$$ There's another question that might be related Understanding the constraints of SVMs in the non-separable case? .
