[site]: datascience
[post_id]: 126603
[parent_id]: 62195
[tags]: 
Please look at Building makemore Part 3: Activations & Gradients, BatchNorm where Andrej Karpathy specifically explains this problem, and how to keep the layer signals to follow normal distribution of variance 1.0. Please see Variance of product of multiple independent random variables to understand the product (matmul) of D dimensional normal distributions becomes a normal distribution of variance D, hence we need to divide $\frac{1}{\sqrt(D)}$ to keep the variance as 1.0. Once these are understood, then Xavier weight initialization and He initialization used in neural network will make sense that they indeed address the problem of keeping variance to 1.0, and you would start using them to keep the variance to 1.0. Please note the initialization depends on the activation functions for you to use. Please see TORCH.NN.INIT on how we need to adjust the weight initialization depending on the functions.
