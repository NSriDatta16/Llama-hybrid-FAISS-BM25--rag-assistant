[site]: crossvalidated
[post_id]: 209785
[parent_id]: 
[tags]: 
Repeated training examples in Gradient Descent

I am new to machine learning and trying to understand stochastic gradient descent. I understand in stochastic gradient descent, in each epoch, randomly an example is picked and given to the model. So is the case for min-batching; randomly picking b number of training examples in each iteration. Based on this definition, if randomization is not perfectly uniform in the range of training set size, depending on the number of iterations till convergence, some samples might be visited multiple times and some might not be visited at all (again relative to number of iterations) which might end up with a model biased towards the luckier examples. Or am I wrong to assume that? My question is, in SGD, is it important that batch randomization is uniform (assuring uniform probability of example being picked) or having a normal randomization algorithm which does not guarantee no-repetition will converge to an optimum model as well?
