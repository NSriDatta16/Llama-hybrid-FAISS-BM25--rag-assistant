[site]: datascience
[post_id]: 27039
[parent_id]: 26618
[tags]: 
I guess you are doing something wrong in your code. I guess its better to use gradient checking approach for figuring out whether the whole code has any problem or not. Based on the comments, if I want to show you exactly what happens, first take a look at here which professor Hinton himself explains that what MLPs learn is like learning masks instead of learning the features of the inputs. For illustrating more you can take a look at here which shows that MLPs can learn what and I hope that you can expand it to higher dimensions, for your case 784 dimensions. If you use MLPs alone, you will face to the problem of something which is like masking. What CNNs try to find is feature. They try to find features which can be better explained by finding hierarchical features. You can also take a look at here which explains that using CNNs you try to find features, and after convolution layers, you use dense layers, MLPs which they try to classify the features. MLPs are good for classifying MNIST data set but they are weak for generalizing unseen data. If your code works fine after using gradient checking try to use different hyper parameters which I guess our friend has explained so much well for you.
