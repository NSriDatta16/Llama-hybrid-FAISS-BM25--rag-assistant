[site]: crossvalidated
[post_id]: 298688
[parent_id]: 298661
[tags]: 
In your point one you use the word "should" and that is the key, while everyone should understand this, most do not. Google for Andrew Gelman's blog (or other sources) for examples of cases where researchers who "should" know better don't and the reviewers that "should" know better don't, and the editors who "should" know better don't and even when this mistake is pointed out, they still don't. In the comic, we know that there were 20 tests done, but does it spell that out in the article? probably not. That is one of the problems is that many times multiple tests are done, but only the interesting ones are reported, so the reader does not know how many tests to adjust for unless the researcher specifically talks about them or does the adjustment. Even worse, some research is done by throwing in as many predictors as can be thought of and declaring "success" if at least one p-value is less than 0.05. If these researchers were not continually told that they needed to adjust for multiple comparisons then they would be rewarded for behaviors like throwing in extra, meaningless predictors, or breaking a single large study into several small studies, or other strategies that increase type I error rates and therefore give them a better chance of "success". You ask why we multiply the p-value rather than divide the significance level. Mathematically it is equivalent, though conceptually there is a difference, and I agree with you that it makes much more conceptual sense to adjust the significance level, but apparently for most people it is easier to let the computer multiply the p-value than think about a different significance level (and in the case of FDR you would need to calculate a different significance level for each comparison). You are right to think about both type I and type II errors, but type I error is much easier to control, so it tends to get much more attention. I did once need to respond to an article review that we intentionally did not control for multiple comparisons because this was a case where the type II error was much more serious than a type I error. Here are a couple of articles that spell some of these issues out in more detail (and give good reasons why sometimes you don't want to adjust as well as times you do): Multiplicity in randomised trials I: endpoints and treatments Kenneth F Schulz, David A Grimes. Lancet 2005; 365: 1591–95 Multiplicity in randomised trials II: subgroup and interim analyses Kenneth F Schulz, David A Grimes. Lancet 2005; 365: 1657–61
