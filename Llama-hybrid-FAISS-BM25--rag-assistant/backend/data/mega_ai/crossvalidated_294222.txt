[site]: crossvalidated
[post_id]: 294222
[parent_id]: 62069
[tags]: 
Andrew Ng class math handles "discrete" data quite like it handles "non-discrete" data. All we have to do is empirically estimate normal distribution parameters, and it can be perfectly done for discrete data. If you think about it, machine learning always deals with discrete data anyways: the number of data points is not infinite and the number of bits handled by computers is not infinite. If discrete data points can be compared between each other then there is no fundamental difference for machine learning methods when dealing with, say, length: 1.15 ft 1.34 ft 3.4 ft or how many branches are on the tree: 1 2 3 5 You can sum and average floating point or whole numbers just the same. Now, to categorical data. Categorical data points cannot be compared {car vs motorcycle vs boat). How do we handle this? The number of categories has to be at least two to make sense, otherwise what's the point in constant feature? In case of 2 categories, we can represent a category feature as a binary feature {0, 1}. 0 and 1 can be used for math, so see above. If number of categories (K) is [3.. inf], we map our single feature to K binary mutually exclusive features . For example, "motorcycle" category becomes a combination of binary features { IsCar: 0, IsMotorcycle: 1, IsBoat: 0}, Boat point becomes { IsCar: 0, IsMotorcycle: 0, IsBoat: 1} and so on. We can estimate empirical distribution parameters from these new features. We will simply have more dimensions, that's all.
