[site]: datascience
[post_id]: 73700
[parent_id]: 
[tags]: 
Should I put time on my Vanilla ANN for classifying MNIST Dataset

I am building a Vanilla Neural Network in Python for my Final Year project, just using Numpy and Matplotlib, to classify the MNIST dataset. Here's the specifications of the model: One Input Layer + One Hidden Layers + One Softmax Layer Number Nodes in each layer :[784, 800, 10] Activation function used: ReLU and Softmax. Also Normalized the Train and Test set, by dividing it by 255 Have used Mini Batch Gradient Descent(Mini Batch Size=4096). The Model shows very low accuracy on the Test set. Around 8% -11% accuracy. And it shows an accuracy of around 66% - 72% for the Train set. I haven't used Regularization until now. And I don't know if that will help. Now fed up with this, I am thinking of just implementing the model in Tensorflow and see if that works. Or is it necessary to implement it from scratch? Because I think I know how every concept works(although not being able to find the loophole in my model). What do you have to say about it? If you want to have a look at my code here's the link . It would be great if I can get any suggestions. P.S.: Does it create any problem if a DL model is not written using OOP?
