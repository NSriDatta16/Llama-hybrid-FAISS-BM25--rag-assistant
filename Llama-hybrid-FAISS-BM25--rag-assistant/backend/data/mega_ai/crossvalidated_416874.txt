[site]: crossvalidated
[post_id]: 416874
[parent_id]: 416870
[tags]: 
The referred paper seems to answer your question, as it is the paper (as far as I know) that suggested using cyclical learning rate. Among other things, they say The essence of this learning rate policy comes from the observation that increasing the learning rate might have a short term negative effect and yet achieve a longer term beneficial effect. This observation leads to the idea of letting the learning rate vary within a range of values rather than adopting a stepwise fixed or exponentially decreasing value. So, as with many things in deep learning, they observed that it works. Everything beyond that would be post-hoc rationalizations why it could work. The paper does not describe any deeper theoretical background behind it, because there is none. It just works. What the paper shows, is that when training the neural networks with cyclical learning rate, they converge faster , i.e. they need less epochs, or iterations , to achieve point where their performance does not really improve any further with more iterations. The paper does not claim that the time per iteration decreased. Given your comment, you seem to be referring to this DataCamp tutorial , where the author claims that cyclical learning rate makes the code run faster (in terms of time, not epochs). First of all, this is a nonsense. The learning rate schedulers adjust learning rate before, or after each epoch, or step (doesn't matter that much, it is about implementation), so the additional few lines of code are run independently of the actual code. It is impossible that running few more lines of code could make the overall code run faster, it should run slightly slower (by negligible amount of time in this case). To prove the claim, the author uses code example with a bug , where he use different batch sizes (by 10x) for both conditions, what obviously has an impact on the run time.
