[site]: crossvalidated
[post_id]: 79508
[parent_id]: 79454
[tags]: 
The derivative is wrong. It should be, $$\frac{\partial h_{j}}{\partial z_{k}} = h_{j}\delta_{kj}-h_{j}h_{k}$$ check your calculations again. Also, the expression given by amoeba for the cross-entropy is not entirely correct. For a set of data samples drawn from $C$ different classes, it reads, $$-\sum_{n}\sum_{k=1}^{C}t_{k}^{n}\ln y_{k}(\boldsymbol{x}^{n})$$ where the superindex runs over the sample set, $t_{k}^{n}$ is the value of the k-th component of the target for the n-th sample. Here it is assumed that you are using a 1-of-C coding scheme, that is, $t_{k}^{n}$. In such case all t's are zero except for the component representing its corresponding class, which is one. Note, that the t's are constant. Hence minimizing this functional is equivalent to minimizing, $$-\sum_{n}\sum_{k=1}^{C}t_{k}^{n}\ln y_{k}(\boldsymbol{x}^{n}) + \sum_{n}\sum_{k=1}^{C}t_{k}^{n}\ln t_{k}^{n} = -\sum_{n}\sum_{k=1}^{C}t_{k}^{n}\ln \frac{y_{k}(\boldsymbol{x}^{n})}{t_{k}^{n}}$$ which has the advantage that the Jacobian takes a very convenient form, namely, $$\frac{\partial E}{\partial z_{j}} = h_{j}-t_{j}$$ I would recommend you to get a copy of Bishop's Neural Networks for Pattern Recognition . IMHO still the best book on neural networks.
