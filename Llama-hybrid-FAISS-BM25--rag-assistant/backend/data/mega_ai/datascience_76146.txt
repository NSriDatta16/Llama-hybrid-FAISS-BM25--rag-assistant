[site]: datascience
[post_id]: 76146
[parent_id]: 76139
[tags]: 
In neural networks meant for classification, you need a linear layer before the softmax to project the internal representation, which has some dimensionality $d_i$ , to the output space, which has dimensionality $d_o$ equal the number of choices (5 in your case). So you either place a Dense(5) layer after the BiLSTM or you take the output of the BiLSTM "manually" and implement the projection. The code above has some strange things: Uses numpy.dot to multiply the output of the BiLSTM. Is this a typo and you actually meant tf.dot or tf.matmul ? The model ends with a tf.keras.layers.Dense(1) , maybe because it was originally meant for binary classification. Has both a Dense layer and then a dot product (i.e. matrix multiplication). These two operations are equivalent to a single Dense layer, so it is pointless to have both. So yo answer your question: assuming that the np.dot actually means a tf matrix multiplication, then the Dense layer in the model is pointless.
