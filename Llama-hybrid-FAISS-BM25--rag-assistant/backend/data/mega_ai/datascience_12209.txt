[site]: datascience
[post_id]: 12209
[parent_id]: 11770
[tags]: 
As with anything, it depends. :) Your question will very much depend on the model. This question is reminiscent of the broader debate about discriminative versus generative models . Usually, we work with discriminative models. That is, you model $\text{P}(Y=1\,|\,X_1, X_2, \dots)$, where $Y$ are your classes and $X_i$ your observations. There is another class of models: generative models, like Restricted Boltzmann Machines , which model joint distributions, $\text{P}(Y,X_1, X_2, \dots)$. They can be used for classification because you can easily turn a joint probability into a conditional probability. These models usually are broader in scope, and usually do not perform as well, which is why they aren't used as much. But they do exist. Assuming that you are talking about discriminative models, like decision trees or the logistic regression, then what you want is how to invert this conditional probability , so that you have a distribution along $X_i$; that is, $\text{P}(X_1=x_1, X_2=x_2, \dots \,|\, Y)$. From Bayes's theorem, $\text{P}(Y=1\,|\,X_1=x_1, X_2=x_2, \dots)=\text{P}(X_1=x_1, X_2=x_2, \dots \,|\, Y=1)\frac{\text{P}(Y=1)}{\text{P}(X_1=x_1, X_2=x_2, \dots)}.$ So, yes, you can. Where is the catch? $X$ and $Y$ have to be discrete. It is very easy to create a probability density function from categorical variables by just getting conditional probabilities out of your model by just trying all combinations of $X$. And, once you have a probability density function, then you can just use a sampling technique to generate values. It is less clear to me how you could make this work for continuous values, unless you are willing to use an approximation by discretizing your observations. I think an exact solution will depend very much on the discriminative model at hand. You cannot do this alone by getting conditional probabilities out of the model. But if sampling is important for you, then you should just use a generative model.
