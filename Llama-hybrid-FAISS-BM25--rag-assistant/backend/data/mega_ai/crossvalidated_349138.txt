[site]: crossvalidated
[post_id]: 349138
[parent_id]: 
[tags]: 
Transfer learning: How and why retrain only final layers of a network?

In this video , at 5:29 , Prof. Andrew Ng says regarding transfer learning: Depending on how much data you have, you might just retrain the new layers of the network, or maybe you could retrain even more layers of this neural network. The new layers he is referring to are ones that are added to replace the original output layer. At this point Prof. Ng says: If you have enough data, you could also retrain all the layers of rest of the network. In transfer learning, is there any difference in how backprop is applied when only training the last few layers? Why would one want to avoid retraining all the layers of a transfer learning network if the fine-tuning dataset was small? That is (if I understand it correctly), why would one not want to apply normal back-propagation through to the input layer?
