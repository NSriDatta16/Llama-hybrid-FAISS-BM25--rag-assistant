[site]: crossvalidated
[post_id]: 616718
[parent_id]: 436124
[tags]: 
This is fine. Ultimately, your neural network is layers of feature extraction followed by a regression on those extracted features. A more routine regression has no restrictions on the coefficients, and neither does that last layer of your neural network. It might just be that a small change to some of the neurons in that final hidden layer results in a large (magnitude exceeding $1$ ) change in the outcome. So be it. If your concern is that the output neuron will give a value above $1$ or below $0$ (so no longer having an interpretation as a probability), note that this can happen when all of the weights are contained within an interval like $(0,1)$ . This is a known issue of the linear probability model . If you want to transform output values to be restricted to an interval like $(0,1)$ so they can be interpreted as probabilities, various activation functions (quite analogous to link functions in generalized linear models) are viable. This is where you might see sigmoid in the output layer of neural network code, though many functions are viable.
