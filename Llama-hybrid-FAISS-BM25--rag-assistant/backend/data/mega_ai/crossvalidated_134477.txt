[site]: crossvalidated
[post_id]: 134477
[parent_id]: 125773
[tags]: 
Word embedding means how words embedded into learning algorithms. For example, "previous word feature", which means the previous word of current word, with a value of "hello", we should transform "hello" of string form into numerical vectors so that machine learning algorithms such as logistic regression and SVM can understand what you mean (They do not understand string "hello"). To obtain word embeddings, there are several different ways. I guess methods where "context information in which the word is used" are something like Mikolov's Skip-gram model or CBOW, or Bengio's Neural network language model. Actually, not only these models can train word representations (or word embeddings), traditional ways like Vector space model, Bag-of-word model, one-hot coding, and PCA, can also provide word embeddings.
