[site]: crossvalidated
[post_id]: 143575
[parent_id]: 104204
[tags]: 
A good explanation of the relationship between these concepts is in Bias and Variance, Under-Fitting and Over-Fitting Neural Computation : Lecture 9 Â© John A. Bullinaria, 2014 Briefly put, a neural network can achieve zero variance very easily by underfitting: just return a constant output value regardless of the input values. This is a case of extreme under-fitting, and there will be a big bias (tendency to be systematically off target) because the network made no effort to fit the training data. A neural network can achieve zero bias easily by overfitting: just make it big and complicated enough to ensure that the outputs exactly match for all the data points from the training data. This is an extreme case of over-fitting, and assuming that there is actually noise in the training data set, you will end up with a large variance when you apply it to different data sets.
