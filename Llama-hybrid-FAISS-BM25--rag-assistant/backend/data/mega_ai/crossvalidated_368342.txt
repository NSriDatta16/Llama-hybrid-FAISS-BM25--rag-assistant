[site]: crossvalidated
[post_id]: 368342
[parent_id]: 
[tags]: 
Finding the maximum of a function $f(x)$ without analytically evaluating $f'(x)$

I'm an experimental physicist, trying to automate a relatively simple (but sensitive) optimization in my measurements that is currently done completely manually and takes up a lot of my time. I figure that machine learning should have ample tools for what I need to do, but my knowledge is quite limited and Google isn't helping me thus far. I've taken away all of the physics/measurement related details and distilled the following scenario, which I hope is general enough. There is some function $f(x)$ that I want to maximize. However, I can only evaluate $f(x)$ ; I cannot evaluate its derivative explicitly. Moreover, I cannot sample a large range of $x$ ; if $f(x) , I am in trouble (and it takes me over a day to recover). Luckily, I have one starting value $x_0$ for which I know that that $f(x_0) > {\rm threshold}$ even before evaluating, and I can guess some initial step size $\varepsilon$ for which $f(x_0 + \varepsilon) > {\rm threshold}$ will also hold (however, I don't know if $f(x_0 + \varepsilon) >$ or $ before evaluating). In addition to that I know that the function is roughly parabolic. Could someone suggest an algorithmic / adaptive / feedback protocol to find the $x$ that maximizes $f(x)$ up to some tolerance $x_{tol}$ ? Note that evaluating $f(x)$ is somewhat costly, but that is not my primary concern at this stage. So far I've found the golden-section search, but that requires choosing some range $(a,b)$ over which you want to maximize which I cannot do; I can't start from some wide range as that might bring me below my tolerance, and I can't figure out #x_T $, the $ x$ for which I go below my tolerance, without actually evaluating. Another option would be something like a gradient descent using finite difference for the derivative, but that sounds quite inefficient? For context, how I currently do it manually is as follows: I evaluate $f(x_0)$ and then $f(x_0 + \varepsilon)$ . If this leads to a decrease, I evaluate $f(x_0-\varepsilon)$ instead. Based on the gradient (essentially I just look at if there are large changes in $f(x)$ w.r.t its distance from the threshold I cannot cross), I either increase or decrease $\varepsilon$ and continue searching in the same direction until a maximum is found, which I notice because $f(x)$ starts decreasing. I then go back to that maximum. This way I am always probing the top part of the maximum and thus remain in a safe range.
