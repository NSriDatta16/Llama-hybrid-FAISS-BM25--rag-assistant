[site]: datascience
[post_id]: 107192
[parent_id]: 64477
[tags]: 
The reason is simple, when you have a Sigmoid function it will give you number between [0,1] which cn be feed to Cross entropy to get loss. Hence for binary classification with sigmoid one function is enough. Softmax assumes that each example is a member of exactly one class. Some examples, however, can simultaneously be a member of multiple classes. For such examples: You may not use Softmax. You must rely on multiple logistic regressions. For example, suppose your examples are images containing exactly one item—a piece of fruit. Softmax can determine the likelihood of that one item being a pear, an orange, an apple, and so on. If your examples are images containing all sorts of things—bowls of different kinds of fruit—then you'll have to use multiple logistic regressions instead.
