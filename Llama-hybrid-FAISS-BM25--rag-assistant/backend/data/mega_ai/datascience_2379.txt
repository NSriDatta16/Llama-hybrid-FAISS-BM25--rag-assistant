[site]: datascience
[post_id]: 2379
[parent_id]: 2378
[tags]: 
A few things where the knowledge of Linear Algebra might be helpful in the context of Machine Learning: Dimensionality Reduction: There are lots of problems where PCA (a special case of an SVD) followed by a simple Machine Learning method applied on the reduced dataset produces much better results than a non parametric model on the full (non-reduced dataset). For an example see Bhat and Zaelit, 2012 where PCA followed by linear regression performs better than more involved non-parametric models. It also suggests reasons why dimensionality reduction performs better in these cases. Visualizing data: Higher dimensional data is complicated to visualize and one often needs to be able to reduce the dimension of the dataset to view it. This comes very handy when one has to "view" the results of clustering on a higher dimensional dataset. Numerical Accuracy: Eigenvalues are generally handy in order to understand condition numbers of matrices and hence be able to figure out if results of Linear regression or other methods that require to solve Ax=b would be numerically accurate. Positive Definiteness of a matrix might also be able to guarantee bounds on numerical accuracies. Recommendations: Some methods like collaborative filtering use matrix factorization (SVD) tuned smartly to solve recommendation problems. Regularization: Regularization is commonly used to reduce over-fitting in machine learning problems. Most of these regularization techniques like Lasso, Tikhonov etc. have both optimization and Linear Algebra at their heart.
