[site]: crossvalidated
[post_id]: 448857
[parent_id]: 448854
[tags]: 
It's for the same reasons you use cross-validation in any situation, as opposed to a single train/test split. You're able to leverage the entire dataset for both training and testing, which provides more robust performance estimates (as it's calculated over a larger N), and protects against "unlucky" train/test splits (which becomes more likely with smaller datasets). Since computational power is rather cheap, it's often preferable to spend a bit more computing power to get a more robust and better-characterized result.
