[site]: crossvalidated
[post_id]: 586983
[parent_id]: 
[tags]: 
Should a neural network be retrained with more data?

I followed the usual steps and found a neural net that works well. I split the data into training, validation, and testing sets, tried a few approaches using the training data, saw how they worked using the validation data, went back and forth a few times, built and estimated one particular neural network, tested it on testing data, and it works! I now need to use the neural net on future data. Question: The network is currently trained only on the training data. Should I retrain the model on all available data, which is (training + validation + testing) data? I see some arguments in favor and against retraining. In favor of retraining: More data means the model will be presumably better estimated. In standard statistics, it's always better to have more data. Against retraining: When I use more or different data, something might just go differently, and I don't know if that will make the model perform worse with future data. Some examples are: The data reduction step might reduce the data differently, creating different inputs to the neural net to begin with. Who knows if the neural net will work well with the new / different inputs. More data with the same architecture might affect the weight estimates somehow, such as over-optimize them. Or, since I have more data, should I include more nodes? That means I have to change the architecture. I have no idea if the new architecture will work better or not, since I will have no data to validate it. Please indicate the source of your answer, such as academic literature, your personal experience with neural nets (if you have a lot of experience with them), etc.
