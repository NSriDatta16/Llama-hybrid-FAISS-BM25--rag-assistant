[site]: datascience
[post_id]: 28344
[parent_id]: 
[tags]: 
Using Machine Learning techniques for text-analysis

I am analysing a bunch of tweets and I want to understand which political party the authors support. I am using Mathematica but I am thinking to re-write my code in Python. If you have any suggestion, do not esitate. Let say, for example that I have five parties labelled by a number: Party1, Party2,...,Party5 and I provide a list of tweets which are for sure associated to the parties. This list is roughly trainingList = {tweet1-> "Party1",tweet2->"Party2",...,tweet5->"Party5",tweetK->"Neutral"} . Moreover, I added a class "Neutral" because some tweets are not classifiable. Actually, I provide more than one tweet par party. In fact, the trainingList is much longer. Notice that I remove stop-words from the tweets and I apply a stemming-words algorithm to simplify the expressions. Then, every tweets is a list of potentially-significant words. I create the classifier function, providing a prior (the explicit for of the prior is not important) c = Classify[trainingList, ClassPriors-> prior] Then, I want to do some checks. I take a bunch of tweets belonging to other party-supporters and try c[{word1,word2,word3,word4,word5,word7},"Probabilities"] which gives me a list of probabilities $p_i$. For example, p1, 'Party2' -> p2, 'Party3' -> p3, 'Party4' -> p4, 'Party5' -> p5, 'Neutral' -> p6 |> Question1 Is there a way to understand how the algorithm associate these probabilites? More specifically, how each word is associated to a party Question2 Is there a way to see which are the most frequent n-grams of words associated to a given party (class)?
