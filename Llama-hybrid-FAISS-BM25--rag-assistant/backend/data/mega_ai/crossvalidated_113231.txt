[site]: crossvalidated
[post_id]: 113231
[parent_id]: 
[tags]: 
How to predict the amount of data needed for modeling?

Is there a way to estimate the amount of data (or the number of records) required to build a statistical model? I read few blogs and I feel that most of the responses concur that there is no way or it is extremely hard to predict sample size for an application. One one blog asks for 10 times the total number of features, less if I am using regularized version of the ML model. A sample size of 50 seem to be the minimum. One method suggests to build the model and check the generalization error. If the error is unacceptable, then gather more data and iterate. Considering the data collection for my application to be time consuming and extremely costly, what options do I have in time-constrained business organization?
