[site]: crossvalidated
[post_id]: 621438
[parent_id]: 620753
[tags]: 
I see no problem at all with using regularisation on high cardinality data. The use case I am thinking of is in e-commerce . In these situations, we typically have a fat-head long tail sales pattern. I am certainly not suggesting its use for other sorts of data, eg chemical molecules, where I would assume there is no https://en.wikipedia.org/wiki/Pareto_principle at work eg Spotify currently has over 11 million artists and creators but of the \$8 Billion paid to its artists only 1060 artists earned more than \$1 Million, while 57,000 artists generated $10,000+ in these situations regularisation will cause those top n artists to have a significant coefficient, whilst the majority of artists will have a zero coefficient. Consider predicting whether a given listener will 'like' a particular song, and we use artist as a dummy variable. You can understand this by considering the regularisation parameter in the objective function as " sum error per unit of coefficient". so the more listeners (rows) an artist has, the more total error the artist coefficient can reduce; conversely the fewer listeners an artist has, the less impact that artist's coefficient has on the error. So I would actually recommend having more dummy variables: in the spotify example you might have artists, genres, decade, song ... the point is that regularisation will push rare occurrences to the average (since there are more plays of a genre than of an individual artist). so eg most RnB artists' coefficient will be pushed to an average RnB genre coefficient, whilst Drake etc would have a significant coefficient. Similarly most songs won't have their own coefficient, but you would expect the top 10 of any year to. spotify stats
