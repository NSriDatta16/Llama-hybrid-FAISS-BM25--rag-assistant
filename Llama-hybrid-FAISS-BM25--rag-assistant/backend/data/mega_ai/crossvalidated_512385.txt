[site]: crossvalidated
[post_id]: 512385
[parent_id]: 504861
[tags]: 
A few days after I posted this question, I attempted simulations to test my intuition that strong seasonality can hide the underlying stochastic trend in the usual ADF test. Consider the following process: \begin{align} y_t &= S_t+y_{t-1} + \varepsilon_t \\ \text{where, } \,\, &S_t \sim N(\mu_i,\sigma_s^2) \,\,\,\text{for }t=4n+i;\tag{$i=1,2,3,4;n \in \mathbb N$}\\ &\sum\limits_i \mu_i = 0; \\ &\varepsilon_t \sim IN(0,\sigma_{\varepsilon}^2) \end{align} We can think of this process as a quarterly time series exhibiting stable seasonality. Intuitively, the strongness of seasonality can be measured by $(\text{range}(\mu_i))/\sigma_{\varepsilon}^2$ (further logic in the end). Higher this ratio, more likely it would be that seasonality will hide unit root in the usual test. Chart below shows the results of the sumulation. 10,000 series of length 1000 are generated from the above model and a pure random walk model. Values for seasonal term are taken as $\mu = (1.7,0.8,-1.0,-1.5)$ . DF-statistic is calculated for each series using urca::ur.df(., lag = 0) and density estimates are plotted. From above chart, it is interesting to see that when $\sigma_{\varepsilon}$ is small the usual unit root test can be very wrong. Theoretical justification: In this paper , the authors have derived the analogous DF distribution in presence of an additive outlier. The distribution is similar to the usual DF distribution with an additional term as function of $\theta/\sigma_{\varepsilon}^2$ ; where $\theta$ is the coefficient of the outlier dummy. The above process is somewhat a special case with four additive type outliers and perhaps that's why we are getting the similar results.
