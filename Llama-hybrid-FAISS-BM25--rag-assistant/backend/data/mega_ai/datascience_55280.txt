[site]: datascience
[post_id]: 55280
[parent_id]: 55121
[tags]: 
The training regime of the discriminator is purely supervised: you give data and labels and it learns to predict the labels by minimizing the loss. The only difference with the training of a normal classification network is that the distribution of one of the classes is not static but changes over time (as the generator gets better at creating real-looking data). During the first training iterations, the generator will generate basically noise so, in the beginning, the discriminator will learn to tell real images apart from that noise. Then both will progressively improve together. Depending on the adversarial loss being used (e.g. vanilla GANs), it may be important that the ability of generator and discriminator is not too imbalanced because if the discriminator learns "too much", the gradients propagated to the discriminator will be very small and the generator will not learn anything. In such cases, pretraining the discriminator may not be a good idea. With Wasserstein GANs (or their gradient penalty-based variant ) you should not have this problem so it should be possible to train the discriminator till optimality and use it to propagate gradients to the generator.
