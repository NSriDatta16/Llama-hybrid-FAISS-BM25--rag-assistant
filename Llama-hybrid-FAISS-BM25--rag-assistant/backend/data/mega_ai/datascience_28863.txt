[site]: datascience
[post_id]: 28863
[parent_id]: 28800
[tags]: 
Not sure if you've tried this already, but you might dig into XGBOOST feature importance to establish how your model is making predictions, then do a more in-depth comparison between the two populations at those splits. This isn't very different from what you've proposed, but it does make for a more focused analysis; you might have time to look at higher order interactions between the more important features. (I'm assuming it's not possible to cheat by training a classifier on your population B and comparing how each model makes predictions!) ELI5 and LIME also come to mind for debugging feature importance and explaining model prediction.
