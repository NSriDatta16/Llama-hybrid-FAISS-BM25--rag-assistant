[site]: crossvalidated
[post_id]: 541677
[parent_id]: 
[tags]: 
Conditional distribution of the weight of a mixture gaussian with data augmentation using gibbs sampling

This question is relate to Differenciate between two distributions using gibbs sampling . For $t=1,\,\dots,\,n$ , let's $r_t\sim\mathcal{N}(0,\,\sigma_t^2)$ and $$\sigma_t^2=\left\{\begin{array}{lcl} \sigma^2 & \text{with probability} & p\\ 1 & \text{with probability} & 1-p \end{array}\right.$$ The sample is $r_t, t=1,\,\dots, n$ , we suppose we the value of $\sigma$ is known and our aim is to find sample from the posterior distribution of the weight $p$ using Gibbs sampling. To do so, I follow the data augmentation algorithm as proposed by Diebolt and Robert (1994) at their section 3.1.1 (Example 2). The algorithm work fine if $\sigma$ is far from $1$ but it does not work anymore for $\sigma$ close to $1$ . The following graph show what I found for $\sigma=3$ and $\sigma=0.9$ : Problem : This algorithm work fine for $\sigma^2$ far from $1$ . But, when $\sigma^2$ is close to $1$ , the algorithm doesn't not work. My questions : Is there another algorithm to better the sampling of the posterior of $p$ ? Theoretically, is it link to the sample size of the number of iteration of my algorithm? In other words, do you think that If I raise the sample size $n$ or the number of iteration $n_{iter}$ I will find the better the sampling of the posterior of $p$ ? What will be the difference between $1$ and $\sigma$ such that the posterior of $p$ can be found properly? Using my empirical trials, $\sigma^2=0.1$ , it works, but when $\sigma^2=0.9$ or $\sigma^2=0.8$ it doesn't work. All kind of contribution will be very helpful. Thanks Here is the code I've made using R software: f.sample As you can see, I've made a bootstrap in other to insure the mean of the sampling is not only relate to the sample.
