[site]: crossvalidated
[post_id]: 261784
[parent_id]: 
[tags]: 
Why do we use logistic regression instead of linear regression?

I'm learning logistic regression on Coursera. In the class, we say the logistic regression model is $h_\theta(x) = p(y=1) =\frac{1}{1+e^{-\theta^Tx}}$. When making predictions, we say that $y=1$ if $h_\theta(x) \ge .5$ and $y=0$ otherwise. The cost function we use to find the parameters $\theta$ given the training set is $J(\theta)=\frac{1}{2m}\sum (-y^{(i)}h_\theta(x^{(i)}) - (1-y^{(i)})(1-h_\theta(x^{(i)})))$. But couldn't we predict the exact same thing using linear regression? Our linear model is $a = \theta^TX$ and we output $y=1$ if $a \ge0$ and $y=0$ otherwise. We could make our cost function $J(\theta)=\frac{1}{2m}\sum (-y^{(i)}g(a) - (1-y^{(i)})(1-g(a)))$ where $g$ is the sigmoid function. Using linear regression with this new cost function and a threshold of $0$ would give us exactly the same predictions as logistic regression. So why do we use logistic regression instead of linear regression with a new cost function? Is it just because the probability in logistic regression is more easily interpretable than the real number outcome in linear regression? Or is there another reason?
