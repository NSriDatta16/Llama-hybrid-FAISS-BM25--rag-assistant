[site]: crossvalidated
[post_id]: 154888
[parent_id]: 
[tags]: 
Combining multiple metrics to provide comparisons/ranking of k objects [Question and Reference Request]

Collecting $n$ metrics about $k$ objects Suppose I collect $n$ metrics about $k$ objects. I am looking into valid ways to compare the $k$ objects so they can be "ranked". I think this may be well-trodden ground (sports statistics like total quarterback rating etc.) but I am unfamiliar with this area. I want to answer the question which object is best ? Information about the collected metrics For each metric $m_i$, where $i$ ranges from $1 \leq i \leq n$, the score for metric $m_i$ ranges from $[0, r_i]$. Note that some of these metrics will have theoretical maximums like $100\%$ percent, other $r_i$'s will just be the maximum collected score in the sample (e.g. top speed, height etc.). Normalising/Standardising the metric scores My intuition is to first normalise all these scores between $[0,1]$, so that each score contributes equally to the overall score, to be calculated later. That is, for each metric $m_i$ the score for that metric would be $\frac{m_i}{\text{max}(r_i)}$, where $\text{max}(r_i)$ is the maximum score for that metric in the sample. My intuition doesn't allow me to be confident that this is valid, so that is my Question 1: is this normalisation procedure valid? Also for each question the implicit question is I am probably completely wrong, what resources and topics should I be studying? Weighting the metrics for my overall comparison Let us further suppose that I wish to weight some metrics over others. There seems to me a few approaches, but I will outline one which I am trying to approximate. I was thinking one possible method would be to do a pairwise comparison for each metric, and ask of each comparison: If I were to see a $10\%$ reduction in metric $m_i$, how much of an increase in metric $m_j$ would compensate for that reduction? If the pairs have no real influence over each other I could score this as a $0$ perhaps? I would end up with a table of values for my weightings, filled with pairwise comparisons of this nature. Question 2: Would I have to be consistent when I compare $m_i$ v $m_j$ and $m_j$ v $m_i$? Or could they be non-symmetric? That is if I say a $10\%$ reduction in $m_i$ needs to be accounted for by a $20\%$ increase in $m_j$, can I say a $10\%$ reduction in $m_j$ needs to be accounted for by a $50\%$ increase in $m_i$? Would this be valid? Perhaps I could take an average of each column and have that as my weighting for the metricy? It would seem to me that a weighting system such as this would quantitatively say things like "for me to value object $a$ over object $b$, when $b$'s metric $m_i$ is 10% less than $a$'s $m_i$, I need to see at least a $20\%$ gain in metric $m_j$" . Question 3: What if I were to start to include more complex considerations so that the comparisons, or compensations would be nonlinear? Or mutlivariable comparisons? Perhaps some scores should be negative etc.? The Essential Question Really I would like to know what topics and books should I be reading about to be able to answer this type of question? Thank you
