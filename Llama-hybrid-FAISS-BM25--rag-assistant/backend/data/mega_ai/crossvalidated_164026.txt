[site]: crossvalidated
[post_id]: 164026
[parent_id]: 
[tags]: 
Naive Bayes - Good for Binary Data?

I have 92 observations with 92 variables. Every observation is a binary outcome (0=no, 1=yes), indicating if that observation co-occurs with a given feature in the feature set. I have 18 classes which these observations can be classified. I want to use probabilistic classifiers, to help explain potentially co-morbidity between patients. The data I have I encoded myself based on reports given from the literature. I only have one row per disorder, though, and the 18 classes represent "major classes" these disorders are currently categorized under. I am getting 90% prediction errors using Naive Bayes (God awful!). This isn't inherently shocking, because the problem I'm trying to solve IS the classification framework, but I need to know how to improve my approach so I can better show what I'm trying to do. My questions: 1. Is Naive Bayes good for binary data? 2. Are there any other classification probabilistic classifiers that may perform better than Naive Bayes for a multi class classification problem ? As far as I understand, logistic regression is better for two class problems. Ideally, I would get higher variances in class percentages predicted by Naive Bayes (or any probabilistic classifier) for patients with co-morbid disease states than I would for patients only diagnosed with one disease. However, what I'm getting right now is really confident Bayes network, with the wrong classifications. Again, this isn't inherently invalid - because the classification (the target labels) are probably not coming from a strong diagnostic framework, but I want to make sure my machine learning knowledge isn't also contributing to this poor classification performance. Any thoughts will help!
