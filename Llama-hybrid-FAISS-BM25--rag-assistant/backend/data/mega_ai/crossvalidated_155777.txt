[site]: crossvalidated
[post_id]: 155777
[parent_id]: 155775
[tags]: 
The point is that the realizations of your time series are stochastic. In your example, by means of autoregressive modelling (AR) you build a model on your data that allows to make a prediction. Of course, your model is not likely to predict exactly the value of your time series at a given time, but you will make a prediction error. Notice that before building your model, you make some assumptions on the error structure of your predictions, which indeed are expected values at some given instants. In other words, when you build a model like the one you mentioned, you take a stochastic process and split it into two parts: a deterministic one and a stochastic one. Then, you make your predictions by considering the deterministic part, and the prediction errors represent the stochastic part of the process (you can't predict the errors, but you make an assumption on their structure --- e.g. they are normally distributed with mean zero and variance $\sigma^{2}$). For example, if you consider a stochastic process $X_{t}$, you can build an AR(2) model such as: $$ x_{t} = x_{t-1}\phi_{1} + x_{t-2}\phi_{2} + \epsilon_{t}.$$ In such a model, $x_{t-1}\phi_{1} + x_{t-2}\phi_{2}$ represents the deterministic part, whereas $\epsilon_{t}$ is the prediction error, i.e. the stochastic part. When you have to make a prediction you use only the deterministic part, i.e. in order to predict $x_{t}$ you substitute to $x_{t-1}$ and $x_{t-2}$ the past realizations of your process and to $\phi_{1}$ and $\phi_{2}$ the estimated coefficients. You don't consider the stochastic part, $\epsilon_{t}$, because you have assumed that it is normally distributed with zero mean and fixed variance, so that its expected value is zero.
