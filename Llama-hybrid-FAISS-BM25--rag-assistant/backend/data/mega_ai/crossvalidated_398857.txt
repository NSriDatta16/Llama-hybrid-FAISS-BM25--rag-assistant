[site]: crossvalidated
[post_id]: 398857
[parent_id]: 
[tags]: 
How is Bidirectional-RNN different from vanilla RNN trained with both original & reverse copies of data?

I have several questions regarding Bi-RNN. The RNN here can be LSTM or GRU as well. (1) What is the input of Bi-RNN when making inference? For RNN, if I want to predict a $\hat{y}(t)$ for the target $y(t)$ , I need to input $x(1), x(2)...x(t-1)$ one by one recursively to get the state $h(t-1)$ , then, together with $x(t)$ , I can compute $\hat{y}(t)$ . So the input needed for RNN is $$ \rightarrow y(t)$$ How about BI-RNN? Does the input require the sequence $x(1)$ to $x(t)$ for the RNN component with normal direction, and the sequence $x(t+n)$ to $x(t)$ for the RNN component with reverse direction?: $$ , \rightarrow y(t)$$ (2) If I also make a reverse copy of the original sequence data, and append it with the original sequence data, and then use the new dataset (size doubled) as the input to train an vanilla RNN, how is it different with Bi-RNN trained with the original data only ? Because the new doubled data seems to me also contains bidirectional context relations for each target value. What I mean is if we are training to predict $y(t)$ , the input data can be two separated series, just like two lines of input data: $$ \rightarrow y(t)$$ $$ \rightarrow y(t)$$ It seems to me this still make sense in terms of the "time travel cycle". One difference I notice is Bi-RNN doesnot share the parameters across the two RNN components. But the vanilla RNN does. Also, it seems the RNN with the doubled data can be trained, but can not be used to predict with the full context (pre- & post- contexts)? Because the input is only one directional. We cannot input both the pre- and post- context at the same time to predict $y(t)$ . Is that the reason why we do not use the vanilla RNN with the doubled data (both original data and reverse data) as training data?
