[site]: crossvalidated
[post_id]: 420840
[parent_id]: 420806
[tags]: 
The loss basically uses $p_{\bar{\theta}}(y|x)$ as the target/label for $p_{\theta}(y|\hat{x})$ , where $\hat{x}$ is the augmented data. The goal is to let the output of augmented data stay close to the output of the original data, hence to enhance the consistency of the prediction function. The following pseudocode should help understanding the loss for x in unlabeled_data: # expectation over the unlabeled data set for x_hat in augment(x): # expectation over the augmentation distribution target = p_theta(x) # don't optimize theta here prediction = p_theta(x_hat) loss_uda += kl_divergence(target, prediction) When combining this loss with another supervised loss, it helps improve model generalizability by leveraging unlabeled data.
