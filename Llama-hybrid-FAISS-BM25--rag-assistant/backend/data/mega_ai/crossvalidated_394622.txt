[site]: crossvalidated
[post_id]: 394622
[parent_id]: 394607
[tags]: 
If you want to learn $f(X)$ where $X$ is a set, that is the order of elements in the set is inconsequential, then check out the work on Deep Sets [A]. People have handled such things even using LSTMs in the following way: Given a set X, say with some label y Randomly sample elements from X to create a sequence Generate multiple such sequences from the same set X Feed all these sequences to LSTM with the same label y These methods have been used particularly in Graph Embedding methods like GraphSage [B]. By the way, average of word embedding as a set representation is a fairly competitive model, so don't dismiss it. In [C], Arora et. al show that a simple weighted average of word embeddings for representing sentences competes amazingly well against LSTMs etc. for many tasks. [A] Paper: https://arxiv.org/abs/1703.06114 Code: https://github.com/manzilzaheer/DeepSets [B] Paper: https://arxiv.org/abs/1706.02216 [C] Paper: https://openreview.net/forum?id=SyK00v5xx
