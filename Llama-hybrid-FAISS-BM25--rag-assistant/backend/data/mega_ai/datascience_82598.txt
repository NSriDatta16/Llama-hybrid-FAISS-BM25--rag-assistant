[site]: datascience
[post_id]: 82598
[parent_id]: 82486
[tags]: 
I would like to augment the answer of @Shubham Panchal, since I feel the real issue is still not made explicit. 1.) $\alpha$ could also contain negative entries so that the sigmoid function maps to $(0,1)$ . 2.) @Stefan J, I think you are absolutely correct: two identical embedding vectors would be mapped to $0.5$ while two vectors that differ would be mapped to (depending on $\alpha$ ) values towards $1$ or $0$ , which is not what we want! @Shubham Panchal mentions the Dense layer and provides a link to an implementation, which is correct. Now to make it very clear and short, in the paper they forgot to mention that there is a bias! So it should be $p = \sigma(b+ \sum_{j}\alpha_{j}|h_{1,L-1}^{(j)} - h_{2,L-1}^{(j)}|)$ . Let $\hat{h} := \begin{pmatrix}\hat{h}_{1} & \ldots & \hat{h}_{n}\end{pmatrix}^{T}$ , where $\hat{h}_{j}:= |h_{1,L-1}^{(j)} - h_{2,L-1}^{(j)}|$ . Then we know that $\hat{h}_{i} \geq 0$ for all $i$ . If you consider now the classification problem geometrically , then $\alpha$ defines a hyperplane that is used to separate vectors $\hat{h}$ close to the origin from vectors $\hat{h}$ further away from the origin. Note that for $\alpha = 1$ , we have $\sum_{j}\alpha_{j}|h_{1,L-1}^{(j)} - h_{2,L-1}^{(j)}| = ||\hat{h}||_{1}$ . Using $\alpha$ results thus in a weighting of the standard $1$ -norm, $\sum_{j}\alpha_{j}|\hat{h}^{(j)}|$ . Already for $n=2$ you can see that you can have two classes where the hyperplane must not go through the origin. For example, let's say two images belong together, if $\hat{h}_{1} \leq c_{1}$ and $\hat{h}_{2} \leq c_{2}$ . Now you can not separate those points from points with $\hat{h}_{1} > c_{1}$ or $\hat{h}_{2}> c_{2}$ using a hyperplane that contains the origin. Therefore, a bias is necessary. Using the Dense layer in Tensorflow will use a bias by default, though, which is why the presented code is correct.
