[site]: datascience
[post_id]: 69062
[parent_id]: 36049
[tags]: 
As a complement to the very practical answer of @BrunoGL, I'd like to give a more theoretical answer. I'd like to suggest everyone trying to adjust hyperparameters of a simple Neural Network to read Efficient Backprop, by Lecun and others ( http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf ). Yes it's from 1998, no it's not outdated. It covers the impact of the main hyperparameters you have to set (activation, solver, learning rate, batches), commons traps, the problems you may encouter if you fall into them, how to spot those problems and how to solve them. A must read for everyone that want to tune a Neural Network. Plus, it's free.
