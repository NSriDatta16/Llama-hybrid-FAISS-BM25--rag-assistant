[site]: datascience
[post_id]: 126389
[parent_id]: 126382
[tags]: 
Bad workaround: TextDataset classes No control of truncation/padding in Transformer TextDataset classes At least for TextDataset and LineByLineTextDataset, there just might be no such attribute: TextDataset canâ€™t set max_seq_length? : TextDatasetForNextSentencePrediction (TDFNSP) in combination with a BertModel cannot handle sequences longer than 512. TDFNSP does not truncate or pad. The input for TDFNSP should be sentences separated by a whiteline (\n), and an additional whiteline between documents. If you preprocess your data this way, I cannot imagine one single sentence to be longer than 512 tokens. If this is the case, then it is not a sentence. Yet, after some time of trying to read the sentences that I marked with a \n as the eos_token (end of sentence), I can say that TextDataset does not care about them. Only the TDFNSP needs them. This does not help here to get a split into sentences or other blocks where the rest of the space is filled with padding. The Textdataset just puts everything into blocks and cares only about not cutting any token in halves, but it does not care about sentence endings. This makes sense since its aim is to read large text files into memory, it is not a sentencizer, let alone a tokenizer. Here is another link that might be a hint for us, though it is again about the TDFNSP: TextDatasetForNextSentencePrediction does not seem to contain truncate function unlike LineByLineWithSOPTextDataset . TextDateForNextSentencePrediction, unlike LineByLineWithSOPTextDataset, does not have the truncate feature like truncate_seq_pair in the create_examples_from_document function. The code then was fixed with: def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens): """Truncates a pair of sequences to a maximum sequence length.""" while True: total_length = len(tokens_a) + len(tokens_b) if total_length len(tokens_b) else tokens_b assert len(trunc_tokens) >= 1 # We want to sometimes truncate from the front and sometimes from the # back to add more randomness and avoid biases. if random.random() From this, we see that there is quite a mess in the code. If you need to patch it to get some truncation done, it can also be that the other class "TextDataset" does not have fully grown code. The TextDataset splits the text input by the block_size=512 into blocks. And for the very last block, there is a sort of truncation without padding: the very last block that does not reach 512 tokens is just dropped. For example, I lose a length of 250 tokens when I load my text into a TextDataset. TextDataset needs a bad workaround and is outdated One could take the TextDataset and add 511 padding "[PAD]" tags hardcoded at the end of the text file so that the last block will always have just padding tags when it gets dropped. I have tried adding this inside the TextDataset by sub-classing it and overriding some methods, but I could not get it to work since the TextDataset seems to lose this last block already at the very beginning, it is not just a matter of a loop that stops one block too early. One could fix it with more work, but it is outdated anyway, you can check this in the next link. Fix: Making a dataset for the Trainer class with the text tokenizer from the pretrained model and the datasets package tokenizer from the pretrained model If the TextDataset loses its last block, I thought I should check the tokenizer again to load the text from a variable and make that a Dataset object. The good thing is that the tokenizer function of the model has all the parameters that I need: from datasets import load_dataset path=r'myfile.txt' context = load_dataset("text", data_files=path) text = context['train']['text'][0] encoded = tokenizer.encode_plus( text, max_length=512, truncation=False, padding="max_length", return_tensors="pt") If you then run tokenizer.decode(encoded['input_ids'][0][-300:]) , you see that it has the missing text since it is not split into blocks at all, while if you ran it with truncation=True , the text would be cut already after 512 tokens, thus, most of the text would not become an input of the model. More on the parameters of the tokenizer.encode_plus() I have not digged into all of the parameters of this method. The encode_plus() method is more powerful than encode() , see what's difference between tokenizer.encode and tokenizer.encode_plus in Hugging Face . > PreTrainedTokenizerBase.encode_plus(self, text, text_pair, > add_special_tokens, padding, truncation, max_length, stride, > is_split_into_words, pad_to_multiple_of, return_tensors, > return_token_type_ids, return_attention_mask, > return_overflowing_tokens, return_special_tokens_mask, > return_offsets_mapping, return_length, verbose, **kwargs) And these parameters of tokenizer.encode_plus() are outdated but might show that there are strategies: > Backward compatibility for truncation_strategy, pad_to_max_length, > padding_strategy, truncation_strategy, max_length, > kwargs = self._get_padding_truncation_strategies(... Fixed code with from datasets import load_dataset And here is how you can put that in the Trainer class as the argument for "train_dataset". We must switch to the from dataset import Dataset class, as the warning (see Outdated Transformers TextDataset class drops last block when text overlaps. Replace by datasets Dataset class as input of Trainer train_dataset? ) /srv/home/my_user/.local/lib/python3.9/site-packages/transformers/data/datasets/language_modeling.py:54: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py tells you to, but we will need only from datasets import load_dataset instead. You can make the needed dataset from your own text file for the Trainer class where you can also handle the needed parameters, see again Outdated Transformers TextDataset class drops last block when text overlaps. Replace by datasets Dataset class as input of Trainer train_dataset? . The code that runs through: from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling, Trainer, TrainingArguments from datasets import load_dataset model_name = "dbmdz/german-gpt2" tokenizer = AutoTokenizer.from_pretrained(model_name) file_path = './myfile.txt' padding = "max_length" bln_truncation = False dataset = load_dataset("text", data_files={"train": file_path}) block_size = 512 tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=block_size, eos_token="\n", pad_token="[PAD]") def tokenize_function(examples): return tokenizer(examples["text"], padding=padding, truncation=bln_truncation) tokenized_datasets = dataset.map(tokenize_function, batched=True) data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False) training_args = TrainingArguments( output_dir="./output", overwrite_output_dir=True, num_train_epochs=1, per_device_train_batch_size=1, save_steps=10000, ) model = AutoModelForCausalLM.from_pretrained(model_name) trainer = Trainer( model=model, args=training_args, data_collator=data_collator, train_dataset=tokenized_datasets["train"], ) trainer.train() Leave Trainer class and switch to native PyTorch/Keras? The Pytorch Trainer class has an open issue since August 2020 that you cannot set the device, it will always default to "GPU 0": Transformers Trainer: "RuntimeError: module must have its parameters ... on device cuda:6 (device_ids[0]) but found one of them on device: cuda:0" , and as we saw above, it is quite hard to feed with the right self-built dataset. Even if I got it to work now with the datasets package and this small workaround of setting "GPU 0" as the device and in front of the device lists, native PyTorch/Keras might be a good choice if this black box Pytorch Trainer class seems to be buggy again. Yet, mind that the Trainer class seems to be first choice at Huggingface Fine-tune a pretrained model .
