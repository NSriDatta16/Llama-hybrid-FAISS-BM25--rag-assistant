[site]: datascience
[post_id]: 123017
[parent_id]: 
[tags]: 
How to use machine learning to classify data as correct and incorrect in time series data?

I have data from thousands of brands. For each brand, I have a single measure - say, weekly purchases - for over five years. However, I know that, in some cases, the values were measured incorrectly, and I can tell that roughly by looking at the distribution of the data (for instance, high values are consistently followed by too many 0s that jump up to high values again). So far, I have manually checked this for several brands to train a supervised machine learning classifier that will then classify unseen data on whether it has been measured correctly or not. However, I never worked with datasets where only a single variable is measured over time. What kind of machine learning algorithms work better with this type of data?
