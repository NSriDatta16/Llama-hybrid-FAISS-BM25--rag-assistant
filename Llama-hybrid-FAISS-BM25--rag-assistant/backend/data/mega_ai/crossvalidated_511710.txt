[site]: crossvalidated
[post_id]: 511710
[parent_id]: 
[tags]: 
Can long run variance of a time series be used to test mean of the series?

Let $Y_t$ be a stationary time series with $Y_t \sim N(\mu, \gamma_0) \,\, \forall t$ . Further define, $$\bar Y_T \equiv (1/T)\sum\limits_{t=1}^TY_t$$ Further, let $\sigma^2_l$ be the long-run variance this time series. From this answer , I think we can say: $$\lim_{T \to \infty} \frac{\bar Y_T-\mu}{\sigma_l/\sqrt{T}} \sim N(0,1)$$ This is because $\bar Y_T$ is a sum of $T$ normal random variables with (unconditional) mean $\mu$ . Now say we have a consistent estimate of $\sigma^2_l, \hat \sigma^2_l$ . Can we use the following statistic to test for hypothesis such as $H_0: \mu = \mu_0$ ? I feel that if the true dependence structure of the time series was known (say for example, we know that $Y_t$ follows $AR(p)$ ), then the standard error of the residuals of the $AR(p)$ model, estimated using MLE, is nothing but an estimate of long-run variance. If true, then the above allows us to test for mean without having to fit a model (although the bandwidth selection is perhaps a similar exercise?).
