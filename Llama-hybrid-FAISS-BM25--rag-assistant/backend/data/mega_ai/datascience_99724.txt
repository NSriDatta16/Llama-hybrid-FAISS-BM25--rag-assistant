[site]: datascience
[post_id]: 99724
[parent_id]: 99723
[tags]: 
If you use pretrained Transformer, then you might need very very small dataset. ( I have achieved good accuracy with as low as 100 training sample with a positive/negative sentiment classification). But if you try to train a transformer from scratch , it will require a huge dataset. Similarly, if you use pretrained Word2Vec/Glove to embed texts and use a LSTM network to train, then you can get good accuracy with very small dataset (again as low as 100 sample for binary classification). But training a Word2Vec from scratch will require a decent amount of training sample. If we compare training from scratch for both cases, from my experience, transformer will require much larger dataset than LSTM.
