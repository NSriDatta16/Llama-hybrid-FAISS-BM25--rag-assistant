[site]: crossvalidated
[post_id]: 76698
[parent_id]: 76693
[tags]: 
There are many - and what works best depends on the data. There are also many ways to cheat - for example, you can perform probability calibration on the outputs of any classifier that gives some semblance of a score (i.e.: a dot product between the weight vector and the input). The most common example of this is called Platt's scaling. There is also the matter of the shape of the underlying model. If you have polynomial interactions with your data, then vanilla logistic regression will not be able to model it well. But you could use a kerneled version of logistic regression so that the model fits the data better. This usually increases the "goodness" of the probability outputs since you are also improving the accuracy of the classifier. Generally, most models that do give probabilities are usually using a logistic function, so it can be hard to compare. It just tends to work well in practice, Bayesian networks are an alternative. Naive Bayes just makes too simplistic an assumption for its probabilities to be any good - and that is easily observed on any reasonably sized data set. In the end, its usually easier to increase the quality of your probability estimates by picking the model that can represent the data better. In this sense, it doesn't matter too much how you get the probabilities. If you can get 70% accuracy with logistic regression, and 98% with a SVM - then just giving a "full confidence" probability alone will make you results "better" by most scoring methods, even though they aren't really probabilities (and then you can do the calibration I mentioned before, making them actually better). The same question in the context of being unable to get an accurate classifier is more interesting, but I'm not sure anyones studied / compared in such a scenario.
