[site]: crossvalidated
[post_id]: 621078
[parent_id]: 589227
[tags]: 
Using class weights is pretty much guaranteed to give you badly calibrated probabilities. You end up biasing probabilities towards too low a probability for classes that you have given lower weights (or if you give higher weights to rarer classes, you are biasing the probability to be too high for those classes). That is, unless you predictors allow you to predict (almost) perfectly. E.g. it's a whole less of an issue with say image classification, where the images make it very clear what the correct class is (so the model does not need to resort to using the prevalence of the class much in its predicted probability - although it could be a problem when the model suddenly faces e.g. very low quality images). Obviously, any scenario between a model where predictors have no value (best possible model is just to predict the class prevalence, which will be very wrong with class weights that try to "fix" those to be equal) to scenarios with lots of data + the ability to classify perfectly from the predictors. Another option would be to correct the probabilities for the over-/under-weighting (or sampling) by adjusting the predicted logit-probability of a class with an offset that corrects for it. Why are over-/under-sampling and class-weights used so much then? There's several reasons. Some people really mostly work with scenarios where it does not do harm (possibly in combination with point 2). Some models could struggle, if they see some classes too rarely. E.g. neural networks might end up getting dead neurons and become unable to predict a class, if they don't see it for too many batches in a row. This might be a genuinely good reason. People focus too much on metrics defined by predicted probability 0.5 (e.g. accuracy defined in that way) and ignore/don't care/don't realize the problem regarding calibration. Lots of software has options for it. Lots of people recommend doing it, because other people have recommended doing it to them. The root cause might be (3), because as far as I'm aware there's no evidence it generally is a good idea unless one focusses on (3).
