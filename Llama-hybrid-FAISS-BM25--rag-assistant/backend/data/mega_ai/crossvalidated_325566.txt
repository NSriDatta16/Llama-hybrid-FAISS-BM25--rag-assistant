[site]: crossvalidated
[post_id]: 325566
[parent_id]: 325563
[tags]: 
Intuitively, convergence in probability means the random variables get close to a nonrandom constant, and convergence in distribution means that it gets close to another random variable. Closeness will mean different things in each situation. Example : Say $X_1, \ldots, X_N$ are independent and identically distributed with mean $\mu$ (we don't have to assume which distribution they come from). By the Weak Law of Large Numbers $$ \bar{X}_n \overset{p}{\to} \mu $$ as $n \to \infty$. Here $\mu$ is a constant, and this result is useful because we know if we have a large dataset, we can effectively "recover" the true and unknown parameter $\mu$. Also, by the Central Limit Theorem, if in addition to the above assumptions we assume the variance of each $X_i$ is $\sigma^2$, then $$ \sqrt{n}\bar{X}_n \overset{D}{\to} \text{Normal}(\mu, \sigma^2) $$ as $n \to \infty$. Notice that the right hand side is a random variable. This is useful for making confidence intervals, and quantifying the uncertainty about estimates of $\mu$. If we have a large dataset, we can be justified in using those $z_{\alpha/2}$ standard Normal quantiles that are commonly found in CI formulas. Moreover, this works even if we don't make any assumptions about what probability distribution is generating the data. Note that $\{\sqrt{n}\bar{X}_n\}$ is a different sequence than $\{\bar{X}_n\}$. We need a weaker type of convergence for the former because scaling it by bigger and bigger $\sqrt{n}$ increases the variance by an increasing $n$; however this "balances out" with the shrinking variance of $\bar{X}_n$. The net result is that the sequence of products coalesces around a random variable for which many formulas are known. Even though convergence in probability for one sequence always implies convergence in distribution for the same sequence (this is mentioned in other answers), in this particular example, we have convergence in distribution for the scaled sequence implying convergence in probability for the sequence of un-scaled sample averages (kind of reversing the order of implication). I mention this because CLTs are often more valuable than results about convergence in probability. This is because if we know how fast we can scale up the sequence of estimators and still have it converge, this tells us something about the convergence "rate." In this particular example, the CLT tells us that $\bar{X}_n$ is "root-n consistent" (not to be confused with regular consistency, which is mentioned in other answers).
