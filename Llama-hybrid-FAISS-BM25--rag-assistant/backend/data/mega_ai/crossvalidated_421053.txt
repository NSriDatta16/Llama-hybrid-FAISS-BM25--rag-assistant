[site]: crossvalidated
[post_id]: 421053
[parent_id]: 421038
[tags]: 
Most classical software packages (and econometrics in general) were designed for model fitting and testing hypothesized relationships on a full sample of data and were never really meant to be tested out-of-sample. It is only rather recently that prediction has gotten so central to statistics, obviously owing to the rise of ML (and so much data that you can actually permit 20% to be used as a test). In general, it used to be highly unusual in at least academic empirical work to ever test a model out-of-sample. I think it is very illustrative that for example scikit learn in Python, which includes a Logistic Regression functionality, does not include some reporting tool on coefficients' significance (like the summary function in R does or the statsmodel in Python). They clearly serve different purposes. In short - most 'academic' summary statistics on residuals will most likely only be available to 'train'-sample models because they fall in the classic model-fitting paradigm. Most 'predictive' model packages usually care less about statistical significance and the type of concerns for which one would evaluate statistical tests or statistics like you mention and 'just' want to predict well. Note all the quotation marks since I don't want to make any value judgement either way - they are just two different beasts..
