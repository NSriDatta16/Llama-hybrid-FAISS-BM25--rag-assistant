[site]: crossvalidated
[post_id]: 329320
[parent_id]: 
[tags]: 
Attention methods

When using Attention, for example with LSTM (but not necessarily), one can use the following methods to attend: MLP: $ug(W^1v+W^2q)$ dot product: $v \cdot q$ biaffine transform: $v^TWq$ ($v$ is the attended vector which is used for prediction, $q$ is the query vector determining the weighted sum, used in the encoded input vectors attention) What are the pros/cons of using each attention method? Or, what are the practical differences in the result?
