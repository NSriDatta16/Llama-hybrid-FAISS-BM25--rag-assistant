[site]: crossvalidated
[post_id]: 244942
[parent_id]: 
[tags]: 
Output of all possible linear models w.r.t Gaussian Posterior

In page 5 of chapter 2 of Gaussian Process for machine learning [1], the author mentions the following: $p(f_*| {\bf x_*}, X, {\bf y}) = \int p(f_*| x_*, {\bf w} )\space p(w | X, {\bf y} ) = \mathcal{N}\space(\frac{1}{\sigma_n^2} {\bf x_*}^T A^{-1}X {\bf y}, {\bf x_*}^T A^{-1} {\bf x_*})$ Is it possible to prove the above without rigrous math? Given that both $p(f_*| x_*, {\bf w} ) $ and $p(w | X, {\bf y} )$ are gaussian ditributions. The distributions are given in equations 2.3 and 2.8 of http://www.gaussianprocess.org/gpml/chapters/RW2.pdf [2] . [1] http://www.gaussianprocess.org/gpml/ [2] http://www.gaussianprocess.org/gpml/chapters/RW2.pdf
