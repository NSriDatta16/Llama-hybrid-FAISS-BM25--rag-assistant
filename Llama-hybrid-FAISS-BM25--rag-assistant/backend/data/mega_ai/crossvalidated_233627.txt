[site]: crossvalidated
[post_id]: 233627
[parent_id]: 
[tags]: 
K nearest neighbours algorithm interpretation

In the book, The Elements of Statistical Learning http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf (page 19-20) The author claims that In the least squares method, we assume the regression function $f(x)$ is well approximated by a globally linear function. In the k nearest neighbors method we assume the regression function $f(x)$ is well approximated by a locally constant function. I can understand this statement when $k=1$ (like we assume that on the line segment joining the target point $x_i$ and it's nearest neighbor $x_j$ the function is constant). I am unable to understand this statement when $k \neq 1$. If $k \neq 1$ we average over the $k$ nearest neighbors. If we assume the function to be locally constant, why do we need to average, we can just pick one of the k neighbors. Can anyone clarify what the author means by statement 2.
