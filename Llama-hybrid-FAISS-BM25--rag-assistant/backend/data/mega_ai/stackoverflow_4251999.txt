[site]: stackoverflow
[post_id]: 4251999
[parent_id]: 
[tags]: 
Adaptive IO Optimization Problem

Here is an interesting optimization problem that I think about for some days now: In a system I read data from a slow IO device. I don't know beforehand how much data I need. The exact length is only known once I have read an entire package (think of it as it has some kind of end-symbol). Reading more data than required is not a problem except that it wastes time in IO. Two constrains also come into play: Reads are very slow. Each byte I read costs. Also each read-request has a constant setup cost regardless of the number of bytes I read. This makes reading byte by byte costly. As a rule of thumb: the setup costs are roughly as expensive as a read of 5 bytes. The packages I read are usually between 9 and 64 bytes, but there are rare occurrences larger or smaller packages. The entire range will be between 1 to 120 bytes. Of course I know a little bit of my data: Packages come in sequences of identical sizes. I can classify three patterns here: Sequences of reads with identical sizes: A A A A A ... Alternating sequences: A B A B A B A B ... And sequences of triples: A B C A B C A B C ... The special case of degenerated triples exist as well: A A B A A B A A B ... (A, B and C denote some package size between 1 and 120 here). Question: Based on the size of the previous packages, how do I predict the size of the next read request? I need something that adapts fast, uses little storage (lets say below 500 bytes) and is fast from a computational point of view as well. Oh - and pre-generating some tables won't work because the statistic of read sizes can vary a lot with different devices I read from. Any ideas?
