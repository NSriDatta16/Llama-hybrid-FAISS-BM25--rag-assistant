[site]: crossvalidated
[post_id]: 390060
[parent_id]: 74499
[tags]: 
Just to clarify, $$ \frac{1}{2}\|w\|^2 $$ is minimized subject to the constraint that the points are linearly separable (I.e. one can draw a hyperplane that perfectly separates the two). In other words, the only allowed values of w that we can consider as solutions are those that separate the two sets of points. Now, it is thought that hard margin SVM "overfits" more readily than soft margin. This is easier to imagine with a RBF SVM with high enough of a $\gamma$ , which can create (overly) complicated and (potentially) over-fit decision boundaries. The harder the margin (emulated imprecisely with a higher "C"), the harder the search will try to find decision boundaries that perfectly classify the two sets of points. When we move to "soft margin", the constraints are relaxed and replaced with a restraint through the introduction of "slack". This slack variable is defined with a "hinge loss" term. After simplification, one arrives at the hinge + l2 like loss term everyone associates with SVMs. FWIW, I like to frame SVMs as more of an optimization problem instead of the omnipresent "follow the gradients" problem.
