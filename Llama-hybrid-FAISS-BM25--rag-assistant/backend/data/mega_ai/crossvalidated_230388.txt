[site]: crossvalidated
[post_id]: 230388
[parent_id]: 
[tags]: 
How does linear base learner works in boosting? And how does it works in the xgboost library?

I know how to implement linear objective function and linear boosts in XGBoost. My concrete question is: when the algorithm it fits the residual (or the negative gradient) is it using one feature at each step (i.e. univariate model) or all features (multivariate model)? Any reference to documentation about the linear boosts in XGBoost will be appreciated. EDIT: Linear boosts can be implemented in XGBoost by setting the 'booster' parameter to 'gblinear'. See: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/ for useful information on linear boosting. Note that I am not speaking about the objective function (which can be also linear) but about the boosts themselve. Thanks!
