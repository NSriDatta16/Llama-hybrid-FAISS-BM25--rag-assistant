[site]: crossvalidated
[post_id]: 55784
[parent_id]: 
[tags]: 
Dimensionality Reduction Algorithm for Large Dataset?

I have a reasonably large (5k variables x 120k cases) that I'd like to run a dimensionality reduction algorithm on. I tried doing a simple Factor Analysis on it in SPSS, but it (predictably) barfed on a 3GB machine. I got an answer by truncating the data set to 2.5k variables and 25k cases, but I started wondering if there is an algorithm other than Factor Analysis/PCA that would handle the data set better. My background in software development is much stronger than my background in statistics. It's my hope that there's an algorithm that could handle the original, unsampled data set more easily in 3GB of RAM. Does anyone know of such an algorithm? Forgive me if this is a FAQ; I tried finding the answer before I posted. EDIT : Ideally I'd like an existing implementation in something like R or SPSS, but since I'm a dev, even a software-based solution (like "try this feature of numpy ") would be extremely helpful.
