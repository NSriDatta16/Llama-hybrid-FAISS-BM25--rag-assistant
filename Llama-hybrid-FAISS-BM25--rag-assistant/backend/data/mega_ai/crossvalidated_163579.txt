[site]: crossvalidated
[post_id]: 163579
[parent_id]: 
[tags]: 
variational inference with KL

i am self-studying variational inference - and in Murphy's book "A probabilistic perspective on machine learning" it is discussed that minimizing the forward KL divergence (which is stated to be zero-avoiding for q) $$KL (p||q) = \sum\limits_{x} p(x)log \frac {p(x)}{q(x)}$$ gives different results than minimizing the reverse KL divergence $$KL (q||p) = \sum\limits_{x} q(x)log \frac {q(x)}{p(x)}$$ (which is stated to be zero-forcing for q). It is subsequently linked to the diagrams per below. I was hoping whehter someone can share the intuition on how the formulas for reverse and forward KL lead to the different approximations per below.
