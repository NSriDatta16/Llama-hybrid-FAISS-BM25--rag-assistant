[site]: crossvalidated
[post_id]: 609646
[parent_id]: 
[tags]: 
Evaluate relative quality of covariance matrix relative to a set

My ultimate goal is a way to evaluate a group of "m" covariance matrices (all size n*n) so I can pick an arbitrary one and calculate "this one is tighter than the average covariance matrix by a quantifiable amount" so I can select the top XX% and weight them according to how good they are. One approach would be to describe the mean and (co)variance of the set of m covariance matrices, maybe by incorporating an additional dimension: the "mean" covariance would be n*n computed by taking the mean in the new dimension. extend normal covariance matrix calculation by computing the covariance matrix in the new dimension for each row and column, then multiply the values where the "grid" of new covariance matrices "collide" to form an n*n*n "covariance cube." or I could reshape the n*n covariance matrices to an n^2*m array where basically each column is the "flattened" version of each covariance matrix. Then I can use standard tools to compute the mean (n^2*1) and covariance (n^2*n^2)...maybe these are even the same values I'd get making a cube?? Alternatively, my mentor suggested just taking the eigenvalues of each covariance matrix and summing them to give an idea of how "tight" each one is and then compare those scalars against each other. Doesn't seem as rigorous but it might work. For background, I am investigating a nonlinear estimation problem where I want to compare the quality of a hypothesis to the set of all the hypotheses that I am investigating (where each hypothesis quality is described by an n*n covariance matrix). Ideally this could extend one dimension further which could result in a hypercube or n^3*n^3 covariance matrix? In the case of the eigenvalues it scales very easily. Thank you!
