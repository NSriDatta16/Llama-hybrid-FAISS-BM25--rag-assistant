[site]: crossvalidated
[post_id]: 288030
[parent_id]: 
[tags]: 
Strange behavior in Tensorflow with increasing data feed speed?

I'm not sure if this is more of a Stack Overflow or a Cross Validated sort of thing, but maybe somebody has some insight as to this. I'm currently trying to train a convolutional, stacked, denoising autoencoder in Tensorflow. Except because I have too much data to fit onto RAM, I have to pull it from disk in order to work with it, which obviously slows down training absurdly if for every batch I need to go fetch it from disk. Though, for the time being, while I developed a faster data feeding mechanism, I just used a slow, single-threaded batch iterator to train the model, and all was fine. The model still learned, and converged nicely after some time (granted it took a while between steps). When I finished the new batch iterator, I used a job queue to queue up batches of lines that need to be fetched, and then worker threads (well "Processes" to be specific, as Python does make a distinction) would pop the jobs from the job queue, generate a new batch, and deposit the batch in an output queue, where a normal iterator grabs these one at a time to feed to the model. The issue becomes, when I use this faster iterator feed the model, my model tends to randomly diverge after about two or three hundred global steps. Just for the heck of it, I even tried out a model configuration I had already used, which had converged nicely using the slow iterator, and under this new iterator, it also diverged consistently by the time it even passed 500 global steps. After that, I decided to temporarily ignore previous evidence, and just try to fix it the way one normally fixes early divergence, so I cranked the learning rate down by a factor of 20, from 0.001 to 0.00005, but that just resulted in no movement at all. And anything between those two numbers still resulted in divergence. I've been staring at this code for a while now, and I can't find anything wrong with the faster iterator as compared to my old slow one. They seem to be both working perfectly. I'm using the Adam optimizer if that helps. Time between global steps is no longer relatively constant (it loads in data in spurts really), so maybe there's some sort of time dependence I wasn't aware of? Or maybe my iterator isn't really working as well as I think it is, in which case, how do I go about debugging something that has such a high throughput?
