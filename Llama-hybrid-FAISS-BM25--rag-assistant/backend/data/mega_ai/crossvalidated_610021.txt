[site]: crossvalidated
[post_id]: 610021
[parent_id]: 609970
[tags]: 
The terms "L1" and "L2" refer to special functions called norms , which measure the length or size of a vector. You are correct in that they are used in two different contexts in statistics and machine learning, but their meanings are the same in both contexts. In the context of regularization , the L1 and/or L2 norm restricts the magnitude of the parameter vector of a model. The difference between L1 and L2 regularization comes down to the differences between the L1 and L2 norms. See e.g. https://medium.com/analytics-vidhya/effects-of-l1-and-l2-regularization-explained-5a916ecf4f06 . As pointed out in other answers, L1 regularization in a regression model corresponds to a Laplace prior on coefficients in Bayesian modeling, and L2 regularization corresponds to a Gaussian prior. In the context of loss functions , the L1 or L2 norm measures the magnitude of the error vector of the model on a train/test/validation set. L1 loss is the Median Absolute Error (MAE), and L2 loss is the Root Mean Squared Error (RMSE). As pointed out in the comments, regression models fitted with L1 loss are models of a conditional median , while models fitted with L2 loss are models of a conditional expectation ( conditional mean ). The latter also happens to correspond with a Gaussian GLM maximum-likelihood model, where the conditional distribution of the data follows a Gaussian distribution centered at the regression prediction. The L2 norm corresponds to our conventional notion of Euclidean distance , which is essentially a multi-dimensional extension of the Pythagorean theorem. You can think of Euclidean distances as the lengths of hypotenuses of right triangles drawn between points. The L1 norm corresponds to the weirder notion of Manhattan (aka "Taxicab") distance , so named because distances resemble the distance traveled by a taxi cab following the grid layout of streets in Manhattan, New York. It's very common in statistics and machine learning to use L2 loss (MSE) with L1 regularization, or even both L1 and L2 regularization in the same model. L1 loss (MAE) is much less common than L2 in general, in part because the absolute value is not differentiable. However there is a "smooth" differentiable L1 loss that attempts to mimic the properties of true L1 loss, see e.g. How to interpret smooth l1 loss? .
