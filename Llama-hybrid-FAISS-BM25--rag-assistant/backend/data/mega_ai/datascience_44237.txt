[site]: datascience
[post_id]: 44237
[parent_id]: 44228
[tags]: 
I have not heard about such an estimation. Think about it like this: You need a method, that given a similar dataset will predict labels for another dataset. Then, you will use the labels as the groundtruth. But how such a method is different from a machine learning model? In other words, any estimated accuracy on such labels will tell you not how close is your model to the real annotations, but how is it close to another model used to generate the pseudo-groundtruth. This will make sense if your 'label generator' performs with a near 100% accuracy, but I am afraid such tasks are very rare. At least, outside labs.
