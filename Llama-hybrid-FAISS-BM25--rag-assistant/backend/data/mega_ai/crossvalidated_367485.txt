[site]: crossvalidated
[post_id]: 367485
[parent_id]: 
[tags]: 
MAP estimation as regularisation of MLE

Going through the Wikipedia article on Maximum a posteriori estimation , it got confusing after reading this: It is closely related to the method of maximum likelihood (ML) estimation, but employs an augmented optimization objective which incorporates a prior distribution (that quantifies the additional information available through prior knowledge of a related event) over the quantity one wants to estimate. MAP estimation can therefore be seen as a regularization of ML estimation. How can the MAP estimation be seen as a regularization of ML estimation? EDIT: My understanding of regularization being penalizing high weights in the context of Machine learning. That being done through modifying the optimization problem by adding a term in the loss function which contains the weights to be learned. And the objective being minimization of loss, the parameters with higher values get penalized more. An intuitive explanation is very welcome.
