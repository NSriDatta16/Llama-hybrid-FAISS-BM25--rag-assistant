[site]: datascience
[post_id]: 126053
[parent_id]: 79995
[tags]: 
I think you should understand the equation like this: When the dimension(i) is even, we calculate the position encoding like But when the dimension is odd, we should make the calculation using the former index which means when we calculate the value of dimension 3， we should take 2 into the equation as the parameter i. I recommend to read the simple code in a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1 . The author realize the postion encoding as : import numpy as np import matplotlib.pyplot as plt def getPositionEncoding(seq_len, d, n=10000): P = np.zeros((seq_len, d)) for k in range(seq_len): for i in np.arange(int(d/2)): denominator = np.power(n, 2*i/d) P[k, 2*i] = np.sin(k/denominator) P[k, 2*i+1] = np.cos(k/denominator) return P P = getPositionEncoding(seq_len=4, d=4, n=100) print(P) in the second for loop, the index range is [0,int(d/2)] For example， when we have dimension 4 [0,1,2,3], the index range is [0,2] which is [0,1,2]. When the i is 2, we calculate the values for dimension 2 and 3 and they have the same denominator. This is consistant with what I am saying when the dimension is odd, we should make the calculation using the former index This is my understanding way
