[site]: datascience
[post_id]: 75405
[parent_id]: 26597
[tags]: 
1.) The optimal number of neurons in each layer depends on your function you try to approximate. For one function, there might be a perfect number of neurons in one layer. But for another fuction, this number might be different. 2.) According to the Universal approximation theorem, a neural network with only one hidden layer can approximate any function (under mild conditions), in the limit of increasing the number of neurons. 3.) In practice, a good strategy is to consider the number of neurons per layer as a hyperparameter. A recent study showed that optimizing these hyperparameters should not be done independently. Instead, one can perform exhaustive grid search on a small neural network, to find the best hyperparameters, and then scale the entire neural network ( EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks ).
