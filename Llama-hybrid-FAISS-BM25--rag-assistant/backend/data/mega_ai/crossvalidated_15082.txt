[site]: crossvalidated
[post_id]: 15082
[parent_id]: 15036
[tags]: 
Except for the inclusion of the same positive class examples in each bag this is S^3Bagging as described in S^3Bagging: Fast Classifier Induction Method with Subsampling and Bagging . (I haven't reviewed this paper in depth, just skimmed it.) I see nothing theoretically wrong with your approach, although I've much more frequently seen subsampling combined with boosting than bagging. This may not exactly address your question, but an excellent paper on different ways of dealing with imbalanced data is Learning From Imbalanced Data . It seams like cost sensitive learning may be more appropriate in your case. Since you're using Decision Forests, Section 3.2.3 Cost-Sensitive Decision Trees would probably be helpful. It states, In regards to decision trees, cost-sensitive fitting can take three forms: first, cost-sensitive adjustments can be applied to the decision threshold; second, cost-sensitive considerations can be given to the split criteria at each node; and lastly, cost-sensitive pruning schemes can be applied to the tree Cost sensitive adjustments to the decision threshold basically means picking your decision threshold based on the ROC or Precision-Recall Curve performance. The PRC performance in particular is robust to imbalanced data. Cost sensitive split criteria comes down to changing your impurity function to deal with imbalanced data. The above mentioned paper states, In [63], three specific impurity functions, Gini, Entropy, and DKM, were shown to have improved cost insensitivity compared with the accuracy/error rate baseline. Moreover, these empirical experiments also showed that using the DKM function generally produced smaller unpruned decision trees that at worse provided accuracies comparable to Gini and Entropy. A detailed theoretical basis explaining the conclusions of these empirical results was later established in [49], which generalizes the effects of decision tree growth for any choice of spit criteria. As to pruning, However, in the presence of imbalanced data, pruning procedures tend to remove leaves describing the minority concept. It has been shown that though pruning trees induced from imbalanced data can hinder performance, using unpruned trees in such cases does not improve performance [23]. As a result, attention has been given to improving the class probability estimate at each node to develop more representative decision tree structures such that pruning can be applied with positive effects. Some representative works include the Laplace smoothing method of the probability estimate and the Laplace pruning technique [49]. [23] N. Japkowicz and S. Stephen, “The Class Imbalance Problem: A Systematic Study,” Intelligent Data Analysis, vol. 6, no. 5, pp. 429- 449, 2002. [49] C. Elkan, “The Foundations of Cost-Sensitive Learning,” Proc. Int’l Joint Conf. Artificial Intelligence, pp. 973-978, 2001. [63] C. Drummond and R.C. Holte, “Exploiting the Cost (In)Sensitivity of Decision Tree Splitting Criteria,” Proc. Int’l Conf. Machine Learning, pp. 239-246, 2000.
