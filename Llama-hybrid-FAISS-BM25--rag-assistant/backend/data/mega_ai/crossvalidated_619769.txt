[site]: crossvalidated
[post_id]: 619769
[parent_id]: 
[tags]: 
Why do we weight the surprisals by the probabilities when computing the entropy?

Let's say that a random variable $X$ takes the values $(x_1, x_2, \dots, x_n)$ over the sample space $\Omega$ . As far as my understanding goes, the entropy of a given variable is meant to give an idea of how small are the probabilities of the same variable taking the values that it takes, and thus knowledge of a variable that has a high value of entropy would significantly limit the possible values taken by other variables over the same sample space. My question is, if we want to measure how small, on average, the probabilities $(p(x_i))_{i \in [1,n]}$ are, why don't we just calculate the result of the term: $$\frac{1}{n}\sum_{i \in [1, n] } log(\frac{1}{p(x_i)})$$ Why do we have to weight it by the probabilities $(p(x_i))_{i \in [1, n]}$ which seem to produce a counter effect to the one we intend to measure (the higher the surprisal the smaller its weigh and thus the less important it is to the overall sum)? I can see that the textbook formula for entropy (for lack of a better term) is indeed the average of the variable $-log(P(X))$ over the sample space $\Omega$ . But it seems more intuitive to me to calculate the expectation of the same variable over $X(\Omega)$ instead, which translates to the formula aformentioned.
