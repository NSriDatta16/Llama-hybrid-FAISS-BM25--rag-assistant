[site]: datascience
[post_id]: 28779
[parent_id]: 28778
[tags]: 
Are you asking if the 300 dimension 'middle layer' has learned to encode words into an embedding like word2vec? If this is the case, I'd say no. There's nothing in the one-hot vector that specifies distance between two words. For instance, the one-hot vectors for cat and table will be the same distance away for cat and lion . With embeddings, clearly you want to have the second example closer together.
