[site]: crossvalidated
[post_id]: 387635
[parent_id]: 377865
[tags]: 
Yes, as authors above said, the Random Forest algorithm is a bagging, not boosting algorithm. Bagging can reduce the variance of the classificator, because the base algorithms, that are fitted on different samples and their errors are mutually compensated for in the voting. Bagging refers to averaging slightly different versions of the same model as a means to improve the predictive power. To apply bagging we simply construct B regression trees using B bootstrapped training sets, and average the resulting predictions A common and quite successful application of bagging is the Random Forest But when building these decision trees in random forest, each time a split in a tree is considered, a random sample of m predictors is chosen as split candidates from the full set of p predictors. The split is allowed to use only one of those m predictors.
