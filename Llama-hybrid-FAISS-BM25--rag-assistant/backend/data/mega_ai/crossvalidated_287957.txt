[site]: crossvalidated
[post_id]: 287957
[parent_id]: 218243
[tags]: 
Addressing your issues one by one: 1) OCR: This is probably the easiest of your problems as there are many algorithms that perform well in this task. As a reference, in the best known handwritten digit dataset, MNIST , several algorithms have achieved over 99.5% accuracy (the state-of-the-art being Convolutional Neural Networks). You can also find many out-of-the-box solutions to your problem; it helps a lot if your data is in English, as the tools there are more advanced. If your scans are noisy you may try denoising them first. 2) You need to do some preprocessing for this issue. First I would suggest, if possible, creating a bag of words , i.e. a list of all unique words in your "corpus". Verify that all these words are correct and perform a string distance comparison (e.g. hamming distance) to correct 1-2 letter typos. Another thing I would do would be to calculate the occurrences of each term in your bag and remove the least frequent ones (e.g. terms that occur less than N times in your corpus are probably typos, or remove least frequent M% of your terms). That should significantly reduce the noise in your dataset. 3) In order to solve this issue you need to perform some sort of semantic labelling. If you are familiar with ontologies, their hierarchical structure can help here a lot. You can create rules like "coca-cola" is a "soft drink" which is a type of beverage, etc. I don't have experience in R, but I'm sure you can find tools to perform all the above quite easily.
