[site]: crossvalidated
[post_id]: 304822
[parent_id]: 
[tags]: 
Does triplet loss help for document similarity search?

I build CNN network over documents with triplet loss. And compare documents with cosine similarity. It does really find similar docs and catches interesting dependencies. But simple tfidf model does it better in terms of my test set. I use convolution over words (with word2vec embedding) in a sentence and then make second convolution over sentences. This gives me an unnormalized document vector, then I L2 normalize it. Did you try to do something like this? Any suggestions?
