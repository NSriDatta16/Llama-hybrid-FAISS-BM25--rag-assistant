[site]: datascience
[post_id]: 76626
[parent_id]: 29287
[tags]: 
DTW per-group is the obvious answer. I've found DTW to be extremely computationally expensive, however. My favorite technique in the world is lesser-known/used, and would handle your problem nicely, however... and have the added benefit of being scalable. The downside to this technique is that it would only cluster on the "shape" of the time series, and not require it to line up at a specific time interval like DTW will. It's called SAX . Basically, you represent the time series as a string of letters. Then, you can treat this string of letters exactly like you do in NLP or text-mining - by creating a frequency matrix for each letter, n-grams, etc. Now, along with all of those features you could add your other features as well, and run normal dimensionality reduction and clustering.
