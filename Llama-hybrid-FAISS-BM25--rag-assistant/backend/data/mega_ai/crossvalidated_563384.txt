[site]: crossvalidated
[post_id]: 563384
[parent_id]: 
[tags]: 
Is it okay to not use batchnorm and relu before global average pooling?

I have built and experiment with a small network by batchnorm-relu-conv rather than conv-batchnorm-relu as suggested by DenseNet(2017). In denseNet, Before global average pooling layer, there are batchnorm and relu added below the last conv layer. What would happen if i removed batchnorm and relu from here? In my experiments, contrary to my expectations, the network showed higher accuracy when removing batchnorm and relu. Can someone explain this result?
