[site]: crossvalidated
[post_id]: 622657
[parent_id]: 
[tags]: 
Why does the variance term in the VICREG paper encourage the variance to be equal to γ?

In section 4.1 of the VICreg paper the authors describe the variance regularization term as the following hinge loss. $$ v(Z) = \frac{1}{d}\sum_{j=1}^{d}max(0, \gamma - S(z^j, \epsilon )) $$ Z being the embedding of an image of shape $ \mathbb{R}^{(n, d)} $ S being a "safe" standard deviation (which uses a small constant $ \epsilon $ ). $ \gamma $ being set to a constant 1 After that they state: " This criterion encourages the variance inside the current batch to be equal to γ along each dimension, preventing collapse with all the inputs mapped on the same vector ". My question is, why / how does this variance term encourages $ v $ to be equal to $ \gamma $ (1 in this case)? I would assume that minimizing $ v(Z) $ would result in having a large standard deviation (S >> $ \gamma $ ) and that this would lead to the hinge loss being 0, instead of 1.
