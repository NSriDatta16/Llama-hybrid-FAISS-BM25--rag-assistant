[site]: crossvalidated
[post_id]: 547032
[parent_id]: 547027
[tags]: 
Question 1: Was I right in $\left(*\right)$ ? If the point can't get out from some local minima, then does it imply that all (or the majority?) these batches have reached their own local minimas? Meaning that there is a common local minima at $k = 0$ in our example. You can see it observing different dotted curves. My line of reasoning it the following one: If most of different batches haven't reached their local minimas, then they will eventually push the red point out of this minima in the picture above (I know it's wrong to say "batches reached their local minimas", but I didn't find a better way to express this). So I believe that the point can't get out of the local minima if this local minima is the local minima for the most (how many?) batches. Not quite. A necessary, but not sufficient , condition of a local minimum is that $\nabla J(k) = 0$ . Therefore, it is possible that $\nabla J(k) = 0$ for all batches, but it is possible that this is just a saddle point for some batches and not a local minimum. A sufficient condition for a local minimum is that $\nabla J(k) = 0$ and the Hessian is positive-definite. If the Hessian is also positive definite for all batches at a single point, then this local minimum is common for all batches. Question 2: It's not clear to me where does this red point tend to? It obviously can't always tend to a global minima (which is $k = 2$ ). My guess is that the point tends to some minima near the point's initial position that is local minima for the most of batches (dotted curves). But I'm not sure if I'm right. And what if there is no common minima for all different batches (dotted curves)? The purpose of stochastic gradient descent, besides making computing the cost function $J(k)$ , is the randomness. As you have observed, the cost function becomes random. Normal gradient descent without the randomness depends on a steady target (where $\nabla J(k) = 0$ ). In stochastic gradient descent, the target is always moving, so how can you expect it to converge to a single point? While it may seem that a moving target is pointless, you may have observed that there is a chance that one of the batches has a large gradient at the local (not global) minimum of another batch that SGD is currently stuck on, which could push it to jump out of the local minimum for another chance at arriving at a global minimum. Furthermore, if your batches have common statistics, then it is reasonable to predict that a local minimum for one batch will be close to a local minimum for another batch. This page on stochastic approximation could be helpful. Responses to comments I know that it's possible for SGD to start in a deep local minimum and end up in a shallower (higher) local minimum. Question 1: is it just because there wasn't a minimum deep enough to reduce batches' gradients enough not to escape this minimum? It could also depend on the curvature of the deeper local minimum. If its curvature is very steep, then it is possible that gradients around the deeper local minimum are big enough to cause a jump outside of it, regardless of how deep the local minimum actually is. Question 2: What should we do to be sure that SGD ends up in a minimum that is not worse than its initial minimum? Just reduce learning rate or batch size to make it less stochastic and therefore to prevent it from escaping all the local minima? I mean if we make the red point less stochastic, then it's going to find a local minimum that is able to hold the red point? Note: increasing, not decreasing, the batch size makes SGD less stochastic. In classic optimization problems, normal gradient descent makes use of a line search to avoid jumping out of minimums. However, in the context of neural networks, where millions of parameters are optimized, this is not emphasized as much. Have a look at section 9.2 of Boyd's Convex Optimization book for details. Sections 9.3 - 9.5 are helpful too. Nevertheless, even line search will struggle, since the cost function keeps changing with every batch. Really, the biggest solution is to try to line up the minima of the cost functions for each batch. Question 3: We believe that if the red point is stuck in a local minimum, then by using SGD, we can push it to jump out of this minimum for a chance at arriving at a better minimum, right? But how do we know that it won't escape from that better minimum? We just hope that there is a local minimum that will reduce enough the error, and therefore the gradient of the majority of batches so that the red point is stuck in such a minimum? I mean: In what kind of minima do we expect the red point to get stuck? And why do we think that these minima are better than one you can get with BGD? There really is no guarantee that the red point will stay in place over different batches, unless the cost function looks very similar for each batch. It could end up at a better or worse place.
