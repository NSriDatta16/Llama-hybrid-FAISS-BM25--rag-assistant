[site]: datascience
[post_id]: 17926
[parent_id]: 11912
[tags]: 
Method of regularization For the following 4 techniques, L1 Regularization and L2 Regularization are needless to say that they must be a method of regularization. They shrink the weight. L1 would concentrate on shrinking a smaller amount of weight if the weights have higher importance. Dropout prevents overfitting by temporarily dropping out neurons. Eventually, it calculates all weights as an average so that the weight won't be too large for a particular neuron and hence it is a method of regularization. Batch Normalization should not be a method of regularization because the main purpose of it is to speed up the training by selecting a batch and forcing the weight to be distributed near 0, not too large, not too small. Choosing it For me, mini-batch is a must because it can speed up the process and improve the performance of network every time. L1 and L2 are both similar and I would prefer L1 in small network. Ideally, dropout should apply if there is a large variation problem or overfitting. Last but not the least, I agree with Neil Slater that it depends on the situation and there will never be an optimum solution. I recommend you to read this for further information. This is a very good material. http://neuralnetworksanddeeplearning.com/chap3.html
