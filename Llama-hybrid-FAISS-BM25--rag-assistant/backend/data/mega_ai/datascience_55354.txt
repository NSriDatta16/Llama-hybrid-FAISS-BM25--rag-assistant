[site]: datascience
[post_id]: 55354
[parent_id]: 55352
[tags]: 
Pooling layers does not have parameters which affect the back propagation. Backpropagation is an algorithm to efficiently implement gradient descent in a neural network by using the chain rule and propagating the error from the final layers to the early ones. The parameters that affect backprop are often called trainable variables (in Tensorflow), and they are the weights and biases of the convolutional filters. Adding pooling layers does not change the number of them. Pooling layers are a way of performing downsampling, and they are used for the following main reasons: To decrease the computational load of the network: smaller outputs means less FLOPS To save memory during training: less memory allocation in the GPUs To increase the receptive field of the neurons in successive layers. You can check this link about this topic: https://medium.com/@santi.pdp/receptive-fields-in-convolutional-neural-networks-6368a699d838 Adding transalation-invariance to the feature maps: when pooling, small translations of a feature will result in this feature located in the same position in the following pooled layer What is true is that pooling layers may have a great impact in the learning capabilities of a network
