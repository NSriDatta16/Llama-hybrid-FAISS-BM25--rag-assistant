[site]: crossvalidated
[post_id]: 373436
[parent_id]: 371082
[tags]: 
Yes, sure. To handle matrix-valued inputs $X_i$ , you just need some appropriate kernel on matrices; the right choice will depend on your problem, but a simple one would be e.g. $k(X_i, X_j) = \exp\left( - \frac{1}{2 \sigma^2} \lVert X_i - X_j \rVert_F^2 \right)$ . (This effectively throws away the matrix structure of the $X_i$ – it's equivalent to "flattening" them into vectors and using a Gaussian kernel on vectors. Unfortunately just plugging in a different norm than the Frobenius is not necessarily valid; the norm must be Hilbertian, i.e. correspond to the metric of some Hilbert space, for that to always work.) For vector-valued outputs, you can extend the algorithm to operate with an output kernel as well. The paper Álvarez, Rosasco, and Lawrence (2011). Kernels for Vector-Valued Functions: a Review . gives an overview, and the landmark theoretical reference is Caponnetto and de Vito (2007). Optimal Rates for the Regularized Least-Squares Algorithm . Foundations of Computational Mathematics, July 2007, Volume 7, Issue 3, pp 331–368. The simplest choice corresponds to running a separate regression for each coordinate of $y$ , but choosing an output kernel allows you to learn correlations in the output space as well.
