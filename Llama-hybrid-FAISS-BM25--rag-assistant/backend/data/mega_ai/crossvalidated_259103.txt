[site]: crossvalidated
[post_id]: 259103
[parent_id]: 
[tags]: 
Neural networks, general inquiry - how mini-batching fits into the big picture?

I am graduate student in math, it is my first look at the Deep learning. One of the things that strike me when looking at this material is what seems to be a confusion between implementation details and the big picture or the big principles. This confusion starts right away with a simple feed-forward network "architecture". Specifically, I now struggle to understand "the proper" context of "mini-batching". There are many online sources on neural networks, but so far I was not able to find, what to me seems, a satisfactory presentation of the material. Here is the list of questions that I believe will help me to see the mini-batching in it's "natural context". I am afraid that the questions might be too vague. But I have no other access to an expert in the field, am looking for comments that can point me to my incorrect understanding of the big picture, or incorrect break up of neural network to its "modules" and how mini-batching fits in those "modules". Edited later : Additionally, I believe that someone who has coded or familiar with neural network framework probably has a conceptual view of what are the main moving parts and what is the best way to describe their proper mutual relationships. How is mini-batching represented by a computational graph? By which I mean that given the "original" computational graph what changes should I make to this graph so that it represents the new computational graph associated with the mini batching? Here is what I try to picture. Suppose we have a general framework of "computation graph". A framework that supports evaluation of gradients for any function represented as a graph (which is used to encode the neural network and then evaluate gradient for the training phase.) Given such a graph e.g which reads: I want to do mini-batching on x . Thinking of x as the training example, and of the value of s (output of the graph) as the loss associated with the training example. w is a parameter that I want to learn by running an optimization algorithm of my choice. So to keep things simple, I want my batch to be of size 2. Is this "new" graph below defines the mini-batching? Here the two training examples are represented by x1 and x2 and I have an additional node in the graph that adds all losses s1 and s2 together (and perhaps normalized i.e. divided by the batch size). If the above graph is indeed the mini-batching, what I do not understand is when exactly the efficiency people associate with the mini-batching can be exploited? . Specifically, is it true that the biggest contribution is when we have an appropriate hardware that can parallelly process each of the sub-graphs (which on the last graph would be the blue and the black sub-graphs), if we do not have such a hardware is there any computational benefit to mini-batching? Why people "couple" mini-batching with stochastic gradient descent averaging or (any other optimization algorithms averaging)? Specifically, given any computational graph, why should optimization algorithm care how the graph was constructed? It seems to me that the way the loss function of the mini-batching is defined (as sum of losses for each sub-graph) gives absolute decoupling between mini-batching and optimization algorithm. If there is benefit for "averaging" over iterations of the optimization algorithms why should this averaging be necessarily coupled with the mini-batch size? Somewhat related to the previous question why people "couple" mini-batching with gradient evaluation? Specifically, If I have a general environment that can evaluate gradients (Jacobians), why does the environment's implementation "cares" which graph it is being fed? I can use general environment and feed it with "the new graph" and ask the environment to find the gradient with respect to w , and feed it to my optimization step. Why is there so much emphasis on the indexing used in the implementation of mini-batching?. Specifically, I encounter relatively long explanations on how one can "introduce" additional index to parameters in a feed-forward network, and use it to implement mini-batching. I again do not understand why this coupling is necessary, wouldn't any reasonable way to pass from an original computational graph (the black graph) to a new graph (combined blue+black graph) do the job? Any comments references to literature are welcome! Thanks
