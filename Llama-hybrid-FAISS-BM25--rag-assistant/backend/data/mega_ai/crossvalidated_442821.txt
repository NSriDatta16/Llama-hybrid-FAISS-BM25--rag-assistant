[site]: crossvalidated
[post_id]: 442821
[parent_id]: 442513
[tags]: 
Deep dense (fully connected) neural network with only linear activation functions reduce to single-layer network, as explained in the following answer on StackOverflow . Using non-linear activation functions is generally good idea, because it lets network to use more complicated features and be more efficient (both in terms of performance and convergence time). More advanced network structures like convolutional, or recurrent networks, enable us to process data in more flexible way, so using such layers even without activation functions would work more efficiently for some data, then simple dense network. Recently there were many papers shaking some of the commonly held beliefs about neural networks. It can be the case, that for some problem, some atypical network structure could work. Such "crazy" ideas worked well in the past. Maybe your data is simple enough, that you don't need to add further complications to network structure? Still, if obtaining such results, you should check if you need to use deep network, as this could suggest that you do not need many layers, but maybe just more neurons and single hidden layer. In every case when getting results that seem strange, double check your code for potential bugs, maybe ask someone for code review. You can check the What should I do when my neural network doesn't learn? thread for some hints on debugging neural networks code. Try different regularization, learning rate, optimizer, etc.
