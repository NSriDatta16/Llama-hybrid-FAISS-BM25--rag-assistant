[site]: crossvalidated
[post_id]: 151429
[parent_id]: 
[tags]: 
Using k-means for reducing the size of the training set of a Kernel SVM

I have a classification problem with the following characteristics: a few million data points around one hundred features non-linearly separable Training a SVM with an RBF Kernel is not feasible because of the size of the data set. My idea is the following: reduce the size of the data set to a few thousand points by applying K-Means clustering transform the data set by replacing all the features with the the similarities between the original points and the centroids of the clusters train a linear SVM on the new dataset Does this approach sound reasonable from a mathematical perspective? What other classification algorithms do you recommend for large data sets that are not linearly separable?
