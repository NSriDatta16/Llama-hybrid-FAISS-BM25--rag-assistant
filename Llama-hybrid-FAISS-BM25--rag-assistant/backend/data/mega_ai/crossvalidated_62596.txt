[site]: crossvalidated
[post_id]: 62596
[parent_id]: 62591
[tags]: 
"In the extreme case of k identical predictors, they each get identical coefficients with 1=kth the size that any single one would get if t alone. From a Bayesian point of view, the ridge penalty is ideal if there are many predictors, and all have non-zero coefficients (drawn from a Gaussian distribution). Lasso, on the other hand, is somewhat indifferent to very correlated predictors, and will tend to pick one and ignore the rest. In the extreme case above, the lasso problem breaks down. The Lasso penalty corresponds to a Laplace prior, which expects many coefficients to be close to zero, and a small subset to be larger and nonzero." Page 4, Regularization Paths for Generalized Linear Models via Coordinate Descent, Jerome Friedman, Trevor Hastie, Rob Tibshirani. So, you can leave all of them in--since Ridge guarantees that the (X'X) matrix is invertible---but I wouldn't recommend it.
