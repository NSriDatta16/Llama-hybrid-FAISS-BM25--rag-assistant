[site]: crossvalidated
[post_id]: 335865
[parent_id]: 332967
[tags]: 
Referencing the initial post, for Q1: yes, the error variance is estimated (without bias if the model is correct) by an appropriate adjustment to the sample residuals. The bias is corrected with division by a value smaller than $df=n-1$, based on the parameters in the model. (E.g., this is a question students ask a lot: ¿why do we have to divide by $n-2$ instead of $n-1$ for the variance/standard deviation of the residuals for a bivariate model?) Thus, we have a “reasonable” estimate for the variance for Q2: First, if there were bias, then the entire interval would be shifted by the bias...so adjustment via chi-square bounds probably is not appropriate. Second, to account for the amount of uncertainty, the prediction interval (PI) is slightly elongated. Again, this references back to the smaller df used to obtain the critical value for this margin of error. I hope this answers the query...happy to provide more detail if it seems appropriate. Addendum #1 The standard error for a predicted value from a regression (using bivariate example here) is $$\hat\sigma \sqrt{\frac{1}{n} + \frac{(x_0-\bar{x})^2}{(n-1)s_x^2}}$$ where $\hat\sigma^2$ is the estimated residual variance, $n$ is the sample size, $x_0$ is the value at which you want the prediction, and $s_x^2$ is the variance of the $x$ values. But to clarify, this is the standard error for the predicted value $\hat{y}$ at $x_0$. As mentioned in Kleinbaum et al. (2014), this can be used to calculate the confidence interval for the parameter (which in this case is the average predicted value at $x_0$). To account for the fact that we often have to use an estimate for $\sigma$ ($\hat\sigma$ in this case), to obtain the margin of error, we multiple this by a slightly larger multiplication factor to accommodate that extra element of uncertainty. (In this case, we use a smaller $df=n-2$ which in turn makes $t_{c.v.}$ slightly larger.) When we move to the next level of estimation, and ask for an interval in which we might observe values of $y$ at $x_0$, then we need to account for the variance of the parameter estimate and the variance of the distribution of errors. However, by doing so, we are no longer talking about a confidence interval, as we are talking about a random variable and not a parameter; we call this a prediction interval. Essentially, the variance used in calculating the “margin of error” would be the sum of the variance associated with the parameter estimate and the variance associated with the residuals. This is captured by adding a one under the radical: $$\hat\sigma \sqrt{1 + \frac{1}{n} + \frac{(x_0-\bar{x})^2}{(n-1)s_x^2}}$$ Conceptually, the 1 “adds” the estimate for $\sigma$ to the standard error.
