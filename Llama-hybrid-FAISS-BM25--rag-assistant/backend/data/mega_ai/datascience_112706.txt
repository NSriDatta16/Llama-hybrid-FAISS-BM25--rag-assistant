[site]: datascience
[post_id]: 112706
[parent_id]: 112701
[tags]: 
There is no way to decide for a good threshold without more information about the data. For example, if the independent variables are highly correlated, you probably want to reduce the dimension. However, they might all be rich in information and only weakly correlated. I would not listen to people telling you that the number of predictors should be the square root of your number of observations. A good rule of thumb is: Whatever gives the best out-of-sample predictions is the best model. I am not familiar with PCA in scikit. That being said, the basic idea is that you apply the same normalization steps (remember that PCA assumes that the variables are distributed with mean 0 and variances 1) using the test data coefficients and then define the principle components as linear combination of your original (scaled) variables. When you've transformed both your train and your test data set, you can use any model you like on these new data sets. Using PCA for dimensionality reduction can improve prediction results at the cost of interpretability. In the example of a linear model, column num1 having a coefficient of 5.7 is interpreted easily. However, what does it mean if principle component 1 has a coefficient of 0.32? Again, in the example of a linear model, there are the Akaike and Bayesian information criteria (AIC and BIC) for dimensionality reduction. They keep the original variables as they are, only removing others. See https://en.wikipedia.org/wiki/Akaike_information_criterion .
