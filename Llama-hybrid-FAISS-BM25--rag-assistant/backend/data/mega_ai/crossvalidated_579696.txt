[site]: crossvalidated
[post_id]: 579696
[parent_id]: 
[tags]: 
GARCH model: convergence of the conditional variance to the unconditional variance

Suppose a monthly, stationary time series. The series seems to have some ARCH effects and I model its variance as a GARCH process. I obtain the following output of a GARCH(1,1) model: alpha ( $\alpha$ ): 0,07 beta ( $\beta$ ): 0,7 intercept ( $c$ ): 0,00008 I obtain the unconditional variance of the GARCH(1,1) model by taking the expectation of the GARCH equation: $$ E(V_t) = E(c + \alpha\epsilon^2_t+\beta V_{t-1}) $$ $$ V = c + \alpha V + \beta V $$ $$ V = c/(1-\alpha - \beta) $$ But now I would like to assess how many months would it take to a regular shock (e.g., one standard deviation?) to dissipate or the conditional variance converge into its unconditional variance. How would someone do this?
