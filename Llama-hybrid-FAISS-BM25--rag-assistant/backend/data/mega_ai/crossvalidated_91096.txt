[site]: crossvalidated
[post_id]: 91096
[parent_id]: 90909
[tags]: 
It sounds like you are using steepest descent. It is standard practice to scale the variables by subtracting the mean and dividing by the standard deviation before using the algorithm. You can also use a tuning parameter (learning rate) to control the rate of descent. Usually, the algorithm will have trouble converging on a parameter if the scale of the variable associated with that parameter is too large. You can find information about this in many of the online machine learning courses (The Stanford machine learning course by Andrew Ng on Coursera is a good one- He has a video called Gradient Descent in Practice) or in Elements of Statistical Learning by Hastie and Tibshirani.
