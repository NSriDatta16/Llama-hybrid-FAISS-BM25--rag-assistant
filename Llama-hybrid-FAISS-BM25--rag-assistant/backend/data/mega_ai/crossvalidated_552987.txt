[site]: crossvalidated
[post_id]: 552987
[parent_id]: 
[tags]: 
How to show that the gradient of the smoothed surrogate loss function leads to perceptron update?

This is about the contents of section 1.2.1 and 1.2.1.1 of the book "Neural Networks and Deep Learning: A Textbook". The link to the sections is here . The question arises from the following sentence on page 9-10 underlined in red: I would like to derive the gradient of Equation (1.8) and show that the derivative will lead to the perceptron update. As the sentence says, the "perceptron update" is $\bar W\Leftarrow\bar W-\alpha\nabla_W L_i$ . Another equation of "perceptron update" is Equation (1.4) on page 7. The two equations imply that $(y_i-\hat y_i)\bar X=\nabla_W L_i$ if we add the subscripts for the $i$ -th sample. However, my derivation cannot lead to this equation. My derivation is as follows: From (1.8), I omit the $\max\{\ldots\}$ operator and take partial derivative of $-y_i(\bar W\cdot\bar X)$ with regard to parameter $w_j$ , which produces $-y_ix_j$ . So, the vector $\nabla_W L_i$ should be $(-y_ix_1, -y_ix_2,\ldots, -y_ix_d)^T=-y_i\bar X$ if we stack all $j$ 's. However, it is not $(y_i-\hat y_i)\bar X$ that is the perceptron algorithm in the right hand of Equation (1.4), unless prediction $\hat y_i$ is always 0 which can't be the case. So, my questions are: 1) How to derive the gradient of Equation (1.8) that leads to the perceptron update? 2) What is the predicted value $\hat y$ ? Should it be $\mathrm{sign}\{\bar W\cdot\bar X_i\}$ or just $\bar W\cdot\bar X_i$ ?. 3) Should I drop the $\max\{\ldots\}$ operator when taking derivative? If should, why can we drop it? It's not differentiable at 0 and they are equal only for positive component. I know this is only an encouragement in the book, but I hope I could understand how to derive it. thank you.
