[site]: crossvalidated
[post_id]: 346725
[parent_id]: 346412
[tags]: 
Since you're using the q-learning tag, I'll assume that your question is concerning the $Q$-learning algorithm. Typically we assume in Reinforcement Learning (RL) that an agent's action will have some level of influence on what state we end up with, but that actually still covers your case. Let $P(S, A, S')$ denote the probability that we end up in state $S'$ after executing action $A$ in state $S$. Essentially, what you say is that, given any pair of states $S$ and $S'$: \begin{equation} P(S, A_1, S') = P(S, A_2, S') = \dots = P(S, A_n, S'). \end{equation} This is simply one special case of the more generally considered case, so it's certainly allowed and you'll certainly learn something useful. $Q$-learning may be unnecessarily complex for such a case though. It will work, but simpler algorithms may work equally well/better in such a specific case (specifically; algorithms that, unlike $Q$-learning, exploit knowledge of the specific properties you have described and do not also have to function in more general cases). For example, in the specific case you have described it may be worth considering Contextual Multi-Armed Bandit algorithms , where your current state $S$ is the context, and the actions $A$ are the arms. These algorithms aim to optimize the immediate one-step reward of selecting an action $A$ in a state $S$, whereas an algorithm like $Q$-learning aims to optimize the discounted long-term rewards, of which the immediate one-step reward is only a single component. In your case, where the selected action has no influence on the state you end up in, those two goals are equivalent.
