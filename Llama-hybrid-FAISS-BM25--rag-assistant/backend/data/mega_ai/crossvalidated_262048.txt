[site]: crossvalidated
[post_id]: 262048
[parent_id]: 261939
[tags]: 
In the terminology of recurrent neural networks, you look at sequences which are typically denoted as $$ x_{1:T} = (x_1, x_2, x_3, \dots, x_T). $$ Here, $T$ is the sequence length and a running variable $t$ is the time step. For example, you can write expressions such as $$ x_1 = 2 \\ x_t = 2 x_{t-1}, t \neq 1. $$
