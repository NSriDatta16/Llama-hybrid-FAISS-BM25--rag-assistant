[site]: crossvalidated
[post_id]: 539927
[parent_id]: 539919
[tags]: 
Yep we can absolutely change the underlying algos and mix-and-match. For example if you know that the trend process is random noise around the mean with a switchpoint to a new mean then you probably want to use that for your trend rather than a moving average which will be wonky around the switchpoint. The only thing to be real concerned about is your trend model biasing the seasonality piece or vice versa. That's why certain configurations work way better, for example complex splines before seasonal estimation could smoothen it out too much and give you wonky trend. These ideas are at the core of a dev package I whipped up: Thymeboost. It combines time series decomposition with boosting (thus the name!) but it enables you to mix and match but also apply a learning rate to each individual signal (level/trend, seasonality, exogenous factor). If you use python you can try it out: pip install ThymeBoost And some examples with some randomly generated time_series but I really need to formalize some documentation: from ThymeBoost import ThymeBoost as tb boosted_model = tb.ThymeBoost(trend_estimator = 'linear', split_cost = 'mse', global_cost = 'maicc', fit_type = 'local', seasonal_period = 0, verbose = 1) output = boosted_model.fit(time_series) boosted_model.plot_results() Here 'local' in fit type specifies to look for changepoints, alternatively we can pass 'global' here to just fit on the whole dataset. And to your point we really can use whatever base estimator for trend so lets use arima: from ThymeBoost import ThymeBoost as tb boosted_model = tb.ThymeBoost(trend_estimator = 'ar', arima_order = (2,1,2), split_cost = 'mse', global_cost = 'maicc', fit_type = 'local', seasonal_period = 0, verbose = 1) output = boosted_model.fit(time_series) boosted_model.plot_results() Using verbose the model prints results from each round: Round 0 cost: -1344.0438903501597 Using Split: 101 Round 1 cost: -1971.2578426717146 Using Split: 151 Round 2 cost: -1953.8696120415427 Boosting Terminated Using round 1 So it found a changepoint at 101 where the coefficients of the arima will be different. That is close enough to the true changepoint at 100 if you ask me! Round 0 is always fit with with the median of the data since this approach is heavily influenced by gradient boosting and it has a nice interpretation of saying 'there is no trend'. Although for your case you may just want to fit all your different models globally for the first round. Now let's try with 3 changepoints to see what happens: from ThymeBoost import ThymeBoost as tb boosted_model = tb.ThymeBoost(trend_estimator = 'linear', split_cost = 'mse', global_cost = 'maicc', fit_type = 'local', seasonal_period = 0, verbose = 1) output = boosted_model.fit(time_series) boosted_model.plot_results() Oof, that's pretty bad. Looks like it is a problem with how I propose the segments since I don't want to exhaustively try every data point, so let's increase the number of proposals (and you will see I commented out approximate_splits = True if set to False it will try every point): from ThymeBoost import ThymeBoost as tb boosted_model = tb.ThymeBoost(trend_estimator = 'linear', split_cost = 'mse', global_cost = 'maicc', fit_type = 'local', seasonal_period = 0, #approximate_splits = True, n_split_proposals = 25, verbose = 1) output = boosted_model.fit(time_series) boosted_model.plot_results() Looks a lot better but it is slow to react to the third changepoint at x = 200. You can also call plot_components() to see each component. And finally for seasonality just add seasonality_estimator = 'harmonic' or 'naive' for fourier or simple averaging. In terms of the seasonal_period you can pass like 12 for monthly data or alternatively [12, 3] for multiple seasonalities. The full list of trend algos is: 'linear', 'mean', 'ewm', 'median', 'ar', 'ses'(simple exp smoothing), 'des' (holt smoothing), and 'loess'. And exogenous_estimator has 'ols' and 'decision_tree'. There are also options for polynomial expansions and regularization and the learning rate piece I mentioned earlier but I really need to fully document it but hopefully this can give you some ideas and you can try out some of the mix-and-matching and hopefully it doesn't break! Probably too much of a plug but this question was geared towards my fun little package.
