[site]: crossvalidated
[post_id]: 9017
[parent_id]: 6498
[tags]: 
I will try and respond to the gentle urging of whuber to simply “respond to the question” and stay on topic. We are given 144 monthly readings of a series called “The Airline Series” . Box and Jenkins were widely criticized for providing a forecast that was wildly on the high side due to the “explosive nature” of a reverse logged transformation. Visually we get the impression that the variance of the original series increases with the level of the series suggesting a need for a transformation. However we know that one the requirements for a useful model is that the variance of the “model errors” needs to be homogenous. No assumptions are necessary about the variance of the original series. They are identical if the model is simply a constant i.e. y(t)=u . As https://stats.stackexchange.com/users/2392/probabilityislogic stated so clearly in his response to Advice on explaining heterogeneity / heteroscedasticty “one thing which I always find amusing is this "non-normality of the data" that people worry about. The data does not need to be normally distributed, but the error term does” Early work in time series often erroneously jumped to conclusions about unwarranted transformations. We will discover here that the remedial transformation for this data is to simply add three indicator dummy series to the ARIMA model reflecting an adjustment for three unusual data points. Following is the plot of the autocorrelation function suggesting a strong autocorrelation at lag 12 (.76) and at lag 1 (.948). Autocorrelations are simply regression coefficients in a model where y is the dependent variable being predicted by a lag of y. ! The analysis above suggests that one model the first differences of the series and study that “residual series” which is identical to the first differences first for it’s properties. This analysis reconfirms the idea that a strong seasonal pattern exists in the data that could be remedied or modeled by a model that contained two differencing operators . This simple double differencing yields a set of residual a.k.a an adjusted series or loosely speaking a transformed series that evidences non-constant variance but the reason for the non-constant variance is the non-constant mean of the residuals.Here is a plot of the doubly differenced series , suggesting three anomalies at the end of the series. The Autocorrelation of this series falsely indicates that “all is well” and there might be a need for any Ma(1) adjustment. Care should be taken as there is a suggestion of anomalies in the data thus the acf is biased downwards. This is known as the “Alice in Wonderland Effect” i.e. accepting the null hypothesis of no evidented structure when that structure is being masked by a violation of one of the assumptions. We visually detect three unusual points ( 117,135,136) This step of detecting the outliers is called Intervention Detection and can be easily , or not so easily, programmed following the following the work of Tsay. If we add three indicators to the model, we get We can then estimate And receive a plot of the residuals and the acf This acf suggests that we add potentially two moving average coefficients to the model . Thus the next estimated model might be. Yielding One could then delete the non-significant constant and get a refined model : We note that no power transformations were needed whatsoever to obtain a set of residuals that constant variance. Note that the forecasts are non-explosive. In terms of a simple weighted sum , we have: 13 weights ; 3 non-zero and equal to (1.0.1,0.,-1.0) This material was presented in a way that was non-automatic and consequentially required user interaction in terms of making modeling decisions.
