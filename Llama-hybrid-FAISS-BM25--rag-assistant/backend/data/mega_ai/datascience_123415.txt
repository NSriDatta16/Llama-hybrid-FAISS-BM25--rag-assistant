[site]: datascience
[post_id]: 123415
[parent_id]: 112001
[tags]: 
Not an expert on transformers, but here is my understanding. Firstly, to compare multi-head and single-head attention, they should be of the same dimensionality. Let the input to each self-attention of the multi-head case be $(\tilde Q_i,\tilde K_i,\tilde V_i)$ , and let the input to the self-attention of the single-head case be $(\tilde Q,\tilde K,\tilde V)$ , then the dimension of $\tilde Q$ should be identical to that of $(\tilde Q_1,\dots,\tilde Q_h)$ where $h$ is the number of heads. The same applies to $K$ and $V$ . Define the above notation as: $\tilde Q_i = Q W_i^Q$ . Let $S(\cdot) = \operatorname{softmax}(\cdot / \sigma)$ be the scaled softmax operator (the scaling factor is omitted for convenience below). By multi-head attention formula, the output is the concatenation of $S(\tilde Q_i\tilde K_i^\top)\tilde V_i$ followed by the output projection. This achieves: Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. so each $S(\tilde Q_i\tilde K_i^\top)$ is a way to attend to information. On the other hand, however, by single-head attention, the output is $$ \begin{aligned} S(\tilde Q\tilde K^\top)\tilde V &= S\left(\begin{pmatrix}\tilde Q_1 & \dots & \tilde Q_h\end{pmatrix}\begin{pmatrix}\tilde K_1^\top\\ \vdots \\ \tilde K_h^\top\end{pmatrix}\right)\tilde V\\ &= S\left(\sum_{i=1}^h\tilde Q_i\tilde K_i^\top\right)\tilde V\\ \end{aligned} $$ followed by the output projection. There's only one set of softmax attention weights now; thus the model is able to attend to information in one manner only. The key point is: notice that there's a summation single-head attention formula. But summation is equivalent to averaging up to a constant. Hence, we may sufficiently state that: averaging inhibits this.
