ain adaptation is the formulation of learning algorithms which generalize well when the training and test data have different distributions. Given training examples { ( x i tr , y i tr ) } i = 1 n {\displaystyle \{(x_{i}^{\text{tr}},y_{i}^{\text{tr}})\}_{i=1}^{n}} and a test set { ( x j te , y j te ) } j = 1 m {\displaystyle \{(x_{j}^{\text{te}},y_{j}^{\text{te}})\}_{j=1}^{m}} where the y j te {\displaystyle y_{j}^{\text{te}}} are unknown, three types of differences are commonly assumed between the distribution of the training examples P tr ( X , Y ) {\displaystyle P^{\text{tr}}(X,Y)} and the test distribution P te ( X , Y ) {\displaystyle P^{\text{te}}(X,Y)} : Covariate shift in which the marginal distribution of the covariates changes across domains: P tr ( X ) ≠ P te ( X ) {\displaystyle P^{\text{tr}}(X)\neq P^{\text{te}}(X)} Target shift in which the marginal distribution of the outputs changes across domains: P tr ( Y ) ≠ P te ( Y ) {\displaystyle P^{\text{tr}}(Y)\neq P^{\text{te}}(Y)} Conditional shift in which P ( Y ) {\displaystyle P(Y)} remains the same across domains, but the conditional distributions differ: P tr ( X ∣ Y ) ≠ P te ( X ∣ Y ) {\displaystyle P^{\text{tr}}(X\mid Y)\neq P^{\text{te}}(X\mid Y)} . In general, the presence of conditional shift leads to an ill-posed problem, and the additional assumption that P ( X ∣ Y ) {\displaystyle P(X\mid Y)} changes only under location-scale (LS) transformations on X {\displaystyle X} is commonly imposed to make the problem tractable. By utilizing the kernel embedding of marginal and conditional distributions, practical approaches to deal with the presence of these types of differences between training and test domains can be formulated. Covariate shift may be accounted for by reweighting examples via estimates of the ratio P te ( X ) / P tr ( X ) {\displaystyle P^{\text{te}}(X)/P^{\text{tr}}(X)} obtained directly from the kernel embeddings of the marginal distributions of X {\displaystyle X} in each domain without any need for explicit estimation of the distributions. Target shift, which cannot be similarly dealt with since no samples from Y {\displaystyle Y} are available in the test domain, is accounted for by weighting training examples using the vector β ∗ ( y tr ) {\displaystyle {\boldsymbol {\beta }}^{*}(\mathbf {y} ^{\text{tr}})} which solves the following optimization problem (where in practice, empirical approximations must be used) min β ( y ) ‖ C ( X ∣ Y ) tr E [ β ( y ) φ ( y tr ) ] − μ X te ‖ H 2 {\displaystyle \min _{{\boldsymbol {\beta }}(y)}\left\|{\mathcal {C}}_{{(X\mid Y)}^{\text{tr}}}\mathbb {E} [{\boldsymbol {\beta }}(y)\varphi (y^{\text{tr}})]-\mu _{X^{\text{te}}}\right\|_{\mathcal {H}}^{2}} subject to β ( y ) ≥ 0 , E [ β ( y tr ) ] = 1 {\displaystyle {\boldsymbol {\beta }}(y)\geq 0,\mathbb {E} [{\boldsymbol {\beta }}(y^{\text{tr}})]=1} To deal with location scale conditional shift, one can perform a LS transformation of the training points to obtain new transformed training data X new = X tr ⊙ W + B {\displaystyle \mathbf {X} ^{\text{new}}=\mathbf {X} ^{\text{tr}}\odot \mathbf {W} +\mathbf {B} } (where ⊙ {\displaystyle \odot } denotes the element-wise vector product). To ensure similar distributions between the new transformed training samples and the test data, W , B {\displaystyle \mathbf {W} ,\mathbf {B} } are estimated by minimizing the following empirical kernel embedding distance ‖ μ ^ X new − μ ^ X te ‖ H 2 = ‖ C ^ ( X ∣ Y ) new μ ^ Y tr − μ ^ X te ‖ H 2 {\displaystyle \left\|{\widehat {\mu }}_{X^{\text{new}}}-{\widehat {\mu }}_{X^{\text{te}}}\right\|_{\mathcal {H}}^{2}=\left\|{\widehat {\mathcal {C}}}_{(X\mid Y)^{\text{new}}}{\widehat {\mu }}_{Y^{\text{tr}}}-{\widehat {\mu }}_{X^{\text{te}}}\right\|_{\mathcal {H}}^{2}} In general, the kernel embedding methods for dealing with LS conditional shift and target shift may be combined to find a reweighted transformation of the training data which mimics the test distribution, and these methods m