[site]: crossvalidated
[post_id]: 643270
[parent_id]: 230218
[tags]: 
You lack information about the values of the explanatory variables. Exploring this issue reveals some useful things to know about logistic regression. For instance, you can go far beyond simple rules of thumb concerning how much data you need for estimating the parameters and you can develop a better intuition for what can and cannot be accomplished depending on simple characteristics of the dataset (sizes, locations, and ranges). Let me explain with some examples. These scatterplots show data generated with the coefficients $\beta = (1,1)$ (and the usual logistic link) for various sets of $x$ values. That is, the log odds of the expected response $y$ is $\beta_1 + \beta_2 x = 1 + x.$ ( This is fully general, because every univariate logistic regression has this form when you choose suitable units of measurement for $x.$ ) To appreciate these plots, understand that The datasets are all the same size, with $n = 100$ observations each. The $x$ values are equally spaced across their ranges. The responses ( $y$ values) of $0$ and $1$ have been vertically jittered to avoid overplotting. The gray curves are all parts of the same model with $\beta=(1,1)$ used to generate the data. They show how the chance of a response equal to $1$ varies with $x.$ The posted equations are the fitted logistic models to each dataset in the form " $\hat\beta_0(\operatorname{se}(\hat\beta_0))\ + \hat\beta_1(\operatorname{se}(\hat\beta_1))x:$ " that is, the standard errors of estimate appear in parentheses after each estimated parameter. (The fits were computed with the glm function in R .) Observe how different these datasets appear. They differ in the ranges and locations of the $x$ values: The ranges across the top in examples 1 through 4 are all equal to $2$ but the locations are shifted to right as you go from left to right. The locations across the bottom in examples 5 through 8 are all centered at $x=0$ but the ranges increase from $2$ to $20.$ Consequently, most of these datasets cover small and very different portions of the full logistic sigmoidal curve, as illustrated by the gray curves. That's how they manage to have such different appearances. (But, as you can check by inspection, all coefficient estimates are within a couple of standard errors of the true model coefficients $\beta = (1,1).$ ) A very close examination of the standard errors will reveal a general truth: the standard errors are smallest when most of the logistic curve is included, as in examples 7 and 8, and is largest when the curve varies only a little, as in examples 1 (the left tail), 4 (the right tail), and even 5 (the middle). This implies you can get some hints about the likely $x$ values from the p-values reported in the paper, provided you know how many $x$ values are involved (which is usually the case): caeteris paribus, tiny p-values suggest you're in a situation like examples 6 through 8 while large p-values indicate otherwise. (This gets much more complicated with multiple regression because the p-values depend on the geometric relationships among the various explanatory variables, making it unlikely you could infer much about the $x$ values used in the paper.) Now (returning to the simple logistic regression case with a single explanatory variable), if you can estimate the range of the $x$ values and the number of $x$ values, you can simulate sample data by generating likely values of $x$ and drawing random binomial responses according to the model. This is illustrated in the first block of R code at https://stats.stackexchange.com/a/40609/919 , but to be fully explicit I offer this general-purpose implementation (used to create the plots). # # Generate data according to a specified logistic regression model. # `x` are the explanatory values (which can be a matrix for multiple regression). # `beta` are the coefficients. # When `intercept` is TRUE, beta[1] is taken to be the intercept term. # # Returns a matrix of `n` columns, one for each independent data set. # rlreg As an example of the use of rlreg , here are three independent simulated datasets sharing a common set of $x$ values (leftmost column). set.seed(17) df x y.1 y.2 y.3 1 -4 0 0 0 2 -3 1 0 0 3 -2 0 0 1 4 -1 1 1 1 5 0 1 0 0 6 1 1 0 1 7 2 1 1 1 8 3 1 1 1 9 4 1 1 1
