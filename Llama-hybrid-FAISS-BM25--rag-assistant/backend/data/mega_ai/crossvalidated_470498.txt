[site]: crossvalidated
[post_id]: 470498
[parent_id]: 
[tags]: 
automatic diffentiation (autograd): when the explicit definition of the gradient function is needed?

In Pytorch and similar machine learning software, the Autograd module computes the gradient of a function without needing to explicit declare the derivative of each single function which composes the main function. However, it is possibile to explicit declare the gradient of one (or more) single functions (e.g., in Pytorch is possibile overloading the backward() function). I don't understand when the explicit backward definition is mandatory and if there is a set of rules which have to be respected to be sure that the explicit derivative definition is not needed to compute the gradient correctly.
