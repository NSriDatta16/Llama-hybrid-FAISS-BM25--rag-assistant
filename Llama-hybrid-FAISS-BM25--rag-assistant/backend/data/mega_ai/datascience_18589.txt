[site]: datascience
[post_id]: 18589
[parent_id]: 18583
[tags]: 
Straight from wikipedia : Leaky ReLU s allow a small, non-zero gradient when the unit is not active. Parametric ReLU s take this idea further by making the coefficient of leakage into a parameter that is learned along with the other neural network parameters.
