[site]: crossvalidated
[post_id]: 205569
[parent_id]: 205566
[tags]: 
Taking your example literally, I'd say the approach is problematic from the outset. If the problem is assessing the total burden, then absolute numbers of deaths and people living with AIDS are key variables, but any PCA is likely to be dominated by a small number of countries with large populations. Even if you use correlation-based PCA, as you should when variables are in very different units, you will have some large outliers in there for most conceivable mixes of countries. If the problem is assessing the total burden given population sizes, then the other variables are relevant. It seems unlikely that mixing together different kinds of variables will help either purpose. The biggest question of all is whether it's a good idea at all to seek a single scale in this way. The best that I can do is flag that statistically-minded people have very different views on this, many highly negative. My own view is that PCA of this sort will only be of interest to those capable of understanding and criticising the PCA and doing their own alternative analysis. A fallacy known under many different names, of which one is the fallacy of misplaced concreteness, is confusing a desire for a single measure with a demonstration that such a measure can be reliably and intelligibly identified from data. It's one thing to have a single name (creativity, intelligence, in this case burden) and another thing to have a single quantifiable dimension. Turning to your results, what's most alarming, as you clearly flag, is that the loadings on the first PC don't even have the same sign. If there is one important shared dimension that justifies trying to quantify burden as a single measure, then it minimally requires all those variables to be positively correlated with each other (or for reversals of sign to be obvious consequences of some measures being direct and some inverse, which doesn't seem the case here). Without seeing the data, I can't interpret further, but I'd expect the variation in sign to be a side-effect of mushing together quite different variables that are also skewed in distribution and with outliers. Plotting the data will help you understand why you got the results you did. I don't have suggestions for a different way to collapse to a single score. I've seen too many applications in which such endeavours were not helpful to be positive there.
