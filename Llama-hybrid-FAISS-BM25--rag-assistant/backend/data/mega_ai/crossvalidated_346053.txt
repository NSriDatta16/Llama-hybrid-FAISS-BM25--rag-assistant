[site]: crossvalidated
[post_id]: 346053
[parent_id]: 346034
[tags]: 
Bayesian analysis commonly uses conjugate priors to give a simple closed-form for the model. In Bayesian regression the conjugate priors are a multivariate normal prior for the coefficients, and an inverse-gamma prior for the error variance (equivalently, a gamma prior for the error precision). For the standard regression model $\boldsymbol{Y} = \boldsymbol{x} \boldsymbol{\beta} + \boldsymbol{\varepsilon}$ with $\boldsymbol{\varepsilon} \sim \text{N}(\boldsymbol{0}, \sigma^2 \boldsymbol{I})$, the conjugate prior is: $$\boldsymbol{\beta} | \sigma \sim \text{N}(\boldsymbol{\beta}_0, \sigma^2 \boldsymbol{\Sigma}_0) \quad \quad \quad \sigma^2 \sim \text{Inv-Gamma}(a_0, s_0).$$ The resulting model form can be further simplified by setting $\boldsymbol{\Sigma}_0 \propto (\boldsymbol{x}^{\text{T}} \boldsymbol{x})^{-1}$, which means that the prior variance matrix for the coefficients is proportional to the posterior variance matrix (and hence, we can easily measure the prior strength relative to the strength of the data). The prior is often also set to be highly diffuse, by taking $a_0 \approx 0$, $s_0 \approx 0$ and $|\boldsymbol{\Sigma}_0^{-1}| \approx 0$. The posterior distribution for this model form is well-known and is easy to work with, so it is a common choice of prior in Bayesian analysis, in the absence of specific prior information. Another common alternative to this conjugate prior is the empirical-Bayes approach, which uses Zellner's g-prior for the coefficients. This "prior" distribution derives from an empirical-Bayes procedure and depends on the response data, so it is not strictly a prior belief (i.e., it cannot be formed prior to seeing the response data). Nevertheless, it is also a popular choice as it has certain desirable posterior properties (e.g., it generalises from maximum-likelihood estimation).
