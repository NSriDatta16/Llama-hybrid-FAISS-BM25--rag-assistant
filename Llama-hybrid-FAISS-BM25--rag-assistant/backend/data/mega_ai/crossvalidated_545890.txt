[site]: crossvalidated
[post_id]: 545890
[parent_id]: 
[tags]: 
Beginner: Understanding difference between pmf, conditional pmf and likelihood

I have a point of confusion regarding the three types functions. I have looked at some other posts here and blogs and scripts and YouTube videos. But I still don't get it. Let's look at the coin toss experiment. When the question is: The probability for heads is 0.7 and I toss a coin 7 times, what is the probability of heads coming up 5 times? Then I do this calculation using the Binomial distribution: $B(5|0.7,7) = \binom{7}{5}0.7^50.3^2$ When I use Bayesian inference to determine the probability for heads in a series of coin tosses, I use the Binomial distribution as my model (I'll put k instead of the binomial factor) So, $P(5|7,\theta) = k \theta^5(1-\theta)^2$ My confusion is this: The formula for the likelihood looks to me like the formula in the first case. In the first case, I have an unconditional PMF, right? But when I read about Bayesian inference, the model is always described as a conditional probability (or at least the term "conditioning" is used and then it is pointed out that likelihoods aren't probabilities). But why then is the formula in both cases the same? Or is the first already a conditional probability? It seems to me like the formula for the likelihood is always just the PMF or PDF and the difference is in how it is interpreted. Is that correct? I guess what I am saying is this: I dont see the difference between the mathematical expression for p(x) in examples like the first one, where the task is to calculate a concrete probability, and p(x|\theta) as it is used in the coin toss examples that try to explain bayesian inference. But from my understanding, they are not the same thing, they are not both formulas for PMF. And if so, why is the term "conditional" used in one context? I hope I kinda got my point across. Edited question Thanks Stataphobia for your answer, and for the comments! So, yeah, the fact that the notation is not used consistently does not make it easier for someone like me. Wouldn't it solve the problem if $P(X|\theta)$ was only used for actual conditional probabilities and in all other cases you either leave out the parameters or use a semicolon, like $P(X;\theta$ )? So let me see if I understood what you said about PMF vs. likelihood: Yes, they have the same form (or yes, we do use the PMF/PDF), but they are interpreted differently and the likelihood is not a conditional probability, even though we are conditioning on something (the data). And the PMF may already be considered a conditional probability (so the notation with the | is justified), but often is not, specifically when $\theta$ is not a random variable (which in the frequentist view it never is, as far as I understand). So back to the coin toss example: if I understand correctly, in the frequentist case, we use the formula for the PMF to calculate $P(X)$ and the likelihood $L(\theta|x)$ . In the bayesian case, it is also the formula for the PMF that is used for the likelihood, but here the PMF is considered an already conditional PMF, because $\theta$ is regarded a random variable. So I guess my big mistake is to somehow expect the formula for the PMF and the conditional PMF to look different. But I guess this again comes from the fact that I mistakenly think the likelihood is a conditional probability. I think it just confuses me that the formula for the PMF can simply be used as a conditional PMF, simply by conceptionally regarding one of its parameters as a random variable. Maybe this shows what I mean (am not sure if this scenario even makes sense): Let's say I have a random varible X, distributed binomially and a r.v. Y, distributed normally. Now I want to calculate $P(X|Y) = \frac{P(X,Y)}{P(Y)}$ , not in the context of inference. Here, in the numerator, I am multiplying the binomial PMF with the normal PDF, right? And then dividing by the binomial PMF. `Will the resulting formula in the right look like the PMF of the binomial? But I guess this is different from what happens when calculating the likelihood in the Bayesian inference: There, I am not really calculating $P(X|\theta) = \frac{P(X,\theta)}{P(\theta)}$ . I am using the formula for the PMF and it is just called a conditional PMF. When you look at those beginners exercises like: You throw a dice, whats the probability that it shows an even number? Well, $\frac{3}{6}$ for a fair dice. Whats the probability is shows an even number given it is less than 5? Thats the conditional probability P(is even|less than five), so $\frac{\frac{3}{6}*\frac{4}{6}}{\frac{4}{6}}$ This looks to me like there is a clear distinction between P(X) and P(X|Y), conceptionally and how it is calculated. And I kind of expect this to be reflected in the formulas for the PMF and conditional PMF. And one more question: In your last statement you say, you kind of think of all probabilities as conditioned on something. But we do distinguish between conditional and marginal, right? So does that mean that the marginal can also be somehow viewded as a conditional?
