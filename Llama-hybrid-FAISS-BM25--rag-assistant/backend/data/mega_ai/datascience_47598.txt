[site]: datascience
[post_id]: 47598
[parent_id]: 
[tags]: 
Newton's method optimization for Deep Learning

I'm reading this paper " Deep learning via Hessian-free optimization " by J. Martens, I am having difficulty figure out the following statement: In the standard Newton's method, $q_{\theta}(p)$ is optimized by computing the $N\times N$ matrix $B$ and then solving the system $Bp = âˆ’\nabla f(\theta)$ . (section 3 of the paper) Is there any theorem, or statement anywhere regarding why the above system needs to be solved to optimize the local approximation? I came across another paper that has a reference to J. Martens and has used the same statement.
