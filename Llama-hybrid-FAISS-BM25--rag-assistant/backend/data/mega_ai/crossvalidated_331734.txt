[site]: crossvalidated
[post_id]: 331734
[parent_id]: 331483
[tags]: 
The basic problem you encounter here is that you have a low sample size, and yet you want to take account of a large number of possible interactions; this is not a realistic expectation. Even when you are using Bayesian methodology, there is a negative consequence to using over-identified models. Parameter estimation is possible in such cases, but the estimates are largely determined by the priors, and so they are not robust. There is some literature on over-parameterisation in a Bayesian context, and this might be of assistance (see e.g., reference below). If you are dealing with $k$ binary variables and you want to measure all interactions between them, you will have $2^k - 1$ coefficients representing main effects and all interaction effects (this leaves out the base term). The number of model terms at different levels of interaction is given by the binomial coefficients: $$\begin{matrix} \text{TERMS} & & & \text{NUMBER OF TERMS} \\ \text{Base term} & & & {k \choose 0} = 1 \\ \text{Main effects} & & & {k \choose 1} = k \\ \text{Interaction of 2 variables} & & & {k \choose 2} = k(k-1)/2 \\ \text{Interaction of 3 variables} & & & {k \choose 3} = k(k-1)(k-2)/6 \\ ... \\ \text{Interaction of } k-1 \text{ variables} & & & {k \choose k-1} = k \\ \text{Interaction of } k \text{ variables} & & & {k \choose k} = 1 \\ \text{TOTAL} & & & 2^k \text{ terms} \\ \end{matrix}$$ As you can see, adding higher levels of interaction rapidly increases the number of terms to estimate. Even if you only went up to an interaction of pairs of variables, that already gives you a large number of model terms. For $k = 1000$ questions, taking main effects plus paired interactions gives $499500$ model terms. Attempting to incorporate all these paired interactions in your model would already lead to a case where you lack identifiability. This is just with paired interactions - if you go to higher-order interactions it gets much worse! In view of this issue, I do not think a model with interactions between the questions is going to be useful for the small amount of data you have. With such a small amount of data you would run into identifiability problems, and these would mean that your inferences would be highly sensitive to prior specification. I would suggest that falling back on an assumption of independence between the questions would be reasonable; it is unlikely that this independence assumption actually holds, but with the small amount of data you have, there is no way for you to make a robust estimate of main effects and interactions. [1] Gustafson, P. (2009) What are the limits of posterior distributions arising from non-identified models, and why should we care? Journal of the American Statistical Association 104(488) , pp. 1682-1695.
