[site]: crossvalidated
[post_id]: 124409
[parent_id]: 
[tags]: 
How to handle missing data in a small $n$ large $k$ machine learning scenario?

I have a sample size $N=130$ and $1000$ variables. I am using machine learning techniques (SVM) for analysing the data. Some variables in the dataset have values that are so huge that they must be errors (so, kind of missing values). Because the sample size is relatively small, I am using nested ten-fold cross-validation and a grid search for deciding the hyperparameters. Thus, the computational load is already very heavy. Now, how should I go about handling the missing data? Clearly, I cannot delete any samples. Excluding variables might work, though I would like to still avoid it, since there is typically one to three bad values per variable, if any. I read that multiple imputation is nowadays widely used, but I might not afford the extra computational load. So I am looking for a one-time imputation method, that is considered good enough to be used in scientific research. Overall, the number of bad values is probably less than 0.1% of the dataset. Anyway, the classifiers will surely fail if some values are 100000x larger than is even theoretically possible considering what they represent.
