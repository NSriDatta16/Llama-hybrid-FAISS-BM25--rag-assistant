[site]: crossvalidated
[post_id]: 359804
[parent_id]: 
[tags]: 
Measuring how "changey" a time series is over time

I am currently working with a medical dataset which among other things contains data on usage of certain instruments as well as power levels. I am attempting to monitor how "fidgety" a care provider is with a given instrument by calculating some statistic which scores how often the power level is changed. This is somewhat of a vague request because I do not have the vocabulary to describe exactly what I need, but I will attempt to give some examples. This statistic would measure very low if over 15 minutes the instrument was off for 5, on an arbitrary power for 3 minutes, and then off for the rest of the period. It would be higher if instead of being at one power level, it fluctuated, or if it was turned on and off multiple times. I was thinking the answer may be to sum a moving standard deviation, but I am not sure how to normalize this so that it is a meaningful value over slightly different encounter lengths.
