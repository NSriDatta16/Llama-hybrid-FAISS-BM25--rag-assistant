[site]: crossvalidated
[post_id]: 441852
[parent_id]: 
[tags]: 
Add new words into the vocabulary in transfer learning?

I learned that if we fine-tune a task based on a pre-trained model and the vocab of the new task is relatively small compared to the original pre-trained model we usually fix the embedding layer. But the issue I encountered is that we have some out of vocabulary(OOVs) words in the new task? For example, we are doing transfer learning and the original vocabulary is of size 200000, and the new dataset for fine-tuning has a vocabulary of only 3010 but 10 of which are new words for the original 200000 vocab? Some OOVs are very similar to the other words in the original vocabulary and I just replaced the original ones with the new ones but some are very different from those in the original vocab.
