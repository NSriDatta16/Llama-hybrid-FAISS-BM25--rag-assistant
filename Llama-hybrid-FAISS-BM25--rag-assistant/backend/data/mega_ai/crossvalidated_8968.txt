[site]: crossvalidated
[post_id]: 8968
[parent_id]: 
[tags]: 
How can I improve a simple regression here?

I'm trying to solve the following issue : Let's say that I have three normal random variables a,b,c, non correlated. Let's say that I only have two observations of these, M and N where : M = a + b N = a + c And another variable I'm trying to explain : R = a + b + c + err I can do a simple regression on this data, lm(R ~ M + N + 0) , I get the same loading for both M and N , equal to 0.66 . My final model is 1.33a + 0.66b + 0.66c . I know that this model is the best linear model here, but what could I use to improve it. Are there methods to properly get a, b, c from M and N and do the regression on them ? I've tried PCA but I only get two vectors, not three. Basically, I'd need a method that would project M and N into the three interesting components here. EDIT I realise I haven't been clear here. $M$, $N$, $R$ are three vectors, with a high number of observations. If they had the relations above with three (unknown) factors $a$, $b$, $c$, then a simple regression would give me : $\hat{R} = 0.66 M + 0.66 N = 1.33 a + 0.66 b + 0.66 c$. I am wondering if there is an alternative technique for modelling $R$, that would give a closer result to the actual $R$. Something that would somehow reconstruct the common term $a$ from $M$ and $N$ automatically, and take that into account to finally get : $\hat{R} = 1 a + 1 * (M - a) + 1 * (N - a)$ I hope I'm a bit more clear in my explanations ...
