[site]: crossvalidated
[post_id]: 260158
[parent_id]: 260147
[tags]: 
What Amoeba and Sycorax said. "Loss of data" is generally interpreted as omitting data points (N getting smaller). Presumably you meant without losing information, which is how Amoeba interpreted it. If that's the case, the answer to your question is 'it depends'. Consider two extremes: if all of your features are linear multiples of one another then you could reduce it down to one dimension without losing any information. If your features are already orthogonal and the variance is the same in all directions then PCA won't buy you anything. Consider looking at the eigenvalues of $(X^TX)$. If the eigenvalues drop off sharply then you can possibly drop your dimension further while retaining total variance. If the eigenvalues are roughly equal then PCA won't help you at all.
