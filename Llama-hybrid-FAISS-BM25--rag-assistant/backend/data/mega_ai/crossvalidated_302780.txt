[site]: crossvalidated
[post_id]: 302780
[parent_id]: 
[tags]: 
Higher order delta / taylor series approximation relationship to normal distribution?

For a normally distributed variable X, one can call on the delta method to provide an asymptotically normally distributed variable for a non-linear function of it, g(X). This is based on a linear taylor series approximation and uses Slutsky's theorem to justify the normality. Recently I programmed some routines in R that calculate the mean and variance of g(x) using higher order taylor series approximations. In my usual non-thinking mode, I never thought to consider that it might not be possible to just treat these new "improved" estimates as mean and variance of a normally distributed variable. Can someone please confirm that my suspicion is correct - ie it is unknown what asymptotic distribution is derived given the extra included random moment terms (except for limited case where second order taylor series is computed and g'(x)=0)? It just strikes me as bit of a shame if this is the case- we are left with "better" estimates of the mean and variance of g(x) but no way to compute confidence intervals from them? Given samples are always finite and the applicability of asymptotic theory to any sample result is always in doubt, it may well be that inserting such derived means and variances into a normal distribution may give better coverage than those obtained under standard delta technique? Don't know if anyone knows of any monte carlo simulation work around this issue? Thanks.
