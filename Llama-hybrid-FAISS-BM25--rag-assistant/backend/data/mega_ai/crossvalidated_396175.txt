[site]: crossvalidated
[post_id]: 396175
[parent_id]: 
[tags]: 
Neural Network Weight Update

I posted this on Software Engineering and was told that it might be better here and on AI. I'm currently reading Artificial Intelligence: A Guide to Intelligent Systems by Michael Negnevitsky (3rd edition) and came across this example of weight updates in a neural network. The equation for updating the weight in this network is: wi(p+1) = wi(p) + alpha * xi * e(p) where e(p) is the difference between the expected and calculated error, x is the input parameter, and alpha is the learning rate.The initial weights of the second epoch aren't different from the initial weights of the first. Aren't they supposed to be updated? What's happening between the final weights of an epoch and the initial weights of the next epoch?
