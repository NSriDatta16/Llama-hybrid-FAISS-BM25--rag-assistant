[site]: crossvalidated
[post_id]: 385218
[parent_id]: 
[tags]: 
Classification accuracy heavily depends on random seed

I want to compare different classification methods and evaluate their prediction measures (such as accuracy etc). I first split the data into training and test set. With the training data I then perform 10-fold CV to tune the classification method (SVM, LASSO). After that I calculate the prediction measures with the best model on the test data. Depending on how I split my data set into training and test set (with which random seeds), the performance of the models heavily change and one method suddenly becomes better than another one. I could repeat the calculation of the performance measures by using the entire data set again (and split it into new training and test data) after having tuned the classification method, multiple times with different random seeds. It seems that like this I am underestimating the 'true' error but I am not sure if this is really the case. Are there better solutions to this problem?
