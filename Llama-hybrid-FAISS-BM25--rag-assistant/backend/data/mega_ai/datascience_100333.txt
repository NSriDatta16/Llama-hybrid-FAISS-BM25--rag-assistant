[site]: datascience
[post_id]: 100333
[parent_id]: 
[tags]: 
Softmax Derivative

I've been trying to build a neural network from scratch in python over the last few weeks. I've gotten it working with sigmoid, but trying to implement softmax has been killing me, entirely thanks to the softmax derivative. I just don't understand how it works. Everywhere I look, people are using crazy math equations to explain it and everybody is just saying, "Oh, yes, that gibberish makes perfect sense!" Well, it doesn't make sense to me. Could somebody please explain how the softmax derivative is supposed to work in plain English, with code examples (preferably in python) instead of math examples?
