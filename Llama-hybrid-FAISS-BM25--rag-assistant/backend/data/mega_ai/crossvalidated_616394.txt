[site]: crossvalidated
[post_id]: 616394
[parent_id]: 
[tags]: 
Are normal errors required for OLS with a large sample size?

It's surprisingly difficult to find a clear answer to this online. Nearly every online source claims that error terms must follow a normal distribution for statistical inference but stops short of proving it. There are a million "data science" and "statistics" articles out there that rehash the same assertions and simple arguments but neither show any math nor link to any sources. What concerns me is that I don't see this assumption being used at all in statistical papers, such as "A Practitioner's Guide to Cluster-Robust Inference" by Cameron and Miller ( link ). In fact, reading through this briefly, I feel increasingly convinced that normality is completely unimportant, except possibly for small sample sizes. As far as I can tell, all that's necessary is for the sampling distribution of the estimated parameter to be approximately normal, and this should be true due to the central limit theorem. Once that is known, then the estimate for the parameter's variance can be used for inference, and this only depends on the variance of the error term and not on its distribution's shape. The problem is that I'm not a statistician, and I don't have complete confidence in my reasoning. For example, I don't know for sure that CLT results in the parameter's sampling distribution being approximately normal, and I don't know for sure that tests using p-values computed this way don't reject the null hypothesis too often.
