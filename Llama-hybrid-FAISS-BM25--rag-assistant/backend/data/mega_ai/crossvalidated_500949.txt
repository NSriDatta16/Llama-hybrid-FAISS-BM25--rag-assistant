[site]: crossvalidated
[post_id]: 500949
[parent_id]: 
[tags]: 
Generative story for a variational auto encoder

I am reading a tutorial on variational autoencoder . In any inference problem we make assumptions about the underlying process of data generation. In VAE's, The tutorial states that data is generated based on the equation $$P(X) = \int P(X|z)p(z)dz$$ I would like to clarify how the underlying data is generated. Since $X$ the data is fixed, $P(X)$ is a constant. Repeat multiple times (approximation) Sample $z \sim N(0,I)$ Compute the likelihood of the data whereby $X = \{x_1,x_2,...,x_n\}$ and $P(X|z) = \prod_{i=1}^NN(x_i|f(z;\theta),\sigma^2*I)$ Take average over all sampled values of $z$ to compute data likelihood $P(X)$ . If my understanding is correct, It seems counterintuitive that in one sample of $z$ , it is able to explain the likelihood over all training data in $P(X|Z)$ . Suppose $z=[0.1,0.5]$ for simplicity, and I use MNIST data of digit handwritings of 1-9. Then this particular $z$ value should be able to generate digits from 1-9 in the dataset ? How can this one value generate digits that are different from each other. This is my understanding of the generative modelling. Correct me ifI am wrong.
