[site]: crossvalidated
[post_id]: 342327
[parent_id]: 342135
[tags]: 
We only (need to) scale the predictor variables. We do this to help our machine learning algorithm converge (faster) to a minimum of the loss function. In the case of a (feed-forward) neural network, the parameters which we want to estimate are the weights and "biases". For scaling between $0$ and $1$, we use the following transform for each predictor variable: $$ \tilde{x}_{ij} = \frac{x_{ij} - \min_i(x_{ij})}{\max_i(x_{ij}) - \min_i(x_{ij})}, $$ where the rows are indexed with $i$ and columns with $j$, as is customary. Given the first point above, one sees that one can simply use the found minima and maxima of the training set for the predictors in the test set.
