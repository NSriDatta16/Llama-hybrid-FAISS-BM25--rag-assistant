[site]: crossvalidated
[post_id]: 501589
[parent_id]: 498272
[tags]: 
The MVUE estimator (say $f$ ) achieves the lowest variance out of all estimators such that bias is zero. However, there is no reason why an estimator with non-zero bias (say $f'$ ) could not have a variance much lower than that of the MVUE, such that overall $\text{MSE}(f) > \text{MSE}(f')$ holds. Consider for example the ridge regression estimator $f_\lambda$ with $\lambda \geq 0$ a regularisation parameter. Then $f_0$ is the least squares estimator which we know is a MVUE (Gauss-Markov theorem). However $$\inf_{\lambda} \text{MSE}(f_\lambda) \leq \text{MSE}(f_0),$$ by definition of an infimum, and frequently $f_\lambda$ can indeed achieve lower MSE for $\lambda > 0$ . Relating that to Bayesian estimators, ridge regression is equivalent to Bayesian linear regression with a Gaussian prior over the weights and a Gaussian likelihood.
