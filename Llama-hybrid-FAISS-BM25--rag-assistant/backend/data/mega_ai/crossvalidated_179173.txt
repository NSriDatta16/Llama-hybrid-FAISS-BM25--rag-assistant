[site]: crossvalidated
[post_id]: 179173
[parent_id]: 
[tags]: 
What are common ways to estimate recall on an unlabelled imbalanced dataset?

Estimating recall is usually easy. You should divide the number of your true positives in the number of positives. However, consider a scenario in which the dataset you have is unlabelled. Typically you can label each case for some cost but the dataset is too big to be labelled entirely. Add to that unknown low probability to be positive. If you will want to estimate the recall in the straight forward way you will have to label a set large enough so the number of positives (and true positives) in the set will be large enough for a reliable estimation of the recall. When the probability of being positive is bellow 0.1%, usually the straight forward method is infeasible. I am aware of the following nice paper "Recall Estimation for Rare Topic Retrieval from Large Corpuses". This paper describes a method used at twitter that copes with the same problem. The idea is close in spirit to co-training . Given two independent classifiers, the recall can be estimated using their precision (which requires way less samples). However, the method requires two independent classifiers. In some scenarios it is hard to have such classifiers. What are other good ways to estimate the recall in such a scenario?
