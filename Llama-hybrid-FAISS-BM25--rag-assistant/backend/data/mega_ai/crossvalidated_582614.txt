[site]: crossvalidated
[post_id]: 582614
[parent_id]: 
[tags]: 
Differential Privacy in Machine Learning: Why do we add noise to gradients and not the outputs?

I recently came across the concept of Differential Privacy in Machine Learning / Deep Learning. As far as I understand, usually, noise is added to gradients during the training epochs (also called Stochastic Gradient Descent Differential Privacy,SGD-DP). Assume that we have a standard classification task. What is the intuition behind this? Why do we add noise to gradients and not (simply) to the output of the classification algorithm? What is the advantage of adding noise to the gradients?
