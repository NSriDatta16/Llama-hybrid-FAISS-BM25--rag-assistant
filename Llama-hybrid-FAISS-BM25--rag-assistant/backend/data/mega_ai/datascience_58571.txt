[site]: datascience
[post_id]: 58571
[parent_id]: 58438
[tags]: 
Seems I found some reasonable justifications for this behavior! Class_weight in the LogisticRegression approach is applied to sample_weight, which is used in a few internal functions like >_logistic_loss_and_grad, _logistic_loss, etc.: #Logistic loss is the negative of the log of the logistic function. . out = -np.sum(sample_weight * log_logistic(yz)) + .5 * alpha * np.dot(w, w) NOTE: ---> ^^^^^^^^^^^^^ Likewise, in decision-tree based approaches like RandomForest and XGBoosting, the class_weigh is applied to the gini or entropy function, which impacts both nominator and denominator --> less impact on the purity function! Source code
