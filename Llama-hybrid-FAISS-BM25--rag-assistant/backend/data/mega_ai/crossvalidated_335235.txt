[site]: crossvalidated
[post_id]: 335235
[parent_id]: 
[tags]: 
How to extract vector representation from a comparison neural networks

From what I understand about embeddings in neural networks, the upper layers (fully-connected) from convolutional neural networks can serve as an image encoding (vector representation of an image), and word2vec where the weights of a simple shallow neural net can provide a rich encoding of a word. However, these are mostly single classification(cnn) or trying to maximize the likelihood(word2vec), now suppose the label does not depend on a single input but a pair, for a naive example, a pair input of an image of a square and an image of a triangle is given the label 0, and the pair of a line and a triangle is given the label 1, then after training a model that takes in a pair of images(I guess this can be done just by adding the second image as extra channels to the first image), how do I get a vector representation of the line, triangle and the square? Or in other words, the rock, paper and scissors game, we can’t just classify one of the items to win or lose, they are intransitive. So we can still train a classifier to classify if the first person going to win with rock and the second person in the pair gives a paper, but how do we get some sort of embeddings for each of the rock, paper, and scissors? These examples remind me of Siamese architecture in CNN, but the setup is for classifying if two inputs are same or not, I assume it doesn’t care about the order of the pair (e.g. the two different face are always different no matter the order in the pair, but the Rock Paper Scissors matters in order to who wins), plus the two subnets are sharing the same weights. Any suggestions?
