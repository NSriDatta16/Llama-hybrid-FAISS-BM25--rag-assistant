[site]: datascience
[post_id]: 19831
[parent_id]: 19612
[tags]: 
I agree mostly with what was already said regarding feature engineering and just to provide you with more material this post has a nice analysis regarding different stages in feature engineering, with many links and references to papers and specific challenges. I think it's worth checking out. Also, to some extend you could automate the task of finding "good" compound features, using kernel methods . You could use some of the standard kernel methods implemented in many libraries (e.g. in sklearn ) as feature extractors and feed these higher-level features as input to a random forest model that inherently uses feature-importance while training. Then keep only most-informative of these higher-dimension features, as scored by the trained model, as compound features alongside the raw data.
