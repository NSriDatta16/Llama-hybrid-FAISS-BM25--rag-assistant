[site]: crossvalidated
[post_id]: 460380
[parent_id]: 460375
[tags]: 
First you need to take a step back and decide what question (or questions) your are trying to answer. Next, look at what question(s) the methods you have already tried actually answer. Now, see if they match. This answer may shed some light on some of the issues with doing smaller regressions, then creating a larger regression based on selecting variables from the smaller regressions. This approach (as shown by the simulations in the linked answer) can both leave out important predictors and include unimportant ones. You mention that one of your regressions only had one significant predictor variable, but how did you determine significance? If this is just based on the individual p-values from a multiple regression, then those p-values measure the significance of that predictor after adjusting for all the others. Any of the "non-significant" predictors could still be useful/important when another term (or terms) is dropped from the model. Think of trying to predict a person's weight (dependent variable) using their height in inches, their height in cm, and their waist circumference (in inches). The 2 height variables will be redundant with both in the model and show not significant (while waist will be significant). The "best" model will have one of the height variables and the waist variable, dropping both height variables will give a much poorer fit. If you main goal is prediction, then don't worry about significance. You can use penalized methods (ridge regression, lasso, elasticnet, model averaging, ...) to help find simpler and stabler models, but science rather than p-values should determine which variables to put into the model. If your main goal is to look for causal effects (e.g. does taking a vitamin C supplement reduce the risk of catching a cold after adjusting for season and demographic variables) then that should suggest a single specific test on a specific model (or full-reduced models). Significance of terms in other models are a distraction at best and can lead to incorrect models being compared.
