[site]: crossvalidated
[post_id]: 130903
[parent_id]: 129989
[tags]: 
This is not really a full answer but maybe is easier to read than stuffing into comments Problem Set up: a) So your raw data is of form ---------- Top-level Site | Theme| Page id | #Clicks| #Impressions ---------- and you collect all the pages for that (top-level site,theme) together ( disregarding the number of impressions on each page) b) the data is gathered rather than from a proper experiment - so you suffer from issues of unbalanced design. This is especially severe because you are dealing with website data (eg certain themes/top level sites will have many orders more pages than others ) Problem Question: How to find an estimate for click through rate (CTR) for those combinations of (Top level site , theme) that have few pages - namely by combining an estimate based on theme with an estimate based on top level site in particular dealing with the unbalanced design. Problem Answer: You should use a regression model - in particular logistic regression would be the natural choice to estimate a probability. A logistic regression model written (in R formula notation) as CTR ~ top level site + theme will provide you with just such an estimate. You might be able to get away with a single model of the form CTR ~ top level site + theme + top level site x theme this will use the interactions when there is enough data and the individual dimensions additively if not. I am not sure whether the regression model will have any issues with the unbalanced design - maybe more experienced members can pipe in. Predicting clicks: Estimating the click-through rate for new ads Web-Scale Bayesian Click-Through Rate Prediction for Sponsored Search Advertising in Microsoft’s Bing Search Engine the second paper aims to address more specifically your issue of developing a general model to fill in the gaps where you have missing data points. however a logistic regression model of CTR~ site + theme + site x theme might work OK (ie the model tries to predict CTR with site and theme separately ) Some remarks a standard assumption for click data is that it follows a binomial distribution with parameters (N =number of trials ie impressions, p =theoretical proportion of successes ie clicks ), so make sure you are comfortable with this. even if your data is generated by the same theoretical process (equal p) your estimate (from data) of p will vary - with amount of variation dependent on number of samples ( and value of p) (see eg binomial proportion confidence interval ). You might want to review your different samples of click through rate based on this ( ie for each combination of site/theme eg {0.3,0.25} is the variation in sample ctr just due to statistical variation ( too few impressions). If you really do want to model the CTR as random then the beta distribution is a better choice than the normal distribution (Normal distribution for CTR doesn't take account variability due to different sample sizes, and that ctr must be between zero and 1) you could then create a linear regression model ( or other statistical model) for each parameter (eg mu ~ site + theme +site x theme ) see eg Bid Landscape Forecasting in Online Ad Exchange Marketplace as I hope is clear from the linked papers, this is a pretty standard approach for web advertising companies - so its definitely 'web scale'. Overfitting with 1000 x 1000 dummy variables over fitting is not a problem, precisely because most of the inputs are zero. you control over fitting with say L2 regularisation (which puts a penalty on the coefficients - with the effect that the a coefficient will be nonzero only if it reduces the overall error sufficiently: by playing with this regularisation you can ensure combinations with few impressions are ignored. the high dimensional (but sparse) representation of dummy variables does require you to pass your input to logistic regression in a sparse format ( as a sparse matrix) or as a dictionary - otherwise you run out of memory.
