[site]: datascience
[post_id]: 34300
[parent_id]: 26597
[tags]: 
Very good question, as there doesn't exist an exact answer to this question yet. This is an active field of research. Ultimately, the architecture of your network is related to the dimensionality of your data. Since neural networks are universal approximators, as long as your network is big enough, it has the ability to fit your data. The only way to truly know which architecture works best is to try all of them, and then pick the best one. But of course, with neural networks, it is quite difficult as each model takes quite some time to train. What some people do is first train a model that "too big" on purpose, and then prune it by removing weights that do not contribute much to the network. What if my network is "too big" If your network is too big, it might either overfit or struggle to converge. Intuitively, what happens is that your network is trying to explain your data in a more complicated way than it should. It's like trying to answer a question that could be answered with one sentence with a 10-page essay. It might be hard to structure such a long answer, and there may be a lot of unnecessary facts thrown in. ( see this question ) What if my network is "too small" On the other hand, if your network is too small, it will underfit your data and therefore. It would be like answering with one sentence when you should have written a 10-page essay. As good as your answer might be, you will be missing some of the relevant facts. Estimating the size of the network If you know the dimensionality of your data, you can tell whether your network is big enough. To estimate the dimensionality of your data, you could try computing its rank. This is a core idea in how people are trying to estimate the size of networks. However, it is not as simple. Indeed, if your network needs to be 64-dimensional, do you build a single hidden layer of size 64 or two layers of size 8? Here, I am going to give you some intuition as to what would happen in either case. Going deeper Going deep means adding more hidden layers. What it does is that it allows the network to compute more complex features. In Convolutional Neural Networks, for instance, it has been shown often that the first few layers represent "low-level" features such as edges, and the last layers represent "high-level" features such as faces, body parts etc. You typically need to go deep if your data is very unstructured (like an image) and needs to be processed quite a bit before useful information can be extracted from it. Going wider Going deeper means creating more complex features, and going "wider" simply means creating more of these features. It might be that your problem can be explained by very simple features but there needs to be many of them. Usually, layers are becoming narrower towards the end of the network for the simple reason that complex features carry more information than simple ones, and therefore you don't need as many.
