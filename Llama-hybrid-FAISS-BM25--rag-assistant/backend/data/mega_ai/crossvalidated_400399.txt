[site]: crossvalidated
[post_id]: 400399
[parent_id]: 400391
[tags]: 
Principle components regression (PCR) can be hard to interpret but is a very useful tool for predictive modeling, and it tends to validate well. A real benefit of PCR is that you don't need feature selection, hence a major source of instability vanishes. And you don't need to worry about inverting an unstable matrix because you only use the first $k$ PCs. Because the order of PCs is pre-specified (we examine them in order of variation explained in $X$ , unlike what stepwise regression and other feature 'selection' methods use), we do not need to penalize for model uncertainty or use anything like lasso . We can enter PCs in descending order of variation in $X$ explained and choose the number of components that optimizes AIC. Then you fit the model with those PCs and be done. As with many methods, scaling of predictors is a huge issue. We routinely scale $X$ s by their standard deviations when computing PCs but this can be questioned. More about PC regression is in my Regression Modeling Strategies book and course notes . But your post mentions both "linear regression" and "classifier". These are at odds with each other, and a "classifier" is something that outputs a forced choice (as opposed to a probability estimator such as logistic regression). Please edit the title and text to state what you are really doing. I assume it is prediction with a linear model, and "classifier" has nothing to do with the problem.
