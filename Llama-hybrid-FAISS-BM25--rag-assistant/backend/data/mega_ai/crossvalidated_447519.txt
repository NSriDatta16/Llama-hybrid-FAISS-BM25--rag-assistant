[site]: crossvalidated
[post_id]: 447519
[parent_id]: 
[tags]: 
Why do we compare sample ACF and theoretical ACF in time series analysis?

Theoretical Autocorrelation Function (ACF): For a weakly stationary time series {$r_t$}, the definition of ACF is (from Ruey Tsay's "Analysis of Financial Time Series") $ \rho_l=\frac{Cov(r_t,r_{t-l})}{\sqrt{Var(r_t)Var(r_{t-l})}}=\frac{Cov(r_t,r_{t-l})}{Var(r_t)} $ It calculates the correlation of two random variables: $r_t$ and $r_{t-l}$ sample ACF calculates the correlation of a time series and a lag $l$ of it, it is two different random variables from $r_t$ and $r_{t-l}$ So what is the point of comparing these two different quantities? E.g., we have calculated the theoretical ACF value between $r_1$ and $r_5$ of a time series, it is actually a random process. We want to check if the theoretical calculation is good, so we instantiate the random process numerous times. For each instantiation, we pick out the value of $r_1$ and $r_5$. Finally, we obtain samples of random variable $r_1$ and $r_5$. Then we use the samples to calculate the sample ACF between $r_1$ and $r_5$. This is the correct way I believe to calculate the sample ACF, and the value to compare with the theoretical ACF. In a word, in my opinion, the correlation between a time series and a lag 5 of it, is NOT the correct way of calculating the sample ACF between $r_1$ and $r_5$. And it is meaningless to compare this value with the theoretical ACF between $r_1$ and $r_5$. Where am I wrong?
