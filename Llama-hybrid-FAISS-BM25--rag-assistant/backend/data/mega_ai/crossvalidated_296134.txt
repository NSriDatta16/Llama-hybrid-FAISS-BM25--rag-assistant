[site]: crossvalidated
[post_id]: 296134
[parent_id]: 
[tags]: 
Why in order to get the smallest number of bits we must use the log of probability of occurrence

I was reading this article on Cross-Entropy. When talking about encoding elements of a distribution in order to minimise the number of bits, it says this: It turns out that if you have access to the underlying distribution $y$ , then to use the smallest number of bits on average, you should assign $\log\frac{1}{y_i}$ bits to the $i$-th symbol. Why $\log\frac{1}{y_i}$? How can we prove that this is the minimum?
