[site]: crossvalidated
[post_id]: 281474
[parent_id]: 281449
[tags]: 
From wikipedia: In overfitting, a statistical model describes random error or noise instead of the underlying relationship. Overfitting occurs when a model is excessively complex, such as having too many parameters relative to the number of observations. A model that has been overfit has poor predictive performance, as it overreacts to minor fluctuations in the training data. So basically when training a model on data, you are both fitting noise and structure. The noise comes from sampling error and as a machine learning designer your job is to design the algorithm such that it fits as much of the stucture as posible without getting to much noise, such that the performance degenerates. So looking at it from a marginal perspective say you add one unit of complexity to your model. The marginal performance change is now composed as a bias reduction term from the additional structure you are fitting and variance term from the noise you are fitting. When the marginal variance effect is larger then the marginal bias effect you are overfitting. Standard illustration of bias and variance below. By the way the assymtotic training error of random forest classification is 0 (at least if there are not identical observations with different classes). This is true since on all predictions are present in on average 62 % of the estimators and each of these estimators have the correct prediction. So given enough trees the law of large numbers will assure that the correct class will have at least a score of 0.62 no matter the predictions when the observation is not used to fit the estimator.
