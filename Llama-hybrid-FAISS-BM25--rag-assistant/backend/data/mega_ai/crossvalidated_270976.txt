[site]: crossvalidated
[post_id]: 270976
[parent_id]: 213911
[tags]: 
If I understand your question correctly, then what you are looking for is a way to make `predictions' with a mixture model, that is, to find out what is the mean of an observation according to the model. Let's start with the obvious case of a one component mixture. That is: $$ y_1,...,y_n \sim N(\mu, \Sigma). $$ We can estimate $\mu$ with the sample mean $\hat\mu = \bar{y}$ and then the prediction for any unobserved value is simply $\hat\mu$. Things are a bit more complicated for mixture models. Assume the following model: $$ y_1,...,y_n \sim \sum_{k=1}^{K}\pi_k N(\mu_k, \Sigma), \qquad \sum_{k=1}^{K}\pi_k = 1. $$ Given estimates for the parameters, the expected value for any new observation is : $$ E(y) = \sum_{k=1}^{K}\hat\pi_k \hat\mu_k. $$ Now this solution is obviously unsatisfactory as what we are looking for is a way to estimate the expected value of an observation $y_i$ based on the model and the observed information. For this purpose, let us pose this problem as a Bayesian one. Suppose that $E(y_i) = \nu_i$ where: $$ \nu_i \sim \sum_{k}\pi_k \delta_0(\mu_k), $$ that is, the prior for $\nu_i$ is discrete with probabilities at the estimated mean values. Now, $$ E(\nu_i | y_i) = \sum_{k=1}^{K} \hat\mu_{k} P(\nu_i = \hat\mu_k | y_i) $$ where, $$ P(\nu_i = \hat\mu_k|y_i) = \frac{\varphi(y_i ; \hat\mu_k, \Sigma)\hat\pi_k}{\sum_{l=1}^{K}\varphi(y_i;\hat\mu_l,\Sigma)\hat\pi_l}. $$ That is, the best estimate for the expected value of $y_i$ given $y_i$ is the posterior mean of $\nu_i$.
