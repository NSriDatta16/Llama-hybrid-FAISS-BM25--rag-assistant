[site]: crossvalidated
[post_id]: 220559
[parent_id]: 
[tags]: 
Python Bayesian invgamma.rvs - joint posterior of normal distribution sampling

(My question is inspired by this blog post: The Bayesian analysis of normal distributions with Python . If you read it, you will get a good background on what I am asking.) I am trying to model the joint distribution and according to German et al., you can simulate the variance parameter of the normal prior using an inverse Gamma distribution. Likewise, according to Wikipedia: Perhaps the chief use of the inverse Gamma distribution is in Bayesian statistics, where the distribution arises as the marginal posterior distribution for the unknown variance of a normal distribution if an uninformative prior is used; and as an analytically tractable conjugate prior if an informative prior is required. I want to know how the unknown variance parameter is derived analytically based on the article referenced. This subject matter to anyone who is new to Bayesian statistics is useful for inferring information from prior data (prior distribution) and likelihood (distribution representing probability of the data given parameter). I have been doing this for two weeks and have a decent beginner's understanding. If you don't know anything about conjugate priors it may appear the question is hard. It should be a piece of cake if you understand priors and specifically conjugate priors. References: Gelman et al, Bayesian Data Analysis , Third Edition, Section 3.3. (Please note I am using Inverse Gamma although Gelman uses inverse-chi-squared and you can reparameterize it as inverse Gamma.)
