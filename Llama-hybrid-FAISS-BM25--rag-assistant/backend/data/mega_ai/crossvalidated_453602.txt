[site]: crossvalidated
[post_id]: 453602
[parent_id]: 
[tags]: 
Different/Unexpected test results using xgboost and gridsearchcv with mean square error loss

I'm noticing very different test results for these two sets of codes: import xgboost as xgb from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error from sklearn.model_selection import GridSearchCV X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) dtrain = xgb.DMatrix(X_train, label=y_train) dtest = xgb.DMatrix(X_test, label=y_test) params = {'max_depth':8, 'eta':0.25, 'subsample':0.7, 'tree_method':'exact', 'lambda':1, 'alpha':0, 'gamma':0} num_round = 7000 bst = xgb.train(params, dtrain, num_round) trainpreds = bst.predict(dtrain, ntree_limit=num_round) preds = bst.predict(dtest, ntree_limit=num_round) print(mean_squared_error(y_train, trainpreds)) print(mean_squared_error(y_test, preds)) 0.12218761804574622 0.3807328933988807 comparing the above with gridsearchcv: params = { 'subsample': [0.7], 'max_depth': [8], } bst = xgb.XGBRegressor(learning_rate=0.25, tree_method='exact', n_estimators=7000) grid = GridSearchCV(estimator=bst, param_grid=params, scoring='neg_mean_squared_error', n_jobs=-1, cv=5, return_train_score=True) grid.fit(X, y) print(grid.cv_results_['mean_train_score']) print(grid.cv_results_['mean_test_score']) -0.12729098 -3.23202986 I understand why we see negatives (as it is defined in Gridsearchcv docs). But I cannot understand why the mean test score is -3.23 and the hold out test set is only 0.38. Yes they are not meant to be exact, but such a large difference seems strange to me - especially considering the training score is so similar. Any ideas? Am I going wrong somewhere? I've tried playing with the random_state parameter to see if maybe the hold out test set was unique in some way but to no avail.
