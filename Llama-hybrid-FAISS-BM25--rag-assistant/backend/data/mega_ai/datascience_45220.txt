[site]: datascience
[post_id]: 45220
[parent_id]: 
[tags]: 
Similarity measure before and after dimensionality reduction or clustering

I have a dataset with 500 000 samples, each sample contains 30 features. The values of the features are in the range 0.0 to 1.0 . I want to measure the distance between two points in the dataset. A simple thing to do could be to measure the euclidean distance between the two 30 dimensional points. However, if I perform for e.g PCA with 3 components, and then measure the euclidan distance, should the measurements yield similar results? My idea is that if PCA is performed, knowledge about the whole dataset is passed to the distance function, which might improve it? - Or is this just a simple way of throwing away information? I would hope that I could gain some knowledge from the whole dataset instead of measuring the distance between two points, and ignoring the distribution of the dataset. EDIT: What I am looking for is. Can I take advantage of the whole data set when doing similarity measures between random pairs? E.g just plain euclidian distance between the raw pairs does just calculate the distance between two points. It ignores the relation for the whole dataset, two points might considered to be even more close, when all other points are more far away.
