[site]: crossvalidated
[post_id]: 560010
[parent_id]: 559774
[tags]: 
There are good arguments to sample points corresponding to high uncertainties and good arguments not to. You could think of these points as points lying in not so dense regions in the features' space; since these points are sparse in your training data, each member of the committee (i.e., the set of classifiers/regressors) will predict their labels rather arbitrarily. Hence, there will be a disagreement between them on the labels of these points. Adding these points to your training dataset will help improve the performance of your final machine learning model in poorly-populated regions of the features' space. This is an argument in favour of sampling points corresponding to high uncertainties . However, data points lying in low-populated regions of the features' space can be considered outliers. And in application, you don't care about the behaviour of your machine learning (ML) model on outliers. You care about its behaviour in densely populated regions of the features space (assuming that your ML model in real life will need to make predictions on data points following the same distribution of the data points in your pool. More on this can be found here ). This is an argument against sampling points corresponding to high uncertainties . Your question touches on the heart of active learning research; how to build an optimal active learning algorithm/query strategy. Recent research indicates that QBC-like algorithms are not the best we can do. More optimal strategies can be achieved by sampling points corresponding to high uncertainties while not deviating much from the true distribution of data points in the pool, i.e., while not sampling too many outliers. The following paper provides a more formal discussion.
