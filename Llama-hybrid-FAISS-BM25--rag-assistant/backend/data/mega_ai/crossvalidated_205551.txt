[site]: crossvalidated
[post_id]: 205551
[parent_id]: 
[tags]: 
Q-Learning: Should I give the sink states a reward, or a Q-value?

I'm implementing Q-learning via neural network to learn the game of Othello/Reversi. Currently, a win gives a reward of 1, a lose gives -1. However, I've run into a dilemma. I don't know whether I should implement this value as a reward or as an initial Q-value, or if it even matters. Let me explain: Option 1: Initialize ALL Q-values randomly (or maybe to 0), even in states that are a Win or a Lose. The Win/Lose values will come from the Reward aspect of Q-learning. So a Win/Lose state has no higher/lower initial value than any other state, but once you get there its reward is propagated backwards. Option 2: Initialize all Q-values randomly (or 0) EXCEPT for winning/losing states, which get initialized to the Win or Lose value. There is no reward, the reward is "built in" to the Q value. So any action that leads to a Win state always gives Q value of 1, and -1 for transitioning to a Losing state. Let me give an example for the difference between option 1 and 2. Say you're doing your first game of learning, and you end up in a state where your set of next moves includes a move that wins the game. In Option 1, you don't know this, because you are only looking to maximize the Q-value of the next move, and the Q-value of a winning state is initialized the same way as all the other states. Instead, if you do happen to pick that move, you will get a reward -- but you don't realize you should pick it, so you might not. In Option 2, you pick it for sure, because it has the highest Q-value possible. Why I care: So far I've been doing all Option 1, with poor results. I tried out Option 2, and it seemed to give much better results, but it could have been an anomaly. I tried to devise a proof that determines whether it matters, but couldn't think of one. After thinking about it for a long time I convinced myself that it shouldn't matter. Even if you don't know from the start that a state is a winning state, once you happen upon it you reinforce the fact that it was a good pick. But I can't think of why Option 2 seems to be giving consistently better results, so I figured I'd ask smarter people. My main concern is that Option 2 is much slower, because for every set of next moves I need to check if any of those moves is a winner/loser, which requires an expensive evaluation. In Option 1, I avoid this evaluation.
