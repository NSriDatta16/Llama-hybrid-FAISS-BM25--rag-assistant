[site]: datascience
[post_id]: 49583
[parent_id]: 49573
[tags]: 
Regarding the code You should plot the decision boundary after training is finished, not inside the training loop, parameters are constantly changing there; unless you are tracking the change of decision boundary. x1 ( x2 ) is the first feature and dat1 ( dat2 ) is the second feature for the first (second) class, so the extended feature space x for both classes should be the union of (1, x1, dat1) and (1, x2, dat2) . Decision boundary Assuming that data is $\boldsymbol{x}=(x_1, x_2)$ ( (x, dat) or (plot_x, plot_y) in the code), and parameter is $\boldsymbol{\theta}=(\theta_0, \theta_1,\theta_2)$ ( (theta(1), theta(2), theta(3)) in the code), here is the line that should be drawn as decision boundary: $$x_2 = -\frac{\theta_1}{\theta_2} x_1 - \frac{\theta_0}{\theta_2}$$ which can be drawn as a segment by connecting two points $(0, - \frac{\theta_0}{\theta_2})$ and $(- \frac{\theta_0}{\theta_1}, 0)$ . However, if $\theta_2=0$ , the line would be $x_1=-\frac{\theta_0}{\theta_1}$ . Where this comes from? Decision boundary of Logistic regression is the set of all points $\boldsymbol{x}$ that satisfy $${\Bbb P}(y=1|\boldsymbol{x})={\Bbb P}(y=0|\boldsymbol{x}) = \frac{1}{2}.$$ Given $${\Bbb P}(y=1|\boldsymbol{x})=\frac{1}{1+e^{-\boldsymbol{\theta}^t\boldsymbol{x_+}}}$$ where $\boldsymbol{\theta}=(\theta_0, \theta_1,\cdots,\theta_d)$ , and $\boldsymbol{x}$ is extended to $\boldsymbol{x_+}=(1, x_1, \cdots, x_d)$ for the sake of readability to have $$\boldsymbol{\theta}^t\boldsymbol{x_+}=\theta_0 + \theta_1 x_1+\cdots+\theta_d x_d,$$ decision boundary can be derived as follows $$\begin{align*} &\frac{1}{1+e^{-\boldsymbol{\theta}^t\boldsymbol{x_+}}} = \frac{1}{2} \\ &\Rightarrow \boldsymbol{\theta}^t\boldsymbol{x_+} = 0\\ &\Rightarrow \theta_0 + \theta_1 x_1+\cdots+\theta_d x_d = 0 \end{align*}$$ For two dimensional data $\boldsymbol{x}=(x_1, x_2)$ we have $$\begin{align*} & \theta_0 + \theta_1 x_1+\theta_2 x_2 = 0 \\ & \Rightarrow x_2 = -\frac{\theta_1}{\theta_2} x_1 - \frac{\theta_0}{\theta_2} \end{align*}$$ which is the separation line that should be drawn in $(x_1, x_2)$ plane. Weighted decision boundary If we want to weight the positive class ( $y = 1$ ) more or less using $w$ , here is the general decision boundary: $$w{\Bbb P}(y=1|\boldsymbol{x}) = {\Bbb P}(y=0|\boldsymbol{x}) = \frac{w}{w+1}$$ For example, $w=2$ means point $\boldsymbol{x}$ will be assigned to positive class if ${\Bbb P}(y=1|\boldsymbol{x}) > 0.33$ (or equivalently if ${\Bbb P}(y=0|\boldsymbol{x}) ), which implies favoring the positive class (increasing the true positive rate). Here is the line for this general case: $$\begin{align*} &\frac{1}{1+e^{-\boldsymbol{\theta}^t\boldsymbol{x_+}}} = \frac{1}{w+1} \\ &\Rightarrow e^{-\boldsymbol{\theta}^t\boldsymbol{x_+}} = w\\ &\Rightarrow \boldsymbol{\theta}^t\boldsymbol{x_+} = -\text{ln}w\\ &\Rightarrow \theta_0 + \theta_1 x_1+\cdots+\theta_d x_d = -\text{ln}w \end{align*}$$
