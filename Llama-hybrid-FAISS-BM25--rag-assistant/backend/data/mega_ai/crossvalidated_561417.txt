[site]: crossvalidated
[post_id]: 561417
[parent_id]: 
[tags]: 
Softmax computation in Transformers

In word embedding model word2vec , computation of softmax is expensive process and hence we use many alternative as provided here . Prominently, hierarchical softmax and negative sampling are used in such case. Now, while working with transformer model such as BERT requires us to compute softmax over vocabulary ( you may check here ). This make me think about the following, Q. Why transformers doesn't use softmax approximation? or do they use it?
