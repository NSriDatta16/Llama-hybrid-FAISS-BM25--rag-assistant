[site]: crossvalidated
[post_id]: 514753
[parent_id]: 
[tags]: 
Basis Functions with Learnable Parameters

When we use least squares to fit a linear basis function model: $y_i = \beta_0+\beta_1b_1(x_i) +\beta_2b_2(x_i)+\beta_3b_3(x_i)+...+\beta_kb_k(x_i)+\epsilon_i$ Is it possible to fit a model where the $b_i$ have parameters that we fit? So for example if my original basis function is $b(x)=ln(x)$ , is there any way to instead fit a model where I use a basis function $b(x)=ln(x+c)$ where c is a learnable parameter? Then the model would be: $y_i=\beta_0+\beta_1ln(x_i+c)+\epsilon_i$ . Also is it possible to have basis functions which are compositions of some set of functions where the composition is learned. For example I could have a set $\{g_1(x),g_2(x),g_3(x),...\}$ . Then my basis functions could be arbitrary compositions of the functions in this set. EDIT: I came up with the following neural network: I computed the gradients for the computational graph above and I got the following: $\partial L/\partial c = 2(\hat{y}-y)\beta_1/f$ $\partial L/\partial \beta_0=2(\hat{y}-y)$ $\partial L/\partial \beta_1 = 2(\hat{y}-y)ln(f)$ Then I wrote a short Python program to use the gradients to perform gradient descent. It seems to work quite well. With 50 data points generated from y=ln(x-1) I get the model: $-0.124+1.092*ln(x-0.939)$ which lies almost exactly over the graph of y=ln(x-1). def step(x, c, beta_1, beta_0, y, step_size=0.001): f=x+c import math g=beta_1*math.log(f) y_hat = beta_0 + g dLdc = 2*(y_hat-y)*beta_1/f dLdB0 = 2*(y_hat-y) dLdB1 = 2*(y_hat-y)*math.log(f) return (c-step_size*dLdc, beta_0-dLdB0*step_size, beta_1-step_size*dLdB1) def gradient_descent(xs, ys): c = 1 beta_0 = 1 beta_1 = 1 NUM_EPOCHS=1000 for epoch in range(NUM_EPOCHS): for i in range(len(xs)): c, beta_0, beta_1 = step(xs[i], c, beta_1, beta_0, ys[i]) return (c, beta_0, beta_1) # Generate 100 data points from y = ln(x-1) import math xs = [] ys = [] for i in range(100): x = 1.1+0.1*i xs.append(x) ys.append(math.log(x-1)) # Compute the parameters to use in our model of y=ln(x-1) # Should return c=-0.9999666014854611, B0=-8.636312629709472e-05 B1=1.0000446912552832 # Therefore our model of y=ln(x-1) generated from the 100 data points is: # y = -8.636312629709472e-05+1.0000446912552832*ln(x-0.9999666014854611) # which lies almost exactly over the graph of y=ln(x-1) gradient_descent(xs, ys) ```
