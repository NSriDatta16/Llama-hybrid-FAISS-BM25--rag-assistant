[site]: crossvalidated
[post_id]: 30004
[parent_id]: 29990
[tags]: 
Neither am I a statistician. Therefore I use my expert knowledge about the data to find outliers. I.e. I look for physical/biological/whatever reasons that made some measurements different from the others. In my case that is e.g. cosmic rays messing up part of the measured signal someone entering the lab, switching on the light just the whole spectrum somehow looks different the first measurement series was taken during normal work hours and is an order of magniture more noisy than the 10 pm series Surely you could tell us similar effects. Note that my 3rd point is different from the others: I don't know what happened. This may be the kind of outlier you're asking about. However, without knowing what caused it (and that this cause invalidates the data point) it is difficult to say that it shouldn't appear in the data set. Also: your outlier may be my most interesting sample... Therefore, I often do not speak of outliers, but of suspicious data points. This reminds everyone that they need to be double checked for their meaning. Whether it is good or not to exclude data (who wants to find outliers just for the sake of having them?) depends very much on what the task at hand is and what the "boundary conditions" for that task are. Some examples: you just discovered the new outlierensis Joachimii subspecies ;-) no reason to exclude them. Exclude all others. you want to predict preying times of mites. If it is acceptable to restrict the prediction to certain conditions, you could formulate these and exclude all other samples and say your predictive model deals with this or that situation, though you already know other situations (describe outlier here) do occur. Keep in mind that excluding data with the help of model diagnostics can create a kind of a self-fulfilling prophecy or an overoptimistic bias (i.e. if you claim your method is generally applicable): the more samples you exclude because they don't fit your assumptions, the better are the assumptions met by the remaining samples. But that's only because of exclusion. I currently have a task at hand where I have a bunch of bad measurements (I know the physical reason why I consider the measurement bad), and a few more that somehow "look weird". What I do is that I exclude these samples from trainig of a (predicitve) model, but separately test the model with these so I can say something about the robustness of my model against outliers of those types which I know will occur every once in a while . Thus, the application somehow or other needs to deal with these outliers. Yet another way to look at outliers is asking: "How much do they influence my model?" (Leverage). From this point of view you can measure robustness or stability with respect to weird training samples. Whatever statistical procedure you use, it will either not identify any outliers, or also have false positives. You can characterize an outlier testing procedure like other diagnostic tests: it has a sensitivity and a specificity, and - more important for you - they correspond (via the outlier proportion in your data) to a positive and negative predictive value. In other words, particularly if your data has very few outliers, the probablility that a case identified by the outlier test really is an outlier (i.e. shouldn't be in the data) can be very low. I believe that expert knowledge about the data at hand is usually much better at detecting outliers than statistical tests: the test is just as good as the assumptions behind it. And one-size-fits-all is often not really good for data analysis. At least I frequently deals with a kind of outliers, where experts (about that type of measurement) have no problem identifying the exact part of the signal that is compromised while automated procedures often fail (it is easy to get them detecting that there is a problem, but very difficult to get them finding where the problem begins and where it ends).
