[site]: crossvalidated
[post_id]: 631018
[parent_id]: 
[tags]: 
Logistic Classification using R::caret

My question stems from this one I recently posted (I've fixed the issue raised in the comments, though that wasn't the topic of the question): Strange interpretation of odds ratio from logistic regression To summarize, I have 281 samples of seafood which are either correctly labeled (1) or mislabelled (0) based on seafood source (grocery, restaurant, sushi bar), state (cooked, raw), appearance (modified, plain), form (chopped, chunk, fillet, whole), and colour (light, red). There are 4 additional samples which are missing the response outcome. I am using these for downstream prediction. To be clear, my dataset of complete cases is quite imbalanced: 211 correctly labelled samples $vs$ 70 mislabelled. I carried out logistic regression to compute point estimates and CIs for log odds, odds, and probabilities (on the complete case data of course). Now, I would like to use the model for classification using the caret R package. The default decision boundary on which to classify samples is 50%. If the computed probability for a given sample is below 50%, then the sample is classified as correctly labelled; otherwise (with probability $\geq$ 50%), it is mislabelled. Based on my model at a 50% threshold, none of the 4 held out samples are mislabelled (hence, they are correctly labelled). Now, the goal is to find a better decision boundary that more accurately balances sensitivity and specificity. To do this, I use ROC curves combined with Youden's J statistic: $J = Sensitivity + Specificity - 1$ which corresponds to a threshold of 27.693%. With this cutoff two samples are predicted to be mislabelled. The AUC is 72.470%. The model's accuracy was 76.512%. My question : Note, that I didn't split data in the traditional sense. One could loosely consider the 281 samples as the "training set", and the 4 missing response samples as the "test set". How valid is this approach? This isn't exactly training and testing on the same data (which would result in overfitting). Also the model accuracy is not suspiciously high (say, 90-98%). Further, I don't want to get too deep in the ML side of things, as this is eventually going to an RSS journal). Edit In the comments, @Dave brought up the cost of misclassification, suggesting that a logistic classifier is pointless at this stage. Sure, we can do classification, and predictions could be wrong, but in the end they only estimates contingent on collected data (and to some degree on the preprocessing and ML analysis techniques (e.g., 5-fold, 10-fold or LOO cross validation?) used). I'll try to justify my thinking. Of the two samples predicted to be mislabelled at a threshold of 27.693%, one is a cooked modified light-coloured fillet from a restaurant, whereas the other is a raw plain light-coloured fillet from a sushi bar. These findings are in line with fraudulent behaviours in the supply chain. With an accurate binary classifier, we could potentially target where mislabelling is potentially happening: With fishers at ports? With retails at point of sale? Addendum Here is the glm output (using all 281 samples): Estimate Std. Error z value Pr(>|z|) (Intercept) 2.23492 1.62772 1.373 0.1697 SourceRestaurant 0.89308 0.90106 0.991 0.3216 SourceSushiBar 0.06103 0.39016 0.156 0.8757 StateRaw -1.18760 1.00139 -1.186 0.2356 AppearancePlain 1.84559 0.80120 2.304 0.0212 * FormChunk -3.57802 1.58606 -2.256 0.0241 * FormFillet -3.55851 1.51865 -2.343 0.0191 * FormWhole -1.57447 1.72502 -0.913 0.3614 ColourRed -1.74047 0.41069 -4.238 2.26e-05 *** Here are the probabilities (computed from exponentiating the odds): (Intercept) SourceRestaurant SourceSushiBar StateRaw AppearancePlain 90.334181 70.952561 51.525186 23.368922 86.360824 FormChunk FormFillet FormWhole ColourRed 2.717201 2.769256 17.157955 14.925305
