[site]: stackoverflow
[post_id]: 2598870
[parent_id]: 2455843
[tags]: 
Note: "You blocked all bots because you're missing the critical Allow: / after User-agent: *" is incorrect . By default, the robots.txt will allow all crawling, you generally do not need to specify any "allow" directives. However, the "noindex" robots meta tag would be a reason not to index content the site. Additionally, the robots.txt currently blocks all crawling so that search engines can't tell that the site can be indexed again. If you wish to have the site indexed again, you need to remove the "disallow: /" from the robots.txt file. You can verify that in Google's Webmaster Tools, either by looking up the latest robots.txt file or by using the "Fetch as Googlebot" feature to test crawling of a page on the site.
