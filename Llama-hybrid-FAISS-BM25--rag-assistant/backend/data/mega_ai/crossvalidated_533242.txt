[site]: crossvalidated
[post_id]: 533242
[parent_id]: 173390
[tags]: 
Although the above answers are really great, I would like to explain the difference in a very simple language. Bagging technique that is Bootstrap Aggregation where we build separate decision trees using bootstrapped set of samples and average the resulting predictions. Each individual decision tree are grown deep without any pruning and hence each of them has high variance and low bias but averaging them reduces the overall variance. They result in improved accuracy over prediction with a single tree. Bagging technique suffers from a disadvantage that of any of the predictor is very very strong than the other predictors. Each bagged tree will look similar because most of them will use that strong predictor. Hence the predictions from the bagged trees will be highly correlated. Unfortunately, averaging many highly correlated quantities does not lead to as large of a reduction in variance as averaging many uncorrelated quantities. Random Forest overcome this problem by forcing each split to consider only a subset of the predictors that are random. The main difference between bagging and random forests is the choice of predictor subset size. If a random forest is built using all the predictors, then it is equal to bagging. Boosting works in a similar way, except that the trees are grown sequentially: each tree is grown using information from previously grown trees. Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set.Unlike in bagging, the construction of each tree depends strongly on the trees that have already been grown. Because the growth of a particular tree takes into account the other trees that have already been grown, smaller trees are typically sufficient. These small trees are mostly Stump which have single split.
