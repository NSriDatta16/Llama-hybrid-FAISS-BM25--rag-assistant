[site]: crossvalidated
[post_id]: 145147
[parent_id]: 
[tags]: 
Two equivalent forms of logistic regression

There are two ways to write the objective function (negative log-likelihood) for logistic regression: Let \begin{align} & p=P(y^{(1)}=1\mid x)= \frac{1}{1+e^{-\beta^\mathrm{T}x}}\\ & P(y^{(1)}=0\mid x)= 1-p=\frac{1}{1+e^{\beta^\mathrm{T}x}} \end{align} The negative log-likelihood will be \begin{align} l=&-\sum_i\left[y^{(1)}_i\log p+(1-y^{(1)}_i)\log(1-p)\right]\\ =&\sum_i\left[y^{(1)}_i\beta^{\mathrm{T}}x_i-\log(1+e^{\beta^{\mathrm{T}}x_i})\right] \end{align} Let \begin{align} & p=P(y^{(2)}=1\mid x)= \frac{1}{1+e^{-\beta^\mathrm{T}x}}\\ & P(y^{(2)}=-1\mid x)= 1-p=\frac{1}{1+e^{\beta^\mathrm{T}x}} \end{align} which can be written into a single formula $$ P(y^{(2)}\mid x)=\frac{1}{1+e^{-y^{(2)}\beta^\mathrm{T}x}} $$ so the negative log-likelihood will be $$ l=\sum_i\log(1+e^{-y^{(2)}_i\beta^\mathrm{T}x_i}) $$ I think these two forms are equivalent because how we denote the labels should not matter. Can I algebraically transform one to the other? I tried $y^{(1)}_i=(1+y^{(2)}_i)/2$ in the first formulation but it seems it doesn't do the trick.
