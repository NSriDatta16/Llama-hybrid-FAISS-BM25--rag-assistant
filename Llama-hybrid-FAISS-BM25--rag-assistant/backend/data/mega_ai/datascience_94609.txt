[site]: datascience
[post_id]: 94609
[parent_id]: 
[tags]: 
SGDClassifier - Why do I need to use argmax instead of argmin to find the lowest threshold satisfying given precision?

I am an experienced programmer, but new to Python and data science. I am following Aurelien Gerone's book and I don't understand one thing. I create SGDClassifier and calculate its precision_recall_curve(). Then I am trying to find the lowest threshold to satisfy precision equal to 90%: precisions, recalls, thresholds = precision_recall_curve(y_train, y_scores) threshold_90_precision = thresholds[np.argmax(precisions >= 0.90)] Why on earth I am searching for arg max if I need to find the minimum threshold value? If I try to use argmin I get the wrong value, with precision equal to 0.1. As I understand this: precisions >= 0.90 creates an array with precision scores only above or equal to 0.90, argmax returns an index, at which I find the highest value in the given array (so this should be as far from 90% as possible, but it's not!), then I choose a threshold with returned index. What am I missing?
