[site]: stackoverflow
[post_id]: 5368045
[parent_id]: 
[tags]: 
Crawling with Node.js

Complete Node.js noob, so dont judge me... I have a simple requirement. Crawl a web site, find all the product pages, and save some data from the product pages. Simpler said then done. Looking at Node.js samples, i cant find something similar. There a request scraper: request({uri:'http://www.google.com'}, function (error, response, body) { if (!error && response.statusCode == 200) { var window = jsdom.jsdom(body).createWindow(); jsdom.jQueryify(window, 'path/to/jquery.js', function (window, jquery) { // jQuery is now loaded on the jsdom window created from 'body' jQuery('.someClass').each(function () { /* Your custom logic */ }); }); } }); But i cant figure out how to call it self once it scrapes the root page, or to populate an array or url's that it needs to scrape. Then there's the http agent way: var agent = httpAgent.create('www.google.com', ['finance', 'news', 'images']); agent.addListener('next', function (err, agent) { var window = jsdom.jsdom(agent.body).createWindow(); jsdom.jQueryify(window, 'path/to/jquery.js', function (window, jquery) { // jQuery is now loaded on the jsdom window created from 'agent.body' jquery('.someClass').each(function () { /* Your Custom Logic */ }); agent.next(); }); }); agent.addListener('stop', function (agent) { sys.puts('the agent has stopped'); }); agent.start(); Which takes an array of locations, but then again, once you get it started with an array, you cant add more locations to it to go through all the product pages. And i cant even get Apricot working, for some reason i'm getting an error. So, how do i modify any of the above examples (or anything not listed above) to scrape a site, find all the product pages, find some data in there (the jquery.someclass example should do the trick) and that save that to a db? Thanks!
