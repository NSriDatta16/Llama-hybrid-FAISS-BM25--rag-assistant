[site]: crossvalidated
[post_id]: 288215
[parent_id]: 288150
[tags]: 
Are XGBoost probability outputs based on the number of examples in a terminal leaf? No. XGBoost is a gradient boosted tree , so it's estimating weights $c \in \mathbb{R^M}$ that assigns weight the $M$ leafs. A sample prediction (on the logit scale) is the sum of its leafs' weights. In the binary case, the inverse logistic function of the logit score yields a predicted probability. The XGBoost paper has a helpful description of how it works. Tianqi Chen, Carlos Guestrin, " XGBoost: A Scalable Tree Boosting System. " See also: In XGboost are weights estimated for each sample and then averaged
