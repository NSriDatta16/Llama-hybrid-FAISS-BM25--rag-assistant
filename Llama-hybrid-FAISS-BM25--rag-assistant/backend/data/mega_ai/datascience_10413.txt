[site]: datascience
[post_id]: 10413
[parent_id]: 10407
[tags]: 
You don't necessarily have to train any differently if you are happy with the result. If you aren't, undersample the majority class and/or adjust the class weights in your loss function. The thing you really have to be more careful about is the evaluation of the result because a naive metric like the accuracy would give you false hope since the imbalance allows you to achieve a high accuracy by simply always predicting the majority class or randomly predicting each class according to its probability of occurrence. It is this baseline against which you have to judge your success; like the coefficient of determination in regression, where the baseline is the sample mean. You could use a graphical approach (ROC or precision/recall curves), or report a size-weighted average of the class positive rate instead of the accuracy. Here are some other ideas: Comparison of Evaluation Metrics in Classification Applications with Imbalanced Datasets .
