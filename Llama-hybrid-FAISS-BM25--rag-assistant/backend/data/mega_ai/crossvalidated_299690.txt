[site]: crossvalidated
[post_id]: 299690
[parent_id]: 299688
[tags]: 
Fitting to a weighted sample works the same way for pretty much any statistical/machine learning model: you minimize the sample weighted loss function. In the case of logistic regression, you would minimize the sample weighted log-loss: $$ \sum_i w_i y_i \log(p_i) + w_i (1 - y_i) \log(1 - p_i) $$ Here $w_i$ are the sample weights. That said, there are two points I should make: 1) Logistic regression is not a hard classifier, while classical AdaBoost assumes your weak learners are, so you will have to pick some threshold on the predicted probabilities of your constituent logistic models. 2) You may be better off just using gradient boosting to minimize the log-loss (i.e. gradient boosted logistic regression). If you are not implementing the model for educational purposes, this is definately something you should consider.
