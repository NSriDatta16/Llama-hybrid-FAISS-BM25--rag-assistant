[site]: crossvalidated
[post_id]: 375284
[parent_id]: 84012
[tags]: 
Let me add some very practical remarks despite the age of the question. As I am not a R user, I cannot let code talk, but it should be understandable nevertheless. Normally you should just pick the hyperparameters (here: $\alpha$ ) with the best CV score. Alternatively, you could select the best $k$ models $f_1, ..., f_k$ and form an ensemble $f(x) = \frac{1}{k}\sum_i{f_i(x)}$ by arithmetic averaging the decision function. This, of course, gives you an increase of runtime complexity. Hint: sometimes geometric averaging works better $f(x) = \sqrt[k]{\prod_{i=1}^k{f_i(x)}}$ . I suppose this is because of a smoother resulting decision boundary. One advantage of resampling is that you can inspect the sequence of test scores, which here are the scores of the cv. You should always not only look at the average but at the std deviation (it is not normal distributed, but you act as if). Usually you display this say as 65.5% (Â± 2.57%) for accuracy. This way you can tell whether the "small deviations" are more likely to be by chance or structurally. Better would be even to inspect the complete sequences. If there is always one fold off for some reason, you may want to rethink the way you are doing your split (it hints a faulty experimental design, also: did you shuffle?). In scikit-learn the GridSearchCV stores details about the fold expirements in cv_results_ ( see here ). With regards to the $\alpha$ : the higher it is, the more your elastic net will have the $L_1$ sparsity feature. You can check the weights of the resulting models, the higher the $\alpha$ is, the more will be set to zero. It is a useful trick to remove the attributes with weights set to zero from your pipeline all together (this improves runtime performance dramatically). Another trick is to use the elastic net model for feature selection and then retrain a $L_2$ variant. Usually this leads to a dramatic model performance boost as intercorrelations between the features have been filtered out.
