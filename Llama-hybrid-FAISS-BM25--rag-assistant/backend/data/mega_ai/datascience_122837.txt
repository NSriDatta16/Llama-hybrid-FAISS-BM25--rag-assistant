[site]: datascience
[post_id]: 122837
[parent_id]: 
[tags]: 
Backpropagation and Gradient Descent: Questions on math behind it

I watched this video which goes over backpropagation calculus and read the Wikipedia page on it. This is my understanding of the equations for the algorithm. I have questions regarding the equations and backpropagation inline in bold font with ? . Definitions: $C_t$ is the cost or loss function for example $t$ , $\mathbf x$ is the input vector of features, $\mathbf y$ is the target output, $L$ is the number of layers, $a$ is the activation function, $z$ is the raw activation, $w_{jk}^{(l)}$ is the weights between layer $l - 1$ and $l$ , where $k$ is the index of the neuron in layer $l - 1$ , $j$ is the index of the neuron in the layer $l$ . $\sigma$ is the sigmoid function. $a_j^l$ is activation of $j$ -th node in layer $l$ . $C_t(\mathbf y, \mathbf a^L) = \langle (\textbf a^L - \textbf y)^2, \textbf 1 \rangle$ . The overall network for one input/output pair can be described as: $$C_t(\mathbf y, g(\mathbf x)), g(\mathbf x) := a^L(W^L a^{L-1}(W^{L-1} \cdots a^1(W^1 \mathbf x)\cdots)), $$ Model training phase: $\mathbf x$ and $\mathbf y$ are fixed, while $w_{jk}^l$ vary. Model evaluation phase: $w_{jk}^l$ and $\mathbf y$ are fixed, while $\mathbf x$ varies. The following equations are from the video. $$a_j^{(0)} = \mathbf x$$ $$C_t = \sum_{j=0}^{n_L - 1} (a_j^{(L)} - y_j)^2 = \langle (\textbf a^{(L)} - \textbf y)^2, \textbf 1 \rangle$$ $$a_j^{(L)} = \sigma(z_j^{(L)}), z_j^{(L)} = \sum_{k=0}^{n_L - 1} (w_{jk}^{(L)} a_k^{(L-1)}) + b_j^{(L)}$$ $$\frac {\partial C_t} {\partial w_{jk}^{(L)}} = \frac {\partial z_j^{(L)}} {\partial w_{jk}^{(L)}} \frac {\partial a_j^{(L)}} {\partial z_j^{(L)}} \frac {\partial C_t} {\partial a_j^{(L)}} = a_k^{(L-1)} \sigma '(z_j^{(L)}) 2(a_j^{(L)} - y_j)$$ $$\frac {\partial z_j^{(L)}} {\partial w_{jk}^{(L)}} = a_k^{(L-1)}, \frac {\partial a_j^{(L)}} {\partial z_j^{(L)}} = \sigma '(z_j^{(L)}), \frac {\partial C_t} {\partial a_j^{(L)}} = 2(a_j^{(L)} - y_j)$$ $$\frac {\partial C_t} {\partial b_{j}^{(L)}} = \frac {\partial z_j^{(L)}} {\partial b_j^{(L)}} \frac {\partial a_j^{(L)}} {\partial z_j^{(L)}} \frac {\partial C_t} {\partial a_j^{(L)}} = 1 \sigma '(z_j^{(L)}) 2(a_j^{(L)} - y_j)$$ $$\frac {\partial C_t} {\partial a_k^{(L-1)}} = \sum_{j=0}^{n_L - 1} \frac {\partial z_j^{(L)}} {\partial a_k^{(L-1)}} \frac {\partial a_j^{(L)}} {\partial z_j^{(L)}} \frac {\partial C_t} {\partial a_j^{(L)}} = \sum_{j=0}^{n_L - 1} w_{jk}^{(L)} \sigma '(z_j^{(L)}) 2(a_j^{(L)} - y_j)$$ $$\frac {\partial z_j^{L}} {\partial a_j^{(L-1)}} = w_{jk}^{(L)}$$ Average over all training examples: $$ \frac {\partial C} {\partial w_{jk}^{(L)}} = \frac {1} {n} \sum_{t=0}^{n-1} \frac {\partial C_t} {\partial w_{jk}^{(L)}}$$ $$\frac {\partial C}{\partial b_j^{(L)}} = \frac {1} {n} \sum_{t=0}^{n-1} \frac {\partial C_t} {\partial b_{j}^{(L)}}$$ QUESTION: I tried to put this together from the video. Is this appropriate notation for definition the gradient of the cost function given all weights and biases? $$C \colon \mathbb R^n \to \mathbb R, \nabla C \colon \mathbb R^n \to \mathbb R^n$$ $$\nabla C = \begin{bmatrix} \frac {\partial C} {\partial w_{00}^{(1)}} \cr \frac {\partial C} {\partial b_{0}^{(1)}} \cr \vdots \cr \frac {\partial C} {\partial w_{MN}^{(L)}} \cr \frac {\partial C} {\partial b_{M}^{(L)}} \cr \end{bmatrix} $$ The gradient descent algorithm: $$\mathbf p_{n+1}=\mathbf p_n - \gamma \nabla C(\mathbf p_n), n \ge 0$$ $$\gamma \in \mathbb R_{+}$$ If the following is the directional derivative, then $\min_{\vec x=\vec 1} \nabla_{\vec x} C(\mathbf {\vec p})$ is a goal, which would minimize the total cost. Where: $$a^{(L)} = \sigma(z^{(L)}), z^{(L)} = W^{(L)} a^{(L-1)} + b^{(L)}, a^{(0)} = \mathbf x$$ $$W^{(L)} = \begin{bmatrix} w_{0,0}^{(L)} & w_{0,1}^{(L)} & \dots & w_{0,n}^{(L)} \cr w_{1,0}^{(L)} & w_{1,1}^{(L)} & \dots & w_{1,n}^{(L)} \cr \vdots & \vdots & \ddots & \vdots \cr w_{k,0}^{(L)} & w_{k,1}^{(L)} & \dots & w_{k,n}^{(L)} \end{bmatrix} , a^{(L)} = \begin{bmatrix} a_0^{(L)} \cr a_1^{(L)} \cr \vdots \cr a_n^{(L)} \end{bmatrix} , b^{(L)} = \begin{bmatrix} b_0^{(L)} \cr b_1^{(L)} \cr \vdots \cr b_n^{(L)} \end{bmatrix} $$ BACKPROPAGATION From Wikipedia, I inferred the following: Computing the above $\frac {\partial C_x} {\partial w_{jk}^{(L)}}$ and $\frac {\partial C_x} {\partial b_{j}^{(L)}}$ using the chain rule is inefficient. Backpropagation computes $\nabla C$ more efficiently by computing the gradient of weighted input $\nabla_{W^l} a^{(l)} = \delta^l$ from $l = L$ (back/output) to $l = 0$ (front/input), which avoids duplicate calculations of both prior pd equations. It also avoids unnecessary computation of $\frac {\partial z^l_j}{\partial w^l_{jk}}$ . $$\nabla_{W^l} a^{(l)} = \delta^l = (a^l)' \circ (W^{l+1})^T\cdot(a^{l+1})' \circ \cdots \circ (W^{L-1})^T \cdot (a^{L-1})' \circ (W^L)^T \cdot (a^L)' \circ \nabla_{a^L} C$$ $$\nabla_{W^l} C = \delta^l(a^{l-1})^T$$ $$\delta^{l-1} := (a^{l-1})' \circ (W^l)^T \cdot \delta^l$$ Where: $$a^{(L)} = \sigma(z^{(L)}), z^{(L)} = (w^{(L)} a^{(L-1)}) + b^{(L)}$$ Total derivative given by: $$df_a = \sum_{i=1}^n \frac{\partial f}{\partial x_i}(a) \cdot dx_i$$ $$\frac{d C}{\partial x}= \frac{d C}{d a^L}\cdot \frac{d a^L}{d z^L} \cdot \frac{d z^L}{d a^{L-1}} \cdot \frac{d a^{L-1}}{d z^{L-1}}\cdot \frac{d z^{L-1}}{d a^{L-2}} \cdot \ldots \cdot \frac{d a^1}{d z^1} \cdot \frac{\partial z^1}{\partial x}$$ or equivalently expanded in functional form: $$ \nabla_x C = (W^1)^T \cdot (a^1)' \circ \ldots \circ (W^{L-1})^T \cdot (a^{L-1})' \circ (W^L)^T \cdot (a^L)' \circ \nabla_{a^L} C $$ Where $\nabla_{a^L} C$ and $\nabla_x C$ are the partial derivatives of $C$ with respect to $a^L$ and $x$ . For comparison if we computed $\nabla_{w^L} C$ forward the steps would be the following with repeated computations: $$ \begin{align} \delta^1 &= (f^1)' \circ (W^2)^T \cdot (f^2)' \circ \cdots \circ (W^{L-1})^T \cdot (f^{L-1})' \circ (W^L)^T \cdot (f^L)' \circ \nabla_{a^L} C\\ \delta^2 &= (f^2)' \circ \cdots \circ (W^{L-1})^T \cdot (f^{L-1})' \circ (W^L)^T \cdot (f^L)' \circ \nabla_{a^L} C\\ &\vdots\\ \delta^{L-1} &= (f^{L-1})' \circ (W^L)^T \cdot (f^L)' \circ \nabla_{a^L} C\\ \delta^L &= (f^L)' \circ \nabla_{a^L} C, \end{align} $$ Computing these values in reverse, avoids the repeated computations.
