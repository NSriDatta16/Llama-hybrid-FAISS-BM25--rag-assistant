[site]: datascience
[post_id]: 93492
[parent_id]: 87898
[tags]: 
I'm not sure there is a need for aggregation, or in other words you may have a pipeline mismatch. BERT sentencepiece tokenization is specifically meant to be passed to some set downstream pipelines, with the aim of the sentencepiece thing being to be able to cater to OOV words. By aggregating the sentencepiece tokens, you might be doing away with the benefit of being able to cater to OOV in your later pipeline. If you are looking for whole word vector tokens, and want to work with OOV words, I would recommend looking at FastText instead. This algorithm more or less uses subwords, and it will also build tokens for OOV words by pretty much aggregating the subword information for that OOV word during a custom training step. The benefit here is that the aggregation step need not be part of your pipeline, and you can use these new vectors in any downstream task (except, of course, the pipelines that accept BERT sentencepiece tokens)
