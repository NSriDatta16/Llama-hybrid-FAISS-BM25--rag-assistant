[site]: crossvalidated
[post_id]: 393347
[parent_id]: 
[tags]: 
Huge difference between training/testing accuracies

I'm working in a kind of a sentiment classification (binary) task. Using google's pre-trained word2vec vectors for the embedding layer (tried other word vectors as well) and 2 Convolutional layers for the model (both appended by MaxPooling and Dropout layers). On top of that, there's a fully-connected (Dense) layer and the network ends with an output layer. Will attach a diagram of my model: The model is performing very well in the training set (600,000 articles with a 50/50 distribution of labels) with an accuracy of ~93%, but performing really poor on the testing set (150,000 articles with a 50/50 distribution of labels) - accuracy: ~58%. This seems like an overfitting issue, however I've appended Dropout layers after each of my regular layers. Another interesting fact: I combined my training and testing sets together (appended the testing set to the training set), shuffled them randomly, and then re-splitted them to training/testing in a 80-20 ratio (using sklearn's train_test_split method), and the accuracy is much more balanced after training the model with these "artificial" datasets: they're both around 93%. Also, while investigating this issue, reduced my vocabulary to only 500 and 1000 most common words (in order to make the model more general) and trained my model with these, but the accuracies didn't change. Anyone has any idea what the issue might be with such a difference in the training/testing accuracies?
