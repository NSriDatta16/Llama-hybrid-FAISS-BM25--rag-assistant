[site]: crossvalidated
[post_id]: 410765
[parent_id]: 
[tags]: 
Possible for batch size of neural network to be too small?

We know that when a batch size is too large, the model might not be able to converge. But what is the drawback of having a batch size be too small (say batch_size = 1) other than taking a long time to train? A batch size of 32 is commonly used and referred to as "small," but why don't we use a smaller batch size of 1 to guarantee convergence?
