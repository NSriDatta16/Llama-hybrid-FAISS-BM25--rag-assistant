[site]: crossvalidated
[post_id]: 487314
[parent_id]: 
[tags]: 
Why taking an average makes convergence to zero faster?

Let $f(x,y)$ be some density, and let the leave-one-out Nadaraya-Watson estimator $\widehat{f}_{-i}(x,y)$ be defined as follows: $\widehat{f}_{-i}(x,y)=\frac{1}{(n-1)h^2}\sum_{j=1,j\neq i}^nK(\frac{(X_j,Y_j)-(x,y)}{h})$ , where $K(\cdot,\cdot)$ is the kernel function and $h\rightarrow 0$ at some specified speed such that we have $\underset{(x,y)\in J}{\sup} |\widehat{f}_{-i}(x,y)-f(x,y)|=o_{P}(n^{-1/4})$ . In a paper I read about the following statement : " $ R_{n}=\frac{C}{n}\sum_{i=1}^{n} \underset{(x,y)\in J}{\sup} |\widehat{f}_{-i}(x,y)-f(x,y)| $ , where $C$ is some positive constant and $J$ is a compact subset of the support. As $\underset{(x,y)\in J}{\sup} |\widehat{f}_{-i}(x,y)-f(x,y)|=o_{P}(n^{-1/4})$ , it follows that $R_n=o_p(n^{-1/2})$ ." Here the $o_{p}(a_n)$ notation means converge in probability to zero at rate $a_n$ . Why we can arrive at the conclusion that $R_n=o_p(n^{-1/2})$ ? Intuitively, why taking an average makes convergence to zero faster? Thanks in advance!
