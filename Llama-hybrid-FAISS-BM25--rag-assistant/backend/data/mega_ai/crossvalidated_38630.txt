[site]: crossvalidated
[post_id]: 38630
[parent_id]: 38436
[tags]: 
Interpretation (or explanation) and prediction are different missions. Asking for a model that does both well, might sometimes be impossible. This will depend on the data generating process. 1. Convert continuous variables to intuitive categories (height-> {tall, short}). While this might bear a cost on goodness of fit, it is much more interpretable. In particular, when allowing for interactions. 2. Stick to interpretable classes of models. A linear predictors, or a CART is much more interpretable than a neural network of random forest. As I previously mentioned, this might (but not necessarily) bear a cost on the goodness of fit and predictions. 3. As you mentioned in the question- a dimension reduction stage before the model fitting could improve interpretability, but not necessarily. Say, PCA could suggest rather weird factors, which are both impossible to interpret and might not improve the prediction (since the dimension reduction was done while disregarding the dependent variable).
