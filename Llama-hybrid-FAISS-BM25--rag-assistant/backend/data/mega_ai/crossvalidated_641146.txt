[site]: crossvalidated
[post_id]: 641146
[parent_id]: 632803
[tags]: 
TLDR: no, the $\sigma$ converge to $0$ Well, seems like that an empirical test was more than enough to answer to this question I trained a linear model on a test dataset with the following objective: $$ L(\mu, \log\sigma) = N^{-1}\sum_{i=0}^N (\beta x - y ), \\ \beta = \mu + \epsilon \cdot \exp(\log\sigma)\\ \epsilon \sim N(0,1) $$ And the result is the following: Clearly, the loss converges, but the $\log\sigma\rightarrow 0$ A colleague of mine correctly pointed out that the "variance" term will just be white noise, so half a time will make the prediction better, and half not. Indeed, this trick is used to train VAEs, where in addition to it, there is also an anchor to a prior (the loss is $L = L' + D_{KL}(p_\theta(z)) || q(z)$ ) thus forcing $\sigma$ to be similar to something specific
