[site]: crossvalidated
[post_id]: 573449
[parent_id]: 573447
[tags]: 
This is what an out-of-sample test set reveals. In fact, machine learning tends not to care much about in-sample (“training set”) performance, since you can play connect-the-dots and memorize the data, but that models coincidences in the data (noise) instead of the true trend (signal). When you say that your SVM model has its performance improve when you add a feature of pure randomness, I suspect that you will see a drop in performance if you assess that on some out-of-sample data. Particularly for generalized linear models like logistic regressions, you might also be interested in various information criteria like Akaike (AIC) and Bayes (BIC) that penalize the model for having many parameters in order to keep the higher-parameter model from having an advantage. This answer of mine discusses how feature removal may or may not result in improved importance and is worth a read, even if it is somewhat tangential. Finally, “accuracy” is a surprisingly poor performance metric. I will include the usual links I post on this topic. Are unbalanced datasets problematic, and (how) does oversampling (purport to) help? https://www.fharrell.com/post/class-damage/ https://www.fharrell.com/post/classification/ https://stats.stackexchange.com/a/359936/247274 Proper scoring rule when there is a decision to make (e.g. spam vs ham email) Why is it that if you undersample or oversample you have to calibrate your output probabilities? https://twitter.com/f2harrell/status/1062424969366462473?lang=en
