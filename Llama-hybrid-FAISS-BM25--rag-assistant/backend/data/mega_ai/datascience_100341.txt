[site]: datascience
[post_id]: 100341
[parent_id]: 
[tags]: 
Why would the accuracy of a model change when the loss doesn't?

I've trained 8 models based on the same architecture (convolutional neural network), and each uses a different data augmentation method. The accuracy of the models fluctuates greatly while the loss doesn't fluctuate as much. In one case, there was a 10% difference in accuracy while the loss value was exactly the same. Why would that be? Shouldn't they both change?
