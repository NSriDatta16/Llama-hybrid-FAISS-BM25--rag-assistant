[site]: crossvalidated
[post_id]: 186674
[parent_id]: 186451
[tags]: 
"An unidentified Bayesian model is one in which the prior and posterior are exactly the same, and nothing is learned from the data". While this is not a main concerns, the mentioned point stays valid in Bayesian setting, if from some $\Theta_1 \ne \Theta_2$ , $p(x|\Theta_1)=p(x|\Theta_2)$ then the posterior distribution will not converge to a "prior independent" solution and the prior will lead a part of inference (even in the asymptotical limit of observations number) which may be an undersirable property. For your to last question, there is a closely related question with answer on this site : Is there any reason to prefer a bayesian model with few variables? . Moreover consider that adding an extra parameter $\theta_2$ through $p(\theta_1|\theta_2)$ and hyperprior on $\theta_2$ , $p(\theta_1)$ writes: $$ p(\theta_1)=\int_R p(\theta_1|\theta_2)p(\theta_2)d\theta_2 $$ I do not know if there are some general results relating $var(\theta_1)$ and $var(\theta_2)$ but for example $ var(\theta_1) \ngtr var(\theta_1|\theta_2=\alpha)$ in general. So adding an hyperparameter to the model does not systematically result in a prior model of higher variance.
