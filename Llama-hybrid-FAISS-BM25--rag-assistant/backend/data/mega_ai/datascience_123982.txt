[site]: datascience
[post_id]: 123982
[parent_id]: 
[tags]: 
Is it a problem to use the test dataset for the hyperparameter tuning, when I want to compare 2 classification algorithms on the 10 different dataset?

I know that we should use the validation set to perform hyperparameter tuning and that test dataset is not anymore really the test if it is used for hyperparameter tuning. But is this a problem if i want to compare the performance of 2 algorithms (e.g., Random Forest and XGBoost) across 10 different datasets, where each time I am using the test data for tuning. I believe that if they are trained and tested under the same conditions, the final performance analysis should be actually true representation which algorithm is better performing on these datasets. Or am i mistaken?
