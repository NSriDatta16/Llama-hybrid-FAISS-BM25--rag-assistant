 C X Y g ⟩ H = ⟨ f ⊗ g , C X Y ⟩ H ⊗ H {\displaystyle \operatorname {Cov} (f(X),g(Y)):=\mathbb {E} [f(X)g(Y)]-\mathbb {E} [f(X)]\mathbb {E} [g(Y)]=\langle f,{\mathcal {C}}_{XY}g\rangle _{\mathcal {H}}=\langle f\otimes g,{\mathcal {C}}_{XY}\rangle _{{\mathcal {H}}\otimes {\mathcal {H}}}} Given n {\displaystyle n} pairs of training examples { ( x 1 , y 1 ) , … , ( x n , y n ) } {\displaystyle \{(x_{1},y_{1}),\dots ,(x_{n},y_{n})\}} drawn i.i.d. from P {\displaystyle P} , we can also empirically estimate the joint distribution kernel embedding via C ^ X Y = 1 n ∑ i = 1 n φ ( x i ) ⊗ φ ( y i ) {\displaystyle {\widehat {\mathcal {C}}}_{XY}={\frac {1}{n}}\sum _{i=1}^{n}\varphi (x_{i})\otimes \varphi (y_{i})} Conditional distribution embedding Given a conditional distribution P ( y ∣ x ) , {\displaystyle P(y\mid x),} one can define the corresponding RKHS embedding as μ Y ∣ x = E [ φ ( Y ) ∣ X ] = ∫ Ω φ ( y ) d P ( y ∣ x ) {\displaystyle \mu _{Y\mid x}=\mathbb {E} [\varphi (Y)\mid X]=\int _{\Omega }\varphi (y)\ \mathrm {d} P(y\mid x)} Note that the embedding of P ( y ∣ x ) {\displaystyle P(y\mid x)} thus defines a family of points in the RKHS indexed by the values x {\displaystyle x} taken by conditioning variable X {\displaystyle X} . By fixing X {\displaystyle X} to a particular value, we obtain a single element in H {\displaystyle {\mathcal {H}}} , and thus it is natural to define the operator { C Y ∣ X : H → H C Y ∣ X = C Y X C X X − 1 {\displaystyle {\begin{cases}{\mathcal {C}}_{Y\mid X}:{\mathcal {H}}\to {\mathcal {H}}\\{\mathcal {C}}_{Y\mid X}={\mathcal {C}}_{YX}{\mathcal {C}}_{XX}^{-1}\end{cases}}} which given the feature mapping of x {\displaystyle x} outputs the conditional embedding of Y {\displaystyle Y} given X = x . {\displaystyle X=x.} Assuming that for all g ∈ H : E [ g ( Y ) ∣ X ] ∈ H , {\displaystyle g\in {\mathcal {H}}:\mathbb {E} [g(Y)\mid X]\in {\mathcal {H}},} it can be shown that μ Y ∣ x = C Y ∣ X φ ( x ) {\displaystyle \mu _{Y\mid x}={\mathcal {C}}_{Y\mid X}\varphi (x)} This assumption is always true for finite domains with characteristic kernels, but may not necessarily hold for continuous domains. Nevertheless, even in cases where the assumption fails, C Y ∣ X φ ( x ) {\displaystyle {\mathcal {C}}_{Y\mid X}\varphi (x)} may still be used to approximate the conditional kernel embedding μ Y ∣ x , {\displaystyle \mu _{Y\mid x},} and in practice, the inversion operator is replaced with a regularized version of itself ( C X X + λ I ) − 1 {\displaystyle ({\mathcal {C}}_{XX}+\lambda \mathbf {I} )^{-1}} (where I {\displaystyle \mathbf {I} } denotes the identity matrix). Given training examples { ( x 1 , y 1 ) , … , ( x n , y n ) } , {\displaystyle \{(x_{1},y_{1}),\dots ,(x_{n},y_{n})\},} the empirical kernel conditional embedding operator may be estimated as C ^ Y ∣ X = Φ ( K + λ I ) − 1 Υ T {\displaystyle {\widehat {C}}_{Y\mid X}={\boldsymbol {\Phi }}(\mathbf {K} +\lambda \mathbf {I} )^{-1}{\boldsymbol {\Upsilon }}^{T}} where Φ = ( φ ( y 1 ) , … , φ ( y n ) ) , Υ = ( φ ( x 1 ) , … , φ ( x n ) ) {\displaystyle {\boldsymbol {\Phi }}=\left(\varphi (y_{1}),\dots ,\varphi (y_{n})\right),{\boldsymbol {\Upsilon }}=\left(\varphi (x_{1}),\dots ,\varphi (x_{n})\right)} are implicitly formed feature matrices, K = Υ T Υ {\displaystyle \mathbf {K} ={\boldsymbol {\Upsilon }}^{T}{\boldsymbol {\Upsilon }}} is the Gram matrix for samples of X {\displaystyle X} , and λ {\displaystyle \lambda } is a regularization parameter needed to avoid overfitting. Thus, the empirical estimate of the kernel conditional embedding is given by a weighted sum of samples of Y {\displaystyle Y} in the feature space: μ ^ Y ∣ x = ∑ i = 1 n β i ( x ) φ ( y i ) = Φ β ( x ) {\displaystyle {\widehat {\mu }}_{Y\mid x}=\sum _{i=1}^{n}\beta _{i}(x)\varphi (y_{i})={\boldsymbol {\Phi }}{\boldsymbol {\beta }}(x)} where β ( x ) = ( K + λ I ) − 1 K x {\displaystyle {\boldsymbol {\beta }}(x)=(\mathbf {K} +\lambda \mathbf {I} )^{-1}\mathbf {K} _{x}} and K x = ( k ( x 1 