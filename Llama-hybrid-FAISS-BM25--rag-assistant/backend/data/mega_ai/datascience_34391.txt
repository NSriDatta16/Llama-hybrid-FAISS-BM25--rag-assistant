[site]: datascience
[post_id]: 34391
[parent_id]: 
[tags]: 
Doc2Vec network architecture

I have been familiarizing myself with Word2Vec and Doc2Vec . After reading multiple papers including the the ones by T Mikolov (the creator of Doc2Vec), I am not clear on how does the neural network for Doc2Vec looks like. I get Word2Vec: a neural network with 3 layers (1 hidden). The input layer has all words in the vocab. The hidden layer is the same size as that of embedding, while the output layer, again the same size as vocab. This is, of course, the CBOW framework. How does Doc2Vec change things?
