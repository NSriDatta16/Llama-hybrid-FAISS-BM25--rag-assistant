[site]: crossvalidated
[post_id]: 318072
[parent_id]: 62069
[tags]: 
In general, for both discrete* & categorical features, this method isn't particularly amenable to outlier analysis. Since there is no magnitude associated with categorical predictors, we are working with: Frequency of the category being observed in the global data Frequency of the category being observed within subspaces of the data Note that neither of these qualities can be analyzed in isolation, as your Gaussian method requires. Instead, we need a method that contextualizes categorical features & considers the correlational nature of the data. Here are some techniques for categorical & mixed attribute data, based on Outlier Analysis by Aggarwal: If you can define a similarity function which builds a positive semidefinite matrix across all observations (regardless of data types), compute the similarity matrix $S$, find its diagonalization $S=Q_k\lambda_k^2Q_k^T$, and use the non-zero eigenvectors $Q_k$ to compute a feature embedding $E = Q_k\lambda_k$ . For each row (observation) in $E$, compute its distance from the centroid; this is your outlier score, and you can use univariate methods to determine outliers. If you have purely categorical features, fit a mixture model to the raw categorical data. Anomalous points have lowest generative probability. Use one-hot encoding for categorical predictors and optionally latent variable analysis ** for ordinal variables with non-apparent continuous mappings Standardize the non-one-hot features (one-hot features are already implicitly standardized) and perform Principal Component Analysis . Perform dimensionality reduction using the top principal components (or a soft PCA approach where eigenvectors are weighted by eigenvalues) and run a typical continuous outlier analysis method (e.g. a mixture model or your Gaussian method) Perform an angle-based analysis. For each observation, compute cosine similarities between all pairs of points. Observations with the smallest variance of these similarities (known as the "Angle-Based Outlier Factor") are most likely outliers. May require a final analysis of the empirical distribution of ABOF to determine what is anomalous. If you have labelled outliers: Fit a predictive model to the engineered data (logistic regression, SVM, etc.). *Discrete features could possibly be handled approximately in your Gaussian method. Under the right conditions, a feature may be well approximated by a normal distribution (e.g. binomial random variable with npq > 3). If not, handle them as ordinals described above. **This is similar to your idea of "replace the category value with the percentage chance of observation"
