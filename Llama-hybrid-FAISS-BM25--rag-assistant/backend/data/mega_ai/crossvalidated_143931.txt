[site]: crossvalidated
[post_id]: 143931
[parent_id]: 
[tags]: 
Sample lower bound for binary classification in Linear Discriminant Analysis?

Below is a description of this problem: Suppose the label $Y\in\{1,0\}$ in binary classification satisfies $\Pr[Y=1]=\Pr[Y=0]=\frac{1}{2}$, and $p(X|Y=1)=\mathcal{N}(\mu_1,\Sigma)$, $p(X|Y=0)=\mathcal{N}(\mu_0,\Sigma)$. Here $\mu_1, \mu_0 \in \mathbb{R}^p$, $\Sigma \in \mathbb{R}^{p \times p}$, where $p$ is the dimension. To train a classifier I use $n$ samples $(X_1, Y_1), (X_2, Y_2), \dots, (X_n, Y_n)$. For any classifier $C$ (not restricted to LDA, logistic regression, SVM, etc.), what is minimum number of samples I need so that the expected 0-1 error of the classifier is smaller than $\frac{1}{2}-\epsilon$ for some small $\epsilon>0$? That is: $$ \Pr[C(X) \neq Y] \leq \frac{1}{2}-\epsilon $$ The sample lower bound should be given in terms of $\mu_1$, $\mu_0$, $\Sigma$, $\epsilon$. Has anybody studied the sample lower bound for this problem? It seems that people have studied sample complexity of estimating the parameters ( ref1 ), but there doesn't seem to be much regarding the sample complexity for classification, which does not necessarily need parameters to be correctly estimated.
