[site]: crossvalidated
[post_id]: 246487
[parent_id]: 245646
[tags]: 
I'd encourage you to think about loss . Each algorithm is outputting a distribution which represents its "bet" for a value of 0 or 1. What should its loss be for such a bet, given the actual value? What would your ideal algorithm minimize? Generally it would be expected loss, but it could be something else (e.g., loss at 95% confidence). Entropy would treat this as an information model; i.e., how many bits would it cost the algorithm to output the actual value. Indeed, if it is completely wrong, it would cost an infinite number of bits. To avoid that you could use Jensen-Shannon Divergence . Note that in its generalized form, you could also include priors as part of it. Interestingly, you ignored the option of a simple linear loss, $|\hat{p}_i - y_i|$, which would generally be the first thing to consider after misclassification which is 0/1 loss.
