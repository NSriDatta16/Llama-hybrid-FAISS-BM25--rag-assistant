[site]: datascience
[post_id]: 80718
[parent_id]: 80531
[tags]: 
I have not heard of any model agnostic way to measure model complexity. There are several strategies but they are model dependant. You can tackle the problem using different families of models. For linear models you can count the number of nonzero parameters that is using. Number of features used for the prediction. For decision tree you can count the maximum depth that the tree achieves. For Neural Networks you can count the number of parameters that your NN is optimizing. For ensemble methods (random forest, gradient boosting) you can use an aggregation of the different weak learners used in the model. For the python implementation there are several implementations depending on for which model you want to measure it. Some of them if you notice are really easy to measure. Its intuitively hard to compare complexity between different model families. What is more complex a linear regression with 4 coefficients or a decision tree with max_depth=3? On the topic of deep learning complexity, Hinton, Oriol, Jeff Dean published a paper Distilling the knowledge of a Neural Network . Where they talk about simplifying the complexity of a Neural Network.
