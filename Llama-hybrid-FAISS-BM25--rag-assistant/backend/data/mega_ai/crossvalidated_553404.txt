[site]: crossvalidated
[post_id]: 553404
[parent_id]: 
[tags]: 
Why do we try to "Reproduce" Hilbert Spaces in Statistics?

I am trying to better understand why people are interested in "reproducing" Hilbert Spaces in Statistics and Machine Learning. I (think) understand the general idea behind Hilbert Spaces. Hilbert Spaces "generalize the methods of linear algebra and calculus from the two-dimensional and three dimensional Euclidean spaces to spaces that may have an infinite dimension". I also (think) I understand the general idea behind a "kernel". In the context of statistics and machine learning, a "kernel" is some mathematical function that can be used to evaluate the ("not always linear") dot product between two inputs. This idea is useful in statistical modelling - here are some applications of kernel based algorithms in statistical modelling: Support Vector Machines : Support Vector Machines (SVM) are an algorithm that can be used for supervised classification and regression problems. SVMs take data which might not be "easily separable" (e.g. non-linear) in their original "space" - and "teleport" them into a "new space" (often higher dimensional compared to the original space) in which the same data might become "better separable" using a a kernel function. Once the data is in the "new space", the classification or regression task is performed in this new space - and the results are then "teleported" back into the "original space". This "teleportation process" to "new space" can be computationally expensive, and SVMs use the famous "kernel trick" to simplify the "teleportation process". (Note: there is a famous theorem called "Cover's Theorem" which states that a "complex pattern-classification problem, cast in a high-dimensional space nonlinearly, is more likely to be linearly separable than in a low-dimensional space, provided that the space is not densely populated.") Kernel PCA (Principal Component Analysis) : Standard PCA algorithm is an unsupervised algorithm that seeks to reduce the dimensionality of the data by compressing (i.e. again, "teleporting") the data into a "simpler space" (i.e. a space of lower dimensionality) compared to the "original space" (SVM brings the data into a higher dimensional space, PCA brings the data into a lower dimensional space). Once in the lower dimensional space, the data might now become easier to understand and model - with a negligible loss in information. The main caveat however, is that standard PCA requires the data to be situated on a "linear manifold" - in other words, standard PCA requires the data to be "fairly linear", and would not work well on "non-linear" data. Once again, kernel functions can be used to "teleport" non-linear data to a "new space" where the data can be better managed. As seen below, the use of Kernels can take data which is non-linearly separable at first, and make them better separable: However, I have the following question: I (somewhat) understand what Hilbert Spaces and Kernels are - but how do these two concepts come together? In statistics and machine learning, why are we interested in "reproducing" the Hilbert Space? Isn't the data already in some Hilbert Space? References: Intuition behind RKHS (Reproducing Kernel Hilbert Space}? https://www.youtube.com/watch?v=zASzj4DkZXY https://en.wikipedia.org/wiki/Hilbert_space https://en.wikipedia.org/wiki/Cover%27s_theorem Note: There is another theorem called Mercer's Theorem which is also necessary when studying kernels, but I am not quite sure why this is so important.
