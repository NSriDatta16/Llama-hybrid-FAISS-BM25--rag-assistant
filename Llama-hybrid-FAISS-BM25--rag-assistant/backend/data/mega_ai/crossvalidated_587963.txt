[site]: crossvalidated
[post_id]: 587963
[parent_id]: 
[tags]: 
Coefficients of the regression line of observed vs predicted data for a model - assumptions and proof

This 2008 paper by Pi√±eiro et al. shows that the 'correct' way to represent the relationship between a model's predicted values (P) and the observed (experimental) values (O) that originated it is to plot O on the y axis and P on the x axis. The expected coefficients of the regression line $y = a + b \cdot x$ are then $a = 0, b = 1$ . The coefficients that would be obtained by swapping the axes (O on the x axis and P on the y axis) would normally be different, except for a perfect model. The paper substantiates these claims both algebraically and numerically. The predictions from the continuous models I use (based e.g. on random forest or deep neural network methods, so nothing as simple as a linear, polynomial or logarithmic regression), seem indeed to produce regression lines close to $y = x$ only if we plot observed (y) vs predicted (x). However, I would like to understand a bit better how general this is expected to be, by clarifying the exact assumptions/conditions that make this happen, and the maths involved. While I have little doubt about the correctness of some crucial statements in the article, I find that much was left to the reader's own presumed prior knowledge, skipping many explicit derivations and explanations. So, I attempted to reconstruct the proof of the coefficients, starting from the initial model, using more commonly known equations, and especially not skipping any logical step. The goal of this post is to get your expert advice on whether my assumptions and reasoning below are correct or not, and if not, learn how this must be done instead. Suppose that a continuous model was made, linking some descriptors $X$ (assumed to be known without error) to some observed variable $Y$ . This means that a function $\hat Y = \hat f(X)$ was fitted to $(X,Y)$ data, with the goal to make $RSS$ as small as possible: $RSS = \sum_{i=1}^N {(Y_i - \hat Y_i)^2}$ [This is actually an important assumption (isn't it?). Given how I proceed below, I think it's necessary that the model be built using this least-squares-like loss; but if not, please let me know]. The terms of this sum may differ from zero because $Y$ may differ from its 'true' value, due to imperfect experimental measurements (aleatoric uncertainty): $Y_i = Y_{true,i} + \epsilon_{al,i}$ and because $\hat f$ is only an approximation of the 'true' function relating $X$ and $Y_{true}$ , i.e. because the model itself may be imperfect (epistemic uncertainty): $Y_{true,i} = f(X_i) = \hat f(X_i) + \epsilon_{ep,i} = \hat Y_i + \epsilon_{ep,i}$ Putting it all together: $Y_i = \hat Y_i + \epsilon_{ep,i} + \epsilon_{al,i} = \hat Y_i + \epsilon_i$ where $\epsilon_i$ is the sum of the two uncertainties. Hoping that the above makes sense up to this point, do you think any specific assumptions need to be made about these uncertainties? E.g. normality, independence, zero mean...? The ordinary least squares coefficients for $y = a + b \cdot x$ , given data $(x,y)$ , are known from the theory: $b = \frac {Cov(x,y)}{Var(x)}$ $a = \bar y - b \cdot \bar x$ As for $b$ , substituting: $x = \hat Y, y = Y = \hat Y + \epsilon$ $b = \frac {Cov(\hat Y, Y)}{Var(\hat Y)} = \frac {Cov(\hat Y, \hat Y + \epsilon)}{Var(\hat Y)} = \frac {Var(\hat Y) + Cov(\hat Y, \epsilon)}{Var(\hat Y)}$ Given that $\hat Y$ and $\epsilon$ are independent(*), $Cov(\hat Y, \epsilon) = 0$ , hence: $b = \frac {Var(\hat Y) + 0}{Var(\hat Y)} = 1$ [(*) side note: this is a point that I accept, but I confess is not fully clear to me. I can see from $Y_i = Y_{true,i} + \epsilon_{al,i}$ that $Y$ is formally a function of $\hat Y$ and $\epsilon$ , so $Y$ depends on $\epsilon$ whereas $\hat Y$ doesn't; but substantially , the model $\hat f(X)$ was built by doing calculations on values of $Y$ that 'contain' $\epsilon$ , so I don't really get exactly how this independence is achieved or imposed. If anyone could please point me to posts or resources that explain this more practically, it would be great]. As for $a$ , it is known from the above that: $a = \bar y - b \cdot \bar x = \bar y - \bar x$ Here I struggled a bit. The paper simply says that the two means $\bar Y$ and $\hat {\bar Y}$ must be identical because the model is 'not biased', therefore $a = 0$ . I had no idea where that came from, so I tried the following. Calculate the variance of $\epsilon$ : $Var(\epsilon) = Var(Y - \hat Y) = E[((Y - \hat Y)-E[(Y - \hat Y)])^2] = ... = \frac 1 N \cdot \sum_{i=1}^N {(Y_i - \hat Y_i)^2} - (\bar Y - \hat {\bar Y})^2$ i.e.: $\frac {RSS} N = Var(Y - \hat Y) + (\bar Y - \hat {\bar Y})^2$ Given that $N$ is a constant and model $\hat Y$ was made by $argmin \ RSS$ , the quantity on the right must be minimal, too, i.e. there can be no model with a lower $RSS$ than $\hat Y$ . Imagine a model that is just a shift of $\hat Y$ , so $\hat Y + \delta$ , with $\delta$ real constant. For that model: $\frac {RSS_{shifted}} N = Var(Y - \hat Y - \delta) + (\bar Y - \hat {\bar Y} + \delta)^2 = Var(Y - \hat Y) + (\bar Y - \hat {\bar Y} - \delta)^2$ This new model cannot have a lower $RSS$ than model $\hat Y$ , i.e.: $(\bar Y - \hat {\bar Y} - \delta)^2 \ge (\bar Y - \hat {\bar Y})^2$ must be always true. Simplifying: $-2 \cdot \delta \cdot (\bar Y - \hat {\bar Y} ) + \delta^2 \ge 0$ This expression is always true only if $\bar Y - \hat {\bar Y} = 0$ ; otherwise there are specific values of $\delta$ for which it is false (i.e. for which $\hat Y$ is not the best possible model). Therefore, $\bar Y = \hat {\bar Y}$ , and $a = 0$ . This last part I find quite hand-wavy, so I'd particularly appreciate your advice, e.g. on whether there is a more direct approach to show that the model's predictions must have the same mean as the observations.
