[site]: datascience
[post_id]: 31277
[parent_id]: 31259
[tags]: 
In general, effective learning is all about making the training error small and the gap between training and test error small. By test data, we mean examples that your model has never seen before. so you need development (validation) set, to fine-tune your hyperparameters such hidden cells, the number of layers, learning rate, etc. Split the training data into train/dev sets, be careful test set must always be generated from the same data distribution that generates your train/dev sets. LSTM might overfit your dataset, start with vanilla RNN, or small GRU. Use early stopping to stop training when the loss of the validation examples stop decreasing.
