[site]: crossvalidated
[post_id]: 253436
[parent_id]: 
[tags]: 
Joint Distribution of Maximum Likelihood Estimation

I have been thinking about this for about 2 hours now and I don't see my error. In Time Series Analysis by Hamilton it is stated that the MLE of a normal distribution can be obtained by regarding the observed sample of size $n$ as a signle draw from the joint distribution. This intuitively clear since the ML estimation requires knowledge of the joint distribution. But now I came across this and I cannot explain it: Let $X_1,\dots, X_n$ be iid normal with mean $\mu$ and variance $\sigma^2$. The log likelihood function is given by $$\ell = -\frac{n}{2}\ln(2\pi) - \frac{n}{2}\ln(\sigma^2) + \frac{1}{2}\sum_{i=1}^n\left(\frac{x_i - \mu}{\sigma}\right)^2.$$ But if we consider the joint distribution (in log) which is given by $$\ell^* = -\frac{n}{2}\ln(2\pi) - \frac{1}{2}\ln(\det(\sigma^2\boldsymbol{I_n})) + \frac{1}{2}(\boldsymbol{x - \mu})'\sigma^{-2}\boldsymbol{I_n}(\boldsymbol{x-\mu})$$ we see that the middle term is different by the factor $n$ which leads to different MLEs for $\sigma^2$. What am I missing? I believe this observation has a simple explanation but I cant see it...
