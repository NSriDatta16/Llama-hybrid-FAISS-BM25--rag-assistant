[site]: datascience
[post_id]: 54476
[parent_id]: 49606
[tags]: 
Firstly, I suggest looking at the following paper: Gal and Ghahramani, A Theoretically Grounded Application of Dropout in Recurrent Neural Networks , 2016 which discusses how to appropriately apply dropout as an approximate variational Bayesian model. The key is to use the same dropout mask at each timestep, rather than IID Bernoulli noise. As for measuring model uncertainty, note that while dropout gives us an approximate variational Bayesian neural network, it does not give access to the variational posterior density, and so we cannot compute e.g. the entropy of the posterior distribution. But, we can trivially compute the predictive variance , which is closely related to model uncertainty. Simply run $n$ forward passes on an input to get outputs $Y_F = (y_1,\ldots,y_n)$ . Then compute your measure (e.g., the covariance matrix in the regression case) on $Y_F$ . For classification as your case seemingly is, you can use predictive entropy, for instance. For more measures see Gal et al, Deep Bayesian Active Learning with Image Data , 2017
