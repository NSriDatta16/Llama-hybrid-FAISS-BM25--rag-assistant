[site]: crossvalidated
[post_id]: 504165
[parent_id]: 504155
[tags]: 
This is a good question. Here is how you can determine the maximum likelihood estimate (MLE): $$ \widehat \iota := \widehat \iota(x) := \arg \max_i f_i(x) \quad (*) $$ where $f_i$ is the density of hypothesis $i$ . Think of $i \in \{1,2,3,4\}$ as the parameter. Then, the likelihood is the function $i \mapsto f_i(x)$ where you fix $x$ at the observed value of 5. We have $$ f_1(x) = C e^{-x^2/2}, \;\; f_2(x) = \frac{C}{\sqrt 3}e^{-x^2/6}, \;\; f_3(x) = C e^{-(x-4)^2/2}, \;\; f_4(x) = \frac{C}{\sqrt 3} e^{-(x-4)^2/6} $$ where $C = 1/\sqrt{2\pi}$ whose exact value does not matter in determining $\widehat \iota$ . At this point you plug in your observed value of $x = 5$ and compare the likelihoods. We have $f_3(5) > f_1(5)$ so we can rule out hypothesis 1. We also have $f_4(5) > f_2(5)$ so we rule out hypothesis 2. So we have to compare $f_3(5)$ and $f_4(5)$ . It is easier to look at the log-likelihood ratio: $$ \log \frac{f_3(5)}{f_4(5)} = \frac{\log 3}2 - \frac{(5-4)^2}{2} + \frac{(5-4)^2}{6} =\frac{\log 3}2 -\frac13 > 0 $$ showing that $f_3(5) > f_4(5)$ , hence the MLE is $\widehat \iota = 3$ . It really doesn't matter if you have a single sample or multiple: You form the likelihood and maximize it. If you have a sample $x_1,\dots,x_n$ , independently drawn from one of $f_i$ , then the likelihood is $i \mapsto \prod_{t=1}^n f_i(x_t)$ . If you have a prior on the hypotheses, you can also compute the maximum a posterior estimate (MAP). The MAP estimate is the one that minimizes the probability of error (i.e., the optimal Bayes estimator for the 0-1 loss). Assuming that the prior is uniform, this coincides with the MLE. This means that if you repeat your experiment many times, every time randomly selecting one of the hypotheses and generating $x$ from it and if you always decide using the MLE rule (*), then your probability of error is (roughly) the smallest possible (among all decision rules). The thing that having more samples achieve is that this smallest best possible error will be smaller with a larger sample size. These more or less can be found in any standard text on theoretical statistics, e.g., Keener 's book or Lehmann and Casella 's point estimation book. About optimality : Equivalently, if you don't want to be a Bayesian, MLE is the estimater that minimizes the sum of the probability of errors under each hypothesis: $$ \widehat \iota(\cdot) = \arg\min_{\delta(\cdot)} \sum_{i=1}^4 \mathbb P_i \big( \delta(x) \neq i\big). $$ where the minimization is over all possible decision rules $\delta(\cdot)$ .
