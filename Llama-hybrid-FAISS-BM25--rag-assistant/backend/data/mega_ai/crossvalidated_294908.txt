[site]: crossvalidated
[post_id]: 294908
[parent_id]: 219747
[tags]: 
Whether it makes sense to create a short-list of just n models or to select a single model depends a lot on how much the data favors the "best" (according to a chosen criterion) model. If a lot of models are "close together", then all of them are plausible models and selecting a single one or a subset of these when you have considered a lot of models is pretty problematic. For that reason model averaging is often a good idea and for prediction tasks weights for each model based on $\text{prior weights} \times \exp\{ -0.5 |\text{AIC}_i - \min_j \text{AIC}_j| \}$ are popular. I guess if you were to consider then when one model or a small number of models get nearly all the weight (e.g. the best model is ahead of the next model in terms of AIC by, say, 10 to 15 or so and not too many models were considered), then it is probably reasonable to concentrate on those. AIC based weights are popular for prediction, because of the link between these and maximum likelihood estimation. For a very enthusiastic view of this type of approach, you could refer to Burnham and Anderson's "Model selection and multimodel inference". As mentioned in the other response naive RMSE is not a justifiable option (although one could get a version adjusted for overfitting by using e.g. cross-validation), but other criteria (e.g. BIC etc.) are also potentially interesting although my personal bias is towards AIC type of approaches.
