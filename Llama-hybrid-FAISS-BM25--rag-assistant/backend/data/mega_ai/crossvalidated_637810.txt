[site]: crossvalidated
[post_id]: 637810
[parent_id]: 637773
[tags]: 
As far as I understand it, the major reason for not using $Y$ in data reduction is that much theory is based on the crucial assumption that this is not done, and doing it will violate the theory in a critical manner. The simplest example is the standard distribution theory for LS regression coefficient estimators and the tests and confidence intervals based on it. These will be hugely overoptimistic (i.e., assume far more precision than we indeed have) if the $Y$ has been used for data reduction before. Furthermore, data-driven assessment of predictive power such as cross-validation is often run ignoring the use of $Y$ for dimension reduction, i.e., the selected lower dimensional representation of the $X$ -space is used without accounting for the fact that there was data dependent selection for it before. Once more, this will lead to an overoptimistic assessment of prediction strength. In machine learning this is known as "information leakage". In fact it can be avoided using techniques such as double cross-validation, double bootstrap, or data splitting plus cross-validation on data not used for dimension reduction. That said, it is in fact true that unsupervised dimension reduction is prone to losing valuable information that supervised dimension reduction when done correctly does not lose. I (and students of mine) have compared, for example, PCA on $X$ -space with partial least squares (PLS; which uses the $Y$ for dimension reduction) in some projects, carefully doing the cross-validation in such a way that information leakage is avoided, and have found that PLS outperforms PCA more often than not. This shouldn't be all too surprising as PLS uses more relevant information than PCA (in fact the concept of "information leakage" implies that there is information in the first place). So ultimately, if your major aim is predictive power, the unsupervised route may not be better. On the other hand, if you plan to run and interpret standard significance tests, using the $Y$ for dimension reduction will for sure mess things up. Note that Harrell himself (who is occasionally active on this site) has stated that he has in the meantime become more Bayesian than he was writing that book. Maybe now he'd say "don't use such significance tests regardless of whether dimension reduction is done in a supervised or unsupervised manner", which would take away a reason to do it in an unsupervised manner. But then information leakage can be a problem in Bayesian analysis as well.
