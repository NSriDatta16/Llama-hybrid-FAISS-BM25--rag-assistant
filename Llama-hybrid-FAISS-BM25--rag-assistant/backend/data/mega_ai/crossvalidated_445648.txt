[site]: crossvalidated
[post_id]: 445648
[parent_id]: 445595
[tags]: 
First of all, I will say that there is an equivalence between logistic regression and the proportional odds model. In fact, with the special case of current status data, the proportional odds model with a log-logistic baseline can be fit in a straightforward manner . In addition, the semi parametric proportional odds can be perfectly fit with right censored data using logistic regression, although this will require a huge expansion of the data that will be incredibly computationally inefficient. To explain the computational inefficiencies, let's first start with the wrong way to do things. A common first-pass idea is to simply just include time (or log-time) as a predictor in your dataset and fit a logistic regression model from that. This is not a good idea . A very simple way to demonstrate this is consider what would happen if we had survival data in which none of the data was right censored, i.e., we observed an event for all subjects of the dataset. In that case, if we tried the naive logistic regression approach, we now have a logistic regression dataset in which the outcome is all 1's. That's not good: it means all predictions for model will be 1, regardless of what's the input, and it also means all the estimates of the parameters are undefined. Yet survival models have no problem with valid estimation when there is no right censored data, so we must be doing something wrong! To demonstrate the issue here, consider for now that we have two time points with $T_1 . In a survival analysis setting, if we know that an event for subject $i$ occurred at $T_2$ , this means that the event did not occur at $T_1$ . So if we wanted to use logistic regression to fit our survival model, we could do this by augmenting our data: every time we have an event that occurred at $T_2$ , we would have one data point in our logistic regression model that a dummy variable indicates time = $T_1$ and outcome = 0 and another data point with another dummy variable that indicates time = $T_2$ and outcome = 1 (all other covariates are the same between the two new data points). If we fit this model, we fully recreate the semi-parametric proportional odds model. However, note that we had to make our dataset bigger, which means it runs slower. If our data set only has two unique times ( $T_1$ and $T_2$ ), then this isn't so bad; our new dataset will be less than twice as big as the original (since events that occurred at $T_1$ don't need any augmentation). However, if we have three unique times, it gets worse: if an event happened at time $T_3$ , we will need to augment one data point at $T_1$ and another at $T_2$ , meaning three data points will be needed to represent this single record. In general, the size of the new dataset will be of order $O(n n_{unq})$ , where $n_{unq}$ is the number of unique times in the dataset. If times are continuous, this means our new data is now of size $O(n^2)$ , with $n$ new covariates. Computationally, that's a nightmare for even moderately sized datasets ( $n = 1000$ )!
