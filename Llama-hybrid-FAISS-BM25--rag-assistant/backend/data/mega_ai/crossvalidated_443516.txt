[site]: crossvalidated
[post_id]: 443516
[parent_id]: 
[tags]: 
Calculating the linear function in a Neural Network vs in a neuron

When implementing the forward propagation in a 2 NN layer, Andrew NG used Z1 = np.dot(W1,X)+b1 A1 = np.tanh(Z1) Z2 = np.dot(W2,A1)+b2 A2 = sigmoid(Z2) But when implementing a single neuron, he used A = sigmoid(np.dot(w.T,X)+b) My question is, where/why did the transpose disappear in the 2 NN layers?
