[site]: crossvalidated
[post_id]: 375886
[parent_id]: 375876
[tags]: 
Welcome to CV. The ideal test would be a binomial regression of your choice with whether the target was correct (1) or not (0) as the outcome, and condition and confidence as your predictors. It would be a good idea to include the interaction between condition and confidence into your model, but you could also test whether that interaction is significant by running two models, one with the interaction and one without, and then using a likelihood ratio test, the null hypothesis of which is that the model with more predictors (i.e., with the interaction) doesn't explain the outcome any better than the model with fewer predictors. If you decide to keep the interaction, the coefficients in your model need to be carefully interpreted. The coefficient on confidence will be the effect of confidence on accuracy in the reference condition (i.e., the condition given the label "0"). The coefficient on the interaction between confidence and one of the conditions is the difference between the effect of confidence in that condition and the effect of confidence in the reference condition. If you omit the interaction, the coefficient on confidence will be the effect of confidence on accuracy controlling for condition. This assumes that the effect of confidence on accuracy is the same in all conditions (which is the assumption of a model without interactions). The interpretation of the "effect" of confidence depends on the choice of regression model you use. If you use a logistic regression, the coefficient on condition will be the change in the log odds of getting the target correct corresponding to a 1-unit increase in confidence. This value is challenging to interpret. If you use a binomial regression with an identity link or the linear probability model, the coefficient on condition will be the change in the probability of getting the target correct corresponding to a 1-unit change in confidence. This interpretation is much easier to understand, but this model may have some problems. Finally, if you have multiple measurements from the same individual (i.e., each individual was present in multiple trials), you need to account for this fact using one of several methods, or else your inferences will be invalid. The most straightforward method would be using cluster-robust standard errors with subject ID as the clustering variable, but multilevel (aka mixed) models and generalized estimating equations (GEEs) can be viable options as well but have additional interpretation challenges associated with them. Whatever you do, do not split confidence into two bins. The failure of this method is well documented .
