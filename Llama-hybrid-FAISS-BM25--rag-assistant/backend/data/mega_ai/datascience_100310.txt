[site]: datascience
[post_id]: 100310
[parent_id]: 
[tags]: 
Handling unknown words when making NER Models

I'm working on my custom Named Entity Recognition model that I'm making in Python's Keras lib. I have read that I should enumerate all words that are appearing, so that I get vectorized sequences. I have done that like this: word2idx = {w: i + 1 for i, w in enumerate(words)} label2idx = {t: i for i, t in enumerate(labels)} # CREATING FEATURES(X) AND RESULTS(Y) max_len = 50 num_words = len(num_words) #number of unique words in dataset X = [[word2idx[w[0]] for w in s] for s in list_of_sentances] X = pad_sequences(maxlen=max_len, sequences=X, padding="post", value=num_words-1) y = [[label2idx[w[1]] for w in s] for s in list_of_sentances] y = pad_sequences(maxlen=max_len, sequences=y, padding="post", value=label2idx["O"]) y = [to_categorical(i, num_classes=num_labels) for i in y] This is my final model: input_word = Input(shape=(max_len,)) model = Embedding(input_dim = num_words, output_dim = 50, input_length = max_len)(input_word) model = SpatialDropout1D(0.2)(model) model = Bidirectional(LSTM(units = 5, return_sequences=True, recurrent_dropout = 0.1))(model) out = TimeDistributed(Dense(num_labels, activation = "softmax"))(model) model = Model(input_word, out) model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 30)] 0 _________________________________________________________________ embedding (Embedding) (None, 30, 50) 2187550 _________________________________________________________________ spatial_dropout1d (SpatialDr (None, 30, 50) 0 _________________________________________________________________ bidirectional (Bidirectional (None, 30, 10) 2240 _________________________________________________________________ time_distributed (TimeDistri (None, 30, 11) 121 ================================================================= Total params: 2,189,911 #LOOK AD THIS NUMBER Trainable params: 2,189,911 Non-trainable params: 0 My accuracy is 98% and loss is 0.07. I like those results, but I have problem with making the prediction, because of the missing words. For example: text = "I live in the Ohio and my name is Alex Wright and I work in AvcCC LTD" text = text.split() text = [word2idx[w] for w in text] text = np.array(text) print(text) text=text.reshape(1,text.shape[0]) max_len = 50 text = pad_sequences(maxlen=max_len, sequences=text, padding="post", value=num_words-1) print('PREDICTION') res = model.predict(text).argmax(axis=-1)[0] print(res) ERROR: KeyError: 'AvcCC' In my dataset, and vocab there are no word 'AvcCC', how to handle that? I want to use that code/model in production. Since my word2idx contains only words that were in starting data, how can I handle words that are not in my word2idx vocabulary? For example, its not possible for my word2idx vocabulary have all names and last names that exists, or all cities/locations, all company names, slang words etc. My vocabulary had around 40k enumerated words (thats the number of unique words in my dataset). Then, I have enriched it with more than 100k other words. (I have made a web crawler that crawled different types of news articles). So now, my vocab has around 140k words. Now, instead of enumerating unique words from dataset, I'm loading my new word2idx/vocabulary. word2idx = open('english-vocab.json') word2idx = json.load(word2idx) max_len = 50 num_words = len(num_words) #number of unique words in dataset X = [[word2idx[w[0]] for w in s] for s in list_of_sentances] X = pad_sequences(maxlen=max_len, sequences=X, padding="post", value=num_words-1) y = [[label2idx[w[1]] for w in s] for s in list_of_sentances] y = pad_sequences(maxlen=max_len, sequences=y, padding="post", value=label2idx["O"]) y = [to_categorical(i, num_classes=num_labels) for i in y] Accuracy and loss remained the same, but my model became much more slower because of the Total params (I can not use num_words anymore because it shows error, I need to use len(word2idx) ) input_word = Input(shape=(max_len,)) model = Embedding(input_dim = len(word2idx), output_dim = 50, input_length = max_len)(input_word) model = SpatialDropout1D(0.2)(model) model = Bidirectional(LSTM(units = 5, return_sequences=True, recurrent_dropout = 0.1))(model) out = TimeDistributed(Dense(num_labels, activation = "softmax"))(model) model = Model(input_word, out) model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_2 (InputLayer) [(None, 30)] 0 _________________________________________________________________ embedding_1 (Embedding) (None, 30, 50) 5596600 _________________________________________________________________ spatial_dropout1d_1 (Spatial (None, 30, 50) 0 _________________________________________________________________ bidirectional_1 (Bidirection (None, 30, 10) 2240 _________________________________________________________________ time_distributed_1 (TimeDist (None, 30, 11) 121 ================================================================= Total params: 7,598,961 # MUCH BIGGER NUMBER Trainable params: 5,598,961 Non-trainable params: 0 With creating my own word2idx I wanted to handle missing words in vocab, but only thing I did is that I slowed down training of my model. How can I handle this kind of problem? How to handle missing/non-existing/unknown words ?
