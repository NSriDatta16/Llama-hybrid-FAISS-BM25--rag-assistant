[site]: crossvalidated
[post_id]: 330660
[parent_id]: 
[tags]: 
Is Monte Carlo cross-validation procedure valid?

I thought K-fold cross-validation consists of the following steps. Split data randomly into $K$ chunks. Fit on $K-1$ chunks. Predict on remaining chunk. Keep predictions. Repeat 2-3 for all remanining $K-1$ combinations of the $K$ chunks that omit 1 chunk. Evaluate Loss statistic that compares all predictions to true values. Now I have seen ( xbart in dbarts package ) the following procedure: Split data randomly into $K$ chunks. Fit on $K-1$ chunks. Predict on remaining chunk. Evaluate loss statistic and keep. Repeat 1-3 $N$ times. Average the $N$ loss statistics or pool in some other way. Note the difference in steps 4 and 5. The first procedure is standard and recommended in major text books. The second procedure seems new. I cannot see immediately why not to do it, but it seems not optimal in terms of variance. Are there arguments in favor or against the second procedure? The second approach is implemented in the package quoted above and I wonder if this is wrong to do.
