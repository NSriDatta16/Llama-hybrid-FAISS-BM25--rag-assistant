[site]: crossvalidated
[post_id]: 252126
[parent_id]: 134401
[tags]: 
Perceptron Learning procedure cannot be generalised to hidden layers • The perceptron convergence procedure works by ensuring that every time the weights change, they get closer to every “generously feasible” set of weights. – This type of guarantee cannot be extended to more complex networks in which the average of two good solutions may be a bad solution. • So “multi-layer” neural networks do not use the perceptron learning procedure. – They should never have been called multi-layer perceptrons. -Reference Coursera.org - Neural net course - Week 3
