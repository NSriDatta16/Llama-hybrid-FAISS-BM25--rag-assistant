[site]: crossvalidated
[post_id]: 477297
[parent_id]: 477291
[tags]: 
By definition, if your observations are independently drawn, they are memoryless in the sense that $P(\text{new event}|\text{past events}) = P(\text{new event})$ . In the context of a binomial random variable, you can think of that as $n$ independent bernoulli's, and so the same concept will hold: by definition of binomial, the $k$ -th experiment is independently of past experiments. You may be confusing memory with issues somewhat similar in flavor to the concept of regression toward the mean . It's not that each individual coin flip (of an equally weighted coin) is aware of the past coin flips and is basing where it lands depending on those past flips to 'even things out', but rather that as you gather more observations, the sample mean is more likely to be closer to the true mean (in this case, the probability of flipping heads). When you have few observations, which happens at the start of a sequence of observations, you'll notice the average may seem quite off from the true average for a single sequence of observations, but as you gather more and more observations, it seems to settle to the true mean. One way of convincing yourself that this is not due to memory is to note that had you taken any similar small number of observations from the end of your sequence of data, you'd more likely observe a similar issue with the mean being far off from the true mean (however, if you even did this over and over, that average would also settle to the true mean!). This is more a reflection of concepts such as the law of large numbers or the fact that for many distributions, as your sample size grows, your estimate of the mean becomes more precise (in some sense, this is exactly what the CLT also says). Edit: To respond to your comment, the point is that the overall mean will (eventually) converge to the true mean, but this says nothing about individual coin tosses. I think the confusion may also stem from confusing a random variable with realized observations. To illustrate, look at the below figures I build (code at end). I sampled 1000 binomial (with $p=.5$ ). On the left, I plot the mean of the first x observations for each x, and on the right, I plot the mean of the last 20 observations for every 20 xs. On the left, you see the law of large numbers kick in: in the long run, the total mean converges to the true probability, but look at the right: there is no pattern at all... sometimes the mean of the next 20 observations is similar to the last one, sometimes totally different! What you may instead see is that if for a given number of observations you happen to randomly observe lots of tails, then if you did it again, just probabilistically, it's unlikely you will again observe lots of tails, since that event is unlikely, but that's not memory, that's just probability. Similarly, if you play the lottery and win, and then next 10 times you play you don't win, its not memory driving that result, that's just probability telling you you won't win, but you're comparing it to an observed win that is extremely unlikely. Code: set.seed(20) n = 1000 samp = rbinom(n,1,.5) tot_mean = sapply(1:n, function(x) mean(samp[1:x])) bin_mean = sapply(1:n, function(x) ifelse(x %% 20 == 0 ,mean(samp[x:(x+20)]),NA)) #now plot both side by side.. require(data.table) require(ggplot2) require(ggpubr) dt = data.table("num" = 1:n, "mean_up_to_x" = tot_mean, "mean_past_20" = bin_mean) g1 = ggplot() + geom_point(aes(x = num, y = mean_up_to_x), data = dt) + geom_hline(yintercept=.5, linetype="dashed", color = "red") g2 = ggplot() + geom_point(aes(x = num, y = mean_past_20), data = dt) + geom_hline(yintercept=.5, linetype="dashed", color = "red") g
