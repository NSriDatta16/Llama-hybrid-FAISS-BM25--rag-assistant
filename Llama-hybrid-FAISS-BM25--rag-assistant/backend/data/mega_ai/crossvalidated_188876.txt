[site]: crossvalidated
[post_id]: 188876
[parent_id]: 188842
[tags]: 
Exchangeability, loosely writing, means you can permute the indices of the random variables in the expression $F(x_1, \dots, x_n)$ without having the result of the probability calculation change. This means, basically, that you can put the observed value of, for example, $x_1$, in where $x_3$ is in the list of values and vice versa (or more complex permutations) without altering the calculated probability. Consider an urn example; 3 black balls and 2 white balls, sampling without replacement. Now let's draw two balls; we get one white and one black. Does the probability of the sequence $(w,b)$ equal that of the sequence $(b,w)$? If so, and if this holds for all sequences and all samples, then the sequence is exchangeable, although the draws themselves are clearly not independent. $P(b,w) = 3/5 * 1/2 = 3/10$ $P(w,b) = 2/5 * 3/4 = 3/10$. If we see $x_1 = w$ and $x_2 = b$, and permute the indices in the probability calculation to (2,1) instead of (1,2), which means we calculate $P(b,w)$ instead of $P(w,b)$, we'll get the same numeric result. The fact that this is universally true in urn models of this sort means that the sequence of draws (from urn models of this sort) is exchangeable. As for why we care, I can hardly do better than to point you to this paper by Bernardo (for the Bayesian perspective.) The tl;dr is that exchangeability is all that's necessary to show the existence of a probability distribution and a prior distribution on the parameter(s) of the probability distribution. So it's pretty fundamental stuff, not something you (directly) use to, e.g., help construct a particular statistical test. To quote: "if a sequence of observations is judged to be exchangeable, then, any finite subset of them is a random sample of some model $p(x_i | \theta)$, and there exists a prior distribution $p(\theta)$ which has to describe the initially available information about the parameter [$\theta$] which labels the model."
