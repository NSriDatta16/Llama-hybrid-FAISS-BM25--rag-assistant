[site]: crossvalidated
[post_id]: 442895
[parent_id]: 
[tags]: 
Is this the correct way of working on an ML problem?

I am working on an assignment where I have been provided a data-set using which I have to do the following: A. The data in the F20 column is missing, I have to use various imputation strategies and then compare the results. B. I have to compare performance of decision tree vs any other two classification techniques for which I am using Naive Bayes and SVM. I have adopted the following strategy to build and evaluate the model: 1.impute Nan in the F20 row 2.balance the classes using SMOTE method 3.using min-max scaler to normalize data between 0 and 1 4.divide the data set into training and testing 80/20 5.use the training data to grid search best parameters with stratified k fold 6.obtain the grid_search._best_score i.e cross validation mean accuracy as a performance metric to compare with other models. 7.also obtain the best_params and set it to fit the model again on complete training data using the best parameters 8.then predict class y_pred using the model for input x_test from testing data set and get the classification report (y_pred, y_test) with accuracy, mean, precision, recall, kappa, etc on the testing data-set. I am mostly wondering about the correctness of steps 6, 7 and 8. Is this the correct way of comparing ML models with each other? Kindly look at my dataset CE802_Ass_2019_Data.csv and my notebook CE802 - CLASSIFICATION COMPARATIVE STUDY.ipynb which can be found here , so that I can be assured I haven't committed any grave sins and would satisfactorily submit this assignment.
