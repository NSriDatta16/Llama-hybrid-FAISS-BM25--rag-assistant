[site]: datascience
[post_id]: 77551
[parent_id]: 77541
[tags]: 
GPT-2 does not use a word-level vocabulary but a subword-level vocabulary, specifically byte-pair encoding (BPE) . This means that it does not predict the next word but the next subword token. BPE tries to find the pieces of words that are most reusable. BPE also keeps character subwords (e.g. "a", "W"). The subword vocabulary used by GPT-2 is 50k tokens big. You can take a look at the vocabulary here . There, you can see that token #12175 is "retro" and token #11147 is "fit", so when tokenizing word "retrofit", you would probably get the 2 subword tokens #68 and #12541. When tokenizing a low-frequency word (i.e. that did not appear in the BPE training data), you probably end up with small subwords tokenization, e.g. "Kilimanjaro" --> "Kil", "iman", "jar", "o". Using a word-level vocabulary is very troublesome, because: the amount of existing word surface forms exceeds the size that it is manageable for a neural net, even for a morphologically simple language like English. For morphologically-rich fusional languages and, especially, agglutinative languages , using word-level vocabularies is much more inconvenient. language is "open", in the sense that new words can be created, either by stitching together existing words (e.g. manspreading), or by making up new words altogether (e.g. bazinga). the less frequently a token appears in the training data, the less our net will learn to use it. If we have many many different tokens, the frequency of appearance of each token in the training data will be very low (i.e. data scarcity), so our net will not learn properly. Therefore, subword vocabularies are the norm nowadays.
