[site]: datascience
[post_id]: 18718
[parent_id]: 18674
[tags]: 
I think I have found the answer, but I'd like to have some validation from the community. Could someone please let me know if this seems like a valid explanation? The reason for the remaining 320M parameters in the input of the encoder , computing the word embeddings requires taking a one-hot vector of 160K words and transforming it into a 1K dimensional word embedding. Thus this accounts for 160M parameters; the softmax layer of the decoder takes a 1K dimensional input (the hidden state of the last layer of LSTM cells) and transforms it into a vector of 80K probabilities. This accounts for 80M parameters; the input of the encoder is given not only by the hidden state of the decoder, but also by the previous output of the encoder. This requires taking a 80K dimensional vector of possible words and transforming it into a 1K dimensional word embedding. Thus this accounts for 80M parameters. Does this explanation make sense? Recurrent connections As far as I understand, once the cell state of the LSTM is computed, it is transferred as is to the next timestep. Thus there are no additional parameters from the recurrent network (such as, e.g., was the case in this post).
