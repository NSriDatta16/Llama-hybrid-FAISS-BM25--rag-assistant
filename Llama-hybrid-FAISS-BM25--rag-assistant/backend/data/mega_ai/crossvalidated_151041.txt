[site]: crossvalidated
[post_id]: 151041
[parent_id]: 149509
[tags]: 
There is a neat way to address this problem. Use validation tecniques. If your method has hyper parameter, SVM has, split your data into 3 groups; training, validation and test. Train your algorithm on training set, find best hyper-params on validation set, and publish the results on test set. You may also consider to use k-fold cross validation. That shows mean of different training settings and variation between them. You may add the method that you mention about in a separate trial and compare results with your previous method. That explicitly shows how each method affects the overall result. Also be careful about methodology. If the algorithm is not specific to the current dataset, use it on another dataset to be sure that nothing is wrong.
