[site]: datascience
[post_id]: 56535
[parent_id]: 55212
[tags]: 
Consider the second row ie. the 16x16x3 example, if you do the sliding window approach you first take the 14x14x3 area marked in red and pass it as input to your trained CNN. What happens then, 5x5x3 filters then convolve the first 5x5x3 of the input ie. starting from top left and then it convolves on the next 5x5x3 section of the same input according to the stride, and so on, just as Convolution happens. Now, consider the 14x14x3 section marked in green, (this will be the second window in Sliding Window Approach) and it like the first red area, goes into your trained CNN and hence is convolved by the same filters that convolved the red one, so far so good. What will be the first part of this green input to be convolved? It would be the same as the second 5x5x3 part to be convolved on the first 14x14x3 green area, so what happens is that the same 5x5x3 area has undergone the same operation ie. same convolution in this case twice, that means you've basically done the same computation again on the same 5x5x3 input, once on the red input and then on the green input. What's the use of doing the same calculation again, that's the reason we use the Convolutional Sliding Window Approach which will do every computation only once and since you aren't doing the same stuff again and again you will save that time and hence the Convolutional Sliding Window Approach will run faster and obviously does the same computation as the normal Sliding Window Approach. Note: Modern implementations of CNN are very deep and involve lots of filters , so you are doing a lot of redundant work which slows down your implementation. Hope it helps.
