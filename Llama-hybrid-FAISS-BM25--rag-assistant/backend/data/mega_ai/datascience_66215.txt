[site]: datascience
[post_id]: 66215
[parent_id]: 66205
[tags]: 
It's not simple, but doable. I suggest you to create training data in the following way: take a text corpus, as large as possibile, and remove words sampled randomly. Then train an seq2seq RNN to map this "deteriorated" text with its original. The RNN you need won't be too different from an NMT model, but it's goal is different of course. It's the first time I encountered this kind of task, therefore I can't say what is the state of the art.
