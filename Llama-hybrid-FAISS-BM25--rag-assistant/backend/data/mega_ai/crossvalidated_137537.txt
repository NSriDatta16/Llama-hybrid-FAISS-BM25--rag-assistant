[site]: crossvalidated
[post_id]: 137537
[parent_id]: 
[tags]: 
What is the architecture of a stacked convolutional autoencoder?

So I am trying to do pretraining on images of humans using convolutional nets. I read the papers ( Paper1 and Paper2 ) and this stackoverflow link , but I am not sure I am understand the structure of the nets (it is not well defined in the papers). Questions: I can have my input followed by a noise layer followed by a conv layer, followed by a pooling layer - there after - do I de-pool before I give my output (which is same a my input image)? Say I have several (135,240) images. If I use 32, (12,21) kernels, followed by (2,2) pooling, I will end up with 32 (62, 110) feature maps. Now do I de-pool to get 32 (124, 220) feature maps and then flatten them? before giving my (135,240) output layer? If I have multiple such conv-pool layers, should I train them one by one - like in stacked denoised autoencoders? Or - can I have something like input-conv-pool-conv-pool-conv-pool-output(output being same as input)? In that case, how is the pooling, depooling supposed to be managed? Should I only de-pool in the last pool layer before output? And again - what should be the resize factor of that de-pooling? Is the intention to bring the feature maps back to the shape of the input? Should I be introducing noise layers after every conv-pool-depool layer? And then when fine tuning - am I supposed to just remove the de-pooling layers and leave the rest the same. Or should I remove both the noise layers and de-pooling layers Can any one point me to a url / paper which has detailed the architecture of such a stacked convolutional auto encoder to do pre training on images?
