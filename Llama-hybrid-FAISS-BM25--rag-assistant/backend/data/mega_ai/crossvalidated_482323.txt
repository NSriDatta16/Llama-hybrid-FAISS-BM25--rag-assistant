[site]: crossvalidated
[post_id]: 482323
[parent_id]: 480953
[tags]: 
You might be able to use a machine learning model (i.e. a neural network) to predict the noise and then obtain the clean signal by difference, or directly predict the clean signal. So this might be an interesting domain to look at. More in detail, suppose that you train a model using as input the noise_plus_target at t-n , ..., t-1 and as target for the prediction the noise_only at t . If the prediction is good (or, if your model can learn the patterns of the noise in the noise_plus_target signal), then you should be able to predict the noise at t just from noise_plus_target . Now, if you subtract the prediction of the noise form your actual recording at t you should get the signal. Instead of thinking about t-n , ..., t-1 you could also think about windows and work in the frequency domain for your prediction (window by window), which might help the model to some extent since you would use the FFT as a feature extraction technique, basically. To better answer the original question, search terms for literature would be around words like "neural networks noise reduction", "machine learning noise reduction", and permutations on the theme (i.e. "recurrent neural networks", "convolutional neural networks", "LSTM" etc. + "noise reduction"). For example, with these keywords I found some interesting work carried out by Google Research and co-authored by Andrew Ng himself [1]: "We introduce a model which uses a deep recurrent auto encoder neural network to denoise input features for robust ASR. The model is trained on stereo (noisy and clean) audio features to predict clean features given noisy input" Hope this helps! [1] https://research.google/pubs/pub45168/
