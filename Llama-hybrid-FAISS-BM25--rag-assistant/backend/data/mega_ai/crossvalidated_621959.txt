[site]: crossvalidated
[post_id]: 621959
[parent_id]: 621952
[tags]: 
To assess the ranking of ranking models, it is common to use metrics such as precision and recall . Another very interesting metric is the F1-score, which provides an average (harmonic) between precision and recall. There are a number of metrics to measure ranking that allow you to compare 2 models like the ones you are using. There is no single metric. Generally, the problem you are working on will require either high accuracy (low errors, eg plant species classification) or high recall (low false positives, eg disease diagnosis). As there is a trade-off between these two metrics, what they usually do is evaluate the scenario, seeking greater accuracy or recall. The same goes for other metrics. On this site there is a list of metrics, many quite common, others more specific that they evaluate considering global and local aspects, data with the presence of outliers, etc: https://scikit-learn.org/stable/modules/model_evaluation.html Additionally, to measure dissimilarity between classes, you can use some dissimilarity metric like Jaccard index . One way of applying precision/recall/f1 to analyze each of the classes would be to analyze its support (number of occurrences of each classes in the set of predictions made). For example, with sklearn.metrics.precision_recall_fscore_support : If the metric is (here best value is 1 and worst is 0) precision_recall_fscore_support(y_true, y_pred, average='micro') = (0.3., 0.3, 0.5). we could say that there were similar mistakes for samples i=0, 1 (both given 0.3) but not for the i=2 . For individual samples , you can apply the Jacard index. For example, jaccard_score(y_true[k], y_pred[k]) = 0.6 will be better than the prediction that has jaccard_score(y_true[j], y_pred[j]) = 0.3. If jaccard_score(y_true[q], y_pred[q]) = jaccard_score(y_true[p], y_pred[p]), then samples corresponding to index p and q ` had similar mistakes.
