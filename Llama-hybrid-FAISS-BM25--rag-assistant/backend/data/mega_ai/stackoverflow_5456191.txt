[site]: stackoverflow
[post_id]: 5456191
[parent_id]: 2271670
[tags]: 
First off, way too many workers, and limits set excessively high. The max worker count for php-fpm alone would bog your server down quite a bit. Uncapping the limits on a server won't necessarily speed it up but may actually have the opposite effect. Worker Count: 20 makes little sense if you do not have a 20 processor/core machine, you're actually causing a negative effect as the workers will have excessive content swapping. If you're running a dual core processor, 2 workers should suffice. Worker Connections: Again, just throwing a limit into the heavens doesn't solve your problems. If your ulimit -n output is something like 1024, then your worker connections would need to be set to 1024 or less (maybe even 768), its unlikely that you'll have 2 x 1024 simultaneous connections especially with something like PHP. Root location, and PHP settings, refer to http://wiki.nginx.org/Pitfalls , it works best if you put your root directive at the server {} level, not the location level. Once you do that you can use $document_root$fastcgi_script_name as the SCRIPT_FILENAME value as $document_root will be automatically propagated to location blocks below it. You may wish to handle static files directly, in other words: location ~* \.(ico|css|js|gif|jpe?g|png)$ { expires max; add_header Pragma public; add_header Cache-Control "public, must-revalidate, proxy-revalidate"; } Use a PHP Accelerator, namely APC (with apc.enabled=1 in php.ini) or XCache, and be mindful of your php settings, such as the memory_limit. For example if you only have a system with 2GB of rams, it makes very little sense to allow 500 workers with a limit of 128MB each. Especially true if you're also running other services on your server.
