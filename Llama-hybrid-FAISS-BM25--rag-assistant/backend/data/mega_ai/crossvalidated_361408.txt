[site]: crossvalidated
[post_id]: 361408
[parent_id]: 
[tags]: 
Calculating the value function by integral in reinforcement learning

In reinforcement learning, the value function of a state is defined as its expected return, or $ v_{\pi} (s) = \mathbb{E} \left[ G_t | S_t = s \right]$, where $\pi$ is a policy and $G_t$ the return. I know that we can express the value of a state $v_\pi(s)$ in terms of the value of other states $v_\pi(s')$ , via the Bellman equation. However, I got stuck when I tried to formulate this expectation in integral form. Just like $\mathbb {E}(X) = \int x f(x) dx$, I calculated $\mathbb{E} \left[ G_t | S_t = s \right]$ and got $\int g_t f(g_t)dg_t$. I was confused by two things: The distribution $f(g_t)$ refers to the probability distribution of the values the return $G_t$ can take at time $t$. How would you find this? How does this distribution link to the probability distribution of the policy $\pi(a|s)$?
