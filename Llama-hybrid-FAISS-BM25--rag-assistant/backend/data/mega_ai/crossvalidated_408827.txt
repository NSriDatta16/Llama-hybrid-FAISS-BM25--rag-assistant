[site]: crossvalidated
[post_id]: 408827
[parent_id]: 408516
[tags]: 
I don't know what you mean that "each bootstrap sample will contain an extra degree of uncertainty" outside of your choice to alter the optimization routine for the bootstrapped samples. The bootstrap sample itself is considered a fixed dataset. It is like the original unbootstrapped dataset in the sense that the bootstrap represents what you might observe if you redo the experiment. Your choice to reduce the number of steps in the optimization routine for the bootstrapped dataset will introduce MCMC error. Perhaps not an issue if the MCMC error is sufficiently small: the bootstrap is preferred not because it is efficient. Unless it is a complicated EM algorithm at each step, it's unusual to require 100 or more steps to maximize a likelihood within a very acceptable tolerance. The MCMC error does not need to be "accounted for". You just need to assess its extent and be sure it's a negligible fraction of the estimator error. I would be more concerned that you are not using the appropriate bootstrap approach to performing inference and calculating confidence intervals (unless you are doing a studentized bootstrap). Precisely one of the strengths of the bootstrap is that it does not require the sampling distribution of the statistic to be normal. You should consider quantile based intervals, studentized, double bootstrap, or BCA.
