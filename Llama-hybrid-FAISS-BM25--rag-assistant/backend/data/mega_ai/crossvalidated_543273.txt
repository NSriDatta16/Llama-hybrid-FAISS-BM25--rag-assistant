[site]: crossvalidated
[post_id]: 543273
[parent_id]: 543264
[tags]: 
Previous questions have already asked about the details of the algorithm, e.g. this one , and Wikipedia also describes it decently . Where your understanding is not quite right: If we knew $Z$ , this would all be a lot easier. We typically use the Metropolis-Hastings algorithm when we do not know $Z$ . "For initialization," we cannot "take a random sample from : $P(\theta | \text{data} )$ ." If we knew how to do this, we would just keep drawing samples from this distribution (instead of using the Metropolis-Hastings algorithm to do it). Thus, for initialization, we typically take some random point in parameter space that makes sense (e.g. randomly picked in some sense in the middle of the parameter space). Even if the distribution of interest lies somewhere else, we hope the MH algorithm will eventually get there. To check this does not go wrong/have too much of an effect, it's good (not just for that reason) to have multiple chains started with different initializations. We typically center the proposal distribution (which you call $g$ ) on the current parameter value $\theta_i$ to propose a new $\theta_{i+1}^*$ . I.e. it is often a multivariate normal (because $\theta$ is usually not one-dimensional, but rather a vector) centered at the current $\theta_i$ . The covariance matrix (i.e. both the standard deviations for each dimension and the correlations between proposals in all dimensions) usually get tuned during some initial warmup steps. Some initial hundreds or thousands of samples only get used for that purpose and then thrown away. Ideally, our proposal distribution would locally resemble the shape of the target distribution we are trying to sample from, but that's hard to achieve in some cases, but a multivariate normal is at least after some tuning capable of approximating various situations decently. We initially have to start with some distribution and one way of going is to make a it normal with a large variance. You are aiming for about the right acceptance rate for samples (about 23% for various theoretical reasons). So, if you variance is very large and you get hardly any acceptances, you can start decreasing the variance to see whether that gets better. I.e. the multivariate normal with a large variance is just a starting point. Making a proposal with a normal distribution center on the previous value also satisfies (some of) the requirements for the Markov chain to converge the posterior of interest (it's of course not guaranteed that it will). Because we, in general, do not know $Z$ , $A(\theta_i, \theta_{i+1}^*)$ cannot involve $Z$ . However, why the MH algorithm is cool is, because it works without Z. What we do is look at $A(\theta_i, \theta_{i+1}^*) = \min(1, \frac{\text{likelihood}(\text{data}|\theta_i) \times \text{prior}(\theta_i)}{\text{likelihood}(\text{data}|\theta_{i+1}^*) \times \text{prior}(\theta_{i+1}^*)})$ . Then, as you say, we compare a uniform random number $U$ with this and if it's greater or equal to $U$ , we set $\theta_{i+1} = \theta_{i+1}^*$ and $\theta_{i+1} = \theta_{i}$ otherwise. I'm not sure what people would have used before MH, so not sure what it's an improvement over. But, yes, it does some sensible things: it always moves to a proposal that has a higher value of $\text{likelihood}(\text{data}|\theta_i) \times \text{prior}(\theta_i)$ than the previous parameter value, but will also sometimes move towards lower values (otherwise you'd just move towards the maximum-a-posteriori estimate and not explore the full distribution). The sequence of points $\theta_1, \theta_2, \ldots$ is what is referred to as a Markov Chain and each value $\theta_i$ is the $i$ th state of the chain. The MH algorithm as described cannot really deal with discrete parameters (of course you can extend it to do so). Additionally, it will sometimes be pretty inefficient (or in some situations completely unreliable). In some of these situations a Gibbs sampler can be better. Gibbs sampling (when possible) is often more efficient (i.e. better samples with less time used) than MH, but theoretically MH is still possible. Gibbs samplers exploit situations, where you know how to directly sample parts of your parameter vector $\theta$ directly from the conditional posterior having fixed the remainder of the parameter vector. That usually happens when you work with a setup that uses conjugacy. And there's many interesting hybrid samplers (e.g. Metropolis-Hastings-within-Gibbs samplers) for situations where for part of your parameter space you can use Gibbs and for part of it you can't.
