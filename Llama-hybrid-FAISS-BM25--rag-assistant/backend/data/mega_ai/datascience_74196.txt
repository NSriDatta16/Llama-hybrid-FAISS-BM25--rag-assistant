[site]: datascience
[post_id]: 74196
[parent_id]: 
[tags]: 
What is wrong with a neural network model which is so dependent on the seed of initialization?

I have a fully-connected neural network with one hidden layer with 2 units which its goal is to classify a dataset with 324 samples and 300 features. 50% of the dataset is used for train and 50% of it is used for the test. There are two classes in the dataset which one of them has 75 samples and the other one has 249 samples. When I train the model with different seeds (actually when I re-run the training function), its results on the test set have a high variance, and the accuracy of the classification for test set changes about 20%. What is wrong with the model? How can I make it stable? How can I report the results of it?
