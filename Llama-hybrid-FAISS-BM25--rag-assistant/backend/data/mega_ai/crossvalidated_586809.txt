[site]: crossvalidated
[post_id]: 586809
[parent_id]: 
[tags]: 
Understanding the loss funcion in DeepAR

The loss function looks like below, where N is the number of time series. ignore N for now and say N = 1. T is length of prediction horizon. During training $t_0$ starts from encoder time step 0 all the way to last decoder time step $T$ . $$\mathcal L= \sum_{i=1}^N\sum_{t =t_0}^T \ln \ell(z_{i, t}|\theta(\mathbf h_{i, t})) $$ \begin{align} \ell_\textrm{NB}(z|\mu, \alpha) &= \frac{\Gamma\left(z+\frac1\alpha\right)}{\Gamma\left(z+1\right) \Gamma\left(\frac1\alpha\right) }\left(\frac{1}{1+\alpha\mu}\right) ^\frac{1}{\alpha}\left(\frac{\alpha\mu}{1+\alpha\mu}\right)^z\\ \mu(\mathbf h_{i, t}) &= \ln (1+ \exp(\mathbf w_\mu^T\mathbf h_{i, t}+b_\mu)) \end{align} and $$\alpha( \mathbf h_{i, t}) =\ln(1+ \exp(\mathbf w_\alpha^T\mathbf h_{i, t}+b_\alpha)) .$$ How does the sum of individual log of negative binomial (nbinom ) likelihood loss function at each time step contribute to the convergence of nbinom parameters? How does maximizing individual time steps likelihood result in convergence of nbinom paramters? e.g. say an input to a time step is 1. the paramters (5,0.75) maximize it like in figure below. However because the input can come from any sub-sequence it could have come from another distribution (10,0.5) for this time step. The encoder input comes from any sub-sequence of the time series (i.e. phase shifted) so the nbinom parameters learnt cannot associate with a fixed time step. So during the initial stages of training how does the network know the right nbinom parameter to update by first detecting the phase of the encoder input? Can it detect the phase of the encoder input?
