[site]: crossvalidated
[post_id]: 484195
[parent_id]: 484157
[tags]: 
It will have the desired out of reducing your feature space, but why not just use the output from the L1 with some tuning of the reg. parameter? Does it not provide good cv accuracy? Are you just taking the parameters chosen and putting it into another model? Do you require even more dimensionality reduction? Exhaustive search will be extremely computationally expensive depending on the number of params. Maybe look at methods which will handle tons of redundant variables naturally such as random forests or boosted trees if you need prediction accuracy. If your goal is explanatory power, I personally do not like exhaustive methods for variable selection and recommend going the bayesian route which have a lot of nice sparse priors to handle this scenario.
