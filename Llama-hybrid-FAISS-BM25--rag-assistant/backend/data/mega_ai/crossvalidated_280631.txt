[site]: crossvalidated
[post_id]: 280631
[parent_id]: 280628
[tags]: 
Both tests are used, but both are also being supplanted by better modeling techniques. The BP test is designed more for regression, but can be adapted to ANOVA type situations. The problem is that both tests are usually used as All or Nothing tests and what do you do when the p-value is just slightly bigger than your cut-off? Are you really confident that you can continue testing assuming that the variances are perfectly equal? A big question is why do you feel the need to test for homoscedasticity? If it is because it is on the list of things you learned to test for in a regression class, then this is usually not a good reason. If you test for everything that could possibly go wrong then you are running into the problems of multiple testing. Are the one or two tests that rejected really identifying a problem with the assumption? or were they significant by chance because of the large number of different "diagnostic test"s that you performed? Is it because a diagnostic plot is suggestive of hereroscedasticity? If you use the same data (residuals) to test this, that suggested the problem in the first place, then the assumptions do not hold and your p-values will be biased (a less obvious multiple comparison problem). Much better to consider the science behind your data. Is there really a reason why the variances should be expected to differ? (for that matter is there really a reason why they should be expected to be the same?) If the science suggests that a meaningful difference is likely or even just reasonable, then I would skip the tests and directly include the possibility of different variances in the model that you fit. Tools that fit mixed effects models can often be used to fit fixed effects models with varying variances as well, or you can use a Bayesian approach and directly specify how the variances can differ. Bootstrapping can also be used (where you resample within the groups to preserve the variances).
