[site]: datascience
[post_id]: 34237
[parent_id]: 
[tags]: 
Updating weights python for REINFORCE policy gradient method

All, I am trying to implement REINFORCE(williams) algorithm. This is a policy gradient reinforcement learning algorithm. I am using python, and hope to use keras. The pseduocode I am using is as follows: from the initial state, run a simulation of the policy, till end. the actions taken . are generated from (in my scenario) a NN. We update the weights by: learning rate * derivative(log policy) * V We are using V as an unbiased sample of Q. V is defined as the average reward being at a particular state. In keras, we can build a neural network, and then update the weights by taking weights + (log of actions * V)? Is this correct? How would I manually update the weights, I appreciate that you can extract the weights from keras sequential models, but there is also a derivative function. Maybe I have misunderstood the algorithm, any thoughts would be greatly appreciated.
