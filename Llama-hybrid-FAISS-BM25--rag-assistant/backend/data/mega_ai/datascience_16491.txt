[site]: datascience
[post_id]: 16491
[parent_id]: 
[tags]: 
Python svm classification, result vs amount of features not as expected

After helpful advice from here, i have started my ML journey with SVM. Initially i started with 15 features and after cross validation i got about 40% accuracy. Problem is when i decided to just load all the features of the data available (205 in total) there appears to be no change at all. Am i doing something wrong? I tried removing all but 3 (many combinations) of the features and got the same results. Here is my code. Thank you for any help. import numpy as np from sklearn import svm, preprocessing from sklearn.model_selection import StratifiedShuffleSplit import pandas as pd FEATURES = ['b1', 'b2', 'b3', 'b4', 'b5', 'b6', 'b7', 'b8', 'b9', 'b10', 'b11', 'b12', 'b13', 'b14', 'b15', 'r1', 'r2', 'r3', 'r4', 'r5', 'r6', 'r7', 'r8', 'r9', 'r10', 'r11', 'r12', 'r13', 'r14', 'r15', 'd1', 'd2', 'd3', 'd4', 'd5', 'd6', 'd7', 'd8', 'd9', 'd10', 'd11', 'd12', 'd13', 'd14', 'd15', 'j1', 'j2', 'j3', 'j4', 'j5', 'j6', 'j7', 'j8', 'j9', 'j10', 'j11', 'j12', 'j13', 'j14', 'j15', 't1', 't2', 't3', 't4', 't5', 't6', 't7', 't8', 't9', 't10', 't11', 't12', 't13', 't14', 't15', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9', 'c10', 'c11', 'c12', 'c13', 'c14', 'c15', 's1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14', 's15', 'a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'a7', 'a8', 'a9', 'a10', 'a11', 'a12', 'a13', 'a14', 'a15', 'pm1', 'pm2', 'pm3', 'pm4', 'pm5', 'pm6', 'pm7', 'pm8', 'pm9', 'pm10', 'pm11', 'pm12', 'pm13', 'pm14', 'pm15', 'w1', 'w2', 'w3', 'w4', 'w5', 'w6', 'w7', 'w8', 'w9', 'w10', 'w11', 'w12', 'w13', 'w14', 'w15', '1l1', '1l2', '1l3', '1l4', '1l5', '1l6', '1l7', '1l8', '1l9', '1l10', '1l11', '1l12', '1l13', '1l14', '1l15', 'pp1', 'pp2', 'pp3', 'pp4', 'pp5', 'pp6', 'pp7', 'pp8', 'pp9', 'pp10', 'pp11', 'pp12', 'pp13', 'pp14', 'pp15', 'rs1', 'rs2', 'rs3', 'rs4', 'rs5', 'rs6', 'rs7', 'rs8', 'rs9', 'rs10', 'rs11', 'rs12', 'rs13', 'rs14', 'rs15', 'dis', 'ral', 'str', 'tot', 'std', 'rstd1'] def Predict(): data_df = pd.DataFrame.from_csv("DATA.csv") data_df = data_df.reindex(np.random.permutation(data_df.index)) X = np.array(data_df[FEATURES].values) y = (data_df["aares"].values) X = preprocessing.scale(X) sss = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=0) sss.get_n_splits(X, y) for train_index, test_index in sss.split(X, y): #print("TRAIN:", train_index, "TEST:", test_index) X_train, X_test = X[train_index], X[test_index] y_train, y_test = y[train_index], y[test_index] clf = svm.SVC(kernel='rbf', C=1).fit(X_train, y_train) print clf.score(X_test, y_test) return X,y Predict() EDIT: I am not necessarily expecting a better result, it seems counter-intuitive that such a large addition of features wouldn't have more of an impact.
