[site]: crossvalidated
[post_id]: 515579
[parent_id]: 442352
[tags]: 
Latent space is a vector space spanned by the latent variables . Latent variables are variables which are not directly observable, but which are $-$ up to the level of noise $-$ sufficient to describe the data. I.e. the observable variables can be derived (computed) from the latent ones. Let me use this image, adapted from GeeksforGeeks , to visualise the idea: Each observable data point has four visible features: the $x, y,$ and $z$ -coordinates, and the colour. However, each point is uniquely determined by a single latent variable, $\varphi$ ( phi in the python code). phi = np.linspace(0, 1, 100) # the latent variable x = phi * np.sin(25 * phi) # 1st observable: x-coordinate y = np.exp(phi) * np.cos(25 * phi) # 2nd observable: y-coordinate z = np.sqrt(phi) # 3rd observable: z-coordinate c = x + y # 4th observable: colour This is, of course, just a toy example. In practice, you often have many, maybe even millions of observable variables (think of pixel values in images), but they can be sufficiently well computed from a much smaller set of latent variables. In such cases it may be useful to perform some kind of dimensionality reduction. As a real-world example, consider spectra of light-emitting objects, like stars. A spectrum is a long vector of values, light intensities at many different wavelengths. Modern spectrometers measure the intensity at thousands of wavelengths. However, each spectrum can be quite well described by the star's temperature (through the black body radiation law) and the concentration of different elements (for the absorption lines). These are likely to be way less then thousands, maybe only a dozen or two. That would be a low dimensional latent space . Note, however, that it's not necessary for the latent space to be smaller than the observable space. It is completely conceivable for many latent variables to influence few observable ones. For example, the value of a particular share at the stock market at a certain point in time is a single value, but it is likely due to many influences which are mostly unknown. In machine learning I've seen people using high dimensional latent space to denote a feature space induced by some non-linear data transformation which increases the dimensionality of the data. The idea (or the hope) is to achieve linear separability (for classification) or linearity (for regression) of the transformed data. For example, support vector machines use the kernel trick to transform the data, but the transformation is only implicit, given by the kernel function. Such data are " latent " in the sense that you (or the algorithm) never know their values; you only know the dot products of pairs of points.
