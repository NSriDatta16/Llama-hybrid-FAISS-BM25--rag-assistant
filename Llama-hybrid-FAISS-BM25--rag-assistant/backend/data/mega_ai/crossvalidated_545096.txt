[site]: crossvalidated
[post_id]: 545096
[parent_id]: 545092
[tags]: 
Bayes optimal classifier assumes that you know the true distribution. The predictions are optimal because you do the maximization in terms of the true distribution. In any other case, you don't know the true distribution, you need to learn or approximate it, hence no classifier can be better. The classifier is not concerned about the true hypothesis not being in $H$ , because, by definition, the true hypothesis distribution is $H$ . Bayesian model averaging averages different models according to their posterior probabilities. Posterior probabilities are estimated from the data and the prior. Here we are talking about distributions that are estimated, rather than known. It cannot be better than the Bayes optimal classifier, because at best it could learn the distribution exactly and achieve the same performance. There are many things with "Bayesian" in their names, for example, naive Bayes classifier , Bayesian statistical models, Bayesian networks, Bayesian neural networks, etc, that have in common that they somehow use probabilistic models and Bayes theorem, but have nothing to do with Bayes optimal classifier.
