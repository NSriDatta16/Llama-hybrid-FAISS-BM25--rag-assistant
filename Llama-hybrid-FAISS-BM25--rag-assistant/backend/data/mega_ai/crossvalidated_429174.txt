[site]: crossvalidated
[post_id]: 429174
[parent_id]: 
[tags]: 
analogy on Tenenbaum's phd thesis (on prior, likelihood, posterior)

This is from the book: Machine Learning from a Probabilistic Perspective page 69 and 70. There is a very interesting analogy/explanation on how to visualise Prior, Likelihood and Posterior Probabilities based on the thesis of Josh Tenenbaum (1999). So the experiment/game goes something like this: I pick a 'simple' mathematical concept which I won't reveal to you. Then, I will verbally say what number is a positive example of the concept. You then need to identify if the next number I will say is part of the concept/rule that I made up. So in Figures 3.2 and 3.3, the prior appears to be the possibilities of the concept or rule that isn't known to the players (but known only to the person who created the game). Ok I understand this. I dont understand what the 'likelihood' is. Are those the probabilities that a number is selected GIVEN the data to be 16? or are they just the probabilities based on the prior (the rules or concept that we intially thought about).
