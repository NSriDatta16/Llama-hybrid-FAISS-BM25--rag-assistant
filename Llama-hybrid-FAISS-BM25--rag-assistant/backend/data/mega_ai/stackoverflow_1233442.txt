[site]: stackoverflow
[post_id]: 1233442
[parent_id]: 1233366
[tags]: 
Well, your question is a little bit vague still. In most cases, a "crawler" is going to just find data on the web in a text-format, and process it for storage, parsing, etc. The "Facebook Screenshot" thing is a different beast entirely. If you're just looking for a web based crawler, there are several libraries that can be used to traverse the DOM of a web page very easily, and can grab content that you're looking for. If you're using Python, try Beautiful Soup If you're using Ruby, try hpricot If you want the entire contents of a webpage for processing at a later date, simply get and store everything underneat the "html" tag. Here's a BeautifulSoup example to get all the links off a page: require 'hpricot' require 'open-uri' doc = Hpricot(open("http://www.stackoverflow.com")) (doc/"a").each do |link| puts link.attributes['href'] end Edit: If you're going to primarily be grabbing content from the same sites (e.g. the comments section of Reddit, questions from StackOverflow, Digg links, etc) you can hardcode the format of them so your crawler can say, "Ok, I'm on Reddit, get everything with the class of 'thing'. You can also give it a list of default things to look for, such as divs with class/id of "main", "content", "center", etc.
