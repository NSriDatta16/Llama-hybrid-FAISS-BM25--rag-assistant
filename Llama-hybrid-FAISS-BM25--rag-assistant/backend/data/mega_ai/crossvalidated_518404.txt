[site]: crossvalidated
[post_id]: 518404
[parent_id]: 518346
[tags]: 
Lots of real-world sequences have different lengths. A very common example occurs when using RNNs for language modeling. Words, sentences, paragraphs and documents are all variable-length sequences because there's no requirement that any two sentences, paragraphs or documents have the same length. For fully-connected networks, variable-length sequences can be a challenge because standard FCNs require fixed-length inputs. But the nice thing about RNNs is that each element of the sequence is processed one at a time, so the model can naturally be adapted to variable-length sequences. Usually, this means that the sequences are processed all the way to the end, and then all time-steps larger than the length of the input are masked.
