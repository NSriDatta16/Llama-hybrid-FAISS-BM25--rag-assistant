[site]: crossvalidated
[post_id]: 278003
[parent_id]: 264630
[tags]: 
The equivalence of MAP estimates and Lasso/Ridge estimates holds by the following argument whenever we're dealing with exponential family likelihoods (which GLMs have). You can see this by just multiplying the prior and the likelihood. For exponential-family likelihoods, we can write (from Wikipedia) \begin{equation} \pi(\mathcal{D}|\beta) = \exp\left(\sum_i\eta(\beta)T(y_i) - A(\beta) + B(y_i)\right). \end{equation} And since the parameters are assumed independent, \begin{equation} \pi(\beta) = \prod_{j = 1}^p \pi(\beta_j). \end{equation} So if we choose $\pi(\beta_j) \propto \exp(-\lambda p(\beta_j))$, the posterior \begin{align} \pi(\beta|\mathcal{D}) &\propto \exp\left(\sum_i\eta(\beta)T(y_i) - A(\beta) + B(y_i)\right) \prod_{j = 1}^p \exp(-\lambda p(\beta_j)) \\ &= \exp\left(\sum_i\eta(\beta)T(y_i) - A(\beta) + B(y_i) - \lambda \sum_{j = 1}^pp(\beta_j)\right) \\ &= \exp\left(\ell(\beta) - \lambda \sum_{j = 1}^pp(\beta_j)\right) \end{align} is maximised at $\hat{\beta}_{pen}$ by definition (i.e. the MAP is $\hat{\beta}_{pen}$). Plugging in the different choices for $p$ gets you normal/Laplace priors. Can you take it from here? Additional note: If you are interested in Bayesian shrinkage, have a look at the horseshoe prior (Carvalho, 2010) and papers that cite that paper.
