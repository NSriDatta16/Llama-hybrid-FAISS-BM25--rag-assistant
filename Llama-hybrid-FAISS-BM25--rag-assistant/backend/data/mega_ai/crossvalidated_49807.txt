[site]: crossvalidated
[post_id]: 49807
[parent_id]: 49703
[tags]: 
To expand on @Glen_b's answer. A beta distribution is the standard answer here, although it should be noted that it is possible (and also not uncommon) to not try to smooth/model the prior distribution at all but just use some raw point estimates... putting that aside, let us say you think the probability of an occurance can be anywhere between 0 and 1 but you have an expert guess of the chance of that probability being correct for each of 0.1, 0.2, etc. That is, your expert (perhaps you) thinks the chance that that probability is 0.1 is only 0.01; the chance it really is 0.2 is 0.015, and so on. You can use the method of moments as neatly set out for you in the Wikipedia article on the beta distribution . Here is some code to get you started: expert That is, the prior estimates of the chance of each probability are: > expert probs prior 1 0.1 0.01 2 0.2 0.15 3 0.3 0.30 4 0.4 0.20 5 0.5 0.14 6 0.6 0.10 7 0.7 0.05 8 0.8 0.04 9 0.9 0.01 So now to use the method of moments to turn this into a beta distribution that can summarise it: u You can see from the image below that the beta distribution is an ok summary of the expert's opinion but it isn't quite the same - basically, the expert's view on the chance of 0.3 being the true probability are a bit of a spike in their prior distribution. From here you can use Bayesian techniques to confront your prior distribution with the likelihood of the actual data, and hence produce a posterior distribution that combines the two.
