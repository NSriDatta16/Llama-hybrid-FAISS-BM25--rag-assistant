[site]: crossvalidated
[post_id]: 340601
[parent_id]: 
[tags]: 
Free lunch Autoencoder? Data dimensionality reduction

I came across Autoencoders, and saw one example were no activation is used - it's simply a linear transform to lower dimension and then back up $$ B(Ax+a)+b=x$$ with $x\in \mathbb{R}^d$ and $A\in \mathbb{R}^{m \times d}$ But wouldn't the exact solution be $a=\vec{0}, \ b=\vec{0}$ with $B$ the left inverse of $A$ (for any $A$ that has a left inverse)? Or if $A$ satisfies $A^TA=I_d$ then just make $B=A^T$? Seems like any of these solutions would be a perfect way to "autoencode" $x$ without loss of information. So $Ax$ would be a lower dimensional representation of the data, and at any time it can be brought back to $\mathbb{R}^d$ by left multiplying by $B$. Sounds like free lunch? Or is this a useless transformation? I'm asking this because in the tutorial I'm doing they're using TensorFlow to solve for $A$ and $B$ by gradient descent, but why not just use my proposed solution?
