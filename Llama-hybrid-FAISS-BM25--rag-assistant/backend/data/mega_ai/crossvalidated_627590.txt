[site]: crossvalidated
[post_id]: 627590
[parent_id]: 514645
[tags]: 
(At least for MSE loss) The optimal hyperparameters $\lambda, \alpha, \gamma$ depend on the scale of your target, but in a relatively predictable way. If $y'=C y$ , then using just one regularization, we have $\hat\lambda'=\hat\lambda$ , $\hat\alpha'\approx C\hat\alpha$ , or $\hat\gamma'=C^2\hat\gamma$ . (When more than one are used, the $\approx$ in $\hat\alpha'$ means the others could change too. I wouldn't imagine it's necessarily better or worse.) A bit of intuition in response to your last paragraph: the $w_j$ indeed change dramatically on scaling the target variable, but so do the gradients of the loss. You're adding the leaf scores onto the sum so far among earlier trees, and the difference between the target values and that partial sum will have been scaled similarly, so you also would like to see the new leaf scores to be scaled similarly. To put that more formally: With loss = $\frac12\sum (p-y)^2$ with $p$ the prediction and $y$ the target, we have $G=\sum (p-y)$ and $H=1$ . I claim that, without L1 or L0 regularization, we have $p'=Cp$ . Before beginning boosting, an initial prediction is made, generally a constant one, and either $p\equiv0$ or $p\equiv \bar{y}$ , and those both scale by $C$ . (I seem to remember something about $p\equiv0.5$ ?...) So, the gradients used for training the first tree are scaled by $C$ , and the hessians remain constant; and so the optimal weight at leaf, $G/(H+\lambda)$ , is also scaled by $C$ , and when added to the base score the total prediction for every row has also been scaled by $C$ . $\alpha$ and $\gamma$ do have an effect though. The optimal weight with $\alpha>0$ turns out to be (I think) $$-\frac{\operatorname{sgn}(G)\cdot (G-\alpha)^+}{H+\lambda}$$ When we scale $y$ and hence $G$ by $C$ but don't change $\alpha$ , the numerator changes significantly. If we scale $\alpha$ by $C$ , then the numerator still doesn't strictly scale by $C$ , although it will be much closer. $\gamma$ doesn't affect leaf scores, but determines whether a further split is made. A little later in the tutorial the gain of a split is given; if the gain from a split fails to exceed $\gamma$ , then no split will be made. But the gain of a split is a linear combination of the nodes' values of $$\frac{G^2}{H+\lambda},$$ and so similar to above is scaled by $C^2$ , and so a corresponding $\gamma$ threshold needs to also be scaled by $C^2$ . Here's a Colab notebook with some experiments.
