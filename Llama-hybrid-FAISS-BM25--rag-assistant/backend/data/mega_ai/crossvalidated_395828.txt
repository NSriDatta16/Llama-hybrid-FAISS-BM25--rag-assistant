[site]: crossvalidated
[post_id]: 395828
[parent_id]: 395652
[tags]: 
The LSTM prediction $y^{(t)}$ is a function $f$ of the previous hidden state data $h^{(t-1)}$ and the previous step's observed data $x^{(t-1)}$ . $$ \hat{x}^{(t)}, h^{(t)} = f\left(x^{(t-1)}, h^{(t-1)}\right) $$ To model dependencies $k$ steps in the past, you need to compute your hidden state to be inclusive of that long-term data. First, initialize the hidden state (usually people initialize with 0s). Then, compute the hidden state $k$ steps into the future using the observed $x$ values and your newly-computed hidden values. At step $k$ , you have a hidden state which has been updated to store data about steps $0, 1, ..., k-1$ . The key idea is that $h^{(t)}$ contains all relevant data about the past, and $x^{(t-1)}$ tells you about the present. If the model is any good, then $f\left(x^{(t-1)}, h^{(t-1)}\right)$ will be a reasonable prediction about the future. You can make 1-step-ahead predictions using the observed $x$ . If you need $m$ -step-ahead predictions, then you'll have to use your predicted $\hat{x}$ s as the inputs instead of observed values $x$ . You always need to have 2 things to use an LSTM: the new observed data, and the hidden state. This is true during training time and testing time. To train an LSTM, the simplest method is to start at the beginning of your training data (the earliest data) and move forward, back-propagating at each time step. Unlike for an MLP, you can't shuffle the data because you need the hidden state from $t-1$ , and that hidden state is dependent on the previous time step and so on back to the start of your time series. The term of art for RNN training is back-propagation through time (BPTT). There are a few different "twists" on this basic idea. Two are mini-batches and using longer time sequences than 1 step. Using mini-batches can improve computation time. The way to do this is to divide your time-series into some number of contiguous, equal-length chunks. You'll lose a little information when predicting at the beginning of each chunk. This is because the time steps on the boundaries of chunks won't be able to "see" the true hidden state because at the start of training you'll be initializing them. (See also: What happens to the initial hidden state in an RNN layer? ) The basic idea is that instead of 1 time series containing 4 years of data, you can divide it up to have 4 time series containing 1 year each. Using longer sequences helps because the parameters of the model see data from more than 1 time step, and can assign credit/blame for predictions. But it's hard to retain data from hundreds or more of time-steps in the past (vanishing gradient). Using longer sequences means taking $n$ time steps at a time and also predicting $n$ time steps at a time, and back-propagating for all $n$ in one step. So the input observed data is time steps $$t, t+1, t+2, ..., t+n$$ and predicting time steps $$t+1,t+2, ..., t+n+1.$$ Then you advance your offset by $n$ so that the hidden state matches the correct time step.
