[site]: crossvalidated
[post_id]: 29338
[parent_id]: 23795
[tags]: 
First, there are RVM models for classification (see section 3 of Tipping, M. (2001). Sparse Bayesian learning and the relevance vector machine. The Journal of Machine Learning Research, 1, 211â€“244.). There are not as many implementations of it compared to the regression model (in R at least). So are your distributed by the error rates, the variance or the number of rv's? This seems pretty straight-forward to me since the outcomes are on a different scale. I'll assume that it is the error estimate. Suppose your outcome ranged from 1 to 144. The RMSE for a model is likely to be very different from one with an outcome ranging from 1 to 12. More unitless metrics, such as R^2, are on the same scale but the error rate is not. Plus, don't use LOO. The bootstrap or repeated k-fold CV will do better at estimating the error rate (variance properties are much better and the bias in the bootstrap can be corrected).
