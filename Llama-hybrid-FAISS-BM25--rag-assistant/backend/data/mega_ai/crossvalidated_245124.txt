[site]: crossvalidated
[post_id]: 245124
[parent_id]: 245063
[tags]: 
No high ground claimed here. I work in a field (Monitoring and Evaluation) that is as rife with pseudo-science as any other social science you could name. But here's the deal, the polling industry is supposedly in 'crisis' today because it got the US election predictions so wrong, social science in general has a replicability 'crisis' and back in the late 2000's we had a world financial 'crisis' because some practitioners believed that sub-prime mortgage derivatives were a valid form of financial data (if we give them the benefit of the doubt...). And we all just blunder on regardless. Everyday I see the most questionable of researcher constructs used as data collection approaches, and therefore eventually used as data (everything from quasi-ordinal scales to utterly leading fixed response categories). Very few researchers even seem to realize they need to have a conceptual framework for such constructs before they can hope to understand their results. It is as if we have looked at market 'research' approaches and decided to adopt only the worst of their mistakes, with the addition of a little numerology on the side. We want to be considered 'scientists', but the rigor is all a bit too hard to be bothered with, so we collect rubbish data and pray to the Loki-like god of statistics to magically over-ride the GIGO axiom. But as the heavily quoted Mr Feynman points out: “It doesn’t matter how beautiful your theory is, it doesn’t matter how smart you are. If it doesn’t agree with experiment, it’s wrong”. There are better ways to handle the qualitative data which we are often stuck with, but they take a bit more work and those nice researcher constructs are often way easier to feed into SPSS. Convenience seems to trump science every time (no pun intended). In short, if we do not start to get serious about raw data quality, I think we are just wasting everyone's time and money, including our own. So does anyone want to collaborate on a 'data quality initiative' in relation to social science methods (yes, there is plenty in the text books about such things, but no one seems to pay attention to that source after their exams). Whoever has the most academic gravitas gets to be the lead! (It won't be me.) Just to be clear about my answer here: I see serious fundamental issues with 'contrived' raw data types so often, that I would like to suggest a need to start at the beginning. So even before we worry about sampling or which tests to run on the data, we need to look at the validity/limitations of the data types we collect in relation to the models we are proposing. Otherwise the overall predictive model is incompletely defined.
