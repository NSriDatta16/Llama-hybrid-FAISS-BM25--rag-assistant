[site]: crossvalidated
[post_id]: 578777
[parent_id]: 534572
[tags]: 
You can, and there is a sense in which doing so would be analogous to a linear probability model after layers of feature extraction, much as using a sigmoid activation function on the output neuron is analogies to logistic regression after layers of feature extraction. In fact, you don’t even have to limit yourself to sigmoid activation in the hidden layers to be analogous to a linear probability model. A common criticism of a linear probability model is that the output is not guaranteed to be restricted to $[0,1]$ , and your neural network that has all sigmoid activation functions in the hidden layers and then no activation function in the final neuron can suffer from this same issue. If this seems like a dealbreaker, then you might not like a model like you’ve proposed that forces the output to be restricted to $[0,1]$ , as the sigmoid activation functions in the hidden layers need not force the final output to be restricted to $[0,1]$ , even though the values in the hidden neurons will have that restriction. Cross-entropy (XEnt) loss is common for these classification neural networks. To be analogous to a linear probability model, square loss would be used. Further, XEnt presents problems with predictions outside of the interval, $[0,1]$ , due to the $\log(\hat y)$ and $\log(1-\hat y)$ terms (where $\hat y$ is the prediction).
