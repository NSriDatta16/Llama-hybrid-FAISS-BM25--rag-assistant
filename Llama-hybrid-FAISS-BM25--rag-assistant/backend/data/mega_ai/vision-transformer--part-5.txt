ds to only the key and value patches k x ′ , y ′ , t ′ , v x ′ , y ′ , t ′ {\displaystyle k_{x',y',t'},v_{x',y',t'}} such that t = t ′ {\displaystyle t=t'} . A time attention layer is where the requirement is x ′ = x , y ′ = y {\displaystyle x'=x,y'=y} instead. The TimeSformer also considered other attention layer designs, such as the "height attention layer" where the requirement is x ′ = x , t ′ = t {\displaystyle x'=x,t'=t} . However, they found empirically that the best design interleaves one space attention layer and one time attention layer. ViT-VQGAN In ViT-VQGAN, there are two ViT encoders and a discriminator. One encodes 8x8 patches of an image into a list of vectors, one for each patch. The vectors can only come from a discrete set of "codebook", as in vector quantization. Another encodes the quantized vectors back to image patches. The training objective attempts to make the reconstruction image (the output image) faithful to the input image. The discriminator (usually a convolutional network, but other networks are allowed) attempts to decide if an image is an original real image, or a reconstructed image by the ViT. The idea is essentially the same as vector quantized variational autoencoder (VQVAE) plus generative adversarial network (GAN). After such a ViT-VQGAN is trained, it can be used to code an arbitrary image into a list of symbols, and code an arbitrary list of symbols into an image. The list of symbols can be used to train into a standard autoregressive transformer (like GPT), for autoregressively generating an image. Further, one can take a list of caption-image pairs, convert the images into strings of symbols, and train a standard GPT-style transformer. Then at test time, one can just give an image caption, and have it autoregressively generate the image. This is the structure of Google Parti. Others Other examples include the visual transformer, CoAtNet, CvT, the data-efficient ViT (DeiT), etc. In the Transformer in Transformer architecture, each layer applies a vision Transformer layer on each image patch embedding, add back the resulting tokens to the embedding, then applies another vision Transformer layer. Comparison with CNNs Typically, ViT uses patch sizes larger than standard CNN kernels (3x3 to 7x7). ViT is more sensitive to the choice of the optimizer, hyperparameters, and network depth. Preprocessing with a layer of smaller-size, overlapping (stride < size) convolutional filters helps with performance and stability. This different behavior seems to derive from the different inductive biases they possess. CNN applies the same set of filters for processing the entire image. This allows them to be more data efficient and less sensitive to local perturbations. ViT applies self-attention, allowing them to easily capture long-range relationships between patches . They also require more data to train, but they can ingest more training data compared to CNN, which might not improve after training on a large enough training dataset. ViT also appears more robust to input image distortions such as adversarial patches or permutations. Applications ViT have been used in many computer vision tasks with excellent results and in some cases even state-of-the-art, such as in image classification, object detection, video deepfake detection, image segmentation, anomaly detection, image synthesis, cluster analysis, autonomous driving. ViT had been used for image generation as backbones for GAN and for diffusion models (diffusion transformer, or DiT). DINO has been demonstrated to learn useful representations for clustering images and exploring morphological profiles on biological datasets, such as images generated with the Cell Painting assay. In 2024, a 113 billion-parameter ViT model was proposed (the largest ViT to date) for weather and climate prediction, and trained on the Frontier supercomputer with a throughput of 1.6 exaFLOPs. See also Transformer (machine learning model) Convolutional neural network Att