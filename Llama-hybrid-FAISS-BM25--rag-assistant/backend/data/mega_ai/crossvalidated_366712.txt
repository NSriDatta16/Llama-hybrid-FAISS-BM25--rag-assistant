[site]: crossvalidated
[post_id]: 366712
[parent_id]: 366707
[tags]: 
A very basic example: logistic regression, as in your image, tries to model the class posteriors. Under no modification, the choice of nonlinearity in this case is sigmoid function; which is a linear function of inputs and neuron weights, i.e. $\sigma(w^Tx)$, where $\sigma(z)=\frac{1}{1+e^{-z}}$. Here, we set a threshold and compare the output of the activation function to decide if it is class 0 or 1; so, we decide if $\frac{1}{1+e^{-w^Tx}} > \theta$, which reduces to a inequality similar to $w^Tx
