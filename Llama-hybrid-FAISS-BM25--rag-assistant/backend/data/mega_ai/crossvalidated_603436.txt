[site]: crossvalidated
[post_id]: 603436
[parent_id]: 
[tags]: 
Bounds of Shapley values for variable importance

Imagine you have either a very good predictive model $f$ for a response $y$ or two highly predictive models $f_1$ and $f_2$ . Is it possible to bound the "true" Shapley values of $y$ in terms of the prediction error and the Shapley values of $f$ (case 1) or can you bound the difference of the Shapley values for $f_1$ and $f_2$ (case 2)? Background If inputs are dependent, variable importance measures such as Sobol indices or permutation feature importance fail to work reliably. One solution in this case may be the use of the Shapley algorithm to attribute whatever value function one is interested in to the inputs. For this question assume the value function is variance explained and the prediction error mean square error. (This is only for concreteness of the question. Feel free to comment/answer on other value functions as well.) Variance explained for an input variable $X_i$ (where $1\leq i\leq d$ and $d$ is dimension of the input space) is the decrease in mean square error achieved by including this variable in a prior set of variables. In the case of linear regression this allocation and the according variable importance measure is called the "Lindeman-Merenda-Gold measure". In machine learning the same idea is often called "Shapley effects". See the paper Iooss et.al. for further background and details. My question Assume mean square error is bounded by some small $\delta>0$ for a set of models. Call $S_i(f)$ respectively $S_i(y)$ the allocation of variance explained for variable $i$ for model $f$ respectively response $y$ according to the Shapley algorithm. What can then be said about the difference $$ S_i(y) - S_i(f)?$$ or in case 2 $$ S_i(f_1) - S_i(f_2)?$$ Is it possible to derive upper and lower bounds, which are computable from the values of $y$ and $f$ (case 1) or $f_1$ , $f_2$ (case 2) on the input dataset? I think the fact "If two models are close, their Shapley values are close as well" should follow more or less directly from continuity. Ultimately the Shapley allocation just boils down to a finite linear combination of squared norms of orthogonal $L^2$ -projections. But a more quantitative version would be helpful, I think. EDIT: Further clarification The question can be rephrased without reference to the Shapley-Values of $y$ . Just take two predictive models $f_1$ and $f_2$ instead. Assume that they are "close" or "similar" in the sense that they both predict $y$ very well. Say with $R^2$ of 99.99% accuracy. The question rephrased is then: Are the Shapley values for $f_1$ and $f_2$ similar? Is it possible to give bounds for their maximum difference? What drives/limits this difference? (i.e. properties of the response, of the models, ...) In my opinion Shapley values were quite useless for feature importance, if very similar models could have very different Shapley values (case 2) or if you could not learn anything from them about variable importance of the underlying ground truth (case 1).
