[site]: crossvalidated
[post_id]: 182083
[parent_id]: 182074
[tags]: 
Typically, Bayesians talk about a "posterior" distribution in light of an update. There are two possible things you could be trying to do here, which are fairly closely related so I'll step through them both. First is a hierarchical modeling problem: you know the distribution of $x$ given a parameter $\lambda$ ( $P(x|\lambda) \sim$ exponential), and you have a distribution on that parameter ( $P(\lambda) \sim$ gamma), but if you want the distribution on $x$ by itself ( $P(x) \sim$ ?), you need to combine those two pieces of information. This is done by integration; any value of $x$ could be achieved by any value of $\lambda$ , and so $P(x)=\int_0^\infty P(x|\lambda)P(\lambda)d\lambda$ . (Since the gamma and exponential distributions are closely related, this actually flows pretty smoothly.) Second is the problem of inferring what the uncertain value $\lambda$ is, given we observe a particular value $x$ from the resulting exponential distribution. Here the role of $P(x)$ becomes clear--we are conditioning on our observation $x$ , and dividing by $P(x)$ normalizes the probability distribution. As pointed out in @ebb-earl-co's answer, you don't actually need to calculate $P(x)$ in this situation, if you're willing to integrate the distribution $P(x|\lambda)P(\lambda)$ in order to determine what normalizing constant you need to apply to make it a well-formed probability. The 'magic' of Bayes is that this normalizing constant is necessarily the probability of the observation that you're updating on.
