[site]: crossvalidated
[post_id]: 222952
[parent_id]: 222948
[tags]: 
I would suggest to go with the "Extreme Gradient Boosting" which is based on Gradient Boosting method, but instead of using only linear model solver, it includes tree based algorithms too. It has become go-to method in Kaggle competitions for regression analysis surpassing Random Forests. Check out Anthony Goldbloom's talk about "how to win Kaggle competitions". If you are using R, go with "xgboost" package which implements Extreme Gradient Boosting algorithm. I was able to to reduce RMSE of my prediction by 10 times compared to that of RF.
