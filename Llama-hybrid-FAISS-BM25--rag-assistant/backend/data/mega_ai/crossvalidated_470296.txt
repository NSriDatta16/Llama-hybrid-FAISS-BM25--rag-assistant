[site]: crossvalidated
[post_id]: 470296
[parent_id]: 
[tags]: 
What is the maximum size of weights update in Momentum optimisation?

Given the optimization rules of momemntum gradient descent for weights update of a neural network: $$m\leftarrow \beta m -\eta \nabla_\theta J(\theta)$$ $$ \theta \leftarrow \theta + m$$ where $\theta$ are the weights, $J$ the cost functionthe book Hands on machine learning says You can easily verify that if the gradient remains constant, the terminal velocity (i.e., the maximum size of the weight updates) is equal to that gradient multiplied by the learning rate η multiplied by $\frac{1}{1-\beta}$ (ignoring the sign). For example, if $β = 0.9$ , then the terminal velocity is equal to 10 times the gradient times the learning rate, so Momentum optimisation ends up going 10 times faster than Gradient Descent Now I get that the the maximum size update is given by the max value of $m$ and it seems to me it does something like $m (1-\beta) = -\eta K$ where $K=\nabla_\theta J(\theta)$ is the constant gradient, but left-side $m$ and right-side $m$ are at different instants so they are basically different. Can someone explain the statement?
