[site]: crossvalidated
[post_id]: 95100
[parent_id]: 95078
[tags]: 
As @adam.r says, any hypothesis you actually tested should be specified if its results are of interest or interpreted substantively. However, logistic regression doesn't strictly test hypotheses. One certainly can (and often will by default) perform tests of model fit and significance of coefficients, but these aren't necessarily important to report if you don't consider them important for your research. For example, it's probably relatively rare to see anyone explicitly state the null hypothesis about the intercept. The hypothesis that a given predictor does not significantly improve the model's fit over the null model is often a useful hypothesis, but you might even be better off dropping the NHST framework and focusing on effect size and confidence interval (CI) estimation. If you report the regression coefficient for a given predictor and its CI, a test of the null hypothesis may be effectively redundant. A potential exception/counterargument would arise in any insignificant result, though: if you're not forcing a fail to / reject decision on your null hypothesis, but looking instead to judge the amount of evidence against the null on a continuous scale, your $p$ value may be more precisely useful than whichever bound of the CI crosses the null value. I.e., if you want to know exactly how likely results at least as extreme as yours are to occur in a replication of your study if the null is literally true, then you want a $p$ value, not the less different-from-zero bound of a 95% CI. Then again, you could also find the CI for the level of confidence corresponding to your $p$ value and thereby get a sense for an equally plausible null on the other side of your CI without losing the info of the $p$ value. That the confidence level for a CI for which one bound is equal to the null (presumably the default, zero) tells you what you need to know to get a $p$ value may not be obvious to your audience though, so even if you go with this approach, you might want to report the $p$ value as well. It generally doesn't take a lot of extra space if you're already reporting more important statistics, and it doesn't do much harm. You can always bury it between other, more useful statistics if you don't want to emphasize it (I mentioned this idea initially in my answer to " Accommodating entrenched views of p-values "). If you ever have time to read up on confidence intervals, you might want to check out the reference I list here. Some others I mention in my answer to " Why are 0.05 " and the responses there in general might also be helpfully informative. There are some big issues underneath the surface of this question. Reference Cumming, G. (2012). Understanding the new statistics: Effect sizes, confidence intervals, and meta-analysis . New York: Routledge.
