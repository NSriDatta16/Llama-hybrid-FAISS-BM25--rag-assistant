[site]: datascience
[post_id]: 94423
[parent_id]: 94406
[tags]: 
We can see this in the original Transformer diagram : The output of the last encoder FF layer is added to the original input of the same layer, then layer normalization is applied and that is the output of the whole encoder, which is used as keys and values for the encoder-decoder attention blocks in the decoder.
