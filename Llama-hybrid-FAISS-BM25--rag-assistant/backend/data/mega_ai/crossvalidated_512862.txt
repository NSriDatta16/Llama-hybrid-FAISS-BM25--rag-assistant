[site]: crossvalidated
[post_id]: 512862
[parent_id]: 512568
[tags]: 
This demonstration uses the clever (yet elementary) method employed in Fisher's reference Burnside & Panton vol. II, section 142. It assumes a familiarity with the basics of determinants: their definition, computing them with row-reduction, and expanding them by minors . I also make liberal use of block-matrix notation to simplify the presentation. (Lurking here is a more modern theorem about the norms of alternating multilinear forms, aka "antisymmetric tensors,* but I will avoid using this language.) To help you see the progress of the argument, I have highlighted three equivalent statements of the inequality as they are obtained. Part of the interest in this solution lies in the incidental but elegant formula for that determinant shown in the question and labeled $(*)$ below. Otherwise we could take a more direct route to the conclusion by first centering the data (to make their sum $S_1=0$ ) and scaling them to unit variance (whence their sum of squares is $S_2=n$ ) and proceeding from the observation that the cross-product $\mathbb X \mathbb X^\prime$ must be positive-semidefinite directly to the inequality by evaluating its determinant. That is, after these preliminary normalization steps, we have $$0 \le \left|\mathbb X \mathbb X^\prime\right| = \left| \matrix{n & 0 & n\\0 & n & n\beta \\ n & n\beta & n\kappa}\right| = n^3\left| \matrix{1 & 0 & 1\\0 & 1 & \beta \\ 1 & \beta & 1\kappa}\right| = \kappa - \beta^2 - 1.$$ Let a dataset be represented by a vector of its values $(x_1, x_2, \ldots, x_n).$ We are going to create a matrix of dimensions $(n+3)\times (n+3)$ out of four blocks based on the $3\times n$ matrix $$\mathbb X = \pmatrix{1&1&\cdots & 1 \\ x_1 & x_2 & \cdots & x_n\\x_1^2 & x_2^2 & \cdots & x_n^2},$$ the $n\times n$ identity matrix $\mathbb{I}_n,$ and the $3\times 3$ zero matrix $\mathbb{O}_3;$ namely, $$\mathbb A = \pmatrix{\mathbb{X} & \mathbb{O}_3 \\ -\mathbb{I}_n & \mathbb{X}^\prime}.$$ The kurtosis-skewness inequality will amount to (a) relating the determinant of $\mathbb A$ to the determinant of a $3\times 3$ matrix formed from sums of powers of the data and then (b) showing $\mathbb A$ has a non-negative determinant. To achieve the first reduction, zero out the upper left $\mathbb X$ block using row operations (which leave the determinant unchanged). This amounts to left-multiplying $\mathbb{A}$ by the block matrix $$\mathbb Q = \pmatrix{\mathbb{I}_3 & \mathbb X \\ \mathbb{O}_{n\times 3} & \mathbb{I}_n}$$ where $\mathbb{O}_{n\times 3}$ is the $n\times 3$ matrix of zeros. We obtain $$\mathbb{Q}\mathbb{A} = \pmatrix{\mathbb{O}_{3\times n} & \mathbb{X}\mathbb{X}^\prime \\ -\mathbb{I}_n & \mathbb{X}^\prime}.$$ Similarly, the lower $\mathbb{X}^\prime$ block can be zeroed out by column operations--but these leave the upper right block untouched. The result is $$|\mathbb A| = \left|\matrix{\mathbb O_{3\times n}&\mathbb X \mathbb X^\prime \\ -\mathbb{I}_n & \mathbb O_{n\times 3}}\right| = (-1)^{3n}\left|-\mathbb{I}_n\right| \left| \mathbb X \mathbb X^\prime\right| = \left| \mathbb X \mathbb X^\prime\right|.$$ Another way to find the determinant of $\mathbb A$ returns to its definition and employs Laplace's expansion in $3\times 3$ minors across the top row. A representative term in this expansion is the determinant of the principal $3\times 3$ minor times the determinant of its complement. That minor is $$\mathbb{A}_{(1,2,3); (1,2,3)} = \pmatrix{1&1&1 \\ x_1 & x_2 & x_3 \\ x_1^2 & x_2^2 & x_3^2}$$ and its complement has the form $$\mathbb{A}^0_{(1,2,3); (1,2,3)} = \pmatrix{\mathbb{O}_{3\times n} & \mathbb{A}_{(1,2,3); (1,2,3)}^\prime \\ -\mathbb{I}_{n-3} & *}$$ Both determinants evidently equal the determinant of that minor, $$\left|\mathbb{A}_{(1,2,3); (1,2,3)} \right| = \left|\mathbb{A}^0_{(1,2,3); (1,2,3)}\right| = (x_3-x_2)(x_3-x_1)(x_2-x_1).$$ Thus the product of the determinants of this minor and its complement is the square of the value on the right hand side. The expansion has one such term for each (not structurally zero) minor, $$\left| \mathbb A \right| = \sum_{n \ge k \gt j \gt i \ge 1} \left[(x_k-x_j)(x_k-x_i)(x_j-x_i)\right]^2.\tag{*}$$ ("Not structurally zero" means that all minors involving any of the last three columns must be zero and so don't appear explicitly in this sum.) This quantity, being a sum of squares, is non-negative, whence $$0 \le \left| \mathbb A \right| = \left|\mathbb X\mathbb X^\prime\right|.$$ The rest is just mopping up but, to be complete, I will give the details. For any $k=0, 1, 2, \ldots, $ writing $$S_k = \sum_{i=1}^n x_i^k$$ for the sum of $k^\text{th}$ powers of the data enables us to represent the cross-product of $\mathbb X$ in the simple form $$\mathbb X \mathbb X^\prime = \pmatrix{S_0 & S_1 & S_2 \\ S_1 & S_2 & S_3 \\ S_2 & S_3 & S_4}.$$ This simplifies, because $(*)$ shows it is a function only of the differences among the data, quantities that do not change when a constant is added to all the data values. Add the negative of their mean to each term (that is, center the data) to make $S_1=0.$ Obviously $S_0=n,$ giving $$ 0 \le \left|\matrix{n & 0 & S_2 \\ 0 & S_2 & S_3 \\ S_2 & S_3 & S_4}\right| = n\left|\matrix{S_2 & S_3 \\ S_3 & S_4}\right| - S_2\left|\matrix{0 & S_2 \\ S_2 & S_3}\right| = n(S_2S_4 - S_3^2) - S_2^3.$$ The kurtosis $\kappa = (S_4/n)/(S_2/n)^2$ and skewness $\beta = (S_3/n)/(S_2/n)^{3/2}$ are defined only when the variance is nonzero, which implies $S_2$ is nonzero. We may therefore divide both sides of this inequality by the positive quantity $n^3S_2^3$ to give $$0 \le \frac{n(S_2S_4 - S_3^2) - S_2^3}{n^3 S_2^3} = \kappa - \beta^2 - 1$$ to conclude $$\kappa \ge \beta^2 + 1.$$ Finally, since the kurtosis and skewness of any distribution can be approximated arbitrarily well by a finite discrete distribution (at least when both are defined and finite), this equality must hold for all distributions for which the kurtosis and skewness are defined and finite, QED.
