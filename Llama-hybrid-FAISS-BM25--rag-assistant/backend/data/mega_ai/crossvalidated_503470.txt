[site]: crossvalidated
[post_id]: 503470
[parent_id]: 
[tags]: 
Larger batch size means faster training for RNN?

When training my LSTM, I realize time per eopch decreases as I increase the batch size. But this isn't true for training a ConvNet. I wonder if this is because for RNNs with unspecified sequence length, everytime it train using a batch of data, it has to do the padding and compile a new graph of computation for that paticular batch. So larger batch size means fewer graph compiling and thus save the run time?
