[site]: crossvalidated
[post_id]: 454825
[parent_id]: 
[tags]: 
Classification losses for binary classification

I'm trying to classify a binary variable using a neural network. I'm currently using CrossEntropyLoss as my criterion, but I don't really like it and was wondering what other options are available. The reason I don't CE is that the data available for input is very limited and I don't think it is possible to properly classify the output with any certainty. CE decreases very slowly at the start and I think it prevents my model from learning properly. What I mean by slowly: If the model always predicts 50/50 the loss would be 0.693 (-ln(0.5)). If the model predicts with accuracy/score of 60% the loss would be 0.673 (-(0.6*ln(0.6)+0.4*ln(0.4))), which is a very small decrease given huge increase in performance. I was thinking about representing the target as a probability and using KL-divergence, but I don't have true probabilities and not sure about how to approximate them. What other losses are there?
