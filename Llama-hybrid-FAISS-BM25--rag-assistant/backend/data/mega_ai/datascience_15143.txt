[site]: datascience
[post_id]: 15143
[parent_id]: 15089
[tags]: 
If you really need to do that (I will argue it is not a good idea), with Postgres you can store an array type and write a stored procedure for new item insertion. This stored procedure can do whatever distance checking you wish, such as checking the distance of the new vector against all others in the database, before storage. I would argue against this design because I suspect the criteria for uniqueness* could easily change over time. I think it'd be a better idea to store all of the vectors except exact matches. Then, create another table which uses the definition of uniqueness. Creating this table would be handled on the application side. If your definition of uniqueness changes, no problem, just make a new table. You could even compare several different definitions to see how your results are sensitive to it. If you do it this way, Cassandra is a great database choice. It's specifically designed for denormalized data storage (you have the same data stored in different forms or variations, so that your application gets exactly what it needs without further computation). *In your post you stated that a similarity of criteria for uniqueness .
