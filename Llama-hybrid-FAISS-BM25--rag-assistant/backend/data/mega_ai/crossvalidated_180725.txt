[site]: crossvalidated
[post_id]: 180725
[parent_id]: 126439
[tags]: 
When using stochastic gradient descent (i.e. batch gradient descent) you update the weights of the network after each batch by the average gradient over all instances in the batch. So in short no you can not parallelize the batches, but you can however parallelise the process of computing the gradient over the batch, as this gradient is the average over the gradient computed for each instance in the batch. This last type of parallelisation is usually implemented by structuring the inputs in a matrix and doing matrix multiplications in the forward and backward pass to compute all gradients at the same time. The actual matrix multiplications are then the ones being parallelised/optimised
