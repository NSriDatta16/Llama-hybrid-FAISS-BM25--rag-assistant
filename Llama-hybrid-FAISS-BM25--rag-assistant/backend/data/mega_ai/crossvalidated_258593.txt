[site]: crossvalidated
[post_id]: 258593
[parent_id]: 
[tags]: 
Sequence to sequence autoencoder - decoder input?

What's the input to the decoder part of a sequence to sequence autoencoder? I've seen certain examples of such an autoencoder (using LSTM's more often than not) but am still unclear. For example, here in this often-cited paper by Dai & Le ('Semi Supervised Sequence Learning'), we have the following diagram: What's the input to the decoder portion of the autoencoder here? In this example it's 'W-X-Y-Z.' But in general, is it the same as the input to the encoder? Or is it using the output from the previous timestep/LSTM cell as input? Similarly, in another popular paper by Srivastava et. al ('Unsupervised Learning of Video Representations using LSTMs'), they have the following diagram: It seems they're using the reversed input from the encoder as input here. However, there's a section as follows: The decoder can be of two kinds â€“ conditional or unconditioned. A conditional decoder receives the last generated output frame as input, i.e., the dotted input in Fig. 2 is present. An unconditioned decoder does not receive that input. In the unconditioned decoder, what input does the decoder receive? Thank you! I'm new here, so please be kind :)
