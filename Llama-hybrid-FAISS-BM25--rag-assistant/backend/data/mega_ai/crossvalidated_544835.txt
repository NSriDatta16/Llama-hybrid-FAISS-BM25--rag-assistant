[site]: crossvalidated
[post_id]: 544835
[parent_id]: 
[tags]: 
Why is binning in Expected Calibration Error done the way it is?

I see the definition of expected calibration error being $$\sum_{m=1}^{M}\frac{|B_m|}{n}|accuracy(B_m)-confidence(B_m)|$$ Where $B_m$ represents a outputs of the model that predicted class $m$ in a multi-classifcation problem. Then I read this nice article with an example of calculating it. So the binning is determined by what the model outputs as the max prob. So for example, if input 0 leads to a 90% prediction of "dog," that'll go into the "dog" bin. Once this is done the accuracy as in how many of them correctly estimated the class subtracted by the average confidence or predictive probability by the model is take. I'm a little bit confused about the binning part. What does binning the predictions in this way tell us exactly? Why not do the bins via the true classes? For example you take all the rows where the true class is $m$ then calculate the accuracy and confidence for class $m$ ?
