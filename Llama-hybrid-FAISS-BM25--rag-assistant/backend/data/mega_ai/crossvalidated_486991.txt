[site]: crossvalidated
[post_id]: 486991
[parent_id]: 
[tags]: 
Bayes Linear Regression - understanding the posterior formula?

In the linked resource , the author defines the posterior probability of Bayesian linear regression as: P(B|y,X) = P(y|B,X)*P(B|X)/P(y|X) I have a couple questions/issues with this. First, B represents the weight(s) vector, but it shouldn't include the bias term, b, in y=mx+b . Does this model just forget to include a bias or is it somehow factored in B? Second, I'm used to seeing Bayes Rule in a very specific form: P(A|B) = P(B|A)*P(A)/P(B) So it's my naive guess that the posterior would take the following form: P(B|y,X) = P(y,X|B)*P(B)/P(y,X) Do the equations, mine & the source, equate to the same value? If not, why am I wrong? I gather that there is something about conditional probabilities and the chain rule that is going over my head. EDIT : According to this video my interpretation is correct! However, I'm still unclear where the bias term comes into play. Likewise, I'd like to what likelihood and priors are typically used. Wikipedia shows: This looks somewhat like a multivariate guassian, without some scaling terms outside of the exponent, and most notably, no precision matrix (aka inverse covariance) in the exponentiated term. Does this likelihood have a name? Where did it come from? Etc.
