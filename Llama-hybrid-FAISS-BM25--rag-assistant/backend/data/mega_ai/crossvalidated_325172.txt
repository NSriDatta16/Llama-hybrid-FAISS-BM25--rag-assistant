[site]: crossvalidated
[post_id]: 325172
[parent_id]: 308773
[tags]: 
A good start would be using some pre-trained embedding like word2vec to transform the sentences into lists of vectors. As opposed to other feature representations for text classification (e.g., TF-IDF) here the structure of the sentence is taken into account. So the first layer of your network could be this word2vec -based embedding; then, you can put some Conv1D/pooling layers, to end with a couple of dense layers and a softmax final layer. Of course, you will have to zero-pad some of the samples to make them all have the same length. You could also use LSTM to manage the sequential nature of words in sentences. However, in my experience if your texts are relatively short and there are not huge length differences, the Conv1D/pooling combo will work well and will be easier to train. Your final architecture could look something like this: We used that kind of network to solve a similar problem in a recent paper [1]. However, in this case the final layer wasn't using softmax, but a simple dense layer trained to perform metric learning [2]. [1] https://link.springer.com/article/10.1007/s10489-017-1109-7 [2] https://cs.nyu.edu/~sumit/research/assets/cvpr06.pdf
