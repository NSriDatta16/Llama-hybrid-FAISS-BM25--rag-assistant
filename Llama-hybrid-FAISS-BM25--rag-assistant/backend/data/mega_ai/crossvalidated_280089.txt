[site]: crossvalidated
[post_id]: 280089
[parent_id]: 280087
[tags]: 
This is a case of the so-called "Wisdom of Crowds" : non-experts pooling their knowledge can perform very well and even outperform experts. The first mention of this involved a crowd of people estimating the weight of an ox at a fair. The average (either the mean or the median) estimated weight was very close to the actual value (and IIRC, better than the estimate of trained butchers). You may be interested in the book . The key points here are: People may not be experts, but they can at least give some information, even if it's only a rough ranking of their dance partners. Your dancers don't actively sabotage your system. Or if a few do, then their effects are washed out by the large number of honest dancers. If more than half your dancers start returning random answers, the system will break. The process is repeated a few times, which will give the algorithm more data to work with and reduce variability. How many dances are required will of course depend on a few factors: More than seven rankings per dance would mean that fewer dances are required. A larger talent spread would require fewer dances (because it's easier to sort people if they differ more strongly - the signal to noise ratio is larger). More experienced dancer raters would mean that fewer dances are required. What you mean by "a reasonable ranking" will have an impact.
