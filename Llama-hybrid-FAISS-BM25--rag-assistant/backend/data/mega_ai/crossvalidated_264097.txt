[site]: crossvalidated
[post_id]: 264097
[parent_id]: 262808
[tags]: 
I cannot speak in terms of machine learning, but I can speak in terms of scaling. From our tag wiki: tl;dr version first: normalization refers to scaling all numeric variables in the range [0,1], such as using the formula: $$x_{new}=\frac{x-x_{min}}{x_{max}-x_{min}}$$ standardization refers to a transform to the data set to have zero mean and unit variance, for example using the equation: $$x_{new}=\frac{x-\overline{x}}{s}$$ That is, normalization does not rely on the underlying distribution; standardization transforms the data based upon the parameters of a Gaussian distribution. Fuller explanations: "Normalization" refers to several related processes: ("Feature scaling") A set of numbers whose maximum is $M$ and minimum is $m$ can be converted to the range from $0$ to $1$ by means of an affine transformation (which amounts to changing their units of measurement) $x \to (x-m)/(M-m)$. A set of positive numbers $\{p_i\}$ representing probabilities or weights can be uniformly rescaled to sum to unity: divide each $p_i$ by the sum of all the $p_i$. Analogously, a distribution (or indeed any non-negative function with a finite nonzero integral) can be normalized to have a unit integral by dividing its values by the integral. A vector in a normed linear space is normalized (to unit length) by dividing it by its norm. This is a general procedure encompassing the two preceding operations as special examples. The range from $0$ to $1$ can be made from $0$ to any desired limit $\alpha$ by multiplying a previously unit-normalized value by $\alpha$. Other kinds of operations exist having a similar intent of re-expressing values in a predetermined range. Many of these are nonlinear and tend to be used in specialized settings. Standardization: Shifting and rescaling data to assure zero mean and unit variance. Specifically, when $(x_i), i=1, \ldots, n$ is a batch of data, its mean is $m=(\sum_i x_i)/n$ and its variance is $s^2 = > v=(\sum_i(x_i-m)^2)/\nu$ where $\nu$ is either $n$ or $n-1$ (choices vary with application). Standardization replaces each $x_i$ with $z_i > = (x_i-m)/s$.
