[site]: crossvalidated
[post_id]: 320926
[parent_id]: 
[tags]: 
When random output is a local maximum, how can a neural network find the global (or a better local) maximum?

I'm trying to train a neural network to play Rock-Paper-Scissors. A random player is a local maximum for the game, as it wins 50% of the time. In a tournament setting, it comes out consistently better than the worst bots and consistently worse than the better bots. Against the very worst bots, my neural networks have found solutions that win 95%+ of the time. Against anything else, my neural networks quickly turn into random-ish players. They stay stuck at that local maximum and never find better strategies. Based on my naive experiments, it seems that, from any given starting point, there are many nearby weight combinations which will generate random-ish output from moderately complex input. In most applications, this isn't a problem because random output scores poorly and gradient descent slides right past those weight combinations. But in Rock-Paper-Scissors, all of those random-ish generating weight combinations - and there seem to be a lot of them, many magnitudes more than the number of weight combinations which produce more sophisticated strategies - act as attractors for gradient descent. Are there any known solutions to this problem or heuristics which might address it? When random-ish output from moderately complex input is a reasonably good solution, can neural networks be set up or trained in a way which consistently finds better solutions? Or is Rock-Paper-Scissors too hard for neural nets? Edit: I'm specifically trying to achieve blank-slate learning-via-playing, rather than training it on an existing corpus of moves. From my research so far, though, it looks like bots in other games which learn via playing learn most effectively when they explore the move space in a more purposeful way, effectively building a corpus of moves to train from.
