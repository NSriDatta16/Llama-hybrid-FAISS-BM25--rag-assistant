[site]: crossvalidated
[post_id]: 406072
[parent_id]: 
[tags]: 
Formal Bayesian justification of conditional modelling

I'm having some trouble following the logic of this passage from Chapter 14 in Bayesian Data Analysis, A. Gelman: The numerical 'data' in a regression problem includes both $X$ and $y$ . Thus, a full Bayesian model includes a distribution for $X$ , $p(X|\psi$ ), indexed by a parameter vector $\psi$ , and thus involves a joint likelihood $p(X,y|\psi,\theta)$ , along with a prior distribution, $p(\psi,\theta)$ . In the standard regression context, the distribution of $X$ is assumed to provide no information about the conditional distribution of $y$ given $X$ ; that is, we assume prior independence of parameters $\theta$ determining $p(y|X,\theta)$ and the parameters $\psi$ determining $p(X|\psi).$ Thus, from a Bayesian perspective, the defining characteristic of a 'regression model' is that it ignores the information supplied by $X$ about ( $\psi$ , $\theta$ ). How can this be justified? Suppose $\psi$ and $\theta$ are independent in their prior distribution; that is $p(\theta,\psi) = p(\theta)p(\psi)$ . Then the posterior distribution factors, $p(\psi,\theta|X,y) = p(\psi|X)p(\theta|X,y)$ , [...] When I work this out I can't obtain the last line. I can get $p(\psi,\theta|X,y) = p(\psi|X,y,\theta)p(\theta|X,y)$ . Intuitively the statement makes sense, but I can't prove to myself that it is true.
