[site]: crossvalidated
[post_id]: 123557
[parent_id]: 
[tags]: 
Neural Network General Learning Dynamics of Gradient Descent

This might be simple to you but can someone tell me step by step how is matrix form of updating rule of $W^{32}$ and $W^{21}$ derived in this case? Consider linear three layer neural network model which has input $\mathbf{x}$, first layer to second layer weight matrix $W^{21}$, second layer to third layer weight matrix $W^{32}$. The output $\mathbf{y}$ is defined as: $$\mathbf{y} = W^{32}W^{21}\mathbf{x}$$ consider P examples $\{\mathbf{x}^\mu,\mathbf{y}^\mu\}, \mu=1\cdots P$. Training is accomplished via gradient descent on the squared error $\sum_{\mu=1}^P||\mathbf{y}^\mu-W^{32}W^{21}\mathbf{x^\mu}||^2$ between the desired feature output, and the networkâ€™s feature output. This gradient descent procedure yields the batch learning rule. $$\Delta W^{21} = \lambda\sum_{\mu=1}^PW^{32^T}(\mathbf{y}^\mu\mathbf{x}^{\mu^T} - W^{32}W^{21}\mathbf{x}^\mu\mathbf{x}^{\mu^T})$$ $$\Delta W^{32} = \lambda\sum_{\mu=1}^P(\mathbf{y}^\mu\mathbf{x}^{\mu^T} - W^{32}W^{21}\mathbf{x}^\mu\mathbf{x}^{\mu^T})W^{21^T}$$ where $\lambda$ is learning rate.
