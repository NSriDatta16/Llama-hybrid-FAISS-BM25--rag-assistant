[site]: crossvalidated
[post_id]: 407708
[parent_id]: 407703
[tags]: 
The most appropriate method will depend on the average signal to noise ratio in the sensor output. You want to make sure that you don't see noise in the features and incorrectly classify it as an anomaly. One of the simplest ways is to implement a sliding-window average ( https://en.wikipedia.org/wiki/Moving_average ) which examines n number of previous points. Then you can check whether the most recent points are within a certain tolerance from the the previous ones. The tolerance is typically something like 2 standard deviations away from the mean of the previous points, but this will depend on your signal to noise ratio and also whether you'd rather error on the side of more anomalies or less anomalies. One method I use is to examine the most recent 5 points, take their average and standard deviation, and compare that to the average and standard deviation of the previous 5 points before that. If the differences in the two averages is greater than a couple standard deviations, this is a good indicator of an anomaly. The number of points you're comparing, and the number of standard deviations (your tolerance) can be tuned to best suit your dataset. Example Say the last 5 data points you acquired have an average of 10 and a standard deviation of 2. That means you can expect the noise in your data to range anywhere from around 8 to 12 (the average plus and minus the std). If the most recent point you acquire is 13, you may not consider it an anomaly since it is so close to your standard range. If it is 16 for example, it is 4 standard deviations away from the average, and likely an anomaly. Keep in mind that this approach is only useful if your data set is stationary (there is no significant long term drift which makes your data continuously increasing or decreasing for example).
