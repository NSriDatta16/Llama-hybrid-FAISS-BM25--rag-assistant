[site]: crossvalidated
[post_id]: 168986
[parent_id]: 168909
[tags]: 
From the fact that you are using [R] code I tend to believe that your question stems from giving these concepts some thought, in which case, I have to say that you're not alone. They are slippery, and just when you think you get them, they become out of focus again. Just read about the controversy about over-reliance on $p-values$ in Nature just when we had it all figured out. I believe that you are asking something along the lines of what is the likelihood of making a type I error even when the samples come from the same population by design. And I think the difficulty arises in coming to terms with the idea that you are at liberty to decide what level of risk of making a type I error is acceptable. You didn't set.seed , which is a good idea to allow other people to reproduce your results exactly, but I ran your code as is, returning a difference between the means of 0.029722 and a t = 0.0917 . Finding a difference in the means of this (very tiny) magnitude leaves you with a probability of 0.4635 of repeating the experiment and obtaining differences just as large or lager ($p-value$), meaning that assuming that these samples come from the same population, $46\,\%$ of the mean differences will be equal or larger by simple randomness. I have carried out a simulation just for fun of $10^4$ runs of your code, and plotted the kernel density of the histogram of the results. On top of the plot I have superimposed the area (in gray) corresponding to the probability of finding under the null a value above what I initially found ( t = 0.0917 ): Clearly, if we decided to take a one-tailed alpha risk of committing a type I error of $46\,\%$ we could conclude that there is a difference in the means. Would that be acceptable? Well, it would be close to just flipping a coin, but... So the $alpha$ value is stated as a voluntary decision to strike a compromise between type I and type II errors, and quoted in the literature - it is arbitrary. On the other hand, the $p-value$ is the area under the curve above the value of the t-statistic. The researcher will consider the result "statistically significant" if the $p-value$ is smaller than the preset $alpha$. Since the most commonly selected $alpha$ is $0.05$, in the same simulation, the percentage of cases with your code that would have a $p-value$ smaller than that (one-tailed) would be: pvalue validating the t-test statistic as valid to check if these samples means come from the same population. Keep on asking questions! And if you want to play with the code, here it is .
