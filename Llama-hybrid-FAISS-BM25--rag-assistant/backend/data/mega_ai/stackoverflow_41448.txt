[site]: stackoverflow
[post_id]: 41448
[parent_id]: 41424
[tags]: 
Actually what Google does is very much non-trivial and also at first counter-intuitive. They don't do anything like check against a dictionary, but rather they make use of statistics to identify "similar" queries that returned more results than your query, the exact algorithm is of course not known. There are different sub-problems to solve here, as a fundamental basis for all Natural Language Processing statistics related there is one must have book: Foundation of Statistical Natural Language Processing . Concretely to solve the problem of word/query similarity I have had good results with using Edit Distance , a mathematical measure of string similarity that works surprisingly well. I used to use Levenshtein but the others may be worth looking into. Soundex - in my experience - is crap. Actually efficiently storing and searching a large dictionary of misspelled words and having sub second retrieval is again non-trivial, your best bet is to make use of existing full text indexing and retrieval engines (i.e. not your database's one), of which Lucene is currently one of the best and coincidentally ported to many many platforms.
