[site]: crossvalidated
[post_id]: 223038
[parent_id]: 223031
[tags]: 
A couple of thoughts without going into articles or maths. If you choose to normalize the data based on the sample data that you have available by dividing by say the max, min values of the data as suggested it is clear that you would need to apply the same normalization on new data. To me it is not clear exactly what is the problem, though, could be more specific in which circumstances this has practical relevance? Speaking hypotetically about it you could come up with a larger normalization constant, not found in the data etc and say this is valid under these circumstances. Say take max and multiply with 100 or something. To make it a bit more meaningful and interesting though I would need more details. Otherwise you risk ending up with a hypotethial question like, without any assumptions on the new data that my machine learning will handle, how can I know that it fits into the data range where my model has somekind of relevance, and the obvious answer to such rather philosophical questions is that you cannot. Don't know if that is helpful. I would be happy to have someone more experienced to give their input as well.
