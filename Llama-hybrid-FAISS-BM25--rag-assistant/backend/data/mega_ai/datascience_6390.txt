[site]: datascience
[post_id]: 6390
[parent_id]: 6384
[tags]: 
I agree with dpmcmlxxvi's answer that the common "output" of PCA is computing and finding the eigenvectors for the principal components and the eigenvalues for the variances, but I can't add comments yet and would still like to contribute. Once you hit this step of calculating the eigenvectors and eigenvalues of the principal components, you can do many types of analyses depending on your needs. I believe the "output" you are specifically asking about in your question is the resultant data set of applying a transformation or projection of the original data set into the desired linear subspace (of n-dimensions). This is taking the output of PCA and applying it on your original data set. This PCA step by step example may help. The ultimate output of this 6 step analysis was the projection of a 3 dimensional data set into 2 dimensions. Here are the high level steps: Taking the whole dataset ignoring the class labels Compute the d-dimensional mean vector Computing the scatter matrix (alternatively, the covariance matrix) Computing eigenvectors and corresponding eigenvalues Ranking and choosing k eigenvectors Transforming the samples onto the new subspace Ultimately, step 4 is the "output" since that is where the common requirements for performing PCA are fulfilled. We can make different decisions at steps 5 and 6 and produce alternative output there. A few more possibilities: You could decide to project the observations with outliers removed Another possible outcome here would be to calculate the proportion of variance explained by one or any combination of principal components. For example, the proportion of variance explained by the first two principal components of K components is (λ1+λ2)/(λ1+λ2+. . .+λK) . After plotting the projected observations into the first two principal components (as in the given example), you can impose a plot of the loadings of each of the original dimensions into the subspace (scaled by the standard deviation of the principal components). This way, we can see the contribution of the original dimensions (in your case a - e) to principal component 1 and 2. The biplot is another common product of PCA.
