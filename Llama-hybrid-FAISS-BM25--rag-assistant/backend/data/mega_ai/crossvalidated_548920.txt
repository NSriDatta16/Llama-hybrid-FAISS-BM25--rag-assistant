[site]: crossvalidated
[post_id]: 548920
[parent_id]: 
[tags]: 
Is this actually overfitting?

I'm quite new to the field and need your advice. I'm training an artificial neural network on a very small dataset (~30,000 samples). I have difficulties judging if my model is overfitting or not. Based on the output from keras.evaluate() everything is fine. Accuracy for training is ~98% while testing is ~96% . But if I look at the history it paints a different picture. There you can clearly see that testing accuracy is above the training accuracy, which would be an indicator for overfitting, right? On the other hand, the loss function is fine. So what is more relevant? The results from the keras.evaluate() or the history plots? Or is it a combination of both? And if you agree that my model is overfitting, is this going to be an issue? All of this will be part of a publication, and as I'm new to the field, I want to make sure that the model holds up to the reviewers. If it helps, the models' architecture is: dense(100) dropout(0.5) dense(50) dropout(0.5) dense --> Output
