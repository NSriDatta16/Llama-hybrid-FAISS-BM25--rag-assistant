[site]: crossvalidated
[post_id]: 461896
[parent_id]: 461281
[tags]: 
Try constructing your model like so: model = Model([X_realA, X_realB, X_realC], [Fake_A, X_realB , X_realC]) I have a hunch your code should work this way. However if you want to update modelA using some calculated loss from X_realB and X_realC that is not going to work. You see when you define the losses ["mse", "mse", "mse"] that means three different losses are calculated and then the nodes that contribute to that loss (/output) are updated by backpropagating. Your modelA network does not contribute to the losses calculated from X_realB , X_realC . If you want to update modelA , I would recommend implementing a custom loss function, where additional losses are added to the loss calculated from your Fake_A output. If I understand you correctly, you have a model output, and some additional information about the environment the input measurement was taken in, and you want to use this additional information when calculating the loss from Fake_A . This is essentially additional information about the expected output, so I would put X_realB and X_realC into the annotation and handle it in the custom loss. If you can provide more information about your use case maybe I can be of more help. Edit 1: In combined_loss you are adding constants to the loss calculated from Fake_A, so when taking the derivatives wrt. model parameters they zero out. This comes from the linearity of differentiation , where differentiating a summation is differentiating by parts. To put it simply in your case: deriv_wrt_params(loss+12+34) = deriv_wrt_params(loss) + deriv_wrt_params(12) + deriv_wrt_params(34) = deriv_wrt_params(loss) + 0 + 0 Also because you are using MSE, your generator will learn to output only ones, since you are punishing values deviating from one : loss0=keras.losses.mse(FakeA,FakeA_ones) I recommend using binary crossentropy. If these added values are not related to the traditional identity loss, generator loss and consistency loss, but come from a prior knowledge, you should use eg. multiplication or something like that so they affect the gradients as well, not just the loss. If you want to implement CycleGAN with identity loss, consistency loss etc. you will have to implement a custom train loop to update the generators and discriminators separately. For this I recommend the official Tensorflow 2.1 CycleGAN tutorial , where they implement a CycleGAN from start to finish.
