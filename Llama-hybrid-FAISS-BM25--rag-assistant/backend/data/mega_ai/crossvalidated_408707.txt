[site]: crossvalidated
[post_id]: 408707
[parent_id]: 
[tags]: 
Go with t-test or Mann-Whitney U test?

I'm measuring performance of 2 methods ( method_A and method_B ) that try to satisfy customers demands. Both methods produce results between 0 (none of the demands were satisfied) and 1 (all demands were satisfied). Also, both methods run for a certain amount of Time and, after that, they return best possible solution. I've generated 30 datasets and solved each one with method_A and method_B with different Time limits. The results are summarized below: My goal is to do some hypothesis testing with the results. I've considered to do a t-test but before that I've decided to do some normality tests. Results below: Histogram of method_A and method_B Q-Q Plot and Shapiro–Wilk test for method_A Q-Q Plot and Shapiro–Wilk test for method_B For small Time values data seem to follow normal distribution so I think that t-test can be done. Am I right? However, when Time values are large the distribution seem to become non normal. This is especially visible when average results of both methods are close to 1: Time equal to 80 and 90 for method_A Time equal to 140 and 150 for method_B So the question is: For these scenarios should I do Mann-Whitney U Test ? For some reason, doing 2 different tests ( t-test and Mann-Whitney U Test ) for same data doesn't seem right... Overall, what is the best way to do hypothesis testing with this kind of data?
