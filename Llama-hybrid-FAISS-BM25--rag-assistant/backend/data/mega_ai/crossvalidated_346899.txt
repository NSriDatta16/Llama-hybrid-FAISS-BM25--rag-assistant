[site]: crossvalidated
[post_id]: 346899
[parent_id]: 
[tags]: 
Adoption of Maxout Activation of Any Famous Neural Network?

I saw the paper on maxout as another activation in neural network and I understand it makes the neural network still a universal approximator. However, I wonder what the advantage of maxout is compared with other activations such as ReLu. Is there any famous neural network structure (such as Inception network) that actually uses this activation?
