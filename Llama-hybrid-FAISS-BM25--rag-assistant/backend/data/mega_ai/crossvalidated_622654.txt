[site]: crossvalidated
[post_id]: 622654
[parent_id]: 
[tags]: 
Is it valid to let a neural network predict the actual value y without denormalizing through the normalized data x?

Let's assume I have a regression task. My data is numerical with x as the input data and y as the target variable. What I usually do is that I normalize my data with both x and y. Then I feed it into the network (f.e. a MLP) and get a prediction. The prediction then is used to calculate the loss through mean squared error. Normally I'd simply let my loss be calculated by the MSE of the normalized prediction and normalized y. My loss is always between 0 and 1. For the final prediction I inversed the normalization algorithm and got the "real" prediction. Now I had to use two frameworks together and tried out SK-Learns Standard Scaler, which only normalizes the values of x. So I tried out to feed my normalized data x into the network and calculate the loss through the prediction and the DENORMALIZED y. The curves seem good to me and the network starts to perform fairly well even on the validation set. When I think about it in theory it simply would "reverse" the normalization by itself by learning the real values. Also the results seem to be good so I don't assume it's an actual mistake. However is the second approach even valid? Or does it simply worsen the performance of the network? Thank you!
