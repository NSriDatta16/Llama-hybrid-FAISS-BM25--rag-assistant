[site]: crossvalidated
[post_id]: 525135
[parent_id]: 
[tags]: 
Interpretation of a tightly paired learning curve with increasing loss

I am assessing models for a binary classification task and have created a model with a very strange learning curve. This is the learning curve of an sklearn AdaBoostClassifier fitted with default parameters: I've also attempted other parameter combinations and still see essentially the same shape. When using the f1-score as the learning curve metric, then the shape is more familiar: For reference I've included the learning curve of a logistic regression model trained on the same dataset: Comparing these two models from a performance standpoint, AdaBoost does much better with an f1-score of 0.72 while the logistic classifier has an f1-score of 0.66. However, the shape of the learning curve has left me uncertain as to whether I have a good model or not. Is log loss inappropriate to use with AdaBoost for some reason?
