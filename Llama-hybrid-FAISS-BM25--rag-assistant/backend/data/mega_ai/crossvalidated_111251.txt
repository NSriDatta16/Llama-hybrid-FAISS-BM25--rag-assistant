[site]: crossvalidated
[post_id]: 111251
[parent_id]: 111243
[tags]: 
I've implemented Bayesian GMM estimators in STAN before and it is pretty straight-forward. The way STAN works is that one simply defines how to calculate he log-likelihood and then a very efficient sampler (NUTS) based on Hybrid Monte Carlo is created automatically, and for the GMM likelihood one just creates the $U_i$ and $\Sigma$ by hand (which will depend on $\beta$) and increment the log-likelihood by doing something like lp__ (make sure to double check that this is correct though). It may even be possible to trick JAGS/BUGS into doing it with the "zeros trick" but STAN seems like a fantastic option for this (although I suppose it may also depend on the amount of data you have). I'm taking for granted that your pseudo-likelihood defines a valid posterior, of course! EDIT: should definitely be a Sigma inverse rather than Sigma in the formula above!
