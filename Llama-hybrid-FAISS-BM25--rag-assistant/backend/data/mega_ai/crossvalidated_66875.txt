[site]: crossvalidated
[post_id]: 66875
[parent_id]: 
[tags]: 
How do we pass from proportionality back to equality in a Bayesian derivation?

Here's an answer from one of the questions in the newsletter. I'm not trying to suggest that it's incorrect, I just don't understand why it's an acceptable practice. I know that simplifying assumptions are made all the time, but why is this useful? \begin{equation} \begin{split} p(\theta|y) &= \frac{p(y|\theta)p(\theta)}{p(y)} \\ &\propto\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}\theta^{\alpha-1}(1-\theta)^{\beta-1}*\binom{n}{y}\theta^y(1-\theta)^{n-y} \\ &\propto\theta^{\alpha-1}(1-\theta)^{\beta-1}*\theta^y(1-\theta)^{n-y} \\ &\propto\theta^{\alpha+y-1}(1-\theta)^{\beta+n-y-1} \\ &=\frac{\Gamma(\alpha+y-1)\Gamma(\beta+n-y-1)}{\Gamma(\alpha+\beta+n-1)}\theta^{\alpha+y-1}(1-\theta)^{\beta+n-y-1} \end{split} \end{equation} My issue is that we go from "proportional-to" to "equals", which is fine, but what exactly is this saying? As in, I understand how we go from each line to the line following it, but I don't understand what this tells us. Sure, we eventually get back to a Beta distribution, but only after removing a ton of values. Are these values not important? Does it mean anything profound that a Beta prior with a binomial likelihood has the property of conjugacy? It can make calculations simpler, sure, but is this useful in practice, if so why? EDIT 1: So are these constants actually thrown out, or accounted for later? I understand that constants don't matter with respect to proportionality and integration, but throwing them out completely just seems counter-intuitive to me. Sure, if you're multiplying/dividing a bunch of numbers by an expression with a constant, and all you care about is the relationship of those numbers to each other, then you can throw out constants and the relationship remains the same. Is that what's going on here? EDIT 2: From what I can tell, we get rid of constants, simplify, and then take the integral and normalize that integral. By virtue of the normalization, we're inherently accounting for the constants that we removed before. Is this correct? I think I understand now, for whatever reason, I kept thinking of the original equation as a scalar value rather than a probability distribution.
