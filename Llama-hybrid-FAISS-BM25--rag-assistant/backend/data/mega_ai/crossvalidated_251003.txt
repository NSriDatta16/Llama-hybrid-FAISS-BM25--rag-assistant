[site]: crossvalidated
[post_id]: 251003
[parent_id]: 250937
[tags]: 
I learned the loss function for logistic regression as follows. Logistic regression performs binary classification, and so the label outputs are binary, 0 or 1. Let $P(y=1|x)$ be the probability that the binary output $y$ is 1 given the input feature vector $x$. The coefficients $w$ are the weights that the algorithm is trying to learn. $$P(y=1|x) = \frac{1}{1 + e^{-w^{T}x}}$$ Because logistic regression is binary, the probability $P(y=0|x)$ is simply 1 minus the term above. $$P(y=0|x) = 1- \frac{1}{1 + e^{-w^{T}x}}$$ The loss function $J(w)$ is the sum of (A) the output $y=1$ multiplied by $P(y=1)$ and (B) the output $y=0$ multiplied by $P(y=0)$ for one training example, summed over $m$ training examples. $$J(w) = \sum_{i=1}^{m} y^{(i)} \log P(y=1) + (1 - y^{(i)}) \log P(y=0)$$ where $y^{(i)}$ indicates the $i^{th}$ label in your training data. If a training instance has a label of $1$, then $y^{(i)}=1$, leaving the left summand in place but making the right summand with $1-y^{(i)}$ become $0$. On the other hand, if a training instance has $y=0$, then the right summand with the term $1-y^{(i)}$ remains in place, but the left summand becomes $0$. Log probability is used for ease of calculation. If we then replace $P(y=1)$ and $P(y=0)$ with the earlier expressions, then we get: $$J(w) = \sum_{i=1}^{m} y^{(i)} \log \left(\frac{1}{1 + e^{-w^{T}x}}\right) + (1 - y^{(i)}) \log \left(1- \frac{1}{1 + e^{-w^{T}x}}\right)$$ You can read more about this form in these Stanford lecture notes .
