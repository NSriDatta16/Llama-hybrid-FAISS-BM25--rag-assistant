[site]: crossvalidated
[post_id]: 429638
[parent_id]: 
[tags]: 
Optimal classification rule given data, model and loss function

Setup Suppose I have a data set with a categorical variable $Y$ (with possible values $j=1,\dots,J$ ) and another variable $X$ . I wish to classify $Y$ based on the information in $X$ . For simplicity, suppose I also know the shape of the model that generated the data, though I do not know the parameter values; I will estimate them. E.g., this could be a (multinomial) logistic regression. I am facing a loss function $l(\hat y,y)$ where $\hat y$ is the predicted class and $y$ is the actual/true class. $l$ can be represented by a matrix $\mathbf{L}$ where rows correspond to actual classes and columns to predicted ones. Each off-diagonal cell $l_{ij}$ ( $i\neq j$ ) of $\mathbf{L}$ contains the loss associated with the specific misclassification. Steps taken I fit a (multinomial) logistic regression to the data. Given the fitted coefficients and a new data point with a known $X$ value $x_0$ but an unknown $Y$ value $y_0$ , I obtain the fitted class probabilities $\hat p_0$ (a vector). I wish to classify $y_0$ , i.e. obtain $\hat y_0$ so as to minimize the expected loss. Questions What is the optimal classification rule in this setting? Could you also recommend a textbook chapter on the topic? My problem is somewhat similar to Classification optimal decisions considering a loss function but my setting is frequentist and I do not have the prior distribution of classes (or class prevalence) available.
