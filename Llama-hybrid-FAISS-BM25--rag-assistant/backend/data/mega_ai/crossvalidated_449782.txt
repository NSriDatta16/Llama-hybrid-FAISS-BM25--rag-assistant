[site]: crossvalidated
[post_id]: 449782
[parent_id]: 92097
[tags]: 
There is an interesting paper proposing to maximize not the observed likelihood, but the expected likelihood Expected Maximum Log Likelihood Estimation . In many examples this gives the same results as MLE, but in some examples where it is different, it as arguably better, or at least different in an interesting way. Note that this is a pure frequentist idea, so is different from what is discussed in the other answers, where it is assumed that expectation is of the parameter itself, so some (quasi-)bayesian idea. One example : Take the usual multiple linear regression model, with normal errors. Then the log-likelihood function is (up to a constant): $$ \log L(\beta) = -\frac{n}{2}\log \sigma^2 - \frac1{2\sigma^2} (Y-X\beta)^T (Y-X\beta) $$ which can be written (with $\hat{\beta}=(X^TX)^{-1} X^T Y$ , the usual least-squares estimator of $\beta$ ) $$ \left[ -\frac{n}{2\sigma^2}+\frac1{2\sigma^4}(Y-X\hat{\beta})^T(Y-X\hat{\beta})\right]+\frac1{2\sigma^4}(\hat{\beta}-\beta)^T X^T X(\hat{\beta}-\beta) $$ The second term here is $\frac12 (\frac{\partial \log L}{\partial \beta})^T (X^T X)^{-1} \frac{\partial \log L}{\partial \beta})$ with expectation $\frac{p}{2\sigma^2}$ , so the estimating equation for $\sigma^2$ becomes $$ -\frac{n}{2\sigma^2}+\frac1{2\sigma^4}(Y-X\hat{\beta})^T (Y-X\hat{\beta}) + \frac{p}{2\sigma^4} $$ where $p$ is the number of columns in $X$ . The solution is the usual bias-corrected estimator, with denominator $n-p$ , and not $n$ , as for the MLE.
