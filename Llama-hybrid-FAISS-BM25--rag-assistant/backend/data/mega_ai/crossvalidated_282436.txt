[site]: crossvalidated
[post_id]: 282436
[parent_id]: 
[tags]: 
The Convergence Plot in Stochastic Gradient Descent (SGD)

Q1: The main question is that can I say my own coded SGD algorithm converged based on the convergence plot below? In the SGD code, the data is randomly shuffled in each epoch before calculating the gradient at each point in the order, the Robbins-Monro rule for step size is chosen, and the exponentially weighted moving average of the negative log likelihood (nll) function is used. The total epoch number is 1000, and the data set has a size of 46. The stopping criteria used is when all the abs of (estimated parameters - previous ones) This is the convergence plot when tol = 1e-5 The 2nd graph is the convergence plot when tol = 1e-4. Q2: is the phenomenon due to the small data size? Q3: A side question is that even the convergence is achieved when tol = 1e-4, the estimated parameters are different slightly every time I run the code. For example, one time is theta = c(1.83605039, 0.06326902, 0.23229382, -0.11517897), another time is theta = c(1.8138466, 0.1432576, 0.1071110, -0.1977483). Is this a problem?
