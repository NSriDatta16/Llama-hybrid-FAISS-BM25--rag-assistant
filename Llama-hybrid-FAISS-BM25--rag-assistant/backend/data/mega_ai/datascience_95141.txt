[site]: datascience
[post_id]: 95141
[parent_id]: 69756
[tags]: 
I think they did the second option. If their network is fitted to a mini batch of 2,048 states for 1,000 epochs, it will be overfitted to the sampled 2,048 states. The trained network would be less likely to beat the old one. There are numerous sample candidates. If we assume average turns of a game are 150, sample candidates are 75,000,000 states. Sampling would be done for each training iteration to reflect the many states. In that case, batch_size will be 2,048 and epochs will be 1. (Actually, they used 64 workers and the batch-size was 32 per worker.)
