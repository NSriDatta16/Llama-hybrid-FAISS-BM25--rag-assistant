[site]: datascience
[post_id]: 96987
[parent_id]: 96554
[tags]: 
You can always build your own custom attention layer using TF or Pytorch. Keras has three popular inbuilt Attention Layer now. Before we had to build our own custom layers even in Keras. The diffrence between Bahdanou and Loung is the way that the attention weights are calculated. One is additive and the other multiplicative(dot product) respectively. Both of these models the key and the value are same. For more advanced transformer techniques refer to BERT or DistilBERT. We also have various variations in BERT.
