[site]: crossvalidated
[post_id]: 189534
[parent_id]: 56950
[tags]: 
I am very late to the game, but I wanted to post to reflect some current developments in convolutional neural networks with respect to skip connections . A Microsoft Research team recently won the ImageNet 2015 competition and released a technical report Deep Residual Learning for Image Recognition describing some of their main ideas. One of their main contributions is this concept of deep residual layers . These deep residual layers use skip connections . Using these deep residual layers, they were able to train a 152 layer conv net for ImageNet 2015. They even trained a 1000+ layer conv net for the CIFAR-10. The problem that motivated them is the following: When deeper networks are able to start converging, a degradation problem has been exposed: with the network depth increasing, accuracy gets saturated (which might be unsurprising) and then degrades rapidly. Unexpectedly, such degradation is not caused by overfitting , and adding more layers to a suitably deep model leads to higher training error ... The idea is if that if you take a "shallow" network and just stack on more layers to create a deeper network, the performance of the deeper network should be at least as good as the shallow network as the deeper network could learn the exact shallow network by setting the new stacked layers to identity layers (in reality we know this is probably highly unlikely to happen using no architectural priors or current optimization methods). They observed that this was not the case and that training error sometimes got worse when they stacked more layers on top of a shallower model. So this motivated them to use skip connections and use so-called deep residual layers to allow their network to learn deviations from the identity layer, hence the term residual , residual here referring to difference from the identity. They implement skip connections in the following manner: So they view the map $\mathcal{F}(x) := \mathcal{H}(x) - x$ as some residual map. They use a skip layer connection to cast this mapping into $\mathcal{F}(x) + x = \mathcal{H}(x)$. So if the residual $\mathcal{F}(x)$ is "small", the map $\mathcal{H}(x)$ is roughly the identity. In this manner the use of deep residual layers via skip connections allows their deep nets to learn approximate identity layers, if that is indeed what is optimal, or locally optimal. Indeed they claim that their residual layers: We show by experiments (Fig. 7) that the learned residual functions in general have small responses As to why exactly this works they don't have an exact answer. It is highly unlikely that identity layers are optimal, but they believe that using these residual layers helps precondition the problem and that it's easier to learn a new function given a reference/baseline of comparison to the identity mapping than to learn one "from scratch" without using the identity baseline. Who knows. But I thought this would be a nice answer to your question. By the way, in hindsight: sashkello's answer is even better isn't it?
