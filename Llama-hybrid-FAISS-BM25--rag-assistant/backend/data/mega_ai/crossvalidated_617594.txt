[site]: crossvalidated
[post_id]: 617594
[parent_id]: 559814
[tags]: 
You can think of a neural network as being layers of feature extraction followed by a regression on the extracted features. That is, it is common for the final hidden layer of a neural network to connect every neuron to the output neuron(s). This is a generalized linear model on the final extracted features that are given in that last hidden layer. The image below that I posted yesterday shows how this works for the final hidden layer and an output neuron. However, nothing requires you to have that final regression be a linear model. Instead of $y = \sigma\left( b + \omega_{blue}x_{blue} + \omega_{red}x_{red} + \omega_{purple}x_{purple} + \omega_{grey}x_{grey} \right)$ , for some activation function $\sigma$ , you could have something like $y = \sigma\left( b + \omega_{blue}x_{blue}^{\beta_{blue}} + \omega_{red}x_{red}^{\beta_{red}} + \omega_{purple}x_{purple}^{\beta_{purple}} + \omega_{grey}x_{grey}^{\beta_{grey}} \right)$ . Then the final layer is a nonlinear regression but also differentiable. Any differentiable nonlinear regression on that final set of extracted features (which will come from your convolutional layers) could be viable. I will give a few more examples below. $$ y = \sigma\left( \dfrac{ \omega_{blue}x_{blue} + \omega_{red}x_{red} }{ \left(\omega_{purple}x_{purple}\right)^2 + \left(\omega_{grey}x_{grey}\right)^2 + 1 } \right) $$ $$ y = \sigma\left( b + \sin\left(\omega_{blue}x_{blue}\right) + \omega_{red}x_{red} + \omega_{purple}x_{purple} + \omega_{grey}x_{grey} \right) $$ $$ y = \sigma\left( b + \exp\left(\omega_{blue}x_{blue} + \omega_{red}x_{red} + \omega_{purple}x_{purple} + \omega_{grey}x_{grey} \right) \right) $$ If you have an idea for a differentiable nonlinear regression on the neurons in the final hidden layer after your convolutional layers, the math says you can use it. You might lose the ability for your network to be a universal approximator , however.
