[site]: crossvalidated
[post_id]: 339912
[parent_id]: 339878
[tags]: 
Based on your screenshot 1, model depths are 10 and 20 and min sample split are 2 and 3. If you have small number of columns (e.g. Similarly, the min sample split is only 2 or 3, which means leaves will not be split further if obs number is less than 2 or 3. Hence, your tree models put nearly each obs in each of leaves, which is over-fitted. This may also be the reason that your training set score is about 0.99 when min sample split is 2 (assume this is the measure of accuracy that 1 means no errors). No sure about the which model both plots are related to, hence I'd assume top plot is related to model 42. After depth decreases to 10 and min sample split increase (a little bit) to 3, you can see that train score decreases while cross validated score increases which make more sense in terms of their moving direction. This is because better parameters chosen to control overfit. Possible Solutions : Decrease tree depths depending on number of variables and your current model (42 and 36) actual tree depth. Increase min_sample_split to 20 or 30. This is to make your classifier more general as oppose to fit to individual obs. Try random forest method which builds lots of trees (lots of weak learners to strong learner).
