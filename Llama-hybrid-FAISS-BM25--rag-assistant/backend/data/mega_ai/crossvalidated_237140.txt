[site]: crossvalidated
[post_id]: 237140
[parent_id]: 237117
[tags]: 
I am afraid that you are misunderstanding what Bayesian inference is about in general. Bayes theorem is $$ \underbrace{p(\theta \mid X)}_\text{posterior} = \frac{\overbrace{p(X \mid \theta)}^\text{likelihood} \, \overbrace{p(\theta)}^\text{prior}}{\underbrace{p(X)}_\text{normalizing constant}} $$ To make it more concrete, you can estimate $\mu$ and $\sigma^2$ parameters from normal distribution (i.e. normal likelihood ) using data $X$, assuming normal prior for $\mu$ with hyperparameters $\mu_0$ and $\sigma^2_0$, and uniform prior for $\sigma^2$ with hyperparameters $a$ and $b$, to obtain posterior distributions for $\mu$ and $\sigma^2$: $$ X \sim \mathrm{Normal}(\mu, \sigma^2) \\ \mu \sim \mathrm{Normal}(\mu_0, \sigma^2_0) \\ \sigma^2 \sim \mathrm{Uniform}(a,b) $$ So priors are assigned to parameters of interest, not to data. You also have to specify assumed distribution of your data ( likelihood ). Finally, posterior is distribution over estimated parameters, so you need to specify what you are actually estimating.
