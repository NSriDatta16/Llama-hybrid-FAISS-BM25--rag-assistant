[site]: crossvalidated
[post_id]: 103168
[parent_id]: 
[tags]: 
Goodness of fit of an approximation of a PDF

I'd like to evaluate the performance of an algorithm which learns to approximate a discrete PDF for a latent variable given some noisy input. Now, is there a standard test to evaluate the goodness of that approximation? I would like to train the algorithm with synthetic data and then see how close the approximation is to the PDF I can compute knowing how the data was generated. I thought of computing the Kullback-Leibler divergence, but I don't know what's a good value. KL divergence is the information gain, so could I argue that eg. losing only How do I combine KL divergences from multiple trials with different input? Can I average? I suppose I might, considering that all the computed PDFs can be seen as one PDF over a multi-dimensional latent variable.
