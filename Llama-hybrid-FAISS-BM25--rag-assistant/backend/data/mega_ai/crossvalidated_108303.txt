[site]: crossvalidated
[post_id]: 108303
[parent_id]: 108284
[tags]: 
It seems that you did this for just one set of tuning parameters so yes your average accuracy over the 10 folds in 0.8. You can also try different tuning parameters, for example gbmGrid which yields the output Stochastic Gradient Boosting 157 samples 60 predictors 2 classes: 'M', 'R' No pre-processing Resampling: Cross-Validated (10 fold, repeated 10 times) Summary of sample sizes: 142, 142, 140, 142, 142, 141, ... Resampling results across tuning parameters: interaction.depth n.trees Accuracy Kappa Accuracy SD Kappa SD 1 50 0.77 0.53 0.1 0.2 1 100 0.78 0.56 0.095 0.19 1 150 0.79 0.58 0.094 0.19 ... So you can see that the accuracy score is different for every set of tuning parameters. You would then select the set which gives you the best accuracy.
