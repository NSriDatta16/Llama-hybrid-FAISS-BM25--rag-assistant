[site]: crossvalidated
[post_id]: 567543
[parent_id]: 567531
[tags]: 
I am not sure what you mean by "Fixing the pretrained word embedding", but you are supposed to refine the pre-trained model by training it on your data. This pre-trained model is just supposed to be a good initialization. However, even if you refine the pre-trained model, the result is often worse than a model trained from scratch. Pretraining is mainly used if you don't have sufficient data and/or resources to train a large model yourself. Otherwise, the model trained from scratch is often better. It just means that the population the pre-trained model was trained on is too different from your population.
