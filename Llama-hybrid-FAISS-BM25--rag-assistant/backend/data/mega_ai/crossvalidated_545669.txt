[site]: crossvalidated
[post_id]: 545669
[parent_id]: 341610
[tags]: 
For some algorithms a bad initialization may matter and may be due to the particular random seed. In such cases, it may make sense to try to find a good initialitzation (=good random seed) that then leads to a good performance (or to find a way of modifying the training to reduce such effects). However, one should really be convinced that this is going on, because what we don't want to do - as others already pointed out - is to overfit our validation set by finding a seed that happens to produce a good result due to some ill-understood combination of the noisiness of the training process and the characteristics of the validation set (or sets in cross-validation). In the particular case of the random forest algorithm, I don't think we are in a case where we want to optimize the seed, at all. What we can do instead is to increase the number of trees until the results no longer depend on the seed in any meaningful way. More trees don't lead to overfitting for RF (unlike for, say, XGBoost, for which the corresponding remedy would be to fit the model multiple times and average the predictions), more trees just takes random noise out of the validaiton set performance (and up to an extent improve performance). For RF, I'd argue such randomness is just "bad" in the sense that it obscures the best hyperparameters with noise and might be due to some chance combination of factors between training process & validation set characteristics, but we have no reason to think these fluctuations would reliably turn up on new data (such as an unseen test set). So, it makes sense to eliminate it as much as possible (to the degree that that's possible in terms of our computational budget for training and inference).
