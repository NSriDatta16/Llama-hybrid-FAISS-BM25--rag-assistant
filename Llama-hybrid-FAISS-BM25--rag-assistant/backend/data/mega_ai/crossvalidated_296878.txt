[site]: crossvalidated
[post_id]: 296878
[parent_id]: 295560
[tags]: 
First of all, shouldn't your equation (1) be the following? $$ \frac{\partial E}{\partial w_{tied}} = \frac{\partial E}{\partial f}(\frac{\partial f}{\partial h_1}\frac{\partial h_1}{\partial w_1}+\frac{\partial f}{\partial h_2}\frac{\partial h_2}{\partial w_4}) $$ As the intuition is, since we are forcing $w_1$ and $w_4$ to be the same, to update them we should just take the optimal update for each and then average (or sum) them? In a stochastic gradient descent step, we update all of the weights at once. So that doesn't change here, just for the weights in $w_{tied}$, their updates will both be the same, as per the equation above. All the other weights $w_i$ will be also updated during the step according to their normally-calculated gradients, $w_i = w_i - \text{[learning rate]} * \frac{\partial E}{\partial w_i}$. For your last question, I get no $-$ sign in front of $x_1$. I think you just need to double-check your derivation / application of the chain rule.
