[site]: crossvalidated
[post_id]: 602737
[parent_id]: 
[tags]: 
What is the proof that non-linearly separable data can't become linearly separable with the results of PCA?

Give a non-linearly separable dataset $X,$ I want to proof that after performing PCA on it, the resulting dataset is guaranteed to be still non-linearly separable. I think we could argue that we still use the eigenvectors of the original feature space to describe the transformed feature space, and if it wasn't possible to find a linear combination for a hyperplane, it's also not possible to do so in the truncated space, but not exactly sure if that's the right approach. A second approach of mine was to try to model PCA as a linear transformation altogether using a matrix for the transformation $$J(XW+b)$$ and thinking about the hyperplane in terms of $$XW+b>0$$ or $$XW+b like in SVM. Unfortunately i didn't get ahead with this approach either (didn't know how to show that the sign for each datapoints stays the same after performing the J matrix), so would be interested in how to solve this. Proof by ChatGPT (not sure if correct):
