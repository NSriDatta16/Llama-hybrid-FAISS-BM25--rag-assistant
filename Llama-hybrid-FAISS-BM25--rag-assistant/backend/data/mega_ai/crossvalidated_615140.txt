[site]: crossvalidated
[post_id]: 615140
[parent_id]: 275197
[tags]: 
Models with L1/L2 regularisation handle these hierarchies naturally. (so ridge/lasso regularisation). The reason is they impose a trade off between reducing the total error and adding/increasing a weight/coefficient: a 'region' weight can reduce error on more datapoints (ie across region's departments) than each individual deparment weight - think for example of the limiting case of only region level effects - what is the norm from having same coefficient repeated on each department's coefficient vs a single coefficient on region level (and zero on department level). Essentially penalising the weight norm means that the average region effect is placed on the region, and ('significant' deviations from the region effect are placed on the department coefficients. crossvalidating the regularisation term will determine how 'significant' the effect is: if you have a little data for each department, you are likely to have a strong regularisation and only learn region level effects.
