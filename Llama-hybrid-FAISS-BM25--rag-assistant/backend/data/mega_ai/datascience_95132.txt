[site]: datascience
[post_id]: 95132
[parent_id]: 94702
[tags]: 
In the original paper there are some clarifying statements: The four inputs to a unit in S2 are added, then multiplied by a trainable coefficient, and added to a trainable bias. The result is passed through a sigmoidal function. (p.7, col.1) Here, sigmoidal function is generic. As in classical neural networks, units in layers up to F6 compute.. This weighted sum .. is then passed through a sigmoid squashing function .. The squashing function is a scaled hyperbolic tangent. (p.8, col.1 - but see also Appendix A for details) Here, "sigmoid squashing function" is used to indicate a scaled "tanh" (remember that tanh is a rescaled logistic sigmoid function). Therefore, I think Wikipedia's suggestion to use the same "sigmoidal function" is correct. For the sake of precision, the tanh should be used.
