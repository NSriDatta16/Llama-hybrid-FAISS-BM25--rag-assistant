[site]: crossvalidated
[post_id]: 136526
[parent_id]: 
[tags]: 
What do these decision boundaries indicate in random forest and svm?

I was working on data science harvard homework problem. It is a two class classification problem in which they plot the decision boundary for random forest, svm and decision tree. The problem has 2 features. The question they asked was: there is a tradeoff between the bias and the variance of a classifier. We want to choose a model that generalizes well to unseen data. With a high-variance classifier we run the risk of overfitting to noisy or unrepresentative training data. In contrast, classifier with a high bias typically produce simpler models that tend to underfit the training data, failing to capture important regularities. Discuss the differences in the above decision surfaces in terms of their complexity and sensitivity to the training data. How do these properties relate to bias and variance? In the solution part they have written the following: Solution: The decision surfaces for the decision tree and random forest are very complex (wiggly contours with complex shapes). The decision tree is by far the most sensitive, showing only extreme classification probabilities that are heavily influenced by single points (see red stripes that seem to be drawn just to encapsulate observed "red" points). The random forest shows lower sensitivity, with isolated points having much less extreme classification probabilities. The SVM is the least sensitive, since it has a very smooth decision boundary. The complexity of the classifier corresponds to lower bias, since it can be more "true" to the training data, but this also makes the classifier more sensitive to random variations in the data, and thus increases variance. I couldnt understand what they are saying in the solution. can some one please explain me what is being explained in the figure and in the solution part?? The figures of the plot is shown below.
