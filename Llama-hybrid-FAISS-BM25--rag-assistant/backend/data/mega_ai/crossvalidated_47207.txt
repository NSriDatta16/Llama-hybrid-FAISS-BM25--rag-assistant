[site]: crossvalidated
[post_id]: 47207
[parent_id]: 32994
[tags]: 
I am late to the game, but a few notes. The $\mathbf{R}$ structure is the residual structure. In your case, the "structure" only has a single element (but this need not be the case). For Gaussian response variable, the residual variance, $\sigma^{2}_{e}$ is typically estimated. For binary outcomes, it is held constant. Because of how MCMCglmm is setup, you cannot fix it at zero, but it is relatively standard to fix it at $1$ (also true for a probit model). For count data (e.g., with a poisson distribution), you do not fix it and this automatically estimates an overdispersion parameter essentially. The $\mathbf{G}$ structure is the random effects structure. Again in your case, just a random intercept, but if you had multiple random effects, they would form a variance-covariance matrix, $\mathbf{G}$. A final note, because the residual variance is not fixed at zero, the estimates will not match those from glmer . You need to rescale them. Here is a little example (not using random effects, but it generalizes). Note how the R structure variance is fixed at 1. # example showing how close the match is to ML without separation m2 Here is the rescaling constant for the binomial family: k Now divide the solution by it, and get the posterior modes posterior.mode(m2$Sol/(sqrt(1 + k))) Which should be fairly close to what we get from glm summary(glm(vs ~mpg, data = mtcars, family = binomial))
