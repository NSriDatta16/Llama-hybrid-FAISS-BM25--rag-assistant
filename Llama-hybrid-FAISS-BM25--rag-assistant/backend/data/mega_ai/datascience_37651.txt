[site]: datascience
[post_id]: 37651
[parent_id]: 37645
[tags]: 
You can think of each feature running along its own axis on a graph. Just because we store all feature e.g. in a single DataFrame – one feature per column – it doesn't mean the data's structure is just 2d (rows and columns). This is not the case only in NLP, but in most contexts involving statistics and modelling. We can see this with your example data. There are text blocks, which you should match to a category (as far as I can tell). The initial dataset contains other meta-data, such as a desctiption of the dataset, the names of the target categories and also the location of each sample's file. We don't really care about these for the pure modelling part. So there are only text blocks, called data , and the target categories, called target . Your input is then 1d - the text blocks. I will show how to put that into a dataframe, being very verbose about dimensions and features: from sklearn.datasets import fetch_20newsgroups groups = fetch_20newsgroups() import pandas as pd # needed to use a dataframe # Get the desired parts from "groups" desired = ['data', 'target'] # we don't care about the 'filenames' ona so on # make a new dictionary with only desired key-value pairs only_data = {k: v for k, v in groups.items() if k in desired} Now we put this into a dataframe df = pd.DataFrame.from_dict(only_data) # Check the shape of the dataframe df.shape (11314, 2) So there are 11314 samples of 1 feature, to 1 target variable. This is therefore 1-dimensional input data (we don't count the target variable). When we have e.g. 50 features, explaining some target variable, it may be referred to as a 50-dimensional input space. People then may use dimensionality reduction techniques, such as Principal Components Analysis, which will attempt to squeeze the 50 dimensions into a lesser number (you can choose how many to use). In your data, you will likely pre-process the text samples to create more features. These will just be new columns in the dataframe, whose shape could become e.g. (11314, 40) if you add another 38 features, by doing things like counting words or constructing some word-embeddings.
