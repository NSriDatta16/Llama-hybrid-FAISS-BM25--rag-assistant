[site]: crossvalidated
[post_id]: 71927
[parent_id]: 69179
[tags]: 
There are many reasons why a neural network could underperform ridge regression. First, simply because ridge regression and a single node NN both return a linear fit doesn't mean that they will return the same one. In fact, they hardly ever will, unless your ridge parameter is zero. If the NN's optimization function is able to find a global minimum (which with a single neuron, it should unless you've misparametrized it badly), then it will actually be doing the equivalent of an ordinary non-penalized least squares regression. So a natural reason why ridge regression might work better (in terms of performance on the test set) is that a ridge regression shrinks coefficients in order to avoid overfitting. Try doing an ordinary linear regression and compare the results to the NN. As for the negative values, there is no way to prevent a linear fit from taking on negative values. If your $Y$ values are always positive, one simple thing to try is fitting $\text{log}(Y)$ instead of $Y$, and then exponentiate the results.
