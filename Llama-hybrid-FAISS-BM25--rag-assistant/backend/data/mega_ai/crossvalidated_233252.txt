[site]: crossvalidated
[post_id]: 233252
[parent_id]: 232727
[tags]: 
In neural networks, the "importance" of each signal is established during the learning phase. It comes hard coded in the model, rather than expressed by a nice numeric parameter. I'm afraid you may not be able to manually alter the importance of a feature. One way of forcing it, if you are using dropout, is to avoid it on the signal the user judges "important". Other than this, I really can't see how to force it. Please, notice that I'm using "forcing" because what you want to do is ... counter-intuitive for almost any machine learning classifier.
