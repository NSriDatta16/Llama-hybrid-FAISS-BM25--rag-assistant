[site]: stackoverflow
[post_id]: 3498502
[parent_id]: 3498491
[tags]: 
robots.txt is the way to tell spiders what to crawl and what to not crawl. If you put the following in the root of your site at /robots.txt: User-agent: * Disallow: / A well-behaved spider will not search any part of your site. Most large sites have a robots.txt, like google User-agent: * Disallow: /search Disallow: /groups Disallow: /images Disallow: /news #and so on ...
