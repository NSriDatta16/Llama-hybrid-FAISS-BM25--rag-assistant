[site]: datascience
[post_id]: 62357
[parent_id]: 62346
[tags]: 
J.D., welcome to Data Science Exchange. First, let's start with the following question: Based on what target distribution are you saying that your accuracy is low? Every problem in modelling, Regression, Binary Classification, Multiclass Classification has a baseline so we can say that our model has an acceptable performance. Sometimes, even humans can be our baseline. Also, what is your dataset size? Second, do you have a balanced dataset? What I mean is, after you calculate your baseline, you might have a class, let's say no malware code that dominates your dataset. You should assign this by using method for imbalanced datasets such as Oversampling and Undersampling. You can read more about here . Lastly, let's ignore everything I wrote and focus on the problem. Yes, you are correct, Bag of Words will ignore order in your data since it will just count the appearance of each word. I will list a few things you can try: You are using SVC class from sklearn. From docs I see the default kernel is rbf, have you tried using linear? Also, you can use LinearSVC. Try out RandomForest models, they perform really good even in text datasets. From CountVectorizer class, you could vary ngram_range parameters. Basically, it will create features based on grams, so let's say you use a 3-gram approach, then you will have for your first row: push_r12_push_rbp counting as one feature. Also, you could try Tf-Idf Vectorizer . TF-IDF Vectorizer is based on an algorithm where not only the count of words is taken into account but also the appearance in each document. Putting in simple terms, if a specific word appears too much in your dataset it will have a high inverse value and decrease its feature value, since this word will not be useful to differentiate your class. You can read more about it here: https://www.quora.com/How-does-TF-IDF-work Lastly, but not less important, you could try using a more advanced technique, WordEmbeddings, for example. It is an algorithm that will create real vectors from your text taking in consideration the enclosing words for each command. It is a little more complicate than that, again, you can learn more here . Note that for word embeddings to work properly, you should not have a small dataset. As a code example you can use this notebook of mine as a guidance . I hope this help.
