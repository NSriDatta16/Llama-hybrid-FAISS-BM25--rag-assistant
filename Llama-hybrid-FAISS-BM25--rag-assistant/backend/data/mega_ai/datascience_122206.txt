[site]: datascience
[post_id]: 122206
[parent_id]: 
[tags]: 
Scikit-learn's SAMME AdaBoost error fraction implementation

I am taking a look at scikit-learn's discrete SAMME implementation and came across the following logic for computing the weighted error fraction. # Instances incorrectly classified incorrect = y_predict != y # Error fraction estimator_error = np.mean(np.average(incorrect, weights=sample_weight, axis=0)) My question: Why the use of np.mean(...) and np.average(...) ? At least in the following example, the output would be unchanged if the outer np.mean(...) call were removed. incorrect = np.array([1, 0, 0, 0, 0, 1]) weights = np.array([0.2, 0.2, 0.2, 0.1, 0.1, 0.2]) average = np.average(incorrect, axis=0, weights=weights) mean_average = np.mean(average) print("average: ", average) print("mean(average): ", mean_average) Output: average: 0.4 mean(average): 0.4 Since the documentation for the AdaBoostClassifer states that y should be an array like , I don't think I am missing anything about the dimensionality. But then, what am I missing?
