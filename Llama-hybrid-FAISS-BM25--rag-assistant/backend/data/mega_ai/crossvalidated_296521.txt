[site]: crossvalidated
[post_id]: 296521
[parent_id]: 
[tags]: 
Generative Adversarial Network, discriminator loss becomes NAN at the same iteration even in different settings

I am training a GAN model, and I am having a hard time fixing a NAN loss problem for my discriminator. Specifically, my discriminator's loss becomes NAN at the exact same iteration even with different hyper parameters. For instance, even if I use relu, tanh, leaky relu, the loss becomes NAN at iteration 4837. I've tried different learning rates from 1e-1 to 1e-5, but it does not solve the issue. At first, I thought it was a vanishing gradient problem, but no matter how big the loss is an iteration before, it becomes NAN at 4837. Has anyone experienced this issue? I'm using python, tensorflow, and adam optimizer.
