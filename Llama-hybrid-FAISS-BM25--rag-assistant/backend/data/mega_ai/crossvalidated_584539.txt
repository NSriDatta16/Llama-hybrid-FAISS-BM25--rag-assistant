[site]: crossvalidated
[post_id]: 584539
[parent_id]: 584456
[tags]: 
This value is computed as the average of the distance between units within a pair on the given covariate. It's easiest to think about this with 1:1 matching. Consider a matched pair, and take the difference between the treated unit in that pair's value of the covariate and the control unit in that pair's value of the covariate. Then take the absolute value of this difference to make it a distance. Then take the average of these distances across all pairs. When standardize = TRUE (the default), the average is then standardized in the same way the standardized mean differences are. It looks like you were just taking the mean of the covariate, which is not right. Taking the mean of the distance measure (i.e., the propensity score) is not right either. You have to compute the differences within pairs between the treated and control members of each pair. So, for example, the value of 0.164750 for alter means that on average, treated and control units within a pair differ on alter by .165 standard deviations. This is quite a low value and indicates the units were closely matched. Had they been exactly matched, this value would be 0. Placing a caliper on a variable is one way to make this value smaller, possibly by discarding units. When comparing two matching specification with similar levels of balance, the specification with lower pair distances should be preferred; this is the main thesis of King and Nielsen (2019) .
