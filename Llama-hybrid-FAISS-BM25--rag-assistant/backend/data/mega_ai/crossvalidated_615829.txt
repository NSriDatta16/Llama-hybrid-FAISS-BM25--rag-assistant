[site]: crossvalidated
[post_id]: 615829
[parent_id]: 
[tags]: 
Shape of biases in Transformer's Feedforward Network

In transformer network ( Vaswani et al., 2017 ), the feedforward networks have equation: $\mathrm{FNN}(x) = \max(0, xW_1 + b_1) W_2 + b_2$ where $x \in \mathbb{R}^{n \times d_\mathrm{model}}$ , $W_1 \in\mathbb{R}^{d_\mathrm{model} \times d_{ff}}$ , $W_2 \in\mathbb{R}^{d_{ff} \times d_\mathrm{model}}$ . We know that the biases $b_1$ and $b_2$ are vectors. But, for the equation to work the shape of $b_1$ and $b_2$ must agree, i.e., $b_1 \in\mathbb{R}^{n \times d_{ff}}$ and $b_2 \in\mathbb{R}^{n \times d_\mathrm{model}}$ . My question: is it true that $b_1 = \begin{bmatrix} (b_1)_{1} & (b_1)_{2} & \dots & (b_1)_{d_{ff}}\\ (b_1)_{1} & (b_1)_{2} & \dots & (b_1)_{d_{ff}} \\ \vdots & \vdots & & \vdots \\ (b_1)_{1} & (b_1)_{2} & \dots & (b_1)_{d_{ff}} \end{bmatrix}$ and $b_2 = \begin{bmatrix} (b_2)_{1} & (b_2)_{2} & \dots & (b_2)_{d_\mathrm{model}}\\ (b_2)_{1} & (b_2)_{2} & \dots & (b_2)_{d_\mathrm{model}} \\ \vdots & \vdots & & \vdots \\ (b_2)_{1} & (b_2)_{2} & \dots & (b_2)_{d_\mathrm{model}} \end{bmatrix}$ ?
