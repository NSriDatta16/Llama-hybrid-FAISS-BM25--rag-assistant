[site]: datascience
[post_id]: 93115
[parent_id]: 93036
[tags]: 
They key is that the 768 dimensional vector of the transformer is the size of a single input... let me explain. You start with a variable length audio input This is passed through a temporal CNN network, which will give you outputs, called $z_1$ to $z_T$ by the paper - where T is variable across the batch and is the number of timesteps in a particular audio input (different for different audio inputs). Each of these $z$ are passed as 768 dimensional vectors to the transformer, i.e. there are $T$ 768-dim vectors being passed to your transformer As you can imagine, practically you would need to pass a mask to the transformer too, so it knows what the variable $T$ is for the different inputs in your batch.
