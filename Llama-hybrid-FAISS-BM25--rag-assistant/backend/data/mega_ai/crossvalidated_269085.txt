[site]: crossvalidated
[post_id]: 269085
[parent_id]: 268983
[tags]: 
Note 1: It is better to split your data into three sets instead of two. That is, split your data into training set , validation set , and test set . The validation set is used in step 2 of your diagram, where you train a model on your training set and evaluate the performance of your hyperparameters based on the validation set . It is not true to test the hyperparameters based on training set since it will probably find the hyperparameter that fits your training data, hence causes overfitting problem. Note 2: The evaluations made in steps 2 and 4 depends on the particular split of your data. Because of this, it is suggested to (i) shuffle your data before splitting, (b) repeat your splitting many times and then report the average performance over these trials. Another solution is to use k-fold cross validation. The most accurate performance evaluation is obtained by leave one out method. The leave one out is really slow, and in the real world applications, it is usually enough to use 10-fold cross validation. Note 3: You are right that if the training distribution is different from test distribution, the learned model and the evaluated performance is not suitable for test data. The topic of this issue in machine learning is domain adaptation . Note 4: Since your data is imbalanced binary, I recommend using f-measure. The precision, recall, and accuracy are not enough to evaluate the performance of a learned model for imbalanced data. Finally, "Andrew Ng", a well-known researcher in machine learning is working on a book called "Machine Learning Yearning". This book focuses on practical aspects of machine learning. Each chapter has around 2 pages. As I know, 12 chapters of this book is released. These chapters are related to your questions and I recommend you to read them.
