[site]: crossvalidated
[post_id]: 237905
[parent_id]: 237085
[tags]: 
The answer is tucked in the abstract (emphasis mine): "We prove that, given training under any $\epsilon$-soft policy [...] to the action-value function for an arbitrary target policy ." The authors also write, "We always use $\pi$ to denote the target policy, the policy that we are learning about." Epsilon-soft is, as defined here , synonymous with epsilon-greedy: An epsilon-soft policy chooses one action with probability $(1 - \epsilon + \frac{\epsilon}{|A(s)|)}$ and all other actions with probability $\frac{\epsilon}{|A(s)|)}$, where $|A(s)|$ is the number of actions in state $s$. This is equivalent to saying that the policy is to choose a random action with some small probability epsilon, and another single (dominant) action otherwise. Wiki defines epsilon-greedy similarly: One such method is $\epsilon$-greedy, when the agent chooses the action that it believes has the best long-term effect with probability $1-\epsilon$, and it chooses an action uniformly at random, otherwise. That is, there are no restrictions on the target policy $\pi$; $b$ is epsilon-soft.
