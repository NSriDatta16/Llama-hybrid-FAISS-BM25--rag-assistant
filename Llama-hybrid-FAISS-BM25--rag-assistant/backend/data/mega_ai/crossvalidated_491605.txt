[site]: crossvalidated
[post_id]: 491605
[parent_id]: 491594
[tags]: 
What you defined in $P(Y|X)=\int P(Y|X,\omega)P(\omega)d\omega$ is the prior predictive function , which is generally used to check if the prior distribution for $\omega$ is reasonable. Notice that the distribution of $\omega$ that appears in this formula is the prior $P(\omega)$ , not the posterior $P(\omega|X,Y)$ . Now, we can define $x^*, y^*$ as new random variables such that: $$y^*|\omega,x^*\sim Y_i|\omega,X_i$$ That is, the new data will follow the same conditional distribution as the training data. Why do we create a new random variable if it has the same distribution as $Y$ ? Because, as good Bayesians, we want to condition on $Y$ . It wouldn't make much sense to condition $Y$ on itself, as it would result in a singular distribution. Also, $Y$ is usually taken to be a vector containing all the outputs of the training dataset. Now, the random variable $y^*$ depends only on $x^*$ and $\omega$ , and $\omega$ depends on $X$ and $Y$ . Using this (in)dependence structure and the total probability law, we have: $$\begin{align} P(y^*|x^*,X,Y)&=\int P(y^*,\omega|x^*,X,Y)d\omega\\ &=\int P(y^*|x^*,\omega)P(\omega|X,Y) \end{align}$$ Which is the posterior predictive distribution.
