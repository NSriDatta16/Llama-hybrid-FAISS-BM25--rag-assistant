[site]: stackoverflow
[post_id]: 4492673
[parent_id]: 1717049
[tags]: 
You could exclude all requests that come from a User Agent that also requests robots.txt . All well behaved bots will make such a request, but the bad bots will escape detection. You'd also have problems with false positives - as a human, it's not very often that I read a robots.txt in my browser, but I certainly can. To avoid these incorrectly showing up as bots, you could whitelist some common browser User Agents, and consider them to always be human. But this would just turn into maintaining a list of User Agents for browsers instead of one for bots. So, this did-they-request-robots.txt approach certainly won't give 100% watertight results, but it may provide some heuristics to feed into a complete solution.
