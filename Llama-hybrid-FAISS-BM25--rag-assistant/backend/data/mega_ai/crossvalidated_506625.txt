[site]: crossvalidated
[post_id]: 506625
[parent_id]: 
[tags]: 
Question about the expression for the generative model in latent Dirichlet allocation (LDA)

I think my question is rather straightforward though I had to include rather a lot of detail for it to make sense. I'm following Bayesian Methods for Machine Learning course on Coursera and the following expression is given for LDA: $$ P(W,Z,\theta) = \prod_{d=1}^D P(\theta_d) \prod_{n=1}^{N_d} P(z_{d_{n}} | \theta_d) P(w_{d_{n}} | z_{d_{n}}) $$ where D denotes the corpus of documents W words in each document Z latent variable, topic of each word $\theta$ latent variables, distribution of topics per document In the subsequent video, describing how to derive the variational EM update equations, some of the above terms were substituted and some new notation was introduced: $P(\theta_d)$ is substituted by the Dirichlet distribution $ \theta_{dz_{dn}} = P(z_{d_{n}} | \theta_d)$ $ \phi_{z_{dn}w_{dn}} = P(w_{d_{n}} | z_{d_{n}})$ and with this, the previous expression became (ref: start of "LDA: E-step, theta" video) $$ \log P(\theta, Z, W) = \sum_{n=1}^{N_d} \left[ \sum_{t=1}^{T} (\alpha_t -1) \log \theta_{dt} + \sum_{n=1}^{N_d}\sum_{t=1}^{T} [ z_{dn} = t] \left( \log \theta_{dt} + \log \phi_{{tw}_{{dn}}}\right) \right] + \mathrm{const} $$ Rules of logarithms aside, I'm confused by the second term in the inner sum, in particular, I don't understand how $z_{dn}$ has been dealt with, i.e. how $$ \prod_{n=1}^{N_d} P(z_{d_{n}} | \theta_d) P(w_{d_{n}} | z_{d_{n}}) $$ is equivalent to $$ \sum_{n=1}^{N_d}\sum_{t=1}^{T} [ z_{dn} = t] \left( \log \theta_{dt} + \log \phi_{{tw}_{{dn}}}\right) $$ References Bayesian Methods for Machine Learning, Coursera, Week 3, videos: "Latent Dirichlet Allocation" and "LDA: E-step, theta".
