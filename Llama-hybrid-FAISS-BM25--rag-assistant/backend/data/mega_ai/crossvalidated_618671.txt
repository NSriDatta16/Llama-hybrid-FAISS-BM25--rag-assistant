[site]: crossvalidated
[post_id]: 618671
[parent_id]: 
[tags]: 
Confusion with the "lower bound"-term in diffusion models

I am trying to understand the maths of diffusion models following this video explanation on youtube and this blog post . Here is what how I understood it so far: The overall goal is, that we want to maximize the log likelihood $log(p_\theta(x_0))$ . We know that maximizing the log likelihood equals minimizing the negative log likelihood. Since we are not able to calculate the log likelihood directly, we use the “variational bound on negative log likelihood” (Ho et al., p. 2) : $$-log(p_\theta(x_0)) \leq -log(p_\theta(x_0)) + KL(q(x_{1:T}|x_0) \parallel p_\theta(x_{1:T}|x_0))$$ The above inequation holds, since the KL-term is always positive, hence, the right-hand side will always be larger than the left-hand side. The right-hand side will be reformulated as a loss function that will be minimized during our model training. If we minimize the right hand side, we automatically minimize the left-hand side and, therefore, our goal of minimizing the negative log-likelihood is reached. My question is: Why is it called a lower-bound if the bound is always larger and comes, so to say, from above? Shouldn't it be an “upper bound”?
