[site]: datascience
[post_id]: 88159
[parent_id]: 87906
[tags]: 
Thank-you!! I'd also missed that multiply in my (fairseq transformer) code study, and it helps clear up a mystery that I'd noted: the (sinusoidal, non-learned) positional embeddings are initialized with a range of -1.0 to +1.0, but the word-embeddings are initialized with a mean of 0.0 and s.d. of embedding_dim ** -0.5 (0.044 for 512, 0.03125 for 1024). So, on the face of it, the positional embeddings would overwhelm any signal coming from the word embeddings. But now I can see word embeddings are scaled by math.sqrt(embed_dim) (22.6 for 512, 32 for 1024), it makes sense again. Following the links in the other answer , it seems it is done this way because the same embeddings can be used in other parts of the transformer model, and that has decided the initialization values.
