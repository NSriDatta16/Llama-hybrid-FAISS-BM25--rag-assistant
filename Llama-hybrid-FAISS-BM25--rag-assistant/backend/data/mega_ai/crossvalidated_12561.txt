[site]: crossvalidated
[post_id]: 12561
[parent_id]: 12526
[tags]: 
I think Dikran (+1) is right pointing to no-free-lunch theorems and the ad hoc nature working with missing values imputations. Best is indeed highly dependent on a particular case you deal with. Moreover the optimality criterion is unclear even if you do some Monte Carlo simulations fixing data generating process, the conclusions won't prove the optimality. You might state though that the data does not contradicts (yet) the fact that a particular imputation is superior to this particular data (this is the only thing you can show by simulations). Thus I only can give some recommendations based on the personal recent experience. It seems that Expectation-Maximization (EM) for time series imputations based on data rich data sets (in the context of factor models to be more precise) returns visually acceptable results for scaled (standardized data) data. The imputed data may be easily unscaled to the original units, thus it is also in favor of EM method as applied to time series. Though to fasten the convergence of EM method I would recommend to impute NA by interpolation (in R you may consider na.approx for linear and na.spline for cubic spline approximations) and then run the iterations. The described method worked pretty well for macroeconomic time series, for (ultra)high frequency financial data the nature of missing values may be important. I also vote for the Dikran's suggestion regarding multiple imputation (MI). You may need additional flexibility that multiple imputation provides you. The only thing as is left unclear is how both methods work with low signal/noise ratio as the common signal may be dominated by the idiosyncratic volatility within the response time series.
