[site]: crossvalidated
[post_id]: 601576
[parent_id]: 
[tags]: 
Why is R^2 negative in my multiple linear regression model in python?

i want to evaluate the organization based on the number of satisfied customers and the r^2 is negative this is the original data SECTOR price profit INSPECTION licenses PR CS(S) CS(NOT S) A(s) A (nonS) 0 A 3809 1643 6834.0 499.0 4053 203.0 45.0 NaN NAN 1 B 18608 16270 6828.0 2815.0 10923 35.0 5.0 1980.0 200 2 C 3814 1861 2375.0 509.0 2107 99.0 43.0 NaN NaN 3 A 15869 20293 2595.0 2206.0 5285 30.0 5.0 1150.0 NaN 4 B 5663 1881 3629.0 734.0 5667 220.0 55.0 NaN 565.0 and A(s) stands for the number of satisfied customers for the whole sectors meaning 200 include the services provided by sector A B and C i foucused on sector B and does it affect A(s) or not converted Sector so dummy and then deleted A and C sectors this is what i have now df1.corr() price profit INSPECTION licenses PR A(s) CLEARANCE 1.000000 0.376304 0.211653 -0.044924 0.397780 0.389236 PERMITS 0.376304 1.000000 -0.021812 -0.158237 0.089504 0.373245 INSPECTION 0.211653 -0.021812 1.000000 0.573478 0.438797 0.245204 Facilities licenses -0.044924 -0.158237 0.573478 1.000000 0.050931 -0.164353 PR 0.397780 0.089504 0.438797 0.050931 1.000000 0.497360 x = np.array(df1.drop(['A(s)'], axis=1)) y = df1['A(s)'].values from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 1/3, random_state = 0) from sklearn.linear_model import LinearRegression regressor = LinearRegression() regressor.fit(X_train, y_train) # Predicting the Test set results y_pred = regressor.predict(X_test) from sklearn.metrics import r2_score r2_score(regressor.predict(X_test), y_test) the result of r^2 -0.6052320362843366 i do not know why the sign is negative please help and thank you
