[site]: datascience
[post_id]: 80747
[parent_id]: 
[tags]: 
When does it make sense to choose gradient descent for SVM over liblinear?

I understand using gradient descent methods with SVM is intractable if you've used the kernel trick. In that case, best to use libsvm as your solver. But in the case that you are not using a kernel and simply treating it as a linear separation problem, when does it make sense to use gradient descent as your solver? As I see it, liblinear is $O(N)$ time and doesn't require hyperparameter tuning. In some past tests , liblinear has converged to a lower error rate at much faster rate than gradient-based methods. Yet, Sklearn's own tests show it can be faster in many cases. When is it optimal to use gradient-based methods with the SVM? Is it with a certain sized dataset or data that is highly linear and convex? What heuristics or explanations are available?
