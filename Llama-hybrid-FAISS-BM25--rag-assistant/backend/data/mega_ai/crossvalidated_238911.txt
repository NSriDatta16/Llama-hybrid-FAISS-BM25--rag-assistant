[site]: crossvalidated
[post_id]: 238911
[parent_id]: 238895
[tags]: 
If we have infinite data and unlimited computational power, any function can be approximated with a very complicated model, such as, neural network with infinite hidden unit. In real world, when we have limited data / samples from the function and limited computational power, simple functions (say, linear function, piecewise functions) may easier to learn. This is because the samples can be more "representative to the function". Think about a line, 2 samples from the line, would fix the function. However, it is tricky to define what is "simple function", because it is heavily depending on what models we use. For example, a simple sine function is "easy" to learn with Fourier basis, but "hard" to learn with polynomial basis. On the other hand, a linear function is hard to learn with "Fourier basis". Similarly, a piecewise constant function is easy to learn with tree. etc. Therefore, it is impossible to say easy or hard, the function is "easy to learn" when you have enough data, and you choose the "appropriate" class of function to approximate.
