[site]: crossvalidated
[post_id]: 78634
[parent_id]: 78609
[tags]: 
Point the first - it may be worth going back to rgb color. It's rarely good to throw away data, and the magnitude of the rgb vector isn't the only way to represent brightness - perceived brightness is different, as is value in HSV. But putting that to one side and dealing with the data you do have, have you considered forming this as a classification problem instead of a modelling one, and doing some machine learning? You have an input, which is a vector with 12 real values in it (the brightness readings). You have an output, which is a vector of 12 binary values (1=inlier, 0=outlier). Get several sets of brightness reading and hand label them yourself, showing which brightness reading in each set is an inlier/outlier. Something like this: x1 = {212.0, 209.6, 211.5, $\ldots$ , 213.0}, y1 = {1,0,1, $\ldots$,1} x2 = {208.1, 207.9, 211.2, $\ldots$ , 208.2}, y2 = {1,1,0, $\ldots$,1} x3 = {223.4, 222.9, 222.8, $\ldots$ , 223.0}, y3 = {1,1,1, $\ldots$,1} $\ldots$ Then, run the whole lot through a classifier of some sort: You could use a single classifier which outputs 12 different binary values - a neural network would let you set this up pretty easily. Or, you could use a standard binary classifier (e.g. SVMlite ) and train 12 different models, one classifying whether each element of the output is an inlier/outlier. And you're done! No need to fuss trying to find the 'rule' which separates inliers from outliers yourself. Just get a few sets of data which look sensible and let the machine do that for you :) ~~~ EDIT: Incidentally, your proposed method, where you iteratively fit a gaussian then classify each sample more than 2 standard deviations away as an outlier, looks a lot like an expectation maximisation algorithm. Something like this: A single gaussian component (modelling the inliers) A uniform background component (the outliers) Some prior probability of each that depends in a non-obvious way on the width of the gaussian (the 'classify at 2 standard deviations' rule). Hard classification at the expectation step. If you do go down that route it may be worth googling for EM algorithms and checking what assumptions you're building into your model.
