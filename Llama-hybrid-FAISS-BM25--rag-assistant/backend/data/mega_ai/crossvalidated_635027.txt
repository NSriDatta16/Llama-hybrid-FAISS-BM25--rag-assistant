[site]: crossvalidated
[post_id]: 635027
[parent_id]: 
[tags]: 
Is it better to use KL Divergence for soft labels instead of cross entropy?

So I was going through this paper: Align before Fuse: Vision and Language Representation Learning with Momentum Distillation (2017) by Junnan Li et al., in this they perform contrastive loss using KL divergence. $$ L_{\text{ITC}} = (1 - \alpha) L_{\text{contrast}} + \frac{\alpha}{2} \left[ \text{KL} \left( q^{i2t} \parallel p^{i2t} \right) + \text{KL} \left( q^{t2i} \parallel p^{t2i} \right) \right] $$ In this equation, LITC-baseâ€‹ is the base image-text contrastive loss, $\alpha$ is a weighting factor, DKL represents the KL divergence, $q^{i2t}$ and $q^{t2i}$ are the predicted distributions (from image to text and text to image, respectively), and $p^{i2t}$ and $p^{t2i}$ are the target distributions for the KL divergence term. However, in the implementation , cross-entropy loss is used, not explicitly KL divergence.KL divergence is used for contrastive loss with soft labels. This approach theoretically avoids large constant entropy in loss graphs as is seen in the image, potentially obscuring decreases even though entropy is not used while back propagation. However, the implementation utilizes cross-entropy loss. This raises questions about why cross-entropy is chosen over KL divergence as specified in the paper, particularly considering the impact on interpreting training loss curves since we wont be able to see the decrease in KL divergence due to the high entropy obscuring it and the paper clearly mentions the usage of soft pseudo labels for contrastive learning. Is this a practical decision for implementation, or does it reflect a deeper aspect of the model's learning process? with torch.no_grad(): self._momentum_update() image_embeds_m = self.visual_encoder_m(image) image_feat_m = F.normalize(self.vision_proj_m(image_embeds_m[:,0,:]),dim=-1) image_feat_all = torch.cat([image_feat_m.t(),self.image_queue.clone().detach()],dim=1) text_output_m = self.text_encoder_m.bert(text.input_ids, attention_mask = text.attention_mask, return_dict = True, mode = 'text') text_feat_m = F.normalize(self.text_proj_m(text_output_m.last_hidden_state[:,0,:]),dim=-1) text_feat_all = torch.cat([text_feat_m.t(),self.text_queue.clone().detach()],dim=1) sim_i2t_m = image_feat_m @ text_feat_all / self.temp sim_t2i_m = text_feat_m @ image_feat_all / self.temp sim_targets = torch.zeros(sim_i2t_m.size()).to(image.device) sim_targets.fill_diagonal_(1) sim_i2t_targets = alpha * F.softmax(sim_i2t_m, dim=1) + (1 - alpha) * sim_targets sim_t2i_targets = alpha * F.softmax(sim_t2i_m, dim=1) + (1 - alpha) * sim_targets sim_i2t = image_feat @ text_feat_all / self.temp sim_t2i = text_feat @ image_feat_all / self.temp loss_i2t = -torch.sum(F.log_softmax(sim_i2t, dim=1)*sim_i2t_targets,dim=1).mean() loss_t2i = -torch.sum(F.log_softmax(sim_t2i, dim=1)*sim_t2i_targets,dim=1).mean()
