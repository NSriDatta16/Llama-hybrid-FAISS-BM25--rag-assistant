[site]: crossvalidated
[post_id]: 459175
[parent_id]: 459030
[tags]: 
As the comments point out, it is very uncommon to test whether forecasts are statistically significantly different from the actuals. I have been forecasting for 14 years now, both academically and practically, and I have never seen this. If you really want to do this, you could calculate errors as $y_t-\hat{y}_t$ and analyze these. I would start by collecting these over time, so you get a time series, and then seeing whether there are any time series dynamics in there. I would expect so: if sales were lower than expected in one period, then inventory piles up, and it may take a while to sell the inventory off, so you might have a high inventory for multiple periods. Or if your business is seasonal, it might make sense to increase inventories in certain periods, which may lead to seasonalities in your forecast errors. If you believe your errors have no dynamics, you could apply a standard t test. Or a multilevel model that includes the SKU as a grouping factor. This page explains common ways of assessing forecast accuracy ( Ulfelder linked to a PDF version, but the entire online textbook is very much worth reading). Note that some error measures may incentivize you to bias your forecast, e.g., the MAPE or the MAE . You can then start thinking about whether your forecasts are "good enough". This may be helpful. Finally, you may want to think about whether your errors are actually meaningful. You are comparing forecasts of required inventory levels against actual inventory levels. Where do safety amounts come in? Did Marketing already include safety amounts in their forecasts, or were their forecasts expectation forecasts, so Operations added safety amounts on top? (Or did both teams add safety amounts because of miscommunications?) How do you account for logistical rounding or batch sizes? Does it actually make sense for Marketing to forecast inventory , and shouldn't they rather forecast sales (which I believe is far more common)?
