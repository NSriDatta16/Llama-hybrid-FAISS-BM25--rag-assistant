[site]: crossvalidated
[post_id]: 509798
[parent_id]: 
[tags]: 
Attention dropout, where was it proposed/used first?

Attention dropout (dropout on the attention weights) is very common for the Transformer model. In the original Attention is all you need paper, dropout is mentioned, but not for the attention weights. However, it is already part in the initial public push of Tensor2Tensor . Where was it proposed first? Where was it used first? Was that written down anywhere?
