[site]: crossvalidated
[post_id]: 391245
[parent_id]: 391225
[tags]: 
In the Learning Task Parameters section in XGBoost Doc , there are various loss functions. Couple of them are multi:softmax and multi:softprob , which are exactly what you want. Both uses softmax objective function, and according to the documentation, the only difference is their output format while making predictions. As far as I see, the python imp. already uses binary:logistic as its objective function (in version 0.81 ) by default , and if number of classes is larger than $2$ , this objective is automatically converted to multi:softprob in any case.
