[site]: crossvalidated
[post_id]: 201080
[parent_id]: 200982
[tags]: 
When performing Bayesian inference, we operate by maximizing our likelihood function in combination with the priors we have about the parameters. This is actually not what most practitioners consider to be Bayesian inference. It is possible to estimate parameters this way, but I would not call it Bayesian inference. Bayesian inference uses posterior distributions to calculate posterior probabilities (or ratios of probabilities) for competing hypotheses. Posterior distributions can be estimated empirically by Monte Carlo or Markov-Chain Monte Carlo (MCMC) techniques. Putting these distinctions aside, the question Do Bayesian priors become irrelevant with large sample size? still depends on the context of the problem and what you care about. If what you care about is prediction given an already very large sample, then the answer is generally yes, the priors are asymptotically irrelevant*. However, if what you care about is model selection and Bayesian Hypothesis testing, then the answer is no, the priors matter a lot, and their effect will not deteriorate with sample size. *Here, I am assuming that the priors aren't truncated/censored beyond the parameter space implied by the likelihood, and that they aren't so ill-specified as to cause convergence issues with near zero-density in important regions. My argument is also asymptotic, which comes with all the regular caveats. Predictive Densities As an example, let $\mathbf{d}_N = (d_1, d_2,...,d_N)$ be your data, where each $d_i$ signifies an observation. Let the likelihood be denoted as $f(\mathbf{d}_N\mid \theta)$, where $\theta$ is the parameter vector. Then suppose we also specify two separate priors $\pi_0 (\theta \mid \lambda_1)$ and $\pi_0 (\theta \mid \lambda_2)$, which differ by the hyper-parameter $\lambda_1 \neq \lambda_2$. Each prior will lead to different posterior distributions in a finite sample, $$ \pi_N (\theta \mid \mathbf{d}_N, \lambda_j) \propto f(\mathbf{d}_N\mid \theta)\pi_0 ( \theta \mid \lambda_j)\;\;\;\;\;\mathrm{for}\;\;j=1,2 $$ Letting $\theta^*$ be the suito true parameter value, $\theta^{j}_N \sim \pi_N(\theta\mid \mathbf{d}_N, \lambda_j)$, and $\hat \theta_N = \max_\theta\{ f(\mathbf{d}_N\mid \theta) \}$, it is true that $\theta^{1}_N$, $\theta^{2}_N$, and $\hat \theta_N$ will all converge in probability to $\theta^*$. Put more formally, for any $\varepsilon >0$; $$ \begin{align} \lim_{N \rightarrow \infty} Pr(|\theta^j_N - \theta^*| \ge \varepsilon) &= 0\;\;\;\forall j \in \{1,2\} \\ \lim_{N \rightarrow \infty} Pr(|\hat \theta_N - \theta^*| \ge \varepsilon) &= 0 \end{align} $$ To be more consistent with your optimization procedure, we could alternatively define $\theta^j_N = \max_\theta \{\pi_N (\theta \mid \mathbf{d}_N, \lambda_j)\} $ and although this parameter is very different then the previously defined, the above asymptotics still hold. It follows that the predictive densities, which are defined as either $f(\tilde d \mid \mathbf{d}_N, \lambda_j) = \int_{\Theta} f(\tilde d \mid \theta,\lambda_j,\mathbf{d}_N)\pi_N (\theta \mid \lambda_j,\mathbf{d}_N)d\theta$ in a proper Bayesian approach or $f(\tilde d \mid \mathbf{d}_N, \theta^j_N)$ using optimization, converge in distribution to $f(\tilde d\mid \mathbf{d}_N, \theta^*)$. So in terms of predicting new observations conditional on an already very large sample, the prior specification makes no difference asymptotically . Model Selection and Hypothesis Testing If one is interested in Bayesian model selection and hypothesis testing they should be aware that the effect of the prior does not vanish asymptotically. In a Bayesian setting we would calculate posterior probabilities or Bayes factors with marginal likelihoods. A marginal likelihood is the likelihood of the data given a model i.e. $f(\mathbf{d}_N \mid \mathrm{model})$. The Bayes factor between two alternative models is the ratio of their marginal likelihoods; $$ K_N = \frac{f(\mathbf{d}_N \mid \mathrm{model}_1)}{f(\mathbf{d}_N \mid \mathrm{model}_2)} $$ The posterior probability for each model in a set of models can also be calculated from their marginal likelihoods as well; $$ Pr(\mathrm{model}_j \mid \mathbf{d}_N) = \frac{f(\mathbf{d}_N \mid \mathrm{model}_j)Pr(\mathrm{model}_j)}{\sum_{l=1}^L f(\mathbf{d}_N \mid \mathrm{model}_l)Pr(\mathrm{model}_l)} $$ These are useful metrics used to compare models. For the above models, the marginal likelihoods are calculated as; $$ f(\mathbf{d}_N \mid \lambda_j) = \int_{\Theta} f(\mathbf{d}_N \mid \theta, \lambda_j)\pi_0(\theta\mid \lambda_j)d\theta $$ However, we can also think about sequentially adding observations to our sample, and write the marginal likelihood as a chain of predictive likelihoods ; $$ f(\mathbf{d}_N \mid \lambda_j) = \prod_{n=0}^{N-1} f(d_{n+1} \mid \mathbf{d}_n , \lambda_j) $$ From above we know that $f(d_{N+1} \mid \mathbf{d}_N , \lambda_j)$ converges to $f(d_{N+1} \mid \mathbf{d}_N , \theta^*)$, but it is generally not true that $f(\mathbf{d}_N \mid \lambda_1)$ converges to $f(\mathbf{d}_N \mid \theta^*)$, nor does it converge to $f(\mathbf{d}_N \mid \lambda_2)$ . This should be apparent given the product notation above. While latter terms in the product will be increasingly similar, the initial terms will be different, because of this, the Bayes factor $$ \frac{f(\mathbf{d}_N \mid \lambda_1)}{ f(\mathbf{d}_N \mid \lambda_2)} \not\stackrel{p}{\rightarrow} 1 $$ This is an issue if we wished to calculate a Bayes factor for an alternative model with different likelihood and prior. For example consider the marginal likelihood $h(\mathbf{d}_N\mid M) = \int_{\Theta} h(\mathbf{d}_N\mid \theta, M)\pi_0(\theta\mid M) d\theta$; then $$ \frac{f(\mathbf{d}_N \mid \lambda_1)}{ h(\mathbf{d}_N\mid M)} \neq \frac{f(\mathbf{d}_N \mid \lambda_2)}{ h(\mathbf{d}_N\mid M)} $$ asymptotically or otherwise. The same can be shown for posterior probabilities. In this setting the choice of the prior significantly effects the results of inference regardless of sample size.
