[site]: crossvalidated
[post_id]: 495887
[parent_id]: 
[tags]: 
Intersection of MSE-Loss and Regularization Term

In several questions [1,2] the graphical intuition of the L1/L2 regularization has been discussed. But, for example in [1], it has been stated that: The solution to the constrained optimization lies at the intersection between the contours of the two functions And this is basically what I wanted to understand. Why is that the case? In general, if you want to minimize $f(x) = g(x) + h(x)$ - like in regularization - the minimum of $f$ is not the sum of the minimums of $g$ and $h$ . Why can we just make the statement quoted above in regularization, as $loss = mse + regularization\text{ }term$ ? For example, if you have two weights $w_1, w_2$ in a neural network and $Loss(w_1,w_2)$ , you can interpret the minimum as the intersection of the MSE function and the regularization function $\mathbb{R}^2 \rightarrow \mathbb{R}$ which is for me really unintuitive, as again, for me, in general, the minimum of the sum of two functions is not their intersection or something like that. Thank you so much. [1] The graphical intuiton of the LASSO in case p=2 [2] L1 L2 regularization
