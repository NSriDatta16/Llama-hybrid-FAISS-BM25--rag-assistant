[site]: crossvalidated
[post_id]: 4173
[parent_id]: 4157
[tags]: 
I have used various algorithms, including Bayesian approaches (and, I am sorry to confess, even Excel many years ago), to fit mixtures. When there is not a clear visual indication of the two (or more components) in the histogram, you can expect the likelihood function to be extremely flat--almost parabolic--near its peak. This is because the visual impression translates mathematically into an ability to trade off some proportion of one mixture with an equivalent proportion of the other (adjusting the parameters of the components to keep a good fit) while making only a minor change to the likelihood. In many cases it's difficult to pin down the maximum likelihood. (This is evidenced by regime-switching in the Markov chains, for instance: a chain will pursue an area where one component predominates and after longish periods switch to an area where another component predominates, never really settling down to a single optimum.) In any event you also want to assess uncertainty. This is reflected by how much change is needed in the mixture parameters to reduce the likelihood by some threshold amount. The near-parabolic flatness near the optimum delineates a long "ridge" of near-optimum values, resulting in a long elliptical confidence region for the mixture. Usually the major axis of that ellipse corresponds to the mixture proportions. Thus, you might conclude that your data are $p$ percent of component A and $1-p$ percent of component B, but $p$ might be anywhere from 0 to 70%. (Yes, there are boundary value problems with mixtures, too.) It can take an extraordinary amount of data to reduce these wide confidence intervals if you can even reliably find them. These problems are exacerbated when only the tails of the data provide most of the information needed to disentangle the distributions. This would often be the case for unimodal data.
