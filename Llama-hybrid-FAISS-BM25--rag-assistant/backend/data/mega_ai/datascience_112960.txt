[site]: datascience
[post_id]: 112960
[parent_id]: 10932
[tags]: 
Yes, Policy-Net & Value-Net are redundant. If you have a Policy-Network you don't need a Value-Network and vice versa. At first glance you might think that these 2 networks would behave exactly the same once fully trained but that ain't the case. A good example for that is the following problem: Imagine there are 2 states (S0 and S1) and in S0 the agent can use 2 different actions (a0, a1) which will both lead to S1. In S1 however the Agent is rewarded or punished depending on the action the Agent has taken. If it reached S1 by using a0 it's rewarded and if it reached S1 by using a1 it is punished. A Value-Network would be helpless in this situation cause it only looks at the states and not the actions that lead to the states. In one pass it would increase the value of S1 and in another pass were it was punished it would decrease the value of S1 again. A Policy-Network on the other hand just looks at the actions that where taken and ignores the states alltogether. It would pick up the fact that a0 leads to reward and a1 leads to punishment very quickly. Imho Value-Networks work best with Board-Games and similar problems where each state clearly defines a reward/punishment value. If that's not the case or the number of possible states is too big then a Policy-Network is the way-to-go.
