[site]: stackoverflow
[post_id]: 2866563
[parent_id]: 2866392
[tags]: 
IMO, and I have no literature to back me on this, but the key difference between our various forms of testing is scope, Unit testing is testing isolated pieces of functionality [typically a method or stateful class] Integration testing is testing the interaction of two or more dependent pieces [typically a service and consumer, or even a database connection, or connection to some other remote service] System integration testing is testing of a system end to end [a special case of integration testing] If you are familiar with unit testing, then it should come as no surprise that there is no such thing as a perfect or 'magic-bullet' test. Integration and system integration testing is very much like unit testing, in that each is a suite of tests set to verify a certain kind of behavior. For each test, you set the scope which then dictates the input and expected output. You then execute the test, and evaluate the actual to the expected. In practice, you may have a good idea how the system works, and so writing typical positive and negative path tests will come naturally. However, for any application of sufficient complexity, it is unreasonable to expect total coverage of every possible scenario. Unfortunately, this means unexpected scenarios will crop up in Quality Assurance [QA], PreProduction [PP], and Production [Prod] cycles. At which point, your attempts to replicate these scenarios in dev should make their way into your integration and system integration suites as automated tests. Hope this helps, :) ps: pet-peeve #1: managers or devs calling integration and system integration tests "unit tests" simply because nUnit or MsTest was used to automate it ...
