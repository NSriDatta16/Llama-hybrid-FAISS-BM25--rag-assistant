[site]: crossvalidated
[post_id]: 261735
[parent_id]: 140652
[tags]: 
Learning in an ESN isn’t primary forced to adapting weights, more respectively the output layer learns which output to produce for the current state the network has. The internal state is based on network dynamics and is called dynamic reservoir state. To understand how the reservoir states shape out, we need to look at the topology of an ESN. The input unit(s) are connected to neurons in the internal units (reservoir units), the weights are randomly initialized. The reservoir units are randomly and sparsely connected and as well have random weights. The output unit is also connected to all reservoir units thus receives the reservoir state and produces a corresponding output. The input activation raises the network dynamics. The signal floats $t$ timesteps through the recurrent connected reservoir units. You can imagine it as an echo reoccurring $t$ times in the net (which gets distorted). The only weights which get adapted are the weights to the output unit. This means, that the output layer learns which output has to belong to a given reservoir state. That also means the training becomes a linear regression task. Before we can explain how the training works in detail we have to explain and define some things: Teacher Forcing means feed time series input into network as well as corresponding desired output (time delayed). To feed the desired output of $T$ at $t$ back is called output feedback. We therefore need some randomly initialized weights stored in the matrix $W_{fb}$. In figure 1 those edges are displayed with dotted arrows. Variable definitons: $r$ = number of reservoir unites, $o$ = number of output units, $t$ = number of timesteps, $o$ = number of output units. $T$ = Matrix (of size $t$ x $o$) which contains the desired output for each timestep. Finally how does the training work in detail? Record reservoir states for $t$ time steps while applying teacher forcing. The output is: A matrix $M$ of ($t$ x $r$) reservoir states. Determine the output weight matrix $W_{out}$ which contains the final output weights. It can be calculated using any regression technique e.g. using the pseudoinverse. This means, look at the reservoir states and find a function to map them multiplied with the output weights to the output. Mathematically: Approximate $M \bullet W_{out} = T -> W_{out} = M \bullet T^{-1}$ Because learning is very fast we can try out many network topologies to get one which fits well. To measure the performance of an ESN: Run the Echo State Network further without teacher forcing (own output is fed back into the ESN’s dynamic reservoir via $W_{fb}$). Record performance, such as squared errors $\left|\left|M \bullet W_{out} – T\right|\right|^2$ Spectral Radius and ESN Some smart people have proven, that the Echo State Property of an ESN may only be given if the Spec-tral Radius of the reservoir weight matrix is smaller or equal than $1$. The Echo State Property means the system forgets its inputs after a limited amount of time. This property is necessary for an ESN to not ex-plode in activity and to be able to learn.
