[site]: crossvalidated
[post_id]: 288478
[parent_id]: 288366
[tags]: 
Being (analytical) chemist , I encounter both approaches: analytical calculation of figures of merit [mostly for univariate regression] as well as direct measurement of predictive figures of merit. The train/test splitting to me is the "little brother" of a validation experiment to measure prediction quality. Long answer: The typical experiments we do e.g. in undergraduate physical chemistry use univariate regression. The property of interest are often the model parameters, e.g. the time constant when measuring reaction kinetics, but sometimes also predictions (e.g. univariate linear calibration to predict/measure some value of interest). These situations are very benign in terms of not overfitting: there's usually a comfortable number of degrees of freedom left after all parameters are estimated, and they are used to train (as in education) students with classical confidence or prediction interval calculation, and classical error propagation - they were developed for these situations. And even if the situation is not entirely textbook-like (e.g. I have structure in my data, e.g. in the kinetics I'd expect the data is better described by variance between runs of the reaction + variance between measurements in a run than by a plain one-variance-only approach), I can typically have enough runs of the experiment to still get useful results. However, in my professional life, I deal with spectroscopic data sets (typically 100s to 1000s of variates $p$) and moreover with rather limited sets of independent cases (samples) $n$. Often $n almost repeated measurements - which leaves us with an unknown effective $n$. Without knowing $n$ or $df$, the classical approaches don't work. But as I'm mostly doing predictions, I always have a very direct possibility of measuring the predictive ability of my model: I do predictions, and compare them to reference values. This approach is actually very powerful (though costly due to increased experimental effort), as it allows me to probe predictive quality also for conditions that were not covered in the training/calibration data. E.g. I can measure how predictive quality deteriorates with extrapolation (extrapolation includes also e.g. measurements made, say, a month after the training data was acquired), I can probe the ruggedness against confounding factors that I expect to be important, etc. In other words, we can study the behaviour of our model just as we study the behavior of any other system: we probe certain points, or perturb it and look at the change in the system's answer, etc. I'd say that the more important predictive quality is (and the higher the risk of overfitting) the more we tend to prefer direct measurements of predictive quality rather than analytically derived numbers. (Of course we could have included all those confounders also into the design of the training experiment). Some areas such as medical diagnostics demand that proper validation studies are performed before the model is "let loose" on real patients. The train/test splitting (whether hold out* or cross validation or out-of-bootstrap or ...) takes this one step easier. We save the extra experiment and do not extrapolate (we only generalize to predicting unknown independent cases of the very same distribution of the training data). I'd describe this as a verification rather than validation (although validation is deeply in the terminology here). This is often the pragmatic way to go if there are not too high demands on the precision of the figures of merit (they may not need to be known very precisely in a proof-of-concept scenario). * do not confuse a single random split into train and test with a properly designed study to measure prediction quality.
