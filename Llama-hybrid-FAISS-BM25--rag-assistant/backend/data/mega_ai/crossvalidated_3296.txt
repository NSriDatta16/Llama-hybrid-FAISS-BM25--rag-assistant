[site]: crossvalidated
[post_id]: 3296
[parent_id]: 
[tags]: 
Dealing with "trouble maker" samples

I have a pretty large data set (~300 cases with ~40 continuous attributes, binary labeled) which I used to create several alternative predictive models. To do this, the set was divided to training and validation subsets (~60:40%, respectively). I have noticed that there are several samples (both in the training and the validation subsets) that are being misclassified by all or most of the alternative models that I test. I suspect that there is something special about these "trouble making" samples. What are the general guidelines for discovering the possible reasons behind the misbehavior of the models on specific samples? Update 1 . I'm using logistic regression for this task. The parameter selection is done by exhaustively searching combinations of up to 4 predictors with 10-fold cross validation. It is worth mentioning that the p-values that is calculated by the model for the misclassified samples are usually very different from the default classification threshold of 0.5. In other words, not only is the model wrong about those cases, it is also very confident about itself. Update 2 - what I have already done. I agree that insights from the study domain are crucial, but to date we have failed to discover anything significant. Also, I tried to remove the "bad" samples from the training set, and keeping the validation set and the parameter selection algorithm untouched. This led to better performance on the training set (naturally), but also improved significantly the performance on the validation set. Is this an indication that the "bad" samples were actually "bad"?
