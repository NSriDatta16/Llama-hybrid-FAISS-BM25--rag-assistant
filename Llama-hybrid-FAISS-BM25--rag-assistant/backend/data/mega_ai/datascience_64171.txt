[site]: datascience
[post_id]: 64171
[parent_id]: 410
[tags]: 
We can choose 2nd order learning rate: minimizing parabola in one step - in parabola $(\theta,g)$ are in line, so e.g. division of their standard deviations gives slope of this line. This line intersects $g=0$ in minimum - we get there in one step if using: learnig rate = $\frac{\sigma_\theta}{\sigma_g}=\sqrt{\frac{var(\theta)}{var(g)}}=\sqrt{\frac{mean(\theta^2)-mean(\theta)^2}{mean(g^2)-mean(g)^2}}$ what requires maintaining four (exponential moving) averages, e.g. adapting learning rate separately for each coordinate of SGD (more details in 5th page here ).
