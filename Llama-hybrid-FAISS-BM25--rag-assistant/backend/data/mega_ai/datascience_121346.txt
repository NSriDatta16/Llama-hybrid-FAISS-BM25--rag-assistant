[site]: datascience
[post_id]: 121346
[parent_id]: 
[tags]: 
AUC drops below 0.5 even though dataset stays similar

I'm programming an anomaly detection on a given dataset: Toyadmos dataset: https://arxiv.org/abs/1908.03299 Of this dataset, I'm investigating the ToyCar data, which has '4 cases': (quote)"Each “case” of the toy car was designed as the combination of two types of motors and bearings; thus, the number of cases was four. "(\end quote) So, the only difference between the cases is the type of motor and bearings, all the rest(like types of damage) is the same for all cases. For the detection I implemented a fully connected Autoencoder(it then compares original input with reconstructed output to detect an anomalous sample, not that important) which gives the following results(these are averages of several runs): The only thing changed between runs is the case number Case 1 Case 2 Case 3 Case 4 0,74 0,72 0,6 0,55 The drop off seems weird but still seems plausible: I then tried using a transformer based approach: Case 1 Case 2 Case 3 Case 4 0,8 0,75 0,7 0,4 Now that last case goes completely wrong. I do not think the preprocessing goes wrong since the other cases work. Types of damages stay the same. Nothing is changes between runs exept case. The 0.4 shows that the transformer is actually learning the opposite of what it is supposed to do, which i am not able to explain. Any ideas?
