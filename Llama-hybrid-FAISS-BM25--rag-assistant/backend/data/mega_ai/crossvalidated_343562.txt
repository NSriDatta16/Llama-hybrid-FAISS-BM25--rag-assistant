[site]: crossvalidated
[post_id]: 343562
[parent_id]: 
[tags]: 
BIC estimate for multidimensional data

I want to use the Bayesian Information Criterium (BIC) to infer the number of clusters in a clustering algorithm, but I have two doubts. Suppose I have $N$ points in a $D$-dimensional space, and that I am trying to fit $K$ clusters. I understand that the BIC is computed as $$ \mathit{BIC} = k \log n - 2 \log \hat L $$ where $k$ is the number of free parameters, $n$ is the "sample size", and $\hat L$ is the maximum value of the likelihood. I am confused because, from what I understand (and see in various implementations of the BIC calculations, for example in the Python sklearn package), the sample size is just the number of datapoints, i.e. $n \equiv N$. However, naively, I would assume that the dimension of the data space $D$ must play a role, and I would rather set $n = N D$. Indeed, I can imagine situations where the $N$ multi-dimensional datapoints are used as $ND$ one-dimensional datapoints, and I see why the BIC should change as a consequence of this difference. Secondly, in some clustering algorithms, I think it is possible to include some weights $w_n$ for each point. In my opinion it would be sensible to reformulate $n$ as $$ n = \frac{\bigl(\sum_{n=1}^N w_n \bigr)^2}{\sum_{n=1}^N w^2_n} \; , $$ or a similar equation in case where the dimension $D$ is to be taken into account. Is such a choice reasonable?
