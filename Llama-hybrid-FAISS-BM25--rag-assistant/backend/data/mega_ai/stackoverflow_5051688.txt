[site]: stackoverflow
[post_id]: 5051688
[parent_id]: 
[tags]: 
HTML text analysis

I have a crawler that gathers articles from the web and stores the title and the body to a database. Until now the programmer has to come up with a set of rules per source (usually XPath and sometimes regular expressions) to point to the article title and body sections of the web page. Now I'm trying to go one step ahead and have the program auto-detect the title and the body of the article. My first approach add a weight to each element based on some common criteria. For example: //@x-weight = 1.0 //h1/@x-weight * 2.0 //h2/@x-weight * 1.8 There are many more rules but you get the point. After assigning the weights based on the markup I take into account and some other aspects such as similarity to /head/title and number of keywords. This approach while producing decent results for most of the web pages (thanks SEO experts :P), it fails catastrophically for some others. I'm thinking the possibility to use an artificial neural network , but I can't find enough evidence that I'll get significantly better results. Another option is to take CSS into the game and adjust the weights by font size. The question(s): Which path should I choose? Am I missing something? Is there a better way to this? PS: I know that there isn't a perfect solution for a problem like this.
