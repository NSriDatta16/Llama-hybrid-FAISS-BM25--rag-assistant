[site]: datascience
[post_id]: 55086
[parent_id]: 35720
[tags]: 
In normal convolutional layers, ReLU activation function is used. ReLU is fixed and cannot be trained in itself. MLP Conv Layer is combination of Convolution operation and Multilayer Perceptron Network. If you use MLP after Convolution, you can learn much more complex function(mapping input to output). It increases capacity of your model as well, and can fit more data. For comparison you have to try it yourself. Some comparison is shown in the paper itself. There is also another paper that does a similar thing. Check: Learning Activation Functions to Improve Deep Neural Networks You can make your own MLP conv layer in deep learning frameworks like Tensorflow and Pytorch. Someone might have already built it as well.
