[site]: crossvalidated
[post_id]: 237096
[parent_id]: 237091
[tags]: 
You could use K-fold CV in this manner: Create K-folds of your dataset. Identify different candidate algorithms you want to evaluate to build your model. Lets assume you have 2 algorithms to evaluate - linear regression, logistic regression. For each one, run parameter selection training runs for each of K-folds and evaluate their accuracy on the held-out Kth fold. Also tune the hyper-parameters for these models in the same training runs - e.g. L1 or L2 regularization parameters. Lets assume you have identified 2 sets of parameters for each algorithm - 2 for linear regression and 2 for logistic regression. For each of these 4 candidate models, evaluate their performance on the K-folds. Assume you prefer selecting the one with lowest variance of True Positive accuracy - let this be model #3. Other selection criteria could be to select the model with highest average TP accuracy, etc. Fit model #3 on the entire training data with the selected hyper-parameters to get the final model. Its not recommended to tune the model on the entire training data (Step #5 , e.g. selecting the parameters, hyper-parameters, etc.) because if you do so, you won't be able to check if you made a robust model which will perform as expected with new data. The Kth held out fold lets you do that and fix any mistakes during model building. Edited to explain queries in comments: Parameter imply all the input features used by the model and Hyper-praameters mean all the other configuration settings of the algorithm used to produce the optimal fit. For example, if we're fitting a neural network , selecting the variables x1, x2, x3 as the best inputs to produce an estimate y implies we're selecting these 3 parameters for the model. Hyper-parameters would be the no. of units, no. of hidden layers and the learning rate of the neural network. When we reach step #5, we will have the final list of input parameters which had achieved the best results in cross-validation, and the hyper-parameters which had given the best results. The K-folds and the coefficients learned for each fold are discarded. Then, we use the entire training data, pick our final list of parameters and run a final training using the selected hyper-parameters to get the model fit, or, coefficients that collectively represent the completed "model".
