[site]: crossvalidated
[post_id]: 546939
[parent_id]: 447842
[tags]: 
There are clearly lots of ways you could attack this, and @Sextus Empiricus lists some good ones. From a slightly more practical point of view, I would think about doing the following. Since each player plays only once, it's safe enough to just calculate change scores for each player. Clustering by team is clearly a sensible thing to do, but it makes it difficult to apply a lot of off-the-shelf models, so I would probably ignore this clustering, and note this as a limitation of the analysis. Obviously, this is a totally exploratory analysis, so I would probably forget about p-values. If you must report them, you'll need to correct for lots of comparisons, probably by controlling the false discovery rate . Estimating the correlation of each moderator with the change scores, and ranking them, is a reasonable place to start. After that, you could plug them all into a huge multiple regression model, and hope it runs. You're overfitting the data at this point, so a next step would be to try penalised regression. In particular, you could use LASSO to identify a subset of moderators that are useful for predicting the change scores. LASSO can be difficult to interpret when your predictors are correlated though. As noted in the other response, clustering the predictors may be useful. I would use factor analysis, rather than PCA, since this produces clusters that are more easily interpretable. Partial least squares would be particularly useful here. Rather than clustering the moderators, and then testing which groups of moderators predict the change scores, which was the previous step, this involves directly identifying the most useful cluster for making predictions.
