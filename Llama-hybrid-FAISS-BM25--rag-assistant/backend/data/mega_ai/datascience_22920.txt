[site]: datascience
[post_id]: 22920
[parent_id]: 22916
[tags]: 
Parametric methods in simple terms follow a particular distribution. The most common example would be that of Normal Distribution, where 64 percent of the data is situated around +-1 standard deviation from the mean. The essence of this distribution is the arrangement of values with respect to their mean. Similarly, other methods such Poisson Distribution etc have their own unique modeling technique. Parametric Estimation might have laid the foundation to some of the most vital parts of Machine Learning but it is an absolute mistake to think supervised learning is the same thing. Supervised Learning may include approaches to fit the aforementioned parametric models but this might not always be the case. More often, the data scatter is quite spread out. It might not just be fitting one parametric model but a hybrid of more than one. Supervised Learning also takes into account the error which most parametric models don't consider unless incorporated manually. You could say that supervised learning is an evolved and more robust version of parametric methods which is highly flexible.
