[site]: crossvalidated
[post_id]: 340040
[parent_id]: 340037
[tags]: 
First, the notation of $M$ for the parameter of the model is quite confusing, as it seems like a model index. If I denote the parameter by $\theta$, the density of the observation $x$ by $p(x|\theta)$, and the prior probability by $\pi(\theta)$, then $$\int \frac{P(D|M) P(M)}{P(D)} dM$$ translates into $$\int \dfrac{p(x|\theta)\pi(\theta)}{\int p(x|\theta)\pi(\theta)\text{d}\theta}\text{d}\theta=\dfrac{\int p(x|\theta)\pi(\theta)\text{d}\theta}{\int p(x|\theta)\pi(\theta)\text{d}\theta}=1$$ hence this is obviously not correct. When computing a posterior expectation of the parameter $\theta$ as a possible Bayesian estimator (associated with a quadratic loss function) $$\hat{\theta}(x)=\int \theta\,\dfrac{p(x|\theta)\pi(\theta)}{\int p(x|\theta)\pi(\theta)\text{d}\theta}\text{d}\theta$$ the estimator (lhs) is a function of $x$, equal to an average over the parameter space (rhs) so there is no contradiction of the type "the parameter M is an average of the parameter M". The rhs is not the parameter behind the data (or the parameter generating the data) but an estimate of the parameter: while they live in the same space, they have different interpretations.
