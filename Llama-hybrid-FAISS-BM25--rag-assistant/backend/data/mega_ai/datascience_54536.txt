[site]: datascience
[post_id]: 54536
[parent_id]: 
[tags]: 
LeCun paper on deeplearning (Nature, 2015)

As I was reading Y. LeCun's paper on Deep Learning (Nature, vol. 521, 2015) , I came across a figure (the 1st one in the paper) which was associated to the backward pass during backpropagation through a convolutional network. I understood everything apart from the very last sentence of the legend that accompanied the figure: "Once the $\frac{∂E}{∂zk}$ is known, the error-derivative for the weight $w_{jk}$ on the connection from unit j in the layer below is just $y_j\cdot\frac{∂E}{∂zk}$ ". It must be purely mathematical but I can't figure out why the error-derivative mentioned above equal $y_j$ times $\frac{∂E}{∂zk}$ . Can someone explain to me this result? Thx. PS : link to the figure mentioned above is https://www.researchgate.net/figure/Multilayer-neural-networks-and-backpropagation-a-A-multi-layer-neural-network-shown-by_fig4_277411157
