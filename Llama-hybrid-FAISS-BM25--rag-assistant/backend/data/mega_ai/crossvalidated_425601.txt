[site]: crossvalidated
[post_id]: 425601
[parent_id]: 425575
[tags]: 
Yes, in fact that's how most language models are trained -- you compute the (log) probability of given input text, and then backpropagate to maximize that probability. There are some generative models with explicit densities: you can easily evaluate the probability/density the model assigns to any data point. There are some other generative models with only approximate densities (you can only get a lower bound on the probability of a point) -- or with implicit density (in which case finding or approximating the density is computationally intractable). RNNs (and more broadly, autoregressive models) are in the explicit density family. There are many CNN based generative models of images (autoregressive, GAN, VAE, flows, etc), some of which are explicit, approximate, and implicit density each.
