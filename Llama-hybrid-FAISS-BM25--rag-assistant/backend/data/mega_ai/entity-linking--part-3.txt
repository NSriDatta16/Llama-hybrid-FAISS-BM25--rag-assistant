entity in multiple and often heterogeneous data-sets. It considered a broader concept than entity linking, and is a key process in digitalizing archives and joining of knowledge bases. Named-entity recognition (NER) locates and classifies named entities in unstructured text into pre-defined categories such as names, organizations, locations, and more. For example, the following sentence: Paris is the capital of France. would be processed by an NER system to obtain the following output: [Paris]City is the capital of [France]Country. NER is usually a preprocessing step of an entity linking system, as it can be useful to know in advance which words should be linked to entities of the knowledge base. Coreference resolution understands whether multiple words in a text refer to the same entity. It can be useful, for example, to understand the word a pronoun refers to. Consider the following example: Paris is the capital of France. It is also the largest city in France. In this example, a coreference resolution algorithm would identify that the pronoun It refers to Paris, and not to France or to another entity. A notable distinction compared to entity linking is that Coreference Resolution does not assign any unique identity to the words it matches, but it simply says whether they refer to the same entity or not. In that sense, predictions from a coreference resolution system could be useful to a subsequent entity linking component. Approaches Entity linking has been a hot topic in industry and academia for the last decade. Many challenges are unsolved, but many entity linking systems have been proposed, with widely different strengths and weaknesses. Broadly speaking, modern entity linking systems can be divided into two categories: Text-based approaches, which make use of textual features extracted from large text corpora (e.g. Term frequency–Inverse document frequency (Tf–Idf), word co-occurrence probabilities, etc...). Graph-based approaches, which use the structure of knowledge graphs to represent the context and the relation of entities. Often entity linking systems use both knowledge graphs and textual features extracted from, for example, the text corpora used to build the knowledge graphs themselves. Text-based The seminal work by Cucerzan in 2007 published one of the first entity linking systems. Specifically, it tackled the task of wikification, that is, linking textual mentions to Wikipedia pages. This system categorizes pages into entity, disambiguation, or list pages. The set of entities present in each entity page is used to build the entity's context. The final step is a collective disambiguation by comparing binary vectors of hand-crafted features each entity's context. Cucerzan's system is still used as baseline for recent work. Rao et al. proposed a two-step algorithm to link named entities to entities in a target knowledge base. First, candidate entities are chosen using string matching, acronyms, and known aliases. Then, the best link among the candidates is chosen with a ranking support vector machine (SVM) that uses linguistic features. Recent systems, such as by Tsai et al., use word embeddings obtained with a skip-gram model as language features, and can be applied to any language for which a large corpus to build word embeddings is available. Like most entity linking systems, it has two steps: an initial candidate selection, and ranking using linear SVM. Various approaches have been tried to tackle the problem of entity ambiguity. The seminal approach of Milne and Witten uses supervised learning using the anchor texts of Wikipedia entities as training data. Other approaches also collected training data based on unambiguous synonyms. Graph-based Modern entity linking systems also use large knowledge graphs created from knowledge bases such as Wikipedia, besides textual features generated from input documents or text corpora. Moreover, multilingual entity linking based on natural language processing (NLP) is d