[site]: crossvalidated
[post_id]: 173259
[parent_id]: 173203
[tags]: 
It is neither correct nor incorrect to include all Features. What you want to end up with is the most parsimonious model that adequately fits the data. In some cases, this may require all Features, but more often, some subset of Features will account for the most variance. This 'feature selection' step is often employed (as Frank Harrell commented) using regularization/penalization/PCA, etc. With decision trees, I believe this is called 'pruning'. Have you considered a modeling approach other than decision trees? Decision trees often grow into an unmanageable forest.
