[site]: crossvalidated
[post_id]: 50427
[parent_id]: 
[tags]: 
Calculating odds for a cricket match

I'm writing a program to help me gamble on cricket, but have run into a wee problem. I've got loads of data from previous games already and have compiled various stats from it. I have calculated that after 1 over of a 20 over match, the average score is 4.5 runs, and the average final score will be 145. Assume standard deviation is 2 and 20 respectively. So from there I can get a fairly decent idea of the odds for number of runs after 1 over, and at the end of the innings. (Assuming normal distributions N(4.5, 2) and N(145, 20)) My problem is, suppose after the first ball, they score 1 runs. How do I calculate the new odds? At first I thought I'd take 4.5/6 (number of balls in an over) = 0.75 as an "expected per ball" value and then say, 1 run scored is 1.33 times more than expected, therefore scale expected final score to 145*1.33 ~= 200 , and over score to 4.5*1.33 = 6. However this is going to lead to all sorts of problems: Firstly if they score 0 off the first ball, they would be expected to score 0 overall, which can't be right. Secondly, scaling 145 by 1.33 makes almost 200 runs, this jump is in no way justifiable after seeing just one ball. So I'm a bit stuck, not sure where to go from here. Sorry if this is a bit complicated, this one may be for cricket enthusiasts only!
