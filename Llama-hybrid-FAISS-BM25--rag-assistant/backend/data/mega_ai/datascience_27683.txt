[site]: datascience
[post_id]: 27683
[parent_id]: 27676
[tags]: 
From quora you'll find a more complete guide, but main ideas are that AdaGrad tries to taggle these problems in gradient learning rate selection in machine learning: 1 Manual selection of the learning rate η. 2 The gradient vector gt is scaled uniformly by a scalar learning rate η. 3 The learning rate η remains constant throughout the learning process. It resolves concerns 2 and 3 simply by dividing each current gradient component by an L2 norm of past observed gradients for that particular component. It has in itself the following issues: 1 Continually decaying learning rate η. 2 Manual selection of the learning rate η. AdaDelta resolves AdaGrad concern 1 by summing the gradients only within a certain window W. Concern 2 solution relates to mismatch in gradient units and thus the actual accumulation process is implemented using a concept from momentum. The last calculation needs understanding on momentum theory and it was shortly explained there in article. My idea was to give the main causes behind what was intended, maybe that makes reading easier.
