[site]: crossvalidated
[post_id]: 236645
[parent_id]: 219874
[tags]: 
I actually do something like k-k-fold validation! I have noticed (especially with smaller datasets) that the results on a holdout/validation set after CV can vary quite a lot (for example, a model might show AUC ranging from 0.55 to 0.7 on 5 different training iterations even in a 10-fold CV). In this case one might decide to take the "best" model as the one that generated 0.7 AUC on a hold out set (selected randomly from the original full data set), but in the real world, I would think actual performance is going to be more like the average of all these models.
