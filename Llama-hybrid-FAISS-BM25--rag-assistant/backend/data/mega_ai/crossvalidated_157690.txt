[site]: crossvalidated
[post_id]: 157690
[parent_id]: 157689
[tags]: 
It's related to a bias-variance tradeoff. If you take $K=n$, your folds will be of size $n-1$, almost as large as your actual training sample. So the predictions from these samples will be based on almost as much information as that contained in the full training sample, thus mimicking its predictive performance quite well on average , resulting in low bias. On the other hand, these folds will be highly correlated, as they are all almost identical. And basic statistics tell us that averaging highly correlated random variables produces an average (here: the CV error, i.e. the average of the prediction errors from the $K$ folds) that is still highly variable. James et al. recommend $K=5$ or $K=10$ as a compromise in this tradeoff.
