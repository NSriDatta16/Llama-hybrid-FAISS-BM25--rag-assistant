[site]: crossvalidated
[post_id]: 482053
[parent_id]: 
[tags]: 
Is the product of conditional probability distributions a valid distribution?

In probabilistic machine learning, the likelihood of the data is usually computed as the product of the individual likelihoods of seeing each data point given the parameters $\theta$ . In logistic regression, the likelihood of the data given the parameters $\theta$ is equal to $$P(Y|X,\theta) = \prod_{i=1}^m[\frac{1}{1+e^{-x_i\theta}}]^{y_i}[1-\frac{1}{1+e^{-x_i\theta}}]^{1-y_i}$$ This is just the product of $m$ bernouli distributions. I have two questions. What is the relation between a conditional density distribution and the likelihood that is used in baye's theorem ? Is the product of distributions in this setting, bernouli distributions going to result in a valid conditional distribution ? EDIT Regarding the first question, suppose I am able to define a joint distribution for $P(Y,\theta|X)$ . This is assuming $\theta$ random variable is follows some distribution. Then, given a fixed data $Y$ , we can slice the joint distribution at $Y=y$ and obtain the marginal density $P(\theta|Y=data,X)$ . Then $argmax_{\theta}P(\theta|Y=data,X)$ represents the $\theta_{MLE}$ . In this case the conditional probability is the likelihood function for $\theta$ . Integrating this yields a value of 1. However, if $\theta$ does not follow some distribution then integrating this likelihood is not equal to 1 ?
