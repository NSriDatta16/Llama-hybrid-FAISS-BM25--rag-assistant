[site]: datascience
[post_id]: 19980
[parent_id]: 
[tags]: 
How much data are sufficient to train my machine learning model?

I've been working on machine learning and bioinformatics for a while, and today I had a conversation with a colleague about the main general issues of data mining. My colleague (who is a machine learning expert) said that, in his opinion, the arguably most important practical aspect of machine learning is how to understand whether you have collected enough data to train your machine learning model . This statement surprised me, because I had never given that much importance to this aspect... I then looked for more information on the internet, and I found this post on FastML.com reporting as rule of thumb that you need roughly 10 times as many data instances as there are features . Two questions: 1 - Is this issue really particularly relevant in machine learning? 2 - Is the 10 times rule working? Are there any other relevant sources for this theme?
