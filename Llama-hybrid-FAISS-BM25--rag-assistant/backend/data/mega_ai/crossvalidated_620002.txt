[site]: crossvalidated
[post_id]: 620002
[parent_id]: 
[tags]: 
Why is the layer normalization same with the instance normalization in transformers (or NLP)?

This picture is from Group Normalization paper and the Layer Norm shows averaging in Channel and H/W dimension. However, this picture is from Power Normalization paper focusing on NLP problems and the Layer Norm does not average the Sequence Length dimension. It is conventional in NLP field that Layer Norm is averaging only last dimension. Is there a specific reason Layer Norm is not averaging Sequence Length dimension in NLP while it is averaging Channel dimension in image data?
