[site]: crossvalidated
[post_id]: 569368
[parent_id]: 
[tags]: 
Noisy Optimization by Regressing the Derivative

Say that we have a neural network with a set of weights. We train the network with SGD. Looking at a single weight w, we can plot the SGD derivative of the error with respect to the weight, against the value of the weight itself, which changes over time. These derivatives might look like the blue dots in the following plot: Here a line of regression is drawn, and where the line intersects 0 is our best estimate for where the gradient averages 0. That would be a local minimum or maximum - a minimum in this case because the slope of the gradient is positive. We could consider the drawing of this line as an optimization method to select w. Assuming we are already near a local minimum, we can allow w to vary randomly a little so we can plot more points, and our best estimate for the optimal value of w is where the line of regression crosses the x-axis. It is possible to compute the line of regression in an online fashion with O(1) memory use. It is also possible to use exponential discounting of older observations. To use this method it would be necessary to have a different optimization strategy for when w is not near a local minimum. For instance, gradient descent with momentum, or random restarts. Is there literature on methods like this, or a search term?
