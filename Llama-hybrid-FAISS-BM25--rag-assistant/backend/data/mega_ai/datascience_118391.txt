[site]: datascience
[post_id]: 118391
[parent_id]: 
[tags]: 
Learning additional parameters that are not weights of a neural network

In addition to training the weights of a neural network, I also want to optimize other parameters (that are constant but satisfy some conditions over the entire data set). As an example, one can consider a loss function as follows: $$ L(x)= \sum_{x \in D} B_{nn}(x) - \eta x, $$ where $D$ is the data set. I want to minimize this value of $\eta$ (a generalization parameter) in addition to optimizing the weights of the network. Can I just simply add $\eta$ into the list of trainable parameters and apply SGD on it? Or is there another simpler way to learn this parameter as well?
