[site]: crossvalidated
[post_id]: 371209
[parent_id]: 
[tags]: 
What is the relation of bias in the Bias-Variance trade-off to underfitting

In machine learning literature, people often talk about the bias-variance trade-off as well as overfitting and underfitting in the same paragraph. However, in these contexts, bias and variance comes from the decomposition of the loss function (as estimator of the model performance). E.g., for MSE, the loss can be decomposed into Variance + Bias^2 + Noise^2 But since bias is essentially "TargetValue - MeanPrediction", I am wondering how this related to the degree of "underfitting" of a model. In other words, the mean prediction is a constant value. If you have a quadratic function, the "mean prediction" value would always be off from a particular datapoint to a large degree no matter how well you fit that quadratic function, so I am wondering how someone can deduce that a model underfits the dataset by looking at the bias component of a loss function. Wouldn't one have to look at n biases (one bias for each target value /data point in the dataset, computed as "TargetValue - MeanPrediction") to draw any conclusions. I am a bit confused how the "bias" component really related to underfitting as I can't see a direct relationship.
