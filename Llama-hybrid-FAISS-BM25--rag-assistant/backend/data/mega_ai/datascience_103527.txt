[site]: datascience
[post_id]: 103527
[parent_id]: 103526
[tags]: 
I have a simple dataset with balanced target y (0 or 1) ,and imbalanced feature (many 0 , few 1's) First, let me note that there's no point using ML if you have a single boolean feature: the only possible model can be written with a simple if .. else .. . I can get high precision of 0.53 if I just assign y=1 if x=1 This only means that 53% of the instances which have 1 as feature belong to class 1. In other words, this score doesn't take into account the majority of the instances, since most of the instances have 0 as feature. but when i train DecisionTree, xgboost, randomforest , they all produce model wihch just outputs 1's for any feature value, i.e. they cant find that simple rule (y=1 if x=1) (precision I get is only 0.38 using these algos) . This simply means that this rule is not a good rule if one takes into account all the instances. Since there's no better choice, the model simply predicts every instance as the majority class. Your mistake is to look only at the precision score. You should also take recall into account, this way you would see that the majority baseline is a better choice than this rule. Or you could look at accuracy in this case, since your classes are balanced.
