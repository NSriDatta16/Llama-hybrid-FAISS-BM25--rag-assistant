[site]: crossvalidated
[post_id]: 483816
[parent_id]: 483785
[tags]: 
The earlier layers of neural network learn more low-level features, while deeper layers learn more complicated, abstract features (see figure from Albawi et al, 2017 ). Autoencoders are build of two networks encoder that encodes the data in terms of some latent variables (usually of lower dimensionality, hence they can be used for dimensionality-reduction ) and decoder that transforms the latent representation back into the initial representation of the data. You can use any layer from autoencoder, depending on your needs. Since autoencoders are usually symmetric, usually for feature generation you would be taking layers from the encoder, or it's output. If you want to use autoencoder for dimensionality reduction, then you would rather take the output of the encoder that has smaller dimension (see figure below from the paper by Windrim et al, 2019 ). On another hand, you could use autoencoder in a same way as any other neural network for transfer learning . In such case, you would train autoencoder and then extract some layer of it as features for other algorithm. In such case, if you wanted lower-level features, you would take earlier layers. If you wanted more abstract features, you would take higher layers. Using the example from first picture, you would take first layer of network trained on human faces and use it for extracting some basic shapes and edges from images other than human faces. On another hand, if you needed to detect faces, you would take deeper layers of the network.
