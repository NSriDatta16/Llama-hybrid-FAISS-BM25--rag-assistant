[site]: crossvalidated
[post_id]: 377537
[parent_id]: 376094
[tags]: 
From a frequentist perspective, Unbiasedness is important mainly with experimental data where the experiment can be repeated and we control the regressor matrix. Then we can actually obtain many estimates of the unknown parameters, and then, we do want their arithmetic average to be really close to the true value, which is what unbiasedness guarantees. But it is a property that requires very strong conditions, and even a little non-linearity in the estimator expression may destroy it. Consistency is important mainly with observational data where there is no possibility of repetition. Here, at least we want to know that if the sample is large the single estimate we will obtain will be really close to the true value with high probability, and it is consistency that guarantees that. As larger and larger data sets become available in practice, methods like bootstrapping have blurred the distinction a bit. Note that we can have unbiasedness and in consistency only in rather freak setups, while we may easily have biasedness and consistency. The variance may superficially look like a "secondary" property (because supposedly we primarily hunt for location), but go tell that to any veteran statistician and don't say I didn't warn you: in practice, business and policy decisions tend to be based on intervals rather than points , and it is the variance that determines the length of the interval into which an unknown parameter lies. This is why Mean-Squared Error r is often considered a better evaluation criterion and comparison tool of estimators, comprehensively including variance and bias, and favoring biased estimators that have considerably lower variance than unbiased ones.
