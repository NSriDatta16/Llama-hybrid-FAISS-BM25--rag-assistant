[site]: datascience
[post_id]: 113686
[parent_id]: 
[tags]: 
Bayesian state description in Reinforcement Learning

What's the best approach to feed a bayesian description of an observed state to a Reinforcement Learning agent? Brief context: I have an agent situated in an environment, which it perceives through a series of noisy sensors. Using Bayesian inference (outside of the RL agent) a number of sensed variables of interest are described in terms of their probability distributions, forming the state vector. As an example, think of the state as the position of an object in space, described at each instant with a multivariate Normal distribution. The agent is tasked with gathering information about the state, meaning that its actions can impact the level of uncertainty used to describe the state. Following the previous example, if the location of the object is perceived through a movable camera, one of the actions could be to focus the camera on the object, lowering the variance in the probabilistic description of said location. The state is assumed continuous, so a function approximation has to be used to retrieve the Q factor in each state. The set of actions is instead finite. The reward function reflects this "variance minimizing" drive, as there's a penalty factor proportional to the variance of the observed state. First question: is this actually doable with RL or am I missing something? Second question: which choice might be better for the function approximator, knowing it has to deal with that probabilistic description of the state? NOTE: The simpler the approach the better, as the training data is rather limited and definitely not enough for a Deep approach.
