[site]: datascience
[post_id]: 32084
[parent_id]: 32083
[tags]: 
A generative model is able to generate instances from a given distribution. So let's try to get our model to generate instance from the distribution of all hand written digits. This includes numbers 0 to 9 and for each of these numbers there are a very large set of possible way of writing the same number. Take the number 4 from the MNIST dataset, which are comprised of handwritten numbers. You can quickly see how many different ways each of these numbers can be written yet still represent the same value. This is the distribution we want to capture using our generative model. The model works by training a generator to effectively try and cheat a discriminator that has been trained to distinguish between real and artificial instances from the desired distribution. When the discriminator can no longer distinguish between the artificial and real instances, we can say the generative model can successfully produce examples which match this distribution. In other words, once the generator starts producing realistic digits, we can say that the generator is producing instances which match the instances captured in the MNIST dataset. This idea of matching a distribution can be applied to wide range of applications where you want to draw from a distribution. Some for demonstration purposes of the techniques without any real useful application and some very useful applications which are currently being used for a number of tasks. Here is a short lists of some examples: Generating artificial faces Text to image generation Producing fake voices or noises Image denoising Changing geometries within an image Coloring images MRI image reconstruction Most importantly, to generate new Pokemon . In theory, given sufficient data, any distribution (anything in the world, including the world itself) can be modeled by a generative model from which we can draw instances. So any time you think of a simulation in the far future some generative model will be able to observe multiple real examples and then produce a large number of simulated examples. For example, simulating car crashes, a generative model can visualize multiple real crashes, then we can use the generative model to produce millions of similar instances for testing purposes without having to buy more cars. Of course GANs are not that powerful, they require a lot of data to learn very constraint distributions. However, through innovation anything is possible, until then we can continue to generate images of handwritten numbers and bedroom scenes.
