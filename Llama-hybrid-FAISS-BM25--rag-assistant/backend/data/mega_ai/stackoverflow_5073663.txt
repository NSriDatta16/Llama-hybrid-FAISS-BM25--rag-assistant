[site]: stackoverflow
[post_id]: 5073663
[parent_id]: 
[tags]: 
Reliable and efficient way to handle Azure Table Batch updates

I have an IEnumerable that I'd like to add to Azure Table in the most efficient way possible. Since every batch write has to be directed to the same PartitionKey, with a limit of 100 rows per write... Does anyone want to take a crack at implementing this the "right" way as referenced in the TODO section? I'm not sure why MSFT didn't finish the task here... Also I'm not sure if error handling will complicate this, or the correct way to implement it. Here is the code from the Microsoft Patterns and Practices team for Windows Azure "Tailspin Toys" demo public void Add(IEnumerable objs) { // todo: Optimize: The Add method that takes an IEnumerable parameter should check the number of items in the batch and the size of the payload before calling the SaveChanges method with the SaveChangesOptions.Batch option. For more information about batches and Windows Azure table storage, see the section, "Transactions in aExpense," in Chapter 5, "Phase 2: Automating Deployment and Using Windows Azure Storage," of the book, Windows Azure Architecture Guide, Part 1: Moving Applications to the Cloud, available at http://msdn.microsoft.com/en-us/library/ff728592.aspx. TableServiceContext context = this.CreateContext(); foreach (var obj in objs) { context.AddObject(this.tableName, obj); } var saveChangesOptions = SaveChangesOptions.None; if (objs.Distinct(new PartitionKeyComparer()).Count() == 1) { saveChangesOptions = SaveChangesOptions.Batch; } context.SaveChanges(saveChangesOptions); } private class PartitionKeyComparer : IEqualityComparer { public bool Equals(TableServiceEntity x, TableServiceEntity y) { return string.Compare(x.PartitionKey, y.PartitionKey, true, System.Globalization.CultureInfo.InvariantCulture) == 0; } public int GetHashCode(TableServiceEntity obj) { return obj.PartitionKey.GetHashCode(); } }
