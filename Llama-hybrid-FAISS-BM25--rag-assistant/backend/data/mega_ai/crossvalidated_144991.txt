[site]: crossvalidated
[post_id]: 144991
[parent_id]: 142432
[tags]: 
I think that you are quite wrong in making a way too fast conclusion that it is not possible to use entropy as a similarity measure. Indeed, entropy can be used as a measure of similarity, both in general (Korhonen & Krymolowski, 2002) as well as for autocorrelated processes , such as time series (Liu, Pokharel & Principe, 2006). In particular, Korhonen and Krymolowski, among other similarity measures, describe cross-entropy , which might be useful in your case. Moreover, Liu et al. describe cross-corentropy , which is also referred to simply as correntropy - an information theory-based similarity measure, which extends auto-correntropy function to two random variables. As an additional aid, you might find helpful the following related answers of mine: on performing time series analysis in R , on time series classification and clustering and (more) on dynamic time warping (disregard the focus on the irregular time series - the information is helpful in general). References Korhonen, A., & Krymolowski, Y. (2002). On the Robustness of Entropy-Based Similarity Measures in Evaluation of Subcategorization Acquisition Systems. In Proceedings of The 6th Conference on Natural Language Learning , 91-97. Retrieved from https://aclweb.org/anthology/W/W02/W02-2014.pdf Liu, W., Pokharel, P. P., & Principe, J. C. (2006). Correntropy: A localized similarity measure. In Proceedings of The International Joint Conference on Neural Networks (IJCNN '06) , pp. 4919-4924. doi:10.1109/IJCNN.2006.247192 Retrieved from http://www.cnel.ufl.edu/~weifeng/filesfordownload/paper/localized_similarity_measure.pdf
