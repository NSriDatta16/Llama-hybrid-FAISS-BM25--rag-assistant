[site]: crossvalidated
[post_id]: 30366
[parent_id]: 30358
[tags]: 
Following answer is based on my own personal insights of doing text analysis. Of course, an increase in the number of categories will increase the time significantly since you have bigger matrix dimensions and so on. But it's not necessary a bad approach. Moreover first strategy looks somehow strange to me since the result of guessing subgroup maybe interfered with bad result of guessing the group (some subcategories can be significally different from other categories or subcategories, but the whole can not). So I would probably go with the second strategy. In second approach you will need quite much computational power. The error you're getting is that your RAM memory is full (also swap if you have one). There are couple basic suggestions concerning this problem. Try to reduce your doc-term matrixes. That includes removing stopwords, punctuation, removing words that have no meaning whatsoever. This is very common procedure but sometime one can consider creating his own bigger filter. Don't use whole amount of articles, instead use only the sample. Well, sampling is one of the most simplest procedures to reduce the amount of computing. The most lazy solution, either get a pc with more operative memory or increase your swap memory and let computer do the rest. These are more common approaches in your second case. I might would skip through the package called RTextTools that makes all of this work easier. Another insight would be using another approach rather than SVM. I'm not sure, but I think there are already implemented classification algorithms that implies that some of your categories have subcategories. And as always don't forget to protect your progress when R crashes by saving workspace, .Rdata and then loading it. Also try to use R's garbage collector gc() .
