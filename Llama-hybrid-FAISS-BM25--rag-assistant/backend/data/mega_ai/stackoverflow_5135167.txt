[site]: stackoverflow
[post_id]: 5135167
[parent_id]: 5135155
[tags]: 
You can use the robots.txt file to block access to crawlers, or you can use javascript to detect the browser agent, and switch based on that. If I understood the first option is more appropriate, so: User-agent: * Disallow: / Save that as robots.txt at the site root, and no automated system should check your site.
