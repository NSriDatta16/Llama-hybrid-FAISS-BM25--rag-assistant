[site]: crossvalidated
[post_id]: 323644
[parent_id]: 168917
[tags]: 
You have to be a bit careful here, since it is unlikely that the regression coefficients would be independent, a posteriori. If I understand correctly, the method you are proposing here samples each coefficient from its marginal posterior distribution. That is not the same as sampling the coefficients from their joint-posterior distribution, which would include dependencies. You say you have separate posterior distributions for the regression coefficients. In Bayesian analysis you usually estimate these distributions from some underlying MCMC chain of the regression coefficients (and other parameters). Is that how you got them? If so, all you need to do is go back to your original MCMC chain, generate a new parameter that is the sum of the regression coefficients in the chain (over each iteration of the chain), and then find its distribution. This would automatically use the joint-posterior distribution and so it would take care of any dependencies between the coefficients. Alternatively, if you got the marginal posterior distributions of the coefficients analytically, then you probably also got their joint distribution somewhere in that process. In that case you might be able to find the distribution of the sum analytically, or you could sample from the joint distribution of the coefficients and create the sum. Note: Technically speaking, you should be referring to the distribution of the sum of random variables, not the sum of the distributions themselves. The sum of probability distributions gives a measure that is no longer a probability distribution (since it no longer obeys the norming axiom), and this is not useful. When you are talking about sums of posterior distributions, I am assuming that you mean to refer to the posterior distribution of the sum of the underlying random variables.
