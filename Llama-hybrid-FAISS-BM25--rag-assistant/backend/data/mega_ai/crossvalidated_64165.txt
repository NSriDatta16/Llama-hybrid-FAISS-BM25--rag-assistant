[site]: crossvalidated
[post_id]: 64165
[parent_id]: 64163
[tags]: 
For sake of simplicity let's assume you're doing binary classification, everything I'll say generalizes straightforwardly to the multiclass case, with $D = X \times Y$ your dataset and $P(Y=1) There are several techniques that are typically employed in Machine Learning to remedy the situation. Oversampling: sample a new dataset $D^\prime$ from $D$ such that $P(Y=1) \approx P(Y=0)$ in the new dataset and $|D^\prime| > |D|$. In this scenario you end up with many duplicates of the positive class. Undersampling: sample a new dataset $D^\prime$ from $D$ such that $P(Y=1) \approx P(Y=0)$ in the new dataset and $|D^\prime| Cost Sensitive Learning: here we associate a different cost $C(\text{actual}, \text{predicted})$, (typically a scalar value in $[0,1]$) with different types of missclassifications. To deal with class imbalance in this framework we may say the cost of miss-classifying the minority class is greater than the cost miss-classifying the majority class. Sticking with our example we would have $C(1, 0) > C(0, 1)$. The nice thing about this from a practical perspective is if you are optimizing some per element loss function, this essentially amounts to multiplying that per element loss function by a scalar value, which isn't going to complicate things much from an optimization perspective. Fixing things after the learning: When doing binary classification typically one uses a decision rule like classify $x$ as 1 if $P(y=1|x)/P(y=0|x) > b$, with $b = 1$. However by adjusting $b$ you can make the classifier "prefer" different types of classifications after learning. A common way to choose a good value for $b$ is to look at precision/recall, f-score, AUC, etc, curves on a heldout validation set. I'm not sure if this technique has a proper name, but it is the approach I've had the most success with in applied setting. For a more in depth look at these techniques see the paper The class imbalance problem in pattern classification and learning . Note: I've copied this answer from one I gave for a similar question on the soon to be defunct ML stackexchange. I don't know if this is bad form of not.
