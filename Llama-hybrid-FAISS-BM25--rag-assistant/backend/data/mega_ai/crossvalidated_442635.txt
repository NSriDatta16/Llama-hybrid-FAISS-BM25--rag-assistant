[site]: crossvalidated
[post_id]: 442635
[parent_id]: 442634
[tags]: 
This study, unfortunately, is probably too small. With respect to your questions: For logistic regression, an often useful rule of thumb is that you should have about 15 members of the smallest outcome category per predictor that you are evaluating in a model. Otherwise you are likely to be overfitting; see this page for example. With 24 cases total you can't have more than 12 cases in the smallest outcome category. Thus you could have a difficult problem even with only 1 predictor. Perhaps if data on a particular predictor of major interest is available for more cases you could do better restricting to that predictor. This isn't really a discrepancy. An omnibus test effectively states the probability that you would have this good a fit if none of the predictors were associated with outcome, assuming that a logistic linear regression model is correct. The Hosmer-Lemeshow test tries to examine whether the linear logistic regression model is correct. That latter test has some important limitations ; there are better ways to check model calibration, noted on the linked page. Your omnibus result barely passes the standard p The value of the Constant (called "Intercept" by other software) can be confusing. For a logistic regression it is the log-odds of the probability of a positive outcome when all of the categorical predictors are at their reference levels and continuous predictors have values of 0. The "mm" as part of the name of 2 of your predictors suggests that those are continuous. My guess is that values of 0 for them aren't realistic, so the Constant doesn't represent log-odds at a realistic baseline condition. That doesn't affect the coefficients for the predictors, though. For example, if you expressed the continuous predictors in terms of differences from their mean values rather than from 0, then the Constant value would represent something more realistic. The regression coefficient values for the predictors and their p -values, however, would still be the same (within rounding error). In response to comment: The way to proceed depends on your knowledge of the subject matter. If that knowledge suggests one or two predictors that would be expected to be related to outcome, then a smaller model more in keeping with the number of available cases would be appropriate. Note that, unlike linear regression, if an omitted predictor is associated with outcome you can still have omitted-variable bias in logistic regression even if the omitted predictor isn't correlated with the included predictors; see this page . On the other hand, if a predictor is not suspected to be related to outcome, then there is no need to include it; that would only reduce the number of degrees of freedom and reduce the power to detect a true relationship between predictors and outcome. So knowledge of the subject matter is central to deciding on the modeling approach. You also could consider other ways to deal with the missing data to increase the sample size and the potential overfitting from a low case-to-predictor ratio. For example, missing data can be handled by multiple imputation . If you have a set of predictors that is highly correlated and essentially measuring the same underlying phenomenon, you could devise a measure incorporating all of their values into a single combined predictor. You might be able to include all predictors expected to be associated with outcome, without overfitting, if you use a penalization method like ridge regression or LASSO , although formal tests providing things like p -values with penalized approaches can require some care.
