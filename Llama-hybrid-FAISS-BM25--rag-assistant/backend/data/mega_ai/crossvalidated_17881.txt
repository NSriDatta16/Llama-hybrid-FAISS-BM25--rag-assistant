[site]: crossvalidated
[post_id]: 17881
[parent_id]: 
[tags]: 
Hypothesis testing and significance for time series

A usual test of significance when looking a two populations is the t-test, paired t-test if possible. This assumes that the distribution is normal. Are there similar simplifying assumptions that produce a significance test for a time series? Specifically we have two fairly small populations of mice that are being treated differently, and we are measuring weight once a week. Both graphs display smoothly increasing functions, with one graph definitely above the other. How do we quantify "definiteness" in this context? The null hypothesis should be that the weights of the two populations "behave in the same way" as time passes. How can one formulate this in terms of a simple model that's fairly common (just as normal distributions are common) with only a small number of parameters? Once one has done that, how can one measure significance or something analogous to p-values? What about pairing the mice, matching as many characteristics as possible, with each pair having one representative from each of the two populations? I would welcome a pointer to some relevant well-written and easily understood book or article about time series. I start as an ignoramus. Thanks for your help. David Epstein
