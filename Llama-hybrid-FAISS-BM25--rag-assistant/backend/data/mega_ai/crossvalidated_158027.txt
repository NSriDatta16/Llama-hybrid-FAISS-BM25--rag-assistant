[site]: crossvalidated
[post_id]: 158027
[parent_id]: 
[tags]: 
How do I improve the accuracy of my supervised document classification model?

Given 1000 legal judgement documents, 900 of which are labeled, my task is to predict the label for the remaining 100 documents. The labeled documents belong to 41 different categories of Law, with some classes having only 1 sample each to some having around 80 samples. I adopted an approach similar to one used on a Kaggle competition: https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words After encountering many errors and fixing it to the best of my abilities, I finished with a very poor accuracy. I'm not very familiar with Python and the object-oriented style of programming, having used MATLAB so far for all Machine Learning related code. My code is shown below. Could anyone kindly tell me what I'm doing wrong or how better to improve the accuracy? Alternate methods too are welcome. class DataCreation: # Read all files into Python Dictionary def read_txtFiles (self , path): data = {} for dir_entry in os.listdir(path): dir_entry_path = os.path.join(path, dir_entry) if os.path.isfile(dir_entry_path): with open(dir_entry_path, 'r') as my_file: (prefix, sep, suffix) = dir_entry.rpartition('.') # Strip away .txt extension from filenames in dict data[prefix] = my_file.read() return data # Read class labels file into List def read_labels(self , csvFile): with open(csvFile, 'rb') as f: reader = csv.reader(f) labels = list (reader) return labels # Separate Unlabeled Data from the rest of the dictionary def separate_data(self , labels , data): # Get filenames of the 'To be Tested' files if labels[0][0] == 'Judgements': del labels[0] out_sample_labels = [] for item in range (0 , 100): if not labels[item][1] == 'To be Tested': print 'Halt! The filename %s has label %s' %(labels[item][0] , labels[item][1]) # Shift the unlabeled class labels to a serarate list out_sample_labels = labels[0:100] del labels[0:100] # Match these filenames to the ones in the dict and store these in a separate dict out_sample_data = {} for name in out_sample_labels: if name[0] in data: out_sample_data[name[0]] = data[name[0]] del data[name[0]] return out_sample_labels , out_sample_data def split_dataset (self , dictionary , n , labels): rand_lst = random.sample (dictionary , n) var_dict = {} for item in rand_lst: if item in dictionary: var_dict[item] = dictionary[item] del dictionary[item] var_labels = [] for obj in labels: if obj[0] in var_dict: var_labels.append([obj[0] , obj[1]]) labels.remove([obj[0] , obj[1]]) if not len(var_labels) == len(var_dict): for k in labels: if k[0] in var_dict: var_labels.append([k[0] , k[1]]) labels.remove([k[0] , k[1]]) return var_labels , var_dict class PreProcess: def process_text (self , raw_text): # input is a single string and output is a single string # Remove HTML tags review_text = BeautifulSoup(raw_text).get_text() # Remove non-letters letters_only = re.sub("[^a-zA-Z0-9]" , " " , review_text) # Convert to lowercase, split into individual words words = letters_only.lower().split() # Convert stopwords to set for faster searching stops = set (stopwords.words("english")) # Remove stopwords meaningful_text = [w for w in words if not w in stops] # Join words back with space and return result return (" ".join(meaningful_text)) def feature_extract(self , list_of_strings): # CountVectorizer vectorizer = TfidfVectorizer(min_df=1) tfidf = vectorizer.fit_transform(list_of_strings) return tfidf def sort_dataset (self , labels , dictionary): final_set = [] for obj in labels: if obj[0] in dictionary: final_set.append([obj[0] , obj[1] , dictionary[obj[0]]]) return final_set class Classify: # Module for Classification def fit_predict(self , clf , feat_matrix , response , val_feat_matrix): # Fit Forest to Training set using bag-of-words as features and # sentiment labels as response variable model = clf.fit (feat_matrix , response) predicted = model.predict(val_feat_matrix) return model , predicted def metrics (self, response, predicted, category): score = metrics.accuracy_score(response, predicted) print("accuracy: %0.3f" % score) print("classification report:") print(metrics.classification_report(response, predicted, target_names=category)) print("confusion matrix:") print(metrics.confusion_matrix(response, predicted)) return score #########$$$$$$$$$$ CLASSIFIERS $$$$$$$$$$$$############ clfr = RandomForestClassifier(n_estimators = 100) clfm = MultinomialNB() clfs = SGDClassifier(alpha = 0.0001 , n_iter = 100 , loss = 'modified_huber') if __name__ == "__main__": trialTest = DataCreation() dic = trialTest.read_txtFiles (path) lbls = trialTest.read_labels(csvFile) # Separate the unlabeled data unlbl_lbls , unlbl_data = trialTest.separate_data(lbls , dics) training_lbls , training_data = trialTest.split_dataset(dics , 540 , lbls) testing_lbls = copy.deepcopy(lbls) testing_data = dics.copy() trialFE = PreProcess() final_training = trialFE.sort_dataset(training_labels , training_set) final_testing = trialFE.sort_dataset(testing_labels , testing_set) # Pass training & testing set through the Pre-processor clean_training = [] for obj in final_training: clean_training.append(trialFE.process_text(obj[2])) clean_testing = [] for obl in final_testing: clean_testing.append(trialFE.process_text(obl[2])) # Pass training & testing set through the Feature Extractor train_feat_matrix = [] train_feat_matrix = trialFE.feature_extract(clean_training) train_feat_matrix.todense() print train_feat_matrix.shape test_feat_matrix = [] test_feat_matrix = trialFE.feature_extract(clean_testing) test_feat_matrix.todense() train_response = [] for ele in final_training: train_response.append(ele[1]) test_response = [] for elg in final_testing: test_response.append(elg[1]) Tr1_lbls , Tr1_set , Tmp1_dict = trialTest.create_sets(lbls , dictss) categories = [] for atom in Tr1_lbls: categories.append(atom[1]) categories.sort() trialClass = Classify() # model , prdct_test = trialClass.fit_predict(clfm , train_feat_matrix , train_response , test_feat_matrix) # multinomial_metrics = trialClass.metrics(test_response, prdct_test, categories) I was unable to perform feature reduction using PCA as the resulting structure after feature extraction was a sparse matrix and didn't work well with PCA.
