[site]: crossvalidated
[post_id]: 633759
[parent_id]: 574296
[tags]: 
Yes, MARS (and Bayesian MARS) is a powerful tool for multi-class classification. Check out the R package $\texttt{cbass}$ , which can be found on CRAN. This package extends the approach of Denison and Holmes (2003) to the multi-class classification setting. A paper accompanying this R package is expected to be published soon, and I will reference it here whenever it becomes available. MARS regression, originally proposed by Friedman (1991), has been successfully applied to regression many times over the years (see Kooperberg, Bose & Stone (1997) for an early reference). Holmes & Denison (2003) tackle the problem of 2-class classification using Bayesian MARS (Denison, Mallick & Smith (1998)). Given a $p$ -dimensional vector of predictors (standardized for simplicity), $x \in [0,1]^p$ , the response variable $y\in \{0,1\}$ is modeled as $$P(y_i=1 | x_i) = g(\eta_i)$$ where $$\eta_i = \beta_0 + \sum_{m=1}^M\beta_m\prod_{j=1}^{J_m}\left(s_{jm}\left[x_{u_{jm}} - \theta_{jm}\right]_+\right).$$ The function $[\cdot]_+ = \max(\cdot, 0)$ is the rectified linear unit (ReLU), $s_{jm} \in \{-1, 1\}$ is called a sign , $\theta_{jm} \in (0, 1)$ is called a knot , and $u_{jm}\in \{1,\ldots,p\}$ selects which variables are activated in each basis function (i.e., each term of the sum). In classification, the most popular choice of link function $g()$ is the logit link , but Holmes and Denison show that using a probit link ( $g(a) = \Phi(a)$ where $\Phi()$ is the standard normal CDF) leads to substantial computational advantages. Without going into too much detail, the probit link model can be represented by adding a set of $n$ latent variables, one for each training data point. Only by adding these normally distributed latent variables, can the coefficients $\beta$ can be integrated out of the model. This leads to drastically better mixing of the MCMC, by allowing for large changes to the model (e.g., by removing, adding, or modifying a basis function), without needing to fuss about a complicated trans-dimensional proposal distribution for the coefficients. See Holmes and Denison (2003) for more details or see Francom et al. (2019) for a modern discussion in the regression setting. References Frank Marrs and Devin Francom (2023). cbass: Classification -- Bayesian Adaptive Smoothing Splines. R package version 0.1. https://CRAN.R-project.org/package=cbass Friedman, J. H. (1991). Multivariate adaptive regression splines. The annals of statistics, 19(1), 1-67. Kooperberg, C., Bose, S., & Stone, C. J. (1997). Polychotomous regression. Journal of the American Statistical Association, 92(437), 117-127. Holmes, C. C., & Denison, D. G. T. (2003). Classification with bayesian MARS. Machine Learning, 50, 159-173. Denison, D. G., Mallick, B. K., & Smith, A. F. (1998). Bayesian mars. Statistics and Computing, 8, 337-346. Francom, D., Sans√≥, B., Bulaevskaya, V., Lucas, D., & Simpson, M. (2019). Inferring atmospheric release characteristics in a large computer experiment using Bayesian adaptive splines. Journal of the American Statistical Association.
