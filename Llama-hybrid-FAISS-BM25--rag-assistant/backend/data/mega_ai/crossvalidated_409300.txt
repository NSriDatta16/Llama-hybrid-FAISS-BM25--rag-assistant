[site]: crossvalidated
[post_id]: 409300
[parent_id]: 409296
[tags]: 
I would convert the coordinates into Cartesian coordinate system, then calculate the two dimensional dispersion. The dispersion would be my measure of localization, and I could compare it between 0 and 1 sequences. The two dimensional dispersion is the square root of the product of eigenvalues of the covariance matrix. You can get fancy and first fit the kernel distribution into the coordinates, then get the covariance matrix using the fitted distribution. If you have a lot of data, this can be more stable (big maybe) in presence of outliers. As with any linear variance based measure, this measure will be sensitive to outliers. If the birds fly far away within each sequence of 0 and 1 then we have a problem with a curvature of earth. My approach is based on a flat surface. I think it's a good approximation when the distances are small within in each sequence. In this case within each sequence we assume a flat surface, locally flat so to speak. If the distances are large, then we'll have to come up with an analog of variance. One such option is geographic standard deviation relative to the geographic mean as described in this write up in MATLAB. They first find the geographic mean, then calculate the square distances from the mean and average. Again, as any distance measure this will suffer from outlier sensitivity, so you may want to apply some robust form of the variance on top.
