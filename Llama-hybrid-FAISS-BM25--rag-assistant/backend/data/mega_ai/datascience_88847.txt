[site]: datascience
[post_id]: 88847
[parent_id]: 
[tags]: 
Deploying ML/Deep Learning on AWS Lambda for Long-Running Training, not just Inference

Serverless technology can be used to deploy ML models to production, since the deployment package sizes can be compressed if too large (or built from source with unneeded dependencies stripped). But there is also the use case of deploying ML for training , not just inference. For example, if a company wanted to allow power users to retrain a model from the front-end. Is this feasible for Lambda given the long training times? Whereas latency wouldn't be issue (cold start delay is fine) the runtime could be fairly long (hours).
