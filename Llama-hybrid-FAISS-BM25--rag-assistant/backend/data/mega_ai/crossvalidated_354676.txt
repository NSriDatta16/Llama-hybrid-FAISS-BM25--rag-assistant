[site]: crossvalidated
[post_id]: 354676
[parent_id]: 333695
[tags]: 
I think that part of the misunderstanding stems from using the symbol $h$ in two different places for two different meanings. The code portion of the question seems to have little to do with the mathematics of XGBoost, since the code snippets are not part of the XGBoost software. Denote the binary cross-entropy loss for a single sample $$ L(y_i, \hat{y}_i) = -\left[ y_i \log(\hat{y}_i) + (1- y_i) \log(1 - \hat{y}_i) \right]. $$ The loss for the model is $\sum_i L(y_i, \hat{y}_i)$. This is a quantity that we want to minimize . The authors provide that $g_i = \partial_{\hat{y}_i^{(t-1)}} L\left(y_i, \hat{y}_i^{(t-1)}\right)$, with the notation $\text{something}^{(t-1)}$ denoting that this is the prediction for trees up to and including tree number $t-1$. We can write, dropping indices on $y$ because life is short, $$ \begin{align} g_i &= \frac{\partial L}{\partial \hat{y}} L(y, \hat{y}) \\ &=\frac{y}{\hat{y}} - \frac{1 - y}{1 - \hat{y}} \\ &=\frac{y(1 - \hat{y}) - \hat{y}(1-y)}{\hat{y}(1 - \hat{y})} \\ &= \frac{ y - y\hat{y} - \hat{y}+y\hat{y} }{\hat{y}(1 - \hat{y})} \\ &= \frac{y - \hat{y}}{\hat{y}(1 - \hat{y})} \end{align} $$ For $h_i$, we can follow the same procedure. $$ \begin{align} h_i &= \partial^2_{\hat{y}_i^{(t-1)}} L\left(y_i, \hat{y}_i^{(t-1)}\right) \\ &= \frac{\partial}{\partial \hat{y}} g_i \\ &= \frac{\partial}{\partial \hat{y}} \left[\frac{y - \hat{y}}{\hat{y}(1 - \hat{y})}\right] \\ &= \frac{ y - 1}{(\hat{y} -1)^2} - \frac{y}{\hat{y}^2} \end{align} $$ but remember that for compactness/ease of reading I dropped all of the super- and sub-scripts.
