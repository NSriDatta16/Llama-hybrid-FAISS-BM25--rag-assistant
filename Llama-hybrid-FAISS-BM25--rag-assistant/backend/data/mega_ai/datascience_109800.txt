[site]: datascience
[post_id]: 109800
[parent_id]: 
[tags]: 
NLP LSTM input basic doubt

I have a basic doubt with regards to conversion of text to numbers and feeding it to LSTM. I am aware of the different methods such as OneHot, CountVectorizer, TfIDF, Word2vec etc. My doubt is, If we use a Count Vectoriser or Tfidf, Then in LSTM, we have to pass through the entire vocabulary of words for each sentence since that's how TFIDF and count vectoriser encodes the sentences. Am I right? My second doubt is, If we use TFIDF or COuntVectorizer, Each word will have different value based on its occurrence and frequency. This is in contrast to Word2Vec where a embedding is learned and used. If each time the LSTM model sees different values for a particular word, How can it learn? Like in a sentence if the word "Hi" appears 6 times, Its encoded with the number 6 in its appropriate index, And in another sentence if it appears 4 times, we encode with the value 4. How does this work? It doesn't make sense.
