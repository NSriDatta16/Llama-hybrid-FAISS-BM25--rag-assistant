[site]: datascience
[post_id]: 28802
[parent_id]: 28650
[tags]: 
We have tried including all of our 8 features (categorical ones being encoded in integer) and doing the linear regression. If it is not dummy encoding and your categories can not be ranged - that is wrong. For example, doing this: [apples, bananas, strawberries] -> [0, 1, 2] for almost all tasks will be not correct. We have also tried taking out the categorical features and running the linear regression algorithm for each possible combination of our 3 categorical features. That of course yields in a lot of regression runs (each category has 4 possible values; so 64 to be exact). Also needed to be revised. If some of your combinations have very small amount of cases you can not trust the results. So, Our dataset has 8 features; 3 of them being categorical. We are willing to perform linear regression to fit our target data. Do dummy encoding. If you see some strict relationship between categories you can also add one-hot dummy encoding: category A - category B 1 - 1 2 - 0 2 - 3 1 - 1 ... Possibly here category A and category B are strongly correlated while both of them have category 1. Create new feature for this case. In general: play with your data. Find possible relationships and add them to your model. If I am forced to step out of linear regression, which algorithm should I try to apply? I would prefer to have one dataset with 8 features instead of having 64 datasets with 5 features. There should be an algorithm that can capture this model. Forests, XGBoost as mentioned earlier. For this you do not need one-hot or dummy encoding. By the way, usage of simple Decision Trees may give you beautiful pattern of relationships between categories and it's influence on target variable. Try simple neural network after dummy and one-hot encoding too.
