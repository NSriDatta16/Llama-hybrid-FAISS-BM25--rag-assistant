[site]: crossvalidated
[post_id]: 443101
[parent_id]: 139368
[tags]: 
A blog post entitled, “Entropy in machine learning” dated May 6, 2019 ( https://amethix.com/entropy-in-machine-learning/ ) gave a very good explanation and summary of the concepts of Mutual Information, KL Divergence and their relationships to Entropy. It had many informative references and it provided useful Python code supporting their explanations. The code that they provided used the numpy.histogram method to create the inputs for the sklearn.metrics. mutual_info_score while never displaying the actual histograms. You can very easily modify it to display the histograms that you need then use the MI as needed. The code and references that they provided as also very enlighting. You might also benefit from their explanation and use of code to calculate KL Divergence. # Import libraries import pandas as pd import numpy as np from scipy.stats import iqr from numpy import histogram2d from sklearn.metrics import mutual_info_score # Read dataset about breast cancer detection df = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00451/dataR2.csv") # Separate input and targets target = df['Classification'] df.drop(['Classification'], axis=1, inplace=True) # Define mutual information function def minfo(x, y): # Compute mutual information between x and y bins_x = max(2,int(2*iqr(x)*len(x)**-(1/3))) # use Freedman-Diaconis's Rule of thumb bins_y = max(2,int(2*iqr(y)*len(y)**-(1/3))) c_xy = histogram2d(x, y, [bins_x,bins_y])[0] mi = mutual_info_score(None, None, contingency=c_xy) return mi # Build MI matrix num_features = df.shape[1] MI_matrix = np.zeros((num_features,num_features)) for i,col_i in enumerate(df): for j,col_j in enumerate(df): MI_matrix[i,j] = minfo(df[col_i],df[col_j]) MI_df = pd.DataFrame(MI_matrix,columns = df.columns, index = df.columns) print(MI_df) I find this post, along with the explanation and code provided in the first answer above when combined, to offer a very interesting solution.
