[site]: crossvalidated
[post_id]: 544087
[parent_id]: 
[tags]: 
Symmetry of the output activation function vs symmetry of the output

Imagine you have a reinforcement learning problem in which your continuous action space is $\mathcal{A} = [-0.01, 1.5]$ which is associated with a deterministic policy. As you notice the upper and lower values are not symmetric with respect to zero. Let us choose tanh as the output activation function; in addition a (keras) network structure could be: inputs = layers.Input(shape=(4)) out = layers.Dense(20, activation="swish")(inputs) out = layers.Dense(30, activation="swish")(out) outputs = layers.Dense(1, activation="tanh", kernel_initializer = my_init)(out) outputs = outputs*1.5 model = tf.keras.Model(inputs, outputs) After that the output model is clipped between the requested action space since it provides values between $[-1.5,1.5]$ My question is: is the tanh symmetry around zero a bad match with a non-symmetric output around zero?
