[site]: datascience
[post_id]: 45524
[parent_id]: 
[tags]: 
Autoencoder doesn't learn to reduce dimesions

I coded a neural network from scratch in Python. I tried it with the XOR problem and it learned correctly. So I tried to encode an Autoencoder with 3 inputs (and therefore with also 3 outputs) to reduce a color (r, g, b) in one dimension. I have normalized the data from 0 to 1 so I can use activation functions like sigmoid, relu etc. I have tried many different activation functions and learning rates, but the Autoencoder error(calculated with the mean squared error) is high (the lowest I got is 0.1), although I have trained it for more than 30,000 iterations. Did I miss something? (I think so, but is it possible to reduce a color to a size with good accuracy in the first place?) Thank you all
