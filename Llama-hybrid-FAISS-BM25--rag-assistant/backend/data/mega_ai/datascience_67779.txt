[site]: datascience
[post_id]: 67779
[parent_id]: 
[tags]: 
Analogy between Autoencoder and PCA

I know that Autoencoders can be regarded as non-linear generalisations of PCA, but I struggle to understand in depth the analogy between the two. Once PCA has been performed on a function $F(\vec{\theta})$ of the parameters $\vec{\theta}$ , that function can be re-written as $F(\vec{\theta}) = \sum_{i=1}^{N_{\mathrm{PCA}}} \alpha_i(\vec{\theta}) \, b_i$ , where $b_i$ are the PCA basis functions and $\alpha_i (\vec{\theta})$ the $N_{\rm{PCA}}$ PCA coefficients. For Autoencoders, one can also build basis functions , by decoding the basis vectors $(1, 0, \dots 0), (0, 1, \dots 0), \dots (0, 0, \dots, 1)$ through the decoder, once the Autoencoder has been trained. Can a function $F(\vec{\theta})$ still be rewritten as $F(\vec{\theta}) = \sum_{i=1}^{N_{\mathrm{AE}}} \alpha_i(\vec{\theta}) \, b_i$ , where $N_{\mathrm{AE}}$ is the number of encoded features in the central layer of the Autoencoder?
