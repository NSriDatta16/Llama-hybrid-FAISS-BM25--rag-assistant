[site]: crossvalidated
[post_id]: 154224
[parent_id]: 
[tags]: 
When using SVMs, why do I need to scale the features?

According to the documentation of the StandardScaler object in scikit-learn: For instance many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the L1 and L2 regularizers of linear models) assume that all features are centered around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected. I should scale my features before classification. Is there any easy way to show why I should do this? References to scientific articles would be even better. I already found one but there are probably many other.
