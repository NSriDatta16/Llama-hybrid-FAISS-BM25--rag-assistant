[site]: crossvalidated
[post_id]: 262392
[parent_id]: 
[tags]: 
Understanding equation in linear regression

The following paragraph is taken from p24 of the book The Elements of Statistical Learning, 2nd.Ed by Hastie, Tibshirani & Friedman, where $X$ and $Y$ are random vectors, and bold $X$ is the $N \times p$ matrix of data: In other words, it is claimed that $$x_0^T (\hat\beta-\beta)=X(X^TX)^{-1}x_0\varepsilon$$ Why is that true? The book's phrasing suggests that it is simple to see it, but it seems that the only similar derivation in the book prior to this is the fact that $\hat\beta = (X^TX)^{-1}X^Ty$ is the least squares solution to $Y=X^T\beta$.
