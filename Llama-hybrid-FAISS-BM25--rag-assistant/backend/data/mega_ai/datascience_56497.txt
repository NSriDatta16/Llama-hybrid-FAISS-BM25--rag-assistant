[site]: datascience
[post_id]: 56497
[parent_id]: 56477
[tags]: 
First of all, I personally think that the code you provided is unnecessary since you are looking for architectural comparison between models. It would have been easier (and less bulky) to include the graphs of your vanilla and stacked LSTM networks for simplicity. You can do that by keras.utils.plot_model() as explained here . Answering your questions: I don't see how we are using only the decoder in the predict step... Uber's model does not use the decoder component of the autoencoder. It uses a separate sequential LSTM model, judging by the graph provided. ...which variable or method in Keras corresponds to the embedding that we would be using as an input for predicting new time series? The input to the forecasting model is simply the latent space of the autoencoder. In particular, the latent space constitutes a condensed representation of the past observations, so that you don't need to include the complete timeseries because it might be very long and include redundant information, repetition, noise, etc. Instead, you compress it into something similar to a "zip" file, that has smaller size but still includes all the essential information. Then, you train the "forecasting" network to predict future values based on the "zipped" past information instead of training it on the original timeseries. How can I implement the code for a model similar to the one in the illustration using Keras? Check the 4th model of this autoencoder example . First, train the autoencoder. Only after it is properly trained and optimized, train a simple LSTM forecasting model, like the one in the code you provided. The key is to use the latent space of the autoencoder to train the forecasting model, which in the current example is the output of the 1st LSTM layer. To do that, you need to first encode your complete dataset into a latent representation using the already trained encoder, and then use these encodings to train the forecasting model in the second step.
