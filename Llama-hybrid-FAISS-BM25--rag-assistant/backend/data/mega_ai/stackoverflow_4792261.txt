[site]: stackoverflow
[post_id]: 4792261
[parent_id]: 4707632
[tags]: 
Speed up the database, use direct organization model. It's the fastest method to store/retrieve data from files. The implementation is as simple, that you don't need any framework or library. The method is: you have to create an algorhytm, which converts the key to a continous record numero (0..max. number of records), you have to use fixed record size, the data is stored in flat files, where the record's position within the file is the rec. no. (based on key, as described in 1.) multiplied by the record size (see 2.). Native data You may create one data file per day for easier maintenance. Then your key is the no. of the sample within the day. So, your daily file will be 18000 * 24 * record size . You should pre-create that file with 0s in order to make operating system's life easier (maybe it does not help much, it depends on underlying filesystem/caching mechanism). So, when a data arrives, calculate the file position, and insert the record to its place. Summarized data You should store summarized data in direct files, too. These files will be much smaller ones. In case of the 1-minute summarized values there will be 24*60*60 records in it. There're some decisions, which you have to take: the stepping of zoom, the steping of the summarized data (it's not sure to worth collect summarized data for each zoom stepping), the organization of the summarized databases (the native data may be stored in daily files, but the daily data should be stored in monthly files). Another thing is to think about, the creation time of the summarized data. While native data should be stored just as the data arrives, summarized data may be calculated any time: as the native data arrives (in this case, a 1-s data is updated 300 times, which is not optimal to write to disk immediatelly, the summing should be done in memory); a background job should process the native data periodically, the sum data should be created lazy way, on demand. Don't forget, not-too-many years ago these issues were the database design issues. I can promise one thing: it will be fast, faster than anything (except using memory for storing data).
