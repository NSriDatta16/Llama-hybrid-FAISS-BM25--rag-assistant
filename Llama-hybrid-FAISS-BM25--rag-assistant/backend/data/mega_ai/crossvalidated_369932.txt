[site]: crossvalidated
[post_id]: 369932
[parent_id]: 
[tags]: 
Train a model on a "fleet" of related datasets

Suppose I have a machine that has some output behavior y over some independent variable x. I set up a predictive model to predict the value of y for an arbitrary x and it's working well. Then I get a second machine which is a lot like the first. I build a model for this as well. The models are completely independent. It works fine, too. When you examine the data, you see that the machines behave in pretty similar ways -- maybe they both have a linear relationship, but a slightly different slope. Probably there's a physical variation between the machines. Maybe my third machine is similar, but I have almost no operating data for it so I can't make a very good model. I know it will behave similarly to the other machines and I want to exploit this knowledge. What sort of options exist for modeling $n$ related items as a "fleet"? Here are some naive ideas: Treat all data as one big dataset, i.e. fit one 'average' model for all machines As above but add as many additional features as possible that might capture variations across machines Cluster the machines into k groups and fit k models Use a more sophisticated method which is designed for this type of problem (what options exist?) If the input data is time-series, does that change anything? Also, my company is calling this "fleet analytics", is there another name for this type of problem?
