[site]: datascience
[post_id]: 99532
[parent_id]: 
[tags]: 
How can - and why does - Random Forest over-forecast?

My understanding of Random Forest Regression is that each leaf node contains one or multiple examples from the training data. When predicting, each tree finds the most appropriate leaf and takes the mean of the target values of the examples, then the forest takes the mean of the values from each tree. Is that correct? Now to my specific example: Using RandomForestRegressor from sklearn I am using the following parameters: { 'n_estimators': 100, 'max_depth': 9, 'max_features': 1 / 3, 'criterion': 'mse', } The training data is in the range between -2.3 and 2.3 [1] . After training I predict for new values and I get an output of 33.5. If my understanding of Random Forest models is correct, that should be impossible because it should only be able to predict values in the range of the training data. To make it even weirder, if I rescale the training data so that it's in the range between 0 and 2.3 [2] and train on that, the predictions are all in the expected range. Can anyone figure out what could be causing this issue? [1] Strictly speaking the raw data is in the range 0.1 to 10, which is put through an ln transform to get -2.3 to +2.3. Normally I wouldn't use a logarithmic transform on data in this range, but this is a subset of a larger dataset which can have anything from 0.1 to 100,000. [2] I round all the raw values less than 1 to 1, so after the logarithmic transform it's 0 to +2.3.
