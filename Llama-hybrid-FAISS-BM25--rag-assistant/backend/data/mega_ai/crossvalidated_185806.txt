[site]: crossvalidated
[post_id]: 185806
[parent_id]: 185800
[tags]: 
Many packages provide cross validation directly. However, since you seem to be learning the principles, why not roll your own. For example like this Divide the data into K, lets say 10, chunks. Round 1: Designate chunk 1 as test data, the other 9 as training data. Build your model on the training data, test it on the test data. Store the testing error. Round 2: Designate chunk 2 as test data, the other 9 as training data. ... And so on. After 10 iterations, you have 10 error scores, one for each round. Average those and you have cross-validated your model. Once you have done that for many models, choose one and retrain it on the entire data set. That is then your final model. Which error score to use? You say it's a logistic regression, is it a binary classifier? Are the two potential mis-classifications equally critical? If so, why not simply use hit-ratio? It does the job and importantly communicates well. If you have other problems, then other error functions might suit you better. AIC / BIC are well understood in the statistical community and are fine to use as well, but communicates less well to people outside the profession. You know your problem and your audience, keep both in mind. With regards to penalized regression, which I didn't bring to mind as your question indicates you're learning CV, but @Edm is right in pointing them out. Here are some decent slides. http://statweb.stanford.edu/~tibs/sta305files/Rudyregularization.pdf Understanding the difference of the L1 and L2 norm will help you grasp why Lasso (L1) might be better suited for model selection that Ridge (L2). Good luck!
