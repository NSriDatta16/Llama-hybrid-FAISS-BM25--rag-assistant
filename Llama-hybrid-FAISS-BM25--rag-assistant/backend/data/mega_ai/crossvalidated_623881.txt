[site]: crossvalidated
[post_id]: 623881
[parent_id]: 623845
[tags]: 
This is meant to add an aspect to the other answers, which have some valuable material. When talking about "falsification" in statistics, in most cases this isn't the same as logical falsification in the sense that a hypothesis would be logically falsified if something happens that under the hypothesis is impossible , so the hypothesis is strictly incompatible with the observed data and must therefore be false (even though philosophy of science teaches that it isn't quite that easy even without taking into account statistical variation, as in many cases something that supposedly refutes a theory/hypothesis can be explained in other ways such as erroneous measurements, or failure of an auxiliary hypothesis that connects the main hypothesis to the data rather than failure of the main hypothesis of interest itself). In statistics, however, standard models will assign nonzero probability (or at least density) to every potential outcome, be the hypothesis true or not, and this means that any claim that a hypothesis "is falsified by the data" will come with a nonzero error probability, i.e., observed data were just very unlikely but not strictly impossible (regardless of whether we're talking Bayesian or frequentist inference). This also means that if we want to make statements such as "the hypothesis is falsified by the data", this needs to be based on a threshold (how small a probability is small enough to talk of "falsification"), or otherwise we can only make "graded" statements (such as p-value or posterior probability of hypothesis or Bayes factor equal to 0.03), but ultimately an interpretation in words is needed anyway. Regarding what the hypotheses are, statistical hypotheses are probability models (often parametric with a restrictive specification of parameter values), whereas "research hypotheses" are often informal (in some fields that strongly rely on mathematics, formal research hypotheses are the standard, but in some other fields almost everything is informal). So a research hypothesis needs to be "translated" into a statistical model, and this usually involves model assumptions such as independence or certain distributional shapes that can be doubted (and checked, but only to a limited extent) and may affect the interpretation of any outcome of the statistical analyses. In fact this adds an additional source of uncertainty on top of any uncertainty already modelled by the statistical model. The question regarding theory vs. hypothesis makes sense regarding the "research hypothesis", but chances are that in different fields and situations connections between what people call "theory" and what people call "research hypothesis" may differ (a research hypothesis may often be a more specified instance/special case of a more general theory); in any case the statistical hypothesis will not normally be identical to the research hypothesis let alone the scientific theory of interest, but will come with additional restrictions (and/or add-ons, as the theory to be tested may not involve observational variation, which is modelled by the statistical hypothesis). This also implies that for any result of a statistical analysis it makes sense to ask: "Could these data have led us to a different conclusion had we chosen another statistical model that is as well compatible with the research hypothesis of interest and - as far as this can be tested - the data?" Another uncomfortable implication is that if many statistical hypothesis are up for "falsification" at a certain probability standard, the probability that an error ("statistically falsifying" a hypothesis that is in fact true) occurs can become quite large, as the probability that one out of many (falsification) events obtains can be quite large even if the probability for every single such event is very small (referred to as the problem of multiple testing in statistics).
