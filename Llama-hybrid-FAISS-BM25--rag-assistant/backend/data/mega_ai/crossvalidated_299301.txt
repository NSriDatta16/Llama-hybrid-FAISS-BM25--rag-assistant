[site]: crossvalidated
[post_id]: 299301
[parent_id]: 299286
[tags]: 
Yeah I would say you're on to something. This reminded me of the beginning of chapter 16 of Murphy's Machine Learning: a Probabilistic Perspective : Although this [kernel methods (from the previous two chapters)] can work well, it relies on having a good kernel function to measure the similarity between data vectors. Often coming up with a good kernel function is quite difficult. For example, how do we define the similarity between two images? Pixel-wise comparison of intensities (which is what a Gaussian kernel corresponds to) does not work well. Although it is possible (and indeed common) to hand-engineer kernels for specific tasks (see e.g., the pyramid match kernel in Section 14.2.7), it would be more interesting if we could learn the kernel Then they go on to say An alternative approach is to dispense with kernels altogether, and try to learn useful features $\phi(x)$ directly from the input data. That is, we will create what we call an adaptive basis function model (ABM), which is a model of the form $$ f(x) = w_0 + \sum_{m=1}^M w_m \phi_m(x) $$ where $\phi_m(x)$ is the mâ€™th basis function, which is learned from data. This framework covers all of the models we will discuss in this chapter. Some of the methods that he includes in this adaptive basis function model category are classification trees, generalized additive models, boosting, feedforward neural networks, ensemble learning, and others.
