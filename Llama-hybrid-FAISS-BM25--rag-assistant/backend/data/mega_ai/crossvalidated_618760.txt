[site]: crossvalidated
[post_id]: 618760
[parent_id]: 618715
[tags]: 
It's not immediately clear to me why you went with scat() in the end; that distribution allows for both positive and negative ranks, if the response is the ranking. Likewise, the poisson() you used originally is for integer counts, while ranks are somewhat different. I'm not sure what the best approach to use here would be. Aside from that not so small problem, the specification of the linear predictor is a little easier. Say you want smooths per gender, age, and discipline, then one way to get that would be to create a factor that is the interaction of those two categorical variables library("dplyr") libray("forcats") test mutate(gd = fct_cross(Gender, Discipline, keep_empty = TRUE)) and then use that in a factor-by model m — noting that you must specify the group means via a parametric factor effect as shown — or, if you want to treat the gd factor as random, you could do use the fs basis m in which case one doesn't need the group mean parametric factor effect as the fs basis includes these group means as a random intercept. However, such a model may not be very parsimonious; it is unlikely that the smooth "effect" of age on the ranking varies that much across disciplines for example, and we wouldn't expect male and female skiers to be that different either. In that situation, it can be beneficial to fit "average" smooth effects and then smooth deviations or group-specific smooths. This is the focus of our HGAM paper that @Roland directed you to in the comments. For example, to have an average smooth effect of age, plus gender-specific smooths of age, plus discipline-specific smooths age, we might use the following model m I have indicated the three smooths by a label S1 , etc and justify the choices behind the representation below. S1 is the average smooth effect of age, over the entire data set. This would capture, for example, the general tendency for the world ranking to increase, reach a peak, and then decline again as a skier ages. S2 is a difference smooth, parametrised in such a way as to be orthogonal to the average smooth of age. This would describe the smooth differences between the "average" age effect and the smooth effects for each gender. S3 is a random factor smooth term, with a smooth of age for each discipline, which are allowed to differ by gender. I suggest modelling this more like a random effect because one could think of many disciplines from which the disciplines in the FIS rankings are drawn, and there are likely quite a few disciplines, but they all have roughly the same wiggliness, but different shapes — I wouldn't expect the smooth effect of age to be really wiggly for the slalom and really smooth for the downhill events for example. As the S3 term is fully penalised and the S1 and S2 terms are orthogonal by design of the "sz" basis, it shouldn't be a problem that you have multiple smooths of the same covariate age . If you do have difficulties, adding m = 1 to the S3 smooths might help, but I wouldn't advise it out of the gate. Of course, there are different ways to reprameterise this model. An obvious one is to not estimate an average smooth, in which case we might have: m But as I now see that you use only four disciplines, an alternative to representing the smooth effects of age by Discipline would be: m where we build up the effects as smooth deviations from the average smooth ( S1 ), first with first order interactions ( S2 and S3 ) and finally a second order interaction. If you want each of the smooths in a given term ( S2 , S3 etc) to share the same smoothing parameter, then you can id = 1 to that term. There's likely not much point in going through all the possibilities; the HGAM paper describes the general ideas and ways to specify the models (although it is missing the "sz" basis as that wasn't available in mgcv at the time of writing.) Nested random effects in GAMs This whole difference between nested, crossed, random effects goes away if you code your factor variables appropriately. Here's a reproducible example: library("dplyr") library("lme4") library("mgcv") data(Oxide, package = "nlme") Oxide In the last line I recode the original Wafer variable (which only takes values 1, 2, 3) to include the Lot information; wafer 1 on lot 1 is not the same wafer as wafer 1 on lot 2. We make this distinction clear by recoding the data. Next I fit several models, first with lmer() amd finally gam() to show how the original coding needs a nested random effect (1|Lot/Wafer) but once we use the unambiguous coding in Wafer2 , it doesn't matter which form we use, we get the same model fits and hence this is the correct way to fit a nested random effect with gam() : lmer_0 So we have lmer_0 , a lmer() model in nested form using the original coding for Wafer lmer_1 , a lmer() model in nested form using the unambiguous coding for Wafer , Wafer2 lmer_2 , a lmer() model in crossed form using the unambiguous coding for Wafer , Wafer2 , and gam_1 , a GAM version of the model using the unambiguous coding of Wafer , Wafer2 If we now extract the variance components of all these models, we can see that they are exactly the same r$> VarCorr(lmer_0) Groups Name Std.Dev. Wafer:Lot (Intercept) 5.9888 Lot (Intercept) 11.3980 Residual 3.5453 r$> VarCorr(lmer_1) Groups Name Std.Dev. Wafer2:Lot (Intercept) 5.9888 Lot (Intercept) 11.3980 Residual 3.5453 r$> VarCorr(lmer_2) Groups Name Std.Dev. Wafer2 (Intercept) 5.9888 Lot (Intercept) 11.3980 Residual 3.5453 r$> gratia::variance_comp(gam_1) # A tibble: 3 × 5 component variance std_dev lower_ci upper_ci 1 s(Lot) 130. 11.4 6.39 20.3 2 s(Wafer2) 35.9 5.99 4.06 8.82 3 scale 12.6 3.55 2.90 4.33 So, to avoid these kinds of quandaries, just recode your lower-level variable such that it includes the higher-level variable, just as I did for the Wafer variable by pasting the Lot and Wafer variables together.
