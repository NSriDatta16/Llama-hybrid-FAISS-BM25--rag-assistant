[site]: datascience
[post_id]: 87700
[parent_id]: 
[tags]: 
How did you evaluate training loss on 100GB of data in deep learning?

Background: In my problem, deep learning is trained on on 100GB data. Question: In order to monitor the training error versus epochs, we need to evaluate the training loss on our datasets, but does that mean we have to forward pass 100GB data in order to get the exact metric? What do people do in reality? You don't want evaluate 100 GB data every 100 epochs....Maybe some random sampling?
