[site]: crossvalidated
[post_id]: 606975
[parent_id]: 
[tags]: 
which one between XGBoost and neural networks is more interpretable?

I have heard people say, "One of the disadvantages of neural networks is that they are generally less interpretable". But I wonder, how is another model, such as XGBoost, more interpretable than neural networks? In XGBoost, one could use feature importance, SHAP, or PDP to explain the model, which I believe can also apply to neural networks?
