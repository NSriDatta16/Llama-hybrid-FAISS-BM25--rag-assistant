[site]: crossvalidated
[post_id]: 384360
[parent_id]: 383958
[tags]: 
I'm unclear why this is exactly the case- why are non-negative elements not useful for comparing documents that don't share terms? Just because two documents don't share terms doesn't mean they're dissimilar. In other words, Bag of Words model works good if it says something is similar (of course you can run into problems with similar words with different meaning), but it does a worse job telling something is dissimilar. An example for the following statement: because two vectors with a cosine distance of 1 will be considered far apart, even if they are semantically similar Let s1 = "A young woman owns a cat" s2 = "A girl has a kitten" Then, after deleting 'a', which would be considered as a stopword by most tools for text preprocessing, these sentences will have biggest possible cosine distance - they do not share a word. But they have very similar meaning. Furthermore, if you really wanted to use negative elements, couldn't you standardize your encoding to have mean of 0? That would be a different encoding. It would also destroy useful properties - bag of words encodings have mostly zeros, so they can be represented using sparse vectors - if you were to represent couple thousand examples that have tens of thousands of dimensions you'd need much more memory. There is other method that sort of does what you described, but it also uses dimensionality reduction - it's called Latent Semantic Analysis .
