[site]: crossvalidated
[post_id]: 487642
[parent_id]: 487459
[tags]: 
There are a few minor issues with your sampler. First, you should always compute the PDF or PMF on a log scale, as for any non-trivial problem you will likely run into overflow/underflow issues (speaking from experience, this happens very quickly). I've switched these functions to a log scale but it's worth checking that I've done this correctly. The reason your sampler didn't work as you posted it is mainly because you didn't assign curr_s to mov_s (ie, you didn't update the current value if the Metropolis-Hastings ratio was above the uniform value). For a symmetric non-negative proposal, I've seen people use a log-normal distribution. However you could also adapt the acceptance ratio to be a Metropolis-Hastings ratio rather than a Metropolis ratio (ie, include the ratio of the densities of the proposal as well as the ratio of the posterior, see this for a quick guide). I don't think a truncated uniform distribution is symmetric so the behaviour of your sampler at values of sigma You also didn't define your data ( X , y ) or import all the libraries you used, which is always nice as it makes it easier to load and debug your code. If I can be so bold as to suggest some simple next steps - it would be pretty straightforward to allow an arbitrary design matrix rather than simply slope and intercept. As for your question, What are the benefits of sampling different sigma values? For the model, $y_i \sim N(X_i\beta, \sigma^2)$ , it is natural in most cases to assume that you do not know the magnitude of noise around the conditional mean. Learning this magnitude will enable you to make better predictions using the posterior estimates. This magnitude is controlled by $\sigma^2$ . I struggle to think of a situation where the regression coefficients ( $\beta$ ) are unknown, but the magnitude of the residuals $\epsilon_i = y_i - X_i\beta$ is known. Furthermore, to quantify uncertainty around predictions, it's necessary to have a good estimate of the variance around the conditional mean. import numpy as np import random import seaborn as sns def normalPDF(x,mu,sigma): num = np.exp(-1/2*((x-mu)/sigma)**2) den = np.sqrt(2*np.pi)*sigma return num/den def invGamma(x,a,b): non_zero = int(x>=0) func = x**(a-1)*np.exp(-x/b) return non_zero*func def lr_mcmc(X,Y,hops=10_000): samples = [] curr_a = random.gauss(1,1) curr_b = random.gauss(2,1) curr_s = random.uniform(3,1) prior_a_curr = normalPDF(x=curr_a,mu=1,sigma=1) prior_b_curr = normalPDF(x=curr_b,mu=2,sigma=1) prior_s_curr = invGamma(x=curr_s,a=3,b=1) log_lik_curr = sum([np.log(normalPDF(x=curr_b*x + curr_a,mu=y,sigma=curr_s)) for x,y in zip(X,Y)]) current_numerator = log_lik_curr + np.log(prior_a_curr) + np.log(prior_b_curr) + np.log(prior_s_curr) count = 0 for i in range(hops): samples.append((curr_b,curr_a,curr_s)) if count == 0: #propose movement to b mov_a = curr_a mov_b = curr_b + random.uniform(-0.25,0.25) mov_s = curr_s count += 1 elif count == 1: #propose movement to a mov_a = curr_a + random.uniform(-0.25,0.25) mov_b = curr_b mov_s = curr_s count += 1 else: #propose movement to s mov_a = curr_a mov_b = curr_b mov_s = curr_s + random.uniform(-0.25,0.25) count = 0 prior_b_mov = normalPDF(x=mov_b,mu=2,sigma=1) prior_a_mov = normalPDF(x=mov_a,mu=1,sigma=1) prior_s_mov = invGamma(x=mov_s,a=3,b=1) if prior_s_mov
