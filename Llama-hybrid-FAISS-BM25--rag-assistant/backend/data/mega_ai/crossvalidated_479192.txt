[site]: crossvalidated
[post_id]: 479192
[parent_id]: 479176
[tags]: 
The KDE model, as opposed to (for example) a t-test, does not assume anything about the values distribution. The resulting distribution is indeed a mixture of Gaussians (given a Gaussian kernel), and if the sample values are far enough from each other (compared to the kernel's 'bandwidth' parameter), the resulting log-likelihood of each new data point will depend almost entirely on the distance from the nearest point in the sample. The reason for this is that the kernel density (i.e. likelihood function) is the average across data points: $f(y)= \sum_{i}^N K(y-x_i;h)$ , where $y$ is the new data point, $x_i$ are the old data points, $K$ is the kernel function and $h$ is the bandwidth parameter. So, if the new point is close to some old point $x_0$ and relatively far from all the rest, we'll have $f(y) \approx K(y-x_0;h) $ , and the log likelihood will be $log(f(y)) \approx log(K(y-x_0; h))$ Now, if you're looking for anomaly detection, you can expect the KDE to basically single out new points that are far enough from the given ("training") sample. Keeping that in mind, the Python implementation in SKlearn ( link ) keeps the data points in a tree structure, which is a bit faster to search through when comparing with new points. As a side note, we must note that "comparing new values to old values" isn't really utilizing anything temporal in the time-series. Hope this answers your question to some extent.
