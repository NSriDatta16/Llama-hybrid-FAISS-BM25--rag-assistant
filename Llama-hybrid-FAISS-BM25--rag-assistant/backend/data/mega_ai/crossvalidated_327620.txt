[site]: crossvalidated
[post_id]: 327620
[parent_id]: 
[tags]: 
Dropout before Batch Normalization?

In the last course of the Deep Learning Specialization on Coursera from Andrew Ng, you can see that he uses the following sequence of layers on the output of an LSTM layer: Dropout -> BatchNorm -> Dropout. To be honest, I do not see any sense in this. I don't think dropout should be used before batch normalization, depending on the implementation in Keras, which I am not completely familiar with, dropout either has no effect or has a bad effect. I might be missing something here, though, and if anyone has any knowledge of why something like this could be useful, I'd love to hear from them.
