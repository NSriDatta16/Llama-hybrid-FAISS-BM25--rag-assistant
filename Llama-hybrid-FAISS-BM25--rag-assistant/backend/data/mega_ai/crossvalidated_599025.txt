[site]: crossvalidated
[post_id]: 599025
[parent_id]: 
[tags]: 
Information criterion for cases where $n\gg k$?

I'm looking for an information criterion metric that effectively penalizes the number of parameters ( $k$ ) in cases where we have huge sample sizes. In my particular case my sample size is $\approx80,000.$ I happen to know that the ground truth model has $k=2$ parameters. But if I use Bayesian information Criterion it will incorrectly indicate $k=3$ as the best model. There's some noise which is why the $k=3$ model fits better and given $n\gg k$ it has lower BIC. Due to high $n,$ the $k$ term is negligible. Unfortunately though, I can't model this noise. Instead, an easy solution would be if I could penalize for $k$ more aggressively, ideally with a method with some theoretical support. Any suggestions?
