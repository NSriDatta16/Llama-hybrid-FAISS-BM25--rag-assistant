[site]: crossvalidated
[post_id]: 82134
[parent_id]: 22787
[tags]: 
In this case the MLE and EM estimators are equivalent, since the MLE estimator is actually just a special case of the EM estimator. (I am assuming a frequentist framework in my answer; this isn't true for EM in a Bayesian context in which we're talking about MAP's). Since there is no missing data (just an unknown parameter), the E step simply returns the log likelihood, regardless of your choice of $k^{(t)}$. The M step then maximizes the log likelihood, yielding the MLE. EM would be applicable, for example, if you had observed data from a mixture of two Weibull distributions with parameters $k_1$ and $k_2$, but you didn't know which of these two distributions each observation came from.
