[site]: crossvalidated
[post_id]: 561152
[parent_id]: 561124
[tags]: 
Unfortunately, without a specific case in mind, it's a bit hard to answer! Reason simply being that as with many things, the true answer is "it depends". I'll outline a couple of potential examples/uses/perspectives for you to consider. I will assume some kind of time series data that you're smoothing. Scenario 1: You having some kind of series with trends but lots of noise around the trend, and you want to just get a better visual/intuitive understanding of what's going on, what the general trend or pattern is. In such a case, I would recommend taking some small portion of the data (first x points) and simply manually playing around with the value of alpha until visually, your smoothed line seems to nicely reflect the denoised/smoothed trend you want to see. Then, once you've arrived at the alpha that seems to work for that first set of the data, apply it on the rest. Scenario 2: You want to use the smoothed data predictively in some kind of model (regression, machine learning etc). Here, I would apply a wide variety of alpha's (maybe 0.1, 0.2, 0.3...0.8, 0.9) and running your model on all of those as variables/features. Then, using cross validation and some basic feature selection, you would be able to select which alpha weighting(s) is/are most appropriate for the predictive task. Scenario 3/other: Broadly speaking, if there is a quantifiable objective you're trying to achieve (find the alpha that best approximates ...), then I think the way to go is just using some kind of cross-validation to try out different alphas and apply it on unseen data. This is all very general (as is the question), and if you have more specific questions or if I was unclear, feel free to ask!
