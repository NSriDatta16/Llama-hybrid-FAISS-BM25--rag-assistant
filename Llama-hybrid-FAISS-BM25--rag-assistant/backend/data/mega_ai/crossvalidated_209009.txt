[site]: crossvalidated
[post_id]: 209009
[parent_id]: 
[tags]: 
How to treat categorical predictors in LASSO

I am running a LASSO that has some categorical variable predictors and some continuous ones. I have a question about the categorical variables. The first step I understand is to break each of them into dummies, standardize them for fair penalization, and then regress. Several options arise for treating the dummy variables: Include all but one of the dummies for each factor, leaving that one as a reference level. The interpretation of a dummy coefficient is relative to the excluded "reference" category. The intercept is now the mean response for the reference category. Group the variables in each factor so they're either all excluded or all-but-one included. I believe that's what @Glen_b is suggesting here : Normally, yes, you keep your factors all together. There's several R packages that can do this, including glmnet Include all of the levels, as suggested by @Andrew M here : You may also want to change the default contrast function, which by default leaves out one level of the each factor (treatment coding). But because of the lasso penalty, this is no longer necessary for identifiability, and in fact makes interpretation of the selected variables more complicated. To do this, set contr.Dummy Now, whatever levels of a factor are selected, you can think of it as suggesting that these specific levels matter, versus all the omitted levels. In machine learning, I have seen this coding referred to as one-hot encoding. Questions: What is the interpretation of the intercept and coefficients under each of these approaches? What are the considerations involved in selecting one of them? Do we un-scale the dummy coefficients and then interpret them as a change of going from off to on?
