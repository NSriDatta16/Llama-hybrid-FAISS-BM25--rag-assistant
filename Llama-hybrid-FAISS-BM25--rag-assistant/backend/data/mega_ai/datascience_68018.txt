[site]: datascience
[post_id]: 68018
[parent_id]: 67549
[tags]: 
the network architecture above is a very strange choice. When you have only 6 input features, it is weird to have so much Dense layers stacked. if network is overfitting, WHERE IS DROPOUT? Why not trying some regularizers, if the latter does not help? +1 for David Waterworth - correlation/causal analysis is not everything yet. Does linear regression provide better R-square values? what is output(target) variable range? Maybe it should be mapped/scaled to something reasonable? (I judge from loss values). activation function and initializers are important too. Try using different values, rather than relu/linear and 'normal' initializer. EDIT: yes, this should be enough data, if your data has only 6 inputs. However, you can try augmenting data too, if it makes sense and you can make reasonable assumptions in your case - sometimes it gives difference in the long run, even if in the beginning you think it does not work. EDIT2: with specific datasets, neural network can get into local plateau (not minima however), where it does not escape. To test this hypothesis, you can set learning rate to small value and all initializers to generate small values too - then network may not go to this plateau suddenly, but goes to global minima instead. EDIT3: increasing batch size leads to faster but poorer convergence on certain datasets. On a smaller network, batch size = 1 sometimes makes wonders.
