[site]: datascience
[post_id]: 26278
[parent_id]: 26271
[tags]: 
This is because you have an imbalanced dataset towards class 0. I have taken a look on the logistic regression coefficient you get. On the below chart 1, I have plotted the decision boundary you get with your logistic regression. On the second chart, I have plotted what you have expected. So why this difference and why does your logistic regression yield chart 1? The reason is even if it is a machine learning algorithm, it is not really smart. The logistic regression algorithm wants to minimize its cost fucntion (cross-entropy). Cross-entropy can be defined in a really simple way as the distance between your points and the decision boundary. But, in your training case, you have two class-1 observations for only one class_0 observations. Thus the machine learning algorithm will see that in order to reach the lowest cost function as possible, the best choice is to promote the two class-1 observations than the only one class-0 observation. The cost function value is larger on chart 2 than on chart 1, this is why it has yielded chart 1 result. Majority wins ! To avoid this problem, here are three ideas you can do : Undersample / oversample your dataset in order to have a similar number of observations in each class of your training set. Change your observations class weight. Use an other cost function like Hinge loss that is the cost function used in SVM and where the goal is to maximize the margin between classes.
