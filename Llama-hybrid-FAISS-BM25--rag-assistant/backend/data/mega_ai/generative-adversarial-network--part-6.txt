u _{G},D):=\operatorname {E} _{c\sim \mu _{C},x\sim \mu _{\text{ref}}(c)}[\ln D(x,c)]+\operatorname {E} _{c\sim \mu _{C},x\sim \mu _{G}(c)}[\ln(1-D(x,c))]} where μ C {\displaystyle \mu _{C}} is a probability distribution over classes, μ ref ( c ) {\displaystyle \mu _{\text{ref}}(c)} is the probability distribution of real images of class c {\displaystyle c} , and μ G ( c ) {\displaystyle \mu _{G}(c)} the probability distribution of images generated by the generator when given class label c {\displaystyle c} . In 2017, a conditional GAN learned to generate 1000 image classes of ImageNet. GANs with alternative architectures The GAN game is a general framework and can be run with any reasonable parametrization of the generator G {\displaystyle G} and discriminator D {\displaystyle D} . In the original paper, the authors demonstrated it using multilayer perceptron networks and convolutional neural networks. Many alternative architectures have been tried. Deep convolutional GAN (DCGAN): For both generator and discriminator, uses only deep networks consisting entirely of convolution-deconvolution layers, that is, fully convolutional networks. Self-attention GAN (SAGAN): Starts with the DCGAN, then adds residually-connected standard self-attention modules to the generator and discriminator. Variational autoencoder GAN (VAEGAN): Uses a variational autoencoder (VAE) for the generator. Transformer GAN (TransGAN): Uses the pure transformer architecture for both the generator and discriminator, entirely devoid of convolution-deconvolution layers. Flow-GAN: Uses flow-based generative model for the generator, allowing efficient computation of the likelihood function. GANs with alternative objectives Many GAN variants are merely obtained by changing the loss functions for the generator and discriminator. Original GAN: We recast the original GAN objective into a form more convenient for comparison: { min D L D ( D , μ G ) = − E x ∼ μ G ⁡ [ ln ⁡ D ( x ) ] − E x ∼ μ ref ⁡ [ ln ⁡ ( 1 − D ( x ) ) ] min G L G ( D , μ G ) = − E x ∼ μ G ⁡ [ ln ⁡ ( 1 − D ( x ) ) ] {\displaystyle {\begin{cases}\min _{D}L_{D}(D,\mu _{G})=-\operatorname {E} _{x\sim \mu _{G}}[\ln D(x)]-\operatorname {E} _{x\sim \mu _{\text{ref}}}[\ln(1-D(x))]\\\min _{G}L_{G}(D,\mu _{G})=-\operatorname {E} _{x\sim \mu _{G}}[\ln(1-D(x))]\end{cases}}} Original GAN, non-saturating loss: This objective for generator was recommended in the original paper for faster convergence. L G = E x ∼ μ G ⁡ [ ln ⁡ D ( x ) ] {\displaystyle L_{G}=\operatorname {E} _{x\sim \mu _{G}}[\ln D(x)]} The effect of using this objective is analyzed in Section 2.2.2 of Arjovsky et al. Original GAN, maximum likelihood: L G = E x ∼ μ G ⁡ [ ( exp ∘ σ − 1 ∘ D ) ( x ) ] {\displaystyle L_{G}=\operatorname {E} _{x\sim \mu _{G}}[({\exp }\circ \sigma ^{-1}\circ D)(x)]} where σ {\displaystyle \sigma } is the logistic function. When the discriminator is optimal, the generator gradient is the same as in maximum likelihood estimation, even though GAN cannot perform maximum likelihood estimation itself. Hinge loss GAN: L D = − E x ∼ p ref ⁡ [ min ( 0 , − 1 + D ( x ) ) ] − E x ∼ μ G ⁡ [ min ( 0 , − 1 − D ( x ) ) ] {\displaystyle L_{D}=-\operatorname {E} _{x\sim p_{\text{ref}}}\left[\min \left(0,-1+D(x)\right)\right]-\operatorname {E} _{x\sim \mu _{G}}\left[\min \left(0,-1-D\left(x\right)\right)\right]} L G = − E x ∼ μ G ⁡ [ D ( x ) ] {\displaystyle L_{G}=-\operatorname {E} _{x\sim \mu _{G}}[D(x)]} Least squares GAN: L D = E x ∼ μ ref ⁡ [ ( D ( x ) − b ) 2 ] + E x ∼ μ G ⁡ [ ( D ( x ) − a ) 2 ] {\displaystyle L_{D}=\operatorname {E} _{x\sim \mu _{\text{ref}}}[(D(x)-b)^{2}]+\operatorname {E} _{x\sim \mu _{G}}[(D(x)-a)^{2}]} L G = E x ∼ μ G ⁡ [ ( D ( x ) − c ) 2 ] {\displaystyle L_{G}=\operatorname {E} _{x\sim \mu _{G}}[(D(x)-c)^{2}]} where a , b , c {\displaystyle a,b,c} are parameters to be chosen. The authors recommended a = − 1 , b = 1 , c = 0 {\displaystyle a=-1,b=1,c=0} . Wasserstein GAN (WGAN) The Wasserstein GAN modifies the GAN