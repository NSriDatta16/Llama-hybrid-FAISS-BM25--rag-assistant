[site]: datascience
[post_id]: 106298
[parent_id]: 
[tags]: 
Why is newer GPU slower than older one during training?

I am a very beginner in deep learning and am training a Tacotron 2 model written in PyTorch (see the link if it matters) on Google Colab and was lately granted an A100-SXM4-40GB (CUDA 11.2) GPU whereas I usually get a P100 PCIE 15GB GPU. I was surprised because A100 (newer GPU) was slower (around 1 second longer for each step at a rate of around 2.5s / iteration ) than the P100 (older GPU) with a rate of around 1.5s / iteration. Based on this benchmark https://lambdalabs.com/gpu-benchmarks I found I would expect A100 (newer) GPU to be way faster than P100 which is said to be much slower than V100 that appears on the graph. I did not mean to make a fair comparison of both since I want the training to be done, so there might be biases. For example the newer GPU took over the training already done by the older GPU from step 35k. Batch size was 32 in both case and increasing it to 64 on the newer GPU did not change the ranking. I am just curious why it is so because I would expect the newer GPU to outperform the older one as it is the case for example with mining or FPS in game. I don't think Google is cheating and reporting whatever GPU they choose to please the user, so can you provide me with explanations on what could cause this relative slowness on this high end GPU ? Or maybe a flag I should add to Python to enable full utilization of the newer GPU capabilities ? Thank you Edit : Here is an excerpt from the output on P100 from yesterday training (on average 1.67 s / it on that output excerpt) : INFO:root:Status - [Epoch 285: Iteration 40623] Train loss 0.49866 Attention score 0.14761 1.51s/it INFO:root:Status - [Epoch 285: Iteration 40624] Train loss 0.51910 Attention score 0.16025 1.32s/it INFO:root:Status - [Epoch 285: Iteration 40625] Train loss 0.44508 Attention score 0.15360 1.72s/it INFO:root:Status - [Epoch 285: Iteration 40626] Train loss 0.48054 Attention score 0.14250 1.40s/it INFO:root:Status - [Epoch 285: Iteration 40627] Train loss 0.44514 Attention score 0.15402 1.65s/it INFO:root:Status - [Epoch 285: Iteration 40628] Train loss 0.45726 Attention score 0.14356 1.53s/it INFO:root:Status - [Epoch 285: Iteration 40629] Train loss 0.37117 Attention score 0.15153 1.84s/it INFO:root:Status - [Epoch 285: Iteration 40630] Train loss 0.47039 Attention score 0.15539 1.67s/it INFO:root:Status - [Epoch 285: Iteration 40631] Train loss 0.48183 Attention score 0.14884 1.51s/it INFO:root:Status - [Epoch 285: Iteration 40632] Train loss 0.48286 Attention score 0.15338 1.55s/it INFO:root:Status - [Epoch 285: Iteration 40633] Train loss 0.48954 Attention score 0.15787 1.51s/it INFO:root:Status - [Epoch 285: Iteration 40634] Train loss 0.28543 Attention score 0.15507 2.88s/it INFO:root:Status - [Epoch 285: Iteration 40635] Train loss 0.49223 Attention score 0.15638 1.44s/it INFO:root:Status - [Epoch 285: Iteration 40636] Train loss 0.41047 Attention score 0.15551 1.84s/it INFO:root:Status - [Epoch 285: Iteration 40637] Train loss 0.38390 Attention score 0.15803 1.86s/it INFO:root:Status - [Epoch 285: Iteration 40638] Train loss 0.40857 Attention score 0.15724 1.81s/it INFO:root:Status - [Epoch 285: Iteration 40639] Train loss 0.51904 Attention score 0.15267 1.45s/it INFO:root:Status - [Epoch 285: Iteration 40640] Train loss 0.36069 Attention score 0.16232 2.25s/it INFO:root:Status - [Epoch 285: Iteration 40641] Train loss 0.50856 Attention score 0.15378 1.49s/it INFO:root:Status - [Epoch 285: Iteration 40642] Train loss 0.47423 Attention score 0.15945 1.66s/it INFO:root:Status - [Epoch 285: Iteration 40643] Train loss 0.37378 Attention score 0.16586 2.02s/it INFO:root:Status - [Epoch 285: Iteration 40644] Train loss 0.47522 Attention score 0.15590 1.64s/it INFO:root:Status - [Epoch 285: Iteration 40645] Train loss 0.48525 Attention score 0.14568 1.61s/it INFO:root:Status - [Epoch 285: Iteration 40646] Train loss 0.47989 Attention score 0.15086 1.56s/it INFO:root:Status - [Epoch 285: Iteration 40647] Train loss 0.41694 Attention score 0.14687 1.72s/it INFO:root:Status - [Epoch 285: Iteration 40648] Train loss 0.47559 Attention score 0.15118 1.65s/it INFO:root:Status - [Epoch 285: Iteration 40649] Train loss 0.46186 Attention score 0.15468 1.60s/it INFO:root:Status - [Epoch 285: Iteration 40650] Train loss 0.50873 Attention score 0.15012 1.47s/it INFO:root:Status - [Epoch 285: Iteration 40651] Train loss 0.44703 Attention score 0.14831 1.73s/it INFO:root:Status - [Epoch 285: Iteration 40652] Train loss 0.41788 Attention score 0.15692 1.76s/it INFO:root:Status - [Epoch 285: Iteration 40653] Train loss 0.46428 Attention score 0.14372 1.56s/it INFO:root:Status - [Epoch 285: Iteration 40654] Train loss 0.44389 Attention score 0.15256 1.67s/it INFO:root:Status - [Epoch 285: Iteration 40655] Train loss 0.47932 Attention score 0.15426 1.48s/it INFO:root:Status - [Epoch 285: Iteration 40656] Train loss 0.48285 Attention score 0.15724 1.51s/it INFO:root:Status - [Epoch 285: Iteration 40657] Train loss 0.43503 Attention score 0.15853 1.75s/it INFO:root:Status - [Epoch 285: Iteration 40658] Train loss 0.45500 Attention score 0.16344 1.56s/it And here an excerpt of A100 output from today (after a night of training) (on average 2.25 s / it on that output excerpt) : INFO:root:Status - [Epoch 429: Iteration 52026] Train loss 0.42092 Attention score 0.19559 1.84s/it INFO:root:Status - [Epoch 429: Iteration 52027] Train loss 0.35296 Attention score 0.18359 2.13s/it INFO:root:Status - [Epoch 429: Iteration 52028] Train loss 0.29359 Attention score 0.18109 2.64s/it INFO:root:Status - [Epoch 429: Iteration 52029] Train loss 0.19015 Attention score 0.18593 4.08s/it INFO:root:Status - [Epoch 429: Iteration 52030] Train loss 0.41178 Attention score 0.17960 2.02s/it INFO:root:Status - [Epoch 429: Iteration 52031] Train loss 0.31377 Attention score 0.18884 2.33s/it INFO:root:Status - [Epoch 429: Iteration 52032] Train loss 0.28386 Attention score 0.19053 2.85s/it INFO:root:Status - [Epoch 429: Iteration 52033] Train loss 0.35741 Attention score 0.18665 2.18s/it INFO:root:Status - [Epoch 429: Iteration 52034] Train loss 0.34859 Attention score 0.18189 2.34s/it INFO:root:Status - [Epoch 429: Iteration 52035] Train loss 0.40888 Attention score 0.18643 1.86s/it INFO:root:Status - [Epoch 429: Iteration 52036] Train loss 0.35856 Attention score 0.18711 2.21s/it INFO:root:Status - [Epoch 429: Iteration 52037] Train loss 0.28734 Attention score 0.18050 2.71s/it INFO:root:Status - [Epoch 429: Iteration 52038] Train loss 0.35206 Attention score 0.18670 2.24s/it INFO:root:Status - [Epoch 429: Iteration 52039] Train loss 0.28182 Attention score 0.18771 2.89s/it INFO:root:Status - [Epoch 429: Iteration 52040] Train loss 0.43182 Attention score 0.19088 1.83s/it INFO:root:Status - [Epoch 429: Iteration 52041] Train loss 0.32728 Attention score 0.18775 2.43s/it INFO:root:Status - [Epoch 429: Iteration 52042] Train loss 0.29947 Attention score 0.19648 2.72s/it INFO:root:Status - [Epoch 429: Iteration 52043] Train loss 0.31517 Attention score 0.18025 2.59s/it INFO:root:Status - [Epoch 429: Iteration 52044] Train loss 0.36604 Attention score 0.18211 2.15s/it INFO:root:Status - [Epoch 429: Iteration 52045] Train loss 0.45129 Attention score 0.18199 1.68s/it INFO:root:Status - [Epoch 429: Iteration 52046] Train loss 0.39778 Attention score 0.18017 1.98s/it INFO:root:Status - [Epoch 429: Iteration 52047] Train loss 0.40937 Attention score 0.18065 1.90s/it INFO:root:Status - [Epoch 429: Iteration 52048] Train loss 0.37384 Attention score 0.18313 2.09s/it INFO:root:Status - [Epoch 429: Iteration 52049] Train loss 0.44678 Attention score 0.17901 1.69s/it INFO:root:Status - [Epoch 429: Iteration 52050] Train loss 0.26380 Attention score 0.18450 3.13s/it INFO:root:Status - [Epoch 429: Iteration 52051] Train loss 0.37418 Attention score 0.18565 2.19s/it INFO:root:Status - [Epoch 429: Iteration 52052] Train loss 0.36242 Attention score 0.18587 2.11s/it INFO:root:Status - [Epoch 429: Iteration 52053] Train loss 0.32111 Attention score 0.18276 2.50s/it INFO:root:Status - [Epoch 429: Iteration 52054] Train loss 0.42933 Attention score 0.18359 1.84s/it INFO:root:Status - [Epoch 429: Iteration 52055] Train loss 0.34809 Attention score 0.18546 2.30s/it INFO:root:Status - [Epoch 429: Iteration 52056] Train loss 0.43713 Attention score 0.17860 1.81s/it INFO:root:Status - [Epoch 429: Iteration 52057] Train loss 0.39184 Attention score 0.18179 2.00s/it INFO:root:Status - [Epoch 429: Iteration 52058] Train loss 0.42319 Attention score 0.18336 1.81s/it INFO:root:Status - [Epoch 429: Iteration 52059] Train loss 0.38753 Attention score 0.18312 2.02s/it INFO:root:Status - [Epoch 429: Iteration 52060] Train loss 0.40894 Attention score 0.17824 1.98s/it
