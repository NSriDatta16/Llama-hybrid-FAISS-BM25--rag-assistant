[site]: datascience
[post_id]: 32381
[parent_id]: 32379
[tags]: 
In Naive Bayes, you need two values : The prior of each class : $p(c_j)$ which is the proportion of each class in the training set. The conditional probability of each term $i$ from a document regarding to each class : $p(w_i|c_j) = \frac{count(w_i, c_j)}{\sum\limits_{w\in V}count(w, c_j)}$ which corresponds to the fraction of times the word $w_i$ appears among all words in documents of class $c_j$ ($V$ is the vocabulary). The classification rule being $\underset{c_j}{argmax} \; p(c_j)p(w|c_j)$ with $p(w|c_j) = \prod\limits_{w_i}p(w_i|c_j)$ thanks to the independence assumption. To prevent underflow errors, we often use the log : $log \;\;p(w|c_j) = \sum\limits_{w_i}log\;\;p(w_i|c_j)$ Therefore, the final word is : to classify a document (or paragraph or whatever piece of text), you take each word $i$ from it, and for each class $j$, see the fraction of times it appears in it (in documents from class $j$), in the training data (which is a probability), and add up the log. It will give you $j$ probabilities (one for each class), and you take the class corresponding to the maximum. What is not clear in your question is the relation between labels and sentiment analysis. See this for a detailed explanation and examples.
