[site]: crossvalidated
[post_id]: 534571
[parent_id]: 534514
[tags]: 
The term "normal dataset" is somewhat ambiguous, but presumably you do not mean "normally distributed", but "without outliers". A problem might be that the "novel" data may overlap with your "normal" data and that you might need to reject a fraction of "normal" data in order to achieve better detection of "novel" data. If you only want to detect "novelties" that are very different from the "normal" data, you can proceed as outlined in section 5 of "Reject Options and Confidence Measures for kNN Classifiers" (disclaimer: the article is by me). The idea is to reject (i.e.: classify as "novelty") data points that have an average distance to the k nearest neighbors among the training samples that is greater than the largest average kNN distance in the training data. Or more formally: Define for a test point $x$ the avergae distance $D(x)$ to the $k$ nearest neighbores $y_1,\ldots,y_k$ in the training set $Y$ : $$D(x) = \frac{1}{k}\sum_{i=1}^k || x-y_i||$$ Comute the maximum value of $A$ on the training data: $$A_{max}=\max_{y\in Y}\{A(y)\}$$ Classify a test sample $x$ as "novel", if $A(x)>A_{max}$ . Like other novelty detection algoithms, it can be weakened by specifying a fraction (a "contamination" parameter as mentioned by @tim) of the "normal" data that is rejcected as "novel" by evaluation the empirical distribution of $\{A(y)\mid y\in Y\}$ .
