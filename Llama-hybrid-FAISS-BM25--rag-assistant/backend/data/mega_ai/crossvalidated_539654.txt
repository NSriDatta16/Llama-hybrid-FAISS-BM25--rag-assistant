[site]: crossvalidated
[post_id]: 539654
[parent_id]: 539548
[tags]: 
Here's a example of how to use Bayesian priors in a way that even "frequentists" agree is useful. Let's say you want to estimate the know how well students at 100 different schools are doing in math, so you can identify schools that are doing particularly well or poorly. But you can only assess math knowledge through a test that not all students took. At most schools well over 100 students take the test but at some schools only a few take it due (assume nonresponse is random). Overall, the average score on test is 85% but obviously you are more interested in the average score at particular schools. Now let's say that at school B only 5 students took the test and their average score was 50%. What are we to make of this? A purely frequentist approach would to take the data at it's word and treat 50% as the best estimate available of the average score at that school, with some fairly large confidence intervals of course. But that seems problematic. We know that overall the average test score across all students is an 85%. Given that we only have data from five students at this school, doesn't it seem MORE likely that the true value is actually somewhere closer to the 85% than 50%? The Bayesian approach to this problem would be to treat the overall mean as a "prior" and then update that prior with the additional data we got from the five students at this school. This is going to "shrink" our final estimate towards the mean by some amount. Since we only have five students at this school it's going to shrink it by quite a bit, since the data are weak. At a school with 150 respondents we would put more trust in the data and only shrink a little bit. This approach is called "empirical Bayes estimation" and it's widely used in multilevel modeling, even by people who don't think of themselves as Bayesian, and more explicitly Bayesian versions of this approach (Google "Multilevel regression with post-stratification") are very common in political science when trying to get estimates of public opinion in small states This is in fact what Fivethirtyeight.com does to predict elections in the US. To estimate the chance that a given candidate (say Trump) will win a state they look at polling at that state, but then they "shrink" the result of that polling towards a prior that they got from other data. For example, let's say that before the 2020 campaign even starts you decide that, based on demographic trends, partisan affiliation and presidential approval numbers, Trump is only likely to get 30% of the vote in Vermont (this is our prior). Then someone does a poll of 100 people and finds that Trump is actually winning in Vermont with 51% of the vote. A frequentist would have to either put total trust in this result or ignore it completely. A Bayesian can do something more subtle: we use Bayes' rule to shrink this estimate towards our prior by some amount. In other words we don't actually believe that this poll means that Trump is actually ahead in Vermont, but we also no longer totally believe our old prior that Trump was only going to get 30%. Maybe now we think that we will win 40% of the vote. Then when we get even more new data, we update the new prior again. Philosophical disagreements aside, this approach really works, which is why Fivethirthyeights's forecasts are so accurate: https://projects.fivethirtyeight.com/checking-our-work/
