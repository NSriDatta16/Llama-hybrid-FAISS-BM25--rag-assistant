[site]: crossvalidated
[post_id]: 193316
[parent_id]: 193315
[tags]: 
The notion of complete data is very weak, in that it simply means you observe the original model, rather than incomplete data. There is no conditional independence assumption behind this notion. In that sense, the quote from Section 11.3 is clearly confusing. In equation (10.27) of Murphy'sMachine Learning , the sampling model is such that it factorises as $$\prod_t p(\mathcal{D}_t|\theta_t)$$ And Murphy rightly states that when the prior also factorises as $$\prod_t p(\theta_t)$$ the posterior reproduces this factorisation $$p(\theta|\mathcal{D})\propto \prod_t p(\mathcal{D}_t|\theta_t)p(\theta_t)$$ i.e. the $\theta_t$'s are independent a posteriori. In your graphical model and Gaussian example, there is no such separation in the sampling model: $\mu_1$ and $\mu_2$ are independent a priori (if we ignore the issue with the infinite mass prior) but not a posteriori.
