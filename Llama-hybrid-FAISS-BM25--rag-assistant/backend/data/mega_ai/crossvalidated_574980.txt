[site]: crossvalidated
[post_id]: 574980
[parent_id]: 574734
[tags]: 
Let me split the answer into two parts. Moments in the likelihood considered as regular data First, I'll make it more general, and instead of considering $$ p(\theta|\hat{m_1},\hat{m_2})\propto p(\hat{m_1},\hat{m_2}|\theta)\,p(\theta) $$ I'll consider the general case $$ p(\theta|X)\propto p(X|\theta) \,p(\theta) $$ where $X$ can be any data. If you can define the likelihood function in terms of the moments $\hat{m_1},\hat{m_2}$ as the data (same as data about $n$ i.i.d. Bernoulli trials can be simplified to considering it as a binomial random variable for the total number of successes), then it is the same. In many cases, however, it might be hard to come up with such a likelihood function. use the actual distribution of $p(\hat{m_1},\hat{m_2})$ in the above formula This question is confusing. Are you talking about the likelihood $p(\hat{m_1},\hat{m_2}|\theta)$ or the unconditional distribution $p(\hat{m_1},\hat{m_2})$ ? If it is the latter, you cannot put unconditional distribution in the place of the conditional distribution, they are different things. It is hard to discuss it as a general case because it is not clear what you mean here and how exactly it refers to the initial problem. Say that you are interested only in the mode of the posterior (maximum a posterior estimation), so you are just maximizing the posterior probability, $p(\hat{m_1},\hat{m_2})$ cannot be maximized with regard of $\theta$ because it is not a function of $\theta$ . If you meant conditional distribution then lucky you! Usually, when deciding on a statistical model, you need to somehow come up with what the likelihood function should be for the problem. If you know it, you don't have to make this step. use the Asymptotic distribution of $p(\hat{m_1},\hat{m_2})$ in the above formula Same as above. Unusually we don't "know" the likelihood function, but make some assumptions. How about the following when I have a frequentist estimate of $\theta$ and its distribution? First, in the frequentist approach, you would not have the estimate for the distribution of the parameter, because the parameter is not considered a random variable. You would have the point estimate. But answering the question, deciding on priors based on in-data statistics is called empirical Bayesian approach, it is controversial , but sometimes used. Can we use moments in the likelihood function? In some cases, it might be hard to come up with a likelihood function, but instead, the distribution can be characterized in other terms. In such cases, we can use likelihood-free inference and things like approximate Bayesian computation (ABC) (see also other threads for approximate-bayesian-computation ). Your problem with using only the moments rather than the data might fall into this category. In ABC, the likelihood is replaced with a distance function that serves as a proxy for it. Finally, "what is Bayesian inference?" is a variant of the Who Are The Bayesians? question that already got some nice answers. As you can learn from the linked thread, Bayesian statistics is not about using the Bayes theorem, but is more general than this. That is how ABC falls into this category, though it does not exactly use the Bayes theorem, just takes from the general idea behind it.
