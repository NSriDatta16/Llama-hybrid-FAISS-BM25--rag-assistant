[site]: crossvalidated
[post_id]: 642584
[parent_id]: 
[tags]: 
Block length in time series bootstrap of AR(1) model, biased AR coefficient

I'm using block bootstrapping for some simple autoregressive time series models, and I'm running into pretty high bias in the bootstrapped estimates of the autoregressive coefficients, even from large block sizes. This is somewhat surprising to me because I am using 1) time series with short dependence structures (e.g. lag-1), and 2) the typically recommended block lengths are between 3 and 5 . Block bootstrapping is very commonly recommended, but I'm finding that it's apparently failing in this application. Define our sample of time series data $\mathbf{y} = (y_{1}, y_{2}, ..., y_{t})$ for $t$ in $(1, ..., N)$ , which is generated from the following AR(1) model: \begin{align} y_t &\sim \mathrm{Normal}(\eta_t, 1)\\ \eta_{1} &= \mu\\ \eta_{2:N} &= \mu + \phi y_{1:(N - 1)}\\ \end{align} We are interested in estimating the parameters $\theta = \{\mu, \phi\}$ via maximum likelihood estimation provided estimates $\hat{\theta}$ , and the confidence intervals/standard errors via block bootstrapping. In each bootstrap replicate $r$ in $\mathbf{r} = (r_{1}, ..., r_{i}, ..., r_{M})$ , we split the data into $l$ overlapping blocks and resample those blocks $N - l + 1$ times, estimate the parameters $\theta^{*}_i = \{\mu^{*}_{i}, \phi^{*}_{i}\}$ for replicate $i$ . This results in a bootstrapped distribution of parameters $\theta^{*} = \{\mu^{*}, \phi^{*}\}$ , where we can infer the estimator bias $E(\theta - \hat{\theta})$ by $E(\hat{\theta} - \theta^{*})$ . Here's some R code to simulate and or fit the previous model: # Log likelihood objective function # Assumes normal likelihood function # @param y: the data vector # @param eta: the mean vector # @param sigma: the standard deviation ll We then use the same function to fit the model via MLE: pars The estimated parameters are not amazingly estimated, but close enough for now. Now we use tseries::tsbootstrap to do a block bootstrap. First, I use blocklength::hjj to estimate the most likely block length: blocklength::hhj(y, nb=100, sub_sample = 10, k = "bias/variance") # Pilot block length is: 3 # Performing minimization may take some time # Calculating MSE for each level in subsample: 10 function evaluations required. # Chosen block length: 4 After iteration: 1 # Chosen block length: 2 After iteration: 2 # Converged at block length (l): 2 # $`Optimal Block Length` [1] 2 ... which suggests that the optimal block length is 2. This is what I'd expect from a lag-1 model. Now we fit the bootstrap with a block length of 2: # Pull the AR(1) parameter values, theta_star = {mu_star, phi_star} ar1.stat $par[1], res$ par[2])) } print(boot The biases are quite high, and while I can reduce the bias for $\mu$ by using a larger original sample of 100, I can't do anything about the bias in $\phi$ . I need to use a block length of $l = \approx N / 2$ to get anywhere close. I'm a bit confused about what to do in this circumstance. Can we trust the standard errors here and just center the confidence interval around the original MLE estimate for $\phi$ ?
