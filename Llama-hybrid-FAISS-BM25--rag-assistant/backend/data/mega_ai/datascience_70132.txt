[site]: datascience
[post_id]: 70132
[parent_id]: 
[tags]: 
Apply Custom Loss in Keras by getting batch_size

I have a python package/function which takes two numpy arrays and returns a value. I want to use this value as my loss in keras. How do I infer batch size to compute average loss (while using egar execution). Using the below implementation gives me error __index__ returned non-int (type NoneType) import myloss def customLoss(yt,yp): loss=0 for i in range(yp.shape[0]): #get batch size; yp is of shape (batch,200,1) loss = loss + myloss(yt[i,:],yp[i,:]) return loss/yp.shape[0] model.compile(optimizer=tf.keras.optimizers.Adam(), loss=customLoss)
