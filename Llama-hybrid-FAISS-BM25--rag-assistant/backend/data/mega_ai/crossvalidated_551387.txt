[site]: crossvalidated
[post_id]: 551387
[parent_id]: 551364
[tags]: 
Consider a population $Y|X$ that follows some distribution according to a true model, and you have a set of trained models $f(X,\theta)$ that make predictions of $Y$ given $X$ and are parameterized by $\theta$ . The goal is to find out what the error of the models is, in making predictions about samples from the population, as function of the parameter $\theta$ , and to select the model with the lowest error. To achieve this goal we can sample the population (the validation data set) and observe the performance/error of the models for the sample as function of $\theta$ and use that to estimate the performance/error for the entire population. Now, our observations based on a sample will not be perfect, but the found empirical distribution of the performance/error (or derived quantities, e.g the average performance/error) will be close to the real value (provided a sufficiently large sample). Or at least, according to the Glivenko-Cantelli theorem the empirical distribution can be made as close to the real distribution as we want by increasing the size of the sample size (the validation data set). Since the convergence of the empirical distribution towards the true distribution of the performance/error is uniform, any derived quantity (e.g. the mean performance/error) will also convergence towards the true value (in the case of the mean one could also use the law of large numbers). So the 'theoretical guarantee' is the law of large numbers, or more general the Glivenko-Cantelli theorem. Note 1: On a side note, I have heard that the "attractive theoretical promises" made by the Central Limit Theorem and the Bootstrap Method don't tend to be as "attractive" in reality... That's why statistics is not simply mathematics. Indeed this theoretical guarantee is only a guarantee that the estimates are consistent . It means that the estimates convergence to the true value, but the practical use might be low if the rate of convergence is slow, or if the initial variance is large.
