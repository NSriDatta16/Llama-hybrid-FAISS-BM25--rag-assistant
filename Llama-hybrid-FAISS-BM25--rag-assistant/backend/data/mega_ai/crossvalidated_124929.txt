[site]: crossvalidated
[post_id]: 124929
[parent_id]: 101431
[tags]: 
They are different because the set of linear combinations of $N$ regression trees of size $S$ does not include all regression trees of size $ND$. This is easy to see when $S=1$, i.e. decision stumps. In that case, the regression function $f(x)$ obtained by boosting is additive $$f(x) = \sum_{i=1}^p f_i(x_i).$$ If instead one were to grow a regression tree of size $N$, there would potentially be branches including splits of more than one variable. These correspond to interactions and thus lead to a function that does not admit an additive decomposition. This is an important feature of boosted stumps. One can often obtain better performance by boosting with a carefully chosen combination of $N$ and $S$ than by fitting a single regression tree. Generally, the base learners are chosen with $S$ not very large, e.g. not more than five. On the other hand, random forests are a popular technique that typically uses much larger trees. In that case, bagging is used to reduce the variance associated with these larger trees.
