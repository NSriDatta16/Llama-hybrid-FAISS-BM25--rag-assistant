[site]: crossvalidated
[post_id]: 457882
[parent_id]: 
[tags]: 
Using the log values vs linear values change the importance of features in Machine Learning (Random Forest)

I am using a certain feature (flux) for which the values range from 1e-14 to 1e-11 . I am trying to use this feature along with many other features to predict two classes, A and B. The flux feature does discriminate between two features when I see the 2D plot, however, it shows 0 as the feature importance when I run the Random Forest Classifier. This can happen if the other features are more dominant. If this flux is changed to its log value before training the data, the feature importance jumps to a high level. What could be the reason for this issue? Do I need to normalize/scale all my data before I train my classifier? I thought that the scaling is not needed in decision tree based algorithms . I would really appreciate for any help in this issue. Please note that the other features have values between 0 and 100.
