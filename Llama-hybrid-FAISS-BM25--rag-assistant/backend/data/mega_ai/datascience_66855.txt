[site]: datascience
[post_id]: 66855
[parent_id]: 23159
[tags]: 
Apart from the great answers mentioned here, one should also think about shift-invariance of softmax (or for that matter any exponential functions). Consider logits output from a classifier network (3 classes) [a, b, c] . Then the probability distribution will remain invariant even if it had been [a+x, b+x, c+x] . For example, if we consider e^e^x - e for normalization (as was mentioned in one of the comments). We miss out on this nice property. Now you may ask why is shift-invariance desired versus say scale invariance of logits. I find shift-invariance as an intuitively better design choice since and do not know a more theoretically backed reasoning or if it exists, Secondly, as for using any other exponents on softmax other than e , it is being used. Look into learning classification with temperature and is a common technique in machine learning. So yes the softmax outputs may not correspond to probabilities and temperature scaling is used to calibrate these probabilities (where temperature may be learnt or treated as a hyperparameter)
