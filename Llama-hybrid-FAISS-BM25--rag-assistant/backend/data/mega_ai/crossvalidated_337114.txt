[site]: crossvalidated
[post_id]: 337114
[parent_id]: 
[tags]: 
Does Maximum a posterior(MAP) estimate of weights in Linear Regression avoid over fitting?

I am new to machine learning. In an online course that I am taking the instructor claims the following: in linear regression, MLE framework gives us the squared loss cost function which overfits. In order to overcome that we use MAP estimate of the weights. I would like to get a theoretical reason behind why MLE overfits(if it indeed does so) and why using MAP we are able to overcome that(if we can indeed do so).
