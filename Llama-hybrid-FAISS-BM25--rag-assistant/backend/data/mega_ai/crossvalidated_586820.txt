[site]: crossvalidated
[post_id]: 586820
[parent_id]: 
[tags]: 
How can I check if the curse of dimensionality negatively affects my clustering results

I would like to cluster 80 days using k-means. Each of the 80 days contains 4 time series (temperature, solar radiation, electricity demand, electricity price) with 288 values each. So all in all I have $4*288=1152$ values for each day and I would like to cluster the days such that similar days with regard to these 4 time series combined will be grouped into the same cluster. For this purpose I use the following code, that I was advised to use (at least for the scaling part) from this answer https://stackoverflow.com/questions/73491673/strange-results-when-scaling-data-using-scikit-learn : import numpy as np import pandas as pd from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans data_Unscaled = pd.read_csv("C:/Users/User1/Desktop/data_Unscaled.csv", sep=";", header=None) X = data_Unscaled.to_numpy() X_narrow = np.array([X[:, i*288:(i+1)*288].ravel() for i in range(4)]).T scaler = StandardScaler() X_narrow_scaled = scaler.fit_transform(X_narrow) X_scaled = np.array([X_narrow_scaled[i*288:(i+1)*288, :].T.ravel() for i in range(80)]) kmeans = KMeans(init="random", n_clusters=3, n_init=10, max_iter=300, random_state=42) kmeans.fit(X_scaled) print(f"resultClustering Example Day 1: {kmeans.predict((X[20].reshape(1, -1)))}") print(f"resultClustering Example Day 2: {kmeans.predict((X[30].reshape(1, -1)))}") print(f"resultClustering Example Day 3: {kmeans.predict((X[40].reshape(1, -1)))}") print(f"resultClustering Example Day 4: {kmeans.predict((X[23].reshape(1, -1)))}") print(f"resultClustering Example Day 5: {kmeans.predict((X[10].reshape(1, -1)))}") In the linked answer it was assumed that my results would suffer from the curse of dimensionality as I have just 80 points in a 1152-dimensional space. Can anyone tell me how can I check if the curse of dimensionality negatively affects the results? I made some test predictions and the are not all grouped into the same cluster which should be correct. Here is the unscaled input data with shape (80,1152): https://filetransfer.io/data-package/CfbGV9Uk#link
