[site]: crossvalidated
[post_id]: 320545
[parent_id]: 319689
[tags]: 
Suppose there's an embedding matrix $M$, of size $V \times d$, where $V$ is the vocabulary size and $d$ is the embedding size. Suppose further an input sentence $(w_1, ..., w_k)$ that is encoded by indices $(i_1, ..., i_k)$, where $i_j \leq V$. In theory, the following two approaches are equivalent : Select the rows from the embedding matrix $M$ that correspond to indices $(i_1, ..., i_k)$. Convert $(i_1, ..., i_k)$ to one-hot encoded matrix $O$, of size $k \times V$, and compute the dot-product $O \cdot M$, like on the picture below. In both cases, the result is a matrix of embeddings for all words in the sentence: $k \times d$. But in programming, the first approach is much more efficient than the second one. Both in terms of computational complexity (select is a cheaper operation than dot-product) and memory (it uses O(k) memory for the input instead of O(k*V) ). Especially when $V$ is large, tens or even hundreds of thousands. Remember that optimization is usually done in batches, not by one sentence at a time. That's why there's tf.nn.embedding_lookup function in tensorflow that accepts sparse representation (the tensor of indices, not one-hot vectors), and no wonder the tutorial that you refer to uses it. Though word2vec paper talks about one-hot vectors, I believe that in code they are using indices as well, because Google's 1T vocabulary size is 13M! So, in general, it's better to avoid one-hot representation when the number of classes is large, like natural language vocabulary.
