[site]: crossvalidated
[post_id]: 325689
[parent_id]: 
[tags]: 
Why and what induces degeneracy in the solution space in logistic regression models with respect to the energy landscape of the logistic loss?

I was trying to understand the sources of having many (equivalent) solutions $w$ with respect to the training set in the context of logistic regression (with a predictor of the form $h_{w}(x) = \text{sigmoid}(y w^\top x) $ ). The conditions for degeneracy in the energy landscape is what interests me most and what conditions does the problem need to have (data, model complexity, the way true distribution looks like, etc) for there to be more than 1 unique minimizer (or more than 1 unique limit point minimizer). Thus, what matters is the set of parameters that achieves minimum loss (not zero classification error). There might be many and the conditions for this is what I am interested in. Recall that the loss function (for the 2 classes cases, otherwise use softmax with cross-entropy loss) is: $$ J(w; X,Y) = \frac{1}{N} \sum^N_{n=1} \ln( 1 + e^{y^{(n)} w^\top x^{(n)}} )$$ in this problem I am wondering what induces having many solutions (or not). My hypothesis are as follow: The number of data points matter. Only when we have $N_{train} = \infty$ can we have a unique solution come out from logistic regression. This is obvious because then we essentially know the true hyperplane. If we lack data my intuition tells me we need some other sort regularizer to choose a unique solution. The funny thing is that because of the log the error keeps going down forever. So perhaps a better way to say "unique" solution is to say unique in the limit. Even Bishop's book mentions that even if $N > D$ over constrained, its possible that there are infinite different solution (but the problem remains convex!?) When the model is "too complex" then there are an infinite set of solutions. For example, if the data lies in $\mathbb{R}^{D^*}$ but the model we have has more features say lies in $\mathbb{R^D}$ where $D > D^*$, then things are problematic. This is because the new features can be changed to sum to zero without actually changing anything. Since logistic regression, we only care about the sign. Thus we can always scale our solution $\hat w$ arbitrarily without changing the decision boundary. This seems to always be true no matter what...so maybe there are always infinite number of solutions in logistic regression? Is there very a unique minimizer in logistic regression? The fact that the loss is always decreasing makes me think that no...(unless a limit exists?) Assume separability. Even in this case, there could be an infinite number o f solution unless we satisfy my first point $N_{train} = \infty$. So my question is: when (and the whys) of when logistic regression has an infinite set of solution (and what conditions do we need)? The reason the questions linked are not helpful (or wikipedia, which as a matter of fact does have a list where a model may not reach convergence which probably is related to my question) is because I don't just want to know what induces degeneracy, but also why . For example, if a solution is possible to be completely separable, then how does that induce a degenerate sol? For example, in linear regression its clear that its induced due to a null space, so any solution can be expressed as $$ \hat x = x_{particular} + \alpha x_{nullspace}$$ which makes it easy to understand where the flatness of the landscape comes from. But for logistic regression, this isn't clear. How does say, rank directly affect the solution of the logistic regression model? As I was reading Bishop section 4.3.2 I found the following statement: Note that the problem will arise even if the number of data points is large compared with the number of parameters in the model, so long as the training data is linearly separable. This means that the number of data points doesn't play an important role. A good answer should explain this too.
