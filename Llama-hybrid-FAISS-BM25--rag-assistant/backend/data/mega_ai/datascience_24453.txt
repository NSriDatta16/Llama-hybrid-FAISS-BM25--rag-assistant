[site]: datascience
[post_id]: 24453
[parent_id]: 24452
[tags]: 
Correlated features in general don't improve models (although it depends on the specifics of the problem like the number of variables and the degree of correlation), but they affect specific models in different ways and to varying extents: For linear models (e.g., linear regression or logistic regression), multicolinearity can yield solutions that are wildly varying and possibly numerically unstable . Random forests can be good at detecting interactions between different features, but highly correlated features can mask these interactions. More generally, this can be viewed as a special case of Occam's razor . A simpler model is preferable, and, in some sense, a model with fewer features is simpler. The concept of minimum description length makes this more precise.
