[site]: datascience
[post_id]: 45639
[parent_id]: 
[tags]: 
Sequence to sequence RNN model, maximum number of training size

So when running this example script from Keras repo ( https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py ), I found that we can easily run into out of memory for the input or output one-hot encoding in this code: encoder_input_data = np.zeros( (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32') decoder_input_data = np.zeros( (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32') decoder_target_data = np.zeros( (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32') When I have a huge size of training samples, this one-hot does not fit into memory. Is there way to handle this issue? UPDATE: I changed float32 to uint, but thats about the smallest one hot array can get.
