[site]: crossvalidated
[post_id]: 351524
[parent_id]: 
[tags]: 
What is the geometric explanation for high variance = overfitting = bad generalization?

I am reading about the L2 regularization . According to Python Machine Learning - Second Edition , "by increasing the regularization strength via the regularization parameter Î» , we shrink the weights towards zero and decrease the dependence of our model on the training data." A nice illustration for L2 regularization is given in the book: I dont understand why the weights need to be penalised? Without the regularization term, wouldn't we be able to get to the center (optimal), which is the exact purpose of optimization of the loss function? With the regularization term, now we can not get to the lowest cost. Why large weights means high variance? Why large weights means the model is overfitted, or not well generalized? Can this be explained with the above figure?
