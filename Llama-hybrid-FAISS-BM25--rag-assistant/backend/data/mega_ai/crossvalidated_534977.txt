[site]: crossvalidated
[post_id]: 534977
[parent_id]: 534697
[tags]: 
Intuitively, this means that the score function is highly sensitive to the sampling of the data. i.e - we are likely to get a non-zero gradient of the likelihood, had we sampled a different data distribution. This seems to have a negative implication to me. Don't we want the score function = 0 to be highly robust to different sampling of the data? That is not the correct intuition for the score function. Remember that the score function is a derivative with respect to the parameter , not the data. The Fisher information is defined as the variance of the score, but under simple regularity conditions it is also the negative of the expected value of the second derivative of the log-likelihood. So, if we write the log-likelihood as $\ell(\theta | \mathbf{X})$ and the score function as $s(\theta | \mathbf{X})$ (i.e., with explicit conditioning on data $\mathbf{X}$ ) then the Fisher information is: $$\mathcal{I}(\theta) = -\mathbb{E} \Bigg( \frac{\partial^2 \ell}{\partial \theta^2} (\theta | \mathbf{X}) \Bigg) = -\mathbb{E} \Bigg( \frac{\partial s}{\partial \theta} (\theta | \mathbf{X}) \Bigg).$$ The thing to note here is that the derivatives are taken with respect to the parameter, not the data. So, we can see that a high (magnitude) value for the Fisher information means that the score function is, on average, highly sensitive to the parameter value , not the data. If the score function is highly sensitive to the parameter value, this means that the root of the equation (which is the MLE) is relatively insensitive to the parameter, and so the MLE has lower variance.
