[site]: datascience
[post_id]: 31630
[parent_id]: 31597
[tags]: 
Nearest neighbour algorithms (kNN and variants) do not have a training phase. They work by storing all the labelled examples, and using them directly for inference on new data. There are some caveats: "Training" is fast as it just stores each example once, but inference is slow, as typically it involves searching for relevant examples. This can be improved by indexing routines (which are not part of the algorithm, but might be part of a specific implementation). Although there are no parameters to train, there may still by hyper-parameters to select, which will involve running a cross-validation exercise with hold-out data and varying the choices, e.g. of number of nearest neighbours to consider, or best distance metric to use to determine closest neighbours. Data preparation (normalisation) and feature engineering are often required to get the best results out of kNN, and also require multiple attempts and tests. In general, approaches that store and query labelled data without a training phase are instance based learning
