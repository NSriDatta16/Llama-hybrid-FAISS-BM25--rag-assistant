[site]: crossvalidated
[post_id]: 256572
[parent_id]: 246089
[tags]: 
As this Wiki article makes clear ( https://en.wikipedia.org/wiki/Feature_engineering ), feature engineering is a key step in machine learning, involving the generation and cultivation of a set of features or attributes that may prove empirically (not necessarily theoretically) useful in the prediction or classification of a target. Andrew Ng (and others) make much of expert, domain knowledge in the development of a set of features but given the multitude of transformations that can be applied to data to improve model fit, the massive numbers of features that are commonly analyzed and the "black-box" nature of many of the algorithms employed, domain knowledge hardly seems a priority. For me, it's always useful to point out that inference vs prediction and classification can be viewed as separate domains, the former belonging to statistics and the latter the focus of machine learning. Obviously, there is much overlap in this terminology and these fields, i.e., they are by no means mutually exclusive. Broadly speaking, statistical inference involves expert, domain knowledge, careful specification of an hypothesis, a finite (small) set of attributes or features, coupled with an experimental design to test the hypothesis out -- classic scientific inquiry with the goal of driving insight and understanding relative to ground truth. ML prediction and classification, on the other hand, may or may not be hypothesis driven, may or may not have descriptive insight as a goal and may or may not have ground truth as a benchmark.
