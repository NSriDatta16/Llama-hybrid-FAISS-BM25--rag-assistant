[site]: datascience
[post_id]: 47880
[parent_id]: 47870
[tags]: 
error is defined just as (outputs - target) This is the correct gradient for cross-entropy loss function with Sigmoid as the last layer. For squared (quadratic) loss $$(y-f(x))^2,$$ the gradient is, as you said, $$(y-f(x))f'(x)$$ (constant $2$ is removed), but for binary cross-entropy loss $$y\text{log}f(x) + (1-y)\text{log}(1-f(x)),$$ the gradient is $$yf'(x)/f(x) - (1-y)f'(x)/(1-f(x)),$$ since for Sigmoid we have $f'(x)=f(x)(1-f(x))$ , by substitution the gradient becomes $$y(1-f(x)) - (1-y)f(x)=y-f(x)$$ To distinguish between these two gradients, author sets cnn.CalcLastLayerActDerivative = 0 to be checked later in an if statement in bpcnn.m file as follows (comments don't exist in the original code): ... else % error = (f(x) - y) er = ( cnn.layers{cnn.no_of_layers}.outputs - yy); ... if cnn.CalcLastLayerActDerivative ==1 % change the error from (f(x) - y) to f'(x)(f(x) - y) er =applyactfunccnn(cnn.layers{cnn.no_of_layers}.outputs,cnn.layers{cnn.no_of_layers}.act_func, 1, er); end which means gradient is $(y-f(x))f'(x)$ for quad and $(y-f(x))$ for cros (bad variable name!). As a side note, author only allows Sigmoid for cross entropy which means only binary classifier is supported (multi-class classifier requires SoftMax). error('cross entropy is implemented only when last layer is sigmoid'); EDIT Thanks to @Edison for pointing out that error and gradient were not handled the same as loss values in the code, which substantially changed the final answer.
