[site]: crossvalidated
[post_id]: 218890
[parent_id]: 
[tags]: 
Reverse-Engineer Time Series Matrix With Machine Learning

I'm trying to figure out the following situation which is almost the same as in this post here Time series with multiple subjects and multiple variables Subject Week x1 x2 x3 x4 x5 y1 A 1 .5 .6 .7 .8 .7 10 B 1 .3 .6 .2 .1 .3 8 C 1 .3 .1 .2 .3 .2 6 A 2 .1 .9 1.5 .8 .7 5 B 2 .3 .6 .3 .1 .3 2 D 2 .3 .1 .4 .3 .5 10 I have a time-series matrix with different output variables (~ 12000) which (often but not always) change every day due to an algorithm I'm not aware of. Each of them has multiple input values I'm trying to regress their values from. Now here is the reason why I'm posting this again: I know that there is an underlying pattern since this data was generated by an algorithm. So there must be an exact pattern in order to predict the output variables with the data I have. Which is why i'd like to avoid estimating too much with dlm but rather reverse-engineer it with a machine-learning algorithm if that is possible. But my question becomes: How do I tell a model, that certain rows are related? I believe this information is critical and the model should know that the columns are not thought of as A,B,C,D,A,B,C,D but AA,BB,CC,DD when trying to figure out how to predict the output variable. How can I tell an algorithm about it and what kind of algorithm should I try? - Optimally I want to extract the underlying rules from the model in order to completely understand it. Would be happy about any kind of help or hint! Best, Heinrich
