[site]: crossvalidated
[post_id]: 497694
[parent_id]: 497631
[tags]: 
Since there is no mention of costs associated with decisions, I assume that the Bayesian decision rule is minimizing the average probability of error. When Hypothesis $H_0$ is true, the observation $(x_1, x_2)$ is uniformly distributed on a square of side $2$ (sides parallel to the axes) and opposite vertices $(0,0)$ and $(2,2)$ . When Hypothesis $H_1$ is true, the observation $(x_1, x_2)$ is uniformly distributed on a rectangle of area $2$ (sides parallel to the axes) and opposite vertices $(1,1)$ and $(2,3)$ . Thus, the likeliihood ratio $\displaystyle \Lambda(x_1,x_2) = \frac{f_1(x_1,x_2)}{f_0(x_1,x_2)}$ has value $\displaystyle\frac{0}{0.25} = 0$ when $(x_1,x_2)$ lies in an L-shaped region of area $3$ , value $\displaystyle\frac{0.5}{0.25} = 2$ when $(x_1,x_2)$ lies in the square of area $1$ with opposite vertices $(1,1)$ and $(2,2)$ , and value $\displaystyle\frac{0.5}{0} = \infty$ $(x_1,x_2)$ lies in the square of area $1$ with opposite vertices $(1,2)$ and $(2,2)$ . Now, the Bayesian decision rule compares $\Lambda(x_1,x_2)$ to the threshold $\displaystyle \frac{\pi_0}{\pi_1}$ where $\pi_0$ and $\pi_1 = 1-\pi_0$ are the prior probabilities of $H_0$ and $H_1$ , and so it is easy to figure out what the Bayesian decision rule is, and what the corresponding decision boundary is. Let $\Gamma_0$ and $\Gamma_1$ denote the disjoint regions such that when $(x_1,x_2) \in \Gamma_i$ , the decision is that $\Gamma_i$ is the true hypothesis. If $\displaystyle\pi_0 so that $\displaystyle\frac{\pi_0}{\pi_1} , $\Lambda(x_1,x_2) > \displaystyle\frac{\pi_0}{\pi_1}$ whenever $(x_1,x_2)$ lies in the rectangular region of area $2$ (sides parallel to the axes) and opposite vertices $(1,1)$ and $(2,3)$ . Thus, $\Gamma_1$ is this rectangular region, and we have \begin{align}\Gamma_1 &= \{(x_1,x_2)\colon x_1 \geq 1, x_2 \geq 1\}\\ \Gamma_0 &= \{(x_1,x_2)\colon x_1 If $\displaystyle\pi_0 > \frac 23$ so that $\displaystyle\frac{\pi_0}{\pi_1} > 2$ , $\Lambda(x_1,x_2) > \displaystyle\frac{\pi_0}{\pi_1}$ whenever $(x_1,x_2)$ lies in the square region of area $1$ (sides parallel to the axes) and opposite vertices $(1,2)$ and $(2,3)$ . Thus, $\Gamma_1$ is this square region, and we have \begin{align}\Gamma_1 &= \{(x_1,x_2)\colon x_1 \geq 1, x_2 \geq 2\}\\ \Gamma_0 &= \{(x_1,x_2)\colon x_1 If $\pi_0 = \frac 23$ exactly so that $\Lambda(x_1,x_2)$ has value $2$ exactly equal to the threshold for all $(x_1,x_2)$ in the square region of area $1$ with opposite vertices $(1,1)$ and $(2,2)$ . In this instance, the average error probability is the same regardless of which points in this region we assign to $\Gamma_0$ and which we assign to $\Gamma_1$ !! So, we arbitrarily choose to assign them all to $\Gamma_0$ resulting in \begin{align}\Gamma_1 &= \{(x_1,x_2)\colon x_1 \geq 1, x_2 \geq 2\}\\ \Gamma_0 &= \{(x_1,x_2)\colon x_1 If the OP desires, he can massage the above information into a single humongous formula that upon plugging in the values of $\pi_0, \pi_1$ gives the exact decision boundaries directly and thereby amaze his friends and TA.
