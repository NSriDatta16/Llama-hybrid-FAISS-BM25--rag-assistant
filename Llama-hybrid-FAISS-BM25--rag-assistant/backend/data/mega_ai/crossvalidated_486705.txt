[site]: crossvalidated
[post_id]: 486705
[parent_id]: 485964
[tags]: 
Let me give a slightly more ML-focused perspective on the accepted answer. Don't conflate training loss and decision loss â€“ they're separate concepts even though the functions can be the same. It's easier to see this distinction in classification than regression. So, let's say we're doing binary classification using logistic regression. The training loss is cross entropy / log loss (maybe with regularization). After the model is trained, we don't care about the training loss anymore. At prediction time, our logistic regression model tells us $P(y|x)$ . We need to translate this distribution into a single class. Do we just pick the class with the highest probability? Do we want to be extra careful about false positives? We formally encode these preferences into a decision loss , which allows us to optimally choose a single class from $P(y|x)$ . For a more academic exposition, I found "Pattern Recognition and Machine Learning" to have a great disambiguation of these two. Determination of $p(x, t)$ from a set of training data is an example of inference and is typically a very difficult problem whose solution forms the subject of much of this book. In a practical application, however, we must often make a specific prediction for the value of $t$ , and this aspect is the subject of decision theory.... We shall see that the decision stage is generally very simple, even trivial, once we have solved the inference problem. It is worth distinguishing between the squared loss function arising from decision theory and the sum-of-squares error function that arose in the maximum likelihood estimation of model parameters. We might use more sophisticated techniques than least squares, for example regularization or a fully Bayesian approach, to determine the conditional distribution $p(t|x)$ . These can all be combined with the squared loss function for the purpose of making predictions.
