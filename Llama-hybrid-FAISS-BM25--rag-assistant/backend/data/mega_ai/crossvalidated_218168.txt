[site]: crossvalidated
[post_id]: 218168
[parent_id]: 218156
[tags]: 
I probably wouldn't call these misconceptions, but maybe common points of confusion/hang-ups and, in some cases, issues that researchers may not be aware of. Multicollinearity (including the case of more variables than data points) Heteroskedasticity Whether values of the independent variables are subject to noise How scaling (or not scaling) affects interpretation of the coefficients How to treat data from multiple subjects How to deal with serial correlations (e.g. time series) On the misconception side of things: What linearity means (e.g. $y = ax^2 + bx + c$ is nonlinear w.r.t. $x$, but linear w.r.t. the weights). That 'regression' means ordinary least squares or linear regression That low/high weights necessarily imply weak/strong relationships with the dependent variable That dependence between the dependent and independent variables can necessarily be reduced to pairwise dependencies. That high goodness-of fit on the training set implies a good model (i.e. neglecting overfitting)
