[site]: datascience
[post_id]: 15149
[parent_id]: 
[tags]: 
Output a word instead of a vector after word embedding?

I'm trying to play around with a toy implementation of translation or text-summarization. I understand now that most people use an embedding layer before whatever model they use, which produces something like 300-dimensional vector. But what does the model output? Like for a encoder-decoder model, it's inputs are sequences of those vectors. Then what does the final layer of decoder come out with? Not like event extraction or something like that, which we classify to a small number of classes. So my core question is: Is the output also a 300-dimensional vector, which then I have to generate the word based on the most similar word vector? Or is it the index of a word from the original word space? In the second case, the network is classifying to more than 100000 classes? And also, is there any existing package that support this "reverse embedding to word"? I have not seen any question alike on the site. Please mark it as duplicate if it is. Any help would be appreciated.
