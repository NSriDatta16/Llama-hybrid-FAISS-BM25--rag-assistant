[site]: crossvalidated
[post_id]: 93836
[parent_id]: 
[tags]: 
Gradient decay in neural networks

I read that in traditional feed-forward neural nets the gradients in the early layers decay very quickly and that this is 'a bad thing'. But I don't understand why. Can someone please explain what does that mean? or at least direct me to places to read about it and understand.
