[site]: crossvalidated
[post_id]: 337926
[parent_id]: 
[tags]: 
Cost function converging always to zero?

When evaluating the cost of a neural network during training, should I always expect the cost function to converge towards exactly zero ? Without regularization being applied, the cost converging to 0 would be the desirable behavior, right? (forgetting about overfitting for now) However, if applying regularization (L2 for example), could be that the cost function converges to 0.4, but it still does a great job when evaluated on new data? Or should I always aim for the cost function to converge to zero?
