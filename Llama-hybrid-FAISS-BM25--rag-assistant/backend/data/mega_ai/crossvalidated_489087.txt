[site]: crossvalidated
[post_id]: 489087
[parent_id]: 
[tags]: 
Why KL Divergence instead of Cross-entropy in VAE

I understand how KL divergence provides us with a measure of how one probability distribution is different from a second, reference probability distribution. But why is it particularly used (instead of cross-entropy) in generative networks such as Variational Autoencoders (VAEs)? As much as I understand, minimizing either Cross-Entropy or KL-divergence is equivalent. So, I struggle to understand why KL divergence is the preferred loss function in VAEs. NB: I originally asked this question in Data Science SE and referred to forward it here.
