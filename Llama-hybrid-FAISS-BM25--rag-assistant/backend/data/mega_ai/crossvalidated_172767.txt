[site]: crossvalidated
[post_id]: 172767
[parent_id]: 172719
[tags]: 
A few thoughts. 1) If models A and B perform equally in cross validation there is no reason to think that either one is more or less susceptible to (over/under)fitting. Cross validation provides a very strong estimate for generalization error. 2) If the goal is really classification, and you are not interested in the probabilities of classes given your IVs, then focusing on classification rate is reasonable. If you are interested in the probabilities of classes MSE is not what you want to use for binary classification. The Cross-Entropy function (the Bernoulli Log-Likelihood) is a much better bet. Think about it this way, if your algorithm gives $p=0.5$ and the class is $0$, SE is $0.25$. If your algorithm gives $p=0.9999999999$ and the class is $0$, SE is about $1$. This does not satisfactorily deal with how bad that prediction was. 3) As for the last paragraph, I'm not sure what you mean. Your selection criterion is estimating generalization error . Minimizing it as much as possible is always preferable. Are you concerned about over-fitting to validation set? Because if so, there's no evidence that stopping the minimization earlier or later would prevent this from happening.
