[site]: crossvalidated
[post_id]: 359176
[parent_id]: 
[tags]: 
What is "Entropic Capacity"?

I found this term on the Keras blog website , quoted below Your main focus for fighting overfitting should be the entropic capacity of your model --how much information your model is allowed to store. A model that can store a lot of information has the potential to be more accurate by leveraging more features, but it is also more at risk to start storing irrelevant features. Meanwhile, a model that can only store a few features will have to focus on the most significant features found in the data, and these are more likely to be truly relevant and to generalize better. There are different ways to modulate entropic capacity . The main one is the choice of the number of parameters in your model, i.e. the number of layers and the size of each layer. Another way is the use of weight regularization, such as L1 or L2 regularization, which consists in forcing model weights to taker smaller values. This definition seems to overlap a lot with the Bias-Variance tradeoff idea, where a high-variance model model would have high "entropic capaity". But a quick Google search didn't show me much results.
