[site]: crossvalidated
[post_id]: 13396
[parent_id]: 13389
[tags]: 
I think that calling the Kullback-Leibler divergence "information gain" is non-standard. The first definition is standard. EDIT: However, $H(Y)−H(Y|X)$ can also be called mutual information. Note that I don't think you will find any scientific discipline that really has a standardized, precise, and consistent naming scheme. So you will always have to look at the formulae, because they will generally give you a better idea. Textbooks: see "Good introduction into different kinds of entropy" . Also: Cosma Shalizi: Methods and Techniques of Complex Systems Science: An Overview, chapter 1 (pp. 33--114) in Thomas S. Deisboeck and J. Yasha Kresh (eds.), Complex Systems Science in Biomedicine http://arxiv.org/abs/nlin.AO/0307015 Robert M. Gray: Entropy and Information Theory http://ee.stanford.edu/~gray/it.html David MacKay: Information Theory, Inference, and Learning Algorithms http://www.inference.phy.cam.ac.uk/mackay/itila/book.html also, "What is “entropy and information gain”?"
