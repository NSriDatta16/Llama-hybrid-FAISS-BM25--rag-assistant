[site]: crossvalidated
[post_id]: 613079
[parent_id]: 613032
[tags]: 
Let's say we have a time variable $t$ such as the one below, with ten time points: $$ t = [1,2,3,4,5,6,7,8,9,10] $$ If we estimate this as a continuous variable, the regression will fit all estimated values under a single coefficient. If we include it as a factor, it effectively splits up the predictions, in turn giving you a unique coefficient for each individual time. This will naturally change the significance of the coefficients in regressions. As an example with some simulated data below, I have created a 10-point time predictor and a normally distributed outcome variable $y$ . Then I fit the data with $t$ as numeric data: #### Sim Data #### set.seed(123) t The summary of the regression looks like this: Call: lm(formula = y ~ t) Residuals: Min 1Q Median 3Q Max -0.234666 -0.059717 -0.001804 0.058016 0.210123 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 0.000725 0.019797 0.037 0.970861 t 0.011512 0.003191 3.608 0.000488 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 0.09164 on 98 degrees of freedom Multiple R-squared: 0.1173, Adjusted R-squared: 0.1083 F-statistic: 13.02 on 1 and 98 DF, p-value: 0.0004877 You can see when I have created a minor association with $t$ and $y$ , the coefficient is significant. However, if I fit that same data as a factor: #### Fit Factor Regression #### fit.f The summary now has 10 coefficients, each with their own significance value: Call: lm(formula = y ~ factor(t)) Residuals: Min 1Q Median 3Q Max -0.208248 -0.064316 0.002569 0.065867 0.195329 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 0.01469 0.02946 0.499 0.6193 factor(t)2 -0.01966 0.04166 -0.472 0.6380 factor(t)3 0.02592 0.04166 0.622 0.5353 factor(t)4 0.04688 0.04166 1.125 0.2634 factor(t)5 0.03664 0.04166 0.879 0.3815 factor(t)6 0.08489 0.04166 2.038 0.0445 * factor(t)7 0.09378 0.04166 2.251 0.0268 * factor(t)8 0.04309 0.04166 1.034 0.3037 factor(t)9 0.07547 0.04166 1.812 0.0734 . factor(t)10 0.10652 0.04166 2.557 0.0122 * --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 0.09315 on 90 degrees of freedom Multiple R-squared: 0.1624, Adjusted R-squared: 0.07865 F-statistic: 1.939 on 9 and 90 DF, p-value: 0.05615 Because this is a categorical regression, it is only comparing each time point to the original reference criterion of Time 1. If we plot the data and draw the regression line based off the continuous fit: You can see that this specific regression is predicting an overall single upward trend. However, because the categorical regression is comparing each time point individually to the reference criterion (Time 1), only the time points which are greatly different from it are flagged as significant. You can see for example why Time 10 is significant because it's data points are on average greater than Time 1, whereas Time 3 has generally the same distribution of data as Time 1, thus it is non-significant. On that note, it should be clear what you are doing with the data in this case. If you are just interested in fitting longitudinal data over a limited number of times of special interest, then fitting them as factors would be preferred, because then you could explain which time points had an actual effect on the response. However, if you have several time points and only want to know what the general trend is, it would be better to fit the data as numeric. Keep in mind there is more to read on this topic, but that would be my advice. One last thing...you may have noticed my data in particular was somewhat nonlinear. This is relatively common with time-based data. I would check to see that your regression isn't fitting a linear trend to a nonlinear one. If it is, you may need to consider nonlinear methods if you do in fact treat the data as numeric.
