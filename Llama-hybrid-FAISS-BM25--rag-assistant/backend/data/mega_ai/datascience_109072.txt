[site]: datascience
[post_id]: 109072
[parent_id]: 108474
[tags]: 
First, there may be a confusion in terminology. "Cross-validation" historically means just scoring a model on dataset(s) not used to train the model, and includes the simple train-test split. Note that sklearn 's hyperparameter tuners can use a simple train-test split, by setting cv to something other than just an integer. Recent (and maybe data science specifically?) usage has tended to emphasize the "(data-)exhaustive" methods like k-fold. Now the question remains: when to use k-fold over a train-test split (for hyperparameter tuning). As Hastie says, and your professor alludes, k-fold is quite nice in the data-sparse setting: it allows you to use all of your data more fully, and gets more stable estimates than a single split, as well as a variance estimate as you note. When there is sufficient data, a single split leave enough data to provide a good estimate, and the larger dataset means higher training times, so repeating the procedure $k$ times may be burdensome; however, as you say, doing k-fold gives you the added benefit of a variance estimate.
