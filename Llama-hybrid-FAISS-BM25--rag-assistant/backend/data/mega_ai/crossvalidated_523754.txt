[site]: crossvalidated
[post_id]: 523754
[parent_id]: 
[tags]: 
What is the PLSR implementation in sklearn?

Having a look to the source code from sklear implementation of PLSRegression I see two differences between what they cite as their reference and what they actually implement: The y_weights in the algorithm are not normalized, but in the reference paper they are. The deflation of $Y$ matrix is performed on a different manner than the one stated in their reference paper. I have seen these differences in the implemenntation of PLS also in R, and I want to understand where they are coming from. Going into more details about the differences: In the sklearn source code it is stated that the main reference for their implementation is: Main ref: Wegelin, a survey of Partial Least Squares (PLS) methods, with emphasis on the two-block case https://www.stat.washington.edu/research/reports/2000/tr371.pdf In pages 9 and 10 of the pdf file, what they call "PLS-C2A" is described in detail. Differences in y_weights Specifically, at the begining of page 10 it is specified that y_weights ( $v_r$ in the document) is computed as the right singular vector of matrix $(X^{r})^{t}Y^{r}$ . This is done in the sklearn algorithm by using the power method (function _get_first_singular_vectors_power_method ) where they compute, for the x_weihts : x_weights = np.dot(X.T, y_score) / np.dot(y_score, y_score) x_weights /= np.sqrt(np.dot(x_weights, x_weights)) + eps This would be equivalent to $$u_r = \dfrac{(X^r)^t\omega}{\omega^t\omega}$$ $$u_r = \frac{u_r}{\|u_r\|}$$ And for the y_weights y_weights = np.dot(Y.T, x_score) / np.dot(x_score.T, x_score) Equivalent to $$v_r = \dfrac{(Y^r)^t\xi}{\xi^t\xi}$$ But they do not normalize $v_r$ after that. Why not? Differences in Y deflation In section 8 (page 24) of the paper it is specified how the deflation process is performed in PLS2 algorithm: $$ \beta_{r} \leftarrow\left(\xi_{r}^{T} \xi_{r}\right)^{-1} \xi_{r}^{T} \omega_{r} $$ and $\mathrm{Y}^{(r)}$ is updated as follows: $$ \mathrm{Y}^{(r+1)} \leftarrow Y^{(r)}-\beta_{r} \xi_{r} \omega_{r}^{T} $$ But in sklearnn, the deflationn process is performed as y_loadings = np.dot(x_scores, Yk) / np.dot(x_scores, x_scores) Yk -= np.outer(x_scores, y_loadings) which would be equivalent to $$ \delta_{r}^{T}=\xi{r}^{T} Y^{(r)}\left(\xi{r}^{T} \xi{r}\right)^{-1} $$ $$ \mathbf{Y}^{(r+1)} \leftarrow \mathbf{Y}^{(r)}-\omega_{r} \delta_{r}^{T} $$ So, Â¿why this difference?
