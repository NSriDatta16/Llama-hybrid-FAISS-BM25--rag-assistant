[site]: crossvalidated
[post_id]: 86505
[parent_id]: 
[tags]: 
Do robust standard errors protect you from proportional odds assumptions?

Cox Proportional Hazards models are traditionally taught alongside proportional hazards assumptions. There is a corresponding test of proportionality. However, if standard errors are calculated from sandwich estimators, there's no need to worry about that assumption, you just interpret the hazard ratio you DO obtain as a weighted average of all the hazard ratios according to the failure times observed in the data. Is anyone aware of any such similar result obtained from proportional odds ordered logit models? One can see a great deal of similarities between the two: in Cox models, the baseline hazard handles the influence of time and in ordered logit, the intercepts handle the intercepts associated with each incremental risk comparison level. The estimation in any case is, then, kind of "semi-parametric" since you borrow that information across time or response groups in such a fashion. So, if we can say that the "parameter" part can be averaged across groups in time, as for Cox models, why not average across response groups? Possible citations would also be helpful!
