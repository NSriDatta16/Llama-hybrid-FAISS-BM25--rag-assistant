[site]: crossvalidated
[post_id]: 279344
[parent_id]: 279268
[tags]: 
Let me clarify what cross-validation is. In machine learning and predicting statistics in general, you propose a model to predict the data (which includes which features you use). To test your model (and your feature-selection), you run it on a dataset. To avoid a bias, you, of course, run it on unseen data and test its performance. Usually, you therefore split your data into a training and test set. However, sometimes you do not have not enough data to be able to split it (you either lack training data then or the test set would be too small). That's where cross-validation (or k-folding) comes into play: you split your data into k folds, train on k-1 and predict on the last one. You repeat this k times. In the end, you get predictions for the whole dataset and it is "as if you used the whole dataset as test set". So your determine the performance by using all predictions for whatever metric you are using. You can now do this (either train/test split if you have enough data or k-folding) with different sets of variables. The one with the best score is to be taken. It is remarked here that you should usually have a validation set which you don't use in the whole training process: doing this kind of feature selection can bias your outcome! So having a validation set you did not use at all can give a good performance estimation in the end.
