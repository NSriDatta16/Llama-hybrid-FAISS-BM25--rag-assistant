[site]: crossvalidated
[post_id]: 327275
[parent_id]: 327255
[tags]: 
All 1s or 0s for some categorical independent variable (or combination of them) in standard logistic regression would be a case of “complete separation”. I.e. the maximum likelihood estimate of the log-odds ratio is not finite and any output from a software is suspect. The random effects in the model might make some difference to what should and does happen, but I don’t think so. Standard strategies when this happens are Firth’s penalized LR, exact methods - but these do not allow random effects -, or Bayesian methods with (weakly-)informative priors. There may be other reasonable approaches that I am not aware off, but the last one one would be my personal choice. In terms of implementation this may require specifying the model in your Bayesian language of choice (Stan, JAGS, PROC MCMC, BUGS etc., which have interfaces to many platforms/programs such as R, SAS, Python etc.), unless the particular model is already implemented somewhere.
