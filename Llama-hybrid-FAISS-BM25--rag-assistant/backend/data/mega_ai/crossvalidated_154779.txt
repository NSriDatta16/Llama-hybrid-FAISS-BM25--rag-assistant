[site]: crossvalidated
[post_id]: 154779
[parent_id]: 154673
[tags]: 
Thanks to A. Donda for the answer to part 2. I was able to investigate my conjecture for part 1 in my original post for this question. Yes, maximum likelihood estimate of the standard error for the coefficient uses a normalization of $n$ (the number of observations) whereas minimum bias estimate uses a normalization of $n$ less the number of coefficients . I counted the number of coefficients from the vgxdisp output for the case of (maximum lag)=3. I then compared the standard errors of the estimated coefficients with the vgxvarx's 'DoFAdj' set to true and to false (thanks, A. Donda), which yields the minimum bias and maximum likelihood estimates, respectively. My comparison of the two sets of standard errors consisted of multiplying the maximum likelihood standard errors by $n$ to cancel out the normalization by $n$, then dividing by $n$ less the number of parameters (the normalization for minimum bias). Note that I had it backward in my original post. There is a huge caveat though. I had to increase the number of parameters by exactly 1 in order for the numbers to match. So there must be an extra estimated parameter which isn't listed in the vgxdisp output. Perhaps it is the cross-covariances between the noise sources for the two time series in the model. It only shows up once in the vgxdisp output, but the cross-covariances were considered to be the same parameter because they should be identical. Maybe for regression purposes, they are not. Here is the code that converts maximum likelihood estimates of standard error to minimum bias estimates, with the number of estimated parameters $nEstParam$ being 1 more than the number listed in vgxdisp's output: n=74; nEstParam=3*4+3+1; x=sqrt(n/(n-nEstParam)) * ... [ 0.138092 18.6127 0.00102525 0.138188 0.151676 29.8176 0.0011261 0.221377 0.131599 19.4701 0.000977039 0.144553 5.81405e+07 349742 3204.78 ]; for i=1:length(x); disp(x(i)), end P.S. I realize that the solver is reporting poor conditioning of the matrix. I am guessing that it is due to the apparent unit root in the 2nd time series, as hinted by the 1st subplot. NOTE: As much as this answer confirms what is done in estimating the parameters, it still doesn't answer the question of whether the maximum likelihood or minimum variance estimate is better. Not just better in a general sense (which probably hs no answer), but which is better for estimating p-value. I am not sure whether there is a basis that can be put forth to favour one over the other. Thoughts? New info: I feel it is important to share that the mystery extra degree of freedom is not due to counting all 4 coefficients in the 2x2 covariance matrix, as suggested by the 3+1 contribution to nEstParam above. This was tested by selecting a diagonal covariance matrix (CovarType='diagonal' for vgxvar). We would expect that replacing 3+1 by 2 in nEstParam would yield the right conversion from ML stderr to min bias. It doesn't, and it turns out that the right conversion factor results from replacing 3+1 by 2+1 in nEstParam above. I suspect that vgxdisp is using the wrong degrees of freedom to calculate stderr because the resulting nEstParam doesn't even match the number of active parameters from vgxcount. I could be wrong, but until the extra DoF can be explained, it's the only possible conclusion. Hence, if I had to extract stderr from vgxdisp, I would request a ML stderr and correct it using the DoF from vgxcount. This is not ideal because it is still guessing, but it's the most sensible guess so far.
