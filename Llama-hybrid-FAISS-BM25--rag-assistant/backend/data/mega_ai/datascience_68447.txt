[site]: datascience
[post_id]: 68447
[parent_id]: 68441
[tags]: 
No, there is nothing really better than recalibrating the model and see if it improve performance. Most techniques will be approximation of what you are doing with your main model. If you happen to find a simpler model that perfectly explain which variable plays a role in your main model and how, you should probably just use the simpler model. Basic a priori techniques (looking at correlation with output, with other variables) will fail in the context of complex machine learning models. One practical way to go nowadays is to look at feature importance. Those techniques depends on the model you use and will provide an importance value for each feature of each instance. If a feature has a low importance for all instances you should try removing it. (you can do that by batches of k least important features).
