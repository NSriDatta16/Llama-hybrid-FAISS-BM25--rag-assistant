[site]: crossvalidated
[post_id]: 330410
[parent_id]: 330360
[tags]: 
The Wilcoxon signed rank test might be used for comparing two algorithms over many data sets*, rather than many algorithms over two data sets ... *(at least, under certain assumptions about how the data sets relate to the notional population of sets that you wish to perform inference over. Are there such assumptions that would make sense though? E.g. a convenience sample would not constitute a random sample of that population. There's also no assignment-to-treatment over which a randomization argument could be used. Is there even a basis on which one could really perform inference here?) Thanks for clarifying. What test would be optimal for my situation? What are we optimizing? On just two data sets? That's a rather different question to the posted one. I probably wouldn't consider a nonparametric test for that because you can't attain typical significance levels unless there are a lot of algorithms being compared (even if you could overcome the above concerns about whether it makes sense to apply statistical inference at all in this situation). If you plan to apply post hoc testing to identify which pairs of algorithms can be said to differ, you are going to have even greater difficulty attaining reasonable significance levels with only two data sets. So I think it would come down to choosing a suitable parametric model for the response (e.g. if you're dealing with proportion correct, you might perhaps look at some form of logistic regression model; other scoring mechanisms might need other models). Once there, and with a formal null and alternative, coming up with a reasonable test wouldn't be so difficult. Edit: With four algorithms and two data sets you cannot even attain a 10% significance level with a Friedman test. If you want to do something with so little data, I'd suggest you need to choose a model.
