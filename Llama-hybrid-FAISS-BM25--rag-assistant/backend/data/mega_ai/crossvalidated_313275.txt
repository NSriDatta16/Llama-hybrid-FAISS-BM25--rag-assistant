[site]: crossvalidated
[post_id]: 313275
[parent_id]: 313259
[tags]: 
One approach is to share information across individual (region,quality) problems. For example, you could use a multi-task learning framework to couple the individual adjustments (at the quality level) of the Canada-5 model. For each region independently, fit a linear adjustment by minimizing some loss function like this one: $L_R(\alpha,\beta) = \sum_{q} (\alpha_{q}+\beta_{q} C_5-Y_{q})^2+\lambda \sum_{q>q^\prime}H(\alpha_{q}+\beta_{q} C_5-\alpha_{q^\prime}-\beta_{q^\prime} C_5)$, where $C_5$ is the output of the Canada-5 model, and $H$ is the hinge-loss function $H(x)=\left\lbrace \begin{array}{cc} -x, & x This is just $L_2$ loss with a hinge-loss penalty on different quality models. This would penalize $\alpha$'s and $\beta$'s in such a way as to encourage the predictions of higher quality models to be higher than lower quality models in the same region. Another approach is to use transfer learning to train a new model for each (region,quality) pair using the Canada-5 dataset as the source domain. One such method is outlined in Want et al. (2014). Active learning under Model Shift . The model shift method is to Fit a model to predict $y_{source}=f(x_{source})$. (You've done this, it's your Canada-5 model) Apply the model to the target domain to get the offsets $z=y_{target}-f(x_{target})$. Fit another model to the offset $z=g(x_{target})$. Apply the offset model to the source domain to get $\hat{y}_{source}=g(x_{source})+y_{source}$. Train the target domain model on the data set $(x_{target},y_{target})\cup(x_{source},\hat{y}_{source})$. In the paper, the authors use Gaussian processes to model the offsets and such, but it seems to me the approach will admit other types of models. Both ideas could be combined; one could conceive of a multi-task transfer learning problem.
