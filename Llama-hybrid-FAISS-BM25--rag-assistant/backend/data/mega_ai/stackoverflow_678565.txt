[site]: stackoverflow
[post_id]: 678565
[parent_id]: 
[tags]: 
In Queuing Theory, what is the relationship between processing time and average queuing delay?

Consider a system where jobs are queued and processed in FIFO order. They currently wait an average of N seconds before being processed. If jobs take an average of M seconds to process, what will be the impact to N if we reduce the processing time to M/2 seconds?
