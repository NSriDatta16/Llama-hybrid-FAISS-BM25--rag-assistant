[site]: crossvalidated
[post_id]: 612941
[parent_id]: 101560
[tags]: 
Generally speaking, $\tanh$ has two main advantages over a sigmoid function: It has a slightly bigger derivative than the sigmoid (at least for the area around 0), which helps it to cope a bit better with the “vanishing gradients” problem of deep neural networks. Here is a plot of the derivatives of both functions: It is symmetric around 0, which helps it to avoid the “bias shift” problem that sigmoid suffer from (which causes the weight vectors to move in diagonals, or “zig-zag”, which slows down learning). Sigmoid has 1 main advantage over $\tanh$ , which is that it can represent a binary probability - hence can be used as the output of the final layer in binary classification problems. You can check out this video I made on YouTube which explains a bit further about these problems. Elaboration on the bias shift problem: Consider a case of activation functions like Sigmoids which only output positive values. Now let’s focus on a single layer $a_l$ . Let’s look at the weight vector associated with the first next neuron: $z_{(l+1),1}=W_{l,1}\cdot a_l + b_{l,1}$ . The gradient w.r.t. this vector will be (according to the chain rule) $a_l \cdot \frac{\partial \mathcal L}{\partial z_{(l+1),1}}$ . That is the gradient up to $z_{(l+1),1}$ (which is a scalar) times the gradient of $z_{(l+1),1}$ w.r.t. $W_{l1}$ which is just $a_l$ . We know that the $a_l$ neurons are all $\ge 0$ , so this $W_{l1}$ vector updates depend on $sign(\frac{\partial \mathcal L}{\partial z_{(l+1),1}}).$ This means that the vector either increase or decrease for all elements $\Rightarrow$ it can only move in Zig-Zag / diagonals, which is not very efficient. This is sometimes called the “bias shift” problem . It also happens when the activations output values which are far from 0 (though to a less extent).
