[site]: crossvalidated
[post_id]: 465611
[parent_id]: 465604
[tags]: 
There is none in Logistic Regression (although some might say the threshold is one, it is actually your decision algorithm's hyper-parameter, not the regression's). If it is regularized logistic regression, then the regularization weight is a hyper-parameter. In decision trees, it depends on the algorithm. But most common ones are maximum depth, and splitting criterion, minimum number of samples to split etc. You can find others in custom library implementations, such as in sklearn . For Support vector machines, the kernel type, regularization parameter are among the most common. Depending on your kernel choice, other hyperparameters can show up, e.g. gamma for rbf and polynomial kernels, degree for polynomial kernel etc.
