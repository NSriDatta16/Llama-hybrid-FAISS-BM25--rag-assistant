[site]: crossvalidated
[post_id]: 396498
[parent_id]: 395940
[tags]: 
Your question does not specify the trials parameter in the multinomial distribution, so I'm going to use the standard generic case where you have $n$ trial and $m$ categories. I will let $\tilde{\boldsymbol{\theta}} = (\tilde{\theta}_1,...,\tilde{\theta}_m)$ denote the normalised values $\tilde{\theta}_k = \theta_k / \sum_i \theta_i$ , so the model of interest is: $$\mathbf{X} | \boldsymbol{\theta} \sim \text{Mu}(n,\tilde{\boldsymbol{\theta}}) \quad \quad \quad \theta_k \sim \text{Ga}(\alpha_k, \beta_k).$$ The required integral: The marginal mass function for $\mathbf{X}$ is given by: $$\begin{equation} \begin{aligned} p(\mathbf{x}|\boldsymbol{\alpha}, \boldsymbol{\beta}) &= \int \limits_\Theta \text{Mu}(\mathbf{x} | \tilde{\boldsymbol{\theta}}) \prod_{i=1}^m \text{Ga}(\theta_i | \alpha_i, \beta_i) \ d \boldsymbol{\theta} \\[6pt] &\overset{\mathbf{x}}{\propto} {n \choose \mathbf{x}} \int \limits_\Theta \frac{\prod_{i=1}^m \theta^{x_i + \alpha_i - 1} \exp( - \beta_i \theta_i )}{(\sum_{i=1}^m \theta_i)^n} \ d \boldsymbol{\theta}. \\[6pt] \end{aligned} \end{equation}$$ If $\beta_1 = ... = \beta_m = \beta$ then $\tilde{\boldsymbol{\theta}}$ has a Dirichlet distribution which means that $\mathbf{X}$ then has a multinomial-Dirichlet distribution. (In your question you incorrectly state that $\mathbf{X}$ has a Dirichlet distribution in this case. That can't be right because the Dirichlet distribution generates non-integer vectors of values.) There are various ways to approximate the above integral, and I will show you one of these methods below. (If time permits, I may come back to this answer and add other methods.) Approximation by direct sampling (useful if hyperparameters are fixed): Since the object of interest is a mass function obtained via a known conditional distribution, the simplest way to approximate the integral is simply to generate a large number of conditioning values and approximate the integral by an analogous sum. To do this, choose some large number of simulations $H \in \mathbb{N}$ and generate independent values: $$\theta_k^{(h)} \sim \text{Ga}(\alpha_k, \beta_k) \quad \quad \quad \text{for all } k=1,...,m \text{ and } h =1,...,H.$$ You can then use the approximation: $$\hat{p}_\text{DS}(\mathbf{x}|\boldsymbol{\alpha}, \boldsymbol{\beta}) = \frac{1}{H} \sum_{h=1}^H {n \choose \mathbf{x}} \frac{\prod_{k=1}^m \theta_k^{(h) \ x_k}}{(\sum_{k=1}^m \theta_k^{(h)})^n} .$$ This method requires you to generate $H \times m$ scalar parameter values and then evaluate the mass function as an average of $H$ multinomial mass functions. Note that this method evaluates the mass function for a fixed set of parameter values $\boldsymbol{\alpha}$ and $\boldsymbol{\beta}$ , so it is not useful if you would like your approximation to serve as a function of those parameter values. Approximation by importance sampling (useful if hyperparameters are variables): The method of direct sampling builds the parameters $\boldsymbol{\alpha}$ and $\boldsymbol{\beta}$ into the simulation, so it is useful when these are fixed (i.e., when you don't need the function to vary over these values). If you would like your approximating function to also be able to vary over these parameters then you can use importance sampling instead. To do this, choose some mid-ranged hyperparameter values $\bar{\alpha}$ and $\bar{\beta}$ , choose some large number of simulations $H \in \mathbb{N}$ and generate independent values: $$\theta_k^{(h)} \sim \text{Ga}(\bar{\alpha}, \bar{\beta}) \quad \quad \quad \text{for all } k=1,...,m \text{ and } h =1,...,H.$$ You can then use the approximation: $$\hat{p}_\text{IS}(\mathbf{x}|\boldsymbol{\alpha}, \boldsymbol{\beta}) = \frac{1}{H} \sum_{h=1}^H {n \choose \mathbf{x}} \frac{\prod_{k=1}^m \theta_k^{(h) \ x_k}}{(\sum_{k=1}^m \theta_k^{(h)})^n} \cdot \prod_{k=1}^m \frac{\text{Ga}(\theta_k^{(h)} | \alpha_k, \beta_k)}{\text{Ga}(\theta_k^{(h)} | \bar{\alpha}, \bar{\beta})}.$$ This method requires you to generate $H \times m$ scalar parameter values and then evaluate the mass function as an average of $H$ weighted multinomial mass functions, where the weighting is used to adjust for the hyperparameters. Note that this method treats the hyperparameters as variables that can be adjusted in the calculation of the approximating function (without generating new simulated values). Here is some R code to implement this latter method: #Load required libraries and set seed library(stats); library(matrixStats); set.seed(1); #Set parameter values for simulation m This code will allow you to generate the log-mass for your distribution at any input value for the observable vector and the hyperparameters. By setting H reasonably large you should get a good approximation to the true log-mass function. Some statistical theory and accuracy bounds for this kind of simulation can be found in O'Neill (2009) .
