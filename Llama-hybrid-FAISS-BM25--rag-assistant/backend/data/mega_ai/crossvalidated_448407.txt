[site]: crossvalidated
[post_id]: 448407
[parent_id]: 447101
[tags]: 
The width of a margin is simply $\frac{2}{||\vec{w}||}$ , and when you use kernel trick $\vec{w}=\sum_i \alpha_i y_i\phi(\vec{x_i})$ , where $\phi(\vec{x_i})$ is a feature mapping function, which maps $x_i$ to a higher dimension. However, quite often we don't know the explicit formula of the feature mapping function $\phi(\vec{x})$ and we only able to compute its dot product $ $ , for example in case of RBF kernel. So, in this case, it's not possible to compute the width of a margin. However, I would argue that you don't need to compute the margin width to compare trained models. When you apply a kernel trick your goal is to transform linearly inseparable examples to a higher dimension, such that they will become linearly separable in a higher dimension. So basically, the better your kernel separates examples the higher will be the (training) accuracy of the SVM. Also, a large margin doesn't mean that SVM will not overfit. So to pick the best model (kernel) I would use cross-validation (or just train-test-validation scenario) to find the best combination of accuracy and regularization parameters for each kernel first, and then I would pick kernel with higher accuracy. Also, I suggest you have a look at number of support vectors that each kernel uses to produce a separating hyperplane, I would tend to pick a kernel with higher accuracy and with a fewer number of support vectors.
