[site]: crossvalidated
[post_id]: 619612
[parent_id]: 
[tags]: 
In transformer architecture, Why does Masked Self Attention layer uses additive masks not multiplicative mask?

We know that in transformer architecture, we first compute $Q,K$ and $V$ matrices from the token embeddings. Then we compute the attention weights $\alpha$ as $$\alpha = Softmax(QK^T)$$ In the case of masked attention, we can use additive mask as follows $$\alpha = Softmax(QK^T+UTMask)$$ where $UTMask$ is a strict upper triangular matrix with elements value $-\infty$ . This serves the purpose. However, we could achieve the same with a multiplicative mask with the output of softmax as follows, $$\alpha = Softmax(QK^T)*LTMask$$ where $LTMask$ is the Lower triangular matrix with elements value $1$ (including the diagonal). I am just wondering why Masked Multi-head Attention layer in transformer architecture prefers additive mask over multiplicative (Boolean) mask. Is it due to the behaviour of how back-prop for work additive and multiplicative operators? Are there any other reasons?
