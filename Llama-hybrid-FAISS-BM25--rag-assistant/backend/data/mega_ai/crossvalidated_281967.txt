[site]: crossvalidated
[post_id]: 281967
[parent_id]: 281960
[tags]: 
When using R to test the mean difference of paired samples with normal distribution, which method is more effective among "t.test" "z.test" and "wilcox.test"? When the population is actually normal the t-test has (slightly) more power than the Wilcoxon signed rank test. In large samples it's like you have about an extra observation for every 20 in the sample. The z-test has the wrong significance level (since the population sd of the differences is not known), so it's not actually comparable unless you know the population standard deviation; use the t-test for preference. As an example, for the two paired samples below(new and ori), they have been verfied to accord with the normal distribution by using shapiro.test. No; failure to reject does not mean that you have normality. Indeed it would be easy to be in a situation where the Wilcoxon signed rank test outperforms the t-test (because the tails were slightly heavier than the normal) but the Shapiro-Wilk test would not be all that likely to reject. As an example, consider the case where both new and ori had the logistic distribution. With a sample size of 10, the Shapiro-Wilk test on the differences has about a 6.5% chance of rejecting normality at the 5% level (that is, with 16 such tests, on average for 15 of them you would have apparently mistakenly concluded that the distribution was normal -- but as I said that's not the correct conclusion, because failure to reject doesn't mean it is normal). This is what that density of differences looks like: $\hspace{2cm}$ It looks pretty normalish right? Nevertheless, the Wilcoxon test actually has slightly better power to pick up small differences in this situation (when the two tests are carried out at the same significance level). If you let the differences be logistic instead (which implies heavier tails on the original pairs than logistic), the Shapiro-Wilk test gets slightly better (about 8% chance of rejecting normality) but the Wilcoxon signed rank test is also more clearly showing greater power against small differences, getting about twice the relative power advantage over the previous case. A suitable conclusion from those examples is that if you expect the population distribution to be roughly normalish but there's a reasonable possibility of there being heavier tails than you get with the normal distribution, it's probably safer to use the Wilcoxon signed rank test, but it depends in part what your precise hypothesis is and what you're prepared to assume. However, when using left tailed and right tailed z.test, both of the p-value are larger than 0.05. Does it mean that the z.test is ineffective under this situation? You can't make that judgement on the basis of a single sample; the tests respond slightly differently to the observations. The outcome of a single test is affected by random variation which may lead to one test having a higher p-value on that particular sample even though it has better power overall. You can't directly compare one tailed tests with two tailed tests. You didn't do a paired z-test, so if you did correctly do the paired test in the other two cases you're not comparing like with like. The p-value for a one-sample z-test (ignoring that we estimated the population sd of the differences) would be 0.01771. ( z.test(new-ori,sigma.x=sd(new-ori)) ) For a large sample (>30 data values) with normal distribution, is the z.test more appropriate than t.test? The t-test is still correct, the z-test is approximate, but as the sample size increases, the t-test gets closer to a z-test. Ultimately in sufficiently large samples it doesn't matter which you do, but if you truly had normality there's no particular reason to use the z-test over the t-test no matter how big the sample size. With computers to do the calculation, there's no need to replace the t with an approximation since there's no additional effort involved.
