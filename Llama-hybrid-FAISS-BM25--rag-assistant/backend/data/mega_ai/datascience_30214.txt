[site]: datascience
[post_id]: 30214
[parent_id]: 
[tags]: 
Image Embeddings - Negative Sampling and Imbalanced Class Issues

I am using the negative sampling approach used in Word2Vec to train some image embeddings. From what I have read, for every positive example, we are creating a number of negative examples. Question: Why do we use an imbalanced dataset here? Presumably we will get the normal issue where the algorithm ends up predicting the negative label to minimise the cost function? I understand that the aim isn't really to use it as a prediction model, but rather to extract the embeddings, but what is the benefit of having an imbalanced class here?
