[site]: crossvalidated
[post_id]: 439068
[parent_id]: 414160
[tags]: 
Nutshell So Polynomial activation functions don't work, since they fail to have the main property which makes neural networks interesting. Mathematical Reason Actually, there is a more rigorous reason why they are not used. In this paper , it is shown that the collection of all feed-forward neural networks can approximate any (reasonable) function if and only if the activation function is not a polynomial. Explicit Counter-Example : As an example, the simplest polynomial functions (which are non-constant) affine affine functions. If affine functions could be used (ie the universal approximation peropty were to hold) then linear regressions could approximate any continuous function. Which isn't the case.
