[site]: stackoverflow
[post_id]: 4060685
[parent_id]: 4060676
[tags]: 
Louder at what point? The signal's average amplitude will tell you which one is louder on average , but that is kind of a dumb, brute force way to go about it. It may work for you in practice though. Getting down to it, I need to be able to determine when a baby is crying vs ambient noise in the room. Ok, so, I'm just throwing out ideas here; I am by no means an expert on audio processing. If you know your input, i.e., a baby crying (relatively loud with a high pitch) versus ambient noise (relatively quiet), you should be able to analyze the signal in terms of pitch (frequency) and amplitude (loudness). Of course, if during he recording someone drops some pots and pans onto the kitchen floor, that will be tough to discern. As a first pass I would simply traverse the signal, maintaining a standard deviation of pitch and amplitude throughout, and then set a flag when those deviations jump beyond some threshold that you will have to define. When they come back down you may be able to safely assume that you captured the baby's cry. Again, just throwing you an idea here. You will have to see how it works in practice with actual data.
