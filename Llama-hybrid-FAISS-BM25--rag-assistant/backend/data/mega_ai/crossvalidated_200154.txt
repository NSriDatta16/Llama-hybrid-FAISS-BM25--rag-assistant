[site]: crossvalidated
[post_id]: 200154
[parent_id]: 179966
[tags]: 
• Among the few dozen books and research papers I've read on neural nets since the late '90s (such as Patrick K. Simpson's excellent cookbook, "Artificial Neural Systems: Foundations, Paradigms, Applications and Implementations"), the most common notation has been to use $Wij$ to denote forward connections from the first layer to the second, at least in two-layer networks. One catch is that some sources label weight matrices in reverse alphabetical order as the number of layers increases; for example, in a three-layer net, the connections from the first layer to the second might be labeled $Vij$. Note that in these cases the alphabetical order of the subscripts remains the same. • $Wji$ usually represents backwards connections from the second to the first layer, at least in the sources I've slogged through; perhaps it's not always true, but it's not typically used as an alternate designation of $Wij$. I've implemented some of them this way and they've worked fine. The reverse order of the subscripts is usually a dead giveaway for a backwards connection from some later layer to some earlier one. This can get complicated quite quickly though in cases with higher-order connections, ragged connection matrices, connections that skip levels (say from layer three to one) and other special cases where you might see strange symbols like $Vjki$ and the like. • I usually use SQL sets rather than matrices to implement neural nets, so I can't answer the third part of the question definitively. It may be depend on if $Wij$ and $Wji$ are symmetric or represented by two separate one-way connections that go in opposite directions to the same neurons. Either way, you'd probably have to flip something around, probably using a transpose; I get the same effect in SQL using inner joins between two or more layers. I wonder if the notation on the Stanford page might be a mistake, because s$i$, a$i$ and x$i$ are used there to denote the number of nodes, activation values and input numbers for layer L, not layer L + 1. On the other hand, given the multiplicity of different notations for neural nets, it may have some practical significance. This smorgasbord of notations is a real problem in this field and can make implementation really tricky. One way I've found of dealing with this is to find formulas with conflicting notations for the same type of neural net and look for commonalities when implementing them; sometimes the meanings intended by the authors then become clearer. I hope this helps.
