[site]: crossvalidated
[post_id]: 625156
[parent_id]: 
[tags]: 
How do zero values affect the correlation between two twitter activity time series?

I have a time series of Twitter activities (lets call it time series 0) from which I extract two separate time series based on different sets of users (i.e. time series 1 contains the activity of a subset of users and time series 2 contains the activity of the remaining users - see table 1). I want to calculate cross correlation between time series 1 and 2. To create the time series I resample the original list of Tweets based on a given time step (e.g. 1 h) and sum all the tweets that were made in this interval: Time time series 0 time series 1 time series 2 00:00 0 0 0 01:00 0 0 0 02:00 0 0 0 03:00 0 0 0 04:00 0 0 0 05:00 0 0 0 06:00 1 1 0 07:00 3 3 0 08:00 6 5 1 09:00 3 3 0 100:00 15 10 5 (This table is just an example, the original data is aggregated at a 10 minute interval and contains over 30 000 time steps and roughly 10 000 zero values in time series 0). This leads to a large amount of zero values in the dataset and the data are power law distributed (see fig. 1). I use only the decomposed residuals of the time series (removing trend and seasonality) and transform the data by applying log(x+1) (+1 because otherwise the zero values would give -inf). Afterwards in order to reduce noise I delete values below a given threshold and then split up the datasets given a previously derived set of user into my new time series 1 and 2. Finally I calculate the Pearson correlation for multiple lags between time series 1 and 2. Now to my question: It seems like the more zero values I keep, I get a very sharp increase of the correlation value from time lag 0 to time lag 1 between time series 1 and 2 (see fig. 2). Can someone possibly explain where this behaviour comes from or encountered something similar? The peak remains when I resample the data on a different interval for example 30 min or 1h). I saw in this post that there might be spurious correlations between skewed data - can anyone give some general intuition where this comes from? Figure 1 shows the power law distribution of the original dataset activity (time series 0). Figure 2 shows the cross-correlation plot where the black curve represents the correlation of the time series 0 with itself (i.e. at time lag 0 = autocorrelation) and the other three curves represent correlations between the subsets of time series 0 - i.e. time series 1 and 2, where the time series were derived for different subsets of users (blue = random sample of users, red and green = systematically derived subset of users). The left plot contains all original values and the right plot only contains time steps were activity in time series 0 was highest. Figure 3 shows the scatter plots at time lag 1 that correspond to figure 2. One can see that the amount of zero values in the left plot is way higher.
