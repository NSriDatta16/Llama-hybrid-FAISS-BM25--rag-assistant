[site]: crossvalidated
[post_id]: 394737
[parent_id]: 394731
[tags]: 
Logistic regression is a linear classifier, i.e. it draws a line (2D datasets) and classifies accordingly (one side is class 0, other side is class 1). So, if classes can be distinguished by a line (or hyperplane in higher dimensions), it is said that the dataset is linearly separable , though this dataset is not. One way to tackle this issue is creating new features, or applying transformations. For example, this dataset seems to be separable if you think radially, i.e. $R>\alpha$ , where $R$ is the radius, or distance to origin, which can be found by $R=\sqrt{X_1^2+X_2^2}$ . Constructing a logistic regression using this feature only, results in perfect classification.
