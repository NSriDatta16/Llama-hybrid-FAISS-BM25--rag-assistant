[site]: crossvalidated
[post_id]: 478154
[parent_id]: 478075
[tags]: 
Well, firstly, it could be a good thing. If it is easy to predict for every case what class it belongs to, then you could see this kind of behaviour. The main problem in that scenario - as mentioned by the answer by cdalitz - is that you might run into perfect separation, which is especially challenging for logistic regression fit using maximum likelihood (possible approaches to deal with it: exact logistic regression, Firth's correction, Bayesian logistic regression, elastic-net-/LASSO-/ridge-logistic regression etc.). Depending on how outcomes are distributed by predictor variable this may or may not be occuring here - one possible hint are odds coefficients (e.g. really huge coefficients like >10 or Secondly, it could be a bad thing in terms of overfitting (esp. if there are few records relative to the number of predictors), where (nearly) perfect separation of classes by predictors occurs, but really only by chance due to the small sample size. This will then not generalize well to new data you want to predict. Some of the same regularization techniques mentioned above might help with logistic regression and picking suitable hyperparameters for boosting models (e.g. through cross-validation) might help for boosting models. Thirdly, especially for boosting (and some other models, e.g. this also happens with neural networks), it is a known thing that predicted probabilities tend to inappropriately cluster towards extremes (the topic to search for is "calibration" - or in this case potentially the lack thereof). In contrast, this tends to be less of a problem with "normal" (or ridge/elastic-net/LASSO logistic regression). There are a number of possible fixes such as isotonic-/Platt-scaling of predicted probabilities and using loss functions that alleviate the problem (e.g. I recently saw focal loss proposed for this purpose). [Added] Final possibility: If the predictions are on the same data that the model are trained on (less of a problem when only applied to out-of-fold predictions in cross-validation, which are typically less overfit, except for the overfitting that occurs due to hyperparameter tuning via the cross-validation), then they will naturally be overfit unless the training data is very large (it gets worse with class imbalance and with some pretty strong and/or imbalanced predictors).
