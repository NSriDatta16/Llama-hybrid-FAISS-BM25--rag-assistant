[site]: crossvalidated
[post_id]: 22639
[parent_id]: 22502
[tags]: 
There are a number of techniques in experimental design and analysis that can help you reduce your bias, but this again always boils down to the same thing: One has to know what one is doing. Big data analysis has the same problem as any other data analysis; it suffers from a lack of hypotheses. A clear example is multiple regression with stepwise variable selection. Very nice, one say, but with 100 variables measured statistical laws dictate that some of them will show a significant relation when evaluated by looking whether the respective coefficient differs significantly from zero. So the more variables in your dataset, the more chance of finding two that show some (meaningless) relation. And the bigger your dataset, the more chance for meaningless models due to eg a small confounding effect. If you test many models (and with even only 10 variables that can be a whole lot of models), you're very likely to find at least one significant. Does it mean something? No. What should one do then? Use your brain: formulate a hypothesis before collecting the data and test that hypothesis. That's the only way to make sure your statistics actually tell a story. Use your covariates to stratify your sampling before doing some tests. Stupid example: If you have 1000 males and 100 females in your dataset, randomly select 50 each if you want to talk about an average population. That's actually something where big data comes in handy: You have more than enough to sample from. Describe the test population thoroughly, so it's clear for which population your conclusions are formulated. If you use your big dataset for explorative purposes, test the hypotheses you come up with during this exploration on a new and different dataset, not just a subset of what you collected. And test them again using all the necessary precautions. These things are all obvious and well-known. Heck, already in 1984 Rosenbaum and Rubin illustrated how to use propensity scores to reduce bias in observational studies, and that's what most big datasets are: observational data. In more recent work of Feng et al , the use of the Mahalanobis distance is also advocated. And in fact, one of my statistical heroes, Cochran, wrote a review about that problem already in 1973! Or what about Rubin, who introduced multivariate matched sampling and regression correcting already in 1979. Old publications are seriously underestimated and far too often ignored, certainly in a field like statistics. All these techniques have pros and cons, and one has to understand that reducing bias is not the same as eliminating bias. But if you are aware of : what you want to test, and how you are doing it Big data is not an excuse to come with bogus results. Edited after the (correc) remark of @D.W. who pointed out I used the term 'overfitting' in a wrong context.
