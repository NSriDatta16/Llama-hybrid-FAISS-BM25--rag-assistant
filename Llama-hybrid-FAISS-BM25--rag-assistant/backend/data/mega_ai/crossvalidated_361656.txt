[site]: crossvalidated
[post_id]: 361656
[parent_id]: 
[tags]: 
Understanding parameter as a random variable in Bayesian statistics

If I understand correctly, in Bayesian statistics, a parameter is a random variable. When estimating the parameter, a prior distribution is combined with the data to yield a posterior distribution. Question: Is every data point (in the sample as well as the population) generated by the same realization of the parameter? If yes, why would I care about the distribution (i.e. other possible realizations and their respective probability masses or probability density values) of the parameter? After all, I am trying to find out something about this particular population from the sample and the prior. If not, how is this reflected in the formulas of Bayesian parameter estimation, if at all? At the same time, I understand that my beliefs, whether initial (reflected by the prior) or updated (reflected by the posterior) come as a distribution, and I have no problem with that. But I wonder if/why I should assume the parameter itself is a random variable. Edit: I have received a couple of answers which are helpful, but I would appreciate another one or a few that are more to the point.
