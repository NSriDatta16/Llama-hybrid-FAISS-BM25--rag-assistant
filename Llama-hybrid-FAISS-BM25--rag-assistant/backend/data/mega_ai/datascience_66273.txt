[site]: datascience
[post_id]: 66273
[parent_id]: 
[tags]: 
Relation of Tensors to artificial neural networks

When dealing with artificial neural networks, one is bound to come across the concept of Tensors. In such applications a Tensor is essentially just a data grid, just like a scalar (rank=0), a vector (rank=1) or a matrix (rank=2), except it has any finite rank. However, this is not how a Tensor is actually defined. In mathematics a Tensor is an abstract multilinear function which maps one algebraic object to another. While all matrices (or "data grids") always represent a certain type of Tensor, I fail to see how they semantically represent a function in case of artificial neural networks, as they are essentially only used as data storage for computations in between the computation graph. I found that in tensor-network-theory, a network of Tensors is used to represent a geometrization of the brain, but in case of artificial neural networks, there is no actual network to begin with. To me an artificial neural network is just some function which maps one algebraic object to another and consists of a concatenation and combination of piecewise partially differentiable functions. So my question is if Tensors as used in artificial neural networks actually have any semantic meaning. Do they represent some kind of function in relation to the inputs? Are artificial neural networks themselves Tensors (or tensor fields)? Or is the term Tensor just used due to lack of a proper name for a data grid without any other meaning?
