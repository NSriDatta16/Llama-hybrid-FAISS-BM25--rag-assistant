[site]: crossvalidated
[post_id]: 545663
[parent_id]: 544928
[tags]: 
While it's not exactly what you asked, Rasmussen and Williams talk about (essentially) this briefly in chapter 5 of Gaussian Processes for machine learning . In particular: Notice that the trade-off between data-fit and model complexity is automatic; there is no need to set a parameter externally to fix the trade-off. For a mathematical explanation of how this occurs, you can check out the "interpretation" paragraph (in subsection 5.4.1) on page 113 of the same chapter, though the whole of chapter 5 would probably be useful reading. A very brief summary is that the likelihood function contains two terms: one encouraging "a good fit for the data" (Rasmussen and Williams call this the "data fit" term), and one penalising complexity (the "complexity penalty").
