[site]: crossvalidated
[post_id]: 322631
[parent_id]: 322596
[tags]: 
That linear increase in training time looks fine. Every iteration you randomly pick one sample (step 2) and update it at the end (step 5). 10 times more vectors 10 times more updates and therefor it takes 10 times more time to train. I think that you can reduce number of training iterations (a.k.a epochs) since you have more data. In both cases you use 1000 iterations, but in second case you have 10 times more data. It means that in first case you make 100x1000 or 100,000 updates, where in second case you make 1,000,000 updates. For larger data sets you can reduce number of iterations. Also you can try to use different library, neupy . It has lots of neural network algorithm, including SOM (a.k.a SOFM) and it has lots of examples . It can be faster for your problem. Another way to speed up training is to use batch algorithm for SOM. In this case you use all your data instead of one training sample and as a consequence you updates all SOM vectors at the same time. But, I'm not sure if any python library has implementation of this approach.
