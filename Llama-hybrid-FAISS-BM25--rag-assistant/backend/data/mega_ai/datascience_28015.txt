[site]: datascience
[post_id]: 28015
[parent_id]: 27637
[tags]: 
The "optimal" stopping point for XGBoost really depends on the data you feed into it. Using Chunk N and Chunk N+1 for instance, consider the two scenarios: data in Chunk N and Chunk N+1 is very different - it's likely that the optimal stopping points for each chunk are very different. data in Chunk N and Chunk N+1 is very similar - it's likely that the optimal stopping points for each chunk are quite similar. I would suggest using a different approach - so for each chunk: use the xgb.cv function to first determine the optimal stopping point use the results from xgb.cv in xgb.train [i.e. set nrounds = xgb.cv$best.iteration ] As a side-note, have you explored any other techniques / tools to handle large data sets? In R, there are packages designed to tackle similar problems - e.g. bigmemory and ff .
