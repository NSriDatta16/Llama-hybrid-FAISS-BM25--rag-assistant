[site]: datascience
[post_id]: 88657
[parent_id]: 80639
[tags]: 
It seems what you need is a language model . You should train it with your "large set of sentences" and then use it to compute the likelihood of any given sentence. KenLM is a classic language model. It implements interpolated modified Kneser Ney Smoothing. The main publications describing it are this and this . Modern neural language models may give you better performance. Depending on your requirements, you may try with the simpler AWD-LSTM , which is based on a regularized long short term memory , or the more complex GPT-2 model , based on the Transformer architecture . For Transformer-based models, I suggest using the HuggingFace Transformers python library , which makes it very easy to train and use such a type of models, and has a repository of pre-trained models. BERT is quite different from a normal language model; it is a masked language model. It is not meant for the kind of likelihood estimation you are aiming at, but there are some proposals to use it that way too.
