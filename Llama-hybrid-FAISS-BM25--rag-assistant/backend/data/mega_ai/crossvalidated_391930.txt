[site]: crossvalidated
[post_id]: 391930
[parent_id]: 391866
[tags]: 
While I have written rather extensively on the limitations of the Bayes factor as a mean to conduct model comparison, the first point is that indeed it is not the Bayesian answer to the testing question Which of model $M_1$ or model $M_2$ is the true model? when evaluated with the Neyman-Pearson loss function. The proper Bayesian answer is the model with the largest posterior probability. When using other loss functions, the proper Bayesian answer is the posterior probability itself. Historically, the Bayes factor has been defended by Jeffreys (circa 1939) as providing a more objective answer than the posterior probability, as it evacuates the impact of the prior weights of the models $M_1$ and $M_2$ , which can be deemed as failing to take advantage of the Bayesian formalism, but truly reflects the immense uncertainty in setting such weights. On the second point, it is well-documented that Bayes factors are providing a natural penalisation for complexity by integrating out over a larger space, as opposed to likelihood ratios that exploit the best fits under both models. The Bayes factor is often seen as a quasi-automatic Occam's razor for this reason, which is also why the Bayesian information criterion (BIC) $$\mathrm{BIC} = {\log(n)k - 2\log(L({\hat \theta}))}$$ is used by non-Bayesians as well.
