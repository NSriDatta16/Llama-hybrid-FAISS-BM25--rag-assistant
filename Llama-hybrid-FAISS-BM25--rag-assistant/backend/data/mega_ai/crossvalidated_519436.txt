[site]: crossvalidated
[post_id]: 519436
[parent_id]: 
[tags]: 
Problem faced by Bayes when developing his method for Bayesian Inference

I am reading Principles of Statistics (MG Bulmer, 1965) and stumbled upon the problem that Bayes considered when developing his theorem. Bulmer makes use of $dP$ and I have no idea what it means. In his words: The problem that Bayes himself considered was the following. Suppose that some event has an unknown probability, $P$ , of occurring and that in $n$ trials it has occurred $x$ times and failed to occur $n - x$ times. What is the probability that $P$ lies between two fixed values, $a$ and $b$ ? Bayes first notes that the probability of the event occurring $x$ times out of $n$ is $\frac{n!}{x!(n-x)!}P^x(1-P)^{n-x}$ . This is the likelihood. He then remarks that if nothing was known about $P$ before the experiment was done it is reasonable to suppose that it was equally likely to lie in any equal interval; hence the probability that $P$ lay in a small interval of length $dP$ was initially $dP$ and so the joint probability that it lay in this interval and that the even occurs $x$ times out of $n$ is $\frac{n!}{x!(n-x)!}P^x(1-P)^{n-x}dP$ . The posterior probability that $P$ lies between $a$ and $b$ is thus proportional to the integral of this expression from $a$ to $b$ and is equal to $\frac{\int_a^b P^x(1-P)^{n-x}dP}{\int_0^1 P^x(1-P)^{n-x}dP}$ . I believe this last ratio comes directly from the conditional probability formula and I understand where the likelihood comes from. I do not understand the argument regarding $dP$ however. What is $dP$ ? Does the author mean $dP = d \times P$ or is $dP$ some infinitesimally small interval that has nothing to do with the parameter $P$ ? In case it's some super small interval, how come the joint probability is $P(X=x) \times dP$ ?
