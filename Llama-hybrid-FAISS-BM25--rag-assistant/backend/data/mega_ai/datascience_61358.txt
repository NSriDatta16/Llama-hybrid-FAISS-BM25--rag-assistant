[site]: datascience
[post_id]: 61358
[parent_id]: 
[tags]: 
ReLU for combating the problem of vanishing gradient in RNN?

For solving the problem of vanishing gradients in feedforward neural networks, ReLU activation function can be used. When we talk about solving the vanishing gradient problem in RNN, we use a more complex architecture (e.g. LSTM). In both of these, activation function is tanh. Can't we use ReLU instead of tanh in RNNs for solving vanishing gradients too rather than opting for a more complex architecture?
