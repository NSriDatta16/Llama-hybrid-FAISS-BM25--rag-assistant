. Some nations, including Russia, Iran, and China, have leveraged AI in their influence operations to tailor polarizing content and spread synthetic media. Authorities worldwide are trying to establish guardrails, with efforts including banning AI-generated voices in robocalls in the U.S, major tech companies signing a pact to prevent AI from disrupting elections, and the EU's AI Act imposing obligations for transparency, detection, and tracing of AI-generated material. Many states in the U.S. have introduced legislation requiring disclosure of AI use in election content. However, enforcing regulations is a significant hurdle, given that deepfakes are challenging to detect and source, and the technology is rapidly advancing. A comprehensive, multifaceted approach combining regulatory tools, technical solutions like watermarking and detection software, and public digital literacy initiatives is considered crucial to safeguard democratic elections AI influence and public trust in politics Artificial intelligence (AI) profoundly impacts public trust in politics by introducing significant risks. The use of AI in politics raises seriousethical and legal concerns. AI tools can process massive amounts of data to analyze user trends and behaviors, enabling highly targeted and persuasive campaign messages that can manipulate public opinion and damage the direct, original dimension of political communication. This phenomenon can lead to widespread deception and damage public trust in democratic institutions, as seen with AI-generated attack videos in political campaigns. The lack of a uniform and binding regulatory framework for AI further exacerbates concerns about privacy and security, and raises questions about accountability for false or biased outcomes produced by AI systems. Furthermore, AI systems are not neutral; they are embedded in social, political, cultural, and economic structures and designed to benefit existing dominant interests, often amplifying hierarchies and encoding narrow classifications. This means that AI systems can reproduce and intensify existing structural inequalities, particularly when used in sensitive areas like the justice system or welfare distribution. AI development often obscures its material and human costs, including energy consumption, labor exploitation, and mass data harvesting, further distancing the public from understanding its true impact. Despite the proliferation of AI ethics frameworks, many lack representation from the communities most affected, are often unenforceable, and may prioritize profit over ethical concerns, leading to a persistent asymmetry of power where technical systems extend inequality. This dynamic makes it challenging to build trust, as the public struggles to discern truth from AI-generated misinformation and holds those responsible for AI's negative consequences accountable. Policy regulations and AI To address the challenges posed by generative AI to democratic processes, many countries have taken a multifaceted approach. Many US states have created policies specifically targeting AI use in elections. The National Conference of State Legislatures has compiled a list of legislation regarding AI use by state as of 2024, some carrying both Criminal and Civil penalties. Critics of AI believe that regulatory and governance tools targeting deepfakes, AI-generated disinformation, and foreign interference are imperative. Some people believe that relying on self-regulation by tech companies is insufficient and that governments must enact robust policies to mitigate the creation and proliferation of synthetic content and hold corporations legally and financially accountable. Policymakers are considering AI content watermarking, though it faces technical challenges, and without robust legislation, companies are unlikely to prioritize such tools. Broader, harmonized standards across jurisdictions may be necessary for effective multilateral governance. The G7 has called on compan