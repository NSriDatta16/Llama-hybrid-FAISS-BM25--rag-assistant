[site]: crossvalidated
[post_id]: 243395
[parent_id]: 243384
[tags]: 
$\mathbb{E}_\pi(\cdot)$ usually denotes the expectation assuming the agent follows policy $\pi$. In this case $\pi(a|s)$ seems non-deterministic, i.e. returns the probability that the agent takes action $a$ when in state $s$. It looks like $r$, lower-case, is replacing $R_{t+1}$, a random variable. The second expectation replaces the infinite sum, to reflect the assumption that we continue to follow $\pi$ for all future $t$. $\sum_{s',r} r \cdot p(s′,r|s,a)$ is then the expected immediate reward on the next time step; The second expectation—which becomes $v_\pi$—is the expected value of the next state, weighted by the probability of winding up in state $s'$ having taken $a$ from $s$. Thus, the expectation accounts for the policy probability as well as the transition and reward functions, here expressed together as $p(s', r|s,a)$.
