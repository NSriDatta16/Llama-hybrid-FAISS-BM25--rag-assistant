nau (additive) attention Attention ( Q , K , V ) = softmax ( tanh ⁡ ( W Q Q + W K K ) V ) {\displaystyle {\text{Attention}}(Q,K,V)={\text{softmax}}(\tanh(W_{Q}Q+W_{K}K)V)} where W Q {\displaystyle W_{Q}} and W K {\displaystyle W_{K}} are learnable weight matrices. Luong attention (general) Attention ( Q , K , V ) = softmax ( Q W K T ) V {\displaystyle {\text{Attention}}(Q,K,V)={\text{softmax}}(QWK^{T})V} where W {\displaystyle W} is a learnable weight matrix. Self-attention Self-attention is essentially the same as cross-attention, except that query, key, and value vectors all come from the same model. Both encoder and decoder can use self-attention, but with subtle differences. For encoder self-attention, we can start with a simple encoder without self-attention, such as an "embedding layer", which simply converts each input word into a vector by a fixed lookup table. This gives a sequence of hidden vectors h 0 , h 1 , … {\displaystyle h_{0},h_{1},\dots } . These can then be applied to a dot-product attention mechanism, to obtain h 0 ′ = A t t e n t i o n ( h 0 W Q , H W K , H W V ) h 1 ′ = A t t e n t i o n ( h 1 W Q , H W K , H W V ) ⋯ {\displaystyle {\begin{aligned}h_{0}'&=\mathrm {Attention} (h_{0}W^{Q},HW^{K},HW^{V})\\h_{1}'&=\mathrm {Attention} (h_{1}W^{Q},HW^{K},HW^{V})\\&\cdots \end{aligned}}} or more succinctly, H ′ = A t t e n t i o n ( H W Q , H W K , H W V ) {\displaystyle H'=\mathrm {Attention} (HW^{Q},HW^{K},HW^{V})} . This can be applied repeatedly, to obtain a multilayered encoder. This is the "encoder self-attention", sometimes called the "all-to-all attention", as the vector at every position can attend to every other. Masking For decoder self-attention, all-to-all attention is inappropriate, because during the autoregressive decoding process, the decoder cannot attend to future outputs that has yet to be decoded. This can be solved by forcing the attention weights w i j = 0 {\displaystyle w_{ij}=0} for all i < j {\displaystyle i<j} , called "causal masking". This attention mechanism is the "causally masked self-attention". See also Recurrent neural network seq2seq Transformer (deep learning architecture) Attention Dynamic neural network References External links Olah, Chris; Carter, Shan (September 8, 2016). "Attention and Augmented Recurrent Neural Networks". Distill. 1 (9). Distill Working Group. doi:10.23915/distill.00001. Dan Jurafsky and James H. Martin (2022). Speech and Language Processing (3rd ed. draft, January 2022) — Chapter 10.4 (Attention) and Chapter 9.7 (Self-Attention Networks: Transformers) Alex Graves (2020). Attention and Memory in Deep Learning — video lecture from DeepMind / UCL