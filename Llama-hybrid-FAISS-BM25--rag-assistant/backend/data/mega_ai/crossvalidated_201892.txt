[site]: crossvalidated
[post_id]: 201892
[parent_id]: 201885
[tags]: 
I used both for a cox regression Ok, before I start talking about R squared values, I'll ask you to check some stuff. Are your sample sizes the same for both regressions? I suspect that you may have lost some more participants in the factor analysis due to preprocessing while trying to consider censoring. Thats the first thing I'd check. I have two models, an original one with many variables, and a new one where I extracted 5 factors...the 5 factors ....performs significantly worse. The R squared value for Cox isn't the same as for OLS regression, but for simplicity I'm going to write as if it is. Cox regressions usually use a likelihood ratio statistic (LRT) based R squared calculation, so keep that in mind. Here's the takeaway: You can only lose information when you reduce your variables with factor analysis or PCA. Try and figure out how much variance you're explaining with the first 5 factors. It's possible that you're only capturing ~65% of your original variance. If that's the case, then you can simply extract more factors until you're closer to ~80% or ~90%. Or simply until your R squared values looks better. So, why use factor analysis? Variable reduction can help you to reduce multicollinearity, model complexity (via degrees of freedom) and even processing power. However, that probably won't help your R squared values. Remember that R squared is a measurement of how much variance you're explaining in your y (dependent) variable. When you include 100% of your variables, you (usually) have the best chance of explaining that y-variable (ie: have the highest R squared). If you're reducing your variables and extracting factors that only capture some of that variance, then you're essentially reducing the amount of variance you could explain. Here's an illustration of what I mean: 100% of x-variables --> explains 35% of variance in y. 5 factors w/ 65% of original variance --> explains 25% of variance in y.
