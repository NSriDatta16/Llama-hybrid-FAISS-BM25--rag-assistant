[site]: crossvalidated
[post_id]: 474692
[parent_id]: 473569
[tags]: 
First, you need to be very cautious about using an automated way to select models. The very name "dredge" has negative connotations among statisticians.This is generally not a good idea , for many reasons. Even the package author cautions against it in a Note at the end of the manual page for dredge() : Users should keep in mind the hazards that a “thoughtless approach” of evaluating all possible models poses. Although this procedure is in certain cases useful and justified, it may result in selecting a spurious “best” model, due to the model selection bias. “Let the computer find out” is a poor strategy and usually reflects the fact that the researcher did not bother to think clearly about the problem of interest and its scientific setting (Burnham and Anderson, 2002). Although the model-averaging approach has advantages over choosing the one best model from a set of models, it still runs a risk of resulting in a model that depends too much on the quirks of the particular data set at hand. If you are going to use this approach, consider repeating the analysis on multiple bootstrapped samples of the data set to see how stable the results of the model selection and averaging are. The answer to your question is that the p-values and importances reported by this package represent different types of averages. Weights are assigned to each of the selected models (e.g., via dredge() ), with the default I understand to be weighting by the relative likelihoods of the models versus the best model, as assessed by the Akaike Information Criterion . You now have a list of models, each with its included predictors variables and included interaction terms, and with its own model weight. In a full model average as you used, the coefficient value for an individual predictor or interaction term is set to 0 in all the models from which it is omitted. Then for each predictor and interaction term, all its coefficient values and associated coefficient variances are combined in an average weighted by the corresponding model weight. The confidence intervals and p-values for each weighted-averaged coefficient are based on a normal distribution.* The function importance() is another name for the sw() function, which reports the "Sum of model weights over all models including each explanatory variable," according to the manual page. From your output it seems to normalize this sum of weights by the sum of all model weights. So this is a different type of average: for each predictor or interaction, it's based on the weights of all the models that happened to include it regardless of the predictor's significance in the model. It seems that all the "significant" predictors and interactions in the averaged model had importance of 1 on that latter basis, but there are several with high importance but high p-values in the averaged model. It would seem that those terms were included in all the models retained by dredge() even though they were typically not significant on their own in many of them. It's possible that their importance has to do with their helping to bring out the significance of other predictors. You would have to examine the individual models and weights to say anything more specific. Some evaluation of why some "insignificant" predictors nevertheless were apparently retained in all the models might say something interesting about the nature of what you're examining. As to what is best to report, why not report both? They represent different things. I would further recommend reporting at least the number of models that were retained versus the number that were evaluated by dredge() , and report in your methods the criteria by which models were selected and averaged. Although reading the software manual might make that clear, your readers should be able to get a general idea of the approach from your own presentation. *Details can be found in the R code, in particular for the par.avg() function.
