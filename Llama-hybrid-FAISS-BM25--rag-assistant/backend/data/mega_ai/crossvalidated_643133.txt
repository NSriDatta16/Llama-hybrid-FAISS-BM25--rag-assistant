[site]: crossvalidated
[post_id]: 643133
[parent_id]: 643130
[tags]: 
When you evaluate feature elimination strategies such as "remove features by importance until there are X features left" through cross-validation, X is a hyper-parameter of the training process. X is simply the hyper-parameter value that gives the best performance on the cross-validation set in terms of the metric you have chosen. It is likely that with much more data, the value of "optimal" X would be higher (less risk of overfitting), and with less data, the "optimal" value of X would likely be lower. This statement also assumes that you "appropriately" jointly optimized X together with all other hyper-parameters (e.g. number of iterations, tree depth, how much data to have in each leaf node, what proportion of the features or data to sub-sample for each tree, and so on), because otherwise it might just be that "with other hyperparameters set to the following values, X is the optimal choice, but something else might be better if only we optimized the other hyper-parameters for that value of X". It might even turn out that you can get approximately the same cross-validated performance for different values of X as long as some other hyper-parameters change appropriately. Does that mean that there are only X truly important variables? Some are likely more important than others. E.g. going to X=4 might only very slightly reduce cross-validated performance, and if that translates to a test set, you could argue that having 5 variables improves performance, but not by all that much. There are likely also additional variables being omitted that have at least some importance (every real process is pretty complex and you likely wouldn't have provided the features to the model if it were completely obvious that they cannot matter), but you don't have enough data (or not enough data in the right part of the distribution) to know. In fact, you are not necessarily selecting the same 5 variables on all cross-validation folds (after all, X=5 is just a particular hyper-parameter choice that says to select 5 variables, but the CV splits might turn out by chance so that different ones get selected), so you're likely not stably selecting the same variables and what they do on each CV split (perhaps explorable through their SHAP explanations) may vary, too. The level of evidence (CV score improves at least a little) may also not be that strong with respect to X=4 vs. 5 vs. 6 vs. 3 vs. etc., so X=5 just happens to produce the best CV-score. I guess you could look at a confidence interval for X=5 vs. 4 vs. 6 to see how strong the evidence is, but that's not terribly robust/compelling because the point estimates & CIs will be biased, because you're evaluating these values of X (and the other hyperparameters you jointly optimized) on the same data you're evaluating them on. So, it's rather that when you look at the CI as if they were from completely new independent evaluations and the story wouldn't even be clear if you had done that, then it's even less clear when you've got this additional data-driven selection of the question. That the same number of features is a good hyper-parameter choice for XGBoost and LightGBM is not at all surprising, because they are extremely similar. Random Forrest is a little bit different (instead of building new trees on the residuals of the previous ones, you build independent ones and average), but they are all tree-based algorithms so likely benefit from the same types of features and likely giving more correlated predictions than predictions from very different models would (see e.g. here Figure 5 in this paper for an illustration of that). In contrast, a very different model e.g. neural network for tabular data, SVM with RBF kernel, an elastic-net regularized linear model with splines etc. might use different features/different numbers of features.
