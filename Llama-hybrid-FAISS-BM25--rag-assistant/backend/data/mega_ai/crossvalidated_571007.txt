[site]: crossvalidated
[post_id]: 571007
[parent_id]: 
[tags]: 
Likelihood and cross-entropy: continuous case

I think it's pretty clear to me that average log-likelihood is equivalent to negative cross-entropy for discrete distributions, as shown here: $$\frac{1}{N}\log\mathcal{L}(\theta) = \frac{1}{N}\log \prod_i q(x_i|\theta)^{Np(x_i)} = \sum_i p(x_i)\log q(x_i|\theta) = -H(p, q)$$ where $N$ is the number of samples, $q(x_i|\theta)$ is sample probability given model parameters, and $p(x_i)$ is the probability of a sample (source: wikipedia). My question is whether a similar equivalence holds for continuous distributions? For instance, say we have access to the data-generating continuous distribution, such that $x_i \sim p(X)$ , and we have the log-likelihood function $\sum^N_i \log q(x_i | \theta)$ for $N$ data samples. Given that we know $p(X)$ and taking the limit of $N$ to infinity, could we somehow relate log-likelihood to the cross entropy integral, $\int p(x) q(x|\theta) dx$ ? $$\lim_{N \to \infty} \frac{1}{N}\sum^N_i \log q(x_i|\theta) = E_{p(x)}[\log q(x|\theta)]?$$
