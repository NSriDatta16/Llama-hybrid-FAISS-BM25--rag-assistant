[site]: datascience
[post_id]: 18201
[parent_id]: 17216
[tags]: 
PCA reduces dimensionality. It does not change the number of observations you have. Nor does it change the order of the data. The n-th observation in your original dataset will still be the n-th observation post-PCA. Choosing the number of components in PCA and choosing the number of clusters in K-Means are independent of each other. Both K-Means and PCA seek to "simplify/summarize" the data, but their mechanisms are deeply different. PCA looks to find a low-dimensional representation of the observation that explains a good fraction of the variance. K-Means looks to find homogeneous subgroups among the observations. For PCA, the optimal number of components is determined visually through the scree plot or mathematically using Kaiser's criterion (drop all components with eigenvalue Screeplot (source) It does not make much sense to go beyond the 4th component. Law of diminishing marginal returns. For K-Means: Since increasing the number of clusters will always reduce the distance from centroid to data points, increasing K will always decrease this metric, to the extreme of reaching zero when K is the same as the number of data points. If we plot the average within cluster distance to centroids vs number of clusters, we will find the "elbow" to be a good place to stop. source
