[site]: crossvalidated
[post_id]: 118369
[parent_id]: 118363
[tags]: 
Your conclusion is correct. Think of two aspects: Statistical power to detect an effect. Unless the power is very high, one can miss even large real effects. Reliability: having a high probability of finding the right (true) features. There are at least 4 major considerations: Is the method reproducible by you using the same dataset? Is the method reproducible by others using the same dataset? Are the results reproducible using other datasets? Is the result reliable? When one desires to do more than prediction but to actually draw conclusions about which features are important in predicting the outcome, 3. and 4. are crucial. You have addressed 3. (and for this purpose, 100 bootstraps is sufficient), but in addition to individual feature inclusion fractions we need to know the average absolute 'distance' between a bootstrap feature set and the original selected feature set. For example, what is the average number of features detected from the whole sample that were found in the bootstrap sample? What is the average number of features selected from a bootstrap sample that were found in the original analysis? What is the proportion of times that a bootstrap found an exact match to the original feature set? What is the proportion that a bootstrap was within one feature of agreeing exactly with the original? Two features? It would not be appropriate to say that any cutoff should be used in making an overall conclusion. Regarding part 4., none of this addresses the reliability of the process, i.e., how close the feature set is to the 'true' feature set. To address that, you might do a Monte-Carlo re-simulation study where you take the original sample lasso result as the 'truth' and simulate new response vectors several hundred times using some assumed error structure. For each re-simulation you run the lasso on the original whole predictor matrix and the new response vector, and determine how close the selected lasso feature set is to the truth that you simulated from. Re-simulation conditions on the entire set of candidate predictors and uses coefficient estimates from the initially fitted model (and in the lasso case, the set of selected predictors) as a convenient 'truth' to simulate from. By using the original predictors one automatically gets a reasonable set of co-linearities built into the Monte Carlo simulation. To simulate new realizations of $Y$ given the original $X$ matrix and now true regression coefficients, one can use the residual variance and assume normality with mean zero, or to be even more empirical, save all the residuals from the original fit and take a bootstrap sample from them to add residuals to the known linear predictor $X\beta$ for each simulation. Then the original modeling process is run from scratch (including selection of the optimum penalty) and a new model is developed. For each of 100 or so iterations compare the new model to the true model you are simulating from. Again, this is a good check on the reliability of the process -- the ability to find the 'true' features and to get good estimates of $\beta$. When $Y$ is binary, instead of dealing with residuals, re-simulation involves computing the linear predictor $X\beta$ from the original fit (e.g., using the lasso), taking the logistic transformation, and generating for each Monte Carlo simulation a new $Y$ vector to fit afresh. In R one can say for example lp
