[site]: crossvalidated
[post_id]: 449059
[parent_id]: 
[tags]: 
Initial loss for a multi-label prediction problem

I have seen this: https://datascience.stackexchange.com/questions/18991/is-there-a-rule-of-thumb-for-the-initial-value-of-loss-function-in-a-cnn Since my question is more mathematical in nature, I have posted it here. My question is: how can I know what my initial loss should be in, say, the following situation: 2 possible binary labels per training example Binary crossentropy loss function sigmoid output layer (i.e. we are not looking for a single correct label - this is a multilabel setting) I know the formula for working with binary crossentropy. Here's what I have so far: I'm assuming that the following is true: for multilabel binary classification, the binary crossentropy loss can be written as follows: $$H_p(q) =-\frac{1}{MN} \sum_{i=1}^{M} \sum_{j=1}^{N} y_{ij} log p(y_{ij}) + (1 - y_{ij})log(1-p(y_{ij}))$$ where $log$ is the natural logarithm; $M$ is the number of labels, and $N$ is the number of (training) examples. Let's suppose that $M=2$ and $N=1$ , that is we have 2 possible binary labels per example and we are working with only one training example. Then: $$H_p(q) = -\frac{1}{(2)(1)} \left[y_{11}logp(y_{11}) + (1-y_{11})log(1-p(y_{11})) \right] + \left[ y_{21}logp(y_{21}) + (1-y_{21})log(1-p(y_{21})) \right]$$ Let's assume that a priori, the probability of correctly guessing a label is 0.5 (there are two possible values that each label could be: 0 or 1, since we have binary labels). So this becomes: $$H_p(q) = -\frac{1}{2} \left[1log(0.5) + (1-1)log(1-0.5) \right] + \left[ 1log(0.5) + (1-1)log(1-0.5) \right]$$ $$H_p(q) = -\frac{1}{2} (log(0.5) + log(0.5))$$ $$H_p(q) = -log(0.5)$$ In general, it doesn't matter what our value of $N$ or $M$ is, we could expect an initial loss of: $$H_p(q) = -log(0.5)$$ My question is: is this correct?
