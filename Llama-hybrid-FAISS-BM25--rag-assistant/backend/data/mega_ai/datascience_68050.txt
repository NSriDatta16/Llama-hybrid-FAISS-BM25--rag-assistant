[site]: datascience
[post_id]: 68050
[parent_id]: 37396
[tags]: 
Autoencoders are used to reduce the dimensionality of the feature space. They can capture nonlinearities that other dimensionality reduction teqniques like PCA can not. Autoencoders are build by training the model to reproduce the input. In this case you can split the data set into three: training cross validation testing Train you model using the training set, check the performance by looking at your loss for the cross validation set, make some changes, and repeat. After you have a model you are confident in, then using the test data set as the final word in its performance. I think the main point is that your loss should have something to do with the difference between an element of the data set, and the encoded-then-decoded version of that element. Therefore you cross validate against a separate chunk of data as you tune the hyper parameters of the encoder.
