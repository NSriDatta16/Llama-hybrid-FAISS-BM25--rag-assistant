[site]: crossvalidated
[post_id]: 602133
[parent_id]: 
[tags]: 
Justification of the fixed variational distribution in diffusion models

Diffusion models can be regarded as latent variable models ( Ho et al. , 2020 ; Section 2), with the latents being an hierarchical chain of random variables $z_T → \dots → z_t → z_{t-1} → \dots → z_1$ (finally, from $z_1$ we sample the observation $x$ ). Under this view the diffusion process (the noise-adding steps, $\dots ← z_t ← z_{t-1} ← \cdots$ ) defines an approximate posterior distribution $q(z_{1:T}|x)$ . However, this distribution is fixed (in contrast to typical latent variable models, such as variational autoencoders, which attempt to learn the approximate posterior). So I was wondering why does this approach work? In my mind, a learnable and flexible variational distribution is essential in ensuring that the evidence lower bound (ELBO) is a tight lower-bound of the marginal log-likelihood $\log p(x)$ (for example in the EM algorithm, the E step closes the gap by setting $q(z|x)$ to $p(z|x)$ ). Otherwise, if the ELBO is loose do we have any guarantee that we maximize the true marginal log-likelihood $\log p(x)$ ?
