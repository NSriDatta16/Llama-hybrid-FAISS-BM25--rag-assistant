[site]: datascience
[post_id]: 87517
[parent_id]: 
[tags]: 
Tensor Backpropagation

I tried to make simple neural network layers as the following, including forward and backward propagation. Here is my reference . Firstly I assume an one layer FC: $Y = X \cdot W + B$ X is input, which size is (N, D), N=batch-size, D=features; W is weight, which size is (D, M), D=features, M=next-layer features then I stack it as two layer FC: $Y_1 = X_1 \cdot W_1 + B_1$ $Y_2 = Y_1 \cdot W_2 + B_2$ Backpropagation: Gradient w.r.t. $W_1$ : $\frac{\partial Y_2}{\partial W_1} = \frac{\partial Y_2}{\partial Y_1} \frac{\partial Y_1}{\partial W_1} = X_1^T \cdot W_2^T$ ( I move $w_2$ to the right side to follow the order in forward prop; also, I perform transpose to sum the features along batch axis.) Gradient w.r.t. $W_2$ : $\frac{\partial Y_2}{\partial W_2} = Y_1^T$ The Problem: I used the following snippet to verify above equations, but the results from Tensorflow are not the same as my assumption, specifically the gradients of $W_1$ and $W_2$ . Which part am I wrong? import tensorflow as tf tf.executing_eagerly() tf_batch_x = tf.constant(x_batch_array) l1_tf_batch_weight = tf.Variable(l1_weight_array) l1_tf_batch_bias = tf.Variable(l1_bias_batch_array) l2_tf_batch_weight = tf.Variable(l2_weight_array) l2_tf_batch_bias = tf.Variable(l2_bias_batch_array) with tf.GradientTape(persistent=True) as tape: tf_out = tf.matmul( tf_batch_x, l1_tf_batch_weight) + l1_tf_batch_bias #tf_out = tf.nn.relu(tf_out) tf_out = tf.matmul( tf_out, l2_tf_batch_weight) + l2_tf_batch_bias #tf_out = tf.nn.relu(tf_out) tf_grad = tape.gradient(tf_out, [l1_tf_batch_weight, l1_tf_batch_bias, l2_tf_batch_weight, l2_tf_batch_bias]) print("tf_out", tf_out) print("tf_grad", tf_grad)
