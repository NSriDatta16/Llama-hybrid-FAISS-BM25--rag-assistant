[site]: crossvalidated
[post_id]: 1288
[parent_id]: 165
[tags]: 
So there are plenty of answers here paraphrased from statistics/probability textbooks, Wikipedia, etc. I believe we have "laypersons" where I work; I think they are in the marketing department. If I ever have to explain anything technical to them, I apply the rule "show don't tell." With that rule in mind, I would probably show them something like this. The idea here is to try to code an algorithm that I can teach to spell--not by learning all of the hundreds (thousands?) of rules like When adding an ending to a word that ends with a silent e, drop the final e if the ending begins with a vowel . One reason that won't work is I don't know those rules (i'm not even sure the one I just recited is correct). Instead I am going to teach it to spell by showing it a bunch of correctly spelled words and letting it extract the rules from those words, which is more or less the essence of Machine Learning, regardless of the algorithm--pattern extraction and pattern recognition. The success criterion is correctly spelling a word the algorithm has never seen before (i realize that can happen by pure chance, but that won't occur to the marketing guys, so i'll ignore--plus I am going to have the algorithm attempt to spell not one word, but a lot, so it's not likely we'll be deceived by a few lucky guesses). An hour or so ago, I downloaded (as a plain text file) from the excellent Project Gutenberg Site, the Herman Hesse novel Siddhartha . I'll use the words in this novel to teach the algorithm how to spell. So I coded the algorithm below that scanned this novel, three letters at a time (each word has one additional character at the end, which is 'whitespace', or the end of the word). Three-letter sequences can tell you a lot--for instance, the letter 'q' is nearly always followed by 'u'; the sequence 'ty' usually occurs at the end of a word; z rarely does, and so forth. (Note: I could just as easily have fed it entire words in order to train it to speak in complete sentences--exactly the same idea, just a few tweaks to the code.) None of this involves MCMC though, that happens after training, when we give the algorithm a few random letters (as a seed) and it begins forming 'words'. How does the algorithm build words? Imagine that it has the block 'qua'; what letter does it add next? During training, the algorithm constructed a massive l*etter-sequence frequency matrix* from all of the thousands of words in the novel. Somewhere in that matrix is the three-letter block 'qua' and the frequencies for the characters that could follow the sequence. The algorithm selects a letter based on those frequencies that could possibly follow it. So the letter that the algorithm selects next depends on--and solely on--the last three in its word-construction queue. So that's a Markov Chain Monte Carlo algorithm. I think perhaps the best way to illustrate how it works is to show the results based on different levels of training. Training level is varied by changing the number of passes the algorithm makes though the novel--the more passes thorugh the greater the fidelity of its letter-sequence frequency matrices. Below are the results--in the form of 100-character strings output by the algorithm--after training on the novel 'Siddharta'. A single pass through the novel, Siddhartha : then whoicks ger wiff all mothany stand ar you livid theartim mudded sullintionexpraid his sible his (Straight away, it's learned to speak almost perfect Welsh; I hadn't expected that.) After two passes through the novel: the ack wor prenskinith show wass an twor seened th notheady theatin land rhatingle was the ov there After 10 passes: despite but the should pray with ack now have water her dog lever pain feet each not the weak memory And here's the code (in Python, i'm nearly certain that this could be done in R using an MCMC package, of which there are several, in just 3-4 lines) def create_words_string(raw_string) : """ in case I wanted to use training data in sentence/paragraph form; this function will parse a raw text string into a nice list of words; filtering: keep only words having more than 3 letters and remove punctuation, etc. """ pattern = r'\b[A-Za-z]{3,}\b' pat_obj = re.compile(pattern) words = [ word.lower() for word in pat_obj.findall(raw_string) ] pattern = r'\b[vixlm]+\b' pat_obj = re.compile(pattern) return " ".join([ word for word in words if not pat_obj.search(word) ]) def create_markov_dict(words_string): # initialize variables wb1, wb2, wb3 = " ", " ", " " l1, l2, l3 = wb1, wb2, wb3 dx = {} for ch in words_string : dx.setdefault( (l1, l2, l3), [] ).append(ch) l1, l2, l3 = l2, l3, ch return dx def generate_newtext(markov_dict) : simulated_text = "" l1, l2, l3 = " ", " ", " " for c in range(100) : next_letter = sample( markov_dict[(l1, l2, l3)], 1)[0] simulated_text += next_letter l1, l2, l3 = l2, l3, next_letter return simulated_text if __name__=="__main__" : # n = number of passes through the training text n = 1 q1 = create_words_string(n * raw_str) q2 = create_markov_dict(q1) q3 = generate_newtext(q2) print(q3)
