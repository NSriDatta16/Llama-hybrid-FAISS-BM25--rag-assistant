[site]: datascience
[post_id]: 46173
[parent_id]: 
[tags]: 
GTX 1660 Ti vs. RTX 2060 for a deep learning pc

I'm planning to build a PC for Deep Learning, after the launch of AMD Ryzen 3rd gen processors. My budget will allow me to choose between a GTX 1660 Ti and RTX 2060. I'm tabulating the specs of both the cards below. ╔════════════════╦════════════╦══════════╗ ║ GPU ║ GTX-1660Ti ║ RTX-2060 ║ ╠════════════════╬════════════╬══════════╣ ║ GPU-cores ║ 1536 ║ 1920 ║ ║ Tensor-cores ║ N/A ║ 240 ║ ║ RT-cores ║ N/A ║ 30 ║ ║ Texture-units ║ 120 ║ 120 ║ ║ Render-outputs ║ 48 ║ 48 ║ ║ Base-clock ║ 1500 ║ 1365 ║ ║ Boost-clock ║ 1770 ║ 1680 ║ ║ Mem-bus-width ║ 192-bit ║ 192-bit ║ ║ Mem-bandwidth ║ 288GB/s ║ 336GB/s ║ ║ Mem-capacity ║ 6GB ║ 6GB ║ ║ TDP ║ 120W ║ 160W ║ ╚════════════════╩════════════╩══════════╝ Given this comparison, I'd like to know which one would be a better choice for me. Would the RTX-2060 be worth the extra 70$ for the supposedly better performance? I had an initial look at the usefulness of tensor cores in Deep Learning applications, and found out the following article snippet from Nvidia website. Note About Tensor Cores Designed specifically for deep learning, Tensor Cores on newer GPUs such as Tesla V100 and Titan V, deliver significantly higher training and inference performance compared to full precision (FP32) training. Each Tensor Core provides matrix multiply in half precision (FP16), and accumulating results in full precision (FP32). This key capability enables Volta to deliver 3X performance speedups in training and inference over the previous generation. All samples are optimized to take advantage of Tensor Cores and have been tested for accuracy and convergence.
