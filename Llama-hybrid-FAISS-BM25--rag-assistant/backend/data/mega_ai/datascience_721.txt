[site]: datascience
[post_id]: 721
[parent_id]: 658
[tags]: 
When it comes to dealing with many disparate kinds of data, especially when the relationships between them are unclear, I would strongly recommend a technique based on decision trees , the most popular ones today to the best of my knowledge are random forest , and extremely randomized trees . Both have implementations in sklearn , and they are pretty straightforward to use. At a very high level, the reason that a decision tree -based approach is advantageous for multiple disparate kinds of data is because decision trees are largely independent from the specific data they are dealing with, just so long as they are capable of understanding your representation. You'll still have to fit your data into a feature vector of some kind, but based on your example that seems like a pretty straightforward task, and if you're willing to go a little deeper on your implementation you could certainly come up with a custom tree-splitting rule without actually having to change anything in the underlying algorithm. The original paper is a pretty decent place to start if you want to give that a shot. If you want pseudo-structural data from your text data though, I might suggest looking into doc2vec , recently developed by Google. I don't think there are any good open-source implementations now, but it's a pretty straightforward improvement on the word2vec algorithm, which has implementations in at least C and python . Hope that helps! Let me know if you've got any more questions.
