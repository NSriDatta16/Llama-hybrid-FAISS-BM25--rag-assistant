[site]: crossvalidated
[post_id]: 635861
[parent_id]: 635852
[tags]: 
You are correct with your intuition that generally errors compound with time for autoregressive models. This is also true for lstms. This has nothing to do with maintaining a hidden state. What you're doing in training is teaching the model to interpolate the next value from the past. This interpolation introduces error. In the next state you're predicting on an erroneous value. Now what are the chances that when you predict from the wrong values you'll predict the correct result? Pretty low generally speaking. In much simpler models a proof that the norm of the error decreases can be derived, but not for neural networks generally. You can, for example, look at the result of using an LSTM on tsla stock here: https://towardsdatascience.com/lstm-time-series-forecasting-predicting-stock-prices-using-an-lstm-model-6223e9644a2f Where it's super easy to visualize the compounding error effect.
