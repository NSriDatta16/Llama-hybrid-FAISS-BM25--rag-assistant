[site]: datascience
[post_id]: 115052
[parent_id]: 
[tags]: 
Semantic similarity on a large dataset

I'm going through this guide on semantic similarity and use the code there as is. I'm applying it to a dataset where each row is typically a paragraph (3-4 sentences, over 100 words). Currently, I have over 100k observations, but this number is likely to grow to 500k. I want to measure semantic similarity between all rows. When I test BoW and TFIDF on around 20-30k sample, I don't get any performance issues (even without cleaning, stopwords, etc.). When I try Word2Vec/Universal Sentence Encoder, however, it takes couple of hours to finish even on 3-4k rows sample . I also get completely different results, but that's beyond the point. Is there a way to improve the performance for Word2Vec/Universal Sentence Encoder, especially the latter. (As far as I understand, in Word2Vec, words "good" and "bad" may cancel each other out , which is not good for my speach-like data.)
