[site]: crossvalidated
[post_id]: 415586
[parent_id]: 415570
[tags]: 
Yes, it is possible and in general there is nothing wrong with it. You just want to treat your test split as validation and wait for a real test set. But, note that the answer might also depend on the choice of ML model. In many models, this won't be a problem; especially in simple models like decision trees. You've already found the best hyper-parameters, or feature set to move on. On the contrary, in neural networks, you'll always need a validation set apart from your training to decide on early-stopping. Back-propagation algorithm needs to run an indefinite number of iterations, that is typically chosen via a validation set. Here, you wouldn't be able to train with all the data you have.
