[site]: crossvalidated
[post_id]: 241012
[parent_id]: 
[tags]: 
When is it One-shot learning?

The Wikipedia Article on one-shot learning described it as Whereas most machine learning based object categorization algorithms require training on hundreds or thousands of images and very large datasets, one-shot learning aims to learn information about object categories from one, or only a few, training images I don't think this is a particularly good definition, since it is very computer vision focused, but maybe I am wrong. I have a method, which I would like to describe as oneshot learning. But I am not sure if it is the the right term. We are not doing computer vision at all. We are learning feature vectors for classes -- think something like a bottle-knecked deep neural network (It is not that). Other methods for learning these feature vectors use many (hundred, thousands) of labelled examples of each class -- sometimes after a unsupervised pretraining step, sometimes not. We make use of particular structure in the problem/feature vector to instead only need a single labelled example of each class, after using a unsupervised pretraining step. but we don't use any existing machine learning method to do so. Infact it is arguable if our method is machine learning at all. The unsupervised part certainly is, but the final step, with the label data ... well it doesn't have any direct optimization step which I have come to expect in a machine learning process (No EM, or Gradient descent). Effectively it is a informed kind of averaging, that works because of the structure of the representation/feature vector. Is it still appropriate to call this one-shot learning?
