[site]: crossvalidated
[post_id]: 234024
[parent_id]: 
[tags]: 
Question with Matrix Derivative: Why do I have to transpose?

In the equation for Recurrent Neural Networks: $$ h_t = \tanh(h_{t-1}W_{hh} + x_tW_{xh} + b) $$ Where $h_t$ is of size (N,H) Where $W_{hh}$ is of size (H,H) Where $W_{xh}$ is of size (D,H) Where $x_t$ is of size (N,D) Where $b$ is of size (N,H) I am trying to find the derivative with respect to $h_{t-1}$ Applying chain rule, the result is: $$ \frac{\partial h_t}{\partial h_{t-1}} = [1-\tanh^2(h_{t-1}W_{hh} + x_tW_{xh} + b)]W_{hh} $$ (Where $1-\tanh^2$ is the derivate of the $\tanh$ function) However, the correct result requires me to transpose $W_{hh}$ as follows: $$ \frac{\partial h_t}{\partial h_{t-1}} = [1-\tanh^2(h_{t-1}W_{hh} + x_tW_{xh} + b)]W_{hh}^T $$ I am at a loss of why I need to transpose to get the correct partial ... can someone provide some intuition here? Any guidance will be greatly appreciated! Thank you!
