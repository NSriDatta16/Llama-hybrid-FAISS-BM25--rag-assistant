[site]: datascience
[post_id]: 74918
[parent_id]: 
[tags]: 
How to use fine tuning of BERT when i have unlabelled dataset of text documents?

I have gained a basic understanding of using BERT for various NLP/text mining tasks. When it comes to fine-tuning of BERT, I always see that fine-tuning is performed using some classification tasks. So, how should I refine the word/sentence embeddings vector given by the BERT model in the case when I have a set completely unlabelled set of documents? I'm aware that the BERT model is originally trained on unlabelled data, so there must be some way.
