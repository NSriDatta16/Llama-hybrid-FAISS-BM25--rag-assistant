[site]: crossvalidated
[post_id]: 441311
[parent_id]: 441162
[tags]: 
First, given the data you plotted, your groups look to have quite different means on your outcome. If the rest of your data is similar, then I can guarantee that a model that somehow accounts for groups is going to be a better fit to your data than one that does not do so. You have a couple of options with 15 groups: Fit a multilevel model that allows each group to have its own intercept, but that imposes a distribution on the values of the intercepts (normal, mean of 0 with an estimated variance). In so doing, this is also referred to as a random intercept model (or a partial pooling model). This is handy if you have different numbers of observations for each group as the intercept for groups with fewer data points will be pulled back toward the overall average outcome for the entire dataset. Since you have only 15 groups, you want to make sure and estimate your model using REML (restricted maximum likelihood), which is the default in lme4() . You could enter your grouping variable as a factor in a OLS model. This is sometimes called a fixed effect model (or no-pooling model). This model also estimates an intercept for each group, but does not adjust the intercept based on the amount of data for a given group. Hypothetically, if a group had only one data point, the value of that group's intercept in this model would be that data point. In contrast, the multilevel model would almost certainly pull that group's intercept value back to the mean value on the outcome in the data. In terms of a formal test of whether the partial pooling (multilevel) model is a better fit to your data than an OLS model without accounting for groups, I will edit this post with some R code for determining that. Edit based on updated original post: Testing whether the partial pooling model is a better fit to the data than the complete pooling or no pooling models requires you to use the exactLRT() function in the RLRsim() package. Assuming you called the complete pooling model cpm the no pooling model npm , and the partial pooling model ppm , the code is as such: # need to use full maximum likelihood on your partial pooling model for exactLRT() to work - I had suggested using restricted ML b/c of sample size ppmA A significant test would indicate that the less parsimonious model is a better fit for the data. In your case, the ppm has more parameters than the cpm while the npm has more parameters than the ppm. I would expect the ppm to win out here, but will be interested to know what you find. The anova() command works for comparing nested lmer() models. Note that when using REML, you can only use likelihood ratio testing to compare models with different random effects specifications.
