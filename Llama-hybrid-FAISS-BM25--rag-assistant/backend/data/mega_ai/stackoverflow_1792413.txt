[site]: stackoverflow
[post_id]: 1792413
[parent_id]: 1792372
[tags]: 
I would recommend using a html parsing library to simplify everything. Namely something like Simple HTML DOM . Using Simple HTML DOM: $html = file_get_html('http://www.google.com/'); foreach($html->find('img') as $element){ //download image } For download files (and html) I would recommend using a HTTP wrapper such as curl, as it allows far more control over using file_get_contents. However, if you wanted to use file_get_contents, there are some good examples on the php site of how to get URLs. The more complex method allows you to specify the headers, which could be useful if you wanted to set the User Agent. (If you are scraping other sites a lot, it is good to have a custom user agent as you can use it to let website admin your site or point of contact if you are using too much bandwidth, which is better than the admin blocking your IP address). $opts = array( 'http'=>array( 'method'=>"GET", 'header'=>"Accept-language: en\r\n" ) ); $context = stream_context_create($opts); $file = file_get_contents('http://www.example.com/', false, $context); Although of course it can be done simply by: $file = file_get_contents('http://www.example.com/');
