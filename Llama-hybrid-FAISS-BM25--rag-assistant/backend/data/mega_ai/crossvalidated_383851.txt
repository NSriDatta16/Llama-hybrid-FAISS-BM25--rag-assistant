[site]: crossvalidated
[post_id]: 383851
[parent_id]: 383731
[tags]: 
On the machine learning side: In machine learning, you usually try to maximize $p(y|x)$ , where $x$ is the target, and $y$ is the input (for example, x could be some random noise, and y would be an image). Now, how do we optimize this? A common way to do it, is to assume, that $p(y|x) = N(y|\mu(x), \sigma)$ . If we assume this, it leads to the mean squared error. Note, we assumed as form for $p(y|x)$ . However, if we dont assume any certain distribution, it is called likelihood-free learning. Why do GANs fall under this? Well, the Loss function is a neural network, and this neural network is not fixed, but learned jointly. Therefore, we dont assume any form anymore (except, that $p(y|x)$ falls in the family of distributions, that can be represented by the discriminator, but for theory sake we say it is a universal function approximator anyway).
