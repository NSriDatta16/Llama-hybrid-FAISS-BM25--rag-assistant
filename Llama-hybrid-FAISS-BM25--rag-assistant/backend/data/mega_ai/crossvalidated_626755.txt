[site]: crossvalidated
[post_id]: 626755
[parent_id]: 532813
[tags]: 
Over-confidence and calibration is a notorious open problem and challenge for neural networks. Neural networks are well-known for being over-confident. There are many research papers that have attempted to construct ways to improve calibration, with partial improvements, but the end result is that even with these improvements, the networks often remain over-confident. I am not sure that anyone really knows for sure why this happens, or how to fix it. I have seen multiple theories, but my impression is that none seem fully convincing. I would imagine that the very high number of parameters in neural networks may be relevant, but I'm not sure it is simply traditional overfitting. Normal engineering practice with training networks is to use regularization, particularly early stopping, and empirically this is effective at reducing overfitting; the gap between performance on the training set and performance on the validation set is often not too large. Many intuitions that we've built up on older statistical models (logistic regression and the sort) don't necessarily translate to modern neural networks. I very much doubt that the problem is due to using non-proper scoring rules for model tuning. Over-confidence occurs even if we don't perform any model tuning. It sounds like you are expecting that if we use a proper scoring rule, then we are guaranteed that the resulting model will be well-calibrated. I don't think there is any such guarantee, at least not for concrete problem instances. Standard arguments for using proper scoring rules talk about the asymptotics (optimizing to find the global minimum), but when training neural networks, we don't do that; we use early stopping. I wonder if covariate shift might be part of the answer. In many settings, neural networks work very well on the exact data distribution they were trained on, but are very fragile: if the data distribution is slightly different than what they were trained on, their performance can deteriorate severely. This can lead to so-called "strong but wrong" predictions on out-of-distribution data, i.e., on instances that fall outside of the training distribution, the neural network's classification predictions are often wrong but it is highly confident in its wrong answer. I suspect another possible perspective is that in practice people use all sorts of techniques to reduce overfitting in neural networks: early stopping, data augmentation, and more. Once you use these techniques as part of training, they can be viewed as effectively modifying the loss function with some additional penalty, and then the loss function is likely no longer a proper scoring rule due to this modification. So those guarantees might not apply. Yet from a practical, engineering standpoint, those techniques are absolutely essential to make neural networks work well at classification. (See also When is it appropriate to use an improper scoring rule? )
