[site]: crossvalidated
[post_id]: 348244
[parent_id]: 326608
[tags]: 
The raw objective which is being optimized in both TRPO and PPO (meaning, the objective less the penalty terms / clipping), is a first order approximation of the policy gradient. If we simply optimized the policy for multiple iterations without sampling new data, then the optimization would fail, since 1. the first order approximation would break down and 2. the expectation in the objective would be over a significantly different distribution than the one we are sampling from. However, since TRPO penalizes the KL-divergence between the old and new policy, and PPO clips the policy ratio to disincentivize going too far from the original policy, we avoid both issues 1 and 2.
