[site]: crossvalidated
[post_id]: 263817
[parent_id]: 263768
[tags]: 
The network in the diagram has an input layer and an output layer, but no hidden layers. This type of network can't perform nonlinear classification or implement arbitrary nonlinear functions, regardless of the choice of activation function. The input is projected onto the weight vector and scaled/shifted along this direction. This is a linear operation that reduces the input to a single value, which is then passed through the (possibly nonlinear) activation function. This linear reduction to a single value is the reason the network can't implement arbitrary functions. Consider a hyperplane in input space that's orthogonal to the weight vector. All inputs falling within this hyperplane are mapped to the same output value (the decision boundary plotted below is an example of such a hyperplane). Here's an example function corresponding to a network with two inputs and a logistic sigmoid output: The function is a surface bent into a sigmoidal shape along the direction of the weight vector. Changing the network parameters can rotate the direction of the sigmoidal surface, and stretch or shift it. But, the fundamental sigmoidal shape will always remain. If the network is used to implement a classifier, the decision boundary will always be linear. For example, say we take the output of the example network to represent the probability that the class is '1', given the input (i.e. the network implements a logistic regression model). We impose a threshold such that inputs are classified as '1' if they produce above-threshold output, otherwise '0'. Here's top view of the same function, where color represents the output: The plotted decision boundary corresponds to a threshold of 0.5 (i.e. we assign the most likely class). All points to the right of this boundary would be classified as '1', and those to the left as '0'. The decision boundary is orthogonal to the weight vector (plotted in red). Changing the weights could rotate or shift the decision boundary, but never make it nonlinear. However, things change radically once the network contains at least one hidden layer with sigmoidal (or other nonlinear) activation function. Such a network can indeed perform nonlinear classification and approximate arbitrary functions (but doing so may require adding vastly more units to the network). This is a consequence of the universal approximation theorem , as @broncoAbierto mentioned.
