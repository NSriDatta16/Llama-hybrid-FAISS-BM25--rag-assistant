[site]: crossvalidated
[post_id]: 48793
[parent_id]: 48441
[tags]: 
Broadly speaking, I can think of three approaches: Stay in your native space: Design a meaningful distance function $d(S_1, S_2)$ between samples. For instance, you could do something like summing $t_1 * t_2$ over every t in $S_1$ and $S_2$ and weighting by the distance between the two. Once you have this function, you can use kernel methods to do your learning (eg. SVMs if you're doing classification). If you want to prove that your SVM will converge you'll need to ensure that your distance function is positive semi-definite, but in practice it may well work fine either way. Your performance here will depend on how well you design your distance function. Extract some features. Ie. map to an m-dimensional space. An example approach: divide your timeline into k (overlapping) bins for various values of k, and count three values for each bin 1 to k: the number of -1s, +1s and 0s (no signal). Then, concatenate all values (for all k, for 1 to k, all three frequencies) into a vector and use that as a representation of your instance. Then, you can use any basic classifier you like. Your performance here will depend on the quality of your feature extraction. It should capture as much relevant information and try to minimize the number of dimensions of the resulting vector. Use a temporal model. A recurrent neural network or a hidden markov model can read the signals coming in and learn to synchronize with the signal. It will then continuously predict the next value at any moment. The drawback is that there is much less of a general framework. You'll have to do more work to implement everything for your domain, and comparing results between models is more difficult. Echo state networks (or reservoir computing) are probably a good model to investigate if your signal has a long memory. I would go for the second option if you want to keep things simple, the first if you want to reduce arbitrary choices and the third if the first two don't work.
