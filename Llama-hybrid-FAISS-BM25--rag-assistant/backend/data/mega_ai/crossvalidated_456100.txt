[site]: crossvalidated
[post_id]: 456100
[parent_id]: 454802
[tags]: 
When adding one datapoint $y_1$ , your posterior parameters are : $\Sigma^{(1)}_{x|y} = [\Sigma_x + A^T\Sigma_y^{-1}A]^{-1}$ and $\mu^{(n)}_{x|y} = \Sigma^{(1)}_{y|x}[A^T\Sigma_y^{-1}(y_1-b)+\Sigma_x^{-1}\mu_x]$ . A nice property of Bayesian statistics is that adding a data point then updating the prior as the obtain posterior and adding another point and so forth for $n$ datapoints is the same as computing directly the posterior with $n$ datapoints from the prior. So to get the posterior parameters from adding two points, you have to replace in the last expression $\Sigma_x$ by $\Sigma_{x|y}^{(1)}$ and $\mu_X$ by $\mu^{(1)}_{x|y}$ . This gives: $\Sigma^{(2)}_{y|y} = [\Sigma_x ^{-1} + 2 A^T\Sigma_y^{-1}A]^{-1}$ and $\mu^{(2)}_{x|y} = \Sigma^{(2)}_{x|y}[A^T\Sigma_y^{-1}(y_1 + y_2 - 2 b) + \Sigma_x^{-1}\mu_x]$ And you can easily show by induction that: $\Sigma_{x|y}^{(n)} = [\Sigma_x ^{-1} + n A^T\Sigma_y^{-1}A]^{-1}$ and $\mu^2_{x|y} = \Sigma^{(n)}_{x|y}[n A^T\Sigma_y^{-1}(\overline{y} - b) + \Sigma_x^{-1}\mu_x]$ You can see that when $n\to\infty$ prior influence vanishes and posteriors parameters are equivalent to $\Sigma_{x|y}^{(\infty)} \sim \frac{1}{n}A^{-1}\Sigma_y A^{-T}$ and $ \mu_{x | y}^{(\infty)} \to A^{-1} (\mathbb{E}({y}) - b)$ . I hope it answered your question.
