[site]: crossvalidated
[post_id]: 411648
[parent_id]: 409665
[tags]: 
The professor replied to me as follows: The penalization term reflects the difference in the signed distance from the decision boundary when compared between the two groups. The distance serves as a proxy for the actual classifications, as trying to minimize the difference between the actual false-positive rates is hard (much like minimizing the non-convex, discontinuous, 0-1 loss). I am not sure about what do you mean that there is no concept of misclassification during the learning phase, but I do understand your confusion. Let me try to explain: The sum of the distances for the proxy of the FPR is calculated, as you mentioned, over FP and TN classified training samples from the group. I agree, this is not optimal, since the average distance for TN can ''interfere'' with the reliability of this proxy, or distances can ''cancel out'' (we can have, for example, a ''problematic case'' where we have only one TN, but maybe it is so far away from the boundary, that it changes the entire measure). However, the rationale here is computational - it is tractable to minimize the suggested proxy since it sums up to a linear function. While if we chose to only include the distances of the training points that are FP by the model, this is no longer an easy problem to solve (I'm sure you can see why). So the full answer to the question you raised here is yes, this proxy is not a reliable measure in every case, and problems as ''canceling out'' distances might occur. However, this proxy is easy to use and is often successful. In cases where we do not expect outliers, we can expect this measure to be a successful proxy under reasonable conditions.
