[site]: crossvalidated
[post_id]: 590935
[parent_id]: 449322
[tags]: 
One additional thought, not fully explored: Maybe we should think whether overfitting can amplify existing bias. In a linear model (where we understand better what happens), we observe overfitting with coefficents being of too large absolute size. Increased bias against women then may be expressed as the coefficient for gender: female being too large (negative), not only compared to that we believe it should be zero, but also compared to the input data. Or a set of coefficients for input variates that are correlated with gender have large size in opposite directions and do not perfectly cancel themselves, leading to a net outcome that gives a too-sharp distinction between female and male candidates. Or, to put in in a different way: overfitted models tend to be overconfident in their predictions (not well-calibrated in a probabilistic sense). If the "attitude" behind the training data is somewhat reluctant (e.g. due to sexistic bias) to hire women, the overconfident prediction may amplify this into confidently rejecting (most) women. In general, I believe we (machine learning community) don't do overly well in terms of preventing overfitting. I did not check whether this would lead to a bias against the minority class as in systematic effect over a large number of models, but: we have an additional selection effect/publication bias here: even if this effect only leads to high variance: we have a number of minorities or protected classes* (sociologial meaning here). We are (rightly!) concerned if an existing bias against any of these is amplified by the AI. But even if there were only the question of bias against women being amplified, and models were equally likely to amplify or deamplify (bias against men) the input data bias, given that there is historically a bias against women, those models would likely be judged differently. Amplified bias against women is bad (again: rightly so); Deamplified bias against women (or maybe even bias against men) is likely seen more leniently since it counteracts the existing bias. (Personally, I'm not so sure I'd agree with that lenience, I very much prefer countermeasures with negative feedback loop which stop exerting force when there is no more bias.) In any case, a model amplifying bias against women is IMHO more likely to be brought to our attention. *I think it somehow weird if we women are considered a 50 % "minority".
