[site]: crossvalidated
[post_id]: 422420
[parent_id]: 
[tags]: 
Improve policy in Reinforcement learning?

In model-free RL, we usually take two steps,one is value evaluation another is an improvement the high-level description of algorithm is : sample data evaluate policy by Q iteration or SARSA iteration improve policy my question is : What are the benefits of improving the policy after each step? do step 3 immediately after step 2. What are the benefits of updating the policy less frequently? do step 3 after some iteration of step 2 I don't understand why to improve policy before getting true evaluation(that is improving after each step) is more popular. I find an intuition from RL textbook, but it's somewhat too vague. The intuition is about GPI in Richard S. Sutton and Andrew G. Barto's book page 87 , which says Each process drives the value function or policy toward one of the lines representing a solution to one of the two goals. The goals interact because the two lines are not orthogonal. Driving directly toward one goal causes some movement away from the other goal
