[site]: datascience
[post_id]: 111760
[parent_id]: 
[tags]: 
LSTM RNN regression: validation loss erratic during training

While training my NN, I am getting unfamiliar behaviors which I don't understand. Firstly: highly erratic validation loss while training loss goes down steadily. Secondly: training loss goes down very slowly (not the usual exponential decay curve) My model architecture is: # Create model model = Sequential() model.add(Bidirectional(LSTM(30, input_shape=(train_x.shape[1:]), return_sequences=True))) model.add(Bidirectional(LSTM(10))) model.add(Dense(4, activation="linear")) model.add(Dense(1, activation="linear")) opt = tf.keras.optimizers.Adam() # Compile model model.compile(loss='mean_absolute_percentage_error', optimizer=opt, metrics=['MeanAbsolutePercentageError']) Training set: composed of 30k sequences, sequences are 180x1 (single feature), trying to predict the next element of the sequence. Validation set: same as training but smaller sample size Loss = MAPE Batch size = 32 Training looks like this (green validation loss, red training loss): Example sequences from training set: From validation set: Please help explain this to me, and how can I improve training. PS I am fairly new to ML, any advice would be appreciated
