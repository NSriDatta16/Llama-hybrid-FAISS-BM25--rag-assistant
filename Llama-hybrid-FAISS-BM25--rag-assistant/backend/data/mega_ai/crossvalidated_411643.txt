[site]: crossvalidated
[post_id]: 411643
[parent_id]: 404858
[tags]: 
You might want to do something like a paper called A Structured Self-Attentive Sentence Embedding . In this paper, they process a sentence using an RNN. Then there is a set of constant, but trained query vectors which are then used as queries in an attention mechanism. For each query vector, you get one context vector (weighted average of the RNN states), which are then concatenated and used for classification. My guess is that you can do a little better if you do the independent projection for attention keys and values as in the Attention is all you need paper. The main difference between the attention in sequence-to-sequence learning and in this paper is that in seq2seq models, the query vector is different for each step (it is the current state of the decoder), here the query vectors are constant.
