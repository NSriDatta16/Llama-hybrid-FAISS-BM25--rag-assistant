[site]: crossvalidated
[post_id]: 222422
[parent_id]: 
[tags]: 
Using PCA to model highly correlated variables

Specifically, Andrew Ng states that PCA should be used to speed up algorithms or to visualize data. He also states that using PCA as a way to prevent overfitting is an incorrect application of PCA. The logic being that PCA only compares predictor variables with other predictor variables and ignores the response; thus, it's possibly removing information that relates to the response variable and therefore it is a sub-optimal way to prevent overfitting. He recommends regularization instead. I have seen PCA recommended as a way to model highly correlated variables, rather than fitting the highly correlated variables themselves. Is this a correct application of PCA (it seems to me that it would have the same flaws as using it to prevent overfitting)? If not, what is the best way to correct for model instability caused by correlated predictor variables? More specifically, if I am fitting a GLM and 2 or more predictor variables are correlated to the point the model is no longer stable; however, there is signal hidden in the correlated predictor variables then what should be my approach? Some options would be: It the variables into a PCA and fit on the output Only use one of the correlated variables Include the correlated variables and their interaction Fit some transform or other linear combination of the variables I said GLM in this example, though the same issue could arise in a machine learning application (neural nets, SVM, etc) as well.
