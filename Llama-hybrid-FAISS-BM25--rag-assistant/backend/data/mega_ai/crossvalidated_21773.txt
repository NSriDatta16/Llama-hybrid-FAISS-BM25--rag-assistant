[site]: crossvalidated
[post_id]: 21773
[parent_id]: 21691
[tags]: 
Let's talk about sensitivity (which we'll denote by $p$), the specificity is similar. The following is a frequentist approach; it would be great if one of the Bayesians here could add another answer to discuss an alternative way to go about it. Suppose you've recruited $n$ people with cancer. You apply your biomarker test to each, so you will get a sequence of 0's and 1's which we'll call x . The entries of x will have a Bernoulli distribution with success probability $p$. The estimate of $p$ is $\hat{p} = \sum x /n$. Hopefully $\hat{p}$ is "big", and you can judge the precision of your estimate via a confidence interval for $p$. Your question says that you'd like to know how big $n$ should be. To answer it you'll need to consult the biomarker literature to decide how big is "big" and how low of a sensitivity you can tolerate due to sampling error. Suppose you decide that a biomarker is "good" if its sensitivity is bigger than $p = 0.5$ (that's actually not so good), and you'd like $n$ to be big enough so there's a 90% chance to detect a sensitivity of $p = 0.57$. Suppose you'd like to control your significance level at $\alpha = 0.05$. There are at least two approaches - analytical and simulation. The pwr package in R already exists to help with this design - you need to install it first. Next you'll need an effect size, then the function you want is pwr.p.test . library(pwr) h1 So you'd need around $435$ people with cancer to detect a sensitivity of $0.57$ with power $0.90$ when your significance level is $0.05$. I've tried the simulation approach, too, and it gives a similar answer. Of course, if the true sensitivity is higher than $0.57$ (your biomarker is better) then you'd need fewer people to detect it. Once you've got your data, the way to run the test is (I'll simulate data for the sake of argument). n The estimate of sensitivity is $0.568$. What really matters is the confidence interval for $p$ which in this case is $[0.527, 1]$. EDIT: If you like the simulation approach better, then you can do it this way: set n and let runTest be runTest so the estimate of power is mean(replicate(nSim, runTest())) [1] 0.887
