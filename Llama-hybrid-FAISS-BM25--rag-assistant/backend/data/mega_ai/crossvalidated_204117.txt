[site]: crossvalidated
[post_id]: 204117
[parent_id]: 204111
[tags]: 
No, you can not reduce overfitting without reducing the information your algorithm can use, this is the whole point of overfitting! You can pick an algorithm that uses less information (more bias, less variance), or provide better information to your algorithm (less noise makes it harder to fit the noise). While you indeed reduced the number of features, you did not change the information the algorithm had access to, the number of possible splits points for the tree. It is as able as before to isolate points. But you have introduced structure in your data that might not exists. Consider the reverse: when a feature has differents levels, say $x \in \{1,2,3\}$, the fact that $3>2>1$ should be relevant, for exemple in the price of an object. If it is not, for exemple if $1,2,3$ are tokens for countries of origin (without clear ordering), then you should split your feature into three distinct binary features, $a_{x = 1}, b_{x = 2}, c_{x = 3}$. This does not increase the number of possible split points, but it makes the splits more meaningful. $\text{price} If you want to reduce overfitting, using random forests, you can Increase the number of trees you grow Grow shallower trees Reduce the number of features that are tried for each split And acting on your features, you can Use dimensionality reduction (PCA, SVD, ...) to extract the most meaningful information, and try to discard noise.
