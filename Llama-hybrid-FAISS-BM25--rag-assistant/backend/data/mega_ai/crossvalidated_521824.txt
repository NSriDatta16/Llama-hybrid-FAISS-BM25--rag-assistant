[site]: crossvalidated
[post_id]: 521824
[parent_id]: 521809
[tags]: 
The problem is that in your derivation you omit the ``truncation noise'' which must be taken into account. Mercer's theorem can be used to get a finite-rank approximation of a kernel. In terms of model, the Gaussian Process $Y(\mathbf{x})$ is approximated by $$ Y(\mathbf{x}) = \alpha_1 e_1(\mathbf{x}) + \alpha_2 e_2(\mathbf{x}) + \dots + \alpha_N e_N(\mathbf{x}) + \varepsilon(\mathbf{x}) $$ where the coefficients $\alpha_i$ are independent Gaussian r.vs with variance $\text{Var}(\alpha_i) = \lambda_i$ . The noise term $\varepsilon$ has a known covariance kernel but it can be viewed as a white noise with variance $\gamma := \sum_{j > N}\lambda_j$ . When $n$ observations are made at $n$ distinct indices $\mathbf{x}_i$ the approximation takes the linear model form $$ y_i = \alpha_1 e_1(\mathbf{x}_i) + \alpha_2 e_2(\mathbf{x}_i) + \dots + \alpha_N e_N(\mathbf{x}_i) + \varepsilon_i \qquad 1 \leqslant i \leqslant n $$ or in matrix form $\mathbf{y} = \mathbf{E} \,\boldsymbol{\alpha} + \boldsymbol{\varepsilon}$ where $\mathbf{E}$ is the $n \times N$ design matrix having the eigenfunctions as its columns. We have an informative prior on $\boldsymbol{\alpha}$ namely $\boldsymbol{\alpha} \sim \text{Norm}(\mathbf{0}, \, \boldsymbol{\Delta})$ with $\boldsymbol{\Delta} := \text{diag}\{\lambda_j\}$ . We can derive predictions using the Bayesian Linear Model. Note that the related approximation for the $n \times n$ Gram matrix $\mathbf{K}$ is $$ \mathbf{K} \approx \mathbf{E}\boldsymbol{\Delta}\mathbf{E}^\top + \gamma \mathbf{I} $$ and the relation between the Kriging prediction and the Bayesian Linear Model one can be made clear by using the matrix inversion lemma . Of course if $\gamma$ is taken as zero this will no longer work. Interestingly we can take $N > n$ if wanted. Note that the approximation will not lead to an interpolation as in exact Kriging but will be very close to it, with less numerical problems.
