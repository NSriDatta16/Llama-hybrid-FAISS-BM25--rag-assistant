[site]: crossvalidated
[post_id]: 319585
[parent_id]: 
[tags]: 
Maximum Likelihood & Bayesian inference minimizing Kullback-Leibler divergence?

I have heard/read that Bayesian and Maximum Likelihood inference can be justified as asymptotically minmizing the KL divergence between the pdf $p(x)$ actually describing the data and the parameterized (approximating) pdf $f(x|\hat{\theta})$, where $\hat{\theta}$ is the MLE or MAP estimate. In other words, the claim is that generically, \begin{align} \hat{\theta} = \arg\min_{\theta} \{\text{KL}\left(p(x)||f(x|\theta )\right) \}. \end{align} I have no trouble seeing this for some special cases (see for instance this excellent post here ), but I would be interested in a general proof if one exists. I would also be happy about counterexamples proving that the claim cannot be generally true.
