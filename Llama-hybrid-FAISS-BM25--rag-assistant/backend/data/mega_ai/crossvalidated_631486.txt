[site]: crossvalidated
[post_id]: 631486
[parent_id]: 
[tags]: 
Bayesian account for maximum likelihood estimate over infinite parameter space

Suppose I have some samples $x_1, \ldots, x_n$ from $\mathcal{N}(\mu, 1)$ for unknown $\mu$ . Then the maximum likelihood estimate for $\mu$ is just $\overline x = \frac1n \sum x_i$ . Ideally, we can find a proper prior for $\mu$ such that after Bayesian update on all samples, the posterior density at $\overline x$ is the largest. However, this seems to require a flat prior on $(-\infty, \infty)$ , which is not possible. Assuming that Bayesian interpretation of probability is "correct", this seems to suggest that MLE will have suboptimal performance by not being fully aligned with Bayesian inference. However, given that MLE does work well in practice, there should be some way to account for its empirical success.
