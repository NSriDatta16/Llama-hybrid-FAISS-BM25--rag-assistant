[site]: datascience
[post_id]: 44176
[parent_id]: 
[tags]: 
Loss and Regularization inference

I'm building a Matrix Factorization model for MovieLens dataset with batch-wise training. Loss function for the batch: $$ L_{batch} = 1/|B|\sum_{(u,i)\in{B}}(r_{ui} - \mu - b_u - b_i - p_u^Tq_i)^2 + \lambda(||p_u||^2 + ||q_i||^2) $$ $$ L_{batch} = (L_{base\_loss} + L_{reg\_loss})/|B| $$ $$ L_{base\_loss} = \sum_{(u,i)\in{B}}(r_{ui} - \mu - b_u - b_i - p_u^Tq_i)^2 $$ $$ L_{reg\_loss} = \sum_{(u,i)\in{B}}\lambda(||p_u||^2 + ||q_i||^2) $$ where $r_{ui}$ is the observed rating, $\mu$ is the global average rating, $b_u$ and $b_i$ are the average deviations of user $u$ and item $i$ from the global average rating respectively, $p_u$ and $q_i$ are the learned user embeddings and movie embeddings respectively, $B$ is the batch. What should I infer from regularization loss going up? Model is not able to capture the underlying information using the current embedding size, or the scale of regularization loss is too less compared to the base loss?
