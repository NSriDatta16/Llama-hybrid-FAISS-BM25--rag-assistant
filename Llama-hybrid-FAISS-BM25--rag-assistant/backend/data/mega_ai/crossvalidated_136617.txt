[site]: crossvalidated
[post_id]: 136617
[parent_id]: 
[tags]: 
predictive distribution of linear bayesian regression with unknow $\Sigma$ and $\Omega$

The posterior distribution for weights in linear regression setup is \begin{equation} \begin{aligned} B &| Y,X \sim \mathcal{N}(\mu, \Lambda) \\ \mu &= \Lambda X^{\mathsf{T}}\Sigma^{-1}Y \\ \Lambda &= (X^{\mathsf{T}}\Sigma^{-1}X + \Omega^{-1})^{-1} \end{aligned} \end{equation} $\Sigma$ is the covariance matrix, $\sigma_{\epsilon}\mathbf{I}$, where $\sigma_{\epsilon}$ is noice on each measurement $y = x \beta + \epsilon$, and $\Omega$ is the covariance on the prior on $\beta$, with $\sigma_{\beta}$ on its diagonal. $B \sim \mathcal{N}(0, \sigma_{\beta})$. My question is, how to estimate $\Omega$ and $\Sigma$ when both are unknown. Should I assume an iterative process, where every time I replace $\mu$ and $\Lambda$ with $\Sigma$ and $\Omega$ from the previous steps ?
