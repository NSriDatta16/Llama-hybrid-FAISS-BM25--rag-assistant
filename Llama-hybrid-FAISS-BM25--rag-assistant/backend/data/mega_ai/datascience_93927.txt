[site]: datascience
[post_id]: 93927
[parent_id]: 
[tags]: 
Why we take $\alpha\sum B_j^2$ as penalty in Ridge Regression?

$$RSS_{RIDGE}=\sum_{i=1}^n(\hat{y_i}-y_i)^2+\alpha\sum_{i=1}^nB_j^2$$ Why we are taking $\alpha\sum B_j^2$ as a penalty here? We are adding this term for minimizing variance in Machine Learning Model. But how this term minimizing variance. If I add suppose $e^x$ or any increasing function then it also minimizing the variance. I want to know how this term minimizing the error
