[site]: datascience
[post_id]: 64326
[parent_id]: 
[tags]: 
How can I feed BERT to neural machine translation?

I am trying to feed the input and target sentences to an NMT model, I am trying to use BERT here, But I don't have any idea how to give it to my model. before that, I was using one-hot encoding and I got issues there and I want to use BERT. Also, I have to note that, I am new in TensorFlow and deep learning. So please share your opinion with me about the use of BERT in NMT. my goal is only to use BERT in my model for translation purpose my model definition: def build_model(in_vocab, out_vocab, in_timesteps, out_timesteps, units): model = Sequential() model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True)) model.add(LSTM(units)) model.add(RepeatVector(out_timesteps)) model.add(LSTM(units, return_sequences=True)) model.add(Dense(out_vocab, activation='softmax')) return model
