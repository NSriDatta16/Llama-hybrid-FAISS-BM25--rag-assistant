[site]: datascience
[post_id]: 67654
[parent_id]: 
[tags]: 
Are vectors generated by doc2vec and similar models uniformly distributed?

I have read that vectors in a word2vec model are very much not uniformly distributed and are thought to follow Zipf's law; is this the same for the associated models like paragraph2vec, doc2vec, etc? My first thought is that they should be similar - if you hand the proverbial monkey a dictionary and allow it to string random words together, almost all of the things it writes won't make sense; the things that do make sense certainly follow some reasonable, very restrictive grammatical rules, etc. I know doc2vec and all the related models seem to pick up on a lot of the grammatical structure (or almost structure), so shouldn't that mean that they'll group grammatical things together? On the other hand, it's true that there are so very many ways to put words together and so many things to say that they could easily fill out a huge space and maybe could end up being uniformly distributed. Thanks!
