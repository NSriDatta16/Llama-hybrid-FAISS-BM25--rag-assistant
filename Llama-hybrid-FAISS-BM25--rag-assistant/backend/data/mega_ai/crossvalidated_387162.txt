[site]: crossvalidated
[post_id]: 387162
[parent_id]: 386075
[tags]: 
I used to work on a project to develop software management best practices. I observed roughly fifty software teams in the field. Our sample was around 77, but we ended up seeing around a hundred teams. In addition to collecting data on things such as certifications, degrees and so forth, we also collected a variety of psychological and demographic data. Software development teams have some very significant self-selection effects in it that, while having nothing to do with gender, are strongly correlated with gender. Also, managers tend to replicate themselves. People hire people they are comfortable with, and they are most comfortable with themselves. There is also evidence that people are being rated in a cognitively biased way. Imagine that, as a manager, I highly value prompt arrival at the start of work. I will then rate on that. Another manager, who just cares that the work gets done, may rate on something entirely different as important. You noted that men use language differently, but it is also true that people with different personalities use language in different ways. There may be ethnic language usage differences as well, see for example the current controversy at Harvard and Asian admissions. Now you assume that the software firms discriminate against women, but there is another form of gender discrimination going on in the software development industry that you haven’t accounted for. When you control for objective things such as certifications, degrees, tenure and so forth, the average woman earns 40% more than the average man. There are three sources of employment discrimination in the world. The first is that managers or owners do not wish to hire someone on the basis of some feature. The second is that coworkers do not wish to work with the people with that feature. The third is that customers do not want people who have a feature. It appears the wage discrimination is being triggered by customers because the work product is different, and from the customers’ perspectives, also better. This same feature causes male dental hygienists to take lower pay than women. It is also seen in a bias toward “born here” in world soccer wages. The best control for this is to understand your data and the social forces involved. Any firm that uses its own data will tend to replicate itself. That may be a very good thing, but it could also make them blind to forces at work. The second control is to understand your objective function. Profits may be a good function, but it may be a bad function. There are values in play in the selection of an objective loss function. Then, finally, there is the issue of testing the data against demographics to determine if unfortunate discrimination is happening. Finally, and this is a bigger problem in things like AI where you cannot get good interpretative statistics, you will want to control for Yule’s paradox. The classic historical example is the discovery that 44% of men were accepted to UC Berkley while only 35% of women were admitted in 1973. This was a huge difference and statistically significant. It was also misleading. This was obviously scandalous, and so the university decided to look at which were the offending majors. Well, it turned out that when you controlled for major, there was a statistically significant bias in favor of admitting women. Of the eighty-five majors, six were biased toward women and four toward men, the remainder were not significant. The difference was that women were, disproportionately, applying for the most competitive majors and so few of either gender were getting in. Men were more likely to apply to less competitive majors. Adding in Yule’s paradox creates an even deeper layer for discrimination. Imagine, instead of a gender test, there was a gender test by type of job. You could possibly pass a company-wide gender neutral test but fail at the task level. Imagine that only women were recruited for V&V and only men for systems administration. You would look gender neutral, and you wouldn’t be. One potential solution to this is to run competitive AIs that use differing objective criteria of “goodness.” The goal is to widen the net, not narrow it. This can also help avoid another problem in the management literature. While 3% of males are sociopaths, that number climbs substantially as you go further and further up the corporate ladder. You don’t want to be filtering for sociopaths. Finally, you may not want to consider using AI for certain types of positions. I am job hunting right now. I am also sure I am being filtered out, and I haven’t figured out how to get around it. I am sitting on a very disruptive new technology. The problem is that my work doesn’t match the magic words. Instead, I have the next set of magic words. Right now, I am worth a fortune to the right firm, but in one case where I applied, I received an automated decline in less than a minute. I have a friend who has served as the CIO of federal agencies. He applied for a job where the hiring manager was waiting to see his application come through so he could pretty much be offered the job. It never came through because the filters blocked it. This sets up the second problem of AI. If I can work out from online resumes who Amazon is hiring, then I can magic word my resume. Indeed, I am working on my resume right now to get it to fit non-human filters. I can also tell from the e-mails from recruiters that some parts of my resume are being zoomed in on and other parts ignored. It is as if the recruiting and hiring process has been taken over by software like Prolog. Logical constraints met? Yes! This is the optimal candidate or set of candidates. Are they optimal? There isn't a pre-built answer to your question, only problems to engineer around.
