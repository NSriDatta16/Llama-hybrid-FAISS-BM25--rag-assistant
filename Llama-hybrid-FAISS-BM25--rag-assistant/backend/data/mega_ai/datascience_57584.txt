[site]: datascience
[post_id]: 57584
[parent_id]: 57502
[tags]: 
Since you did not specify your exact problem, first some general remarks. Heteroscedasticity does not affect estimated coefficients, but the (estimated) standard deviation of coefficients, p-values, and confidence intervals in linear models. Heteroscedasticity means that the model's variance is contingent on features (explanatory variables). The Breusch-Pagan (BP) test is suitable for linear models. Say you have a linear model $$ y = \beta X + u. $$ . You can estimate the model using $\hat{\beta} = (X'X)^{-1} X'y$ and obtain the residuals $\hat{u}=y-X\hat{\beta}$ . The BP test checks if residuals $\hat{u}$ are contingent on explanatory variables $X$ . To do so, an auxiliary regression is used: $$ \hat{u}^2=\gamma X + e.$$ The general idea is that $\hat{u}^2$ should be unrelated to $X$ , so that coefficients $\gamma$ (apart of the intercept) should be statistically not different from zero. This conjecture could be used to drill up problems with heteroscedasticity. In your case you are interested in how strong heteroscedasticity is. Looking at the vector $\gamma$ may be one option. A second option would be to simply look at the outcome of the BP test (the $\tilde{\chi}^2$ and related p-value). R code (regression and BP test): library(MASS) data(Boston) # Regressions reg1 = lm(medv~indus+lstat, data=Boston) reg2 = lm(medv~indus+crim, data=Boston) # BP test library(lmtest) bptest(reg1) bptest(reg2) # Null hypothesis of BP test = homoscedasticity (= variance does not depend on auxiliary regressors) # If the p-value is "small" ( heteroscedasticity # reg2 -> no heteroscedasticity Result: > bptest(reg1) studentized Breusch-Pagan test data: reg1 BP = 20.542, df = 2, p-value = 3.462e-05 > bptest(reg2) studentized Breusch-Pagan test data: reg2 BP = 0.053288, df = 2, p-value = 0.9737 For reg1 , the Null is rejected, for reg2 , the Null is not rejected. This suggests that errors are not homoscedastic for reg1 . Let us try to drill the problem up a bit. Auxiliary regression: # 1) Get residuals Boston $u1 = residuals(reg1)^2 Boston$ u2 = residuals(reg2)^2 # 2) Aux regression aux1 = lm(u1~indus+lstat, data=Boston) summary(aux1) aux2 = lm(u2~indus+crim, data=Boston) summary(aux2) Result: Regression 1 Call: lm(formula = u1 ~ indus + lstat, data = Boston) Residuals: Min 1Q Median 3Q Max -64.78 -36.94 -22.64 -2.13 581.64 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 62.0549 7.6859 8.074 5.06e-15 *** indus 1.0020 0.6539 1.532 0.126 lstat -2.7603 0.6282 -4.394 1.36e-05 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 80.36 on 503 degrees of freedom Multiple R-squared: 0.0406, Adjusted R-squared: 0.03678 F-statistic: 10.64 on 2 and 503 DF, p-value: 2.974e-05 Regression 2 Call: lm(formula = u2 ~ indus + crim, data = Boston) Residuals: Min 1Q Median 3Q Max -61.72 -56.21 -42.16 -16.11 995.65 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 61.49061 13.08357 4.700 3.37e-06 *** indus 0.01188 1.08488 0.011 0.991 crim -0.18560 0.86527 -0.214 0.830 --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 152.8 on 503 degrees of freedom Multiple R-squared: 0.0001053, Adjusted R-squared: -0.00387 F-statistic: 0.02649 on 2 and 503 DF, p-value: 0.9739 It seems that lstat in regression 1 causes heteroscedasticity in this model since the coefficient is statistically different from zero. In regression 2 all coefficients (except intercept) are not statistically different from zero. Now, if you try to mitigate heteroscedasticity, you could use the results from the auxiliary regression(s) to see if some (linear) transformation of the data helped to make the variance more harmonic (note that the interpretation of $\gamma$ will change when you make linear transformations, but p-values will give you some idea).
