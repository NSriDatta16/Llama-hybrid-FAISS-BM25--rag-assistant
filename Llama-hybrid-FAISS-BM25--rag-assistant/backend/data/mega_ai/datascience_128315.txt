[site]: datascience
[post_id]: 128315
[parent_id]: 128274
[tags]: 
There are 3 layers to the answer why: ChatGPT is fine tuned with around 11,000 examples of humans asking questions, and receiving helpful and intelligent responses. On the surface, LLMs have only one job. To look at as much of the preceding text as possible, and give the most likely (or one of the top few most likely) next words. By taking the lead, you can say things that would be statistically more likely to be followed by intelligent responses. The transformer architecture encourages the model to look beneath the surface of language, and discover deep abstractions, and layer these abstractions into complex combinations. By triggering keywords with strong connections to your desired content, the model will be more likely to respond with the vibe you are hoping for. And yes “vibe” is probably the most accurate way to describe this!
