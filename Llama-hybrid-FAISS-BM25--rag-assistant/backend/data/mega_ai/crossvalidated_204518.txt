[site]: crossvalidated
[post_id]: 204518
[parent_id]: 40447
[tags]: 
If you are going to populate the entire user x article matrix with dwell-times, you are going to run in to sparsity issues very quickly. Also, a simple average of dwell-times is prone to many problems, for example, what if you have very few records, or if one user left her browser open for a month ? Step #1: Filling in the blanks From my experience dealing with user dwell time, The amount of users that spend $t$ seconds viewing a site, decreases greatly as $t$ increases. I found out that modelling user dwell-time as an Exponential curve, is a good approximation. Using the Bayesian approach , and using the Gamma distribution as the prior distribution on the mean of each site's dwell-time, we get a familiar formula: Harmonic mean : $$\frac{n+m}{\frac{m}{b}+\frac{1}{t_1}+\dots++\frac{1}{t_b}}$$ Where $t_i$ is the time spent on site $i$, $b$ is the bias you introduce and $m$ is its strength. For example, setting $b=3,m=2$ is like assuming two fictional users viewed a site for 3 seconds when we have no data for that user x article combination. And note that this formula is much more immuned to outliers, since it assumes the exponential distribution (and not the Gaussian distribution like the arithmetic mean) Step #2: Populating the matrix Times are positive, and they have a certain bounds that make sense (for example, maximum of one day). However, after the matrix factorization, any numeric value can appear in the matrix cells, including negative terms. The common practice is to populate the user x article matrix with $$logit(t)$$ Where logit is the inverse of the sigmoid function. And then when interpolating the dwell time for a user $i$ and article $j$, we use: $$sigmoid( )$$ Instead of only using the dot product. This way we can be certain that the end result would be bounded to a certain range that makes sense.
