[site]: crossvalidated
[post_id]: 74456
[parent_id]: 
[tags]: 
A kernel in the context of kernel smoothing is a local similarity function $K$, which must integrate to 1 and is typically symmetric and nonnegative. Kernel smoothing uses these functions to interpolate observed data points into a smooth function. For example, Watson-Nadaraya kernel regression estimates a function $f : \mathcal X \to \mathbb R$ based on observations $\{ (x_i, y_i) \}_{i=1}^n$ by $$ \hat{f}(x) = \frac{\sum_{i=1}^n K(x, x_i) \, y_i}{\sum_{i=1}^n K(x, x_i)} ,$$ i.e. a mean of the observed data points weighted by their similarity to the test point. Kernel density estimation estimates a density function $\hat{p}$ from samples $\{ x_i \}_{i=1}^n$ by $$ \hat{p}(x) = \frac{1}{n} \sum_{i=1}^n K(x, x_i) ,$$ essentially placing density "bumps" at each observed data point. The choice of kernel function is of theoretical importance but typically does not matter much in practice for estimation quality. (Wikipedia has a table of the most common choices.) Rather, the important practical problem for kernel smoothing methods is that of bandwidth selection : choosing the scale of the kernel function. Undersmoothing or oversmoothing can result in extremely poor estimates, and so care must be taken to choose an appropriate bandwidth, often via cross-validation. Note that the word "kernel" is also used to refer to the kernel of a reproducing kernel Hilbert space, as in the "kernel trick" common in support vector machines and other kernel methods. See [kernel-trick] for this usage.
