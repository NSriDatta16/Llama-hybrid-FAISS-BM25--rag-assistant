[site]: crossvalidated
[post_id]: 390140
[parent_id]: 355260
[tags]: 
A note about PCA I'd like to clarify the language I'm going to use. Principal Component Analysis is an extremely wide umbrella term that is used for algorithms that try to capture "the gist" of the data (usually expressed as a matrix). There are two components in those algorithms: what are they trying to capture exactly and model that they employ to capture this information (the general idea is that the compressed version of the data should occupy less space and maybe reveal some patterns in the data). Given these definitions, SVD is an optimal way to capture L2 norm of the matrix (meaning, we are minimizing $||A-\hat A||$ ) given low-rank linear model $\hat A = USV^T$ (U and V are matrices with orthonormal columns and S is diagonal). SVD++ or FunkSVD are not exactly the same, they have different loss function (they either do not take zeros into account or subsample them), they have an explicit regularization and they have no guarantees about orthogonality of found singular vectors (which you might not care much about). Best distributed SVD libs If you think that capturing the L2 norm of your matrix is a good idea, then I can suggest using one of two libraries that implement Randomized SVD algorithm: SparkRSVD library in Scala for Spark or Dask ML Truncated SVD in Python for, well, Dask. We've written the first and tested it against the second on matrices up to 100 mln x 100 mln rows and columns with very small sparsity (see our blog post here ). They are both I think good options, the choice depends mainly on the language/framework you're most comfortable with. Also, we found Dask to be faster on relatively small matrices, but the Spark lib can scale to larger matrices (from what we tested). Now, why Randomized SVD and not SVD from MLLib, for instance? If you look under the hood, Spark MLLib implements SVD only for RowMatrix (with the limitation on num_cols $G=A^T A$ ) to produce a smaller matrix (in your case it should become almost dense 1 mln x 1 mln) that is easy to work with which should either fit in memory of a single node or at least num_rows * num_factors should. Which might or might not be okay for you depending on what kind of hardware you have or how many latent factors you need to reconstruct. In contrast, Randomized SVD algorithms are based on the ideas of randomized algorithms and efficient QR decomposition . They do not require candom access to the matrix A, just the ability to multiply by it, and do not require storing the result of those multiplications at the same node, just a $\text{num_factors} \times \text{num_factors}$ matrix. P.S. Updated the answer to be more precise about what Spark MLLib implements.
