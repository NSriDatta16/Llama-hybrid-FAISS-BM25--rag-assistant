[site]: crossvalidated
[post_id]: 363670
[parent_id]: 363640
[tags]: 
DenseNet is a generic successor to ResNet and achieves 3.46% error on CIFAR-10 and 17.18 on C-100. Compare to 3.47 and 24.28 mentioned on the leaderboard. Shake-shake , Shake-drop and possibly other variants are regularization techniques which can be used any ResNet-like architectures, and achieves 2.86/2.31% error on C-10 and 15.85/12.19 on C-100 (shake-shake/shake-drop). These techniques work on only multi-branch architectures, which is why I mention them even though they are not strictly architectures in themselves. Efficient Neural Architecture Search (using reinforcement learning to search for architectures) finds a network which achieves 2.89% error on C-10, using the Cutout regularization technique. Performance is 3.54% without cutout. In summary: Dense Net and possibly some ENAS produced network may perform slightly better than ResNet, but the use of sophisticated regularization techniques makes the comparison admittedly difficult. I don't know of any leaderboard which is really up to date, but typically any new paper which claims good or state-of-the-art performance on any task will have a fairly comprehensive results table comparing with previous results, which can be good way to keep track.
