[site]: crossvalidated
[post_id]: 376723
[parent_id]: 376716
[tags]: 
Aah, that is a classic in applied machine learning: You have many different "things" you want to predict and you are tempted to create a different model for every of them. However, you will run into big trouble when doing so: First of all you have to maintain a lot of models and if you have many models you cannot do that. Also philosophically, this would be wrong because each model would have only very little data and it would be much harder to detect a "general" pattern in the data (that is common among all "things"). But worry no more, friend, (at least IMHO) there is a general solution to deal with this problem. What I usually do is to introduce features 'with respect to that current thing but normalised over all products' to make this common pattern available to the model. Furthermore you are confronted with a time series problem but let us ignore that for a moment and deal with that below. Let us further assume that you just want to predict how many products you need on the current day (no fancy stuff with packages, etc to keep it simple for this answer). Example for such a feature as mentioned above: Price. Let us say that you just use one single feature: Price. One "common pattern among all products" could be "if the price is high then people will not buy the product". However, price itself is hard to interpret because how should the model know whether the price is "high" or "low" at the moment? Let us say that we compute the average price $p_w$ for each product $w$ and then on the new dataset you create a feature "price_diff" that is "price on the current day for product $w$ - $p_w$ ". If that feature has a value of $100$ then the model knows that the product is 100 units more expensive than "usual" and hence, it is pretty expensive. If, on the contrary, this feature contains the value $-10$ then the model knows that the product is cheaper than usual. Now that we have this feature "w.r.t. that product" the model must be enabled to do a "fair comparison". That is why you normalise this feature. In that case, $100$ can be "a lot more" if the product usually costs $1000$ and it could mean "just a tiny bit more" if the product usually costs $100.000.000$ . Hence, I suggest you normalise and craft a feature that states how much (in %) the price is higher/lower than the average. In the first case, the feature would contain $10$ % and in the second it would contain ... well, I'm bad at math... is it $1/10000$ ? I think so... well, you get the pattern ;-) Now let's deal with the time series part. At the very first stage, time series analysis on some dataset $y_0, y_1, y_2, ...$ is nothing else than "take any regression model and feed it a training set with lagged target variables". I.e. let's say you want the model to be able to look at the last $3$ time points then you feed it the training set feature_1 feature_2 feature_3 target variable y_0 y_1 y_2 y_3 y_1 y_2 y_3 y_4 y_2 y_3 y_4 y_5 ... Combining these things for your example means: Take one single regression model (for example, gradient boosting) and feed it cleverly crafted features that are generated "with respect to the current product", normalised and lagged. I.e. how much did the current product cost yesterday, the day before yesterday, etc. How much more (in%) does it cost today than average, how much (in%) did it cost more/less than average yesterday, ... and the most important: How was the demand of that product yesterday, the day before yesterday, over the last week, over the same day of week in the last week, ...... there is an endless pool of possibilities here. Also note that this lagged stuff does not necessarily relate to the product itself but rather to products that "share" many properties. I.e. if the current product is a western movie then I would not only feed the model the features modelling the development of the price of this particular western movie but rather the average price over all western movies (and the %ish deviations on the last days, etc). In short: Start just with two variables: price and demand and create these lagged, normalised features from them and then let a first regression model run and see how it performs.
