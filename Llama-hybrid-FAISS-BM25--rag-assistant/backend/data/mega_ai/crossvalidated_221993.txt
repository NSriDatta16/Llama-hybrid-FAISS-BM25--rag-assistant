[site]: crossvalidated
[post_id]: 221993
[parent_id]: 24825
[tags]: 
1) Most neural networks cannot perform multiplications; they can only calculate sums (which are then individually fed through through an activation function ). They must instead estimate those multiplications if they are important, which requires a lot of neurons, especially if the factors can span large ranges. If it would turn out that the house area is in fact an important feature, you will help the network if you provide it with the area, because it can then use the neurons it would have required to estimate the multiplication of the width and the length to do other things. Hence, including polynomial features may in some cases be beneficial to the network, but has in other cases no significant effect. Furthermore, polynomial features are only one type of derived features that may be helpful to the network. Another type of derived feature that may turn out to be helpful is for example the logarithms of the input variables (considered they are positive) which the network also must estimate to obtain. An idea would be to allow the network to perform more operations between numbers than only additions, to enable it to efficiently calculate things like polynomial features itself, but it is not clear how that would work. One architecture that looks like it does something similar is the sum-product network . 2) Except from the computational cost which John mentioned, increasing the number of parameters in the model, which inevitable happens when you introduce more inputs, also increases the risk for the network to overfit , especially if you have little training data. However, this can be made into much less of a problem if a good regularization method is used. (Dropout seems to work extremely well for that) Theoretically, with a good enough regularization method, overfitting shouldn't be a problem at all. As Hinton points out, a human has in the order of 10^14 synapses in the brain (corresponding to the connections in the neural network), but only lives in the order of 10^9 seconds, but we still seem to be able to generalize quite well. So clearly, having many parameters that can be tuned should with the right algorithm only be an advantage.
