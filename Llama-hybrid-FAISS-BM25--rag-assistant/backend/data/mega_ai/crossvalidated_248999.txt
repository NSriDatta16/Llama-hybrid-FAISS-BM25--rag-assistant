[site]: crossvalidated
[post_id]: 248999
[parent_id]: 
[tags]: 
Bayesian dictionary learning derivations

I am trying to do the derivations and implementation of dictionary learning/sparse coding in a Bayesian way. I am not sure if the derivations are correct, or maybe my approach is totally wrong. So below are first the derivations, followed by a pseudocode, and my questions. $x = Da + n$ where $n$ is the Gaussian error with zero mean and $\sigma_2$ variance, $D$ is the dictionary, and $a$ is the sparse coding vector distributed with Laplace distribution in order to achieve sparsity. Let $x_i$ be the $i^{th}$ sample, and $x_i^j$ is the $j^{th}$ element of it where $i=1,...,N$ and $j=1,...,L$. The log-likelihood function of $n$ is as follows: $$ \sum_{i=1}^N -\frac{L}{2}\log2\pi-\frac{L}{2}\log\sigma^2-\frac{1}{2\sigma^2}(x_i-Da_i)^T(x_i-Da_i) $$ By taking the derivative of log-likelihood w.r.t. $\sigma^2$, estimator of variance is as follows: $$ \hat{\sigma^2}=\frac{1}{NL}\sum_{i=1} ^N(x_i-Da_i)^T(x_i-Da_i) $$ Similarly, dictionary can be found as follows: $$ \hat D =\frac{1}{N}\sum_{i=1}^N x_ia_i ^T(a_ia_i^T)^{-1} $$ But the inverse cannot be calculated since the product $a_ia_i^T$ is singular, so I try to estimate each atom $D_j$ in the dictionary as follows: $$ \hat D_j = \frac{\sum_{i=1}^N a_i^j(x_i-\sum_{l \neq j} D_l a_i^l)}{\sum_{i=1}^N(a_i^j)^2} $$ I assume coding vectors are i.i.d. and each of them are distributed by the Laplace distribution as follows: $$ p(a_i|\mu, b) = \frac{1}{2b}\exp\left(-\frac{|a_i-\mu|}{b}\right) $$ Therefore, for MAP estimation, I try to minimize the log-likelihood times N priors as follows: $$ \frac{\partial}{\partial a_i}\left(\left(\sum_{k=1}^N-\frac{1}{2\sigma^2}(x_k-Da_k)^T(x_k-Da_k)\right)+\sum_{k=1}^N \log p(a_k) \right) $$ which is equal to the following since derivative of the terms $k \neq i$ w.r.t. $a_i$ are zero because of the i.i.d. assumption: $$ -\frac{1}{2\sigma^2}(-2D^T(x_i-Da_i))-\frac{\textbf{sgn}(a_i-\mu)}{b} $$ where I assume $\mu=0$. The last derivative cannot be solved in closed form, so I tried to use gradient descent. My implementation is something like the following: for i=1:maxIter for j=1:numAtoms calculate_atom(j); end for n=1:N for k=1:gradDescIter a_n = a_n_old + derivative(a_n_old); end end calculate_variance(); end What are the possible reasons that I cannot get sparse vectors, and the reconstructions ($Da_i$) are totally wrong? In the dictionary learning implementation of Scikit-learn, there is a constraint that $||D_j||_2=1$. How can I apply this criteria to my approach? By introducing a prior for atoms $D_j$? What should be a reasonable choice for the number of atoms? Should I denote the error as $n_i$ instead of $n$, and variance as $\sigma_i^2$, similar to $a_i$? Can I use this model for constructing a mixture model for the purpose of classification, or use the sparse codes directly with SVM?
