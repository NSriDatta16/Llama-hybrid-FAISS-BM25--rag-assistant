[site]: datascience
[post_id]: 62538
[parent_id]: 
[tags]: 
What is the reason behind having low results using the data augmentation technique in NLP?

I used Data augmentation technique on my dataset, to have more data to train. My data is text so the data augmentation technique is based on random insertion of words, random swaps and synonyms replacement. The algorithm I used performs well in other datasets, but in mine, it gives lower accuracy results comparing to the original experiment. Are there any logical interpretations?
