[site]: crossvalidated
[post_id]: 24090
[parent_id]: 5043
[tags]: 
Your post seems to be mostly correct. The way that multiclass linear classifiers are set up is that an example, $x$, is classified by the hyperplane that give the highest score: $\underset{k}{\mathrm{argmax}\,} w_k \cdot x$. It doesn't matter if these scores are positive or negative. If the hinge loss for a particular example is zero, then this means that the example is correctly classified. To see this, the hinge loss will be zero when $1+w_{k}\cdot x_i The 1 in the hinge loss is related to the "margin" of the classifier. The hinge loss encourages scores from the correct class, $w_{y_i}\cdot x_i$ to not only be higher that scores from all the other classes, $w_k\cdot x_i$, but to be higher than these scores by an additive factor. We can use the value 1 for the margin because the distance of a point from a hyperplane is scaled by the magnitude of the linear weights: $\frac{w}{|w|}\cdot x$ is the distance of $x$ from the hyperplane with normal vector $w$. Since the weights are the same for all points in the dataset, it only matters that the scaling factor—1—is the same for all data points. Also, it may make things easier to understand if you parameterize the loss function as $L(x,y;w)$. You currently have the loss functions as a function of the linear margin, and this is not necessarily the case.
