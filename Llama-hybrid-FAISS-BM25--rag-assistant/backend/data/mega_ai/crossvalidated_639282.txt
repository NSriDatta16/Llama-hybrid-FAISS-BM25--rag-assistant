[site]: crossvalidated
[post_id]: 639282
[parent_id]: 639271
[tags]: 
Tree classifiers typically use features independently (univariate) for node splits and are not really multivariate. Thus, they're not likely to knock down the importance score of a feature because it's correlated with other features. I use importance scores to identify features for input into other classifiers with their own feature selection methods, which can reduce the impact of correlated features. This is mostly because tree classifiers were not developed to throw out correlated features. One of the advantages of random forests (RF), another tree classifier, is that frequencies for all splits don't always track with importance scores. (1st node splits seems to track with importance scores). Below are example importance scores and split frequencies for a dataset showing that the sort order for importance is not the same as the number all node splits. (Split frequencies here are based on descending Gini index for the tree).
