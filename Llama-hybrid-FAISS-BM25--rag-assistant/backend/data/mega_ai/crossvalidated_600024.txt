[site]: crossvalidated
[post_id]: 600024
[parent_id]: 
[tags]: 
Does there always exist a support vector such that $0 < a_n < C$ in SVM?

I have a question about SVM training in overlapping class distributions, where we are trying to minimize $$ \dfrac{1}{2} \Vert \mathbf w \Vert_2^2 + C \sum_{n = 1}^N \xi_n \, , $$ subject to $y_n(\mathbf w \cdot \boldsymbol \phi(\mathbf x_n) + b) \geq 1 - \xi_n$ , where $N$ is the number of training samples, $\xi_n \geq 0$ is a slack variable indicating whether sample $\mathbf x_n$ with label $y_n \in \{-1, 1\}$ has been misclassified or not and $C > 0$ . (Specifically: If $\xi_n > 1$ , then $\mathbf x_n$ lies on the wrong side of the decision boundary and is misclassified. If $0 , then $\mathbf x_n$ lies inside the margin, but on the correct side of the decision boundary. If $\xi_n = 0$ , then $\mathbf x_n$ is correctly classified and is either on the margin or on the correct side of the margin.) Using Lagrange multipliers, we equivalently seek to minimize $$ \dfrac{1}{2} \Vert \mathbf w \Vert_2^2 + C \sum_{n = 1}^N \xi_n - \sum_{n=1}^N a_n[y_n(\mathbf w \cdot \boldsymbol \phi(\mathbf x_n) + b) - (1 - \xi_n)] - \sum_{n=1}^N \mu_n \xi_n \, , $$ where $a_n, \mu_n \geq 0$ for all $n \in [N]$ . Setting the derivatives over $\mathbf w, \mathbf a, b$ equal to zero and using KTT conditions, it can be shown that $0 \leq a_n \leq C$ for all $n \in [N]$ (see Bishop's Pattern Recognition and Machine Learning). In the same book, it is stated that for all $n \in [N]$ such that $0 , we have $$ y_n \left(b + \sum_{m=1}^N a_m y_m \boldsymbol \phi(\mathbf x_n) \cdot \boldsymbol \phi(\mathbf x_m) \right) = 1 \, , $$ from which we can determine $b$ . My question is: Is it guaranteed that there always exists some $n \in [N]$ such that $0 , so that the last equation holds (or could there be cases where $a_n$ can only be $0$ or $C$ )?
