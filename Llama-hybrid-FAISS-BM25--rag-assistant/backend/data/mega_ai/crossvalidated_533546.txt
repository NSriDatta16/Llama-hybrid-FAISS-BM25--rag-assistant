[site]: crossvalidated
[post_id]: 533546
[parent_id]: 
[tags]: 
Likelihood values from Sigmoid

Repost of Mathemetics StackExchange question . There are multiple doubts of mine associated around this theme: In MLE, we try to find the PDF parameters ( $\theta$ ) which maximise the likelihood of the observed data ( $L(\theta | data)$ ). To get likelihood for a given data point for $\theta = \theta_1$ we simply evaluate the PDF for that data point. Now, we know that probability at any one particular point of a PDF is $0$ . What is the correct reasoning behind evaluating the PDF at $x=x_1$ for its likelihood? Clearly, the Sigmoid Function is not a PDF. But in the MLE estimates of Logistic Regression we see Sigmoid being used as if it is a PDF. Is my understanding correct ? If not, how to see it correctly? If yes, what is the reason behind it? This is related to the previous question. I have seen at multiple places that people take the Sigmoid to infer probability. However there is not any constraint put to ensure that sum of all those probabilities must be $1$ . What is the correct explanation behind it?
