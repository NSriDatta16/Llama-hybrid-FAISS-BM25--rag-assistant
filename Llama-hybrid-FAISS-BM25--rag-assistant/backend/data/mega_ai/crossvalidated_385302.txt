[site]: crossvalidated
[post_id]: 385302
[parent_id]: 
[tags]: 
Combining classification models for fraud detection

i have a classification problem : fraud / non fraud. My classes are inbalanced ( 0.8% fraud rows ). I first split my data in train and test sets. Let's say I have 10 fraud and 100 non fraud rows in a train set. What I want to do using python ( and sklearn package) is the following : put in front of my 10 fraud rows , 10 non fraud rows fit a model mdl_01 ( i think of xgboost rigth now but it could be anything else ... later studies will help decide ) put in front of my 10 fraud rows, 10 other non fraud rows fit a model mdl_02 ... put in front of my 10 fraud rows, 10 other non fraud rows fit a model mdl_10 So I have a bunch MDL := ( mdl_01 , ... , mdl_10 ) of models fitted on all the fraud rows available on the train set and on dedicated subset of non fraud rows available in the train set. I want to then use MDL on the entire train set so that if more models predict a row as fraud then it is classified as such. Finally, I want to make prediction on my test set to see how well it performs. My questions are : Is it a proper way to do what i describe? How can I store each model mdl_ to later on combine them ( voting step ) ? How can I combine each model mdl_ to have one final unique classification ? A few code snipets would help. Thanks.
