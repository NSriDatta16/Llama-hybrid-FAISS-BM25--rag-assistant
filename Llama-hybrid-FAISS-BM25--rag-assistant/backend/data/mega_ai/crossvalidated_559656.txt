[site]: crossvalidated
[post_id]: 559656
[parent_id]: 
[tags]: 
How can I perform a hypothesis test for this harmonic mean?

Abstract I have a software process that I try to measure with a calculation I call the EHD. The EHD value is computed this way: I perform searches for a "best fit" number in a search space, as a particular parameter is adjusted from the integer values 1 through 12. The EHD is then the harmonic mean of those 12 resulting search-function outputs. The underlying process is based on random variables, so sampling error is inherent in this calculation. A prior value for the EHD is stored in a database. As a software-regression check, I would like to be able to detect if the underlying software process has changed such that the EHD value has "truly" changed, to some specified level of confidence (say, one or two sigma if that makes things convenient). What is the best way for me to do that? System Details Comments have indicated it might be helpful to see more detail about this process. Ultimately, I'm trying to gauge "power level" for different agents in a game simulation (there are a few hundred different such agents). I assume each agent has a probability of success $S(p, n)$ where $p$ is the power level of confronting opponents (range 1 to 12), and $n$ is the total of the levels of said group of opponents (always positive, either an integer or fraction of form $1/n$ , possibly infinite in theory, but capped at 256 currently for convenience). The exact composition of the opponents and the exact performance of the agent are predicated on a large number of random variables. Note that $S$ is decreasing with increasing $n$ in almost every case (assume it's monotonic nondecreasing in $n$ ). In order to estimate the success distribution per agent, for each agent and $p$ -number I perform a binary search on the space of $n$ -values. The criteria here is whether a given $n$ -value produces an $S$ which is greater than or lesser than 0.5 (that is, whether the agent is usually winning or losing against that number of opponents). The sample size at each stage of this is usually 100 or 1000, but can be more if desired. The end result of this search is an estimate of the $n$ -value which produces the $S$ -value closest to 0.5, that is, the number of opponents that makes for a "fair" fight. That's done separately for each of the 12 $p$ -scores. To produce a single summary reporting statistic, I take the harmonic mean of the 12 critical $n$ -values. This statistic was chosen because the $n$ 's can't be zero, but can be infinite -- if the arithmetic mean were used in these cases, it would be infinite, but the harmonic mean is bounded by 12 times the minimum, so it produces a usable value. (In no case are all of the critical $n$ -values infinite.) Since the underlying game simulation has many interacting random elements, repetitions of this process produce somewhat varying final scores (EHDs) for each agent. There are times when I add features or refactor parts of the code in ways that I don't expect will change the outcomes of the game, but I use a scan of all the agents versus the recorded EHD scores as a regression check to smoke-test whether any operational behavior has unintentional changes or not. Certain system changes can effect the behavior of some of the agents, while leaving others unchanged. Ultimately the desired hypothesis test is: "This game behaves no differently than how it used to work." In practice for each agent I'm asking, "Is the EHD value the same as it used to be?" What is the best way to statistically test for such changes?
