[site]: datascience
[post_id]: 110658
[parent_id]: 97901
[tags]: 
The claim "Performance of machine learning algorithms decreases as the number of data increases" is definitely wrong, at least in general. Performance of DL model also plateau after its model capacity is reached, just like any ML model. If you think about it, any DL model is characterized by a finite set (no matter how many) of parameters, so there must be a limit on its expressiveness and thus performance. Modern DL models usually have way more parameters than classical ML models, which is the root cause of its superior performance but not without limit. FYI, Andrew Ng draws the complete chart in his DL course on Coursera Besides, remember DL is a subset of ML (not alternative), so make sure you know what it means to "compare" them. Recommendation: stop taking anything from these sources; they are harmful to your health.
