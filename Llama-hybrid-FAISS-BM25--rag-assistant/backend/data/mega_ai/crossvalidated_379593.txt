[site]: crossvalidated
[post_id]: 379593
[parent_id]: 
[tags]: 
State space and latent space instability

Reinforcement learning assumes an MDP with an a priori state space representation. Assume the state space is the raw images from a game, and we use convNets or another method to generate s latent state space. If we built the latent space first, we could run our RL algorithms on the fixed latent space representation, but what if we switched between learning a policy or value function on the latent state space, and also updating the latent space generator itself? Is there any theory or research on how to make such a system work?
