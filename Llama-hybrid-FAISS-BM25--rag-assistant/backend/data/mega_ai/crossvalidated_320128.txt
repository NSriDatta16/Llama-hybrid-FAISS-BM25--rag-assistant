[site]: crossvalidated
[post_id]: 320128
[parent_id]: 320114
[tags]: 
First, let me be very clear that the distribution function $F$ of a random variable $X$ has to be defined on the entire real line $\mathbb{R}$ . A very simple argument is, if you agreed with me that $$\lim_{x \to -\infty} F(x) = 0, \quad \lim_{x \to +\infty} F(x) = 1$$ holds for any distribution function $F$. If $F$ were only defined on a finite interval $[a, b]$, then how would you make sense of the above two limits? In the subsequel, I would like to discuss your question in somewhat more theoretical flavor. There may be some applied scearnios under which people used this term loosely, which is not within the scope of my answer here. To deeply understand some arguments below, you may need some advanced probability knowledge. First let's make it clear that the distribution function $F$ is a derivative of: An original probability space $(\Omega, \mathscr{F}, P)$. A random variable $X$ which is defined on $(\Omega, \mathscr{F}, P)$ and takes values on $\mathbb{R}$. Note that all random variables, regardless of discrete, continuous, mixed, singular, are subdumed in 2. The thing that matters the concept of distribution $F$ is not whether $X$ is continuous or discrete, but it is a measurable function from $\Omega$ to $\mathbb{R}$. In other words, $X$ is a function that sends every $\omega \in \Omega$ to $X(\omega) \in \mathbb{R}$, subject to some constraint required by measurability. We can ignore the term "measurable" temporarily. The definition of the distribution function $F$ of $X$ is $$F(x) := P[X \leq x] \equiv P[\{\omega: X(\omega) \leq x\}], \quad x \in \mathbb{R}. \tag{1}$$ Well, if I stopped here, it would not answer your question at all, as $x \in \mathbb{R}$ is embedded in the definition. But let's just pause and ask ourselves why mathematicians put $\mathbb{R}$ as the domain of $F$? For example, suppose $X$ is a binomial random variable whose range is just two points $\{0, 1\}$. Is it OK that for this $X$, to restrict the domain of $F$ to be $[0, 1]$? The usefulness of a distribution function $F$ lies on that we can directly reads from it about the probability of the event that $X$ is no more than $x$ for any $x \in \mathbb{R}$. If, you restricted the domain of $F$ on $[0, 1]$, then what's your immediate answer if someone asked you what is the probability that $X$ is no more than $100$? This may look trivial to you because you would say "come on, this is of course $1$, you even don't need a distribution function $F$ to answer it". However, it makes perfect sense for a stubborn mathematician to ask such a question and demand an answer that is only based on the information of $F$. Note that, the answer from you is based on logic, instead of relying on $F$ only and directly, i.e., you gave $1$ as the answer because you know $X$ is binomial. By contrast, probabilists introduced the concept of distribution with the hope that (and they made it) $F$ summarizes all the uncertainty information of the random variable $X$. In other words, they hoped that even you forgot what the type of $X$ at all in your head, as long as you had access to its distribution function $F$, you would still be able to answer the question "what is the probability that $X$ is no more than $100$?", by simply evaluating $F$ at $100$. It is also a piece of cake if he continued to challange you by replacing $100$ with $10000, -\pi, \sin(e), \ldots$. You might feel it is difficult to those questions if you are only willing to define $F$ on a finite interval $[0, 1]$. In summary, to completely summarize the uncertainty of a random variable $X$, its distribution has to be defined on the entire real line $\mathbb{R}$ . This intuitive argument corresponds to the more technical reason that the the Borel $\sigma$-field $\mathscr{R}^1$ is generated by the collection of sets $\{(-\infty, x]: x \in \mathbb{R}\}$. If $x$ didn't go over the entire $\mathbb{R}$, the collection wouldn't be sufficient to recover $\mathscr{R}^1$ (the family of events that we are interested in). Another reason is more mathematical: to require that the distribution function has domain $\mathbb{R}$ helps give a unified formula for computing expected value . At its original form, the expected value of $X$ is defined to be the Lebesgue integral on the original probablity space $(\Omega, \mathscr{F}, P)$ with respect to the probablity measure $P$: $$E[X] = \int_\Omega X(\omega) P(d\omega).$$ This equation is less familiar to people who didn't take measure-based probability course. They are more familiar to the formula: \begin{align} E[X] = \begin{cases} \sum_{i = 1}^m x_ip(x_i) & \text{ if } X \text{ is discrete with pmf } p, \\ \int x f(x) dx & \text{ if } X \text{ is continuous with pdf } f. \end{cases} \tag{2} \end{align} In fact, it can be shown that by defining $F$ as in $(1)$, $E[X]$ can be expressed much more compactly than $(2)$ as: $$E[X] = \int_{\mathbb{R}} x dF(x), \tag{3}$$ regardless of $X$ is continuous or discrete. The integral in $(3)$ can be interpreted in Riemann-Stieltjes sense, which only makes sense if $F$ is defined on the entire real line. Note that $(3)$ is correct because of the equality $$\int_{\Omega} X(\omega) P(d\omega) = \int_\mathbb{R} x dF(x) \tag{4}$$ holds. Suppose, if $F$ were only defined on a finite interval of $\mathbb{R}$, then part of the integral on the right hand side of $(4)$ would be undefined and we would not be able to use the elegant formula $(3)$. As for the example, Alex R. gives an illustrative one. If you refused to complete $F(x)$'s definition as $1$ for $x \geq 1$, it would be impossible for you to get the correct answer of $E[X]$ using the formula $E[X] = \int_0^\infty (1 - F(x)) dx$. Let's see what is the consequence of if $F$ is not completely defined. For a $\text{Bin}(1, 1/2)$ random variable $X$, OP defines its distribution as \begin{equation} F(x) = \begin{cases} 1/2 & x = 0 \\ 1 & x = 1 \end{cases} \tag{5} \end{equation} and left $F$ undefined on other points (if I interpret correctly). The definition $(5)$ has fundamental issues as it didn't uniquely recovered the probabilistic property for $X \sim \text{Bin}(1, 1/2)$. Because OP didn't define $F$ on $(1/2, 1)$, there are infinitely many possibilities that $F$ can be on this interval, which would lead infinitely many probabilities that a $\text{Bin}(1, 1/2)$ should not have. For example, since you give me the freedom to define $F$ arbitrarily on other points other than $\{0, 1\}$ (as you gave up defining them!), I can define $F$ as \begin{equation} F(x) = \begin{cases} 0 & x The culprit of this is simply because you didn't define $F$ unambiguously on the entire real line $\mathbb{R}$. Again, to define $F$ completely is not only for elegance, it is more about unambiguity and accuracy to describe the probabilistic property of a random variable. The ambiguity above can be eliminated if you define $F$ conventionally as follows: \begin{equation} F(x) = \begin{cases} 0 & x Based on $(7)$, you get all correct probabilities of a Bernoulli r.v. should have. You might argue that $X$ can't take value at $1/2$ on the above example, but remember that my point is, the distribution function $F$ should be such that it gives all correct probabilities by itself alone of a random variable, without any other further prior information. In other words, $X$ and $F$ should be able to determine each other unambiguously, given the availability of one of them . Can your representation $(5)$ unambiguously identify a $\text{Bin}(1, 1/2)$ r.v. successfully? I showed you it cannot. Therefore, elegance is second place, correctness is the probabilist's first concern.
