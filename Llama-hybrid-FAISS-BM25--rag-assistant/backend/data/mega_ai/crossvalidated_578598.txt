[site]: crossvalidated
[post_id]: 578598
[parent_id]: 
[tags]: 
How do I calculate the variability in the time taken for a given number of random events of a given frequency to occur?

I'm trying to build a clock in Minecraft that measures approximate time according to random events. For a deeper explanation as to why I'll make a comment below, but basically I want to make a clock that fires to approximate a given number of minutes by counting the number of sugar cane items that randomly grow from a given number of plants. It's an unusual problem which I suspect is why answers are difficult to find. So in my case, sugar cane grows on average every 18 minutes, so if I have 18 plants, I can expect one new growth every minute on average. If I have 36 plants then I can expect one growth every 30 seconds on average. So if I have 18 plants then I should should count 100 growth events to approximate 100 minutes passing. If I have 36 plants I should count 200 events, which should be more accurate. The question I have is, how accurate can I expect this timer to be? If it counts 10,000 growths from 1,800 plants then how much more accurate will it be compared to counting 100 growths from 18 plants? What would be the distribution? For practical reasons I'm trying to count 167 minutes by observing 32 plants, so I'm counting 1/18 * 32 * 167 ~= 297 (rounded) growth events, but I'd like to know the general method for any number of plants and amount of time.
