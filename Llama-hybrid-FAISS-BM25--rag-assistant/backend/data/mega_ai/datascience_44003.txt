[site]: datascience
[post_id]: 44003
[parent_id]: 43986
[tags]: 
OK, first regarding the real time part. Machine learning (particularly supervised learning) has two parts Training Predicting Training is a complex and slooooow process, you need to figure out a good model, then train it with the correct data... not simple and computationally expensive. Predicting however is MUCH simpler, as normally consist (in the world of neural networks) of a simple forward pass in the neural network, and a forward pass is, at its core, a bunch of matrix multiplications. So once a model has been built and train, predicting something using the existing model is quite quick, also one can retrain the model with new data too. Now, regarding with the text searches and how to deal with them. There is a very cool idea called word embeddings, it simply consist on transforming words into vectors. Imagine a 2d grid (like the x,y coordinates we studied in school), now imagine that the vertical axis (y) denotes how powerful something is, the hifher the y value, the more powerful it is. And now imagine that the x axis denotes the gender of something. (lets assume negative x means male and positive x means female). And now thing of the word : King. King is male (x=Negative value) and powerful (y=Positive value). With those two coordinates, and starting from position 0,0 you could have a vector. Now think of Queen, in this case the coordinates will be x=Positive value and y=Positive value, now you have another vector. And now think of the word Spartacus (he was a slave in ancient rome), so that word will have x=Negative and y=Negative. Think of this, suddenly not only you can represent the words as numbers, you can also represent them with vectors, which allows you to possibly figure out that King and Emperor are both SIMILAR words.
