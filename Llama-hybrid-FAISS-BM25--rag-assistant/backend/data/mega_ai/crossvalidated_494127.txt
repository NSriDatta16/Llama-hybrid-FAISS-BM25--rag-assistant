[site]: crossvalidated
[post_id]: 494127
[parent_id]: 494109
[tags]: 
Your post seems to be starting from the assumption that we are all making a silly mistake by not using $n^{3/4}$ -sized training sets. This is not true, as there are precise senses in which AIC and LOO are the "right thing to do" despite the fact they are not model-selection consistent . Nothing helps clarify these things better than a simple simulation study. Let's look at a linear model with $N = 625$ samples with the model $$ Y_i = \sum_{j=1}^3 X_j + \epsilon_i $$ with a total of $5$ predictors, all $X_{ij} \sim N(0,1)$ . We will consider five models, each containing $(X_{i1}, \ldots, X_{ik})$ for $k = 1,\ldots, 5$ . We will do the following: Select the best model according to AIC. Select the best model according to a train/validation split which takes a training set size of $125$ . Compute the mean prediction error on a third untouched dataset with $N = 10,000$ , using the selected models fit to both training and validation (since the $n^{3/4}$ approach will obviously not do well otherwise). We will repeat this some number of times and see what we conclude. set.seed(11235813) N_rep Results AIC outperforms the approach suggested by OP in terms of average prediction error, by a small-but-detectable amount. The reason is that, while AIC will often select models which are too large and take a small hit in accuracy, the $n^{3/4}$ approach will occasionally leave a predictor out. When a predictor gets left out, you take a huge hit in prediction, and its enough to offset the times you selected the correct model when AIC did not. None of this contradicts any theory in any papers. It is indeed true that the $n^{3/4}$ model selects the correct model more often than AIC. But that is not much consolation if your goal is prediction, which is the case in most machine learning applications.
