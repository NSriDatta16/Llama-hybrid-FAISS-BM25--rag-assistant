[site]: crossvalidated
[post_id]: 493043
[parent_id]: 
[tags]: 
Understanding usefulness of log of odds in logistic regression

I'm trying to understand the importance of the log of odds in logistic regression, typically I understand the formulation but unable to get the intuition behind it. Linear regressor gives a continuous value for prediction while Logistic regression is used to classify multi-label classes. According to the formula: y = Xβ+u wherein the case of logistic regression y is the probability measure given: log(p / 1−p)= Xβ ---(1) And eq (1) is called logits, why is this so, why can't it be just raw values and further passed to softmax will convert it into probability measure anyway? In the case of multi-label classification we just want to know the probability values corresponding to the respective classes which tell us what's the most probable mapping from given feature vectors to class labels. I'm just not able to process the existence of logit. Why is that helpful/required, what's the usefulness of taking log of odds, how LHS equals RHS, what does it signifies or why does that exist in this case? I'm not from a statistical background and beginner in ml, so wanted a deep insight. If I'm missing something, please clarify!
