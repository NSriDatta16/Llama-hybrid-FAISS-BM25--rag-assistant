[site]: crossvalidated
[post_id]: 185155
[parent_id]: 184952
[tags]: 
True, some details used for improving performance are considered as tricks and you won't always know if these tricks yield the same improvement for your data and your network. Some things that you will definitely need: Data , lots of it GPUs will let you run experiments faster and try out more things in a shorter time span. Learning curve analysis. In the end it comes down to performance on the test set, but looking at the both train and test metrics you can identify reasons for bad performance. Strong bias? Overfitting from too many hidden nodes? The activation function . I don't think it counts as a trick to know which kind of activation function you need. ReLU have a critical charactersitic in that they do not saturate like sigmoids and tanh. A neuron with ReLU will longer have probability-like output, but you don't need this for neurons in mid-level layers anyway. The advantag you get is mitigating the vanishing or exploding of gradients and speed up convergence. Regularization . Might apply as tricks, but if you're using any of the mainstream deep learning libraries you can get off-the-shelf implementations for regularization via dropout. Data augmentation. You're basically expanding your dataset synthetically without the added cost of manual annotation. The key is to augment the data with transformations that actuall make sense. So that the network gets to see variants of the data it may encounter in the test phase or when it gets deployed into the product. For visual data it horizontal flipping is trivial and adds a lot of gain. Jitter is probably dependent on the type of data and how noisy it is. Diving into hyperparameter exploration can be frustrating. Start off with small networks and simple training procedures. Smaller networks are faster to train. Add more layers when you see signs of overfitting. Good initialization . Random intitialization are appropriate for gauging the network's ability to converge but will not necessarily lead t optimal performance. At the same time, just keeping on iterating might lead to the network overfitting to the training data. If possible use a pre-trained network that has already learned a representation and fine tune it to your dataset. Unsupervised pre-training is another way to go and can allow the supervised training procedure to start from a far more promising position in weight space. Scrutinize tricks. Understand what the trick really does. A paper describing a small detail that was used in improving the performance of a network will focus on that new aspect. The paper could be part of a sequence of projects that the authors have been working on. The context of the trick may not always be clear right away but for the authors it's not a trick but a technique that solves a problem they had. Sometimes a technique comes out and is treated as a trick and later someone will analyze its impact and describe its function. For example that this trick is equivalent to L2 regularization which more people are familiar with. We can the decide if we should try out this new technique or stick with the L2 regularization that we already know about. A lot of these tricks try to solve problems in deep learning, like risk of overfitting, costly computations, over parameterization and highly redundant weights. It's worth taking the time to understand what these tricks really do. By understanding the problem they try to solve we can judge the applicability of different tricks and pick the one that works well with constraints we may have (e.g. little computing power, small dataset)
