[site]: crossvalidated
[post_id]: 232935
[parent_id]: 232911
[tags]: 
If measurement errors are unbiased and also independent and identically distributed , averaging your measurements together will reduce the error. This is sometimes known as "signal stacking", and is commonly done in electrical engineering and geophysics to increase the signal-to-noise ratio. Conceptually, the independent errors cancel each other out. Mathematically, this is essentially the standard error of the mean , where here the mean is taken over an ensemble of signals, and so is time varying. If you have $N$ time series, $y_i[t]=x[t]+\epsilon$, for $i=1\ldots N$, then you compute the average signal as $$\bar{y}[t]=\frac{1}{N}\sum_{i=1}^Ny_i[t]$$ If the errors are independent with mean $\langle\epsilon\rangle=0$ and variance $\langle\epsilon^2\rangle=\sigma^2$, then your ensemble-averaged signal will have a reduced error variance given by $$\langle(\bar{y}[t]-x[t])^2\rangle=\tfrac{1}{N}\sigma^2$$ This all holds even if you do not know the noise distribution. In your case, you do know the distribution. Since the errors are Gaussian, the ensemble average will also have Gaussian errors, and is the maximum likelihood estimate of your signal. Because you also know $\sigma$, you can explicitly compute the reduced error variance. From this, you can calculate confidence intervals (bands) on your reconstructed signal.
