[site]: crossvalidated
[post_id]: 326553
[parent_id]: 326508
[tags]: 
After some further thinking I now came to a (sort of) intuitive interpretation myself. It is actually not that hard if one is familiar with PCA in the context of covariance and variance. We denote by $X$ a dataset $X_1 \dots X_n$ with $X_i$ being a random vector of dimension $d$ (this is a slightly different notation than I used in my question). If we look at the eigendecomposition of $\Sigma$ one observes that multiplying $r$ with $\Sigma$ just stretches $r$ in the directions of the principle components of the dataset $X$ by the according eigenvalues component (the variances of the dataset in the directions of the principle components). A nice property of this linear mapping is, that $r^T \Sigma r$ returns the variance of the dataset in direction of $r$ (assuming $|r| = 1$ , otherwise it returns the variance times $|r|^2$ ). With this being said, lets look at the term $r^T \Sigma^{-1} r$ . The term $\Sigma^{-1} r$ now retracts the vector $r$ to $r' = \Sigma^{-1} r$ again according to the principle components (scales by the reciprocal value of the eigenvalues of $\Sigma$ along the principle components (eigenvectors)), as this is just the reversed operation. This means, that we receive with the previous term just the variance along $r'$ (again assuming $|r'| = 1$ . A small proof of this statement: $$r^T \Sigma^{-1} r = \left( \Sigma^{-1} r \right)^T r = \left( \Sigma^{-1} r \right)^T \Sigma \Sigma^{-1} r = r'^T \Sigma r'$$ which is equal to the variance of $X$ along $r'$ .
