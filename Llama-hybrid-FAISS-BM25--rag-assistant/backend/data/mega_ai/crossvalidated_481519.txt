[site]: crossvalidated
[post_id]: 481519
[parent_id]: 481457
[tags]: 
All three of your questions are yes. Feature importance is one of the most significant purposes of machine learning. Including features that are highly correlated with one another is almost always avoided because it is better to only include one representative from a correlated group of features in your model while discarding the other ones that are deemed to be redundant. Using only one representative feature per group is known to enhance performance and essentially underlies the motivation of principal component analysis (PCA), which is a dimensionality reduction technique, and ridge regression and lasso. Other usages of correlation analysis of machine learning besides feature importance can probably found in the functionality of clustering algorithms which are unsupervised learning models that employ distance metrics to group blobs of data together based on their similarities. The correlation that exists amongst the features guides the clustering process.
