[site]: crossvalidated
[post_id]: 407440
[parent_id]: 
[tags]: 
what is the best approach in dealing with large dimension custom data for training and predicting deep learning models

i am trying to implement semantic segmentation for satellite images.My custom dataset has dimensions(height,width)in range (3000, 3000)what is the best approach for feeding(for training) and predicting on this dataset. Should i take random cropping or use resizing to a target size or something else? Will i loose resolution in case of resizing? I am asking because pretrained networks(like unet) have height and width in the range 300-400 only.
