[site]: datascience
[post_id]: 112238
[parent_id]: 69529
[tags]: 
To some extent, I disagree with every one of these. Nonlinear regression models (e.g., SVM) do not assume a linear functional form, and nonlinear basis functions (e.g., polynomials or splines) can allow linear models to fit trends that have curvature. Constant error variance could be useful for certain forms of inference, but it is hardly necessary for predictions. Time series models handle autocorrelation. Error term normality means that the OLS solution for a linear model corresponds to maximum likelihood estimation of the regression weights, and this is nice for doing inferences with the weights or with nested models. However, such inferences are pretty robust to deviations from normality (especially with large sample sizes). Further, other regression models like quantile regression assume a different error distribution in order to correspond to maximum likelihood estimation. In particular, quantile regression at the median (minimizing MAE) corresponds to maximum likelihood estimation for a Laplace-distributed error term. Even in OLS regression, there is no assumption of a lack of feature multicollinearity. Depending on the complexity of the model, my assumption might be that I have missed an important feature. If I can produce a useful model despite this, good for me.
