[site]: crossvalidated
[post_id]: 494688
[parent_id]: 
[tags]: 
Best loss functions and data scaling methods for dataset with large variance in output variables?

I work on a regression model using feed forward neural networks. I have a dataset of $\sim 10^7$ entries, connecting a vector with a vector . The dataset is such that the elements of $Y$ have a very large variance, e.g. their range is from $10^{-2}$ to $10^3$ . Nevertheless I am interested in having a minimal relative error over the whole domain of $Y$ . So far I standardize by removing the mean and scaling to unit variance, while removing the linear correlation across before training. I've tested various approaches using RMSE, MAPE, sMAPE and MSLE loss functions and log-scaling $Y$ . But none of the trials gave, very convincing results. MAPE, sMAPE and MSLE all do not punish strong enough outliers, hence a large fraction of points have significant errors > 10%. The best outcome I got when prior to training, two of the three elements of $Y$ were log-scaled, then all the elements of $Y$ scaled to unit variance and the RMSE loss function was used during training. Though the network failed predicting accurately the small output values of the element of $Y$ which was not log scaled. What are good heuristics for such a regression problem? Or are there relative loss functions which punish outliers more than MSLE or MAPE? I find also very few literature on this kind of problem, some recomendations would already be usefull.
