[site]: crossvalidated
[post_id]: 612818
[parent_id]: 612617
[tags]: 
In principle, multiple imputation might be applicable. The missing data might be considered "missing at random" in the technical sense explained in that reference, because your data seem to contain the reason for the missingness: some facilities provide it, others don't. Using all of your available data in a well-designed multiple imputation model could provide a way forward. That might particularly be true in your situation, where you hypothesize that your (often missing) non-invasive test of interest adequately represents the results of invasive tests, whose results are presumably also available in your data. In that case, the results of the invasive tests might be used to get reasonably consistent imputations of the results of the non-invasive test. That approach repeats the modeling on multiple data sets, each with imputations done probabilistically. You combine the results of the multiple models in a way that takes the uncertainty in imputation into account. In general, with so many missing data values, you might have very wide confidence intervals around your estimates for the association between the non-invasive test of interest and the disease status. If your hypothesis about the ability of the non-invasive test to capture information provided by other tests holds, however, then there might be little enough variability in its imputed values to provide reasonable estimates. You say that random forests and gradient boosting handle missing data well. That can be true, but be sure to know which of several approaches are used in your implementation. See the discussion on this page . Imputation is one type of missing-data handling in those types of models, too.
