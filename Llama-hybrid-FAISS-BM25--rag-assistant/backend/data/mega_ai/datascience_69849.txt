[site]: datascience
[post_id]: 69849
[parent_id]: 69844
[tags]: 
I think your interpretation does not focus on the right places. First, let's understand where the predecessor idea of Transformer XL in LSTM Language Models: Truncated Back-Propagation Through Time (TBPTT) . In LSTM Language Models, the training data text is concatenated into a single very long sequence. From this sequence, we create training batches in a way that for batch $k$ , the text sequence at position $i$ is the continuation of the sequence at the same position in batch $k-1$ . When training the LSTM, the last hidden states generated at batch $k-1$ were used as initial hidden states when processing batch $k$ . The gradients were not propagated through batches, only the values of the hidden states. This allowed the LSTM to be able to profit from unlimited-length context. The same idea cannot be directly applied to Transformers, as there is not a single hidden state, but as many hidden states as the number of tokens in the sequence times the number of layers. TransformerXL solves that by incorporating all those hidden states from the previous batch into the computation of the new batch, without propagating the gradients (see figure below from the original article). This, however, poses a problem in how the positional information in managed in the Transformer, as the positional embeddings encode absolute positions. Therefore, the authors reformulated them to be relative. Therefore, the main advantage of Transformer XL over vanilla Transformer LMs is that it can profit from unlimited-length context. Note that Transformer XL is "only" a Language Model, it does not translate, just computes probabilities of the next token based on the previous ones.
