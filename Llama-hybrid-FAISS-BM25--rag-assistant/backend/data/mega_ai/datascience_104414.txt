[site]: datascience
[post_id]: 104414
[parent_id]: 
[tags]: 
Standardization vs min-max scaling

In the book Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow 2nd Edition by Aurélien Géron, the author quoting: Unlike min-max scaling, standardization does not bind values to a specific range, which may be a problem for some algorithms (e.g., neural networks often expect an input value ranging from 0 to 1) Does this mean that standardization is not good for neural networks? Please explain how neural networks do not work well with standardization but are good with the min-max scale.
