[site]: datascience
[post_id]: 20528
[parent_id]: 20511
[tags]: 
One promising algorithm is "Word Mover's Distance" . It treats document similarity as an optimization problem between word embedding vectors. The goal is to minimize the cost of "travel" between sets of word vectors. Given the semantic encoding inherent in word vectors, it can model aspects of intention and meaning. Word Mover's Distance (WMD) is relatively simple to implement. However, it is computationally expensive because of the number of comparisons it makes. WMD completely ignores POS, NER, parse trees, and all other related NLP techniques. Those restrictions make it easier to apply, however, limits the properties of language it can model.
