[site]: datascience
[post_id]: 663
[parent_id]: 662
[tags]: 
Check out this link. Here, they will take you through loading unstructured text to creating a wordcloud. You can adapt this strategy and instead of creating a wordcloud, you can create a frequency matrix of terms used. The idea is to take the unstructured text and structure it somehow. You change everything to lowercase (or uppercase), remove stop words, and find frequent terms for each job function, via Document Term Matrices. You also have the option of stemming the words. If you stem words you will be able to detect different forms of words as the same word. For example, 'programmed' and 'programming' could be stemmed to 'program'. You can possibly add the occurrence of these frequent terms as a weighted feature in your ML model training. You can also adapt this to frequent phrases, finding common groups of 2-3 words for each job function. Example: 1) Load libraries and build the example data library(tm) library(SnowballC) doc1 = "I am highly skilled in Java Programming. I have spent 5 years developing bug-tracking systems and creating data managing system applications in C." job1 = "Software Engineer" doc2 = "Tested new software releases for major program enhancements. Designed and executed test procedures and worked with relational databases. I helped organize and lead meetings and work independently and in a group setting." job2 = "Quality Assurance" doc3 = "Developed large and complex web applications for client service center. Lead projects for upcoming releases and interact with consumers. Perform database design and debugging of current releases." job3 = "Software Engineer" jobInfo = data.frame("text" = c(doc1,doc2,doc3), "job" = c(job1,job2,job3)) 2) Now we do some text structuring. I am positive there are quicker/shorter ways to do the following. # Convert to lowercase jobInfo $text = sapply(jobInfo$ text,tolower) # Remove Punctuation jobInfo $text = sapply(jobInfo$ text,function(x) gsub("[[:punct:]]"," ",x)) # Remove extra white space jobInfo $text = sapply(jobInfo$ text,function(x) gsub("[ ]+"," ",x)) # Remove stop words jobInfo $text = sapply(jobInfo$ text, function(x){ paste(setdiff(strsplit(x," ")[[1]],stopwords()),collapse=" ") }) # Stem words (Also try without stemming?) jobInfo $text = sapply(jobInfo$ text, function(x) { paste(setdiff(wordStem(strsplit(x," ")[[1]]),""),collapse=" ") }) 3) Make a corpus source and document term matrix. # Create Corpus Source jobCorpus = Corpus(VectorSource(jobInfo$text)) # Create Document Term Matrix jobDTM = DocumentTermMatrix(jobCorpus) # Create Term Frequency Matrix jobFreq = as.matrix(jobDTM) Now we have the frequency matrix, jobFreq, that is a (3 by x) matrix, 3 entries and X number of words. Where you go from here is up to you. You can keep only specific (more common) words and use them as features in your model. Another way is to keep it simple and have a percentage of words used in each job description, say "java" would have 80% occurrence in 'software engineer' and only 50% occurrence in 'quality assurance'. Now it's time to go look up why 'assurance' has 1 'r' and 'occurrence' has 2 'r's.
