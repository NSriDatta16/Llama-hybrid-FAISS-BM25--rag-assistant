[site]: crossvalidated
[post_id]: 455529
[parent_id]: 
[tags]: 
When MAP beats ADVI

I currently observed surprising results, I'd gratefully if someone could help me with understanding them. Unfortunately, I would not be able to share details of the data, nor of the discussed model, since both are proprietary, but I'll try giving as much details as possible. I am working with a complicated Bayesian model, implemented in PyMC3 . Due to computational reasons, we need to be using ADVI (Kucukelbir et al, 2015 , 2016 ), rather then MCMC. Additionally, we are using minibatches, since we observed them to speed up the training and improve the results (as measured by test set performance and posterior predictive checks). Since what we also need to do, is to be able to make the predictions on demand, we were exploring ways to speed up making the predictions, including using maximum a posteriori (MAP) estimate (using L-BFGS-B optimization), instead of sampling from posterior in such cases. When I compared errors of the results obtained using MAP vs ADVI, I observed that both train and test root mean squared error were lower for MAP. The point estimates for ADVI were obtained by taking 5000 samples from the posterior distribution, making predictions based on them, and then taking mean of the predictions (but same results hold for median). This is pretty surprising, since ADVI is considered as a state of the art optimization method for such problems. What could be the potential reasons for such results?
