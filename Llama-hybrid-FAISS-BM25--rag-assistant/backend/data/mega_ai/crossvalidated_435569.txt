[site]: crossvalidated
[post_id]: 435569
[parent_id]: 164267
[tags]: 
"Why does it work when it works?" Another answer here quotes E. T. Jaynes. He says that other machine learning algorithms are ad hoc, but neural network are not. The reason is that the algorithm corrects itself. In reality, if you have $n$ instances in a sample, why would it be better to use them one after another rather than use them all together? There is no proof that NN approach is better. In most of cases with limited data NN is, actually, worse. So, all machine learning are similarly ad hoc. Machine Learning is similar to alchemy: there are plenty of enigmatic recipes, you apply one, and you may get gold. If not, just apply another recipe. Nobody asks the question you asked, at least not in the publications I know. On top of this, there is statistical learning theory. Statistical learning theory assumes that the size of the training set goes to infinity. Most of results I know have the form: "under certain conditions, if you have a large enough training set, you can get almost as good result as possible using this procedure". The estimates of what is "large enough" are beyond imagination. Of course, the problem is, the training set size is not going anywhere, let alone to infinity. So, I think, it is a good time to (1) ask this question, (2) to develop a mathematical apparatus to answer the question about all possible machine learning algorithms and (3) answer this question.
