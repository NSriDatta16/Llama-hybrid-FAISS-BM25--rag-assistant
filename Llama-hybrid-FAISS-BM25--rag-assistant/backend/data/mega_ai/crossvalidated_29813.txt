[site]: crossvalidated
[post_id]: 29813
[parent_id]: 29760
[tags]: 
No, association rule mining is not the way to go. Theoretically it could be applied in such a way that only rules are generated where the premise does not contain click=yes and the conclusion only contains click=yes. However, as soon as one of your premise variables is numerical, you are going to run into trouble. Hence I suggest to take a look at white box classification models, for example decision trees . By learning such a model one can either predict the clickrate (regression, numerical label) or the click itself (classification, binomial label) for a given add. There was a related question here about whether to model a related problem as regression or classification, but without conclusion: Performance metric for algorithm predicting probability of low probability events . As a "side effect", such a model delivers ... differentiation power of variables, i.e. how much influence this variable has on the performance of the add rules with the premise/conclusion constraints as described above I recommend to try Random Forests , the current state of the art in the area of decision trees, first. It is more stable then a single decision tree, but the downside is that extracting meaningful rules is more complicated. In the book The Elements of Statistical Learning is a whole chapter devoted to this algorithm, also note that we have already a tag random-forest and hence a bunch of helpful questions here on stats.SE. Regarding the problem of class imbalance (small amount of clicks, many showns without clicks), you may want to look at the tag unbalanced classes and at this question How do I report error from imbalanced data in a random forest algorithm? , i.e. at the comments to the question itself.
