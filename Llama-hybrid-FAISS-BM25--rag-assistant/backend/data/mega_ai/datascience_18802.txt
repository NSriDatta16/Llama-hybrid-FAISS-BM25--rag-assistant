[site]: datascience
[post_id]: 18802
[parent_id]: 
[tags]: 
Does MLP always find local minimum

In linear regression we use the following cost function which is a convex function: We Use the following cost function in logistic regression because the preceding cost function is not convex whenever the hypothesis (h) is logistic function. We have changed the equation of cost function to have a convex shape to find its global (the only one which exists). There is a fact that I can not understand. In Multi Layer Perceptrons ANNs I have seen a lot that they can be stuck in local minimums. Why is that? We have used this cost function for each perceptron and gotten the rules for updating the values for the weights in back propagation algorithm; So why do we stuck?
