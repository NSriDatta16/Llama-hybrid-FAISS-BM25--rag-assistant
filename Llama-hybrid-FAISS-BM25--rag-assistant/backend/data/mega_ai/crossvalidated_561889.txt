[site]: crossvalidated
[post_id]: 561889
[parent_id]: 561842
[tags]: 
Your question isn't quite clear because statistics is a vast field and you are referencing some commonly used tools that may or may not be appropriate for the problem. I will try and answer it, nonetheless. The first question seems to be about using probability distributions. It is important to remember that they are theoretical models of the world created by the physics of the situation. If you change the specifics of the problem to the point that you change the physics, then you may change the probability distribution. A probability distribution is the solution to a problem. There are many standard named solutions because there are many types of problems that people have already tried to solve. The empirical distribution using the histogram or other graphical methods such as kernel densities may not be representative of the population's density. If you had a much larger sample, the shape may change. The population density will not change, ceteris paribus. One specific issue, though, you keep mentioning averages. The properties of averages may not be the properties of the sample. Anytime you aggregate information you are engaging in a mathematical transformation that will often change the rules of math regarding probability distributions. There may already be a proposed probability distribution in the literature. It may even have a rigorous proof around it. The second question is implicit but missing. The wording of your question may change the methodology that you use. Let us assume that you have worked out the physics of your particles, that you have a probability distribution and a model. You have collected your data. What you have not clearly described is "so what?" You stated Specifically, I calculate the probability of a random particle forming an aggregate of a particular size None of us know how to actually do that. It is an unsolvable problem. What we can tell you are multiple competing ways to model this problem, but some are better than others depending on the usage and underlying facts. Unless it is a toy problem, there will be things that domain experts know that we do not which are built into the problem. If we knew them, it may change the rules to model the system. For example, if there were boundary conditions, it may change the probability estimates. There is nothing wrong with binning the data as you have and using that as an estimate of your probability. The rest of the histogram doesn't matter because you can approximately answer the question of $\Pr(a for the one bin of interest. That assumes that your sample is representative and large enough. Your intuition is correct, though, that knowing the distribution could allow better statements about probabilities. The reason is that if you know the shape of the population, or the sampling distribution of an average when using averages, and your empirical shape does not match, then you can incorporate the uncertainty of your estimate into your probability estimate. If your data is lopsided and you believe it should be symmetric, then you can still make reasonably accurate statements despite the fact the sample does not conform to the population. Conversely, if your data is symmetric and you believe it should be lopsided, you can still make accurate statements despite its symmetry. Finally, the explicit question at the top of the page is: Why are raw data analyzed rather than the probability distribution the data are sampled from? Some methods use the likelihood function, which is closely related to the probability distribution involved, to analyze data. It is a very powerful tool to use, provided that you get the distribution correct. Then the raw data is analyzed with respect to the distribution involved explicitly. On the other hand, there are methods that do not use the likelihood function. Their advantage is that they work on many or even all possible probability distributions. Their solutions are robust but do not have as much precision. Unlike other areas of mathematics, the field of statistics lacks uniqueness proofs. There is rarely just one way to do things. Binning data, as long as the sample is large enough and representative, works fine. Implicitly, the physics are present in the histogram. If you can be explicit about the math involved, there may be a way to improve your estimate. The difference is the difference between using calculus in physics and using no math at all. You could perform a very large number of experiments on a specific phenomenon in a narrow way to determine outcomes. If you could convert the system to a set of equations, however, you can generalize them and determine if there is an effect in the system that you had not accounted for. You also would need less data to determine the same thing that more data provided in the experiment-only case. The former works well but is very expensive. The latter costs less and allows you a warning that all is not as you believe it should be. Some branches of statistics analyze data with respect to distribution and some do not. Your goal should determine what method you use.
