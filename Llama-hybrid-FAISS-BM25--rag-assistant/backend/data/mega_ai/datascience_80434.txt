[site]: datascience
[post_id]: 80434
[parent_id]: 79772
[tags]: 
BERT is a transformer based model. The pre-trained BERT can be used for two purposes: Fine-tuning: This involves, fine-tuning with the new data to a specific task such as classification or question-answering , etc. Here, the BERT itself acts like a classifier. Extracting embeddings : Here, you can extract the pretrained embeddings. The difference between Word2Vec (or other word embeddings) and BERT is that BERT provides contextual embeddings, meaning, the embeddings of each word depends on its neighbouring words. However, since it's contextual embeddings, we can make an assumption that the fist token which is '[CLS]' captures the context can be treated as sentence embeddings as be used as input to 'SVM' or other classifer. But, in the case of RNN, you may want to take the pretrained embeddings of each token to form a sequence. So, how you want to use BERT still remains a choice. But if you can fine-tune the BERT model, it would generally yield higher performance. But you'll have to validate it based on the experiments.
