[site]: datascience
[post_id]: 98251
[parent_id]: 77044
[tags]: 
The CLS token helps with the NSP task on which BERT is trained (apart from MLM). The authors found it convenient to create a new hidden state at the start of a sentence, rather than taking the sentence average or other types of pooling. However this does not mean that the BERT authors recommend using the CLS token as a sentence embedding. It 'could' be used for classification or other tasks, but there could be other poolers you may want to write yourself using the embeddings of the individual words. In my experience max pooling works for sentiment analysis. For other NLU types of tasks mean pooling works better. In other cases an extra attention head and a fine-tuning can lead to excellent results. Each problem domain could have a specific solution.
