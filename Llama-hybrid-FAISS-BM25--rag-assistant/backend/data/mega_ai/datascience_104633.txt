[site]: datascience
[post_id]: 104633
[parent_id]: 16416
[tags]: 
The reason is hardware based. For Neural networks, and deep learning, matrix operations are the main computations and source of floating point operations (FLOPs). Single Instruction Multiple Data (SIMD) operations in CPUs happen in batch sizes, which are powers of 2. And for GPUs: https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html Memory allocated through the CUDA Runtime API, such as via cudaMalloc(), is guaranteed to be aligned to at least 256 bytes. Therefore, choosing sensible thread block sizes, such as multiples of the warp size (i.e., 32 on current GPUs), facilitates memory accesses by warps that are properly aligned. (Consider what would happen to the memory addresses accessed by the second, third, and subsequent thread blocks if the thread block size was not a multiple of warp size, for example.) This means that any multiple of 32 will optimize the memory access, and thus, the processing speed, while you are using GPU. Consider take a look if you are interested: https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37631.pdf
