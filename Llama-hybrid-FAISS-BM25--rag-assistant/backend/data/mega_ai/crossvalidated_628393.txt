[site]: crossvalidated
[post_id]: 628393
[parent_id]: 628369
[tags]: 
They are closely related, but not the same. The first relates to the actual regressors observed, whereas the second to the population from which we draw. When a LLN works, that will amount to the same thing in large samples (that is at least my intuition, I would not rule out maybe somewhat pathological counterexamples), but not necessarily in small samples. Consider a regression model with $K=2$ dummy regressors $x_{1i}$ (say, rural/urban) and $x_{2i}$ (say, employed/not employed), $$ y_i=\beta_0+\beta_1x_{1i}+\beta_2x_{2i}+u_i $$ Assume $P(x_{1i}=1)=p_1$ and $P(x_{2i}=1)=p_2$ and, for simplicity (that is not necessary for the argument) that the two regressors are independent. Then, $$ E(x_ix_i')=\begin{pmatrix}1&p_1&p_2\\p_1&p_1&p_1p_2\\p_2&p_1p_2&p_2\end{pmatrix} $$ will have full rank $K+1=3$ (barring the pathological case when $p_j\in\{0,1\}$ , $j=1,2$ ---the determinant is $(p_1 - 1) p_1 (p_2 - 1) p_2$ ). In turn, that the actual observed regression matrix $X$ does not have full rank can, in particular in small samples, still occur. Try, e.g., set.seed(13) n X [,1] [,2] [,3] [1,] 1 1 1 [2,] 1 0 0 [3,] 1 0 0 [4,] 1 0 0 > qr(X)$rank [1] 2 For the "converse case" showing that we also cannot conclude from full rank for any $n$ to identification (see D F's +1 answer), consider $$ y_t=\beta \frac{1}{t}+u_t, $$ i.e., a deterministic regressor $x_t=1/t$ that tends to zero as $t\to\infty$ . Here, the "expectation" hence is actually just $1/t$ itself and therefore also not constant. In such cases, we have heterogeneously distributed data (I do not know if that is necessary for such a converse case) and need to resort to limit theorems such as Exercise 3.14 (for consistency) or 5.12 (for asymptotic normality) in White, Asymptotic Theory for Econometricians, that requires the "average" variance matrix of the regressors $M_n\equiv E(X'X/n)$ to be uniformly positive definite Here, $$ E(X'X/n)=\frac{X'X}{n}=\frac{1}{n}\sum_{t=1}^n\frac{1}{t^2} $$ This quantity clearly is positive for any $n$ , but, since $$ \sum_{t=1}^n\frac{1}{t^2}\to\frac{\pi^2}{6}, $$ as $n\to\infty$ , we have that $$ \frac{1}{n}\sum_{t=1}^n\frac{1}{t^2}\to 0 $$ as $n\to\infty$ . Hence, the condition asymptotically fails in this case. As for the requirement for the variance matrix to be finite, you could turn this example on its head and consider a trend regressor $\beta t$ whose sum of squares obviously diverges. (This is not to say that regressions with such regressors do not "work", but that they are not covered by the present conditions.)
