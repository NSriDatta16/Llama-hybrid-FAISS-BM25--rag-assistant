[site]: crossvalidated
[post_id]: 565119
[parent_id]: 564045
[tags]: 
Your first question: Since you are using the default setting of XGB, you are not using any built-in features to fight overfitting, so your model is probably indeed overfitting. XGBoost provides two randomization techniques to fight overfitting, see the section "Control Overfitting" here . Your second question: Your setting of the seed will currently only effect your cross-validation (CV) splitting. IIUC, your variations in model performance refer to the different folds in CV. Once you have turned on the XGB internal randomization features, the overfitting should disappear and the results in CV should become similar. CV is used for two purposes: first, to predict the error on the test dataset, and second, to use this predicted error to tune hyperparameters. But since you don't have any hyperparameters to tune (do you?), the only reason left to use CV is the prediction of the generalization error. But since you have a large dataset (100k), I recommend using just one single 80/20 partition, with training on the larger and testing on the smaller part. This should be totally sufficient to estimate your generalization capabilities, no need for CV. But, of course, you must make sure that your 80/20 partition is really random . And then you leave the seed alone. Don't try to tune the seed, this is kind of a rule.
