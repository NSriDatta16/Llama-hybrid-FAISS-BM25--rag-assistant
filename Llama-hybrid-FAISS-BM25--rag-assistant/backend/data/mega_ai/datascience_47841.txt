[site]: datascience
[post_id]: 47841
[parent_id]: 47839
[tags]: 
Imagine a simple input vector $\boldsymbol{x}=[x_1,x_2,x_3]^T$ , with $x_i\in \{0,1,\ldots,255\}$ . If our neural network has a layer which normalizes the inputs into the range of rational numbers between $0$ and $1$ . The output of this operation would be $$\boldsymbol{x}_\text{Norm}=[1/255, 1/255, 1/255]\boldsymbol{x}.$$ As you can see we have $3$ parameters but all are not trainable. A more common situation is given when you do transfer learning with pretrained neural networks. In this situation, you take a trained neural network and modify it (e.g. add additional layers or chop of some layers and add your own layers). Then when you want to train the new network your data set you can decide if you want to use the pretrained parameters (non-trainable parameters) or train the network from the scratch. In the last case, you will have a model in which all parameters are trainable.
