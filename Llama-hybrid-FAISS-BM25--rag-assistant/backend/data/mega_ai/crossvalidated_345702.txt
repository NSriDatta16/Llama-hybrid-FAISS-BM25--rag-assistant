[site]: crossvalidated
[post_id]: 345702
[parent_id]: 272853
[tags]: 
A better way to handle this situation is to use logistic regression with soft labels rather than hard labels. Labelling an instance $x$ with the label 1 indicates your judgement that $p(X_1|F)=1$. In your situation, that's not the case. You have a way to sample from $X_0$; for those instances, we have $p(X_0|F)=1$ and $p(X_1|F)=0$. You also have a way to sample from $X$; for those instances, we have $p(X_0|F)=p$ and $p(X_1|F)=1-p$, where is the probability that a randomly chosen element will be in $X_0$. So, assuming that you know the class balance (the prevalence of $X_0$ vs $X_1$), this gives you a way to assign labels to all of your samples in the training set. You assign the distribution $(1,0)$ to instances sampled from $X_0$, and the distribution $(p,1-p)$ to instances sampled from $X_1$. Then, you fit a logistic regression model to this training set. The logistic loss ( cross-entropy loss ; see also here ) handles these soft labels fine and is not restricted to hard labels. You might also be interested in one-class classifiers , for a different approach and one that is not limited to linear models.
