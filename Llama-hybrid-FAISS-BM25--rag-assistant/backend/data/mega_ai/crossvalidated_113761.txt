[site]: crossvalidated
[post_id]: 113761
[parent_id]: 63313
[tags]: 
Multiple imputation models for missing data are rarely employed in practice as simulation studies suggest that the chances of the true underlying parameters lying within the cover intervals are not always accurately depicted. I would strongly recommend a testing of the process based on simulated data (with parameters known precisely), based on real data in the area of investigation. A simulation study reference https://www.google.com/url?sa=t&source=web&rct=j&ei=Ua4BVJgD5MiwBMKggKgP&url=http://www.ssc.upenn.edu/~allison/MultInt99.pdf&cd=13&ved=0CCEQFjACOAo&usg=AFQjCNF1Rg6SbFPwLv5n3jYIVNA_iTMPCg&sig2=d2VORWbqTNygdM6Z51TZEg I suspect employing say five simple/naive models for the missing data may be better in producing less bias and cover intervals that accurately include the true underlying parameters. Rather than pooling of the parmeter estimates, one may do better by employing Bayesian techniques (see work with imputation models in this light at https://www.google.com/url?sa=t&source=web&rct=j&ei=mqcAVP7RA5HoggSop4LoDw&url=http://gking.harvard.edu/files/gking/files/measure.pdf&cd=5&ved=0CCUQFjAE&usg=AFQjCNFCZQwfWJDrrjzu4_5syV44vGOncA&sig2=XZUM14OMq_A01FyN4r61Zw ). Yes, not much of a ringing endorsement of standard missing data imputation models and to quote a source, for example, http://m.circoutcomes.ahajournals.org/content/3/1/98.short?rss=1&ssource=mfr :"We describe some background of missing data analysis and criticize ad hoc methods that are prone to serious problems. We then focus on multiple imputation, in which missing cases are first filled in by several sets of plausible values to create multiple completed datasets,..." where I would insert "(?)" after plausible as naive models, for one, are not generally best described as producing plausible predictions. However, models incorporating the dependent variable y, itself, as an independent variable (so called calibration regression) may better meet this characterization.
