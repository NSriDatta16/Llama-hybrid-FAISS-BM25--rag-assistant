[site]: crossvalidated
[post_id]: 12895
[parent_id]: 12888
[tags]: 
The general comment is that in the abstract it is usually better to make the model space as small as possible. This is sort of vague and obvious at the same time (we would love the model space to be a singleton set!). "As small as possible" really constitutes a decision problem (basically weighing the losses associated with inconsistent versus inefficient inferences). From a frequentist perspective you (roughly) enhance the degrees of freedom by estimating fewer parameters, which will give you lower variance estimates, more powerful tests etc. From a Bayesian perspective, if you have $\theta = (\alpha, \beta)$ and are interested in $\alpha$ then you're after the marginal posterior $p(\alpha|Y) = \int p(\alpha, \beta|Y)d\beta$. Integrating over $\beta$ inflates the variance in this distribution, so if you're really confident that $\beta$ takes a fixed value you'll get more efficient inferences by fixing it (assuming you're correct!). In practice you must also be concerned about parameter constraints complicating inference too, although this is outside the scope of your question...
