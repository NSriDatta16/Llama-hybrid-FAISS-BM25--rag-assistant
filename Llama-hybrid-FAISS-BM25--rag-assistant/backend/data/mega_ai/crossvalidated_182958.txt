[site]: crossvalidated
[post_id]: 182958
[parent_id]: 
[tags]: 
Logistic regression: How can I penalize false positives different than false negatives during training?

I developed a simple fraud detection example to test logistic regression. I have n features (e.g. credit score, account balance, etc.), m samples for training and I try to compute my output y with 0 - fraud or 1 - no fraud . Everything works well so far. As next step, I would like to penalize the misclassification. It isn't great, but not the end of the world, that the classifier computes 0 and the actual result is 1 . However, the case 1 - no fraud calculated and training output was 0 should be penalized higher than the other case (real world: costs of not detecting fraud is higher than blocking a good customers credit card). I assume I have to adjust my gradient function to change my theta's more in one case or the other. Is that right? Or do I need to adjust my cost function? I am using this cost function below J(θ) = (1/m) * Sum(−y(i)*log(hθ(x(i))) − (1 − y(i))*log(1 − hθ(x(i)))) and this standard gradient descent approach ∂J(θ)/∂θ = (1/m) * Sum((hθ(x(i)) − y(i)) * x(i)) Can anybody recommend some literature how such a penalty would be applied?
