[site]: datascience
[post_id]: 76919
[parent_id]: 76900
[tags]: 
One common metric to determine if 2 columns have a linear relationship is R-Squared. You can use a function like this to calculate the value. rsq The closer the value is to 1, the more linear the relationship is. a similar metric to use for measuring the correlation between 2 variables (linear or otherwise) would be Pearson correlation R cor_p The closer the absolute value is to 1 (can also be negative), the stronger the relationship is. This can be useful in many situations. Other metrics to consider would be MSE (mean squared error) or RMSE (root mean squared error) If the metric value is low, you can also look at transforming one of the columns and see if the transformed column is more linearly related than the original column. Some common transforms are log(), sqrt(), exp(), etc. Also, some models are typically fine with the data as is, no need to use a transform. One example of this is any Random Forest or Decision Tree model. In any case, excluding data just because it does not have a linear relationship is usually not the best solution as you may be removing some of the variance. Some of the valid reasons to remove a feature would be low variance or low correlation to the response, sparseness/missing, etc. The model can choose to ignore the data if it doesn't help improve the results.
