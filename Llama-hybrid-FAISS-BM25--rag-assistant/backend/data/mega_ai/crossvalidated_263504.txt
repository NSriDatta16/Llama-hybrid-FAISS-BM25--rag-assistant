[site]: crossvalidated
[post_id]: 263504
[parent_id]: 263429
[tags]: 
As @hxd1011 mentioned, one approach is to formulate linear regression as an optimization problem, then solve it using an iterative algorithm (e.g. stochastic gradient descent). This approach can be parallelized but there are a couple important questions: 1) How should be problem be broken into subproblems? 2) Given that optimization algorithms like SGD are inherently sequential, how should solutions to the subproblems be combined to obtain a global solution? Zinkevich et al. (2010) describe some previous approaches to parallelizing across multiple machines: 1) Parallelize SGD as follows: Split the data across multiple machines. At each step, each local machine estimates the gradient using a subset of the data. All gradient estimates are passed to a central machine, which aggregates them to perform a global parameter update. The downside of this approach is that it requires heavy network communication, which reduces efficiency. 2) Partition the data evenly across local machines. Each machine solves the problem exactly for its own subset of the data, using a batch solver. Final parameter estimates from the local machines are averaged to produce a global solution. The benefit of this approach is that it requires very little network communication, but the downside is that the parameter estimates can be suboptimal. They propose a new approach: 3) Allow each local machine to randomly draw data points. Run SGD on each machine. Finally, average the parameters across machines to obtain a global solution. Like (2), this method requires little network communication. But, the parameter estimates are better because each machine is allowed to access a larger fraction of the data. The parallelized optimization approach is very general, and applies to many machine learning algorithms (not just linear regression). Another alternative would be to use parallel/distributed matrix decomposition algorithms or linear solvers. Least squares linear regression has special structure that allows it to be solved using matrix decomposition methods. This is how you'd typically solve it in the case of a smaller data set that fits in memory. This can be parallelized by distributing blocks of the matrix across multiple machines, then solving the problem using parallel/distributed matrix computations. Given that this approach is more specialized to solving linear systems, it would be interesting to see how its performance compares to the more general distributed optimization approach. If anyone can provide more information about this, I'd be glad to hear. References: Zinkevich et al. (2010) . Parallelized Stochastic Gradient Descent.
