[site]: crossvalidated
[post_id]: 514253
[parent_id]: 512630
[tags]: 
Adding to the excellent answer by @RCarnell, I wanted to note that there are different approaches to dimensionality reduction with different levels of generality. The great thing about PCA is that it allows you to get rid of useless information if you know your signal-to-noise ratio. Namely, the eigenvalues significantly smaller than SNR may be discarded, as any information that may have been contained in them is already corrupted beyond recognition. The same philosophy applies to other techniques from this family such as ICA, FA, NMF. However, if you have a noise-less data structure and you can't guarantee that small changes in the structure won't result in big changes in classification, then there is no way to proceed. One approach could be to try to find precise symmetries in the data. It is important to represent data in a sensible way, namely, split categorical data into separate dimensions, as well as splitting categorical data hidden in floating-point values into separate dimensions. Generally, expanding data into many dimensions all of which are as simple as possible before applying a dimensionality reduction procedure is a good way to go. Further, nonlinear dimensionality reduction techniques can be used to try to hunt for that dimensionality. Thinks like kernel PCA and even total correlation come to mind. But, as @RCarnell said, the only thing that eventually matters is whether any given feature is actually predictive. Without prior knowledge, the only way to know is to check
