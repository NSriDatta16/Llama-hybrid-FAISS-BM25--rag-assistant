[site]: crossvalidated
[post_id]: 316192
[parent_id]: 51428
[tags]: 
If you used log returns, then you made a slightly biasing error but if you used future value divided by present value then your likelihood is wrong. Actually, your likelihood is wrong in either case. It is wrong enough to matter. Consider that a statistic is any function of the data. Returns are not data, they are transformations of data. They are a future value divided by a present value. Prices are data. Prices must have a distribution function, but the distribution function for returns must depend solely on the nature of the prices. For securities in a double auction, there is no "winner's curse." The rational behavior is to bid your expectation. With many buyers and many sellers, the limit book should converge to the normal distribution as it is a distribution of expectations. So $p_t$ should be normally distributed. Also $p_{t+1}$ should be normally distributed. Therefore returns should be the ratio of $$\frac{p_{t+1}}{p_t}-1.$$ The likelihood function for your regression should have been $$\frac{1}{\pi}\frac{\sigma}{\sigma^2+(y-\beta_1x_1-\beta_2x_2\dots-\beta_nx_n-\alpha)^2}.$$ OLS forces a best fit to the observed data even if it is the wrong solution. Bayesian methods attempt to find the data generating function through the likelihood. You had the likelihood wrong, so it could not find it. I have a paper out on this if you need additional information. EDIT I think you have misunderstood. If you would convert the likelihood to a density function and take the expectation, you would find that it has none. By proof by Augustin Cauchy in 1852 or maybe 1851, any form of least squares solution is perfectly imprecise. It will always fail. It isn't that you should use standard regression because the Bayesian is sensitive to the likelihood, it is that Bayes is the only available solution that is admissible, with some special exceptions for some unusual special cases. In doing the empirical testing on this, and before I had read enough of the math, I naively thought that the Bayesian and the Frequentist solution should match. There is, approximately, a theorem that says that as the sample becomes large enough, the two will converge. I used all end of day trades in the CRSP universe from 1925-2013 to test it. That isn't what the theorem says though. I was misunderstanding the rules. I also tried the problem in logs, and it still did not match. So I realized something, all distributions are shapes, and so I constructed a geometric solution to determine which solution was correct. I treated it as a pure geometry problem to determine which algebraic answer matched the data. The Bayesian one matched. This lead me down a very mathematical path because I couldn't figure out why the unbiased estimator was so wrong. Just for the record, using disaggregated returns over the period 1925-2013 and removing shell companies, closed-end funds and so forth, the discrepancy between the center of location is 2% and the measure of risk is understated by 4% for annual returns. This discrepancy holds under log transformation, but for a different reason. It may be different for individual indices or subsets of the data. The reason for the discrepancy is two-fold. The first is that the distributions involved lack a sufficient statistic. For certain types of problems, this doesn't matter. For projective purposes, such as prediction or allocation, however, they matter quite a bit. The second reason is that the unbiased estimator is always a version of the mean, but the distribution has no mean. The density above is not a member of the exponential family as the normal or the gamma distribution is. By the Pitman–Koopman–Darmois theorem, no sufficient point statistic exists for the parameters. This implies that any attempt to create a point estimator must throw away information. This is not a problem for Bayesian solutions because the posterior is an entire density and if you did need a point estimate, you could find the predictive density and minimize a cost function over it to reduce it to a single point. The Bayesian likelihood is always minimally sufficient. The minimum variance unbiased estimator for the above function is to keep the central 24.6% of the data, find its trimmed mean, and to discard the rest of the data. That means over 75% of the data is dropped, and the information is lost. Just a note, it might be 24.8%, as I am working from memory. You can find Rothenberg's paper at: Rothenberg, T. J. and F. M. Fisher, and C. B. Tilanus, A Note on Estimation from a Cauchy Sample, Journal of the American Statistical Association, 1964, vol 59(306), pp. 460-463 The second issue was surprising to me. Until I worked through the geometry, I didn't realize what the cause was. Returns are bound on the bottom at -100%. This shifts the median by 2% and the interquartile range is shifted by 4% although the half-mass is still at the same points. The half-mass is the proper measure of scale, but the half-width is not. If there were no truncation, then the half-width and the half-mass would be at the same points. Likewise, the median and the mode would remain at the same point. The median is the return for the mean actor or at least the mean trade. As such, it is always the location of the MVUE and the log mean. The correct understanding of the theorem is that all Bayesian estimators are admissible estimators. Frequentist estimators are admissible estimators if one of two conditions obtain. The first is that in every sample, the Frequentist and the Bayesian solution are identical. The second is that if the limiting solution of the Bayesian method matched the Frequentist solution, then the Frequentist solution is admissible. All admissible estimators converge to the same solution once the sample size is large enough. The Frequentist estimator presumes that its model is the true model and the data is random. The Bayesian assumes the data is true, but the model is random. If you had an infinite amount of data, then the subjective model must converge to reality. If you had an infinite amount of data, but the wrong model, then the Frequentist model will converge to reality with probability zero. In this case, the Bayesian solution, under reasonable priors, will always stochastically dominate any Frequentist estimator because of the truncation and the loss of information to create the estimator. In logs, the likelihood function is the hyperbolic secant distribution. It has a finite variance, but no covariance. The covariance matrix found using OLS is an artifact of the data and does not point to a parameter that exists in the underlying data. As with the raw form, nothing in the log form covaries, but nothing is independent either. Instead, a far more complex relationship exists that violates the definition of covariance, but in which they can comove. Markowitz and Usman almost found it in their work on distributions, but the hyperbolic secant distribution isn't in a Pearson family and they misinterpreted the data by not noticing that when you change a distribution from raw data to log data you also change its statistical properties. They basically found this out but missed it because they had no reason to look for it and they didn't realize the unintended consequences of using logs. I don't have the Markowitz and Usman cite with me where I am at, but they did one of the few very good jobs at estimating the distribution that are out there. In any case, I don't use JAGS. I have no idea how to do it. I code all my MCMC work by hand. I have a paper that is far more complete and accurate on this topic at: Harris, D.E. (2017) The Distribution of Returns. Journal of Mathematical Finance, 7, 769-804. It will provide you with a method to construct distributions for any asset or liability class, also accounting ratios. I was wordy, but I could see you were misunderstanding the relationship between Bayes and the Pearson-Neyman methods. You had them reversed. Bayes always works, but you are trapped with a prior density that will perturb your solution. With a proper prior you are guaranteed a biased estimator and for this type of likelihood function, I believe you must use a proper prior to guarantee integrability to unity. Frequentist methods are fast and usually work. They are unbiased, but may not be valid.
