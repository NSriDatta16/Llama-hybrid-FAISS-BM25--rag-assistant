[site]: crossvalidated
[post_id]: 428738
[parent_id]: 428693
[tags]: 
"The marginal distribution of any subset of variables can be approximated by simply considering the samples for that subset of variables, ignoring the rest." If you have draws $\mathbf{x}^{(1)}$ , ..., $\mathbf{x}^{(M)}$ from an MCMC algorithm like the Gibbs sampler, one sense in which these draws approximate the $n$ -dimensional target distribution $p(\mathbf{x})$ is that $$\frac{1}{M}\sum\limits_{m=1}^Mh(\mathbf{x}^{(m)})\to\mathbb{E}[h(\mathbf{x})]\quad\text{as}\quad M\to\infty$$ for any "well-behaved" function $h$ . So there are laws of large numbers and central limit theorems that establish that averages of the MCMC draws are good approximations of expectations with respect to the target. You can learn more about this here and here . So, say you picked the function $h(\mathbf{x})=x_i$ that simply returns the $i$ th component of $\mathbf{x}$ . Then \begin{align*} \mathbb{E}[h(\mathbf{x})]&=\int h(\mathbf{x})p(\mathbf{x})\,\textrm{d}\mathbf{x}\\ &=\int...\int x_ip(x_1,\,...,\,x_n)\,\textrm{d}x_1...\textrm{d}x_n\\ &=\int x_i\left[\int...\int\int...\int p(x_1,\,...,\,x_n)\,\textrm{d}x_1...\textrm{d}x_{i-1}\textrm{d}x_{i+1}...\textrm{d}x_{n}\right]\,\textrm{d}x_i\\ &=\int x_i\,p(x_i)\,\textrm{d}x_i\\ &=\mathbb{E}[x_i]. \end{align*} So the expectation with respect to the full joint distribution of a function that just isolates a component of $\textbf{x}$ is equal to the marginal expectation of that component (and more generally, any marginal functional that you want). So you can take your MCMC draws, apply $h$ to them (which means ignoring every component but the $i$ th), take an average, and it will be a good approximation of the marginal expectation. Moreover I really don't know why should I be sure that by taking the mode of the array referring to a parameter, this value, together with the modes of the other parameters, maximizes the total posterior You should not be sure. The mode of a joint density does not generally occur at the point containing the maximizers of the marginals. Formally, if $p(x)$ and $p(y)$ are the marginal densities of the joint $p(x,\, y)$ , and \begin{align*} \hat{x}&=\underset{x}{\arg\max}\,p(x)\\ \hat{y}&=\underset{y}{\arg\max}\,p(y),\\ \end{align*} then $$(\hat{x},\,\hat{y})\neq\underset{(x,\,y)}{\arg\max}\,p(x,\,y)$$ in general. More here . If you already know the target density up to a normalizing constant (which you usually do in Bayesian inference), you could simply take $$\hat{\mathbf{x}}=\underset{\mathbf{x}^{(1)},\,...,\,\mathbf{x}^{(M)}}{\arg\max}\,p(\mathbf{x}),$$ but this is not a great approximation. See again Section 3.2 here for better options. As for marginal modes, this might be of interest.
