[site]: stackoverflow
[post_id]: 1717150
[parent_id]: 1717049
[tags]: 
EDIT (10y later): As Lukas said in the comment box, almost all crawlers today support javascript so I've removed the paragraph that stated that if the site was JS based most bots would be auto-stripped out. You can follow a bot list and add their user-agent to the filtering list. Take a look at this bot list. This user-agent list is also pretty good. Just strip out all the B's and you're set. EDIT: Amazing work done by eSniff has the above list here " in a form that can be queried and parsed easier. robotstxt.org/db/all.txt Each new Bot is defined by a robot-id:XXX. You should be able to download it once a week and parse it into something your script can use " like you can read in his comment. Hope it helps!
