[site]: crossvalidated
[post_id]: 590913
[parent_id]: 
[tags]: 
Too many statistically significant results with propensity score weighting

I'm currently performing a drug-wide associatinon study (DWAS) in a population-based cohort, in which I'm iterating over commonly used drugs to assess whether they have an association with my outcome of interest. Given that medications are not randomly assigned, but rather prescribed for an underlying condition, we wanted to 'correct' for confounding by using inverse probability score weighting. We used age, sex and common diagnosis to generate the propensity scores and subsequently used that information to calculate a corrected odds-ratio for said drug and the outcome of interest (see code below). import pandas as pd import numpy as np from sklearn.linear_model import LogisticRegression import statsmodel.api as sm import statsmodels.formula.api as smf for drug in drugs: X = df.drop([outcome_of_interest, drug_usage], axis = 1) y = df[drug_usage] model_prop = LogisticRegression() model_prop.fit(X, y) df['prop_scores'] = model_prop.predict_proba(X)[:, 1] #probability scores df['ATE'] = np.where(df[drug_usage] == 1, #calculate average treatment effect weights 1 / df['prop_scores'], 1 /(1 - df['prop_scores'])) model_outcome = smf.glm( #Model to generate the OR, p-value etc. formula = f'outcome_of_interest ~ {drug_usage}', data = df, family = sm.families.Binomial(), freq_weights = np.asarray(df['ATE'])).fit() My question is this: How can I test whether the propensity scores generated here make sense? Performing my analysis in this way leads to suspiciously many statistically significant results (around 70% of all drugs tested), even when correcting for multiple comparisons. Any help would be greatly appreciated!
