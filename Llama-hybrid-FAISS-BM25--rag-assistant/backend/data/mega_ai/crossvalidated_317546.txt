[site]: crossvalidated
[post_id]: 317546
[parent_id]: 317521
[tags]: 
"Bayesian statistics is not based on large samples (i.e., the central limit theorem) and hence may produce reasonable results even with small to moderate sample sizes, especially when strong and defensible prior knowledge is available" (p. 240) So, my question is: Do the authors also mean that the idea of sampling with replacement from a population infinitely many times, and averaging the point estimate to arrive at the original mother population's parameter (Central Limit Theorem) is a Frequentist concept and does not apply to Bayesian statistics? The central limit theorem does not depend upon the interpretation of probability. However, its importance is substantially diminished because of how the different methods build their inference. The repetition element is important in sampling-based statistics because you are working in the sample space and the concept of optimality is that the estimators minimize some loss and are optimal on average. Bayesian estimators are optimal, given the actually observed sample. If you repeat the experiment, then the Bayesian estimates are updated. Each Frequentist experiment is reported out as separate events, except in meta-analysis. Using your graphic, the Frequentist conducts each experiment and produces twenty sets of inference. The Bayesian would use all prior experiments to update the current one, so in a sense, those twenty experiments are one Bayesian sample. In a Frequentist solution, the concern is with the distribution of $\bar{x}-\mu$ and since $\bar{x}$ is a random variable made up of the sum of many random variables, the sampling distribution of $\bar{x}$ is normal and so the central limit theorem holds and is important. For a user of Bayesian methods, however, the sample isn't random. An observation isn't a random variable. The central limit theorem still holds, but it isn't directly used for inference. The posterior depends only on the exact sample that was observed and not samples that could have happened but did not happen. I cannot say what the authors mean as I haven't read the book. Nonetheless, I do not think that you can infer as much as you are from that statement. Since Bayesian inference is based on the sample that was observed, there is no direct concept of "power." Bayesian methods depend on sample size just as null hypothesis methods do to get information, but there is no idea of exploring a sample space in order to perform inference. I think they are saying something smaller than what you are saying. Let me give you an example. Let us imagine that you are about to patent a new species of bean. A very similar bean has 31 calories per 100 grams, with a variance of 25 calories$^2$ per 100 grams. You have your first ten beans. They weight 55 grams. You have a sample size of ten. That isn't much. In doing this, you have basically constrained the search area for $\mu$ to (16,46). This increases the precision of the test because the method of maximum likelihood is going to treat the entire set of real numbers as the possible number of calories per hundred grams until it sees the sample. Then the confidence interval will be based only one what the sample provides. This will make for a wider estimate since it lacks the additional information. I don't think they are saying anything more. If you did do infinite repetition, then if $\mu$ is a population mean from a density, and the prior is not degenerate, then the marginal posterior density of $\mu$ will converge to the normal distribution. The central limit theorem has returned, albeit in another form and it isn't the "central limit theorem" anymore, but it is the same idea. Now $\mu$ is the random variable. The distinction here, however, is that nobody can collect the data from infinite repetition, but you can collect ten beans.
