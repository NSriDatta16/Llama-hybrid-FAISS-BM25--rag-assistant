[site]: datascience
[post_id]: 56636
[parent_id]: 
[tags]: 
How to approach a problem when the information is in the relationship between the points, and not the points itself?

I am trying to analyze vehicular mobility models, where I am trying to learn how a particular vehicle moves and then detect similar patterns from the testing data. Here's what I have done for now: I have mobility data for 9 users, collected over 5 minutes for each of them at a frequency of 2 seconds. I have a userID value against each point, and the features that I used are speed and distance travelled 2 points as features. For training, I combined details of all 9 users into a single dataframe. When I use a similar structure for testing data (5 minute data without the userIDs for each user combined into a single dataframe), I get pretty weird results: The accuracies I got are: Random Forest - 98.00, Decision Tree - 98.00, Naive Bayes - 39.66, KNN - 60.74, SVM - 34.05, Linear SVC - 30.85, Logistic Regression - 30.69, Perceptron - 19.95, Stochastic GD - 19.95 I do not much about the mathematics behind most of the ML algorithms, so I am not able to figure out if there is any mistake in the dataset or pre-processing the data. Coming to my questions: Is it better to train for each vehicle separately, and then also test for each vehicle separately (without combining them into a single dataframe)? What might be the reason for such huge differences in the accuracies in the different approaches as given in the table. I think the important information in the dataset for this particular problem is the relationship between the different points (how much distance is covered every 2 seconds or how the speed varies every 2 seconds), is there a way the testing gives me a result for a whole cluster of values; and not for every testing value.
