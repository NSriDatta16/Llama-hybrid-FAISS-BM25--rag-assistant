[site]: crossvalidated
[post_id]: 488371
[parent_id]: 486406
[tags]: 
Maybe you are not considering the fact that for a given input you get its corresponding latent vector (mu and sigmas in VAE that assumes Gaussian distribution for z). This extends to a mini-batch of inputs. For e.g., if you have 100 images in your mini-batch, the encoder is outputting 100 latent vectors (each vector is of dimension D). See the below image to get better visualization. The batch is shown as ? and the dimension of the latent vector is 2. You then generate a sample (according to VAE paper, 1 sample is sufficient thanks to re-parameterization trick low variance properties) using the mu and sigma that corresponds to its corresponding input image. This means for a batch of 100 inputs you will have 100 samples. So if your first input image is that of Bob (a male) and the second input image is that of Alice (a female) you will have latent vectors [mu_bob, sigma_bob] & [mu_alice, sigma_alice]. You would then have sample_bob (using mu_bob & sigma_bob) and sample_alice (using mu_alice & sigma_alice). Now, at the onset of your training regime, these mus and sigmas will not be good but the idea is that the encoder network will learn to generate "relevant/appropriate" mus and sigmas for your inputs. As such this is no different than any representation learning-based setup.
