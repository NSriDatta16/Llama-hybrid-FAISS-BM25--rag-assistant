[site]: crossvalidated
[post_id]: 306849
[parent_id]: 
[tags]: 
Sampling from predictive distribution with unknown mean and variance

I am following Pattern Recognition and Machine Learning and trying to implement Bayesian linear regression with unknown mean and variance, so that the posterior is given by a Normal-Gamma distribution $NG(w,\beta\mid w_0,\Lambda^{-1},a,b)$ I update the precision matrix by $\Lambda = \Lambda_0 + X^TX$, the weights by $w = \Lambda^{-1}(\Lambda_0w_0 + X^Ty)$, the shape parameter by $a = a_0 + \frac{n}{2}$, and the rate parameter by $b = b_0 + \frac{1}{2}\sum_i{(y_i - w^Tx_i)^2}$ After fitting the parameters to a simple polynomial curve, I then use the parameters to sample from the predictive distribution given by a Student's t-distribution in Python like so, where the variance according to the book should be equal to $(\beta\Lambda)^{-1} = (\frac{a}{b}\Lambda)^{-1}$ x1 = np.linspace(left, right, 250) chol = np.linalg.cholesky(np.linalg.inv(posterior.L * (posterior.a / posterior.b))) for k in range(100): w_s = np.dot(chol, np.random.standard_t(2. * posterior.a, size=len(posterior.w))) + posterior.w y1 = w_s[3] * x1**3 + w_s[2] * x1**2 + w_s[1] * x1 + w_s[0] plt.plot(x1, y1, color='red', alpha=0.2) However, the sampled lines do not seem right as they are far too narrow given the data as you can see here (blue dots are the data that is being fit to, the green line is the maximum a posteriori, and the red lines are the samples) For some reason I had the intuition to remove the $a$ from the variance term, so that it is instead given by $(\frac{1}{b}\Lambda)^{-1}$, like so chol = np.linalg.cholesky(np.linalg.inv(posterior.L / posterior.b)) After doing that, the samples actually seem to make sense But I'm at a loss as to why. I'm sure it's something obvious, but my brain doesn't seem to want to help me. Can anyone point it out to me? Thanks.
