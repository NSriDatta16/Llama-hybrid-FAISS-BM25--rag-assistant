[site]: datascience
[post_id]: 16239
[parent_id]: 16235
[tags]: 
Okay. These are the steps to follow. Pad your sentences to a fixed length. Use the maximum length. If you are comfortable with creating buckets, you can pad the sentences to maximum length for individual bucket. To pad, add PAD tokens to each sentence till they become the size of the fixed sequence length you want to use. Extract the unique words out of the padded corpus and assign a numeric id to every word. You can use there index in the unique list as IDs. Filtering out rare words and replacing them with unknown token will prove useful. Now if you had a sentence like "I am John", and your maximum sequence length was 5 the sentence will become, "I am John PAD PAD." Once you extract the unique words, you can represent the sentence as IDs like [1,2,3,4,4] Now if you had 100 sentences you will have a matrix of size 100 X 5. Now you can go in two ways, represent each word as a one hot vector i.e.a vectors of zeros with a one at the index it represents or a simpler way would be to train a word embedding. For details look into this RNN tutorial by Denny Britz, this is from scratch. If you are of the deep learning persuasaion then check out Denny Britz's CNN text classification tutorial where he uses tensorflow and trains his own word embedding. The first blog will give you all the information you need on how to prepare your text dataset.
