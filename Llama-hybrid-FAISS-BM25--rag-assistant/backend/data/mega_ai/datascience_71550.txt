[site]: datascience
[post_id]: 71550
[parent_id]: 
[tags]: 
Traditional ML Model or Deep Learning for ~200-300 samples?

I'm working on a resume parser that is integrated with an RPA (robotic process automation) platform. The robot has OCR to extract text from a PDF resume, and it supplies the tokenized contents as well as the X and Y coordinates of each word. My goal is to create a predictive model to identify key sections of a resume and then use traditional resume parsing for the remainder of the workflow. Some of the features included in the model are as follows: | Word | X_norm | Y_norm | Length_Word | Num_Word_per_Line | Special Char? | Contains numbers? | Where normalization is min-max. We are also working on getting font size and normalized color (i.e boldness ). My gut feeling is that the traditional ML model might be better as we only have the resources to label 200-300 resumes, not thousands. But I wanted to be open to deep learning (something I don't have much experience with). The RPA workflow is such that a Resume Specialist can give feedback to the model in real-time, and something that can improve itself on the fly might be better for this use case. Can traditional ML models also efficiently self-improve?
