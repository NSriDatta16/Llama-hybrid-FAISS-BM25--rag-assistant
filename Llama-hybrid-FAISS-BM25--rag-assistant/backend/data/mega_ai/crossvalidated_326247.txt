[site]: crossvalidated
[post_id]: 326247
[parent_id]: 326228
[tags]: 
Cross-validation is great! You can and should use cross-validation for this purpose. The trick is to perform cross-validation correctly for your data, and k-fold is too naive to deal with the autocorrelation. You've correctly identified the fact that sequential data (like time series) will be subject to autocorrelation. In other words, the traditional supervised learning assumption of i.i.d. observations doesn't hold in this case. In fact, most cross validation schemes appear to rely on having i.i.d. data because the training-test splits do not take time indices into account. For example, 5-fold cross validation applied naively over 5 time periods would ignore the sequential nature of time, mixing up past, present & future: This would be wrong because Your autoregressive models require a contiguous block of data, since they rely on the presence of autocorrelations at predefined lags (instead of having training sets split into 2 parts). Indeed, this is roughly the purpose of models like ARIMA - to capture autocorrelation in a way that many other models don't. You should not train models on future data anyway, to avoid look-ahead bias Hyndman (who has already commented on your question to post 2 great links ), has lots of good examples of using a rolling or sliding window approach to cross validation to avoid this issue. For 5 time periods, you would split the sets as follows: Another approach is to use an expanding window , though this may not be appropriate in your case: Both of these schemes deal with the issues we identified earlier, and shouldn't be too hard to code up.
