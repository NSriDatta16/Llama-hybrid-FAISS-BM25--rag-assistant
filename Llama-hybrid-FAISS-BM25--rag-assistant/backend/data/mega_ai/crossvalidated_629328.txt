[site]: crossvalidated
[post_id]: 629328
[parent_id]: 
[tags]: 
What is this nonparametric goodness-of-fit test?

I wrote down a goodness-of-fit test that I have not seen before. However, it is quite elementary and has many applications, so I bet it must have been known. Could someone tell me its name? Setup. The setup is the same as the Pearson's $\chi^2$ goodness-of-fit test . Fix $N$ and a finite outcome set $o = \{o_1, \ldots, o_r\}$ . Each $o_i$ has a certain probability $p_i$ (unknown to the user) to be drawn with $\sum p_i = 1$ . Draw $N$ i.i.d. samples from $o$ , and let $O_i$ be the amount of $o_i$ drawn. From $O_i$ the user wants to test the null hypothesis. $$H_0: p_i = p_i' \quad\text{ for each } i.$$ Usual Statistics. The statistics usually used for this problem is the Pearson's statistics [4]. It examines the statistics $$P_N := \sum_{i=1}^r \frac{(O_i - Np_i')^2}{Np_i'},$$ whose $N \to \infty$ limit has density function being the chi-squared with $r-1$ freedom $\chi^2_{(r-1)}$ [3]. My Statistics. My test statistics is the $L^2$ -deviation $\sum_{i=1}^r \left|\frac{O_i}{N} - p_i'\right|^2$ . The underlying inequality resembles the DKW inequality (it provides a clear convergence rate for the Kolmogorov-Smirnov test [1]). It is quite simple and only uses Markov's inequality and $\text{Var}(O_i)=p_i(1-p_i)$ : Given any $\epsilon >0$ , $$P\left(\sum_{i=1}^r \left|\frac{O_i}{N} - p_i'\right|^2 > \epsilon \right) \leq \frac{E\left(\sum_{i=1}^r \left(\frac{O_i}{N} - p_i'\right)^2\right)}{\epsilon} = \frac{\sum_{i=1}^r p_i' (1-p_i')}{N^2\epsilon} \leq \frac{1 - \frac{1}{r^2}}{N^2\epsilon} $$ where the last equality holds under the null hypothesis. Hence under the null hypothesis, one has a bound for the probability that deviates from the target $\{p_i'\}$ , from which tests and confidence intervals can be easily provided. One drawback is that this only decays in $O(\frac{1}{N^2})$ . Question. What is the name of the tests related to this inequality? Reference. [1] Dvoretzky–Kiefer–Wolfowitz inequality [3] The rate of convergence of some asymptotically chi-square distributed statistics by Stein’s method-[Robert E. Gaunt and Gesine Reinert] [4] It admits a one-parameter family generalization, namely the Cressie-Read statistics. This includes Pearson statistic ( $\lambda=1$ ), the log likelihood ratio statistic ( $\lambda = 0$ ), the Freeman-Tukey statistic ( $\lambda=-1/2$ ), the modified log likelihood ratio statistic ( $\lambda=-1$ ), and the Neyman modified chi squared ( $\lambda=-2$ ). All of them tend to $\chi^2_{(r-1)}$ as well [3].
