[site]: crossvalidated
[post_id]: 411985
[parent_id]: 411984
[tags]: 
RNN are useful in situations where memory of past inputs is required for successful classification or regression. In situations like text and speech processing, not only the current input word, but past words are also relevant for the model output. Same thing in time series forecasting: If you want to predict the value for this week, then not only the data from last week, but also the data from last month and last year are relevant to the modeling problem. You also get this situation in various video processing applications, or in any situation where the ordering and sequence of the data is essential to the problem. As an example, consider the classification problem of determining whether a social media comment is hateful or not. The problem is a binary classification problem, but the input data is inherently sequential: Use of the word "immigrant" or "gay" in the comment doesn't tell you much, it is how the words "immigrant" or "gay" are positioned with regards to other words and which other words appear in the comment that matter as much as the actual word itself. The feed back loop in RNN allow the neural net to have memory. See here for an in-depth discussion.
