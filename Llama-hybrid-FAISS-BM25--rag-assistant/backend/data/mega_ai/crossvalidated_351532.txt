[site]: crossvalidated
[post_id]: 351532
[parent_id]: 351009
[tags]: 
Well, TF-IDF and Word2Vec are two very different methods to represent documents as numerical vectors. It is hard to predice which one will perform better in your particular application scenario without actually experimenting with them, but of course knowing how they work and their main advantages/disadvantages will allow you to make a reasonable design decission. Essentially, TF-IDF transforms each document to a fixed-length vector where each feature corresponds to a specific term or word. The value of the feature associated to a term for a given document will be directly proportional to the number of occurrences of the term in the document. Also, as opposed to Sklearn's simpler CountVectorizer , TF-IDF also considers how common each term is among your training documents, giving more importance to less frecuent terms. The main limitation of TF-IDF is that it is not as robust as modern algorithms such as word2vec. For example, TF-IDF will not capture by itself the semantical similarity between two documents containing the words "Madrid" and "Spain". Summing up: TF-IDF Advantages: Fixed-lenght representation of documents The resulting vectors will likely be sparse Relatively easy and efficient to train and use in test phase Can be improved by convining it with vector space model techniques such as Latent Semantic Indexing (LSI) or Latent Dirichlet Allocation (LDA) TF-IDF Disadvantages: Lack of robustness to semantically similar but different words Information about the order of words in documents gets lost Regarding Word2Vec, it is a neural-based embedding word embedding technique that learns a feature representation for words on a corpus based on the context where they occur. The fundamental idea is that words with a similar meaning will be closer in the embedding space. Moreover, when representing documents with word2vec the order of words is relevant and can be taken into consideration as opposed to what happens with TF-IDF. Training your own Word2vec embedding might be difficult and require a large corpus, but the good news is that you can probably use a Word2vec embedding learned by someone else on a big corpus, which hopefully will be general enough for your application. Another thing to consider is that since by default Word2vec transforms words to fixed-length vectors, documents with different number of words will produce embeddings of different lengths. This can be usually solved by padding or using alternative approaches like doc2vec . Summing up: Word2vec Advantages: Robust to semantically similar words Possibility to re-use the embeddings learned by someone else Information regarding the order of words in the document can be preserved Easy integration with deep-learning models (see Keras embedding layer ) Word2vec Disadvantages: Documents' representation might have different lenghts (need to pad) Computationaly demanding in terms of memory and computation Possibly there are other aspects to consider, but I believe I have highlighted the main features. You may also want to take a look at recent research comparing these approaches, such as: López-Sánchez, D., Herrero, J. R., Arrieta, A. G., & Corchado, J. M. Hybridizing metric learning and case-based reasoning for adaptable clickbait detection . Applied Intelligence, 1-16.
