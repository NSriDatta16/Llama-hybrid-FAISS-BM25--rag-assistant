[site]: crossvalidated
[post_id]: 125557
[parent_id]: 
[tags]: 
Deriving the maximum likelihood for a generative classification model for K classes

In Christopher Bishop's book "Pattern Recognition and Machine learning", there is the following question: Consider a generative classification model for $K$ classes defined by the prior class probabilities $p(C_k) = \pi_k$ and general class-conditional densities $p(\phi|C_k)$ where $\phi$ is the input feature vector ... show that the maximum-likelihood solution for the prior probabilities is given by: $$ \pi_k = \frac{N_k}{N} $$ where $N_k$ is the number of data points assigned to class $C_k$ and $\textbf{t}_n$ is a binary target vector of length $K$ which uses the 1-of-$K$ coding scheme. While reviewing the solution (4.9, freely available here: http://research.microsoft.com/en-us/um/people/cmbishop/prml/pdf/prml-web-sol-2009-09-08.pdf ) I was particularly stuck at a certain step. So first, we obtain the log-likelihood, given by: $$ \ln p(\{\phi_n, \textbf{t}_n\}\lvert\{\pi_k\}) = \sum_{n=1}^N \sum_{k=1}^K t_{nk} \{\ln p(\phi_n \lvert \textbf{C}_k) + \ln \pi_k \} \tag1 $$ Then, in order to maximise the likelihood, we need to preserve the fact that $\sum_{k=1}^{K} \pi_k = 1$, so we introduce a Lagrange multiplier as follows: $$ \ln p(\{\phi_n, \textbf{t}_n\}\lvert\{\pi_k\}) + \lambda \bigg( \sum_{k=1}^{K} \pi_k - 1 \bigg) \tag 2 $$ Then, in order to obtain the maximum likelihood, we need to differentiate the log likelihood with respect to $\pi_k$ and set the derivative to $0$, giving us: $$ \sum_{n=1}^{N} \frac{t_{nk}}{\pi_k} + \lambda = 0 \tag 3 $$ And here is where I get lost, since my own derivation of the derivative of equation $2$ does not result in eliminating the summations over K. In fact, what I get is the following: $$ \sum_{n=1}^N \sum_{k=1}^K \frac{t_{nk}}{\pi_k} + \lambda k = 0 \tag 4 $$ What exactly am I missing?
