[site]: crossvalidated
[post_id]: 282331
[parent_id]: 282164
[tags]: 
If you want to get a similar solution to SVM (max margin) then you will similarly need some sort of regularizer term for what you're solving - e.g. $\frac{\lambda}{2}||w||^2_2$. If you wish to consider $\lambda = 0$ you could do that as well, although convergence would be smaller, and I'm less sure about whether it will indeed be max margin. In any case, to keep your format similar to what you have, you could solve using various stochastic schemes (e.g. SGD would apply for $\lambda = 0$ or not, and if you do have $\lambda > 0$, you could also consider Pegasos ). Those aren't the latest results, but their algorithms are simple; in both cases, you'd only update $w$ when you encounter loss (not misclassification but loss according to e.g. the hinge loss), and the step-sizes are not constant. If you want constant step sizes, consider SAGA , although I do believe that requires your loss function to be Lipschitz smooth with constant $L$ which it isn't for the hinge loss (but you could use the Huber loss as a good substitute)
