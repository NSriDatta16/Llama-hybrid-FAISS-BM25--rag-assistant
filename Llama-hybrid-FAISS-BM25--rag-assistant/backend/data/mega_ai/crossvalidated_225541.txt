[site]: crossvalidated
[post_id]: 225541
[parent_id]: 225433
[tags]: 
I assume that you also have a (bigger) training set, and that the training and test set have the same relation between features and target variable (there is no significant difference). Then: no, bootstrapping your test set and considering the performance over the resulting test sets is most likely not helpful . You use your training data to e.g. select features, train, and evaluate different model types and hyperparameters. From those results you chose one "best suited" model for the job. This is the one you should evaluate on your test set once for reassurance that everything is OK. With this setup, splitting the test set does not bring benefits anymore: if would not reuse samples during bootstrapping, you would just get "subresults", which then would be averaged to one scalar result (and cause less granularity thereby), or you would bootstrap with replacement, which with few partitions would cause better or worse results by chance (and on an infinite amount of rounds would give you very similar results again). If you are really stuck with a very small test set, and test performance is the only thing that counts (why would it be? You aim for generalization, right?), the question boils down to how well test data represents the real application case data - because few samples might just be too less to allow for any good estimate of real world performance . If you think this might be true for your case, getting/asking for more test data might be required anyway.
