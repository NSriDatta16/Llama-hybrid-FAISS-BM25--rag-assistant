[site]: datascience
[post_id]: 29732
[parent_id]: 29725
[tags]: 
You could train a recurrent many-to-many network using GRU/LSTM cells. input an item as a one-hot encoding a date encoded such that there is no discontinuity when switching from day 365 to 1., e.g. the days can be encoded as $(x,y)$-positions on a 1-ring. The year can be added as a third number. output a probability vector for the next item bought. Select the top 3 for your question. (optional) a date in the same encoding as above. It may be useful to predict when the user comes back. loss When predicting the date, the loss function must be adapted to reflect the combined input, e.g. $||year_{diff}||_2$ for the year part and e.g. distance on the 1-ring or $||(x,y)_{diff}||_2$ for the $(x,y)$ part. missing This network is missing an important part for a real world scenario: Adding new items changes the input size and requires retraining. This can be solved by using embeddings instead of one-hot encoding of the items.
