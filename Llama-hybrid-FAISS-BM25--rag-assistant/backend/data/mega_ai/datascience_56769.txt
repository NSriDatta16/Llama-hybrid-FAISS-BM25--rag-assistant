[site]: datascience
[post_id]: 56769
[parent_id]: 56752
[tags]: 
It is hard to tell what is going on without knowing the data and your actual approach/method/model/code. Just some remarks. Linear regression is parametric and results hinge on the assumptions behind the parameterization. So maybe not too good in a noisy setup. Single trees are often weak learners. The advantage of trees however is, that there is no parameterization behind. So they are in principle rather flexible. It is state of the art to combine many trees to one ensemble (random forest). Especially with noisy data, boosting (also tree based) is really good. Boosting makes a lot of very small trees and (by updating weights to specific observations), it focusses on cases which are hard to predict. I guess this would be a thing to try if you work with predictive modeling. Here are some very basic Python examples for boosting if you want to give it a try: https://github.com/Bixi81/Python-ml
