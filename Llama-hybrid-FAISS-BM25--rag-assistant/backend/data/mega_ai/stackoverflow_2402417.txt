[site]: stackoverflow
[post_id]: 2402417
[parent_id]: 2400643
[tags]: 
So the problem is not that each file is too big, but that there are too many of them, and they seem to be adding up in memory. Python's garbage collector should be fine, unless you are keeping around references you don't need. It's hard to tell exactly what's happening without any further information, but some things you can try: Modularize your code. Do something like: for json_file in list_of_files: process_file(json_file) If you write process_file() in such a way that it doesn't rely on any global state, and doesn't change any global state, the garbage collector should be able to do its job. Deal with each file in a separate process. Instead of parsing all the JSON files at once, write a program that parses just one, and pass each one in from a shell script, or from another python process that calls your script via subprocess.Popen . This is a little less elegant, but if nothing else works, it will ensure that you're not holding on to stale data from one file to the next. Hope this helps.
