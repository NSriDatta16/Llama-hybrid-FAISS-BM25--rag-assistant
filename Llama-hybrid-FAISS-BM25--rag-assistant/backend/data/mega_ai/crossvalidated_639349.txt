[site]: crossvalidated
[post_id]: 639349
[parent_id]: 
[tags]: 
Why is the regularization term multiplied by the error term in the cost function of SVM?

The cost function of the Optimal Margin Classifier(non-kernelized SVM) is given as : $$ J(\mathbf{\vec w}, b) = \frac{1}{2}\|\mathbf{\vec w}\|_{2}^{2} + C \sum_{i=1}^{n}\max(0, 1-y ^{(i)}(\mathbf{\vec w}\cdot \mathbf{\vec x}^{(i)} + b)) $$ Why do we multiply the regularization parameter $C$ with the error term instead of the regularization term $\frac{1}{2}\|\mathbf{\vec{w}}\|_2^2$ ? The general form of a cost function is typically this : $$ J_\lambda(\boldsymbol \theta) = J(\boldsymbol \theta) + \lambda R(\boldsymbol \theta) $$ where $R(\boldsymbol \theta)$ is the regularization term. As you can see, the regularization parameter $\lambda$ is always multiplied by the regularization term. So why is the cost function of SVM so peculiar?
