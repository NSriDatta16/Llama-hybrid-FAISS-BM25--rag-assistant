[site]: crossvalidated
[post_id]: 549159
[parent_id]: 131142
[tags]: 
a bit more hands on answer one way to evaluate is input the kpca output into a supervised model and see if the performance increases, from sklearn.model_selection import GridSearchCV from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline clf = Pipeline([ ("kpca", KernelPCA(n_components=2)), ("log_reg", LogisticRegression()) ]) param_grid = [{ "kpca__gamma": np.linspace(0.03, 0.05, 10), "kpca__kernel": ["rbf", "sigmoid"] }] grid_search = GridSearchCV(clf, param_grid, cv=3) grid_search.fit(X, y) if you print the best result print(grid_search.best_params_) {'kpca__gamma': 0.043333333333333335, 'kpca__kernel': 'rbf'} another approach would be too look at the reconstruction error meaning you go into lower dimension and then try to reconstruct and calculate the error from the original space rbf_pca = KernelPCA(n_components = 2, kernel="rbf", gamma=0.0433, fit_inverse_transform=True) #make sure fit_inverse_transform is true X_reduced = rbf_pca.fit_transform(X) X_preimage = rbf_pca.inverse_transform(X_reduced) from sklearn.metrics import mean_squared_error mean_squared_error(X, X_preimage) 32.786308795766132 you can use grid search to find the best hyperparameter.
