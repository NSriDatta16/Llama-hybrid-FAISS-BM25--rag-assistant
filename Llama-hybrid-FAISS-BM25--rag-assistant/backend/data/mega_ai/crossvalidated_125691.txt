[site]: crossvalidated
[post_id]: 125691
[parent_id]: 
[tags]: 
Distance measure for categorical attributes for k-Nearest Neighbor

For my class project, I am working on the Kaggle competition - Don't get kicked The project is to classify test data as good/bad buy for cars. There are 34 features and the data is highly skewed. I made the following choices: Since the data is highly skewed, out of 73,000 instances, 64,000 instances are bad buy and only 9,000 instances are good buy. Since building a decision tree would overfit the data, I chose to use kNN - K nearest neighbors. After trying out kNN, I plan to try out Perceptron and SVM techniques , if kNN doesn't yield good results. Is my understanding about overfitting correct? Since some features are numeric, I can directly use the Euclid distance as a measure, but there are other attributes which are categorical. To aptly use these features, I need to come up with my own distance measure. I read about Hamming distance , but I am still unclear on how to merge 2 distance measures so that each feature gets equal weight. Is there a way to find a good approximate for value of k? I understand that this depends a lot on the use-case and varies per problem. But, if I am taking a simple vote from each neighbor, how much should I set the value of k? I'm currently trying out various values, such as 2,3,10 etc. I researched around and found these links, but these are not specifically helpful - a) Metric for nearest neighbor , which says that finding out your own distance measure is equivalent to 'kernelizing', but couldn't make much sense from it. b) Distance independent approximation of kNN talks about R-trees, M-trees etc. which I believe don't apply to my case. c) Finding nearest neighbors using Jaccard coeff
