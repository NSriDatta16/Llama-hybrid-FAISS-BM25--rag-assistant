[site]: datascience
[post_id]: 88055
[parent_id]: 17661
[tags]: 
Oh, well, let's say it depends on your task and model. For instance, in autoencoders, there are two main choices for upsampling. You can employ transposed convolution or rescaling. The better choice is the latter case due to the fact that the former can lead to checker-board artifacts. About zero padding, it is usually done in almost all CNNs if you use the same convolution which means the height and width of the output activation maps should be equal to the size of their input counterparts. Zero padding is usually done when you want to extract features from inputs with conv layers. The main reason we use zero padding is that the boundary locations in the inputs of the conv layers affect more entries in the output activation map.
