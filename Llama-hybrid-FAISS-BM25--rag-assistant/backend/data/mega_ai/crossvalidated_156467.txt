[site]: crossvalidated
[post_id]: 156467
[parent_id]: 156449
[tags]: 
Seasonality is probably not very strong. Different algorithms will give different results, unless seasonality is glaringly obvious. The best measure is always to compare forecast accuracy on a holdout set: hold back the last $n$ observations, fit your models to all other observations, forecast into the last $n$ time periods with both models, then compare forecast accuracy using your error measure of choice (see 5 below). Yes, this is a common complaint. I don't think there is an easy way to get the in-sample fit. But you can get the residuals: auto.arima(WWWusage)$residuals . Best to look into the code of auto.arima() to see whether you need to add or subtract them from the original series to get the fit. I'd say you have to subtract ("actuals=model+residuals"), but better check. I recommend a good forecasting textbook. This is a very good start. Otherwise, read through the help pages. The appropriate error measure will depend on your personal loss function. Is your pain symmetric, and will it increase more strongly with larger errors? Then use MSE. Is your pain proportional to absolute errors? Then use MAE. Best to look at multiple error measures. One tip: averaging forecasts will usually improve accuracy. Consider taking the average of your two models' forecasts per future time bucket. auto.arima() apparently fits no drift, even if you allow it.
