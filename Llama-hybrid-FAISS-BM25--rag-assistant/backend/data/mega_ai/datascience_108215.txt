[site]: datascience
[post_id]: 108215
[parent_id]: 108205
[tags]: 
You could use a " masked language model " (MLM) which predicts if a (short) piece of text or sentence belongs to some class (labels can be derived from the indices, I guess). With LSTM you only go in one direction (start to end) while with bidirectional encoder (BERT like models), you go in both directions which is a great improvement. Original BERT uses MLM as well as "next sentence prediction" (NSP) during learning. However, maybe MLM with classification at the end may be sufficient. MLM works in the way, that you first learn the nature of the text by "masking" random words and try to predict them. This is very helpful to make a final (downstream) classification regarding to what kind of category a text belongs. You may also use a pretrained BERT model and fine tune it. Finding relevant parts of text is one of the downstream tasks BERT can do.
