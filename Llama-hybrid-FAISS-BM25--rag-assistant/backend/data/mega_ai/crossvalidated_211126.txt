[site]: crossvalidated
[post_id]: 211126
[parent_id]: 210229
[tags]: 
Developing hierarchical models involves a fair amount of decision making from the modeler, so most of what I present here should be viewed as a recommendation, not as solid statistical fact. Based on your comments above, your response rates are actually estimated from a number of success/failure trials. Estimating these error rates outside of the model is essentially equivalent to assuming that this is the true error rate with 100% certainty; for observations where you calculated a 0% error rate, you would essentially be saying that errors are impossible. Estimating these rates as part of the model would allow you to incorporate and account for that uncertainty. In this response, I'm not going to address your three questions individually; Questions 1 and 2 are superseded by my suggestion that you use the the raw number of trails with and without errors instead of the proportion. The majority of this answer addresses prior selection. First, here is the model I would use: $$\begin{align*} Y_{i} &\sim \mbox{Binomial}(N_{i},p_{i})\\ \mbox{logit}(p_{i}) &= \alpha_{j|i}+\beta_{k|i}\\ \alpha_{j} &\sim \mbox{Normal}(\mu,\sigma_{\alpha})\\ \mu_{i} &\sim \mbox{Normal}(0,2)\\ \sigma_{\alpha} &\sim \mbox{Cauchy}^{+}(0,1)\\ \beta_{k} &\sim \mbox{Normal}(0,\sigma_{\beta})\\ \sigma_{\beta} &\sim \mbox{Cauchy}^{+}(0,1)\end{align*}$$ For each of your 1500 data points, you have $N_{i}$ total trials, out of which $Y_{i}$ is the number of trials that did not contain an error. $\alpha_{j}$ is the baseline intercept for subject $j$ (where the value for $j$ for any given $i$ is determined by your $x_{2}$ indicator). $\beta_{k}$ is the effect of treatment $k$ (likewise, the value of $k$ is determined by the $x_{1}$ indicator). This assumes that the effect of each treatment does not vary among subjects. $\alpha_{j}$ is modeled hierarchically around $\mu$, which is the overall baseline log-odds of a trial not having an error (ignoring treatment and subject). Feel free to vary the standard deviation of its prior a bit; I chose 2 because the inverse $\mbox{logit}^{-1}(4)=.98$, meaning that 95% of the probability mass for the average of $p$ is between .02 and .98. $\sigma_{\alpha}$ controls the variation among each subject's baseline; Gelman (2006) recommends half-Cauchy priors for hierarchical scale parameters. A half Student-t prior with 3-7 degrees of freedom could also be appropriate, and would allow for greater shrinkage towards $\mu$. I would go with a scale parameter of 1 for this prior (because a shift of 1 in logit-space is rather large), but larger values would allow for greater variation. While $\beta$ could be treated as a “fixed effect” (and many people would argue that that it should be), modeling them with a common prior for standard deviation would impose regularization that would help guard against multiple comparisons (Gelman et al., 2012) . A half Cauchy-prior would again be appropriate in this case, since it would allow $\sigma_{\beta}$ to be very large (which would essentially turn it into a no-pooling/fixed effects model, with each $\beta_{k}$ estimated separately), or very small (which shrink each effect to 0, meaning no difference among treatments). After running the model, you could estimate the overall error rate of each treatment as $\mbox{logit}^{-1}(\mu+\beta_{k})$. I would also recommend using a posterior predictive check for your model (i.e., generating new data with your posterior distribution and checking it against your real data). This is a particularly effective way to check the adequacy of your model. All of this is fairly easy to implement in Stan, and will almost certainly be faster than a JAGS implementation. Here is some sample Stan code: data{ int nObs; int N[nObs]; int Y[nObs]; int nSubject; int nTreatment; int x1[nObs]; int x2[nObs]; } parameters{ vector[nTreatment] alpha; real mu; vector[nSubject] beta; real sigma_alpha; real sigma_beta; } model{ alpha ~ normal(mu, sigma_alpha); sigma_alpha ~ cauchy(0, 1); mu ~ normal(0,1); beta ~ normal(0,sigma_beta); sigma_beta ~ cauchy(0,1); Y ~ binomial_logit(N, alpha[x2] + beta[x1]); } generated quantities{ // This part of the model does not involve sampling vector[nTreatment] prob_treat; // This is the posterior probability of each treatment, accounting for variation within subjects vector[nTreatment] prob_Y; // This is the posterior probability of having no errors (i.e, your error rate) int yNew[nObs]; // New observations for a posterior predictive check for(i in 1:nTreatment) prob_treat[i]
