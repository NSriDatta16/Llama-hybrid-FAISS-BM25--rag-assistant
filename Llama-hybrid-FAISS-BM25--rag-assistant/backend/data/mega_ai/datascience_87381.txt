[site]: datascience
[post_id]: 87381
[parent_id]: 87355
[tags]: 
Maybe thinking about the relation between neural networks and regression might help you understand the role of weights. If you take a shallow single layer neural network, the output for a single input sample with $m$ features is: $y = f(w_0 + \sum_{i=1}^m w_i x_i)$ where $w_0$ is the bias and $f$ is your activation function. You can recognize this is a non-linear regression formula, with the bias being the intercept - it reduces to linear regression when $f$ is the identity. So, the weights are just the parameters of this regression, this is why you multiply the input values by the weights. A deep neural network just adds other layers to allow for arbitrary (not limited to the function $f$ ) non-linearity - actually two layers already give the ability to fit arbitrary functions, provided enough neurons are present, but this networks might be practically impossible to fit to data and this is where additional layers with some structure come to rescue but this is another story. Your input features are the values of the pixels, so they get multiplied by the weights because this is how the model is formulated, similar to a regression. Hope this helps clarify.
