[site]: datascience
[post_id]: 112211
[parent_id]: 
[tags]: 
Multilayer/deep recurrent layer

I might be missing something, but I'm completely unable to find any reference about this topic. In the literature, there are many references about RNN, GRU, LSTM, STAR and many other architecture that exploits some sort of trick to avoid having vanishing gradient and stable learning. However, to me seems that there is no work about actually improving how much a RNN can "save" inside the recurrent state. From this my question: why don't we use deep recurrent layers in order to enable our model to better encode the recurrent state? In my opinion, this is neither equivalent to stacking multiple RNN nor using MultiRNNCell from tensorflow Anybody has any reference to some paper about this? i tried to search for "deep recurrent layer" or "multilayer FFN in recurrent layer" but I was not able to find anything
