[site]: crossvalidated
[post_id]: 161973
[parent_id]: 
[tags]: 
Cross validation for feature selection: still possible to overfit?

I would like to find a good pair of predictors out of about 400 available pairs. To do this I am using LOO cross validation. Since there are so many pairs available, don't I run into the issue that the best pair could be a spurious finding? I'm thinking this is much like multiple testing. To be more concrete, for a given pair of predictors I perform LOO cross validation to estimate the loss function using those predictors. That is, for each of the n folds I leave out a sample, build the model using the given predictor pair, and then compute its loss function on the held out sample. These n loss values are then averaged to determine the merit of the predictor pair. I select the pair with the lowest average loss. Much of what I have read about CV in optimizing models is to optimize over a few parameters (like lambda in Lasso) rather than many (e.g. whether to include a feature). So is there a rule of thumb for how many such tests one can perform with CV?
