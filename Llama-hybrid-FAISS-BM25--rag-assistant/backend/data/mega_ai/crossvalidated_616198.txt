[site]: crossvalidated
[post_id]: 616198
[parent_id]: 
[tags]: 
How to limit the neural network regression output between 0 and 1 without sigmoid or tanhï¼Ÿ

I need to limit the output value to [0,1], but using tanh or sigmoid activation function after the last layer of convolution or full linkage will result in a return gradient of 0, (falling in the level of the activation function), but also do not need to let the value of the sum of 1 (softmax), is there any good way, thanks. I've used laynorm to limit the output size, but it still doesn't work, very hard to train.
