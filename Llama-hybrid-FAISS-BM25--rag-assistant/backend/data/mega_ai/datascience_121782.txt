[site]: datascience
[post_id]: 121782
[parent_id]: 
[tags]: 
Some simple questions about confusion matrix and metrics in general

I will first tell you about the context then ask my questions. The model detects hate speech and the training and testing datasets are imbalanced (NLP). My questions: Is this considered a good model? Is the False negative really bad and it indicates that my model will predict a lot of ones to be zeros on new data? Is it common for AUC to be higher than the recall and precision when the data is imbalanced? Is the ROC-AUC misleading in this case because it depends on the True Negative and it is really big? (FPR depends on TN) For my use case, what is the best metric to use? I passed the probabilities to create ROC, is that the right way? Edit: I did under-sampling and got the following results from the same model parameters: Does this show that the model is good? or can it be misleading too?
