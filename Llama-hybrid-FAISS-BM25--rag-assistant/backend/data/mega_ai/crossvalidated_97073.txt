[site]: crossvalidated
[post_id]: 97073
[parent_id]: 
[tags]: 
Is the following procedure to measure the quality of an imputation ok?

I'd like to compare different kinds of imputation techniques, i.e. methods which allow to fill missing data fields in a data frame. For now, I'm only using the R package mice , which uses multiple imputation with chained equations. Below, I'm describing the steps I am following, and I'd appreciate comments if this is a reasonable approach. The data I am currently using is an excerpt from the boys dataset (which is included in mice ), where I make sure that the region variable reg is always there. df To be able to make the imputation results more comparable, I log-transform and normalize/centralize df column-wise. An extra column contains a numeric value numeric.class as an indicator of the reg value is added. I'm imputing the values of the 5 numeric variables "age", "hgt","wgt","bmi","hc" with a two-level linear model which uses numeric.class as class variable, which I do by predMatr[1:5,1:5] and calling imp . To measure the performance of this, I actually replace the df in the mice call from the last step by a validation dataframe which has been preprocessed in the following way. Setting the validation percentage to, say, 10%, and going column by column, I randomly select and replace 10% of all non- NA entries in a given column by NA . I picked this stratified approach since, usually, there are not complete rows missing, but rather only single (or possibly multiple) entries. For the entries where I introduced extra NA s, I compute the RMSE column-wise, i.e. the mean of the square of the difference between the imputed values and the acual values. I repeat steps 5-7, replacing in each column 10% of the entries by NA , where I make sure that these entries were not replaced earlier. This way, I get 10 folds of cross-validation data frames and 10 RMSEs for each numerical variable. I have the following questions: Are there any methodical flaws in this approach? In particular, did I implement the two-level linear model in a correct way? (Should I have introduced dummy variables?) Is the RMSE per numerical feature, averaged over the 10 folds, a reasonable way to measure the quality of the imputation? As I describe it, I only compare the imputed values of the first imputation ( m=1 ) with the actual values. What's the most reasonable way to include multiple imputations in the RMSE? Edit : Below you find my R script: library("mice") library("lattice") library("mi") # determine the number of folds. k length # size of block of indices for fold number i of feature j is given by idx.meta.df$foldsize[j] # except for the first idx.meta.df$largerfolds[j] elements. They are 1 element larger. if (i
