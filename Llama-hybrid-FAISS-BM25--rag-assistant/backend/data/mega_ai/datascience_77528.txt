[site]: datascience
[post_id]: 77528
[parent_id]: 77298
[tags]: 
When evaluating xgboost (or any overfitting prone model), I would plot a validation curve. Validation curve shows the evaluation metric, in your case R2 for training and set and validation set for each new estimator you add. You would usually see both training and validation R2 increase early on, and if R2 for training is still increasing, while R2 for validation is starting to decrease, you know overfitting is a problem. Be careful with overfitting a validation set. If your data set is not very large, and you are running a lot of experiments, it is possible to overfit the evaluation set. Therefore, the data is often split into 3 sets, training, validation, and test. Where you only tests models that you think are good, given the validation set, on the test set. This way you don't do a lot experiments against the test set, and don't overfit to it.
