[site]: crossvalidated
[post_id]: 429027
[parent_id]: 429000
[tags]: 
Since we are treating $\theta$ as vector of random variables i think the term error is thematically wrong, indeed one of the strength of Bayesian methods is the richness of information granted about $\theta$ through it's posterior distribution. For MCMC sampling methods (of which the Gibbs sampler is one kind) the posterior distribution of paramters $\theta$ can generally be obtained in this manner: Let the chains converge to stationary (if it's possible for the problem) - generally known as the burn-in period. Keep every x sample of $\theta$ . Since the chain is correlated in time we only keep every xth value, this concept is known as thinning. The value of x is generally a trade-off between time and accuracy. The kept samples of $\theta$ describes it's posterior distribution for which any moment can be calculated if so desired.
