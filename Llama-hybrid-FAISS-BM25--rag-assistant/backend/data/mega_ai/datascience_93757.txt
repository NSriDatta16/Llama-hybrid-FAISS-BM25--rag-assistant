[site]: datascience
[post_id]: 93757
[parent_id]: 76322
[tags]: 
You don't need to do the n-gram creation for an RNN like you're showing. The point of Neural Language modeling with RNN/LSTM is to avoid having to make the Markov assumptions you state. To use an RNN, you just feed the whole sentence as-is to the RNN as a sequence, and as a target, you feed a sequence with each word from the input shifted one to the right. You can look at this repo for an example of an RNN language model: https://github.com/pytorch/examples/tree/master/word_language_model it may be better to use LSTMs which are better able to capture long range dependencies (longer than a Markov model might allow!). I suspect you're getting random sequences because of the repetition of your short sequences, which just adds a lot of noise for the Neural Net.
