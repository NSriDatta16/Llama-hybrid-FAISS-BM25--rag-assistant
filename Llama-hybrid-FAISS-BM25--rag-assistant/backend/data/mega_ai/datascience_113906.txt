[site]: datascience
[post_id]: 113906
[parent_id]: 
[tags]: 
Working with massive data what is the right approach

let's say I have database with massive data (millions of rows) additionally Let's say 26 Million rows are entered every day I want to build a fraud model to check these 26 Million rows every day.. as far as i understand the right approach is: pick randomly small amount of data 500k+- and do an EDA (use pandas) Perform data preprocessing, feature engineering, build the model and evaluate it on the small data sets you do the EDA (use pandas) take the new rows that enter to the database (around 26 Million rows) use Spark to Perform data preprocessing, feature engineering again use Spark to create the same feature you create to train the model (with the small data) on the big dataset (26 Million rows) and clean null) am i right that is the right approach?
