[site]: crossvalidated
[post_id]: 488017
[parent_id]: 
[tags]: 
Understanding mini-batch gradient descent

I would like to understand the steps of mini-batch gradient descent for training a neural network. My train data $(X,y)$ has dimension $(k \times n)$ and $(1 \times n)$ , where $k$ is the number of the features and $n$ is the number of observations. For each layer $l=1,...L$ my parameters are $W^{[l]}$ of dimension $(n^{[l]} \times n^{[l-1]})$ , where $n^{[0]}=k$ a) First I randomly initialize the parameters $W^{[l]}$ for $l=1,...L$ . b) I take a sample of length $p\in[1,n]$ of my training data, denoted by $(X^{(1)},y^{(1)})$ for sample number $1$ . c) I compute the cost $J^{(1)}(W)$ with the first initialization of the parameters and the first sample of the train data. d) In back-propagation I update the parameters for $l=L,...1$ according to a learning rate $\alpha$ : $$ W^{[l]} = W^{[l]} - \alpha \text{ } \frac{\partial J^{(1)}(W)}{\partial W^{[l]}}$$ Now I've done one step of the gradient descent with one sample of the train data. How does the algorithm continue? Does it repeat step (c) and (d) with the "new" $W^{[l]}$ on a second sample of the train data $(X^{(2)},y^{(2)})$ ? In this case, will it continue until convergence when every update in the gradient descent is done with different samples of the train data? Please let me know if something is not clear.
