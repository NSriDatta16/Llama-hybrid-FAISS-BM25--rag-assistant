[site]: crossvalidated
[post_id]: 631808
[parent_id]: 
[tags]: 
Does Positional Interpolation Change Llama's Architecture?

I'm currently exploring Meta's positional interpolation method, which aims to increase the context size in their large language model. This method extends the context length from n x n into n′ x n′ . It achieves this by interpolating new token values in between existing ones. My question: Does this interpolation technique necessitate changes to the underlying model architecture? Specifically, how does the self-attention matrix, configured as an n × n matrix, adapt to accommodate the extended context size of n′ x n′?
