[site]: crossvalidated
[post_id]: 336520
[parent_id]: 
[tags]: 
Does Cross-Entropy cost affect earlier layers in comparison to MSE cost?

Most of my knowledge about cross-entropy cost is from this tutorial With cross-entropy the partial derivative to the error with respect to a weight in the output layer is: $$ \dfrac{∂C}{∂w_{k}} = a_k*\dfrac{σ′(z)}{σ(z)(1−σ(z))} * (o_{out} - o_{target}) $$ Which simplifies, according to the guide, to: $$ \dfrac{∂C}{∂w_{k}} = a_k * (o_{out} - o_{target}) $$ Where in MSE it's: $$ \dfrac{∂C}{∂w_{k}} = a_k * a'_{output} * (o_{out} - o_{target}) $$ The removal of $ a'_{output} $ is what should give cross-entropy it's superiority from my understanding, since the partial derivative will be bigger? But my question is about earlier layers, and if this new cost function affect the training of these layers? The tutorial did only define a change for the output layer, but not for any amount of hidden layers. So if the calculation of the partial derivative is the same, how does this new cost function affect the training of earlier layers? If so? And a follow up question would be about a deep neural network with 100 hidden layers. If this new cost function does not affect the training of earlier layers, does it then really do much for a very deep neural network, and its training? Edit: When I above use the phrases like "... does this new cost function affect the training of earlier layers?" I mean in comparison to MSE. And more clearly: Will the partial derivative be the same for any hidden layer as if calculated using the MSE cost function? The output layer is a sigmoid layer.
