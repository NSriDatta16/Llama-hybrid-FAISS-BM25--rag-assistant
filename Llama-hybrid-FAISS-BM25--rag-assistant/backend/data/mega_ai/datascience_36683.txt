[site]: datascience
[post_id]: 36683
[parent_id]: 
[tags]: 
What is max aggregation on a set of word embeddings?

In a paper I see: $\mathcal{Q}$ is a set of words. $\psi_{G^w}$ are word embeddings. so, $\{\psi_{G^w}(w_t), \forall w_t \in \mathcal{Q}\}$ gives me a set of embeddings for all words in $\mathcal{Q}$. For example, if I have: Q = {'a', 'b', 'c'} embedding_gw.shape = (1000, 8) # 1000 words in vocab, embedding size is 8 I will get: { [1,2,3,4,5,6,7,8], #embedding of 'a' [8,7,6,5,4,3,2,1], #embedding of 'b' [4,5,3,6,7,8,1,2] #embedding of 'c' } The result of $\max\{\psi_{G^w}(w_t), \forall w_t \in \mathcal{Q}\}$ is supposed to be a single vector. My question is, how do I get this single $\max$ vector? Do I sum all values in each embedding and pick the largest one? Do I pick the $\max$ value of each $i^{th}$ position, creating a completely new vector? Do I do something else?
