[site]: crossvalidated
[post_id]: 597242
[parent_id]: 270157
[tags]: 
Late answer, but people may find it useful. I have some recent results on this topic. Specifically, it turns out that the GAN theory presented in Proposition 1 (optimal discriminator) of the original 2014 paper by Goodfellow et al is missing a key condition. That condition is: ${\rm dim}(z)\geq{\rm dim}(x)$ , that is, the dimension of the latent variable is at least equal to that of the data, which is the opposite of what usually applies in practice since GANs map a low-dimensional latent space to a high-dimensional generated data space. This is not to say that the GAN technique does not work, but the theory must be approached carefully. The result I am referring to is contained in the paper: Convergence & Optimality Analysis of low dimensional GANs , which was published in Dec 2021 in the IEEE Access online journal, a peer-reviewed source. Now to the main part of your question that I can help with: "I feel that I do not fully understand the mathematics or usefulness of GANs" The usefulness of GANs is in their practical applicability: synthetic image generation, where they are a powerful unsupervised learning technique, even if there are convergence problems. To understand the maths, I recommend you read the second part of the paper referenced above. In particular, there is a simple ${\rm dim}(z)={\rm dim}(x)=1$ least squares GAN (LSGAN) example that has the nice property that the loss function is quasi-analytical - just needing a 1-D Monte Carlo integration to evaluate. Thus the parameter trajectories can be obtained for any starting point and step size in the gradient algorithm. No stochastic sampling is needed; the results are always the same to several significant figures. There is also a comparison with the usual stochastic gradient ascent-descent algorithm. This is the only work I am aware of where an analytical GAN has been compared with its sampled counterpart and the results shown to agree. Note that the GAN loss function usually can't be obtained exactly since it needs the data PDF, which is unknown; but in the example it is explicit. It turns out that the parameters of this very simple LSGAN converge to a plateau, not a saddle point. A good amount of insight into the functioning of GANs in higher dimensions can be obtained from this quite simple example, although it doesn't explain mode collapse. The paper also has some interesting visualisations, including a powerpoint presentation 1-D Analytical LS-GAN talk with more material available in the log at Low-dimensional GAN project . I hope you find the material interesting and helpful.
