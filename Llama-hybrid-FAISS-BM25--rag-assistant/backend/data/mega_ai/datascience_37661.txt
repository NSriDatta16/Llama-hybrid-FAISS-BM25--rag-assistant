[site]: datascience
[post_id]: 37661
[parent_id]: 37645
[tags]: 
Not sure how much about NLP you already digested, so shortly from the begining: Text processing usually starts with tokenization into words (and other segments like numbers and punctuation marks) and each word has an index in dictionary created from whole corpus (set of texts). Each item (word) in the dictionary becomes a separate feature. When you encode a given text, each word in one-hot encoded, so the text becomes a vector of the length that equals size of the dictionary (classical bag of words approach). That usually goes into thousands (e.g. Yelp review dataset has over milion unique words) so clearly this is a highly dimensional problem. There is a number of techniques then to reduce this dimentionality, like embeddings of LDA. Appendix for the question in comment: When you decide for the number of feature (which is number of words that matter for your taks), usually you exlude stop words (like 'a', 'an', 'the' and others that are very popular in texts but does not carry meaning for e.g. classification). I usually manually select them from frequency dictionary (list of words with counts in corpus, sorted by the count). You can compare your list with that used in NLTK. And from the other side of the dictionary, you would usually skip words that occur very rare, like less than 3 times (number depends on the corpus). So you end with your one dictionary which has it's size and this is the number of your features.
