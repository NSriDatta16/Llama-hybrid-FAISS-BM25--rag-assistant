[site]: crossvalidated
[post_id]: 197792
[parent_id]: 
[tags]: 
On the choice of activation functions for neural networks (coming from signal processing)

As far as I understood the gist of this paper , learning a representation that is invariant under some one parameter group $\{U_t\}$ (e.g. 1D translations) can be accomplished by letting the convolutions act as diagonalization of the infinitesimal generator $A$ of $\{U_t\}$ (that is: $U_t=e^{itA}$) and then taking the (pointwise) complex modulus. (The basic idea seems to be that, once the input is in the basis that diagonalizes $A$, $U_t$ just acts as a multiplication with a complex phase, which would be eliminated by taking absolute values.) Now to my question: Assuming I interpreted everything correctly and furthermore assuming that there is no problem working in a real setting (just like $\mathbb R^2\approx\mathbb C$ in some sense), wouldn't that paper motivate an activation function that acts more like an absolute value instead of the popular ReLU? (I do have my concerns for the applicability of this paper though, since, in the case of the one parameter family being transformations, the change of basis would be given by the Fourier transform which is as non-local as it gets — and this wouldn't really go together with those $3\times 3$, $5\times 5$, … kernels in use)
