[site]: crossvalidated
[post_id]: 168552
[parent_id]: 
[tags]: 
verified procedure for calculating gradient descent?

I'm wondering if there is a good procedure people are using for gradient descent that is pretty well validated--something like a package for R or Python, or generic code many people adapt. After taking Andrew Ng's machine learning course in Coursera I was able to implement gradient descent in Octave, but I'm hoping to work with it in R or Python, with which I am more familiar. I'm also hoping for something more standardized than code I would write itself, to give it more credibility with my place of employment or other researchers. Most of what I've found on this site is questions about writing code for gradient descent, like this post ( https://stats.stackexchange.com/questions/115425/multiplicative-gradient-descent ) and this one ( https://stats.stackexchange.com/questions/142257/procedure-for-gradient-descent ). And I've found examples of code from other sites, like this http://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/ . So I think there is no gradient descent package for R. But I was wondering if people had any thoughts for a good way to start implementing it in R or Python that doesn't rely on trust in the individual coder (e.g. me). Is there a blog post or tutorial with good generic code that can easily be used to standardize different gradient descent implementations?
