[site]: crossvalidated
[post_id]: 381585
[parent_id]: 265572
[tags]: 
A short answer. The approach of doing data-driven model selection or tuning, then using standard inferential methods on the selected/tuned model (Ã  la Zuur et al. , and many other respected ecologists such as Crawley), will always give overoptimistic results : overly narrow confidence intervals (poor coverage), overly small p-values (high type I error). This is because standard inferential methods assume the model is specified a priori ; they don't take the model tuning process into account. This is why researchers like Frank Harrell ( Regression Modeling Strategies ) strongly disapprove of data-driven selection techniques like stepwise regression, and caution that one must do any reduction of the model complexity ("dimension reduction", e.g. computing a PCA of the predictor variables and selecting the first few PCA axes as predictors) by looking only at the predictor variables. If you are interested only in finding the best predictive model (and aren't interested in any kind of reliable estimate of the uncertainty of your prediction, which falls in the realm of inference!), then data-driven model tuning is fine (although stepwise selection is rarely the best available option); machine learning/statistical learning algorithms do a lot of tuning to try to get the best predictive model. The "test" or "out-of-sample" error must be assessed on a separate, held-out sample, or any tuning methods need to be built into a cross-validation procedure. There does seem to have been historical evolution in opinions on this topic; many classic statistical textbooks, especially those that focus on regression, present stepwise approaches followed by standard inferential procedures without taking the effects of model selection into account [citation needed ...] There are many ways to quantify variable importance, and not all fall into the post-variable-selection trap. Burnham and Anderson recommend summing AIC weights; there's quite a bit of disagreement over this approach. You could fit the full model (with appropriately scaled/unitless predictors) and rank the predictors by estimated magnitude [biological effect size] or Z-score ["clarity"/statistical effect size].
