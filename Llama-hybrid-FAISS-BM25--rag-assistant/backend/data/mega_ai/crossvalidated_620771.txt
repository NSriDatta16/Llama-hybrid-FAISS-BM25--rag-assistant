[site]: crossvalidated
[post_id]: 620771
[parent_id]: 620520
[tags]: 
There's some ambiguity (and I can't ask for clarification in the comments because I'm a new account). I'd say first off all, in the scenario that you described, you're looking at an ordinal regression problem, not just a multi-class classification problem. That is, it's somewhere between a continuous regression task and a classification task. Accuracy is a bad measure, as you mention, because it 1) doesn't tell you whether you were close or not and 2) doesn't account for imbalance in the dataset. Here is a paper that performs and ordinal regression task (specifically targetin age) in a neural network (CNN using faces as the inputs): Rank consistent ordinal regression for neural networks with application to age estimation , where they propose a metric they call CORAL (COnsistent RAnk Logits), specifically as their loss function for training. Prior to that they talk about how it's common to break the task up into K binary classification tasks (i.e. >= x or For this paper, they chose to use MAE and MSE both as measures of model performance. I recently did some ordinal regression on a task where the ranks happened to very nicely follow a Poisson distribution and found that mean Poisson deviance was a far better criterion than MSE or MAE. In classical ordinal logistic regression, some measures of pseudo-R^2 are also occasionally used (such as McFadens, see another comment on that here ). Now, if you're discussing a true multi-class classification problem (where there is no inherent ordering to the classes). You most likely want either weighted or regular categorical cross-entropy (weighted if you have imbalance during training). For evaluating model performance of a multi-task classifier it depends on whether your classes are balanced or not, but generally I report both F1 score and accuracy as the major metrics and break precision/recall/accuracy out by class deeper in any reporting or examination of the model that I perform. Both F1 score and accuracy can be balanced in various ways. For F1 score I like to report both weighted and micro-averaged F1 score. For a model that performs really well on all classes, all of these measures will converge.
