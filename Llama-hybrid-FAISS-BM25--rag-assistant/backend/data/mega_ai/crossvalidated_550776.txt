[site]: crossvalidated
[post_id]: 550776
[parent_id]: 
[tags]: 
How to measure machine-human agreement in the light of human-human agreement

Suppose you have a set of subjects, and for each one a measurement of a certain property by a machine and measurements of the same property by several separate humans. Each measurement is a single real number. All human measurements are regarded as acceptably accurate, even though different humans give somewhat different measurements for the same subject. The research question is: How good is the machine-human agreement? Or: Is the machine good enough to use instead of the humans? Let's say this has to be answered by a single numerical metric. E.g. if there was only one human, you could calculate MSE between the machine and human measurements, and see if that was small enough in terms of practical significance. My question is: Is there a way to measure machine-human agreement that takes human variation into account by using human-human agreement as a kind of target? The idea is that if the machine agrees with humans as closely as humans agree with other humans, then the machine is acceptable. Here is one metric, and some advantages (features I want the metric to have) and disadvantages (features I want it to avoid): Calculate MSE between the machine and each of the humans in turn, and take the average (this is machine-human agreement). Calculate MSE between each pair of humans in turn, and take the average (this is human-human agreement). If machine-human agreement is better than human-human agreement, the machine is acceptable, and otherwise it is not. Advantages: This is interpretable, if you ignore the issue of how many humans there are. Disadvantages: This metric gives no idea about how many humans you need in order to judge whether the machine is acceptable. (Related to the previous point) The metric needs to have confidence intervals, and these need to get smaller as we include more humans. It is a binary metric, but there ought to be degrees of acceptability/agreement, because that is the case in reality. It might behave strangely. E.g. if you apply it to a human, pretending that they are the machine, it might often say that the human is unacceptable (it is impossible to avoid this completely, because there might be a very erratic human, but we need to avoid it as much as possible). To minimize assumptions, I would like to avoid using a statistical model if possible.
