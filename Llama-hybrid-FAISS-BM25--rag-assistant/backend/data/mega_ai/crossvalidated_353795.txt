[site]: crossvalidated
[post_id]: 353795
[parent_id]: 19181
[tags]: 
Here's one reason why the dual formulation is attractive from a numerical optimization point of view. You can find the details in the following paper : Hsieh, C.-J., Chang, K.-W., Lin, C.-J., Keerthi, S. S., and Sundararajan, S., “A Dual coordinate descent method forlarge-scale linear SVM”, Proceedings of the 25th International Conference on Machine Learning, Helsinki, 2008. The dual formulation involves a single affine equality constraint and n bound constraints. 1. The affine equality constraint can be "eliminated" from the dual formulation. This can be done by simply looking at your data in $R^{d+1}$ via the embedding of $R^d$ in $R^{d+1}$ resuling from adding a single " $1$ " coordinate to each data point, i.e. $R^d \to R^{d+1}: (a_1,..., a_d) \mapsto (a_1, ..., a_d, 1)$ . Doing this for all points in the training set recasts the linear separability problem in $R^{d+1}$ and eliminates the constant term $w_0$ from your classifier, which in turn eliminates the affine equality constraint from the dual. 2. By point 1, the dual can be easily cast as a convex quadratic optimization problem whose constraints are only bound constraints. 3. The dual problem can now be solved efficiently, i.e. via a dual coordinate descent algorithm that yields an epsilon-optimal solution in $O(\log(\frac{1}{\varepsilon}))$ . This is done by noting that fixing all alphas except one yields a closed-form solution. You can then cycle through all alphas one by one (e.g. choosing one at random, fixing all other alphas, calculating the closed form solution). One can show that you'll thus obtain a near-optimal solution "rather quickly" (see Theorem 1 in the aforementioned paper). There are many other reasons why the dual problem is attractive from an optimization point of view, some of which exploit the fact that it has only one affine equality constraint (the remaing constraints are all bound constraints) while others exploit the observation that at the solution of the dual problem "often most alphas" are zero (non-zero alphas corresponding to support vectors). You can get a good overview of numerical optimization considerations for SVMs from Stephen Wright's presentation at the Computational Learning Workshop (2009). P.S.: I'm new here. Apologies for not being good at using mathematical notation on this website.
