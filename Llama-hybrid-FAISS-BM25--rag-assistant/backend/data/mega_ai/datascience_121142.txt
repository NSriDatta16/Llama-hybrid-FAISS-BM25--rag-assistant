[site]: datascience
[post_id]: 121142
[parent_id]: 121120
[tags]: 
Good question! You are correct that AdaBoost works by iteratively adding weak classifiers to the overall model to improve its accuracy. The key to understanding how AdaBoost ensures that each new classifier does better than the previous one lies in the fact that it assigns a weight to each training example, which is updated after each iteration. Each iteration $m$ adjusts the weights of the misclassified samples from the previous iteration. The idea is that the next weak classifier should focus on the samples that were misclassified by the previous weak classifiers, so that the overall classification error decreases. At the start of the algorithm, all weights are set to $w_i = \frac{1}{N}$ , where $N$ is the number of training examples. After each iteration, the weights are updated based on whether the classifier correctly classified each example or not. Specifically, the weights of the misclassified examples are increased, while the weights of the correctly classified examples are decreased. This way, the misclassified examples receive more attention in the next iteration, allowing the classifier to focus on these examples and improve its performance on them. At the end of each iteration, the weights are normalized so that they sum to 1. The resulting weight vector is used to train the next weak classifier, with the aim of minimizing the weighted training error. The weight vector also determines the importance of the weak classifier in the final model: classifiers that perform well on the high-weight examples are given more weight in the final model. More precisely, let $D_m$ be the weight distribution over the training samples at iteration $m$ . Initially, $D_1(i) = 1/N$ for all $i$ . Let $h_m$ be the weak classifier at iteration $m$ that minimizes the weighted error on the training set: $$ h_m=\operatorname{argmin}_{h \in \mathcal{H}} \sum_{i=1}^N D_m(i) \cdot\left[y_i \neq h\left(x_i\right)\right] $$ where $\mathcal{H}$ is the set of weak classifiers (e.g., decision stumps). The coefficient $\alpha_m$ is then chosen to minimize the exponential loss: $$ \alpha_m=\frac{1}{2} \ln \left(\frac{1-\epsilon_m}{\epsilon_m}\right) $$ where $\epsilon_m$ is the weighted error of $h_m$ : $$ \epsilon_m=\sum_{i=1}^N D_m(i) \cdot\left[y_i \neq h_m\left(x_i\right)\right] $$ Then, the weight distribution is updated as follows: $$ D_{m+1}(i)=\frac{D_m(i) \cdot \exp \left(-\alpha_m y_i h_m\left(x_i\right)\right)}{Z_m} $$ where $Z_m$ is a normalization constant. The intuition is that if $y_i h_m(x_i) > 0$ , then the sample $x_i$ was classified correctly by $h_m$ , and its weight $D_m(i)$ should be decreased. Otherwise, if $y_i h_m(x_i) , then $x_i$ was misclassified, and its weight should be increased. In other words, the weight distribution $D_m$ is biased towards the misclassified samples. Finally, the weak classifier $h_m$ is combined with the previous weak classifiers to form the final strong classifier $C_M$ : $$ C_M(x)=\operatorname{sign}\left(\sum_{m=1}^M \alpha_m h_m(x)\right) $$ So, to answer your question, AdaBoost reassures us that it will do better after each iteration because it adjusts the weight distribution $D_m$ so that the next weak classifier $h_{m+1}$ focuses on the misclassified samples from the previous iteration. This should lead to a reduction in the training error $E_m$ of $C_m$ compared to $C_{m-1}$ . Additionally, the coefficient $\alpha_m$ is chosen to give more weight to the more accurate classifiers, which helps to further improve the performance of the final strong classifier $C_M$ .
