[site]: crossvalidated
[post_id]: 575314
[parent_id]: 
[tags]: 
Balancing Multiple Evaluation Metrics for a Model

When evaluating a machine learning (or other statistical model) against multiple evaluation metrics, is there a standardized way to choose the "best" model? As a concrete example, for a two class instance segmentation model, both $F_1$ and symmetric best dice are important indicators of the model's performance, but the metrics may give different answers to which model is best. Is there an appropriate way to combine the two metrics, such as harmonic mean to combine precision and recall?
