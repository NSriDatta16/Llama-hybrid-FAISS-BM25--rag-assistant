[site]: crossvalidated
[post_id]: 629646
[parent_id]: 581201
[tags]: 
No, standardizing the variables is not necessary , though it is commonly done for convenience and interpretability. Description of MARS Model Given $\bf x$ , a p-dimensional vector of input variables, then a MARS algorithm models a response variable $y$ as a linear combination of basis functions $$y = a_0 + \sum_{m=1}^M a_mB_m({\bf x}) + \epsilon,$$ where $\epsilon$ is (usually) a mean zero Gaussian residual, and the basis functions $B_m({\bf x})$ take the form $$B_m({\bf x}) = \prod_{i=1}^p(s_{im}[t_{im} - x_i]_+)^{u_{im}}.$$ This basis function is a type of multivariate spline where $[\cdot ]_+ = \max(\cdot, 0)$ is the rectified linear unit (ReLU), $s_{im} \in \{-1, +1\}$ is called a sign , $t_{im}$ is called a knot and should lie somewhere in the domain of $x_i$ , and $u_{im} \in \{0, 1\}$ selects which variables are "activated" in each basis function. In most variations of the MARS algorithm, only a handful of $u_{im}$ terms are allowed to be $1$ for each $m$ . This constraint can be written as $\sum_{i=1}^p u_{im} \leq J_\text{max}.$ Equivalence of Standardized Models Given a MARS model for inputs $x_i$ , we can show that for any linear rescaling of the inputs, denoted $\tilde{x}_i$ , there is another MARS model which is equivalent. Suppose that each input $x_i$ has a domain $[c_i, d_i]$ and consider a any particular basis function $B_m({\bf x})$ and it's corresponding coefficient. If we standardize $\tilde{x}_i = (x_i-c_i)/(d_i-c_i)$ so that $\tilde{x}_i \in (0, 1)$ then we can also define $$\begin{aligned} \tilde{t}_i &= (t_{im}-c_i)/(d_i-c_i) \\ \tilde{a}_m &= \left(\prod_{i=1}^p(d_i-c_i)^{u_{im}}\right)a_m. \end{aligned}$$ Now it is easy to see that $$\begin{aligned} \tilde{a}_m\tilde{B}_m({\bf x}) &= \left(\prod_{i=1}^p(d_i-c_i)^{u_{im}}\right)a_m\prod_{i=1}^p\left(s_{im}\left[\frac{t_{im}-c_i}{d_i-c_i} - \frac{x_i-c_i}{d_i-c_i} \right]_+^{u_{im}}\right) \\ &= \left(\prod_{i=1}^p(d_i-c_i)^{u_{im}}\right)a_m\prod_{i=1}^p(d_i-c_i)^{-u_{im}}\left(s_{im}\left[t_{im}-x_i \right]_+^{u_{im}}\right) \\ &= \left(\prod_{i=1}^p(d_i-c_i)^{u_{im}}\right)\left(\prod_{i=1}^p(d_i-c_i)^{-u_{im}}\right)a_m\prod_{i=1}^p\left(s_{im}\left[t_{im}-x_i \right]_+^{u_{im}}\right) \\ &= a_mB_m({\bf x}). \end{aligned}$$ Practical note: With such a small data set, I would encourage you to consider using a Bayesian version of MARS, which is less prone to overfitting and may (more easily) capture uncertainty in the model fit Depending on the number of inputs, the noise level, and the mean function of the underlying data-generating mechanisim, you may have limited success. But it's certainly worth trying! If you work with R or Python, I highly recommend the BASS package, which is an efficient and modern implementation of Bayesian MARS. With this package, you do not have to scale the inputs (this will be handled internally). Some links: R package Python package BASS paper
