[site]: crossvalidated
[post_id]: 213092
[parent_id]: 211917
[tags]: 
Giving an alternative, more applied, view from Rod's excellent answer above - In many, if not most, cases, impropriety of the posterior is a result of choices made for convenience, not a true "I'm absolutely certain of my likelihood function and prior distribution, and look what happened!" effect. Given this, we shouldn't take impropriety too seriously in our applied work unless it's going to mess up our computations . As someone famous (Huber? Tukey?) once observed, in a different context, the difference between a standard Cauchy and a Cauchy truncated at $+/- 10^{100}$ is undetectable, but one has no moments and the other has moments of all orders. In this context, if I have a posterior distribution for demand for hot dogs at AT&T Park next weekend with upper tail proportional to $1/x$, that's bad news for algorithms that calculate expected values, but if I truncate it at the estimated number of people in San Francisco, a number somewhat larger than the number of hot dogs that will in fact be sold at AT&T park next weekend, all is well, at least in terms of existence of moments. In the latter case, you can think of it as a sort of two-stage application of the real prior - one I use for calculation, which doesn't have an upper bound, and the "extra feature" of it where it's equal to zero above the population of San Francisco...", with the "extra feature" being applied in a step subsequent to the generation of the sample. The real prior is not the one that's used in the MCMC computation (in my example.) So in principle I would be quite OK with using an MCMC-generated sample from an improper distribution in applied work, but I'd be paying a lot of attention to how that impropriety came about, and how the random sample will be affected by it. Ideally, the random sample wouldn't be affected by it, as in my hot-dog example, where in a reasonable world you'd never actually generate a random number greater than the number of people in San Francisco... You should also be aware of the fact that your results may be quite sensitive to the feature of the posterior that caused it to be improper, even if you do truncate it at some large number later on (or whatever alteration is appropriate for your model.) You'd like your results to be robust to slight changes that shift your posterior from improper to proper. This can be harder to ensure, but is all part of the larger problem of making sure your results are robust to your assumptions, especially the ones which are made for convenience.
