[site]: crossvalidated
[post_id]: 510685
[parent_id]: 
[tags]: 
How does a Gaussian Process define a probability distribution in the functions space?

I am studying Gaussian Process Regression. I will post a text from the book Gaussian Process for Machine Learning, by C. E. Rasmussen & C. K. I. Williams: We first consider a simple 1-d regression problem, mapping from an input x to an output f (x). In Figure 1.1 (a) we show a number of sample functions drawn at random from the prior distribution over functions specified by a particular Gaussian process which favors smooth functions. This prior is taken to represent our prior beliefs over the kinds of functions we expect to observe, before seeing any data. The definition of a Gaussian process that I know of is given by: Definition (Gaussian process) A random process $\left(X_{t}\right)_{t \in T}$ is called a Gaussian process if, for any finite subset $T_{0} \subset T,$ the random vector $$ \left(X_{t}\right)_{t \in T_{0}} $$ has normal distribution. Equivalently, $\left(X_{t}\right)_{t \in T}$ is Gaussian if every finite linear combination $$ \sum_{t \in T_{0}} a_{t} X_{t} $$ is a normal random variable. I would like to better understand the connection of this definition with the possibility of defining a distribution in the functions space, because to draw functions according to a distribution, I would have to define sigma algebras in the function space and a measure of probability in this sigma algebra. Or perhaps there is another meaning when we say "sample functions drawn at random from the prior distribution over functions specified by a particular Gaussian process". Does anyone help to build this bridge?
