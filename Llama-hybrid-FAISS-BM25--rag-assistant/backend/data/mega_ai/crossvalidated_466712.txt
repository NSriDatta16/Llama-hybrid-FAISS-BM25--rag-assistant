[site]: crossvalidated
[post_id]: 466712
[parent_id]: 
[tags]: 
If we are taught to not remove outliers without investigation, how do robust methods (median, trimmed mean) can be even suggested?

I just saw an article, which taught to nor remove outliers without investigation, because it may be a unusual but valid observation or naturally skewed data, for example in chemistry or medicine. Only true errors were advised to remove. But how does this relate to using quartile based methods, like quantile regression, median or trimmed means based methods? Trimmed mean is nothing but a mean calculated on data with removed XX% of observations from both sides. Typically 10% - 20%. Together it makes 20% - 40% of observations removed. And, usually, it removes most of the skewness, making symmetric, or even Gaussian data from, say, log-normal. So it changes everything! Median is even "worse" - it removes 99% of the data, as only the mid point, or the average of two mids points, is returned. 99% of the data are totally ignored. From the other side, the median is equal to the geometric mean in log-normal distribution, so maybe it is justified? Quantile regression is based on quantiles, including the median, so it share the same behavior. My question is how is using those methods different from just automatically removing everything that is beyond some threshold (like 3 times standard deviations, or certain quantile)? We "don't delete outliers", using robust methods, which removes much more, even 40% (trimmed) or 99% (median) of the data! Isn't this all cheating?
