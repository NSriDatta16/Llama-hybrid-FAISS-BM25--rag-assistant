[site]: crossvalidated
[post_id]: 575751
[parent_id]: 
[tags]: 
Does the attention mechanism (in CNNs) bring additional parameters/weights to learn to the network?

The idea of the attention mechanism is based on using some weighted sum of the output of some layers in deep networks. I see the process in forward propagation, and it seems that the attention mechanism does not bring additional weights to learn to the system. Do I get it right?
