[site]: crossvalidated
[post_id]: 94558
[parent_id]: 94109
[tags]: 
This should be easily solved using bayesian inference. You know the measurement properties of the individual points with respect to their true value and want to infer the population mean and SD that generated the true values. This is a hierarchical model. Rephrasing the problem (Bayes basics) Note that whereas orthodox statistics give you a single mean, in the bayesian framework you get a distribution of credible values of the mean. E.g. the observations (1, 2, 3) with SDs (2, 2, 3) could have been generated by the Maximum Likelihood Estimate of 2 but also by a mean of 2.1 or 1.8, though slightly less likely (given the data) than the MLE. So in addition to the SD, we also infer the mean . Another conceptual difference is that you have to define your knowledge state before making the observations. We call this priors . You might know in advance that a certain area was scanned and in a certain height range. The complete absence of knowledge would be to have uniform(-90, 90) degrees as the prior in X and Y and maybe uniform(0, 10000) meters on height (above the ocean, below the highest point on earth). You have to define priors distributions for all parameters that you want to estimate, i.e. get posterior distributions for. This is true for the standard deviation as well. So rephrasing your problem, I assume that you want to infer credible values for three means (X.mean, Y.mean, X.mean) and three standard deviations (X.sd, Y.sd, X.sd) which could have generated your data. The model Using standard BUGS syntax (use WinBUGS, OpenBUGS, JAGS, stan or other packages to run this), your model would look something like this: model { # Set priors on population parameters X.mean ~ dunif(-90, 90) Y.mean ~ dunif(-90, 90) Z.mean ~ dunif(0, 10000) X.sd ~ dunif(0, 10) # use something with better properties, i.e. Jeffreys prior. Y.sd ~ dunif(0, 10) Z.sd ~ dunif(0, 100) # Loop through data (or: set up plates) # assuming observed(x, sd(x), y, sd(y) z, sd(z)) = d[i, 1:6] for(i in 1:n.obs) { # The true value was generated from population parameters X[i] ~ dnorm(X.mean, X.sd^-2) #^-2 converts from SD to precision Y[i] ~ dnorm(Y.mean, Y.sd^-2) Z[i] ~ dnorm(Z.mean, Z.sd^-2) # The observation was generated from the true value and a known measurement error d[i, 1] ~ dnorm(X[i], d[i, 2]^-2) #^-2 converts from SD to precision d[i, 3] ~ dnorm(Y[i], d[i, 4]^-2) d[i, 5] ~ dnorm(Z[i], d[i, 6]^-2) } } Naturally, you monitor the .mean and .sd parameters and use their posteriors for inference. Simulation I simulated some data like this: # Simulate 500 data points x = rnorm(500, -10, 5) # mean -10, sd 5 y = rnorm(500, 20, 5) # mean 20, sd 4 z = rnorm(500, 2000, 10) # mean 2000, sd 10 d = cbind(x, 0.1, y, 0.1, z, 3) # added constant measurement errors of 0.1 deg, 0.1 deg and 3 meters n.obs = dim(d)[1] Then ran the model using JAGS for 2000 iterations after a burnin of 500 iterations. Here's the result for X.sd. The blue range indicates the 95% Highest Posterior Density or Credible interval (where you believe the parameter is after having observed the data. notice that an orthodox confidence interval does not give you this). The red vertical line is the MLE estimate of the raw data. It is usually the case that the most likely parameter in Bayesian estimation is also the most likely (maximum likelihood) parameter in orthodox stats. But you should not care too much about the top of the posterior. The mean or median is better if you want to boil it down to a single number. Notice that MLE/top is not at 5 because the data were randomly generated, not because of wrong stats. Limitiations This is a simple model which has several flaws currently. It doesn't handle the identity of -90 and 90 degrees. This can be done, however, by making some intermediate variable which shifts extreme values of estimated parameters into the (-90, 90) range. X, Y and Z are currently modeled as independent though they are probably correlated and this should be taken into account to get the most out of the data. It depends on whether the measurement device was moving (serial correlation and joint distribution of X, Y and Z will give you a lot of information) or standing still (independence is ok). I can expand the answer to approach this, if requested. I should mention that there's a lot of literature on spatial Bayesian models which I am not knowledgeable about.
