[site]: datascience
[post_id]: 55725
[parent_id]: 38745
[tags]: 
Try to raise the nlp.max_length parameter (as your error message suggests): nlp = spacy.load('en_core_web_sm') nlp.max_length = 1500000 #or any large value, as long as you don't run out of RAM also, when calling your spaCy pipeline, you can disable RAM-hungry intensive parts of the pipeline that are not needed for lemmatization: doc = nlp("The sentences we'd like to do lemmatization on", disable = ['ner', 'parser']) Finally, you should get the results you expect with the following: print([x.lemma_ for x in doc])
