[site]: crossvalidated
[post_id]: 228990
[parent_id]: 228946
[tags]: 
"Likelihood" and "probability" in their colloquial usage are synonyms, but in statistics they have different meanings. Let's start with defining our setting. We observed some independent and identically distributed datapoints $x_1,\dots,x_n$ and we have some model for this data that can be defined in terms of probability distribution $f$ parametrized by some unknown parameter $\theta$. Function $f_\theta(x_i)$ tells us about probability (or density ) of observing $x_i$ value given some parameter value $\theta$. Usually we calculate probabilities to check how probable is some $x$ given the distribution $f_\theta$ (e.g. what is the probability of obtaining $x$ points on some test knowing that the distribution of test scores is $f_\theta$). In case of likelihood function, we use it other way around and think about $f_\theta$ as of a loss function that returns higher values for $x$'s that better fit the distribution $f_\theta$ and lower values for $x$'s that are unlikely given this distribution. To gain some more intuition, look at the plot below. It shows three probability density functions A , B and C and datapoints marked as red rug plot below the main plot. As you can see, in case of distribution A only a single point falls into highest density region, so this does not seem to be a good model. More points fall into highest density regions of functions B and C , but since B catches majority of points in it's highest density region, we can conclude that it fits the data better. This is very informal way of showing how likelihood works. More formally, likelihood function as a function of data given some fixed parameter that calculates total fit of $f_\theta$ across all the points $$ \mathcal{L}(\theta|X) = P(X|\theta) = \prod_i f_\theta(x_i) $$ We call it likelihood since we do not care about actual probabilities and use the $f_\theta$ functions simply as loss functions. Second, in non-Bayesian setting $\theta$ is not a random variable, so we cannot compute $P(\theta|X)$, the direct answer for our question, so instead we use likelihood as a proxy. Moreover, if we become Bayesians for a while and assume that $\theta$ is a random variable that follows continuous uniform distribution from $-\infty$ to $\infty$ (suppose this is the range of values that $\theta$ can take), then Bayes theorem reduces to calculating likelihood $$ P(\theta|X) \propto P(X|\theta) P(\theta) = P(X|\theta) \times \text{const} \propto P(X|\theta)$$ and this is the same as maximizing the likelihood function over whole range of possible values of parameter $$ \underset{\theta\in\Theta}{\operatorname{arg\,max}} ~ \mathcal{L}(X; \theta) $$ so it does not really make that much difference. Check also: Maximum Likelihood Estimation (MLE) in layman terms What is the difference between "likelihood" and "probability"? Wikipedia entry on likelihood seems ambiguous
