[site]: datascience
[post_id]: 41391
[parent_id]: 41378
[tags]: 
TL;DR Time-series algorithms assume that data points are ordered . Traditional K-Fold cannot be used for time series because it doesn't take into account the order in which data points appear. One approach to validate time series algorithms is with Time Based Splitting . K-Fold vs Time Based Splitting The two graphs below show the difference between K-Fold and Time Based Splitting. From them, the following characteristics can be observed. K-Fold always the all data points . Time Base Splitting uses a fraction of all data points . K-Fold lets the test set be any data point . Time Base Splitting only allows the test set to have higher indexes than the training set . K-Fold will use the first data point for testing and the last data point for training . Time Base Splitting will never use the first data point for testing and never use the last data point for training . Scikit-learn implementation Scikit-learn has an implementation of this algorithm called TimeSeriesSplit . Look at their documentation, you find the following example: from sklearn.model_selection import TimeSeriesSplit X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]]) y = np.array([1, 2, 3, 4, 5, 6]) tscv = TimeSeriesSplit(n_splits=5) for train_index, test_index in tscv.split(X): print("TRAIN:", train_index, "TEST:", test_index) X_train, X_test = X[train_index], X[test_index] y_train, y_test = y[train_index], y[test_index] >> TRAIN: [0] TEST: [1] >> TRAIN: [0 1] TEST: [2] >> TRAIN: [0 1 2] TEST: [3] >> TRAIN: [0 1 2 3] TEST: [4] >> TRAIN: [0 1 2 3 4] TEST: [5]
