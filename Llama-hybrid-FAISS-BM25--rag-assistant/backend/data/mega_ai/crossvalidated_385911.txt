[site]: crossvalidated
[post_id]: 385911
[parent_id]: 
[tags]: 
how the long term memory works in an LTSM network

I am trying to understand how lstm networks work in particular the long term memory aspect. For example if we have a very long paragraph and at the start we have someone's name, Bob for example. Bob is not used again till near the end of the paragraph. Say we have a batch size of 10 words which includes the first use of Bob. So as I understand it (please correct me if I am wrong) the network will take this batch do forward propagation and then backward propagation to get the gradients which are used to minimise the error. So the model is updated with these weights. It will then keep what it needs and 'throw out' what is not required. Then another batch will be feed into the model. If however at the first stage it decides to throw out, Bob, how will it know later on that it was required? Guess what I am asking is if some input is thrown out by the model is it possible at a later step for it to be re-added?
