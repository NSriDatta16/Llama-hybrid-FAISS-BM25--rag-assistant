 adoption of autonomous vehicles have included concerns about the resulting loss of driving-related jobs in the road transport industry, and safety concerns. On March 18, 2018, a pedestrian was struck and killed in Tempe, Arizona by an Uber self-driving car. AI-generated content In the 2020s, automated content became more relevant due to technological advancements in AI models, such as ChatGPT, DALL-E, and Stable Diffusion. In most cases, AI-generated content such as imagery, literature, and music are produced through text prompts. These AI models are sometimes integrated into creative programs. AI-generated art may sample and conglomerate existing creative works, producing results that appear similar to human-made content. Low-quality AI-generated visual artwork is referred to as AI slop. Some artists use a tool called Nightshade that alters images to make them detrimental to the training of text-to-image models if scraped without permission, while still looking normal to humans. AI-generated images are a potential tool for scammers and those looking to gain followers on social media, either to impersonate a famous individual or group or to monetize their audience. The New York Times has sued OpenAI, alleging copyright infringement related to the training and outputs of its AI models. In 2024, Cambridge and Oxford researchers reported that 57% of the internet's text is either AI-generated or machine-translated using artificial intelligence. Eradication Scientists such as Stephen Hawking are confident that superhuman artificial intelligence is physically possible, stating "there is no physical law precluding particles from being organised in ways that perform even more advanced computations than the arrangements of particles in human brains". According to Nick Bostrom, a superintelligent machine would not necessarily be motivated by the same emotional desire to collect power that often drives human beings but might rather treat power as a means toward attaining its ultimate goals; taking over the world would both increase its access to resources and help to prevent other agents from stopping the machine's plans. As a simplified example, a paperclip maximizer designed solely to create as many paperclips as possible would want to take over the world so that it can use all of the world's resources to create as many paperclips as possible, and, additionally, prevent humans from shutting it down or using those resources on things other than paperclips. A 2023 Reuters/Ipsos survey showed that 61% of American adults feared AI could pose a threat to civilization. Philosopher Niels Wilde refutes the common thread that artificial intelligence inherently presents a looming threat to humanity, stating that these fears stem from perceived intelligence and lack of transparency in AI systems that more closely reflects the human aspects of it rather than those of a machine. AI alignment research studies how to design AI systems so that they follow intended objectives. Warnings Physicist Stephen Hawking, Microsoft founder Bill Gates, and SpaceX founder Elon Musk have expressed concerns about the possibility that AI could develop to the point that humans could not control it, with Hawking theorizing that this could "spell the end of the human race". Stephen Hawking said in 2014 that "Success in creating AI would be the biggest event in human history. Unfortunately, it might also be the last, unless we learn how to avoid the risks." Hawking believed that in the coming decades, AI could offer "incalculable benefits and risks" such as "technology outsmarting financial markets, out-inventing human researchers, out-manipulating human leaders, and developing weapons we cannot even understand." In January 2015, Nick Bostrom joined Stephen Hawking, Max Tegmark, Elon Musk, Lord Martin Rees, Jaan Tallinn, and numerous AI researchers in signing the Future of Life Institute's open letter speaking to the potential risks and benefits associated with artifici