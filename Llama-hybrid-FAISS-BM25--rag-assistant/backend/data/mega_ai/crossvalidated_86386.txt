[site]: crossvalidated
[post_id]: 86386
[parent_id]: 23395
[tags]: 
it seems that all neural network algorithms use some form of gradient descent in their training algorithms and even non ANN models use gradient descent. there seems to be no theory about how to apply gradient descent in a temporal fashion over SNNs. one possibility is a rise in neuromorphic computing which uses more biologically realistic models similiar to SNNs. but it seems like there are not strong machine-learning benchmarks/breakthroughs achieved in the neuromorphic field to date as with very definite benchmarks achieved with ANNs in many standard ML problems like handwriting recognition, speech recognition, object recognition, language translation, etc.
