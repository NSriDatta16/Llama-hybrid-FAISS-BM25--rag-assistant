[site]: crossvalidated
[post_id]: 618364
[parent_id]: 
[tags]: 
A question about conditional heteroscedasticity

Consider the linear model regression (matricial form): $$y= X\beta + \epsilon$$ I am reading the book Econometrics, by Fumio Hayashi. Considere the following: Assumption 1.4 states that the $n\times n$ matrix of conditional second moments $E[\epsilon \epsilon' | X]$ is spherical, that is, proportional to the identity matrix. Without the assumption, each element of the $n \times n$ matrix is in general a nonlinear function of $X$ . If the error is not (conditionally) homoskedastic, the values of the diagonal elements of $E[\epsilon \epsilon' | X]$ are not the same, and if there is correlation in the error term between observations (the case of serial correlation for time-series models), the values of the off-diagonal elements are not zero. I'm trying to find an example of a time series for the part of the quote mentioned above: if there is correlation in the error term between observations (the case of serial correlation for time-series models), the values of the off-diagonal elements are not zero. I even tried an AR(1): $$y_t = \beta x_t + \epsilon_t, \quad x_t = y_{t-1}, \quad \epsilon_t \sim IID(0,1) $$ By definition $cov(\epsilon_t, \epsilon_{t-1})=0$ . However, perhaps when we condition $X$ it is not null: $$ \begin{aligned} cov(\epsilon_t, \epsilon_{t-1}|X) &= cov(y_t - \beta y_{t-1} \, ,\, y_{t-1} - \beta y_{t-2}|X)\\ & =cov(y_t, y_{t-1}|X) - cov(y_t , \beta y_{t-2}|X) - cov(\beta y_{t-1}, y_{t-1}|X)+ cov(\beta y_{t-1}, \beta y_{t-2}|X)\\ & =cov(y_t, y_{t-1}|X) - \beta cov(y_t , y_{t-2}|X) - \beta cov( y_{t-1}, y_{t-1}|X)+ \beta^2 cov( y_{t-1}, y_{t-2}|X)\\ & =\gamma(1|X) - \beta \gamma(2|X) - \beta \gamma( 0|X)+ \beta^2 \gamma( 1|X) \end{aligned} $$ where $\gamma(h|X)$ is the conditional autocovariance function. The unconditional is given by: $$\gamma(h)=\beta^h \frac{1}{1-\beta^2} $$ I don't know if I'm on the right path or I have to think of some other example. Help!
