[site]: crossvalidated
[post_id]: 424981
[parent_id]: 
[tags]: 
Optimizing parameters for a classification model which predicts unseen future data

In my understanding, typical gridsearch (e.g. sklearn's GridSearchCV) evaluate a predefined parameter space and determines the optimal set of parameters within this space through iterating through the space and using cross validation to evaluate performance. When a classification model (let's say a random forest) is supposed to predict unseen future data (which cannot be reasonably assumed to be similar to the training set given the time lag), it seems like caution is required in optimizing the parameter space on training data. Any suggestions on striking the right balance? I suppose a solution would be to optimize not using cross validation, but with a validation set of future data, but this seems equally prone to overfitting on a specific time period.
