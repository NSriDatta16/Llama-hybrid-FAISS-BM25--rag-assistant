[site]: crossvalidated
[post_id]: 358071
[parent_id]: 358067
[tags]: 
Edit based on comments : Yes, the idea of plats is to use the output of another model but as an additional feature and hoping that the logistic regression will perform similar as original but, with smoother probabilities. I have tried this and others from http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html , mainly for Kaggle competitions and they don't seem to work that well on large and sparse datasets. I feel the reason has more to do with the features are not suitable for logistic regression and so the final model performs worse than original xgboost/SVM on test dataset. Something that has worked for me few times is to have the logistic regression trained on predictions of multiple classifiers (meta learners/stacking) but the effort has not really been worth the gain. Correct link for R - http://danielnee.com/2014/10/calibrating-classifier-probabilties/
