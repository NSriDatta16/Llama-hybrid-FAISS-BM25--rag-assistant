[site]: crossvalidated
[post_id]: 421622
[parent_id]: 421619
[tags]: 
"Multivariate regression" means that your response variable is a vector. "Multiple linear regression" is the term you mean, and using linear activation functions turns your neural network into a multiple linear regression. Your layers are linear transformations over and over, and the result of composing linear transformations is another linear transformation. The power of a neural network is in its ability to figure out all kinds of nonlinear decision boundaries. Using ReLU activation functions, you can get two neurons to give a decision boundary in the shape of $\vert x \vert$ . Using more neurons, you can approximate $x^2$ or $x^7 - 13x^4+x^2$ (or plenty else). Without the nonlinear activation function, you're only doing linear decision boundaries. What you're doing with a threshold for neurons firing at all confuses me and, from the comments, others. Could you please clarify that?
