[site]: crossvalidated
[post_id]: 461511
[parent_id]: 
[tags]: 
Adam converges while SGD does not improve at all

I am trying to build a model based movie recommendation system with a neural network. The architecture looks as follows: user_id_input = Input(shape=[1], name='user') movie_id_input = Input(shape=[1], name='movie') user_embedding = Embedding(output_dim=embedding, input_dim=num_users, input_length=1, name='user_embedding')(user_id_input) movie_embedding = Embedding(output_dim=embedding, input_dim=num_movies, input_length=1, name='movie_embedding')(movie_id_input) user_vecs = Reshape([embedding])(user_embedding) movie_vecs = Reshape([embedding])(movie_embedding) y = Dot(1, normalize=False)([user_vecs, movie_vecs]) model = Model(inputs=[user_id_input, movie_id_input], outputs=y) This model is taken from this blog post. In the blog post the Adam optimizer is used to optimize the loss which also works well for my data (fast convergence after 3 epochs). For an assignment I need to use the SGD optimizer because I want to use the momentum parameter as well. However, if I use SGD the network does not improve its accuracy any more. I did some research and I did not find any reason for a significant difference in the learning between those two optimizers. Are the two optimizers that different that one of them can learn of the data perfectly while the other one is not working at all?
