[site]: crossvalidated
[post_id]: 495247
[parent_id]: 495228
[tags]: 
Here are some ideas how you might proceed. Maybe others on this site have better ideas: Data. Suppose sample 1 has $n_1 = 4, \bar X_1 = 10.78, S_1 = 0.77$ and sample 2 has $n_2 = 6, \bar X_2 = 15.07, S_2 = 0.41.$ set.seed(115) x1 = rnorm(4, 10, 2); mean(x1); sd(x1) [1] 10.78053 [1] 0.7696925 x2 = rnorm(6, 15, 1); mean(x2); sd(x2) [1] 15.07297 [1] 0.4074099 stripchart(list(x1,x2), ylim=c(.5,2.5), pch="|") For equal variances. Then use $$\bar X_c = \frac{n_1\bar X_1 + n_2\bar X_2}{n_1+n_2} = \frac{n_1}{n_1+n_2}\bar X_1 + \frac{n_2}{n_1+n2}\bar X_2,$$ a sample-size weighted average of the two sample means. and, if you assume the two populations have the same variance, use $$S_c^2 = \frac{(n_1-1)S_1^2 + (n_2 - 1)S_2^2}{n_1 + n_2 - 2} = \frac{\nu_1}{\nu_1+\nu_2}S_1^2 + \frac{\nu_2}{\nu_1+\nu_2}S_1^2,$$ where degrees of freedom $\nu_i = n_i - 1,$ a DF-weighted average of the two sample variances. Use $S_c = \sqrt{S_c^2}.$ a.c = (4/10)*10.78 + (6/10)*15.07; a.c [1] 13.354 s.c = sqrt((3/8)*0.77^2 + (5/8)*0.41^2); s.c [1] 0.5721888 Finally, then a 95% CI for $\mu_c = (\mu_1 + \mu_2)/2$ uses the standard error $\mathrm{SE}_c = S_c\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}$ and probability factor $t^*,$ which cuts probability 0.025 from the upper tail of $\mathsf{T}(\nu_1+\nu_2) \equiv \mathsf{T}(n_1+n_2 - 2),$ so that the CI is $\bar X_c\pm t^*\mathrm{SE}_c.$ Notice that the standard error is the same as in a pooled two-sample t test. CI = a.c + qt(c(.025,.975), 8)*s.c*sqrt(1/4 + 1/6); CI [1] 12.50229 14.20571 Possibly unequal variances. If you do not assume population variances to be equal, then you can use standard error $\sqrt{{\frac{S_1^2}{n_1}}+\frac{S_2^2}{n_2}}.$ And the degrees of freedom for $t^*$ is $\nu^\prime,$ the same as for a Welch two-sample t test, where $\min(n_1-1, n_2-1) \le \nu^\prime \le n_1+n_2-2.$ CI = a.c + qt(c(.025, .975), 4) * sqrt(.77^2/4 + .41^2/6); CI [1] 12.18842 14.51958 More than two samples: For the case where population variances are equal, generalization to more than two samples is straightforward. Otherwise, the degrees of freedom could use the same Satterthwaite correction as in the one-factor ANOVA for possibly-unequal variances implemented in the R procedure oneway.test . If sample sizes are moderately large, one might settle for $\nu^\prime \approx \min(n_i - 1).$ Bootstrap CI: @Stochastic has suggested a bootstrap, which may have a chance of being useful for two somewhat larger sample---as for my fake data above. A simple quantile nonparametric bootstrap in R, shown below gives the 95% CI $(11.97, 14.66).$ set.seed(2020) a.re = replicate(10^6, mean(sample(c(x1,x2), 10, rep=T))) CI = quantile(a.re, c(.025,.975)) CI 2.5% 97.5% 11.97321 14.65932 summary(a.re) Min. 1st Qu. Median Mean 3rd Qu. Max. 10.24 12.90 13.38 13.36 13.84 15.53 hdr = "Bootstrap Dist'n of Weighted Means" hist(a.re, prob=T, br=50, col="skyblue2", main=hdr) abline(v=CI, col="red", lwd=2, lty="dotted")
