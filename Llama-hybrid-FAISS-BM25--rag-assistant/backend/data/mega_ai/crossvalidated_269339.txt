[site]: crossvalidated
[post_id]: 269339
[parent_id]: 
[tags]: 
Why does increasing number of observations in linear mixed model cause Bayesian modelling approach to fail?

I have a fairly good understanding of the theory behind Bayesian modeling and I have started to attempt some practical modeling using jags in R. I have been following examples online to get used to the syntax. However in the below example I simply increase the number of observations and it has an unexpected effect, inducing bias in the results. I am trying to figure out why. I have been using the resource here . In the third example, they attempt to create a linear mixed model: b_mass ~ b_length + r.e(site) . Body mass (of a snake) has a linear relationship with the continuous variable length, and a random intercept for the site at which the snake was found. Questions: 1. Why does the Bayesian model perform so poorly when the number of observations increases? 2. Is the ratio of observations to number of random effects important in a mixed model? It doesn't seem to be when fitting a standard linear mixed effects model under a frequentist framework. 3. What do trace plots with lots of outliers/long tails indicate? And what adjustments should be made for them? Details: SCENARIO (A): The code to generate the data and fit the model, taken directly from the website above is: ## First create the data set.seed(42) samplesize The output is: ## Output fit_lm3 mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff alpha 42.583 4.481 30.875 41.144 43.476 45.208 48.110 1.004 3600 beta 9.893 0.349 9.208 9.662 9.895 10.127 10.582 1.001 19000 sigma 4.755 0.248 4.300 4.581 4.745 4.915 5.272 1.001 22000 sigma_a 8.803 4.386 4.610 6.292 7.689 9.845 19.898 1.002 2400 The correct answers should be: alpha (average intercept) = 45. beta (coefficient for length)= 10. sigma (residual sd)= 5. sigma_a (sd of intercepts) = 10. The results are quite good, 42.583 is close to 45, 9.893 is close to 10, 4.755 is close to 5, and 8.803 is close to 10. SCENARIO (B): However I thought it would be nice to get ever more accurate results, so I increased the number of observations to 1000. samplesize The rest of the code remains the same, and the output is now: mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff alpha 33.587 8.982 11.181 28.906 35.602 40.078 45.933 1.001 6100 beta 9.839 0.164 9.519 9.727 9.838 9.950 10.165 1.001 22000 sigma 5.265 0.118 5.038 5.184 5.263 5.343 5.503 1.001 22000 sigma_a 19.635 9.749 8.960 12.965 16.852 23.377 45.792 1.001 5300 The coefficient for length and residual variation estimates are still good (beta and sigma), but the estimates for variation of intercepts, and the mean of the intercepts are now very poor. SCENARIO (C): I thought this might have to do with the ratio of #observations/#random intercepts, so I increased the number of sites to 50, and the model performs well again. However I'm not sure why this is the case. samplesize Output: mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff alpha 43.398 1.533 40.296 42.395 43.420 44.436 46.353 1.001 22000 beta 9.836 0.162 9.522 9.726 9.836 9.945 10.156 1.001 22000 sigma 5.013 0.114 4.794 4.936 5.011 5.089 5.241 1.001 22000 sigma_a 10.816 1.151 8.832 10.009 10.728 11.517 13.362 1.001 22000 I have attached trace plots below for scenario (A) . Clearly there are some issues as the densities for alpha and sigma_a have long tails, and many 'outlier' values. I would like to post more but my reputation is not high enough. The trace plots for scenario (B) look similar (poor) whereas for scenario (C) they look good. This does not fit in with the results. Finally, I fit a standard linear mixed effects model in all these scenarios, and it performed consistently well across all three. The following are trace plots for scenario A : The following are trace plots for scenario B : The following are trace plots for scenario C : The following are trace plots for when sample size = 200 and number of sites = 40. In response to Tommaso's comments below.
