[site]: crossvalidated
[post_id]: 587991
[parent_id]: 587809
[tags]: 
First some notation: Assume $N$ unique words, sampled with the same probability. We are doing $R$ (independent) rounds of simple random sampling without replacement, each round the sample size is $n \le N$ (generalization to unequal sample sizes should be straightforward). Define indicator random variables $$ X_{ij} =\begin{cases} 1 & \text{word $j$ sampled in round $i$} \\ 0 & \text{otherwise} \end{cases} $$ Note that we have \begin{align} \sum_j X_{ij} &= n ~(\text{for all $i$}) \\ \sum_i X_{ij} &\sim \mathcal{Binom}(R, n/N) \end{align} but all these binomial random variables are not independent, since they have a fixed, constant sum $nR$ . But if $N$ is large and $n/N$ small the dependence would be slight. Define $Y_j = \sum_i X_{ij}$ each having the binomial distribution defined above. The exact likelihood function for this problem will be intractable, so I will not write it out. But based on this binomials $Y_j$ we can construct a composite likelihood function, see the references at Parameter Estimation for intractable Likelihoods / Alternatives to approximate Bayesian computation . But before going into the details, we can also define pair statistics $$ Y_{jl} = \sum_{i=1}^R X_{ij} X_{il} \quad \text{for $j\not=l$} $$ which will also have binomial distributions: $\mathcal{Binom}(R,\frac{n}{N}\cdot\frac{n-1}{N-1})$ . The idea with composite likelihood (a specific form of pseudo-likelihood) is to construct individual likelihood functions from parts of the data , and then just multiply them together, as would be correct if they where independent ... even if they are not independent. See the linked page above for details. First, we find the composite likelihood only based on the $Y_j$ . First, suppose the total number of unique words sampled in the $R$ rounds are $M \le N$ . Then the composite likelihood for the one unknown parameter $N$ becomes $$ \prod_{j=1}^M \binom{R}{y_j}(n/N)^{y_j} (1-n/N)^{R-y_j} \cdot \left[ (1-n/N)^R \right]^{(N-M)} $$ the last factor coming from the $N-M$ unobserved words. (sorry, now it is late night here, so I will continue this answer tomorrow)
