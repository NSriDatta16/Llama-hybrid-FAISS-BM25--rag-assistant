[site]: crossvalidated
[post_id]: 338459
[parent_id]: 338452
[tags]: 
Frequentist statistics, which is what I'm sure is meant by "classical statistics" is not wrong . As a commenter on the Andrew Gelman blog once said (sorry, don't recall who), "Frequentist statistics isn't even in the category of things that can be wrong". Really, the issue with Frequentist statistics, and more specifically significance testing, is that it is confusing . In particular, it's very easy to misinterpret significance tests, which can then lead to backwards conclusions. Many statistical researchers, myself included, believe that the level of confusion about these methods has had very negative effects on other scientific areas of research that may misuse these methods. To help illustrate, most significance tests look at the evidence against the hypothesis "a certain effect size is 0", otherwise known as the null hypothesis, and then computes the probability of seeing data as extreme as we observed, under the assumption that the effect size is 0 . This is the p-value, and small p-values are considered evidence against the null hypothesis. If the p-value is below some pre-specified values $\alpha$, then we say we have enough evidence to reject the null hypothesis. Sound confusing? I warned you. Some (but definitely not all!) of the common mistakes that occur: Some researchers make the mistake of thinking failing to reject the null means we should accept the null. But the lack of evidence is not itself strong evidence: small sample sizes often lead to failing to reject the null, even though it is false. Multiple testing. Significance testing requires that all your methods for analyzing data be planned in advance of seeing the data . In reality, this is not what usually happens. The strength of evidence presented by a p-value is greatly distorted when you have looked at 30 different questions, then present the one question that looks most promising. Thinking that rejecting the null hypothesis is a big deal. Very often, researchers will over emphasize how important a finding is by saying it is statistically significantly different than 0. However, that by itself is often not very interesting: if I tell you that I have strong evidence that my after school study program improves grades, if you really care, you should follow up with "by how much (magnitude of effect)? If I'm already studious, will it still help (consistency of effect)?". Typical usage of significance testing does not address this, yet some research papers don't pursue much beyond "we found our treatment had a statistically significant improvement over control". In fact, if your goal is to find statistical significance in your data, the typical usage of these methods in many fields discourage asking these deeper questions! As for how to avoid making such mistakes, I'm not sure Bayesian statistics is an immediate cure-all. After all, Bayesian statistics has its own form of significance testing and Andrew Gelman, a firmly pro-Bayesian, anti-significance testing kinda fellow isn't a huge fan of Bayesian null hypothesis testing either. But at the very least, I think taking a course in Bayesian statistics is extremely helpful in helping you think properly about statistical methods, even if you always use "classical" methods. A full explanation of why I think a course in Bayesian statistics helps is a bit long for this answer, unfortunately. Most importantly, to protect yourself from making mistakes with classical methods, you should understand what significance testing does and does not tell you about your data. Most books that feature classical methods are very careful about properly explaining what significance testing really implies, but not all readers pick up on some of the very important subtle details about how to properly interpret p-values.
