[site]: crossvalidated
[post_id]: 360938
[parent_id]: 
[tags]: 
RL - Policy Proximal Optimization and clipping

As I've understood, PPO utilizes clipping to stabilize the training such that the updates are smooth. I'm not sure though that I understand how this clipping mechanism works. Essentially, we look to increase the likelihood of an action, $a_t$, if the advantage function, $A_t > 0$ and we clip the value of the ratio at $1+\epsilon$. If $A_t They describe this as being a lower bound for policy optimization. If what I've stated above is correct then I think that this algorithm is saying basically that we prioritize focusing on moving away from "bad" actions and then updates that move us closer to "good" actions take over as the policy improves. Because of this I don't really follow how this incentivizes the agent to have $r(t)$ between $1-\epsilon$ and $1+\epsilon$ in general since it's dependent on whether or not the action is good or not? Thanks
