[site]: crossvalidated
[post_id]: 81869
[parent_id]: 81719
[tags]: 
Unless you are confident with the numerical dependence of $Y$ on your various $X$, I would prefer your second approach (personal intuition though). So I am just trying to comment on your second approach. Normally logistic regression (sometimes probit models too, it depends ) is used in the classification problem. This assumes a binomial probability model for the outcome. Under this assumption, the logistic regression model is maximum likelihood that quantifies the relative changes on $X$ in the risk of the outcome $Y$ difference in the predictor. The loss function of logistic regression model is also related to the likelihood. So in your second approach, maybe you can try a one-vs-all strategy in which each class is distinguished from all the other 2 classes. Prediction is then performed using each binary classifier. The classifier with the highest confidence score is chosen (it's like a naive bayesian classifier). Since your training set is small compared with feature size, it is possible that logistic regression suffers from the overfitting issue. A cross validation method helps you confirm such a possibility. If that is the case, you may consider reduce the features, or use SVM with a Gaussian kernel (mapping your data to a space that is linear separable). EDIT Regarding the regression, the basic strategy is almost the same in my analysis on classification. Note that logistic regression can do both classification and regression . While SVM is mostly used in classification, neural network can give you a better performance on regression by just changing the cost function from the derivative of sigmoid function to a least square based quadratic function, while the back propagation procedure retains. You have only two independent variables, so overfit may not be a problem. You can try with a small amount of dataset with the cross validation strategy, and observe whether the validation error curve converges to the same expected error as training error curve does.
