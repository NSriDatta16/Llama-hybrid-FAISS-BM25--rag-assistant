[site]: datascience
[post_id]: 33981
[parent_id]: 32585
[tags]: 
Ultimately, the exact representation of your input will be dependent on the tool you are feeding it in. More generally, for text generation, you will want to model your input and output as sequences of tokens . These tokens can be words, sentences, characters, n-grams, whatever floats your boat. Each token should then be represented by a vector . That vector could be a one-hot encoding of the token, or as pcko1 suggests, a word embedding . Word embeddings are real vectors which represent your token. They can be used in the same way as one-hot vectors, but they have shown to carry more meaning. Ultimately, you will need to have some sort of vector_to_string(vector) and string_to_vector(string) functions, which will map a token to its corresponding vector, and vice versa. That way, you transform your input string into a sequence of vectors, and then, your output which will be a sequence of vectors can be turned back into a string. For text generation, it is useful to add a start_of_sentence and end_of_sentence tokens, to know when to stop generating.
