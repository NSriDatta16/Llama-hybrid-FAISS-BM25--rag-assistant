[site]: crossvalidated
[post_id]: 363907
[parent_id]: 358496
[tags]: 
No. An important feature for Markov or Hidden Markov model is the Markovian property. Where given the current, past and future are independent. In the case of the first order Markov model, it can be described as $$ P(X_i|X_1,\cdots, X_{i-1})= P(X_i|X_{i-1}) $$ On the other hand, for recurrent neural network, we can view the neural network is a very complicated function that is trying to estimate $$ P(X_i|X_1,\cdots, X_{i-1}) $$ using large amount of the data. In The Unreasonable Effectiveness of Recurrent Neural Networks , Fun with RNN part, we can see the model can generate a valid (but random) xml. Antichrist 865 15900676 2002-08-03T18:14:12Z Paris 23 Automated conversion #REDIRECT [[Christianity]] This would be extremely hard to use Markov or Hidden Markov model on character level. Since we need to remember things happens in far past, e.g., and '' tag are 300 + characters apart. If we are building a 300th order Markov model, the transition matrix will be $26^{300}$, even if we only assume there are 26 letters.
