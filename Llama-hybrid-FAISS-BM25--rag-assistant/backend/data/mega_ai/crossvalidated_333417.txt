[site]: crossvalidated
[post_id]: 333417
[parent_id]: 
[tags]: 
How to account for different baselines when analysing extremes in time series data

I want to see if Wikipedia articles for topics featured in popular documentaries get a boost in page views on the day the documentary is aired. I then want to see if time on screen is a predictor of this boost. I have to account for baseline popularity but I'm not sure how. For instance, the Wikipedia article for global warming will have a different average number of hits per day than the article for acid rain. My working solution has been to use the yearly average Wikipedia hits for each article as a baseline. I will have data on the number of hits for each article from the day the documentary aired. So I can calculated a percentage change from baseline to airing date ( doc_hits ). Article Wikipedia_baseline doc_hits %_change seconds_on_screen global_warming 300 450 50 60 acid_rain 100 260 30 160 plastics 250 600 75 140 But if I feed this into a GLM, I'm worried I'm being circular e.g. glm(%_change ~ seconds_on_screen + Wikipedia_baseline) Here the %_change has been generated using the Wikipedia_baseline but I obviously need to take the popularity of the articles into account.
