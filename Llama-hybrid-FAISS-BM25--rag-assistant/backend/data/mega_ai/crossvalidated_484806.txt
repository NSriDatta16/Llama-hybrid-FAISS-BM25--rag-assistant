[site]: crossvalidated
[post_id]: 484806
[parent_id]: 484798
[tags]: 
Both of the instances of the authors referring to known quantities are assumptions that are necessary for standard linear regression to maintain its standard properties. The positive weights are known. Typically a regression book would say that standard OLS assumes that the error is homoskedastic, that is that the idiosyncratic variance of each observation is the same for all observations. In the authors notation this would be: $$Var(y_i) = \sigma^2$$ , but I would write it as: $$Var(y_i|x_i) = \sigma^2$$ to stress the fact that the idiosyncratic error is condition on the data. It is the variation no due to the covariates. So this is the normal way that this is presented. Then typically a book will say that often this assumption is violated and that the idiosyncratic error may be more complicated, such as being heteroskedastic (each observation has it's own variance $Va(y_i|x_i) = \sigma_i^2$ ) or autocorrelation (the errors are correlated amoungst each other, common in time series). There are modifications to the model such as weighted least squares or feasible weighted least squares or modifications to the way we calculate the standard errors such as heteroskedasticity robust standard errors which can deal with this. In the book you are following, they point out that you can still more or less use ordinary least squares if there is heteroskedasticity of the form $Var(y_i|x_i) = \sigma_i^2 = \sigma^2/w_i$ and for some reason you know what the weights $w_i$ are for all $i$ . In practice most of the time you would not know this, but what it means in loose terms is you know which observations are noisier or less noisy than others and can quantify that in terms of the weight $w_i$ . The way that this would work is by running the regression of $\frac{y_i}{\sqrt(w_i)}$ on $\frac{x_{i,1}}{\sqrt(w_i)}, \frac{x_{i,2}}{\sqrt(w_i)}, \dots, \frac{x_{i,p}}{\sqrt(w_i)}$ and an intercept. If $w_i$ is large, you are effectively downweighting the influence of that observation because it is noisy. If $w_i$ is small you are upweighting it because it is giving you lots of information. Again, these are just assumptions and as I mentioned there are ways to weaken these assumptions if the analyst feels they are too strong. Where $E[\mu_i]$ is known. This is again an assumption. One way to think about linear regression is as specifying a model for the conditional expectation. Again, it is more common and my personal preference to express this as a conditional expectation: $E[\mu_i|x_i] = E[y_i|x_i] = \beta_0 + \sum_{i=1}^px_i\beta_i$ The idea is that in order to recover the true conditional expectation, it needs to be linear (in the coefficients) of the model. In practice, do we usually know that this is true. No not typically, it is an assumption. If you go to chapter 2.3 of the text you reference, they show examples where the assumptions are violated. It is usually easy to check that assumptions are violated when they are violated grossly, but we can never fully verify they are satisfied without some outside knowledge coming outside of the data. This is beyond the scope of this answer, but linear combination of variables have nice properties that still may justify them even when the assumption is not quite true. Sometimes we can think of a linear regression as a taylor expansion or local approximation to the true conditional expectation. By including things like higher order terms $x^2, x^3$ etc or other basis expansions (or things like splines) these approximations can become more accurate (in terms of ability to predict the outcome in or out of sample) or plausible. Such an approximative model will not necessarily be unbiased or enjoy some of the efficiency properties that OLS can have, but can still be quite useful. This is often how people think of linear regression in practice anyway, particularly in industry. The answer to how do we know is extremely case by case. What do you know about the variables. As an agronomist you may sometimes be able to look to other studies or theories about how crops behave to partially justify the assumptions you make in a particular model. The art of statistics is about matching plausible assumptions about the real world that produces the data with models. Understanding the assumptions of a model, how they can or cannot be weakened or strengthened, and when they plausibly hold is the entire battle of an applied statistician or data scientist.
