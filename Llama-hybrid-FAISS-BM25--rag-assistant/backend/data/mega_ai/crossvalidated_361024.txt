[site]: crossvalidated
[post_id]: 361024
[parent_id]: 
[tags]: 
ICA for noise reduction of covariance matrix

Trying to understand ICA in the context of noise reduction of covariance matrices (of dimensionality M). I understand in PCA, you can reconstruct the covariance matrix by squaring the first N principal component loadings (where N loadings = pca.components_.T * np.sqrt(pca.explained_variance_) low_rank_covariance = numpy.dot(loadings, loadings.T) Then you can add back in a diagonal matrix D which contains the unexplained/missing variance. I'm not super familiar with ICA, but is there an analog to this procedure for ICA? I understand that there no longer are the explained variance (eigenvalues) because ICA is maximizing the kurtosis rather than the variance. However you still get a basis with ICA. Can you infer the loadings somehow? Thanks
