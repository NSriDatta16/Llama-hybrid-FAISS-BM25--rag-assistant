[site]: crossvalidated
[post_id]: 80759
[parent_id]: 80751
[tags]: 
Actually, an earlier paper (predating the "random forest" terminology) from Leo Breiman considered all features as you suggest [Breiman 1996] . Below are some relevant excerpts from this article. First, some notation: A learning set of $\mathcal{L}$ consists of data $\left\{ (y_n,x_n),n=1,...,N\right\}$ where the $y$ 's are either class labels or a numerical response. Assume we have a procedure for using this learning set to form a predictor $\varphi(\textbf{x},\mathcal{L})$ -- if the input is $\textbf{x}$ we predict $y$ by $\varphi(\textbf{x},\mathcal{L})$ The idea behind bagging (and, by extension, random forests) is to stochastically construct many different predictors $\varphi$ and aggregate this population predictors into a single predictor, denoted $\varphi_B$ , which generally has superior performance to any of the individual predictors. Stochasticity is added by constructing each predictor on a bootstrap sample of the original training set, rather than the entirety of $\mathcal{L}$ . As Gavin Simpson mentioned, selecting a random subset of features adds even more stochasticity to the construction of individual predictors. Breiman (1996) writes: A critical factor in whether bagging will improve accuracy is the stability of the procedure for constructing $\varphi$ . If changes in $\mathcal{L}$ , i.e. a [bootstrap] replicate of $\mathcal{L}$ , produces small changes in $\varphi$ , then $\varphi_B$ will be close to $\varphi$ . Improvement will occur for unstable procedures where a small change in $\mathcal{L}$ can result in large changes in $\varphi$ . . . The evidence, both experimental and theoretical, is that bagging can push a good but unstable procedures a significant step towards optimality. On the other hand, it can slightly degrade the performance of stable procedures. [Emphasis Added] The paper then goes on to provide said theoretical and experimental evidence. Breiman's seminal description of random forests (Breiman, 1999) credits Tim Kam Ho for developing "the random subspace method" which adds additional instability by selecting $m$ random features to grow each tree (rather than each node). The original publication (Ho, 1998) appears to be behind a paywall unfortunately. One final note -- one interesting question is what to choose for $m$ ? That is, how many random features should we select to split on for each node? There is a section in Breiman (1999) devoted to this question. The basic conclusion is that the choice does not matter substantially, as long as $m$ is relatively small compared to the total number of features.
