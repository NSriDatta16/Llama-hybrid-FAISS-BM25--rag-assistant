[site]: crossvalidated
[post_id]: 618895
[parent_id]: 614980
[tags]: 
Take a look at: Why do large LMs use the transpose of the word embeddings matrix in the classification head? and https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf From the paper above, you can see that they generate the probability of the token by applying softmax to the multiplication of output embedding by the transpose of the embedding matrix h0 = UWe + Wp hl = transformer_block(hl−1)∀i ∈ [1, n] (2) P (u) = softmax(hn WeT ) After that you can use cross-entropy loss between P(u) and your label vector for training. In word2vec the modeling the P(u) has a summation on the denominator (over the vocabulary), most of the approximation techniques are to avoid doing the full sum
