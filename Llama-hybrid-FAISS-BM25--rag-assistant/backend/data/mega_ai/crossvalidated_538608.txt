[site]: crossvalidated
[post_id]: 538608
[parent_id]: 538595
[tags]: 
Facebook's revenue in 2020 was 86 billion dollars, mostly by selling ads. So if your large scale logistic regression model can improve the click-through rate by 0.01%, you would make facebook 8.6 million dollars per year (86000000000*0.0001), which is enough money to justify spending a couple of millions on some grad students or engineers to work on that. I don't know if there is a use case for fitting 250 PARAMETERS on billions of samples. But fitting tens of thousands of parameters is quite common. A paper might showcase a method on the 250 parameter toy problem, but my guess would be the real motivation is to fit bigger models. Also, these papers usually don't just fit your vanilla logistic regression, but some kind of regularized generalized linear model and a lot of things can be expressed as regularized logistic regression or GLM. Even without being Facebook, you can find a problem that could be expressed as a model with billions of samples and hundreds of thousands of parameters. A lot of things can explode really quickly if you allow them to. For example, on a map, each point in a grid is a separate observation that, of course, needs to be modeled to take spatial relationships into account. Nevertheless, all of a sudden, you might have millions of "data points." You might have a time series, where of course, each measurement is a separate data point, and if you measure every millisecond, or if you measure for a long time, all of a sudden, you have millions or billions of data points. If you measure how something spatial changes across time, possibly with respect to some variables, e.g., fMRI signal in a brain, EEG signal on a scalp, smog in a city, global temperature, algae on a lake... you can easily get a problem of a scale you were talking about. These problems might not always be looked at as large-scale logistic regression problems, but if there was a technology to efficiently fit those models, use cases would come by themselves. tl;dr: time series, maps, and images. Could you just downsample it? Yes, but you wouldn't need to.
