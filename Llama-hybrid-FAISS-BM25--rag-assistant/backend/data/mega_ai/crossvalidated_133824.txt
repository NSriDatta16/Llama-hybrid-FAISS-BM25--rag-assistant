[site]: crossvalidated
[post_id]: 133824
[parent_id]: 133778
[tags]: 
The one-class SVM was designed for just this situation. Basically, instead of trying to separate your two classes from each other with a hyperplane, it tries to find the hyperplane that separates your data set from the origin, while being as far away from the origin as possible. In kernel space, this essentially corresponds to finding the smallest possible blob that contains your data (without overfitting). It's pretty cool. That said, if you have enough of these outliers, you should also try just training standard classifiers on the problem (e.g. random forests or gradient boosting machines). The less-parametric classifiers can sometimes do quite well even when one class doesn't have that much in common--it really depends on the dataset.
