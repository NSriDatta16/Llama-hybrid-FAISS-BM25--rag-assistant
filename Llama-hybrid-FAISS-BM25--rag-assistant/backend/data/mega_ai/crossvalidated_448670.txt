[site]: crossvalidated
[post_id]: 448670
[parent_id]: 
[tags]: 
Are the conditions of metric space satisfied in the latent space of a classification task?

Specifically, in the case of a neural network trained in a categorical classification task (cross-entropy loss function), does the final layer embedding space preserve the definition of distance between samples? Are distances between samples in the latent space interpretable under the conditions of positivity, symmetry, and the triangle inequality? Or does the probabilistic nature of the loss function degrade metric properties in the embedding layers?
