[site]: datascience
[post_id]: 37571
[parent_id]: 16693
[tags]: 
It is not necessary that the more important a feature is then the higher its node is at the decision tree. This is simply because different criteria (e.g. Gini Impurity, Entropy-Information Gain, MSE etc) may be used at each of two these cases (splitting vs importance). For example, at SkLearn you may choose to do the splitting of the nodes at the decision tree according to the Entropy-Information Gain criterion (see criterion & 'entropy' at SkLearn ) while the importance of the features is given by Gini Importance which is the mean decrease of the Gini Impurity for a given variable across all the trees of the random forest (see feature_importances_ at SkLearn and here ). If I am right, at SkLearn the same applies even if you choose to do the splitting of the nodes at the decision tree according to the Gini Impurity criterion while the importance of the features is given by Gini Importance because Gini Impurity and Gini Importance are not identical (see also this and this on Stackoverflow about Gini Importance).
