[site]: crossvalidated
[post_id]: 43325
[parent_id]: 43312
[tags]: 
This is more a set of rambling comments and thoughts than an actual answer, but it's a bit long for a comment. It might end up as something approximating an answer of at least some slight value eventually, though, if it is edited enough times by me or other people. Generally, the Neyman-Pearson Lemma doesn't apply except in special cases, so the base answer is 'well, no'. However, I am going to go on a ramble that basically says 'Clearly in general you won't have an asymptotic chi-square distribution , but maybe there's something to looking at the ratio of likelihoods'. To begin - I think the Likelihood Principle should apply, which might at least give us some hope that the ratio of likelihoods could be informative about the problem. I've seen it done (use a ratio of likelihoods to derive a statistic) for goodness of fit tests (testing a specific null against a specific alternative) - but just to get a form of statistic rather than its distribution. In a number of particular cases (e.g. some specific symmetric nulls vs specific symmetric alternatives), the likelihood ratio does often seem to lead to a very sensible test statistic, one that has excellent power properties. Alternatively efficient scores have been used as a way to get to test statistics that sometimes end up looking like (a monotonic function of) a likelihood ratio, again, suggesting that likelihood ratios may be informative in testing one distribution against another. Then again, I've also seen people claim that you can't do that kind of thing at all ; that the ratio of likelihoods isn't meaningful. From a Bayesian point of view such comparisons seem to present no immediately obvious problem (unless I missed something, which certainly happens), as long as everything is appropriately normalized. With equal prior probability, we boil down to looking at the Bayes factor. If instead of the Bayes factor integral, the likelihood corresponding to the maximum likelihood estimate of the parameter for each model is used, then we get back to a likelihood ratio, as mentioned here . Alternatively, we might look at approximating the integrals using Laplace's method -- at least in some circumstances the likelihood ratio can come up as a term in it, though there's another factor there; such things are at least suggestive that the likelihood ratio is the appropriate way to make use of the likelihood principle, even if we don't have a distribution for the ratio. For Gaussian vs gamma, you can parameterize that (note that they're both special cases of the Tweedie distribution), so that may make a difference even if the general case isn't okay, though in the Tweedie family the Gaussian is a rather special case, since there's a "boundary" there (as there is for the Poisson), so again, it's not a standard situation - indeed, there's a 'gap' in the parameter space between the Gaussian and the gamma.
