[site]: datascience
[post_id]: 66828
[parent_id]: 66825
[tags]: 
The (maximum) number of features is a hyperparameter, i.e. it depends on what you set it to or what the default of the implementation you are using is. In "Introduction to Machine Learning with Python" by Mueller and Guido the authors recommend the following: As described earlier, max_features determines how random each tree is, and a smaller max_features reduces overfitting. In general, it’s a good rule of thumb to use the default values: max_features=sqrt(n_features) for classification and max_features=log2(n_features) for regression. Which is, for example, in line with the default in scikit learn. This what the parameter looks like for classification in scikit learn: max_features: int, float, string or None, optional (default=”auto”) The number of features to consider when looking for the best split: If int, then consider max_features features at each split. If float, then max_features is a fraction and int(max_features * n_features) features are considered at each split. If “auto”, then max_features=sqrt(n_features). If “sqrt”, then max_features=sqrt(n_features) (same as “auto”). If “log2”, then max_features=log2(n_features). If None, then max_features=n_features. Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max_features features. Source: scikit learn documentation
