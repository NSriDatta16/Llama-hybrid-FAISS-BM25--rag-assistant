[site]: crossvalidated
[post_id]: 371057
[parent_id]: 
[tags]: 
What do the results of tuning β1, β2 in ADAM imply about the gradients or the data?

For example: If I have a simple 3-layer neural network that demonstrates better performance on the test set when the value of β2 is .95 when compared to the default .999 over several trials of cross-validation, is there any assumption that we can make about properties of the data or gradients? Is there any literature on this matter?
