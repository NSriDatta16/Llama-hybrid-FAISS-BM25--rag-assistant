[site]: crossvalidated
[post_id]: 211012
[parent_id]: 
[tags]: 
In Ordinary Least Squares (OLS), one typically decomposes a model's $R^2$. One common method is to add regressors to the model one by one and record the increase in $R^2$ as each regressor is added. Since this value depends on the regressors already in the model, one needs to do this for every possible order in which regressors can enter the model, and then average over orders. This is feasible for small models but becomes computationally prohibitive for large models, since the number of possible orders is $p!$ for $p$ predictors. Gr√∂mping (2007, The American Statistician ) gives an overview and pointers to literature in the context of assessing variable importance. For other models, e.g., logistic regression or zero-inflated regression , one can use a similar approach once one has decided on an appropriate analogue of $R^2$, e.g. one of various pseudo-$R^2$s .
