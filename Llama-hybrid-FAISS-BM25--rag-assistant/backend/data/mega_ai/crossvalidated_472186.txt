[site]: crossvalidated
[post_id]: 472186
[parent_id]: 
[tags]: 
What type of functions can/cannot be handled by backprop?

I have a very basic question about backprop, which is what type of function it can and cannot calculate the gradient of, and whether if anyone have examples of such functions. I interpret backprop as basically the "black-box" algorithm that modern machine learning framework uses in order to compute the partial gradients with respect to learnable parameters in a system. Obviously to calculate backprop, you have to be able to take the partial derivative of its variables, which means that the variables have to come from a continuous space. Ok, so "continuously differentiable functions over continuous (say, convex) spaces". Hence any network that is composed of add, multiply and continuous activation functions can be handled by backprop. But it seems that the backprop algos implemented by many frameworks does more. For example, a network containing Relu is not differentiable in the ordinary sense. It is subdifferentiable. So our class of functions that can be handled by backprop extends to "subdifferentiable functions over continuous spaces", or maybe "Lipschitz continuous functions over continuous spaces". Is this the largest class of function we can use the backprop algo on? What about discontinuous functions? What are the limits of backpropagation?
