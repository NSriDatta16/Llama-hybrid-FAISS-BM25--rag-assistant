[site]: crossvalidated
[post_id]: 321012
[parent_id]: 320995
[tags]: 
We may be interested in means and standard deviations of some function of $\theta$,. When comparing two or more populations we may be interested in the posterior distributionof $|\theta_{1} âˆ’\theta_{2}|, \theta_{1}/\theta_{1}$, or max$({\theta_{1}, . . . , \theta_{m}})$, all of which are functions of more than one parameter. Obtaining exact values for these posterior quantities can be difficult or impossible, but if we can generate random sample values of the parameters from their posterior distributions, then all of these posterior quantities of interest can be approximated to an arbitrary degree of precision using the Monte Carlo method. Let $\theta$ be a parameter of interest and let $y_{1}, . . . ,y_{n}$ be the numerical values of a sample from a distribution $p(y_{1}, . . . ,y_{n})$. Suppose we could sample some number $S$ of independent, random $\theta$-values from the posterior distribution $p(\theta|y_{1}, . . . ,y_{n})$: $$\theta^{(1)}, . . . , \theta^{(s)} \sim p(\theta|y_{1}, . . . ,y_{n})$$. Then the empirical distribution of the samples $\theta^{(1)}, . . . , \theta^{(s)}$ would approximate $p(\theta|y_{1}, . . . ,y_{n})$, with the approximation improving with increasing $S$.The empirical distribution of $\theta^{(1)}, . . . , \theta^{(s)} $is known as a Monte Carlo approximation to $p(\theta|y_{1}, . . . ,y_{n})$. Additionally, let $g(\theta)$ be (just about) any function of $\theta$. The law of large numbers says that if $\theta^{(1)}, . . . , \theta^{(s)} $ are i.i.d. samples from $p(\theta|y_{1}, . . . ,y_{n})$, then as $S\rightarrow \infty$ $$\frac{1}{S}\sum_{s=11}^{S}g(\theta) ^{(s)}\rightarrow E(g(\theta)|y_{1}, . . . ,y_{n}) $$ Example : Suppose we are interested in the posterior distribution of some computable function $g(\theta)$ of $\theta$. In the binomial model, for example, we are sometimes interested in the log odds : $$log odds (\theta) = log \frac{\theta}{1-\theta}=\gamma$$ this quantity of interest can be computed using a Monte Carlo approach: sample $\theta^{(1)} \sim p(\theta|y_{1}, . . . ,y_{n})$, compute $\gamma^{(1)} = g(\theta^{(1)})$ sample $\theta^{(2)} \sim p(\theta|y_{1}, . . . ,y_{n})$, compute $\gamma^{(2)} = g(\theta^{(2)})$ . . . sample $\theta^{(s)} \sim p(\theta|y_{1}, . . . ,y_{n})$, compute $\gamma^{(s)} = g(\theta^{(s)})$ Then the sequence $\{\gamma^{(s)}, . . . , \gamma^{(s)}\}$ constitutes $S$ independent samples from $p(\gamma|y_{1}, . . . ,y_{n})$. Let $\theta$ be a population proportion . Using a binomial sampling model and a uniform prior distribution the posterior distribution of $\theta$ will be beta distributed say with beta(45, 420). Using the Monte Carlo algorithm described above, we can obtain samples of the log-odds from the posterior distribution of $\theta$ . In R, the Monte Carlo algorithm involves only a few commands: set.seed(1) a For further reading I recommend A First Course in Bayesian Statistical Methods and Doing Bayesian Data Analysis: A Tutorial Introduction with R and BUGS
