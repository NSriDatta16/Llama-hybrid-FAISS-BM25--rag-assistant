[site]: datascience
[post_id]: 115773
[parent_id]: 115758
[tags]: 
In essence, it is said that "there is no data as more data". So the training process should be done on as much data as you have, this also involves hyper-parameter tuning. Now one thing you can do to reduce the size of the data, is to drop duplicate or 'similar' rows of data. You have to define the criteria for 'similarity' your self, check for distant metrics as a start. This point goes in the direction of manually crafting your dataset. Second, try to drop correlated columns (you can use simple Pearson correlation for that and take any cut-off threshold that suits you). This is btw the most effective method. There is a huge set of analytical tool-box available for timeseries such as ARIMA, GARCH, ARCH, etc. There you will also find methods on comparing time series as well. I would suggest to have look in there to help filter out unwanted/redundant timeseries. By unwanted I mean that there might be some timeseries simple enough to be represented by MA or AR processes so no need to fit the entire prophet model on them. There is also a nice blog on predicting stock prices using FBprophet on empirischtech.at website, have a look in there for more insights. Plus, DeepAR from SageMaker is already capable to processing multiple timeseries data but I believe it can only be used with AWS account.
