[site]: crossvalidated
[post_id]: 244612
[parent_id]: 244567
[tags]: 
There's a mathematical theorem called the "law of large numbers." Imagine that you want to determine the probability that a coin will come up heads. The "population" of coin flips is infinity - much larger than the 300,000,000+ people in the United States. But according to the Law of Large Numbers, the more coin flips you do, the more accurate your estimate will be. The ideal poll: In the ideal poll, the pollsters would randomly choose names from the U.S. Census, they would find out where those people live, then they would go and knock on their door. If the person says they're planning on voting, the pollster asks who they're voting for and records their answer. Polling like this is mathematically guaranteed to work and the amount of error in your measurement for any given confidence level can be calculated easily . Here's what the error means: Suppose that based on your poll, you got that there's a 52 percent chance Candidate Awesome McPerfect is going to win, with a 3% error with 98% confidence. That means that you can be 98% confident that the true portion of voters who favor candidate Awesome McPerfect is between 49% and 55%. A Note on Error and Confidence For a given sample size, the more confident you are, the bigger your error will be. Think about it - you're 100% confident that the true proportion that support canditate Awesome is between 0% and 100% (most error possible), and you're 0% confident that the true proportion that supports canditate Awesome is exactly 52.0932840985028390984308% (zero error). More confidence means more error, less confidence means less error. However, the relationship between confidence and error is NOT linear! (See: https://en.wikipedia.org/wiki/Confidence_interval ) Polls in the real world: Because it's expensive to helicopter pollsters out to all parts of the country to knock on the doors of random people (although I'd love to see that happen; if you're a billionare and you see this, please consider funding this), polls in the real world are more complex. Lets look at one of the more common strategies - calling up random voters and asking them who they'd vote for. It's a good strategy, but it does have some well-aknowledged failings: People often choose not to answer the phone and respond to pollsters (ex. me) Some demographics are more likely to have a landline (ex. older voters) Some demographics are more likely to respond to pollsters (ex. older voters) Because different demographics vote in different ways, pollsters have to do their best to control for the differences in their raw data (based on who decided to answer the phone) and the outcomes of actual elections. For example, if 10% of people who picked up the phone were hispanic, but 30% of voters in the last election were hispanic, then they're going to give three times the weight to hispanic voters in their poll. If 50% of people who answered the phone were older than 60, but only 30% of the people who voted in the last election were older than 60, they're going to give less weight to the older voters who responded. It's not perfect, but it can lead to some impressive feats of prediction (Nate Silver correctly predicted the results in each of the 50 states in the 2012 election using statistics, and he was correct in 49 out of the 50 states in the 2008 prediction). A word of caution to the wise: Pollsters make the best predictions they can based on how things worked out in the past. Generally speaking , things work out about the same now as they did in the past, or at least the change is slow enough that the recent past (which they focus most on) will resemble the present. However, occasionally there are rapid shifts in the electorate and things go wrong. Maybe Trump voters are slightly less likely than your average voter to answer the phone, and weighting by demographics doesn't account for that. Or Maybe young people (who overwhelmingly support Hillary) are even more unlikely to answer the phone than the models predict, and the ones that do answer the phone are more likely to be republican. Or perhaps the opposite of both is true - we don't know. things like that are hidden variables that don't show up in commonly collected demographics. We would know if we sent pollsters to knock on random doors (ahem, imaginary billionare reading this), since then we wouldn't have to weight things based on demographics, but until then, fingers crossed.
