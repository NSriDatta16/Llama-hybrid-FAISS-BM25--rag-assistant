[site]: crossvalidated
[post_id]: 71546
[parent_id]: 71542
[tags]: 
The problem is that your classes are so unbalanced that the classifier relies on the prior probabilities of each class instead of learning a mapping from features to class labels. This problem can occur with many different kinds of models, including, as you discovered, decision trees. Some decision tree construction algorithms "prune" their trees to make them smaller and less prone to over-fitting. This can be done with information-theoretic criteria (e.g., Information Gain , as in ID3/C4.5/J48), but some programs also have more ad-hoc criteria (e.g., at least $n$ cases per node). You might be able to turn off or reduce the pruning step. Some algorithms allow you to provide a "loss function" that describes the cost of misclassifying someone who stays as someone who churns (or vice versa). If you set the weights appropriately, you can force the classifier to pay more attention to churners. You could start by adjusting each example's weights so that the total weights for each class are equal: each minority-class example gets a weight of 1.0, while the majority class examples get a weight of $\frac{\#_{\textrm{minority}}}{\#_{\textrm{majority}}}$. Your business needs may also suggest weights if, for example, you'd rather misidentify customers as churners (and try to retain them) than misclassify them as non-churners (and watch them walk away). A final quick-and-dirty approach might be to select equal numbers of positive and negative examples from your data set (i.e., use all 1700 churn examples and randomly select another 1700 no-churn examples). This might introduce a lot of sampling variability (e.g., if you chose a different 1700 no-churn examples), but it might give you a rough idea of how things look.
