[site]: crossvalidated
[post_id]: 387166
[parent_id]: 387150
[tags]: 
layer dense method in keras library uses linear activation by default, which means between the hidden and output layers, you don't accommodate nonlinear transformations. If you stack layers without nonlinearity in-between them, your neural network will be equivalent to a one-layer neural network. Please check here for a simple explanation if you're interested. So, your neural network is actually one-layer, and that is the output neuron, which means we're trying to solve $wx+b=y$ . You have the following least squares problem, $Xa=y:$ $$\begin{bmatrix}-1 & 1\\0 & 1 \\ 1 & 1\end{bmatrix}\begin{bmatrix}w \\ b\end{bmatrix}=\begin{bmatrix}-0.2445248 \\ 0.1232554 \\ 0.1713998\end{bmatrix}$$ The solution to this linear system is $\hat{a}=(X^TX)^{-1}X^Ty$ . Mean squared error is $\text{MSE}=\frac{1}{3}||y-\hat{y}||^2$ , where $\hat{y}=X\hat{a}=X(X^TX)^{-1}X^Ty$ . When you calculate this, you'll obtain $0.0057$ , which is actually your global optimum . Now, put an activation function in the first layer, and watch the system converge to a better optimum.
