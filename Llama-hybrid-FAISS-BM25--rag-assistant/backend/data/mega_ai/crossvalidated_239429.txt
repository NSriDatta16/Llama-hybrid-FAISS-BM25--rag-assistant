[site]: crossvalidated
[post_id]: 239429
[parent_id]: 
[tags]: 
Does $L_1$ regularization help ameliorate the credit assignment problem in (deep) neural nets?

Caveat: this is just a thought that occurred to my while I was driving to work, so maybe it's not well-considered. One challenge to deep neural networks is the credit assignment problem: a node at one layer might have many, many inputs from previous iterations of the network, so disentangling how to adjust parameters during training can be very challenging. There are other approaches to solving this problem, such as parameter sharing in CNNs and dropout/drop connect (which zero out some connections or nodes during training to approximate the separate effects of different paths through the network). But $L_1$ would seem to help to ameliorate this also, since it will zero out some weights entirely, and attribute the outcome to solely the nonzero paths. Clearly $L_1$ regularization alone won't solve problems like co-adaptation (where two or more neurons work to negate each other), but does it at least help address the credit assignment problem? Has there been any research in this area? I want to emphasize that I'm not asking about whether or not practitioners use $L_1$ regularization in general. I'm specifically asking whether $L_1$ can help ameliorate the credit assignment problem.
