[site]: crossvalidated
[post_id]: 470909
[parent_id]: 
[tags]: 
Gradient boosting and feature to target correlations?

I'm working on a data science challenge, I have a tabular data with around 80 features (79 predictors and a target variable between 0 and 1). I've tried two approaches: Using autoencoders to do reduce the number of features to 20 and reduce noise too .... (1) Using manual feature selection by inspecting feature importances and domain knowledge....(2) For each approach, I'm using a gradient boosting machine, "lightGBM", I got an RMSE of 0.093 using the first approach and an RMSE of 0.095 (between 0.095-0.10) using the 2nd approach. At first, I thought that the autoencoder helped me achieve a better score by reducing the noise in the data (which is quite noisy). Then I checked for the correlations between these new features created by the autoencoder and the target y and I was shocked that the highest correlated feature was only around 0.3 which is quite low compared to the 2nd approach where I had features highly correlated with y (0.7 and 0.6). This seems counterintuitive, I checked Spearman and Pearson correlations for the autoencoded feature and they are both quite low. Is this normal? Or is correlation not informative about the performance of gradient boosting machines? I need to understand why to choose an approach and fine-tune it further.
