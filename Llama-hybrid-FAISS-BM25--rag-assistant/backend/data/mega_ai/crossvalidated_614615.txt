[site]: crossvalidated
[post_id]: 614615
[parent_id]: 
[tags]: 
Differential Entropy of Zero-Mean Gaussian Mixtures

Introduction Consider a univariate circularly symmetric complex Gaussian (CSCG) mixture $Y$ with pdf $$p_Y(y) = \sum_i c_i p_i(y) = \sum_i c_i \frac{\exp(-\lvert y \rvert^2/\sigma_i^2)}{\pi \sigma_i^2},$$ where $c_i$ and $\sigma_i^2$ are the weight and variance of component $i$ . Its differential entropy can be decomposed as \begin{align} h(Y) & = h(Y|C) + I(C;Y) \\ & = \sum_i c_i \log(\pi e \sigma_i^2) - \int \sum_i c_i p_i(y) \log \frac{\sum_j c_j p_j(y)}{p_i(y)} dy. \end{align} The first term is trivial but the second term involves non-elementary integral. Some research There are many bounds/approximations for differential entropy of general mixture distributions, for example: Lower bound by Jensen's inequality [1] $$I_{\text{Jensen}}(C;Y) = -\sum_i c_i \log \sum_j c_j \frac{\sigma_i^2}{\pi (\sigma_i^2 + \sigma_j^2)} - 1$$ Approximation by 1st order Taylor's expansion at $y=0$ [2] $$I_{\text{Taylor1}}(C;Y) \approx - \log \sum_i c_i \frac{1}{\pi \sigma_i^2} - \pi$$ Estimation over arbitrary distribution-distance function $D(p_i || p_j)$ [3] $$I(C; Y) \approx - \sum_i c_i \log \sum_j c_j \exp(-D(p_i || p_j)) \tag{1} \label{1}$$ Specifically, Bhattacharyya distance $- \log \int \sqrt{p(x) q(x)} dx$ gives an lower bound $$I_{\text{Bhattacharyya}}(C; Y) = - \sum_i c_i \log \sum_j c_j \frac{2 \sigma_i \sigma_j}{\sigma_i^2 + \sigma_j^2}$$ KL divergence $\int p(x) \log \frac{p(x)}{q(x)} dx$ gives an upper bound $$I_{\text{KL}}(C; Y) = - \sum_i c_i \log \sum_j c_j \frac{\sigma_{i}^2}{\sigma_{j}^2} \exp \Bigl(1 - \frac{\sigma_{i}^2}{\sigma_{j}^2}\Bigr)$$ Those are all based on fantastic generic results but I want a tighter, optimization-friendly bound for CSCG mixture. Thoughts In terms of Renyi divergence , we can write Bhattacharyya distance as $0.5 D_{0.5}(p||q)$ and KL divergence as $D_1(p||q)$ . Heuristically, what if we use $D_{0.5}(p||q)$ in \eqref{1}? For CSCG mixture that is, $$I_{\text{Renyi0.5}}(C;Y) = -\sum_i c_i \log \sum_j c_j \frac{4 \sigma_i^2 \sigma_j^2}{(\sigma_i^2+\sigma_j^2)^2}$$ Initial simulation results suggest it is not only much tighter than other bounds/approximations, but also works as an upper bound in all tested zero-mean Gaussian mixture cases (but not true when some components are with non-zero mean). Here is an example: Each column is $I(C;Y_k)$ for $k=1,2,3$ . The first column is $I(C;Y_k)$ by Monte Carlo simulation while the last column is $I_{\text{Renyi0.5}}(C;Y_k)$ . For zero-mean Gaussian mixture, the latter appears a finer upper bound than all others. Question For zero-mean Gaussian mixture, it is possible to prove $I_{\text{Renyi0.5}}(C;Y)$ is an upper bound for $I(C;Y)$ ? Explicitly, can we prove the inequation $$- \sum_i c_i \int \frac{\exp(-{\lvert y \rvert^2}/{\sigma_i^2})}{\pi \sigma_i^2} \log \frac{\sum_j c_j \exp(-{\lvert y \rvert^2}/{\sigma_j^2})}{\exp(-{\lvert y \rvert^2}/{\sigma_i^2})} dy \le - \sum_i c_i \log \sum_j c_j \frac{4 \sigma_i^2 \sigma_j^2}{(\sigma_i^2 + \sigma_j^2)^2}$$ Thank you very much for your attention and help. [1] Huber, Marco F., et al. "On entropy approximation for Gaussian mixture random vectors." 2008 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems. IEEE, 2008. [2] Gu, Yujie, Nathan A. Goodman, and Amit Ashok. "Radar target profiling and recognition based on TSI-optimized compressive sensing kernel." IEEE Transactions on Signal Processing 62.12 (2014): 3194-3207. [3] Kolchinsky, Artemy, and Brendan D. Tracey. "Estimating mixture entropy with pairwise distances." Entropy 19.7 (2017): 361.
