[site]: crossvalidated
[post_id]: 615445
[parent_id]: 
[tags]: 
Overfitting models in mlr3

I'm trying to compare multiple learners on my dataset (called "data") in order to predict a target called "lesionResponse", with custom resampling. Since mlr3 package doesn't allow to group and stratify at the same time, I used a customized resampling in order to avoir the leakage of datas between sets. There is also a ratio of 3 for the 0/1 modalities of my target. Here is a short example of my dataset : structure(list(PatientID = c("P1", "P1", "P1", "P1", "P1", "P1", "P2", "P2", "P3", "P4", "P5", "P5", "P5", "P5", "P5", "P6", "P6", "P6"), LesionResponse = structure(c(2L, 1L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 1L),.Label = c("0", "1"), class = "factor"), F1 = c(1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 0.625, 0.625, 0.625, 0.625, 0.625, 0.625, 1.25, 0.625, 0.625, 1.25, 1.25, 1.25), F2 = c(1, 5, 3, 2, 1, 1, 6, 9, 0, 5, 0, 4, 4, 4, 5, 2, 1, 1), F3 = c(0, 4, 3, 1, 1, 0, 3, 8, 4, 5, 0, 4, 4, 3, 5, 2, 0, 0), F4 = c(0, 9, 0, 7, 4, 0, 3, 8, 4, 5, 9, 1, 1, 3, 5, 3, 9, 0)), row.names = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L), class = "data.frame") I manually splited the datas in train, validation, and test sets, with inner and outer resampling for the analyse : data data $weights = ifelse(data$ LesionResponse == "1", 3, 1) task = as_task_classif(data, target = "LesionResponse") task$set_col_roles("weights", roles = "weight") # Création du OUTER resampling via customisation resampling_outer = rsmp("custom") resampling_outer$instantiate(task, train = list(train_rows_outer), test = list(test_rows)) #Création du INNER resampling via customisation resampling_inner = rsmp("custom") resampling_inner$instantiate(task, train = list(train_rows), test = list(valid_rows)) Explanation : data = my dataset outer resampling = I divide my entire dataset into train_rows_outer and test_rows for final prediction and benchmark. Inner resampling = Inside the train_rows_outer, I created a train and a valid sets to tune the parameters of my models. Here are my models : -ranger -rpart -svm -knn -xgboost I'll just post two of them to show the code : #Auto tuning Ranger learner_ranger = lrn("classif.ranger", predict_type = "prob", num.trees = to_tune(1, 2000), mtry.ratio = to_tune(0, 1), sample.fraction = to_tune(1e-1, 1)) rr_ranger = tune_nested( tuner = tnr("grid_search", resolution = 5), task = task, learner = learner_ranger, inner_resampling = resampling_inner, outer_resampling = resampling_outer, measure = msr("classif.ce"), term_evals = 20, store_models = TRUE, terminator = trm("none") ) rr_ranger $score()[, .(iteration, task_id, learner_id, resampling_id, classif.ce)] predictions = rr_ranger$ prediction() predictions$confusion truth response 0 1 0 163 65 1 0 0 #Auto tuning svm learner_svm = lrn("classif.svm", type = "C-classification", cost = to_tune(p_dbl(1e-5, 1e5, logscale = TRUE)), gamma = to_tune(p_dbl(1e-5, 1e5, logscale = TRUE)), kernel = to_tune(c("polynomial", "radial")), degree = to_tune(1, 4)) rr_svm = tune_nested( tuner = tnr("grid_search", resolution = 10), task = task, learner = learner_svm, inner_resampling = resampling_inner, outer_resampling = resampling_outer, measure = msr("classif.ce"), term_evals = 20, store_models = TRUE, terminator = trm("none") ) extract_inner_tuning_results(rr_svm)[, .SD, .SDcols = !c("learner_param_vals", "x_domain")] extract_inner_tuning_archives(rr_svm, unnest = NULL, exclude_columns = c("resample_result", "uhash", "x_domain", "timestamp")) rr_svm $score()[, .(iteration, task_id, learner_id, resampling_id, classif.ce)] predictions = rr_svm$ prediction() predictions$confusion truth response 0 1 0 163 65 1 0 0 Even with the weights added, the normalization, the feature selection, I have all my models predicting the majority modality of my set. Could the problem be In my code or my choice of classif ?? Would another metric be better ?
