[site]: crossvalidated
[post_id]: 332330
[parent_id]: 332313
[tags]: 
Without knowing the exact procedure in question it's a bit hard to critique, but at this point I'm pretty confident in saying that you have a lot of better tools at your disposal than what is being suggested and you'd be better off not doing it. It sounds like you want to estimate the conditional distribution $P(Y=1 | X = x)$. A logistic regression does this by making additivity and linearity assumptions. You're considering an alternative approach where you change this from being continuous to instead being discrete, and then just use sample averages to estimate this probability within each cell of the $N_1 \times \dots \times N_p$ grid that now supports your distribution. If that's really the proposed estimator, there are a number of fatal flaws. First, how could you produce an estimate for a new point that's not exactly like other ones you saw? Second, the vast majority of cells will be completely empty, and the ones that aren't will probably only have a couple of data points. Each individual cell's estimates will be terrible. The only way to save this is to make some structural assumptions like independence, so you can attempt to recreate this joint distribution using lower-variance marginal information (I believe one such method is iterative proportional fitting where you use lower dimensional marginals to approximate a joint distribution table). This could also be done by fitting a logistic regression on the binned features, where now we can estimate each cell's probability by pooling information and we've made simplifying assumptions about the distribution. But now we're just going back to where we started, and I think we're worse off than if we didn't bin at all and just fit a more flexible model. Binning is generally a bad idea : don't do it unless you have to (like if there are industry requirements or something). From what you've described I'm not sure how this is different from any other black box machine learning prediction problem, so unless there's something I'm missing there's really no need to cook up something new and questionable here. Continuous features are fantastic for fitting flexible models. If you mainly care about prediction why not use that random forest? That's perfect for this sort of thing. Or use a GAM, Gaussian process, or any other appropriate model that you like. It's definitely possible that I've missed something here, and what with no free lunch it's not guaranteed that this model would perform poorly in your setting, but I wouldn't bet on it doing well.
