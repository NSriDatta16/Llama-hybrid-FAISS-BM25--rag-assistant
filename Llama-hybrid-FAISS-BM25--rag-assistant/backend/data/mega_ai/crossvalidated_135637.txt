[site]: crossvalidated
[post_id]: 135637
[parent_id]: 20622
[tags]: 
I agree with most of the points mentioned in tdc comment. however, I have to add and correct few things. As shown in L2Boost by Peter BÃ¼hlmann, as the number of weak learners (rounds of boosting) increases, the bias converges exponentially fast while the variance increases by geometrically diminishing magnitudes which means: It overfits much slower than most of the other methods. It was wrongly mentioned in Zach comment that it is better than random forest in terms of overfit. It is completely wrong. In fact, according to theory (look at original random forest paper by Breiman), Random Forest is absolutely immune against overfitting as long as its weak classifiers don't overfit to data. Unlike what mentioned in tdc comment, most of boosting methods are highly sensitive to the labeling noise and may easily overfit in the presence of labeling noise. In datasets where Bayes error rates are far from 0 (i.e., features are not discriminative enough) boosting methods can easily overfit , as well. Because they try to reduce the training error to zero while in reality even the optimal classifier, i.e., Bayes classifier can reach to a lets say 40% error rate. finally, and this has not been published any where (to the best of my knowledge) there is a kind of overfitting in which the generalization error does not increase as the boosting rounds increases but it does not decreases either. It means the algorithm has stuck in a local optima. In this situation, the training error constantly decreases while the test error remains almost constant. So far, we never considered this phenomenon as an indication of overfitting but I believe it is a sign of overfitting and by using more complex weak learners, (strange!) we may in fact go against it (This last point should be considered with caution :D)
