[site]: crossvalidated
[post_id]: 558569
[parent_id]: 558537
[tags]: 
There is no clear definition of what does "neural network architecture" mean. People would often would use the term for the models described in the papers, so LeNet-5 became "LeNet-5 architecture", etc. But if you change something about the model, would it be a different architecture? If "architectures" can be confusing, grouping them into "types" is even harder. For example, if you add residual connections over LSTM layers, is this a new architecture? Is it of LSTM or "residual connections" type? Most of neural networks are glued together of many different building blocks, so the decision on what defines the type of the architecture would be rather arbitrary. To complicate this even more, there are known examples of deep learning results being hard to reproduce when changing some of the hyperparameters, using a different optimization algorithm, or even different implementation. What I'm trying to say is that there is no single or definite classification like that, nor it would make much sense, at least at this stage of our understanding of neural networks.
