[site]: crossvalidated
[post_id]: 409980
[parent_id]: 
[tags]: 
Gaussian process - Why adding data points cannot increase the predictive bias?

I've seen this question here: How to increase variance in Gaussian Process regression? And trying to complete the proof. I'm looking at this book: Rasmussen & Williams 2006: Gaussian Processes for Machine Learning Page 31 starts the relevant subject, and question 2.9.4 is just the question I want to ask: Let $\operatorname{var}_{n}\left(f\left(\mathbf{x}_{*}\right)\right)$ be the predictive variance of a Gaussian process regression model at $x∗$ given a dataset of size $n$ . The corresponding predictive variance using a dataset of only the first $n − 1$ training points is denoted $\operatorname{var}_{n-1}\left(f\left(\mathbf{x}_{*}\right)\right)$ . Show that $\operatorname{var}_{n}\left(f\left(\mathbf{x}_{*}\right)\right) \leq \operatorname{var}_{n-1}\left(f\left(\mathbf{x}_{*}\right)\right)$ , i.e. that the predictive variance at x∗ cannot increase as more training data is obtained. One way to approach this problem is to use the partitioned matrix equations given in section A.3 to decompose $\operatorname{var}_{n}\left(f\left(\mathbf{x}_{*}\right)\right)=k\left(\mathbf{x}_{*}, \mathbf{x}_{*}\right)-\mathbf{k}_{*}^{\top}\left(K+\sigma_{n}^{2} I\right)^{-1} \mathbf{k}_{*}$ . I want to represent $K_{n}+\sigma^{2} I_{n}$ as a function of $K_{n-1}$ , and then to decompose $K_{n}+\sigma^{2} I_{n}$ by the following lemma from the book : (A.11 page 219) Let the invertible $n \times n$ matrix $A$ and its inverse $A^{-1}$ be partitioned into $$ {A=\left( \begin{array}{cc}{P} & {Q} \\ {R} & {S}\end{array}\right), \quad A^{-1}=\left( \begin{array}{cc}{\tilde{P}} & {\tilde{Q}} \\ {\tilde{R}} & {\tilde{S}}\end{array}\right)} $$ where $P$ and $\tilde{P}$ are $n1 \times n1$ matrices and S and $\tilde{S}$ are $n2 \times n2$ matrices with $n = n1 + n2$ . The submatrices of $A^{-1}$ are given in Press et al. [1992, p. 77] as: $$\tilde{P}=P^{-1}+P^{-1} Q M R P^{-1}$$ $$\tilde{Q}=-P^{-1} Q M$$ $$\tilde{R}=-M R P^{-1}$$ $$\tilde{S}=M$$ , Where $M=\left(S-R P^{-1} Q\right)^{-1}$ But then I don't know how to reach $k_{*}^{\top}\left(K_{n}+\sigma^{2} I_{n}\right)^{-1} k_{*} \geq k_{n-1}\left(x^{*}\right)^{\top}\left(K_{n-1}+\sigma^{2} I_{n-1}\right)^{-1} k_{n-1}\left(x^{*}\right)$ in order to complete the proof. It will finish the proof because of $\operatorname{var}_{n}\left(f\left(\mathbf{x}_{*}\right)\right) \leq \operatorname{var}_{n-1}\left(f\left(\mathbf{x}_{*}\right)\right)$ and the fact that $\operatorname{var}_{n}\left(f\left(\mathbf{x}_{*}\right)\right)=k\left(\mathbf{x}_{*}, \mathbf{x}_{*}\right)-\mathbf{k}_{*}^{\top}\left(K_{n}+\sigma^{2} I_{n}\right)^{-1} \mathbf{k}_{*}$ ... How can follow the clue there? Thanks
