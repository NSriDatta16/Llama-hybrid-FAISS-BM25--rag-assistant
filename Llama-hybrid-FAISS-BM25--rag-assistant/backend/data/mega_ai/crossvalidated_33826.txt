[site]: crossvalidated
[post_id]: 33826
[parent_id]: 33566
[tags]: 
Percentage agreement (with tolerance = 0): 0.0143 Percentage agreement (with tolerance = 1): 11.8 Krippendorff's alpha: 0.1529467 These agreement measures state that there is virtually no categorial agreement - each coder has his or her own internal cutoff point for judging comments as "friendly" or "unfriendly". If we assume that the three categories are ordered, i.e.: Unfriendly The average bivariate correlation between two coders is .34, which also is rather low. If these agreement measures are seen as a quality measure of the coders (who actually should show good agreement), the answer is: they are not good coders and should be better trained. If this is seen as a measure of "how good is spontaneous agreement amongst random persons", the answer also is: not very high. As a benchmark, the average correlation for physical attractiveness ratings is around .47 - .71 [1] [1] Langlois, J. H., Kalakanis, L., Rubenstein, A. J., Larson, A., Hallam, M., & Smoot, M. (2000). Maxims or myths of beauty? A meta-analytic and theoretical review. Psychological Bulletin, 126, 390â€“423. doi:10.1037/0033-2909.126.3.390
