[site]: crossvalidated
[post_id]: 23414
[parent_id]: 20010
[tags]: 
This is a very good question and a very subtle problem. Of course there are the bad intentioned mistakes, which derive from someone trying to deceive you. But there is a deeper question of how to avoid accidental leaking and avoid honest mistakes. Let me list some operational good practices. They all stem from honest mistakes I've made at some point: Separate your data into three groups: train, validate and test. Understand the problem setup to be able to argue what is reasonable and what isn't. Understand the problem, many times subtle misunderstanding in what the data represents can lead to leaks. For example while no one would train and test on the same frame of one video, it is more subtle when two frames of the same video fall in different folds, two frames of the same video probably share the same individuals the same lighting and so on. Be extremely careful with previously written cross validation procedures. More so with ones not written by you (LIBSVM is a big offender here). Repeat every experiment at least twice before reporting anything, even if reporting to your office mate. Version control is your friend, before running an experiment commit and write down what version of the code you're running. Be very careful when normalizing your data. Many times this leads to thinking you will have the full dataset on which you want to test at the same time, which again is often not realistic.
