[site]: crossvalidated
[post_id]: 319149
[parent_id]: 318819
[tags]: 
Definition A Gaussian Process (GP) is a Gaussian distribution over functions: $$f(x) \sim GP(m(x), k(x, x^\prime))$$ where $m(x)$ is the mean function and $k(x, x^\prime)$ is a kernel function. The solid blue line in the middle on your pictures is the mean $m(x) = \mathbb{E}[f]$, while the two lines around it are functions that are one standard deviation away from the mean: $\mathbb{E}[f] \pm \sigma[f]$. Each iteration is an evaluation of an outcome for some $x$. These are the blue dots on your pictures, after two and three evaluations. As you can see, the standard deviation collapses at these points, because their outcomes have been calculated , i.e., there's nothing to vary. All other points are still unknown, but GP models what their probable value is, given the evaluated points. Intuitively, the further you go from the points, the larger the variance. How is GP calculated? Initially, $m(x)$ is a constant, usually zero (though it depends on the prior settings, in your case it might be 50%). Each new evaluation gives a lot of information, because it fixes one more point for $m$ and changes the shape of the whole curve. The exact formula is derived from Multivariate Gaussian Theorem (e.g., discussed here ). Intuitively, let's look at the transformation at step 3 on your picture, which evaluates $x_3=200$. At step 2, $m(x_3)$ is roughly $70\%$, because that's kind of average of known values $f(x_1)$ and $f(x_2)$, but with huge variance: any value between $55\%$ and $95\%$ is within one standard deviation. Basically, everything beyond $x > 150$ is a big unknown area. Evaluation of $f(x_3) = 85\%$ makes it certain: $m(x_3)$ is about the same as $m(x_2)$, but slightly better, which makes $m(x)$ monotone between $x_2$ and $x_3$. The variance around $x_2$ and $x_3$ is small, however the value at $x=150$ can be both much better or much worse. Note that at each step, $m(x)$ is smooth, because that's exactly what GP models. Hope this clarifies why the shape of $m(x)$ makes sense. How the kernel affects GP? It makes the variance larger or smaller by assuming the likelihood of big changes of $f$ on a small interval. In other words, it controls what a small interval of $x$ is. Remember that GP models the smooth functions over $x$, but changing the measure of $x$ changes how "wigly" the functions of $x$ are allowed to be. For example, here's the effect of changing the width parameter of the Gaussian kernel, from $0.1$ to $1$ to $10$ (the gray area is the variance): The first one allows a lot of variation in $f$ by making the distances between $x$'s very large. The last one is the opposite. The exact formula again follows the Multivariate Gaussian Theorem. Highly recommend the lectures by Nando de Freitas available on YouTube (the first one is here ) to get a better understanding of GP. Hyperparameter tuning via Gaussian Optimization is also discussed in this question .
