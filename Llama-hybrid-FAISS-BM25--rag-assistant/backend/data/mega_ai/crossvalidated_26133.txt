[site]: crossvalidated
[post_id]: 26133
[parent_id]: 
[tags]: 
Is there a difference between an autocorrelated time-series and serially autocorrelated errors?

I'm pretty sure that I'm missing something obvious here, but I'm rather confused with different terms in the time series field. If I understand it correctly, serially autocorrelated errors are a problem in regression models (see for example here ). My question is now what exactly defines an autocorrelated error? I know the definition of autocorrelation and I can apply the formulas, but this is more a problem of comprehension with time series in regressions. For example, let's take the time series of daily temperatures: If it is a hot day today (summer time!), it's probably hot tomorrow as well, and vice versa. I guess I have a problem to call this phenomenon a phenomenon of "serially autocorrelated errors" because it just doesn't strike me as an error, but as something expected. More formally, let's assume a regression set-up with one dependent variable $y_t$ and one independent variable $x_t$ and the model. $$ y_t = \alpha + \beta x_t + \epsilon_t $$ Is it possible that $x_t$ is autocorrelated, while $\epsilon_t$ is i.i.d? If so, what does that mean for all that methods that adjust standard errors for autocorrelation? Do you still have to do that or do they only apply to autocorrelated errors? Or would you always model the autocorrelation in such a setting in the error term, so it basically doesn't make a difference if $x_t$ is autocorrelated or $e_t$? This is my first question here. I hope it's not too confusing and I hope I didn't miss anything obvious...I also tried to google it and found some interesting links (for instance, here on SA ), but nothing really helped me.
