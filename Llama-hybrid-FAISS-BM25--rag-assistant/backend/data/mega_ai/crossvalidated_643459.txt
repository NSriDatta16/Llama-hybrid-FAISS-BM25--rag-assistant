[site]: crossvalidated
[post_id]: 643459
[parent_id]: 
[tags]: 
How to estimate mean and standard deviation of individual result given a sample of batch results?

I apologize in advance if my title is not well-worded, or if I am using the wrong tags; this is my first time asking a question here. I want to analyze how long it takes for a system in production to import data tokens in bulk. Currently, the system generates a log line every time 500 tokens are imported; for a given sample of 30000 tokens, 60 lines are generated (technically 61, since 0 % 500 = 0, but of course I ignore this first figure.) From this sample of 60 numbers, I can easily calculate the average and standard deviation for each set of 500 tokens, but can I extrapolate this to estimate that for each token import? For instance, please assume that my average is 9878.85 and my SD is 3441.64. Can I divide the average by 500 to estimate the time taken for each import - in this case, 19.76? A quick Google search indicates that I should divide the standard deviation not by 500, but by the root of 500. However, when paired with the answer above, I get 153.92, which does not make sense. Am I doing this the right way?
