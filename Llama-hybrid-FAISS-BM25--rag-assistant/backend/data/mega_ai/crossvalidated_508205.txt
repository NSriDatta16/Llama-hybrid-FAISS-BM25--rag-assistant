[site]: crossvalidated
[post_id]: 508205
[parent_id]: 507970
[tags]: 
The differences in variance are most definitely the reason for the observed differences in importance. Broadly, you would only expect the result to show ~ .167 for a lm 's set of coefficients. Because of the way simulated_set[["dep"]] is defined, the mean value is determined by mulitplying all X variables by one-sixth. > sapply(coef(lm(dep ~ X1 + X2 + X3 + X4 + X5 + X6, simulated_set)), round, digits=5) (Intercept) X1 X2 X3 X4 X5 0.00000 0.16667 0.16667 0.16667 0.16667 0.16667 X6 0.16667 Importance statistics usually involve model fit, and xgboost is no exception. This idea is easier to see for simpler metrics though like lm -based importance metrics such as Dominance Analysis /DA which is explained well in the linked article by GrÃ¶mping. Take the same model and run it through a DA (implemented by domir but equivalent results can be obtained by dominanceanalysis and relaimpo ) and you get: > sapply(domir::domin(dep ~ X1 + X2 + X3 + X4 + X5 + X6, lm, list(summary, "r.squared"), data=simulated_set)$General_Dominance, round, digits=5) X1 X2 X3 X4 X5 X6 0.01624 0.28521 0.00035 0.01866 0.67623 0.00331 > sapply(simulated_set[-7], sd) X1 X2 X3 X4 X5 X6 29.288122 122.741150 4.287464 31.398400 188.995556 13.216770 Very much dependent on the variances as can be seen. Whereas this article focuses on random forest, a comparison between linear models and random forest is discussed here ; it provides good description of similarities and differences between the approaches that might help to explain the results.
