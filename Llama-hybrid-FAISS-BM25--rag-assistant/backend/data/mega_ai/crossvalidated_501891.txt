[site]: crossvalidated
[post_id]: 501891
[parent_id]: 
[tags]: 
How to avoid overfitting (while training a model and predicting) in a dataset that is basically overfitted?

I need to solve a multi-label classification problem where the dataset itself is the definition of overfitting, in the sense that some labels appear a lot (almost 50% frequency), some rarely appear, some have a lot of empty values (almost 90% frequency) which are meaningful considering the business rule, etc. I trained a Transformer model and the loss is low over time, the F1 score is 1, the LRAP is .99%, but the classification results are insanely bad. All the attributes are categorical. I tried to 'fix' the empty values, but since they have a meaning in the business rule that is not a good preprocessing. Besides this, I tried to apply the most frequent metric, which will 'reduce' the overfitting in a side and increase the bias in another. Any ideas if is it even possible to solve and if yes, how?
