[site]: crossvalidated
[post_id]: 147867
[parent_id]: 137537
[tags]: 
I am currently exploring stacked-convolutional autoencoders. I will try and answer some of your questions to the best of my knowledge. Mind you, I might be wrong so take it with a grain of salt. Yes, you have to "reverse" pool and then convolve with a set of filters to recover your output image. A standard neural network (considering MNIST data as input, i.e. 28x28 input dimensions) would be: 28x28(input) -- convolve with 5 filters, each filter 5x5 --> 5 @ 28 x 28 maps -- maxPooling --> 5 @ 14 x 14 (Hidden layer) -- reverse-maxPool --> 5 @ 28 x 28 -- convolve with 5 filters, each filter 5x5 --> 28x28 (output) My understanding is that conventionally that is what one should do, i.e. train each layer separately. After that you stack the layers and train the entire network once more using the pre-trained weights. However, Yohsua Bengio has some research (the reference escapes my memory) showcasing that one could construct a fully-stacked network and train from scratch. My understanding is that "noise layer" is there to introduce robustness/variability in the input so that the training does not overfit. As long as you are still "training" pre-training or fine-tuning, I think the reconstruction part (i.e. reversePooling, de-convolution etc) is necesary. Otherwise how should one perform error-back-propagation to tune weights? I have tried browsing through numerous papers, but the architecture is never explained in full. If you find any please do let me know.
