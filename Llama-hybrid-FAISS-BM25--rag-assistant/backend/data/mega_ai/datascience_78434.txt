[site]: datascience
[post_id]: 78434
[parent_id]: 78431
[tags]: 
You need character embeddings . I assume you are already familiar with word2vec technology. Its goal it to make a model "learn" the relative meaning of words, placing them into a highly dimensional space. The same can be done with single characters, instead of whole words. The preprocessing steps you need will be a little bit different, but the embedding technique is the same. In that way, you can generate representations of characters, feed their sequences into some RNN model, and perform the final classification task. Therefore, RNNs are perfectly suitable for this task. If you are working with tensorflow.keras you can simply tokenize characters, and feed them through an Embedding() layer that will do the job for you. An alternative to RNNs is 1D conv layers, that can do the job as an alternative to recurrent cells. That's up to your preference.
