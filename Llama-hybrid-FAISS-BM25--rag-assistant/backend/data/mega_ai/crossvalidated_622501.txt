[site]: crossvalidated
[post_id]: 622501
[parent_id]: 622484
[tags]: 
You are correct that $X=XI$ is a zero-error NMF factorisation of any non-negative matrix $X$ , so the software is not finding the correct optimum. It's probably not a bug, though. The NMF problem has local optima, unlike PCA. Even worse, if $X=WH$ is a solution, then $X=WAA^{-1}H$ is another solution exactly as good, as long as $(WA)$ and $(A^{-1}H)$ are non-negative, so even local optima aren't isolated. In addition, to the extent that the problem is identifiable, it's only because the non-negativity constraints reduce the options, so that many of the optima are right up against the boundary of the parameter space. It's quite a hard problem, which is why there are so many algorithms in the NMF package (if you have six algorithms implemented for something, it's unlikely that any of them is really satisfactory). That's not really surprising. The singular value decomposition underlying PCA is what's unusual here in having a fast globally-convergent algorithm. The brunet algorithm is described here . It starts with randomly initialised matrices and does alternating multiplicative updates of the two factors, which apparently works well for low-rank approximations, but you can easily imagine it getting stuck on a high-rank fit. I tried the various algorithms on a single random matrix of $U[0,1]$ values with rank=number of columns, the lee and ls-nmf did best in terms of residual mean squared error; you might try them if you want a high-rank solution. (I worked on a variation of this problem in air pollution, where the goal was to decompose chemistry x time data into chemistry x source and source x time, but with the additional complication that the error variances were all different and poorly estimated)
