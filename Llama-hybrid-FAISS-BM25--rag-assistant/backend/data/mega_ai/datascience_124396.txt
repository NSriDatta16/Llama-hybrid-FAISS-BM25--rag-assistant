[site]: datascience
[post_id]: 124396
[parent_id]: 
[tags]: 
Semantic search with pretrained BERT models giving irrelevant results with high similarity

I'm trying to create a Semantic search system and have experimented with multiple pretrained models from the SentenceTransformers library: LaBSE, MS-MARCO etc. The system is working well in returning relevant documents first with high probability but issue is documents which are not relevant are also coming with relatively high probabilities. Hence it has become difficult to determine a cutoff threshold for what is relevant and what isnt. For computing vector similarity i have experimented with Elasticsearch approximate KNN and FAISS with similar results in both. Have checked exact cosine similarities with Scikit-learn also. My corpus generally has sentences of 15-30 words and the input sentence is Corpus text 1: is a Fashion House. We design, manufacture and retail men's and women's apparel Input sentence 1: men's fashion Cosine similarity 1: 0.21 Corpus text 2: is an app for pizza delivery Input sentence 2: personal loan Cosine similarity 2: 0.16 Please suggest pretrained models that might be good for this purpose.
