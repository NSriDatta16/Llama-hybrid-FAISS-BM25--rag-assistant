[site]: crossvalidated
[post_id]: 218273
[parent_id]: 
[tags]: 
Interpresting R^2 and F-test of data with different variances

I have results where the sum of squared errors are very similar (SSE=Sum(y_Est-y)^2) but the corresponding R^2 are very different as are the F-test results. The coefficient of determination can be defined as R^2 = 1 - SSE/SST which can be interpreted as 1-(unexplained variation)/(total variation). Now I have a situation in which the SSE's are very similar, but the SST are very different. The net result is one regression has an R^2 of over 0.6 and the other has 0.05! It's having a similar effect on the F-test: SSM/SSE. For similar reasons the SSM for the less volatile regression is much lower and so it has a much lower F-score. It seems strange to me that although both regressions give a fit of similar quality the second one gets penalised simply because it is applied to less volatile data? Clearly the disparity in volatility between the two time series is skewing the results. In such a situation how should I compare the regression results between data that has different levels of volatility. Thanks
