[site]: crossvalidated
[post_id]: 145912
[parent_id]: 145902
[tags]: 
In classification problems, maximizing the likelihood is the most common way to train a neural network (both supervised and unsupervised models). In practice, we usually minimize the negative log-likelihood (equivalent MLE). The only constraint to use the negative log-likelihood is to have an output layer that can be interpreted as a probability distribution. A softmax output layer is commonly used to do so. Note that in the neural-networks community, the negative log-likelihood is sometimes referred as the cross-entropy. Regularization terms can of course be added (and sometimes can be interpreted as prior distributions over the parameters, in that case we are looking for the maximum a posteriori ( MAP )).
