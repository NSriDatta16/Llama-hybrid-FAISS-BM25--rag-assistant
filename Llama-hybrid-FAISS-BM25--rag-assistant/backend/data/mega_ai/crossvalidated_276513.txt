[site]: crossvalidated
[post_id]: 276513
[parent_id]: 
[tags]: 
Using categorical predictive distribution to predict an outcome

I'm a bit new to Bayesian statistics and probability theory in general, but I've been trying my hand at a quick utilization of it of where there are 26 outcomes and I am trying to predict the next outcome. As usual, I gave my mixtures a Dirichlet prior distribution and my distributions for the mixtures are Categorical Distributions of 1-of-26. With this, I observe some observations $$\textbf{X} = \{ x_1 = 0, x_2 = 15, x_3 = 22, ..... x_N\}$$ And then I try to use the predictive distribution. The problem is, I am almost certain I am using it incorrectly. My current method is this: Compute the conditional probabilities for each mixture. This gives $K$ probabilities that "should" sum to 1. Sample from this distribution $K$ vector as if it were a categorical distribution to pick some mixture $k$. Sample from the $k$ mixture model (using its respective categorical distribution). For step 1 - I compute for each $K$ mixture models, the following probability: $$p(z = k|X) = p(z)\cdot\underbrace{\Pi_{i=1}^Np(x_i|z)}_{\text{Each observation}}$$ The problem is that...for Step 1, I get extremely small numbers. Even when I use log probabilities, I have to eventually renormalize them and this yields very unstable computations. Most of the time, only one mixture will survive and all the others will die out. (So $p(z = k|X) \approx 1$ for some k mixture model). This leads me to believe that my procedure is incorrect. Could anyone please enlighten me?
