[site]: datascience
[post_id]: 502
[parent_id]: 488
[tags]: 
Regarding prediction, statistics and machine learning sciences started to solve mostly the same problem from different perspectives. Basically statistics assumes that the data were produced by a given stochastic model. So, from a statistical perspective, a model is assumed and given various assumptions the errors are treated and the model parameters and other questions are inferred. Machine learning comes from a computer science perspective. The models are algorithmic and usually very few assumptions are required regarding the data. We work with hypothesis space and learning bias. The best exposition of machine learning I found is contained in Tom Mitchell's book called Machine Learning . For a more exhaustive and complete idea regarding the two cultures you can read the Leo Breiman paper called Statistical Modeling: The Two Cultures However what must be added is that even if the two sciences started with different perspectives, both of them now now share a fair amount of common knowledge and techniques. Why, because the problems were the same, but the tools were different. So now machine learning is mostly treated from a statistical perspective (check the Hastie,Tibshirani, Friedman book The Elements of Statistical Learning from a machine learning point of view with a statistical treatement, and perhaps Kevin P. Murphy 's book Machine Learning: A probabilistic perspective , to name just a few of the best books available today). Even the history of the development of this field show the benefits of this merge of perspectives. I will describe two events. The first is the creation of CART trees, which was created by Breiman with a solid statistical background. At approximately the same time, Quinlan developed ID3,C45,See5, and so on, decision tree suite with a more computer science background. Now both this families of trees and the ensemble methods like bagging and forests become quite similar. The second story is about boosting. Initially they were developed by Freund and Shapire when they discovered AdaBoost. The choices for designing AdaBoost were done mostly from a computational perspective. Even the authors did not understood well why it works. Only 5 years later Breiman (again!) described the adaboost model from a statistical perspective and gave an explanation for why that works. Since then, various eminent scientists, with both type of backgrounds, developed further those ideas leading to a Pleiads of boosting algorithms, like logistic boosting, gradient boosting, gentle boosting ans so on. It is hard now to think at boosting without a solid statistical background. Generalized Linear Models is a statistical development. However new Bayesian treatments puts this algorithm also in machine learning playground. So I believe both claims could be right, since the interpretation and treatment of how it works could be different.
