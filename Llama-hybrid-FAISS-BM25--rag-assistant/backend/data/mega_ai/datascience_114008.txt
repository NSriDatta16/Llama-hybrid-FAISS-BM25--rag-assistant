[site]: datascience
[post_id]: 114008
[parent_id]: 
[tags]: 
Extend BERT or any transformer model using manual features

I have been doing a thesis in my citation classifications. I just implemented Bert model for the classification of citations. I have 4 output classes and I give an input sentence and my model returns an output that tells the category of citation. Now my supervisor gave me another task. You have to search that whether it is possible to extend BERT or any transformer model using manual features. e.g. You are currently giving a sentence as the only input followed by its class. What if you can give a sentence, and some other features as input; as we do in other classifiers? I need some guidance about this problem. How can I add an extra feature in my Bert model and the feature would be categorical not numerical. This is my code what I have done for implementation of BERT model I want to add manual features in my code I am using Bert Tokenizer tokenizer = BertTokenizer.from_pretrained('bert-base-cased') Then I am making dataset of inputs ids and attention masks of size 256 X_input_ids = np.zeros((len(df), 256)) X_attn_masks = np.zeros((len(df), 256)) This is my function of tokenization of sentences def generate_training_data(df, ids, masks, tokenizer): for i, text in tqdm(enumerate(df['Citing Sentence'])): tokenized_text = tokenizer.encode_plus( text, max_length=256, truncation=True, padding='max_length', add_special_tokens=True, return_tensors='tf' ) ids[i, :] = tokenized_text.input_ids masks[i, :] = tokenized_text.attention_mask return ids, masks Then I am generating input_ids and attention_masks from tqdm.auto import tqdm X_input_ids, X_attn_masks = generate_training_data(df, X_input_ids, X_attn_masks, tokenizer) Then I made set of size 4 because my output has 4 categories Related work Comparison Using the work Extending the work One-hot encoded target tensor. Here follow-up is my output array array labels = np.zeros((len(df), 4)) labels[np.arange(len(df)), df['Follow-up'].values] = 1 dataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels)) def CitationDatasetMapFunction(input_ids, attn_masks, labels): return { 'input_ids': input_ids, 'attention_mask': attn_masks }, labels converting to required format for tensorflow dataset dataset = dataset.map(CitationDatasetMapFunction) batch size, drop any left out tensor dataset = dataset.shuffle(10000).batch(16, drop_remainder=True) for each 4 batch of data we will have len(df)//16 samples, take 80% of that for train. p = 0.8 train_size = int((len(df)/16)*p) train_dataset = dataset.take(train_size) val_dataset = dataset.skip(train_size) Summarising the model from transformers import TFBertModel model = TFBertModel.from_pretrained('bert-base-cased') # bert base model with pretrained weights input_ids = tf.keras.layers.Input(shape=(256,), name='input_ids', dtype='int32') attn_masks = tf.keras.layers.Input(shape=(256,), name='attention_mask', dtype='int32') bert_embds = model(input_ids, attention_mask=attn_masks)[1] # 0 -> activation layer (3D), 1 -> pooled output layer (2D) intermediate_layer = tf.keras.layers.Dense(512, activation='relu', name='intermediate_layer')(bert_embds) output_layer = tf.keras.layers.Dense(4, activation='softmax', name='output_layer')(intermediate_layer) # softmax -> calcs probs of classes citation_model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer) citation_model.summary()
