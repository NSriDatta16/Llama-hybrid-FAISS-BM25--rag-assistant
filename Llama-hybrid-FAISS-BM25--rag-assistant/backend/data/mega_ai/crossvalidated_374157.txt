[site]: crossvalidated
[post_id]: 374157
[parent_id]: 374153
[tags]: 
Yes, this is a perfectly valid procedure. You need to make sure that your model can distinguish between "similar" and "not similar". So you simulate both kinds of datasets, and make sure that they are flagged correctly. You can also take a "similar" dataset and perturb it. This is a form of sensitivity analysis . It's also a good way to test the robustness of a model, by taking a "similar" dataset and perturbing it in different ways. The perturbations can be deterministic or randomized; I recommend both. Here are some general recommendations for sensitivity analysis in machine learning models that might apply to you. Introducing errors/perturbations by hand isn't a bad thing. It only introduces "bias" in the sense that you will only be able to evaluate robustness against errors you can think of. That's fine; something is better than nothing. However, I don't recommend going back and adjusting your model in response; not all errors are equally probable, and it's possible that the errors you manually introduce aren't representative of the errors your model will encounter in the wild. If possible, see if you can figure out what kinds of errors you are most likely to encounter. Then you can (at least heuristically) know if you need to worry about them, and use that knowledge to choose the errors you introduce.
