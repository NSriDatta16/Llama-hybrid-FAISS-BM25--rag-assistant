[site]: crossvalidated
[post_id]: 31057
[parent_id]: 31053
[tags]: 
I can think of no better place than Owen's book to learn about empirical likelihood. One practical way to think about $L = L(p_1, \ldots, p_n)$ is as the likelihood for a multinomial distribution on the observed data points $x_1, \ldots, x_n$. The likelihood is thus a function of the probability vector $(p_1, \ldots, p_n)$, the parameter space is really the $n$-dimensional simplex of probability vectors, and the MLE is putting weight $1/n$ on each of the observations (supposing they are all different). The dimension of the parameter space increases with the number of observations. A central point is that empirical likelihood gives a method for computing confidence intervals by profiling without specifying a parametric model. If the parameter of interest is the mean, $\mu$, then for any probability vector $p = (p_1, \ldots, p_n)$ we have that the mean is $$\mu(p) = \sum_{i=1}^n x_i p_i,$$ and we can compute the profile likelihood as $$L_{\text{prof}}(\mu) = \max \{ L(p) \mid \mu(p) = \mu \}.$$ Then we can compute confidence intervals of the form $$I_r = \{ \mu \mid L_{\text{prof}}(\mu) \geq r L_{\text{prof}}(\bar{x}) \}$$ with $r \in (0,1)$. Here $\bar{x}$ is the empirical mean and $L_{\text{prof}}(\bar{x}) = n^{-n}$. The intervals $I_r$ should perhaps just be called (profile) likelihood intervals since no statement about coverage is made upfront. With decreasing $r$ the intervals $I_r$ (yes, they are intervals) form a nested, increasing family of confidence intervals. Asymptotic theory or the bootstrap can be used to calibrate $r$ to achieve 95% coverage, say. Owen's book covers this in detail and provides extensions to more complicated statistical problems and other parameters of interest.
