[site]: crossvalidated
[post_id]: 91906
[parent_id]: 83104
[tags]: 
You could resample the data to over represent the more recent data points. Rf involves a sampel-with-replacment step anyways and "roughly balanced bagging" for unbalanced classes uses sampling to overrepresent the minority class and produces results as good or better then class weighted random forest in my experience. You could resample at the level of constructing your training matrix ( reference ) instead of during bagging to keep implementation easy though I would suggest doing many repeats in that case. Internally some implementations of random forest including scikit-learn actually use sample weights to keep track of how many times each sample is in bag and it should be equivalent to oversampling at the bagging level and close to oversampling at the training level in cross validation.
