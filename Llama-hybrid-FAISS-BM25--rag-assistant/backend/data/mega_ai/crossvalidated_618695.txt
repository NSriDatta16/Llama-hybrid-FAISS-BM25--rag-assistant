[site]: crossvalidated
[post_id]: 618695
[parent_id]: 618168
[tags]: 
Say that you have a speed of 20 mph. You will have variability in the measurements of your different devices, and from what you say, you expect each device to be within 3% of the actual value. Let's say then that 3% is the standard deviation of the variability between the different instruments, so the standard deviation between devices is 0.6 mph for your 20 mph real speed. Let's call that standard deviation $\sigma_{device}$ . To solve the problem we next need to make two assumptions: The 'error' of the devices is independent and identically distributed. Regarding independence, this means that we don't expect that on a given measure all devices overestimate or all devices overshoot (or undershoot) at the same time. Regarding identically distributed, this means that we don't expect some devices to be highly flawed and some highly accurate, but rather they all have similar degrees of precision, at least in principle. The errors of the devices are spread around the actual value of speed. That is, it's not the case that all devices always overestimate the speed, but rather some overestimate and some underestimate, by on average the same amount. These are some typical assumptions. Then, if on average the devices are correct, even if the individual devices aren't, we can add more devices to get a better estimation of what the true speed is, because their errors will average out more as we add more devices. How variable is the average between the devices, given how variable is each device and how many devices we are averaging? That's just given by the standard deviation of the mean, which is discussed here . That is the variability that the average will have, and it is given by: $$\sigma_{average} = \frac{\sigma_{device}}{\sqrt{N}}$$ where $N$ is the number of devices that you have. So, say that you have 5 devices with a $\sigma_{device} = 0.6$ as described above, then the formula says that the average will have a standard deviation of $\sigma_{average} = 0.268 mph$ . In the assumptions above we assumed that the average is centered around the true mean (20 mph), and $\pm 1.96\sigma_{average}$ is the region that will contain 95% of the average estimates, so 95% of your device averages will be within 19.474 and 20.526. To get to the number of devices needed, you just need to: Pick a reference speed Obtain the corresponding $\sigma_{device}$ that you guess Pick a $\sigma_{average}$ that is acceptable for your purposes Solve for $N$ Your specification of the problem is not clear though, so it's not certain whether this specification will match your actual real problem, but it seems like a standard approach to what you describe. And needless to say, the performance of the method will depend on how good your guess of the % of variance of the devices is, whether they are on average correct, etc. Also, we assumed that 3% is the standard deviation of the devices errors, but maybe this would mean that a given measurement would be within 3% of the true mean only ~62% of the time, which is maybe not what you meant by "I expect that each instrument will record the value to within 3% of the actual value." Statistics is not magic, and if you start from wrong assumptions, you will end up with wrong conclusions. If this answers your question, please consider marking as the accepted answer.
