[site]: crossvalidated
[post_id]: 64416
[parent_id]: 64410
[tags]: 
There's no such thing as constant error when fitting vectors of dichotomous outcomes. This is why fitting a logistic curve by minimizing squared errors is far less favorable than logistic regression for binary outcomes, iteratively reweighting the errors according to their mean-variance relationship gives you much more efficient modeling. Utilizing such parametric assumptions is one of the strengths of Bayesian statistics. It's natural that the posterior distribution of the proportion appears approximately normally distributed due to the analogous frequestist test of proportions and the central limit theorem. But, as we know, normality of posteriors (like parameter estimates) does not imply that errors are normally distributed. With "2" spaces, the beta prior is favorable because it is the conjugate prior for a bernoulli observed random variable. The beta prior is flexible and can appear approximately normal while maintaining the correct support $\Omega = (0, 1)$. With more than two spaces, the analogue is the use of dirichlet distribution for a multinomial outcome. As we know, choosing conjugate priors is not necessary for approximately correct Bayesian inference, but they tend to behave much more regularly in small samples and/or with rare events.
