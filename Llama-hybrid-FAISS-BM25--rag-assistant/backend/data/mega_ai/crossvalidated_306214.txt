[site]: crossvalidated
[post_id]: 306214
[parent_id]: 275160
[tags]: 
The difference is 'type of test' and not 'sample size' The difference between the two formula, $\sigma$ vs $s$ , is not in the difference of sample size. The difference is whether $\sigma$ is known or estimated. The first formula uses a normalization with a "known" standard deviation, and the second formula uses a normalization with the sample estimate of the standard deviation. The first, $\sigma$ , is a constant , the second, $s$ , is a random variable (with chi-square distribution). So the difference is: you use the t-distribution $\mathcal{N}(0,1)/\sqrt{\chi_{n-1}/(n-1)}$ to describe the distribution of the difference between a 'sample mean' and 'the population mean', if this difference is normalized based on the sample estimate of the standard deviation and you use the normal distribution $\mathcal{N}(0,1)$ to describe the distribution of the difference between a 'sample mean' and 'the population mean', if this difference is normalized based on the standard deviation of the population . Note: for large sample sizes you do get that the distribution of this chi-square denominator becomes closer to a peak around 1 $$\lim_{n \to \infty} \sigma_{\left(\frac{\chi_{n-1}}{n-1}\right)} = \sqrt{\frac{2}{n-1}}= 0 \qquad \mathrm{and} \qquad \mu_{\left(\frac{\chi_{n-1}}{n-1}\right)} = 1 $$ or in other words the sample estimate of the standard deviation is less variable $$ \lim_{n \to \infty} s = \sigma$$ and the t-distribution becomes approximately a normal distribution $$ \lim_{n \to \infty} t_n = \mathcal{N}(0,1)$$ So you could say that: for large sample sizes the formula that are used with sample estimated standard deviation approximate the formula that are used with known standard deviation. This is a different thing than the central limit theorem in which a mean of sample of variables from a non-normal distribution becomes a normal distribution for large $n$ . Note: The standard deviation is often not 'really' known. But it can be 'hypothetically' known. For instance in testing a hypothesis or in Bayesian inference you 'assume' a certain deviation. (in the same way as $\mu$ is not known but you can still use it in the formula and use it hypothetically, for instance in determining confidence intervals)
