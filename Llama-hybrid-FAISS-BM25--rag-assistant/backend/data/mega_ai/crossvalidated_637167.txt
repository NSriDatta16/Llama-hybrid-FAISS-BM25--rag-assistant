[site]: crossvalidated
[post_id]: 637167
[parent_id]: 637147
[tags]: 
It really depends on the context. In many prediction situations, there is actually a lot of information in missingness and even missigness at random does not apply (never mind missingness completely at random), because it might indicate something has not happened/attitudes of people/"having something to hide"/newness of something (e.g. missing price of last ordered product because customer has never ordered something, refusal to answer a question might indicate something, lack of publicly available information on something might indicate something etc.). For that reason it is often popular to have a flag for missingness and then some simple imputation on the variable that is missing (which has also been suggested to perform decently for dealing with missing baseline information when comparing treatments in randomized clinical trials, where one might normally be tempted to use multiple imputation) in order to capture the fact that something is missing. That would be a good reason to prefer something else over multiple imputation, but you could of course speculate that multiple imputation in addition to a missingness indicator might still be a good idea. Additionally, you might worry that a multivariate normal (or even worse MI by chained equations) imputation model is insufficiently complex to appropriately impute missing values due to non-linearities and/or complex interactions. Some standard implementations of these approach may also struggle to adequately deal with some non-standard data types like high cardinality categorical data, ordered categorical data, count data etc. - although one can of course extend MI to these settings. Whether that's a good reason not to do it (or whether you should look for a MI method that allows for more complexity) depends on your particular case. In settings were missingness at random is a sensible assumption and a multiple imputation model you can use is sufficiently complex, it may very well be a good idea to do MI and I suspect it's underutilized. That's probably due to several factors: No good tooling for combining multiple imputation with standard machine learning workflows. E.g. do you really need to manually code up that you fit a ML model for every imputation, then a apply imputation to all new validation/test data coherent with each imputed dataset, followed by the ML model fit on that? See also the next point and also here and here ) It's time consuming, because you currently need to fit one model per imputed dataset, although there might be alternative like sampling an imputed dataset for each tree in a xgboost model or something like that (but I'm not aware of any software that support doing that). Several MI tools don't actually support applying the imputation model to new unseen data (most are intended for when you have all data at once and some is partially missing), especially in a way that makes them coherent (in terms of having the same underlying parameters used for imputed training dataset 1 and imputed validation/testing dataset 1 etc.). A lot of things in the data science/ML space come from the computer science community, while multiple imputation is more from the statistics community, so people may just not be familiar with the approach. I'm not aware that anyone has done any investigation into MI + ML and shown that it helps a lot in some situations. I've done a little bit of simulation that actually suggests that the default method of imputation in xgboost is actually really hard to beat with MI, but I don't want to exaggerate the implications of a small simulation study. I suspect one promising situation could be one where you have censored observations where you e.g. only know that income is >10,000 USD/month (where it seems conceivable that MI could beat setting the value to 10,000 and having an extra flag for >10,000).
