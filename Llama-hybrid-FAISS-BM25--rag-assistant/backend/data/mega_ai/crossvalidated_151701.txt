[site]: crossvalidated
[post_id]: 151701
[parent_id]: 
[tags]: 
Why don't people use deeper RBFs or RBF in combination with MLP?

So when looking at Radial Basis Function Neural Networks, I've noticed that people only ever recommend the usage of 1 hidden layer, whereas with multilayer perceptron neural networks more layers is considered better. Given that RBF networks can be trained with version of back propagation is there any reasons why deeper RBF networks wouldn't work, or that an RBF layer couldn't be used as the penultimate or first layer in a deep MLP network? (I was thinking the penultimate layer so it could essentially be trained on the features learned by the previous MLP layers)
