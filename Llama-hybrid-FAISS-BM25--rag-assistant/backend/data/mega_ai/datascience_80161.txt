[site]: datascience
[post_id]: 80161
[parent_id]: 80157
[tags]: 
What are Bias and Variance? Let's start with some basic definitions: Bias : it's the difference between average predictions and true values. Variance : it's the variability of our predictions, i.e. how spread out your model predictions are. They can be understood from this image: ( source ) What to do about bias and variance? If your model suffers from a bias problem you should increase its power. For example, if the prediction of your neural network is not good enough, add more parameters, add a new layer making it deeper, etc. If your model suffers from a variance problem instead, the best possible solution is coming from ensembling. Ensembles of Machine Learning models can significantly reduce the variance in your predictions. The Bias-Variance tradeoff If your model is underfitting, you have a bias problem, and you should make it more powerful. Once you made it more powerful though, it will likely start overfitting, a phenomenon associated with high variance. For that reason, you must always find the right tradeoff between fighting the bias and the variance of your Machine Learning models. ( source ) Learning how to do that is more an art than a science!
