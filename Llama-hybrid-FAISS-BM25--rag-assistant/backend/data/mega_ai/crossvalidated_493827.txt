[site]: crossvalidated
[post_id]: 493827
[parent_id]: 
[tags]: 
Time series and first order difference correlation coefficient to adjust for autocorrelation

I have come across an adjustment method that uses the correlation coefficient and a first difference to adjust for autocorrelation in a times series. All data points transformed using the formula: $ y_{t}=\frac{x_{t} -\rho x_{t-1}}{1-\rho} $ where $\rho$ is the correlation coefficient between $x_{t}$ and $x_{t-1}$ Can anyone guide me to the theory behind this transformation method for autocorrelation/stationarity and how the $1-\rho$ enters the denominator ? Thank you Christian
