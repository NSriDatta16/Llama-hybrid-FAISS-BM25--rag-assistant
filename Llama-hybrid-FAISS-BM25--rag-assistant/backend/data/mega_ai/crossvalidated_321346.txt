[site]: crossvalidated
[post_id]: 321346
[parent_id]: 111486
[tags]: 
I believe that using bigrams may be a limiting choice in your case. Bigrams are a proxy to capture the context in your example and the fact that you are taking all the possible combinations will become problematic. There are various ways to deal with the issue, two of the most popular are: Using your data to obtain word embeddings. In particular, Global Vectors (Glove) are built on very similar arguments with yours. Here is a very nice post that explains the rationale. Use a topic model like Latent Dirichlet Allocation. In this case each of your files will be a document. LDA learns topics, that is multinomial distributions over the vocabulary of your corpus. In more simple words, topics are sets of words that are semantically similar (e.g., for a topic football the algorithm will assign high probability to words like goal, messi etc.). LDA uses exactly this assumption: words that co-occur often, belong to the same topic with high probability. In terms of visualization, people use different ways depending on the model: For embeddings, a dimensionality reduction method is used lile t-SNE and the word representations are shown in a 2-dimensional space. There you will observe that words that often co-occur are drawn close. For topic models, people tend to visualize the most probable words for some topics. To see an example, check this nice post for instance where I introduce topic models.
