[site]: crossvalidated
[post_id]: 206822
[parent_id]: 206807
[tags]: 
Altough these 3 approaches consists in dividing a dataset into several subsets, they are still different in the main purpose of this division. K-Fold Cross Validation (CV) It consists in dividing the original set of observations into k subset of more or less same size. Then, you will use one of the subset as test set and the remaining subsets will be used to form your training set. You will repeat this kth times, where each time the subset used as test set will change. As an example, if you use 3-fold CV, your original set will be divided into k1, k2, k3. First, k1 will form the test set, k2 and k3 will form the training set. Then, k2 will form the test set, k1 and k3 will form the training set. Finally, k3 will form the test set, k1 and k3 will form the training set. For each fold, you output the results and you aggregate these to obtain the final result. Bootstrap A bootstrap is a random subset of your original data, sometimes drawn with replacement (check http://www.stat.washington.edu/courses/stat527/s13/readings/EfronTibshirani_JASA_1997.pdf on the .632 rule), sometimes not. But the idea is that a bootstrap contains only a part of your whole set of observations. It is different from CV as it does not contain a testing set. Bootstrap is used to train a different classifier each time on a different set of observations. To output your results, a combination method is used, like averaging for example. Out-of-bag As said above, not all observations are used to form bootstrap. The part not used forms the out-of-bag classifier, and can be used to assess the error rate of your classifier. Out-of-bag are typically used to compute the error-rate, and not to train your classifier.
