[site]: crossvalidated
[post_id]: 153116
[parent_id]: 
[tags]: 
Standardization for regularized, sparse hashed logistic regression

As the question states, I'm fitting large, sparse logistic regressions (with hashed interactions, a la vowpal wabbit) for a machine learning system. The features are on different scales, and I'm a little nervous that dividing by the feature standard deviation really isn't suitable, because it seems that features that are mostly zero but are occasionally very large (I have a number of these) aren't scaled enough. Since I have a number of hash collisions, it seems that the large values might be effectively 'swamping' the larger ones. So, my question is: what is the advantage of scaling by the standard deviation (or root-mean-squares) instead of just dividing each feature by its maximum value, which would scale everything to the unit interval? Thanks for any help. (note: I can't center features as that would destroy the sparsity)
