[site]: crossvalidated
[post_id]: 19993
[parent_id]: 19902
[tags]: 
No, if the weights are bounded then there will be a bound on the log odds ratio going into the logit function, and hence a bound on the range of probabilities that the model can predict. However in practice to get a 1 from the logit function you don't need an infinite log odds ratio, because of numerical precision issues. I would imagine that positive weights is O.K., provided that the bias parameter is allowed to be negative. However, if you want to estimate probabilities, you want to use something like kernel logistic regression, or build the logit into the training procedure rather than just tacking it onto a trained RBF network, see this paper by Ian Nabney for details. Appologies if I have not understood the question properly.
