[site]: datascience
[post_id]: 77196
[parent_id]: 49581
[tags]: 
You’ve got the right idea with a k-d tree. The basic idea is that you don’t calculate an embedding for every face every time you do a query - you keep your database up to date and store the embeddings you have already calculated in your index. When you get a “query” (a new photo of a face) you generate the embedding for that and then search the index (possibly a k-d tree) for embeddings nearby that new embedding. You can also have more than one index, so that if the face is spotted in London you can first query the index that only covers faces likely to be seen in London. The most difficult part of your work is likely to be creating a good embedding, and you’d probably create this by generating a high-dimensional embedding with a neural network and then mapping this to a much smaller embedding using manifold learning (e.g. t-SNE or UMAP). This article is about a team using a similar approach: https://datascopeanalytics.com/blog/building-a-visual-search-algorithm/
