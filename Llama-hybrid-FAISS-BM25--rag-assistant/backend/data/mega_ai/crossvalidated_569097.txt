[site]: crossvalidated
[post_id]: 569097
[parent_id]: 264904
[tags]: 
One answer is that by definition if you're using past experiences that were obtained using an outdated policy, then your method is off policy. The question of why using experience replay is 'wrong' if you're using vanilla policy gradient i.e REINFORCE still remains though. The critical point is that Q-learning methods depend on an expectation that is independent of the policy itself i.e. it does not matter exactly how you found out what is the expected return of a particular series of actions, whether it was by accident or through exploration or by following a policy, it's useful and stable data (see notes for equations 2 and 3 in https://arxiv.org/abs/1509.02971 ). In contrast, the expectation of the gradient in REINFORCE is dependent on the policy, so if you use the datapoints from an outdated policy to calculate the gradient of the parameters, it simply does not represent the expectation of the gradients of the updated policy anymore e.g. if the optimal theta_1 is 0.5 and then initially it's set to 0.0 and your sampled gradient tells you to increase it by 0.5, you will make the policy actually worse if you increase it yet again by 0.5 to have it equal to 1. Where this difference shows itself though, is that with experience replay you cannot learn stochastic decision making. It's a trade-off between using fewer data points for converging to a possibly worse determinstic policy (a determinstic policy is a subset of a stochastic policy) against using more datapoints for obtaining a stochastic policy.
