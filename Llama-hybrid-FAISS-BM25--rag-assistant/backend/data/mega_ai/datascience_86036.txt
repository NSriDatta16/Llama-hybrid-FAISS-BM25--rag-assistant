[site]: datascience
[post_id]: 86036
[parent_id]: 35720
[tags]: 
The term normally used to refer to "MLP conv layers" nowadays is 1x1 convolutions . 1x1 convolutions are normal convolutions, but their kernel size is 1, that is they only act on one position (i.e. one pixel for images, one token for discrete data). This way, 1x1 convolutions are equivalent to applying a dense layer position-wise. The term "MLP convolutional layers" used in the network-in-network paper is a reference to this fact. While normal convolutions use the spatial information and therefore they can detect local patterns (spatial locality inductive bias), 1x1 convolutions do not, as their window of action is a single position. They are simply used to change the dimensionality of representations, specifically to change the number of channels in images, or to change the embedding dimensionality in discrete data. For instance, if at some point of a 2D convolutional network we have a tensor of width $w$ , height $h$ and $c$ channels, we can use a 1x1 convolution to obtain a tensor of width $w$ , height $h$ and $c'$ channels, where $c \neq c'$ .
