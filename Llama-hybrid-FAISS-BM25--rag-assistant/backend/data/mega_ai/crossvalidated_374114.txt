[site]: crossvalidated
[post_id]: 374114
[parent_id]: 374100
[tags]: 
Using a policy network and a value network seems related to the trick that dueling DQNs use. The simplest dueling DQN is a single network that "branches" near the final layers to compute advantage values $A(s,a)$ for each action, as well as value $V(s)$ for the current state. Explicitly separating the advantage for each action from the values for the current state has better training stability, faster convergence and better performance on the Atari benchmark ( Dueling Network Architectures for Deep Reinforcement Learning , Wang et al. 2015). This is because it makes explicit the two components of the $Q$ values: $$ Q(s,a) = V(s) + A(s,a) $$ The difference between AlphaZero and a dueling DQN appears to be that instead of using the same network with different branches to approximate $V(s)$ and $A(s,a)$ , a distinct value and policy network are used instead. This is just a parallel, but I think it gives a sense of why distinguishing different components of the problem can make learning a complex problem easier.
