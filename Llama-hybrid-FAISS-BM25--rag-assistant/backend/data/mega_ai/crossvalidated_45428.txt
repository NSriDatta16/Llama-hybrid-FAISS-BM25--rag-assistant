[site]: crossvalidated
[post_id]: 45428
[parent_id]: 
[tags]: 
Autoregressive model with exponential lags

I have a very highly sampled time series that I would like to fit an autoregressive model (AM) to (~3 million samples). From knowing what they represent, I have believe there should be unique information out to ~1k sample points in the past. Thus I'm considering modifying a more standard AM. This is what I'm calling the "standard" AM (from Wikipedia ): $$X_t = c + \sum_{i=1}^p \varphi_i X_{t-i} + \epsilon_t$$ The problem with this for my purposes is that to go out 1k samples require $p=1000$, which is a lot of parameters and I worry about over fitting. What I'd like to do is use exponentially spaced history points to do the fit: $$X_t = c + \sum_{i=0}^{p-1} \varphi_i X_{t-2^i} + \epsilon_t$$ So with $p=11$ I would be considering points up to 1024 samples in the past. My question is, is there something wrong with this approach? Are there frequencies that can fall in between the powers of two that I can't fit with this model? If it does work, why doesn't everyone do it like this?
