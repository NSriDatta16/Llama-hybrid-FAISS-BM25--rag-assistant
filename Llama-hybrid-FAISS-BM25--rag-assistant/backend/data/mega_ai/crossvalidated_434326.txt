[site]: crossvalidated
[post_id]: 434326
[parent_id]: 434322
[tags]: 
Cross-validation as an alternative in case of a lack of training data is quite an understatement. Unless your sample size is very large, validation performance can vary widely among different random splits. Cross-validation suffers less from this, since it considers the results from multitude folds. Even better would be to average over multiple runs of cross-validation, each with a different random split into $k$ folds. What you consider ordinary validation is really just cross-validation with a single fold. Examples of where you might intentionally want to use $k=1$ over multiple folds include: Your cannot afford $k>1$ computationally; You have, say, millions of records and can confidently split your data randomly; You are performing external validation and want to demonstrate that your model still performs well on data from a source never before seen by the model. In case of the latter, your model would likely generalize better if you include data from multiple sources for training (e.g. data from different institutes, studies, or data bases). However, if you use all sources for training, you still don't have a real estimate of the performance on a new source.
