[site]: crossvalidated
[post_id]: 342809
[parent_id]: 342759
[tags]: 
Is there a good explanation for how/why this misconception has spread? Is its origin known? We generally teach undergraduates a "simplified" version of statistics in many disciplines. I am in psychology, and when I try to tell undergraduates that p -values are "the probability of the data—or more extreme data—given that the null hypothesis is true," colleagues tell me that I am covering more detail than I need to cover. That I am making it more difficult than it has to be, etc. Since students in classes have such a wide range of comfort (or lack thereof) with statistics, instructors generally keep it simple: "We consider it to be a reliable finding if p p -value. I think this is where the explanation for why the misconception has spread. For instance, you can write the model as: $Y = \beta_0 + \beta_1X + \epsilon$ where $\epsilon \sim \text{N}(0, \sigma^2_\epsilon)$ This can be re-written as: $Y|X \sim \text{N}(\beta_0 + \beta_1X, \sigma^2_\epsilon)$ Which means that "Y, conditional on X, is normally distributed with a mean of the predicted values and some variance." This is difficult to explain, so as shorthand people might just say: "Y must be normally distributed." Or when it was explained to them originally, people misunderstood the conditional part—since it is, honestly, confusing. So in an effort to not make things terribly complicated, instructors just simplify what they are saying as to not overly confuse most students. And then people continue on in their statistical education or statistical practice with that misconception. I myself didn't fully understand the concept until I started doing Bayesian modeling in Stan, which requires you to write your assumptions in this way: model { vector[n_obs] yhat; for(i in 1:n_obs) { yhat[i] = beta[1] + beta[2] * x1[i] + beta[3] * x2[i]; } y ~ normal(yhat, sigma); } Also, in a lot of statistical packages with a GUI (looking at you, SPSS), it is easier to check if the marginal distribution is normally distributed (simple histogram) than it is to check if the residuals are normally distributed (run regression, save residuals, run histogram on those residuals). Thus, I think the misconception is mainly due to instructors trying to shave off details to keep students from getting confused, genuine—and understandable—confusion among people learning it the correct way, and both of these reinforced by ease of checking marginal normality in the most user-friendly statistical packages.
