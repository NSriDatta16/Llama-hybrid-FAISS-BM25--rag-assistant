[site]: datascience
[post_id]: 112774
[parent_id]: 96473
[tags]: 
I'm not at all sure this is the ideal solution, but it is an easy one: just use a regression tree (e.g. DecisionTreeRegressor from sklearn), training it with both the independent and dependent variables ( X and y ) being the questions' "pain scores". At each split point, the feature you split on will of course give excellent information about that target variable, but it will also inform about the other targets, which is what you suggest as the rough idea in your post. The split that gets chosen will be the one that gives the best information about all the questions' answers on average. This is, as Nuclear Hogie mentions in a comment, a greedily built solution. This is complicated a bit by your pain score being calculated from two separate scores, but I think splitting on and predicting pain scores may still be fine? Maybe the bigger issue is that you may split on the same question more than once, at different thresholds of pain. I suppose what you really want is to make more than a binary split, one child for every possible question response; the Quinlan family of trees (as opposed to CART) could do that.
