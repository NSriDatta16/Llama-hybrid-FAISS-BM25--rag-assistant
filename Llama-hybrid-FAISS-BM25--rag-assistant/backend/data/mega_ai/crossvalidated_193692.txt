[site]: crossvalidated
[post_id]: 193692
[parent_id]: 
[tags]: 
How to choose t-distribution degrees of freedom in "robust" Bayesian linear models

It is well known that in both frequentist and Bayesian linear models, outliers can greatly influence the parameter estimates. Consider the simple example where one outcome variable, $y$, is predicted by one independent variable, $x$. Under the conventional Bayesian linear model, $y$ would be normally distributed with mean $\beta_{0} + \beta_{1}x_i$ and variance $\sigma^2$. An outlier could skew the slope coefficient, $\beta_{1}$, for instance, making it look like there is a strong relationship between $y$ and $x$. One solution to this problem I've seen proposed by Krushke and others is to replace the normal distribution for $y$ with a student $t$ distribution, which has an additional parameter (degrees of freedom, sometimes denoted $v$) that increases the density of the tails and thus compensates for outliers. In his textbook, Krushke allows this parameter to be informed by the data, but it seems (based on his figures) that it is greatly influenced by the choice of prior. My question is, why not just set $v$ to $n-1$, as in a frequentist $t$ test?
