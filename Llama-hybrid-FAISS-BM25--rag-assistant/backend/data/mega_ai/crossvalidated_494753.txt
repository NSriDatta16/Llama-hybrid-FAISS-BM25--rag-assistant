[site]: crossvalidated
[post_id]: 494753
[parent_id]: 494629
[tags]: 
If you just do a "roll out" (technical terminology meaning that you just play the game / run the MDP forward) according to your policy $\pi_\theta$ for long enough, you'll be sampling states from $d^\pi$ . There are some mild technical conditions, but basically you'll always converge to the stationary distribution after enough time. However, usually people play games of finite episode length, and in that case there's a finite horizon policy gradients which is almost exactly the same thing but instead of sampling from $d^\pi$ , people just sample over the distribution of states from rollouts of finite length. And you can sample from that distribution the same way -- just play the game a bunch of times.
