[site]: crossvalidated
[post_id]: 146131
[parent_id]: 146121
[tags]: 
"Is it wrong to optimise a model not overfitting on the training set, but on the out-of-sample data?" It is only "wrong" if you assume that the out-of-sample still gives an unbiased performance estimation. It is quite common (at least in machine learning) to divide the available data into a training, a validation and a test set. The models are trained on the training set, the choice of model (and/or model hyper-parameters etc) is made using the validation set and the performance of the chosen model evaluated on the test set. This is O.K. as the test set is not used to optimize the model in any way. I was quite surprised when I found out just how susceptible models can be to this form of "over-fitting" in model selection/optimization and just how much this can give an optimistic bias to performance estimation if not done with care. I wrote a paper demonstrating how this issue arises in machine learning. G. C. Cawley and N. L. C. Talbot, Over-fitting in model selection and subsequent selection bias in performance evaluation, Journal of Machine Learning Research, 2010. Research, vol. 11, pp. 2079-2107, July 2010. ( www ) John Langford has a great blog post on " clever methods of over-fitting ", which is well worth reading, the question here would be a case of "Parameter tweak overfitting". "Human-loop overfitting." is another form of over-fitting that we definitely need to be aware of!
