[site]: crossvalidated
[post_id]: 80620
[parent_id]: 80611
[tags]: 
For logistic Regression, our hypothesis is: $$ h_\theta (X) = p = \frac{1}{1 - \exp(-\beta.X)} $$ now simplifying it: $$ 1 - \exp(-\beta.X) = \frac{1}{p} $$ $$ 1 - \frac{1}{p} = \exp(-\beta.X)$$ taking log on both sides, and simplifying it, $$ \ln{\frac{p}{1-p}} = \beta.X $$ now, if we look at this final equation, the LHS of it is logit fuction of $p$, and RHS is the dot product of two vectors, its expansion will look like: $$ \beta.X = \beta_0 + \beta_1.X_1 + \beta_2.X_2 + ... $$ parameters $ \beta_{n * 1} $ and input features $ X_{1 * n} $. Since, the parameters $ \beta $ are learned due to our learning algorithms and are constant, and there is involvement of input features in first degree only (power 1) e.g. terms like $X_1^2, X_1.X_2^2$ do not appear, and no mixed features terms like $X_1.X_2$, the logit of our hypothesis/or probability is linear function/interpretation of input features $ X$.
