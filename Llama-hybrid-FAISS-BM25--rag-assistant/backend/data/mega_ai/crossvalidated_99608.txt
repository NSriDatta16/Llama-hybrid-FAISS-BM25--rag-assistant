[site]: crossvalidated
[post_id]: 99608
[parent_id]: 
[tags]: 
What theory can explain why after several rounds of bootstrapping, the result come to converge?

When I use bootstrapping (updating the negative samples generated by existed classifier model and then retraining classifier model for next round; And do this for several rounds) in machine learning, I find that after several round (maybe 10 rounds), the result come to converge and stable even if the classifier model still can generate some negative sample. What theory can explain this theory that bootstrapping will come to converge and the termination existed? Is there a margin of it? Thanks.
