[site]: crossvalidated
[post_id]: 607012
[parent_id]: 604321
[tags]: 
no, by doing that you change the state distribution, which causes a ton of problems, like the change in value estimation... which means that if you don't prioritize, your policy might learn states on which it might never see again, because it already knows that they are very bad the IS is the correction the agent will learn the Q value for the starting and ending state, thus the update will pretty much produce a 0 TD error, causing no issue, and the new priority will be close to 0
