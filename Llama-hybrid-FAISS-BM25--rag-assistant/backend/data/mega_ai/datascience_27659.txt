[site]: datascience
[post_id]: 27659
[parent_id]: 27656
[tags]: 
Yes. This is how a face recognition algorithm might work for example, where two pictures might be of the same person or different person. To build such a system, just pair up your data in a training set, double the input vector space and run a binary classifier that outputs "true" if the two items are the same. Any ML classifier could be adapted to this problem. In practice, you may want more control over classification, and be robust against new classes that your algorithm has not seen before. A big problem in face recognition is the large number of potential classes, including classes not seen in training data, coupled with an equally large variance of images in the wild. This means that a naive approach as above will have poor performance in practice. There is a more sophisticated approach: Triplet Loss . This requires that you train with three inputs for each example. Unlike the naive version: The input is one image at a time. You train in triplets - an "anchor", and a postive match, and a negative match, in order to calculate one loss value for backprop. The output is not a same/different class, but a vector description of the object. Similarity of objects is the distance between vectors. You do not need a label for this vector, even though this is supervised learning, thanks to how the loss function works. The loss function is based on difference of distance from anchor example to positive example compared with a desired higher distance to negative example. This encourages learning key features of the inputs by making distance between vectors low (ideally zero) when they represent the same class, and as high as possible when they represent different classes. In both the naive approach and triplet loss approach, you need to be careful about selecting training data. You want to make the learning algorithm work hard to learn key differences, otherwise it is too easy to get a good loss. So taking MNIST as an example, when looking at negative matches, you want to pair up more (1, 7), (3, 2), (3, 8), (4, 9) etc pairs than (0,1), (9, 5). There are ways to drive this selection based on feedback from previous training epoch.
