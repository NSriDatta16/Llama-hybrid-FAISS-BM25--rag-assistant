[site]: crossvalidated
[post_id]: 314207
[parent_id]: 
[tags]: 
Proof help: Coincidences in higher dimensions

Background I recently watched a 2014 Talk by Geoffrey Hinton (a key researcher in Machine Learning literature) where he discusses the concepts behind the recently published Capsule Networks . In the talk, he makes the general statement that coincidences are less likely to occur in higher dimensions . Intuitively I can understand that this is a reasonable statement, but I wanted to write a proof to convince myself. Numeric Proof (Python script) First I wrote a python script to numerically prove this. """ test_highdim.py Hinton says in a 2014 MIT presentation that 'conincidences are less likely to occur in higher dimensional spaces'. This script tests this hypothesis. """ import numpy as np def main(): # Gist of the numerical proof: # epsilon = some float # # Start with m=1, M>>m # While loop: # Generate a random point in R^m # If this point is closer than epsilon to any other previous point, break # Print how many times we looped # Increase m to a higher dimension and go again until m=M log_file = "test_highdim.csv" open(log_file, 'w').close() epsilon = 0.5 max = 1 min = 0 M = 400 # What norm to use when finding distances (L1=1, L2=2 etc.) norm_ordinance = 2 for m in range(1, M): # Run at each dimension (m) a few times to get an average measure average_n = 0.0 num_runs = 100 for j in range(num_runs): # Create a new empty array of R^m points n = 0 pts = np.empty(shape=(0, m), dtype=float) coincidence_occured = False # Loop while True: # Pick a new random uniform point in R^m r = np.random.uniform(low=min, high=max, size=m) # See if this point is close to any previous point for i in range(pts.shape[0]): d = np.linalg.norm(pts[i, :], ord=norm_ordinance) # If so, we got a coincidence if d As expected, when I graph the output with $m$ (the dimension) on the x axis, I see some kind of stochastic exponential growth in the number of iterations it takes ($n$) to find a coincidence. I want to find a closed form probabilistic expression for that relationship. My work-in-progress proof is below. Proof attempt The gist of my proof is as follows; Sampling uniform random points in $\mathbb{R}^{m}$ is the same as sampling $n \over m$ uniform randoms in $\mathbb{R}^m$, provided each dimension is independent. I consider that a coincidence has ocurred if two random points are close together. I then try and find a probability expression for how likely this is to occur after $n$ random draws. More formally; Consider a random sample $(U_1, ... U_n)$ of i.i.d. random uniform draws on the range $[0, 1]$. In this situation $n$ draws can be considered identical to $n \over m$ draws from multinomial uniform random variables in $\mathbb{R}^m$ as each dimension's draw is independent. Let $U_{(1)}, ... U_{(n)}$ denote the order statistics of the sample such that $U_{(1)} = min(U_i)$ and $U_{(n)} = max(U_i)$. Wikipedia's Order Statistic page then tells me that $U_{(k)} \sim Beta(k, n+k-1)$ for $k = 1, ... n$. If $\epsilon$ is some small value, I then state that a coincidence occurs when the expected values of $m$ order statistics are less than $\epsilon$ apart, which I believe is equivalent to using the L1 norm or 'Manhattan distance' in $\mathbb{R}^m$. $$ E[U_{k+m)}] - E[U_{k}] \leq \epsilon;\quad \begin{aligned} k &= 1, ... n-m,\\ n &> m \end{aligned} $$ Using the expected value of a beta distribution , I then simplify; $$ \begin{aligned} \frac{k + m}{n+1} - \frac{k}{n+1} &\leq \epsilon \\ \\ \frac{k + m - k}{n + 1} &\leq \epsilon \\ \\ n &\ge \frac{m}{\epsilon}-1 \end{aligned} $$ When I re-run my numeric solution using the L1 norm, it doesn't match the values predicted by my proof at all (more than I would expect from stochastic noise). Questions Is my proof correct if I use the L1 norm? How would I generalize this to use any norm? I feel like this result should have already been proved, but I don't know what to search for to find a proof. What terms should I be looking for? This is my first attempt at writing a proof ever. Please be gracious with your feedback :)
