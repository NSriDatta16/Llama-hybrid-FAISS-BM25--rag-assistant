[site]: crossvalidated
[post_id]: 525201
[parent_id]: 
[tags]: 
Compute inter-rater reliability for categorical ratings with variable raters per target

What is the correct statistic to use for computing inter-rater agreement from data of the following form? data = pd.DataFrame({ "rater" : [1, 1, 1, 2, 3, 2, 1, 2, 1, 3], "target" : ["a", "b", "c", "a", "a", "c", "d", "b", "f", "g"], "rating" : [1, 1, -1, 0, 0, 1, 1, 1, 1, -1] }) I've been advised to try Cohen's Kappa, but it seems this requires that each target have the same number of raters. In this case, the number of raters per target can be one or more. I recognize one solution is ICC, but implementations of ICC (eg. pingouin.intraclass_corr) only provides coefficients indicating the reliability of individual raters or the average of raters ( Shrout and Fleiss, 1979 ). But since the the data are nominal I want to take the mode when reviewers disagree, not the mean. Note that these are toy data. The actual data have 241 unique targets and between 1 and 8 raters (only rating options are -1, 1, and 0). Thanks in advance!
