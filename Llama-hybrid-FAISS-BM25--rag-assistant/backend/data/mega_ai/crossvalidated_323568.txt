[site]: crossvalidated
[post_id]: 323568
[parent_id]: 
[tags]: 
Help Understanding Reconstruction Loss In Variational Autoencoder

The reconstruction loss for a VAE (see, for example equation 20.77 in The Deep Learning Book ) is often written as $-\mathbb{E}_{z\sim{q(z | x)}} log(p_{model}(x | z))$, where $z$ represents latent variables, $x$ represents an image, and $q$ is an approximate encoder. If we assume that $x | z \sim \mathcal{N}(\mu_{x|z}, \Sigma_{x|z})$, then I understand how that minimizing reconstruction loss is just a typical maximum likelihood problem. But when I see VAEs implemented in practice, people often seem to represent the reconstruction loss as the L2 loss between each $x$ (training image) and $\hat{x}$ (decoded image). Can anyone explain how to translate the reconstruction loss in the first expression above to L2 loss between $x$ and $\hat{x}$?
