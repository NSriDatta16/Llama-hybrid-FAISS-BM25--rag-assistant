[site]: crossvalidated
[post_id]: 81487
[parent_id]: 71221
[tags]: 
@ahfoss already pointed you to LDA as the classification analogon to PCA. Actually, these two methods are related to each other and also to PLS: nature of dependent variable (for supervised) unsupervised supervised or structure of data (unsupervised) continuous PCA PLS factor/groups/classes LDA The relation is in the projection: PCA projects data so that the variance-covariance matrix of the scores will be $\mathbf I$. LDA does a similar projection, but instead of the variance-covariance matrix of the whole data set, the pooled within-class variance-covariance matrix becomes $\mathbf I$. As @ahfoss also said, the usual LDA is not feasible for $n \ll p$. But PLS-LDA is a feasible way of dealing with that, see e.g. Barker, M. & Rayens, W.: Partial least squares for discrimination, J Chemom, 17, 166-173 (2003). DOI: 10.1002/cem.785 (this paper also discusses the relation between PLS and LDA). PLS can be seen as a regularization like the LASSO, and also sparse PLS is available (though I haven't used it: my data is more suitable to normal PLS, which doesn't assume sparsity). For a nice discussion of different regularization methods, see e.g. the Elements of Statistical Learning . One nice property of doing PLS-LDA is that you can write the final model in bilinear form if you take care to use the same centering for both steps. This allows to interpret the model just the same way you'd interpret the LDA model that you could calculate if you had more cases and $n \gg p$. (In case you're forced to do PCA, this applies for PCA-LDA as well). Say, PLS scores $\mathbf T = $ data matrix $\mathbf X \times$ PLS weights $\mathbf W$ and for a usual LDA, the scores $\mathbf L = $ data matrix $\mathbf X\times$ LDA coefficients $\mathbf B$. Then doing the LDA in PLS (X-)score space, we get: $\mathbf L'^{(n \times k - 1)} = \mathbf T^{(n \times m)} \mathbf B'^{(m \times k - 1)}$ $\phantom{\mathbf L^{(n \times k - 1)}} = \mathbf X^{(n \times p)} \mathbf W^{(p \times m)} \mathbf B'^{(m \times k - 1)}$ $\phantom{\mathbf L^{(n \times k - 1)}} = \mathbf X^{(n \times p)} \mathbf B''^{(p \times k - 1)}$ The dashes mark that these LDA scores ($\mathbf L'$) may be (slightly) different from LDA scores you'd obtain without regularization, and so are the coefficients $\mathbf B''$. $\mathbf B'$ will be very different from $\mathbf B$ as they apply to PLS score space and not to the original data space. Practical note: in case you work in R, I have a package under development that provides PLS-LDA and PCA-LDA models. Let me know if you'd like to give it a try. In order to avoid data dredging, you need to validate your final model (= measure its performance) with independent data. Independent here means that this case (patient?) did not contribute to the model fitting in any way. In particular, did not enter any kind of preprocessing that involves multiple cases, such as centering or standardization did not enter the PCA/PLS/... calculation. was not used for hyperparameter estimation. As you have only few cases, a resampling strategy would be appropriate. In this situation, it is probably best to fix any hyperparameters (like number of PCs or PLS latent variables, or the LASSO bound) by external knowledge in order to avoid a second, inner split of your training data for the optimization of the hyperparameter.
