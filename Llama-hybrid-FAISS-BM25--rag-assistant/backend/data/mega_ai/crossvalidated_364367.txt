[site]: crossvalidated
[post_id]: 364367
[parent_id]: 363600
[tags]: 
The best way to model this is using Recurrent Neural Networks (Seq2Seq models). There are fair amount of tutorials available online. https://github.com/cmusphinx/g2p-seq2seq https://aws.amazon.com/blogs/machine-learning/create-a-word-pronunciation-sequence-to-sequence-model-using-amazon-sagemaker/ Edit - General classification models treat rows/records independently. So when you train a simple classifier, it is going to predict pronunciation of the whole at based on only current feature set rather than based on previous state (which is what you want). If you want a normal classifier to predict sequence then you need either add the previous state in feature set (difficult when the size changes dynamically) or change the model architecture to add memory (which is a better solution). The architecture/design of RNNs is created to consider sequential input(or memory). RNN and its variants are widely used for sequential predictions like predicting next word or sentence generation. how do they work - https://towardsdatascience.com/learn-how-recurrent-neural-networks-work-84e975feaaf7
