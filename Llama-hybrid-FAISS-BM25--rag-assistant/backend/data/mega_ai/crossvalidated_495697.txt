[site]: crossvalidated
[post_id]: 495697
[parent_id]: 494648
[tags]: 
It is important that you understand how the OOB predictions are made.Each tree in the RF is trained on a bootstrap resample of the training data, and on average 1/3 of the original training data is not used in training each tree (not the same 1/3 for all trees). For each training example, the OBB prediction for that data is the result of passing that data only to the trees that did not use it in the training, and compounding the result from each tree into a final decision (in RF the final decision is done by voting for classification and averaging for regression - I am 905 sure of the regression statement, and 100% sure of the classification statement). The whole point of the OOB is to have an estimate of the error for unknown data. But that is only an estimate, since not all trees in the RF are contributing to the decision. It should be used only if you do not have a test set that is different than the training set. It could also be used to select the hyperparameters to the RF. But again this is only useful if either you do not have a separate training set, or you have very strict time issues - the OOB is calculates in the training, and thus you do not have to run a test phase - but testing is usually very fast in RF in comparison to training. predict(model) is the way to retrieve the OOB results only from the R package RandomForest . Another frequently used random forest package in R is ranger which has a different interface. Now you should realize that the OOB predictions are constructed during the training of the RF, and that it way it is an atribute of the model object. predict(model)[1] is the prediction for the first OOB example (which I think is the first example in the training set) but as discussed above, using only the trees that did not include that example in their training. predict(model,newdata=training.data)[1] is the prediction for the prediction for the first data in newdata, which in this case happens to be the training data. Which one should I consider? does not make much sense - they report very different things (none of which is much useful if you have a real separated test set). 10% error in the OOB against 0% error in the training set seems to me an example of overfitting. 0% training error is already indicative of overfitting - your RF learned too much from the training set. Since OOB error is an estimate of the error for unseen data, you should expect something similar to 10% as test error.
