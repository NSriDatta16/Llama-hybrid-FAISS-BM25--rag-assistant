[site]: crossvalidated
[post_id]: 623199
[parent_id]: 592159
[tags]: 
I think in the end your goal is to have inference for binary data resembling logistic regression where the variance of various model quantities (parameter estimates, predictions, etc.) does not depend on the probabilities in the model, hence your search for variance-stabilizing transformations. If that's your desire, you should not transform logistic regression itself. Instead, you should use a different link function other than the logistic link function. Generalized linear models connect the mean of the response variable to the model factors via: $$g(p)=x^\top \beta,$$ where $x$ is your vector of factors, $\beta$ the linear model coefficients, $p$ is the mean of the response variable (in this case, a probability), and $g$ is a link function. In logistic regression, $g$ is logistic function, but you are not required to use that link function. You could instead choose a link function that is variance stabilizing. McCullagh and Nelder (1989) mention on pg. 43 that one can make the weights of the Fisher information matrix constant by choosing a link function such that $$g'(p)=V^{-1/2}(p)=(p(1-p))^{-1/2}.$$ The angular link function $g(p)=\arcsin(p^{1/2})$ has this property; notice that this is the variance stabilizing transformation for binomial models, in general. The downside of this link function is that unlike the logistic link function, it is not an injective and surjective mapping from $(0,1)$ to $\mathbb{R}$ , but it does eliminate dependence on variances at each point, which can be important in applications. So in short, if you want variance stabilization, use a variance-stabilizing link function instead of the logit link function used in logistic regression. McCullagh, P., Nelder, J. (1989). Generalized Linear Models, Second Edition. Chapman & Hall. ISBN: 9780412317606
