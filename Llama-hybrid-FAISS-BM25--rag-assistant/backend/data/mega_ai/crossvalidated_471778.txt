[site]: crossvalidated
[post_id]: 471778
[parent_id]: 471726
[tags]: 
The description in the second paragraph of OP's question describes the phenomenon where regression coefficients cannot be uniquely determined when the design matrix is not full rank. To construct a model, we need to decide the parameters of models and the number of parameters is proportional to the number of predictors. And for the wide data, we don't have enough data to decide all the parameters reliably. I guess with the wide data, the parameters of the model will change all the time with the small change of data. There would not be any stable solution for the model. And the instability indicates that there would be large model variance which will worsen the prediction performance. The answer to this is two parts. Not all machine learning models involve estimating a coefficient vector for a matrix-vector product. For example, random forest can find a best binary split for some data even when $n \ll p$ because finding a split doesn't involve solving a linear system. For machine learning models that do involve a matrix-vector product (e.g. OLS or logistic regression), the addition of a penalty term can make the optimization problem strongly convex, and therefore identify a unique minimum of the loss function. See: Why does ridge estimate become better than OLS by adding a constant to the diagonal? Three common examples of penalized regression are ridge-regression , lasso regression, and elastic-net regression. This penalty is a form of regularization, because it limits the flexibility of the model. The other answers are correct that regularization is why machine learning models can do well in terms of prediction when $n \ll p$ , but they don't quite connect that concept to the rank deficiency component of your question.
