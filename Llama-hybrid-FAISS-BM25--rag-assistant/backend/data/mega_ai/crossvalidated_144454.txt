[site]: crossvalidated
[post_id]: 144454
[parent_id]: 
[tags]: 
Validating Neural Network

I have a relatively large set of labelled data which I am using to verify how well-trained a neural-network I wrote is. The network uses a competitive learning technique. It has N output neurons, each time a sample is fed into it this is labelled as belonging to the class corresponding to the network that returned the highest output. The end-result of my verification is an NxM matrix A. Each row in A is an output neuron and each column is a class. For example, say: A = [ 1 2 3 ] [ 7 0 9 ] [ 3 4 7 ] This translates to: 1 sample belonging to class 1, 2 samples belonging to class 2, 3 samples belonging to class 3 have been classified as belonging to cluster 1. (Output neuron 1 returned the highest value) 7 samples belonging to class 1, 0 samples belonging to class 2, 3 samples belonging to class 3 have been classified as belonging to cluster 2. etc. Now, as my network's learning strategy improves (I am testing different learning algorithm), I expect the heterogeneity of the rows to decrease. That is, each output neuron should predominantly fire at one class of inputs, so moving to something like A = [ 0 0 5 ] [ 14 0 0] [ 0 14 0] (Perhaps not this well-defined, but I hope you get the idea.) My question is: Is there any way to formally prove this? I.e.: given two matrices A and A', is there a way to show that rows tend to converge to including mostly samples from one class?
