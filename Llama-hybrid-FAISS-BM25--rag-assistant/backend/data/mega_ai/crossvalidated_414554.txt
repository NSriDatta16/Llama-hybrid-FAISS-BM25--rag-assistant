[site]: crossvalidated
[post_id]: 414554
[parent_id]: 
[tags]: 
Why can logistic regression always outperform naive Bayes for every conditional distribution?

We know that the generative model assumes that $X_i \perp X_{-i}| Y$ ; while the discriminative model assumes that $p(Y=1|x; \alpha)=\frac{e^{\alpha_0+\sum_{i=1}^n\alpha_ix_i}}{1+e^{\alpha_0+\sum_{i=1}^n\alpha_ix_i}}=\frac{1}{1+e^{-\alpha_0-\sum_{i=1}^n\alpha_ix_i}}$ . And it(slide 19) says that the later can be derived from(as below, and more information can be found here ) the former because if $p(x_i|y)\sim \mathscr{N}(\mu_{iy}, \sigma_i)$ and $p(y)\sim Bernoulli(\pi)$ the P(y|x_1,...,x_n) would have a logistic form. \begin{align} p(y = 1 \mid \mathbf{x}) &= \frac{p(\mathbf{x} \mid y = 1) p(y = 1)}{p(\mathbf{x} \mid y = 1) p(y = 1) + p(\mathbf{x} \mid y = 0) p(y = 0)} \\ &= \frac{1}{1 + \frac{p(\mathbf{x} \mid y = 0) p(y = 0)}{p(\mathbf{x} \mid y = 1) p(y = 1)}} \\ &= \frac{1}{1 + \exp\left( -\log\frac{p(\mathbf{x} \mid y = 1) p(y = 1)}{p(\mathbf{x} \mid y = 0) p(y = 0)} \right)} \\ &= \sigma\left( \sum_i \log \frac{p(x_i \mid y = 1)}{p(x_i \mid y = 0)} + \log \frac{p(y = 1)}{p(y = 0)} \right) \end{align} It seems that I can truly obtain a logistic form but I don't understand why we can claim the following: that every conditional distribution that can be represented using naive Bayes can also be represented using the logistic model What are the other conditional distributions? Do I understand it right that every conditional distribution implies some other conditional distributions except $p(y|x)$ ? In On Discriminative vs. Generative classifiers: A comparison of logistic regression and naive Bayes , Ng said that given sufficient data logistic regression would be at least as good as Naive Bayes. Could we see that from the formulas? Is it because of these two assumptions: $p(x_i|y)\sim \mathscr{N}(\mu_{iy}, \sigma_i)$ and $p(y)\sim Bernoulli(\pi)$ ? But why these two assumptions are necessary?
