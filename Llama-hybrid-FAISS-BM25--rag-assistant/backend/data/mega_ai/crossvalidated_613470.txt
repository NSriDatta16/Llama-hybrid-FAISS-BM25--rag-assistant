[site]: crossvalidated
[post_id]: 613470
[parent_id]: 613468
[tags]: 
You should not use the output from BFGS as the true inverse of the hessian. The reason for this is that BFGS does not approximate the hessian based on the current rate of change of the derivatives of the target function. Rather, it builds up a useful approximation by tracking the change of the first derivative at each state of the optimization algorithm. I believe it also uses some regularization to keep the step sizes in check. Note that without regularization, you can't have an invertable Hessian based on solely first order differences of the gradient if you've got less steps than parameters of your model. This means, at best, you have something like the inverse of the weighted average of the changes of the first derivative through the different iterations of your algorithm, which may be very different than the inverse of the Hessian at the MLE. At worst, you have a matrix that's heavily regularized as to make sure that your next step isn't too large, where the regularization has nothing to do with the uncertainty of the parameters of your model. Wikipedia cautions about this issue, but I think they understate it: In statistical estimation problems (such as maximum likelihood or Bayesian inference), credible intervals or confidence intervals for the solution can be estimated from the inverse of the final Hessian matrix[citation needed]. However, these quantities are technically defined by the true Hessian matrix, and the BFGS approximation may not converge to the true Hessian matrix
