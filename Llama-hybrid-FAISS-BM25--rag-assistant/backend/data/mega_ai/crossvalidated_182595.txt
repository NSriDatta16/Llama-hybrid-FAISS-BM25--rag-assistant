[site]: crossvalidated
[post_id]: 182595
[parent_id]: 
[tags]: 
How to interpret all zero coefficients in the results of cv.glmnet?

I'm trying to use sparse linear model for my data,input x(29*50),output y(29*1). In R, the package of glmnet can be used. Firstly, cv.glmnet() choose lambda and coefficients(at min error), here with leave-one-out cv method,and then plot it. cv.fit = cv.glmnet(x,y,family="gaussian",nfolds=29) plot(cv.fit) the plot of mse aganist log(lambda) in cv model Next, print the coefficients coef(cv.fit,s="lambda.min") 51 x 1 sparse Matrix of class "dgCMatrix" 1 (Intercept) 267.7241 cluster_0 . cluster_1 . cluster_2 . cluster_3 . cluster_4 . ... cluster_47 . cluster_48 . cluster_49 . Finally, to measure the model's ability for prediction, accuracy is calculated(defined as 1 minus average absolute error divided by numeric range of y) py V1 267.7241 V2 267.7241 ... v29 267.7241 ave_abs_error 0.918365 Although the acc(0.918365) is very high, there is a serious problem. As seen from the plot above, the lambda.min is very large(73.03439),and all coefficients are zero(only with intercept value 267.7241), all predicted py are the same as intercept. That's really weird! I searched lots of threads in forum, here An example: LASSO regression using glmnet for binary outcome explains that there is no local min for too few observations and all coefficients were shrunk to zero with the shrinkage penalties. Does anybody has other interpretations? Thanks in advance!
