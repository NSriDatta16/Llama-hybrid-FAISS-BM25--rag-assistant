[site]: crossvalidated
[post_id]: 158096
[parent_id]: 158092
[tags]: 
Easy question: Yes you are correct. A single sigmoid neuron output is exactly what you want for this. Could also use any other 0,1 bounded neuron: eg tanh . Like your choice of sigmoid vs tanh, your loss (ie error, ie cost) function doens't really matter. I'ld use cross-entropy, but mean squared world work fine -- any differences are going to vanish over a few epochs of training. I like to initialise with small normally distributed values, generaly mean zero, varience 0.01 (Its often with testing dropping or raising that varience by an order of magnitude). But once again, small uniformly distributed also works fine. A futhur recommendation is that you blanance the number of positive and negative examples. Otherwise your network will learn the prior (that things are for example 2x more likely to have as not to have.). You should take a good look through Yann Lecun's "Effient Back-propergation" . It's written by the creator of the convoltional neural network.
