[site]: crossvalidated
[post_id]: 403371
[parent_id]: 401239
[tags]: 
(This isn't a real answer, because we'd probably need more details as Kjetil asked for, but slightly beyond the scale of a comment.) One thing to keep in mind is that the whole idea of fractional factorial designs is predicated on linear or at least additive models, while many ML models won't satisfy this assumption. I'm sure it wouldn't be hard to come up with an example where a traditional fractional factorial design would be useless for some concept that an equal number of random query points would be fine. Machine learning people have worked a fair amount on an area called active learning , closely related to optimal experimental design. Here's one such scheme (a simple version of disagreement-based active learning ), though there are many options: Fit a bunch of models on the data you have so far. Identify the data point that the set of models you have disagree on the most, so that learning the label of that point would allow you to pin down the candidate models the most. Go get the true label for that data point from the real world. Update the models and repeat. This scheme works more or less well than various others in different situations, which will depend a lot on the kind of models you're using, as well as your budget for getting more labels, whether you can go back and forth many times or if you just need to do it in a few batches, etc.
