-{\frac {1}{m}}\langle 1,\triangledown _{y_{i}}L\rangle ^{2}-{\frac {1}{m}}\langle \triangledown _{y_{i}}L,{\hat {y}}_{j}\rangle ^{2}{\bigg )}} . Since the gradient magnitude represents the Lipschitzness of the loss, this relationship indicates that a batch normalized network could achieve greater Lipschitzness comparatively. Notice that the bound gets tighter when the gradient ▽ y i L ^ {\displaystyle \triangledown _{y_{i}}{\hat {L}}} correlates with the activation y i ^ {\displaystyle {\hat {y_{i}}}} , which is a common phenomena. The scaling of γ 2 σ j 2 {\displaystyle {\frac {\gamma ^{2}}{\sigma _{j}^{2}}}} is also significant, since the variance is often large. Secondly, the quadratic form of the loss Hessian with respect to activation in the gradient direction can be bounded as ( ▽ y j L ^ ) T ∂ L ^ ∂ y j ∂ y j ( ▽ y j L ^ ) ≤ γ 2 σ 2 ( ∂ L ^ ∂ y j ) T ( ∂ L ∂ y j ∂ y j ) ( ∂ L ^ ∂ y j ) − γ m σ 2 ⟨ ▽ y j L , y j ^ ⟩ | | ∂ L ^ ∂ y j | | 2 {\displaystyle (\triangledown _{y_{j}}{\hat {L}})^{T}{\frac {\partial {\hat {L}}}{\partial y_{j}\partial y_{j}}}(\triangledown _{y_{j}}{\hat {L}})\leq {\frac {\gamma ^{2}}{\sigma ^{2}}}{\bigg (}{\frac {\partial {\hat {L}}}{\partial y_{j}}}{\bigg )}^{T}{\bigg (}{\frac {\partial L}{\partial y_{j}\partial y_{j}}}{\bigg )}{\bigg (}{\frac {\partial {\hat {L}}}{\partial y_{j}}}{\bigg )}-{\frac {\gamma }{m\sigma ^{2}}}\langle \triangledown _{y_{j}}L,{\hat {y_{j}}}\rangle {\bigg |}{\bigg |}{\frac {\partial {\hat {L}}}{\partial y_{j}}}{\bigg |}{\bigg |}^{2}} . The scaling of γ 2 σ j 2 {\displaystyle {\frac {\gamma ^{2}}{\sigma _{j}^{2}}}} indicates that the loss Hessian is resilient to the mini-batch variance, whereas the second term on the right hand side suggests that it becomes smoother when the Hessian and the inner product are non-negative. If the loss is locally convex, then the Hessian is positive semi-definite, while the inner product is positive if g j ^ {\displaystyle {\hat {g_{j}}}} is in the direction towards the minimum of the loss. It could thus be concluded from this inequality that the gradient generally becomes more predictive with the batch normalization layer. It then follows to translate the bounds related to the loss with respect to the normalized activation to a bound on the loss with respect to the network weights: g j ^ ≤ γ 2 σ j 2 ( g j 2 − m μ g j 2 − λ 2 ⟨ ▽ y j L , y ^ j ⟩ 2 ) {\displaystyle {\hat {g_{j}}}\leq {\frac {\gamma ^{2}}{\sigma _{j}^{2}}}(g_{j}^{2}-m\mu _{g_{j}}^{2}-\lambda ^{2}\langle \triangledown _{y_{j}}L,{\hat {y}}_{j}\rangle ^{2})} , where g j = m a x | | X | | ≤ λ | | ▽ W L | | 2 {\displaystyle g_{j}=max_{||X||\leq \lambda }||\triangledown _{W}L||^{2}} and g ^ j = m a x | | X | | ≤ λ | | ▽ W L ^ | | 2 {\displaystyle {\hat {g}}_{j}=max_{||X||\leq \lambda }||\triangledown _{W}{\hat {L}}||^{2}} . In addition to the smoother landscape, it is further shown that batch normalization could result in a better initialization with the following inequality: | | W 0 − W ^ ∗ | | 2 ≤ | | W 0 − W ∗ | | 2 − 1 | | W ∗ | | 2 ( | | W ∗ | | 2 − ⟨ W ∗ , W 0 ⟩ ) 2 {\displaystyle ||W_{0}-{\hat {W}}^{*}||^{2}\leq ||W_{0}-W^{*}||^{2}-{\frac {1}{||W^{*}||^{2}}}(||W^{*}||^{2}-\langle W^{*},W_{0}\rangle )^{2}} , where W ∗ {\displaystyle W^{*}} and W ^ ∗ {\displaystyle {\hat {W}}^{*}} are the local optimal weights for the two networks, respectively. Some scholars argue that the above analysis cannot fully capture the performance of batch normalization, because the proof only concerns the largest eigenvalue, or equivalently, one direction in the landscape at all points. It is suggested that the complete eigenspectrum needs to be taken into account to make a conclusive analysis. Measure Since it is hypothesized that batch normalization layers could reduce internal covariate shift, an experiment is set up to measure quantitatively how much covariate shift is reduced. First, the notion of internal covariate shift needs to be defined mathematically. Specifically, to quantify the adjust