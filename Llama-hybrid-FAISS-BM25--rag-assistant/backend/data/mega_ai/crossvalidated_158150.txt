[site]: crossvalidated
[post_id]: 158150
[parent_id]: 158149
[tags]: 
Yes, you can use for example linear output layer which will return some real value by just summing the inputs. You could encode all of your classes like 0 - dog, 1 - cat, 2 - bird etc. and get an out put of, say 1.75, which you will need to round to understand what is your real class label. But I strongly recommend to avoid this - the performance will be horrible. Also you could just have the structure with multiple neurons (each one is for one specific class) and just add one neuron with argmax() function (which will just return the number of the previous neuron with biggest output). It will also work. But I never heard that anyone has problems with the multiple output neurons, why you don't want to use them? Never heard about such rule, honestly. I could just say that there is a problem with the vanishing gradients in back prop, which means that when you use backprop to train a network with really big amount of layers the weights which are far away from output just don't get any significant updates on each iteration. That means that only several layers close to output layers are actually trained by backprod. There are ways to fight this issue, actually (which are studied within Deep learning domain - LSTM, multi-level hierarachy, usage of pretraining with RBM etc) I don't know where you've got this quote. It is just not true - deep learning domain could demonstrate cases where you have, say, 20 layers, and they could have different types (some of them convolutional, some are max-pooling layers, some are ordinal layers).
