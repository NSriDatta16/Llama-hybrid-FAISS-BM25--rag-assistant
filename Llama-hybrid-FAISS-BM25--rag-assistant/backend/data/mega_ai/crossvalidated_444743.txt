[site]: crossvalidated
[post_id]: 444743
[parent_id]: 
[tags]: 
Subsequent data transformation in Deep Belief Networks

I have a Deep Belief Network made of 4 Restricted Boltzmann Machines. The lowest level RBM seems to train correctly. The deeper layers however seem not to learn anything at all. I plot the receptive fields (weights matrices of some hidden neurons) of the first RBM and some features emerge, see Figure attached, 100 random neurons are chosen. The second, third and fourth layers (images below are in this order) display noisy weights matrices, I guess that nothing is learned there. My doubt is that I use the wrong data to train the subsequent RBM layers. My workflow is as follows. Train the first RBM with raw MNIST data batches (binarized) Once the RBM is trained, transform raw data as hidden = sample(sigmoid(W1 * data + b1)) . W1 and b1 are the weights and the bias of the first hidden layer respectively Train the second RBM using hidden as data. Once the second RBM is trained, update hidden = sample(torch(W2 * hidden + b2)) . Repeat for all the RBMs constituting the stack. sample means that I sample from the factorial posterior (conditional distribution over the hidden variables given the activity pattern of the previous layer, might it be the visible layer or an hidden layer), $\boldsymbol{h} \sim p(\boldsymbol{h} | \boldsymbol{v})$ . Given the distribution one can sample easily since the hidden units are independent, hence the sampling is $h_i \sim p(h_i | \boldsymbol{v}) = \text{sigmoid}(W_1\boldsymbol{v} + \boldsymbol{b}_1)$ , referring to the first layer. Question : are the data reconstructions made correctly in this way? Any help is greatly appreciated. Details MNIST fetched with the torchvision.datasets.MNIST module. Deep Belief Network made of four RBMs of dimensions [28*28, 23*23, 18*18, 10*10, 25*25] units, from the visible to the deepmost. 10 epochs of training, learning rate lr = 0.0001 , weights decay wd = 0.00001 , momentum m = 0.5 , mcmc_steps = 2 block Gibbs sampling steps. EDIT I think I solved: The weight matrices by themselves could in principle not to display any significant signature of learned feature. It is rather the combination of these that yields some meaningful insight. What I did is to multiply the matrices. Assume the DBN is a stack of three RBMs, having layers sizes of 784x500 , 500x500 , 500x2000 . The receptive fields of the first hidden layer are given by the weight matrix $W_1^T \in \mathbb{R}^{784,500}$ simply. I pick 100 among the 500 hidden units and plot the respective matrices, which have size 28x28 ( 28 = sqrt(784) ). The receptive fields of second hidden layer are given by the matrix $W_1^T W_2^T \in \mathbb{R}^{784,500 \times 500,500}$ (forgive the abuse of notation). Again one plots 100 28x28 matrices. For the third layer, one composes all the matrices, that is $W_1^T, W_2^T, W_3^t \in \mathbb{R}^{784,2000}$ . I report below the receptive fields plots I could produce. This DBN has dimensions (784x529) (529x529) (529x2000) . The weight decay factor has been set to 0.0001 .
