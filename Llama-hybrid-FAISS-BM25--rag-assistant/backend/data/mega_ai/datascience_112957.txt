[site]: datascience
[post_id]: 112957
[parent_id]: 
[tags]: 
Predicting deterioration of equipment on a production line

Background There is a production line where there is a machine that interacts with some tools. The process goes as such. Machine makes a product Product is moved into the tool [tool here is essentially a box the condition of which deteriorates over time with use] When the product is moved into a tool, there is a failure where this transformation fails. Hence there is an unsuccessful transfer of the product into the tools. This is a fault. This failure occurs because overtime, there is a gradual decline in the quality of the tool. I have built an analytics system where I can see which tool is faulting the most and capture each failure and the associated tool ID. Problem Statement Great, now I can identify the damaged tools and fix them. This raises the question of How many tools do I need to fix per month to keep the number of faults at an acceptable level? . But how do we turn this question into a mathematical problem that can be solved? Currently, on average, each tool has 20 faults per year however around >30% of the tools have 40+ faults per year. One approach could be to set 'limits'. So for example, if a tool has 20 faults per year, this is OK but if it has 40+ faults per year or is on track to achieve this, then it needs to be fixed. This method isn't the best due to various reasons. Therefore, I would like to build a model which is able to tell me which tools are in bad condition and hence needs to be fixed. I believe a good way to do this is to create a multiclass classifier where per given tool, it can be classified as 'good' , 'OK' or 'bad' and the bad needs to be fixed. Are there better ways of approaching this? The Data I Have Initially I have a table which captures the following information. For example, for the instance that tool 104 was in use (this could be an hour, 2 hours, 4 hours etc, depends) it had 1 fault. There could be multiple faults per instance a certain tool was used and similarly this could be zero. In addition to this, I can have another column which specified the datetime that specific failure occurred. Tool ID Faults Datetime 104 1 26/07/2022 16:00:00 321 2 21/07/2022 04:00:00 1043 0 2/04/2022 12:00:00 321 2 6/01/2022 08:00:00 ....... .... ..... This information can then be 'grouped' where for example, for a given time frame, all instances that Tool ID 104 was used, we will have one row with the Tool ID and all the faults that happened. For most tools, there is not much data as they are 'good' however for the 30% of tools that have many faults, they may have around 40 data points per year. The best model should be able to classify a bad tool as soon as possible so it can be fixed asap before it causes many faults. How do we solve this problem? The data is not exactly normally distributed and infact doesn't really fit any distribution (Here is the histogram for total faults per tool id for a given time frame) - What is the appropriate model to use for such a problem and the best approach to start working on this?
