[site]: stackoverflow
[post_id]: 4621574
[parent_id]: 4621291
[tags]: 
Quite generally speaking, if you have N nodes and an average of X outgoing links per node, X much smaller than N, you're going to need XN ln N bits of information to represent this, unless you can find patterns in the link structure (which you can then exploit to bring down the entropy). XN ln N is within an order of magnitude from the complexity of your 32-bit adjacency list. There are some tricks you could do to bring down the size some more: Use huffman codes to encode link destinations. Assign shorter codes to frequently referenced pages and longer codes to infrequent pages. Find a way to break down the set of pages into classes. Store each link between pages within the same class as "0" + "# within class"; links between pages in different categories as "1" + "destination class" + "# within class". Links from Giuseppe are worth checking, but only the experiment will tell you how well those algorithms are applicable to Wikipedia.
