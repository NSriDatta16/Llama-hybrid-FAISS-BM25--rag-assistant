[site]: crossvalidated
[post_id]: 236565
[parent_id]: 214124
[tags]: 
First a note on Kodiologist's answer, in order to avoid confusion: It can be appropriate to use ICCs to estimate intra-rater reliability. The result yielded by ICCs will be different from (and potentially more or less useful than) what Pearson's $r$ would yield. Pearson's $r$ tests the linearity of the relationship between the two time points: $t_2 = m * t_1 + b$, whereas consistency ICCs test the additivity of the relationship: $t_2 = t_1 + b$, and absolute agreement ICCs test the equality of the relationship: $t_2 = t_1$. Now to respond to the original question: ICCs work by partitioning the observed variance into different sources and comparing these sources. As you allude to in your question, it is a well-known property of ICCs that they can become spuriously low in the presence of low between-subjects variance (like mdewey, I think your question has this backwards). If we look at the model of the ICC formulation you are using (McGraw & Wong, 1996), that should make it more clear: $$\rho=\frac{\sigma_s^2}{\sigma_s^2 + \sigma_t^2+\sigma_e^2}$$ where $\sigma_s^2$ is the between-subjects variance, $\sigma_t^2$ is the within-subjects variance, and $\sigma_e^2$ is the error or residual variance. When there is little variance within-subjects and due to error, $\rho$ should approach $1$. However, when there is very little between-subjects variance, it becomes almost impossible for $\rho$ to take on a high value, even if we would consider the within-subjects variance to be small. In practice, when estimating $\rho$ using sample mean squares (as in the formula below), low between-subjects variance can even cause an ICC to take on negative values. $$ICC(A,1) = \frac{MS_s-MS_e}{MS_s + (k-1)MS_e+k/n(MS_t-MS_e)}$$ So what can you do about these negative values? If you are calculating many ICCs and averaging them, negative values can really drop the mean. If you believe that the ICC is spuriously negative (e.g., due to sampling error), it has been suggested (e.g., by Bartko, 1976) that resetting the ICC to $0$ can be an appropriate solution. Another option, as suggested by LeBreton et al. (2003) is to use a different measure of reliability that does not depend on correlation; their recommendation is the RWG score from James, Demaree, & Wolf (1984). This score compares the observed variance to a "null distribution" that could be expected to occur by chance. References Bartko, J. J. (1976). On various intraclass correlation reliability coefficients. Psychological Bulletin, 83 (5), 762–765. James, L. R., Demaree, R. G., & Wolf, G. (1984). Estimating within-group interrater reliability with and without response bias. Journal of Applied Psychology, 69 (1), 85–98. LeBreton, J. M., Burgess, J. R. D., Kaiser, R. B., Atchley, E. K., & James, L. R. (2003). The restriction of variance hypothesis and interrater reliability and agreement: Are ratings from multiple sources really dissimilar? Organizational Research Methods, 6 (1), 80–128. McGraw, K. O., & Wong, S. P. (1996). Forming inferences about some intraclass correlation coefficients. Psychological Methods, 1 (1), 30–46.
