[site]: datascience
[post_id]: 75231
[parent_id]: 75048
[tags]: 
Both papers are attempting to incentivize exploration for reinforcement learning agents. Curiosity-driven Exploration by Self-supervised Prediction does that by adding an inverse model, increases the complexity of the loss function of the overall model. Exploration by Random Network Distillation does that having two separate models. One of the model tracks and learns the value of exploration. There are pros and cons to both approaches. It is easier to train a single loss function/model. However, two models that learn to interact are more flexible.
