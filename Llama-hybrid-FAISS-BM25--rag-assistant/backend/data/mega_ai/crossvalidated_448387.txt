[site]: crossvalidated
[post_id]: 448387
[parent_id]: 414554
[tags]: 
You quote: that every conditional distribution that can be represented using naive Bayes can also be represented using the logistic model This only means what you had written down before: If the model behaves as assumed by naive Bayes , it can be written down as a special case of a logistic model. This does not talk about "other conditional distributions". However, this statement does not imply that logistic regression always "outperforms" naive Bayes, because logistic regression estimates a different parameterisation, which if the naive Bayes model assumption is fulfilled may not be optimal, and naive Bayes may make more efficient use of the data. The "given a sufficient amount of data" qualification is crucial here; if the number of variables doesn't grow with $n$ , logistic regression can emulate naive Bayes more and more precisely, ultimately reaching a performance that is arbitrarily close to the one of naive Bayes in case naive Bayes's model assumption is fulfilled . Otherwise one can expect the more flexible logistic regression to be better, however I'm not sure if examples can be constructed that violate model assumptions of both approaches in such a way that naive Bayes will do better even for large $n$ . The plain fact that logistic regression can emulate naive Bayes is not enough to rule this out in my opinion, as it is conceivable that the logistic regression parameter estimates are led astray in a way not possible for naive Bayes. I should also add that I have seen the term "naive Bayes" for a method that assumes x-variables independent but not necessarily normal (for example using kernel density estimators), in which case it is not always a special case of logistic regression.
