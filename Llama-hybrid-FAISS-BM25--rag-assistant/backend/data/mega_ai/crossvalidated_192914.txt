[site]: crossvalidated
[post_id]: 192914
[parent_id]: 192772
[tags]: 
Short Answer: Requiring $\sum_j |\psi_j| Longer: When building a model, you're almost certainly going to want a finite variance (unless you're specifically building a 'heavy-tailed' model). For this, we only need the slightly weaker condition that $\sum_j \psi_j^2 $$ \begin{align*} \text{Var}\left[\sum_{j=1}^{\infty} \psi_j Z_j\right] &= \sum_{j=1}^{\infty} \text{Var}[\psi_j Z_j] \\ &= \sum_{j=1}^{\infty} \psi_j^2 \text{Var}[Z_j] \\ &= \sigma^2 \sum_{j=1}^{\infty} \psi_j^2 \end{align*} $$ If the variance exists (is finite), it's a standard result that the mean exists (is finite) as well [S03, Section 1.3.2]. However, if we don't require absolute convergence on the $\psi_j$, the series $\sum_j \psi_j$ may only be conditionally convergent and not absolutely convergent , which leads to strange things like the Riemann Rearrangement Theorem applying. In practice, it's not the RRT that you're worried about - that's just an example of the strange properties of conditionally convergent series. One of the great things about absolute convergence is that it lets you switch around integrals (expectations) and sums: this lets us assume that the sum gives a sensible random variable. Another, more serious, issue is that, without assuming absolute summability, you can't prove the ergodicity of the mean of the series (ergodicity means that given a long enough observation, you can get a good estimate of the mean which is useful because we typically only have one realization of a time series). The series may be 'long-memory' (long-range dependence) and having more observations won't necessarily make the variance of your mean estimate decay: roughly, any shocks will 'stick around' forever and pollute your estimate of the mean. (See [SS15, Section 5.2]; I also like [S06] for a more general overview of long-memory processes, but it's not the easiest read just because the subject is hard.) Hamilton [H94] discusses this briefly in section 3.3, particularly footnote 3, where he refers the reader to [R73, p.111] for details, and appendix 3.A but I don't have the Rao reference handy. [H94] James D. Hamilton, Time Series Analysis , 1st Ed. (1994) Princeton University Press. [R73] C. Radhakrishna Rao, Linear Statistical Inference and Its Applications 2nd Ed. (1973) Wiley. [S03] Jun Shao, Mathematical Statistics , 2nd Ed. (2003) Springer. Springer Texts in Statistics. [S06] Gennady Samorodnitsky, "Long Range Dependence". Foundations and Trends in Stochastic Systems 1(3). p.163-257 (2006). [SS15] Robert H. Shumway and David S. Stoffer, Time Series Analysis and Its Applications , 3rd Ed. Blue Printing (2015-12). Springer. Freely available at http://www.stat.pitt.edu/stoffer/tsa3/
