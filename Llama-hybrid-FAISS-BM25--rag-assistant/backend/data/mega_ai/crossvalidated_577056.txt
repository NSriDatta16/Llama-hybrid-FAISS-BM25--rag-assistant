[site]: crossvalidated
[post_id]: 577056
[parent_id]: 576547
[tags]: 
So, now I understand that one-hot encoding is not applicable (due to curse of dimensionality) That is not true. The curse of dimensionality is a problem for some algorithms, but it doesn't mean that you can't use data with many columns. You need to deal with it, for example, by using regularization. In many real-life data problems, you would have the number of columns as high or higher than in your case, for example in recommender systems (all users, all movies on Netflix) or natural language processing (all the words in the vocabulary). In cases like this, you have specialized models that deal with such data and technological solutions to deal with problems like high memory consumption (e.g. using sparse matrices). See One-hot-encoding gives untractable amount of classes for more details. and I don't wish to use hash-encoding (interpretability issue), Hashed representations are harder to interpret but not impossible to interpret. What it does is just randomly merge some categories to the same bins, so you have the OR relationship within the bin. label/ordinal encoding (imposes a rank order), Correct, it would not work here. target encoding (data leakage issue) etc. Data leakage would be a problem only if you implemented it incorrectly. If it always lead to leakage it won't be ever used, while it is used quite successively. So, I wish to transform high cardinal variable without using any encoding approach. You can't send the data to an algorithm "without representation". Every algorithm expects the data encoded as numbers (maybe except for tree-based models). Even for the algorithms that take as input "raw" data and do the feature engineering by themselves like neural networks you need to encode the data (images are numerical tensors, natural language data is words encoded somehow, etc). Even if instead of providing the data as one-hot columns, but "as is" to the algorithm, it would need to have something like separate weight per each category, or separate IF statement, etc so internally the representation of the data would be equally big as when it used as input a one-hot matrix because the model would need to know how it should react to every one of the categories. "No representation" at best means that the representation is learned by the algorithm, so skipping this step doesn't solve any problem, just makes life harder for your algorithm.
