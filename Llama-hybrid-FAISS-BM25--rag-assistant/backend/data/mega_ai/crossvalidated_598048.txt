[site]: crossvalidated
[post_id]: 598048
[parent_id]: 598020
[tags]: 
Monotonic constraints are not implemented for multiclass classification currently. One of the reasons is that it is convoluted or even potentially infeasible to define a monotonic constraint moving in the same direction for all classes (as in the example shared). If anything such a choice would create a race condition such that the overall effect in class $i$ where the condition is least prominent than in class $j$ , the relative pairwise PDP is negative. The basic method is actually quite simple: We have a parent node that has minimum and maximum constraint values for our variable(s) of interest; say $x_1$ has $x_1^{\text{min}} =-1$ and $x_1^{\text{max}} =1$ . We perform a node split based on standard splitting criteria (e.g. loss function reduction). We calculate the new min/max constraint values for the left and right child nodes. If they respect our constraints (e.g. the left child node constraints are $x_{1L}^{\text{min}} =-1$ and $x_{1L}^{\text{max}} =0.4$ and the right child node constraints are $x_{1R}^{\text{min}} =0.45$ and $x_{1L}^{\text{max}} =1$ , we accept the split; otherwise (e.g. if $x_{1R}^{\text{min}}$ was $0.35$ ) the split is rejected. Particularly, for LightGBM this issue has been discussed in the GitHub issue here . That said, XGBoost has a very nice tutorial page on Monotonic Constraints that is fully applicable to LightGBM too. The behaviour you see though definitely warrants an issue as it is buggy. It appears that LightGBM does try to enforce some monotonic constraints during training of multi-class case, the same way it would be at a binary case. Unfortunately, while in the case of a binary classification task, satisfying the constraints for the positive class automatically satisfies the constraint for the negative class, in the case of multi-class classification task, if the constraint is satisfied for one of the classes, it does not necessarily covers the other classes too.
