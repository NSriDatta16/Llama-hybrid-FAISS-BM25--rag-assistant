[site]: crossvalidated
[post_id]: 23942
[parent_id]: 
[tags]: 
Pre-process classification data according to ‘amount of evidence’

I have an area which is divided into polygons of different sizes. Each polygon has the same associated features/predictors and I know whether something occurs within the polygon or not. If something occurs in the polygon I have a number between 1 ... n which indicates the ‘amount of evidence’. I am trying to use logistic regression (or GAMs later on) to model the occurrence probability given a polygon’s features $(X_1, ..., X_n)$. At the moment I do not adjust for polygon size and ‘amount of evidence’ so I have just a vector $(X_1, ..., X_n)$ plus a zero or one value for the response. I am just wondering whether it may be useful to adjust the data (somehow?) to account of the different sizes of the polygons and the amount of evidence. Perhaps one should repeat the data rows more often depending on the size and the amount of evidence?
