[site]: crossvalidated
[post_id]: 633768
[parent_id]: 
[tags]: 
Questions about computation of Friedman's H-statistic

Background Christoph Molnar's Interpretable Machine Learning book introduces estimation of machine learning model interactions by the decomposition of partial dependences: If two features do not interact, we can compose the partial dependence function as follows: $$ PD_{jk}(x_j, x_k) = PD_j(x_j) + PD_k(x_k)$$ . Likewise, if a feature has no interaction with any of the other features, we can express the prediction function $\hat{f}(x)$ as a sum of partial dependence functions, where the first summand depends only on $j$ and the second on all other features of $j$ : $$\hat{f}(x) = PD_j(x_j) + PD_{-j}(x_{-j})$$ Question 1 What is the computation of the second term in the second equation above, $PD_{-j}(x_{-j})$ ? Is it computed using predictions averaged over the grid values of $x_j$ ? Question 2 In the same chapter, it is claimed that computing j-vs-all interaction H-statistic requires, at the most, $3n^2$ calls to predict . Shouldn't this depend on grid.size , i.e., the resolution over which the partial dependences are evaluated? For example, I would expect that a univariate partial dependence requires grid.size $\times n$ calls to predict. Extending this to a two-way partial dependence $PD(x, y)$ , in the worst case it would be grid.size $\times n^2$ . Where is Molnar's calculation coming from?
