[site]: datascience
[post_id]: 38526
[parent_id]: 
[tags]: 
Boundaries of Reinforcement Learning

I finally developed a Game Bot that learns how to play the videogame Snake with Deep Q-Learning. I tried with different neural networks and hyper-parameters, and I found a working set-up, for a specific set of rewards. The problem is is: when I reward the agent for going in the right direction - positive rewards in case the coordinates of the agent increase or decrease accordingly to the coordinates of the food - the agent learns pretty fast, obtaining really high scores. When I don't reward the agent for that, but only negative rewards for dying and positive for eating the food, the agent does not learn. The state takes into account if there's any danger in proximity, if the food is up, down, right or left and if the agent is moving up, down, right or left. Here's the question: is rewarding the agent for going into the right direction a "correct approach" in Reinforcement Learning? Or it's seen as cheating, cause the system needs to learn that by itself? Is passing the coordinates of the food as state an other way of "cheating"?
