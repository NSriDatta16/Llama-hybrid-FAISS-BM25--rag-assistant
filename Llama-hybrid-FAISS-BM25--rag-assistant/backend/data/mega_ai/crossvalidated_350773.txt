[site]: crossvalidated
[post_id]: 350773
[parent_id]: 
[tags]: 
Is double Q-learning redundant when using target networks?

Generally speaking, the purpose behind target networks is to reduce the impact of current changes on the model. i.e. if I performed action a and got some reward r , I want to reduce the impact of this specific tuple on the model. When using double-q learning, I keep a different model for action choosing and reward estimation, also to make the model more robust. But, and here is my question, generally speaking, both of those methods delays some samples to gain robustness, in slightly different scheme, and as I see it they are solving the same problem. So are they redundant?
