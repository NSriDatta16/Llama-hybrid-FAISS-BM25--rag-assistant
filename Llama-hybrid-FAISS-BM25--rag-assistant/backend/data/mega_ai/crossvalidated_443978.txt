[site]: crossvalidated
[post_id]: 443978
[parent_id]: 443935
[tags]: 
This reminds me of a recent conversation with my dad who was surprised to hear that I still run models taking days which he did decades ago: Are you still doing computations that take longer than a day? Shouldn't everything run fast now? What are you people (physicists) improving on modeling nowadays, when there have already been satisfying models in the past? If a model is sufficient why improve it? Probably he was joking/provoking. There are always more problems with extra layers of complexity or that require more precise answers. Computing power is never enough to be able to solve all problems. And there will always be problems that are conceived before computational power is sufficient. ( the ultimate question ) There are a lot of examples that are beyond the current limits of computational power. These examples are continuously changing. A search (e.g. on google) will give you a reasonable list*: https://scholar.google.com/scholar?as_ylo=2019&q=supercomputer+days Often these problems involve some sort of high dimensionality: time and space as in computational fluid dynamics or solving other partial differential equations. testing variations of parameters as in parametric sweeps in physical models or hyperparameter optimization in statistical models like in your example of Ridge regression . multiple entities as in approximating probability distributions by simulating many outcomes (see Monte Carlo method ), approximation of parameters with large datasets and observing many outcomes ( machine learning and big data , which includes regularized regression that is suitable for situations with many covariates, and samples). And entities that are interacting as in molecular modelling or artificial neural networks which makes the problem complexity (computation cost) grow rapidly (This is why a chess computer can use brute-force but a go computer should use some learning algorithm). You could split this dimensionality into two types. Either the problem/question is larger (increase system complexity and size that is being investigated) or the method to solve them is larger (like introducing hyperparameters or using monte carlo method, sometimes these might not be neccesary and a more direct analytical solution/approximation exists). The ridge regression is a bit of both: it is applied to problems of large size (many covariates) and it is introducing a hyperparameter that needs to be optimized (the regularization parameter). * Although that list reveals more current state of the art. It is what is currently at/near the limits of computational power (and thus, a lot of that will still develop when computational increases and can be considered ahead of computational power). The search does not so much reveal theory that is so far ahead of time such that it is not even applied on supercomputers in a simple form (or only rarely). Because, if it is not yet applied on a supercomputers then this will not easily generate hits when searching with the searchterm 'supercomputer'. A very particular example of some method that is ahead of time that you will not find easily in that particular google search is quantum computing .
