[site]: crossvalidated
[post_id]: 317045
[parent_id]: 316937
[tags]: 
I'm reverting to 'answering' your queries as this statement is too long for a comment. Econometricians have written extensively about using IVs as a control for endogeneity while, at the same time, acknowledging the difficulties in, first, finding an appropriate IV measure to use as well as its many weaknesses as a solution. My view is that introducing IVs into a model creates as many problems as it intends to solve. So, if you choose to drop the use of IVs, that leaves you with a choice between PSM, Heckman and 2SLS approaches. 2SLS is given extensive, theoretical treatment in Wooldridge's classic book Econometric Analysis of Cross Section and Panel Data . It also happens to be a method I don't have a lot of experience with and, therefore, don't have much to recommend. Definitely check it out. There are many criticisms of PSM as a tool for matching. One of the most cogent is by Gary King, Harvard Distinguished Professor, titled Why Propensity Scores Should Not Be Used for Matching here ... https://gking.harvard.edu/files/gking/files/psnot.pdf . King proposes using Mahalanobis distance instead of PSM, convincingly demonstrating its superiority over PSM. If you, in fact, want to match your data, then King's recommendation is to be preferred. That still leaves Heckman selection bias to discuss. In the absence of additional information about the challenges you face and the issues you are trying to solve, my obvious preference is to leverage Heckman's method over the other approaches. Here's a link to his original (1979) paper ... https://faculty.smu.edu/millimet/classes/eco7321/papers/heckman02.pdf . Essentially, Heckman's method creates an additional variable (parameter) that acts like a 'weighting' factor for selection bias in downstream models, adjusting the remaining observations back to the full population proportions. Note that his method is not hard to code or program in any software language. Here's a link to one of the clearest programming solutions from SAS Support ... https://support.sas.com/resources/papers/proceedings14/SAS207-2014.pdf . This SAS solution can be translated into the software of your choice. Sparse or rare data introduces a new issue from the ones discussed up until now. The problem with the standard models, such as logistic regression, is that they don't fit the tails of the logistic curve well at all. This means that for variables (both dependent and independent) that are sparsely observed, sparsity can cause an error warning of 'quasi-' or 'complete separation of points' and convergence failure of maximum likelihood estimation. For an excellent review of convergence and estimation problems, see Allison's paper Convergence Failures in Logistic Regression ... https://pdfs.semanticscholar.org/4f17/1322108dff719da6aa0d354d5f73c9c474de.pdf . As Allison notes, one of the most common solutions is to drop the offending feature. Collapsing features to create a new, combined, more robust feature is a second, less satisfactory solution. However, when it comes to sparsely observed target or dependent variables, dropping or collapsing information is not an option. Fortunately, there are workarounds to this challenge as well. Here's a link to one of the best solutions as proposed (again) by Gary King https://gking.harvard.edu/category/research-interests/methods/rare-events . And here's another link to a PhD dissertation that reviews the literature wrt rare event data, A Comparison of Different Methods for Modelling Rare Events Data here ... https://lib.ugent.be/fulltxt/RUG01/002/163/708/RUG01-002163708_2014_0001_AC.pdf . Let me know, first, if you have any questions and, second, if you have any trouble accessing any of these references. If you do, we'll figure out another way for you to obtain them.
