[site]: crossvalidated
[post_id]: 259236
[parent_id]: 175463
[tags]: 
There are various forms of row normalization and the OP is not stating which one s/he has in mind. A specific form of row normalization (Eucledian norm normalization) where each row is normed (divided by its Eucledian norm) is quiet popular. Reasons for using this form of row normalization are well summarized in section $3.2$ of this paper [0]: Denote $\pmb x$ a column centered row $p$-vector and $$r(\pmb x)=||\pmb x||^{-1}_2\pmb x\label{a}\tag{0}$$ the row normalized vector. Consider what this transformation is doing to your data geometrically . Geometrically, if your original data has $p>1$ variables and is centered, by applying the euclidean row normalization, you project your data on the surface of the $p$ dimensional unit circle. For example, if your original data is centered (like the black dots in this image) and you apply row normalization to it, you obtain the red stars. library(car) p = 2 n = 1000 m = 10 C = matrix(.9, p, p) diag(C) = 1 set.seed(123) x = matrix(runif(n * p, -1, 1), n, p) %*% chol(C) z = sweep(x, 1, sqrt(rowSums(x * x)), FUN = '/') plot(rbind(x, z), pch = 16, type = 'n', ann = FALSE, xaxt = 'n', yaxt = 'n') points(x, pch = 16) points(z, pch = 8, col = 'red') The green dots represent a small number of outliers in the original data. If you apply the row normalization transformation to them, you obtain the blue stars. x_1 = sweep(matrix(runif(m * p, -1, 1), m, p), 2, c(2, -2)) z_1 = sweep(x_1, 1, sqrt(rowSums(x_1 * x_1)), FUN = '/') plot(rbind(x, x_1, z, z_1), pch = 16, type = 'n', ann = FALSE, xaxt = 'n', yaxt = 'n') points(x, pch = 16) points(x_1, pch = 16, col = 'green') points(z, pch = 8, col = 'red') points(z_1, pch = 8, col = 'blue') Now the advantage of this transformation are clear. The projection on the unit circle dampens the effect of far out outliers. If you original data $x$ is drawn from a centered model $\mathcal{F}$ except for a few far out outliers (such as the green dots) then any estimate of the shape of your data based on $x$ will be spoiled by the outliers. If you base your estimates on the normalized data $z$ (the stars, red and blue), the outliers (now mapped to the blue stars) have less leverage to spoil your estimates. You can see this most clearly by comparing the shape matrices (or contour ellipses) fitted in turn to the data, their contaminated version and the row normalized transformation thereof: ellipse(crossprod(rbind(x, x_1)) / (n + m - 1) / det(crossprod(rbind(x, x_1)) / (n + m - 1))^(1 / p), center = rep(0, p), col = 'green', radius = 1) ellipse(crossprod(rbind(z, z_1)) / (n + m - 1) / det(crossprod(rbind(z, z_1)) / (n + m - 1))^(1 / p), center = rep(0, p), col = 'red', radius = 1) ellipse(crossprod(rbind(x)) / (n - 1) / det(crossprod(rbind(x)) / (n - 1))^(1 / p), center = rep(0, p), col = 'black', radius = 1) As you can see, the shape ellipses fitted to the transformed contaminated data (red ellipse) is a better estimated of the shape matrix of the uncontaminated data (black ellipse) than that obtained from the contaminated data (green ellipse). This is because the green stars have a smaller pull on the red ellipse (constructed from the transformed data $z$) than on the green one (constructed from the original data $x$). [0] S. Visuri, V. Koivunen, H. Oja (2000). Sign and rank covariance matrices, Journal of Statistical Planning and Inference Volume 91, Issue 2, 557â€“575.
