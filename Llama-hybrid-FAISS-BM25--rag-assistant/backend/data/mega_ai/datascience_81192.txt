[site]: datascience
[post_id]: 81192
[parent_id]: 81184
[tags]: 
We generally rely on sampling to validate the result of our Model. We make train/test to test a model on a separate unseen dataset. If we are doing a Hyperparameter tuning, we keep another set as a validation set to validate the result of new hyperparameters. Random forest builds each new tree on a Bagged sample from the original sample(train data). Bagging means sampling is done with replacement i.e. you pick one data point, put it back and then pick the next . In this process, many duplicated data is sampled and many data points are not sampled. ~63% of the data points are selected Read here The other "not selected" 37% data points is called Out of Bag samples. Hence, the way Bagging, and RF is designed, we got another set of data to do our validation. It an opportunity to do a level of validation with these samples. What it means, You are getting 37% data points to validate your model But OOB is not done with the fully grown Ensemble. It is done using all the trees in the random forest ensemble for which the particular data point was omitted during training Read here It's not equivalent to K-Fold or Train/test on fully built RF but it gives a decent idea about the validation error about to come.
