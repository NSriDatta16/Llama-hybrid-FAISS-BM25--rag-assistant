[site]: datascience
[post_id]: 22856
[parent_id]: 22816
[tags]: 
I will answer, assuming that since you wanted to do text classification, your features were created using some TFIDF feature extraction technique. Next time, please specify the dimensions of your features and not only their storage. According to the documentation ( http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html ), the object has a coef_ attributes that stores all of the weights of your model. Since for text, feature vectors are about the size of your vocabulary, it can be fairly large, especially if you do not preprocess your text using stemming or if you do not remove stopwords or irrelevant words. The documentation also says that the size of the coef_ is number of classes * number of features. So depending on how many features and classes you have, this can get large quickly. This could explain partially why the object is so large. Other factors could be that Python often stores the predictions in the object after you train it, so that you can access them quickly, so you would need to add another 600,000 numbers to your object. And for this object, it seems to also store the confidence scores, which is another 600,000.
