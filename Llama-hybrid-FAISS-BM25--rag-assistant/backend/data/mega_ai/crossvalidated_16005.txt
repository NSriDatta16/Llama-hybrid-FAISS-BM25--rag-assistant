[site]: crossvalidated
[post_id]: 16005
[parent_id]: 15983
[tags]: 
The LD-vectors, which are in the scaling entry of the list returned by lda in R, are not so easily comprehended. The help page says that scaling is a matrix which transforms observations to discriminant functions, normalized so that within groups covariance matrix is spherical. which is not super informative. Inner products with these vectors give the coefficients for a projection to a $K-1$ dimensional space (with $K$ groups, that is, a one-dimensional space if there are two groups). Everything is centered using the total mean of the predictors before projection. In this projection, classification happens to the group with the nearest mean, as measured by the usual euclidean distance, if the prior probabilities are equal. Perhaps the best thing to do to understand precisely how the computation of the predictions work is to read the R-code in MASS:::predict.lda . The MASS book by Ripley and Venables may also be useful, but I find that Chapter 3 in Ripley's other book Pattern Recognition and Neural Networks is more complete. Here you will find the complete explanation behind the computations implemented in the R function lda , and a more detailed description of what the projection actually is. For the two group case things simplify, the LD1 vector is then proportional to $\hat{\Sigma}^{-1}(\hat{\mu}_2 - \hat{\mu}_1)$, and the linear discriminator is given by taking inner product with LD1. In the case of equal prior probabilities on the two groups you don't need to know the constant of proportionality and the cutoff is $\frac{1}{2}(\hat{\mu}_2 + \hat{\mu}_1)^T\text{LD1}$. With unequal priors it seems to me that you need to know the constant of proportionality to compute the cutoff in this formulation. In the alternative, and this is essentially what MASS:::predict.lda does, you can for a given $x$ compute the inner products $c = x^T \text{LD1}$ and $c_i = \hat{\mu}_i^T \text{LD1}$ for $i=1,2$ and then $$d_i = \frac{1}{2}(c - c_i)^2 - \log \pi_i.$$ Finally, you classify to the group with the smallest $d_i$.
