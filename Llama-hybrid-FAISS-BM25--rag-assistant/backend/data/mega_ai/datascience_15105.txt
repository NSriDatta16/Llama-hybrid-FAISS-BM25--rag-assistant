[site]: datascience
[post_id]: 15105
[parent_id]: 15056
[tags]: 
When you create a Numpy array like this: x_data = np.array( [[1,2],[4,5,6],[1,2,3,4,5,6]]) The internal Numpy dtype is "object": array([[1, 2], [4, 5, 6], [1, 2, 3, 4, 5, 6]], dtype=object) and this cannot be used as a Tensor in TensorFlow. In any case, Tensors must have same size in each dimension, they cannot be "ragged" and must have a shape defined by a single number in each dimension. TensorFlow basically assumes this about all its data types. Although the designers of TensorFlow could write it in theory make it accept ragged arrays and include a conversion function, that kind of auto-casting is not always a good idea, because it might hide a problem in the input code. So you need to pad the input data to make it a usable shape. On a quick search, I found this approach in Stack Overflow , replicated as a change to your code: import tensorflow as tf import numpy as np x = tf.placeholder( tf.int32, [3,None] ) y = x * 2 with tf.Session() as session: x_data = np.array( [[1,2],[4,5,6],[1,2,3,4,5,6]] ) # Get lengths of each row of data lens = np.array([len(x_data[i]) for i in range(len(x_data))]) # Mask of valid places in each row mask = np.arange(lens.max()) Output is: [[2, 4] [8, 10, 12] [2, 4, 6, 8, 10, 12]] You don't have to remove the padding at the end - only do this if you require to show your output in the same ragged array format. Also note that when you feed the resulting padded data to more complex routines, the zeros - or other padding data if you change it - may get used by whatever algorithm you have implemented. If you have many short arrays and just one or two very long ones, then you might want to consider using a sparse tensor representation to save memory and speed up calculations.
