[site]: datascience
[post_id]: 123858
[parent_id]: 122115
[tags]: 
We have 2 embedding matrices(U,V) that are learnt during word2vec training . U has shape (vocab_size, dimensions) V has shape (dimension, vocab_size) For any given pair of target, context: We pick embedding of target from U(one of the rows in U matrix) and calculate it"s dot product with all the vectors in V matrix leading to a score vector(third from last vector). score_vector has dimension vocab_size*1 This score vector(output vector in question) is taken into probability space by using softmax operation. Now we have predicted probabilities ie softmax(score_vector) and actual probability vector ie one-hot vector of context word. In other words we want probability distribution that predicts context word(here quick ) with higher probability than the rest of the words. Once we have predicted vector and actual vector we calculate loss(ie. cross entropy) and backpropogate it to modify both U and V.
