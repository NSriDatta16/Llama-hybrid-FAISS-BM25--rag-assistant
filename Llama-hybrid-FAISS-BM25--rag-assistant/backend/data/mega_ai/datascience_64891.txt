[site]: datascience
[post_id]: 64891
[parent_id]: 
[tags]: 
Using a model to generate training data for another model

I have a text classification problem. Lets say I need to classify text into labels A,B,C & D. I have two different approaches and want to understand the difference: Approach 1: Traditional approach of proving labeled training data to a single label classifier model and training it Approach 2: Step 1: Building 4 different models, one for each label, that predicts the likelihood of that label (Yes/No). This will use the same training data is Approach 1 split in 4 ways, one for each label, with additional annotations for "No" label. Step 2: Using the above models to generate labeled training data with tens of thousands of rows with 99.9% accuracy (discard any text where the previous model has low score), and all four labels A,B,C,D Step 3: Use the generated data to train new model to predict the label for a given text. I'm new to NLP so would like to understand, is Approach #2 going to be better than #1 in terms of precision and recall? I assume it will be, but what doesn't make sense intuitively is that Approach 2 is using the same human labeled data as input as Approach 1 (with exception of some additional data in Step 1 for the "No" Labels. Is the accuracy of the models going to be much different?
