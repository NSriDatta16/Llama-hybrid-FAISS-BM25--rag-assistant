[site]: crossvalidated
[post_id]: 472826
[parent_id]: 472822
[tags]: 
It is funny that you ask now, since just today I came across a paper by Wang, Khabsa, and Ma (2020) To Pretrain or Not to Pretrain who show that if you have large enough training set, the difference in performance between huge, "SOTA" model (RoBERTa), and LSTMs is small for NLP task. There was another recent paper by Merity (2019) Single Headed Attention RNN showing similar results, the abstract is worth quoting in full The leading approaches in language modeling are all obsessed with TV shows of my youth - namely Transformers and Sesame Street. Transformers this, Transformers that, and over here a bonfire worth of GPU-TPU-neuromorphic wafer scale silicon. We opt for the lazy path of old and proven techniques with a fancy crypto inspired acronym: the Single Headed Attention RNN (SHA-RNN). The author's lone goal is to show that the entire field might have evolved a different direction if we had instead been obsessed with a slightly different acronym and slightly different result. We take a previously strong language model based only on boring LSTMs and get it to within a stone's throw of a stone's throw of state-of-the-art byte level language model results on enwik8. This work has undergone no intensive hyperparameter optimization and lived entirely on a commodity desktop machine that made the author's small studio apartment far too warm in the midst of a San Franciscan summer. The final results are achievable in plus or minus 24 hours on a single GPU as the author is impatient. The attention mechanism is also readily extended to large contexts with minimal computation. Take that Sesame Street. I don't think there's much to add. Here is another example from very recent paper by Abnar, Dehghani, and Zuidema (2020) Transferring Inductive Biases through Knowledge Distillation Several studies, however, have shown that LSTMs can perform better than Transformers on tasks requiring sensitivity to (linguistic) structure, especially when the data is limited [ 37 , 6 ]. This is mainly due to the recurrent inductive biases of LSTMs that helps them better model the hierarchical structure of the inputs. hence authors show how distilling information from LSTMs can positively impact Transformer model. This another, of many, examples that LSTMs, and RNNs in general, are used and perform good for a particular class of problems. Sure, they have limitations, but for language they are standard model, that is taught on on every NLP course (like Stanford's CS224n ), and mentioned in every modern handbook on this topic. The above examples focus on language data, because in this area this model is very popular, but of course it is successfully applied to other kinds of time-series data as well, as mentioned in other answers.
