[site]: crossvalidated
[post_id]: 447428
[parent_id]: 
[tags]: 
Estimating error for a sample minimum

Let's say I'm benchmarking some computer program and, due to non-randomness in the input data, I'm interested in the minimum running time (as opposed to the average over random inputs). In addition to that, I'm also interested in understanding how reliable is my estimate of the minimum running time. So let's say my measurements are [ 7.26, 7.20, 7.21, 7.16, 7.21 ] seconds. The minimum is 7.16 s, but how stable it is? What would even be the right notion of stability in this case? Of course, I could calculate the sample standard deviation, and everybody is used to this measure, but it shows the average deviation from the mean, not from the minimum (although it still can show how stable are my measurements overall). Another intuitive thing is to compute sample deviation from the minimum instead of the mean by swapping $\overline{X}$ with $\min X$ in the corresponding formula, but I'm not sure if it makes sense. How is this problem typically approached?
