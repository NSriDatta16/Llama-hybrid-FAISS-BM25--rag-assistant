[site]: crossvalidated
[post_id]: 625414
[parent_id]: 625388
[tags]: 
The range of normalized input $X_i$ and output $\hat{X}_i$ indexed by $i=1,\dots,N$ matters, because the MSE loss is not scale invariant with respect to $X_i$ or $\hat{X}_i$ . But it should not be a problem because normalization can be $[-1,1]$ as well. Change the normalization of $X_i$ . Note that $2U - 1 \in (-1, 1)$ whenever $U\in (0, 1)$ . If you must use standardization in the input layer, standardizing the output from $\tanh$ can help matching the range. Of course, you need to change it because $||X_i - \hat{X}_i||^2$ takes different scales when $X_i$ and $\hat{X}_i$ are normalized in $[0, 1]$ or $[-1, 1]$ . In extreme case, $||X_i - \hat{X}_i||^2 \le 1$ in $[0,1]$ case whereas $||X_i - \hat{X}_i||^2 \le 2$ in $[-1,1]$ case. For the KL term, it is always composed of the parameters from the neural network encoder with $N(0, I)$ as prior.
