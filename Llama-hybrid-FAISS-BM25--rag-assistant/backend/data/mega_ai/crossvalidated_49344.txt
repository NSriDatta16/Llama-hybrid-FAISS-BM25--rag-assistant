[site]: crossvalidated
[post_id]: 49344
[parent_id]: 
[tags]: 
Distribution of eigenvalues given one is known

I'm familiar with using insights from Random Matrix Theory to determine the number of principal components from the PCA of a covariance/correlation matrix to use to form factors. If the eigenvalue associated with the first PC is large, then it means that the remaining eigenvalues must be small (since the sum of the eigenvalues must equal the trace of the correlation matrix). When the first PC is large enough, it is thus possible that all of these eigenvalues are below the lower bounds on the Marcenko-Pastur distribution. This makes sense that they are low not because of random chance, but because the first eigenvalue is very large. However, that does not mean that they contain significant information. Rather, it would make sense to instead ask the question "given the first PC is some large number, what would the distribution of the remaining eigenvalues look like if random data were responsible for them?" Is there any research that addresses this issue? If it is possible to get the Marcenko-Pastur distribution conditional on knowing one or more eigenvalues, then it would be possible to proceed iteratively to determine whether the factors reflect significant information.
