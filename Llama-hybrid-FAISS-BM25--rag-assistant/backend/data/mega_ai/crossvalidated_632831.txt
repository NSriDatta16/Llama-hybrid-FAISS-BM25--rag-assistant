[site]: crossvalidated
[post_id]: 632831
[parent_id]: 632741
[tags]: 
I think I have found an explanation for why $\psi(\mu, \tau) := \exp(\mu + \tau^2/2)$ can be useful to answer the question "What is the population-mean of the $\mu_j$ 's?": Imagine you have an overall population that you are sampling your groups $j$ from, and then you are sampling units $i$ from these groups in turn. Say $j \in \cal{J}$ , where $\cal{J}$ is the whole population of $j$ 's, and you have drawn $J$ of them in your sample, so you observe data $X_{ij}, j = 1, \dotsc, J$ , and for each of them units $i = 1, \dotsc, n_j$ , say. Let's call this overall data $X$ . Now you calculate your posterior distributions via MCMC etc., in particular obtaining $M$ samples $\mu_m, \tau_m$ , $m=1, \dotsc, M$ , from the posterior given $X$ . And finally you are interested in statements about a new group $j = J+1$ from the same population $\cal{J}$ , for which you don't have any data $X_{ij}$ . It makes sense to assume that this group is exchangeable with the other groups that you have observed $X_{ij}$ for. The important fact now is that the full conditional distribution of the new group's mean $\mu_{J+1}$ is again $\text{LogNormal}(\mu, \tau)$ , i.e. for the density we have: $f(\mu_{J+1} | X, \mu_1, \dotsc, \mu_J, \mu, \sigma, \tau) = f(\mu_{J+1} | \mu, \tau)$ and this is the density of the $\text{LogNormal}(\mu, \tau)$ distribution. Therefore, in particular we have $\mathbb{E}(\mu_{J+1} | X, \mu_1, \dotsc, \mu_J, \mu, \sigma, \tau) = \mathbb{E}(\mu_{J+1} | \mu, \tau) = \psi(\mu, \tau)$ as defined above. So, given we know everything else, including the data $X$ from the other $J$ groups, $\psi$ is the mean for $\mu_{J+1}$ . Now that is not yet the marginal posterior expectation yet, because we still condition on $\mu$ and $\tau$ . However, based on the law of iterated expectations we have $\mathbb{E}(\mu_{J+1} | X) = \mathbb{E}[\mathbb{E}(\mu_{J+1} | \mu, \tau) | X] = \mathbb{E}[\psi(\mu, \tau) | X]$ where the outer expectation is with regards to the posterior of $\mu$ and $\tau$ , and we can approximate this marginal mean easily by the Monte Carlo estimate $\hat\psi = \frac{1}{M}\sum_{m = 1}^{M} \psi(\mu_m, \tau_m)$ based on samples $\mu_m, \tau_m$ from the posterior given $X$ . Two notes at the end: It is important that even in the prior model, the marginal prior distribution for the $\mu_j$ 's is actually not log normal. Only the full conditional prior distribution, i.e. conditioning on all other parameters, is log normal. This then carries over to the posterior predictive distribution for $\mu_{J+1}$ , as shown above. The overall concept here is applied a lot in the meta-analytic prior context ( https://onlinelibrary.wiley.com/doi/10.1111/biom.12242 )
