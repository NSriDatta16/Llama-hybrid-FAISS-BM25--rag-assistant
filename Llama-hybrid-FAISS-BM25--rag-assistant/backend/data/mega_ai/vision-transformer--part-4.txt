verage of the student network's past parameters: θ t ′ = α θ t + α ( 1 − α ) θ t − 1 + ⋯ {\displaystyle \theta '_{t}=\alpha \theta _{t}+\alpha (1-\alpha )\theta _{t-1}+\cdots } . The inputs to the networks are two different crops of the same image, represented as T ( x ) {\displaystyle T(x)} and T ′ ( x ) {\displaystyle T'(x)} , where x {\displaystyle x} is the original image. The loss function is written as L ( f θ t ′ ( T ( x ) ) , f θ t ( T ′ ( x ) ) ) {\displaystyle L(f_{\theta '_{t}}(T(x)),f_{\theta _{t}}(T'(x)))} One issue is that the network can "collapse" by always outputting the same value ( y {\displaystyle y} ), regardless of the input. To prevent this collapse, DINO employs two strategies: Sharpening: The teacher network's output is sharpened using a softmax function with a lower temperature. This makes the teacher more "confident" in its predictions, forcing the student to learn more meaningful representations to match the teacher's sharpened output. Centering: The teacher network's output is centered by averaging it with its previous outputs. This prevents the teacher from becoming biased towards any particular output value, encouraging the student to learn a more diverse set of features. In January 2024, Meta AI Research released an updated version called DINOv2 with improvements in architecture, loss function, and optimization technique. It was trained on a larger and more diverse dataset. The features learned by DINOv2 were more transferable, meaning it had better performance in downstream tasks. In August 2025, Meta AI Research released DINOv3, an update to DINOv2. It introduced image-text alignment like CLIP. It scaled up the model to 7B parameters and the training dataset to 1.7B images (obtained by diversity-sampling an initial dataset with 17B images). Architecturally, it introduced two improvements: Gram anchoring and axial RoPE (Rotary Positional Embeddings) with jittering. Gram anchoring applies teacher-student self-distillation for the Gram matrix between the feature vectors of the patches of an image. It avoids the previously observed problem of degradation of dense feature maps: While performance on global tasks (like classification) continued to improve, performance on dense tasks (like segmentation) would peak early and then decline, with feature maps becoming noisy. Axial RoPE makes the model more robust to varying image resolutions, scales, and aspect ratios. Swin Transformer The Swin Transformer ("Shifted windows") took inspiration from standard CNNs: Instead of performing self-attention over the entire sequence of tokens, one for each patch, it performs "shifted window based" self-attention, which means only performing attention over square-shaped blocks of patches. One block of patches is analogous to the receptive field of one convolution. After every few attention blocks, there is a "merge layer", which merges neighboring 2x2 tokens into a single token. This is analogous to pooling (by 2x2 convolution kernels, with stride 2). Merging means concatenation followed by multiplication with a matrix. It is improved by Swin Transformer V2, which modifies upon the ViT by a different attention mechanism: LayerNorm immediately after each attention and feedforward layer ("res-post-norm"); scaled cosine attention to replace the original dot product attention; log-spaced continuous relative position bias, which allows transfer learning across different window resolutions. TimeSformer The TimeSformer was designed for video understanding tasks, and it applied a factorized self-attention, similar to the factorized convolution kernels found in the Inception CNN architecture. Schematically, it divides a video into frames, and each frame into a square grid of patches (same as ViT). Let each patch coordinate be denoted by x , y , t {\displaystyle x,y,t} , denoting horizontal, vertical, and time. A space attention layer is a self-attention layer where each query patch q x , y , t {\displaystyle q_{x,y,t}} atten