[site]: crossvalidated
[post_id]: 295041
[parent_id]: 295016
[tags]: 
Of course HPO makes a difference. Take for example, the number of layers in a neural net. A neural net with 7 layers is much more likely to perform better on a large data set than say, a 1 layer neural network. In terms of real life examples, if you look at the results from the Image Net competition, the winners of the competition are coming up with good combinations of hyperparameters for the # of neurons in a layer, # of layers, activation functions, etc. The difference between good hyperparameter optimization and poor HPO can make a huge impact in the accuracy/error.
