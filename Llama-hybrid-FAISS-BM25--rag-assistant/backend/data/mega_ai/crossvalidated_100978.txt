[site]: crossvalidated
[post_id]: 100978
[parent_id]: 100974
[tags]: 
They define $\tau$ & $\bar{y}$ too: $\tau$ is the fraction of 1's in the population; $\bar{y}$ is the observed fraction of 1's in the sample (based on prior information). You'd typically use prior correction when you've sampled based on the outcome; which I'd guess you haven't here. But if you have, then $\bar{y}=\frac{450}{90450}$ & you need to know or estimate $\tau$ in some other way. Down-sampling, as described (quite correctly) in your last paragraph, can help if the full sample is too large for your computer's memory to hold or for its processor to deal with quickly, by sacrificing a little precision. But in this case you've fit the model on all the data already (I doubt it took very long). [What you describe in your edit is what I called down-sampling, & you're applying the prior correction correctly. In medical statistics it's called a case–control design—see here . You might want to do it when you have the response but not yet the predictors, & there's an extra cost to measuring those. I don't know why you're calling it "bias correction for a rare event" though: it's a correction of the intercept for the deliberately introduced sampling bias. Section 5 of the paper deals with correcting the bias of maximum-likelihood estimates of log odds ratios & predicted probabilities.]
