[site]: crossvalidated
[post_id]: 363155
[parent_id]: 362331
[tags]: 
The optimal way of combining estimators or predictors depends on the loss function that you are trying to minimize (or the utility function you are trying to maximize). Generally speaking, if the loss function measures prediction errors on the response scale, then averaging predictors on the response scale is correct. If, for example, you are seeking to minimize the expected squared error of prediction on the response scale, then the posterior mean predictor will be optimal and, depending on your model assumptions, that may be equivalent to averaging predictions on the response scale. Note that averaging on the linear predictor scale can perform very poorly for discrete models. Suppose that you are using a logistic regression to predict the probability of a binary response variable. If any of the models give a estimated probability of zero, then the linear predictor for that model will be minus infinity. Taking the average of infinity with any number of finite values will still be infinite. Have you consulted the references that you list? I am sure that Hoeting et al (1999) for example discuss loss functions, although perhaps not in much detail.
