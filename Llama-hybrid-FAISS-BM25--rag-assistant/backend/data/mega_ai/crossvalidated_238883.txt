[site]: crossvalidated
[post_id]: 238883
[parent_id]: 238548
[tags]: 
Data fed to a neural network is standardized (vague term) s.t the magnitude of the data is not too large because too nominally large values leads to slow training (many steps to get right size of weights) and possibly exploding gradients (too big steps at the same time) leading to NaN, killing the ANN. It's also typically very nice to have data that is centered around 0 depending on the activation function. 'Standardizing' as $(x-\bar{x})/s$ is one way of doing this. If data becomes approximately normal it's nice but not necessary. (This won't happen anyway as the left tail won't grow further than the estimated mean as $W\geq 0$) Point is that you don't need to spoonfeed an ANN, it'll eat almost anything . Besides you use it to learn the distribution of the data so I wouldn't worry too much. Your question is vague, but if data is approximately Weibull$(\alpha,\beta)$ you can easily estimate the parameters and invert it to approximately uniform distribution since $W=\alpha(-\log(U))^{1/\beta}$. This can then be mapped as $(2U-1)\in [-1,1]$ if you really worry about it. Much less headache is to transform it as $\log(W+\epsilon)$ if you really want it to behave normalish. The standardized sample mean of $log(W)$ and any $(-\infty,\infty)$ random var is approximately standard normal but not $log(W)$ itself. ps. I think the whole discussion about discretization is OT. First of all a discrete weibull with high $\alpha$ i.e resolution is almost perfectly fit with a continuous Weibull. I included a discretized distribution below Continuous Weibull Discretized (rounded up) Weibull a=100 b=1.1 n=100000 u = rnorm(n) #png(filename="answer_discrete.png") par(mfrow=c(2,2)) w =(a*(-log(u))^(1/b)) w = ceiling(w) w_std = (w-mean(w,na.rm=T))/sd(w,na.rm=T) hist(w,100,main='w',xlab='') hist(w_std,100,main='standardized w',xlab='') w = log(w) w_std = (w-mean(w,na.rm=T))/sd(w,na.rm=T) hist(w,100,main='log(w)',xlab='') hist(w_std,100,main='standardized log(w)',xlab='') #dev.off() edit: had W=a log(u)^(1/b), should be a (-log(u))^(1/b). Updated graphs/code to reflect this
