[site]: crossvalidated
[post_id]: 355766
[parent_id]: 355744
[tags]: 
Since I'm using relu as activation function so I suppose gradient vanishing is not the reason, what other reason could cause the behaviour? Biases can directly move the output of your network no a non-zero mean level of Q value, when your problem is designed such that Q values are positively or negatively biased. Moreover, I read a couple of opinions that adding bias weights is not a good option in neural network Q learning, since they introduce the bias in estimates, making NN outputs dependent. (This is not a solid recommendation though.) The reason why other weigths do not change a lot is that learning your Q value is very noisy and any change in weights is pointless. Is there any direction for hyperparameter tuning since grid search costs a lot of time. Approaches exist. You can for example use a general purpose optimization method to tune hyperparameters to maximize the NN convergence (maximization of Q value). I believe some form of gradient descent can be an option with some limitations.
