[site]: crossvalidated
[post_id]: 388723
[parent_id]: 377747
[tags]: 
For the first question, you have to use Bayes' theorem. Let $\pi_0, \pi_1$ be prior probabilities on the two classes, and denote the multinormal density function by $f$ . Then $$ \DeclareMathOperator{\P}{\mathbb{P}} \P(Y=1 \mid X=x) = \frac{\pi_1 \exp \left( -\frac12 (x-\mu_1)^T \Sigma^{-1}(x-\mu_1) \right)}{\pi_1 \exp \left( -\frac12 (x-\mu_1)^T \Sigma^{-1}(x-\mu_1) \right) + \pi_0 \exp \left( -\frac12 (x-\mu_0)^T \Sigma^{-1}(x-\mu_0) \right)} \\ = \frac1{1+\frac{\pi_0}{\pi_1} \exp\left( -\frac12 (x-\mu_0)^T \Sigma^{-1} (x-\mu_0) + \frac12 (x-\mu_1)^T\Sigma^{-1} (x-\mu_1)\right)} $$ and after some more algebra this finally simplifies to $$ \P(Y=1 \mid X=x) = \frac1{1+\exp\left( -\frac12 (x-\mu_0)^T \Sigma^{-1} (x-\mu_0) + \frac12 (x-\mu_1)^T\Sigma^{-1} (x-\mu_1)\right)+\ln(\pi_0/\pi_1)} $$ which has the logistic regression form. But this is somewhat different from the claim you attributes to Andrew Ng, as he seems to leave the prior probabilities out (or just assume both is $\frac12$ .) You can replicate this argument in the case of three or more classes.
