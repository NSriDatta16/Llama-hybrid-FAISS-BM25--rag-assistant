[site]: datascience
[post_id]: 108854
[parent_id]: 108838
[tags]: 
I don't think there is much to understand here. In cases where your error can be sometimes very large, you don't want your overall average to be skewed by these freak events if $\alpha=\arctan\left(x\right)$ then: $\alpha\approx x$ when $x$ is small $\alpha\to\pi/2\approx 1.57$ when $x\to+\infty$ . So if you have errors $E_i=\left(A_i -F_i\right)/A_i$ for actual ( $A_i$ ) and forecast ( $F_i$ ) at time $i$ , transforming your errors using $\arctan$ and averaging the result (assuming $i=1\dots N$ ): $$ MAAPE=\frac{1}{N}\sum_{i=1}^N \arctan(\left|E_i\right|) $$ Will simply give you a measure that is approximately equal to normal MAPE for small errors, and is only marginally increased for large ones. Arctan is not special in this respect. One could choose a sigmoid, for example. Excell support ARCTAN https://support.microsoft.com/en-us/office/atan-function-50746fa8-630a-406b-81d0-4a2aed395543
