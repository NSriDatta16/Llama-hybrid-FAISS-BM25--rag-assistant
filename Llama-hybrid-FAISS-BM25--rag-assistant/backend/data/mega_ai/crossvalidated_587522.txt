[site]: crossvalidated
[post_id]: 587522
[parent_id]: 
[tags]: 
When to use standard deviation versus standard error in linear error propagation

I have a question about linear error propagation. Let's say that I want to use an equation to calculate n, where n = (PV)/(RT) (eq.1) I only take one measurement of P, and one measurement of T, but I know their nominal uncertainties (e.g., .08% and 0.5) and will use those as the 'standard deviation' terms in standard linear error propagation to find the error of n. However, let's say I perform a separate experiment and collect some number N samples estimating the parameter V. In eq. 1, I will use the average of these N samples, Vavg, assuming Vavg estimates the 'true' value of V. For the error associated with the Vavg parameter I have chosen, should I use the standard deviation of the N samples collected, or the standard error (= standard deviation/sqrt(N))? My thought is to use the standard error, and this is because we surely expect to be more confident in the estimator for the true value of V (Vavg in this case) given that we take many samples. However, just using the standard deviation, I would have a similar error taking 50 samples compared to, say, 1000, since the standard deviation of the sample distribution will not continually decline when taking more samples as the standard error does. In this case, let's assume the Vavg is an unbiased estimator as a result of there being no systematic error in the individual measurements of V or how Vavg is calculated. I've looked in many places but could not find much discussion or consensus on this topic. What is the correct approach in this situation?
