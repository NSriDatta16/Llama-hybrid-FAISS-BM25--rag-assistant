[site]: crossvalidated
[post_id]: 599013
[parent_id]: 
[tags]: 
Why does a neural network trained with random data and fixed initialization have different weights between runs?

I wrote a simple code that creates a neural network with two dense layers and then trains it. In this code, the initial coefficients of each layer are fixed. Why do the answers change every time? (In situations where layers such as dropout are not used and the initial coefficients are also fixed) ``` import numpy as np import keras from keras import layers x = np.random.random([200,4]) y = np.random.random([200,1]) model = keras.Sequential() model.add(layers.Dense(units=4,activation='relu',input_shape (x.shape[1:]),kernel_initializer='ones',bias_initializer='ones')) model.add(layers.Dense(units=1,kernel_initializer='ones',bias_initializer='ones')) opt = keras.optimizers.Adam(learning_rate = 0.0001) model.compile(loss = keras.losses.categorical_crossentropy, optimizer= opt, metrics=['mae']) history = model.fit(x, y, epochs=50) ```
