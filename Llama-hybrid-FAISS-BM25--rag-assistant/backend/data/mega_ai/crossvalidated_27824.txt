[site]: crossvalidated
[post_id]: 27824
[parent_id]: 27345
[tags]: 
In understanding the difference between likelihood ratios and Bayes factors, it is useful to consider one key feature of Bayes factors in more detail: How do Bayes factors manage to automatically account for the complexity of the underlying models? One perspective on this question is to consider methods for deterministic approximate inference. Variational Bayes is one such method. It may not only dramatically reduce the computational complexity of stochastic approximations (e.g., MCMC sampling). Variational Bayes also provide an intuitive understanding of what makes up a Bayes factor. Recall first that a Bayes factor is based on the model evidences of two competing models, \begin{align} BF_{1,2} = \frac{p(\textrm{data} \mid M_1)}{p(\textrm{data} \mid M_2)}, \end{align} where the individual model evidences would have to be computed by a complicated integral: \begin{align} p(\textrm{data} \mid M_i) = \int p(\textrm{data} \mid \theta,M_i ) \ p(\theta \mid M_i) \ \textrm{d}\theta \end{align} This integral is not only needed to compute a Bayes factor; it is also needed for inference on the parameters themselves, i.e., when computing $p(\theta \mid \textrm{data}, M_i)$. A fixed-form variational Bayes approach addresses this problem by making a distributional assumption about the conditional posteriors (e.g., a Gaussian assumption). This turns a difficult integration problem into a much easier optimisation problem: the problem of finding the moments of an approximate density $q(\theta)$ that is maximally similar to the true, but unknown, posterior $p(\theta \mid \textrm{data},M_i)$. Variational calculus tells us that this can be achieved by maximising the so-called negative free-energy $\mathcal{F}$, which is directly related to the log model evidence: \begin{align} \mathcal{F} = \textrm{log} \; p(\textrm{data} \mid M_i) - \textrm{KL}\left[q(\theta) \; || \; p(\theta \mid \textrm{data},M_i) \right] \end{align} From this you can see that maximising the negative free-energy does not only provide us with an approximate posterior $q(\theta) \approx p(\theta \mid \textrm{data},M_i)$. Because the Kullback-Leibler divergence is non-negative, $\mathcal{F}$ also provides a lower bound on the (log) model evidence itself . We can now return to the original question of how a Bayes factor automatically balances goodness of fit and complexity of the involved models. It turns out that the negative free-energy can be rewritten as follows: \begin{align} \mathcal{F} = \left\langle p(\textrm{data} \mid \theta,M_i) \right\rangle_q - \textrm{KL}\left[ q(\theta) \; || \; p(\theta \mid M_i) \right] \end{align} The first term is the log-likelihood of the data expected under the approximate posterior; it represents the goodness of fit (or accuracy ) of the model. The second term is the KL divergence between the approximate posterior and the prior; it represents the complexity of the model, under the view that a simpler model is one which is more consistent with our prior beliefs, or under the view that a simpler model does not have to be stretched as much to accommodate the data. The free-energy approximation to the log model evidence shows that the model evidence incorporates a trade-off between modelling the data (i.e., goodness of fit) and remaining consistent with our prior (i.e., simplicity or negative complexity). A Bayes factor (in contrast to a likelihood ratio) thus says which of two competing models is better at providing a simple yet accurate explanation of the data.
