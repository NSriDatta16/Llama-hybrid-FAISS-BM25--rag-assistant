[site]: crossvalidated
[post_id]: 517710
[parent_id]: 488947
[tags]: 
The word embeddings are weights in the model. They get learned just like any other. What you input to your model isn't a word embedding. Instead, you provide one-hot vectors for each word. These get multiplied by the matrix of embeddings to select the embedding for each word. That matrix is a parameter of your model. Equivalently and more space-efficiently, you can provide a numerical index to select the row with the word's embedding, rather than the one-hot vector. The multiplication route says that we multiply the embedding matrix W by [0 1 0 0 0] ; the indexing route says that we select the row W[2] instead. Either way, all of the embeddings are stored in that matrix W which is a parameter of the model. There is one row for each word in the model's vocabulary.
