[site]: datascience
[post_id]: 118123
[parent_id]: 118122
[tags]: 
The purpose of splitting our data into training, validation, and test sets is to evaluate our models with data that they have not seen before and to directly compare them (and to choose the best model based on that comparison). However, the more we use the test and validation sets to compare models or choose hyperparameters, the more we are relying on them to choose models that perform the best on them, that is, our models begin to depend indirectly (through us selecting them) on the test set, therefore defeating their initial purpose of evaluating our models with data they have not seen before. The problem with randomly re-partitioning the sets into different train/validation/test sets for each problem is that it does not enable you to directly compare the performance of the different models, therefore defeating the very purpose of having a test set. A solution is proposed in the paragraph you cited: If possible, it's a good idea to collect more data to "refresh" the test set and validation set. Starting anew is a great reset.
