[site]: crossvalidated
[post_id]: 546235
[parent_id]: 
[tags]: 
How prior distribution over neural networks parameters is implemented in practice?

I have read some papers that mentioned about using standard normal distribution as prior distribution over deep neural networks parameters or use normal distribution with some configurable variance. But so far I can't find information on how this is actually implemented in practice. In my naive understanding, for every parameters in the DNN, I would draw samples from normal distribution in the form of tensors with dimensionality matching the dimensionality of the DNN parameters. Is this how it is commonly done in practice, if not how the prior over DNN parameters supposed to be implemented? Because I can't find any example implementation that sample normal distribution to set DNN parameters so far. For example, this paper mentioned that "the prior on the parameters is set to $p(\theta) = N(0, \sigma^2I)$ ", where $\sigma^2$ is set to 1 by default and 100 in one of the experiment scenario. But I can't find anything that sample from normal distribution to set the DNN parameters anywhere in the official source code for that paper. I also vaguely remember reading about prior in DNN implemented in the form of regularization term or weight decay, but couldn't find any explanation on how regularization or weight decay maps to implementing normal distribution prior over the parameters i.e how the regularization or weight decay formula supposed to be changed if I would increase the variance of the normal distribution prior.
