[site]: datascience
[post_id]: 26094
[parent_id]: 492
[tags]: 
Here are some ideas about how to use word vectors for NER, which adopts a mostly unsupervised word2vec-centric approach. Given a set of word vectors (either something you trained, or something off-the-shelf like GoogleNews-vectors-negative300.bin), discover clusters in the vector space. These clusters are basically your definitions for various unnamed concepts. With minimal supervision, you can map/convert the unnamed clusters to match human knowledge, thus creating named concepts grounded on top of the known word vectors and unnamed concepts. For example, a method findCluster(['joy', 'surprise', 'disgust', 'trust', 'fear', 'sadness', 'anger', 'anticipation']) might return a list containing hundreds of words mostly related to emotion. If you name this list 'emotion', then there you have a named concept 'emotion' defined based on the vector space. You can also do vector math to find the intermediate concept between two given ones. For example, the vector math might tell you that when given two words 'surprise' and 'disgust', the following are found between the two: dismay, astonishment, disappointment, amazement, bewilderment, exasperation, disbelief, shock, etc. This allows you to build up relationship between concepts. You can repeat the above to build up various types of named concpts, like: weekdays, all emotions, happy emotions, vehicles, etc. Once you have built up layers of named concepts, you can then train a RNN on a text corpus that has been augmented with the named concepts, so 'brown fox jumps' is also '{color} {animal} {action}' etc. This way the RNN should be able to learn some rudimentary grammar in an unsupervised manner. If you have built up a sufficiently powerful grammar from the above, then you should be able to apply it toward some of your NER tasks.
