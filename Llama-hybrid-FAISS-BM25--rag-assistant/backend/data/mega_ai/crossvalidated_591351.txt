[site]: crossvalidated
[post_id]: 591351
[parent_id]: 
[tags]: 
Combining Dirichlet and Gamma-Normal distributions

I have a model that describes 2 dimensional data where each data points is define as d = [category, x]. The category dimension can take 3 different values with respective probability $p_1$ , $p_2$ and $p_3$ (with $p_1+p_2+p_3=1$ ). This can be modeled by a multinomial distribution $MN(p)$ with $p=[p1,p2,p3]$ . The $x$ dimension is taken from a normal distribution with mean $\mu$ and standard deviation $\sigma$ which both depends on the category, hence $\mu=[\mu_0, \mu_1, \mu_2]$ and $\sigma=[\sigma_1, \sigma_2, \sigma_3]$ . All parameters are a priori unknown. Here is the python code that generates the data. # Imports import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns # for freq stat from scipy.stats import norm, multinomial, dirichlet, gamma # category p = [0.3, 0.6, 0.1] # multinomial probabilty N = 500 # number of data category_nbr = multinomial.rvs(n=N, p=p) # number of data for each category # normal distribution for each category means = [150, 250, 400] sigmas = [50, 80, 120] x0 = norm.rvs(loc=means[0], scale=sigmas[0], size=category_nbr[0]) x1 = norm.rvs(loc=means[1], scale=sigmas[1], size=category_nbr[1]) x2 = norm.rvs(loc=means[2], scale=sigmas[2], size=category_nbr[2]) category_data = np.concatenate((np.full(category_nbr[0], 0), np.full(category_nbr[1], 1), np.full(category_nbr[2], 2))) x_data = np.concatenate((x0, x1, x2)) data = pd.DataFrame({"category":category_data, "x":x_data }) sns.histplot(x0, color="blue", label="category 0", bins=20) sns.histplot(x1, color="red", label="category 1", bins=20) sns.histplot(x2, color="green", label="category 2", bins=20) sns.histplot(x_data, color="gray", label="total", alpha=0.3, bins=20) plt.legend() plt.show() Now I which to use Bayesian inference in order to estimate a posterior distribution of the total mean. Instead of performing brute force MCMC sampling using PyMC3, I though that I could solve the problem by calculating the posterior for each of the parameters separately and then, somehow joining them together. For the category, the conjugate prior of the multinomial distribution is the Dirichlet distribution $D(\alpha)$ which is parameterized by the vector $\alpha=[\alpha_0, \alpha_1, \alpha_2]$ . Starting with a non-informative prior $\alpha=[1,1,1]$ , the posterior distribution is easily obtained from the data. Here is the code and the resulting posterior (the black line corresponds to the "real" value of $p$ ). # Dirichlet posterior for the multinomial probabilty alpha_prior = np.ones(3) # non-informative prior alpha_posterior = alpha_prior+np.asarray([len(data[data["category"]==0]), len(data[data["category"]==1]), len(data[data["category"]==2])]) rvs_dirichlet = dirichlet.rvs(alpha_posterior, size=1000) # samples from Dirichlet posterior for i in range(1000): plt.plot(rvs_dirichlet[i,:], alpha=0.3) plt.plot(p, 'k-o') plt.xticks(label=[0,1,2], ticks=[0,1,2]) plt.show() For the distribution of $x$ , i.e the parameter $\mu$ and $\sigma$ can be obtained from the Gamma-Normal conjugate prior of the normal distribution when both the mean and variance (or precision) are unknown or from a Normal conjugate prior if we assume one or the other to be fixed. I managed to get the former following this reference , leading to the following results for the posterior distribution (here for category 2 ). The black dot corresponds to the "real" value $[\mu_2, \sigma_2]$ . The total mean is given by $\mu_{total} = \sum_{i=0,1,2}p_i \mu_i$ So now I have (the analytical form of) the posterior distributions for p and for each $\mu_i$ . How can I combine them to obtain a posterior distribution of $P(\mu_{total}|p,\mu)$ without relying on sampling ?
