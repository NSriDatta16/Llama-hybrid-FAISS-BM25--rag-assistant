[site]: crossvalidated
[post_id]: 625198
[parent_id]: 538747
[tags]: 
Here is another approach to address this problem by means of combining p-values You can perform one-sided t-tests individually for each website, obtaining p-values. Here, significant difference would imply rejection of the null hypothesis (that application of the technique has no effect) in favour of the alternative hypothesis (that application of the technique improves performance measure). Note that if you would choose to stop here, all p-values would need to be corrected for multiple testing, for example using a Bonferroni correction . Since you only have 10 datapoints per website and 26 websites, it is likely that this approach would not find any significant changes because we ask too many questions Instead of doing a Bonferroni correction, we can combine p-values of all of the above tests (e.g. using Fisher's method ), resulting in a single p-value. In this case, we are effectively performing only a single hypothesis test, so there is no need for multiple comparisons. In this case, the null hypothesis is that the treatment has no effect over all websites, and that any significant effects in individual websites are due to chance. I think this approach answers precisely the question that you are asking. In practice it will perform similarly to the model-based approach proposed by @RobertLong. Last thing to mention is the choice of the test. The combination of p-values will work the same way if you choose Wilcoxon test. However, with 10 datapoints, it will be more of a test of consistency of the effect across websites than a test of magnitude. For example, if the effect is very strong in only one website, then the combined t-test would find the overall result significant even if there are no significant changes in the rest of the websites, whereas Wilcoxon might not, as getting 5 random points ranked higher than 5 other random points by chance is unlikely but not impossible. I think you should pick between t-test and Wilcoxon depending on the exact question you are trying to answer. I think it is ok to use a t-test even if its assumptions are not perfectly met. If you observe large differences in magnitude and get a p-value of 1.0E-10, then it does not matter if it is wrong by an order of magnitude. If you get a p-value of 0.01 with any method, you always have to question whether you have gathered enough data, regardless if the method is perfectly accurate.
