[site]: crossvalidated
[post_id]: 335585
[parent_id]: 335523
[tags]: 
In general, the derivations in RL work with expectations, because it is possible to show important relations analytically, such as Bellman optimality. In Dynamic Programming, where you have a model, it is possible to expand from an expectation, using known probabilities from the model. Your reference text does this on page 4 for $Q^{\pi}(s,a)$. If you know the policy and environment model, the precise values of Q can be calculated. This is how Policy Iteration and Value Iteration work. When you have a formula involving an expectation of some value from a system, and access to the system but not its operating parameters (the state transition probabilities in an MDP), it is possible to sample the value, and use that sample in stochastic approximations, because over many samples the eventual average will be the same as the expectation. Essentially, Q Learning, SARSA, Monte Carlo Control are all algorithms that approximate Value Iteration from Dynamic Programming, by taking samples to resolve expectations in the long term, instead of calculating them over a known probability distribution. As an aside, often $\pi(a|s)$ is known or under control of the learning agent, so it can be used to partially resolve expectations even when sampling. This knowledge is used in extensions to basic algorithms such as importance sampling and Expected SARSA.
