[site]: datascience
[post_id]: 116824
[parent_id]: 53858
[tags]: 
Proof will only work for convex spaces. Though not even once have I stumbled upon one in professional work. Drift analysis might be more helpful for non-convex spaces. â€“ Piotr Rarus Adding more to Piotr Rarus comment: It appears that the convergence of the REINFORCE Monte Carlo algorithm for vanilla policy gradient procedures has not been formally proven . The literature, including Sutton and Barto's book on reinforcement learning , only provides claims of convergence under certain stochastic approximation step size requirements. The paper by Bottou on online stochastic gradient descent algorithms provides a general proof of convergence, but it is not clear if it is applicable to the REINFORCE algorithm due to the changing distribution of the on-policy state. Bertsekas and Tsitsiklis provide convergence proof for updating the rules of the form $x_{t+1} = x_t +\gamma_t (s_t + w_t)$ where, $w_t$ is some error with $\mathbb{E}[w_t | \mathcal{F}_t] = 0$ for ascending $\sigma$ -fields $\mathcal{F}_t$ . This may be applicable to the REINFORCE algorithm , but further analysis would be needed to verify this. Overall, it seems that a formal proof of convergence for the REINFORCE Monte Carlo algorithm for vanilla policy gradient procedures is currently lacking .
