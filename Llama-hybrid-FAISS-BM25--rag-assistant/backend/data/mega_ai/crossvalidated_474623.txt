[site]: crossvalidated
[post_id]: 474623
[parent_id]: 474621
[tags]: 
At some point, adding more data will result in overfitting and worse out-of-sample prediction performance. Always. That papers report improved accuracy by leveraging additional data is not surprising at all. After all, people (both in academia and in industry) are heavily incentivized to report precisely this. Here is the relevant algorithm: 1. Pick an external dataset D. 2. Can you tell a story about how D *might* improve accuracy? If no: GOTO 1 3. Fit your model using D. Does it improve accuracy? If no: GOTO 1 4. Publish your accuracy improvement using D. Bonus points if you can get a press release. Note how a publication only happens if accuracy improves. You don't see all the loops where accuracy didn't improve. This is called a "file drawer effect" (everything that is not successful ends up in a file drawer). The end result is a strong publication bias . Note also that step 2 is crucial. An ability to tell a story about how the accuracy improvement might have come about is indispensable, because if you don't have such a story, it's too blatant that you went on a wild goose chase. So: in order to know whether your external data actually did improve matters, you always need to keep from "overfitting on the test set", as the algorithm above does. If you follow this algorithm, don't be surprised if the "winner" does not perform as well in production as after this selection process (which in itself is an example of regression to the mean ).
