[site]: crossvalidated
[post_id]: 395007
[parent_id]: 394989
[tags]: 
Typically in machine learning (or statistics), we don't think of features/covariates as random variables. Or at least, we often don't care about the random nature of our features. We just want to model $P(y | X)$ , where $y$ is the outcome of interest, and $X$ is our set of covariates. We care about the random nature of $y$ given $X$ , but we assume $X$ is known so we don't bother with modeling the random nature of it. Naive Bayes is a special case, though. Under Naive Bayes, we think of things in terms of a hierarchical process. At the very top is the outcome $y$ . This is simply a binomial random variable (assuming binary outcomes). Then, our features follow a distribution conditional on the value of $y$ . If we know the $P(y = 1)$ and $P(X | y = 0)$ and $P(X | y = 1)$ , we can use Bayes Theorem to compute $P(y = 1 | X)$ . The key point here is that using this approach, we need a model for $P(X | y)$ to make inference about $P(y | X)$ . So for this particular model, you do need to think about the distribution of $X$ , in contrast with most other models that allow you to directly compute $P(y |X)$ and ignore the random aspect of $X$ .
