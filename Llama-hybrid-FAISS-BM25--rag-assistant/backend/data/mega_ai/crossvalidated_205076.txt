[site]: crossvalidated
[post_id]: 205076
[parent_id]: 204154
[tags]: 
After some research, is seems that my intuition and Alex R. comment are right. In order to build a continuous model with predictions in $[0,1]$, one can put the model $H$ into a logistic function (Wikipedia) , such that for $H \in \mathbb{R}$, we have $$\frac{1}{1 + e^{-H}} \in [0,1]$$ The gradient boosting steps then take the derivative with respect to $H$ and update the model, as if the logistic function was part of the cost function, and it works. This has been suggested in the paper Additive logistic regression: a statistical view of boosting , by Friedman, Hastie and Tibshirani, to build LogitBoost (Wikipedia) , an adaptation of AdaBoost (Wikipedia) to the Logistic Loss. In very basic terms, if it is possible to go from linear regression to logistic regression by the addition of a sigmoid, then it also works to convert regression boosting to classification boosting.
