[site]: datascience
[post_id]: 121731
[parent_id]: 121727
[tags]: 
The processing learned by neural networks is often referred to as a "black box" because we can't fully characterize it to understand it, that is, it's not "interpretable". This way, the processing you refer to is something that the network learns during its training but it's not something that we can interpret. Therefore, the answer to your question is that "we don't know" the exact characteristics of the processing done by the network due to its very black-box nature. You can check this answer for some more context on the interpretability of neural networks.
