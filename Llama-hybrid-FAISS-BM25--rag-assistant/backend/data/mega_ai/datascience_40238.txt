[site]: datascience
[post_id]: 40238
[parent_id]: 26980
[tags]: 
First, you should be sure on that your data is large enough when working with tree-based algorithms like XGBoost and LightGBM, such sudden changes may indicate overfitting. (10,000 samples at least, rule of thumb) Second, how is your cardinality; if you have 3-4 features, it would be expected that a change of feature causing such an affect. Third, what are your selection of hyperparameters? Tree-based models are much sensitive to changes of the parameters. Be sure that you carefully implement your hyperparameter tuning. Lastly, when dealing with binary classification; error metrics gets really important. You can do a combination of binary log loss and binary error (XGBoost allows you to choose multiple); also be sure to implement early stopping by choosing early_stopping_rounds = N in the train method of XGBoost, where N is the selection of iterations. By that, your algorithm will stop early at a reasonable point where your loss stops to decrease, avoiding overfitting.
