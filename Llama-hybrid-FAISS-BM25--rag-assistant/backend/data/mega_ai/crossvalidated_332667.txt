[site]: crossvalidated
[post_id]: 332667
[parent_id]: 235600
[tags]: 
For this dummy problem you can increase the number of features. One particular way that I found to work is using extreme learning machines. Basically, you create a random matrix $K$ with columns equal to number of old features, $d$, and rows equal to number of new features $d'$(I had to use $d'=300d$). Also, create a random bias vector $b$ with length equal to $d'$. And you need a non-linear activation function $f$. Relu in particular works well --- $Relu(X) = max(X,0)$. Then perform linear logistic regression on the new data $X'=f(XK+b)$ (sloppy numpy or matlab notation for adding $b$ to every row of $XK$). Here is a small code using the linear logistic regression of scikit-learn in python. import numpy as np import matplotlib.pyplot as plt import sklearn.linear_model f = np.loadtxt("spiral.data") x, y = f[:, :2], f[:, 2] new_feature_ratio = 300; def relu(Y): return np.maximum(Y, 0) cls = sklearn.linear_model.LogisticRegression( penalty='l2', C=1000, max_iter=1000) K = np.random.randn(x.shape[1], x.shape[1]*new_feature_ratio) b = np.random.randn(x.shape[1]*new_feature_ratio) cls.fit( relu(np.matmul(x,K) + b) ,y) xmin, xmax = x[:,0].min()-0.1, x[:,0].max()+0.1 ymin, ymax = x[:,1].min()-0.1, x[:,1].max()+0.1 xx, yy = np.meshgrid(np.arange(xmin, xmax, 0.01), np.arange(ymin, ymax, 0.01)) xnew = np.c_[xx.ravel(), yy.ravel()] ynew = cls.predict(relu(np.matmul(xnew,K) + b)).reshape(xx.shape) fig = plt.figure(1) plt.set_cmap(plt.cm.Paired) plt.pcolormesh(xx, yy, ynew) plt.scatter(x[y>0,0], x[y>0,1], color='r') plt.scatter(x[y spiral.data is the same as Frank's answer. This strategy is basically a neural network where the first layer is chosen randomly rather than being trained.
