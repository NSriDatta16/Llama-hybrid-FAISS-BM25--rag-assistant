[site]: crossvalidated
[post_id]: 64271
[parent_id]: 
[tags]: 
Variable Selection for a binary classification problem

I have a binary response variable (0,1) with 300 (approximately) normally distributed variables as predictors all on (0,1). The training set has only 250 observations and the test set has approx. 20k observations. The goal is to make a model to best classify the test set observations as either 0 or 1. (it's an approximately balanced training set, with around 125 each 0 and 1). With 300 variables and only 250 observations, I'm attempting to do a little bit of variable reduction. First I simply checked the correlation between the predictor and each of the response variable, to see if there were any variables that were not terribly correlated with the response (relative to the others): there doesn't seem to be any real break where I could reasonably say these variables are much more correlated with the response and i'll toss the rest. So I tried a PCA analysis: train_pca I graph the rotation of each of the vectors to see again if there are any that are much greater that explain the variation than the others. Again not any big break where I can decide that certain variables are better than the others. Lastly I built a random forest with the importance control on: randy_1 and made an importance graph to look at the variables: There appear to a few variables that appear better than others, but if you look at the mean gini reduction for all the variables, there really is just a uniform descent. So basically at this point I'm more or less looking to either (1) use all the variables which isn't right (I don't think), b/c the question I've been posed suggests that variable reduction was necessary, or (2) pick a point that seems sort of arbitrary to cut off which variables to use and which to not consider (either the top half of the PCA or top half of random forest importance variables for example). Anyone have any other ideas for variable selection that I haven't considered but should be? Thanks.
