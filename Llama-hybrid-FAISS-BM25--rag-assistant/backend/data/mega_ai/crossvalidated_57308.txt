[site]: crossvalidated
[post_id]: 57308
[parent_id]: 
[tags]: 
Activation value at output neuron equals 1, and the network doesn't learn anything

I'm implementing a typical neural network with 1 hidden layer. The network does well with the logic XOR and other simple problems, but fails miserably when encountering a (16-input, 20~30 hidden, 3 output) training set. The problem is that the network does not learn anything, even with learning rate = 1.0 and momentum 0.8 ~ 0.9. I did a lot of debug and found that at some output, the activation value is 1.0, even though mathematically the result of the logistic function (1/(1+exp(-signal)) can not be 1, but due to rounding, the computer produces 1.0 for signal that is >40. Now, with the activation value equals 1.0, the gradient is always 0 regardless of the desired output because the term (1-activation) will be 0, so in the equation: delta = (1-activation) * (activation) * (activation - desired_output) delta will be zero all the time, so no learning at all. I tried to initialize the weights at very small value (0.00001 or so) but eventually all of them reach 3.0 ~4.0, or even some extreme value like 30~40, so with 20~30 hidden neurons it's very easy to end up with output signal >40. Obviously I must have missed something, but with this kind of neural net (with hundreds of weights) I can't debug it too see what's wrong, so my question is: do all of the above symptoms signal anything wrong with my code? Here are some important code: class NNWeight { double ** weights; double *bias; } double sigmoid(double value) { return 1.0/(1.0+exp(-value)); } double Dsigmoid(double value){ //gradient return value*(1-value); } //wInpHid = input-to-hidden layer weights (of type NNWeight) //wHidOut = hidden-to-output layer weights (of type NNWeight) void updateWeight() { for (int iInp = 0; iInp
