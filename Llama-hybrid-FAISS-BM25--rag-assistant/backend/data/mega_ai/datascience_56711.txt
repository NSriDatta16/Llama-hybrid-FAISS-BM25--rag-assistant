[site]: datascience
[post_id]: 56711
[parent_id]: 56672
[tags]: 
In general, for random forests and boosting the regularization hyperparameters (mtry, max depth, colsample_byX, lambda, alpha and maybe gamma among others) are often sufficient enough such that explicit wrapper based feature selection often makes the model not perform any better or possibly even worse. Feature selection is a binary choice, either we leave a feature completely out or completely in the model. Through regularization, we include all variables but ideally only allow each one to influence predictions to the point where it is beneficial. Wrapper based feature selection can still be useful, but usually for models that have very little regularization. Removing redundant features does improve computational speed which can in turn allow you to try more hyperparameters or more experimentation in other areas like feature engineering. However, it is not like wrapper based feature selection is blazingly fast either since you end up fitting hundreds of models and evaluating them to see if the feature selection method is working anyway. Furthermore, using feature importances properly such that you don't leak information and bias your test set scores is an expensive and honestly annoying task in itself (deriving the feature importances, and then evaluating say the top k features quickly becomes time consuming, albeit easily made parallel). There are also business reasons for why you might do explicit wrapper based feature selection, ex: collecting variables that are not useful could be expensive. I think a case can be made to try wrapper based feature selection if you have the time and patience to do it properly (along with a decent amount of data to get consistent variable importance scores). Otherwise, it is not always necessary and might not even give any improvement especially if you are using newer ML algorithms that have large amounts of regularization already built in.
