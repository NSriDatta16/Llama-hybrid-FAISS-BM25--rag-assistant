[site]: datascience
[post_id]: 1019
[parent_id]: 810
[tags]: 
I think it always depends on the scenario. Using a representative data set is not always the solution. Assume that your training set has 1000 negative examples and 20 positive examples. Without any modification of the classifier, your algorithm will tend to classify all new examples as negative. In some scenarios this is O.K. But in many cases the costs of missing postive examples is high so you have to find a solution for it. In such cases you can use a cost sensitive machine learning algorithm. For example in the case of medical diagnosis data analysis. In summary: Classification errors do not have the same cost!
