[site]: crossvalidated
[post_id]: 641114
[parent_id]: 
[tags]: 
Does Bootstrapping Overcome Limitations in Estimations?

Part 1: Suppose we have some sample data (univariate). We believe that this data came from a Normal Distribution. Using MLE, we can show that the mean estimator will be unbiased but the variance estimator will be biased: $$L(\mu, \sigma^2 | x_1, ..., x_n) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x_i-\mu)^2}{2\sigma^2}}$$ $$l(\mu, \sigma^2 | x_1, ..., x_n) = -\frac{n}{2} \log(2\pi) - \frac{n}{2} \log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^{n} (x_i - \mu)^2$$ Taking the derivative with respect to $\mu$ and setting it equal to zero gives: $$\frac{\partial l}{\partial \mu} = \frac{1}{\sigma^2} \sum_{i=1}^{n} (x_i - \mu) = 0$$ $$\hat{\mu} = \frac{1}{n} \sum_{i=1}^{n} x_i = \bar{x}$$ Taking the derivative with respect to $\sigma^2$ and setting it equal to zero gives: $$\frac{\partial l}{\partial \sigma^2} = -\frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2} \sum_{i=1}^{n} (x_i - \mu)^2 = 0$$ $$\hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2$$ However, we can see that this will be a biased estimate: $$E[\hat{\sigma}^2] = E\left[\frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2\right]$$ $$E[\hat{\sigma}^2] = \frac{1}{n} \sum_{i=1}^{n} E[x_i^2 - 2x_i\bar{x} + \bar{x}^2]$$ $$E[\hat{\sigma}^2] = \sigma^2 - \frac{\sigma^2}{n} \neq \sigma^2 $$ Usually, we can correct for this bias by adding a correction factor - but let's not to do this for argument sake. For example, perhaps this is some complex distribution and its difficult to verify if the estimator is biased or not. Part 2: It seems to me that the variance of the mean estimate will also be biased, since it will depend on $\hat{\sigma}^2$ (i.e. $s^2_{MLE}$ ): $$s^2_{MLE} = \frac{1}{n}\sum_{i=1}^{n}(X_i - \bar{X})^2$$ $$\bar{X} = \frac{1}{n}\sum_{i=1}^{n}X_i$$ $$\text{Var}(\bar{X}) = \text{Var}\left(\frac{1}{n}\sum_{i=1}^{n}X_i\right) = \frac{1}{n^2}\sum_{i=1}^{n}\text{Var}(X_i) = \frac{1}{n^2}n\sigma^2 = \frac{\sigma^2}{n}$$ $$E\left(\frac{s^2_{MLE}}{n}\right) = \frac{1}{n}E(s^2_{MLE}) = \frac{1}{n}E\left(\frac{1}{n}\sum_{i=1}^{n}(X_i - \bar{X})^2\right) = \frac{1}{n}\cdot\frac{n-1}{n}\sigma^2 = \frac{n-1}{n^2}\sigma^2$$ $$E(s^2_{MLE}) = \frac{n-1}{n}\sigma^2 \neq \frac{\sigma^2}{n} $$ My Question: Suppose we take this biased estimator $s^2_{MLE}$ and calculate it many times using random bootstrapped samples, thus creating an empirical sampling distribution of $s^2_{MLE}$ . In this situation (i.e. using the biased estimator), will the properties of this bootstrapped/empirical sampling distribution (e.g. average variance) be equal to the calculations had we used the unbiased estimator (adjusted with the correction factor)? That is, in finite samples - can the bootstrap method overcome limitations stemming from flawed parameter estimation? Or will the bootstrap still produced biased results if the underlying estimation is done incorrectly? Can the bootstrap correct the bias of an "incorrect estimator" in certain situations?
