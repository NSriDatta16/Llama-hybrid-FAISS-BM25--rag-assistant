[site]: crossvalidated
[post_id]: 134577
[parent_id]: 134380
[tags]: 
Rather than computing a probability, why not predict how many products might fail? Modeling the Observations There are $n=100000$ products in the field and another $m=10000$ under consideration. Assume their failures are all independent and constant with probability $p$ . We may model this situation by means of a Binomial experiment: out of a box of tickets with an unknown proportion $p$ of "failure" tickets and $1-p$ "success" tickets, draw $m+n=110000$ tickets (with replacement, so that the chance of failure stays the same). Count the failures among the first $n$ tickets--let that be $X$ --and count the failures among the remaining $m$ tickets, calling that $Y$ . Framing the Question In principle, $0\le X \le n$ and $0 \le Y\le m$ could be anything. What we are interested in is the chance that $Y = u$ given that $X+Y=u$ (with $u$ any number in $\{0,1,\ldots, m\}$ ). Since the failures could occur anywhere among all $n+m$ tickets, with every possible configuration having the same chance, it is found by dividing the number of $u$ -subsets of $m$ things by the number of $u$ -subsets of all $n+m$ things: $$p(u;n,m) = \Pr(Y = u\,|\, X+Y=u) = \frac{\binom{m}{u}}{\binom{n+m}{u}} \\= \frac{m(m-1)\cdots(m-u+1)}{(n+m)(n+m-1)\cdots(n+m-u+1)}.$$ Comparable formulas can be used for the calculation when $X=1, 2, \ldots.$ An upper $1-\alpha$ prediction limit (UPL) for the number of failures in those last $m$ tickets, $t_\alpha(X;n,m)$ , is given by the smallest $u$ (depending on $X$ ) for which $p(u;n,m) \le \alpha$ . Interpretation The UPL should be interpreted in terms of the risk of using $t_\alpha$ , as evaluated before either $X$ or $Y$ is observed. In other words, suppose it is one year ago and you are being asked to recommend a procedure to predict the number of failures in the next $m$ products once the first $n$ have been observed. Your client asks What is the chance that your procedure will underpredict $Y$ ? I don't mean in the future after you have more data; I mean right now, because I have to make decisions right now and the only chances I will have available to me are the ones that can be computed at this moment." Your response can be, Right now the chance is no greater than $\alpha$ , but if you plan to use a smaller prediction, the chance will exceed $\alpha$ . Results For $n=10^5$ , $m=10^4$ , and $X=0$ we may compute that $$p(0,n,m)=1;\ p(1,n,m)=\frac{1}{11}\approx 0.091;\ p(2,n,m)=\frac{909}{109999}\approx 0.0083; \ldots$$ Thus, upon having observed $X=0$ , For up to $1-\alpha=90.9\%$ confidence (that is, when $9.1\%\le \alpha$ ), predict there is at most $t_\alpha(0;n,m)=1$ failure in the next $10,000$ products. For up to $99.2\%$ confidence (that is, when $0.8\%\le \alpha \lt 9.1\%$ ), predict there are at most $t_\alpha(0;n,m)=2$ failures in the next $10,000$ products. Etc. Comments When and why would this approach apply? Suppose your company makes lots of different products. After observing the performance of $n$ of each one in the field, it likes to produce guarantees, such as "complete no-cost replacement of any failure within one year." By having prediction limits for the number of failures you can control the total costs of having to back those guarantees. Because you make many products, and expect failures to be due to random circumstances beyond your control, the experience of each product will be independent. It makes sense to control your risk in the long run . Every once in a while you might have to pay more claims than expected, but most of the time you will pay fewer. If paying more than announced could be ruinous, you will set $\alpha$ to be extremely small (and you likely would use a more sophisticated failure model, too!). Otherwise, if the costs are minor, then you can live with low confidence (high $\alpha$ ). These calculations show how to balance confidence and risks. Note that we don't have to compute the full procedure $t$ . We wait until $X$ is observed and then just carry out the calculations for that particular $X$ (here, $X=0$ ), as shown above. In principle, though, we could have carried out the calculations for all possible values of $X$ at the outset. A Bayesian approach (described in other answers) is attractive and will work well provided the results do not depend heavily on the prior. Unfortunately, when the failure rate is so low that very few (or no failures) are observed, the results are sensitive to the choice of prior.
