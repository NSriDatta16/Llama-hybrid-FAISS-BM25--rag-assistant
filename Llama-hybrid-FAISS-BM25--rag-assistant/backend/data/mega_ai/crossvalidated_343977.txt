[site]: crossvalidated
[post_id]: 343977
[parent_id]: 343606
[tags]: 
Conditioning on $\lambda$, the posterior on $\mu$ can be expressed in a reasonably closed form: \begin{align} \pi(\mu|x_1,\ldots,x_n) &\propto \pi(\mu) \exp\left(-\frac{1}{\lambda}\sum_{i=1}^n|x_i-\mu|\right)\\ &= \pi(\mu) \exp\left(-\frac{1}{\lambda}\sum_{i=1}^n|x_{(i)}-\mu|\right)\\ &= \pi(\mu) \sum_{j=0}^n \mathbb{I}_{(x_{(j)},x_{(j+1)})}(\mu) \exp\left(-\frac{1}{\lambda}\sum_{i=1}^n|x_{(i)}-\mu|\right)\\ &= \pi(\mu) \sum_{j=0}^n \mathbb{I}_{(x_{(j)},x_{(j+1)})}(\mu) \exp\left(\frac{1}{\lambda}\sum_{i=1}^j[x_{(i)}-\mu|]\right)\exp\left(\frac{1}{\lambda}\sum_{i=j+1}^n[\mu-x_{(i)}|]\right)\\ &=\sum_{j=0}^n \exp\left(\frac{1}{\lambda}\sum_{i=1}^jx_{(i)}-\frac{1}{\lambda}\sum_{i=j+1}^nx_{(i)}\right)\pi(\mu)\exp\left(\frac{2j-n}{\lambda}\mu\right)\mathbb{I}_{(x_{(j)},x_{(j+1)})}(\mu) \end{align} since using a Normal prior $\pi(\mu)$ returns a mixture of truncated Normals. For instance, if the prior is a ${\cal N}(0,\sigma^2)$, then \begin{align} \pi(\mu|x_1,\ldots,x_n) &\propto \sum_{j=0}^n \overbrace{\exp\left(\frac{1}{\lambda}\sum_{i=1}^jx_{(i)}-\frac{1}{\lambda}\sum_{i=j+1}^nx_{(i)}\right)}^{\omega_j}\exp\left(\frac{2j-n}{\lambda}\mu-\frac{\mu^2}{2\sigma^2}\right)\mathbb{I}_{(x_{(j)},x_{(j+1)})}(\mu)\\ &\propto \sum_{j=0}^n \omega_j\exp\left(2\sigma^2\frac{2j-n}{\lambda}\frac{\mu}{2\sigma^2}-\frac{\mu^2}{2\sigma^2}\right)\mathbb{I}_{(x_{(j)},x_{(j+1)})}(\mu)\\ &\propto \sum_{j=0}^n \omega_j\exp\left(\sigma^2\frac{(2j-n)^2}{\lambda^2}\right)\exp\left(\frac{-1}{2\sigma^2}\left[\mu-\frac{(2n-j)\sigma^2}{\lambda}\right]^2\right) \mathbb{I}_{(x_{(j)},x_{(j+1)})}(\mu)\\ \end{align} a mixture of $n+1$ truncated Normal distributions in $\mu$, truncated respectively to the intervals $(x_{(j)},x_{(j+1)})$ with original mean $(2n-j)\sigma^2/\lambda$ and original variance $\sigma^2$. (While obvious, the weights of the mixture are cumbersome in that they imply the coverage probability of $(x_{(j)},x_{(j+1)})$ by the Normal distribution with mean $(2n-j)\sigma^2/\lambda$ and variance $\sigma^2$.) And the conditional posterior on $\lambda$ associated with an Inverse Gamma ${\cal IG}(a,b)$ is indeed an Inverse Gamma$${\cal IG}\left(a+n,b+\sum_i|x_i-\mu|\right)$$which implies that a two-step Gibbs sampler can be implemented.
