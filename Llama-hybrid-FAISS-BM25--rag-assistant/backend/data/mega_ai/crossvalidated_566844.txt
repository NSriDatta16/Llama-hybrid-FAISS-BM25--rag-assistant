[site]: crossvalidated
[post_id]: 566844
[parent_id]: 
[tags]: 
DNN architecture for regression with small inputs, large output

TL;DR what DNN architecture for 1xn (n I'm facing a supervised regression problem where I need to predict the outcome of the numerical simulation of a physical process. Each simulation takes as input a 1xn tensor of features ( $n = 10-50$ ) and outputs a 256x256x6 tensor of real numbers. There is no randomness involved in the simulations and the input-output map is bijective. My main goal is prediction accuracy vs. a test set, with sub-percent RMS prediction error being the overall goal. My dataset contains 100k well-distributed samples. It looks like some kind of CNN could work, given also the structure I expect in the output samples, but most examples I'm aware of run "in reverse" i.e. start from a NxNx3 image and evolve into an array of outputs. This prevents the use of transfer learning approaches. What I have tried: classical ML approaches in scikit-learn using a MSE metric (kernel ridge regression, SVM, XGBoost): extremely poor results ( $\sqrt{MSE}$ comparable to output magnitude), apparently no impact of tweaking hyperparameters shallow, fully connected NN with Dropout with Keras: regression plateaus early with $\sqrt{MSE}$ very large, no amount of tweaking seems to solve the issue I'm a complete novice in practical deep learning so I'm a bit at a loss over what architecture to start with for this problem. Any help or pointers to where to look would be appreciated.
