[site]: stackoverflow
[post_id]: 391421
[parent_id]: 293000
[tags]: 
In my company we have a product which does this and also performs well. I did most of the work on it. I can give a brief idea: You need to split the paragraph into sentences and then split each sentence into smaller sub sentences - splitting based on commas, hyphen, semi colon, colon, 'and', 'or', etc. Each sub sentence will be exhibiting a totally seperate sentiment in some cases. Some sentences even if it is split, will have to be joined together. Eg: The product is amazing, excellent and fantastic. We have developed a comprehensive set of rules on the type of sentences which need to be split and which shouldn't be (based on the POS tags of the words) On the first level, you can use a bag of words approach, meaning - have a list of positive and negative words/phrases and check in every sub sentence. While doing this, also look at the negation words like 'not', 'no', etc which will change the polarity of the sentence. Even then if you can't find the sentiment, you can go for a naive bayes approach. This approach is not very accurate (about 60%). But if you apply this to only sentence which fail to pass through the first set of rules - you can easily get to 80-85% accuracy. The important part is the positive/negative word list and the way you split things up. If you want, you can go even a level higher by implementing HMM (Hidden Markov Model) or CRF (Conditional Random Fields). But I am not a pro in NLP and someone else may fill you in that part. For the curious people, we implemented all of this is python with NLTK and the Reverend Bayes module. Pretty simple and handles most of the sentences. You may however face problems when trying to tag content from the web. Most people don't write proper sentences on the web. Also handling sarcasm is very hard.
