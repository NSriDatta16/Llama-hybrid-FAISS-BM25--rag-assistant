[site]: datascience
[post_id]: 77478
[parent_id]: 77439
[tags]: 
Andrew Ng is making this point in comparison to a simple Neural network. Let's say you have a 10x10 image, In a dense neural network, - We will connect every 100 neurons to the 100 in the next layer. (Dense) - Over that, each all will have a distinct weight (No sharing) So, total parm = 10K In a Convolution Neural Network, the approach is as shown in this image, Now, Weight sharing - The kernel will have the same weight for each pixel in the next layer i.e. it will not have distinct 9 weights for each slide. Sparsity - The pixel at the next layer is not connected to all the 100 from the first layer i.e. only a local group is connected to one pixel of next layer . It is not trying to get information from the full image every time. We are harnessing the properties of an image that a group of near-by pixels has better info than grouping distant pixels So, total parm( definitely size, number, and stride of the kernel will control it ) With a 3x3 kernel, (3 * 3) + 1 per kernel = 10 per kernel Even with 200 kernels, it will be only 2K as compared to 10K
