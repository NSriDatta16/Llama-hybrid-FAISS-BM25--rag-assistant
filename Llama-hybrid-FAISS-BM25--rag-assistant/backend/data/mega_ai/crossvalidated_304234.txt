[site]: crossvalidated
[post_id]: 304234
[parent_id]: 
[tags]: 
Why does SGD converge for optimizing an expectation if expected update equals the actual gradient?

When optimizing an expected value, why is it true that stochastic updates will converge as long as the expected value of the update is equal to the corresponding gradient of the expected value? For example, I see this fact used in the proof of the Gradient Bandit Algorithm in Section 2.8 of the Reinforcement Learning book by Sutton & Barto: http://people.inf.elte.hu/lorincz/Files/RL_2006/SuttonBook.pdf I also see this fact put to use in this paper by Sutton et al: https://nips.cc/Conferences/2008/Schedule?showEvent=1346 I would appreciate if anyone could provide a proof, outline of a proof, or link to a proof of this fact. UPDATE: Iâ€™ve found this is due to Robbins-Monro, which provides a stochastic approximation technique for finding zeros of a function when only a fuzzy version of the function is available. In this case that function is the gradient of the objective function, which also happens to be an expectation.
