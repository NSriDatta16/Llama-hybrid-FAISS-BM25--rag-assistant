[site]: datascience
[post_id]: 77336
[parent_id]: 74710
[tags]: 
As I understand them, Bayesian optimization approaches are already somewhat robust to this problem. The evaluated performance function is usually(?) considered noisy, so that the search would want to check nearby the "best solution" $h$ to improve certainty; if it then finds lots of poorly performing models, its surrogate function should start to downplay that point. (See e.g. these two blog posts.) If the instability is large due to random effects (e.g. initializations of weights that you mention), then just repeating the model fit and taking an average (or worst, or some percentile) of the performances should work well. If it's really an effect of "neighboring" $h$ values, then you could similarly fit models near the selected $h$ and consider their aggregate performance. Of course, both of these add quite a bit of computational expense; but I think this might be the closest to "the right" solution that doesn't depend on the internals of the optimization algorithm.
