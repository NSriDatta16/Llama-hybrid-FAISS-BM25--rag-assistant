[site]: crossvalidated
[post_id]: 463545
[parent_id]: 463516
[tags]: 
It's true that if a neural network uses regular gradient descent it will only be able to properly optimize convex functions. In order to address this, most neural networks use some variant on Stochastic Gradient Descent which introduces noise by considering fewer points so the algorithm can jump out of local minima. However, it's worth noting that there is no guarantee to find the global optima and this is usually used over simulated annealing because it is (typically) faster.
