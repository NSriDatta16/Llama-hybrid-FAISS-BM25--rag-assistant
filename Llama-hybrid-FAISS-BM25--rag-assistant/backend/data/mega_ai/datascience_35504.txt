[site]: datascience
[post_id]: 35504
[parent_id]: 
[tags]: 
Titanic Kaggle Data: Why am I getting lower accuracy on Kaggle submissions than on held-out data?

I am going through my first solo machine learning project and would like to gain some insight into what I am doing wrong/what is going on here as I am a bit stuck. I have been applying machine learning to the Titanic data set with SKlearn and have been holding out 10% of the training data to calculate the accuracy of my fitted models. I also use K-fold cross valdation with 10 folds to evaluate the model performance and choose hyper-parameters. I have so far applied logistic regression and a linear Kernel SVM and in both cases I get 78-80% accuracy on the K-fold validation sets and when applying the fitted classifiers to my held-back previously unseen testing data. However when I predict on Kaggle's test data and submit my predictions it comes back with values around 76% which is significantly less than I'd expect, and well outside the variance in the accuracy values I get with K-fold cross validation. A link to the Jupyter notebook where I do this is provided below: http://nbviewer.jupyter.org/github/AshleySetter/Kaggle_Competitions/blob/master/Titanic_project/Titanic_machine_learning_clean.ipynb Could anyone give me some insight into what is going on here and what I am doing wrong?
