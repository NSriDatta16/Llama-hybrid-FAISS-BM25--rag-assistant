[site]: crossvalidated
[post_id]: 459454
[parent_id]: 
[tags]: 
Why is optimizing the value function $V^\pi(s)$ for $\pi$ the same as optimizing the Q-function $Q^\pi(s,a)$ for $\pi$?

I know that in reinforcement learning the value function $V^\pi(s)$ and the Q-function $Q^\pi(s,a)$ are related by the formula $$V^\pi(s) = \sum_a \pi(s,a) Q^\pi(s,a)$$ (where $\pi(s,a)$ is often written as $\pi(a|s)$ ). One is usually interested in finding a policy/control law that maximizes the expected discounted reward for a given state; i.e., one seeks $\pi^\star$ such that $$\mathbb{E}\left[\sum_t \gamma^tR_t^\pi\right] = V^\pi(s_t)\rightsquigarrow \max_{\pi}$$ which means one wants to find $V^\star$ . An alternative to finding an optimal value function is to find an optimal Q-function $Q^\star$ via $$Q^\pi(s_t,a) \rightsquigarrow \max_\pi$$ for all actions $a$ in $s_t$ . I recently realized that I have simply accepted that every $\pi^\star_Q$ that optimally solves the Q-function for all states and actions also is an optimal policy $\pi^\star_V$ under the value function and vice versa, but I was wondering if this could be formally proven. Is it possible to prove that both optimization problems are equivalent?
