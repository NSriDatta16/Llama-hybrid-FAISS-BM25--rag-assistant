[site]: crossvalidated
[post_id]: 355678
[parent_id]: 214851
[tags]: 
Probably the core appeal of neural networks is the so-called "universal approximation theorem". Loosely stated, it's possible to use a neural network to approximate a continuous function on a compact subset of $\mathbb{R}^n$ using only a single hidden layer and finitely many neurons. The continuous and compact portions of that statement are the immediately obvious caveats. Other caveats include those related to actually training the neural network: it may not be easy to train achieve a specific precision because training neural networks is hard. More information can be found in Chapter 4 of Neural Networks and Deep Learning by Michael Nielsen. Wikipedia has a formal statement of the theorem.
