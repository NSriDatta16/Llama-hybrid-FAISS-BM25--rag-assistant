[site]: crossvalidated
[post_id]: 324204
[parent_id]: 
[tags]: 
Privacy through moving averages?

I am considering the following hypothetical situation: I have a time series of data. In general, 'the public' should have access to features of this data. However, making the time series available would constitute a privacy leak. I am considering making a moving average available instead. Can anyone recommend either some literature on this, or some alternative methods? I understand that this is a case by case question. However, I think there should be a general answer available along the following lines: 1) Privacy leaks occur because you can match up the time stamp to an individual, by using outside information. 2) Therefore, you want to make it so that each window aggregates the data of several individuals. (The data is of a form where the mean is a meaningful quantity.) There are probably adversarial ways to break this privacy, if one is sufficiently determined. I think in this case no one is. So I'm looking for literature that deals with some real world case studies, if possible. (This situation is hypothetical. I do not have access to the data. I am 'the public' that wants the data, and I want to suggest a reasonable approach for aggregation.) In general, the moving average is not invertible. However, it's plausible that there are some situations when data can be leaked in a clever way. Cross posted here: https://datascience.stackexchange.com/questions/26851/privacy-through-moving-averages
