[site]: datascience
[post_id]: 48962
[parent_id]: 
[tags]: 
What is "posterior collapse" phenomenon?

I was going through this paper on Towards Text Generation with Adversarially Learned Neural Outlines and it states why the VAEs are hard to train for text generation due to this problem. The paper states the model ends up relying solely on the auto-regressive properties of the decoder while ignoring the latent variables, which become uninformative. please simplify and explain the problem in a lucid way.
