[site]: crossvalidated
[post_id]: 458747
[parent_id]: 
[tags]: 
What is the best way to compare two reinforcement learning policies given offline data and estimated value functions?

So the Off-policy Policy Evaluation (OPE) problem is (more or less) defined as estimating V(s) or Q(s,a) from a set of offline trajectories D collected with a behavior policy that is different from the policy being evaluated. Please refer to (1) for more detailed description of the OPE. My question is what would be the best way to compute some numeric metric using D and estimated value function (from some OPE solution) that would allow us to pick the best policy? In my case, environment that produced D is episodic with relatively short episodes. Would mean estimated value of starting states suffice? Would average over all steps be better? References: 1. Precup, Doina; Sutton, Richard S.; and Singh, Satinder, "Eligibility Traces for Off-Policy Policy Evaluation" (2000). ICML '00 Proceedings of the Seventeenth International Conference on Machine Learning. 80
