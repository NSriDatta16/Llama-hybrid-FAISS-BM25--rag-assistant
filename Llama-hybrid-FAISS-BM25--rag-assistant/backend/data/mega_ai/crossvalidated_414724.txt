[site]: crossvalidated
[post_id]: 414724
[parent_id]: 413811
[tags]: 
I do not have a definite answer regarding the use of observation weights, but I don't think it's ideal. You could see if the approach works by simulating fake data and check if you can retrieve the original parameters. If you want to continue using a GLM, you could apply a logit transform to your label and then do a standard linear regression (and transform back the output into probabilities with the inverse logit). If you don't want to transform your label, maybe this unconventional but you could keep the logit link and change the binomial distribution to a normal distribution. This should allow labels between 0 and 1, but the predictions will not be constrained (you could always apply min/max in post-processing). Having said that, if I understood your problem correctly, I would recommend something like a mixture model. Indeed, you could write the likelihood of your model as: $$ p(x, y | \theta) = p(x, y = 1 | \theta) p(y = 1) + p(x, y = 0 | \theta)p(y=0) $$ Where $y$ is the label, $p(y=1) = 1 - p(y=0)$ the soft label for $y=1$ , $x$ the features and $\theta$ the parameters. $p(x, y = 1 | \theta)$ and $p(x, y = 0 | \theta)$ are the likelihoods of the logistic regression when $y=1$ or $y=0$ respectively. If you are feeling Bayesian, you could do this using the probabilistic programming language Stan for example. The Lasso could be implemented using a Laplace distribution for the prior $p(\theta)$ . For Elastic Net, you would use a mixture of Gaussian and Laplace distribution. Alternatively, you could also use something like a horseshoe prior.
