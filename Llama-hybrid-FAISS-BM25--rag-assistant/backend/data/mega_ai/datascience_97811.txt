[site]: datascience
[post_id]: 97811
[parent_id]: 
[tags]: 
What is the standard output of the GloVe algorithm?

When I look at the loss function of the GloVe algorithm for generating word vectors, I see that $w$ and $\tilde{w}$ are symmetric: $$ J=\sum_{i,j=1}^Vf(X_{ij})(w_i^T\tilde{w}_j+b_i+b_j-logX_{ij})^2 $$ (eq. 8 from https://nlp.stanford.edu/pubs/glove.pdf ) However, the authors never seem to describe whether $w$ and $\tilde{w}$ are both used to generate the embedding, or only one? When I watch Andrew Ng describe the algorithm ( https://www.youtube.com/watch?v=EHXqgQNu-Iw ), he suggests averaging the two, but when I try to read the original c-implementation of GloVe ( https://github.com/stanfordnlp/GloVe/blob/master/src/glove.c ), it seems that they implement both saving $w$ , and their concatenation, depending on user settings. What do people usually do?
