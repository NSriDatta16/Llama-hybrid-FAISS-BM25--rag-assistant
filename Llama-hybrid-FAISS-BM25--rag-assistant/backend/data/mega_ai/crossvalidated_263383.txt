[site]: crossvalidated
[post_id]: 263383
[parent_id]: 
[tags]: 
Why is the power of studies that only report significant effects not always 100%?

As I was reading the following passage of this blog, which defines R-(replicability-)indices: To correct for the inflation in power, the R-Index uses the inflation rate. For example, if all studies are significant and average power is 75%, the inflation rate is 25% points. The R-Index subtracts the inflation rate from average power. So, with 100% significant results and average observed power of 75%, the R-Index is 50% (75% â€“ 25% = 50%). ..the following question came to my mind: If statistical power is (please correct me if I am wrong) the sensitivity of a test to detect an effect when it truly exists (and thus indicates the true positive rate), then how can it be any less than 100% when computed on the basis of studies that only report significant effects? Surely, for all that those studies know, they (think they) have detected some effects out of an unknown total number of effects, and it is unknown to them what other effects (also) "truly" existed but were in fact not detected. What specific stats reported in a study (one that says nothing of undetected effects) would one need to compute its power? And how is this computation different when done analytically (post-hoc, e.g. by someone else later in a meta-analysis) as opposed to by the experimenters themselves prior to data collection (i.e. as a power analysis)? I know I am probably confused about many of the key concepts here, so would be grateful for a clarification
