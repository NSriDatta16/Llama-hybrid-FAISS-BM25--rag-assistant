[site]: crossvalidated
[post_id]: 549871
[parent_id]: 
[tags]: 
How can I show that these two variables in a Markov network are marginally independent?

I am reading "Bayesian Reasoning And Machine Learning" and I'm doing exercise 4.2 on page 79. This is the exercise: Consider the Markov network $$p(a,b,c)=\phi(a,b)\phi(b,c)$$ Nominally, by summing over $b$ , the variables $a$ and $c$ are dependent. For binary $b$ , explain a situation in which this is not the case, so that marginally, $a$ and $c$ are independent. My attempt: We have that $$p(a,b,c)=p(a,c)=\sum_bp(a,b,c)=\sum_b\phi(a,b)\phi(b,c) \\ p(a)=\sum_{a,b}\phi(a,b)\phi(b,c)\\p(c)=\sum_{a,b}\phi(a,b)\phi(b,c)\\ \Rightarrow p(a,c)\neq p(a)p(c)$$ We let $b\in\{-1,1\}$ , $$\phi(a,b=1)=0\\ \phi(a,b=-1)=1 \\ \phi(c,b=1)=0 \\ \phi(c,b=-1)=1$$ Now, $$p(a,c)=\sum_b\phi(a,b)\phi(b,c)=0(0)+1(1)=1\\ p(a)p(c)=\sum_{a,b}\phi(a,b)\phi(b,c)\sum_{a,b}\phi(a,b)\phi(b,c)\\=(0+1)(0+1)=1$$ So in this situation $p(a,c)=p(a)p(c)$ and hence $a$ and $c$ are marginally independent. Is this correct?
