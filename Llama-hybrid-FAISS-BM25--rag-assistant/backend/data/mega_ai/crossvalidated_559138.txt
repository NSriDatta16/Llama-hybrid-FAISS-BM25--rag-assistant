[site]: crossvalidated
[post_id]: 559138
[parent_id]: 
[tags]: 
Empirical Risk Equal to True Error under condition

[Note: Crosspost from stackoverflow, I think I maybe should've started here] I've been working through "Understanding Machine Learning: From Theory to Algorithms", and on pg. 46, after the realizability assumption has been dropped from the defintions of loss/risk "L", a note is made that: where D is an arbitrary joint probability distribution X x Y, X is the feature set, Y is the label set, S is a sample (sampled from the distribution D) of size m, and h is a learned hypothesis. However, I don't understand this notation - what does it mean to have a probability distribution "uniform over S"? Does this mean that, whatever the distribution D is, that S is sampled uniformly over it according to the probabilities (over the valid domain) defined in D? What happens when the data size m isn't nearly large enough to encapsulate some notion of "uniform"?
