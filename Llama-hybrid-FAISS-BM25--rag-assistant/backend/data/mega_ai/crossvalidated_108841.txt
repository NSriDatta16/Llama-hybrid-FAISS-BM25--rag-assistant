[site]: crossvalidated
[post_id]: 108841
[parent_id]: 108807
[tags]: 
The questions depends greatly on if you're sold on hypothesis tests per se, versus parameter estimation techniques. Bayesian hypothesis testing has received much of the same criticism as frequentist hypothesis testing. One alternative, which ameliorates the problems you're concerned about, is parameter estimation in the sense @John Kruschke explains here . Kruschke discusses the two approaches in detail on his blog . The method works like this: you set up a region of possible effects which you consider insignificant - where insignificant is meant in the colloquial sense, not the statistical sense. For example, you could say that coefficients or subject means or a Cohen's d between -0.1 and +0.1 entail that the relationship is practically uninteresting and without implications. Then, you produce a Bayesian credible interval for your data. If the interval is wholly contained within your region, your Bayes-optimal decision would be to assume the irrelevance of the relationship. For this, it is not even relevant if 0 itself is in the interval. It is actually rather hard to "prove the null" in this way because to reach a CI small enough to fit inside a non-trivial region requires a lot of data. Strong priors could stabilize your estimates towards 0 (leading to an interval that is more similar to your region), but unless they are overwhelmingly strong, they don't tend to strongly overestimate the support for such a null, since narrow intervals depend on rich data. If you're too concerned, you could also try either very weak priors on the variance parameter, or uniform priors on the parameter itself. And as always, simulate some worst-case data and check the bias of your test.
