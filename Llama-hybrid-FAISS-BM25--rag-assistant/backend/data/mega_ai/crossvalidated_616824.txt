[site]: crossvalidated
[post_id]: 616824
[parent_id]: 281609
[tags]: 
I think it makes sense to look at the concept of Extreme Classification (XC) where we deal with thousands of labels; "The Extreme Classification Repository" has a lot of good examples. A key difference to "standard classification" is that when working in these extreme (and often multi-label) classification use cases, we are moving towards a "metric at top $k$ " approach; we compute our metric of interest using the top $k$ labels predicted (usually ranked by predicted scores). Metrics like precision@k and nDCG@k are the obvious choices and that is because we are needing to have a more "lenient" while still relevant metric - a bit like going from FWER to FDR view in hypothesis testing. Aside from these two, there are also some more specialised metrics like propensity-scored prediction at $k$ which are also relevant and potentially helpful as they can be more easily associated with misclassification costs. Another point to mention are what are called classifier chains . In this methodology we usually build a series for classifiers to predict the presence or absence of a particular class in addition to the classes predicted by the previous classifiers in the chain. This is usually employed in multi-label classification but it generalises reasonably to XC too, as we partition our output space such that we have a more compact output space to deal with in each "link" of the chain. This plays in the general idea of reducing the output space either by standard dimensionality reduction techniques (e.g. random projections, or word embeddings). Read et al. (2021) Classifier Chains: A Review and Perspectives is a nice read on this. A final point is that sometimes we need specialised database infrastructure to perform XC as scalability in real-time setting might be an issue. (but this is primarily a data engineering point) Most applications are unsurprisingly associated with online advertising and/or product recommendations; XC problems are often overlapping with multi-label learning so their literature also overlaps. A reasonably well-cited source in this niche area of ML is Parabel: Partitioned Label Trees for Extreme Classification with Application to Dynamic Search Advertising (2018) by Prabhu et al. if you want to read more on this. Lie et al. (2021) The Emerging Trends of Multi-Label Learning seems also good and gives out a very nice hierarchy too.
