[site]: crossvalidated
[post_id]: 64172
[parent_id]: 
[tags]: 
Variance of annual return based on variance of monthly return

I'm trying to understand the whole variance/std error thing of a time series of financial returns, and I think I'm stuck. I have a series of monthly stock return data (let's call it $X$), which has expected value 1.00795, and variance 0.000228 (std. dev is 0.01512). I'm trying to calculate the worst case of the annual return (let's say expected value minus twice the standard error). Which way is the best way to do it? A . Calculate it for a single month ( $\mu_X-2\cdot \sigma_X=0.977$ ), and multiply it by itself 12 times (= 0.7630 ). B . Assuming the months are independent, define $Y=X\cdot X\cdot ...\cdot X$ 12 times, find it's expected value $E[Y]=(E[X])^{12}$) and variance $\operatorname{var}[Y]=(\operatorname{var}[X]+(E[X])^2)^{12} - ((E[X]^2)^{12}$. The standard dev in this case is 0.0572, and the expected value minus twice the std. dev is 0.9853 . C . Multiply the monthly std. dev with $\sqrt{12}$ to get the annual one. use it to find the worst case annual value ($\mu - 2\cdot \sigma$). It comes out as 0.9949 . Which one is correct? What is the right way to calculate the expected annual value minus twice the std. dev if you know these properties only for the monthly data? (In general - if $Y=X\cdot X\cdot ...\cdot X$ 12 times and $\mu_X$, $\sigma_X$ are known, what is $\mu_Y-2\cdot \sigma_Y$ ?)
