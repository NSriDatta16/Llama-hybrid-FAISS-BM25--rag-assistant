[site]: crossvalidated
[post_id]: 307592
[parent_id]: 
[tags]: 
More Loss in Training than Testing using multi-layer LSTM Neural Networkin Keras/TF

Unsure why I'm consistently seeing a higher training loss than test loss in my model: from keras.models import Sequential from keras.layers import Dense from keras.layers import LSTM from keras.layers import Activation from keras.layers import Dropout model = Sequential() model.add(LSTM(200, return_sequences=True,input_shape=(train_X.shape[1], train_X.shape[2]))) model.add(Activation('relu')) model.add(Dropout(0.5)) model.add(LSTM(100, return_sequences=True)) model.add(Activation('relu')) model.add(Dropout(0.5)) model.add(LSTM(50)) model.add(Activation('relu')) model.add(Dropout(0.5)) model.add(Dense(1)) model.compile(loss='mean_squared_error',optimizer='adam') history = model.fit(train_X,train_y, epochs=50, batch_size=30, validation_data=(test_X, test_y), verbose=2, shuffle=False) plt.plot(history.history['loss'], label='train') plt.plot(history.history['val_loss'], label='test') plt.legend() plt.show() The plot this produces: What am I doing wrong?
