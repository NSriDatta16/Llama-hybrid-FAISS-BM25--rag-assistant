[site]: crossvalidated
[post_id]: 482575
[parent_id]: 
[tags]: 
Some Confusions Regarding Variable Importance Extraction of Several Machine Learning Models

I'm trying to apply several machine learning algorithms in R using caret (decision trees, ensemble methods (bagging, boosting, random forests), logistic and penalized logistic regression models, svm and neural networks) to an imbalanced dataset (0: 90%, 1: 10%) with 1621 observations containing ordinal and nominal features. My primary concern is to extract the importance of each level in the mostly categorical dataset in the black box models but my sources of confusion are: For two level categories, one hot encoding is unnecessary but how can I know which level is more important and which one of them affects negatively or positively the model's prediction? Logistic regression has a good way of dealing with that but what about neural networks for example? I'm trying to apply the Olden algorithm [1] in order to extract the importance and the way each level affects the latter model's prediction. Would one hot encoding for two level categories in this case be justified? Should I use any oversampling methods here? If so, to which algorithm or set of algorithms should they be applied to? Would their use significantly impact the model's interpretability and explainability? Can we compare the variable importance output of each model to one another and draw conclusions even if the methods used to compute these variable importance values are different (Permutation for random forests, Olden for neural networks, Shapley values for svm, etc.)? [1] https://www.rdocumentation.org/packages/NeuralNetTools/versions/1.5.2/topics/olden
