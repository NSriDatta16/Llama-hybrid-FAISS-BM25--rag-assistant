[site]: crossvalidated
[post_id]: 415936
[parent_id]: 
[tags]: 
Calculating Average trend with possible missing data

I am conducting a research on a group of subjects with population about 90. For each of them I have a time-serial datapoints such as: P1 = [ [T1, 100], [T2, 23], [T3, 33], ... ] P2 = [ [T1, 80], [T3, 77], [T4, 88] ... ] ... Given that some datapoints are missing, for example, P2 doesn't have a datapoint for T2, how should I calculate the average for each time T? Should I simply use available points like P1[T2] + P3[T2] + ... / 83 assuming that there are 83 datapoints at time T2 or should I "compensate" the missing data points like P1[T2] + P2[somecompensited number] + P3[T2] + ... / 90 assuming 90 is the total population
