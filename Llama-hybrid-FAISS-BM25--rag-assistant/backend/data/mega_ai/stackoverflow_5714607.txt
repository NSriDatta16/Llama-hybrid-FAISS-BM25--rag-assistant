[site]: stackoverflow
[post_id]: 5714607
[parent_id]: 5714206
[tags]: 
use N-Grams N-Gram will tell you what is the probability that word is meaningfull. Read about markov chains and n-grams (http://en.wikipedia.org/wiki/N-gram) . Treat each letter as state, and take the set of meaningfull and meaningless words. For example: Meaningless word are B^^@, #AT Normal words: BOOK, CAT create two Language models for them (trigram will be the best) http://en.wikipedia.org/wiki/Language_model and now you can check in which model word was probably generated and take language model with probability greater than in other one. this will satisfy your condition remember that you need set of meaningless words ( i think around 1000 will be ok) and not meaningless
