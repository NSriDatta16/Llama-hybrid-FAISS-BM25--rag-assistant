[site]: crossvalidated
[post_id]: 242969
[parent_id]: 242965
[tags]: 
Wait, it is not a big deal to demonstrate. Let's take $$p_{3a} = {a\cdot p_{2a}\over a\cdot p_{2a}+ bp_{2b}} = {a\cdot {a\cdot p_{1a}\over a\cdot p_{1a}+ bp_{1b}}\over a\cdot {a\cdot p_{1a}\over a\cdot p_{1a}+ bp_{1b}}+ b{a\cdot p_{1a}\over a\cdot p_{1a}+ bp_{1b}}} = {{a^2\cdot p_{1a}}\over {a^2\cdot p_{1a}}+ {b^2\cdot p_{1b}}}$$ Similarly, $$p_{3b} = {{b^2 p_{1b}}\over {a^2p_{1a}}+ {b^2p_{1b}}}$$ which makes $$p_3 = {a^2 p_{1a}, b^2 p_{1b} \over {a^2p_{1a}}+ {b^2p_{1b}}}$$ as expected from the double heads immediate observation! You see, the normalization factor from the previous iterative update cancels out and is replaced by new normalization factor at every next update. It is easy to understand why this happening. We have got new priors in the iteration by normalizing the joint probabilities P(Evidence and Prior) with that factor. On the next iteration, every P(Evidence and Prior) has this factor in it and we use new normalizing factor, which is the sum of the joint probabilities, which means that the sum also contains it. So, both nominator and denominator contain this factor and we, thus, can just skip all the intermediate normalizations and normalize only once in the end, as Allison Dawny put it when offers to do all the bayesian updates with denominator ignored .
