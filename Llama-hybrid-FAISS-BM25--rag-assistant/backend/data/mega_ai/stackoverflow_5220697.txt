[site]: stackoverflow
[post_id]: 5220697
[parent_id]: 5220472
[tags]: 
If you're OK with converting the binary keys to a hex implementation, you can try out any database you like, but on low-end machines, the main bottleneck will be the disk IO. Will you have many tables and relationships between them, or will it just be a large bucket of key/value pairs? If it is such a simple list, you can write something yourself. A base for this could be the Structured Storage implementation of Windows. This is actually kind of a filesystem within a file and is used by Microsoft Office applications including Word and Access to store their data in. Writing to these files can be very fast, but they get fragmented over time, because data is written at the end if there isn't a large enough free spot. In that case you will need to compact the file, which obviously takes a little time. Therefor, it would be best not to create huge files, but combine the powers of the file system with the power of these files, because it will keep you from cereating millions of files (wasting millions of partial clusters) while it can still be maintained easily because the data files aren't to big either. Of course you could come up with your own datafile too, if structured storage appears to be too complex or slow or does have too much overhead. But remember, it will be tough to get a 100 GB database to run fast and instantly from a simple single disk desktop setup.
