[site]: crossvalidated
[post_id]: 151609
[parent_id]: 151216
[tags]: 
I'll answer from an intermediate perspective. I'm not a statistician, I'm chemist. However, I've spent the last 10 years specializing in chemometrics = statistical data analysis for chemistry-related data. I simply believe that researchers are not doing their statistics well enough. That is probably the case. Short version: Now about the assumptions. IMHO the situation here is far too heterogeneous to deal with it in one statement. Understanding of both what exactly the assumption is needed for and in which way it is likely to be violated by the application is necessary in order to judge whether the violation is harmless or critical. And this needs both the statistics as well as the application knowledge. As a practitioner facing unachievable assumptions, however, I need something else as well: I'd like to have a "2nd line of defense" that e.g. allows me to judge whether the violation is actually causing trouble or whether it is harmless. Long version: From a practical point of view, some typical assumptions are almost never met. Sometimes I can formulate sensible assumptions about the data, but often then the problems become so complicated from a statistical point of view that solutions are not yet known. By now I believe that doing science means that you'll hit the borders of what is known likely not only in your particular discipline but maybe also in other disciplines (here: applied statistics). There are other situations where certain violations are known to be usually harmless - e.g. the multivariate normality with equal covariance for LDA is needed to show that LDA is optimal, but it is well known that the projection follows a heuristic that often performs well also if the assumption is not met. And which violations are likely to cause trouble: It is also known that heavy tails in the distribution lead to problems with LDA in practice. Unfortunately, such knowledge rarely makes it into the condensed writing of a paper, so the reader has no clue whether the authors did decide for their model after well considering the properties of the application as well as of the model or whether they just picked whatever model they came across. Sometimes practical approaches (heuristics) evolve that turn out to be very useful from a practical point of view, even if it takes decades until their statistical properties are understood (I'm thinking of PLS). The other thing that happens (and should happen more) is that the possible consequences of the violation can be monitored (measured), which allows to decide whether there is a problem or not. For the application, maybe I don't care whether my model optimal as long as it is sufficiently good. In chemometrics, we have a rather strong focus on prediction. And this offers a very nice escape in case the modeling assumptions are not met: regardless of those assumptions, we can measure whether the model does work well. From a practicioner's point of view, I'd say that you are allowed to do whatever you like during your modeling if you do and report an honest state-of-the-art validation. For chemometric analysis of spectroscopic data, we're at a point where we don't look at residuals because we know that the models are easily overfit. Instead we look at test data performance (and possibly the difference to training data predicitve performance). There are other situations where while we're not able to predict precisely how much violation of which assumption leads to a breakdown of the model, but we're able to measure consequences of serious violations of the assumption rather directly. Next example: the study data I typically deal with is orders of magnitude below the sample sizes that the statistical rules-of-thumb recommend for cases per variate (in order to guarantee stable estimates). But the statistics books typically don't care much about what to do in practice if this assumption cannot be met. Nor how to measure whether you actually are in trouble in this respect. But: such questions are treated in the more applied disciplines. Turns out, it is often quite easy to directly measure model stability or at least whether your predictions are unstable (read here on CV on resampling validation and model stability). And there are ways to stabilize unstable models (e.g. bagging). As an example of the "2nd line of defense" consider resampling validation. The usual and strongest assumption is that all surrogate models are equivalent to a model trained on the whole data set. If this assumption is violated, we get the well-known pessimistic bias. The 2nd line is that at least the surrogate models are equivalent to each other, so we can pool the test results. Last but not least, I'd like to encourage the "customer scientists" and the statisticians to speak more with each other . The statistical data analysis IMHO is not something that can be done in a one-way fashion. At some point, each side will need to acquire some knowledge of the other side. I sometimes help "translating" between statisticians and chemists and biologists. A statistician can know that the model needs regularization. But to choose, say, between LASSO and a ridge, they need to know properties of the data that only the chemist, physicist or biologist can know.
