[site]: crossvalidated
[post_id]: 573969
[parent_id]: 407033
[tags]: 
If you downsample time-series data it would not remove the dependence, it would just dilute it. Say that your data follows the relationship $$ y_{t+1} = f(y_{t}) + \varepsilon_{t+1} $$ so the current time-point directly depends on the previous time-point. What is the relationship between $y_t$ and $y_{t+k}$ ? Well, it's $$ y_{t+k} = f( \dots f(f(f(y_{t}) + \varepsilon_{t+1}) + \varepsilon_{t+2}) + \varepsilon_{t+3}) \dots) + \varepsilon_{t+k} $$ so there is still some dependence on the value $k$ steps before, but the relationship is much noisier than with the value one step before. So if you remove the intermediate steps, there would be still some dependence. For exactly the same reason, thinning the MCMC chains, which once was a standard approach, is not recommended anymore as it does not "remove" the dependency. That said, regression models are pretty common also when dealing with time-series data . In such a case, you are usually looking at the data "one row at a time" and the model is $$ y_t = \mathbf{X}_t\boldsymbol{\beta} + \varepsilon_t $$ so it doesn't consider the past or the future. The assumption is that the errors are independent. If they are not, you would need a model that allows for auto-correlated errors, i.e. a time-series model like dynamic regression . There are ways to deal with it without a time-series model , but time-series models are the tools for the job.
