[site]: crossvalidated
[post_id]: 468475
[parent_id]: 
[tags]: 
Where are W^Q,W^K and W^V matrices coming from in Attention model?

In the paper Attention Is All You Need the matrix of outputs is computed as follows: . In the blog post The Illustrated Transformer it says that the matrices were trained during the process. So for each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process It is just not clear where do we get the WQ,WK and WV matrices that are used to create Q,K,V. All the resources explaining the model mention them if they are already pre-trained somewhere.
