[site]: crossvalidated
[post_id]: 443454
[parent_id]: 281653
[tags]: 
Let's write down a model matrix the way that we would for ANOVA. The difference is that the response variable will be binary instead of continuous, but that has no impact on the model matrix. As is typical, let's include of column of $1s$ for the intercept and let $0$ s in the rest of the columns represent some baseline group. (I will pick the group labeled 1, though it doesn't really matter.) We have $k$ groups. Let $X_1$ represent membership in group 2, $X_2$ represent membership in group 2, and so on up to $X_{k-1}$ for membership in group $k$ . So far, this is identical to doing ANOVA. However, now we fit a logistic regression instead of the OLS linear regression that we would do for ANOVA with a continuous response variable. Like how linear regression has the sum of squared errors, logistic regression has something called deviance. We can use the comments to get into what deviance is (it's fine if that means an edit of this post), but it measures by how much the model deviates from perfection. The deviance from our model with all of the group variables in the regression equation tells us how well we can predict the outcome. If our ability to predict the outcome is better when we consider group membership, it must be that the proportions for each group are not the same, so we compare the deviance of this "full model" to the deviance of the model that only considers an intercept term, thus pooling the data from all groups. This "reduced model" also has a deviance that measures by how much the predictions differ from perfection. It turns out that the deviance of the full model minus the deviance of the reduced model has a $\chi^2_{k-1}$ distribution under the null hypothesis that the parameters on the group membership variables are zero (that all groups come from the same binomial distribution). Use this to calculate a p-value. Because deviance is related to likelihood (in the statistical sense), this is called a likelihood ratio test. I do not know how the power or computation time of this test compares to Pearson's chi-squared test or Fisher's exact test, though this was what first came to mind. An advantage that the likelihood ratio test has over Pearson or Fisher is that it is straightforward to include covariates: the reduced model has an intercept and the covariates, and the full model has an intercept, the covariates, and the group membership variables. (Think about testing if three people flip heads at the same rate. Air temperature is a possible covariate.) This is analogous to ANCOVA. Reference : Agresti, Alan. Foundations of linear and generalized linear models . John Wiley & Sons, 2015.
