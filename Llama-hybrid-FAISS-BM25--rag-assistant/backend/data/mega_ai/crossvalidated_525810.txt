[site]: crossvalidated
[post_id]: 525810
[parent_id]: 525697
[tags]: 
For machine learning, I've predominantly seen the usage OP describes, but I've also encountered lots of confusion coming from this usage. Historically, I guess what happened (at least in my field, analytical chemistry) is that as models became more complex, at some point people noticed that independent data is needed for verification and validation purposes (in our terminology, almost all testing that is routinely done with models would be considered part of verification which in turn is part of the much wider task of method validation). Enter the validation set and methods such as cross validation (with its original purpose of estimating generalization error). Later, people started to use generalization error estimates from what we call internal verification/validation such as cross validation or a random split to refine/optimize their models. Enter hyperparameter tuning. Again, it was realized that estimating generalization error of the refined model needs independent data. And a new name was needed as well, as the usage of "validation set" for the data used for refining/optimizing had already been established. Enter the test set. Thus we have the situation where a so-called validation set is used for model development/optimization/refining and is therefore not suitable any more for the purpose of model verification and validation. Someone with e.g. an analytical chemistry (or engineering) background will certainly refer to the data they use/acquire for method validation purposes as their validation data* - and that is correct usage of the terms in these fields. *(unless they know the different use of terminology in machine learning, in which case they'd usually explain what exactly they are talking about). Personally, in order to avoid the ongoing confusion that comes from this clash of terminology between fields, I've moved to using "optimization data/set" for the data used for hyperparameter tuning (Andrew Ng's development set is fine with me as well) and "verification data/set" for the final independent test data (the testing we typically do is actually verification rather than validation, so that avoids another common mistake: the testing we typically do is not even close to a full method validation in analytical chemistry, and it's good to be aware of that) Another strategy I find helpful to avoid confusion is moving from splitting into 3 data sets back to splitting into training and verification data, and then describing the hyperparameter tuning as part of the training procedure which happens to include another split into data used to fit the model parameters and data used to optimize the hyperparameters.
