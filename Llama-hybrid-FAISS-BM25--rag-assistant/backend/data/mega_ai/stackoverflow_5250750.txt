[site]: stackoverflow
[post_id]: 5250750
[parent_id]: 
[tags]: 
How to speed up summarise and ddply?

I have a data frame with 2 million rows, and 15 columns. I want to group by 3 of these columns with ddply (all 3 are factors, and there are 780,000 unique combinations of these factors), and get the weighted mean of 3 columns (with weights defined by my data set). The following is reasonably quick: system.time(a2 The problem is that I want to use weighted.mean instead of mean to calculate my aggregate columns. If I try the following ddply on the same data frame (note, I cast to immutable), the following does not finish after 20 minutes: x This operation seems to be CPU hungry, but not very RAM-intensive. EDIT: So I ended up writing this little function, which "cheats" a bit by taking advantage of some properties of weighted mean and does a multiplication and a division on the whole object, rather than on the slices. weighted_mean_cols When I run as: a2 I get good performance, and somewhat reusable, elegant code.
