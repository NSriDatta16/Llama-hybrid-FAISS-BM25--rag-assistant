[site]: crossvalidated
[post_id]: 348868
[parent_id]: 348855
[tags]: 
You're absolutely right, it makes no sense, and it's a huge problem. The loss function is locally linear. You are effectively saying that $L(w+h) = L(w)+h\cdot dL(w)$ , where $L$ is your loss function, $w$ your vector of weights, and $dL$ is the gradient vector. In this sense, the contribution of changes of each weight produces an approximately linear change in $L$ as a function of the step-size. $dL$ and in particular $dL_i$ , the individual components, definitely depends on weights in a nonlinear way. This helps to decouple the range of the output from the range of how $L$ changes as the weights change. Due to the sensitivity you point out, there are also strategies which assign smaller learning rates to earlier layers, especially for the purposes of fine-tuning a network. Even though we choose a larger learning rate initially, this learning rate is still tuned to how sensitive $L$ is, so that the above linear relationship is roughly held. In theory the above is right, but in practice the situation is considerably more complex. For example in a recent paper The Shattered Gradients Problem: If resnets are the answer, then what is the question? Balduzzi, et. al , evidence is presented exactly to your point, that vanilla deep learning networks have gradients that are extremely sensitivity to both weights and input images, even with batch normalization . It is shown that residual networks considerably improve this sensitivity, which makes a convincing case for why they work so well. At the same time, initially almost anything is better than random guessing. For example if you're training a network to tell "1" apart from "7", then it's clear that "1"'s have more vertical features, vs 7's have a ton of diagonal and horizontal features, so that even first level convolutions that correlate slightly with vertical vs diagonal features will contribute to improving the model.
