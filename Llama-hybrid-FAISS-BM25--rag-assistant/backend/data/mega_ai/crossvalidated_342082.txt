[site]: crossvalidated
[post_id]: 342082
[parent_id]: 342046
[tags]: 
SVD is a generalization of PCA in the following sense. If one apples the singular value decomposition to a covariance matrix, then one gets the pca decomposition of that matrix. Viewing the matrix as a linear transformation, the matrix takes an orthonormal vector to a linear subspace spanned by one of the orthonormal vectors in the target space. What both results state is that if we consider a matrix as a linear transformation from one vector space to another, then there are orthonomal (perpendicular and of unit length) basis in both spaces such that the matrix takes a vector in one basis to the one dimensional space spanned by a vector in the other basis. In the case of a symmetric matrix one can take both vector spaces to be the same and there is only one basis. In either case once you have an orthonormal basis, , all the linear transformation can do is take a orthonormal vector to a multiple of a member of the orthonormal basis in the other. That scalar multiplication in the target space is a diagonal matrix in the case of pca,and in the case of a svd, it is a pseudo diagonal matrix. In words, with the right basis any linear transformation is just scaling the basis. Once one has the last statement, the dimension reduction statement is, it seems to me, clear. If one has a diagonal matrix, what is the one dimensional matrix that is closest to it ? It is the diagonal matrix with all zeros except that it agrees with the original diagonal on the element of largest absolute value.
