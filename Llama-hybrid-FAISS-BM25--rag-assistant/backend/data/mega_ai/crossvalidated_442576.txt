[site]: crossvalidated
[post_id]: 442576
[parent_id]: 91044
[tags]: 
Following is an example of a multi-class confusion matrix assuming our class labels are A, B and C A/P A B C Sum A 10 3 4 17 B 2 12 6 20 C 6 3 9 18 Sum 18 18 19 55 Now we calculate three values for Precision and Recall each and call them Pa, Pb and Pc; and similarly Ra, Rb, Rc. We know Precision = TP/(TP+FP), so for Pa true positive will be Actual A predicted as A, i.e., 10, rest of the two cells in that column, whether it is B or C, make False Positive. So Pa = 10/18 = 0.55 Ra = 10/17 = 0.59 Now precision and recall for class B are Pb and Rb. For class B, true positive is actual B predicted as B, that is the cell containing the value 12 and rest of the two cells in that column make False Positive, so Pb = 12/18 = 0.67 Rb = 12/20 = 0.6 Similarly Pc = 9/19 = 0.47 Rc = 9/18 = 0.5 The overall performance of the classifier will be determined by average Precision and Average Recall. For this we multiply precision value for each class with the actual number of instances for that class, then add them and divide them with total number of instances. Like , Avg Precision = (0.55* 17 + 0.67 * 20 + 0.47 * 18)/55 = 31.21/55 = 0.57 Avg Recall = (0.59* 17 + 0.6 * 20 + 0.5 * 18)/55 = 31.03/55 = 0.56 I hope it helps
