[site]: crossvalidated
[post_id]: 242127
[parent_id]: 242124
[tags]: 
You simply feed all input-output pairs into the training algorithm. There is no requirement in neural networks that the training data exhibit deterministic separability as a function of the input values. Most real-world problems will involve training data that are not perfectly separable, which is expected when there is some noise in the responsible generating process. Most useful classifiers will respond to this condition by reflecting the tendency in a given decision region to be a 1 or a 0 (in a binary classifier). This tends to manifest as greater uncertainty (equivalent to steeper gradient of pre-thresholded classifier output value with respect to the inputs) in regions where greater overlap is seen in the training data. This will be true of most classifiers, including neural networks. A true probabilistic classifier may express this explicitly in terms of probability. A neural network won't necessarily do this as it depends on structure, although if a logit activation is used for the output layer (just for instance) it will share some characteristics of probabilistic logistic regression. However, one has to be careful with any strong statements about equivalency here because the function of preceding layers will influence outcomes greatly depending on choice of activations, layer depth, number of hidden features, and so forth. As an aside: your sample data shows no variation in the input feature space (but the outputs vary). That's not likely to result in anything useful in terms of training since these data are purely co-linear (stochastic gradient descent needs to be able to sample many different points in the input space to produce anything other than trivial behavior from the network).
