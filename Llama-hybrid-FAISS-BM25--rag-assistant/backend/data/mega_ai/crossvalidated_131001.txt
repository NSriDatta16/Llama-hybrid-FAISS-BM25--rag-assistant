[site]: crossvalidated
[post_id]: 131001
[parent_id]: 130985
[tags]: 
If you validate on the entire training set, your ideal model is the one that just memorizes the data. Nothing can beat it. You say that "realistically this is not a model that just memorizes the data". But why do you prefer other models? This is the point of my reduction to absurdity of validating on all the data: the main reason you don't like the model that memorizes everything it has seen is that it doesn't generalize at all. What should it do given an input that it hasn't seen? So you want a model that works in general rather than one that just works on what it has seen. The way that you encode that desire for working well on unseen data is to set the validation data to be exactly that unseen data. However, If you know that your training examples completely represent the true distribution, then go ahead and validate using them! Also, contrary to the claims in your final paragraph, the quotation you cited is not "plainly wrong" and that "particular evaluation strategy" does have to do "with overfitting models". Overfitting means fitting (the noise of) the provided training examples rather than the statistical relationships of general data. By validating using seen data, you will prefer models that fit noise rather than those that work well using unseen data.
