[site]: datascience
[post_id]: 67080
[parent_id]: 
[tags]: 
Why does reducing polynomial regression to linear regression work?

Getting into machine learning, have a reasonable background in statistics and understand the basic principles of linear algebra (matrix multiplication etc.) - but am having a damn hard time figuring out why reducing a polynomial regression works. For example, say we have this function: $y =$ $\beta_0$ + $\beta_1$$x_1$ + $\beta_2$$x_2$$^2$ From what I've seen on 4 videos and 6 articles, we can use the following substitutions: $x_2$ = 1 $x_3$ = $x$ $x_4$ = $x^2$ To create the following model: $y =$ $\beta_0$ + $\beta_1$$x_1$ + ( $\beta_2$$x_2$ + $\beta_3$$x_3$ + $\beta_4$$x_4$ ) And then, fine - we can solve that as a normal multiple linear regression, and all is well. But why, why does this work? I really cannot find an explanation for this. Thanks!
