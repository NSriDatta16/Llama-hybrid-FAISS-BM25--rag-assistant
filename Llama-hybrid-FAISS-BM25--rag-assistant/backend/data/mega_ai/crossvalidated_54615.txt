[site]: crossvalidated
[post_id]: 54615
[parent_id]: 53345
[tags]: 
There are some good answers here. Let me fill a small gap by providing a simple answer to the question as stated : "Why must there be a tradeoff between bias and variance?" The answer is fairly straightforward. Estimates of population parameters based on sample data will vary from sample to sample, because the sample data will vary. This means that there will be a distribution of parameter estimates under repeated sampling . To understand the pros and cons of different algorithms for estimating population parameters, we need to know about the properties of their sampling distributions. Specifically, we want to know if the sampling distribution is centered on the true value (i.e., if it's biased ), and we want to know how 'wide' the sampling distribution is (i.e., how far away from the true value the estimates tend to be on average --this is the variance part). Now it happens that there will be an estimation procedure that will have the smallest variance of all possible estimation procedures that are unbiased, but there will also often be another estimation procedure that will have even smaller variance (but which will be biased). In this situation, the analyst is faced with a decision regarding which procedure they want to use, depending on whether unbiasedness or lower variance is more important to them in that situation. In sum: there must be a tradeoff between bias and variance anytime there is more than one possible estimation procedure where the one with the minimum variance of all unbiased procedures is not the procedure with the minimum variance overall .
