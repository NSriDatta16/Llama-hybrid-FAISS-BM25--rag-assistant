[site]: crossvalidated
[post_id]: 171474
[parent_id]: 171463
[tags]: 
In the c implementation of D. Blei hyperparameters are optimized using an Expectation maximization algorithm that is initialized by n random document seeds. This could produce slightly different results...try to set the hyperparameters fixed and see whether you get the same results... Regarding re-running the algorithm several times (with slightly different settings of the hyperparameters): You can measure the similarity between the topics in the different iterations in order to measure "how much" the topics differ from each others. If in 2 consecutive iterations you find 4 pairs of topics with a good similarity (~90%?) you can consider the topics consistent. Choosing the right similarity metric is another question.... In principle you can use any similarity (or dissimilarity) function between histograms (Jeffrey divergence?), or be elengant: http://www.machinelearning.org/proceedings/icml2007/papers/105.pdf
