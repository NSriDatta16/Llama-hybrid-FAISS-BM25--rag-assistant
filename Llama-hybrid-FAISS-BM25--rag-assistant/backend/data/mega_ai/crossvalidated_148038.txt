[site]: crossvalidated
[post_id]: 148038
[parent_id]: 148023
[tags]: 
Very basically, an HMM is a Markov model in which the state is not fully observable, rather it is only observed indirectly via some noisy observations. The Markov model part is a simple way of imposing temporal dependencies in the state. Correspondingly, problems in which HMMs are useful are those where the state follows a Markov model, but you don't observe the state directly. There are various things you can do with an HMM. One useful thing you can do is as follows -- given a set of noisy observations up to the present time, perhaps you want to know what the most likely present state of the system is. To do this, you would appropriately combine the Markov chain structure with the observations to infer the state. Similarly, you can extend this to infer the whole sequence of states from the sequence of observations ( this is standard). In science and engineering, this model gets used all the time. For example, perhaps you are recording video of a simple animal like c. elegans (a worm), and it only has some small number of discrete behavioral states. From the video, you want to label each frame with the behavioral state of the animal. From a single frame, the labelling algorithm has some error/noise. However, there are also temporal dependencies that you can model with a Markov chain...If in one frame the animal was in one state, it is likely to be in the same state for the next frame (and perhaps some states only permit transitions to certain other states). Basically, by combining your noisy single-frame observations with the structure of the transitions (by the HMM), you can get a well-smoothed and better constrained sequence of state estimates.
