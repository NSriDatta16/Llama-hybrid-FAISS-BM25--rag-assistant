[site]: crossvalidated
[post_id]: 492727
[parent_id]: 
[tags]: 
Bayesian optimization with xgb.cv and xgb.XGBClassifier - Mismatch between AUC scores

I'm doing bayesian hyperparameter optimization with bayes_opt and maximizing the AUC. I'm noticing a big discrepancy between the cross-validation scores that I obtain during optimization and the scores that I obtain when predicting and testing the model. Here's my code. To simplify, I'll be optimizing gamma only and do only n_iter = 10 from bayes_opt import BayesianOptimization import xgboost as xgb def optimize_xgb(train, params): def xgb_crossval(gamma = None): params['gamma'] = gamma cv_results = xgb.cv( params, train, num_boost_round=100, # default n_estimators in XGBClassifier is 100 stratified = True, seed=23, nfold=5, metrics='auc', early_stopping_rounds=100 ) return cv_results['test-auc-mean'].max() optimizer = BayesianOptimization( f=xgb_crossval, pbounds={ "gamma": (0, 1), }, random_state=12, verbose=10) optimizer.maximize(init_points = 3, n_iter=10, acq='ei', kappa = 3, alpha = 1e-3) print("Final result:", optimizer.max) return optimizer.max Because xgb.cv and xgb.XGBClassifier have different default parameters for eta/learning_rate (0.3 vs 0.1) and for max_depth (6 vs 3), I initialize params with the defaults values of xgb.XGBClassifier (so that xgb.cv gets the same default values) params = { 'max_depth': 3, #default value in xgb.XGBClassifier 'eta':.1, #default value in xgb.XGBClassifier 'objective':'binary:logistic', } train = xgb.DMatrix(X_train, y_train) best = optimize_xgb(train, params) And I obtain the following: | iter | target | gamma | ------------------------------------- | 1 | 0.8212 | 0.1542 | | 2 | 0.8103 | 0.74 | | 3 | 0.8344 | 0.2633 | | 4 | 0.8344 | 0.2633 | | 5 | 0.8321 | 0.28 | | 6 | 0.8312 | 0.2355 | | 7 | 0.8006 | 1.0 | | 8 | 0.8203 | 0.4947 | | 9 | 0.8359 | 0.0 | | 10 | 0.8335 | 0.0401 | | 11 | 0.8 | 0.6017 | | 12 | 0.8159 | 0.4012 | | 13 | 0.8394 | 0.01264 | ===================================== Final result: {'target': 0.8394116, 'params': {'gamma': 0.012638204185692193}} Notice how AUC's are all above 0.8. However, when I try to test my model, AUC values are always lower than during optimization. params['gamma'] = best['params']['gamma'] # learning rate is already 0.1 as default in xgb.XGBClassifier xb_es = xgb.XGBClassifier(**params) xb_es = xb_es.fit(X_train, y_train, early_stopping_rounds=5, eval_metric=["auc","logloss"], eval_set=[(X_train, y_train), (X_val, y_val)], verbose = 1) And I obtain (used early_stopping_rounds=5 to simplify): [0] validation_0-auc:0.926353 validation_0-logloss:0.647199 validation_1-auc:0.630303 validation_1-logloss:0.676592 Multiple eval metrics have been passed: 'validation_1-logloss' will be used for early stopping. Will train until validation_1-logloss hasn't improved in 5 rounds. [1] validation_0-auc:0.923706 validation_0-logloss:0.613415 validation_1-auc:0.719192 validation_1-logloss:0.651203 [2] validation_0-auc:0.928882 validation_0-logloss:0.584485 validation_1-auc:0.69899 validation_1-logloss:0.641244 [3] validation_0-auc:0.946353 validation_0-logloss:0.558989 validation_1-auc:0.731313 validation_1-logloss:0.62868 [4] validation_0-auc:0.953706 validation_0-logloss:0.53031 validation_1-auc:0.727273 validation_1-logloss:0.627571 [5] validation_0-auc:0.958647 validation_0-logloss:0.505348 validation_1-auc:0.715151 validation_1-logloss:0.628144 [6] validation_0-auc:0.960059 validation_0-logloss:0.487721 validation_1-auc:0.715151 validation_1-logloss:0.61783 [7] validation_0-auc:0.973882 validation_0-logloss:0.459568 validation_1-auc:0.682828 validation_1-logloss:0.629029 [8] validation_0-auc:0.978706 validation_0-logloss:0.438147 validation_1-auc:0.658586 validation_1-logloss:0.637697 [9] validation_0-auc:0.980471 validation_0-logloss:0.421054 validation_1-auc:0.640404 validation_1-logloss:0.645252 [10] validation_0-auc:0.980941 validation_0-logloss:0.406797 validation_1-auc:0.626263 validation_1-logloss:0.653234 [11] validation_0-auc:0.984824 validation_0-logloss:0.38987 validation_1-auc:0.642424 validation_1-logloss:0.647176 Stopping. Best iteration: [6] validation_0-auc:0.960059 validation_0-logloss:0.487721 validation_1-auc:0.715151 validation_1-logloss:0.61783 When I then try to predict: pred_probs_xb = xb_es.predict_proba(X_test,ntree_limit=xb_es.best_ntree_limit) print(roc_auc_score(y_test, pred_probs_xb[:,1])) I obtain: 0.5164285714285715 I don't know where this discrepancy originates. I suspected it had to do with optimizing parameters using xgb.cv and then fitting the model using sklearn wrapper xgb.XGBClassifier but I've set the same parameters for both and there's still a big discrepancy. Also I suspected it had to do with my train-test-split, but no matter the split, the the xgb.cv scores are always (significantly) higher than the predicted scores. I seem to missing something out while transfering the best parameters from the bayesian optimization onto model training, but I can't figure out exactly what. Is it related to different defaults? Objective function? optimizer.maximize parameters? Any help with be very much appreciated
