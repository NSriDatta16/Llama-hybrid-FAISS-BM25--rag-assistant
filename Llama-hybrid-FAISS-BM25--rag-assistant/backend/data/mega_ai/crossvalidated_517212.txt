[site]: crossvalidated
[post_id]: 517212
[parent_id]: 517193
[tags]: 
In my opinion, a good answer is written in Pawitan (2001, p.29) "In All Likelihood: Statistical Modelling and Inference Using Likelihood." It is a better information utilization. You may consider the basic definition of a likelihood function: $$l(\theta;x) = f(x; \theta)$$ However, this implies: $$f(\theta|x) = constant \times f(\theta)l(x|\theta)$$ . It is obvious here, that classical likelihood function works if $f(\theta) = 1.$ If we do not know anything about $\theta$ prior to estimation, this approach works well. Otherwise, we need to incorporate this information into our model. Bayesians work with this particular idea and do not restrict it to 1. However, I am not a Bayesian.
