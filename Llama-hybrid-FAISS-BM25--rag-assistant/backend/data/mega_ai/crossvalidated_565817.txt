[site]: crossvalidated
[post_id]: 565817
[parent_id]: 565537
[tags]: 
I actually do this quite often, in general because the data may work for regression, but the scenario isn't necessarily a regression problem even if it could be. Here's a common scenario: Let pretend you're a data scientist at a company and they say to you that they want to forecast monthly sales. They hand you a bunch of data that includes historical sales, perhaps other continuous data, and a large number of categorical data about the products, consumers, marketing approaches, etc. You immediately see this data and think regression is a likely good choice. You dig into the data to see if regression is a good fit, perhaps doing an EDA, and find that there's 100s of categorical data with 100s of levels each. You then go back and ask the sales team if all of the categoricals are useful to them. They say yes, but then they clarify that they really only care if they're making 10x above spend (which is also one of the pieces of data you have). Suddenly you have a choice to regress on monthly saies ad report on 10x or not, or to lump sales into levels of = 10x. Now you have a logistic regression as an option. You then one-hot all your categoricals as a first pass and find that the data are too expansive (too many fields and levels) for you to run the regression quickly. The sales team needs the model by the end of the week. You go back and propose that they give you more time, but they say no. You also tell them your option of logistic regression, but they say that maybe they want to know 0.5x, 2x and then 10x and above for it to be really useful. You still have regression on the table, but now you have a clear classification possibility. At this point, you can hash the categoricals quickly, greatly reducing the number of features and the size of the problem. You can bin the sales numbers into 0.5x, 2x, and >=10x. You can quickly run a tree-based classifier like Random Forest , XGBoost , or LightGBM classifier on your local machine, pull out feature importance, look at some trees, etc. and gain insight into what features matter without having to figure out how all the coding should work for a regression. Perhaps at this point the prediction quality is poor, but nevertheless, you've delivered a predictive model in time, gained insight on the potentially useful features via classification, and opened up a few more options for proceeding on a better model. That said, if you have multiple data types that require different loss functions and regularlizations, GLRM helps formulate all that quite nicely.
