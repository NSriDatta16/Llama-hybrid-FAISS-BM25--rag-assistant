[site]: datascience
[post_id]: 15994
[parent_id]: 15993
[tags]: 
The theory of backpropagation should work with complex numbers, it doesn't make any assumptions about all numbers being real. However, most NN libraries would not be able to calculate with them. Your weights and biases would also need to be complex. I have no idea if you could reasonably expect convergence using gradient descent - many activation functions have periodic output and numerically unstable regions when calculating with complex numbers, which makes me think it would be risky. The simplest thing to do would be to split each complex number up into its components at the input, representing as a 2-dimensional vector of real numbers - either using cartesian or polar approach. Neural networks should cope just fine with splitting features up like this.
