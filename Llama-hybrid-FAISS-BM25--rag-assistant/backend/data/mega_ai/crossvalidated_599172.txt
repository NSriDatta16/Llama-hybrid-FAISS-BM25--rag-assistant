[site]: crossvalidated
[post_id]: 599172
[parent_id]: 
[tags]: 
Is there any advantage from using Momentum Schedulers in training models using SGD than using a constant momentum of 0.9?

Recently I noticed that some pytorch repos of papers use Learning Rate Scheduler and momentum rate Scheduler , a lot of momentum rate schedulers exist similar to LR scheduler ranging from Lambda, Cosine, Cyclic schedulers , one article that caught my eye that was very interesting was something called Demon (paper: link ) where the momentum starts at a very high value such as 0.9 and then reduces to a very low value towards the end. So my idea was to implement a OneCycle Learning Rate Scheduler with a warmup of 15 epochs which starts from 0.0002 and then goes to 0.1 and then falls back to a very low value towards the end of the last epoch to about 0.00002 and also implement the Demon momentum Scheduler. Now my question is will using a demon momentum where we end towards a very low value useful. Now according to the stochastic gradient update equations, initially we would give importance to the average value of the change in gradients and towards the end we would give importance to the next immediate change in gradients using the demon momentum (due to low value) and since the learning rate would be low, the change in the weights would also be minimal is this useful ? , or in other articles they suggest using low momentum for high learning rate and towards the end of the epoch increase the momentum and decrease the learning rate. So basically first it starts with a momentum with SGD then slowly towards the end it becomes a vanilla SGD. I currently tested only SGD with a constant momentum and I couldn't find much information regarding momentum schedulers and there are no API's available for them in Pytorch or Tensorflow as most of them use only LR schedulers, is there some reason momentum schedulers are not used frequently.Any help would be appreciated. Thanks
