[site]: datascience
[post_id]: 38793
[parent_id]: 
[tags]: 
Predict compatibility of 2 people as boolean classification problem

How can I predict the compatibility of 2 people as a boolean classification problem? I want to know if below is an appropriate approach to modelling compatibility, or if I should be using "market basket analysis" or some other approach instead? I'm less interested in the specific result below, and more interested in if this is a realistic way to frame this data science problem. Background: Assume people only have 3 attributes: compassion, extroversion and humor. These are also boolean and can be modelled as 1s and 0s in a list ( [compassion, extroversion, humor] ). So someone with all 3 characteristics would be [1,1,1] and someone with only humor would be [0,0,1] . We have pairs of people who match and do not match, specified by 1 or 0, where 1=match and 0=no_match. How to solve this? I don't consider this a simple distance problem (ie: euclidean distance) because its very possible that generally an introvert and extrovert get along, but 2 extroverts don't. Data: person1 person2 match? -------- -------- ------ ([1,1,0], [1,0,1]) => 1 ([0,0,0], [1,1,1]) => 1 ([1,0,1], [1,0,0]) => 0 ([1,1,1], [0,1,0]) => 0 ([0,0,0], [0,1,1]) => 1 ([1,1,0], [1,1,1]) => 0 ([1,0,0], [1,0,1]) => 0 ([0,0,1], [0,0,0]) => 0 ([0,0,0], [0,0,1]) => 0 ([0,0,0], [0,1,1]) => 1 ([0,1,0], [0,1,1]) => 0 ([0,1,0], [0,1,1]) => 0 ([0,1,0], [1,0,0]) => 1 What I've tried: My first thought was to concatenate both individuals' data for each example. Then use that to fit the model. Data structured as python code: X_train = [ [1,1,0,1,0,1], [0,0,0,1,1,1], [1,0,1,1,0,0], [1,1,1,0,1,0], [0,0,0,0,1,1], [1,1,0,1,1,1], [1,0,0,1,0,1], [0,0,1,0,0,0], [0,0,0,0,0,1], [0,0,0,0,1,1], [0,1,0,0,1,1], [0,1,0,0,1,1], [0,1,0,1,0,0], ] y_train = [1,1,0,0,1,0,0,0,0,1,0,0,1] X_test = [ [0,1,1,0,0,0], [1,1,0,1,0,1], [1,0,0,1,0,0], [0,0,0,1,0,0], [0,1,0,0,0,0], [0,0,0,0,0,0], ] y_test = [1,1,0,0,1,0] Computing the match: from sklearn.metrics import classification_report from sklearn.ensemble import RandomForestClassifier rf = RandomForestClassifier() rf.fit(X_train, y_train) y_pred = rf.predict(X_test) print(classification_report(y_test,y_pred)) precision recall f1-score support 0 0.60 1.00 0.75 3 1 1.00 0.33 0.50 3 avg / total 0.80 0.67 0.62 6 I'm less concerned about the results here and more interested in if this is a proper way to frame this problem. Can you offer a suggestion?
