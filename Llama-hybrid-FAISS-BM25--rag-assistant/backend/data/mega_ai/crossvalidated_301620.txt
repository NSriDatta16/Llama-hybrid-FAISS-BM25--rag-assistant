[site]: crossvalidated
[post_id]: 301620
[parent_id]: 301602
[tags]: 
The specific answer to your question is that one uses the validation accuracy as a measure of how well your model fits -- if validation accuracy is low but training accuracy is high, you've overfit. If both are low, then your model generalizes well and you can use it on your test set. Traditionally, when testing neural network training and performance, one splits the data into three parts: Training, validation, and testing. Training data is fed to the algorithm; validation data is used to determine appropriate hyperparameters and see which random start is best; testing data is never shown to the classifier but is used to look at generalization performance (ideally, I think, after all training is over). 20k samples is not large at all for a neural network. I'm a bit confused by the graph you show--it shows training, validation, and testing all going down to essentially zero over many epochs. This is to be expected for training error, but validation should normally do slightly worse than training. Further, at least as I see it, test error should only be computed at the end of training (but I suppose MATLAB does not agree with me). It makes me a bit suspicious, but I would say that based on this graph and knowing nothing else it appears your model has achieved good performance (assuming, at least, that the input labels were normalized).
