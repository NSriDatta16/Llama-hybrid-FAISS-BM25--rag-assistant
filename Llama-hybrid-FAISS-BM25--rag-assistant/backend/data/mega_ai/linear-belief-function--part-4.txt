butions, and ignorance. The unification is significant not only for knowledge representation in artificial intelligence but also for statistical analysis and engineering computation. For example, the representation treats the typical logical and probabilistic components in statistics — observations, distributions, improper priors (for Bayesian statistics), and linear equation models — not as separate concepts, but as manifestations of a single concept. It allows one to see the inner connections between these concepts or manifestations and to interplay them for computational purposes. Knowledge operations There are two basic operations for making inferences in expert systems using linear belief functions: combination and marginalization. Combination corresponds to the integration of knowledge whereas marginalization corresponds to the coarsening of knowledge. Making an inference involves combining relevant knowledge into a full body of knowledge and then projecting the full body of knowledge to a partial domain, in which an inference question is to be answered. Marginalization Marginalization projects a linear belief function into one with fewer variables. Expressed as a moment matrix, it is simply the restriction of a nonswept moment matrix to a submatrix corresponding to the remaining variables. For example, for the joint distribution M(X, Y), its marginal to Y is: M ↓ Y ( X , Y ) = [ μ 2 Σ 22 ] {\displaystyle M^{\downarrow Y}(X,Y)=\left[{\begin{array}{*{20}c}\mu _{2}\\\Sigma _{22}\end{array}}\right]} When removing a variable, it is important that the variable has not been swept on in the corresponding moment matrix, i.e., it does not have an arrow sign above the variable. For example, projecting the matrix M ( X → , Y ) {\displaystyle M({\vec {X}},Y)} to Y produces: M ↓ Y ( X → , Y ) = [ μ 2 − μ 1 ( Σ 11 ) − 1 Σ 12 Σ 22 − Σ 21 ( Σ 11 ) − 1 Σ 12 ] {\displaystyle M^{\downarrow Y}({\vec {X}},Y)=\left[{\begin{array}{*{20}c}\mu _{2}-\mu _{1}(\Sigma _{11})^{-1}\Sigma _{12}\\\Sigma _{22}-\Sigma _{21}(\Sigma _{11})^{-1}\Sigma _{12}\end{array}}\right]} which is not the same linear belief function of Y. However, it is easy to see that removing any or all variables in Y from the partially swept matrix will still produce the correct result — a matrix representing the same function for the remaining variables. To remove a variable that has been already swept on, we have to reverse the sweeping using partial or full reverse sweepings. Assume M ( X → ) {\displaystyle M({\vec {X}})} is a fully swept moment matrix, M ( X → ) = ( μ ¯ Σ ¯ ) {\displaystyle M({\vec {X}})=\left({\begin{array}{*{20}c}{\bar {\mu }}\\{\bar {\Sigma }}\\\end{array}}\right)} Then a full reverse sweeping of M ( X → ) {\displaystyle M({\vec {X}})} will recover the moment matrix M(X) as follows: M ( X ) = ( − μ ¯ Σ ¯ − 1 − Σ ¯ − 1 ) {\displaystyle M(X)=\left({\begin{array}{*{20}c}{-{\bar {\mu }}{\bar {\Sigma }}^{-1}}\\{-{\bar {\Sigma }}^{-1}}\\\end{array}}\right)} If a moment matrix is in a partially swept form, say M ( X → , Y ) = [ μ ¯ 1 Σ ¯ 11 Σ ¯ 21 μ ¯ 2 Σ ¯ 12 Σ ¯ 22 ] {\displaystyle M({\vec {X}},Y)=\left[{\begin{array}{*{20}c}{\begin{array}{*{20}c}{{\bar {\mu }}_{1}}\\{{\bar {\Sigma }}_{11}}\\{{\bar {\Sigma }}_{21}}\\\end{array}}&{\begin{array}{*{20}c}{{\bar {\mu }}_{2}}\\{{\bar {\Sigma }}_{12}}\\{{\bar {\Sigma }}_{22}}\\\end{array}}\\\end{array}}\right]} its partially reverse sweeping on X is defined as follows: M ( X , Y ) = [ − μ ¯ 1 ( Σ ¯ 11 ) − 1 − ( Σ ¯ 11 ) − 1 − Σ ¯ 21 ( Σ ¯ 11 ) − 1 μ ¯ 2 − μ ¯ 1 ( Σ ¯ 11 ) − 1 Σ ¯ 12 − ( Σ ¯ 11 ) − 1 Σ ¯ 12 Σ ¯ 22 − Σ ¯ 21 ( Σ ¯ 11 ) − 1 Σ ¯ 12 ] {\displaystyle M(X,Y)=\left[{\begin{array}{*{20}c}{\begin{array}{*{20}c}{-{\bar {\mu }}_{1}({\bar {\Sigma }}_{11})^{-1}}\\{-({\bar {\Sigma }}_{11})^{-1}}\\{-{\bar {\Sigma }}_{21}({\bar {\Sigma }}_{11})^{-1}}\\\end{array}}&{\begin{array}{*{20}c}{{\bar {\mu }}_{2}-{\bar {\mu }}_{1}({\bar {\Sigma }}_{11})^{-1}{\bar {\Sigma }}_{12}}\\{-({\bar {\Sigma }}_{11})^{-1}{\bar {\Sigma }}_{12