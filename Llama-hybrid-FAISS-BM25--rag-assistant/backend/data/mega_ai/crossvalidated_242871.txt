[site]: crossvalidated
[post_id]: 242871
[parent_id]: 242833
[tags]: 
Note: This post is fairly old, and might not be correct. Use it only as a starting point, not an authoritative answer. The random forest model is built on decision trees, and decision trees are sensitive to class imbalance . Each tree is built on a "bag", and each bag is a uniform random sample from the data (with replacement). Therefore each tree will be biased in the same direction and magnitude (on average) by class imbalance. However, several techniques exist for mitigating imbalance in classification tasks. Some of these are general and apply to a wide variety of situations. Search for the unbalanced-classes tag on this SE site, and the class-imbalance tag on the Data Science SE site . In addition, random forests are amenable to at least two kinds of class weighting. The first technique is to weight the tree splitting criterion (For information on how this works, see https://datascience.stackexchange.com/a/56260/1156 ). The other technique is to either oversample or undersample data points during the bootstrap sampling process. In Python, weighted tree splitting is implemented in the Scikit-learn class RandomForestClassifier , as the class_weight parameter. Weighted bootstrap sampling is implemented in the Imbalanced-learn class BalancedRandomForestClassifier . Note that the Imbalanced-learn BalancedRandomForestClassifier also supports the same class_weight parameter as the Scikit-learn RandomForestClassifier . In R, both techniques are implemented in the Ranger , in the main ranger function, as the class.weights , case.weights , and sample.fraction parameters. See https://stats.stackexchange.com/a/287849/36229 for a usage example; there is helpful information in the other answers on that same question as well. Apparently, in every extreme cases of class imbalance, you might need to adjust the minimum node size or other "detailed" parameters to get the model to work at all. See, e.g. https://stackoverflow.com/a/8704882/2954547 .
