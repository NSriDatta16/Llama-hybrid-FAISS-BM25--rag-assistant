[site]: crossvalidated
[post_id]: 221577
[parent_id]: 221513
[tags]: 
The 'shared weights' perspective comes from thinking about RNNs as feedforward networks unrolled across time. If the weights were different at each moment in time, this would just be a feedforward network. But, I suppose another way to think about it would be as an RNN whose weights are a time-varying function (and that could let you keep the ability to process variable length sequences). If you did this, the number of parameters would grow linearly with the number of time steps. That would be a big explosion of parameters for sequences of any appreciable length. It would indeed make the network more powerful, if you had the massive computational resources to run it and the massive data to constrain it. For long sequences, it would probably be computationally infeasible and you'd get overfitting. In fact, people usually go in the opposite direction by running truncated backpropagation through time, which only unrolls the network for some short period of time, rather than over the entire sequence. This is done for computational feasibility. Interestingly, RNNs can still learn temporal structure that extends beyond the truncation length, because the recurrent units can store memory from before.
