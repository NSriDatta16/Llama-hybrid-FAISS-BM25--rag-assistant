[site]: crossvalidated
[post_id]: 136700
[parent_id]: 134104
[tags]: 
Why do we divide by the standard deviation whats wrong with dividing by the variance? as @Silverfish already pointed out in a comment, the standard deviation has the same unit as the measurements. Thus, dividing by standard deviation as opposed to variance, you end up with a plain number that tells you where your case is relative to average and spread as measured by mean and standard deviation. This is very close to the idea of $z$-values and the standard normal distribution : If the data are normally distributed, standardization will transform them to a standard normal distribution. So: standardization (mean centering + scaling by standard deviation) makes sense if you consider the standard normal distribution sensible for your data. Why not some other quantity? Like...the sum of absolute values? or some other norm... Other quantities are used to scale data, but the procedure is called standardization only if it uses mean centering and dividing by standard deviation. Scaling is the generic term. E.g. I work with spectroscopic data and know that my detector has a wavelength-dependent sensitivity and an (electronic) bias. Thus I calibrate by subtracting the offset (blank) signal and multiplying (dividing) by a calibration factor. Also, I may be centering not to the mean but instead to some other baseline value, such as the mean of a control group instead of the grand mean. (Personally, I almost never standardize as my variates already have the same physical unit and are in the same order of magnitude) See also: Variables are often adjusted (e.g. standardised) before making a model - when is this a good idea, and when is it a bad one?
