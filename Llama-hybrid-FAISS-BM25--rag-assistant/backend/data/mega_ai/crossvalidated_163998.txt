[site]: crossvalidated
[post_id]: 163998
[parent_id]: 163991
[tags]: 
TL;DR: Garbage in, garbage out. Selecting better features will promote a better model. (Sometimes the answer really is that simple!) What follows is a description of one path forward to selecting higher-quality features in the context of fitting an SVM. SVM performance can suffer when presented with many garbage features because the model only works with the data through the kernel function, rather than working with the features directly as in a more traditional regression analysis. I'll illustrate by comparison to the standard linear kernel and the so-called "automatic relevance determination" method. The standard linear kernel function is $K_1(x,x^\prime)=x^Tx^\prime.$ All features contribute to the output of $K_1$: first we compute the element-wise product, then we sum the products. There's no step that assesses which components of $x$ are more useful than others. We could, if we were so inclined, include a scalar factor $\gamma$ to yield $K_2(x,x^\prime)=x^T\gamma x^\prime,$ but a scalar $\gamma$ simply has the effect of re-scaling $C$, so there are contours of equal performance quality in $(\gamma,C)$ space. But if we replace $\gamma$ with diagonal, symmetric positive semi-definite (SPSD) $\Gamma$, we have $K_3(x,x^\prime)=x^T\Gamma x.$ We can think of this as estimating a coefficient for each entry in $x$, i.e. each feature. We may interpret diagonal elements of $\Gamma$ nearer to zero as contributing relatively little to the classification output, while diagonal elements larger in absolute value contribute more to the output. On the one hand, for $d$ features, you now have $d+1$ tuning parameters (each element of $\Gamma$ and $C$), but on the other, you may submit all features directly to the SVM. This procedure can be further generalized to non-diagonal, but still SPSD, $\Gamma$ to admit nonzero correlation among features. This will yield $\frac{d(d+1)}{2}+1$ tuning parameters, which rapidly becomes unattractive as $d$ grows. Finally, this ARD approach may be extended to other kernels. The RBF kernel varies through the squared Euclidean distance, so we may write $K_4=\exp\left(\frac{(x-x^\prime)^T\Gamma(x-x^\prime)}{\sigma}\right),$ and in general replace any squared Euclidean distance with $(x-x^\prime)^T\Gamma(x-x^\prime).$ P.S. I would like to solve the problem (if a solution exist) without feature selection So... you want to find a subset of features which have high predictive value, but you don't want to do "feature selection"? Perhaps I'm being dense, but it sounds like what you're after is a contradiction in terms. I suppose what an acceptable solution would look like to you depends on how expansive your definition of "feature selection" is.
