[site]: datascience
[post_id]: 118369
[parent_id]: 
[tags]: 
The val_loss is nan, but loss is printing. Both train and validation losses are nan in model.evaluate(), and the acc improves during training

There is a 2-class classification problem, and my loss function is custom. The labels are categorical, and the final activation function is Softmax. During the training, the loss is printed, but the val_loss is nan(inf). Using model.evaluate(X_train,Y_train) at the end of training, the train loss is the same as the vaidation loss, and both are nan. This is my custom loss function. def custom_loss(y_true, y_pred): import tensorflow as tf bce = tf.keras.losses.BinaryCrossentropy( from_logits=False, label_smoothing=0.0, axis=-1, reduction="none", name="binary_crossentropy", ) intra = tf.constant(1, dtype=tf.float64) inter = tf.constant(0.01, dtype=tf.float64) zeros = tf.gather_nd(y_pred,tf.where(tf.argmin(y_true, axis = 1))) ones = tf.gather_nd(y_pred,tf.where(tf.argmax(y_true, axis = 1))) centroid_zero = tf.reduce_mean(zeros,0) centroid_one = tf.reduce_mean(ones,0) loss_zero_intra = tf.math.squared_difference(zeros,centroid_zero) loss_one_intra = tf.math.squared_difference(ones,centroid_zero) loss_zero_intra = tf.cast(loss_zero_intra, tf.float64) loss_one_intra = tf.cast(loss_one_intra, tf.float64) loss_intra = tf.zeros_like(y_pred, tf.float64) loss_intra = tf.tensor_scatter_nd_update(loss_intra,tf.where(tf.argmin(y_true, axis = 1)),loss_zero_intra) loss_intra = tf.tensor_scatter_nd_update(loss_intra,tf.where(tf.argmax(y_true, axis = 1)),loss_one_intra) loss_inter_value = tf.math.sqrt(tf.math.squared_difference(centroid_zero[0],centroid_one[0]) + tf.math.squared_difference(centroid_zero[1],centroid_one[1])) loss_inter = tf.fill(tf.shape(y_pred),loss_inter_value) binary_cross_entropy= tf.tile(tf.expand_dims(bce(y_true,y_pred),axis=1), tf.constant([1,2],tf.int32)) loss_intra = tf.cast(loss_intra, tf.float64) loss_inter = tf.cast(loss_inter, tf.float64) binary_cross_entropy= tf.cast(binary_cross_entropy, tf.float64) loss = tf.math.multiply(intra, loss_intra) - tf.math.multiply(inter, loss_inter) + binary_cross_entropy return loss And Also you can see my model code here: def create_model(kernelLength = 32, nb_classes = 2, Chans = 19, Samples = 512, dropoutRate = 0.5 , F1 = 8, D = 2, F2 = 16, norm_rate = 0.25, dropoutType = 'Dropout', optimizer_type = 'Adam', lr=0.0005, **kwargs): K.clear_session() gc.collect() if dropoutType == 'SpatialDropout2D': dropoutType = SpatialDropout2D elif dropoutType == 'Dropout': dropoutType = Dropout else: raise ValueError('dropoutType must be one of SpatialDropout2D ' 'or Dropout, passed as a string.') input1 = Input(shape = (1, Chans, Samples)) block1 = Conv2D(F1, (1, kernelLength), padding = 'same', input_shape = (1, Chans, Samples), use_bias = False)(input1) block1 = BatchNormalization(axis = 1)(block1) block1 = DepthwiseConv2D((Chans, 1), use_bias = False, depth_multiplier = D, depthwise_constraint = max_norm(1.))(block1) block1 = BatchNormalization(axis = 1)(block1) block1 = Activation('elu')(block1) block1 = AveragePooling2D((1, 4))(block1) block1 = dropoutType(dropoutRate)(block1) block2 = SeparableConv2D(F2, (1, 16), use_bias = False, padding = 'same')(block1) block2 = BatchNormalization(axis = 1)(block2) block2 = Activation('elu')(block2) block2 = AveragePooling2D((1, 8))(block2) block2 = dropoutType(dropoutRate)(block2) flatten = Flatten(name = 'flatten')(block2) dense = Dense(nb_classes, name = 'dense', kernel_constraint = max_norm(norm_rate))(flatten) softmax = Activation('softmax', name = 'softmax')(dense) model = Model(inputs=input1, outputs=softmax) if optimizer_type == 'Adam': optimizer = Adam(learning_rate = lr) if optimizer_type == 'Adamax': optimizer = Adamax(learning_rate = lr) if optimizer_type == 'AdamW': optimizer = AdamW(learning_rate = lr) model.compile(loss=custom_loss, optimizer=optimizer, metrics = ['accuracy']) return model The custom_loss function returns three distinct terms. One of them is the binary_cross_entropy. The model works fine with this term, which works the same as binary_cross_entropy in Keras. Therefore, there is no problem with the data. The train and validation accuracy improves throughout training, and the train loss decreases. The number of validation samples is the same as the number of train samples. After the training was accomplished, by using the model.evaluation(X,Y) function, the loss was shown as "nan," however calculating the loss using the custom loss function, resulting in a "number" not a "nan". Increasing the batch size, scaling the data, and using clipnorm or clipvalue within the optimizer all had no effect. Furthermore, no nan appears in the model predictions (y_pred).I suspect that the problem is caused by the below extreme value inside the model prediction: An example of model prediction with extremes: Can anyone suggest a solution to this problem? Thanks in advance.
