[site]: datascience
[post_id]: 109204
[parent_id]: 109197
[tags]: 
You are right in that "they are simply an ensemble of identical models but weights randomly initialized". If you think about it, the different filters in convolutional layers are also just that. Having multiple heads increases the model's capacity. The randomly initialized weights are certainly the key for each head to learn different things. What the heads really learn has been an active area of research, normally studied by either pruning away heads to see the effect, by measuring the attention patterns to attribute effect, or by probing them in control tasks. These are some conclusions in that regard: From "Are Sixteen Heads Really Better than One?" (publised at NeurIPS'2019) we know that, in many cases, having multiple heads is needed especially at training time, while at inference time is it possible to prune a number of heads (depending on the task) without significant performance loss. From "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned" (published at ACL'2019) we confirm the previous article's conclusion and that the most important and confident heads play consistent and often linguistically-interpretable roles. From "Revealing the Dark Secrets of BERT" (published at EMNLP'2019) we know that "there is a limited set of attention patterns that are repeated across different heads, indicating the overall model overparametrization", therefore confirming the previous findings. From "What Does BERT Look at? An Analysis of BERTâ€™s Attention" (BlackBoxNLP'2019) we learn that "attention heads correspond well to linguistic notions of syntax and coreference". From "Probing for Bridging Inference in Transformer Language Models" (published at NAACL'2021) we learn that "heads at higher layers prominently focus on bridging relations incomparison with the lower and middle layers, also, few specific attention heads concentrate consistently on bridging".
