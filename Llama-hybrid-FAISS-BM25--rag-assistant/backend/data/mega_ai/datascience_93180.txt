[site]: datascience
[post_id]: 93180
[parent_id]: 
[tags]: 
Trying to extend this code to include additional feature volume (in addition to adj close) RNN to predict adj close

I read this article on medium https://medium.com/swlh/a-technical-guide-on-rnn-lstm-gru-for-stock-price-prediction-bce2f7f30346 prep import sys !{sys.executable} -m pip install statsmodels matplotlib numpy yfinance yahoofinancials keras tensorflow sklearn import platform print(platform.python_version()) #import os #os.system("conda install pytorch-cpu torchvision-cpu -c pytorch") import pandas as pd import yfinance as yf import numpy as np import sklearn from yahoofinancials import YahooFinancials AMZN = yf.download('AMZN', start='2013-01-01', end='2019-12-31', progress=False) # AMZN = yf.download('AMZN') for all all_data = AMZN[['Adj Close','Open', 'High', 'Low', 'Close', 'Volume']].round(2) all_data.head(10) Modified part of the code to accept volume def ts_train_test(all_data,time_steps,for_periods): ''' input: data: dataframe with dates and price data output: X_train, y_train: data from 2013/1/1-2018/12/31 X_test: data from 2019 - sc: insantiated MinMaxScaler object fit to the training data ''' # create training and test set #all_data.iloc[:,[0, -1]].values ts_train = all_data[:'2018'].iloc[:,[0, -1]].values ts_test = all_data['2019':].iloc[:,[0, -1]].values ts_train_len = len(ts_train) ts_test_len = len(ts_test) # create training data of s samples and t time steps X_train = [] y_train = [] y_train_stacked = [] for i in range(time_steps,ts_train_len-1): X_train.append(ts_train[i-time_steps:i,0:]) y_train.append(ts_train[i:i+for_periods,0:]) X_train, y_train = np.array(X_train), np.array(y_train) # Reshaping X_train for efficient modelling X_train = np.reshape(X_train, (X_train.shape[0],X_train.shape[1],2)) inputs = pd.concat((all_data[["Adj Close","Volume"]][:'2018'], all_data[["Adj Close","Volume"]]['2019':]),axis=0).values inputs = inputs[len(inputs)-len(ts_test) - time_steps:] inputs = inputs.reshape(-1,2) #inputs # Preparing X_test X_test = [] for i in range(time_steps,ts_test_len+time_steps-for_periods): X_test.append(inputs[i-time_steps:i,0:]) X_test = np.array(X_test) X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],2)) return X_train, y_train , X_test X_train, y_train, X_test = ts_train_test(all_data,5,2) X_train.shape[0],X_train.shape[1] but got stuck on the actual training part the article gives 1 paragraph on what I'm supposed to do but it doesn't make a lot of sense "Figure (D.5) explains the hidden dimensionality. Each time step xt-4 to xt is a vector of the number of features. Our case has one feature, so the dimension for each of xt-4 to xt is 1, i.e., Nx = 1. Nh is the dimension of the hidden layer. If Nh=32, then the parameter matrix U is (32 x 1). The dot product of U and xt has the dimension (32 x 1) x (1 x 1) = (32 x 1). Likewise, the parameter matrix W is (32 x 32), the dot product of W and ht-1 is (32 x 32) x (32 x 1) = (32 x 1). The noise vector therefore should be a (32 x 1) vector." def simple_rnn_model(X_train, y_train, X_test): ''' create single layer rnn model trained on X_train and y_train and make predictions on the X_test data ''' # create a model from keras.models import Sequential from keras.layers import Dense, SimpleRNN my_rnn_model = Sequential() my_rnn_model.add(SimpleRNN(64, return_sequences=True)) #my_rnn_model.add(SimpleRNN(32, return_sequences=True)) #my_rnn_model.add(SimpleRNN(32, return_sequences=True)) my_rnn_model.add(SimpleRNN(64)) my_rnn_model.add(Dense(2)) # The time step of the output my_rnn_model.compile(optimizer='rmsprop', loss='mean_squared_error') # fit the RNN model my_rnn_model.fit(X_train, y_train, epochs=100, batch_size=150, verbose=0) # Finalizing predictions rnn_predictions = my_rnn_model.predict(X_test) return my_rnn_model, rnn_predictions my_rnn_model, rnn_predictions = simple_rnn_model(X_train, y_train, X_test) rnn_predictions[1:10] all the code is pulled/modified from here https://github.com/dataman-git/codes_for_articles/blob/master/From%20regression%20to%20RNN.ipynb
