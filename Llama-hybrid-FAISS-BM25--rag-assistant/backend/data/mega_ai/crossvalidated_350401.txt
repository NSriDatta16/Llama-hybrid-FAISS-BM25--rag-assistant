[site]: crossvalidated
[post_id]: 350401
[parent_id]: 350364
[tags]: 
The purpose of setting a threshold is to remove post hoc fudging and provide clarity in the decision making process. This is not to say this is how it is actually used, but it is the ideal behind it. The thinking behind the process is that you decide how much risk you are willing to take on missing a real effect of a size that matters to you (powering the study on the assumption of an expected effect size) versus the risk of believing data that would not be distinguishable from no effect if the experiment were repeated a vat number of times. This is meant to force you to design the whole experiment, including analysis and decision making up front. If you can't do this, then you are probably in exploratory analysis phase and relying on crude rules of thumb, in which case any outcomes that appear interesting should be validated in independent experiments. Why has the significance of the result have anything to do with the researcher's guess about alpha levels? The alpha shouldn't be a wild guess, rather an educated one. It is about managing risk and should be matched with appropriate powering procedures during design to be most useful. Why can't we treat p value as a continuous variable, instead of randomly picking some alpha significance level before we start? This is reasonable, as long as there are clear rules about how it is handled. For example exploratory analysis may rank p values and filter the top x percent for further testing. I've used this approach to filter principal components for discriminant analysis. Another alternative is to have the post hoc decision tied to the value of p, so one example could be if the results are extremely strong a full R&D budget could be rubber stamped to continue development but is tapered off over a range of p values. In a major multinational R&D intensive organisation this would help balance risk across a research portfolio The example you give is inappropriate as you want to fudge interpretation once you know your data. This is one aspect of investigator bias. Clear logical rules need decided before you see the data so as not to bias interpretation. The rules don't have to be the standard rules of thumb or threshold, but they need clearly and unambiguously explained and be logically consistent.
