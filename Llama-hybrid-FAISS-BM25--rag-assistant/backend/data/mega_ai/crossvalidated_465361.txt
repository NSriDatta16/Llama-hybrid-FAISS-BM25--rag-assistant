[site]: crossvalidated
[post_id]: 465361
[parent_id]: 
[tags]: 
How should I assess a model in the presence of large irreducible noise in my training data?

Suppose my target variable $Y$ is generated from $X$ through a simple linear relationship but with large noise $\epsilon \sim N(0, \sigma^2)$ with $\sigma >> Y$ : $$Y = 0.1 \times X + N(0, 100), X \in [0, 10]$$ Plotting $Y$ against $X$ shows mostly noise: When I fit a linear model, I get a pretty accurate estimate of the coefficient $0.093 \pm 0.035$ . But all the "traditional" model accuracy metrics have terrible values; for example, $R^2 = 0.0007$ and I find it very difficult to convince stakeholders that such models are the best we can do, and are in fact adequate because we are interested in estimating the main effects, not necessarily achieving high accuracy for individual points. How should I deal with such situations, when the target variable is plagued with large irreducible noise but I'd like to know whether the model accurately estimates the main effects? (Note that I'd ideally like a method that also works with non-parametric models such as random forests, so I can't look at p-values or t-values.) Here's the R code for this example: n |t|) (Intercept) -0.02196 0.20184 -0.109 0.91335 x 0.09296 0.03507 2.651 0.00805 ** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 10.09 on 9998 degrees of freedom Multiple R-squared: 0.0007022, Adjusted R-squared: 0.0006023 F-statistic: 7.026 on 1 and 9998 DF, p-value: 0.008046
