[site]: crossvalidated
[post_id]: 567124
[parent_id]: 567095
[tags]: 
I think there's confusion about what constitutes target leakage here. There are two places where this can come up: The more serious kind, and the kind which I think is generally meant by "target leakage": you incorporate some test set information into your model building, and so your test scores are biased. Incorporating training target information into your encoding process, which can lead to too much information available to your eventual model, leading to overfitting. But if you consider the encoding steps as part of the larger modeling pipeline, this isn't really "wrong", it's just another flavor of overfitting. You can reduce this effect by various kinds of regularization, see e.g. https://arxiv.org/abs/2104.00629 . I don't understand why your source does nested cross-validation; the outer split seems sufficient to enforce that the target values for a row are taken from an averaged on other samples, reducing the overfitting effect described in point 2 above. And averaging the means across the inner folds seems likely to be very close to the simple average without the inner folds, assuming the distribution of categories is roughly preserved. There are other ways to accomplish this too, like smoothing with the prior, which is maybe what you meant in your second question? To your third question, usually you just use the average for the entire training set. You wouldn't want to use the targets from the test set if they were available, because that would fall into point 1 above. The entire modeling procedure assumes that test data is iid with the test data, so your last sentence of question 3 is not any different from any other aspect of the modeling process: you use the training data to make predictions about the test data.
