[site]: crossvalidated
[post_id]: 638150
[parent_id]: 
[tags]: 
Why did the OpenAI's scaling law paper underestimate the importance of data in model scaling?

The Chinchilla paper (Hoffmann, Jordan, et al. " Training compute-optimal large language models ." arXiv preprint arXiv:2203.15556 (2022).) famously found that when scaling a model, you should roughly increase your parameter count and data amount equally, as opposed to the earlier OpenAI scaling law paper which said you should increase your parameter count by substantially more than your data amount. On page 3, the Chinchilla paper gives the following explanation for why the OpenAI paper made this mistake: First, the authors use a fixed number of training tokens and learning rate schedule for all models; this prevents them from modelling the impact of these hyperparameters on the loss. In contrast, we find that setting the learning rate schedule to approximately match the number of training tokens results in the best final loss regardless of model sizeâ€”see Figure A1. For a fixed learning rate cosine schedule to 130B tokens, the intermediate loss estimates (for ' But why should this be the case? If your learning rate schedule leads to overestimating the loss for smaller amounts of training data, shouldn't that lead you to overestimate the effect of data on loss and thus recommend increasing data size quicker than parameter count instead of the other way around?
