[site]: crossvalidated
[post_id]: 491936
[parent_id]: 491920
[tags]: 
To quote verbatim from Wikipedia The EM algorithm is used to find (local) maximum likelihood parameters of a statistical model in cases where the equations cannot be solved directly. Typically these models involve latent variables in addition to unknown parameters and known data observations. That is, either missing values exist among the data, or the model can be formulated more simply by assuming the existence of further unobserved data points. For example, a mixture model can be described more simply by assuming that each observed data point has a corresponding unobserved data point, or latent variable, specifying the mixture component to which each data point belongs. When considering the Normal likelihood, the maximum likelihood equation can be solved directly there is no obvious missing data structure with latent variable $Z$ such that $$\int_\mathcal Z f(x,z;\mu)\,\text{d}z = \varphi(x;\mu) \tag{1}$$ and of course, there is (are?) an infinity of ways to create (1), e.g., $$f(x,z;\mu)=\mathbb I_{(0,\varphi(x;\mu))}(z)$$ or $$f(x,z;\mu)=\varphi(x;\mu)\varphi(z;\mu)\tag{2}$$ and one can try to apply EM to such completions but there is no reason that EM will be manageable in such cases. (Note: it works with (2).) As a side remark, there is a fundamental misunderstanding in the following paragraph in Wikipedia Finding a maximum likelihood solution typically requires taking the derivatives of the likelihood function with respect to all the unknown values, the parameters and the latent variables, and simultaneously solving the resulting equations. since maximising in both $(\theta,z)$ returns the joint mode, which differs from the marginal mode.
