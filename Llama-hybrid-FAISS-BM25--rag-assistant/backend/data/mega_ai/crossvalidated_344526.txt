[site]: crossvalidated
[post_id]: 344526
[parent_id]: 344498
[tags]: 
Layers don't map onto successively more abstract features as cleanly as we'd like. A good way to see this is to compare two very popular architectures. VGG16 consists of many convolutional layers stacked on top of each other with the occasional pooling layer -- a very traditional architecture. Since then, people have moved on to designing residual architectures, where each layer is connected to not only the previous layer, but also one (or possibly more) layers farther down in the model. ResNet was one of the first to do this, and has around 100 layers, depending on which variant you use. While VGG16 and similar networks do have layers act in a more or less interpretable manner -- learning higher and higher level features, ResNets do not do this. Instead, people have proposed that they either keep refining features to make them more accurate or that they're just a bunch of shallow networks in disguise , neither of which matches the "traditional views" on what deep models learn. While ResNet and similar architectures handily outperform VGG in image classification and object detection, there seem to be some applications for which the simple bottom-up feature hierarchy of VGG is very important. See here for a good discussion. So given that more modern architectures don't seem to fit into the picture anymore, I would say that we can't quite say CNNs are interpretable yet.
