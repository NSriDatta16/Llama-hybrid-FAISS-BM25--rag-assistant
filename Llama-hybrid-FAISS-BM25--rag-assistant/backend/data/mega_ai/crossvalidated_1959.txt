[site]: crossvalidated
[post_id]: 1959
[parent_id]: 1912
[tags]: 
I still feel negatively about what seems to be a gratuitous insult on King's part but I can see where he might be coming from. "Scale-invariance" is a restriction on a statistical procedure. Thus, limiting our choice of procedures to scale-invariant ones (or to linear ones or to unbiased ones or minimax ones, etc.) potentially excludes procedures that might perform better. Whether this is actually the case or not depends. In many situations, data are reported in units that are essentially independent of what is being studied. It shouldn't matter whether you measure distances in angstroms or parsecs, for example. In this context, any procedure that is not scale invariant is therefore an arbitrary one--and arbitrariness is not a positive attribute in this field. In other situations, though, there is a natural scale. The most obvious of these concern counted data. A procedure that treats counted data as if they were measurements on a continuous scale (e.g., using OLS for a counted response) is potentially inferior to other available procedures and may be (likely is, I suspect) inadmissible in the decision-theoretic sense. This can be a tricky and subtle point because it's not always obvious when we have counted data. One example I'm familiar with concerns many chemical or radioactivity measurements, which ultimately originate as counts on some machine. Said counts get converted by the laboratory into a concentration or activity that forever after is treated as a real number. (However, attempts to exploit this fact in the chemometrics literature have not yielded superior statistical procedures.) Just to stave off one possible misunderstanding: I wouldn't view a selection of an informative prior for a scale parameter (in a Bayesian analysis) as a scale-dependent procedure. Such a prior obviously favors some ranges of values over others, but does not affect the scale invariance of the procedure itself.
