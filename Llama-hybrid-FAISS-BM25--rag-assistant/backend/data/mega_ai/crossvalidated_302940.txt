[site]: crossvalidated
[post_id]: 302940
[parent_id]: 302915
[tags]: 
What went wrong? You're comparing apples with oranges You computed training scores, meaning you scored the model on data you trained it with. You should compute test scores, i.e. a score on data the model has not yet seen. The manual check you did is just one experiment. If you repeat this several times with different $X$ and $y$, you will find model.score to give different values each time. Since you did only one experiment, i.e. used only one X and one y, your manual check is not very robust. Therefore, your manual check produces a one-sample training score . RidgeCV is doing something more robust: It takes the $X$ and $y$ you provided, splits them into 3 parts It internally does model.fit on the first 2 parts and then model.score on the 3rd part, a test score. It repeats step 2, but doing model.fit on part 2 and 3, then model.score on part 1 It repeats step 2 again, but doing model.fit on part 3 and 1, then model.score on part 2 Finally it averages the model.score values from the three steps before, this is what it returns as modelCV.score . That means, RidgeCV produces an averaged test score . The above steps are repeated for all alphas you provide. Each alpha produces such an average. The above process is called 3-fold cross validation, you can read more about it on wikipedia or the scikit-learn documentation .
