[site]: crossvalidated
[post_id]: 380228
[parent_id]: 
[tags]: 
Universal approximation

From my understanding, neural networks are universal approximators, which was proven by Cybenko in 1989 for sigmoid activation functions with output units being linear. Castro showed, that this property also holds true even if the output is not linear, but is a squashing function (non-decreasing with values in $[0,1]$ ). But did someone manage to prove that this property is also true for modern NNs with ReLU activations? I'm quite new to this topic and I haven't been able to find such work, I'd be grateful for your help.
