[site]: crossvalidated
[post_id]: 531746
[parent_id]: 
[tags]: 
What is the interpretation of this modified Metropolis algorithm?

Modified Metropolis-Hastings Consider a model with parameters $\theta = (\alpha, \gamma)$ and consider a modified Metropolis-Hastings algorithm which can be summarized (with brevity) as follows. Propose a new value $\alpha_\star$ Conditional on $\alpha_\star$ , fix $\gamma_\star$ according to some estimation rule (e.g. MLE or MAP) Calculate MH acceptance probability and choose whether or not to accept $\theta_\star = (\alpha_\star, \gamma_\star)$ . An Example To help clarify, we can consider an extremely simple example. Let $X_1, X_2, \ldots X_{n}$ be iid samples from a $N(\alpha, \gamma)$ distribution. Let $\pi(\alpha, \gamma, {\bf X})$ denote the posterior distribution. Initialize $\alpha^0$ . Set $\gamma^0 = \arg\max_{\gamma}\left\{ \pi(\alpha^0, \gamma, {\bf X})\right\}$ Compute $\pi^0 = \pi(\alpha^0, \gamma^0, {\bf X})$ For $t = 1, 2, \ldots T$ Propose $\alpha_\star$ (using a proposal, symmetric about $\alpha^{t-1}$ for simplicity). Set $\gamma_\star = \arg\max_{\gamma }\left\{ \pi(\alpha_\star, \gamma, {\bf X})\right\}$ Calculate $\pi_\star = \pi(\alpha_\star, \gamma_\star, {\bf X})$ Set $\alpha^t = \alpha_\star, \gamma^t = \gamma_\star$ and $\pi^t = \pi_\star$ with probability $\max\left(\frac{\pi_\star}{\pi^{t-1}}, 1\right)$ and set $\alpha^t = \alpha^{t-1}$ , $\gamma^t = \gamma^{t-1}$ and $\pi^t = \pi^{t-1}$ otherwise. Discussion and Questions I have performed a simple simulation study using the simple example described above, and found that this modified approach performed similarly (slightly better, actually) in terms of inference for $\alpha$ than the standard Bayesian approach. It seems that this approach can be viewed as placing a prior distribution on $\theta = (\alpha, \gamma)$ which places all of the prior density on regions of the parameter space where $\gamma$ is maximized with respect to $\alpha$ (a manifold, I believe). I am looking at this for much more complicated problems than the simple example above. In particular, problems where it is very difficult to propose reasonable values of $\gamma$ and maintain a computationally tractable Metropolis-Hastings ratio. Questions: Is this a valid approach to Bayesian inference? Does this algorithm have a name? Is there a nice way to interpret this approach? Literature discussing this in any way is quite welcome.
