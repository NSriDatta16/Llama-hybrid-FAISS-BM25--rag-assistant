[site]: crossvalidated
[post_id]: 250080
[parent_id]: 250007
[tags]: 
Unbiasedness means that under the assumptions regarding the population distribution the estimator in repeated sampling will equal the population parameter on average. This is a nice property for the theory of minimum variance unbiased estimators. However, I think unbiasedness is overemphasized. The mean square error is a good measure of the accuracy of an estimator. It equals the square of the estimator's bias plus the variance. Sometimes estimators with small bias have smaller mean square error than unbiased estimators that have large variances. Biased estimators can be asymptotically unbiased, meaning the bias tends to 0 as the sample size gets large. If the estimator is both asymptotically unbiased and the variance goes to 0 as the sample size gets large, then the estimator is consistent (in probability). Technically, in measure theory, there is a difference between convergence in probability and convergence almost surely. The Cramer-Rao lower bound is a mathematical result that shows, in a particular parametric family of distributions, that no unbiased estimator can have a variance less than the bound. So if you can show that your estimator achieves the Cramer-Rao lower bound, you have an efficient estimator.
