[site]: crossvalidated
[post_id]: 431624
[parent_id]: 326446
[tags]: 
You asked: in the case where is 10's of millions does Gaussian process regression still work? Not in the standard sense of constructing and inverting a large matrix. You have two options: 1)choose a different model or 2) make an approximation. 1) Some GP-based models can be scaled to very large data sets, such as the Bayesian committee machine linked in the answer above. I find this approach rather unsatisfactory though: there are good reasons for choosing a GP model, and if we are to switch to a more computable model we might not retain the properties of the original model. The predictive variances of the BCM depend strongly on the data split, for example. 2) The 'classical' approach to approximation in GPs is to approximate the kernel matrix. There's a good review of these sorts of methods here: http://www.jmlr.org/papers/volume6/quinonero-candela05a/quinonero-candela05a.pdf . In fact, we can usually see these matrix approximations as model approximations, and lump them in with the Bayesian committee machine: they're changes to the model and it can be hard to understand when those changes might be pathological. Here's a super review: https://papers.nips.cc/paper/6477-understanding-probabilistic-sparse-gaussian-process-approximations.pdf The way I advocate for making approximations for large GPs is to avoid approximating the kernel matrix or the model, and approximate the posterior distribution using variational inference. A lot of the computations look like a 'low rank' matrix approximation, but there is one very desirable property: the more computation you use (the more "ranks") the close the approximation is to the true posterior, as measured by the KL divergence. These articles are a good starting point: http://proceedings.mlr.press/v5/titsias09a/titsias09a.pdf https://arxiv.org/pdf/1309.6835 I wrote a longer article on the same argument here: https://www.prowler.io/blog/sparse-gps-approximate-the-posterior-not-the-model In practice, the variational approximation works really well in a lot of cases. I've used it extensively in real applications. And more recently there's been some excellent theory to back up why it should work ( https://arxiv.org/abs/1903.03571 ). A final plug: variational inference in GPs is implemented in gpflow ( https://github.com/GPflow/GPflow )
