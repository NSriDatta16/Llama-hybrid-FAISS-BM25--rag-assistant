[site]: crossvalidated
[post_id]: 402485
[parent_id]: 
[tags]: 
Is a cross-validation list needed when there are no hyperparameters

Original problem formulation - 12/04/2019 : I have a data set of 14 observations with six features each and an output variable of two classes. I applied a logistic regression model with the following workflow: Do not specify any hyperparameters, thus have a first degree decision boundary and a regularization parameter of zero. Split the data set into a training set (8 observations) and a test set (6 observations). Fit a model on the training set and calculate the test error from test set (by averaging over +-1000 random lists) Plot the learning curves and observe that we have a high variance/low bias issue Conclude that the model complexity cannot be reduced since the decision boundary is already of the first degree and thus I can only examine the effect of removing certain features or try to get a larger data set (which is not possible in the foreseeable future) The questions if have are the following: Can I conclude 5. from my previous points? Is it correct to say that I am calculating the test error (and not approximating it) as I did not have the need to finetune any hyperparameters and thus my test set still qualifies as truly unseen? Is it strange to have a model of such low complexity and then still have a high variance/low bias error type? Assuming the second bullet point is correct, can I apply LOOCV (leave one out cross validation) to calculate the test error (or am I then approximating the test error again)? The following was added after reading the answer by EdM - 14/04/2019 Model adjustments made : Following the comments by EdM and by Reading up on "An introduction to statistical learning". I realized that I need to incorperate some sort of feature shrinkage/selection, as I am working with a high-dimensionality problem. This was done by applying ridge-regression in combination with the Leave-One-Out Cross-Validation method (feature standardization was also applied). The graph below indicates thus the error on the cross validation set for different k-order polynomial decision boundaries and different values for the tuning parameter. The higher order-polynomials are given as an informative feature, as I don't really want to use these because it would only enlarge my high-dimensionality problem (as also indicated by EdM above). note: I am not trying to convince a skeptical reader with my model. The goal is more to provide a proof-of-concept of application of Machine Learning in my research field. Question : Ridge aregression still indicates that with a tuning parameter (lambda) of zero and a first degree factorization (k), the lowest error on the CV set is obtaind. This would thus effectively reduce the problem again to a least squares fit. Which, as I understood from the literature, cannot be used due to the high dimensionality. Am I missing something here or should I just do what the graph indicates and stick with the least squares fit?
