[site]: datascience
[post_id]: 126190
[parent_id]: 126184
[tags]: 
My first thought is that xgboost is interpreting the OrdinalEncoder as any other numeric variable rather than distinct, unordered categories. If you have a true categorical variable (where each category is exchangeable with another), then it is incorrect to use the OrdinalEncoder because this will assign a ranking to each category. For example: if I have a categorical variable with categories red, blue, and orange, running OrdinalEncoder on this feature will replace this with values 1, 2, and 3, respectively. Say missing categories are assigned -2 like in your example. In this case, notice that this implies in some sense that orange is closer to blue than it is to red, red is closer to "orange" then it is to missing, and so on! This is likely not what you intend. The result is that xgboost will view this as a numerical variable where the following splits can be made within a particular decision tree with this feature: $\leq 1$ = category 1 and missing vs. 3 $\leq 2$ = category 1, 2 and missing vs. 3 Notice that I can't group categories 1 and 3 together as a possible split as well when treating the feature as an Ordinal variable. If you switch the encoding method to OneHotEncoding instead, the performance of xgboost should improve. I'd also read this page of documentation, which describes the (limited) native handling of categorical variables within xgboost 1.6. EDIT: Upon further clarification from OP in the comments, the above text is not likely the culprit (assuming that in their pandas dataframe, a categorical variable is declared as the type of the column and enable_categorical = True in the call to the xgboost fit function). I am still convinced, however, that running the catboost encoder on the categorical variables and then fitting as per usual will give results similar to running catboost natively, as per below. catboost handles categorical variables in a much more clever way - instead, it numerically encodes the categorical variables in a supervised fashion . Instead of OneHotEncoding with xgboost, you could switch to using the CatBoost encoder from the category_encoders package as a drop in replacement and likely get similar results to catboost.
