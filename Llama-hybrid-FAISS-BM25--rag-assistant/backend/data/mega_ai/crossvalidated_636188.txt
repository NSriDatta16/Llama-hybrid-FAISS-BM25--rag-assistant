[site]: crossvalidated
[post_id]: 636188
[parent_id]: 
[tags]: 
How does the chain-rule look for the gradient of a loss function?

When we are computing the gradient of the loss function, $L$ , of a Word2Vec model, for the context word-embedding, $w_i$ , and the target word-embedding, $t$ . Where the loss function, $L$ , looks like: $$L(z(w,t)) = -\log(\frac{e^{z_i}}{\sum_{j=1}^{N} e^{z_j}})$$ Does the chain-rule set-up for $\frac{\partial L}{\partial w_i}$ look like: $$\frac{\partial L}{\partial w_i} = \frac{\partial L}{\partial z} \cdot \frac{\partial z}{\partial w_i}$$ Or does it look like: $$\frac{\partial L}{\partial w_i} = \frac{\partial L}{\partial z_i} \cdot \frac{\partial z_i}{\partial w_i}$$ And for $\frac{\partial L}{\partial t}$ , would it look like: $$\frac{\partial L}{\partial t} = \frac{\partial L}{\partial z} \cdot \frac{\partial z}{\partial t}$$ Or does it look like: $$\frac{\partial L}{\partial t} = \frac{\partial L}{\partial z_i} \cdot \frac{\partial z_i}{\partial t}$$ The question being, do we specifically use $z_i$ , or do we use $z$ ? Where $z$ is the input to the SoftMax Function. This is important to me because the calculated gradient will be different depending on which is used, that being $z$ or $z_i$ .
