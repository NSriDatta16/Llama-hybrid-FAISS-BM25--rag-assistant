[site]: crossvalidated
[post_id]: 544702
[parent_id]: 
[tags]: 
Sensibly dealing with the precision available in likelihood functions

I am running some simulations involving Bayesian updating of prior odds given a succession of measurements and corresponding likelihood functions. Inevitably, repeated multiplication of the prior by the likelihood functions results in ever decreasing values. At the extremes, where the probabilities rapidly approach zero, the loss of precision doesn't matter. The real problem is in the neighbourhood of the maximum likelihood value where the resulting function becomes flattened out as a consequence of the precision loss. What sensible approaches are there to dealing with this?
