[site]: datascience
[post_id]: 50923
[parent_id]: 50917
[tags]: 
The complete tree gets a random subset of features, then works with it at a tree level. Correction: Correct answer As correctly stated in this answer, every node gets its own random selection of features. The explanation to what you ask in the second paragraph is that the trees are not globally optimal but locally optimal. The random subset is selected because is more likely to find a global optimum if a random selection is done. As an example: Suppose we have a dataset with Age, Gender, Salary, Marital Status and Nationality. The optimal tree is always unknown, but let's suppose that we know it and is made from the path Age-Gender-Salary . What Random Forest does is random select between Age, Gender, Salary, Marital Status and Nationality. What a normal decision tree does is to select all the features to make the tree. The decision tree selects this tree: Salary-Marital Status-Age ALWAYS. Why is it possible to have this decision tree? Because salary is the local optimum (the best variable at the first step), but Salary-first-then-others could not be the best combination. The Random Forest selects many possible combinations of the variables, in which we could find Age-Gender-Salary which is the optimal. The way Random Forest works is to try many combinations of variables, one of them could be the optimal.
