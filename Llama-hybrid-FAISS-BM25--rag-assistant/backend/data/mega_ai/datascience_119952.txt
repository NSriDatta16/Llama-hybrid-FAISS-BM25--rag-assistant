[site]: datascience
[post_id]: 119952
[parent_id]: 
[tags]: 
What does Codex take as tokens?

The typical default for neural networks in natural language processing has been to take words as tokens. OpenAI Codex is based on GPT-3, but also deals with source code. For source code in general, there is no corresponding obvious choice of tokens, because each programming language has different rules for tokenizing. I don't get the impression Codex uses a separate tokenizer for each language. What does it take as tokens?
