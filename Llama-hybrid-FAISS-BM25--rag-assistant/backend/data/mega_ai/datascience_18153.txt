[site]: datascience
[post_id]: 18153
[parent_id]: 18093
[tags]: 
A neural network can approximate any continuous function, provided it has at least one hidden layer and uses non-linear activations there. This has been proven by the universal approximation theorem . So, there are no exceptions for specific functions. You ask: I read somewhere on the StackExchange that a neural network can't approximate the Pi number as a function of circles length and radii. A neural network to approximate $\pi$ is very easy. Possibly what you read is that a neural network cannot generate new digits of $\pi$ that it has not already been shown. More on that later . . . What about the sum or multiplication of any arbitrary numbers? Yes, a neural network can approximate that. Are there any other specific functions neural networks can't approximate or not? No, there are no specific functions that a neural network cannot approximate. However, there are some important caveats: Neural networks do not encode the actual functions, only numeric approximations. This means there are practical limits on the ranges of inputs for which you can achieve a good approximation. A neural network being able to approximate a function in theory is not the same thing as you or I being able to construct a neural network that approximates that function. There is no known method to construct a neural network by analysis of a function alone (it can be done for specific simple functions such as xor). The usual way to achieve approximation is to train a neural network by giving example data. The network will approximate to data it has been shown. There is no guarantee that this will generalise to new inputs that it has not been trained on and approximate the correct outputs. In fact for certain types of input/output it cannot possibly do so. For instance, it will not learn how to generate the 4th digit of $pi$ if it has been shown digits 1,2,3,5,6,7,8,9. The best generalisation results occur for functions that have smooth transitions between the training examples. Neural networks do not extrapolate well to inputs outside of the data they have used for training. They "fit" to the training data (imagine a rubber sheet draped over all the points in the training set). Neural networks do not learn to copy algorithms, only functions. So if you take a complex algorithm, such as AES encryption, and attempt to train a neural network to perform this given lots of input examples, has no real chance of working. Now, AES encryption can be considered a function e.g. $output = encrypt( input, key )$. So the NN can approximate it. But it will only do so for the specific inputs and outputs it has been shown. In addition AES does not respond well to approximation - a single bit wrong will cause it to be a bad encryption. So you won't see NNs used to encrypt or decrypt in cryptography. The capability of a neural network to approximate is limited by the number of neurons and connections it has. More complex functions require larger networks. In order to train a larger network on a more complex function takes more time and more training data. You could in theory train a neural network to learn a random number generator function. However, that would take an impossible amount of resources - memory to store the network, and time to train it against the whole output of the RNG.
