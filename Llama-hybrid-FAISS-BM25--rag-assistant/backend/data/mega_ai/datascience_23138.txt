[site]: datascience
[post_id]: 23138
[parent_id]: 23124
[tags]: 
Why isn't it very very simple? The problem description is simple, but not in a way that is accessible to the standard RL algorithms. First of all, the analogy of switches and "corresponding" has no meaning to the RL. Secondly, the low complexity is not detectable and is outside of the assumptions of RL agents in general. Reinforcement Learning is based on an internal model of the problem as a generic Markov Decision Process (MDP). The RL agent has to discover, by trial and error, what the MDP does, typically starting with no knowledge or assumptions other than the set of states and allowed actions. The agent needs to discover enough about the MDP to make optimal actions, whilst the only assumption is that the environment is a MDP. The simple switch problem can definitely be modelled as a MDP. The agent then has to discover the simple configuration in the toy problem. What makes it harder than the problem description: State/action space. There are 1024 states and 10 actions. There is no a priori reason to link any action with any outcome, so to be thorough the agent has to try them all. A minimal search would be across 10,240 state/action pairs. State transitions. There are 1024 states where an action can be taken, and a terminal state. So there are 1024 * 1025 = 1,049,600 possible state transitions for every action. In your case, each state/action pair has one outcome, and there are roughly 5,000 different transitions (because half of them are to the terminal state). The agent does not know that the state transitions are deterministic and sparse. It can only discover that by trial and error. For instance, perhaps in a particular configuration, taking one particular action would cause some bits to re-set. Maybe that only happens one time in four attempts. Once you add the possibility of arbitrary probability distributions here, then the number of potential MDPs is infinite. RL algorithms are designed to cope with this, still learning a good approximation of an optimal policy eventually. There is no simple way to make them dumber and faster against a simpler environment without encoding some of your knowledge about that environment. The agent does not know that the rewards are deterministic and simple. Again it can only discover this by trial and error. To the agent the different state vectors and actions are neither similar to each other (all switches and bits) nor have any association (action 0 is not associated in any way with bit 0). Even after the associations are observed, they are not encoded in a way that the agent can reason by analogy. Again, once you add the possibility of arbitrary probability distributions for reward, then the number of potential MDPs is infinite. Suggest a reinforcement learning agent that will learn to efficiently switch on a bit vector You have made things a lot harder for yourself by trying a policy gradient method with a neural network function approximator. Although this probably can be made to solve your problem, it is an advanced technique, and also known to be unstable and hard to train. It might be a good choice when the relationship between the state, transitions and rewards was very complex, and there were more states possible with meaningful similarity between them (in a numerical/statistical sense, not in terms of analogy, the neural network still won't find the analogy with switches and numbered bits). Even then, it will learn slowly and cautiously, needing to repeat state/action pairs multiple times in order to learn that the transitions and reward are reliable. Unless there is some good reason for you to stick with a policy gradient method, I suggest using a tabular algorithm (i.e. no function approximation, just a table of action value estimates) and something like single step Q-Learning. That has the advantage that because you know the algorithm is deterministic, you can set a high learning rate and it will remain stable. In fact Q-learning will probably learn an optimal policy in much less than 10,000 episodes. However, initially it will learn just one of the many possible optimal policies and tend to stick with it - i.e. some permutation of taking each action 0 to 9, once each, but always in the same order because it has learned that is safe.
