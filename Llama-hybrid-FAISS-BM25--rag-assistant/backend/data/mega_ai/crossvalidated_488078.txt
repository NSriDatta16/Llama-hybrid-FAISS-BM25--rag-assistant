[site]: crossvalidated
[post_id]: 488078
[parent_id]: 
[tags]: 
how to get total Fisher matrix that makes cross synthesis of 2 Fisher matrix

I have initially posted on physics.stackexchange but I think my issue is more adapted on Cross-Validated (so I am going to delete the initial post on physics.stackexchange). I have 2 Fisher matrixes which represent information for the same variables (I mean columns/rows are the same in the 2 matrixes). Now I would like to make the cross synthesis of these 2 matrixes by applying for each parameter the well known formula (coming from Maximum Likelihood Estimator method) : $$\dfrac{1}{\sigma_{\hat{\tau}}^{2}}=\dfrac{1}{\sigma_1^2}+\dfrac{1}{\sigma_2^2}\quad(1)$$ $\sigma_{\hat{\tau}}$ represents the best estimator representing the combination of a sample1 ( $\sigma_1$ ) and a sample2 ( $\sigma_2$ ). Now, I would like to do the same but for my 2 Fisher matrixes, i.e from a matricial point of view. For this, I tried to diagonalize each of these 2 Fisher matrix. Then, I add the 2 diagonal matrix and I have so a global diagonal Fisher matrix, but I don't know how to come back in the space of start (since the diagonalization don't give the same combination of eigen vectors for each matrix). If I could get back in the same time to starting parameters space, I could do matrix products to get the final Fisher matrix by doing : $$\text{Fisher$_{\text{final,cross}}$} = P.\text{Fisher$_{\text{diag,global}}$}.P^{-1}\quad(2)$$ with $P$ the passing matrixes (composed of eigen vectors) and I could get directly the covariance matrix by inverting $\text{Fisher}_{\text{final,cross}}$ How can I come back from $(2)$ of the $\text{Fisher$_{\text{diag,global}}$}$ diagonal matrix to starting space, i.e the single parameters ? My difficulties come from the fact that the diagonlization of the 2 Fisher matrixes will produce different passing matrix $P_1$ and $P_2$ , that is to say, different eigen vectors, so a different linear combination of variables between both. I have written the passing matrix $P$ above but it is not defined, I think an expression of $P$ as a function of $P_1$ and $P_2$ passsing matrixes is the key point of my issue. There is surely a linear algebra property whih could allow to circumvent this issue of taking into account the 2 different linear combination of variables while being able to come back in the space of start, i.e space of single parameters which represent the Fisher matrixes. If someone could help me to perform this operation, I hope that I have been clear. If you have any questions, don't hesitate, I would be glad to give you more informations. EDIT 1: I take the following notations : $D$ diagonal matrix is the sum of the 2 diagonalized matrix $D_1$ and $D_2$ (from Fisher1 and Fisher2 initial matrixes) : $D=D_1+D_2$ $P_1$ and $P_2$ are respectively the "passing" matrixes (from Fisher1 and Fisher2 diagonalization) composed of eigen vectors. $M_1$ is the Fisher1 matrix and $M_2$ is the Fisher2 matrix. So, I am looking a way to find an endomorphism $M$ that checks : $$D=P^-1.M.P\quad(3)$$ where $P$ is the unkonwn "passing" matrix. So, there are 2 unknown quantities in my issue : The "passing" matrix, i.e the eigenvectors (I am yet trying to build it from $P_1$ and $P_2$ matrixes). The $M$ matrix which represents this endomorphism. But in this world of unknown quantities, I know however the eigen values of this wanted endomorphism $M$ (they are equal to the diagonal of matrix $D$ ). Would anyone help me to find a way to build this $P$ "passing" matrix from $P_1$ and $P_2$ ? As you can see, a simple sum is not enough. If an exact building of $P$ from $P_1$ and $P_2$ is not possible, is there a way to approximate it ? ps : the links that @AJKOER gave are pretty hard to extract useful informations from these papers. EDIT 2: @AJKOER : thanks for your help. Your idea is a good beginning in my research to carry out a full cross-correlation between my 2 Fisher matrixes, at least for the moment rather an approximation. I say that since in a previous study, I have used antoher method which is also an approximation of an exact formulation of cross-correlation (that will be pretty hard if I have understood your opinion). For example, with your trick, I can get constraints closed to another method that I have test, so we are on the right way. This old known test gives for 1 sigma error the following constraints : wm +/- 0.0012417036832725796 wb +/- 0.0005995521931530912 w0 +/- 0.020152731097000408 wa +/- 0.07473741589196892 h +/- 0.001003370899834334 ns +/- 0.0018175790165196942 s8 +/- 0.0010147066130214034 and with the method of average eigen values into diagonal Fisher matrix1 matrix2, I get : wm +/- 0.002003104719056934 wb +/- 0.0006792309014032004 w0 +/- 0.023872510159754036 wa +/- 0.08488679494420406 h +/- 0.0008331587961258862 ns +/- 0.002039401547697202 s8 +/- 0.0019000745102872813 Except for the first and last parameters (wm and s8 parameters), as I already said, contraints are closed. I have also a criterion of precision (determinant of the sub-block (w_0,w_a)) which is pretty similar. But I think I can do better, what are your suggestions ? A colleague told me to apply a prior but How could I proceed ? The method of average on eigen values is already a "prior method", isn't it ? By the way, what does this average from a Fisher information point of view or physics point of view mean ? I have to keep on studying the papers given by @AJKOER , I have difficulties to assimilate them and extract a more rigorous method ! Any help is welcome.
