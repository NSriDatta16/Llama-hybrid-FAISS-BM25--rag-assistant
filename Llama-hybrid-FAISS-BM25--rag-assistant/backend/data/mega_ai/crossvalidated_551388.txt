[site]: crossvalidated
[post_id]: 551388
[parent_id]: 551375
[tags]: 
@RichardHardy already gave a partial answer 1) AIC does have an interpretation as twice the negative expected log-likelihood (as mentioned e.g. here ). Hence, it is not only a relative measure. Moreover, it is in a sense a measure of error. 2) Cross validation used to be computationally infeasible for some complex models, but AIC/BIC are algebraically infeasible since the models' likelihood and degrees of freedom can be very hard to obtain. Extending it, notice that things like LR tests or AIC are measured on your training data as compared to out-of-sample approaches like having held out test set for validation, $k$ -fold cross-validation, LOOCV, etc. When using the former metrics you are making the assumption that what they measure tells you something that is relevant for judging the potential out-of-sample performance of the model. When using some form of cross-validation you are directly measuring the out-of-sample performance. Of course, your test set is a subsample of the data you gathered, so if your data is not representative for the population, the cross validation metrics would be biased as well. Moreover, as noted by Richard, using cross validation may be simpler to do (it works for whatever model you want, no math is needed), but more computationally expensive, so there would be cases where you would prefer one of the approaches as compared to the another. You are not always concerned with out-of-sample performance. Machine learning is concerned about making predictions and it favors cross-validation, statistics is concerned about inference and it often uses the in-sample metrics. See The Two Cultures: statistics vs. machine learning? for details. Finally, the metrics do not necessarily have sense in machine learning scenario, for example, AIC penalizes the number of parameters, you wouldn't do that for a deep learning model where the number of parameters is always huge and it's not your biggest concern.
