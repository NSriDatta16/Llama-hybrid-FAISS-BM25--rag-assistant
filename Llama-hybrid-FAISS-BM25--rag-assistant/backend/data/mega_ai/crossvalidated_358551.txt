[site]: crossvalidated
[post_id]: 358551
[parent_id]: 
[tags]: 
How should I choose the Encoder hyperparameters to make its memory state suitable for the Decoder in a Bidirectional Neural Network?

I'm trying to implement Neural Machine Translation following the tutorial on the Tensorflow website here https://www.tensorflow.org/versions/r1.8/tutorials/seq2seq and I was able to build a unidirectional model without attention mechanism that manages to translate small sentences very well. Now I tried to introduce the attention mechanism and make the encoder a bidirectional neural network (I did the two things together because I read that attention works much better this way). My problem is that I don't know how to manage the internal state that the encoder needs to pass to the decoder. In the unidirectional case I directly took the state that was returned to me by the tf.nn.dynamic_rnn method which had size HIDDEN_SIZE (it was a tuple containing the values ​​of H and C). The method I'm using now is tf.nn.bidirectional_dynamic_rnn, which returns a tuple of two elements, both of HIDDEN_SIZE size and representing the forward part state and the backward part state of the bidirectional network. I know that I should concatenate them but in this case the size of the state doubles and I can no longer pass it to the decoder. I noticed that I can handle this problem in two ways: by halving the size of the encoder state or the number of its layers. What is the best approach? There are other ways?
