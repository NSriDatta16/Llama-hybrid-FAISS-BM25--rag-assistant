[site]: crossvalidated
[post_id]: 473564
[parent_id]: 
[tags]: 
Equivalence of Contextual Bandit formulations

I find two different types of formulations for Contextual Bandits: Definition 1 : In a contextual bandits problem, there is a distribution $P$ over $(x,r_1,...,r_k)$ , where $x$ is the context, $a \in [k]$ is one of the $k$ arms to be pulled, and $r_a$ is the reward for arm $a$ . The problem is a repeated game: on each round, a sample $(x, r_1, ..., r_k)$ is drawn from $P$ , the context $x$ is announced, and for precisely one arm $a$ chosen by the player, its reward $r_a$ is revealed. Definition 2 : The algorithm observes the current user $u_t$ and a set $A_t$ of arms or actions together with their feature vectors $x_{t,a}$ for $a \in A_t$ . The vector $x_{t,a}$ summarizes information of both the user $u_t$ and arm $a$ , and will be referred to as the context. Based on observed payoffs in previous trials, the agent selects an arm $a_t âˆˆ A_t$ , and receives payoff $r_t$ whose expectation depends on both the user $u_t$ and the arm $a_t$ . When stating def. 2, the authors of the paper cite the paper from def. 1: "Following previous work" [(def. 1)] "we call it a contextual bandit." This is very confusing to me. In particular, def. 1 assumes that only one context is revealed to the learner. In the second formulation, you observe "contexts" or better features for all the arms. I was thus wondering if there is any equivalence between the two formulations or a way to relate them.
