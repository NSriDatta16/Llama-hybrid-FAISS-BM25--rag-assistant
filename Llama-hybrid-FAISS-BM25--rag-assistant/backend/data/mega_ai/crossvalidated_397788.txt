[site]: crossvalidated
[post_id]: 397788
[parent_id]: 396279
[tags]: 
First off, we need to specify a parametric form for both the mean function $\mu$ (e.g. linear function $\alpha + \beta^Tx$ ) and the covariance $k$ (e.g. RBF or Mat√©rn). These parameters $\alpha$ , $\beta$ , $\sigma^2$ etc. are often known as hyperparameters in GP regression. In the general case when the function paths are not i.i.d., we can index each with a variable $u$ . Then the solution is to simply perform GP regression with the mean and covariance functions taking the forms $\mu(x, u)$ and $k((x, u), (x', u'))$ , and then optimize the hyperparameters using e.g. maximum marginal likelihood or Bayesian inference. However, the complexity for the regression is $O(N^3)$ , where N is the total number of points $(x, u)$ , i.e. $N = \sum_{i=1}^n p_i$ . However, in the i.i.d. case, we can simply run a GP regression on each of the $n$ datasets $f_i$ individually, optimizing or Bayesian updating the same hyperparameters for all of these. In this case the complexity is (assuming the number of datapoints per dataset $p_i = p$ is fixed) $O(np^3)$ instead of $O(n^3p^3)$ . The resulting hyperparameter values (if using maximum likelihood) or hyperparameter distribution (if using Bayesian inference) provide the learned "distribution over functions". Thanks @Yves for the helpful comments.
