[site]: crossvalidated
[post_id]: 495763
[parent_id]: 
[tags]: 
How can I best calculate the expected amount of life lost over the next 50 years due to a constant yearly 1.5% risk of death?

Say that Person X has 50 years left to live until they die from cause Z. Cause Y is a constant 1.5% risk of death per year for the next 50 years. I want to determine what the expected amount of life lost is due to cause Y. I've tried three approaches to determining this that all yield vastly different results. Is any of these valid? If so, which and why aren't the others? I thought I'd be able to calculate the expected amount of life lost due to cause Y by: (1-(1-0.015)^50)*25.5 Where (1-0.015) is the chance of surviving in a year, raised to the 50th power to calculate the chance of survival over the next 50 years, 1- this amount is to convert from the chance of surviving to the chance of dying, and multiplied by 25.5 as the average number of years remaining in that persons life over the 50 year timespan. This yields a value of 13.52 expected years lost due to cause Y. I also thought that this was equivalent to saying that you lose 1.5% of your life each year. So then, over the next 50 years you'd lose 50*0.015 = 0.75 years of life. My last approach was to say that for each of the next 50 years, if the risk is realized, you lose not only that year, but also each of the following. Therefore the amount of time lost can be calculated by: sum(0.015*(years remaining)) over the next 50 years = 19.13 years. shown differently: 0.015*49+0.015*48 + ... + 0.015*1 = 19.13 years With such big discrepancies, clearly at least two of these approaches are flawed. What's the best way to calculate this and why are the others flawed?
