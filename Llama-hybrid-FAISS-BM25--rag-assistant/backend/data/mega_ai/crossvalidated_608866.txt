[site]: crossvalidated
[post_id]: 608866
[parent_id]: 
[tags]: 
The confusing derivation in the book 'Machine Learning: A Probabilistic Perspective' by Kevin P. Murphy

In the section 15.5 of the book 'Machine Learning: A Probabilistic Perspective' by Kevin P. Murphy, it discusses the Gaussian Process Latent Variable Model. The log-likelihood objective function is given By $$l = -\frac{D}{2}\ln|K| - \frac{1}{2}\text{tr}(K^{-1}YY^{T})\tag 1$$ Where $K = ZZ^T + \beta^{-1}I$ and the gradient with regard to Z is given by: $$\frac{\partial l}{\partial \mathbf{Z}_{ij}} = \frac{\partial l}{\partial \mathbf{K}}\frac{\partial \mathbf{K}}{\partial \mathbf{Z}_{ij}}\tag 2$$ and $$\frac{\partial l}{\partial K} = K^{-1}YY^TK^{-1} - DK^{-1}\tag 3$$ (I think the author omits the ' $ \frac{1}{2}$ ' here). Anyway, I can get the equation (3) by the rules in MatrixCookbook . The author then says we can have $$\frac{\partial K}{\partial Z} = Z$$ (I think the author omits 2 here) Finally, we get $$\frac{\partial l}{\partial Z} = K^{-1}YY^TK^{-1}Z - DK^{-1}Z\tag 4$$ The result matches the result in Lawrence 2005 . And there is a similar derivation in this site, see the answer . It seems that the chain rule( $$\frac{\partial l}{\partial \mathbf{Z}} = \frac{\partial l}{\partial \mathbf{K}}\frac{\partial \mathbf{K}}{\partial \mathbf{Z}}\tag 5$$ ) works here. But as I know, only $$\frac{\partial \text{Tr}[K]}{\partial Z}= 2Z\tag 6$$ and $$\frac{\partial l}{\partial \mathbf{Z}_{ij}} = \text{Tr}\left[\left(\frac{\partial l}{\partial \mathbf{K}}\right)^T\frac{\partial \mathbf{K}}{\partial \mathbf{Z}_{ij}}\right]\tag 7$$ . I assume the author omits the'Tr', but How can I get (4) by using (6) and (7).And are there any connections between (5) and (7)? As far as I know, the equation (5) is invalid for matrix-multiplying-shape match.
