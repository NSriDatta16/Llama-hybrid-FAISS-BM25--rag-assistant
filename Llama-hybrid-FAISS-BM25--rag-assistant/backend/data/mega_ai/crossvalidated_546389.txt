[site]: crossvalidated
[post_id]: 546389
[parent_id]: 546277
[tags]: 
Short answer: A $2\times 2$ table has 1 degree of freedom (df), and one odds ratio (OR) correspondingly. A $2\times 3$ table has two df, and two ORs. In that case, the null hypothesis tested by Fisher's test can be formulated as both the ORs being equal to 1. You could calculate the ORs by splitting into two $2\times 2$ tables, or simply by using a logistic regression, where the coefficents (forget the intercept) are log odds ratios. With your data, using R: obs $Group Group) yourdf $Result Result) First we fit a binary logistic regression model (LR) using Group as response variable. That doesn't sound natural, but for a binary LR we need a two-valued response ... mod1 |z|) (Intercept) -1.0986 0.3481 -3.156 0.00160 ** ResultSame 0.6466 0.4238 1.526 0.12710 ResultImprove 1.2809 0.4411 2.904 0.00368 ** --- Compare the results using a multinomial LR, which looks more natural here: library(nnet) mod2 Now compare the log ORs from the two models! To see how closely related these two model formulations are, let us compare the likelihood ratio tests comparing to the null model (corresponding to the null hypothesis): mod2_0 Chi) NULL 5 230.65 Result 2 9.1435 3 221.50 0.01034 * anova(mod2_0, mod2, test="Chisq") Likelihood ratio tests of Multinomial Models Response: Result Model Resid. df Resid. Dev Test Df LR stat. Pr(Chi) 1 1 10 368.7937 2 Group 8 359.6502 1 vs 2 2 9.143495 0.01033987
