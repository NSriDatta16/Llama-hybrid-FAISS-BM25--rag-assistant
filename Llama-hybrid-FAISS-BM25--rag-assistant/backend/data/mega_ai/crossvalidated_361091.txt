[site]: crossvalidated
[post_id]: 361091
[parent_id]: 361079
[tags]: 
The answer by @shimao covers the important points fairly well: The "correct" model for your data will not be a model with zero training error. Thus, using zero training error as a criterion for model selection will not lead to the correct model. Some non-parametric algorithms are able to memorize the training dataset theoretically (Neural networks, see this answer for more details) or practically (1-NN classifier). In such case, zero training error tells us nothing about the capability of the model to recover the underlying structure of the problem and model it well. So, to sum up: No, don't try super hard to get zero training error . Training error is a useful indicator for debugging (and trying to overfit is a good debugging step), but you should not overdo it. There is a very interesting old question along similar lines (although certainly not a duplicate), unfortunately missing a good conclusive answer. However, the thread links a comic describing the issue better than a long answer:
