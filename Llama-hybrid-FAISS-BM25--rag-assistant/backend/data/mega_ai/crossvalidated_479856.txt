[site]: crossvalidated
[post_id]: 479856
[parent_id]: 475999
[tags]: 
An independent proposal does not depend on the previous realisation of the Markov chain, while a random walk proposal does in a symmetric way. Therefore when making a proposal from the $\mathcal N(\hat\beta,1/X'X)$ distribution, there is independence from the previous realisation (since $\hat\beta$ is fixed). The generic acceptance probability is $$\alpha=\min\left( \frac{p(\beta^{*}|y) g(\beta^{(m)}|\beta^{(*)})}{p(\beta^{(m)}|y)g(\beta^{(*)}|\beta^{(m)})}, 1 \right)$$ which, in this case, is $$\alpha=\min\left( \frac{p(\beta^{*}|y) g(\beta^{(m)}|\hat\beta)}{p(\beta^{(m)}|y)g(\beta^{(*)}|\hat\beta)}, 1 \right)$$ and there is no cancellation for symmetry. However, $$p(y|\beta) \propto \exp\left\{- \frac{1}{2} ||y-\hat y||^2- \frac{1}{2} ((X'X)(\beta-\hat\beta)^2\right\}$$ and hence $p(y|\beta)\propto g(\beta|\hat\beta)$ , meaning $$\alpha=\min\left( \frac{\pi(\beta^{*})}{\pi(\beta^{(m)})}, 1 \right)$$ Therefore if $\gamma$ is large, $\pi(\beta)$ is almost constant and the acceptance probability about equal to one. If $y_{t+1}|\beta,x_{t+1}\sim\mathcal N(\beta x_{t+1},1)$ , the posterior predictive distribution of $y_{t+1}$ is obtained by simulating $\beta^*$ from the posterior, i.e., sampling a term in the Markov chain, and simulating $y_{y+1}$ given $\beta^*$ . The value of $z_{t+1}=\exp(y_{t+1})$ follows. (Do not use the notation $p(\exp(y_{t+1})|\ldots)$ since this is most likely incorrect, depending on the meaning of $p(\cdot|\cdot)$ .
