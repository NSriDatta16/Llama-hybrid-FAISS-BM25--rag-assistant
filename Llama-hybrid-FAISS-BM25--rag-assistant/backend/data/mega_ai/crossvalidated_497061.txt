[site]: crossvalidated
[post_id]: 497061
[parent_id]: 497051
[tags]: 
The reason it is such a useful method reduces to to Bayes' formula: $$p(x \vert \text{data}) = \frac{p(\text{data} \vert x) p(x)}{p(\text{data})}$$ Typically, we denote $p(\text{data} \vert x)$ the likelihood function, $p(x)$ the prior distribution and in your notation $C = p(\text{data})$ is the marginal likelihood of the data. As statisticians or machine learners or whatever we desire to know the reasonable values $x$ can take under this model formulation. If you have a model of the above form and you want to estimate the mean of $x$ , or the variance of $x$ , or some other function of $x$ , you need to be able to integrate with respect to $p(x \vert \text{data})$ . The problem is, for nearly all choices of likelihood and prior, the product is not a known distribution, and it is furthermore also completely impossible to actually integrate numerically due to the dimensionality of $x$ . For interesting applications of MCMC to (Bayesian) models, you could look at Gerrymandering in North Carolina Target-tracking of people in videos Melting snow in the Himalayas Plankton ecosystems Condensed matter physics Radio interferometric imaging
