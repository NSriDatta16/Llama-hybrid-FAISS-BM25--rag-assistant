[site]: datascience
[post_id]: 124912
[parent_id]: 124902
[tags]: 
First of all, it is necessary to figure out what kind of attention mechanism we are talking about. If this is a «classical» attention mechanism, then Q and K are calculated for completely different sequences: input and output. If this is a self-attention, then, indeed, they are calculated for the same sequence, but for different tasks. Q – represents the token for which the attention is calculated. On the other hand, K – represents tokens that can be paid attention to.
