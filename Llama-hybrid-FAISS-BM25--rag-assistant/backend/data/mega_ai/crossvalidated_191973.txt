[site]: crossvalidated
[post_id]: 191973
[parent_id]: 191803
[tags]: 
I would be interested in feedback on the paragraph beginning "Upon reflection...", since particular part of the model has been keeping me up at night. The Bayesian model The revised question makes me think that we can develop the model explicitly, without using simulation. Simulation introduced additional variability due to the inherent randomness of sampling. Sophologists answer is great, though. Assumptions : the smallest number of labels per envelope is 90, and the largest is 100. Therefore, the smallest possible number of labels is 9000+7+8+6+10+5+7=9043 (as given by OP's data), 9000 due to our lower bound, and the additional labels coming from the observed data. Denote $Y_i$ the number of labels in an envelope $i$. Denote $X_i$ the number of labels over 90, i.e. $X=Y-90$, so $X\in\{0,1,2,...,10\}$. The binomial distribution models the total number of successes (here a success is the presence of a label in an envelope) in $n$ trials when the trials are independent with constant success probability $p$ so $X$ takes values $0, 1, 2, 3, ..., n.$ We take $n=10$, which gives 11 different possible outcomes. I assume that because the sheet sizes are irregular, some sheets only have room for $X$ additional labels in excess of 90, and that this "additional space" for each label in excess of 90 occurs independently with probability $p$. So $X_i\sim\text{Binomial}(10,p).$ (Upon reflection, the independence assumption/binomial model is probably a strange assumption to make, since it effectively fixes the composition of the printer's sheets to be unimodal, and the data can only change the location of the mode, but the model will never admit a multimodal distribution. For example, under an alternative model, it's possible that the printer only has sheets of sizes 97, 98, 96, 100 and 95: this satisfies all the stated constraints and data doesn't exclude this possibility. It might be more appropriate to regard each sheet size as its own category and then fit a Dirichlet-multinomial model to the data. I do not do this here because the data are so scarce, so posterior probabilities on each of the 11 categories will be very strongly influenced by the prior. On the other hand, by fitting the simpler model we are likewise constricting the kinds of inferences that we can make.) Each envelope $i$ is an iid realization of $X$. The sum of binomial trials with the same success probability $p$ is also binomial, so $\sum_i X_i\sim\text{Binomial}(60,p).$ (This is a theorem -- to verify, use the MGF uniqueness theorem.) I prefer to think about these problems in a Bayesian mode, because you can make direct probability statements about posterior quantities of interest. A typical prior for binomial trials with unknown $p$ is the beta distribution , which is very flexible (varies between 0 and 1, can be symmetric or asymmetric in either direction, uniform or one of two Dirac masses, have an antimode or a mode... It's an amazing tool!). In the absence of data, it seems reasonable to assume uniform probability over $p$. That is, one might expect to see a sheet accommodate 90 labels as often as 91, as often as 92, ..., as often as 100. So our prior is $p\sim\text{Beta}(1,1).$ If you don't think this beta prior is reasonable, the uniform prior can be replaced with another beta prior, and the math won't even increase in difficulty! The posterior distribution on $p$ is $p\sim\text{Beta}(1+43,1+17)$ by the conjugacy properties of this model. This is only an intermediate step, though, because we don't care about $p$ as much as we care about the total number of labels. Forunately, the properties of conjugacy also mean that the posterior predictive distribution of sheets is beta-binomial , with parameters of the beta posterior. There are $940$ reamining "trials", i.e. labels for which their presence in the delivery is uncertain, so our posterior model on the remaining labels $Z$ is $Z\sim\text{BB}(44,18,940).$ Since we have a distribution on $Z$ and a value model per label (the vendor agreed to one dollar per label), we can also infer a probability distribution over the value of the lot. Denote $D$ the total dollar value of the lot. We know that $D=9043+Z$, because $Z$ only models the labels that we are uncertain about. So the distribution over value is given by $D$. What's the appropriate way to consider pricing the lot? We can find that the quantiles at 0.025 and 0.975 (a 95% interval) are 553 and 769, respectively. So the 95% interval on D is $[9596, 9812]$. Your payment falls in that interval. (The distribution on $D$ is not exactly symmetric, so this is not the central 95% interval -- however, the asymmetry is negligible. Anyway, as I elaborate below, I'm not sure that a central 95% interval is even the correct one to consider!) I'm not aware of a quantile function for beta binomial distribution in R, so I wrote my own using R's root-finding. qbetabinom.ab Another way to think about it is just to think about the expectation. If you repeated this process many times, what's the average cost you would pay? We can compute the expectation of $D$ directly. $\mathbb{E}(D)=\mathbb{E}(9043+Z)=\mathbb{E}(Z)+9043.$ The beta binomial model has expectation $\mathbb{E}(Z)=\frac{n\alpha}{\alpha+\beta}=667.0968$, so $\mathbb{E}(D)=9710.097,$ almost exactly what you paid. Your expected loss on the deal was only 6 dollars! All told, well done! But I'm not sure either of these figures is the most relevant. After all, this vendor is trying to cheat you! If I were doing this deal, I'd stop worrying about breaking even or the fair-value price of the lot and start working out the probability that I'm overpaying! The vendor is clearly trying to defraud me, so I'm perfectly within my rights to minimize my losses and not concern myself with the break-even point. In this setting, the highest price I would offer is 9615 dollars, because this is the 5% quantile of the posterior on $D$, i.e. there's 95% probability that I'm underpaying . The vendor can't prove to me that all the labels are there, so I'm going to hedge my bets. (Of course, the fact that the vendor accepted the deal tells us that he has nonnegative real loss... I haven't figured out a way to use that information to help us determine more precisely how much you were cheated, except to note that because he accepted the offer, you were at best breaking even.) Comparison to the bootstrap We only have 6 observations to work with. The justification for the bootstrap is asymptotic, so let's consider what the results look like on our small sample. This plot shows the density of the boostrap simulation. The "bumpy" pattern is an artifact of the small sample size. Including or excluding any one point will have a dramatic effect on the mean, creating this "bunchy" apperance. The Bayesian approach smooths out these clumps and, in my opinion, is a more believable portrait of what's going on. Vertical lines are the 5% quantiles.
