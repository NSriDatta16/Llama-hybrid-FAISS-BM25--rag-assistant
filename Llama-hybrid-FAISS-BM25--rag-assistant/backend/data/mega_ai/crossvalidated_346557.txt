[site]: crossvalidated
[post_id]: 346557
[parent_id]: 
[tags]: 
Explaining poor model performance on an unbalanced test dataset when trained on a balanced one

I built a machine learning two-class classification model on an approximately balanced training set and estimated its performance via cross-validation. Its accuracy is about 70%. My boss asked me to assess the model performance on a new dataset. The new data is unbalanced: only 10% belongs to the positive class. Unsurprisingly, my model performed poorly on this data. For this data, just assigning every case to the negative class will give me 90% accuracy. My boss asked me why my model perform so poorly. I am unable to explain the result well. Can anyone help me explain this? Especially to an audience of business folks.
