[site]: crossvalidated
[post_id]: 50536
[parent_id]: 
[tags]: 
Why is it that a lower R-Squared on a difference regression model could be better than higher R-squared on a levels regression model

I am reviewing a time series regression model that uses the log of the year over year change in sales as the dependent variable and the log of the year over year change in another economic index as the independent variable. The regression model produced an R-squared of .60. When I expressed concern about the low R-squared, I was told it was a very good R-squared given the use of log differences instead of levels and that I can't really compare it to an R-squared derived from using levels. I thought R-squared represented goodness of fit. So why would a regression with a 0.60 R-Squared based on difference be better than a regression with a 0.85 R-sqared based on levels. Thanks for your input.
