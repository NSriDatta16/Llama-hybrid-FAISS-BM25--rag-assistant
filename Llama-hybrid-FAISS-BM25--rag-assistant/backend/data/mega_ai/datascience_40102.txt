[site]: datascience
[post_id]: 40102
[parent_id]: 40087
[tags]: 
Unraveling the causality involved in this type of recommender system is complicated, but tractable. Given that you have a good experimental methodology, and your evaluation numbers are generated on numbers not used for training (among other concerns), it sounds like you have the best estimate that you can get (or are in the neighborhood) using past data. The next step would be to form a new experiment, in which you (ideally) randomly provide the recommendation for one group and not provide it for another group, which otherwise are the same. You then measure the outcomes for both groups and compare them statistically to determine the presence of an effect and its size. Proposing that study is your next step if you're in an organization, or performing it if you have the agency to do it on your own.
