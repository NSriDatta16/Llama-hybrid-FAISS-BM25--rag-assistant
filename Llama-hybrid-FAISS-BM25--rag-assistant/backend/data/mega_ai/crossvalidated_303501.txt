[site]: crossvalidated
[post_id]: 303501
[parent_id]: 303489
[tags]: 
Is this true? Should I group similar-length sequences into the same batch for training? It seems it would be more efficient, but also it seems it may introduce some biases since similar-length sequences may share other similarities (for example, in classification, shorter sequences may be more likely to be a certain class). This is true. Grouping similar-length sequences into the same batch will speed up training (as well as testing), while introducing some bias. What is the best way to select, from a corpus of N sequences of varying lengths, a subset of sequences which are "similar" in size? I was thinking of ordering the sequences by lengths, picking one randomly, and sampling its neighbors somehow. That sounds like a good trade-off between shuffling batches and speed. I'm not aware of any standard solution to this trade-off: from what I have read so far, it's an empirical decision. FYI: Neural networks: why do we randomize the training set? When the data set size is not a multiple of the mini-batch size, should the last mini-batch be smaller, or contain samples from other batches?
