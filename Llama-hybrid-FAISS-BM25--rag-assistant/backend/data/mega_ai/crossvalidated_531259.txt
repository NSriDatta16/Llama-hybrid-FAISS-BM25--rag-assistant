[site]: crossvalidated
[post_id]: 531259
[parent_id]: 
[tags]: 
Why are my cross-validation scores unrealistically high?

I am working on a two-class classification task that involves the classification of samples from 10 subjects who belong to two different classes. The three main variables are: X - feature matrix (1650 x 30). y - target array (1650,) groups - array holding the participant id corresponding to each sample (1650,) My classification pipeline consists of three steps: standardization, feature selection and SVM classification. I have applied grid search cross-validation to find the best hyperparameters and perform cross-validation simultaneously. I have selected leave-one-group-out (LOGO-CV) as cross-validation scheme because the feature matrix contains ordered data from the participants. # Pipeline pipe = Pipeline([('sca', StandardScaler()), ('sel', SelectKBest(f_classif)), ('clf', SVC(kernel = 'rbf', random_state = 100))]) # Parameter grid param_grid = [{'clf__C' : [0.1, 1, 10, 100, 1000], 'clf__gamma' : [1, 0.1, 0.01, 0.001, 0.0001], 'sel__k' : [10, 20]}] # Grid-search cross validation cv = LeaveOneGroupOut() search = GridSearchCV(estimator = pipe, param_grid = param_grid, scoring = 'f1_weighted', return_train_score = True, cv = cv).fit(X, y, groups = groups) The following figure displays the cross-validation scheme (left) and the test and training scores per fold (subject) obtained during cross-validation for the best set of hyperparameters (right). I am very skeptical about the results. First, I noticed the training score was 100% on every fold, so I thought the model was overfitting. Then, I checked the test score and realized it was very high (91% average across folds). As I understand, perfect training score and very high test score occur when a model is trained and evaluated on very similar data, however, in this case I am applying LOGO-CV, so there should not exist a subject bias. Can someone explain this (in my opinion) unrealistic performance?
