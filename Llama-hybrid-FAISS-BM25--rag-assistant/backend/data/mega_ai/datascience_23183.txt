[site]: datascience
[post_id]: 23183
[parent_id]: 
[tags]: 
Why convolutions always use odd-numbers as filter size

If we have a look to 90-99% of the papers published using a CNN (ConvNet). The vast majority of them use filter size of odd numbers :{1, 3, 5, 7} for the most used. This situation can lead to some problem: With these filter sizes, usually the convolution operation is not perfect with a padding of 2 (common padding) and some edges of the input_field get lost in the process... Question1: Why using only odd_numbers for convolutions filter sizes ? Question2: Is it actually a problem to omit a small part of the input_field during the convolution ? Why so/not ?
