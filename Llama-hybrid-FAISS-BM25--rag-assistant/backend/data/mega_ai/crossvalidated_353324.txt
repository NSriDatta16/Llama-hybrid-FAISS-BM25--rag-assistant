[site]: crossvalidated
[post_id]: 353324
[parent_id]: 353317
[tags]: 
Many algorithms struggle with unbalanced classes, and can have their performance improved by artificially balancing the classes, but Logistic Regression isn't one of them. In fact the only thing that is affected in a Logistic Regression by class imbalance is the intercept coefficient. For a 90/10 class imbalance, you should expect to see an intercept coefficient of about 2 (or -2, depending on your sign convention) assuming your other independent variables are centered. Every other term in the model can be interpreted as an odds ratio relative to that base-line probability of 0.1. Logistic Regression is not called the Logistic Classifier after all - and this is not a nomenclature error. It really is a regression with the dependent variable being (a function of) the probability of a class conditioned on all the data you've given it. While in extreme cases of imbalance, say probabilities of 1e-5, we can get into the realm of numeric instability because internal values used to represent such probability as close to what floating point numbers can accurately express, and then the gradients (used internally when fitting a model) are several orders of magnitude smaller than that, and the numeric error can accumulate. It's still a convex optimization problem and therefore reasonably robust against these kinds of problems, but when it starts to deal with extremes any algorithm is going to struggle. But my point is that your 90/10 imbalance is still very far from the realm where any of this starts to be a problem. The bottom line is, if in the real world the chance of booking of only 10%, you can and should train logistic regression on a training set that has only 10% successes: it will return the favor by giving you the best unbiased probability estimates that it can for the model you've chosen. You definitely should be using stratified sampling when you do your test/train split though. This ensures that class proportion of both is the same as the population class proportion. Also, after your model is fitted and your using it to predict class labels, don't make the mistake of using the default threshold of 0.5. Remember: Logistic Regression emits probability estimates. It's entirely possible that given your data, the conditional probability of a booking never rises to 0.5 and the best your logistic regression model can ever do given the data that its seen is report a 0.2 or 0.3 chance of booking. This still represents a 2:1 or 3:1 lift over baseline, which may be a very strong model in certain business contexts.
