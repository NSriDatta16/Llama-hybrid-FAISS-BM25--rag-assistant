[site]: crossvalidated
[post_id]: 251641
[parent_id]: 
[tags]: 
Explaining feature importance to non technical persons

I am collaborating with a team of biologists who are trying to classify some datasets based on lab experiments and observations. The dataset is interesting for a binary classification, I compared different algorithms on the dataset and was able to achieve a good classification after some data cleanup and normalization. The question I do have is that despite the process of feature selection and the feature importance I generated, the team still need to see cutoffs on the feature values that switch the outcome to lets say A or B class, and this is fair enough, only the classic decision tree algorithm will be able to output such a visualization as far as I can tell, but other algorithms perform much better than a simple decision tree and visualization of such "cutoff" is not something I can think of. In your experience, is there a way of representing feature importance differently in the sense "cutoff" that determines which class will be predicted before or after this cutoff I am aware that this is a univariate way of looking at the data, but I am trying to approximate things for a general public and not for machine learning practitioners
