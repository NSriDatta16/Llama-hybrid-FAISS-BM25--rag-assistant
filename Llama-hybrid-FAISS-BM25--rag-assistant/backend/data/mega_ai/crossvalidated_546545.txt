[site]: crossvalidated
[post_id]: 546545
[parent_id]: 
[tags]: 
Derive the criterion for minimizing the expected loss when there is a general loss matrix and general prior probabilities for the classes

In the book "Pattern Recognition and Machine Learning" I am trying to do exercise 1.23 (p.63): Derive the criterion for minimizing the expected loss when there is a general loss matrix and general prior probabilities for the classes. On p.42 it states that (given a loss matrix $L$ which entries $L_{kj}$ denote the loss of assigning an observation $x$ to class $j$ when in reality it's in class $k$ ) we need to assign an observation $x$ to the class that minimizes the quantity $$\sum_kL_{kj}p(C_k|x)$$ So to answer the question: We have that $p(C_k|x)=\frac{p(x|C_k)p(C_k)}{p(x)}$ , we can express the quanity we want to minimize in terms of $p(C_k)$ : $$\sum_kL_{kj}p(C_k|x)=\sum_kL_{kj}\frac{p(x|C_k)p(C_k)}{p(x)}\propto \sum_kL_{kj}p(x|C_k)p(C_k)$$ So we assign an observation $x$ to $j$ for which $\sum_kL_{kj}p(x|C_k)p(C_k)$ is minimized. Is this what the question is asking for?
