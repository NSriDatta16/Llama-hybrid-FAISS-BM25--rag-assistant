[site]: crossvalidated
[post_id]: 625273
[parent_id]: 623886
[tags]: 
A language model is a probability distribution over strings. These strings are sequences of tokens from some vocabulary $\Sigma$ plus a special end-of-sequence symbol $\textrm{EOS}$ . Every string $\mathbf{x}=w_1w_2\ldots w_n \textrm{EOS}$ has a probability that is expressible with the chain rule: \begin{equation} p(\mathbf{x}) = p(w_1)\cdot p(w_2 \mid w_1) \cdot \ldots \cdot p(\textrm{EOS} \mid w_1w_2\ldots w_n) \text{.} \end{equation} You've said that you have a finite set of movie titles, so you have a finite set of strings $\mathcal{X} \subset \Sigma^*$ . In this circumstance, you can renormalize the probability over that finite set: \begin{equation} p'(\mathbf{x}) = \frac{p(\mathbf{x})}{\sum_{\mathbf{x'} \in \mathcal{X}} p(\mathbf{x}')} \text{.} \end{equation} Now that we have a distribution over your set of movie titles, it's straightforward to compute the cross-entropy using the standard formula.
