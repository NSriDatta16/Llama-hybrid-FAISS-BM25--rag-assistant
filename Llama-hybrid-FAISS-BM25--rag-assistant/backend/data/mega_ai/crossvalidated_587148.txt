[site]: crossvalidated
[post_id]: 587148
[parent_id]: 
[tags]: 
Why does the masked language modelling (MLM) task produce useful embeddings?

Masked language modelling is the standard way of training a language model such as a transformer. Each input token has some probability (e.g. 15%) of being replaced with a token. The model must predict the original input token at these positions. There are various variants of this task, such as sometimes replacing the token with a random token or the original token instead of , but the original BERT paper (in the ablation studies in the appendix) found that the simple approach of always replacing with works well. My question is: why does this task produce useful embeddings for non- tokens? This is the main use of such models, when we use their embeddings for downstream tasks. When the token is replaced with , it makes sense to me that the model learns how to use the surrounding tokens to create a contextual representation of what the token is likely to be. But when the token is not , the model should know that this is the correct token, and might as well just leave the token alone, yielding a perfect "prediction". In fact, for non- tokens, there was no loss during training, so the model had no incentive to do anything in particular for these tokens. So why does it produce good embeddings?
