[site]: crossvalidated
[post_id]: 409244
[parent_id]: 408811
[tags]: 
He is referring to a problem where you have a one output, a sigmoid neuron. When you initialize the weights of the network you can set bias to approximately $-2.3$ . Why? The last layers bias is going to be dependant only on the statistics of the data, since it doesnt have any connection to the input. Meaning that if you have an imbalanced dataset, where you have 10 times as much negative examples as positive, and the network would consist only of this bias, then it would output on average the number 1/11. Output of 1/11 corresponds to input to sigmoid layer equal to $-log(10)$ , which is $-2.3$ And as a comment, the unbolded sentence is bullshit. It's much better to normalize the targets in the data. Don't take someone's word for granted just because they are a big name in the field.
