[site]: crossvalidated
[post_id]: 328704
[parent_id]: 328702
[tags]: 
No one actually computes the dot product explicitly, since with a one-hot encoding you just need to read off the corresponding row/column of the matrix you're multiplying with (which takes constant time and is very fast). Otherwise it's prohibitively expensive and extremely slow. If your vocabulary size is very large (i.e. millions), then usually people use word-embeddings like word2vec, Glove, etc as inputs for RNNs and other models. However, to train these embeddings, you do one-hot encoding for your words. In word2vec, you randomly sample word/context pairs, and you never actually multiply the one-hot encoding as explained above. You hash your word vectors, so after selecting a pair of words, you look up their vectors, and then do a gradient descent step. So the only dot products you end up computing are with respect to the in/out embeddings of your words.
