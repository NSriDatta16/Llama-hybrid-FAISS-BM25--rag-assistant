[site]: crossvalidated
[post_id]: 136186
[parent_id]: 136182
[tags]: 
I expect that there would be some difference in the training and CV AUC scores, but should this much of a difference be of concern? If not, how should I interpret and report these results? If it is of concern, what are some possible reasons for the differences and strategies I can take to fix them? You are overfitting the training data. The stark decrease in AUC shows that given new data, your model would likely not perform as well as it does for the training data. The data are ordered by date, but I permutated the data row-wise before using gbm.fixed and predict.gbm. Also, from what I understand gbm.step also randomized the data. Structured dependency in the data is something that you should try to capture in the model. If date/time is important then you should find a way to include it. Admittedly this is ignored by most machine learning algorithms. Could I have too few observations or too many variables? Could overfitting be an issue? Yes, your results are the definition of overfitting. All of the individual animals are pooled together, could differences in the preferences of each individual animal be contributing. It is possible. Another consideration for model development. Could the number CV folds in the gbm.step or gbm. simplify be at play? Yes, read about the bias-variance trade-off.
