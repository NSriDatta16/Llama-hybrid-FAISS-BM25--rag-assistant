[site]: crossvalidated
[post_id]: 594758
[parent_id]: 476911
[tags]: 
I know this is a late reply and you probably do not need this answer, however I believe that I can add valuable information for future Pattern Recognition and Machine Learning by Christopher Bishop readers. Answers from Match Maker EE and Dikran Marsupial provide good explanation on the logic behind the formula of the rebalancing of the classifiers, so I won't go into details on the logic of the formula. I will instead provide explanation on what the author was intending to convey since that was your question. The author was explaining three ways predicting input x into its correct classifier. One of the ways was to get the different conditional posterior class probabilities. The advantage of solving for the posterior class probability is now we are able to apply balancing on our data set using (from Match Maker EE's answer): $$P′(class=j∣x)=\frac{\frac{P′(class=j)}{P(class=j)}P(class=j∣x)}{\frac{P′(class=j)}{P(class=j)}P(class=j∣x)+\frac{P′(class≠j)}{P(class≠j)}P(class≠j∣x)}$$ Since P(class = j |x), P(class = j) and P'(class = j) are all known. P(class = j |x) was solved originally. P(class = j) is the fraction of the original data set that lies in class j. P'(class = j) is the balancing that we want to apply on the data set. In short, the author wanted to convey the advantage of getting the different conditional posterior class probabilities. Hope this is helpful.
