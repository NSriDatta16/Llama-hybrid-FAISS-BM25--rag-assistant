[site]: datascience
[post_id]: 117985
[parent_id]: 117815
[tags]: 
Interesting problem. The 99 x 3000 = 297,000 possible classes (if I understood you correctly) makes this tricky with normal deep learning approaches: the softmax layer at the end gets very large. If you want to go down that road, Amazon recommender systems have this issue, when trying to suggest which of their millions of project; see https://pytorch.org/blog/introducing-torchrec/ as a good starting point. You could also treat it as a fraud detection machine learning problem (google for those keywords will provide plenty of leads). I'd do a variation on that idea, where a few typos are considered to be fine, but anything more is "fraud". I am going to assume you have an ideal string for each category. So "Hiring of Vehicles" is the desired string for 110000032. (If not, an SQL query can give you the most common value, can't it.) For each category, calculate the edit distance from the ideal value. And write that back in as a new column in your database. (It looks like this can be done from inside Postgres; notice that levenshtein_less_equal is a more efficient version that is ideal for you, as you don't need to know the actual distance when it is high, you only care about if it needs a lot of edits or not.) You can now define false entries as any over a certain edit distance, all from within the comfort of SQL. Those below an edit distance of say 3 could be UPDATEd, if you like your data clean. Do you also need to automate fixing the remaining ones? The obvious way is to take each bad entry, and get edit distance against the ideal string for each category. That is potentially very slow, but there is a low-hanging fruit speed-up: try just the adjacent digits, and work out from there. So for each bad entry for 110000032, first try against 110000033 and 110000031. As soon as you a low edit distance, assume that was the typo and move it there. I'm betting you only need to try the other 9 last digits, to fix 90% of your bad entries.
