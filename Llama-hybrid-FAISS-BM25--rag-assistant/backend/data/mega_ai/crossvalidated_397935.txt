[site]: crossvalidated
[post_id]: 397935
[parent_id]: 396167
[tags]: 
The OP has done a great job exploring a variety of different techniques. As commented, given that the response variable is Gamma-distributed it makes sense to consider a GLM and/or a GAM for Gamma distributed variables. Particularly for the use of GAMs, if the computation burden appears too much we might want to consider restricted the basis functions used by the GAM (in the case of pyGAM used here that being achieved by setting s(..., n_splines=X) where X is something smaller than the default $20$ . The main point to rectify is the use of evaluating the error of each method. Simple random resampling by cross-validation is providing us an indication on "interpolation" rather than "extrapolation" performance. Here, given $x_e = \{1, \dots, 10\}$ , we focus on predicting $x_e =0$ ; therefore it is more reasonable to use instances where $x_e = 1$ in our validation set and instances where $x_e =\{2, \dots, 10\}$ in our training set. Note that in-sample errors are rather misleading for a extrapolation task; there is no "overfitting" perse because the validation and training set do not refer to the same sample/population. On that matter, the fact we get simple models (Elastic Net regression and Bayesian Ridge regression) as our top-performing routines is not too surprising. When extrapolating most bets are off (e.g. see the CV thread: What is wrong with extrapolation? ) and commonly simple methods outperform complex ones (e.g. see the CV thread Best method for short time-series ). As a final note, it is always prudent to get estimates of the variability of our performance metric. If possible we should set aside a number of observations, fit our candidate models to the remaining data, and evaluate the models in the data we set aside. This should be repeated multiple times. In effect, what is described is nested cross-validation for model selection; only particular will be that the for each loop the hold-out set is such that $x_e = 1$ . (Once again) CV has a great thread on the matter: Nested cross validation for model selection and Model selection and cross-validation: The right way . In short, the outer loop will be used to assess the performance of the particular model (e.g. Ridge regression), and the inner loop will be used to select the best model (the regularisation parameter $\lambda$ for the case of Ridge). A simplified and very succinct Python example of nested CV would be as follows: myS = cross_val_score(GridSearchCV(linear_model.ElasticNet(), param_grid, cv=5), myX, myY, cv=5) print("CV scores: ", myS) print("Mean CV scores & Std. Dev.: {:.3f} {:.3f}".format(myS.mean(), myS.std()))
