[site]: stackoverflow
[post_id]: 692073
[parent_id]: 
[tags]: 
Is any group or foundation developing an algorithm for better storing massive amounts of data?

I've looked at several approaches to enterprise architecture for databases that store massive amounts of data, and it usually comes down to more hardware, database sharding, and storing JSON objects. Has any group been doing research, or does anyone have a more dynamic approach that processes the available data and tells you how to better store it, and then instructs you how to retrieve it given the new method of storage? I know it sounds a bit fanciful, but I figured I would ask anyway.
