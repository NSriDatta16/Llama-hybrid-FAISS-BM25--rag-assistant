[site]: datascience
[post_id]: 16719
[parent_id]: 16717
[tags]: 
There's more than one type of generative network. However, I am not aware of a generic approach that can take a trained RNN-based network and essentially run it backwards to sample an input that is expected to produce a given output. So I am suggesting a couple of generative approaches that I have seen working, but that will require that you construct and train a new network. You can bring in some knowledge about the typical size of network that learns the regression model, but you cannot AFAIK directly re-use the regression model and somehow reverse it. A caveat: Although I have played briefly with both types of generative network, I have never constructed one conditioned on desired goal like the one you want to work with. With a purely RNN-based approach, you might do OK by making your network predict the next letter/word in the title - a classifier - whilst taking the title so far (or X characters/words of it) and the rating (normalised) as inputs. Then, once trained, you can sample from the RNN randomly to generate new strings. This is the technique used by e.g. Karpathy in his now famous blog " The Unreasonable Effectiveness of Recurrent Neural Networks ". There are many examples of such sequence sampling generators available to study. You could also take the output of such a model and see if it matches your regression model from earlier. But I don't think there is much you can do if it does not, except perhaps filter out generated titles if they don't meet expectations - e.g. generate many titles with intended rating of 10, and only display one when your initial model also agrees with a close to 10 rating. The most relevant Keras example for this I could find is lstm_text_generation.py A RNN-based generative adversarial network (GAN) might also be able to achieve what you want. However, please note that GANs are notoriously fiddly to train. A GAN is actually 2 networks. You create a discriminator and a generator and train them in parallel. The generator takes a small completely random vector (e.g. 10 numbers sampled from N(0,1)), plus the rating you want to achieve. Then it generates a text sequence output. The discriminator would take a text input, and a rating, and outputs 1 if it is real, or 0 if it is fake. You present either real training data or output form the generator to the discriminator, and use these to train it. You train the generator based on whether it fools the discriminator. The tricky part is to maintain balance between the two components - if either becomes too good relative to the other, training will stall. However, if you can get it to work, you will have a true generative model, which samples from a population space (the noise vector that you input) plus is conditioned on the rating. The most relevant Keras example for this I could find is mnist_acgan.py which generates an image, not text sequence, but hopefully should give a start.
