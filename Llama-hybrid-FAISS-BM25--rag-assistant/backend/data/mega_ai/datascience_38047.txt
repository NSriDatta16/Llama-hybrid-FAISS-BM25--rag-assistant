[site]: datascience
[post_id]: 38047
[parent_id]: 38022
[tags]: 
Word embeddings are generally used as input features, which like you noticed for image based models, do not get modified during training. It is in fact quite difficult to update embeddings during training (or at all after they have been computed!), because they are often in some latent space that holds information regarding their relationships to one another. It is for this reason that adding new vocabulary to existing embeddings is extremely challenging, and updating computed embeddings is also difficult without using all the data that was originally used to create them. I am afraid I don't have an example I can point to where embedding are used during training and simultaneously updated/improved upon. I can imagine that there are ways to store them, however, such that it is technically feasible (although perhaps memory intensive!)
