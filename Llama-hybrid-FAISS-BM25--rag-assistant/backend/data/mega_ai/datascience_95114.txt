[site]: datascience
[post_id]: 95114
[parent_id]: 95105
[tags]: 
One hot encoder is your best choice. Still, you have to deal with enlarged dimensional size, though, as long as you don't drop categories you are not giving preferences. Dropping is needed to avoid collinearity which is a fancy way to say: "I have four friends, Anne, Bart, Carrie and Dylan. One of them is here with me. It is not Anne, it is not Carrie and it is not Dylan." Collinearity would be created if I would blatantly say that "it is Bart". Some models cannot handle collinearity. A way to reduce factors and have a meaningful dropping would be to reduce the dimensions needed for your model to a minimum by using approaches as PCA. If you have a linear regression to perform, you can use L1, L2 or both (also called Ridge, Lasso and elastic) regression. Though, you will have to find a metric that informs you when enough dimensions are taken into account.
