[site]: datascience
[post_id]: 61888
[parent_id]: 
[tags]: 
Cross-fold validation done on whole dataset or training set?

I have a dataset of 77 samples with 302 features with two labels (0,1). I trained an SVM with gridsearch (cv=5) to perform binary classification. In one run of my script, I do a test-train split, and then pass the training data into gridsearch. I am getting wide range of training accuracy (>70-90%) and test accuracy (40-75%). My question, is how do I evaluate my model if it appears that based on the train/since it seems like my data is getting different results based on the splitting. What I've done is: run the entire scrip in a loop and iterate over 10 different splits and then plot the error bars on the ROC curve. Is this reasonable?
