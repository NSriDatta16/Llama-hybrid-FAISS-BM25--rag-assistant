[site]: datascience
[post_id]: 126495
[parent_id]: 126453
[tags]: 
Your description sounds a bit more like AdaBoost than Gradient Boosting (XGBoost and others). AdaBoost trains its weak estimators on the original targets, just weighting/sampling the observations according to misclassification. In contrast, Gradient Boosting trains each weak learner on different targets , namely the "pseudo-residuals", i.e. the gradient (wrt the observations) of the objective function. (That has a similar effect, because misclassified observations will tend to have higher pseudoresiduals.) See also: Adaboost vs Gradient Boosting CV: Is Gradiant Boosting a generalization of Adaboost? CV: Intuitive explanations of differences between Gradient Boosting Trees (GBM) & Adaboost
