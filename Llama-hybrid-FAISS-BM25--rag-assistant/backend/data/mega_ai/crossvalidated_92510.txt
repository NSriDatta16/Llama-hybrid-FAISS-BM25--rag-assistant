[site]: crossvalidated
[post_id]: 92510
[parent_id]: 92500
[tags]: 
The output of an SVM solver are the coefficients in the resulting model. The decision function in an SVM for a test instance $\mathbf{z}$ is as follows: $$ f(\mathbf{z}) = \mathbf{w}^T\phi(\mathbf{z}) + b, \\ $$ where $\mathbf{w}$ is the separating hyperplane, $\phi(\cdot)$ is the embedding function and $b$ is the bias term. A binary label is assigned based on the sign of $f(\mathbf{z})$. Note that this formulation contains the inner product between the separating hyperplane $\mathbf{w}$ in feature space and the feature-space representation of the test instance $\mathbf{z}$. For some kernels, the embedding function is unknown or leads to an infinite dimensional vector (such as the Gaussian kernel). Due to the representer theorem and the kernel trick, this can be rewritten as follows: $$ f(\mathbf{z}) = \sum_{i\in \mathcal{S}} \alpha_i y_i \kappa(\mathbf{x_i}, \mathbf{z}) + b, $$ with $\alpha$ the vector of support values, $\mathbf{y}$ the vector of training labels, $\mathcal{S}$ the set of support vectors and $\kappa(\mathbf{x}_i,\mathbf{x}_j) = \phi(\mathbf{x}_i)^T\phi(\mathbf{x}_j)$ the kernel function. An SVM model is entirely defined by the coefficients $\alpha$ and $b$ as solution of the training problem and the support vectors with their associated labels.
