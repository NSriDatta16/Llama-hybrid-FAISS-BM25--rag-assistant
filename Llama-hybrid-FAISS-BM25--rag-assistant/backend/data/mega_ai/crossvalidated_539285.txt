[site]: crossvalidated
[post_id]: 539285
[parent_id]: 
[tags]: 
What can i use to fix SVM overfitting when all expected solutions have failed?

I am trying to predict if a certain day will be good for agriculture based off a select number of features, the data has an imbalance of 5:1. There's a total of 1794 samples and 15 variables, columns are named "col_1" to "col_15". I have tried scaling my data, adding PCA before and after scaling and feature selection based off XGBoost's feature importance. Lowering the C parameter did reduce the overfit but it also lowered my overall test score, so i decided to keep it at 1. The variables also have low correlation between each other, ranging from -0.55 to 0.4. There are a total of 1794 samples and I'm using repeated stratified kfold as the cross validation and f1 as the evaluation metric. What can i do to decrease the overfit without also decreasing the test score? code: #drop columns previously selected by recursive feature elimination X = df.drop(['target','id','col_5','col_9','col_14'], axis = 1) y = df['target'] svc = SVC(C=1,gamma=.1) sc = StandardScaler() over = SMOTE(sampling_strategy = .67) steps = [('pca', PCA(n_components = 10)), ('scaler', sc), ('over', over), ('model', svc)] pipe = Pipeline(steps = steps) cv = RepeatedStratifiedKFold() score = cross_validate(pipe, X, y, scoring = 'f1', cv = cv, return_train_score = True) print('train:') print('mean:',np.mean(scores['train_score'])*100) print('std:',np.std(scores['train_score'])*100) print('test:') print('mean:',np.mean(scores['test_score'])*100) print('std:',np.std(scores['test_score'])*100) print('difference between means:',np.mean(scores['train_score'])*100 - np.mean(scores['test_score'])*100)``` evaluation results: [1]: https://i.stack.imgur.com/1JIBb.png
