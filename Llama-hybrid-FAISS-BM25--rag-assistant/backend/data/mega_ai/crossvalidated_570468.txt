[site]: crossvalidated
[post_id]: 570468
[parent_id]: 
[tags]: 
How to implement Rubin's Rules to assess model fit on imputed test data with continuous outcome? (e.g. RMSE and 95% CI)

I'm working on a project now which involves the use of multiple imputation while developing machine learning models (using a training/test split, ~7000 observations total) for a continuous outcome. I am aware of Rubin's Rules as a method of combining metrics from multiple models developed on different imputed datasets. However, now that the time has come to implement these rules, I've confused myself a bit. One performance metric being considered here is RMSE. With 5 imputed datasets, should I compute each of the 5 component RMSEs (1 for each imputed dataset), then take the average of these as a final, summary RMSE? Would this also be the case for something like the AIC/BIC/R^2? Further, I've really confused myself trying to think about how to generate a confidence interval for the RMSE (or to find some method to quantify uncertainty of the model fit, if this is meaningful). My thought is to use a bootstrap approach to assess the pooled RMSE in 10,000 simulated examples and construct a CI based on percentiles. I can understand the implementation of this approach, but I'm not sure if this is really the best approach (for example, if certain assumptions/criteria must be met for this to be a reasonable method). If any additional details would be helpful in answering this question, I am happy to provide them. The combination of multiple imputation and train/test splitting in particular has made me question whether I am using a correct approach to assess model fit in a meaningful and valid way.
