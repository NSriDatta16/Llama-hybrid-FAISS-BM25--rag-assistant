[site]: crossvalidated
[post_id]: 349475
[parent_id]: 349464
[tags]: 
The general idea of a Ridge penalty is to add L2-norm penalty to the optimization problem. In a regression context, it "raises" the solution above the minimum had you never introduced regularization, and in this solution some of the coefficients are shrunk towards zero - so essentially you reduce variance in exchange for some bias (because your solution is above the supposedly unbiased one). To see why some coefficients are shrunk towards zero it is best to look at the Ridge solution geometrically (in a 2-dimensional space for simplicity). The Ridge regression is trying to minimize the penalized sum of squares: \begin{equation*} \sum_{i=1}^n (y_i - \sum_{j=1}^p x_{ij}\beta_j)^2 + \lambda \sum_{j=1}^p \beta_j^2 \end{equation*} Which is equivalent to the minimization of $\sum_{i=1}^n (y_i - \sum_{j=1}^p x_{ij}\beta_j)^2$ subject to, for some $c>0$, $\sum_{j=1}^p \beta_j^2 Geometrically, in a 2-dimensional space it looks like the following: Where $\hat{\beta}$ is the original OLS solution and $\beta_R$ is the Ridge solution. The ellipses in the north-eastern corner indicate RSS (in your case, the error). Each ellipse represents a higher level of RSS going from the "minimum", which is the OLS solution, and to south-west. The circle around the origin represents the Ridge constraint. Now, in the image you can see that $\beta_2$ is a "large" coefficient and $\beta_1$ is somewhat smaller. When the RSS ellipses touch the "circle" constraint a solution has been reached. But look at $\beta_1$ and $\beta_2$ at the Ridge solution - both of them are smaller compared to the OLS solution (got "shrunk" toward zero), and specifically - the "large" coefficient which is $\beta_2$ got shrunk more than the smaller one, which is $\beta_1$. This shows you that Ridge will tend to shrink larger coefficients more than smaller ones (this is why it is important to standardize the coefficients beforehand, but this is out of the question's scope). In the Neural Network case it is a similar concept, only difference is that you have also $\alpha$s in addition to the $\beta$s, and the RSS is the error function $R(\theta)$. Hope it helps!
