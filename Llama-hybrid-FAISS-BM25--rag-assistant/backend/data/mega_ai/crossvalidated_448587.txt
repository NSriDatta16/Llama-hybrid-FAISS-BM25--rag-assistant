[site]: crossvalidated
[post_id]: 448587
[parent_id]: 448487
[tags]: 
Unfortunately the implementation of random forests between Python and R are not always directly comparable. If anything, because the random forest algorithm inherently performs bagging and random selection of explanatory variables (i.e. it samples both the rows and the columns of our training set when training), if this resampling is not done in the same way it can lead to slightly different results. Transferring random seeds between R and Python is not straightforward. I think that neither implementation is "better". If I was forced to choose between the two in this case, I would simply pick the one with the small CV error; that way I would be able to point out that a fair experiment irrespective of implementation choice has been made and the "most performant" algorithm was picked. An excellent overview of different cross-validation procedures can be found in Arlot & Celisse (2010) A survey of cross-validation procedures for model selection , I would suggest looking at it carefully. Finally yes, the whole dataset should be used when training our final model. That way we can get the maximum insights from our available data. Notice that this concerns "using the model" to production (or in this case reporting the model weights), rather than "comparing the model" against another one (in which case the CV (or resampling) is more relevant). In case you are unaware of it you might find the R package randomForestExplainer , it offers some excellent visualisation functions for RF in terms of variable importances. I strongly suspect similar packages exist for Python but I have not come across them personally.
