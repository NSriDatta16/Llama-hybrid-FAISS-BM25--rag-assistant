[site]: datascience
[post_id]: 82786
[parent_id]: 82371
[tags]: 
In a full gradient descent step, the loss function is defined as the average of the loss term at individual sample points. To minimize the loss function, we need to average over the individual gradients. In the stochastic gradient descent, if there is no bias in selecting the batches, the averaging over the batches would result in a unbiased estimate of the full gradient. Please take a look at this lecture notes http://www.stat.cmu.edu/~ryantibs/convexopt-F18/scribes/Lecture_24.pdf
