[site]: crossvalidated
[post_id]: 419219
[parent_id]: 419199
[tags]: 
I would start with the most simple possible solution, and only if that doesn't work I'd start to look into more complicated options. The first place to start when you wish to reduce the number of variables in your data is principal components analysis (PCA). This looks for new features that are linear combinations of your old features, and summarize your dataset, minimizing the information lost in the process. If your data is sparse (you said that the ICD-10 codes are just ones and zeros), the chances are that just a few PCA features (called principal components ) will summarize your dataset well. PCA is implemented out-of-the box in scikit-learn. You can import it with sklearn.decomposition.PCA , train it with pca = PCA(n_components=q).fit(X_train) and use it on your train or test data with pca.transform(X_train_or_test) . I would keep the rest of your features, run PCA just on the ICD-10 codes and find a few tens or hundreds of principal components for the ICD-10 codes. In scikit-learn, you can choose the number of principal components by looking at the pca.explained_variance_ratio_ . An accumulated explained variance ratio of 0 explains nothing of your dataset, and a ratio of 1 explains everything. Good luck! ( Gist of PCA ) If $\mathbf{X}$ is your data matrix (called the design matrix , with $N$ samples along the row dimension and $p$ features along the column dimension), PCA finds the $q$ principal components $\mathbf{v}$ , $q \leq p$ , that most minimize the difference between $\mathbf{X}$ and $\mathbf{Xvv}^\text{T}$ . $$ \DeclareMathOperator*{\argmin}{arg\,min} \mathbf{v^*} = \argmin_\mathbf{v} \;\;\lVert \mathbf{X} - \mathbf{Xvv}^\text{T} \rVert^2, $$ where $\lVert \cdot \rVert$ is the Frobenius norm (essentially the same as the $L2$ norm but for matrices). Notice that if $\mathbf{v}$ is an unit vector that specifies a direction, $\mathbf{Xvv}^\text{T}$ is simply the reconstruction of $\mathbf{X}$ along that direction. So PCA is finding the directions with the most information! ( End of gist of PCA )
