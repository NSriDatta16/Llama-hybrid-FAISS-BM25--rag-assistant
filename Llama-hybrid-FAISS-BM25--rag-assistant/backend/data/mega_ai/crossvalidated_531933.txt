[site]: crossvalidated
[post_id]: 531933
[parent_id]: 530850
[tags]: 
First of all, if you know that for one category the estimated probability is 0.9, and for the two others it is 0.05, then assuming that they all are uniform is completely contradictory to your data. Worse, you give those results the highest (0.7) weight among all the results, so they will overwhelm the result. It would probably be wiser to just ignore those results, rather than replacing them with arbitrary values. Since you didn't give us details on why you have missing data in the estimated probabilities, let me make an educated guess. I assume that you have several models, trained on different data sets (or re-samples of the same data), which resulted in not having all the classes represented equally in the training sets. As a result, some of the categories were not predicted by the models, which is equivalent to models assigning probabilities equal to zero for the categories. What can be done to prevent such cases is using a Bayesian approach, where using a non-zero prior, would lead to non-zero posterior probabilities. A simple example of such an approach is using Laplace smoothing for preprocessing the data for the naive Bayes model. You can also use it to smooth the predicted probabilities $$ \tilde p_i = \frac{p_i + \alpha}{\sum_j p_j +\alpha} $$ where $\alpha \in (0, 1)$ is a small value. If those were counts, it would be a pseudo-count. To transform probability into a count you would take $Np_i = n_i$ , so the pseudo-count would be $N\alpha$ . This would help you get rid of the zero probabilities and replace them with small, non-zero probabilities that may be more reasonable. As a side note, using weighted average is not the only way to combine the probabilities as you can learn from the Combining probabilities/information from different sources thread.
