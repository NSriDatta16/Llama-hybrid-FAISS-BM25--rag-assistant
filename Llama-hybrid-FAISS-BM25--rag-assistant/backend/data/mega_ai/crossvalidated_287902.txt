[site]: crossvalidated
[post_id]: 287902
[parent_id]: 287881
[tags]: 
Let's define $\hat{y}_0^f, \ldots, \hat{y}_M^f$ as the forward predictor you built. That is, $\hat{y}^f_0$ is your estimate for $y_{s + 0}$ based on values before $s + 0$, $\hat{y}_1^f$ is your estimate for $y_{s + 1}$ based on values before $s + 1$, and so on. It stands to reason that the MSE of $\hat{y}_i^f$ is increasing in $i$ (that is, the error grows as you try to predict farther into the future). You could also go the other way, and build a backward predictor. That is, $\hat{y}^b_0$ is your estimate for $y_{s + M - 0}$ based on values after $s + M - 0$, $\hat{y}_1^b$ is your estimate for $y_{s + M - 1}$ based on values after $s + M - 1$, and so on. Since the forward predictor will probably be better for values at time close to $s$, the backward predictor will probably be better for values at time close to $s + M$, and you want continuity, then one way to go would be to use the weighted average $$ \hat{y}_i^c = \frac{w_i^f \hat{y}_i^f + w_{M - i}^b \hat{y}_{M - i}^b}{w_i^f + w_{M - i }^b}. $$ where $w_i^f$, $w_{M - i }^b$ are the estimations of the MSE of the forward and backward predictor MSEs. It's possible to assess them from the rest of the data. Alternatively, you could guess that $$ w_i^f \sim i^2 \\ w_{M - i}^b \sim \left( M - i \right)^2 , $$ because of the variance of a random walk behaves this way.
