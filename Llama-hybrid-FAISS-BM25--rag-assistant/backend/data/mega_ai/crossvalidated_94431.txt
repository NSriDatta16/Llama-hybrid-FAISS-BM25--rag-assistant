[site]: crossvalidated
[post_id]: 94431
[parent_id]: 94430
[tags]: 
You probably have very few positives compared to negatives. This is called an imbalanced setting. In this case, predicting everything as negative will yield high accuracy even though it is useless. If you indeed have an imbalanced data set, you can improve your models by assigning a higher misclassification to positive instances during training. In libsvm you can do this using the -w flag. A good heuristic to assign class weights is to use the following equation: $$C_{pos} \times n_{pos} = C_{neg} \times n_{neg}.$$ Ex.: if you have 1 positive and 100 negatives, $C_{pos}$ should be $100 C_{neg}$.
