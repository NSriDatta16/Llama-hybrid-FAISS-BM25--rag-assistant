[site]: stackoverflow
[post_id]: 566079
[parent_id]: 566014
[tags]: 
don't waste time trying to find 'expensive' operations. there's almost none in C, besides libraries, of course. instead, try to estimate how many times you're executing each part of your code. for example, say you're comparing each line of a file, with each line of another one. if each file has around a hundredth lines, you'll do around ten thousand comparisons. nothing to worry... but if you pick each line counting from the beginning of the file, you'll read each line half a million times. now that's no good. you'll need some really random-access way to read each line.... or, even better, read about hashing in 'big O' notation: a full set comparison is O(n x m) , or roughly O(n^2) if n and m are similar. but sequentially reading is O(n/2) on average, so the whole thing is O(n^3/2) on reading plus O(n^2) on comparing. with hashing it would be aO(2n)+bO(2n)+cO(n^2) , or just O(n^2) optimise algorithms, not code.
