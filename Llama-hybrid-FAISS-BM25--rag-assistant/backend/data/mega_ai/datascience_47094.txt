[site]: datascience
[post_id]: 47094
[parent_id]: 
[tags]: 
Why does the classic Neural Network perform better than LSTM in Sentiment Analysis

My goal is to predict the polarity of some reviews (negative, positive or neutral). I tried two different neural networks: left_branch = Input((7000, )) left_branch_dense = Dense(512, activation = 'relu')(left_branch) right_branch = Input((14012, )) right_branch_dense = Dense(512, activation = 'relu')(right_branch) merged = Concatenate()([left_branch_dense, right_branch_dense]) output_layer = Dense(3, activation = 'softmax')(merged) model = Model(inputs=[left_branch, right_branch], outputs=output_layer) model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) model.fit([np.array(review_matrix), np.array(X_train)], labels,epochs=2, verbose=1) model.save('model.merged') ############################################################################# ############################################################################# #We will try to merge two different models in a different way: Accuracy: 70 # Prepare the review column for embedding: review_matrix_for_embedding = prepare_for_encoding(train_set[4].tolist(), 7000) # Shape: (1503,100) second_matrix = np.array(pd.concat([onehot_category, aspect_matrix],axis=1)) left_branch = Input(shape=(100,), dtype='int32') # input_dim: Size of maximum integer (7001 here); output dim: Size of embedded vector; # input_length: Size of the array left_branch_embedding = Embedding(7000, 300, input_length=100)(left_branch) lstm_out = LSTM(256)(left_branch_embedding) lstm_out = Dropout(0.7)(lstm_out) lstm_out = Dense(128, activation='sigmoid')(lstm_out) right_branch = Input((7012, )) merged = Concatenate()([lstm_out, right_branch]) output_layer = Dense(3, activation = 'softmax')(merged) model = Model(inputs=[left_branch, right_branch], outputs=output_layer) model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) model.fit([review_matrix_for_embedding, second_matrix], labels,epochs=5, verbose=1) The first one does 80% accuracy while the second one does 70%, with embedding vectors and LSTM layer. How is it possible? Is there anything wrong in my architecture?
