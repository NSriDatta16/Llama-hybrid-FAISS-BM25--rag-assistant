[site]: crossvalidated
[post_id]: 596732
[parent_id]: 
[tags]: 
How do I assess the significance of classification accuracy, after determining hyperparameters with GridSearchCV and testing with train_test_split?

So I've recently learned that if I use GridSearchCV to select the best hyperparameters AND evaluate model performance, this can lead to an optimistically biased measure of performance due to overfitting (Cawley, G.C.; Talbot, N.L.C. On over-fitting in model selection and subsequent selection bias in performance evaluation. J. Mach. Learn. Res 2010,11, 2079-2107.). Here's how I had originally run that, using a permutation test to assess significance of the (overestimated) classification accuracy: def classify_high_low(X, y): # shuffle X, y = shuffle(X, y) # specify model svm_model = SVC() sc = StandardScaler() pipeline = make_pipeline(sc, svm_model) param_grid = { 'svc__C': np.logspace(-6,6,30), 'svc__gamma': np.logspace(6,-6,30), 'svc__kernel': ['rbf', 'poly', 'sigmoid'] } cv_search = GridSearchCV(pipeline, param_grid, scoring='accuracy', cv=5) cv_search.fit(X,y) # update pipeline with new estimator pipeline = cv_search.best_estimator_ # run permutation test score, p_scores, p = permutation_test_score( pipeline, X, y, scoring='accuracy', cv=5, n_permutations=1000) return score, p_scores, p, cv_search After my revelation, I set aside a separate testing set, ran GridSearchCV on the training set to determine the best hyperparameters, and evaluated the final model with the left out testing set: def classify_high_low(X, y): # Split the data into training/testing subsets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0, stratify=y) # specify model svm_model = SVC() sc = StandardScaler() pipeline = make_pipeline(sc, svm_model) param_grid = { 'svc__C': np.logspace(-6,6,30), 'svc__gamma': np.logspace(6,-6,30), 'svc__kernel': ['rbf', 'poly', 'sigmoid'] } cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0) clf = GridSearchCV(pipeline, param_grid, scoring='accuracy', cv=cv) clf.fit(X_train, y_train) # Update pipeline with best model pipeline = clf.best_estimator_ y_pred = pipeline.predict(X_test) return accuracy_score(y_test, y_pred) But - this just returns a single accuracy score, and the result of this obviously depends on the split in train_test_split. I'm not quite sure how to overcome the potential bias in this (fairly arbitrary) split, or if I could assess this with some sort of permutation So is it possible to combine a search for the best hyperparameters with gridsearch, evaluate the model performance with a left out testing set, and run some sort of permutation test to assess classification accuracy? Thanks
