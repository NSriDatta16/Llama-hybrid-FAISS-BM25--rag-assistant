[site]: crossvalidated
[post_id]: 586096
[parent_id]: 
[tags]: 
KL divergence and the MAP approximation in BNNs

I was reading this blog post on bayesian neural networks, where the author shows that if we use as a variational distribution a product of delta function, then minimizing the loss function of a BNN is equivalent to minimize the loss function of a standard neural network with a l2 regularization mechanism. However, I have read that the Kl divergence is only defined between continuous distribution and not between discrete and continuous distribution, so I was wondering if the demonstration is still approximatively right or not and if not, what changes should be made. thanks
