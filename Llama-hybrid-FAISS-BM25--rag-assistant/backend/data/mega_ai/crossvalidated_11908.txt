[site]: crossvalidated
[post_id]: 11908
[parent_id]: 11903
[tags]: 
The idea looks related to using the bootstrap, see Section 7.11 in ESL , as an alternative to cross-validation. The bootstrap also resamples subsets for training and uses the original data for estimation of the generalization error (evaluation of the model). The difference, as far as I can see, is that when using the bootstrap we train the model on the subsampled data sets, whereas the suggestion here is "merely" to evaluate a given parameter on a subsampled data set. The training happens inside the magic genetic algorithm (that I don't know anything about) by modifications of the parameters before the next evaluation. For the bootstrap various issues arise from the fact that subsamples share observations with the date set used for evaluationO, and the average number of distinct observations in a subsample is somewhat smaller than the size of the data set. This leads to strange corrections like the .632-estimator, see (7.56) in ESL, and I don't think that the bootstrap has any edge over cross-validation. I have no reason to rule out the idea presented, but I would be skeptical about the final evaluation on the full data set. As I understand the suggested idea, the algorithm will produce a sequence of parameter estimates that get better and better when evaluated on subsampled data. Because you constantly subsample, I don't see that the algorithm can be convergent, but it may stabilize after a number of iterations. I would imagine that the parameters would then be "around" the MLEs for the subsampled data (but I may be wrong here), and that there would be issues similar to those for the bootstrap with evaluating parameters/models on the full data set. By the way, if I remember correctly, the fitting of e.g. generalized additive models by the R package mgcv uses an algorithm where everything is optimized in one go.
