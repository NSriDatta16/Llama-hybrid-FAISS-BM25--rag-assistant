[site]: datascience
[post_id]: 93623
[parent_id]: 63997
[tags]: 
They're not disjunctive, seq2seq models can use LSTMs in their architecture, i.e both encoder and decoder can be made of deep bi-directional LSTMs. Seq2seq models are a type of architecture that's useful for tasks like Machine translation. LSTMs are just a building block of neural networks. It's not possible to compare relative performance of both. This is sort of like asking the question, why is a car better than its engine, or a house better than the internal plumbing inside. Seq2seq models in their latest form now use totally new technology called the Transformer, and may not use LSTMs anymore which might be considered old fashioned! I guess that's another reason for any perceived 'superiority' For why LSTMs are better than RNNs, there's a lot of information out there if you google for the 'vanishing/exploding gradient problem'.
