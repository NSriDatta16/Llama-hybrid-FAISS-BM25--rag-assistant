[site]: crossvalidated
[post_id]: 564335
[parent_id]: 
[tags]: 
What's the role of masking in transformer and BERT?

I've recently implemented the architectures of Transformer and BERT and found that they both have common property - masked layers among one of them. I have come to questions like below. As far as I know, the technique of "masking" has been first introduced in Transformer. Is this true? (Weren't there anything like "masking" before Transformer?) I've googled around about the effect of "masking" but not too many effective results come up (Most of the articles say that some models exploit the masking layer, not talking about its effect in detail). What's the effect of this masking layer discovered so far?
