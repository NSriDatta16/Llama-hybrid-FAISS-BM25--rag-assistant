[site]: crossvalidated
[post_id]: 255120
[parent_id]: 255118
[tags]: 
Judging by the way the charts look, I would say there is room for improvement for R^2. You can probably wait for more iterations until the loss function reaches a flat zone. You can also experiment starting with a higher learning rate and apply a decay during training. Or do Neural Networks typically go through a single "phase" of improvement? I think this depends more on the data you are using for training. There can be cases where training might get stuck in a local optimum making the network for example to predict correctly only 3/5 classes and just randomly guess for the other 2. If the limit between classes is sensible, the training might exit the local optimum and you will see another sharp descend in the error function.
