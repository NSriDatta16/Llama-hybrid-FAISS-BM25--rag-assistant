[site]: datascience
[post_id]: 88936
[parent_id]: 
[tags]: 
Why there is only one type of artificial neuron?

I find it strange that so many deep learning tricks and improvements have been invented in the past decade but I never heard about someone trying out different models of the artificial neuron other than the one based on perceptron: y = w*x + b I haven't been able to find much info on this online which is surprising. I am not a machine learning expert but from the little I know, it would make sense to me to at least experiment with other options. A trivial example, what would happen if there was a layer in a network consisting of neurons where y = w*x^2 + b Maybe there is an obvious answer to why my suggestion above isn't a good idea but I would prefer an answer that explains why generally this is not being looked into (at least as far as i know).
