[site]: crossvalidated
[post_id]: 594125
[parent_id]: 593669
[tags]: 
The first three autocorrelation coefficients in your time series are 0.7, 0.49, 0.41, which is just a little off from an AR(1) model with an autocorrelation of 0.7. A seasonality of 7 days is also apparent from the peaks at 7, 14, 21 and 28 days. The persistence of a significant positive autocorrelation out to 30 days and possibly beyond, is typical when a trend exists, but I do not believe this is your case, judging from the timeseries plot (top) which exhibits no trend. So it is possible that your data is better described via either integration (I1 or I2), or a long-range moving average (MA). From the additional information in the comment, I gather that the trend plot is misleading as you fed the wrong value to the period parameter. While it is supposed to be 7 based on the autocorrelation, you fed it round(len(df) / 2) , or half the length of the time series - so statsmodel was trying to find a periodicity of approximately 2.8 years in your data. My suggestion is to correct the following in your analysis: analyze the first difference of the Load column rather than the data in the column. use period=7 Turn off trend extrapolation as there should not be any trend in your data. In code, this translates to: df['Load_d1'] = df['Load'].diff() decomp = seasonal_decompose(df['Load_d1'], period=7, extrapolate_trend=0) Check the standard deviations of your residuals: np.std(decomp.resid) The value of your residual standard deviation should be less than the standard deviation of the original data, as the seasonality and integration components should account for most of it. At this point, you can use linear regression on the residuals, to find the AR(1) coefficient in the de-seasonalized data. Remember that the whole thing should be attempted on Load_d1 rather than Load . To reconstruct the prediction of Load from the differentiated timeseries, use np.cumsum() .
