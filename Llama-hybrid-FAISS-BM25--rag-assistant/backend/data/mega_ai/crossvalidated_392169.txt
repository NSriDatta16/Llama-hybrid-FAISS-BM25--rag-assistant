[site]: crossvalidated
[post_id]: 392169
[parent_id]: 
[tags]: 
Estimating true positive rate with a confidence interval given a classifier + unknown dataset truths

Let's say I've built a binary classifier - one that for instance, can classify whether a particular transaction is fraudulent or not. This classifier outputs a 1 or a 0, with a given degree of confidence (i.e 0.84) - aka Logistic Regression. I can evaluate my model on a test dataset (where the truths are known), and use stratified sampling to ensure that my test dataset and train dataset follow the same distribution. This allows me to get the standard binary classification metrics (accuracy, precision, recall, F1, etc.) However - what can I do to verify how the performance of my classifier compares with respect to the actual dataset (where the truths are unknown)? In the case of fraud, the target dataset is usually heavily skewed (most transactions are not fraudulent). How do I know what % of the actually fraudulent transactions am I correctly predicted to be fraudulent? Is there a statistical approach that I can use to estimate the true positive rate - perhaps with a given confidence? I've seen random sampling thrown around, but I'm not exactly sure what that process would look like (would we just run the classifier on the random samples?) in a skewed dataset. Ideally - I want to be able to say "This model covers 73% of all fraudulent transactions, with a 95% confidence".
