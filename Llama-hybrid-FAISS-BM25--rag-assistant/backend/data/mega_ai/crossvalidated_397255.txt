[site]: crossvalidated
[post_id]: 397255
[parent_id]: 186182
[tags]: 
I don't know each library you are using. But most ML libraries have model optimizers built in to help you with this task. For instance, if you using sklearn, you can use RandomizedSearchCV to look for a good combination of hyperparameters for you. For instance, if you training a Random Forest classifier": #model MOD = RandomForestClassifier() #Implemente RandomSearchCV m_params = { "RF": { "n_estimators" : np.linspace(2, 500, 500, dtype = "int"), "max_depth": [5, 20, 30, None], "min_samples_split": np.linspace(2, 50, 50, dtype = "int"), "max_features": ["sqrt", "log2",10, 20, None], "oob_score": [True], "bootstrap": [True] }, } scoreFunction = {"recall": "recall", "precision": "precision"} random_search = RandomizedSearchCV(MOD, param_distributions = m_params[model], n_iter = 20, scoring = scoreFunction, refit = "recall", return_train_score = True, random_state = 42, cv = 5, verbose = 1 + int(log)) #trains and optimizes the model random_search.fit(x_train, y_train) #recover the best model MOD = random_search.best_estimator_ Note that the parameters scoring and refit will tell the RandomizedSerachCV which metrics you are most interested in maximizing. This method will also save you the time of hand tuning (and potentially overfitting your model on your test data). Good luck!
