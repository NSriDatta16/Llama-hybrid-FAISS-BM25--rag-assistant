[site]: crossvalidated
[post_id]: 566099
[parent_id]: 
[tags]: 
does assumptions effect the bias or variance?

in machine learning text it is often said that assumptions affect bias like the following text from Kevin Murphy: "Given the large variety of models in the literature, it is natural to wonder which one is best. Unfortunately, there is no single best model that works optimally for all kinds of problems â€” this is sometimes called the no-free lunch theorem [Wol96]. The reason is that a set of assumptions (also called inductive bias) that works well in one domain may work poorly in another." but my question is that it is totally possible that I make an assumption say, for example, the density can not be estimated with any finite set of parameters so we should use non-parametric methods, which makes the variance worst. we know that given infinite data and computation non-parametric models would estimate the density perfectly and they are universal approximators, so we won't have any bias because the limit would be the density itself but this assumption makes the variance a lot worst. so which one is it? the assumptions make bias worst or they can affect both bias and variance?
