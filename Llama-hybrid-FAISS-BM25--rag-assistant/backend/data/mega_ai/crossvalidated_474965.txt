[site]: crossvalidated
[post_id]: 474965
[parent_id]: 473582
[tags]: 
Indeed, the procedure you describe is what it is typically done in mixed-effects models. When you fit the models under maximum-likelihood you only get $\hat \theta$ , and then using empirical Bayes you get an estimate of $\hat b_i(\hat \theta)$ , which you plug-in the equation to obtain a prediction for a particular subject. In the context of linear mixed models, the resulting predictions are calls Best Linear Unbiased Predictions (BLUPs), and you can find more info, for example in, Searle, Casella and McCulloch (1992, Chapter 7) or in McLean, Sanders and Stroup (1991, The American Statistician 45 , 54-64). Calculating standard errors or a confidence interval is more tricky but could be done with a procedure that mimics the Bayesian approach. That is, you could repeat the following steps, say $M = 1000$ times: Simulate $\theta^*$ from $\mathcal N\{\hat \theta, \mbox{var}(\hat\theta)\}$ . Simulate $b_i^*$ from the posterior distribution of the random effects, i.e., $[p(y_i \mid b; \theta^*) p(b; \theta^*)]$ . Calculate $y_{pred}^*(m) = f(t, \theta^*, b_i^*)$ The first step accounts for the uncertainty in the maximum likelihood estimates, and the second step accounts for the uncertainty in the random effects. You can get as an estimate for standard error using the sample standard deviation of the $y_{pred}^*(1), \ldots, y_{pred}^*(M)$ values.
