[site]: crossvalidated
[post_id]: 523578
[parent_id]: 402644
[tags]: 
Can LogisticRegressionCV be used with StandardScaler without data leakage? No, it can't. Can LogisticRegression be used with Cross Validation and StandardScaler without data leakage? Yes. As it is stated in the LogisticRegressionCV documentation (as explained below), that LogisticRegressionCV is equivalent to LogisticRegression used inside GridSearchCV . So instead of using LogisticRegressionCV we can use LogisticRegression + GridSearchCV , which works in exactly the same way and produces exactly the same results, but this new duo is more modifiable and allows us to introduce StandardScalar the right way. Can you explain this workaround in more detail and provide an example? Let's do it in three steps. First, investigate how LogisticRegressionCV is implemented by just using the documentation. Then, implement it ourselves. Finally, extend our implementation with the StandardScaler. 1. Investigation First, let's figure out how LogisticRegressionCV is implemented. From the documentation It mentions ( "For the grid of Cs values and l1_ratios values" ) that a grid search is used, therefore we will use grid search as well, it's implemented as GridSearchCV() . For l1_ratios it says Only used if penalty='elasticnet' . The default value of penalty is penalty=’l2’ , therefore l1_ratios is not optimized by default. Only Cs are searched. Default value of Cs is Cs=10 . The documentation says, If Cs is as an int, then a grid of Cs values are chosen in a logarithmic scale between 1e-4 and 1e4 . We can implement this behavior with Cs = numpy.logspace(-4, 4, num=10) We know that the best hyperparameter is selected by the cross-validator StratifiedKFold , but it's also used by default by the GridSearchCV , so we can ignore this. 2. Create an equivalent to LogisticRegressionCV Let's say we have this Logistic Regression example: from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegressionCV n = 100 X, y = make_classification(random_state=1, n_samples=n, n_features=5, n_informative=3, n_redundant=2) model = LogisticRegressionCV() model.fit(X, y) print(f'Train set accuracy: {100 * sum(model.predict(X) == y) / n:.2f} %') # Train set accuracy: 88.00 % Putting our knowledge together, we can equivalently replace it with: import numpy as np from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression from sklearn.model_selection import GridSearchCV n = 100 X, y = make_classification(random_state=1, n_samples=n, n_features=5, n_informative=3, n_redundant=2) lr = LogisticRegression() param_grid = {'C': np.logspace(-4, 4, num=10)} model = GridSearchCV(lr, param_grid) model.fit(X, y) print(f'Train set accuracy: {100 * sum(model.predict(X) == y) / n:.2f} %') # Train set accuracy: 88.00 % 3. Extend with StandardScaler We add the StandardScaler to the pipeline like this: import numpy as np from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression from sklearn.model_selection import GridSearchCV from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler n = 100 X, y = make_classification(random_state=1, n_samples=n, n_features=5, n_informative=3, n_redundant=2) pipeline = Pipeline(steps=[ ('scaler', StandardScaler()), ('lr', LogisticRegression()) ]) param_grid = {'lr__C': np.logspace(-4, 4, num=10)} model = GridSearchCV(pipeline, param_grid) model.fit(X, y) print(f'Train set accuracy: {100 * sum(model.predict(X) == y) / n:.2f} %') # Train set accuracy: 88.00 % That's it!
