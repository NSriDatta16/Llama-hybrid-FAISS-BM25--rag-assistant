[site]: crossvalidated
[post_id]: 427298
[parent_id]: 
[tags]: 
How can I coumpute Policy Gradient LOSS in tensorflow

I am self-studying RL and currently doing hw2 from Berkeley CS294-112. The thing I cannot figure out is how to compute loss in policy gradients. Basically, REINFORCE algorithm has the following update process: I thought I need to sum up log probability and reward for a single episode (time t=1 ... t=T) then multiply them, which means And for all episodes (i=1, ..., i=N) I tried to compute "MEAN". Most of codes in github, however, do differently: with tf.variable_scope("log_probability_weighted_by_advantage"): sy_weighted_logprob_n = tf.multiply(self.sy_logprob_n, self.sy_adv_n) with tf.variable_scope("loss"): self.sy_loss = - tf.reduce_mean(sy_weighted_logprob_n) self.update_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.sy_loss) In this code, sy_logprob_n and sy_adv_n have [total # of states from all the episode N, 1] shape. So how I understand this code is that do not care about summing up and multiplying "for a single episode", but rather multiplying each states of all the episodes and sum them up(or compute mean). And why does this works?
