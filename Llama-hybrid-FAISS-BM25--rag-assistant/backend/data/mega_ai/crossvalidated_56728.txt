[site]: crossvalidated
[post_id]: 56728
[parent_id]: 56725
[tags]: 
I find that high profile medical journals in the State (NEJM, Lancet, JAMA) tend to publish retrospective studies that are of high quality from a biostatistician's point of view. There are a few nuanced aspects to such studies, notably that the interpretation of model parameters are often different and not immediately palpable as relevant association measures. For instance, hazard ratios and odds ratios from Cox and logistic regression models approximate relative risks for outcomes comparing exposure levels, but only when the outcome is rare. In association studies of rare events, like the Wilm's tumor or Non-Hodgkin's lymphoma, retrospective studies work at the population level. The great benefit of retrospective designs is that one can sample individuals based on their case status (e.g. whether they're diseased) and compare them to controls in their prevalence of exposure. I think this seems like a paradox at first blush, but mathematically it works out. We can explore associations in the populations this way. In general, I critique retrospective designs using all the same tools with which I critique clinical trials: was the outcome prespecified? Were eligible populations well defined? Were matching and confounding variables clearly described and accounted for? Are the major conclusions consistent with the data obtained?
