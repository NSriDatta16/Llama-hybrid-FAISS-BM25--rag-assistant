[site]: crossvalidated
[post_id]: 494445
[parent_id]: 410534
[tags]: 
The problem is solved once you can solve it for lag 1, because you can take $k$ such independent (or at least uncorrelated) time series $X_t^{(1)},$ $X_t^{(2)},$ through $X_t^{(k)}$ and interleave them to form a time series $$X_t = X_1^{(1)}, X_1^{(2)}, \ldots, X_1^{(k)},\ X_2^{(1)}, \ldots, X_2^{(k)},\ X_3^{(1)}, \ldots$$ with zero autocorrelation at all but lags that are multiples of $k.$ Let's call this the " $k$ -interleaved series." Consider a portion of a time series process $X_t$ , which we may index at times $t=1,2,\ldots,n.$ For the autocorrelations to be meaningful, we must assume the entire process is at least weakly second order stationary. The question requires the covariance matrix of the random vector $\mathcal{X}=(X_1,X_2,\ldots,X_n)$ to have the form $$\mathbb{P}_n(\rho) = \pmatrix{1 & \rho & \color{gray}0 & \color{gray}0 & \cdots & \color{gray}0 &\color{gray}0\\ \rho & 1 & \rho & \color{gray}0 & \cdots & \color{gray}0& \color{gray}0\\ \color{gray}0 & \rho & 1 & \rho & \cdots & \color{gray}0& \color{gray}0\\ \vdots & \vdots & \vdots & \ddots & \cdots & \vdots& \vdots\\ \color{gray}0 & \color{gray}0 & \color{gray}0 & \color{gray}0 & \cdots &\rho & \color{gray}0\\ \color{gray}0 & \color{gray}0 & \color{gray}0 & \color{gray}0 & \cdots &1& \rho\\ \color{gray}0 & \color{gray}0 & \color{gray}0 & \color{gray}0 & \cdots & \rho & 1 }$$ where $\rho$ is the common lag-1 autocovariance. Regardless what value $\rho$ might have, this matrix has the eigenvectors $$q_{j;n} = \left(\sin\left(\frac{2\pi i j}{n+1}\right)\right)_{i=1,2,\ldots,n}$$ with corresponding eigenvalues $\lambda_{j;n}(\rho)$ (which may easily be computed from the eigenvalue equation: $\lambda_{j;n}$ is the common ratio of the components of $\mathbb{P}_n(\rho)q_{j;n}$ and the components of $q_{j;n}$ ). Provided $|\rho|\le 1/2,$ all those eigenvalues will be non-negative, which implies there exists a random vector $\mathcal X$ with $\mathbb{P}_n(\rho)$ as its covariance matrix. The orthogonal matrix $\mathbb{Q}_n = (q_{ij}) = (q_1;q_2;\cdots;q_n)$ whose columns comprise the normalized eigenvectors diagonalizes $\mathbb{P}_n(\rho).$ Consequently, when $\mathcal{Y}_n(\rho) = (Y_{j;n}(\rho))$ is any vector of $n$ independent zero-mean variables with variances $$\operatorname{Var}(Y_{j;n}) = \lambda_{j;n}(\rho),$$ the variable $$\mathcal{X}_n(\rho) = \mathbb{Q}_n \mathcal{Y}_n(\rho)$$ has all the properties desired of $X_t.$ This demonstration leads to a simple and fairly efficient algorithm to generate realizations of the process $X_t.$ (It requires $O(n^2)$ computation, but likely could be sped up to $O(n\log(n))$ using the FFT.) On this workstation it requires less than 0.02 seconds to generate one realization with $n=2000.$ Here are the empirical acfs of eight simulated series with $n=2000$ and lag-1 coefficient $\rho=1/2:$ All the lags but the first differ only randomly from zero, while the lag-1 coefficient differs only randomly from $\rho.$ When these $8$ series are interleaved , they form one series of length $8\times 2000=16000$ and, by design, the only nonzero autocorrelation coefficient occurs at lag $8,$ where it equals $\rho:$ What would such a series look like? Here are the first $128$ values of the interleaved series from the simulation: There is a hint of a seasonal cycle of period 8. However, as we know, this series has no seasonal fluctuations. For instance, the stl function in R finds only a tiny seasonal component of variance $0.0017$ compared to residuals of variance $0.82,$ almost 500 times larger. This is the R code used to produce the simulated data and graphics. n
