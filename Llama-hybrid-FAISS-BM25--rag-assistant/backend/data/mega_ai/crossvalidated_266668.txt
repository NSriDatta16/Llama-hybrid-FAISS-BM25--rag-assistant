[site]: crossvalidated
[post_id]: 266668
[parent_id]: 266652
[tags]: 
PCA works by computing the eigenvectors of the covariance matrix of the data. That is, those eigenvectors correspond to the choices of $a_{1:M}$ that maximize the equations and meet the constraints given in your book. If you chose different vectors, they wouldn't fit all those criteria, and it wouldn't be a PCA anymore (you would still find a number of "components" but they would no longer be "principal"). Eigenvectors can be computed from any square matrix and don't have to be orthogonal. However, since any proper covariance matrix is symmetric, and symmetric matrices have orthogonal eigenvectors, PCA always leads to orthogonal components. The orthogonality of $y_1$ and $y_2$ doesn't follow solely from the requirement that $a_1^Ta_2=0$ - it follows from all the constraints together. It's easy to see why orthogonality of $a_1$ and $a_2$ isn't enough, because the original basis $\mathbf{b}$ in which the data is expressed is also orthogonal. E.g. in 2 dimensions, you would have $b_1=\begin{bmatrix}1\\ 0 \end{bmatrix}$ and $b_2=\begin{bmatrix}0\\ 1 \end{bmatrix}$ and clearly your data don't have to be uncorrelated along those dimensions (if they were, your PCA would just return the original basis, up to a scaling factor). The text is worded a bit awkwardly, but I think the "which" in "which ensures..." refers to the entire clause that came before.
