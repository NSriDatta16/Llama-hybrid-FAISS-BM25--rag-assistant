[site]: crossvalidated
[post_id]: 355267
[parent_id]: 354717
[tags]: 
For Bayesian estimation, the point of MCMC is only to simulate data from--and thereby obtain an estimate of--the posterior. In general, MCMC (Markov Chain Monte Carlo) only refers to generating realizations or simulating data from a probability model. This is necessary for Bayesian analyses where the posterior if often not solvable in a closed form. Gibbs Sampling is perhaps the most popular technique for obtaining a posterior and it happens to be MCMC, but there are other approaches . These simulation methods are imprecise, but we do not call this loss (in the same way that a blurry image could be called "lossy"). We call this precision. Increasing the number of iterations of Gibbs sampling will generally increase the precision with which the density of the posterior can be estimated. The lack of precision is called MCMC error. Rather, loss is a general concept of Bayesian estimation . In practice, we make no distinction between posteriors which are estimated from MCMC or posteriors which are known in exact form when we discuss the loss of estimators. The risk (expected loss) is calculated the same way. Optimal Bayes estimators minimize risk.
