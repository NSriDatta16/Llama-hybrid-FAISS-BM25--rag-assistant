[site]: crossvalidated
[post_id]: 563941
[parent_id]: 563912
[tags]: 
I think it’s important to remember what $R^2$ means in the linear case. $$ R^2=1-\left(\dfrac{ \overset{N}{\underset{i=1}{\sum}}\left( y_i-\hat y_i \right)^2 }{ \overset{N}{\underset{i=1}{\sum}}\left( y_i-\bar y \right)^2 }\right) $$ If we want to measure our ability to predict conditional means by how low of a square loss we have, we better have lower square loss than the naïve model that guesses $\bar y$ every time! This is exactly what is going on in the pseudo- $R^2$ . If you do a worse job of predicting the conditional probability (not label) than a naïve model that always predicts the overall prevalence, then the numerator is larger than the denominator, resulting in pseudo- $R^2 . In the case of a probability model, square loss is called Brier score and is not the usual loss function. Brier score is, however, a strictly proper scoring rule, which means, a little loosely speaking, that it seeks out the true conditional probability values. The typical loss function in logistic regression is log loss, which corresponds to maximum likelihood estimation of the coefficients. It makes sense to compare the log loss values in a similar way. This is McFadden’s $R^2$ . Indeed, I say that it always makes sense to compare how your model does on a loss function of interest compared to some baseline model. In OLS linear regression, there is a convenient interpretation about the “proportion of variance explained”, but even if we lack such an interpretation , comparing our performance to the performance of a baseline model gives us some idea of if our model provides value. UCLA has a nice webpage about $R^2$ -style metrics for probability models like logistic regression . Vanderbilt's Frank Harrell has some thoughts on how to measure the value added by a model. EDIT Worth remembering is that, if the logistic regression coefficients are estimated by minimizing log loss (equivalent to maximum likelihood estimation), even the in-sample Efron $R^2$ can be less than one, since the objective of the coefficient estimation is not to minimize Brier score but log loss. In contrast, this cannot happen for in-sample McFadden's $R^2$ that addresses the explicit objective function used to estimate the model, except for computational funkiness.
