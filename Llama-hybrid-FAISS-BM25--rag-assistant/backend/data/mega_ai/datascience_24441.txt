[site]: datascience
[post_id]: 24441
[parent_id]: 24426
[tags]: 
You are working with word classification so you really don't have any contextual information that you can leverage. So the best thing would be to use freely available contextual information called as word2vec. I would suggest using the pretrained GLOVE Word2Vec as they are much cleaner than the Google ones. I am going to suggest two approaches for doing this: Naive Approach Load the pretrained word vectors. For every new word calculate cosine similarity with every item in your training list. Predict the label for the word which is closest to your new word. Alternatively you can calculate cosine similarity with the labels and just predict the label which comes the closest. Better Approach Load the pretrained word vectors. Extract the vectors for every item in the training set. One hot encode the labels. Train a neural net or any multiclass classification algorithm using the vectors as features and save the model. For every new word extract the vector and run the model on this vector. The predicted label is your output. You can use the first approach to run quick tests and check the efficacy of the approach and then move on to the second one if you found it fit to your use.
