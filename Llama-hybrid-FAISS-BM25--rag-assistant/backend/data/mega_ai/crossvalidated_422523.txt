[site]: crossvalidated
[post_id]: 422523
[parent_id]: 
[tags]: 
Why does absolutely-summable weights ensures a linear series itself summable (convergent)? Some questions on def'n of Linear Series

A "linear series" $y_t$ is the linear combination $$y_t - \mu = \sum_{i=-\infty}^{\infty}\psi_iL^i\nu_t = \sum_{i=-\infty}^{\infty}\psi_i\nu_{t-i}=S(L)\nu_t $$ of weighted (by $\psi_i$ weights) lags and forwards of a white noise series $ν_t \thicksim wn(0,σ^2 )$ [ $S(L)\equiv \sum_{i=-\infty}^{\infty}\psi_iL^i$ ; $\mu \equiv E(y_t)$ ; $\psi_i$ weights are absolutely-summable: $\sum_{i=-\infty}^{\infty}|\psi_i| ]. (Shumway, 2017:25). The absolutely-summability condition of $\psi_i$ weights in the definition of linear series ensures that the infinite sum in the definition to be convergent (the series itself becomes summable). Proof: WLOG assume $μ≡0$ . By the def'n of a linear series, $E(ν_t )=0$ . The convergence is obvious since $E(\nu_t) \leq\sigma$ , and $$E(y_t)=E(\sum_{i=-\infty}^{\infty}\psi_i\nu_{t-i})$$ $$\leq \sum_{i=-\infty}^{\infty}(|\psi_i| E(\nu_{t-i}))$$ $$\leq \left (\sum_{i=-\infty}^{\infty}|\psi_i| \right)\sigma QED. Source of the context: Brockwell 2016; Introduction to Time Series and Forecasting. Page 44. My questions are: 1. By $\sigma$ , what is meant? (my guess: $\sigma_{\nu_t}$ ) 2. Why the author employs expected value in the proof? (though what we have to show is just summability). Why does it materialize in the proof? 3. In the proof, passing from 1st line to 2nd line, and 2nd to 3rd line is not clear to me.
