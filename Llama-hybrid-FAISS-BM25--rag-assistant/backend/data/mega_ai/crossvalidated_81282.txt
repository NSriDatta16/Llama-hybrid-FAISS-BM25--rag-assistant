[site]: crossvalidated
[post_id]: 81282
[parent_id]: 
[tags]: 
Combining information from multiple studies to estimate the mean and variance of normally distributed data - Bayesian vs meta-analytic approaches

I have reviewed a set of papers, each reporting the observed mean and SD of a measurement of $X$ in its respective sample of known size, $n$. I want to make the best possible guess about the likely distribution of the same measure in a new study that I am designing, and how much uncertainty is in that guess. I am happy to assume $X \sim N(\mu, \sigma^2$). My first thought was meta-analysis, but the models typically employed focus on point estimates and corresponding confidence intervals. However, I want to say something about the full distribution of $X$, which in this case would also including making a guess about the variance, $\sigma^2$. I have been reading about possible Bayeisan approaches to estimating the complete set of parameters of a given distribution in light of prior knowledge. This generally makes more sense to me, but I have zero experience with Bayesian analysis. This also seems like a straightforward, relatively simple problem to cut my teeth on. 1) Given my problem, which approach makes the most sense and why? Meta-analysis or a Bayesian approach? 2) If you think the Bayesian approach is best, can you point me to a way to implement this (preferably in R)? Related question EDITS: I have been trying to work this out in what I think is a 'simple' Bayesian manner. As I stated above, I am not just interested in the estimated mean, $\mu$, but also the variance,$\sigma^2$, in light of prior information, i.e. $P(\mu, \sigma^2|Y)$ Again, I know nothing about Bayeianism in practice, but it didn't take long to find that the posterior of a normal distribution with unknown mean and variance has a closed form solution via conjugacy , with the normal-inverse-gamma distribution. The problem is reformulated as $P(\mu, \sigma^2|Y) = P(\mu|\sigma^2, Y)P(\sigma^2|Y)$. $P(\mu|\sigma^2, Y)$ is estimated with a normal distribution; $P(\sigma^2|Y)$ with an inverse-gamma distribution. It took me a while to get my head around it, but from these links( 1 , 2 ) I was able, I think, to sort how to do this in R. I started with a data frame made up from a row for each of 33 studies/samples, and columns for the mean, variance, and sample size. I used the mean, variance, and sample size from the first study, in row 1, as my prior information. I then updated this with the information from the next study, calculated the relevant parameters, and sampled from the normal-inverse-gamma to get the distribution of $\mu$ and $\sigma^2$. This gets repeated until all 33 studies have been included. # Loop start values values i Here is a path diagram showing how the $E(\mu)$ and $E(\sigma^2)$ change as each new sample is added. Here are the desnities based on sampling from the estimated distributions for the mean and variance at each update. I just wanted to add this in case it is helpful for someone else, and so that people in-the-know can tell me whether this was sensible, flawed, etc.
