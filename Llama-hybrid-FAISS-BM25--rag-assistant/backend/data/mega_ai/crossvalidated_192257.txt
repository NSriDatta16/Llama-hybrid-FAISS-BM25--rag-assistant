[site]: crossvalidated
[post_id]: 192257
[parent_id]: 
[tags]: 
Gaps of methods to evaluate prediction accuracy

There are many methods to evaluate prediction models based in prediction errors, such as MSE, MAE, MAPE, WMAE, etc. These methods are usually used in data prediction competitions, where one is given a set of data used to discover potentially predictive relationships (training set), and must apply her predictions in another set of data used to assess the strength of her model (test data). Some of these methods have gaps that can be explored. I can think of two cases: The Mean Absolute Error (MAE) does not take into account the relative size of the error, so I guess conservative predictions may not be a good idea if this is the evalutation measure. I am not sure how, but weighted measures such as WMAE may be explored since one may pay more attention in predicting values that are more important for the evaluation method. Is my line of thought right? Can you elaborate more on that? What are other gaps that can be explored in the most common prediction evaluation measures and how one may explore this gaps? Is there a measure that you consider most appropriate for these data competitions?
