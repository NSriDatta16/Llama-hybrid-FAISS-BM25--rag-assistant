[site]: datascience
[post_id]: 71950
[parent_id]: 
[tags]: 
Artificially expanding the datasets through rotation of images in MNIST

I came across this question from the 3rd chapter of the book Neural Networks and Deep Learning by Michael Nielsen, this is a question given in his exercise. One way of expanding the MNIST training data is to use small rotations of training images. What's a problem that might occur if we allow arbitrarily large rotations of training images? I would happy if someone explain why large rotations would be problematic.
