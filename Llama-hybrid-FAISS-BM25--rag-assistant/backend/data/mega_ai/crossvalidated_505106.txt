[site]: crossvalidated
[post_id]: 505106
[parent_id]: 
[tags]: 
After Deep Learning Hyperparam tuning, what adjustments should be made when dataset size is scaled up?

I'm dealing with a fully connected NN, and I'm wondering if there are any rules of thumb for adjusting hyperparameters for changes to dataset size. For example, if I increase number of obs by 20%, then I should reduce epochs by 20%, or increase batch size by 20%, or decrease learning rate by X%, or whatever... For context: After hyperparam tuning on a validation or test set, I'm taking my final model, and retraining on all available training data to maximize performance. Since the training data has now slightly increased in size, I want to know if I should make final fine-tune adjustments (which can't be validated) to any part of the model. If using 10-fold cv or something, then this increase is only 10%, so not a big deal. But two situations come to mind where the increase could be more substantial. 1) Feature space is so big that 10-fold, or even 5-fold could be computationally cost-prohibitive. 2) With time series data out-of-time validation is preferred, which means the validation data always must come after training. So it is not possible to get 10 folds trained on 90% of the data. If you want many "folds", you are likely using 50% or less of training data in each fold.
