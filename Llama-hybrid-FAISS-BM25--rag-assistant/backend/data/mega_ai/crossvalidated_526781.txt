[site]: crossvalidated
[post_id]: 526781
[parent_id]: 526779
[tags]: 
You're right, that the input sentence is encoded by a matrix of 1-hot vectors. Just write out the matrix dimension. A sentence has $w$ words in it, and there are $v$ total words in the entire vocabulary. The embedding matrix puts them into $k$ vectors. So the input matrix has dimension $w \times v$ and the embedding matrix has shape $v \times k$ . After multiplication, the result is a $w \times k$ matrix: $w$ different vectors of $k$ elements. I think the key detail here is that the input matrix needs one dimension to have $v$ entries to encode which word in the vocabulary appears at that position.
