[site]: crossvalidated
[post_id]: 506321
[parent_id]: 
[tags]: 
Why is the Kullback-Leibler divergence defined with a negative sign?

I am aware of Gibbs' inequality , but I still want to know why the Kullback-Leibler divergence is defined with a negative sign. Here is my reasoning so far: Let $X$ be a Bernoulli random variable with $p=0.6$ . Suppose that someone tells me that the next sample of $X$ that I observe will be $0$ . Therefore, my new probability mass function will be: $$ p_{\text{new}}(X = 1) = p_{\text{old}}(X=1) \times 0 = 0.6 \times 0 = 0 \\ p_{\text{new}}(X = 0) = p_{\text{old}}(X=0) \times \frac{1}{p_{\text{old}}(X=0)} = 0.4 \times 2.5 = 1 $$ From the way I updated my probability mass function above, it is clear that this person has sent me: $$ \log_2 \left(\frac{1}{p_{\text{old}}(X=0)}\right) = \log_2 (2.5) \approx 1.32 $$ bits of information. Suppose that this person tells me the value of the next infinite number of samples of $X$ . On average, this person will send me: $$ p_{\text{old}}(X=0) \cdot \log_2 \left(\frac{1}{p_{\text{old}}(X=0)}\right) + p_{\text{old}}(X=1) \cdot \log_2 \left(\frac{1}{p_{\text{old}}(X=1)}\right) \\ = 0.4 \cdot \log_2(2.5) + 0.6 \cdot \log_2 \left(\frac{5}{3}\right) \approx 0.97 $$ bits of information. This is how I define entropy : how much I will change my belief about a random variable on average. I want to define the Kullback-Leibler divergence in a similar way, but I'm having trouble with that. Suppose instead that someone tells me that the next sample of $X$ that I observe will be $0$ with probability $0.8$ . Therefore, my new probability mass function will be: $$ p_{\text{new}}(X = 1) = p_{\text{old}}(X=1) \times \frac{1-0.8}{p_{\text{old}}(X=1)} = 0.6 \times \frac{1}{3} = 0.2 \\ p_{\text{new}}(X = 0) = p_{\text{old}}(X=0) \times \frac{0.8}{p_{\text{old}}(X=0)} = 0.4 \times 2 = 0.8 $$ From the way I updated my probability mass function above, it is clear that this person has sent me: $$ \log_2 \left(\frac{p_{\text{new}}(X=0)}{p_{\text{old}}(X=0)}\right) = \log_2 \left(\frac{0.8}{0.4}\right) = 1 $$ bit of information. Suppose that this person tells me the value of the next infinite number of samples of $X$ , each with the same probabilities, denoted as $p_{\text{new}}(X=0)=0.8$ and $p_{\text{new}}(X=1)=0.2$ . On average, this person will send me: $$ p_{\text{old}}(X=0) \cdot \log_2 \left(\frac{p_{\text{new}}(X=0)}{p_{\text{old}}(X=0)}\right) + p_{\text{old}}(X=1) \cdot \log_2 \left(\frac{p_{\text{new}}(X=1)}{p_{\text{old}}(X=1)}\right) \\ = 0.4 \cdot \log_2(\frac{0.8}{0.4}) + 0.6 \cdot \log_2 \left(\frac{0.2}{0.6}\right) \approx -0.551 $$ bits of information. Clearly, it does not make sense to send negative amounts of bits. In fact, if I multiply my answer by $-1$ , I will get the Kullback-Leibler divergence. Is this why the Kullback-Leibler is defined with a negative sign on the front? To avoid the case of sending negative bits?
