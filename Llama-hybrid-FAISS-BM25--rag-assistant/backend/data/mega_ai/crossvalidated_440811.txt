[site]: crossvalidated
[post_id]: 440811
[parent_id]: 440798
[tags]: 
For single fit statistics, commonly used comparisons are often: which model has a smaller average size of regression errors, and which model explains more of the dependent data variance. For average magnitude of regression error, the RMSE (Root Mean Square Error) is usually used. While it seems simpler to take the mean (average) size of the absolute value of error, for historical reasons this was not done as "absolute value" has no continuous derivative. Instead the regression errors were squared to make them positive, the mean value of those squared errors was determined, and then the square root of that mean value was found. This was decided in the early days of statistical modeling because both square and square root have continuous derivatives in the calculus. For explained variance, the R-Squared statistic is used. For example, if the R-squared value is 0.95 then the model explains 95% of the dependent data variance. There is no single value that determines which model is better under all circumstances, as the purpose of statistical modeling can have different goals. As an example, if the purpose of the modeling is to find the lowest percent regression error, or the lowest peak-to-peak regression error, then these two common fit statistics are not useful and - although rare - I have used both lowest percent error and lowest peak-to-peak error in industrial equipment calibrations.
