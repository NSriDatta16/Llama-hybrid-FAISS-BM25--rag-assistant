[site]: datascience
[post_id]: 67690
[parent_id]: 67682
[tags]: 
I am not sure you can neatly fit the above tools and libraries into your schematic. What even is the meaning of "feature" here, is it an output (like "text classification" is an output of the algorithm)? In which case I challenge the usefulness of this schematic. Nevertheless nltk, sklearn, etc. are libraries that contain multiple and diverse tools to help you do everything along the NLP AI modeling workflow. From feature extraction to modelling and validation. As an example all three libraries of spacy, sklearn and nltk have the ability to construct models. So they certainly fit into "ML algorithm" but they also help in feature extraction so they fit in that bucket. Beyond that they offer you general data wrangling capabilities as well as validation measures which do not appear in your schematic. The main difference NLTK and spacy are mainly focused on NLP and text-based data whereas sklearn is very much multipurpose. The Alternatives The alternatives really depend on your use cases, NLTK and spacy do very similiar stuff so are already alternatives to each other for NLP. Sklearn is almost standard when it comes to ML in python but if you want to build deep-learning you will probably work more with Keras, pytorch, etc.
