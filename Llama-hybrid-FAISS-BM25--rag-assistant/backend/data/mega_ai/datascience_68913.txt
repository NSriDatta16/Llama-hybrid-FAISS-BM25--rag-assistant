[site]: datascience
[post_id]: 68913
[parent_id]: 68637
[tags]: 
Let me take a crack at your questions: The article specifies the features are concatenated. How does a concatenation layer work internally? Does it concatenate all the values in a single variable, in a very literal sense? how does that work computationally? The concantenation of information in this context is, concatenation of vector represententations of the text. You could concatenate using a concatenation layer as described here . This is a very common approach followed where you want to feed your network information by taking various contexts depending on the problem you need to solve. How can a Bag of Words be a feature, when its a key-value pair? Or is it also just all concatenated into one variable. Which again, how can that work computationally? Bag of words, is typically a vector representation of the context. The above answer should help you. You could take a look at how to combine embeddings here as well The text specifies multiple words are used as a single feature; e.g. Left context: five words before the entity. Is this again concatenating the embedding / vectors? Yes, you are spot on. You combine the embedding vectors that you generate using skipgram or cbow, or you could even use one-hot encoded vectors. Entity end: last three words of the entity (they can be duplicated with the previous feature if they overlap, or padded if there are not that many) does this mean a variable amount of features as input to the NN (or concatenation layer) or is this more intended as a configuration? Less context available so fewer amount of 'hard' coded input features? Its always good to pad and use well defined dimensional vectors. Helps you structure your architecture better. You could use masking layer to ensure that the network ignores the paddings. I hope this helps.
