[site]: crossvalidated
[post_id]: 7288
[parent_id]: 7141
[tags]: 
Ok, I've thought of two possible ways to answer to this problem using Bayesian analysis. I will assume $\sigma$ to be known throughout this answer. First start with a "baby" case, where $n=2$ (or alternatively, only using the last two observations as a first approximation). You would usually start this by assuming a "flat" prior for $\mu$, just proportion to 1. But you have additional information, so we just restrict the prior to conform to this. So the prior is: $$f(\mu_1,\mu_2|\gamma) \propto I_{|\mu_1-\mu_2|\leq\gamma}$$ (the improper prior should be fine, because you are dealing with normal RVs, and you aren't dividing them) Combining this prior with the likelihood, and integrating out $\mu_1$ gives (Writing $\phi(x)$ as standard normal pdf and $\Phi(x)$ as standard normal cdf): $$f(\mu_2 | X_1,X_2,\sigma,\gamma) \propto \phi\big(\frac{\mu_2-X_2}{\sigma}\big) \Bigg[\Phi\Big(\frac{\mu_2-X_1+\gamma}{\sigma}\Big)-\Phi\Big(\frac{\mu_2-X_1-\gamma}{\sigma}\Big)\Bigg]$$ So in order to calculate the "p-value" for the hypothesis, we need to take $Pr(\mu_2 > 0 |X_1,X_2,\sigma,\gamma)=P$. This is given by the ratio of two integrals of the posterior: $$P=\frac{\int_{-\frac{X_2}{\sigma}}^{\infty}\phi\big(y\big) \Bigg[\Phi\Big(y+\frac{X_2-X_1+\gamma}{\sigma}\Big)-\Phi\Big(y+\frac{X_2-X_1-\gamma}{\sigma}\Big)\Bigg]dy}{\int_{-\infty}^{\infty}\phi\big(z\big) \Bigg[\Phi\Big(z+\frac{X_2-X_1+\gamma}{\sigma}\Big)-\Phi\Big(z+\frac{X_2-X_1-\gamma}{\sigma}\Big)\Bigg]dz}$$ It is beyond my abilities to do either of these integrals exactly, and even if it was possible, you probably would learn anything intuitive about the problem (except that the integral was friggin hard! you'd think it could be derived using something to do with convolutions, but I couldn't work it out). So I would just numerically evaluate these two integrals. For the whole data set, you will almost surely need some kind of numerical technique, or analytic approximation. This is a rather quick numerical technique. Okay, so it basically goes like this: if you knew $\mu_1$, then you could generate a sample of the remaining $\mu_i$ values sequentially, using the uniform distribution $(\mu_{i}|\mu_{i-1}) \sim U(\mu_{i-1}-\gamma,\mu_{i-1}+\gamma)$. An obvious way to sample $\mu_1$ is from a gaussian with large variance $\mu_1 \sim N(0,\delta^2)$ ("large" meaning relative to your data, say $\delta\approx 10\sigma$). Use the notation $\mu_{i}^{(b)}$ for the $b$th sample of means $b=1,\dots,B$. Now you calculate the total likelihood for each iteration. This will be used as a weight: $$w^{(b)}=\prod_{i=1}^{n} \phi \Big(\frac{\mu_{i}^{(b)}-X_i}{\sigma}\Big)$$ Then you take a "weighted probability" of the alternative hypothesis: $$\hat{P}=\frac{\sum_{b=1}^{B}w^{(b)} I(\mu_{n}^{(b)}>0)}{\sum_{b=1}^{B}w^{(b)}}$$ If $P$ is too big (in either case), then you reject the null hypothesis. A standard value would be $P>0.95$.
