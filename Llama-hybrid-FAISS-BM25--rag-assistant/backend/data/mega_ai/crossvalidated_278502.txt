[site]: crossvalidated
[post_id]: 278502
[parent_id]: 278481
[tags]: 
If the original variable X100 is highly informative, then the model can improve in quality when both X100 and X101 are present because X100 or its cousin X101 can be sampled at each split, improving the probability that the feature appears in the model. That is, the model could sample X100, X101 or both when constructing a new split, so the probability that the information in X100 is present when considering a split is higher than for other features. Under the assumption that X100 is useful for predicting the outcome, this means the model gets an artificial boost. This is a well-known phenomenon and is related to the reason that random forest importances aren't completely trustworthy -- in this case, X100 and X101 will "share" the variable importance metric, driving both of their rankings down.
