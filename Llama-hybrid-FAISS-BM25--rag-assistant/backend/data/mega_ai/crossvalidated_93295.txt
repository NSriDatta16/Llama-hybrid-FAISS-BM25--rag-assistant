[site]: crossvalidated
[post_id]: 93295
[parent_id]: 
[tags]: 
Outlier detection in binary classification

I have a question about outlier detection in my system. I’m designing a system (in Matlab) that optimize both features and parameters of a classification method (like mlp) together with optimization algorithms. My input data are financial statement and some these statements have strange and unusual values. (For example someone use only round values like 10000, 100000 in every field or used 1 or 2 in some fields. So I want use outlier detection in my system. Besides it is an option in my system to use dimensionally reduction (like FDA – my system is binary classification) for preprocessing. My critical questions is here: Now I’m using k-means clustering for finding outliers. I cluster every binary class [0 or 1] and remove samples with highest distance from center (for example in every cluster 5% of data). Is this a good approach for outlier detection in my binary classification system? Which outlier detection techniques do you suggest in this case? Where should I put outlier detection function? Before data normalization (mapmaxmin or mapstd), after it? Before or after dimensionally reduction? Or finally because my system find best inputs in among 21 inputs of my system (financial ratios) in every iteration for chosen inputs I must outlier detection? (I think maybe these outliers leaving negative impacts on normalization or outlier detection) After training my system and find best system structure I want use it in real with new dataset and find class of inputs [0 and 1]. So in this phase, I must use outlier detection for this data? If yes, where should I put outlier detection in this phase? Before normalization, after it? Before dimensionality reduction or after it?
