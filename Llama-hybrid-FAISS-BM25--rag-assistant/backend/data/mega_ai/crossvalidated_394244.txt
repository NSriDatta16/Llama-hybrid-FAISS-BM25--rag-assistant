[site]: crossvalidated
[post_id]: 394244
[parent_id]: 394232
[tags]: 
The short answer is: it depends on the rest of the feature set. Unless you restrict your random forest to 'stumps' (i.e. trees of depth 1) then your algorithm will test combinations of features when it builds deeper trees. Feature 1 may perform very well in positively identifying $A$ , but that leaves many observations for which you need other features. Feature 2 may not be informative on its own, but in combination with other features may perform well. So you can't really judge them on their own in most random forests. There are at least couple of ways to tackle this: Try building a basic decision tree and see how your features interact. It may not be the model you'll use, but it can be a useful indicator. Inspect variable importance plots for your random forest. (I don't know exactly how to do this with Python as I mostly use R, but it seems that you can do this with scikit-learn .) This will indicate which features are most important (i.e. have the most predictive power). In the end the question of what makes a 'good' feature is far from trivial in general, but this may help with answering it in this particular context. NB. You might also find Max Kuhn's new book interesting: Feature Engineering and Selection is all about this topic, and the draft version is available online for free.
