[site]: crossvalidated
[post_id]: 496163
[parent_id]: 
[tags]: 
Convergence of the variance of the posterior expectation

Consider a classical Bayesian model : $$ \begin{array}{cc} \theta \sim \pi \\ X = (X_1, ..., X_n) \overset{i.i.d.}{\sim} \pi(.\mid\theta) \end{array} $$ where the prior does $\pi$ and the likelihood $\pi(.\mid \theta)$ do not change with $n$ and data $X$ are i.i.d. conditionnaly on $\theta$ (no weird stuff here). I'm wondering wether the variance of the expectation a posteriori should coincide asymptotically with the variance of $\theta$ . That is, wether the following property is true: $$ \mathrm{Var}\left(\mathbb{E}[\theta \mid X]\right) \underset{n\to\infty}{\longrightarrow}\mathrm{Var}[\theta] .$$ I expect it to be true from an informal reasonning based on variance decomposition, but I would like to have that intuition confirmed (or unvalidated) by a more formal argument. So my questions are : Is this result true ? If yes, under what hypothesis ? How would one prove this rigorously ? Are there cases (not totally degenerated like a point mass prior) where this result does not hold? What about the posterior variance ? Shouldn't it provide information on the variance of the posterior expectation ?
