[site]: datascience
[post_id]: 37995
[parent_id]: 37855
[tags]: 
You have to see this approach more general and not so much from a neural network point of view. The reason one uses the re-arranged version is really just because in the end the gradient of your objective function ends up being an expectation again. This is also pointed out by David Silver at min 44:06 . For an example of the benefits of using $\log$, see UC Berkeley's DeepRL lecture by S. Levine here . The name of the video is "CS294-112 9/6/17" At some later point , S. Levine also goes into more details regarding the math of the policy gradient and draws comparisons to the maximum likelihood approach with neural networks.
