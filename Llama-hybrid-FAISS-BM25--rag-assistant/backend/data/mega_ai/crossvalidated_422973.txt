[site]: crossvalidated
[post_id]: 422973
[parent_id]: 
[tags]: 
Are there autoencoders or similar neural networks that behave like generalized maps?

As per the wikipedia definition, an autoencoder consists of two maps $$\phi: \mathcal{X} \rightarrow \mathcal{F}$$ $$\psi: \mathcal{F} \rightarrow \mathcal{X}$$ such that $$\underset{\phi, \psi}{\mathrm{argmin}} ||X - (\psi \circ \phi)X||^2$$ Since autoencoders reconstruct the original input $\phi$ and $\psi$ are basically learning the identity function. Is there a type of neural network that can learn any transformation? Basically it solves $$\xi: \mathcal{X} \rightarrow \mathcal{Y}$$ $$\underset{\xi}{\mathrm{argmin}} ||Y - (\xi)X||^2$$ In a practical sense, how can I train a neural network that will take pictures of dogs and make them look like cats? Can it be generalized to taking an input from one distribution and transforming it to any arbitrary distribution, e.g. taking pictures of people and making them look like chairs?
