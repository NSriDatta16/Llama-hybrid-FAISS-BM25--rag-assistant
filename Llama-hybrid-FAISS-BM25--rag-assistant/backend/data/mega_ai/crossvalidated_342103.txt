[site]: crossvalidated
[post_id]: 342103
[parent_id]: 
[tags]: 
Early stopping criteria when training neural networks

What happens if the early stopping criteria suggest to stop training at a very early stage (i.e. after 10 epochs or so). Is it an indicator that more regularization should be applied to the model (I am already using L2 and dropout)? Or maybe the model is overly complex for the specific task I am trying to solve? Here is how the validation cost per epoch looks like (evaluated on the whole validation set) --------------------------------------EDITED-------------------------------------------------- I believe that 10 epochs is a low stage for stoping training since the training accuracy function looks like the picture below. And I know, from previous research, that it is possible to achieve test accuracies of about 70% --------------------------------------- EDITED 2 ----------------------------------------------- The training cost decreases steadily (the color of the line changed but it is the same run). Size of minibatch = 256.
