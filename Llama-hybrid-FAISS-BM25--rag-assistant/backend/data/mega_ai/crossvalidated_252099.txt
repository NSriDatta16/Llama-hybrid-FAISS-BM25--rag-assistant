[site]: crossvalidated
[post_id]: 252099
[parent_id]: 252074
[tags]: 
Yes possible , although you can't always guarantee to exactly start at $100\%$ accuracy on all datasets and on all learning algorithms. But I think your point was not that. I think your point was to start around some extremely high accuracy due to over-fitting, then gradually generalize until you reach your sweet-spot. I think that your logic is correct and you are onto something. In fact, I think this is why some learning algorithms include parameters that allow you to tune how aggressively the resultant models over fits the learning set. For example, if you choose Random Forests , you can start with $\texttt{n_features} = 4,500$ and $\texttt{n_estimators}=1000$ (the latter parameter becomes probably irrelevant as all trees will agree as there is no randomness after choosing all features). This should give you highest classification accuracy on the learning set. You can test this this on the testing set, and notice that its performance is poor on the testing set. Then you can repeat the above but with $\texttt{n_features} = 4,500 - 100 = 4,400$ instead. And test on both learning and testing sets. Keep repeating that many times, each time with a further reduction of $\texttt{n_features}$ until you reach $\texttt{n_features} = 1$. When $\texttt{n_features} = 1$, then you will create a highly random decision forest that is is most likely no longer over-fitting, but rather under-fitting. If you plot that, you should get something like this (classification error = 1 - accuracy): You can also try Extremely-randomized Trees (ET). With this, setting $\texttt{n_features} = 1$ will be way more random (fully random trees), but you will also need to start with much more than $\texttt{n_features} = 4,500$.
