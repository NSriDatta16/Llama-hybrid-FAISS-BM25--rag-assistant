[site]: datascience
[post_id]: 19140
[parent_id]: 
[tags]: 
K-Fold Cross validation confusion?

I am using K-Fold cross validation to test my trained model, but was amazed that for every K-fold the accuracy is different. For instance, if I use 5 K-fold, every fold has a different accuracy. So, which fold should I use? Is averaging all 5 of the folds the best choice? Secondly, why is the data-set split ratio (70/30) different in 5 fold Cross validation and 10 fold cross validation? Shouldn't it be the same?
