[site]: crossvalidated
[post_id]: 600487
[parent_id]: 
[tags]: 
Proof of Coding Theorem for a Single Random Message

Apologies if this has been asked before. I am currently reading the book "A Student's Guide to Coding and Information Theory" by Stefan M. Moser and Po-Ning Chen. It claims that: "The codeword lengths $l_i$ of a Fano code satisfy the following: $$l_i \le \lceil \log_{2}{\frac{1}{p_i}} \rceil$$ where $\lceil \xi \rceil$ denotes the smallest integer not smaller than $\xi$ ." (This is specifically referring to the Shannon-Fano code where the symbols are arranged in order of probability, divided so the probabilities of both sides are as equal as possible, and then recursively doing this for the smaller blocks.) This is used as a lemma in order to prove that for a Fano code, the average codeword length $L_{av}$ satisfies $H(U) \text{ bits} \le L_{av} \lt H(U) + 1 \text{ bits}$ where H is the entropy and U is an r-ary random message. However, I think I have found a counterexample to the lemma. If the symbols have probabilities 0.4, 0.25, 0.05, (0.04) x 7 and 0.02, then the symbol with probability 0.25 should have codeword length 3, contrary to the lemma's claim that it should be less than or equal to 2. In this case, the final result about $L_{av}$ still holds, however. Is there an alternate proof of the result, or is my construction incorrect?
