[site]: datascience
[post_id]: 9402
[parent_id]: 9085
[tags]: 
To robustify your estimator, you might model your ratings as a Gaussian mixture model (GMM) that is a mixture of two Gaussian rvs: 1) true ratings, 2) junk rating that are equal to one. Scikit-learn already has a canned GMM classifier: http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_classifier.html#example-mixture-plot-gmm-classifier-py Digging in a little more, a simple approach would be to let scikit-learn partition your ratings into two gaussians. If one of the partitions ends up with a mean near one, then we can throw out those ratings. Or, more elegantly, we can take the mean of the other, non-near-one Gaussian, as the true rating mean. Here is a bit of code for a ipython notebook that does this: from sklearn.mixture import GMM import numpy as np %matplotlib inline import matplotlib.pyplot as plt import collections def make_ratings(mean,std,rating_cnt): rating_sample = np.random.randn(rating_cnt)*std + mean return np.clip(rating_sample,1,5).astype(int) def make_collection(true_mean,true_std,true_cnt,junk_count): true_ratings = make_ratings(true_mean,true_std,true_cnt) junk_ratings = make_ratings(1,0,junk_count) return np.hstack([true_ratings,junk_ratings])[:,np.newaxis] def robust_mean(X, th = 2.5, agg_th=2.5, default_agg=np.mean): classifier = GMM(n_components=2) classifier.fit(X) if np.min(classifier.means_) > th or default_agg(X) The output for one run looks like: vars = [ 0.22386589 0.56931527] means = [ 1.32310978 4.00603523] mean = 2.9 median = 3.0 robust mean = 4.00603523034 true mean = 4.2 prob(rating=1|class) = [ 9.99596493e-01 4.03507425e-04] prob(rating=true_mean|class) = [ 1.08366762e-08 9.99999989e-01] prediction: [1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0] We can simulate how will this works with a few monte carlo trials: true_means = np.arange(1.5,4.5,.2) true_ratings = 40 junk_ratings = 10 true_std = 1 m_out = [] m_in = [] m_reg = [] runs = 40 for m in true_means: Xs = [make_collection(m,true_std,true_ratings,junk_ratings) for x in range(runs)] m_in.append([[m]*runs]) m_out.append([[robust_mean(X, th = 2.5, agg_th=2,default_agg=np.mean) for X in Xs]]) m_reg.append([[np.mean(X) for X in Xs]]) m_in = np.array(m_in).T[:,0,:] m_out = np.array(m_out).T[:,0,:] m_reg = np.array(m_reg).T[:,0,:] plt.plot(m_in,m_out,'b.',alpha=.25) plt.plot(m_in,m_reg,'r.',alpha=.25) plt.plot(np.arange(0,5,.1),np.arange(0,5,.1),'k.') plt.xlim([0,5]) plt.ylim([0,5]) plt.xlabel('true mean') plt.ylabel('predicted mean') plt.title("true_ratings=" + str(true_ratings) + "; junk_ratings=" + str(junk_ratings) + "; std="+str(true_std)) The output is pasted below. The red is the mean rating and the blue is the proposed rating. You can tweak the parameters to get slightly different behaviors.
