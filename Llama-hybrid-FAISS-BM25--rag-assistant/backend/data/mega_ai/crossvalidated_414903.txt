[site]: crossvalidated
[post_id]: 414903
[parent_id]: 225573
[tags]: 
First let me be clear the terms used in the question as I understand. We normally start with one training dataset, use k-fold cross validation to test different models (or sets of hyperparameters), and select the best model with lowest CV error. So the 'cross-validation estimate of test error' means using the lowest CV error as test error, not just a random model's CV error (which the case discussed by cbeleites , but it is not what we normally do.). The 'actual test error' in question is error we get when applying the best CV model to infinite test dataset, assuming we can get that. CV error is dependent on the particular dataset we have, and actual test error is dependent on best CV model selected , which is also dependent on the training dataset. So the difference between the CV error and test error is dependent on different training datasets. Then the question become , if we repeat above process many times with different training datasets and average the two errors respectively , why the average CV error is lower than average test error, ie CV error is biased downward? But before that , does this always happen? Normally it is impossible get many training datasets and test dataset containing infinite rows. But it is possible to do so using data generated by simulation. In " chapter 7 Model Assessment and Selection" of the book "The Elements of Statistical Learning" by Trevor Hastie, et al. , it includes such simulation experiment. The conclusion is that, using CV or bootstrap, "... estimation of test error for a particular training set is not easy in general, given just the data from that same training set". By 'not easy', they mean the CV error could be either underestimate or overestimate the true test error depending on different training data sets, ie variance caused by different training datasets is pretty big. How about bias? The kNN and linear model they tested are almost not biased: CV error overestimate the true test error by 0-4%, but some models "like trees, cross-validation and boot-strap can underestimate the true error by 10%, because the search for best tree is strongly affected by the validation set". To sum up, for a particular training dataset, the CV error could be higher or lower than the true test error. For the bias, expected CV error could range from a little bit higher to much lower than expected true test error depending on the modeling methods. The reason for the underestimation, as mentioned above, is that selection of hyperparameters for best model is ultimately dependent on the particular training dataset we get. The information in validation dataset could somehow flow into model fitting process. On the other hand, CV error could also a little bit overestimates true test error, as discussed by cbeleites. This is because k fold CV error is obtained by using a little bit less training data to train the model (for 10 fold cv, use 90% data), it is biased upward against true error, but not much. So there are two biases going different directions. For modeling method tends overfit,using less fold CV, eg 5-fold vs 10-fold,might result in less bias. All being said, it does not help too much in practice: we usually only get one 'particular' dataset. if we hold out 15% to 30% as test data ,and select best model by CV on the rest as training data, chances are CV error will be different from test error as both differ from expected test error. We might be suspicious if the CV error is much lower than test error, but we will not know which one is closer to the true test error. The best practice might be just to present both metrics.
