[site]: crossvalidated
[post_id]: 626327
[parent_id]: 
[tags]: 
The role of variance of the distribution plays in Bayesian inference

Given prior $ \mu \sim \mathcal{N}(\mu_0, \tau^2) $ , likelihood $ X_i | \mu \sim \mathcal{N}(\mu, \sigma^2) $ , we know the closed-form solution of posterior is $ \mu | X_1, X_2, \ldots, X_n \sim \mathcal{N}(\mu_n, \tau_n^2) $ with $ \frac{1}{\tau_n^2} = \frac{1}{\tau^2} + \frac{n}{\sigma^2} $ and $ \mu_n = \frac{\frac{1}{\tau^2} \mu_0 + \frac{n}{\sigma^2} \bar{X}}{\frac{1}{\tau^2} + \frac{n}{\sigma^2}} $ . It can be observed that given the same number of observations $n$ ,the larger the $\sigma$ is, the larger the variance in the posterior is. I find it hard to get an intuition of this, because I feel fitting the same number of observations (i.e., MLE) should have the same level of certainty regardless of how spread out the observations are. In other words, I feel it is easy to understand as the number of observations gets large, the uncertainty in posterior diminishes, but the uncertainty should be independent of the sample variance (obviously I am wrong but I want to have the intuition why that is the case).
