[site]: crossvalidated
[post_id]: 352800
[parent_id]: 352688
[tags]: 
We used to have a phrase in chemistry: " Two weeks spent in the lab can save you two hours on Scifinder ". I'm sure the same applies to machine learning: " Two weeks spent training a neuralnet can save you 2 hours looking at the input data ". These are the things I'd go through before starting any ML process. Plot out the density of every (continuous) variable. How are the numbers skewed? Do I need a log transform to make the data make sense? How far away are the outliers? Are there any values that do not make physical or logical sense? Keep an eye out for NAs. Usually, you can just discard them, but if there are a lot of them, or if they represent a crucial aspect to the behaviour of the system, you might have to find a way of recreating the data. This could be a project in and of itself. Plot every variable against the response variable. How much sense can you make out of it just by eyeballing it? Are there obvious curves that can be fitted with functions? Assess whether or not you need a complicated ML model in the first place. Sometimes linear regression is all you really need. Even if it isn't, it provides a good baseline fit for your ML model to improve upon. Beyond those basic steps, I wouldn't spend much additional time looking at the data before applying ML processes to it. If you already have a large number of variables, complicated nonlinear combinations of them get increasingly difficult not only to find, but to plot and understand. This is the sort of stuff best handled by the computer.
