[site]: crossvalidated
[post_id]: 215715
[parent_id]: 215641
[tags]: 
It seems to me this question is more about " why do we keep the support vectors (because after all what we want is the decision boundary) " than " what are support vectors for ". AFAIK it's because SVMs are often used together with kernels. Without kernels, it is sufficient to store only the decision boundary $wx+b=0$, and throw away the support vectors. Kernels can be thought of as mapping the input to an implicit feature space, of which the dimensionality can be infinite (e.g. for RBF kernels). So using the decision boundary explicitly in such high dimensional space would be inefficient (or impossible). If we decompose the decision boundary as a function of some support vectors (as shown in Daneel Olivaw's answer) then the computation would depend only on the number of support vectors and the kernel function. If fact if we don't use kernels, NOT to keep support vectors is more efficient in both time and space. Say the dimension of the data is $m$ and the number of support vectors is $k$, we need $O(km)$ space to store $k$ vectors, and the time complexity for inference is also $O(km)$ because we need the inner product between the input vector and all the support vectors. Well if we only keep the parameters, the time and space complexity is both $O(m)$.
