[site]: datascience
[post_id]: 121927
[parent_id]: 121895
[tags]: 
If you want to compare your different models it is essential to have appropriate evaluation techniques and to perform the same method on all models to make them comparable. In your scenario, the approach you mentioned evaluating using the k-fold cross-validation is definitely appropriate. However, keep in mind that it would be even better to still have a separate test set (not part of the cross-validation ) to have a final evaluation. A slightly optimized approach would be: If possible, split your data set into a training, validation, and test set. Train your three different models ( linear regression with all features, linear regression with correlated features and linear regression with PCA features). Evaluate the performance on the k-fold cross-validation of the validation set. Evaluate the performance on the new, unseen test set. That way, you have two ways of comparing the approaches.
