[site]: crossvalidated
[post_id]: 475480
[parent_id]: 475303
[tags]: 
Consistent estimators $(\hat{x}_1, \hat{x}_2)$ for $(\overline{x}_1^*, \overline{x}_2^*)$ are given by $$\textrm{Choose } \hat{x}_1, \hat{x}_2 \textrm{ to maximise } \left(\underline{x} - \hat{x}_1\right)\left(\underline{x} - \hat{x}_2\right),$$ $$\textrm{subject to } \hat{x}_1 \hat{x}_1, x_{2i} > \hat{x}_2, y_i = 0\right\}\vert = 0.$$ That is, if you plotted the $(x_{1i}, x_{2i})$ points corresponding to $y_i = 0$ inside the rectangle $\left[\overline{x}, \underline{x}\right]^2$ , you choose $\hat{x}_1, \hat{x}_2$ to make the empty rectangle in the top-right (with $x_1 > \hat{x}_1$ and $x_2 > \hat{x}_2$ ) as large as possible. The estimator I gave above is consistent because it converges to the true values $(\overline{x}_1^*, \overline{x}_2^*)$ . Let $\epsilon > 0$ . I will show that $P\left((\hat{x}_1, \hat{x}_2) \in [\overline{x}_1^* - \epsilon, \overline{x}_1^*] \times [\overline{x}_2^* - \epsilon, \overline{x}_2^*]\right) \rightarrow 1$ as $n \rightarrow \infty$ . In order to get $(\hat{x}_1, \hat{x}_2) \in [\overline{x}_1^* - \epsilon, \overline{x}_1^*] \times [\overline{x}_2^* - \epsilon, \overline{x}_2^*]$ we need to have at least one point $(x_{1i}, x_{2i}) \in (\overline{x}_1^* - \epsilon, \underline{x}] \times (\overline{x}_2^* - \epsilon, \overline{x}_2^*)$ with $y_i = 0$ , and at least one point $(x_{1i}, x_{2i}) \in (\overline{x}_1^* - \epsilon, \overline{x}_1^*) \times (\overline{x}_2^* - \epsilon, \underline{x}]$ with $y_i = 0$ . I will go through the first case: define $R = (\overline{x}_1^* - \epsilon, \underline{x}] \times (\overline{x}_2^* - \epsilon, \overline{x}_2^*)$ . The area of $R$ is $(\underline{x} - \overline{x}_1^* + \epsilon)\epsilon > 0$ , and $P(x, y) in $R$ due to the strict monotonicity of $P$ outside $[\overline{x}_1^*, \underline{x}] \times [\overline{x}_2^*, \underline{x}]$ . So $P(Y = 0) > 0$ for $(X_1, X_2) \in R$ . So for a random sample $(Y, X_{1}, X_{2})$ , $P(((X_{1}, X_{2}) \in R) \wedge (Y = 0)) > 0$ . Let this probability be called $\zeta$ . The probability that after $n$ samples we have at least one point $(X_1, X_2) \in R$ with $Y = 0$ is $1 - (1 - \zeta)^n \rightarrow 1$ as $n \rightarrow \infty$ . The second case is similar. Hence our estimator is consistent. In change-point analysis you generally either have a parametric model for both regimes or for neither. In this case, if we knew something about $P(x_1, x_2)$ , e.g. $P(x_1, x_2) = c_1x_1 + x_2x_2$ , capped above at $1$ and below at $0$ , we could probably make an estimator that converged much faster, and made use of the $y = 1$ observations (since they are just thrown away with my estimator). My approach was inspired by the solution to the problem of finding the maximum likelihood estimator for $\theta$ given data $X_i \sim \textrm{Uniform}(0, \theta)$ . In that case the MLE is the maximum of the observed data. The likelihood of an estimator $\hat{\theta}$ there is $0$ if there is ever an observation $X_i > \hat{\theta}$ , and in your puzzle, if we ever observe a sample with $Y_i = 0$ then we know that $\overline{x}_1^* > X_1, \overline{x}_2^* > X_2$ . Another feature of this problem is that you keep observing data near to the change-points. If you imagine detecting a change-point in a time series $X_1, \dots, X_n$ , so there is some unknown time $1 when the data distribution changes, the estimator will not necessarily be consistent. This is because more data $X_{-1}, X_{-2}, \dots$ or $X_{n +1}, X_{n+2}, \dots$ , does tell you more about the parameters before and after the change, but you get no new data near the change point, which is the most useful data. If you keep getting fresh data near the change point then an estimator for it will be consistent.
