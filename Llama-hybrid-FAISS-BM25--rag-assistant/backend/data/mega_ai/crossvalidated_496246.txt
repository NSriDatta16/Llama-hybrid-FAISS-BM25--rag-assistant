[site]: crossvalidated
[post_id]: 496246
[parent_id]: 492923
[tags]: 
ERM is concerned with risk that are expressible as functions. You can just as well use the negative log-likelihood as the risk function. In those cases, MLE is equal to ERM. Now, ERM does not limits itself to that particular class of risk functions. A very successful example of ERM in machine learning is the SVM. The hinge loss in SVMs is not a negative log-likelihood (even though there are SVM reformulations which define negative log-likelihoods). Instead it's a discriminative loss, a convex approximation to the 0-1 loss (accuracy).
