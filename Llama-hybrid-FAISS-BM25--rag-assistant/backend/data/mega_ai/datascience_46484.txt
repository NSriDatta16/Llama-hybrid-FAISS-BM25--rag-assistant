[site]: datascience
[post_id]: 46484
[parent_id]: 29019
[tags]: 
why we actually need two matrices (and not one) for these models. Couldn't we use the same one for U and V? In principle, you are right, we can. But we don't want to, since the increase in the number of parameters is beneficial in practice, but how about the meaning of the vectors? What the papers say? Word2vec : there is only one mention of input/output vectors just to introduce the variables. GloVe : "the distinction between a word and a context word is arbitrary and that we are free to exchange the two roles". Weak justification: Vector pairs (word, context), also named (target, source) or (output, input), are used to reflect two different roles that a single word would play. However, since a word would play both "target" and "context" roles in the same sentence the justification is weak. For example, in sentence "I love deep learning", upon $P(deep | learning)$ , $deep$ is a target, but next upon $P(learning | deep)$ , it is a context, despite the fact that $deep$ has the same semantics in the whole sentence. We can conclude that this separation is not backed by the data. So in the case of word embedding, this separation is just a story for the performance boost we get by increasing the number of parameters, nothing deeper whatsoever. Solid justification: However, in a task like node embedding in directed networks, the justification is solid because the roles are reflected at data level . For example, for a link $(a, b)$ , $a$ is the source which is semantically different than being at the receiving end of the link $b$ . A source node never plays the "target" role unless it is a target in another link. Thus you can expect to have a meaningful separation of semantics by using target (source) representations for target (source) nodes. This justification fails for undirected graphs the same as word embedding. In some papers, authors even opt for four representations based on additional roles that an entity plays at data level, which is semantically justifiable and further boosts the performance. Conclusion: The whole premise is "if increase in the number of parameters pays off, do it. If it is semantically justifiable, even better."
