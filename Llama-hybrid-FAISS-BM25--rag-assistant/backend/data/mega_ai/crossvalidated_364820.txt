[site]: crossvalidated
[post_id]: 364820
[parent_id]: 
[tags]: 
Drop-out as a multiplicative noise in deep neural networks

I am reading Ian Goodfellow's deep learning book, and I cannot understand the following lines: Another important aspect of dropout is that the noise is multiplicative. If the noise were additive with fixed scale, then a rectified linear hidden unit with added noise could simply learn to have become very large in order to make the added noise insignificant by comparison. Multiplicative noise does not allow such a pathological solution to the noise robustness problem. I dont understand what does it mean by "a rectified linear hidden unit with added noise could simply learn to have become very large in order to make the added noise insignificant by comparison. Multiplicative noise does not allow such a pathological solution to the noise robustness problem"?
