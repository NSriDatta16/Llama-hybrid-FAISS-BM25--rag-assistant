[site]: datascience
[post_id]: 78524
[parent_id]: 78513
[tags]: 
First of all though SGD gives frequent updates which help improve performance of the model but it can result in noisy result and frequent updates is computationally expensive for large dataset. Batch gradient has less update frequency which result is stable error gradient but as it stores whole training data the learning process is slow(when data is large). Mini-batch gradient descent seeks to find a balance between the robustness of stochastic gradient descent and the efficiency of batch gradient descent. Mini batch GD allows to not have all training data in memory and can be performed in distributed manner. For more info you can check chapter 8 of Deep Learning book by Ian Goodfellow.
