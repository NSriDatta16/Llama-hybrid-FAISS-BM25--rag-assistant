[site]: crossvalidated
[post_id]: 386704
[parent_id]: 385898
[tags]: 
If you are comparing/selecting from 4 models that were all set up according to your knowledge of the application at hand as good and sensible candidates for a good model, there's really nothing that will guarantee different performance. After all, there can be various models that reach Bayes error for a given application (which doesn't mean you got there, there are more possibilities that can lead to equal or very similar predictive performance of models of different type). However, what made me write this late answer is: I'd suggest looking a bit more into that relation of $MSE_{CV}$ (at least in my field, $MSE_P$ is reserved for test set predictions) vs $k$ . CV error really should not increase systematically with k, and standard deviation across iterations (as opposed to std across folds ) measures instability of your models (or more precisely, their predictions). The fewer actual training cases available for surrogate model training may lead to worse models for small $k$ , which may be more unstable and/or worse on average. However, you observe the opposite. This could be a symptom of some programming mistake , and programming mistakes when calculating model perforance can cause the whole verification/selection/optimization to go wrong. (I'm speaking as someone who once retractred an already submitted manuscript after finding a tiny single character typo that created a data leak and caused severe optimistic bias in cross validation results).
