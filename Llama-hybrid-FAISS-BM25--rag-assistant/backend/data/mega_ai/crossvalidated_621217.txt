[site]: crossvalidated
[post_id]: 621217
[parent_id]: 
[tags]: 
What is going on? Contradictory results on the variance of random vector with random mean and covariance

Suppose $f\mid\mu, F\sim N(\mu, F) \in \mathbb{R}^n$ , where $\mu , F$ are both random (random vectors and random matrix respectively). What is the correct way to derive $Var(f)$ ? First, let $\tilde{f} = f - \mu$ , I obtain: \begin{align*} E[ff^T] &= E[(\tilde{f} + \mu)(\tilde{f} + \mu)^T]\\ &= E[\tilde{f}\tilde{f}^T + \tilde{f} \mu^T + \mu \tilde{f}^T + \mu\mu^T]\\ &= E[E[\tilde{f}\tilde{f}^T \mid \mu]] + 0 + E[\mu\mu^T]\\ &= E[F] + E[\mu\mu^T]\\ & = E[F] + Var[\mu] +E[\mu]E[\mu]^T\\ \end{align*} Approach 1: \begin{align*} Var(f) &= E[ff^T] - E[f]E[f]^T\\ &= E[ff^T] - E[E[f\mid\mu]] \quad E[E[f\mid\mu]]^T\\ &=E[ff^T] - E[\mu]E[\mu]^T\\ &= E[F] + Var(\mu) \end{align*} Approach 2: \begin{align*} Var(f) &= E[(f-\mu)(f-\mu)^T]\\ &= E[ff^T - f\mu^T - \mu f^T + \mu \mu^T]\\ &= E[ff^T] - E[E[f\mu^T\mid\mu]] - E[E[\mu f^T\mid\mu]] + E[\mu \mu^T]\\ &=E[ff^T] - E[\mu \mu^T]\\ &= E[ff^T] - Var(\mu) - E[\mu]E[\mu]^T\\ &= E[F] \end{align*} Approach 3: \begin{align*} Var(f) &= E[Var(f\mid F)] + Var(E[f\mid F])\\ &= E[F] + Var(\mu)\\ \end{align*} This type of model is commonly used in Bayesian statistics, which one is the correct derivation? I feel like Approach 2 is since Appraoch 1 & 3 seems to miss one more expectation outside. Similar confusion when deriving the covariance: Approach 1: \begin{align*} Cov(a^T f, b^T f) &= E[a^Tff^T b] - E[a^Tf]E[b^Tf]\\ & = a^T E[ff^T] b - a^T E[f]E[f]^T b\\ &= a^T E[ff^T] b - a^T E[E[f\mid\mu]]E[E[f\mid\mu]]^T b\\ &= a^T E[ff^T] b - a^T E[\mu]E[\mu]^T b\\ &= a^T (E[F] + Var(\mu)) b\\ \end{align*} Approach 2: \begin{align*} Cov(a^T f, b^T f) &= E[(a^Tf - E[a^T f]) (b^T f - E[b^T f])^T]\\ & = E[(a^Tf - a^T \mu) (b^T f - b^T \mu]T]\\ &= E[a^T(f-\mu)(f-\mu)^T b]\\ &= a^T E[E[ (f-\mu)(f-\mu)^T\mid F]] b\\ &= a^T E[F] b \end{align*}
