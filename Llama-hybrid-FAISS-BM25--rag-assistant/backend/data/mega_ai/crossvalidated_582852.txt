[site]: crossvalidated
[post_id]: 582852
[parent_id]: 582842
[tags]: 
It is quite common to assess statistical significance via resampling, bootstrap or permutation tests, all of which are randomized (see, e.g., the textbook by Good, 2006 ). In such cases, people will often run their procedure just a single time and report the resulting p value. More sophisticated modelers will re-run their analysis a few times to see how stable the p values are. (Unscrupulous modelers will re-run them until they get a low p value.) Running the analysis multiple times and averaging the resulting p values seems to me to be an excellent way of getting more stable p values. Essentially, this treats the p value as an estimate of an underlying test statistic itself (which it is!), and harnesses the power of averaging. The relevant criticisms of p value averaging do not apply. (If anyone disagrees, I would be interested in their argument.)
