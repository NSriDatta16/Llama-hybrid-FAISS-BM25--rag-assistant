[site]: crossvalidated
[post_id]: 567085
[parent_id]: 
[tags]: 
Solve Local Minima Problem through Averaging

I am using neural networks (created in R: neuralnet ) to predict county-level food insecurity. I want to use Olden's connection weight approach to analyze relative variable importance ( NeuralNetTools::olden ). However, the relative variable importance metrics vary quite wildly from trial to trial, due to the existence of local minima. Therefore, I am solving the problem through averaging. In other words, I am creating the network 1000 times, randomizing the train/test splits each time, and calculating the olden metrics as well as MSE on each trial, then averaging them. I was wondering if this was a suitable approach, or if there is more justification to arrive at relative variable importance through another method, such as choosing the variable importance metrics from the single trial with the lowest MSE. For reference, I am justifying my chosen method with the following quotation from the literature: "The first approach involves combining different local minima...by averaging the outputs of networks using the connection weights corresponding to different local minima...This approach would also presumably involve averaging the contributions of input variables" Here is my code for reference. This code only creates the neural network 10 times, I don't want to waste any of your afternoons. RVI stands for relative variable importance. library(neuralnet) library(NeuralNetTools) sctg_1 $net.result*(max(sctg_data$ foodinsecurity)-min(sctg_data $foodinsecurity))+min(sctg_data$ foodinsecurity) test.r $foodinsecurity)*(max(sctg_data$ foodinsecurity)-min(sctg_data $foodinsecurity))+min(sctg_data$ foodinsecurity) MSE.nn $V1, df1$ V2, df1 $V3, df1$ V4, df1 $V5, df1$ V6, df1 $V7, df1$ V8) colnames(m1) $sctg_1), mean(m1$ sctg_2), mean(m1 $sctg_3), mean(m1$ sctg_4), mean(m1 $sctg_5), mean(m1$ sctg_6),mean(m1 $sctg_7)) colnames(means1) MSE),] means1 best1 ```
