[site]: datascience
[post_id]: 27179
[parent_id]: 
[tags]: 
Several fundamental questions about CNN

I am trying to make a CNN for 3D image recognition but everything is predicted to only one class out of three. And the prediction even quickly converges during the first epoch. I have been working on this for an week and totally lost. I have my own several guess why it always converges to one class. My 3D tensor is huge as 40x35x30 and there are a lot of white spaces because I had to put my objects in cubic box. Would this be problematic? I have only ~5000 samples for training and ~500 for test sets. Do I have too little samples? My labels have 3 classes and ~75% of the whole data belong to the class number 1. During the first epoch, my model quickly converges to predict 99% of data as this class. Would this unbalanced data cause the problem? My model is designed as conv1(kernel_size=10,stride=1,filter=32)-batch_norm1-maxpool1-conv2(8,1,64)-batch_norm2-maxpool2-conv3(8,1,64)-maxpool3-fc4(1024)-batchnorm4-dropout4(20%)-fc5(384)-batchnorm5-dropout5(20%)-fc6(3) I standardized the input between -1 and 1 I use leaky-relu activation for conv layers. I use Adam optimizer with decay rate of 0.99. First, I am not sure if it is okay to perform batch normalization at every layer. Do I miss any important concept for designing CNN model here? or maybe my data is just bad.. I kind of suspect that my data are not significantly different from each other and there is no pattern at all. In this case, is there any statistical method/model to check if my data have meaningful differences? (The 3D images are some chemical/physical data in 3D space that I converted as numpy matrix) But I think it is more like vanishing problem because when I initialize the variable with Xavier's way, the convergence to the class #1 is slower. Please someone help me :(
