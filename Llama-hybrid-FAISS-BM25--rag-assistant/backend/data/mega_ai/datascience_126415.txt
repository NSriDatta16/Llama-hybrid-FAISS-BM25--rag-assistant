[site]: datascience
[post_id]: 126415
[parent_id]: 
[tags]: 
Building machine learning model with variable targets and on the run

I want to add Machine learning predictive capabilities to an existing application; the data that will be used to train the model is stored in a database. My problem is as follows: in a typical machine learning project, you have a set of features that you use to train the model, and a single column, which is the target. In my case, most columns can be the target; for instance, the user can select "sales." in this case, the "sales" column would be the target, and it won't be used as a feature in training. On the other hand, if the user wants to predict the "Quantity", which is another column, then the "sales" column would be a feature used in training and so on, and once deployed, the user would need to provide the "sales" value to predict the quantity required to achieve this sales amount. How do I approach this, as I have more than 20 columns that can be either a feature or a target? the machine learning model should be customized for each customer; due to data residency requirements, I won't have access to each user's data, so the training of the models shall be done in the customer's premises dynamically, is there any way to ensure the model's performance? I thought about training a CAT boost model or any other model with reasonable defaults on the run, storing it somewhere and using it again whenever needed. However, I have no means of ensuring the model's performance. All that I need is some hints on how to approach this problem so that I can read more into it.
