[site]: crossvalidated
[post_id]: 522232
[parent_id]: 521835
[tags]: 
Probably not your situation, but you can use machine learning as a memory. Suppose you have some inputs where you can train on the entire domain of inputs. That is, there is no possible input that wasn't previously known and used for training. That way, you can always generate the correct output. If the concept you wish to train on can be expressed in this way, then over training is memorising, and in limited situations, can be useful. Otherwise, as the other comments have stated, overfitting means you are biased to the training data and will do poorly on anything not in the training set. The goal is for generalisation, for the model to understand the concept, not memorise. The syllabus answer above explains it well.
