[site]: crossvalidated
[post_id]: 363111
[parent_id]: 363098
[tags]: 
One possibility is that you've simply found a good model. There's no reason a learning algorithm must necessarily overfit as training progresses. One way for things to go wrong is if the validation and training sets are not independent, causing the validation set error to be overoptimistically biased. There are many ways this could happen. For example, preprocessing the entire dataset before splitting it can allow information to improperly leak from the validation set to the training set. Procedures that make use of the target values (e.g. feature selection) are particularly egregious here, but even seemingly benign, unsupervised procedures could have an effect in some cases. Alternatively, the training and validation sets might not be split in a way that respects natural correlations in the data. For example, with time series data, it would be a mistake to independently subsample the validation set points at random. In this case, points in the validation set would not be independent of training points that are nearby in time, due to temporal correlations. Many other dependence structures are possible, and the procedure for splitting the training and validation sets must take this into account.
