laystyle \left(\operatorname {prox} _{\gamma R}(x)\right)_{i}={\begin{cases}x_{i}-\gamma ,&x_{i}>\gamma \\0,&|x_{i}|\leq \gamma \\x_{i}+\gamma ,&x_{i}<-\gamma ,\end{cases}}} which is known as the soft thresholding operator S γ ( x ) = prox γ ‖ ⋅ ‖ 1 ⁡ ( x ) {\displaystyle S_{\gamma }(x)=\operatorname {prox} _{\gamma \|\cdot \|_{1}}(x)} . Fixed point iterative schemes To finally solve the lasso problem we consider the fixed point equation shown earlier: x ∗ = prox γ R ⁡ ( x ∗ − γ ∇ F ( x ∗ ) ) . {\displaystyle x^{*}=\operatorname {prox} _{\gamma R}\left(x^{*}-\gamma \nabla F(x^{*})\right).} Given that we have computed the form of the proximity operator explicitly, then we can define a standard fixed point iteration procedure. Namely, fix some initial w 0 ∈ R d {\displaystyle w^{0}\in \mathbb {R} ^{d}} , and for k = 1 , 2 , … {\displaystyle k=1,2,\ldots } define w k + 1 = S γ ( w k − γ ∇ F ( w k ) ) . {\displaystyle w^{k+1}=S_{\gamma }\left(w^{k}-\gamma \nabla F\left(w^{k}\right)\right).} Note here the effective trade-off between the empirical error term F ( w ) {\displaystyle F(w)} and the regularization penalty R ( w ) {\displaystyle R(w)} . This fixed point method has decoupled the effect of the two different convex functions which comprise the objective function into a gradient descent step ( w k − γ ∇ F ( w k ) {\displaystyle w^{k}-\gamma \nabla F\left(w^{k}\right)} ) and a soft thresholding step (via S γ {\displaystyle S_{\gamma }} ). Convergence of this fixed point scheme is well-studied in the literature and is guaranteed under appropriate choice of step size γ {\displaystyle \gamma } and loss function (such as the square loss taken here). Accelerated methods were introduced by Nesterov in 1983 which improve the rate of convergence under certain regularity assumptions on F {\displaystyle F} . Such methods have been studied extensively in previous years. For more general learning problems where the proximity operator cannot be computed explicitly for some regularization term R {\displaystyle R} , such fixed point schemes can still be carried out using approximations to both the gradient and the proximity operator. Practical considerations There have been numerous developments within the past decade in convex optimization techniques which have influenced the application of proximal gradient methods in statistical learning theory. Here we survey a few important topics which can greatly improve practical algorithmic performance of these methods. Adaptive step size In the fixed point iteration scheme w k + 1 = prox γ R ⁡ ( w k − γ ∇ F ( w k ) ) , {\displaystyle w^{k+1}=\operatorname {prox} _{\gamma R}\left(w^{k}-\gamma \nabla F\left(w^{k}\right)\right),} one can allow variable step size γ k {\displaystyle \gamma _{k}} instead of a constant γ {\displaystyle \gamma } . Numerous adaptive step size schemes have been proposed throughout the literature. Applications of these schemes suggest that these can offer substantial improvement in number of iterations required for fixed point convergence. Elastic net (mixed norm regularization) Elastic net regularization offers an alternative to pure ℓ 1 {\displaystyle \ell _{1}} regularization. The problem of lasso ( ℓ 1 {\displaystyle \ell _{1}} ) regularization involves the penalty term R ( w ) = ‖ w ‖ 1 {\displaystyle R(w)=\|w\|_{1}} , which is not strictly convex. Hence, solutions to min w F ( w ) + R ( w ) , {\displaystyle \min _{w}F(w)+R(w),} where F {\displaystyle F} is some empirical loss function, need not be unique. This is often avoided by the inclusion of an additional strictly convex term, such as an ℓ 2 {\displaystyle \ell _{2}} norm regularization penalty. For example, one can consider the problem min w ∈ R d 1 n ∑ i = 1 n ( y i − ⟨ w , x i ⟩ ) 2 + λ ( ( 1 − μ ) ‖ w ‖ 1 + μ ‖ w ‖ 2 2 ) , {\displaystyle \min _{w\in \mathbb {R} ^{d}}{\frac {1}{n}}\sum _{i=1}^{n}(y_{i}-\langle w,x_{i}\rangle )^{2}+\lambda \left((1-\mu )\|w\|_{1}+\mu \|w\|_{2}^{2}\right),} where x i ∈ R d and y i