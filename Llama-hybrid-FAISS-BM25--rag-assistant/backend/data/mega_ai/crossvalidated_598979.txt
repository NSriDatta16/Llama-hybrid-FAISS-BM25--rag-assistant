[site]: crossvalidated
[post_id]: 598979
[parent_id]: 597552
[tags]: 
If different cross-validation schemes make a statistically significant difference, then your dataset is probably too small to draw any useful conclusion. Having lots of cross-validation procedures and taking the best result over models and cross-validation methods is a recipe for over-fitting, in this case over-fitting the model selection criterion (by having more than one to choose from). These sorts of biases can be more substantial than might be expected. See my paper (with Mrs Marsupial) GC Cawley, NLC Talbot, On over-fitting in model selection and subsequent selection bias in performance evaluation, The Journal of Machine Learning Research 11, 2079-2107 ( pdf ) This is essentially because performance estimates have variance - if you try it again and again using different samples of data, you will get slightly different results. So some of the difference in apparent performance between cross-validation estimates is due to the sampling variation favouring one method a little more than it does another. So choosing the best estimator can exploit this variation, rather than genuinely indicating a better model, which would be over-fitting in model selection. This may be a substantial problem unless you have lots of data. Also, as well as variance of the estimator being an issue cross-validation methods can be pessimistically biased. Most classifiers perform better the more training data you have. For leave-one-out CV, you get an almost unbiased as the training set in each fold is almost as large as the full dataset. If you use 5-fold cross-validation, the training set in each fold is only about 80% of the full dataset, so on average the classifier is expected to perform a little worse than it would appear using leave-one-out cross-validation. If you just want the best model, estimate peformance using some form of bootstrap , and use as many resamples as you can within your computational budget. Better still, use bagging to combine the classifiers from each resampling, and use the out-of-bag estimate to estimate the performance of the bagged model.
