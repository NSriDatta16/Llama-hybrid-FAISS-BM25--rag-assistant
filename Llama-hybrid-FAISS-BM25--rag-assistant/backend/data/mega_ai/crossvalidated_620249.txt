[site]: crossvalidated
[post_id]: 620249
[parent_id]: 
[tags]: 
Intuitive understanding of Markov Chains

I am looking for an intuitive understanding of Markov chains. As far as I understood, barring some edge cases, the probability of arriving at state j when starting from state i converges to a specific value after n timesteps, where n tends to infinity . What do these n timesteps mean? Does it mean after n timesteps, someone can confidently say what the probability of arriving at state j when starting from state i is without performing any computation? Why couldn't they just say that from n=0 ? Major edit - I realized that I did not clarify n to be infinite. I am basically confused with the meaning of “steady state ” probability. Does it mean that your probability of arriving at some state is fixed given that you start now and traverse the chain for n time steps, where n is huge? For simplicity we can assume that it’s a single recurrent system that doesn’t depend on initial state.
