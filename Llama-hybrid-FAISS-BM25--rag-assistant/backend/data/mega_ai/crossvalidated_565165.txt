[site]: crossvalidated
[post_id]: 565165
[parent_id]: 
[tags]: 
Estimated Optimal Policy vs True Optimal Policy

In an MDP / in Reinforcement Learning, the optimal policy is often defined as something similar to: a policy that maximizes the value of all states at the same time . But I am not sure whether this refers to the "actual" value of the states, or the agent's "estimated" value of the states. For example, an agent could explore an environment and estimate the values of certain states, using the Bellman equation. But the actual values may be different to these estimates, and the estimates will only converge to the actual values with sufficient exploration and sufficient optimisation of the MDP. In the case that the estimated values are not the same as the true values, and hence, the estimated optimal policy is not (necessarily) the same as the true optimal policy, do we still call the estimated optimal policy the optimal policy ? If not, what do we call this policy? Is it the greedy policy ?
