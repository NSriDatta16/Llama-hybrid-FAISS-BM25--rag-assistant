[site]: crossvalidated
[post_id]: 269370
[parent_id]: 269265
[tags]: 
This is answered in the Sutton page you link to, emphasis mine: Recall that a policy, $\pi$, is a mapping from each state, $s$, and action, $a$, to the probability of taking action $a$ when in state $s$. Informally, the value of a state $s$ under a policy $\pi$, denoted $V^{\pi}(s)$, is the expected return when starting in $s$ and following $\pi$ thereafter . Expressing the expectation isn't that simple, because subsequent rewards depend on subsequent transitions from state to state. This is why Sutton writes, "A fundamental property of value functions used throughout reinforcement learning and dynamic programming is that they satisfy particular recursive relationships." Check equation 3.10 on that same page for an expression that expresses this relationship.
