[site]: crossvalidated
[post_id]: 310893
[parent_id]: 309268
[tags]: 
The answer is no , because bias and variance are attributes of model parameters, rather than the data used to estimate them. There is a partial exception to that statement that pertains to bias and variance varying (ha!) through the predictor space; more on that below. Note that this has absolutely nothing to do with knowing some "true" function relating the predictors and response variables. Consider the estimate of $β$ in a linear regression, $\hatβ=(X^TX)^{-1}X^TY$, where $X$ is an $N×P$ matrix of predictors, $\hatβ$ is a $P×1$ vector of parameter estimates, and $Y$ is an $N×1$ vector of responses. Let's assume for argument's sake that we have an infinite population of data from which to draw (this is not completely ridiculous, by the way -- if we were actively recording data from some physical process we could record predictor and response data at a rapid rate, thus practically satisfying this assumption). So we draw $N$ observations, each consisting of a single response value and a value for each of the $P$ predictors. We then compute our estimate of $\hatβ$ and record the values. Let us then take this entire process and repeat it $N_{iter}$ times, each time making $N$ independent draws from the population. We will accumulate $N_{iter}$ estimates of $\hatβ$ over which we can compute the variance of each element in the parameter vector. Note that the variance of these parameter estimates is inversely proportional to $N$ and proportional to $P$, assuming orthogonality of the predictors. The bias of each parameter can be estimated similarly. While we may not have access to the "true" function, let's suppose we can make an arbitrarily large number of draws from the population in order to compute $\hatβ_{best}$, which will serve as a proxy for the "true" parameter value. We'll assume that this is an unbiased estimate (ordinary least squares) and that the number of observations used was sufficiently large such that the variance of this estimate is negligible. For each of the $P$ parameters, we compute $\hatβ_{best_j}-\hatβ_j$, where $j$ ranges from $1$ to $N_{iter}$. We take the average of these differences as an estimate of the bias in the corresponding parameter. There are corresponding ways of relating bias and variance to the data itself, but they're a little more complicated. As you can see, bias and variance can be estimated for linear models, but you will require quite a bit of hold-out data. A more insidious problem is the fact that once you start working with a fixed dataset, your analyses will be polluted by your personal variance, in that you'll have already begun wandering through the garden of forking paths and there's no way of knowing how that would replicate out-of-sample (unless you just came up with a single model and ran this analysis and committed to leaving it alone after that). Regarding the matter of the data points themselves, the most correct (and trivial) answer is that if there is any difference between $Y$ and $\hat{Y}$, you need a more complex model (assuming that you could correctly identify all the relevant predictors; you can't). Without going into a boring treatise on the philosophical nature of "error," the bottom line is that there was something going on that caused your model to miss its mark. The problem is that adding complexity increases variance, which will likely cause it to miss the mark on other data points. Therefore, worrying about error attribution at the individual data point level is not likely to be a fruitful endeavor. The exception (mentioned in the first paragraph) stems from the fact that bias and variance are actually functions of the predictors themselves, so you may have large bias in one part of the predictor space and smaller bias in another (same for variance). You could assess this by computing $Y-\hat{Y}$ many times (where $\hat{Y}=X\hatβ$ and $\hatβ$ was not estimated based on $Y$) and plotting its bias (average) and variance as a function of the values of $X$. However, I think that's a pretty specialized concern.
