[site]: stackoverflow
[post_id]: 386855
[parent_id]: 386823
[tags]: 
Not that I know of. However, is there a proven need? Hash tables don't have a large overhead, even for very small N it should be faster just to use the normal hash table than to search linearly. EDIT I don't have a benchmark to prove it but just by comparing the algorithms I conclude that hash tables should be faster than linear search as soon as N > 6 on average (for string keys or similar nontrivial hashes) so there is really no reason for a hybrid implementation. The argument is as follows. In linear search, on average half the elements have to be compared to your input, i.e. N / 2. In a hash table, it is known that the expected number of comparisons is 2, regardless of input size (for very small hash tables with a load factor of less than 0.1 this is actually approaching 1). Additionally, the hash has to be calculated. This results in 3 operations on your input, plus a very small overhead that can be neglected. Thus, we search for which N it is true that 3 > N / 2, which is trivially N > 6. Note that the above calculation is actually wrong because for this small number of elements, the load factor of .NET's Dictionary will be much less than 0.1. The tipping point is therefore actually even lower.
