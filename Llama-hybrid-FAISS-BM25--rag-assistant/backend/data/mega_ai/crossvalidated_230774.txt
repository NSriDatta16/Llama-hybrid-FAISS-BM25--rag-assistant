[site]: crossvalidated
[post_id]: 230774
[parent_id]: 
[tags]: 
Using a time series as covariate in regression model

I have a binary outcome variable (Disease/No disease). In a diagnostic test for this disease, 20 different sensors record a time series value. These time series are relatively correlated, but each sensor measures a different thing. In order to predict the disease, the standard in my field is to average all of the sensors' outputs together and use the maximum value obtained at any one time as a regression covariate. My hypothesis is that the dynamics of the time series are very important, and could be more predictive than using just the maximum value of all the time series averaged together. Can anyone help me conceptualize how I could fit a predictive model to this data? I have looked around for examples of using time series as covariates but I have been unsuccessful.
