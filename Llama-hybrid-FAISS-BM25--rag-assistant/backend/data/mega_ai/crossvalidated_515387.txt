[site]: crossvalidated
[post_id]: 515387
[parent_id]: 
[tags]: 
Can test set performance be too poor?

I've trained a binary classifier (PCA-LDA) on some clinical data for which I have 18 patients, each with ~250 observations. I am performing leave-one-patient-out cross-validation, each left out patient's ~250 observations all from the same class. The training set accuracy is ~55%, which is poor but I was expecting as much. However, the test set accuracy is ~20%. I'd expect a terrible classifier to give 50% accuracy. With 20% I need only to switch the labels and suddenly I've got 80% accuracy, which would be ridiculously good in this application. Are my expectations reasonable and am I right to be suspicious? Any clues as to why this might be happening? I tried leave-two-patients-out CV, ensuring those two patients were different classes, which gave far more reasonable results (test accuracy ~50%), but I can't fathom why this should be so. Thanks for any input.
