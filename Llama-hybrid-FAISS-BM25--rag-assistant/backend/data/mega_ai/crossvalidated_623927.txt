[site]: crossvalidated
[post_id]: 623927
[parent_id]: 
[tags]: 
How to test for significance of differences between metrics for two models? (Machine learning model selection)

Problem - I want to test whether the difference in a metric (say AUC) between two models is significant. I have one vector of binary class predictions from a custom function and one from sklearn.ensemble.GradientBoostingClassifier. What I tried - So far, the closest thing I could find is the 2-sample Kolmogorov-Smirnov (KS-)test. I am not sure whether I should just test for differences between the prediction vectors, or between each prediction vector and true class vector. It does not make sense to me to do so, as I suspect that would rather tell me whether the estimator is biased. Related questions - I also found out that a KS-test is valid on a discrete distribution (see this paper by John Walsh, 1963: https://doi.org/10.1007/BF02865912 ). Similar questions have been asked, but not answered here: Tests for significant difference between prediction errors of machine learning regression models Testing for statistical significance of the true positive detection rate between different machine learning models
