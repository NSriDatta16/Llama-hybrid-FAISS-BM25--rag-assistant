[site]: crossvalidated
[post_id]: 555244
[parent_id]: 
[tags]: 
Learning a confidence interval is there any idea to solve this problem?

I was with my friend trying to figure a generic confidence interval for any real model $f$ (already trained) so we'd like to get: $CI(x)=(L(x,\mathbf{\Theta)}),U(x,\mathbf{Z}))$ Where $L$ and $U$ are the lower and upper confidence limit to $f$ at the observation $x_i \in \Re^p$ . That's we're looking for a confidence interval for the response of any $x$ in feature space. Then we proposed ourselves to minimize $L$ at $\mathbf{\Theta},\mathbf{Z}$ , where $L$ is: $L=\displaystyle\sum_{i=1}^{N}(f(x_i)-L(x_i,\mathbf{\Theta)})^2+\sum_{i=1}^{N}(f(x_i)-U(x_i,\mathbf{Z}))^2+(1-\alpha-P^{*}(\{L(x_i,\mathbf{\Theta})\leq y_i \leq U(x_i,\mathbf{Z})\}_{i=1}^{N}))^2$ Where $P^{*}(.)$ is the sample-proportion of $(.)$ smoothed by a differentiable kernel density. Of course we assumed $L,U$ to be non parametric models such as neural networks, decision trees, gradient boosting and so on. We all know that there are so many packages to fit those non parametric models but note that this problem is not a simple regression fitting so here's my question: Could I fit $L,U$ knowing how to fit $U$ and $L$ ? That's knowing how to minimize $\displaystyle\sum_{i=1}^{N}(f(x_i)-L(x_i,\mathbf{\Theta)})^2$ and $\sum_{i=1}^{N}(f(x_i)-U(x_i,\mathbf{Z}))^2$ ? if so how can I do this? if not what kind of optimization theory would be interesting to try solve this?
