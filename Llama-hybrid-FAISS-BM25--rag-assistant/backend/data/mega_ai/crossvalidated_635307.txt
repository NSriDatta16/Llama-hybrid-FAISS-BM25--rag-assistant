[site]: crossvalidated
[post_id]: 635307
[parent_id]: 545353
[tags]: 
Generally speaking, queues are modelled using the Poisson Process. It is quite common to model the number of arrivals and/or departures after service as Poisson point processes . It isn't strictly required, but yes it is common. Why aren't time series models used to model and simulate the behavior of queues? They are. I don't know where you got the impression that they weren't. But one class of reasons why someone might prefer to simulate a queueing model instead of using a time series regression with a Lindley-type equation is to run scenarios that have never been done for their service network. Clients are often interested in trying out different schedules for their servers and different service disciplines. You don't have the data to train a model in such cases, and the a priori mathematics can be quite difficult, if not infeasible. In practice, couldn't a ARIMA style time series model be used to predict how many people will be waiting in the queue at equidistant time points? You could certainly try... ARIMA wouldn't be my first pick, but sure, you could try. Keep in mind that people have already derived approximations for the stionary cases, and the non-stationary cases are often generalizations of the stationary ones in which certain parameters now depend on time. Is there a particular reason that queueing models like M/M/K models are generally used for these kinds of problems instead of classical time series models? I addressed this above.
