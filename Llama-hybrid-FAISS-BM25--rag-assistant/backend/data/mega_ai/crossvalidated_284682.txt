[site]: crossvalidated
[post_id]: 284682
[parent_id]: 284679
[tags]: 
The short answer is that you generally need to do some kind of gradient descent to train your model, and this will rely on selecting initial conditions. Poor initial conditions will lead to poor convergence results. As an example, consider a single neuron in a neural network $\sigma(ax+b)$ with only one feature, where say $\sigma$ is a ReLu. In this case, if you intialize $a,b$ to be $N(0,1)$, they will have magnitudes on the order of 1. Now imagine your inputs are data with $x$ being around $-100$. Most of your activations will be zero and hence gradient descent becomes impossible. You could remedy this with a leaky ReLu, but then the training would be slow. Whereas if you normalize your data to be commensurate with your initial conditions, you'll have nonzero gradients.
