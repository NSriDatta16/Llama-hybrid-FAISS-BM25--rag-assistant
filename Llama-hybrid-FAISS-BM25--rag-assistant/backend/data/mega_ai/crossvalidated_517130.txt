[site]: crossvalidated
[post_id]: 517130
[parent_id]: 
[tags]: 
What are the methods to find the maximum data size (in python and pandas) that can be handled smoothly by a system before even executing a dataset

I want to know about the various ways to calculate (or approximate) the maximum data size (in python and pandas) that can be handled smoothly by the current processor or the system as a whole before even starting to execute a dataset. Maybe we can assume that we dont run any other applications after the start of our ml or deep learning application. Are there any python packages that can perform this kind of computation? Or maybe another thing is to get the time that most approx. be taken by any algorithm to converge for a dataset of a given size (of the data matrix or the memory size). Or if this issue is directly related to RAM, and if whether or not just the analysis of the amount of free memory and the dataset size would give us an approximation. But there will be some temporary memory required to process data for every algorithm right? How can a person predictably approx that and include it in his calculation to end up with a more reasonable approximation?
