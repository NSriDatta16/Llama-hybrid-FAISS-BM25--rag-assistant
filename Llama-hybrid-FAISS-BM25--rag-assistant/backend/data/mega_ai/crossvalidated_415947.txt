[site]: crossvalidated
[post_id]: 415947
[parent_id]: 
[tags]: 
Why is K-Fold Validation rarely used in Neural Network Publications?

I can't recall a single neural network based paper that uses K-fold Validation. Is there a reason for this? Are dropout and other forms of regularization enough to prevent over fitting and make K-fold unnecessary? What am I missing?
