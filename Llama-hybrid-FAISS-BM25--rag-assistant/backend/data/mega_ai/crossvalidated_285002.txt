[site]: crossvalidated
[post_id]: 285002
[parent_id]: 284204
[tags]: 
Even though you've set the seed to be the same for both runs, changing the order of the predictors means that the random draws for each predictor within BSTS are going to be different between the runs. Note that when Causal Impact (CI) calls BSTS to generate the model, it resets the random seed to 1. So it doesn't matter what the seed is going into the CI call. https://github.com/google/CausalImpact/blob/master/R/impact_model.R#L216 This seed reset prevents you from seeing the random variation that should be appearing in the results from each call of Causal Impact. By default, CI calls BSTS with a parameter of 1000 iterations. If you crank up the iterations in the call to BSTS, the results converge. Half million iterations may be overkill, but does the trick: # order 1 data1 As an aside, you can see the variation in BSTS results directly if you build the BSTS model externally and look at the variation in the estimated coefficient. results_data1 | Min. | 1st Qu.| Median | Mean | 3rd Qu. | Max. | -0.0496 | 0.3055 | 0.3156 | 0.3140 | 0.3260 | 0.3591 | summary(results_data2) | Min. | 1st Qu.| Median | Mean | 3rd Qu. | Max. | 0.0260 | 0.3052 | 0.3161 | 0.3146 | 0.3260 | 0.3698 results_dif | Min. | 1st Qu.| Median | Mean | 3rd Qu. | Max. | |-0.3633710 | -0.0143417 | -0.0009655 | -0.0006387 | 0.0137142 | 0.3039858 Over a 1000 calls of a 1000 iterations, you can see the sizeable variation in the coefficient estimated from any given 1000 iteration call, and a small average difference remains. t.test(results_data1,results_data2) Welch Two Sample t-test data: results_data1 and results_data2 t = -0.62233, df = 1975.4, p-value = 0.5338 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -0.002651600 0.001374129 sample estimates: mean of x mean of y 0.3139709 0.3146096 But the differences between the runs with X1 first and the runs with X2 first are not significant.
