[site]: crossvalidated
[post_id]: 416278
[parent_id]: 416240
[tags]: 
EDIT Yea, my previous answer is BS. Here is a Bayesian take as consolation. There seems to bet some controversy about the multiple testing (and rightly so. I'm still researching). I think a quick and easy way to get around this is to do a bayesian logistic regression effect_prior = prior('normal(0,0.5)', class = 'b') model = brm(clicks|trials(N) ~ variant, data = experiment, family = binomial(), prior = c(effect_prior)) We know the effects can't be enormous. In online experiments, they are usually quite small. The prior reflects that. Results of the model are similar to the one above Population-Level Effects: Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat Intercept -1.98 0.09 -2.16 -1.80 2396 1.00 variant_B 0.12 0.13 -0.12 0.38 2631 1.00 variant_C 0.37 0.12 0.13 0.61 2535 1.00 A credible interval for the difference between variant C and B is 0.010 to 0.475. Again, we are estimating that C is the best variant overall, and is likely to be better than B all things considered. Even if C was not better than B, C would still be the better option since we are quite certain B is not better than A
