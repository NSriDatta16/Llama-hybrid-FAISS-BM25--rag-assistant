[site]: crossvalidated
[post_id]: 506453
[parent_id]: 
[tags]: 
Bayesian logistic regression from scratch in Python

I am trying to code in Python the predictive distribution of a bayesian logistic regression. From the course of nando de Freitas we know that this is: $$ p(Y_{n+1} \mid X_{n+1}, D) = \int P(Y_{n+1} \mid \theta, X_{n+1}) P(\theta \mid D) $$ Where $D=\{ X_{1:n}, Y_{1:n}\}$ . We will compute the conditional probability $P(\theta \mid D)$ by MCMC that is we first have to write the unnormalized pdf $$ P(\theta \mid D) = \frac{1}{Z} \prod_{i=1}^{n}[\pi_i^{y_i}(1-\pi_i)^{1-y_i}]e^{(-1/2s)\theta^T \theta} $$ Where we denote $\pi_i = \frac{1}{1+e^{-x_i \theta}}$ . Then by MCMC we can get the samples: $$ \theta^{(i)} \sim P(\theta \mid Y_{1:n} , X_{1:n}) $$ Next we can approimate the predictive distribution with: $$ p(Y_{n+1} \mid X_{n+1}, D) \approx \frac{1}{n} \sum_{i=1}^{n} P(Y_{n+1} \mid \theta^{(i)}, X_{n+1}) $$ Regarding the Python code I have tried the following I took the following dataset adult_us_postprocessed.csv import pandas as pd import numpy as np from scipy.stats import multivariate_normal data = pd.read_csv("adult_us_postprocessed.csv") len(data) #32561 data.head(20) We have the Metropolis hasting code: eps = 1e-12 def sigmoid(x): return 1 / (1 + np.exp(-x)) def log(x): x = x.copy() x[x 0] = np.log(x[x > 0]) return x def log_p_star(data, theta, s=100): y = np.array(data['income_more_50K'].tolist()) p = sigmoid(np.dot(np.hstack((np.ones((len(data), 1)), data[['age', 'educ']].to_numpy())), theta)) return np.sum(y*log(p)+(1-y)*log(1-p)) \ - np.dot(theta.T, theta)/2/s**2 - 3*np.log(s) def mh(param_init, num_samples=5000, stepsize=1.0): ## Random-Walk Metropolis-Hastings algorithm ## step 1, initialize theta = param_init samples = np.zeros((num_samples+1, param_init.shape[0])) samples[0] = theta ## Step 2, iterate for i in range(num_samples): ## step 2a theta_cand = multivariate_normal(mean=theta,cov=[stepsize**2]*len(theta)).rvs(size=1) ## step 2b ll_cand = log_p_star(data = data,theta=theta_cand) ll_old = log_p_star(data = data , theta=theta) ## step 2c if ll_cand > ll_old: theta = theta_cand else: u = np.random.uniform() if u We sample 5000 observations: log_p = mh(param_init=np.zeros(3), num_samples=5000) Finally to compute the predictive probability I am not so sure of what I am doing. I have modified the sigmoid to avoid overflow and I have modified the likelihood to be for only one sample and I have added 0.0000001 to avoid overflow again. def sigmoid_pred(x): if x The last observation will be our couple $(X_{n+1}, Y_{n+1})$ . And finally I think I have to do the following I loop over the last 2000 iterations of the $\theta$ and I evaluate the likelihood for this test data point for $y=0$ and for $y=1$ score_list = list() for i in range(3000,5000): score_list.append(p_new(x, 0, theta=log_p[i])) np.mean(score_list) >>> 9.999e-0.8 score_list = list() for i in range(3000,5000): score_list.append(p_new(x, 1, theta=log_p[i])) np.mean(score_list) >>> -16.11 When I look at the histogram of Nando's course I think I am wrong, it is not the same data but I do not return probabilities and it is predicting that the class of the observation is 0 rather than 1 its true class.
