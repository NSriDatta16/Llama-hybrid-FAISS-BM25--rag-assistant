[site]: crossvalidated
[post_id]: 512849
[parent_id]: 
[tags]: 
Compare the performance of a machine learning informed classification regime against a random one

I am building an active machine learning toolkit to speed up articles' classification for systematic reviews. This method has three steps: Order records heuristically according to some relevant term frequency. Label the first n records and use a probabilistic machine learning algorithm to label the rest. Ask the human to review uncertain and positive (as defined according to another heuristic) labels and repeat the ML step until no unreviewed uncertain/positive labels are present. I'd like to have some benchmarking that shows the usefulness of the method. I thought about comparing the results of my algorithm against a simple random ordering of the records for a fixed number of reviewed records. I already compute train/test/global AUC, Sens, Spec, PPV, NNP but I would like to add something with a cost/benefit intepretation. Since I use a Bayesian ML algorithm, a plus would be to stay in the paradigm, but it's not fundamental. I thought of two possible parameters: Global sensitivity given the number of classified records. Number of records to label randomly to reach the performance reached by the algorithms for a number of classified records. Are there better alternatives (possibly referenced)? Furthermore, I am troubled about how to build the comparison with the random ordering. One solution would be to permute the records N time and see in which % of these the sensitivity would be lower (method 1) and the number to reach sensitivity would be higher (method 2) than then what is achieved using the algorithm. Is there a Bayesian alternative to this?
