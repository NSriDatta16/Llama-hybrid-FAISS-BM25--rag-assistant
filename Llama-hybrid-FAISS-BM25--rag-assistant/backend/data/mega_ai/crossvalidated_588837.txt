[site]: crossvalidated
[post_id]: 588837
[parent_id]: 
[tags]: 
Curse of dimensionality using trees

The curse of dimensionality refers to the fact when a model tries to fit the data in a very high dimensional space (and there is not enough training data). In my mind, I believe that this curse happens for all machine learning models. However, someone told me that trees (for instance xgboost and lgbm) are not affected by this curse and this family of models will learn to ignore some dimensions on their own. How true is this statement and why? There is already an answer in this post , but can I have a more in depth answer?
