[site]: datascience
[post_id]: 116452
[parent_id]: 116310
[tags]: 
Any function $f(X)$ can be represented by a neural network of sufficient complexity, and as the size of the network is increased (number of nodes and/or number of layers), it can represent more complex functions. While you can think of this as similar to adding interaction terms in a linear model, neural networks are not limited to linear functions due to the use of non-linear activation functions. The other difference is that we don't control what terms or functions get included in the model as they are learnt during the training process.
