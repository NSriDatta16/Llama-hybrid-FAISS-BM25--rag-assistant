[site]: crossvalidated
[post_id]: 111144
[parent_id]: 111128
[tags]: 
Since you're interested in prediction, here are my thoughts. Random Forests are excellent at regression tasks with abrupt discontinuities. See the figure on page 55 (60 of the PDF) of Decision Forests for Classification, Regression, Density Estimation, Manifold Learning and Semi-Supervised Learning. Stochastic Gradient Boosting is a similar model worth considering. It's more sensitive to the hyperparameters, but it tends to be a top performer in regression tasks. GBM in R and Scikit-Learn in Python provide implementations of Stochastic Gradient Boosting that can be trained on quantile loss functions, giving you confidence intervals. Following the advice given in this lecture, if you're forecasting beyond the immediate horizon then it is advisable to use multi-output models. Random Forests can do this well. The idea behind nonparametric time-series prediction is that you create a function that approximates $$f(x_{i-1}, x_{i-2}, .., x_{i-n}) = x_i$$ The choice of $X$ is dependent on the data at hand. If your data is strictly positive, then consider a log-transform. If your data changes in scale as a function of time, then consider differencing. If there's an obvious cyclical trend to your data, then consider building a simple regression model over the period of the trend. In any case, the final model for a predictive pipeline is feeding the residuals of these functions onto subsequent functions. If you have side-information on market events, then it is advisable to include this into your model as an explanatory variable. A simple trick is include the event as a dummy variable. $$ k_i = \left\{ \begin{array}{lr} \exp(\alpha(v - i)) & : i \ge v\\ 0 & : i Where $\alpha$ is a scaling factor and $v$ is the time index of the event that occurred. This looks like the following function. As with all forms of predictive modelling, cross-validation is your friend. In the time-series case, your training, validation and testing set should be sequential.
