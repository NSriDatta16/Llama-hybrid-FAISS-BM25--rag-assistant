[site]: crossvalidated
[post_id]: 638546
[parent_id]: 637677
[tags]: 
Your question focuses primarily on the construction of the Bayesian model and the assumptions being made, rather than the specifics of the Expectation-Maximization (EM) algorithm. Suppose we have a set of data points $x_{1}, x_{2},\ldots,x_{M}$ . The EM method is an iterative optimization technique for maximizing the likelihood function where data may be incomplete and missing $x_{M+1}, x_{M+2}, \ldots, x_{M+n}$ or for latent variable models containing unobserved variables $z_{1}, z_{2},\ldots,z_{M}$ . The data points $x$ are typically assumed to be independent and identically distributed IID. This however does not have to hold, as in the case of structured approximations. Likewise, for latent variable models, it is usually assumed each observation $x_{i}$ has a corresponding single latent variable $z_{i}$ . This serves to simplify the graphical structure of the model, leading to an inherent tradeoff between model complexity and posterior expressiveness for capturing all possible dependencies. A typical latent variable model diagram depicts an observation $x_{i}$ , latent variable $z_{i}$ and model parameters $\Theta$ . We note that the described derivations (in the answers above and in the Bishop text book) assume IID data and the mean-field approximation for the latent variables. The following excerpts are taken from my book on variational inference. Learn more on the topic by visiting https://www.thevariationalbook.com/
