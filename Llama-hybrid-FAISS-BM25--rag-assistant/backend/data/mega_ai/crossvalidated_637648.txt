[site]: crossvalidated
[post_id]: 637648
[parent_id]: 
[tags]: 
Gradient boosting and quantile regression performance issues

My goal is to develop a ML model that predicts the remaining flight time. To do so I have different features: distance altitude speed vertical rate Here is a plot showing the actual remaining time vs. distance for multiple flights: I want to perform a quantile regression, therefore I have trained a quantile regression (q=0.9) using a gradient boosting regressor (max_depth=4). Unfortunately it does not work as expected, the model performs poorly for shorter distances, as shown here: Interestingly, at the median (0.5 quantile), the model performs as expected: Changing the quantile alters the distance at which the predictions deteriorate. Here is an example with the 0.1 quantile: I have tried 2 different gradient boosting libraries (XGboost and LightGBM) and observe a similar outcome. I believe this is a fundamental issue, not just software or overfitting. Can anyone explain this behavior? Progress: I think I have identified the issue (or at least part of it): a smaller learning rate seems to worsen the problem. By 'worsen', I am referring to the portion of data for which the error is abnormally large. Edit: As suggested by @user2974951, here is the result for a linear quantile regression with the .9 quantile:
