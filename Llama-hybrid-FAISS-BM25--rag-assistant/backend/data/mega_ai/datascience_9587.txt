[site]: datascience
[post_id]: 9587
[parent_id]: 9585
[tags]: 
Artificial Neural Networks are algorithms loosely based in how a brain functions so they shouldn't be treated as the equivalent of brain learning. The science that studies all the details of how neurons and brains learn is Computational Neuroscience and the question you ask is pretty much an open question but it has many great hypothesis. The method you described about how artificial neural networks update their "weights" is called backpropagation and some authors included O'Reilly, Randall; Munakata, Yuko (2000) in Computational Explorations in Cognitive Neuroscience: Understanding the Mind by Simulating the Brain argue that backpropagation is actually the algorithm that drives learning in the brain. In later publications they propose architectures that include the Hippocampus region to re-create short term memory. However in recent years, with the advent of mainstream deep learning new ideas have arrived along with old ideas with new approaches. The ones that look to replicate the brain and its function or at least approximate it as much as we currently can are: Spiking neural networks Hierarchical temporal memory / Numenta (They also have resources and solutions for data science problems like anomaly detection and classification) Other theories in Computational Intelligence Spiking neural networks are computationally expensive and hard to tune, their performance isn't close to what deep learning can do, however natural brains use some form of Spiking neural networks which tells you about the potential of these kind of models. Another interesting mention is Neural Turing Machines . It's an interesting approach on how certain brain inspired processes can be used to compute diverse problems.
