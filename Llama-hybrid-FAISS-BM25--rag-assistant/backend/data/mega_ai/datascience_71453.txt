[site]: datascience
[post_id]: 71453
[parent_id]: 71451
[tags]: 
You are average pooling your embedding layer. Make sure that it is done over the words rather than over the embedding axis. If your embedding output is (Batch, Words, 300), you want to apply GlobalAveragePool1D to axis 1 (words). One issue here could be though that by averaging over the words, you are messing things up by padding your input. If you pad a 10 word input to 25, and then take an average over all 25, where 15/25 are all zero, you will bias your averaging in an almost random way (random to the length of the input) which your non-recurrent model cannot understand. You would want to add an additional lambda layer to only average over the length of the input, and pass the length of the input as an additional input.
