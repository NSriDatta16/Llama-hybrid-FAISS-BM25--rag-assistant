[site]: crossvalidated
[post_id]: 590356
[parent_id]: 590350
[tags]: 
If you can compute the likelihood of your recordings for each model, I would recommend using model selection criteria, such as the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC) . They allow to compare different models based on their capacity at explaining the observations while accounting for their complexity. Classically used model selection criteria contain two terms: A likelihood term, which measures the ability of the model to explain the observed data; A regularization term, which penalizes the complexity of the model (i.e. its number of free parameters, or features in your case). The point of the latter term is precisely to avoid overfitting and to allow that the selected model will generalize well to unobserved data, i.e. to avoid that the more complicated model (with the highest number of features) will be necessarily selected. This can be interpreted as a form of Occam's razor: if two models explain the data equally well, the simpler one should be favored. An interesting reference on model selection criteria: Burnham, K. P., & Anderson, D. R. (2004). Multimodel inference: understanding AIC and BIC in model selection. Sociological methods & research, 33(2), 261-304. Alternatively, if you consider that you have enough observations to split them into a training set and a validation set, you could simply use cross-validation and see what is the ideal number of features to maximize the validation accuracy.
