[site]: crossvalidated
[post_id]: 430274
[parent_id]: 430186
[tags]: 
The key here is that the error process $\omega_t$ is not known and so you must estimate it using a statistical model. In SARIMAX and in R, this estimation is basically done via Bayesian updating - you begin with a guess (your prior) and then you use a new observation to update your guess (now the posterior). Notice that at any time $t$ , you will never have information about $\omega_{t+1}$ , so your prior is the unconditional distribution, $\omega_{t+1} \sim N(0, \sigma^2)$ . What does change is how much information you have about $\omega_t$ . Before observing any information at all, the best you can do about $\omega_0$ is to again set your prior according to the unconditional distribution. Given this, your prior about $X_1 = \Theta \omega_0 + \omega_1$ is $E(X_1) = 0$ and $Var(X_1) = (1 + \Theta^2) \sigma^2$ . Once you observe $X_1$ , you update your estimate of $\omega_1$ via Bayesian updating. Because you now have some information, this is a better estimate than the unconditional estimate, but you still have some uncertainty about $\omega_1$ . As you observe more data $X_2, X_3, ...$ , your time $t$ uncertainty about $\omega_t$ continues to fall as you learn about the error process by comparing predictions to observations. Eventually, this conditional uncertainty becomes negligible (i.e eventually $Var(\omega_t \mid X_t, X_{t-1}, \dots) \approx 0$ ), and at this point you can be essentially certain that you know the value of $\omega_t$ at time $t$ . Under this condition, your prior expectation for $X_{t+1}$ becomes $E[X_{t+1} \mid X_t, X_{t-1}, \dots] \approx \Theta \omega_t$ , and this is what you expected in your question. The main point is that this is not the appropriate prior unless you are certain about $\omega_t$ , and that can't happen at the beginning of the sample when you have no information about it.
