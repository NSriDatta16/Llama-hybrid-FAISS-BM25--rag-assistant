[site]: crossvalidated
[post_id]: 480915
[parent_id]: 
[tags]: 
Can entropy be used to minimize prediction surprises in machine learning?

Information theory deals with signal/noise identification, while one of its tools, entropy, measures the surprise in random probabilistic outcomes. Has there been any application of using entropy or its variants towards actively managing the bias-variance trade off, or in-sample/out-of-sample surprises faced in machine learning during training and prediction?
