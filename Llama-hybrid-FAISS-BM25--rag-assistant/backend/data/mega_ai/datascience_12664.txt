[site]: datascience
[post_id]: 12664
[parent_id]: 989
[tags]: 
SVM solves an optimization problem of quadratic order. I do not have anything to add that has not been said here. I just want to post a link the sklearn page about SVC which clarifies what is going on: The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. If you do not want to use kernels, and a linear SVM suffices, there is LinearSVC . You'll have to normalize your data though, in case you're not doing so already, because it applies regularization to the intercept coefficient, which is not probably what you want. It means if your data average is far from zero, it will not be able to solve it satisfactorily. What you can also use is stochastic gradient descent to solve the optimization problem. Sklearn features SGDClassifier . You have to use loss='epsilon_insensitive' to have similar results to linear SVM. See the documentation. I would only use gradient descent as a last resort though because it implies much tweaking of the hyperparameters in order to avoid getting stuck in local minima. Use LinearSVC if you can.
