[site]: datascience
[post_id]: 53480
[parent_id]: 
[tags]: 
A Deep CNN model delivering better results with standardization, when compared with normalization

I developed a deep CNN model, based on the architecture discussed in this paper , to generate predictions for time series data. My training data is shown in the figure below: In order to train the model, I tested two approaches. The first one where I normalize the data between 0 and 1, and in the second one I standardize the data based on the approach discussed here . I noticed that in my case, standardization performs far far better than normalization (which gave me a practically flat line as predictions) for generating the predictions. I was wondering if there is any usually suspected general reason for why this could be the case, or is it just that one method works well for a particular type of problem?
