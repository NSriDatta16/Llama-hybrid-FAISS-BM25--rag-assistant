[site]: datascience
[post_id]: 16835
[parent_id]: 
[tags]: 
PyTorch vs. Tensorflow Fold

Both PyTorch and Tensorflow Fold are deep learning frameworks meant to deal with situations where the input data has non-uniform length or dimensions (that is, situations where dynamic graphs are useful or needed). I would like to know how they compare, in the sense of paradigms they rely on (e.g. dynamic batching) and their implications, things that can/cannot be implemented in each one, weaknesses/strengths, etc. I intend to use this info to choose one of them to start exploring dynamic computation graphs, but I have no specific task in mind. Note 1: other dynamic computation graph frameworks like DyNet or Chainer are also welcome in the comparison, but I'd like to focus on PyTorch and Tensorflow Fold because I think they are/will be the most used ones. Note 2: I have found this hackernews thread on PyTorch with some sparse info, but not much. Note 3: Another relevant hackernews thread , about Tensorflow Fold, that contains some info about how they compare. Note 4: relevant Reddit thread . Note 5: relevant bug in Tensorflow Fold's github that identifies an important limitation: impossibility to do conditional branching during evaluation. Note 6: discussion on pytorch forum about variable length inputs in relation to the algorithms used (e.g. dynamic batching).
