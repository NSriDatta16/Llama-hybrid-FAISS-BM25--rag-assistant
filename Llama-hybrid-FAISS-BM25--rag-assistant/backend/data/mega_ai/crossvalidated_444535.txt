[site]: crossvalidated
[post_id]: 444535
[parent_id]: 280594
[tags]: 
Your numbers are correct, but the things is (as Alex R. pointed out in comments), $h_t$ is the internal state and not necessarily the final output of your system. Hence, if you believe a 1-d state vector won't be able to capture much of a state/history/context, then you can make it wider. Then for the output, you can employ an output network that takes the state $h_t$ and produces an output, e.g. the next number. I like the figure in this page, which shows vector sizes in parentheses: https://embarc.org/embarc_mli/doc/build/html/MLI_kernels/comm_lstm.html To summarize, below are the input and output tensor dimensions of the components of the LTSM box: Assume input $x_t$ has dim n , and state $h_t$ has dim k . (Ignoring batch, sequence, samples, etc. Assuming no peepholes into $C_{t-1}$ ). There are 4 1-layer neural nets (actually no hidden layers, just a bunch of logistic regression units, more or less) that compute: forget gate ( $f_t$ ) (activation sigmoid ) input gate ( $i_t$ ) (activation sigmoid ) cell state update ( $\hat C_t$ ) (activation tanh ) output gate ( $o_t$ ) (activation sigmoid ) All of these networks have: Input dim = (n+k) (concatenation of input $x_t$ and hidden state $h_t$ ) Output dim = k (dimension of the vectors $f_t, i_t, \hat C_t, o_t$ ) Then, cell state $C_t$ and hidden state $h_t$ are computed as ( $*$ is element-wise multiplication): $C_t = f_t * C_{t-1} + i_t * \hat C_t$ (all have dim k ) new cell state = forget some of old cell state + add some of new cell state update $h_t = o_t * tanh(C_t)$ (all have dim k ) choose some of cell state, after passing it thru tanh
