[site]: datascience
[post_id]: 69481
[parent_id]: 69358
[tags]: 
The restriction in the maximum length of the transformer input is due to the needed amount of memory to compute the self-attention over it. The amount of memory needed by the self-attention in the Transformer is quadratic on the length of the input . This means that increasing the maximum length of the input, increases drastically the needed memory for self-attention. The maximum length is that which makes the model use up the whole memory of the GPU for at least one sentence (once the other elements of the model are also taken into account, like the embeddings which take a lot of memory). Transformer-XL is certainly a way to take into account as much context as possible in language modeling (its role is analogous to truncated back-propagation through time in LSTM language models). However, the gradients are not propagated through the attention over the memory segment, only through the current segment. There have been several architectural attempts to reduce the amount of memory needed by transformers, like using locality-constraints in the attention ( Dynamic Convolutions model ) or using locality-sensitive hashing ( Reformer model ). There have been other implementation attempts, like gradient checkpointing (e.g. this ), which is a general technique to run computations that don't fit at once in the GPU memory
