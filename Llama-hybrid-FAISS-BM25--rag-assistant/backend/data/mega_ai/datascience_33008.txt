[site]: datascience
[post_id]: 33008
[parent_id]: 
[tags]: 
Is it always better to use the whole dataset to train the final model?

A common technique after training, validating and testing the Machine Learning model of preference is to use the complete dataset, including the testing subset, to train a final model to deploy it on, e.g. a product. My question is: Is it always for the best to do so? What if the performance actually deteriorates? For example, let us assume a case where the model scores around 65% in classifying the testing subset. This could mean that either the model is trained insufficiently OR that the testing subset consists of outliers. In the latter case, training the final model with them would decrease its performance and you find out only after deploying it. Re-phrasing my initial question: If you had a one-time demonstration of a model , such as deploying it on embedded electronics on-board an expensive rocket experiment, would you trust a model that has been re-trained with the test subset in the final step without being re-tested on its new performance?
