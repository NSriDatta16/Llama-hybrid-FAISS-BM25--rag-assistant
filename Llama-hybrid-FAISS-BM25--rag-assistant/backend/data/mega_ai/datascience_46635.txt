[site]: datascience
[post_id]: 46635
[parent_id]: 46630
[tags]: 
This is called batch updation which is the most popular method of updating weights. We also call it BatchSGD in context of SGD. Yes, what you mentioned is true. If we have 1000's of weights which we typically have in deep neural networks. It is not efficient to calculate partial derviatives at each weights for every input. Instead, We do the batch updation by which we will aggregate all the loss for last 100 inputs(as in your case) and at the end of 100th input, we take the average fo the losses and update the weight of the network. Keep in mind that calculation partial derivatives is one of the most compute intensive tasks that we perform in neural networks. So making 100 updates to Just 1 save a lot of compute. Hope it helps
