[site]: crossvalidated
[post_id]: 110211
[parent_id]: 110185
[tags]: 
Here's a good paper comparing various dimension reduction techniques to PCA: http://www.iai.uni-bonn.de/~jz/dimensionality_reduction_a_comparative_review.pdf In brief, the paper covers the following techniques, though there are many more: (1) multidimensional scaling, (2) Isomap, (3) Maximum Variance Unfolding, (4) Kernel PCA, (5) diffusion maps, (6) multilayer autoencoders, (7) Locally Linear Embedding, (8) Laplacian Eigenmaps, (9) Hessian LLE, (10) Local Tangent Space Analysis, (11) Locally Linear Coordination, and (12) manifold charting In fact, many of the methods not covered are briefly described in Appendix A. Moving into the paper, you can see that those above techniques are described in some detail as to their theoretical underpinnings. Table 1 compares the algorithms on convexity of optimization, parameters, computational complexity, and memory complexity. Table 4 shows the results of the algorithms being run on many example datasets, and the resulting generalization error rates. In the end, the authors believe that PCA is quite useful/good. For more info on dimension reduction, see his references. Edit: here's a specific sentence highlighting what you wanted to know about MDS vs Isomap: Performing MDS using geodesic distances is identical to performing Isomap. Bonus: My favorite paper using PCA analysis
