[site]: crossvalidated
[post_id]: 123252
[parent_id]: 123175
[tags]: 
Because the log-transformed, more normally distributed variables are more likely to fulfill linear regression's assumptions, particularly linearity, homoscedasticity, and normally distributed residual. In order for the regression slope estimate to be unbiased, the association between the dependent and independent variables need to be linear. Skewed variable often have extreme outliers that can distort the linear relationship. (If you have learned leverage, you can also think that skewed independent variable would have some data points with very high leverage, potentially able to bias the regression slope.) For the standard error of the slope to be unbiased, the distribution of the error along the predicted values needs to be i) normal, and ii) spreading up and down the predicted line equally (known and homoscedasticity). As you can see form the follow two scenario, with the left one using skewed original variables and the right one using log-transformed variables, the skewed version seriously violates homoscedasticity and normality assumptions (aka the regression standard error and p-value are likely incorrect), while the right hand side one appear to fulfill the two assumptions.
