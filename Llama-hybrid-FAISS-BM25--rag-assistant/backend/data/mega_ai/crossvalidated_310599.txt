[site]: crossvalidated
[post_id]: 310599
[parent_id]: 310587
[tags]: 
Somewhat anticlimactically, I think that the most efficient as well as universal feature engineering approach is domain expertise ; pretty much everything is second to that. That said, a dominant second approach is using no feature engineering at all. Going to a bit more detail, one might argue about transforming categorical features to numerical features being extremely helpful. Indeed it can be tremendously helpful when dealing with categorical variables of high cardinality (eg. see all the options that catboost offers); I fully agree with that idea. Nevertheless these techniques is not necessarily better than one-hot encoding when cardinality is not high or when one uses binary-encoding (eg. see here a more careful investigation); ie. they do not constitute an approach with " proven accuracy ". Similarly, normalising numerical features to have a specific range (eg. $[-1,+1]$) or distribution (eg. $\sim N(0,1)$) is a necessity when working with regularisation methods like ridge or LASSO regression but realistically it will make little difference to random forests and other tree-based learners who are mostly using ordering information. Again, no guarantee that normalisation is not just a waste of time. That's why I argue for domain expertise. Notice domain expertise does not need to be vast rather enough to guarantee "common sense" within the domain of application. On certain specific tasks there might be features that are worth always testing . For example, for image recognition, speech recognition and time-series analysis, edge-detection, spectral decomposition and change-point analysis respectively are obvious and most likely helpful features to augment a training set. So... is domain expertise an out-of-the-box approach? I would argue yes but if we convenience ourselves for no then our best feature engineering option is using no feature engineering at all. That means using learners that are very flexible and can learn strong non-linear associations themselves. Gradient boosting machines and (deep) neural networks stand out as immediate examples. Clearly we now pick the box , but hey, it is an out-of-the-box choice after all. :)
