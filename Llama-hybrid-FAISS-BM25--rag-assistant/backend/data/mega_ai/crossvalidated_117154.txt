[site]: crossvalidated
[post_id]: 117154
[parent_id]: 
[tags]: 
Reduction of sparse features for machine learning

I'm trying to use a 1D histogram as a feature for machine learning. A histogram instance can be very sparse and the range of its bins is theoretically unbounded. Moreover it is expected that non-zero values for one instance would be grouped together with some holes between. On the other hand machine learning algorithms generally expect dense features with constant number of parameters. The problem seems to be quite similar to transforming text in the bag-of-words representation to a dense low-dimensional feature space. An obvious approach would be to transform the sparse vector into a dense one. I've came across several suggestions for solving this problem: dictionary learning semantic hashing SparsePCA random projections What would you recommend and why?
