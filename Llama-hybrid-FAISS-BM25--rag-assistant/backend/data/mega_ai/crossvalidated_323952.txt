[site]: crossvalidated
[post_id]: 323952
[parent_id]: 
[tags]: 
How to compare the expected performance of algorithms (not models)?

Given a single training dataset, cross-validation allows to estimate the expected performance on unseen data of models inferred from the training dataset with different algorithms. Since the tests sets from different folds are independent, it also allows to estimate the uncertainty of the performance estimate and therefore to assess the statistical significance of observed differences between algorithms/models. In contrast to such a model selection problem, one might want to claim that a certain algorithm is better than another in that if applied to a new dataset (from the same source) it yields the better model. This requires to estimate the expected performance and its uncertainty (where the expectation is over the training data as well). Since the training sets from different cross-validation folds overlap, performance estimates obtained from the different test-sets are not independent and would yield biased uncertainty estimates. In [1] Bengio & Grandvalet describe this problem in great detail and show that the required unbiased estimator for the variance of cross-validation does not exist. In the machine learning community this problem appears to be avoided by simply comparing algorithms based on their ranked performance on different related dataset (e.g. MNIST, CIFAR10 and Imagenet). Although one would need at least 6 such datasets to claim significant improvements with a binomial test and I have never seen an significance test performed on such results, I believe that this practice to be reasonable. But given only a single training dataset, how can I test if a model produced by one algorithm will still likely be better than one produced by another algorithm in case another dataset from the same source was used to train the models? References [1] Y. Bengio, Y. Grandvalet, No Unbiased Estimator of the Variance of K-Fold Cross-Validation, J. Mach. Learn. Res. 5 (2004) 1089â€“1105.
