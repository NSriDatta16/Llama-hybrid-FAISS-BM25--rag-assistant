[site]: crossvalidated
[post_id]: 419421
[parent_id]: 345915
[tags]: 
Actually, using the answer in https://stackoverflow.com/questions/26079881/kl-divergence-of-two-gmms (and the fact, that the author factored out the 1/2 from the logarithm, made the montecarlo approximation sample from both distributions to average the result), I would say, that the symmetrized numerical code for jensen shannon divergence using monte carlo integration, even for general scikit.stats distributions (_p and _q), should look like this: def distributions_js(distribution_p, distribution_q, n_samples=10 ** 5): # jensen shannon divergence. (Jensen shannon distance is the square root of the divergence) # all the logarithms are defined as log2 (because of information entrophy) X = distribution_p.rvs(n_samples) p_X = distribution_p.pdf(X) q_X = distribution_q.pdf(X) log_mix_X = np.log2(p_X + q_X) Y = distribution_q.rvs(n_samples) p_Y = distribution_p.pdf(Y) q_Y = distribution_q.pdf(Y) log_mix_Y = np.log2(p_Y + q_Y) return (np.log2(p_X).mean() - (log_mix_X.mean() - np.log2(2)) + np.log2(q_Y).mean() - (log_mix_Y.mean() - np.log2(2))) / 2 print("should be different:") print(distributions_js(st.norm(loc=10000), st.norm(loc=0))) print("should be same:") print(distributions_js(st.norm(loc=0), st.norm(loc=0))) For noncontinuous, change .pdf to probabilities of samples.
