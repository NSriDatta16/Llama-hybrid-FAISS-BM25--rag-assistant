[site]: datascience
[post_id]: 114509
[parent_id]: 
[tags]: 
memory bound for kernel tricks in machine learning

Based on Andrew Ng's lecture on Kernel , you use training samples (referred as landmarks l) and use them during prediction to construct the higher dimensional representation of the given sample. This new representation of x (f) gets multiplied by w for the final prediction. If I were to implement this, does this mean I need to store all the training samples? (I guess I need them for implementing both training and prediction logic) My gut feeling tells me that there is something more because storing every training sample sounds nonsense to me. How does prediction with kernel actually get implemented in reality?
