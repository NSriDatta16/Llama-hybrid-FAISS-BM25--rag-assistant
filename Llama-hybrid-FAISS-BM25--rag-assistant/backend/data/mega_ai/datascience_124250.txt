[site]: datascience
[post_id]: 124250
[parent_id]: 103091
[tags]: 
I suggest to have a look at bertopic (with default umap + hdbscan ) and dense embeddings from a Bert-like model. You best take a word embedding model with a big number of tokens like a Longformer with ~4000 input tokens which you then wrap in a sentence transformer. Potentially you can make use of an Embedder with 100000 input tokens so that all your documents fit into the input of your Embedder completely.
