[site]: datascience
[post_id]: 28364
[parent_id]: 28360
[tags]: 
To your question "is it really a problem to train a classifier with 80% class A if the testing set will have the same proportion?", the answer is "it depends on the cost of misclassifications", but normally it is certainly a problem, because your classifier will be biased to classify an individual into the over-represented class regardless of the values of its features because, in average , doing so increased the chances of correct classification during training. There are different oversampling and undersampling strategies . Maybe the most popular one is SMOTE (Synthetic Minority Over-sampling Technique), which creates synthetic training data based on k-nearest neighbors.
