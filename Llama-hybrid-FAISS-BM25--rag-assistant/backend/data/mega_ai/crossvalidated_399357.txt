[site]: crossvalidated
[post_id]: 399357
[parent_id]: 399318
[tags]: 
Christoph has a great answer (+1). Just writing this because I can't comment there. The crucial point here is that the likelihood only depends on the coefficients $\beta$ through the linear term $X \beta$ . This makes the likelihood unable to distinguish between " $X \beta$ " and $(XD) (D^{-1}\beta)$ ", causing the invariance you've noticed. To be specific about this, we need to introduce some notation (which we can do since we're writing an answer!). Let $y_i | x_i \stackrel{ind.}{\sim} \mathrm{bernoulli}\left[ \mathrm{logit}^{-1} (x_i^T \beta) \right]$ be independent draws according to the logistic regression model, where $x_i \in \mathbb{R}^{p+1}$ is the measured covariates. Write the likelihood of the $i^{th}$ observation as $l(y_i, x_i^T \beta)$ . To introduce the change of coordinates, write $\bar{x}_i = D x_i$ , where $D$ is diagonal matrix with all diagonal entries nonzero. By definition of maximum likelihood estimation, we know that maximum likelihood estimators $\hat{\beta}$ of the data $\{y_i | x_i\}$ satisfy that $$\sum_{i=1}^n l(y_i, x_i^T \beta) \leq \sum_{i=1}^n l(y_i, x_i^T \hat\beta) \tag{1}$$ for all coefficients $\beta \in \mathbb{R}^p$ , and that maximum likelihood estimators for the data $\{y_i | \bar{x}_i\}$ satisfy that $$\sum_{i=1}^n l(y_i, \bar{x}_i^T \alpha) \leq \sum_{i=1}^n l(y_i, \bar{x}_i^T \hat\alpha) \tag{2}$$ for all coefficients $\alpha \in \mathbb{R}^p$ . In your argument, you used a closed form of the maximum likelihood estimator to derive the result. It turns out, though, (as Cristoph suggested above), all you need to do is work with the likelihood. Let $\hat{\beta}$ be a maximum likelihood estimator of the data $\{y_i | x_i\}$ . Now, writing $\beta = D \alpha$ , we can use equation (1) to show that $$\sum_{i=1}^n l(y_i, \bar{x}_i^T \alpha) = \sum_{i=1}^n l\left(y_i, (x_i^T D) (D^{-1} \beta)\right) \leq \sum_{i=1}^n l(y_i, x_i^T \hat\beta) = \sum_{i=1}^n l(y_i, \bar{x}_i^T D^{-1} \hat{\beta}).$$ That is, $D^{-1} \hat{\beta}$ satisfies equation (2) and is therefore a maximum likelihood estimator with respect to the data $\{y_i | \bar{x}_i\}$ . This is the invariance property you noticed. (For what it's worth, there's a lot of room for generalizing this argument beyond logistic regression: did we need independent observations? did we need the matrix $D$ to be diagonal? did we need a binary response? did we need the use logit? What notation would you change for this argument to work in different scenarios?)
