[site]: crossvalidated
[post_id]: 571252
[parent_id]: 106334
[tags]: 
The composition of multiple layers is what makes the cross-entropy or least-squares loss function of multi-layer neural networks non-convex with respect to the set of all weights and biases. The composition is via multiplications of functions of the weights/biases and that is the main culprit for non-convexity, not the non-linearity of activation functions nor the inherent over-parameterization (re the arguments around permutations). To understand how multiplying parameters can result in non-convexity, consider the function $f(x,y)=xy$ . It is convex in $x$ when $y$ is constant and convex in $y$ when $x$ is fixed, but it is not convex in $x$ and $y$ jointly. Here is a plot of this function showing its non-convexity in $x$ and $y$ : Another example is $f(x,y)=x^2y^2$ , which is non-convex as it is zero on the axes and positive elsewhere, while $x^2$ and $y^2$ are strictly convex.
