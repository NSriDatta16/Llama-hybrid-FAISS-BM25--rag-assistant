[site]: stackoverflow
[post_id]: 1760640
[parent_id]: 1760495
[tags]: 
Just be careful: the fib.cache trick only works if fib is indeed the name of the relevant function object in the scope that's active while it's executing (for example, when you decorate it as you have done, you must assign the starting value for the cache to the decorator wrapper, not to the decorated function -- and if it gets further decorated after that, things break). This is a bit fragile compared to the standard memoization idiom: def fib(n, _memo={0:1, 1:1}): if n in _memo: return _memo[n] else: _memo[n] = fib(n-1) + fib(n-2) return _memo[n] or the decorator equivalent. The standard idiom's also faster (though not by much) -- putting them both in mem.py under names fib1 (the .cache-trick one, without prints and undecorated) and fib2 (my version), we see...: $ python -mtimeit -s'import mem' 'mem.fib1(20)' 1000000 loops, best of 3: 0.754 usec per loop $ python -mtimeit -s'import mem' 'mem.fib2(20)' 1000000 loops, best of 3: 0.507 usec per loop so the standard-idiom version saves about 33% of the time, but that's when almost all calls do actually hit the memoization cache (which is populated after the first one of those million loops) -- fib2's speed advantage is smaller on cache misses, since it comes from the higher speed of accessing _memo (a local variable) vs fib.cache (a global name, fib, and then an attribute thereof, cache), and cache accesses dominate on cache hits (there's nothing else;-) but there's a little extra work (equal for both functions) on cache misses. Anyway, don't mean to rain on your parade, but when you find some new cool idea be sure to measure it against the "good old way" of doing things, both in terms of "robustness" and performance (if you're caching, presumably you care about performance;-).
