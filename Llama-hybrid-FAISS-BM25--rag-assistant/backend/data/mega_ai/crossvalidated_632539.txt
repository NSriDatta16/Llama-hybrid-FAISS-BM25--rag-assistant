[site]: crossvalidated
[post_id]: 632539
[parent_id]: 
[tags]: 
best practices on optimizing feature transforms for a model

I have a regression model that Transforms some time series features using a different halflife for each feature Uses the transformed features along with some other features to create a prediction During the fitting stage the model fits the halflife parameters as well as calculating the model coefficients. For example, something like this from sklearn.metrics import mean_squared_error class MyModel: def __init__(self): self.coef_: np.ndarray def _hypothesis(self, X: pd.DataFrame, coef_: List): # add time series decays using the first 3 coefs which correspond to halflifes X = add_time_series_decays(coef_[0], coef_[1], coef_[2]) # calculate our prediction using all but the first 3 coefs return X * coef_[3:] def _loss_func(self, X: pd.DataFrame, y: pd.Series, coef_): result = self._hypothesis(X, coef_) return mean_squared_error(result, y) def fit(self, X: pd.DataFrame, y:pd.DataFrame, initial_params: List, verbose=False, **kwargs): # This minimizes our loss function by altering our coefs, which include both the halflives used to do feature transforms # as well as the multipliers used to create a prediction result = minimize( self._loss_func, initial_params, args=(X.copy(), y.copy(), verbose), **kwargs, ) self.coef_ = result["x"] return self def predict(self, X: pd.DataFrame): return self._hypothesis(X, self.coef_) Fitting the halflives and the model coefficients at the same time doesn't feel correct since it forces feature transformations to be carried out in the same place as the model fitting and quickly becomes monolithic rather than modular Does anyone know if there is a standard way to optimize feature transforms like this? I'm in 2 minds about whether these halflives should be considered hyper parameters and optimized using hyper parameter tuning
