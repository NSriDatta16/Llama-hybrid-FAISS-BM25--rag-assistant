[site]: crossvalidated
[post_id]: 310958
[parent_id]: 
[tags]: 
Reference showing that only deep learning algorithms benefit from using huge datasets

Andrew Ng in his deep learning course on Coursera.org states that there is a boundary on sample size where machine learning algorithms stop improving and such boundary is nonexistent for the deep learning algorithms, as they always improve when feeded with more data. Could you point any reference that goes into more details and describes actual research on the phenomenon?
