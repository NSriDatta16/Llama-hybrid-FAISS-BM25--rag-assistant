[site]: crossvalidated
[post_id]: 641533
[parent_id]: 
[tags]: 
A question about K-fold cross validation in the context of neural networks

Let's say that I want to perform K-fold cross validation for a neural network model. So, as usual, I split my dataset D into $K$ different folds. The fold $k$ is the validation fold while the remaining $k-1$ are training folds. My question is: would it be correct to use the validation fold $k$ as validation set for the neural network, or would this lead to information leakage, if we, for example, use early-stopping or other training schemes that could bias the learning process towards the specific data in the validation? Personally, my approach so far has been extracting an additional 10% of data from the $k-1$ training folds to be used as validation set for the network, in a way that the validation fold remains completely untouched. Do you think that this approach would eventually be better?
