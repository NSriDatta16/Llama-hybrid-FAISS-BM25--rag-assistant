[site]: crossvalidated
[post_id]: 137508
[parent_id]: 101237
[tags]: 
Some part of Matt Krause's answer doesn't seem to me correct (Lambda and Beta), also epsilon is not mentioned. Actually this post should be a comment, but I'm answering the question because of 50 reputation restriction. Feel free to comment if you see any mistake. From http://web.stanford.edu/class/archive/cs/cs294a/cs294a.1104/sparseAutoencoder.pdf Lambda is coefficient of weight decay term which discourage weights to reach big values since it may overfit. Weight decay term (or weight regularization term) is a part of the cost function like sparsity term explained below. rho is sparsity constraint which controls average number of activation on hidden layer. It is included to make autoencoder work even with relatively big number of hidden units with respect to input units. For example, if input size is 100 and hidden size is 100 or larger (even smaller but close to 100), the output can be constructed without any lost, since hidden units can learn identity function. Beta is coeffecient of sparsity term which is a part of the cost function. It controls relative importance of sparsity term. Lambda and Beta specify the relative importance of their terms in cost function. Epsilon (if they've used the same notation with Andrew Ng) is regularization parameter for whitening process which has low-pass filter effect on input. That has some important effects on reconstruction methods. Check the link for that under reconstruction based models. I think they have used linear activation for the output layer and used some type of whitening (univariating features). Update rate of parameters (weights and biases) is called learning rate and denoted by eta in general. However, it has been used as alpha by Andrew Ng. Check the first link.
