[site]: datascience
[post_id]: 55913
[parent_id]: 54806
[tags]: 
Generating word embeddings for " OOV "(out of vocabulary) words is one of the major limitations of many standard embeddings like Glove and word2vec. However, fastText circumvents this problem to some extent. Instead of the traditional approaches which have distinct vectors for each word, they take a character n-grams level representation. For instance, a word with n= 3, will be represented by the character n-grams: and the special sequence: Here,<>are part of the n-grams. $$ s(w,c) = \sum_{g\varepsilon G_{_{w}}} z_{g}^{T} v_{c} $$ Here, $G$ represents the size of a dictionary of n-grams, and given a word $w$ , then $G_{w}\subset \left \{ 1, ..., G \right \}$ represents the set of n-grams appearing in $w$ . They associate a vector representation $z_{g}$ to each n-gram $g$ and represent a word by the sum of the vector representations of its n-grams. This helps them tackle OOV words via knowing some representations of a subword. For an instance, an OOV word: sechero The 3-gram: since, these 3-grams may have been encountered during learning, through other known words, like: che - cheer ro> - hero Hence, it can form at least some sensible embedding, instead of returning a useless Fastext in fact is an extension to word2vec, with majorly the feature explained above.
