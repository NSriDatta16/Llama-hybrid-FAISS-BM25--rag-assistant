[site]: crossvalidated
[post_id]: 197229
[parent_id]: 
[tags]: 
Reinforcement Learning in a Continuous Environment

Many of the tutorials on reinforcement learning use the example of a robot navigating a discrete grid environment or a game of chess to demonstrate the principle of learning with delayed rewards. Often Q-learning is represented as a table listing the optimal outcome for each state. Obviously for many situations, the environment may not be discrete but continuous. How does the Q-learning approach work, if at all, in a continuous environment. The example I am trying to understand is buying and selling stocks on the stock market. Here, clearly the input features (price etc) would be continuous an the reward (profit) is dynamic. I understand that neural networks can be used to try to predict future rewards but I haven't been able to understand how Q-learning applies to back-propagation. Related: How to fit weights into Q-values with linear function approximation
