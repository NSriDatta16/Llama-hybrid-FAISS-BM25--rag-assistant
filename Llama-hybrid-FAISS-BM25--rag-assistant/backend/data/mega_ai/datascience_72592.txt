[site]: datascience
[post_id]: 72592
[parent_id]: 72584
[tags]: 
Sure, that's feature engineering. If you're fitting a linear model, then you are looking for features that have a linear relationship with the predicted value. If you're predicting, say, the cost per hour of a device consuming current I, then clearly that's directly related to power not current, so $I^2$ is more likely to be useful. What you want to be careful about is trying a bunch of transformations of the input blindly; it's possible for a small data set that one odd function of an input happens to be predictive in that sample, but doesn't generalize.
