[site]: crossvalidated
[post_id]: 584812
[parent_id]: 
[tags]: 
Why use IsolationForest over other supervised methods for semi supervised learning?

I have a dataset with labels that I'm using to explore unsupervised learning (IsolationForest) with. IsolationForest has a few hyperparameters, and some can be heuristically determined like maybe you know the rough contamination rate, but other variables can't be tuned without labels. When I've seen people discuss this model, they tune it with labelled data, akin to supervised learning. Doesn't this defeat the purpose? I understand that you could take a subset of your data and label it manually and train/tune your model there, but at that point, why use IsolationForest over other methods? Does IsolationForest perform better with small datasets? My thought is something like XGBoost tries to model all data points, while IsolationForest may just try to identify the few outliers, and not model all data, and therefore needs less, but not sure if this is accurate. So why use IsolationForest if you have a small labelled subset of the data? Thanks for helping a novice.
