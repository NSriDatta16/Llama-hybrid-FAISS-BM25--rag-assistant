[site]: datascience
[post_id]: 112632
[parent_id]: 
[tags]: 
Deep learning approach for calibration of raw data using reference measurements and a recurrent neural network (LSTM)

I am using Keras and R for my calibration problem. I have raw temperature time series data of a low-cost measurement device, which has a strong linear relationship with reference measurements of a high-cost device. The measurements of the low-cost device are not as accurate as the measurements of the high-cost device though. Therefore I want to calibrate the raw measurements of the low-cost device considering reference measurements of the high-cost device using LSTM. I recently bought "Deep Learning with R, Second Edition" to go through the examples in the book to get a better understanding about Deep Learning. LSTM is also introduced there, but they use it for forecasting temperatures. I want to calibrate, not forecast. The forecast example and source code I use as a base can be found here: https://github.com/t-kalinowski/deep-learning-with-R-2nd-edition-code/blob/main/ch10.R Basically they prepare the input data in a way to solve their forecasting problem: sampling_rate % normalize_input_data() %>% as.matrix() targets Because I don't want to forecast, but calibrate, I would guess, that there is no need for a delay as seen in the former code? The target has to be the last element in the reference measurements sequence in my case. I've changed the parameter values accordingly: sampling_rate Do I have the right idea as seen through the changes in the parameters and if not so, please correct me. Thanks! PS. As far as I've seen, LSTM seems to be exclusively used as a forecasting method when used with low-cost sensors. So it feels like I can't really use it in the way I want to use it.
