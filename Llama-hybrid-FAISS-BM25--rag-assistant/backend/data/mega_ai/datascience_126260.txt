[site]: datascience
[post_id]: 126260
[parent_id]: 
[tags]: 
Learning curve dip after plateau when adding more samples

I have a questions regarding my keras machine learning model. Context : I am working on elementary particle physics, specifically LHC related data. I am training a regression model of 4 Dense layers each with 100 neurons to predict an invariant mass distribution of events with a top-antitop quark pair system. The system has 2leptons, 2 bjets and 2 neutrinos in the final state, which makes a complete mass reconstruction from reconstructed data impossible (because of the neutrinos). This is why I try to use a model to predict the final mass distribution. The target (truth) distribution is calculated directly from the matrix element to second leading order. My input values for the model are the invariant masses of the leptons and bjets (6 in total), aswell as missing transverse energy (neutrinos). Because I use simulated reconstruced data, there are a lot of events (samples) to potentially feed into the model. Problem : My learning curve when I use under 2 million events is pretty flat: and it does get more accurate (little bit) if I use u2 million events instead of 100 000. But at some point, here with 3.5 Mil events, this happens to the loss curve where after epoch 20: I am using Adam optimizer with a fixed learning rate of 0.001. Do you guys think the optimizer needs adapting? I am very new to machine learning in general. I also scaled the data down to values between 0 and 1, with MinMaxScaler, but the same issue persists. Also, when I let the model predict, I usually see with not too many events, that it undershoots the target distribution: but then with the loss dip I have shown, the prediction gets a huge bias towards the peak: Do you guys think the model is too simple to do this? Or could it be an optimizer problem? I have also tried to add weights to the training, essentially training it on a flat target distribution to get rid of the bias, but that didnt work out. I also tried to add other low-level information
