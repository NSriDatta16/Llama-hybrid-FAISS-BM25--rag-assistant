[site]: crossvalidated
[post_id]: 306684
[parent_id]: 
[tags]: 
Using maximum Likelihood regression to get closer to the true posterior when doing Approximate Bayesian Computation : contradiction?

Post-hoc adjustments are used to get closer to the true posterior distribution when doing Approximate Bayesian Computation. This is particularly important when using a rough algorithm like rejection/sampling. I have been using the multiple weighted linear regression as proposed by Beaumont et al (2002) for a while. This is particularly convenient since there is an analytic answer (very fast) to get the slopes and the intercepts. But as this linear model is somehow part of the ABC model, would it be more respectful (probably not the good word to use) to the whole Bayesian framework employed until that post-hoc adjustment to use a Bayesian regression model. That way, it could be possible to account for the Bayesian incertitude in the objective of getting closer to the true distribution. I don't know exactly what to think about that, and the maximum Likelihood framework is good at getting closer to the true posterior distribution. Any line of thought or philosophical perspectives on that? And What about considering the model used in the post-hoc operation as part of the ABC model? The same question could be extended to more recent post-hoc adjustments using more sophisticated machine learning algorithms (e.g., neural network and random forest). Maybe there is too many questions here?
