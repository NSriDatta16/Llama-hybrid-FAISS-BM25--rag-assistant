[site]: datascience
[post_id]: 113613
[parent_id]: 113612
[tags]: 
do they mean that $\sum\limits_{k=1}^{t-1} \omega(f_k) = constant$ ? Yes! From the paragraph preceding that: we use an additive strategy: fix what we have learned, and add one new tree at a time. That is, we're just trying to build the next tree $f_t$ , given that $f_1, \dotsc, f_{t-1}$ have already been built, and so $\omega(f_1), \dotsc, \omega(f_{t-1})$ are all already determined. Gradient boosting is greedy in that sense (earlier trees don't try to look ahead to how later trees will fare, nor do later trees attempt to modify earlier trees), but moreso the tree building process is greedy (earlier splits don't try to look ahead to how later splits will fare).
