[site]: datascience
[post_id]: 36931
[parent_id]: 36895
[tags]: 
Currently your model is not restricted from guessing any number. In this case, you may want to use something like a ReLU activation function to restrict your output domain. The ReLU activation needs to be added to the last layer of your model. If you add it to an intermediate layer but not the last layer then your model can still output negative numbers, e.g. in the case of the last layer having a negative weight. Note this is not really a new idea. There are strong similarities between logistic regression and what we’re doing here. In both cases we want to limit the possible guesses, in your case to non-negative numbers and in logistic regression‘s case to [0, 1].
