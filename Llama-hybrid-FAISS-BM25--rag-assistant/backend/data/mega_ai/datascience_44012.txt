[site]: datascience
[post_id]: 44012
[parent_id]: 43772
[tags]: 
What you need is Encoding Categorical variables. This topic is discussed in tons of blogposts, it is definitely worth checking out this recent article that nicely and extensively go through most of the methods, since I do not intent to rewrite again what is out there but rather giving you my personal experience. I asked you earlier what algorithms you are after, simply that could change your choice of encoding method. Some encoding methods like One-hot-encoding makes your feature space very sparse when your categorical variable is very cardinal (usually not recommended!), and it is best to go with sparse-aware algorithms. In a nutshell, I suggest to start with the following (classic and simple), explanations borrowed from that article: OneHot — one column for each value to compare vs. all other values. Binary — convert each integer to binary digits. Each binary digit gets one column. Hashing — Like OneHot but fewer dimensions, some info loss due to collisions. Backward Difference — the mean of the dependent variable for a level is compared with the mean of the dependent variable for the prior level. Target — use the mean of the dependant variable. The above-mentioned methods can be used pretty much for all algorithms. And each single has its pros and cons. Some you have more information loss than the other and so on. Good news is that most of these methods are quite easy to use, e.g. via this Python package . Another method that I found quite interesting and is suitable for Neural Networks is: Entity Embeddings - An special encoding method borrowed from NLP, in which an embedding space is learned on the fly for each categorical variable. See these blogpost 1 , 2 , 3 . I am writing an up-and-running code in Google Colab to fully demonstrate this, but you still find pieces of code in those blogposts to give it a try. I find this method way better than for example OneHot for Neural Networks. Hope these help!
