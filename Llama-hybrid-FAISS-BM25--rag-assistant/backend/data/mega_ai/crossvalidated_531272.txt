[site]: crossvalidated
[post_id]: 531272
[parent_id]: 531057
[tags]: 
I suspect that Gelman and Hill are either overstating the case concerning whether normality is an issue, or that their comments are taken out of context. While linearity (or more precisely, correct functional specification) is usually more important, there are cases of non-normality that indeed should be of great concern. These include: Extremely heavy-tailed distributions. In such a case, you definitely should not use ordinary least squares to estimate the trend, even if it is linear. Quantile regression is a good alternative. Binary or highly discrete dependent variable. In this case, some maximum likelihood method would be preferred, even if the response function happened to be linear (which is very unlikely in this case). As far as needing normality and homoscedasticity specifically for prediction (assuming OLS), imagine you are regressing an individual stock return percentage ( $Y$ ) on the S&P 500 return ( $X$ ). You would like to predict what would happen to your return when the market declines by 1% (i.e., when $X=-1$ ). In this case, you know that your $Y$ is a random variable, with some distribution. Now, unthinking application of OLS would give you the (essentially) the following prediction: $Y$ will lie within $\pm 3\times rmse $ of the regression prediction, with approximate 99.7% probability. The problem regarding the homoscedasticity assumption of OLS is that $rmse$ , being a pooled estimate across all values of $X$ , may over- or under- estimate the conditional standard deviation of $Y$ when $X=1$ , depending upon the nature of the heteroscedasticity. This results in a prediction interval that may be too wide, or it may be too narrow. The problem with the normality assumption of OLS is that, say if the distribution of returns is heavy-tailed (and it probably is), there will occasionally be returns that are much farther than three standard deviations from the conditional mean. So the usual predictions are overly optimistic in terms of bounding risk. Further, if the distribution of returns is asymmetric, then the prediction interval should be correspondingly asymmetric. Another good example is multinomial logistic regression. Say $Y$ is choice of toothpaste brand, and $X$ is age. What is the prediction of $Y$ when $X=50$ years old? It is certainly not some average of Crest, Colgate, Mr. Toms, etc. The prediction is the distribution itself, which is certainly not a normal distribution. Rather, it is a simple discrete probability distribution on all the brands. Regression may be best understood as a model for the conditional distribution of $Y$ , given $X$ . This representation resolves a lot of supposed conflicts between OLS and ML; it leads seamlessly to likelihoods and Bayes; it puts ordinary regression, heteroscedastic regression, ANOVA, Poisson regression, multinomial logistic regression, survival analysis, quantile regression, neural net regression, tree regression, etc., all under a common umbrella; and it includes the mean portion of the model that people are usually interested in as a special case.
