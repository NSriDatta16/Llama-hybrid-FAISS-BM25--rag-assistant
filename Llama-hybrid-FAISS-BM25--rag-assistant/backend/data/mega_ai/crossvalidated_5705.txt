[site]: crossvalidated
[post_id]: 5705
[parent_id]: 4687
[tags]: 
Consider two simple cases, 1) a real valued variable X is the parent of another real valued variable Y 2) a real valued variable X is the parent of a discrete valued variable Y Assume that the Bayes net is a directed graph X -> Y. The Bayes net is fully specified, in both cases, when P(X) and P(Y | X) are specified. Or, strictly speaking when P(X | a) and P(Y | X, b) are specified where a and b are the parameters governing the marginal distribution of X and the conditional distribution of Y given X, respectively. Parametric Assumptions If we are happy for some reason to assume that P(X | a) is Normal, so a contains its mean and its variance. Consider case 1. Perhaps we are happy to assume that P(Y | X, b) is also Normal and that the conditional relationship is linear in X. Then b contains regression coefficients, as we normally understand them. That means here: an intercept, a slope parameter, and a conditional variance. Now consider case 2. Perhaps we are willing to assume that P(Y | X, b) is Binomial and that the conditional relationship is linear in X. Then b contains the coefficients from a logistic regression model: here that means an intercept and a slope parameter. In case 1 if we observe Y and try to infer X then we simply use Bayes theorem with the parametric assumptions noted above. The result is rather well known: the (posterior) mean of X given Y is a weighted average of the prior mean of X (gotten from a) and the observed value of Y, where the weights are a function of the standard deviations of X (from a) and Y given X (from b). More certainty about X pulls the posterior mean towards its mean. In case 2 there may be no handy closed form for P(X | Y) so we might proceed by drawing a sample from P(X), making use of your assumptions about P(Y | X) and normalising the result. That is then a sample from P(X | Y). In both cases, all we are doing is computing Bayes theorem, for which we need some parametric assumptions about the relevant distributions. All this gets more complicated when you want to learn a and b, rather than assuming them, but that is, I think, beyond your question. Sampling When thinking about continuous parents it may be helpful to think about generating data from the network. To make on data point form the network in case 1 (with the assumptions as above) we would first sample from P(X) - a Normal distribution - then take that value and generate a conditional distribution over Y by plugging the value we generated into the regression model of P(Y | X) as an independent variable. That conditional distribution is also Normal (by assumption) so we then sample a Y value from that distribution. We now have a new data point. The same process applies when P(Y | X) is a logistic regression model. The model gives us a probability of 1 vs. 0 given the value of X we sampled. We then sample from that distribution - essentially tossing a coin biased according to the model probabilities - and we have a new data point. When we think about inference in a Bayes net, we are, conceptually speaking, just reversing this process. Perhaps that helps?
