[site]: datascience
[post_id]: 123131
[parent_id]: 123062
[tags]: 
Take a look at the ALiBi paper: https://arxiv.org/abs/2108.12409 For me, the takeaways were: The sin/cos idea in the "Attention is All You Need" added complexity in the hope it would extrapolate beyond sentence lengths seen in training. Turns out it doesn't. The relative positional embedding ideas used in T5 and GPT-J work better, but still not that well. It was chosen for and works well in BLOOM I think ALiBi results only apply to a decoder-only transformer; I need to track down if there has been follow-up work for encoder-only and encoder-decoder transformers. Your simple idea will also work, as it has a different value for each position. I suspect it won't extrapolate well, and I suspect it will be inferior to the more established ways, mainly as it is an obvious approach, implying people have tried it. (I'm sure I've seen it mentioned in a paper before, but couldn't tell you which one, sorry.)
