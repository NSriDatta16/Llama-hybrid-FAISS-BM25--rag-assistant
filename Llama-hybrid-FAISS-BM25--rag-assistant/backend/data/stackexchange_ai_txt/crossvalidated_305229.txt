[site]: crossvalidated
[post_id]: 305229
[parent_id]: 
[tags]: 
PRML: Why the elements of y(x) sums to 1? (Chapter 4.1.3)

I could not understand the reason why PRML ( Pattern Recognition and Machine Learning by Christopher Michael Bishop ) says: An interesting property of least-squares solutions with multiple target variables is that if every target vector in the training set satisfies some linear constraint $$ a^\top t_n+b=0 \tag{4.18} $$ for some constants a and b, then the model prediction for any value of x will satisfy the same constraint so that $$ a^\top y(x)+b=0. \tag{4.19} $$ Thus if we use a 1-of-K coding scheme for K classes, then the predictions made by the model will have the property that the elements of y(x) will sum to 1 for any value of x. Why? If the element of $y(x)$ sum to 1, it should be that $\mathbb{a} = \mathbb{1}$ and $b=-1$.
