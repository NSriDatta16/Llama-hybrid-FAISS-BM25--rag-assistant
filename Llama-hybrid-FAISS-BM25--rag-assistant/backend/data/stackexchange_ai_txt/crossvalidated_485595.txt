[site]: crossvalidated
[post_id]: 485595
[parent_id]: 485583
[tags]: 
In the early days of CNNs, feature/kernel sizes were as large as 11x11 or 13x13. These large kernels create a large number of trainable parameters per kernel - 121 and 169, resp. - which contributes to a problem of vanishing or exploding gradients However, it has been found that smaller kernels - typically 5x5 or 3x3 - provide similar performance while lowering the gradient problem. (I cannot find the specific paper that investigated this, but if I find it I'll link it here.) Also, while a larger kernel can identify more complex features, keeping kernels smaller allows them to find more basic features that may be pertinent to different types of images. These simpler features are usually edges or gradients which can then be combined into more complex features in later layers of the network.
