[site]: datascience
[post_id]: 109861
[parent_id]: 
[tags]: 
is my confusion matrix is showing differently or is it correctly showing?

I am totally new to Python and machine learning. I have only basic knowledge in both fields. I am trying to train pre-trained model with other data. Below is my code please look into it. I have warning saying precision value is set to 0 and also my confusion matrix picture is also attached. And when I check the test data and predicted data using the set() function, there is only one label in predicted data. Can any one help me how to solve this? import pandas as pd from pathlib import Path import os.path from keras.applications.inception_v3 import InceptionV3 from keras.models import Model, load_model from keras_preprocessing.image import ImageDataGenerator from sklearn.model_selection import train_test_split import tensorflow as tf import numpy as np from keras.layers import MaxPooling2D, GlobalAveragePooling2D, Dropout, Dense import matplotlib.pyplot as plt from sklearn import metrics from sklearn.metrics import classification_report from sklearn.metrics import confusion_matrix import seaborn as sns image_dir = Path('gaussian_filtered_images') # Get filepaths and labels filepaths = list(image_dir.glob(r'**/*.png')) labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths)) filepaths = pd.Series(filepaths, name='image_path').astype(str) labels = pd.Series(labels, name='stage') # Concatenate filepaths and labels image_df = pd.concat([filepaths, labels], axis=1) # Shuffle the DataFrame and reset index image_df = image_df.sample(frac=1).reset_index(drop = True) train_df, test_df = train_test_split(image_df, train_size=0.9, shuffle=True, random_state=1) train_gen = ImageDataGenerator( preprocessing_function=tf.keras.applications.inception_v3.preprocess_input, rotation_range=90, horizontal_flip=True, vertical_flip=True, zoom_range=0.2, fill_mode='nearest', validation_split=0.1 ) test_gen = ImageDataGenerator( preprocessing_function=tf.keras.applications.inception_v3.preprocess_input, ) train_images = train_gen.flow_from_dataframe( train_df, x_col='image_path', y_col='stage', shuffle=True, subset='training', color_mode='rgb', class_mode='categorical', target_size=(224, 224), batch_size=32, seed=0 ) validation_images = train_gen.flow_from_dataframe( train_df, x_col='image_path', y_col='stage', shuffle=True, subset='training', color_mode='rgb', class_mode='categorical', target_size=(224, 224), batch_size=32, seed=0 ) test_images = train_gen.flow_from_dataframe( test_df, x_col='image_path', y_col='stage', shuffle=False, color_mode='rgb', class_mode='categorical', target_size=(224, 224), batch_size=32 ) model = load_model('inception_V3.h5') results = model.evaluate(test_images, verbose=0) print(" ## Test Loss NORM: {:.5f}".format(results[0])) print("## Accuracy on the test set NORM: {:.2f}%".format(results[1] * 100)) # Predict the label of the test_images pred = model.predict(test_images) pred = np.argmax(pred, axis=1) # Map the label labels = train_images.class_indices labels = dict((v, k) for k, v in labels.items()) pred = [labels[k] for k in pred] # Display the result print(f'The first 5 predictions NORM: {pred[:5]}') y_test = list(test_df.stage) print(set(y_test)) print("---------------------") print(set(pred)) print("---------------------") print(set(y_test) - set(pred)) print('Accuracy NORM:', np.round(metrics.accuracy_score(y_test, pred), 5)) print('Precision NORM:', np.round(metrics.precision_score(y_test, pred, average='weighted'), 5)) print('Recall NORM:', np.round(metrics.recall_score(y_test, pred, average='weighted'), 5)) print('F1 Score NORM:', np.round(metrics.f1_score(y_test, pred, average='weighted'), 5)) print('Cohen Kappa Score NORM:', np.round(metrics.cohen_kappa_score(y_test, pred), 5)) print(classification_report(y_test, pred)) cf_matrix = confusion_matrix(y_test, pred, normalize='true') plt.figure(figsize=(10, 6)) sns.heatmap(cf_matrix, annot=True, xticklabels=sorted(set(y_test)), yticklabels=sorted(set(y_test))) plt.title('Normalized Confusion Matrix NORM') plt.show() OUTPUT ## Test Loss NORM: 1.35346 ## Accuracy on the test set NORM: 50.41% The first 5 predictions NORM: ['Normal', 'Normal', 'Normal', 'Normal', 'Normal'] {'Proliferate', 'Severe', 'Normal', 'Moderate', 'Mild'} --------------------- {'Normal'} --------------------- {'Mild', 'Moderate', 'Proliferate', 'Severe'} Accuracy NORM: 0.50409 D:\NEW_DRD\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) Precision NORM: 0.2541 Recall NORM: 0.50409 F1 Score NORM: 0.33788 Cohen Kappa Score NORM: 0.0 D:\NEW_DRD\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) D:\NEW_DRD\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) D:\NEW_DRD\lib\site-packages\sklearn\metrics\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) precision recall f1-score support Mild 0.00 0.00 0.00 31 Moderate 0.00 0.00 0.00 101 Normal 0.50 1.00 0.67 185 Proliferate 0.00 0.00 0.00 28 Severe 0.00 0.00 0.00 22 accuracy 0.50 367 macro avg 0.10 0.20 0.13 367 weighted avg 0.25 0.50 0.34 367
