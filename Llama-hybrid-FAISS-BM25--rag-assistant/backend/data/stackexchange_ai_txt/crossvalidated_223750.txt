[site]: crossvalidated
[post_id]: 223750
[parent_id]: 
[tags]: 
Quantify compatibility between posterior estimates

I am performing $n$ distinct, independent experiment $E_1$, $E_2$, $\ldots, E_n$ to ideally measure the same quantity $X \in \mathbb{R}$ of interest. For each experiment, I can compute the posterior pdf of $X$ via MCMC, according to some underlying model, which we call $f_1(x)$, $f_2(x)$, $\ldots, f_n(x)$. There is a chance, hard to quantify and model, that the experiments are not measuring the same thing. What would be the most reasonable/established method to: test whether the $n$ estimates are compatible (we can focus for simplicity on the case $n =2$); and/or quantify such compatibility? For example, as a test I could simply compare the 95% credible intervals (computed via empirical quantiles of the MCMC samples), and check that they all overlap; as a quantitative metric I could perhaps report the percentage of overlap between the 95% CI, or something along these lines.
