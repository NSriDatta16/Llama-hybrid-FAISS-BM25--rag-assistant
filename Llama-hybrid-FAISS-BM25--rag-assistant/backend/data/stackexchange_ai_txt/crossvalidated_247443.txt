[site]: crossvalidated
[post_id]: 247443
[parent_id]: 222584
[tags]: 
Standard RNNs (Recurrent Neural Networks) suffer from vanishing and exploding gradient problems. LSTMs (Long Short Term Memory) deal with these problems by introducing new gates, such as input and forget gates, which allow for a better control over the gradient flow and enable better preservation of “long-range dependencies”. The long range dependency in RNN is resolved by increasing the number of repeating layer in LSTM. RNN LSTM For Further Details : Understanding LSTM
