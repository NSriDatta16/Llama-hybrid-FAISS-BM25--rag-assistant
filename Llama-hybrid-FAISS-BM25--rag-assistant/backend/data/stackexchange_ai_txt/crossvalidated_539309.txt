[site]: crossvalidated
[post_id]: 539309
[parent_id]: 41016
[tags]: 
Inter-rater reliability does not increase with more raters because inter-rater reliability is (loosely) the average agreement between raters. Additional raters do not change how often (or the degree to which) raters agree. Inter-rater reliability is instead affected by the skill of the raters (relative to the difficulty of the task) and the degree to which raters are making the same assessment (i.e., if raters understand the task in the same way). However, more raters will increase the reliability of the average rating, which is presumably what the OP was wondering. That is, if you have a single rater, the correlation between their ratings and the average rating of the population of potential raters would be the inter-rater reliability of the task. However, if we had more than one rater, the correlation between the average of their ratings and the population of potential raters would be greater than the inter-rater reliability. Thus, if inter-rater reliability is lower than required, ratings from multiple raters can be aggregated to reduce the noise of individual ratings. A better question might then be, "How many raters are required to get an aggregate rater reliability of x, if inter-rater reliability is y?"
