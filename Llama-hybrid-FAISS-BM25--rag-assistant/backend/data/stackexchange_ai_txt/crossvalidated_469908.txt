[site]: crossvalidated
[post_id]: 469908
[parent_id]: 
[tags]: 
why in Gaussian linear regression they put $y_i-\theta^Tx_i$ instead of $x$?

I am new to the machine learning area. Then I am studying a paper that considers the problem of logistic regression in Bayesian. In general, in regression or logistic regression when they assume the prior or likelihood is Gaussian why in the formula of Gaussian $p(x)= \frac{1}{\sqrt{2\pi \sigma^2}}\mathrm{exp}(-\frac{(x-\mu)^2}{2\sigma^2})$ instead of $x$ they put $y_i-\theta^tx_i$ ?
