[site]: crossvalidated
[post_id]: 36347
[parent_id]: 35971
[tags]: 
In the areas where Big Data is gaining popularity: Search, Advertising, Recommender Systems like Amazon, Netflix , there is a very Big incentive to explore the entire data set. The objective of these systems is to tailor recommendations / suggestions to every single member of the population. Also, the number of attributes being studied is enormous. The average web analytics system may measure click-through rate, "thermal tracking" of the "hot areas" in a page, social interactions, etc and weigh these against a large set of predetermined objectives. More importantly, most of the places where Big Data is now ubiquitous are "online" data streams i.e data is constantly being added / updated. Devising a sampling scheme which covers all these attributes without an inherent bias and still deliver promising results (read better margins) is a challenge. Sampling still remains highly relevant for surveys, medical trials, A/B testing, quality assurance. In a nutshell, sampling is very useful when the population to be studied is very large and you are interested in the macroscopic properties of the population. 100% checking (Big Data) is necessary for exploiting the microscopic properties of the system Hope this helps :)
