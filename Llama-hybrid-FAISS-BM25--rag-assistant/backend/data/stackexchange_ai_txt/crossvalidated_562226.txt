[site]: crossvalidated
[post_id]: 562226
[parent_id]: 562223
[tags]: 
It's a good question. If you don't want to just keep that rating in the average (because it pulls it too hard on the average), but if you also don't want to completely delete it (therefore just tossing out any "dissenting" opinion), I would recommend trying to calculate the median rating, rather than the mean. This way, their rating will still have an impact, but nowhere near the same heavy "pull" as when calculating mean. EDIT: Curious about whuber's suggestion in the comments to this answer, I did a quick "back-of-the-envelope" test of median vs. Winsorized mean. I gave my code below. But essentially what I did is generate a normally distributed cluster of 7 ratings, and then added an outlier (randomly placed, but far away from the main cluster of ratings) on the 0-10 scale. I then calculated the arithmetic mean, the median and the Winsorized mean, to see where the Wind. mean and the median end up relative to the arithmetic mean. I repeated this 1000 times just to get an average difference between the mean vs. Wind. Mean and median. Just to give some examples When the main group averaged a rating of 4 and the outlier was around 8, here's the average result: Arithmetic mean: 4.49 Median: 4.17 Wind. Mean: 4.24 When the main group averaged a rating of 9 and the outlier was around 3, here's the average result: Arithmetic mean: 8.17 Median: 8.83 Wind. Mean: 8.72 When the main group averaged a rating of 5 and the outlier was around 10, here's the average result: Arithmetic mean: 5.57 Median: 5.17 Wind. Mean: 5.23 So you can see that the median consistently gives slightly less weight to the outlier than Wind. mean, but generally the two results are quite close. Maybe it's splitting hairs here, I'm not sure. Something to note, though, is that the median ratings also had a slighter higher variance (over the 1000 trials), than either the arithmetic mean or the Wind. mean. Here's my R code: set.seed(100) arithmean 10){ ratings[i] EDIT 2: I think whuber brings up an excellent point in the comments that in an ideal scenario, the raters' opinion should be weighted by some metric of how "valuable" it is. Perhaps it's worth trying to do a weighted average, where the weight is determined by some measure of the rater's experience/reliability, so that if a rater is an outlier but has great experience and insight, they might be weighted more heavily and vice versa. Maybe even a very simple implementation of this would be useful (ex: "senior" raters get 1.25x or 1.5x weight to their ratings).
