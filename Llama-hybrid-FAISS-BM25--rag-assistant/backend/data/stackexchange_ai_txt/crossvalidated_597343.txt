[site]: crossvalidated
[post_id]: 597343
[parent_id]: 
[tags]: 
How are the weight matrices in self-attention learned?

Learning the weights of logistic regression using gradient descent is quite intuitive. The input $x$ is multiplied with the weight $w$ to produce $y$ , and we know the true target $\hat{y}$ . Therefore, during backpropagation, we tweak the value of $w$ so that the next $y$ is closer to $\hat{y}$ . A self-attention module has a query, key, and value matrices trained with the same target. How are these respective weights learned during gradient descent?
