[site]: crossvalidated
[post_id]: 290557
[parent_id]: 
[tags]: 
Under what conditions is posterior expected 0-1 loss minimized by the MAP estimate?

Suppose we observe data $x$ from a distribution with density function $p_\theta(x)$ with unknown $\theta$, we have a prior density $\pi(\theta)$, and we want to minimize the following loss function: $$L_\epsilon(\theta, \hat{\theta}) = \begin{cases} 0 & \text{if } |\theta-\hat{\theta}| section 4.1.2 of The Bayesian Choice . And I believe it, at least for discrete $\theta$ and for continuous $\theta$ where the density functions $p_\theta(x) \pi(\theta)$ are smooth in some sense. What I can't find is, exactly what sense of smoothness do we need? What are the general conditions under which this holds? And when does it fail, and why? I get the intuition behind it (explained very well in these lecture notes , by the way). What I'm looking for are the mathematical conditions.
