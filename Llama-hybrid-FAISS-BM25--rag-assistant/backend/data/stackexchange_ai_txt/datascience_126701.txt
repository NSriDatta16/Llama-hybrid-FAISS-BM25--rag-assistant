[site]: datascience
[post_id]: 126701
[parent_id]: 
[tags]: 
Scaling imbalanced binary features

I am interested in a discussion in encoding and scaling categorical features, notably imbalanced categorical features. The context is neural networks (gbdts should handle this easily). It is known that numerical values should be rescaled with a mean near zero and a covariance constant (1?) since at least Lecun 98 . However for rare event this mean it will bring unusually high values. Typically, having a 1 in 1000 features: x = np.random.random(1000000)>0.999 np.unique(((x-x.mean())/x.std())) imply having two values after rescaling: array([-3.01134784e-02, 3.32077214e+01]) That is 0.03 and 30. Is this the correct way to go? Are there any strong source on this?
