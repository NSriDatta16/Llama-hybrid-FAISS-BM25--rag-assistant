[site]: crossvalidated
[post_id]: 608911
[parent_id]: 533395
[tags]: 
It is. For instance, the Python machine learning package sklearn has the sklearn.metrics.r2_score function that can be applied to either in-sample or out-of-sample data. Others have thought about an out-of-sample $R^2$ (1) , (2) . It even has a relationship to the common PRESS statistic . As far as why an in-sample measure is used at all, there are a few reasons. One is that the difference between an in-sample and out-of-sample metric can shine light on overfitting issues. Another is that out-of-sample testing is a more advanced topic that, for better or for worse, isn't covered in the introductory statistics classes. Third, for simple models, the usual in-sample $R^2$ probably isn't such a high-biased estimator, and the little bit of optimistic bias might be offset (in some sense) by the ease of interpreting the value as the correlation between the predictions and true values in the OLS linear regression setting that is covered in introductory courses. I suspect that even adjusted $R^2$ , which intends to account for the optimistic bias by penalizing the model for having many parameters, is not covered in the introductory classes. Finally, it isn't totally clear what an out-of-sample $R^2$ should be. For instance, I totally disagree with the sklearn implementation (even if my formula and the sklearn formula should not differ by much, and a large disagreement would be a signal about some kind of data drift, which is enormously problematic for separate reasons). With that in mind, what exactly would you calculate as your out-of-sample $R^2$ , and how do you explain that calculation to stakeholders who only took the introductory class and only know the usual $R^2$ formula? Overall, I can see why in-sample $R^2$ values are calculated, even if I absolutely see value to more sophisticated forms of model evaluation.
