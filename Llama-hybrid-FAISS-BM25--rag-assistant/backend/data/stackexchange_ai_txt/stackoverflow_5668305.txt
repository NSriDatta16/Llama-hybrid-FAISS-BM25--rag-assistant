[site]: stackoverflow
[post_id]: 5668305
[parent_id]: 5664224
[tags]: 
Let me assume the reason you are doing this is you want to locate any performance problems (bottlenecks) so you can fix them to get higher performance. As opposed to measuring speed or getting coverage info. It seems you're thinking the way to do this is to log the history of function calls and measure how long each call takes. There's a different approach. It's based on the idea that mainly the program walks a big call tree. If time is being wasted it is because the call tree is more bushy than necessary, and during the time that's being wasted, the code that's doing the wasting is visible on the stack. It can be terminal instructions, but more likely function calls, at almost any level of the stack. Simply pausing the program under a debugger a few times will eventually display it. Anything you see it doing, on more than one stack sample, if you can improve it, will speed up the program. It works whether or not the time is being spent in CPU, I/O or anything else that consumes wall clock time. What it doesn't show you is tons of stuff you don't need to know. The only way it can not show you bottlenecks is if they are very small, in which case the code is pretty near optimal. Here's more of an explanation.
