[site]: crossvalidated
[post_id]: 533028
[parent_id]: 
[tags]: 
How does LSTM/RNN deal with the correlated samples and the samples with different lengths

We know that LSTM/RNN has the following advantages compared with the basic machine learning algorithms: Account the correlation between samples (usually the samples from time series are always correlated) I agree that the hidden layer of RNN accounts the correlation between states within one sequential sample, however where does RNN reflect the correlation between samples? For example, Gaussian process regression reflects the correlation between sample between their correlation matrix, e.t.c. Performs well on the samples with different lengths like time series. At least, from the following reference, in Keras, LSTM/RNN just simple truncate and pad the raw input sequential samples to obtain the sequence samples with a fixed length and width. I think this way can be applied to any machine learning algorithms, how is it specific to RNN/LSTM? https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/
