[site]: datascience
[post_id]: 98399
[parent_id]: 65241
[tags]: 
First need to understand what problems BERT can solve or what kind of inference/prediction it can achieve. BERT Neural Network - EXPLAINED! Encoder in Transformer itself can learn: Relations among words (what word is most probable in a context). For instance, what word will fit in the BLANK in the context I take [BLANK] of the opportunity . Relations among sentences. For instance A: "In a glossary store" can follow B: "I bought the ingredients". Having these traits or capabilities, BERT can predict a word to follow a sequence of words. BERT can classify if a text is negative or positive. As far as you can achieve the predictions you want with the Encoder part only, you do not need the Decoder. Hence it would be better focus on what problems require Decoder. Or what problems BERT cannot solve.
