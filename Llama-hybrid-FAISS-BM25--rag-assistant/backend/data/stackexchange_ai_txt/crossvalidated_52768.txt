[site]: crossvalidated
[post_id]: 52768
[parent_id]: 50254
[tags]: 
Unless you want to see if you can implement training and decoding algorithms of HMMs, you can easily use existing toolkits such as the HTK (HMM toolkit for speech recognition). The advantage of using this is that it makes the job easier for you. Also it has very good documentation and examples for beginners. However, if your goal is to implement these algorithms, it requires a very good understanding of HMMs and coding skills(trust me). In order to use HMMs for useful applications, 3 basic problems have to be solved: Evaluation problem: given a set of HMMs, how to determine the probable model that generated the test data. The solution is either the forward or backward algorithm. These are iterative algorithms that make this evaluation simpler. Determining the best state sequence: this is where the hidden state is uncovered and the solution is the Viterbi algorithm. Training problem: this deals with estimating the parameters of an HMM but unlike the previous 2 problems, there is no closed form solution for this. However, the Baum-Welch algorithm is commonly used. This is a variant of the well known Expectation-Maximation (EM) algorithm. References Rabiner, L. 1989. A tutorial on hidden Markov models and selected appli- cations in speech recognition. Baum, L.E., T. Petrie, G. Soules and N. Weiss. 1970. A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains. The Annals of Mathematical Statistics Vol. 41, No. 1, pp. 164-171. Dempster, A. P., N. M. Laird, and D. B. Rubin. 1977. Maximum likeli-hood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B (Methodological) 39 (1), pp. 1-38.
