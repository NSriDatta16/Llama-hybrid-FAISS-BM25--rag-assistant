[site]: datascience
[post_id]: 121369
[parent_id]: 
[tags]: 
Learning from aggregated data

Online and in the literature there seems to be a general consensus that training a machine learning model using aggregated data is harder and/or fundamentally different from training on raw event data. I am unable to intuit why this would be the case. Lets say for example we have a data set from the online advertising domain: feature_1 feature_2 feature_3 click a g z 0 b f z 1 We could group by each of the categorical features and instead of a binary click target have a click through rate (CTR) target which is calculate as sum(clicks)/sum(displays)*100 . We could even decide the threshold for CTR, for good / bad CTR and convert that aggregated data table back into a binary classification problem. Now I am unable to understand why the two datasets differ when fed into a model, in the raw event case, the model will see each example and over many passes learn the aggregation, i.e what is the probability of a click given a set of features. Now this question also bring me to another thought - what is a model doing differently here VS just aggregating the historical data and calculating the historical probs of a click? If we have all possible feature crosses in our dataset, is even the most complex DL model somehow able to learn something more superior, and if yes - how?
