[site]: datascience
[post_id]: 35647
[parent_id]: 35640
[tags]: 
I thought of Experience and Replay as a mechanism to handle this delayed effect. Experience Replay will not help directly here, it is a mechanism to make learning more stable and effective from limited data, but it will not address delayed returns. Any approach which learns Q should cope fine with delayed rewards. So Monte Carlo Control, SARSA, Q-learning, DQN and all their variants are in theory capable of learning the delayed reward. The total reward is the ultimate effect of any action. However, I'm guessing just because it is not mentioned and you are asking this question, that you may still have a problem . . . You claim that the action has a "delayed effect on the environment". You need to change that, because it implies that your state representation is incorrect for your problem. Clearly making an order for something changes the environment. An environment with an order in progress is different, in a way that is critically important to your problem, than one without an order in progress. Probably what you are missing is a state representation that captures what your action has actually done. Without that, there are hidden variables (orders currently being handled) that a Q function cannot learn about because it is not in the state, $s$ for $Q(s,a)$. For RL to be reliable, the state value $s$ has to capture all relevant information about how future state changes and rewards will progress. For instance, if you were writing an agent to control a swinging pendulum (a standard toy problem), then you don't just need the position of the pendulum, you also want its velocity in order to predict where the pendulum might end up before any action is taken. For your issue, the current stock is like the pendulum's position, and you need to track orders "in progress" as they are a bit like velocity in that they will cause further changes to state regardless of action. To address this, you will need to add a representation of recent stock orders to your state. I would guess something that describes the contents of the order (similar to your current stock representation) and a countdown for how many days left until it is delivered (or if they are not that reliable, something similar that correlates with likely arrival time). That state should be changed immediately in response to the action that caused the order, otherwise the agent will not learn the association between action and its effects, and will treat orders arriving as some kind of random effect from the environment. Once you have a representation like this, then I think that is enough, and the next steps are finding the right hyper-parameter values to learn effectively. This will be easier initially if you have a simulated environment, although you could also learn off-policy from historical data using something like DQN, provided you have a lot of historical data to work with. I thought of just adding the quantity reordered to the shelf at a later time and let the agent learn it's effects. Will this suffice? I think this is similar, but probably not enough. The most important thing to do is associate change to state with the action that caused it. Adding the new stock as it arrives you should do anyway, but the agent needs to learn that the earlier action is what caused this, and that is only possible if that action actually changes the state in some way.
