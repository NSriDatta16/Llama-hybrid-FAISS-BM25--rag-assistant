[site]: crossvalidated
[post_id]: 455992
[parent_id]: 
[tags]: 
Transformers for embedding sequences as fixed-length vectors

An encoder decoder network of recurrent neural networks can be used to learn the identity function over some set of sequences. If you do this without attention, the output of the decoder can be thought of as a fixed length representation of these sequences. I've been calling this idea a recurrent autoencoder, but I haven't seen it explored by anyone else, yet. This could be very useful for machine learning tasks on sequences, since you can learn from fixed-length vectors. (Especially if you have lots of unlabeled data, but a small amount of labels) My question is, "Can a similar thing be done with Transformers?" I see that this question is a bit vague, but this is my best effort at asking it in a way that could be answered.
