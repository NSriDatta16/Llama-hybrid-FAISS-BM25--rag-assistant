[site]: datascience
[post_id]: 71478
[parent_id]: 
[tags]: 
Keras LSTM using timesteps = 1 and train_on_batch

I'm using an LSTM to achieve a classification problem. I have a dataset composed by sentences, each sentence is composed by a variable number of words and I have to predict a label for each word of each sentence. For example, I have a dataset of shape (300000, 800) , so 300000 words and each word is made of 800 features (word embeddings, etc...). Moreover, each sentence is divided in subtrees, which each subtree is a syntactic dependency tree between words. My task is to learn the dependencies between words of a subtree. I thought to train the network using timesteps=1 and the train_on_batch function, where the batch size is variable and equal to the subtree cardinality. Is it a reasonable process?
