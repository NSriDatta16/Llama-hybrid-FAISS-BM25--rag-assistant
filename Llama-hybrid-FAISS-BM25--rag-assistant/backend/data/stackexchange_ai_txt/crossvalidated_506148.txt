[site]: crossvalidated
[post_id]: 506148
[parent_id]: 
[tags]: 
Is there a way to do such kind of smoothing for log likelihood?

Let's say we have a sequential decision making problem. At each step, we need to make a decision, and the decision made in this step will determine the possible actions for the next step. Now I have a trained model to tell me the likelihood of each action at each step, and I want to use this model to find the optimal decision sequence. One straightforward way to do this is to use beam search to find the sequence with the largest sum of log likelihood along the sequence. However, this will be biased to decision sequence with less candidate actions along the path. For example, the k-th step of the first beam only have one possible action, while the k-th step of the second beam have 1000 possible actions for the second beam, and the sum of log-likelihood for the first two beams are same at the first k-1 steps. The model will be biased to the first beam because the log likelihood for the k-th step can only be log1 = 0 , while for the second beam, even if the model tells us the probability of taking a certain action is 999/1000 , it will still be ranked lower than the first beam, because log(999/1000) . I know this kind of bias makes sense in some scenario, but in my scenario, I don't really want it. Is there any smoothing method can address this issue? I feel Bayesian smoothing might be a potential solution, but I am wondering is there any smoothing method more specific to log likelihood. Edit: No, actually I think Bayesian smoothing won't work, because there is not really a way to estimate a prior here.
