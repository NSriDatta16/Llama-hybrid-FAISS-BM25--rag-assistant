[site]: crossvalidated
[post_id]: 372815
[parent_id]: 
[tags]: 
Is the parameter vector of an indentifiable distribution of a transformed random vector always a subvector...?

I would like, after further considerations about this problem, to reformulate this question of mine again. I kept a record of the past words and remarks as the appendix below. I think the question that really bothers me can be stated as follows: If $\theta \in \mathbb{R}^{k}$ , if $W$ is a random vector in $\mathbb{R}^{r}$ from a distribution having $\theta$ as the value of its parameter vector, if $q \leq k$ , if $f: \mathbb{R}^{r} \times \mathbb{R}^{q} \to \mathbb{R}$ is measurable (in the suitable sense), if the distributions $\{ \mathscr{P}_{f(W, b)} \}_{b \in \mathbb{R}^{q}}$ induced by the random variable $f(W, \cdot)$ is identifiable as $\mathbb{R}^{q}$ , and if (at no cost of specialization as can be examined) $\beta$ is such that the first moment of $\mathscr{P}_{f(W, \beta)}$ is $= 0$ , then is it necessary that $\beta$ is a subvector of $\theta$ ? An example of the above more general setting is a orthogonal projection model (identifiable linear model with stochastic regressors). It took a long while to raise the question in my mind in the current form. I could say it would be what is closest to what I would like to know at the current stage. Note. Words and remarks before the above revised version of this question are in the following paragraphs: Let $p \in \mathbb{N}$ ; let $\theta \in \mathbb{R}^{p}$ ; let $(X,Y) \sim F^{\theta}_{X,Y}$ be a random vector in $\mathbb{R}^{2}$ such that $F^{\theta}_{X,Y}$ is the joint CDF of $X$ and $Y$ . If there is exactly one $b \in \mathbb{R}$ such that $Y = Xb + U$ and $\mathbb{E}(U \mid X) = 0$ , is it necessary that $b$ is a component of $\theta$ ? How to prove it no matter the answer is affirmative or not? Thanks (for sure at least to Whuber's questions below that pushed to form the current neat form of the question). It seems that until now did I realize the crux of my question. Thanks again to the feedback providers below, directly or not. Let me use this simple example below to illustrate my confusion. I hope this example would also explain the first paragraph better. Suppose the expectation $\mathbb{E}(Y - Xb)^{2}$ is finite. If $F_{U}^{b}$ is the CDF of $U$ , then the expectation can be obtained via two integrals, namely $$ \mathbb{E} (Y - Xb)^{2} = \int_{\mathbb{R}^{2}} (y - xb)^{2} dF_{X,Y}(x,y) = \int_{\mathbb{R}} u^{2} dF^{b}_{U}(u). $$ If the expectation is regarded as the last integral, then it depends on $b$ by assumption. I wonder if $F_{X,Y}$ also depends on $b$ when regarded as the first integral? The notation $\mathbb{E}(Y - Xb)^{2}$ itself provides no information about the distinction, correct? This example does not fit the question title very much; however, I would ask the reader to examine it with a more careless viewpoint unless this mismatch does make the reader completely clueless.
