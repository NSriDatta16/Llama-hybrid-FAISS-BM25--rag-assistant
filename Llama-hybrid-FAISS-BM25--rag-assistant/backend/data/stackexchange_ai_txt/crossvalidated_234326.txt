[site]: crossvalidated
[post_id]: 234326
[parent_id]: 234321
[tags]: 
Oh dear -- looks like you fell into the extrapolation-trap! Remember that most statistical methods do no "predicting" at all -- it's actually quite an underappriciated -- but nuanced -- idea.* What most are actually doing is simply finding most representative values, according to association. Looking at your code you are generating a training set by picking n=500 random integers in the values range of 10 to 1000, then you ask your models to extrapolate outside of that range to predict the square of a value of 7 -- which is not in the range [10, 1000] I ran your code by changing the range from 0 to 20 as follows: # Generate lot of random integers for train data x = np.random.randint(0, 50, (500, 1)) And found better results: ('LinearRegression', array([-64.08766168])) ('LogisticRegression', array([36])) ('KNeighborsClassifier', array([49])) ('DecisionTreeClassifier', array([49])) ('GaussianNB', array([49])) ('SVC', array([49])) Please note that your results are poor for both linear and logistic regression, as should be expected. Both of these methods are strictly linear, by construction, and would therefore poorly predict an exponential function. The others are closer to "picking correctly based on past observations" (to simplify extremely), and so long as your dataset well covers the x-value to predict (7, in this case), you should get good results. Improve your results by generating more training data around the value of 7 by modifying the upper and lower bounds of the "randint" function. *For deeper discussion on the idea see the eloquent answer to this stack exchange question
