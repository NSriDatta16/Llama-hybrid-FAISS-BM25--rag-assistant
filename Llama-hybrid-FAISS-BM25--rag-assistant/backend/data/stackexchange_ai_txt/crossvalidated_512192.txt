[site]: crossvalidated
[post_id]: 512192
[parent_id]: 512182
[tags]: 
Preliminaries: The book Introducing Monte Carlo methods with R (no exclamation mark in the title, even though the Springer book series is called Use R! ) was co-authored by my late friend George Casella. Here is a full reproduction of the paragraph containing the quote: The study of independent Metropolis-Hastings algorithms is certainly interesting, but their practical implementation is more problematic in that they are delicate to use in complex settings because the construction of the proposal is complicated—if we are using simulation, it is often because deriving estimates like MLEs is difficult—and because the choice of the proposal is highly influential on the performance of the algorithm. Rather than building a proposal from scratch or suggesting a non-parametric approximation based on a preliminary run—because it is unlikely to work for moderate to high dimensions—it is therefore more realistic to gather information about the target stepwise, that is, by exploring the neighborhood of the current value of the chain. If the exploration mechanism has enough energy to reach as far as the boundaries of the support of the target $f$ , the method will eventually uncover the complexity of the target. (This is fundamentally the same intuition at work in the simulated annealing algorithm of Section 2.3.3 and the stochastic gradient method of Section 2.3.2.) A Metropolis-Hastings algorithm will "work", i.e., will eventually produce simulations from the target of interest if every region of the support of the target has a positive probability to be visited by the simulated Markov chain. Otherwise, the Markov chain is (practically if not theoretically) not irreducible and remains in a (strict) subset of the support of the target, hence cannot be considered as a (dependent) sample from said target. The ability to reach out to every part of the support of the target $\pi(\cdot)$ crucially depends on the proposal distribution $q(\cdot\,;\cdot)$ used in the Metropolis-Hastings algorithm: if the possible moves allowed by $q(\cdot\,;\cdot)$ are too limited with regard to the support of $\pi(\cdot)$ , the Markov chain will remain stuck in a subset of the support of $\pi(\cdot)$ and hence produce a simulation of $\pi(\cdot)$ restricted to this subset. On the opposite, if the possible moves allowed by $q(\cdot\,;\cdot)$ are wide-ranging, again with regard to the support of $\pi(\cdot)$ , the Markov chain will have a positive probability to reach everywhere in the support of $\pi(\cdot)$ (if not necessarily in one step). The term energy was used to translate this notion, inspired from physics as many terms in the area, and with no formal definition in mind. It may actually be confusing since energy is also used in association with the target, esp. in simulated annealing and with the opposite intuition, since the goal is to reach the lowest energies .
