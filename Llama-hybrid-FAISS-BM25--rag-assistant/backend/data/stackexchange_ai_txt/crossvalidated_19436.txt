[site]: crossvalidated
[post_id]: 19436
[parent_id]: 19398
[tags]: 
Just to add to @jbowman's answer (+1), while many machine learning algorithms that are commonly used today (e.g. support vector machines, radial basis funcion neural networks) involve convex optimisation problems, these algorithms generally have hyper-parameters (such as kernel and regularisation parameters). These also need to be tuned and the algorithm itself is often only convex for fixed values of the hyper-parameters. The optimisation problem for tuning the hyper-parameters on the other hand is generally non-convex, so overall the problem of local minima has not been eliminated, merely shiften from the first level of inference (optimising the model parameters) to the second level of inference (model selection i.e. optimising the hyper-parameters). It is also worth noting that seeking a global optimum of a training criterion based on the loss over a finite sample of data is a bit of a recipe for over-fitting. For example early stopping is a method often used to avoid over-fitting in multi-layer perceptron neural networks, where better generalisation is obtained by stopping even before reaching a local minimum. This not to say that convexity is not a good thing, of course!
