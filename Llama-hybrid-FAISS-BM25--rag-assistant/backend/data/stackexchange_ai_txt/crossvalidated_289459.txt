[site]: crossvalidated
[post_id]: 289459
[parent_id]: 289424
[tags]: 
It's possible to have multicollinearity without any individual correlation being high. e.g. here's a correlation matrix of 10 variates, the largest one of which (in absolute value) is below 0.3: [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [1,] - -0.264 -0.102 -0.140 -0.138 -0.115 0.156 -0.262 -0.085 -0.001 [2,] -0.264 - -0.040 -0.028 -0.216 -0.005 -0.246 -0.075 -0.004 -0.231 [3,] -0.102 -0.040 - -0.133 -0.082 -0.292 -0.144 -0.079 0.028 -0.206 [4,] -0.140 -0.028 -0.133 - -0.128 0.022 -0.249 -0.204 -0.139 -0.078 [5,] -0.138 -0.216 -0.082 -0.128 - -0.144 -0.049 -0.080 -0.116 -0.202 [6,] -0.115 -0.005 -0.292 0.022 -0.144 - -0.123 0.032 -0.131 -0.077 [7,] 0.156 -0.246 -0.144 -0.249 -0.049 -0.123 - -0.188 -0.222 0.071 [8,] -0.262 -0.075 -0.079 -0.204 -0.080 0.032 -0.188 - 0.050 -0.052 [9,] -0.085 -0.004 0.028 -0.139 -0.116 -0.131 -0.222 0.050 - -0.236 [10,] -0.001 -0.231 -0.206 -0.078 -0.202 -0.077 0.071 -0.052 -0.236 - Yet the set of variables is perfectly collinear. A VIF of 3 doesn't say "not correlated". It says the effect of the amount of multicollinearity you have on variance of estimates is not really large. Consequently I'd pay more attention to the VIF than the individual correlations, but dropping variables isn't the only option when you have multicollinearity.
