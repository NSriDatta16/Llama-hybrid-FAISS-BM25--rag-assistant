[site]: datascience
[post_id]: 54713
[parent_id]: 54708
[tags]: 
With expected values you have a fair bit of freedom to expand/resolve or not. For instance, assuming the distributions $X$ and $Y$ are independently resolved (i.e. the values are not correlated): $$\mathbb{E}[X + Y] = (\sum_x xp(x))+ \mathbb{E}[Y]$$ $$\mathbb{E}[XY] = \sum_x xp(x)\mathbb{E}[Y]$$ Each time step of a MDP is independent in this way, so you can use this when handling sums and products within expectations in the Bellman equations (provided you separate terms by time step). For the Bellman equation, the goal is to relate $v_\pi(s_t)$ to $v_\pi(s_{t+1})$ , and the definition of value is given as an expectation, so it makes sense to preserve the second expectation rather than expand it. Something has to change though, as within the second sum a time step is effectively taken from $s$ to $s'$ , so the new expectation has to include that. It has in some sense been expanded, just not fully broken down into the full product of every following policy decision and state transition. You could try to write out the full expansion from expected to products of sums over distributions using some container like $\Pi_{n=t+1}^{T}$ - showing how to calculate the expected value over full tree of all possibilities - and the maths would still work. But it would be a very longhand way of showing the same relationship.
