[site]: datascience
[post_id]: 104380
[parent_id]: 38745
[tags]: 
TLDR: Dynamically set the nlp.max_length according to the length of the document. This makes it simpler while handling documents/text of unknown length. Or you can remove some of the parts of SpaCy object pipeline which you will not need. Let us say: txt --> text document then set nlp.max_length = len(txt) + 100 (100 is just a cushion not necessary really) Example: I faced the same issue, I had to loop over a directory of text files and perform NER on the text files to extract entities present in them. for file in folder_text_files: with open(file, 'r', errors="ignore") as f: text = f.read() f.close() nlp.max_length = len(text) + 100 So doing this might help you worrying about the text size.
