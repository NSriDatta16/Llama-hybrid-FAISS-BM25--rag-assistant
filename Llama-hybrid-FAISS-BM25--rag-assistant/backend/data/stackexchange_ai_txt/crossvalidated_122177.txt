[site]: crossvalidated
[post_id]: 122177
[parent_id]: 
[tags]: 
How to do cross-validation when comparing different feature selection methods?

I am using SVM for a prediction task. My sample size is small, only N=140. Suppose I want to compare the prediction accuracy when using two different feature selection methods. Would it be better to: create a hold-out set that contains e.g. 40 samples, and train the two classifiers (with cross-validation) on the 100 training samples, or use cross-validation directly on the 140 samples but just do it twice. IF I would do (1), may I balance the training and test sets so that they are similar with respect to some variables?
