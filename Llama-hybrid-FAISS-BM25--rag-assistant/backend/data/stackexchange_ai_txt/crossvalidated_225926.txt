[site]: crossvalidated
[post_id]: 225926
[parent_id]: 225882
[tags]: 
What these results tell you is something that is almost inevitable in this type of modeling: a model based on a training set will not fit a test set as well. The type of problem you face, with a difference in the slope of the relation between your linear predictor and the outcome variable between the training and test sets, seems to be one of calibration, as noted here in the context of logistic regression. Your suggestion to "pivot" the slope of the line is similar to the general idea of "shrinking" regression coefficients to improve predictive ability on new data, but you would be better off using established methods like those provided by the rms package in R. Note that these efforts necessarily entail making a bias-variance tradeoff in predictive modeling. If you are unfamilar with that tradeoff, you should read An Introduction to Statistical Learning or a similar general reference. Also, separate training and test sets might not be the most efficient way to use your data; developing the model on the entire data set and checking and adjusting calibration by bootstrap resampling may be better. Consider consulting Frank Harrell's course notes or his book for more detail. Please check that your axes are labeled correctly. Typically one expects predicted values to be over-optimistic, with a wider range of predicted values than of observed. Also, the range of your results suggests that you might be using proportions as your outcome variable. If so, then there might be an issue if you used standard linear regression to develop your model.
