[site]: datascience
[post_id]: 13419
[parent_id]: 
[tags]: 
Python: how to handle categorial values in dataset to build models

I have a training dataframe dfTrain and the output of dfTrain.head() is shown below: C0 C1 C2 C3 C4 C5 C6 0 1 73 Not in universe 0 0 0 Not in universe 1 2 58 Self-employed-not incorporated 4 34 0 Not in universe 2 3 18 Not in universe 0 0 0 High school 3 4 9 Not in universe 0 0 0 Not in universe 4 5 10 Not in universe 0 0 0 Not in universe There are total 38 features and they are both categorical and numerical . Ignoring C1 and scaling numerical features, I am trying to build a Logistic Regression model. Since, the dataframe has categorical features, I am creating another dataframe which has dummy variables. X = pd.get_dummies(dfTrain) The shape of X now has 160 features which is much more than that of dfTrain . Then I pass X and y (where y is target variable) to Logistic Regression Classifier modelLogistic = LogisticRegression(C=10**-2, class_weight = 'balanced') modelLogistic.fit(X, y) The reason to use class_weight = 'balanced' is that there are 17 classes in y and highly imbalanced. My question is: is my approach correct? Am I missing anything?
