[site]: crossvalidated
[post_id]: 301490
[parent_id]: 301484
[tags]: 
I'm not sure I entirely understand that question, but so far as I understand it you're asking how to train a classifier to predict on samples lying outside the domain of the samples it has already seen. This is, generally speaking and so far as I know, not possible. Machine learning theory is based on the idea of "empirical risk minimization," which boils down to assuming that your training set is a good approximation of your true distribution over samples and labels. If that assumption is violated, there aren't really any guarantees. You mention unlabeled data -- I don't know if this would solve your problem, but semi-supervised learning has many methods for trying to learn classifiers given both labeled and unlabeled data, and you may want to consider looking into those (for example, transductive SVMs).
