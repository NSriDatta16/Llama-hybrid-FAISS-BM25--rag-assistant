[site]: crossvalidated
[post_id]: 501585
[parent_id]: 
[tags]: 
How is the input to the encoder noted mathematically in a variational autoencoder?

We’re calculating VAE loss (the reconstruction part) as: $$E_q[\log \space p_\theta(x|z)]$$ I don’t know what the exact breakdown is, and how it became the cross entropy between the encoder’s input and decoder’s output. It would really help if I’d know the breakdown, because it would show mathematically how is the input and output denoted. I read that the output of the decoder isn’t, $p(x|z)$ just a sample from that distribution. How is that denoted, and how is it relevant to the expected log likelihood? Also, what is the input of encoder is then? What is the connection between them mathematically, if those aren’t probability distributions? Let's call the input fed into the encoder $x$ , and the decoder's output $\hat{x}$ . In practice we're calculating: $$-(x \space \log(\hat{x}) + (1 - x)\space \log(1 - \hat{x}))$$ So the question is what should $x$ , and $\hat{x}$ be replaced with to match the notations of variational autoencoders, and what the steps are between $E_q[\log \space p_\theta(x|z)]$ and $-(x \space \log(\hat{x}) + (1 - x)\space \log(1 - \hat{x}))$ .
