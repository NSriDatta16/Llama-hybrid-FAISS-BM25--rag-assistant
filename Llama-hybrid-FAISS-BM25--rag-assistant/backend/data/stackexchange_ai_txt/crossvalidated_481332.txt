[site]: crossvalidated
[post_id]: 481332
[parent_id]: 
[tags]: 
yolo v1: why we weigh localization error more than classification error

I'm reading yolo v1 paper. I am trying to understand this part of the paper: "We use sum-squared error because it is easy to optimize, however it does not perfectly align with our goal of maximizing average precision. It weights localization error equally with classification error which may not be ideal." In the loss function, $\lambda_{coord}$ parameter is used to weigh localization loss more. It seems like the aim is to maximize average precision. By weighing localization error more, I feel like it implies that localization error plays more important role in average precision. But doesn't both classification and localization error both play equally important role? (as precision also depends on the predicted class). Any elaboration in this regard would be much appreciated.
