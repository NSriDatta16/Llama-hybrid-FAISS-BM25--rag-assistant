[site]: crossvalidated
[post_id]: 472583
[parent_id]: 
[tags]: 
How to avoid data snooping when doing a leave-one-subject-out train-test split?

I have 13 subjects, and I am trying to create a model which can do well on unseen subjects. The problem is that, when I leave one subject for the test set and train the model on others, the accuracy of the model highly depend on the selected subject for the test. I can do a train-test split 13 times, leaving every subject for the test once and only once. Then I can average the results. However, it is a machine learning principle that one should not tune the parameters based on test results. This is why we have a validation set. Of course, for each of the 13 train-test splits, I can create a leave-one-subject-out cross validation set. That is, one subject will be out for the test; from the remaining 12 subjects, each of them will be a validation set (like a 12-fold cross validation). So, the average (over 13) of the average (over 12) cross validation results is a predictor for the test results. Here comes the big problem: Isn't there a data leakage here? When subject X is left out for the test, and subject Y is selected for validation, and the others for training, the results will be very similar to the case when subject Y is chosen for the test, and the others for training. How can I do a model selection and parameter tuning without data snooping?
