[site]: crossvalidated
[post_id]: 549824
[parent_id]: 
[tags]: 
Finding the maximum point in a linear regression model

I have a set of data points that I used to train a linear model (with polynomial interpolation as described here . In my understanding, the linear model can be described as a response function approximating the function underlying the data points. My question is now: How can I find the optimum (maximum) point of that response surface / linear model? I In my example code I'd expect to get x=2.93 as a result. Since the polynomial interpolation should be an analytical function, I'd expect analytical methods of finding the optimum (= root of the derivative). But I couldn't find anything implemented in scikit-learn. Alternatively, any hints on how to apply numerical algorithms to find the maximum would be appreciated as well. from sklearn.preprocessing import SplineTransformer from sklearn.linear_model import Ridge from sklearn.pipeline import make_pipeline import numpy as np import matplotlib.pyplot as plt def Fun(x): return 1*np.sin(0.5*x)+0.2*np.sin(7*x) # data generation x_plot = np.linspace(0,6.28,300) x_train = np.linspace(0.2,6,17) y_train = Fun(x_train) y_plot = Fun(x_plot) # create 2D-array versions of these arrays to feed to transformers X_train = x_train[:, np.newaxis] X_plot = x_plot[:, np.newaxis] y_predictions=list() labels = list() # spline fits model = make_pipeline(SplineTransformer(n_knots=20, degree=3,extrapolation='linear'), Ridge(alpha=1e-3)) model.fit(X_train,y_train) y_predictions.append(model.predict(X_plot)) labels.append('spline fitting') # plotting fig, ax = plt.subplots() #plot original function ax.plot(x_plot,y_plot, 'k',label='original data') # plot training points ax.plot(x_train,y_train,'xk',label='training points') #plot fits for i in range(len(y_predictions)): ax.plot(x_plot,y_predictions[i],':',label=labels[i]) plt.legend() plt.show()
