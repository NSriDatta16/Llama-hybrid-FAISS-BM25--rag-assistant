[site]: datascience
[post_id]: 62944
[parent_id]: 61533
[tags]: 
What are you referring is a very subjective term. 1-layer NN could be called perceptron, but when you see it from other perspective, simple logistic regression have similar formulation. When people refers to MLP usually they are referring to simple stacks of perceptron layers, and usually does not make use any fancy functions. When we talk about deep learning it becomes a much broader subject. It is not only about the depth but also its complex design. For ANN nonlinearities will only applies when you apply it usually in the form of activation function. So if I stack hidden layers without applying activation function then it will only become a linear operator. You can try thinking about this for practice.
