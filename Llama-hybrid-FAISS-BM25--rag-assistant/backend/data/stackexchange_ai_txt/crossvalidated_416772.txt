[site]: crossvalidated
[post_id]: 416772
[parent_id]: 
[tags]: 
Computation time with respect to Dropout

I've been recently attempting to speed up neural network training (in PyTorch). My question is the following. Does the computation time of a given feedforward neural network vary based on Dropout percentage? So, does increasing Dropout decrease computation time? Assuming we have a network: $L_2 = \sigma(Drop(\textrm{ReLU}(X^{T} \cdot W +b))^{T} \cdot W + b_2)$ , does e.g., Dropout= 0.2 mean slower computation than Dropout=0.99? Thus, is the multiplication sparse or remains dense and as such offers no speedups? Thanks!
