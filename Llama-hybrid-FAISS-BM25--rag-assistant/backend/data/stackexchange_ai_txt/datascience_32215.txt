[site]: datascience
[post_id]: 32215
[parent_id]: 32151
[tags]: 
You can try running SVM just on this similarity matrix. But you'll then need to provide the sikikaritirs also for new data points. Furthermore, SVMs rely on the similarities bring dot products in some vector space. If they aren't you may get inconsistencies. They may rely on the triangle inequality being satisfied for the distance function d(x,y)=sqrt(2k(x,y)-k(x,x)-k(y,y)). Although I cannot find a clear reference on whether or not this is needed. If k is a scalar product in some vector space, this obviously is satisfied. Last but not least, SVMs are good for larger amounts of data, where you cannot afford to keep the entire similarity matrix in memory! By reducing the data set to the support vectors only, the resulting classifier will need much less memory and much less time. Much of the challenge of learning a SVM is to manage memory during training.
