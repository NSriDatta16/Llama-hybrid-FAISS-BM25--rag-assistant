[site]: crossvalidated
[post_id]: 470875
[parent_id]: 469485
[tags]: 
As a precursor: It is worth thinking about how these problems arise in statistical practice. Optimising over $x$ is rare - usually, $x$ has already been observed. It is more common to be optimising over $\boldsymbol{\theta}$ , given an observation $x$ , e.g. to find the maximum likelihood estimator of $\theta$ , one would solve $$\max_\boldsymbol{\theta} \left\{ \log p(\mathbf{x};\boldsymbol{\theta}) = \boldsymbol{\phi}(\mathbf{x})^\top\boldsymbol{\theta} - \log Z(\boldsymbol{\theta}) \right\}.$$ If one is aiming to optimise this function, it is clear that one needs some sort of control on $Z(\boldsymbol{\theta})$ , and/or its derivatives. To address your specific comments: Consider this thought experiment: imagine you are given an oracle who computes $Z(\boldsymbol{\theta})$ efficiently. What can you now do that you could not do before? [...] can you now compute expected values more easily? Indeed you can. If you have oracle access to $Z(\boldsymbol{\theta})$ , then you can also estimate its gradient by finite differencing. This lets you compute the specific expectation $$\nabla_\boldsymbol{\theta} \log Z(\boldsymbol{\theta}) = \mathbb{E}\left[\boldsymbol{\phi}(\mathbf{x})\right]\equiv\boldsymbol{\mu}.$$ It does not allow you to compute arbitrary expectations (unless you change to thinking about a different exponential family), but one is typically not looking for arbitrary expectations. Personally, I would rather have an oracle that tells me which regions of $\mathbf{x}-$ space to look in -- solve the search problem for me. What would this mean? This seems very close to being able to sample from $p(\mathbf{x};\boldsymbol{\theta})$ , which is of similar difficulty to computing $Z(\boldsymbol{\theta})$ . I agree that this would be a useful oracle, but it is not an easier one. This is how Self-Normalized Importance Sampling (SNIS) works - you draw samples from a proposal distribution that is essentially guess about where $\mathbf{x}$ has non-negligible mass, then plug in an estimate of $Z(\boldsymbol{\theta})$ based on those samples. The hard problem in SNIS is constructing a good proposal distribution $q$ , then you get $Z(\boldsymbol{\theta})$ "for free." Yes. For many problems of interest, constructing a good $q$ is very difficult, and is usually more difficult than computing $Z(\boldsymbol{\theta})$ . One way to find the relevant regions of $\mathbf{x}$ would be to find the mode(s) of $p$ . [...] But the difficulty of this depends on $\boldsymbol{\phi}$ ; the partition function is not involved. The extent to which this is useful will depend on the problem at hand. For calculation of expectations, in high-dimensional problems of interest, modes are not as useful as one might think, unless $p$ is very well-concentrated. The difficulty is in integration over the (many) possible states. To summarize, I see inference as having two core problems: (a) a search problem for the relevant region of $\mathbf{x}$ (high-probability regions, modes, etc.), and (b) a normalization problem of computing (log) $Z(\boldsymbol{\theta})$ . I am puzzled why the latter (b) receives so much attention, especially since solving (a) can give (b) for free, but not the other way around as far as I can tell. So, what is the intuition behind the emphasis on the log partition function? To recapitulate: (a) does not give (b) for free, nor does (b) give (a) for free. (a) is a problem of optimisation over $x$ , which does not depend (as much) on the value of $\boldsymbol{\theta}$ . (b) is a problem of integration over $x$ , which depends intimately on the value of $\boldsymbol{\theta}$ . As stated at the top of this post: statistically, you are usually interested in inference over $\theta$ , and $x$ is given already. It is thus more common to be in a situation where (b) is relevant.
