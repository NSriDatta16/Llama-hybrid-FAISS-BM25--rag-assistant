[site]: crossvalidated
[post_id]: 463372
[parent_id]: 463324
[tags]: 
In your data, the reason statsmodel crashes is due to highly correlated variables. If you look at your predictors: import seaborn as sns sns.clustermap(X_train.corr()) You can see this bunch of variables on diagonal left, perimeter,mean radius etc that is highly correlated. So if we include all of them into the fit, it is going to be hard to estimate these coefficients. We can remove these first, and fit: import statsmodels.api as sm excl = ['mean radius','mean perimeter','mean area', 'worst area','worst perimeter','worst radius', 'radius error','perimeter error','area error'] Xs = scaler.fit_transform(X_train.drop(excl,axis=1)) res = sm.Logit(y_train, Xs).fit() Optimization terminated successfully. Current function value: 0.073780 Iterations 12 This fits ok, we can check the accuracy and it's lower than what you have with scikit-learn, and I tried adding back the area, perimeter variables using PCA but it doesn't get much better: y_scores = res.predict(scaler.fit_transform(X_test.drop(excl,axis=1))) y_pred = (y_scores >0.5).astype(int) accuracy_score(y_test.values.ravel(), y_pred) 0.9210526315789473 So fitting a model via maximum likelihood like in statsmodel is going to be highly unstable with high degree of collinearity. As @Bj√∂rn pointed out, scikit-learn's logistic regression works because it by default uses a L2 penalty.
