[site]: crossvalidated
[post_id]: 559575
[parent_id]: 
[tags]: 
How does addition of a regularization term ensures that the matrix is nonsingular? ( least squares )

In Bishop's Pattern recognition book , in 3.1.2 Geometry of least squares section (page 143, last paragraph of section), it is stated that: In practice, a direct solution of the normal equations can lead to numerical difficulties when ΦTΦ is close to singular. In particular, when two or more of the basis vectors ϕj are co-linear, or nearly so, the resulting parameter values can have large magnitudes. Such near degeneracies will not be uncommon when dealing with real data sets. The resulting numerical difficulties can be addressed using the technique of singular value decomposition, or SVD (Press et al., 1992; Bishop and Nabney, 2008). Note that the addition of a regularization term ensures that the matrix is nonsingular, even in the presence of degeneracies. I did not understand how adding a regularization term can prevent Φ^TΦ matrix to be singular. This is just a sum ( λI+Φ^TΦ instead of Φ^TΦ (I: identity matrix)) Particularly, this part is confusing me: Note that the addition of a regularization term ensures that the matrix is nonsingular, even in the presence of degeneracies. Pdf of book is available here: http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf
