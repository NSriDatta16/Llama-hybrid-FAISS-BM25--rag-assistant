[site]: crossvalidated
[post_id]: 339424
[parent_id]: 339375
[tags]: 
It’s actually very simple. Just use $k-2$ folds for training, 1 for validation and 1 for testing. You probably never encountered this issue before, because fitting most of the “classical” statistical learning methods(SVMs, OLS, PLS, splines, GAMs, Gaussian Processes, etc.) corresponds to solving a convex optimization method: there is one and only one solution, and approximating it more or less accurately is “just” an issue of numerical analysis. Also, these methods don't have an overwhelming capacity, such as Deep Neural Networks do, and you don't use early stopping (as one of tools) to control overfitting. This is why you never had to use a training/validation/test split when doing cross-validation before. Another possibility would be to skip the validation fold altogether. This requires that you use modern regulation methods in its place, such as Path-SGD, batch normalization and dropout, together with maybe older tools such as weight decay. These are usually applied together with early stopping, not in its place, but you May experiment and see what you get. It depends a lot on your architecture, also - I implicitly assumed that you are using a CNN, but you haven't told us anything about it.
