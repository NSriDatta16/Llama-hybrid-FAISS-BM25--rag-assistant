[site]: crossvalidated
[post_id]: 639908
[parent_id]: 639698
[tags]: 
A mixed effects logistic regression (if the outcome is binary/binomial) is certainly one plausible approach that could deal with the situation that all studies were essentially identical, in identical populations and there's just a bit of random variation in what you'd have expected to happen on a control group (if one had been in included in all studies). It is even a coherent method for combining randomized and single-arm studies. However, there's some big issues with doing such an analysis that is not solely based on randomized comparisons. You'd definitely want to account for differences in study population (esp. those that might change the outcome you're looking at such as disease severity, background medications, age, baseline measurements related to disease state...) and time period when studies were conducted (are outcomes for newer treatment just better because recently patient outcomes are just better due to better medical care, shifts over time in how early diseases get diagnosed) and study design such as inclusion criteria (if I enroll only those with a hypertension diagnosis and baseline blood pressure > 140 mmHg, I will get a larger within group change [for the same exact intervention] from baseline than if I recruit all with a hypertension diagnosis - the same if I look at achieving SBP treatment/study duration (if you turn the outcome into binary dead/not dead, then a longer study will just have a higher proportion of deaths) allowed concomitant medications outcome definitions/measurement methods... Any one of these could otherwise due to the lack of randomized comparisons massively bias comparisons between single-arm outcomes (to the extent that they could make a completely ineffective intervention look better than a highly effective one). In a way, this might be a much worse scenario than an observational study, where you at least usually have a common data source with all data from the same time period and access to individual patient data (important confounders might still be unavailable). Note that you don't need to account for just for the things you have data published for, but all the relevant things that might meaningfully matter. If you cannot get information on really important differences (or if some study design differences are completely confounded with evaluated intervention, e.g. because all studies for one intervention are old and all studies for a new intervention are new, and this leads to a complete confounding vs. substantially changed standard of care), you might just not be able to trust your analysis. If it's relatively few studies and a lot of potentially important differences between studies, one solution might be to set prior distribution in a Bayesian analysis (because you probably do have some a rough idea how much certain things could potentially change outcomes in what direction).
