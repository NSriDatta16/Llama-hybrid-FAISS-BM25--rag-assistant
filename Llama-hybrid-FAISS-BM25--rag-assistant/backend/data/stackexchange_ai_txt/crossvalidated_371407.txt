[site]: crossvalidated
[post_id]: 371407
[parent_id]: 351429
[tags]: 
Only the third question remains to be answered, the case where $X$ has infinite variance. When $n \gt 1,$ you can split the data into two smaller nonoverlapping (and therefore independent) samples, estimate $\mu$ separately in each subsample, and multiply the estimates. The independence assures the expectation of that product is the product of the expectations, so if each one of the estimates of $\mu$ is unbiased, so is your product estimate. The simplest form of this idea is to let $X_i$ be one subsample of size $1$ and $X_j$ (for $j\ne i$ ) a different subsample of size $1.$ Using the estimator of $\mu$ in the question (for the case $n=1$ ) gives the estimator $$t_{ij}(\mathbf{X}) = \left(\frac{1}{1} X_i\right) \left(\frac{1}{1} X_j\right) = X_iX_j.$$ Clearly $t_{ij}$ is unbiased because $$\mathbb{E}(t_{ij}(\mathbf{X})) = \mathbb{E}(X_iX_j) = \mathbb{E}(X_i)\mathbb{E}(X_j) = \mu^2.$$ We can go further. Intuitively, this approach ignores a lot of information available in the sample. The theory of U statistics is based on generating all possible estimates $t_{ij}, 1\le i \lt j \le n,$ and averaging them: $$U(\mathbf{X}) = \frac{1}{\binom{n}{2}}\sum_{1 \le i \lt j \le n} t_{ij}(\mathbf X) = \frac{1}{\binom{n}{2}}\sum_{1 \le i \lt j \le n} X_iX_j.$$ (The linearity of expectation shows this average remains unbiased.) Computations of variances show that when the underlying variance is finite, the "U statistic" has smaller variance than any individual $t_{ij}.$ (You might enjoy carrying out this approach for parts (1) and (2) of the question, because it leads directly and easily to the solutions given.) It would seem, however, that all bets are off when the underlying variance is infinite. Indeed, $t_{ij}$ may tend to vary less than the U statistic. Simulations with power-law tails suggest the U-statistic approach still has merit. The estimates produced by any individual $t_{ij}$ tend to be less extreme than the U statistic, because they have less of a chance of sampling the occasional whopping big outlier that such distributions produce. Consequently, there's potentially a high risk in any given application that a $t_{ij}$ will grossly underestimate $\mu^2.$
