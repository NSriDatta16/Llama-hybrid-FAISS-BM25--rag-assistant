[site]: crossvalidated
[post_id]: 615241
[parent_id]: 
[tags]: 
What is the relationship between walk_forward_validation (WFV)/series_to_supervised (STS) with Direct/Recursive multi-step forecasting & Backtesting

I'm experimenting with univariate time series using the following approaches that suit time-series analytics in order to transform the data in a way that the inputs to the model are the targets of previous K observations: 1. lookback period , example1 , example2 #ِDataset matrix is formed def create_dataset(df , lookback=1): data = np.array(df.iloc[:, :-1]) label = np.array(df.iloc[:, -1]) #create X_train and Y_train and MinMaxScaler on Y_train X = list() Y = list() for i in range(lookback , len(data)): X.append(data[i-lookback:i]) Y.append(label[i]) X = np.array(X) Y = np.array(Y) Y=np.expand_dims(Y,-1) return X,Y # Lookback period lookback = 5 X_train, Y_train = create_dataset(train, lookback) X_val, Y_val = create_dataset(validation, lookback) X_test, Y_test = create_dataset(test, lookback) # print(X_train.shape , Y_train.shape) #(155, 5, 1) (155, 1) # print(X_val.shape, Y_val.shape) #(35, 5, 1) (35, 1) # print(X_test.shape, Y_test.shape) #(69, 5, 1) (69, 1) 2. transform a time series dataset into a supervised learning dataset posted by Jason Brownlee : # Finalize model and predict monthly births with random forest from numpy import asarray from pandas import read_csv from pandas import DataFrame from pandas import concat from sklearn.ensemble import RandomForestRegressor # transform a time series dataset into a supervised learning dataset def series_to_supervised(data, n_in=1, n_out=1, dropnan=True): n_vars = 1 if type(data) is list else data.shape[1] df = DataFrame(data) cols = list() # input sequence (t-n, ... t-1) for i in range(n_in, 0, -1): cols.append(df.shift(i)) # forecast sequence (t, t+1, ... t+n) for i in range(0, n_out): cols.append(df.shift(-i)) # put it all together agg = concat(cols, axis=1) # Drop rows with NaN values if dropnan: agg.dropna(inplace=True) return agg.values # load the dataset series = read_csv('daily-total-female-births.csv', header=0, index_col=0) values = series.values # transform the time series data into supervised learning train = series_to_supervised(values, n_in=6) # split into input and output columns trainX, trainy = train[:, :-1], train[:, -1] Qs: The followings are my question to understand if the abovementioned methods have, e.g. lookback or walk_forward_validation (WFV) & series_to_supervised (STS) have some alignment with used methods on skforecast package. Q1: What is the difference between these two approaches? ( lookback period Vs transform a time series dataset into a supervised learning dataset ) I also tried in meantime walk_forward_validation() approach inspired from this post : Walk forward validation is a method for estimating the skill of the model on out-of-sample data. We contrive out of sample, and each time step one out of sample observation becomes in-sample. We can use the same model in ops, as long as the walk-forward is performed each time a new observation is received. My observation shows no difference between using: just series_to_supervised (STS) walk_forward_validation (WFV) & series_to_supervised (STS) comparing outputs, especially MAE results, but a bit in quality of forecasting up & downs(could be due to weights during training): Q2: Does walk_forward_validation (WFV) & series_to_supervised (STS) are equivalent to the following: Recursive multi-step forecasting Direct multi-step forecasting Q3: Is there any alignment/equivalent using walk_forward_validation (WFV) & series_to_supervised (STS) with one of the following Backtesting ? Backtesting with refit and increasing training size (fixed origin) Backtesting with refit and fixed training size (rolling origin) Backtesting without refit All pics sources in Q2 & Q3: credit from Joaquín Amat Rodrigo, Javier Escobar Ortiz
