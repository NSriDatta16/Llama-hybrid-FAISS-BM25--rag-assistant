[site]: crossvalidated
[post_id]: 387159
[parent_id]: 386075
[tags]: 
This paper provides an excellent overview of how to navigate gender bias especially in language-based models: Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings - Bolukbasi et. al. . A nice blog summary can be found here: https://developers.googleblog.com/2018/04/text-embedding-models-contain-bias.html You'll find a larger compendium of resources here: https://developers.google.com/machine-learning/fairness-overview/ You'll find a slew of techniques in the above links to mitigate gender bias. Generally speaking they fall into three classes: 1) Under/Over sampling your data. This is intended to oversample high-quality female resumes and under sample male resumes. 2) Subtracting out the "gender subspace." If your model is gender-biased, then you could demonstrate it to be so by using your resume embeddings to directly predict gender. After building such an auxiliary model (even just sampling common terms belonging to either gender, and then applying PCA), you can in effect subtract out this dimension from the model, normalizing the resume to be gender-neutral. This is the main technique used in Bolukbasi's paper. 3) Adversarial Learning. In this case you try to generate additional data by trying to generate more versions of high quality female resumes that are otherwise indistinguishable from real resumes.
