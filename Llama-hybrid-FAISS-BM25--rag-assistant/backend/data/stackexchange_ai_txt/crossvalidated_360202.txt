[site]: crossvalidated
[post_id]: 360202
[parent_id]: 194142
[tags]: 
The main reason I didn't understand 1x1 convolutions is because I didn't understand how $any$ convolutions really workedâ€”the key factor is how computing a convolution of multiple channels/filters works. To understand this, I found this answer useful as well: https://datascience.stackexchange.com/questions/9175/how-do-subsequent-convolution-layers-work In particular, Type 2.2 is the correct description of a convolution there. Another helpful answer: https://ai.stackexchange.com/questions/5769/in-a-cnn-does-each-new-filter-have-different-weights-for-each-input-channel-or This answer explains how you have a separate filter for each in/out channel combination . After calculating each of these, the results get summed over the input channel axis leaving with output channel number of values. Here's a video I found which helped me understand how a 1x1 convolution works. https://www.coursera.org/lecture/convolutional-neural-networks/networks-in-networks-and-1x1-convolutions-ZTb8x Here are the main things I got out of it: The input to a 1x1 convolution is usually previous convolutions which have size $m$ x $n$ . But if there were $f_1$ filters in the last layer of convolutions, you're getting a $(m, n, f_1)$ shaped matrix. A 1x1 convolution is actually a vector of size $f_1$ which convolves across the whole image, creating one $m$ x $n$ output filter. If you have $f_2$ 1x1 convolutions, then the output of all of the 1x1 convolutions is size $(m, n, f_2)$ . So a 1x1 convolution, assuming $f_2 , can be seen as rerepresenting $f_1$ filters via $f_2$ filters. It lets the network train how to reduce the dimension most efficiently.
