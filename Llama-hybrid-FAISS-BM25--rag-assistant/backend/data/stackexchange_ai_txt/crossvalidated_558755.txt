[site]: crossvalidated
[post_id]: 558755
[parent_id]: 
[tags]: 
Decision trees and Feature Scaling for regularization

For all tree-based models like xgboost, lightgbm, random forest, they do not require feature scaling given the nature in which they compute their splits. However, when you perform regularization, one generally has to scale their features to account for any improper penalization. Does this apply to these types of models like xgboost and lightgbm, when you have input parameters to tune L1 and L2 regularization?
