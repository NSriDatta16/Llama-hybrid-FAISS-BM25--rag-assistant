[site]: crossvalidated
[post_id]: 549255
[parent_id]: 
[tags]: 
Why do faster (eg sparse) versions of Transformers focus on the query-key product?

A lot of recent research on Transformers has been devoted to reducing the cost of the self-attention mechanism: $ softmax(\frac{Q K^T}{\sqrt{d}}) V $ , As I understand it, the runtime, assuming $\{Q, K, V\}$ are each of shape $(n, d)$ has complexity $O(n^2 d + n d^2)$ . In general, the issue is the $n^2 d$ term, because the sequence length $n$ can be much bigger than the model dimension $d$ . So far, so good. But as far as I can tell, current research focuses on speedups for $Q K^T$ which is $O(n^2 d)$ . There's less focus on computing $A V$ , where $A = softmax(\frac{Q K^T}{\sqrt{d}})$ -- which also has complexity $O(n^2 d)$ . Why is the first matrix product the limiting factor?
