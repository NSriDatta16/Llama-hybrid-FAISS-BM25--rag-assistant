[site]: crossvalidated
[post_id]: 412966
[parent_id]: 
[tags]: 
Converting the standard deviation of an averaged score to the standard deviation of a total score

I need to obtain the standard deviation of a total score on a 3-item scale that has been tested on a non-clinical population. The total score is calculated by summing scores for the three questions that make up the scale. Each of these questions is a 0-10 score. The average total scale score can be expressed either as a total score/30 or as an average item score/10. My problem is that the study that tested this scale on a non-clinical population has reported the average item score (/10) not the summed score (/30). They tested it on n =1081 participants and report an average item score (/10) of 4.6 with a sd of 8.1 (which I calculated myself as they reported the standard error, which was 0.061, so sd = se x sqrt(n) = 0.061 x sqrt(1081) = 8.1). Now extrapolating the average item score (/10) to a total score (/30) is easy, just multiply by 3, so 3 x 4.6 = 13.8/30. But how do I perform a similar extrapolation on the standard deviation of the average item score? . How do I reverse-engineer the standard deviation of an averaged score to convert it to the standard deviation of a total score? Pretty sure it's not possible with the info I have, but just wanted to be sure.
