[site]: crossvalidated
[post_id]: 369172
[parent_id]: 86351
[tags]: 
Call : This is just the call that you made to the function. It will be the exact same code you typed into R. This can be helpful for seeing if you made any typos. (Deviance) Residuals: You can pretty much ignore these for logistic regression. For Poisson or linear regression, you want these to be more-or-less normally distributed (which is the same thing the top two diagnostic plots are checking). You can check this by seeing if the absolute value of 1Q and 3Q are close(ish) to each other, and if the median is close to 0. The mean is not shown because it's always 0. If any of these are super off then you probably have some weird skew in your data. (This will also show up in your diagnostic plots!) Coefficients : This is the meat of the output. Intercept : For Poisson and linear regression, this is the predicted output when all our inputs are 0. For logistic regression, this value will be further away from 0 the bigger the difference between the number of observation in each class.. The standard error represents how uncertain we are about this (lower is better). In this case, because our intercept is far from 0 and our standard error is much smaller than the intercept, we can be pretty sure that one of our classes (failed or didn't fail) has a lot of more observations in it. (In this case it's "didn't fail", thankfully!) Various inputs (each input will be on a different line): This estimate represents how much we think the output will change each time we increase this input by 1. The bigger the estimate, the bigger the effect of this input variable on the output. The standard error is how certain about it we are. Usually, we can be pretty sure an input is informative is the standard error is 1/10 of the estimate. So in this case we're pretty sure the intercept is important. Signif. Codes : This is a key to the significance of each :input and the intercept. These are only correct if you only ever fit one model to your data. (In other words, they’re great for experimental data if you from the start which variables you’re interested in and not as informative for data analysis or variable selection.) Wait, why can't we use statistical significance? You can, I just wouldn't generally recommend it. In data science you'll often be fitting multiple models using the same dataset to try and pick the best model. If you ever run more than one test for statistical significance on the same dataset, you need to adust your p-value to make up for it. You can think about it this way: if you decide that you'll accept results that are below p = 0.05, you're basically saying that you're ok with being wrong one in twenty times. If you then do five tests, however, and for each one there's a 1/20 chance that you'll be wrong, you now have a 1/4 chance of having been wrong on at least one of those tests... but you don't know which one. You can correct for it ( by multiplying the p-value you'll accept as significant by the number of tests you'll preform ) but in practice I find it's generally easier to avoid using p-values altogether. (Dispersion parameter for binomial family taken to be 1): You'll only see this for Poisson and binomial (logistic) regression. It's just letting you know that there has been an additional scaling parameter added to help fit the model. You can ignore it. Null deviance: The null deviance tells us how well we can predict our output only using the intercept. Smaller is better. Residual deviance: The residual deviance tells us how well we can predict our output using the intercept and our inputs. Smaller is better. The bigger the difference between the null deviance and residual deviance is, the more helpful our input variables were for predicting the output variable. AIC: The AIC is the "Akaike information criterion" and it's an estimate of how well your model is describing the patterns in your data. It's mainly used for comparing models trained on the same dataset. If you need to pick between models, the model with the lower AIC is doing a better job describing the variance in the data. Number of Fisher Scoring iterations: This is just a measure of how long it took to fit you model. You can safely ignore it. I suggest this toturial to learn more. https://www.kaggle.com/rtatman/regression-challenge-day-5
