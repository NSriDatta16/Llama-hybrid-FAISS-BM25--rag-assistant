[site]: crossvalidated
[post_id]: 118545
[parent_id]: 
[tags]: 
Deep neural network fine-tuning with Random Forest being the last layer

After layerwise pretraining (typically unsupervised) the dnn, we tend to fine tune it in a supurvised manner. I know how to do the fine-tuning if the last layer is a softmax classifer, but how to do it if I have a Random Forest being the last layer? What is the loss function then and how to calculate the gradient w.r.t. the weights?
