[site]: crossvalidated
[post_id]: 482766
[parent_id]: 479835
[tags]: 
The posterior distribution only appears to not depend on the data. In fact, it places equal probability on all observed data values and zero probability on all unobserved values. As Rubin says near the top of p. 131: Each BB replication generates a posterior probability for each $x_i$ where values of $X$ that are not observed have zero posterior probability. "Non-informative" is a less popular term now than it was in the past, because it's hard to define it in way that is meaningful and useful. The prior on $\pi$ is not flat -- it is more spread out than a flat prior, so it has more chance of $\pi_i$ being near $0$ or $1$ . The posterior of $\pi_i|X$ is flat. The posterior of $X$ is not flat: it is concentrated on the $n$ observed values, with no probability assigned anywhere else. There's no problem with a flat posterior on a bounded space, as here. You just have to start out with a prior that's more spread out than a flat one. What you can't have is a flat posterior on an unbounded space, because that's not a proper distribution. Check this out. You can't derive the posterior of $X$ using Bayes' Rule, because what we really have a posterior distribution for is just the weights. The posterior puts zero weight on all unobserved $X$ values, so the prior would also have to put zero weight on all unobserved $X$ values, but we don't know yet what they're going to be. In that sense, there is something dodgy going on. Since 1981 we have more satisfactory Bayesian analogues, such as a Dirichlet Process( $\alpha$ , $G$ ) model, where there's a parameter $\alpha$ such that posterior puts weight $1/(n+\alpha)$ on each observed value and weight $\alpha/(n+\alpha)$ on everything else, proportional to a specified distribution $G$ . You can sample from the DP posterior by sampling from the data with probability $n/(n+\alpha)$ and from $G$ with probability $\alpha/(n+\alpha)$ . Even here, you can't derive the posterior for an uncountable space such as the real line using Bayes' Rule. The space of possible distributions is too big; they can't all be written as densities with respect to the prior (or with respect to any other single probability measure). The posterior is derived by a conjugate-prior argument instead.
