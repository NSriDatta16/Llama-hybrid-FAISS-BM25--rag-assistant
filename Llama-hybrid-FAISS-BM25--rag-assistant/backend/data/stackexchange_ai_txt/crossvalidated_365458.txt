[site]: crossvalidated
[post_id]: 365458
[parent_id]: 364981
[tags]: 
I believe the reasons are practical and the main one for preferring mAP to accuracy is class imbalance. By evaluating against mAP, to perform well the classifier needs to be able to handle the smaller classes. In this respect, I think the justification is similar to why average precision is used in information retrieval and NLP. In the ILSVRC classification and subsequent localisation/detection challenges from 2010 to 2017 the main metric for the contest was top-k accuracy. Top-k was selected because contest organizers did not want to penalise algorithms for detecting objects which were present in the scene, but were not labelled in the ground truth. The instructions for ILSVRC make reference to the test data being carefully selected for various relevant properties and I would assume that class balance was one of those. People care about mAP in the context of object detection not image classification generally. Why mAP? A pragmatic reason was that it was popularised by PascalVOC. The PascalVOC people gave a few justifications for their choice of evaluation metrics which are informed by the dataset they created: there can be multiple instances of multiple classes in each image the number of instances of each class wasn't balanced they wanted the metrics to be algorithm independent AUC didn't differentiate well between the competitive algorithms in the contest There's further elaboration in PascalVOC paper: Everingham, Mark, et al. "The pascal visual object classes (voc) challenge." International journal of computer vision 88.2 (2010): 303-338. Personally, I think mAP makes sense because you want to identify multiple objects in an image, and it's hard to avoid class imbalance in object detection datasets once you start counting object instances. Photos of people tend to have more than one person but photos of hairdryers tend to only have a single hair dryer. This is partly due to bias in the underlying social media and photoshare websites from which these datasets are drawn. See the following diagram showing the object instance distributions and imbalance in MSCOCO and PascalVOC: ( Source ) There is a similar issue in semantic segmentation because the pixel-wise class distributions are frequently quite skewed. Hence why the preferred metrics there are usually either the Dice score or the Jaccard coefficient which are close related to the F1 score.
