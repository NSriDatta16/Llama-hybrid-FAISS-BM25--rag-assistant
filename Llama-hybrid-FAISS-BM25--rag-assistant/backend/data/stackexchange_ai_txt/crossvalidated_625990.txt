[site]: crossvalidated
[post_id]: 625990
[parent_id]: 
[tags]: 
Is there any bias introduced by evaluating a model and decisions based on this model on the same data set?

As an example, let's say we have some financial time series such as closing prices of some stock and we would like to evaluate the ability of different models to forecast future closing prices as well as the ability of a different set of models to make decisions based off of these forecasts, e.g. whether to buy, hold, or sell the stock. Let us also assume that we are going to use this data to evaluate our decisions by utilizing some form of trading strategy backtesting based upon the closing price time series data. Do we introduce any bias into the evaluation of either our forecasts or decisions based on our forecasts by using the same data for evaluation of both sets of models? Let's assume we do some sort of train/validation/test split on our initial series of closing prices for performance evaluation of both forecasts and decisions. This example concerns quantitative finance, but I feel as if this generalizes to any situation where we are evaluating both forecasts and decisions made based on those forecasts and wanted to know if there's a common solution to this if it's even a problem.
