[site]: crossvalidated
[post_id]: 600926
[parent_id]: 600764
[tags]: 
I will show you more visually the difference between evaluating the PDF (probability density function, or $f$ ) and sampling the PDF. Sampling a PDF is usually much harder and this problem gets worse the more dimensions your PDF has. Using the following Mathematica code we can draw ('evaluate') the PDF. I chose a kind of arbitrary distribution whose PDF is just the sum of two normal distributions. n = 1000; distr = MixtureDistribution[{2, 1}, {NormalDistribution[], NormalDistribution[3, 1/2]}]; Plot[PDF[distr][x], {x, -3, 5}, PlotRange -> Full] Now we can sample the distribution. This means generating a random real number whose probability is proportional to the PDF at that real number. Or in simpler words: more points where the PDF is high. Note that the height y in the code below is just for visual purposes. x = RandomVariate[distr, n]; y = RandomReal[{1, 2}, n]; ListPlot[{x, y}\[Transpose], PlotRange -> {{-3, 5}, {.5, 2.5}}, AspectRatio -> Automatic, Axes -> {True, False}] If you draw a histogram of x it will look just like the PDF if you sample enough points. There is also a 'direct' way to sample a distribution, but this is only possible when the CDF increases monotonically. See inverse transform sampling . In physics there are two common use cases for Metropolis-Hastings. If we can write down an expression for the energy of a certain configuration of the system, we immediately know the probability distribution for that system. The probability is $p(\text{configuration})\propto e^{-E/kT}$ , where $E$ is the energy of the configuration, $k$ is a constant and $T$ is the temperature. If we now perform Metropolis-Hastings on this system it would produce different configurations as if you were looking at a simulation. So MH can be used to simulate thermal systems, which is the first use case I was telling about. The second use case is to calculate expectation values, which was also told by Hunaphu. Naively one would calculate an expectation value as follows: $$E[g(x)]\approx\frac{\sum_i g(x_i)f(x_i)}{\sum_i f(x_i)}$$ where $x_i$ are evenly spaced on the number line. A lot of time is now spent calculating values that are basically zero. For a normal distribution the PDF quickly goes to zero when $x$ gets large. Imagine we can now efficiently sample from the PDF to produce samples $X_i$ . In that case we can also calculate the expectation value as follows $$E[g(x)]\approx\frac{1}{N}\sum_i g(X_i)$$ wherever $f$ is high, $g$ gets sampled many times and contributes more to the expectation value. Perhaps it is easier to grasp this expectation using a discrete distribution. Imagine a large number of students have taken test which has integer outcomes in the range $[1,10]$ . If we know the probability distribution $f$ of the grades we could calculate the average grade using $$\frac{\sum_{x=1}^{10} x \cdot f(x)}{\sum_{x=1}^{10}f(x)}.$$ But, if a large number of students has taken the test we could equivalently calculate the average of these sample grades $$\frac 1 N\sum_i X_i$$
