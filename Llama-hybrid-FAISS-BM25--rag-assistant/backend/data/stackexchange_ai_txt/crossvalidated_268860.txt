[site]: crossvalidated
[post_id]: 268860
[parent_id]: 
[tags]: 
Jensen's inequality; Hazard, Likelihood and Prior?

Primer : A simple coin game: You sequentially flip a coin in each trial that can either land heads or tails $(H,T)$. If you flip $T$ you are allowed to play on, if you flip $H$ you lose. Importantly , for each new round you are handed a coin with a (unknown) bias to either side. Can we describe the probability of being allowed to continue (i.e. only getting $T$) in a Bayesian non-parametric sense? Under this formulation (given it is correct) Jensen's inequality yields a bound on the survival function. Is there a intuition for this? Setup and definitions : Assume that some unknown stochastic process or model $m$ with parameters $\theta\in\Omega$ is producing binary data, over discrete time $t_1 Given that all prior n observations were one and the parameter of the model is described by $\theta$, the immediate (or event) probability of an entry of one to the dataset at time $t_k$, is given by the hazard function $$ \lambda(t_k\vert\theta)=\mathbb{P}(D(t_k)=1\vert D(t_{k-n}=1),\theta). $$ Alternatively, let $T=t_k$ denote the time of the event $D(t_k)=0$.Treating $T$ as a random discrete variable, the hazard function can be written as $$ \lambda(t_k\vert\theta)=\mathbb{P}(T=t_k\vert T\geq t_k,\theta) = \frac{f(t_k\vert\theta)}{1-F(t_k\vert\theta)}. $$ This says that the hazard function is the conditioned probability of an event (the entry being zero), given that the event has not happened yet. In discrete time this is a probability, and not a rate. Think of playing a round of " elimination coin ". You flip a coin every time $t_1,t_2..t_k$ and if you realise $H$ on the $t_k$ trial, you lose. The coin may not be fair and is described by the parameter $\theta\in\Omega$. If the event time $T$ is exponentially distributed with a fixed $\theta$, the hazard is constant at $\beta=\frac{1}{\lambda}$, the scaling of the exponential distribution. Let $\phi(t_k\vert\theta)=(1-\lambda(t_k\vert\theta))$ clearly, this is the immediate event probability (of survival) given $\theta$. Define the marginalized event probability as $$ \phi(t_k\vert\theta) = \sum_{\theta\in\Omega}\phi(t_k,\theta) = \sum_{\theta\in\Omega}\phi(t_k\vert\theta)p(\theta). $$ Here, $p(\theta)$ plays the role of a "mixing" distribution or prior. Colloquially; If we expect $\theta$ to be some specific parameter value, $\theta^{'}$, half of the time, then the above equation is a weighted average over different parameters. Again, think of " elimination coin ", but this time $\theta$ is a random variable with the density $p(\theta)$. If there is just two states; a fair coin and a very biased one - then the probability of $H$ is a weighted average over the probability (or rate) with which these different parameters are realized. The definition of the Survival function $S(\cdot)$ is $$ S(t_{k}\vert\theta)=\mathbb{P}(T\geq t_{k}\vert\theta) $$ and can be written as the simple recursive relation of the hazard as $$ S(t_{k}\vert\theta)=(1-\lambda_{1})(1-\lambda_{2})...(1-\lambda_{k-1}) $$ where $\theta$ is removed for notational simplicity. With a bit of simple algebra it is easy to show that $$ S(t_{k}\vert\theta) = \prod_{1}^{k-1}\sum_{\theta\in\Omega}\phi(t_k\vert\theta)\cdot p(\theta).$$ Is this result correct / uncontroversial? Jensen's inequality: To find the time-average lifespan from $t_k=1$ to $\tau$ $$ \left\langle S(t_k\vert\theta)\right\rangle _{\tau} = \frac{1}{\tau}\prod_{1}^\tau\sum_{\theta\in\Omega}\phi(t_k\vert\theta))\cdot p(\theta). $$ which is a product (over time) of a sum (over parameters) over products (over likelihoods times priors), and does (to me) not afford any meaningful simplification. Taking the log, we realize Gibbs version of Jensen's inequality: $$ \ln\left\langle S(t_k\vert\theta)\right\rangle _{\tau} \leq \frac{1}{\tau}\sum_{1}^\tau\ln\sum_{\theta\in\Omega}\phi(t_k\vert\theta))\cdot p(\theta). $$ In this setup, does this bound have any intuition? Alternatively, is there anyway to derive "interesting" quantities from the above, like Shannon entropy or surprise? Summary: Is this expression equal (or come close to) time-average (Shannon) surprise? (entropy) $$ \left\langle S(t_k\vert\theta)\right\rangle _{\tau} = \frac{1}{\tau}\prod_{1}^\tau\sum_{\theta\in\Omega}\phi(t_k\vert\theta))\cdot p(\theta). $$ Does this boundary bear any intuition to a survival/reliability/first passage time? $$ \ln\left\langle S(t_k\vert\theta)\right\rangle _{\tau} \leq \frac{1}{\tau}\sum_{1}^\tau\ln\sum_{\theta\in\Omega}\phi(t_k\vert\theta))\cdot p(\theta). $$ I realize this is quite lengthy. I am not a mathematician, so notation might not be correct. But I hope that the intuition is correct. Any calls for clarification is appreciated.
