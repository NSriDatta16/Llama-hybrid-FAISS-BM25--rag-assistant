[site]: crossvalidated
[post_id]: 348509
[parent_id]: 348496
[tags]: 
First, remember that gradient descent is just a numerical technique for optimizing differentiable functions, which often means finding a local minimum or a place where the gradient of the function is effectively zero. In most cases, the equation $\nabla f =0$ cannot be solved symbolically. So even once you've derived an express for the gradient, you will still need a numerical technique to find a root. One exception is a linear neural network, which is ultimately linear regression, and therefore you could reverse engineer a set of optimal weights from the least-squares solution.
