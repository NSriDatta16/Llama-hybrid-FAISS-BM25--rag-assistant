[site]: datascience
[post_id]: 120172
[parent_id]: 106627
[tags]: 
In the support vector machine (SVM) algorithm, the optimization goal is to find the hyperplane that maximally separates the positive and negative examples in the training data . This is known as the maximum margin hyperplane . Reference: https://vitalflux.com/classification-model-svm-classifier-python-example/ To find this hyperplane, the SVM algorithm first identifies the examples in the training data that are closest to the decision boundary (a conceptual line, surface, or hyperplane that separates the input space into different regions or classes) , known as the support vectors . The hyperplane is then defined by the support vectors , and the distance between the hyperplane and the nearest support vectors is known as the margin . The optimization goal of the SVM is to maximize the margin , so that the hyperplane is as far away from the support vectors as possible. In scikit-learn , the SVM algorithm uses a quadratic programming solver to find the maximum margin hyperplane . This solver iteratively improves the hyperplane by adjusting the weights and bias parameters of the model, with the goal of maximizing the margin between the positive and negative examples . The support vectors are identified during the optimization process, and the resulting hyperplane is used to make predictions on new data. https://vitalflux.com/classification-model-svm-classifier-python-example/ https://www.researchgate.net/figure/Support-vector-machine-The-optimization-problem-of-Eq-1-may-be-modified-to-min-w-w_fig1_330833082 https://jeremykun.com/2017/06/05/formulating-the-support-vector-machine-optimization-problem/
