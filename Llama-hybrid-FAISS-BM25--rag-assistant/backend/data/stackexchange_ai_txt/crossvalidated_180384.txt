[site]: crossvalidated
[post_id]: 180384
[parent_id]: 180306
[tags]: 
Per the relevant sklearn docs , predict_proba() returns an array with dimensions equal to the number of samples and the number of classes. (NB that the number of classes is not the number of trees.) More, the returned probabilities are the average of the trees: The predicted class probabilities of an input sample is computed as the mean predicted class probabilities of the trees in the forest. Per the np.unique() docs , this array will be flattened if it's more than single-dimensional. Meaning, without your data at hand, there's really no way to know why you're winding up with 38 elements. But in any case, you wouldn't expect predicted probability dimension to align with the number of classifiers. Since predict_proba() averages the predictions of the set of $k$ classifiers, it will only give a multiple $\frac{1}{k}$ when all of these classifiers give $0$ or $1$ probabilities for a given datum. This will only occur when: The data are separable by a set of boolean conjunctions. Meaning, rules can be constructed such that every node of the tree is solely of a single class. The trees are trained to adequate depth to fully fit—overfit, some might say—the data. If your code is working to separate toy sets, say load_digits() in sklearn , it would seem your data can't be fully separated by decision trees. RandomForestClassifier does make the underlying classifiers available, however, as detailed in this SO answer . To get a clearer picture, you could access this attribute to find the predictions of the underlying trees.
