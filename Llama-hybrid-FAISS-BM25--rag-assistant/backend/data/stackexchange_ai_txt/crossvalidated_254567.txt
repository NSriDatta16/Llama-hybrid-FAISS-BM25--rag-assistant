[site]: crossvalidated
[post_id]: 254567
[parent_id]: 
[tags]: 
Problem with kernel estimates in Metropolis-Hastings

I am performing an experiment on Metropolis-Hastings. I have written a proof that returns for stocks have to be a function of the Cauchy distribution and that the distribution of $\beta$ in the equation $x_{t+1}=\beta{x_t}+\epsilon_{t+1},\beta>1$ is also the Cauchy distribution. I chose to seed the random numbers so that the maximum likelihood estimator is $\beta=.99553.$ and $\alpha=.09452$. This violates the assumption $\beta>1$. White in 1958 showed that there was no Likelihoodist solution for this problem as the MLE is the OLS estimator but the test statistic is the Cauchy distribution, so a Bayesian method is being used, hence the Metropolis-Hastings problem. The data generating function was $x_{t+1}=1.00004x_t+\epsilon_{t+1}$, where $\epsilon\sim\mathcal{N}(0,1.00004^t)$ and $x_0=10$. There is an added restriction that $x_t>0,\forall{t}$. The likelihood function for this problem is $$\frac{1}{\pi}\frac{\gamma}{\gamma^2+(x_{t+1}-\beta{x_t}-\alpha)^2}.$$ The prior has the following properties: $\Pr(\beta\le{1})=0$ $\Pr(\gamma\le{0})=0$ Over the region that is reasonable for $\beta$ and $\sigma$, the prior is $1\times{1}\times{1}/(1+\alpha^2)$. The marginal prior is flat over $\beta$ and $\sigma$ in reasonable regions, but not $\alpha$. In unreasonable regions the prior decays as a half-Cauchy distribution. This only impacts the burn-in as the likelihood of different starting points is at least ten orders of magnitude from the MLE. This prior creates the problem for me. It is a cliff and the MLE is over the cliff. If $\beta$ were allowed to range freely over the reals, then it would roughly marginalize out as a Cauchy distribution peaked at .99553, ignoring the prior. The true parameter is 1.00004, which is close to the cliff since $\gamma\approx{.65}$. Where I am having difficulty in my posterior sampling is that the kernel estimates of the posterior fall off fast too far from the cliff, for largish sample sizes. When I do not thin, the problem goes away because I have so many bins, but of course thinning is required as there is autocorrelation in this sample out to 133 lags. I lose a lot of sample from thinning. I am using the normal distribution as my proposal density, with covariances of zero. Other than letting my computer run for 20 hours to get a massive sample, which will result in thin bins, any suggestions on changes I could make to keep the kernel estimates from falling off too far from $\beta=1$. I am using the density() function in R. I am hoping to avoid programming my own kernel smoothing function.
