[site]: datascience
[post_id]: 11968
[parent_id]: 11756
[tags]: 
Your problem is called multi-label classification. If you search for this term you will find a lot of literature. In short, these are the most common approaches: Treat each label independently and build a binary classifier per label (baseline approach, Eric Lecoutre's answer) Predict the power set, means create a target variable that represents all possible combinations and predict it using a multi-class classifier (Upper_Case's answer) Use a classifier that can deal with multi-labels inherently, e.g. a neural net with multiple output nodes and cross-entropy as loss function, or respective multi-label variants of decision trees or SVM. For example, scikit-learn supports multi-label implementation for decision trees. Classifier chains: Train a binary classifier per label, but use the predictions of one classifier as input to another classifier Depending on the nature of the data, one or the other approach will work best. Start with a binary classifier per label to get a baseline. If the labels are correlated (some labels frequently occur together), there is potential for improvement using one of the other methods. Using one of the existing implementations of multi-label classifiers is probably the easiest next step. The power set approach only makes sense if the number of possible combinations is reasonable. I am not sure if classifier chains are worth the effort, if you are not doing this for a Kaggle competition :) As always, setup a proper cross-validation strategy, define the evaluation metric (e.g. micro/macro f-measure or cross-entropy loss) so that you are able to identify the best model.
