[site]: crossvalidated
[post_id]: 574278
[parent_id]: 574257
[tags]: 
The problem you describe is inferring the parameters of a Categorical distribution . For the maximum likelihood estimator, note that you can write the likelihood as $$\mathcal L = \Pi_{i=1}^m P(x_i | \theta) = \Pi_{i=1}^N \theta_i^{n_i} $$ Where $n_i$ is the number of occurrences of the $i$ -th discrete value. The only slightly tricky part here is that you want to maximize it over the simplex, namely under the constraint $\sum \theta_i = 1$ . But that can be easily done using a Lagrange multiplier : we want to find (taking the log of the likelihood and adding the constraint) $$ \frac{\partial}{\partial \theta_i} \left( \sum_i n_i \log \theta_i - \lambda( \sum_i \theta_i - 1) \right) = 0 $$ which trivially gives $$ \hat \theta_i = \frac{n_i}{\lambda} $$ and the constraint implies that $\lambda = \sum_i n_i$ , so the MLE turns out to be exactly like your na√Øve expectation. For a Bayesian analysis, the Dirichlet distribution is a conjugate prior of this model, which means that the posterior is also a Dirichlet distribution with parameters that are straightforward to find. You can find the details in this Section of the Wikipedia article.
