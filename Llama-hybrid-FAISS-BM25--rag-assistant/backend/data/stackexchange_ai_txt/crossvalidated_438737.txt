[site]: crossvalidated
[post_id]: 438737
[parent_id]: 380029
[tags]: 
First consider the case that you have no gold answers. i.e. no $t_i$ known. For simplicity, consider only one sample $i$ here. Then the negative likelihood function (loss function) is $$\mathcal{L}(\mathbf{c}_i; \mathbf{p}, \pi) = -\log p(\mathbf{c}_i | \mathbf{p}, \pi)$$ , where $\mathbf{p}$ and $\pi$ are the parameters of the likelihood function. No matter what optimization method you use, SGD or EM, the MLE objective is this one. Expanding the loss function gives \begin{equation} \begin{split} \mathcal{L}(\mathbf{c}_i; \mathbf{p}, \pi) &= -\log p(\mathbf{c}_i | \mathbf{p}, \pi) \\ &= -\log \sum_{j=1}^J p(\mathbf{c}_i, j| \mathbf{p}, \pi) \\ &= -\log \sum_{j=1}^J p_{j}\prod_{k=1}^K \pi^{(k)}_{j, c_i^{(k)}}\\ \end{split} \end{equation} Now you can see this is a mixture of multinomial model. Just like mixture of gaussians, let $\mathbf{t}_i$ be the $J$ item vector denoting the posterior estimation of problem $i$ , the E step should be $$ \forall j, \mathbf{t}_i[j] := p(t_i = j | \mathbf{c}_i, \mathbf{p}, \pi) = \frac{p_{j}\prod_{k=1}^K \pi^{(k)}_{j, c_i^{(k)}}}{\sum_{j'=1}^J p_{j'}\prod_{k=1}^K \pi^{(k)}_{j', c_i^{(k)}}}$$ and M step should be $$ \mathbf{p} \leftarrow \frac{1}{I} \sum_{i=1}^I \mathbf{t}_i $$ and $$ \forall j, k, \pi^{(k)}_{j, \cdot} \leftarrow \sum_{i=1}^I \mathbf{t}_i[j] \pi_{j, \cdot}^{(k)} $$ IMHO, in the deep learning era, for most tasks, gradient descent can replace EM entirely. There will not be much performance degrade. It saves the time to derive and debug the formula.
