[site]: stackoverflow
[post_id]: 652504
[parent_id]: 652485
[tags]: 
index lookup, addition and deletion remains constant You could ensure it remains constant by rebuilding the indexes every insert (just constantly really slow - no performance drop off at all :)), or close to constant by running index maintenance every hour/day etc. that the performance might drop off as the key value range moves continuously upwards? As long as you've got an index, it should be logN performance - e.g. having 1,000,000 rows will be around half the speed of 1,000 rows (when searching for an indexed value). (1,000,000,000,000 will be half that speed again). So no, you shouldn't need to worry about performance. The numbers will probably wrap back to 1 after 100M or so. Ok - if you want. Generally, no need really - just use a big int. As always with performance: test what you want to do. Make a script that inserts 10,000,000 rows, and see what happens. My point here being that if you're going to wrap ids at 100M records, the worst you can do is actually have them all allocated. This would represent the fragmented index condition as well (where you only have say 100K records, but they're distributed in a space of 10M) - but you will do index/database maintenance right?
