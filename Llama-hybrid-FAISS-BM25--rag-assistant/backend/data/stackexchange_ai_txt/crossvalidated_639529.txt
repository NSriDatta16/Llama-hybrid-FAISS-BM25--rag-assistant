[site]: crossvalidated
[post_id]: 639529
[parent_id]: 639424
[tags]: 
Clearly, there is no single correct evaluation protocol. It depends on what you aim for in your detection system. The properties of the detector that you want to quantify are typically determined by use cases. Greedy matching is commonly employed when evaluating a detection system on a single frame (a full image). The prediction with the most overlap is the only one that is matched when there are multiple predictions that correspond to the same ground truth. Any detection that is not matched is considered a false positive. The precision in your example is $1/4$ when you apply this rule. Since 3 detections of a single dog count as 1 correct detection and 2 false detections. And the additional prediction that did not even meet the criteria causes another false detection. This rule results in a penalty for a noisy detection system that returns multiple bounding boxes around each object. For instance, the PASCAL challenge metric implemented this rule. In addition, various metrics are employed for evaluating object detection, including average precision (AP) and mean average precision (mAP). You can take a look at review_object_detection_metrics . The detection system should employ non-maximum suppression (NMS) to merge nearby predictions. Also a score or confidence is typically returned by the detection system for each prediction. And only predictions with a high enough score are returned.
