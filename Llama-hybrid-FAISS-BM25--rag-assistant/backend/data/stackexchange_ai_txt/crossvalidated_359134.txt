[site]: crossvalidated
[post_id]: 359134
[parent_id]: 358629
[tags]: 
A lot of fine comments and answers here, but is it all really necessary? This is a very simple model. Let's rewrite it as $$ y = \beta_1(x_1 - \mu_1) + \beta_2(x_2 - \mu_2) + \phi_f + \epsilon $$ where $\phi_f$ is the offset due to factor $f$. Clearly, $\phi_f$ is the average value of the response when the independent variables are at their mean values. With the vast number of factor levels you have, it makes sense to treat $\phi_f$ as a random effect, so don't use glm or lm Using lme might still cost a lot of time, but would you really lose that much by fitting the linear model and the factor model independently? If the sample is random and the number of items at each level is roughly the same, I'm not sure that you would lose much by estimating the random effects as the average over the $y$'s at each level. Then do the regression without the factors -- the factor effects will basically be rolled into the error $\epsilon$. You won't get exactly the same answer as if you did it the right way, but realistically, is the extra computing time worth it, if that's causing you serious grief. Another option, which I have never tried, but might help, would be to get the naive estimates I described above, and then do one or two iterations through the EM algorithm towards improved versions, but don't attempt to go the whole way to convergence with all that data and all those parameters.
