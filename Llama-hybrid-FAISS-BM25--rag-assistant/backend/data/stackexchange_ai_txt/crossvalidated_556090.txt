[site]: crossvalidated
[post_id]: 556090
[parent_id]: 519922
[tags]: 
I agree with the comment that regularization is a way to constrain parameters. In that regard, not only are $L_1$ and $L_2$ penalties regularization, but neural network dropout is regularization, since it forces certain parameters to be zero. Likewise, convolutional neural networks could be argued to perform regularization by forcing some parameters to be equal and others to be zero. Even if the terminology is a bit loose, the field of machine learning seems to do okay. Many fields are loose with terminology, too. For instance, ask a musician to explain the difference between a "riff" and a "lick".
