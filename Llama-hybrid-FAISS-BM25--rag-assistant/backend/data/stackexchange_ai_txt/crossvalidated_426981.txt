[site]: crossvalidated
[post_id]: 426981
[parent_id]: 
[tags]: 
Why do the XGBoost predicted probabilities of my test and validation sets look well calibrated but not for my training set?

I am using an XGBoost classifier to predict propensity to buy. After drawing a calibration curve to check how well the classification probabilities (predict_proba) produced are vs actual experience, I noticed that it looks well calibrated (close to diagonal line) for my test and even validation data sets but produces a "sigmoid" shaped curve (actual lower for bins with low predicted probabilities and actual higher for bins with high predicted probabilities) for the training set. I do not understand why this is the case and might be misunderstanding XGBoost's hyperparameters or functionality. Any explanation would be appreciated. I used my test set to do limited tuning on the model's hyper-parameters. I also used sklearn's train_test_split to do a stratified (tested without the stratify argument as well to check if this causes sampling bias) split 65:35 between train and test and I also kept an out-of-time data set for validation. XGBClassifier(learning_rate =0.05, n_estimators=500, max_depth=8, min_child_weight=3, gamma=0.1, subsample=0.6, reg_lambda = 2, reg_alpha=1, colsample_bytree=0.6, objective= 'binary:logistic', scale_pos_weight=1, seed=21)
