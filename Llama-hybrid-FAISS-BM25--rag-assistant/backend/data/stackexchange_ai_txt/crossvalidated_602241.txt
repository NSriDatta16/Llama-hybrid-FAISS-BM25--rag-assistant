[site]: crossvalidated
[post_id]: 602241
[parent_id]: 592995
[tags]: 
A guiding principle of autoencoders is that they must obtain a bottleneck. If they don't, they may simply learn the identity function (i.e. to perfectly replicate the input, but without building any meaningful internal representation of it), which is useless for sampling in the case of VAEs. I think a practical example of this behaviour is shown in this question . The dimension of the latent space does affect results, and finding the right dimension brings the tradeoff that Simon stated.
