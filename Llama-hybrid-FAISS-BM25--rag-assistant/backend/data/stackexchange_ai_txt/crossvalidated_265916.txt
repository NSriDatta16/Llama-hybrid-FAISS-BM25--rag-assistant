[site]: crossvalidated
[post_id]: 265916
[parent_id]: 
[tags]: 
Sampling from distribution defined variationally

Suppose I define a probability distribution $\mu\in\mathrm{Prob}(\Sigma)$ over some compact $\Sigma\subseteq\mathbb{R}^n$ using a variational problem: $$ \mu:=\arg\min_{\mu\in\mathrm{Prob}(\Sigma)} F[\mu]. $$ Here, $F[\cdot]:\mathrm{Prob}(\mathbb{R}^n)\rightarrow[0,\infty)$ is an energy functional on the space of probability distributions. Feel free to put any reasonable regularity assumptions you want to on $F[\cdot]$. Is there a way to sample approximately from $\mu$ without explicitly solving the variational problem? For instance, consider the following inefficient and likely incorrect strategy: Randomly generate a sample of $k$ points $p_1,\ldots,p_k\in\Sigma$ uniformly from $\Sigma$ Evaluate $f:=F[\sum_i \delta_{p_i}]$, where $\delta_x$ is a delta function centered at $x$. With probability "proportional to" $f$ (e.g. after repeating this process a few times to get an idea of how $F[\cdot]$ behaves), return a uniformly random sample from $\{p_1,\ldots,p_k\}$; otherwise return to step 1. This algorithm obviously makes no sense, but hopefully you see what I'm after. I could imagine for some class of $F$'s (uniformly Lipschitz?) and large enough $k$, a strategy like this could get a reasonable approximation. Or maybe there's an MCMC interpretation?
