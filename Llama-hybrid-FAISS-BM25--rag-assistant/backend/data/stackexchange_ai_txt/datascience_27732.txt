[site]: datascience
[post_id]: 27732
[parent_id]: 27725
[tags]: 
While it is always preferred to have more informative variables for building a model, in reality though, perhaps like your case, we have to live with what we have. Although, if I were you I would think of digging other external data relevant to water level like geographical data, climate (there should be many things out there that you can add to your current feature space). Anyhow your current situation, my educated guess is that a simple regression is not a good choice of model anyway. Simply it won't capture any nonlinear correlation between your independent and dependent variables suitable for prediction i.e. water levels of rivers. I would strongly suggest, if you want to build a better predictive model hopefully performing better than a simple regression and still fast and easy, to go with Gradient Boosting Decision Trees (GBDT) either using LightGBM , XGBoost , or recently my favorite Catboost implementations (otherwise you could think of Neural Network as well, but depends how much data you have etc). Each has its own pros and con, please check out this nice video by Mateusz Susikin in PyData Conference 2017 going through some of their differences. Please note when building a GBDT model, you need to be careful how to encode your categorical variables, and not least, how to include your independent continuous variables if they exist. Please go through these stackexchange post1 , post2 where I discuss ways to handle them, or at least they may give you some hints.
