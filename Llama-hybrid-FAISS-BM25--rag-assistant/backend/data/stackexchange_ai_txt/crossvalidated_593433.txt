[site]: crossvalidated
[post_id]: 593433
[parent_id]: 593427
[tags]: 
The Cybenko universal approximation theorem for neural networks and Stone-Weierstrass theorem for polynomials both state that, under what should typically be reasonable conditions for a regression, functions can be approximated as well as is demanded. In that sense, theorems like these can be seen as giving “universal” equations that will but able to fit decent regression models like linear, polynomial, or exponential, as long as you give enough flexibility (so don’t try to approximate a quadratic function with a linear polynomial). The trouble is that these do not address how to estimate those equations in the presence of noise. For instance, if you throw a gigantic polynomial regression at a set of data, assuming that Stone-Weierstrass makes it likely to pick up on even complicated trends, you can pick up on coincidences in the data, rather than the true underlying pattern that you aim to model (and what Cybenko and Stone-Weierstrass mean can be approximated). Fighting this overfitting is where a lot of the magic of machine learning and predictive modeling happens (e.g., regularization). If you want to allow for flexibly modeling nonlinear functions, a popular approach to a linear model (remember, it’s about linearity in the parameters) would be to use splines, rather than polynomials.
