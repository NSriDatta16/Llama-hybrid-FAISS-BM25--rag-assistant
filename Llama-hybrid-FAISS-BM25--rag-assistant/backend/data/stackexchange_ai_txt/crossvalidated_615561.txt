[site]: crossvalidated
[post_id]: 615561
[parent_id]: 
[tags]: 
Estimand dependent on the sample

Suppose we have a sample $S$ of IID data and two different real-valued functions of $S$ , say $\theta(S)$ and $f(S)$ , and the latter is intended to estimate the value of the former. For example, the former may be the out-of-sample prediction performance of a machine learning model trained with $S$ and the latter a performance evaluation method carried out on S, such as cross-validation. One could think the latter to be the estimand and the former its estimator, whose bias variance and other properties could be analyzed. However, since the latter also depends on the same sample as the former, I presume that one can not consider it as the estimand in the usual sense. My question is whether the correct interpretation would be to consider 0 (i.e. the constant zero) as the actual estimand and $f(S) - \theta(S)$ as its estimator? Then, the estimand would be a constant independent of the sample as it should and, for example, the bias of the estimator could be defined in the usual way as: $E[f(S) - \theta(S)]$ .
