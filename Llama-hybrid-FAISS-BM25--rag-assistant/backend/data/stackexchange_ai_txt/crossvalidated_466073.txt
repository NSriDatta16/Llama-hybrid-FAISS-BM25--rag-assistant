[site]: crossvalidated
[post_id]: 466073
[parent_id]: 466069
[tags]: 
Let's emphasize the key part of the original statement. it is possible to design a Markov chain with a stationary distribution equal to the posterior distribution, even though we don't know exactly what that distribution is. While we don't know exactly what the distribution is, we can compute the density function up to a normalizing constant. This normalizing constant disappears when if we start thinking of ratios, i.e., if we have that $f$ is our posterior density (up to a multiplicative constant) then even without knowing that normalizing constant, if we know that \begin{equation} f(\theta_1) = 2f(\theta_2) \end{equation} then we know that $\theta_1$ is twice as likely as $\theta_2$ in the posterior. Note that this relation alone completely specifies the distribution; if you were able to make a function that could take random samples and in the long run, this relation of ratios is preserved, then you can take draws from the posterior exactly. This is the goal of an MCMC algorithm. While there are a lot of different MCMC algorithms, the general theme is coming up with a really clever way of drawing samples where, in the long-run, one can reason that these ratios are preserved. For simplicity, let's think about the Metropolis-Hasting algorithm. Without going into a full proof, we can think that in the MH algorithm, we are taking draws from a random process (i.e. a random walk). We then essentially upweight draws that have very high posterior densities by getting "stuck" at that spot for several iterations, relative to spots that have low density in the posterior. Math that we apparently want to avoid tells us that in the long run, this will preserve the ratios given by the non-normalized posterior density function.
