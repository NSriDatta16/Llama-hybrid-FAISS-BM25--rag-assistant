[site]: crossvalidated
[post_id]: 442475
[parent_id]: 
[tags]: 
How chain rule and partial derivative are applied on gradient descent in deep learning?

Suppose there is a DNN like this: $h = t(w_1x+b_1)$ $g = t(w_2h+b_2)$ $\hat y = t(w_3g+b_3)$ $Loss = \frac 12\sum (y - \hat y)^2$ Just one input $x$ $t$ is activation function. Could you explain with equations how the gradients of $w_1$ , $w_2$ , $w_3$ is calculated? By the way, when combining partial derivative and chain rule, $\frac {dk}{dg}\frac {dg}{dh}\frac {dh}{dx}$ -> $\frac {\partial k}{\partial g}\frac {\partial g}{\partial h}\frac {\partial h}{\partial w_1}$ is the above fine?
