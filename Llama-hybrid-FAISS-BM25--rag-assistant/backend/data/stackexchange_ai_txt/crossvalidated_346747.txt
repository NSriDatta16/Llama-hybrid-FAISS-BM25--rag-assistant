[site]: crossvalidated
[post_id]: 346747
[parent_id]: 
[tags]: 
PyMC3: Using AR1 process as prior causes Bad initial energy error

I'm fairly new to MCMC and pymc3 in particular so apologies if this is something obvious. I'm trying to do parameter inference using PyMC3 on what I thought was a relatively simple model defined as: $$y_t \sim \mathcal{Poisson}(\lambda_t)$$ $$(\lambda_t - \mu) = \theta(\lambda_{t-1}-\mu) + \eta I_{t-1} + \epsilon$$ With $\mu \in \mathbb{R}^+$, $0 We observe $\textbf{y} = [y_0, ..., y_T]$ and $\textbf{I} = [I_1, ..., I_T]$ and we can further assume that $\lambda_t + \mu > 0$ for all $t$ avoiding any strange behaviour with the Poisson distribution. Since the builtin AR1 assumes $\mu = 0$, I have modified it as below: import theano as T import theano.tensor as tt import pymc3 as pm from pymc3.distributions.distribution import generate_samples, draw_values class NonzeroMeanAR1(pm.distributions.continuous.PositiveContinuous): """ Autoregressive process with 1 lag. Parameters ---------- k : tensor effect of lagged value on current value tau_e : tensor precision for innovations """ def __init__(self, k, tau_e, mu, *args, **kwargs): super(NonzeroMeanAR1, self).__init__(*args, **kwargs) self.k = k = tt.as_tensor_variable(k) self.tau_e = tau_e = tt.as_tensor_variable(tau_e) self.tau = tau_e * (1 - k ** 2) self.mu = tt.as_tensor_variable(mu) self.mode = tt.as_tensor_variable(0.) def logp(self, x): k = self.k tau_e = self.tau_e mu = self.mu x_im1 = x[:-1] x_i = x[1:] boundary = pm.Normal.dist(mu=mu, tau=tau_e).logp(x[0]) innov_like = pm.Normal.dist(mu=k * (x_im1 - mu) + mu, tau=tau_e).logp(x_i) return boundary + tt.sum(innov_like) def random(self, point=None, size=None): tau = draw_values([self.tau], point=point)[0] mu = draw_values([self.mu], point=point)[0] k = draw_values([self.k], point=point)[0] eps = generate_samples(sps.norm.rvs, loc=0, scale=tau**-0.5, dist_shape=self.shape, size=size) y, _ = T.scan(lambda err, prev, k, mu: k*(prev - mu) + mu + err, outputs_info=mu*tt.ones((1,), dtype='float64'), sequences=[eps], non_sequences=[k, mu]) return y Recovering the parameters for this works just fine, as below: def simple_ar(n_samples, k, sig, mu): out = np.zeros((n_samples,), dtype='float64') out[0] = mu for i in range(1, n_samples): out[i] = k * (out[i-1] - mu) + sps.norm.rvs(scale=sig, size=(1,)) + mu return out y_obs = simple_ar(500, 0.9, 0.2, 100) with pm.Model() as ar_model: k = pm.HalfFlat('k') tau = pm.HalfNormal('tau', sd=100) mu = pm.HalfFlat('mu') y = NonzeroMeanAR1('y', k=k, tau_e=tau, mu=mu, observed=y_obs) trace = pm.sample(1000, tune=500) pm.summary(trace) mean sd mc_error hpd_2.5 hpd_97.5 n_eff Rhat k 0.8959 0.0208 0.0008 0.8562 0.9360 659.9843 1.0038 tau 23.4081 1.4514 0.0535 20.8658 26.4553 689.1551 0.9997 c 100.0275 0.0829 0.0028 99.8813 100.1941 792.9259 1.0038 However, if you go and say import scipy.stats as sps yp_obs = [sps.poisson.rvs(x) for x in y_obs] And modify the model as below: with pm.Model() as par_model: k = pm.HalfFlat('k') tau = pm.HalfNormal('tau', sd=100) mu = pm.Normal('mu', mu=100, sd=5) l = NonzeroMeanAR1('l', k=k, tau_e=tau, mu=mu, shape=500) y = pm.Poisson('y', mu=l, observed=yp_obs) trace = pm.sample(1000, tune=500) This simply yields ValueError: Bad initial energy: inf. The model might be misspecified. Analysis of the error as suggested here shows that the problem lies with the variable l : for RV in par_model.basic_RVs: print(RV.name, ':', RV.logp(par_model.test_point)) k_log__ : 0.0 tau_log__ : -0.7698925914732451 mu : -2.528376445638773 l_log__ : -inf y -inf However, I'm at a complete loss regarding how to fix this. Does anyone have some thoughts? Thanks in advance!
