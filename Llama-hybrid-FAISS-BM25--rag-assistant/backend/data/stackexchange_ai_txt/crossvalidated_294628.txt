[site]: crossvalidated
[post_id]: 294628
[parent_id]: 204731
[tags]: 
If I understand the question, you're looking to use a cross-validation for tuning your random forest parameters, resulting in two holdout sets: one for cross-validation // model tuning one for a final test (from which you generate an estimated overall performance, RMSE, MAE, etc) Is that correct? Assuming it is, I would suggest first splitting your dataset into two sets -- train and the rest, then split "the rest" again into two additional datasets, thereby resulting in a CV and Test dataset. Example (Python 3.x && sklearn's train_test_split ) from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.3, random_state=10) X_cv, X_test, y_cv, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=10) I've used a seed so the datasets would be repeatable across experiments // iterations. Note that the CV and Tests datasets are derived from the first test and that I elected to make X_Train 70% of the set and a 15% / 15% split on CV and Test.
