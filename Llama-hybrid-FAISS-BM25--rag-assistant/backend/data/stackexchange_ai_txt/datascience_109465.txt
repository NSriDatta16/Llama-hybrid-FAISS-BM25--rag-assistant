[site]: datascience
[post_id]: 109465
[parent_id]: 109431
[tags]: 
Alexnet was a revolution in image recognition in 2012 ( https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf ) and the paper was titled "Deep convolutional neural networks". Referring to neutral networks as "deep" was unusual at that time. They used 8 layers, and hugely increased the state of the art (SOTA). I conclude from that, that at least in 2012, 8 layers was considered deep. Since then, I'd argue that 8 layers is no longer considered deep, but pretty standard for many image processing tasks. I'd therefore argue that the definition of deep has changed and increased over time, and we've become better at training ever-deeper networks.
