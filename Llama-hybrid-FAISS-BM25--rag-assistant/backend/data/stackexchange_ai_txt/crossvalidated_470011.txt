[site]: crossvalidated
[post_id]: 470011
[parent_id]: 469935
[tags]: 
From $$p(y=1|X,\theta)= \frac{\exp \theta^Tx}{1+\exp\theta^Tx}$$ (you forgot the $\exp$ ) and the corresponding $$p(y=0|X,\theta)= \frac{1}{1+\exp\theta^Tx}$$ the likelihood will be $$L=\prod_i \left(\frac{\exp\theta^Tx_i}{1+\exp\theta^Tx_i}\right)^{y_i} \left(\frac{1}{1+\exp\theta^Tx_i}\right)^{(1-y_i)},$$ that is, $p(y=1|X,\theta)$ for observations with $y=1$ and $p(y=0|X,\theta)$ for observations with $y=0$ . The likelihood has to include $Y$ , since $Y$ is what's random (or from a Bayesian viewpoint, it's what provides relative evidence about different values of $\theta$ ) The likelihood for exponential family models such as the binomial can be written in the form $$L(\eta; y) \propto \exp\left(y\eta -A(\eta) \right)$$ For a canonical-link regression model you take $\eta=\theta^Tx$ , and for logistic regression $\eta$ is the log odds: $\mathrm{logit}\, p(Y=1|X,\theta)$ . That's the form used in the posterior density you quote. Converting between the form in probabilities that I wrote first and the form in log-odds is slightly annoying but straightforward algebra.
