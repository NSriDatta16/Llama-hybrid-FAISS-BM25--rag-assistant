[site]: crossvalidated
[post_id]: 561783
[parent_id]: 561769
[tags]: 
TL;DR, multiply one of the inputs by 0. The pre-Softmax layer in a neural network produces some real 2-element vector $z_i$ . If you choose $b_i = 0$ and $w_1 = 1$ and $w_2=0$ , then $\text{Softmax}(z)_i = \sigma(z_i)$ . $$\begin{align} \text{Softmax}(x)_i &= \frac{\exp(x_i)}{\exp(x_1)+\exp(x_2)} \\ \text{Softmax}(z)_i &= \frac{\exp(w_i z_i + b_i)}{\exp(w_1 z_1 + b_1)+\exp(w_2 z_2 + b_2)} \\ \implies \sigma(z_1) &= \frac{\exp(z_1)}{\exp(z_1)+1} \\ \sigma(z_2) &= \frac{1}{\exp(z_1)+1} \\ \end{align}$$ This will work as long as the softmax and sigmoid networks have the same parameters (except for $w,b$ ). The easiest way to assure that is to just make a single model object that has options for sigmoid or softmax output. See also: Sigmoid equivalent to Softmax exercise
