[site]: crossvalidated
[post_id]: 460216
[parent_id]: 460024
[tags]: 
I believe that condition $\Delta t \gamma = 1$ is not an approximation, but rather, a deliberate choice of simulation parameter $\Delta t = 1 / \gamma$ . This is from the paper's Model development section: " $t$ may be defined as $I=R_0^t$ ". It's a rather loose specification, and I believe that what is meant is that $I=R_0^{t/\Delta t}$ , with the approximations describe in the posted question. Specifically, for small $t$ , $I \ll N$ , $S \simeq N^-$ , and $R_e \simeq R_0^- = ( \beta / \gamma )^- $ . The result in the posted question $ I_{t+1} = R_e I_t $ becomes $ I_{t+1} \simeq R_0 I_t $ It is then clear how the model yields $I = R_0^t$ . It occurred to me that the article does not distinguish between a time $t$ and integer enumeration of time step, $t/\Delta t$ . Rather, it uses $t$ for the latter. This is why $I = R_0^t$ instead of $I = R_0^{t / \Delta t}$ . Since this is my own oversight, I might delete this question. AFTERNOTE caveat There seems to be a discrepancy between $\Delta t = 1 / \gamma$ and the paper's term "serial interval" (SI). SI is the time from a individual being infected to the time when they pass that on to someone else. From the simple SIR model , the infectious period is $1/\gamma$ . The reason why the number of people infected by one individual is $R_0=\beta/\gamma$ is because the individual contacts people at a rate of $\beta$ throughout the duration of $1/\gamma$ . So the average time from the when the individual got infected to when they infect a contact would be approximately half of $1/\gamma$ , i.e., SI= $1/2\gamma$
