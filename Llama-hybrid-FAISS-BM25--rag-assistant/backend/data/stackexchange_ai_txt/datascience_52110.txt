[site]: datascience
[post_id]: 52110
[parent_id]: 
[tags]: 
Q-value estimate in neural episodic control

Disclaimer: I'm not that familiar with reinforcement learning, so I might lack some basic knowledge on that topic. I was reading the neural episodic control (NEC) paper by Pritzel et al., 2017 and I'm a bit unclear on the Q-value estimates. Eq. 3 says: $Q^{(N)} = \sum_{j=0}^{N - 1} \gamma^jr_{t+j} + \gamma^N\text{max}\underset{a'}Q(s_{t+N}, a').$ The $\text{max}\underset{a'}Q(s_{t+N}, a')$ term is found by querying all the memories in the differentiable neural dictionary (DND) for each action and taking the max. This estimate is then added to the replay buffer: $(s_t, a_t, R_t),$ where $R_t = Q^{(N)}(s_t, a)$ If I understand the paper correctly, once the Q-value estimate is added to the replay buffer the only way to remove it is by overloading the buffer limit, meaning that outdated estimates are continuously being used in the training process. The question is: isn't it better to re-evaluate the Q-value estimate at each iteration? If, instead of storing the Q-value in the buffer we estimated it on the fly from the current state of DND it would perhaps help the model converge even faster?
