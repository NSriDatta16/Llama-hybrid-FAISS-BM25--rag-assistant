[site]: crossvalidated
[post_id]: 306289
[parent_id]: 
[tags]: 
Interpretation of order of approximation of errors

I'm looking for a simple interpretation of order of approximation error. To be precise, I'm reading Azevedo-Filho and Shachter (1994) in order to explain how to use Laplace approximation for posterior approximation in a bayesian context, and it says that applying Laplace approximation gives errors $O(n^{-2})$, instead of errors $O(n^{-1})$ which is the result of applying the bayesian CLT. Why is it better to have errors $O(n^{-2})$ than errors $O(n^{-1})$? I'm thinking that I could use the term superconsistency, that is, $d$ is superconsistent when $T^n(\delta-d)$ converges to a random variable for $n>1/2$. Here the explanation would be that errors $O(n^{-2})$ converges to zero at a faster rate that $O(n^{-1})$, or am I wrong?
