[site]: crossvalidated
[post_id]: 95038
[parent_id]: 
[tags]: 
How does Factor Analysis explain the covariance while PCA explains the variance?

Here is a quote from Bishop's "Pattern Recognition and Machine Learning" book, section 12.2.4 "Factor analysis": According to the highlighted part, factor analysis captures the covariance between variables in the matrix $W$ . I wonder HOW ? Here is how I understand it. Say $x$ is the observed $p$-dimensional variable, $W$ is the factor loading matrix, and $z$ is the factor score vector. Then we have $$x=\mu+Wz+\epsilon,$$ that is \begin{align*} \begin{pmatrix} x_1\\ \vdots\\ x_p \end{pmatrix} = \begin{pmatrix} \mu_1\\ \vdots\\ \mu_p \end{pmatrix} + \begin{pmatrix} \vert & & \vert\\ w_1 & \ldots & w_m\\ \vert & & \vert \end{pmatrix} \begin{pmatrix} z_1\\ \vdots\\ z_m \end{pmatrix} +\epsilon, \end{align*} and each column in $W$ is a factor loading vector $$w_i=\begin{pmatrix}w_{i1}\\ \vdots\\ w_{ip}\end{pmatrix}.$$ Here as I wrote, $W$ has $m$ columns meaning there are $m$ factors under consideration. Now here is the point, according to the highlighted part, I think the loadings in each column $w_i$ explain the covariance in the observed data, right? For example, let's take a look at the first loading vector $w_1$, for $1\le i,j,k\le p$, if $w_{1i}=10$, $w_{1j}=11$ and $w_{1k}=0.1$, then I'd say $x_i$ and $x_j$ are highly correlated, whereas $x_k$ seems uncorrelated with them , am I right? And if this is how factor analysis explain the covariance between observed features, then I'd say PCA also explains the covariance, right?
