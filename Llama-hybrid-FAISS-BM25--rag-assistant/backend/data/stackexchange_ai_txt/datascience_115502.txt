[site]: datascience
[post_id]: 115502
[parent_id]: 40483
[tags]: 
After 4 years and much more experience with Machine Learning (I started a full-time career in the field) I think I have a better idea of the answer. Prediction accuracy can be used here, but as mentioned by @mucho, it's not the best for data which naturally have multiple categorical answers. Here, the ground-truth is a probability distribution. If our task is to perfectly mimic this distribution, we should use a metric which compares probability distributions, such as Cross-Entropy (which is mathematically equivalent to KL Divergence). If our objective is to get a good language model, there are other ways. A popular objective since 2018 is Masked Language Modeling (a Cloze task) which is employed by the BERT architecture l
