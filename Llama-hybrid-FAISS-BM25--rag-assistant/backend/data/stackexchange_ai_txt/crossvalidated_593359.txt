[site]: crossvalidated
[post_id]: 593359
[parent_id]: 592742
[tags]: 
$z_e(x)$ is the output of the encoder network, and $e$ is the embedding. They are mutually-related, and both need to be optimized. In general, the separation using stop-gradients can be understood, in my opinion, as an Alternating Projections kind of optimization algorithm, where you need to simultaneously optimize 2 mutually-related subsystems, so you do it by "freezing" one while optimizing the other, so that the optimization will not "collapse" into a trivial wrong solution. In the context of the VQ-VAE paper, this is almost the same as the way the $k$ -means algorithm operates, by alternating between (phase 1) estimating centroids and (phase 2) deciding which element belongs to each centroid. You can see that equations (1) and (2) in the paper are essentially a $k$ -means criterion. The similarity to a $k$ -means criterion is not just my own opinion -- note in Appendix A.1 of the paper, where the authors explicitly mention this close similarity to $k$ -means. The paper further justifies the stop-gradient in the 2 paragraphs before equation (3), by emphasizing that the stop-gradient in different terms of the loss will cause the loss to effect learning (optimizing) in different subsystems of the overall system.
