[site]: crossvalidated
[post_id]: 46841
[parent_id]: 46826
[tags]: 
I agree with John. Here are a few more points. An influential observation is (strictly) one that influences the parameter estimates. A small deviation in the Y value gives a big change in the estimated beta parameter(s). In simple regression of 1 variable against another, influential variables are precisely those whose X value is distant from the mean of the X's. In multiple regression (several independent variables), the situation is more complex. You have to look at the diagonal of the so called hat matrix $X(X'X)^{-1}X'$, and regression software will give you this. Google "leverage". Influence is a function of the design points (the X values), as your textbook states. Note that influence is power. In a designed experiment, you want influential X values, assuming you can measure the corresponding Y value accurately. You get more bang for the buck that way. To me, an outlier is basically a mistake - that is, an observation that does not follow the same model as the rest of the data. This may occur because of a data collection error, or because that particular subject was unusual in some way. I don't much like stattrek's definition of an outlier for several reasons. Regression is not symmetric in Y and X. Y is modelled as a random variable and the X's are assumed to be fixed and known. Weirdness in the Y's is not the same as weirdness in the X's. Influence and outliership mean different things. Influence, in multiple regression, is not detected by looking at residual plots. A good description of outliers and influence for the single variable case should set you up to understand the multiple case as well. I dislike your textbook even more, for the reasons given by John. Bottom line, influential outliers are dangerous. They need to be examined closely and dealt with.
