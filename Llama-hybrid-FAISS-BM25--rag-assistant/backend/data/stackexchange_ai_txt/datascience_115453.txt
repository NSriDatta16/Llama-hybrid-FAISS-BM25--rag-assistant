[site]: datascience
[post_id]: 115453
[parent_id]: 
[tags]: 
Do large pretrained language models already "know" about NLP tasks?

Nowadays the state-of-the-art in NLP is to finetune a large pretrained language model such as BERT/GPT etc. on specfic tasks. These language models are pretrained on a huge amount of data and then basically evaluated on popular labeled datasets published for e.g. Question Answering, Machine Translation etc. . As those datasets became the de facto default of evaluating these model, those datasets have been published over and over again on various websites. Used and reused for people that are building their own small model etc. So basically these datasets (train and test data) including their e.g. label in classification tasks or the answer in Q/A tasks "stray" around in the internet. So now when training a new large language model (with a novel architecture) it is fed with text data that is often scraped from the internet as well. Wouldn't it be possible that in the training phase of these LMs, the networks have already seen this exact data (and learned on its co-occurence) which they are evaluated on later on? This would basically defeat the purpose of the evaluation as the test data already leaked into the process of pretraining the language models. Are there any prefiltering steps happening in pretraining these models such that this doesn't happen? And secondly, even if the network has seen the exact test data with e.g. test set question+answer among billions of other textual data, would it even pick up on that or is it just too much data anyways for the model to adjust the weights accordingly and "remembering" these exact datapoints.
