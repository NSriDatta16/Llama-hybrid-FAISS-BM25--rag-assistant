[site]: crossvalidated
[post_id]: 511541
[parent_id]: 
[tags]: 
Regarding bagging, boosting and the NFL theorem

According to my understanding, bagging and boosting work in the following way: Bagging: Combine several high-variance/low-bias models to produce an ensemble model with lower variance and equal bias. Boosting: Combine several low-variance/high-bias models to produce an ensemble model with lower bias. At this point, I am not sure what happens to variance. On the one hand the fact that models are averaged should reduce variance (hypothesis 1). On the other hand, the increasing effort to reduce bias may lead to overfitting and cause a higher variance model (hypothesis 2). Regarding XGBoost vs GradientBoostingClassifier, I 've read that the two main distinction between them is (a) the exploitation of parallel computation and (b) XGBoost introduces regularization terms to Gradient Boosting. For me, point (b) is a hint that reinforces the idea that Gradient Boosting is prone to overfitting as stated in (hypothesis 2). Otherwise, why introduce the regularization terms in XGBoost? With Bagging, specifically Random Forest, variance is reduced by combining several uncorrelated models. While boosting also combines models, these may be too correlated in order to reduce variance, which to me discourages (hypothesis 1). The No Free Lunch theorem states that "assumptions made by machine learning algorithms mean that some algorithms will fit certain data sets better than others". source: https://www.kdnuggets.com/2019/09/no-free-lunch-data-science.html Questions: How is bagging not a free lunch? Essentially, if it reduces variance while maintaining bias, should there be any reason not to use bagging (on top of some base learner) in every scenario and dataset? With boosting, I assume bias is decreased, but what happens to variance? Is it (hypothesis 1) , (hypothesis 2) or something else? Again, if (hypothesis 1) is correct, how is this not a free lunch? Ignoring the improvements in computational speed, would it be right to say that the introduced regularization terms are the main reason why XGBoost performs better than e.g. scikit-learn's GradientBoostingClassifier?
