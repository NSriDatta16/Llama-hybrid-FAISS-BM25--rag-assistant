[site]: datascience
[post_id]: 107552
[parent_id]: 107446
[tags]: 
In general, the common way to do it is indeed at every time step. Whether this is expensive or not, I suppose it depends fully on your problem/computational means rather than on the algorithm. That being said, there are special categories of methods that may admit other answers: off-policy and off-line methods: Those are equivalent to compiling a dataset and train on your own time, similar to supervised learning. Approaches that use Monte Carlo estimates (like policy gradients or monte carlo tree search): You need to collect trajectories and then you perform the updates/training step(s) when a trajectory finishes. Approaches that use target networks (like DQN or DDPG): The target networks are usually trained on a slower rate compared to the "principal" networks. Model-based approaches (like World Models): Such may have separate training periods for learning the environment dynamics and for constructing a policy. There are most certainly more categories and a more fine-grained can be done but this list probably covers most cases.
