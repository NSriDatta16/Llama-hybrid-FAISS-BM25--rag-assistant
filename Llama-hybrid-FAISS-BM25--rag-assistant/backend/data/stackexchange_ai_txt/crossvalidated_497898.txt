[site]: crossvalidated
[post_id]: 497898
[parent_id]: 
[tags]: 
adjustment based on k neighbors

I am using a nearest neighbor approach based on features to pick the most similar neighbor-tickers in order to predict the performance of target-ticker . My target-ticker has certain features and I'm picking neighbor-tickers based on a smallest euclidean distance between the features of neighbor-ticker and target-ticker . I am picking k neighbors and at present I am weighting all features equally. To evaluate how important my features are in my prediction of the target-ticker , I'm taking either the squared or absolute value of the error between the average of neighbor-tickers and target-tickers performance . Explicity, this is error = target-ticker_performance - average(neighbor-tickers_performance) . Now across many target-tickers , looking at the sum of squared errors SSE and sum of absolute errors SAE gives me an indication of how well my neighbor-tickers are predicting the target-ticker , or really, how well my features are allowing me to pick better performing neighbor-tickers . Now I'm trying to determine the optimal number of neighbors k : As a sanity check, I decided to run the process with features that I believe have no relationship to performance , and as I increase k , the SSE and SAE decline in a non-linear manner. Based on this, it appears that I need an adjustment to my errors based on k . I think the adjustment would be in some way related to the distribution of my underlying population's performance from which my neighbor samples are drawn. For example, the distribution of performance in my population is very right skewed. Any ideas for how to adjust my errors based on the number of neighbors , k , in my sample? Thank you
