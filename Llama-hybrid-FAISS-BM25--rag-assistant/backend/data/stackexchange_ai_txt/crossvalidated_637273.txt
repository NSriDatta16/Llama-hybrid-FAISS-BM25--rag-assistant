[site]: crossvalidated
[post_id]: 637273
[parent_id]: 
[tags]: 
When is PR curve more informative than ROC curve?

I am reading the paper A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks and the section 2 discusses the properties of AUROC vs AUPR. Some conclusions in the text confuse me and I would appreciate clarification Consequently, a random positive example detector corresponds to a 50% AUROC, and a “perfect” classifier corresponds to 100%. The AUROC sidesteps the issue of threshold selection, as does the Area Under the Precision-Recall curve (AUPR) which is sometimes deemed more informative (Manning & Schütze, 1999). This is because the AUROC is not ideal when the positive class and negative class have greatly differing base rates, and the AUPR adjusts for these different positive and negative base rates. For this reason, the AUPR is our second evaluation metric. So in ROC, a random detector corresponds to a diagonal line and a perfect classifier has 100% area. For PR, the baseline, however, depends on the probability of true class. For example if the P(T = 0.9) then an unskilled predictor has already an area of 90%. So why do the authors conclude that PR is sometimes more informative? I think the passage that confuses me the most is This is because the AUROC is not ideal when the positive class and negative class have greatly differing base rates, and the AUPR adjusts for these different positive and negative base rates. As far as Wikipedia states ( https://en.wikipedia.org/wiki/Base_rate ), the base rate is basically the probability of a class. Did I understand it falsely?
