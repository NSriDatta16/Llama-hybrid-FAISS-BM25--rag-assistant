[site]: datascience
[post_id]: 121925
[parent_id]: 
[tags]: 
Can I add a new output class to a decoder and train only the final layer?

I am wondering how to approach a project, where I would like to increase the number of output classes of an already trained network. I have very good reason to believe that the model has already learnt the relevant information to be able to predict this new class, that is why I would aim only for fine-tuning (also, I have a lot less data for this class than for the rest and I do not have the hardware to train from scratch). The model that I want to use is a transformer, where the decoder's final layers are 2 fully connected layers and a layernorm. To my understanding, new classes to a network can be added by freezing the model except for the final layer(s), increase the output dimension and fine-tune only this part of the network. Is this a reasonable approach? If yes, do you usually take the weights for these layers and just increase the size of the weight matrix with some random weights (or just try both, and see which one gives better results)?
