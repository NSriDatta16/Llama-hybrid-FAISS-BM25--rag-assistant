[site]: crossvalidated
[post_id]: 292612
[parent_id]: 25645
[tags]: 
(Edited) Since, no one had added this. Let me contribute here. Sethuraman's stick breaking representation for Dirichlet Process is incredibly useful for understanding DP model and also for simulating DP. Basically, Sethuraman tells us that one can think of the weights which appear in Dirichlet Process as a product of stick breaking process (beta). Here's the link for the paper. http://www.jstor.org/stable/24305538?seq=1#page_scan_tab_contents One of the main reason, why DP is hard to handle because it's literally an infinite mixture model. An excellent paper by Ishwaran tells us that it's alright to approximate DP as a finite mixture model. Here's the paper https://people.eecs.berkeley.edu/~jordan/sail/readings/archive/ishwaran-Mixture.pdf . Once, you have internalized above methods, it's fairly straightforward to do Gibbs Sampling with DP with say Normal Base Distribution. Let's say that your data y also comes from the Normal Distribution. let's also that the cluster mean of your data (y) comes from DP. Then, here are the steps to do a simple Gibbs Sampling with DP Choose a large K (total number of cluster). Initialize cluster mean and variance for each cluster. Initialize the stick breaking prior. 2#. Choose a data point Compute the probability that data point lies in a particular cluster . Do the Step 3 for all of the cluster. Normalize the above probability, so that it adds up to 1. Using the above probability allocate the data point to a particular cluster. Do step 3-6 for all your data points. Now update the number of points in all the cluster. Using basic Bayesian Methods, update the cluster means and cluster variance. Repeat the above process for however number of iteration you want. Discard the firs half of your sample. Viola, you did a MCMC for your DP model. Here's an excellent R code shared by Brian Neelon, which uses the above two concept to make an elegant DP code. http://people.musc.edu/~brn200/r/DP03.r
