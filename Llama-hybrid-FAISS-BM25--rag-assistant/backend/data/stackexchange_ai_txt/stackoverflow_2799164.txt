[site]: stackoverflow
[post_id]: 2799164
[parent_id]: 2785382
[tags]: 
There is one problem with this approach: if you have vectors from R^n and your network maps those vectors into the interval [0, 1], it will not be guaranteed that the network represents a valid probability density function, since the integral of the network is not guaranteed to equal 1. E.g., a neural network could map any input form R^n to 1.0. But that is clearly not possible. So the answer to your question is: no, you can't. However, you can just say that your network never sees "unrealistic" code samples and thus ignore this fact. For a discussion of this (and also some more cool information on how to model PDFs with neural networks) see contrastive backprop .
