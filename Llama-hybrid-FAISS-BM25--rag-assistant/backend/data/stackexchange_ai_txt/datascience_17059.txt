[site]: datascience
[post_id]: 17059
[parent_id]: 17057
[tags]: 
I'm confused. As you have said, the targets are ratings. It's definitely a regression problem to me, instead of a classification problem. There's several problems in your code: For regression problem, we usually use linear as activation function of the last layer (sometimes relu , even sigmoid ). And also we use mse as metric(sometimes mae , msle , etc). categorical_crossentropy is used for classification problem, and sparse_categorical_crossentropy is used for sparse input classification problem. Ref KerasClassifier is used for classification problem, use KerasRegressor instead. metrics=['accuracy'] is used for classification problem, and it's meaningless in regression problem. Ref Assuming df is a Pandas DataFrame, then df.values is naturally a ndarrary , there's no need to cast np.array . Now answering your question: As I've never used wrappers for scikit-learn AP I, I'm not very sure why you have integer output, my best guess is because of KerasClassifier and .predict (in scikit-learn APIs, .predict returns integer, basically the predicted class and .predict_proba returns float, indicating the probability of each class). Try to use .predict_proba , it would help. BTW, you should really use KerasRegressor . As you have mentioned, there's so many classes to predict. In fact, it's a regression problem. Could you please add the links of these 3 papers? Neural networks having only 1 neuron in output layer seem to be a regression NN to me. Regression NN usually uses Dense(1, activation='linear') as the output layer. Here's my version of your code, it might work: # fix random seed for reproducibility seed = 7 np.random.seed(seed) X = df[FEATURES].values # no need to cast np.array Y = df["MTPS"].values # define baseline model def baseline_model(): # create model model = Sequential() model.add(Dense(10, input_dim=(len(FEATURES)), init='normal', activation='relu')) model.add(Dense(1, init='normal', activation='linear')) # one neuron, linear activation # Compile model model.compile(loss='mse', optimizer='adam') # mse loss return model #build model estimator = KerasRegressor(build_fn=baseline_model, nb_epoch=100, batch_size=5, verbose=2) # KerasRegressor for regression problem #cross validation X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=seed) estimator.fit(X_train, Y_train) #print class predictions print estimator.predict(X_test) print Y_test References
