[site]: crossvalidated
[post_id]: 49169
[parent_id]: 
[tags]: 
Logistic regression, loss function and KL divergence

In decision theory, a loss function signature is supposed to be output space * output space -> error There seems to be many different definition of 'the logistic loss' on the web Some define it as 'the negative of the log likelihood' boyd : this is clearly not a loss function in the decision theory point of view Some define it as a function of R * [K ] -> R upen : same pb wih the domain So would the correct decision theory loss be expressed in term of distribution ? That is: given an x, I get back a distribution over the K cases, given by the softmax equation in the case of the logistic regression (output space = distributions) my loss function is the KL distance between this distribution and the observed one (signature = distributions * distribution -> R) PS: R is for the real number and [K] is the discrete set of integer from 1 to K
