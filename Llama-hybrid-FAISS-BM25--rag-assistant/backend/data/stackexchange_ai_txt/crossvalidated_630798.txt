[site]: crossvalidated
[post_id]: 630798
[parent_id]: 353552
[tags]: 
Yet another solution: First let's note the support vectors (SV) output by sklearn implementation: import numpy as np from sklearn.svm import SVC X = np.array([[3,4],[1,4],[2,3],[6,-1],[7,-1],[5,-3]] ) y = np.array([-1,-1, -1, 1, 1 , 1 ]) clf = SVC(C = 1e5, kernel = 'linear') clf.fit(X, y) clf.support_vectors_ # array([[ 2., 3.], # [ 6., -1.]]) By observing that out of the $n=6$ training data-points, two of the data-points $A=(6,-1)$ and $B=(2,3)$ suffice to construct the set of support vectors (and define the max-margin decision boundary), and the dual maximization objective $L$ being $L(\alpha)=\sum\limits_{i=1}^6 \alpha_i - \frac{1}{2}\sum\limits_{i=1}^6\sum\limits_{j=1}^6 \alpha_i\alpha_j y_i y_j \langle x_i, x_j \rangle$ , s. t. $\quad\quad\quad\quad\sum\limits_{i=1}^6 \alpha_i y_i = 0$ and $\alpha_i \geq 0 \quad \forall{i}$ . Noting that $\alpha_i=0, \quad \forall{i}$ except the points $A, B$ (due to KKT complimentary slackness condition), the dual Lagrangian reduces to the following: $L(\alpha_A, \alpha_B, \lambda)$ $= (\alpha_A + \alpha_B) - \frac{1}{2}\left(\alpha_A^2 (y_A)^2 \langle A, A \rangle + \alpha_B^2 (y_B)^2 \langle B, B \rangle + 2\alpha_A \alpha_B (y_A y_B) \langle A, B \rangle\right) + \lambda (\alpha_A y_A + \alpha_B y_B)$ , $=(\alpha_A + \alpha_B) - \frac{1}{2}\left(37\alpha_A^2 + 13\alpha_B^2 - 18\alpha_A \alpha_B\right) + \lambda (\alpha_A - \alpha_B)$ , where $\lambda$ is the lagrange multiplier. Also, $y_A$ and $y_B$ have opposite signs (let $y_A=+1 \implies y_B=-1$ ). At a critical point, we shall have, $\frac{\partial L}{\partial \alpha_A} = 0 \implies 37\alpha_A-9\alpha_B-\lambda=1$ and $\frac{\partial L}{\partial \alpha_B} = 0 \implies 13\alpha_B-9\alpha_A+\lambda=1$ $\frac{\partial L}{\partial \lambda} = 0 \implies \alpha_A=\alpha_B$ Putting $\alpha_A=\alpha_B$ in first two equations yields $28\alpha_A-\lambda=1$ $4\alpha_A+\lambda=1$ $\implies \alpha_A=\alpha_B=\frac{1}{16}$ Hence, $w=\sum_\limits{i=1}^6 \alpha_i y_i x_i = (\alpha_A y_A) A + (\alpha_B y_B) B = \frac{1}{16}(6, -1) - \frac{1}{16} (2, 3) = \left(\frac{4}{16},-\frac{4}{16}\right)$ $\implies w = \left(\frac{1}{4},-\frac{1}{4}\right)$ Also, from $y_i (w^T x_i + b) = 1$ , we have $b = y_i - w^T x_i$ . $\implies b = 1 - \left\langle \left(\frac{1}{4}, -\frac{1}{4}\right), (6, -1) \right\rangle = 1 - \frac{7}{4} = -\frac{3}{4}$ . We can follow the above calculations with SV set containing $3$ data-points $A=(6,-1),\; B=(2,3), \; C=(3,4)$ , there will be $3$ dual decision variables and the equations will be more complex, but we shall end with the same $w$ and $b$ .
