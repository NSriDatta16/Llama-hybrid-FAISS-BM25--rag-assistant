[site]: crossvalidated
[post_id]: 158018
[parent_id]: 157985
[tags]: 
Yes there are. Imagine your goal is to build a binary classifier. Then you model your problem as estimating a Bernoulli distribution where, given a feature vector, the outcome belongs to either one class or the opposite. The output of such a neural network is the conditional probability. If greater than 0.5 you associate it to a class, otherwise to the other one. In order to be well defined, the output must be between 0 and 1, so you choose your labels to be 0 and 1, and minimize the cross entropy, $$ E = y(x)^{t}(1-y(x))^{1-t} $$ where $y(x)$ is the output of your network, and $t$ are the target values for your training samples. Hence, you need $t \in \left\{0, 1\right\}$.
