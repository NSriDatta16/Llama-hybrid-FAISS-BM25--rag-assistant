[site]: datascience
[post_id]: 76835
[parent_id]: 76622
[tags]: 
If you want a DL approach, I recommend substituting the tf-idf by some kind of word embeddings. For instance, you can take a pre-trained word embedding model, like glove, and average its outputs both in resume and job description, and then compute cosine similarity. However, I recommend to use a contextual word embedding (BERT-like), as the terms in resumes might be very dependent on the context. The following article also introduces sentence-bert, which I think is very suited for your problem.
