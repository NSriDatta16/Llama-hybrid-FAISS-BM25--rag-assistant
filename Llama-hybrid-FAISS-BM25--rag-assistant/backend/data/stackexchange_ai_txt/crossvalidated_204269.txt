[site]: crossvalidated
[post_id]: 204269
[parent_id]: 71054
[tags]: 
Two qualitative answers that seem reasonable are that: the more layers you have, the more computation you have to perform during a training step. This computation (think for example of backpropagation as the simplest example) may be linear with the number of layers, and with deep networks you easily get to 20-30 layers. Tools such as TensorFlow require to build a data flow graph in order for the computation to be parallelized while respecting dependencies between the single weight updates. there are several hyperparameters of the network that have to be trained from data too. For example, consider choosing which filters to use in the first layer of a convolutional neural network: each combination that you try in an grid search or a random search results in a new network to train from scratch, and that has to be evaluated with all the others to find the one with the lowest error on a validation set.
