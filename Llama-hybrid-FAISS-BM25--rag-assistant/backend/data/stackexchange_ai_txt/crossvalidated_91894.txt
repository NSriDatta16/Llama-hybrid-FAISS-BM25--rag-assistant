[site]: crossvalidated
[post_id]: 91894
[parent_id]: 91453
[tags]: 
A simple Bayesian approach would be to model each question response as a categorical distribution, with a prior using the Dirichlet distribution . The categorical and Dirichlet distributions are multivariate generalizations of, respectively, the Bernoulli and beta distributions. The categorical takes a vector parameter, say $\theta$, of the probability of each response to the question, and the Dirichlet describes the density of a vector of of $k$ values summing to one, given a parameter $\alpha$ of $k$ positive-valued reals. In this case, $\theta$ is the parameter of interest: We compare the distributions of $P(\theta_i|D)$ and $P(\theta_j|D)$ for some questions $i,j$. The Dirichlet is the conjugate prior for the categorical and multinomial, and the posterior distribution of $\theta$ will take the form of a Dirichlet distribution with parameter $\alpha + c$, where c is the vector of observed counts of responses to each question. As nicely described on wikipedia : Intuitively, we can view the hyperprior vector Î± as pseudocounts, i.e. as representing the number of observations in each category that we have already seen. Then we simply add in the counts for all the new observations (the vector c) in order to derive the posterior distribution. A typical convenience prior for the hyperparameter $\alpha$ is the Jeffreys prior, $\alpha_i = \frac{1}{2}$ for all $i = 1, ..., k$. (The Jeffreys priors are a class of non-informative priors or so-called "objective" priors, often used in cases of scant prior information.) In sum, the above model would produce a posterior model $P(\theta|D)$ that takes the form of a $Dir(\alpha_{post})$ distribution, where $\alpha_{post}$ is a vector of the observed number of responses to each category plus the corresponding prior parameter, $\frac{1}{2}$. Then it's just a matter of interpreting the posterior.
