[site]: crossvalidated
[post_id]: 347970
[parent_id]: 
[tags]: 
(1995) Bishop's cite on weight decay regularization

On the book "Neural networks for pattern recognition" [Bishop, 1995], in chapter 9 about regularization there is a paragraph that says: Some heuristic justification for the weight-decay regularizer can be given as follows. We know that to produce an over-fitted mapping with regions of large curvature requires relatively large values for the weights. For small values of the weights the network mapping represented by a multi-layer perceptron is approximately linear, since the central region of a sigmoidal activation function can be approximated by a linear transformation. Okey so I understand the first part I think. Looking at the image below (taken from Goodfellow.I, deeplearningbook.org), it is clear that for the rightmost figure, the learned function has regions of large curvature, which are explained by high values of the weights of the network. However, I don't understand what he meant with the second part of the paragraph, namely with the sentence "For small values of the weights the network mapping represented by a multi-layer perceptron is approximately linear, since the central region of a sigmoidal activation function can be approximated by a linear transformation" . Does he mean that, if the weights are small, then $w_i * feature_i$ will be small and will therefore (assuming sigmoid activation function) lie in the linear region of the sigmoid activation function? Because if that is what he means, then why would we use sigmoid functions? It wouldn't make sense if we are not using the non-lineariy that it provides, right? Or am I missing something? Thanks!!
