[site]: crossvalidated
[post_id]: 65691
[parent_id]: 65683
[tags]: 
I can give you one possible reason why two variables can sometimes perform better than three. It could be that a model with three variables is overfitting. One way to decrease overfitting is to reduce the complexity of the model (eg. using two variables instead of three). This way the model will generalize better to unseen test data and perform better. There are ways to see if there is an overfitting problem (eg. comparing training error to test error). In terms of systematic ways to select the optimal set of input variables, this is called feature selection. One such approach is to try every single possible combination of input variables and see which combination performs the best. This can be intractable if you have a large number of possible features so sometimes approximation techniques are used (eg. greedy search). The impact of correlation among input variables depends on the prediction algorithm that you are using. Every model has its own inductive bias. If you are worried about redundancy within your input variables you should try dimensionality reduction (eg. PCA) which will reduce your input variables to only the most informative (i.e. variables that contribute most of the variation within your dataset).
