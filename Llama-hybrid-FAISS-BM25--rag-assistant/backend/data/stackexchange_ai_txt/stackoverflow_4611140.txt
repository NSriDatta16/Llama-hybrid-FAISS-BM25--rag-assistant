[site]: stackoverflow
[post_id]: 4611140
[parent_id]: 4608180
[tags]: 
We do Acceptance TDD at my work. When I first started I was told I could implement whatever policies I wanted so long as the work was completed in a timely and predictable fashion. Having done unit testing in the past I realized that one of the problem we always ran into were integration bugs. Some could take quite a long time to fix and were often a surprise. We would run into subtle bugs we introduced while extending the app's functionality. I decide to avoid those issue I had run into in the past by focusing more on the the end result features that we were suppose to deliver. We would write tests that tested the acceptance behavior, not just at the unit level, but at the whole system level. I wanted to do that because at the end of the day I don't care of the unit works correctly, I care that the entire system works correctly. We found the following benefits to doing automated acceptance tests. We NEVER regress end user functionality because it is explicitly tested for. Refactors are easier because we don't have to update a bunch of unit tests. We just have to make sure our acceptance test still passes. The integration of the "units" are implicitly covered. The tests become a very clear definition of required end user functionality. Integration issues are exposed earlier and are less of a surprise. Some of the trade offs to doing it this way Tests can be more complex in terms of usage of mocks, stubs, fixtures, etc. Tests are less useful for narrowing down which "unit" has the defect. We also make our test suite runnable via a Continuous Integration server which tags and packages for deployment. It runs with every commit as with most CI setups. With regard to your points/concerns: Setup: The whole webapp is bootstrapped (like it would be seen from end-user). One compromise we do tend to make is to run the test in the same process space ala unit tests. Our entry point is the top of the app stack. We don't bother to try and run the app as a server because that adds to the complexity and doesn't add much in terms of coverage. Test Entry: HTTP call itself. Browser can be involved as test executer (e.g. Selenium) All of our automated tests are driven by a simulating a HTTP GET, POST, PUT, or DELETE. We don't actually use a browser for this though, a call into the top of the app stack the way the particular HTTP call get's mapped in works just fine. Assert Targets: The test output is the complete rendered response (HTML and other artifacts like javascript). Asserts on the database (e.g. data got inserted) can also be included. I think this where automated acceptance tests really shine. What you assert is the end user functionality you want to guarantee that you are implementing. Controller tests are close to general system behaviour (e.g. submit login form, password validation, successful login). This is very close what an End-to-End test would do. In the end "double-testing" could happen, which is highly inefficient. We actually do very little unit testing and rely almost solely on our automated acceptance tests. As a result we don't have much in the way of double testing. Controller are more white-boxed tests and tend to be brittle because they rely on many dependencies of lower layers (in difference to very fine grained unit-tests). Beause of this setting up maintaining Controller tests is high effort, End-to-End test where the whole application is started as black box is more trivial and have advantage being closer to production. They may have more dependencies, but those can be mitigated through the usage of mocks and fixtures. We also usually implement our test with 2 modes of execution. Unmanaged mode where the tests runs fully wired to the network, dbs, etc. And Managed mode where the test runs with the unmanaged resources mocked out. Although you are correct in your assertion that the tests can be alot more effort to create and maintain.
