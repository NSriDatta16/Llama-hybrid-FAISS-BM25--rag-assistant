[site]: datascience
[post_id]: 93537
[parent_id]: 93419
[tags]: 
Yes you can get the embedding of words. In sentence-transformers you need to see where and how the embedding of words are extracted. Usually the sentence embedding in BERT is simple a max or mean over all word embeddings, so BERT does have word embeddings. The question is if it helps you at all as, like you mentioned, at the end of the day you have a bunch of words not a language. The point is that if you have enough data you can fine-tune BERT (see sentence-transformers documentation) and probably capture the difference-similarity between each sample. Another way is to get your embedding trough LSTM which is more suitable for such sequences and you can train it using for example Keras. I suggest trying SGT as well and compare its embedding capability with other methods so you can choose the best.
