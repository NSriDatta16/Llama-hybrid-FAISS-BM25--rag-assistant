[site]: crossvalidated
[post_id]: 375011
[parent_id]: 375006
[tags]: 
You can check other questions tagged as likelihood for more details, but basically likelihood function $L$ is a probability mass function, or probability density function, $f$ evaluated on some data $X$ and parametrized by $\theta$ : $$ L(\theta | X) = \prod_i f_\theta (X_i) $$ The definition is the same in both frequentist and Bayesian settings, but with the difference that Bayesians treat $\theta$ as random variable while frequentists treat $\theta$ as an unknown parameter, where likelihood function is maximized to find the "most likely" value of it. My wild guess is that what the author means is that is you just maximize over function, then it doesn't matter if the function integrates to unity or not, so you can omit the normalizing constant from $f$ and simplify it. In Bayesian setting likelihood is a conditional probability distribution, but if you use MCMC same thing happens since the algorithms also don't care about normalizing constants.
