[site]: crossvalidated
[post_id]: 631208
[parent_id]: 
[tags]: 
Bayesian Optimization: number of iterations as function of search space dimensionality?

I am performing Bayesian Optimization to select a hyperparameter configuration for my supervised learning model. I understand that with each additional hyperparameter that I choose to optimize, the search space grows exponentially. My initial idea was to increase the number of search iterations exponentially as search space dimensionality increases, but I realized that it wouldn't offer much (if any) benefit over using a grid search, whose runtime would increase in the same way with added dimensions. Is there some rule of thumb for increasing the number of iterations as search space dimensionality increases? For what it's worth, my BO algorithm uses a Gaussian process as the surrogate model, and I'm leaning towards using a linear function, specifically $n = 20*D$ , where $D$ is the search space dimension.
