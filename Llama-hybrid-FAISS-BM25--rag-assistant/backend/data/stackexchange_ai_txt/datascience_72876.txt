[site]: datascience
[post_id]: 72876
[parent_id]: 
[tags]: 
Relationship between log-odds and weighted sums in Logistic Regression

I've read several articles/tutorials on Logistic Regression and I've come across this idea of log-odds being equal to the weighted sum of features. i.e. if $p$ is the probability of a sample belonging to positive class(target variable: 1), $(1-p)$ is the probability of it belonging to the negative(target variable: 0). Then, for input features $x_1$ , $x_2$ , $x_3$ ... $x_n$ and weights $\theta_0, \theta_1, \theta_2... \theta_n$ , they write \begin{equation} log\bigg(\frac{p}{1-p}\bigg) = x_0 \theta_0 + x_1 \theta_1 + x_2 \theta_2 \dots x_n \theta_n \end{equation} where $x_0$ is bias Now I get what features are, I get what weighted sum of feature is, I know about the odds, I know log, what I cannot comprehend is, how are those two quantities equal? I've gone through plenty of online articles but they always say that these two are equal, but they don't explain how.
