[site]: crossvalidated
[post_id]: 155580
[parent_id]: 
[tags]: 
Cost function in OLS linear regression

I'm a bit confused with a lecture on linear regression given by Andrew Ng on Coursera about machine learning. There, he gave a cost function that minimises the sum-of-squares as: $$ \frac{1}{2m} \sum _{i=1}^m \left(h_\theta(X^{(i)})-Y^{(i)}\right)^2 $$ I understand where the $\frac{1}{2}$ comes from. I think he did it so that when he performed derivative on the square term, the 2 in the square term would cancel with the half. But I don't understand where the $\frac{1}{m}$ come from. Why do we need to do $\frac{1}{m}$? In the standard linear regression, we don't have it, we simply minimise the residuals. Why do we need it here?
