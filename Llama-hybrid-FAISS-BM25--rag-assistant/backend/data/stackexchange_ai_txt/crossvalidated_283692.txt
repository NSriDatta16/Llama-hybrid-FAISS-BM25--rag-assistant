[site]: crossvalidated
[post_id]: 283692
[parent_id]: 211076
[tags]: 
Linearly solvable MDPs are a continuous relaxation of normal MDPs. The differ from normal MDPs in that they don't use a traditional notion of an "action" variable and replace "action" with a next-state distribution u(x'|x). Normally in MDPs there is an action variable that indexes a distribution over next states. In LMDPs, however, the optimal policy is not a mapping of states to action variables, it is a next-state distribution which minimizes the accumulated state costs, q(x), of the agent traversing the state-space while minimizing a divergence cost between the distributions of the optimal policy, u(x'|x), and the passive uncontrolled dynamics of the agent p(x'|x) through the state space. The linear bellman equation looks like this $v(x) = \min_{u(x'|x)} \Big[q(x) + KL(u||p) + \mathop{\mathbb{E}}_{x'\sim u}[v(x')]\Big]$ which can be solved in polynomial time as a largest eigenvector problem. It has many other nice properties. You can also do inverse control with them or embed traditional MDPs inside of them. http://www.pnas.org/content/106/28/11478.full https://homes.cs.washington.edu/~todorov/papers/TodorovNIPS06.pdf
