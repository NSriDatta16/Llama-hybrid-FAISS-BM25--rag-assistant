[site]: crossvalidated
[post_id]: 580594
[parent_id]: 
[tags]: 
Simulating the error term with an AR(2) model

I'm working on some simulations in R where I have a time series in the format $$y_t=\mu_t+e_t$$ where $\mu_t$ is a trend function that I have defined as $\mu_t=5*t+4*sin(2t)$ and where $t$ goes from $ \{ 0,4\pi \}$ with $n=100$ datapoints. Generally $e_t \sim N(0, \sigma^2)$ however in this case of these simulations I wish to replace $e_t$ with an AR(2) process. The reasoning behind this is that normally if $e_t \sim N(0, \sigma^2)$ then we assume that each $e_t$ is i.i.d however by limiting the error term to a known process the i.i.d is dropped from our assumptions, which is what I am trying to simulate here and I would like to know how I would go about changing the error term to be AR(2). I've posted a short R-code snippet that shows the function defined above with a normal $e_t$ term which would be replaced with an AR(2) process. n_sim Question: How do I simulate a vector of error values from the AR(2) process?
