[site]: crossvalidated
[post_id]: 38130
[parent_id]: 38087
[tags]: 
You're on the right track, but always have a look at the documentation of the software you're using to see what model is actually fit. Assume a situation with a categorical dependent variable $Y$ with ordered categories $1, \ldots, g, \ldots, k$ and predictors $X_{1}, \ldots, X_{j}, \ldots, X_{p}$. "In the wild", you can encounter three equivalent choices for writing the theoretical proportional-odds model with different implied parameter meanings: $\text{logit}(p(Y \leqslant g)) = \ln \frac{p(Y \leqslant g)}{p(Y > g)} = \beta_{0_g} + \beta_{1} X_{1} + \dots + \beta_{p} X_{p} \quad(g = 1, \ldots, k-1)$ $\text{logit}(p(Y \leqslant g)) = \ln \frac{p(Y \leqslant g)}{p(Y > g)} = \beta_{0_g} - (\beta_{1} X_{1} + \dots + \beta_{p} X_{p}) \quad(g = 1, \ldots, k-1)$ $\text{logit}(p(Y \geqslant g)) = \ln \frac{p(Y \geqslant g)}{p(Y (Models 1 and 2 have the restriction that in the $k-1$ separate binary logistic regressions, the $\beta_{j}$ do not vary with $g$, and $\beta_{0_1} \ldots > \beta_{0_g} > \ldots > \beta_{0_k}$) In model 1, a positive $\beta_{j}$ means that an increase in predictor $X_{j}$ is associated with increased odds for a lower category in $Y$. Model 1 is somewhat counterintuitive, therefore model 2 or 3 seem to be the preferred one in software. Here, a positive $\beta_{j}$ means that an increase in predictor $X_{j}$ is associated with increased odds for a higher category in $Y$. Models 1 and 2 lead to the same estimates for the $\beta_{0_g}$, but their estimates for the $\beta_{j}$ have opposite signs. Models 2 and 3 lead to the same estimates for the $\beta_{j}$, but their estimates for the $\beta_{0_g}$ have opposite signs. Assuming your software uses model 2 or 3, you can say "with a 1 unit increase in $X_1$, ceteris paribus, the predicted odds of observing '$Y = \text{Good}$' vs. observing '$Y = \text{Neutral OR Bad}$' change by a factor of $e^{\hat{\beta}_{1}} = 0.607$.", and likewise "with a 1 unit increase in $X_1$, ceteris paribus, the predicted odds of observing '$Y = \text{Good OR Neutral}$' vs. observing '$Y = \text{Bad}$' change by a factor of $e^{\hat{\beta}_{1}} = 0.607$." Note that in the empirical case, we only have the predicted odds, not the actual ones. Here are some additional illustrations for model 1 with $k = 4$ categories. First, the assumption of a linear model for the cumulative logits with proportional odds. Second, the implied probabilities of observing at most category $g$. The probabilities follow logistic functions with the same shape. For the category probabilities themselves, the depicted model implies the following ordered functions: P.S. To my knowledge, model 2 is used in SPSS as well as in R functions MASS::polr() and ordinal::clm() . Model 3 is used in R functions rms::lrm() and VGAM::vglm() . Unfortunately, I don't know about SAS and Stata.
