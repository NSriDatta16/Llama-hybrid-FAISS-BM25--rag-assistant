[site]: crossvalidated
[post_id]: 88568
[parent_id]: 
[tags]: 
Comparing Averages of Pearson Correlation Coefficients for Significance

How can I test if two averages of Pearson's correlation coefficients are significantly different? For instance, I have data pertaining to participants' estimates of certain percentages (e.g. how many percent of US-citizens are taller than 6 feet) in a number of different conditions. For instance, I want to know whether the average degree of correlation between participants is significantly different across conditions. I have data from 20 participants who each answer the same 10 questions in two different conditions. Within each condition it is straightforward how to estimate the average between participant correlation; just calculate a Pearson's for the answers of each pair of participants and then divide the sum of these correlations with the number of comparisons. But how do I determine whether or not these averages are significantly distinct? At first I thought that I could use a test for the difference between non-independent r's (p.281 in Howell's "Statistical Methods for Psychology"). But occasionally I get results where the equation requires me to take the square root of a negative number which I take as evidence that using the equation is inappropriate. So I'm at a loss of how to do this. EDIT: Here is some background information: I'm interested in the relation between the average validity (understood as the correlation between a participant's answers and the correct answers) of participants in a group and the validity of the mean answer for that group. The reason I use Pearson is that there is an equation (found in e.g. Hogarth 'A Note on Aggregating opinions' in Organizational Behavior and Human Performance 21, 1978) that states the validity of the mean answer in a group as a function of the average degree of correlation between group members and group members' average validity.
