[site]: crossvalidated
[post_id]: 595288
[parent_id]: 595287
[tags]: 
One point to consider is that main modern neural networks are substantially over-parameterized by "traditional" standards. And that's not necessarily a bad thing and even happens in more traditional settings e.g. with splines as illustrated in this nice discussion of the double-descent phenomenon . So, I would not assume the number of trainable parameters needs to be less than the number of training samples, although that may be true. Additionally, there's a lot of recent work on this kind of set-up (e.g. here ), so it's worth researching that.
