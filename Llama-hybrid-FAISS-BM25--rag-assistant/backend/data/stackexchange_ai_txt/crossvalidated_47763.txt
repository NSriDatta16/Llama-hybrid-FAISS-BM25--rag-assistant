[site]: crossvalidated
[post_id]: 47763
[parent_id]: 47752
[tags]: 
Since you mention sparse coding, I assume you are referring to natural images. For natural images, standardization is often carried out because natural image patches have pretty stable statistical properties once you subtracted the constant part (and whitened them; see below). You may look at it like this: A natural image patch $p$ has a mean lumination (the mean of the patch) and a contrast (the standard deviation of the patch). If you are interested in the content of the patch, then it is a good idea to subtract the mean lumination and divide out the contrast to map all image patches with the same content on the same point. Natural image patches have pretty stable statistical properties after subtracting the mean (also often called DC component). For reference you could look at papers by David Field, Bruno Olshausen, David Ruderman, Eero Simoncelli, Matthias Bethge, or Aapo Hyvaerinen. Interestingly, the statistics of the DC component varies a lot from image to image (if you sample many patches from one image), but the statistical properties of the patches are quite stable. This is true in particular for whitened patches, i.e. when you divide by the standard deviation in the PCA basis (a whitening matrix is not unique, but the PCA version is one possible choice). Note that many sparse coding models are actually trained on DC-subtracted and whitened natural image patches. In short: For natural images you like to do the standardization because the probability models fitted to the standardized patches generalize better from image to image.
