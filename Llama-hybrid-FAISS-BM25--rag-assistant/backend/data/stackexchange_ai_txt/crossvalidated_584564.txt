[site]: crossvalidated
[post_id]: 584564
[parent_id]: 
[tags]: 
If I engineer a new feature such that feature C = feature A/feature B, must I drop features A and B from a Gaussian Naive Bayes model?

As the question asks, is it bad data science not to drop the dividend and divisor features when creating a new feature that is their quotient when working with a Naive Bayes model? My understanding of Naive Bayes is that it assumes conditional independence of the variables. In my example, feature C would not be conditionally independent of features A and B if, knowing C and B, I could derive A; or if, knowing C & A, I could derive B. Is this correct?
