[site]: crossvalidated
[post_id]: 482342
[parent_id]: 
[tags]: 
Bayesian estimator not converging

I am trying to run a simple experiment using python. I have a binomial distribution (n = 100, p = 0.6). I am trying to estimate the proportion p of this binom distribution using a Beta(1, 1) as a prior. I am observing 100,000 samples (For each sample, I am running 20 Bernoulli trials, with p = 0.6 and summing up the values as k, using numpy's inbuilt function). The posterior value of p is then simply (a + k) /(a + b + 20). I expect these values to converge very closely to 0.6, but they seem to be erratic. For smaller number of trails, sometimes the values are more closer to 0.6. 10000 0.5988901109889011 20000 0.5993450327483626 30000 0.5998716709443018 40000 0.6003287417814555 50000 0.6006119877602448 60000 0.600478325361244 70000 0.6005364209082728 80000 0.6003837452031849 90000 0.6004372173642515 I am interested to know if there is any finite number of trials, which will give the closest approximation, or what is the rule to determine the optimum number of trials.
