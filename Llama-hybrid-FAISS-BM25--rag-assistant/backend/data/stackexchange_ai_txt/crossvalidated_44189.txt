[site]: crossvalidated
[post_id]: 44189
[parent_id]: 
[tags]: 
Central limit theorem with unknown variance

In my experiment I compute the average latency of operations per second. I would like to define N, i.e: how many times do I need to run my experiment to compute a close-to-real average latency? I figured that I could apply the CLT here, because If I repeat the same experiment 1000 times and plot a histogram, I get a normal distribution curve. Is the central theorem useful in my case? In the definitions I found, to be able to compute an estimated mean with a certain error, one needs to know the variance beforehand, and I don't know it.
