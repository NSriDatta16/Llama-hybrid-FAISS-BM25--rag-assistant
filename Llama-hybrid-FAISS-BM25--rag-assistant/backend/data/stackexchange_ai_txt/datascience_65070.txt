[site]: datascience
[post_id]: 65070
[parent_id]: 65067
[tags]: 
I will take as reference fairseq 's implementation of the Transformer model . With this assumption: In the transformer, masks are used for two purposes: Padding: in the multi-head attention , the padding tokens are explicitly ignored by masking them. This corresponds to parameter key_padding_mask . Self-attention causality: in the multi-head attention blocks used in the decoder, this mask is used to force predictions to only attend to the tokens at previous positions, so that the model can be used autoregressively at inference time. This corresponds to parameter attn_mask . The weight mask, which is the combination of the padding and causal masks, is used to know which positions to fill with $-\infty$ before computing the softmax, which will be zero after it. You don't need to preserve any zeros in the output, as the attention blocks take care of that (see answer (1)). In the original Transformer article, the attention works without bias, but the bias does not change performance. Actually, in fairseq the bias are used by default. Yes, padding_idx is certainly used to zero out padded tokens.
