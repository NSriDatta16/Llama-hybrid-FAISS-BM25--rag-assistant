[site]: crossvalidated
[post_id]: 151469
[parent_id]: 151467
[tags]: 
From the sklearn docs : Support Vector Machine algorithms are not scale invariant, so it is highly recommended to scale your data. For example, scale each attribute on the input vector X to [0,1] or [-1,+1], or standardize it to have mean 0 and variance 1. Note that the same scaling must be applied to the test vector to obtain meaningful results. See section Preprocessing data for more details on scaling and normalization. Decision trees though, are scale invariant, and in fact, are invariant under any monotonic transformation of the features. To see why, notice that the default kernel, the rbf kernel, contains a scale parameter $\gamma$: $$ exp( -\lambda |y - x|^2) $$ The $\lambda$ parameter controls the width of the kernel, so the scale is built in right there. You can pass a $\lambda$ into the svm as a parameter to the fit function, and by choosing it appropriately you can probably recover your initial results with all the features re-scaled. On the other hand, the decision tree is just looking for optimal splits between data points, and the concept of a split is not dependent on scale, only ordering.
