[site]: crossvalidated
[post_id]: 546929
[parent_id]: 
[tags]: 
What should unseen data comply mathematically for reliable measure of generalisation (gap)?

The way machine learning models validated for their generalisation performance, generalisation gap is used, see Predicting the Generalization Gap in Deep Neural Networks . This is essentially standard holdout method. Random splitting is a standard practice. We take test set which was not used in model building granted as unseen data . What should unseen data comply mathematically for reliable measure of generalisation (gap)? (at least from statistical learning theory perspective). While, unseen data is open to interpretation: if it means outside the domain of training data ("completely out-of-sample"), then random splitting is not sufficient or reliable way of measuring generalisation, mathematical setting or tests that should be applied to unseen data would clarify this possible ambiguity.
