[site]: crossvalidated
[post_id]: 211492
[parent_id]: 
[tags]: 
How exactly does pretraining benifit us in deep learning methods?

I know we use pretraining in order to get better performance. but this is a high level explanation. I want to know what exactly happens when we use pre training. does it help the network not to get stuck in local minimas? If its so why is it said (e.g in Stanford's convnet videos here ) that Local minimas are not of that importance that they are in the normal and conventional neural networks, and as it is said be Justin Johnson in their video lectures ?
