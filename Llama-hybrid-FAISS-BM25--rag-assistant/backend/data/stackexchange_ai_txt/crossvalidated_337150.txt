[site]: crossvalidated
[post_id]: 337150
[parent_id]: 337113
[tags]: 
Yes -- in general Orthogonal Matching Pursuit (or any other local-minimum finding algorithm for the $\ell_0$-problem) will not give the same solution as the $\ell_1$-problem. That said, we know a very special set of conditions under which two closely related problems have the same solution. If the so-called restricted nullspace property (RNP) applies, then the following two problems have the same solution: $$\text{arg min}_{\beta} \|\beta\|_0 \text{ such that } y = X\beta$$ and $$\text{arg min}_{\beta} \|\beta\|_1 \text{ such that } y = X\beta$$ Note that I'm talking about the global solution to these problems being equal. It's easy to get the global solution for the $\ell_1$ problem, but harder to get the global solution for the $\ell_0$ since it's nonconvex. If you want to guarantee that a particular algorithm applied to the $\ell_0$-problem gets the same answer, you typically need stronger conditions. See, e.g., [2] for the case of OMP. RNP essentially says that there are no vectors in the nullspace of $X$ which decrease the $\ell_1$ norm of the true solution. More precisely, there are no vectors in the nullspace of $X$ which have a lower $\ell_1$-norm off the true support than $\ell_1$-norm on the true support than the "truth." Hence, of the set of vectors which satisfy the constraint, the one with the minimal $\ell_1$ norm is the "truth" and hence is also the solution to the $\ell_0$-problem. You might ask why we care about the nullspace: it's because if we have any $\hat{\beta}$ which satisfies $y = X\hat{\beta}$ ( i.e. , is feasible), then any $\hat{\beta} + \beta_{\mathcal{N}(X)}$ (where $\beta_{\mathcal{N}(X)}$ is a vector in the nullspace of $X$) is also feasible (because $X\beta_{\mathcal{N}(X)} = 0$ by definition). Hence, if we assume the true solution is feasible (which it always is in the noiseless model), we only need to show that it minimizes the objective, which is exactly what RNP give us. Not that this result only applies in the $y = X\beta$ (noiseless)case. If we only require $y \approx X \beta$, the $\ell_1$ and $\ell_0$ cases don't generally coincide. If your $X$ satisfies the RNP but there is noise in your $y$, I'm pretty sure the two solutions ($\ell_1$ and $\ell_0$) will still coincide, but they are no longer guaranteed to be correct. If there is noise, however, you wouldn't typically enforce $y = X\beta$ because that guarantees overfitting. See Section 10.4 of [1] for details. A few minor points from your question: soft thresholding is not (in general) the solution to the lasso problem (It is when you have orthogonal design $X$); it is associated with the lasso because it is a key component in some (not all) algorithms to solve the lasso problem Tikhonov regularization typically only refers to $\ell_2$, not $\ell_1$ [1] Hastie, Tibshirani, and Wainwright. Statistical Learning with Sparsity 2015. CRC Press. Available at https://web.stanford.edu/~hastie/StatLearnSparsity_files/SLS_corrected_1.4.16.pdf [2] Tropp and Gilbert. "Signal Recovery From Random Measurements Via Orthogonal Matching Pursuit" IEEE Transactions on Information Theory 53(12), 2007. DOI: 10.1109/TIT.2007.909108
