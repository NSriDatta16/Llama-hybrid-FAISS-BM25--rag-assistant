[site]: crossvalidated
[post_id]: 267371
[parent_id]: 267345
[tags]: 
Ridge regression encourages all coefficients to becomes small. Lasso encourages many/most[**] coefficients to become zero, and a few non-zero. Both of them will reduce the accuracy on the training set, but improve prediction in some way: ridge regression attempts to improve generalization to the testing set, by reducing overfit lasso will reduce the number of non-zero coefficients, even if this penalizes performance on both training and test sets You can get different choices of coefficients if your data is highly correlated. So, you might have 5 features that are correlated: by assigning small but non-zero coefficients to all of these features, ridge regression can achieve low loss on training set, which might plausibly generalize to testing set lasso might choose only one single one of these, that correlates well with the other four. and there's no reason why it should pick the feature with highest coefficient in the ridge regression version [*] for a definition of 'choose' meaning: assigns a non-zero coefficient, which is still a bit hand-waving, since ridge regression coefficients will tend to all be non-zero, but eg some might be like 1e-8, and others might be eg 0.01 [**] nuance: as Richard Hardy points out, for some use-cases, a value of $\lambda$ can be chosen which will result in all LASSO coefficients being non-zero, but with some shrinkage
