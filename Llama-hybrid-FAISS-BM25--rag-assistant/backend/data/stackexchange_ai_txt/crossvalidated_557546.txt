[site]: crossvalidated
[post_id]: 557546
[parent_id]: 550021
[tags]: 
As mentioned in the comments already (+1) a priori no type of learning algorithm is guaranteed to always outperform another type of learners. This is related to the No free lunch theorem ; the Wikipedia article on No free lunch in search and optimization substantiates this saying when it comes to Machine Learning applications. Regarding the actual question of " why rf would perform better than gbm "? Because likely the GBM is unable to properly regularise its response function so it fits on noise. The RF is "harder" (not impossible) to overfit compared to a GBM so it may generalised better all other things being equal. Indeed usually GBMs are as competitive as RFs in terms of performance but that is not a guarantee. Also notice that nowadays certain GBM implementations use a number of computational tricks that do not always guaranteed better predictive performance in terms of metrics (e.g. LightGBM's Exclusive Feature Bundling or CatBoost's Symmetric Trees are super helpful for speed-up of training but are not guaranteed to outperform the "standard" techniques they are supposed to replace). Finally, I would argue that neither the RF or the GBM appear to very well-tuned. I don't know the particulars of this application but all metrics are at least 33% worse on the test set. This doesn't suggest a well-tuned algorithm; some performance decrease might be inevitable by not at that level. I would suggest revisiting your validation schema (e.g. if simple CV is used, replace it with repeated CV with more repeats; ensure that there is no data leakage and the appropriate unit of analysis is stratified, etc.).
