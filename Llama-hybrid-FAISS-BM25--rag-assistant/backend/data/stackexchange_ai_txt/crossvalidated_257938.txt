[site]: crossvalidated
[post_id]: 257938
[parent_id]: 257328
[tags]: 
This is one of those things that has been observed for a while but not necessarily theoretically explained. In one of the original random forest papers, Breiman hypothesized that adaboost functions as a kind of random forest in its latter stage as the weights are essentially drawn from a random distribution. His full supposition hasn't been proven but gives reasonable intuition. In modern gradient boosting machines etc it is common to use the learning rate and sub-sampeling of the data features to make the tree growth explicitly randomized. Its also notable that their are relatively few hyper-paramaters to tune and they function pretty directly to combat overfitting. So while it is possible to overfit with a boosted model its also easy to dial back the tree depth, leaf size, learning rate etc and/or add in randomization to combat this.
