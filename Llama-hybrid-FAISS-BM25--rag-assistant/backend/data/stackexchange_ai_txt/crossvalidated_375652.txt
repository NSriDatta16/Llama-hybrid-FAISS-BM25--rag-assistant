[site]: crossvalidated
[post_id]: 375652
[parent_id]: 
[tags]: 
PCA's eigenvector with low variance, why people think they are 'noise'?

When we do a textbook PCA decomposition, get a series of eigenvalue $\lambda$ and eigenvector $v$ that fulfill: $ Av= \lambda v $ we can sort these eigenvalues (together with the corresponding eigen vectors), so that the eigenvector corresponding to the biggest eigenvalue will be the basis that capture the biggest variance of A's distribution. Most often we focus on those big eigenvalues, which explain the variance of distribution A However, I am curious what those eigenvectors corresponding to small eigen values imply? What I understand is that, these eigenvectors correspond to the basis of $A$ where variance is smaller. Therefore, given a new set of samples B, the projection of $Bv_{small}$ will have very small variance. Why texbook says $Bv_{small}$ are noises? I think $Bv_{small}$ represents the 'stationary' part of the distribution of $B$ , isn't it? Can anyone clarify this please? thanks
