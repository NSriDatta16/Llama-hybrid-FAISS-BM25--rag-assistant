[site]: crossvalidated
[post_id]: 182746
[parent_id]: 
[tags]: 
Maximizing likelihood vs. minimizing cost

I keep coming across two different kinds of optimization: Cases where you maximize the likelihood of the data directly (for example CRF learning, or EM). Cases where you minimize some cost function (for example, fitting least squares) I also have noticed that people use gradient methods for solving each of these two kinds of problems. For maximization, the gradient update rule looks like this. The intuition is you want to maximize so you climb the hill of the curvature in the direction of the gradient. $$\lambda_{i+1} = \lambda_i + \frac{\partial f(x)}{\partial \lambda_i}$$ For minimization, you want to minimize the cost function, so you subtract the gradient to roll down the hill of the curvature. $$\lambda_{i+1} = \lambda_i - \frac{\partial f(x)}{\partial \lambda_i}$$ It also seems like some optimization packages ask you to flip the sign of a maximization problem to get a minimization problem instead. Example: Note that since minimize only minimizes functions, the sign parameter is introduced to multiply the objective function (and its derivative) by -1 in order to perform a maximization. Is minimization more canonical? Am I getting this right? That is, is my description of the machine learning landscape correct? How do I know when I should minimize a cost function or maximize a likelihood? (Or a log likelihood). My first thought was that maximizing log likelihood is for unsupervised learning (where you can't generate a cost function, because there are no labels) -- but CRF learning maximizes log likelihood directly too.
