[site]: crossvalidated
[post_id]: 402893
[parent_id]: 402440
[tags]: 
Rasch vs. non-Rasch Some of the models you pointed to have one model-wide discrimination parameter. I believe this makes them Rasch models. The rating scale model (RSM) and partial credit model (PCM) are Rasch models. The graded response model (GRM) and the generalized partial credit model are non-Rasch. So, part of your question is asking about Rasch vs non-Rasch. I am self-taught in IRT. I have not used Rasch models, nor do I prefer them over non-Rasch models (i.e. models that estimate one discrimination parameter for each item). My understanding is that some people may prefer Rasch models over others. Because I don't know Rasch models that well, I call on others to point out if I make a misstatement. I believe Rasch models should have a lower standard error than non-Rasch models, because they estimate fewer parameters. Also, some specialized statistics to detect mis-fitting persons are defined for Rasch models but not for non-Rasch ones (i.e. infit and outfit statistics; persons who consistently answer questions that are too hard for their ability can be said to misfit). However, as I understand things, you will need to select items that have pretty similar discrimination parameters in order to use a Rasch models (else the items that have much higher or much lower discrimination than the others will get poor item fit statistics). My impression (which could be wrong) is that Rasch practitioners would probably drop those items and choose better-fitting ones. In contrast, you may be in a situation where you can't easily generate more items, or you are applying an IRT model to a widely used test and you have no option to change the items in the test. For example, the Patient Health Questionnaire is a 9-item depression screening questionnaire. It basically enumerates the criteria in the then-current DSM-III (they haven't substantially changed since the DSM-IV and -V were published, so the PHQ-9 hasn't been changed either). It is in very widespread use. Forkmann et al ran a Rasch model on it, and found insufficient fit for 3 of the items (plus they recommended dropping another item entirely). If I could make the healthcare system switch to the PROMIS depression item bank, I would probably do so, but nobody is going to listen to me. From my perspective, then, why would I fit a Rasch model to the PHQ-9 when it clearly doesn't fit well? Miscellaneous model features Some of the models you identified are more restricted versions of the others. As described by Stata's IRT manual , In the [Rating Scale Model (RSM)], items vary in their difficulty but share the same discrimination parameter. The distances between the difficulties of adjacent outcomes are equal across the items. For example, in the Stata example for the RSM (page 125), you'll note that the distance between the difficulty parameters for 1vs0 and 2vs1 are always about 2.04 for each item. The distance between 2vs1 and 3vs2 are always about 1.12. So, the RSM is a more restrictive model than the partial credit model. I believe RSM is nested within the PCM. I imagine you would have to inspect your model results from the partial credit model, and then run a likelihood ratio test between the two models. I have not seen the rating scale model used anywhere I've searched in health services research literature, but I may simply not have searched widely enough. You referred to the linear rating scale model (LRSM). I am not familiar with this model at all. It is referenced in the manual for the R package eRM , which deals with extended IRT models. They describe the LRSM as even more restricted than the RSM, and similarly the linear logistic test model is like the Rasch model but more restrictive. I would read up on this myself if it matters that much to you. I wasn't able to easily comprehend the issue. Ordinal logistic-type vs multinomial-type models The last distinction I can see in your list is this. The graded response model is based on ordinal logistic regression. Below, let $i$ index items, $j$ index persons, and $c$ index the response categories available to each item. $discrim$ and $diff$ refer to each item's discrimination and difficulty parameters. $P(Y_{ij}≥c| θ_j)=logit^{-1}[discrim_i(θ_j-diff_{ic})]$ To calculate the probability of responding in a category, we just use the law of total probability, e.g. $P(Y_{ij} = c | \theta_j) = P(Y_{ij}≥c| θ_j) - P(Y_{ij}≥(c-1)| θ_j)$ In contrast, the rating scale and partial credit models are based on multinomial regression. For the generalized partial credit model: $P(Y_{ij} = c | \theta_j) = \frac{exp[\sum^C_{t=1}discrim_i(\theta_j-diff_{it}]}{1 + \sum^K_{s=1}exp[\sum^s_{t=1}discrim_i(\theta_j-diff_{it}]}$ (For the Rasch PCM, you just have one model-wide discrimination parameter). I've heard the partial credit model described as a "divide by total" sort of model. So, how to select between the ordered logistic-style GRM and the multinomial-style PCM? I have no clue, and my Google search revealed little. I found some discussion of that issue in one piece of grey literature , but I don't really grasp the technical issues that well. If anyone has better insight, that would be nice.
