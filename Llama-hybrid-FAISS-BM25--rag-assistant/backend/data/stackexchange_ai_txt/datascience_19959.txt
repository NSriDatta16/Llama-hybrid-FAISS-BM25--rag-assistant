[site]: datascience
[post_id]: 19959
[parent_id]: 18820
[tags]: 
It looks like you might be trying to use MultiRNNCell as a recurrent neural network. This isn't really its purpose; that is what dynamic_rnn, static_rnn, and raw_rnn are for. MultiRNNCell is intended for creating a feedforward, multilayer network which is the "cell" to be iterated to produce a recurrent neural network. The old version of your code, if I understand it correctly, creates a network consisting of a sequence of LSTM layers, each with its own set of weights. If your goal is to make the weights shared between layers (so that your network is recurrent), you want to instantiate a dynamic_rnn and pass it a single instance of one of the RNNCell subclasses. That instance generates all of the layers of the network by having its __call__ method invoked repeatedly, once for each layer. The dynamic_rnn instance handles the scoping and variable reuse issues for you and connects the layers together. You have at least three options for the cell to pass to the dynamic_rnn constructor: 1) A DropoutWrapper which was created using a BasicLSTMCell (essentially, the output of one call to attn_cell , but get rid of that reuse option). Do this option. 2) A MultiRNNCell consisting of an LSTM layer, followed by a dropout layer (unneeded here, but this is what you would do in some more general case). 3) Subclass rnn_cell and create your own type of layer.
