[site]: datascience
[post_id]: 42587
[parent_id]: 
[tags]: 
How to break down large SVM classification model?

I have a classification problem with large number of classes: feature set is 512 Dimension, number of classes are around 3000. This is a face identification problem. (identify among 3000 celebrities, whose face it is. The feature is extracted using FaceNet.) The issue for training such a SVM model is too slow: I used sklearn SVC, it came with below result: RAM usage: > 100GB // I eventually ended up using virtual memory of 100GB Training time: > 30 hours Classification time: > 1 hour per face Other issue: Single CPU usage, no parallelization To summarize, it is not practical to use above naive way of training for such a SVM model. My Question: What is a practical way of optimizing SVM training / usage of this scale input data? (~ 3000 classes, feature_size = 512)
