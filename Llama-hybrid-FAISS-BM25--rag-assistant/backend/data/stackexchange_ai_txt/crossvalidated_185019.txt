[site]: crossvalidated
[post_id]: 185019
[parent_id]: 184589
[tags]: 
What you describe would be one approach. For classification, TreeBagger by default randomly selects sqrt(p) predictors for each decision split (setting recommended by Breiman). Depending on your data and tree depth, some of your 50 predictors could be considered fewer times than others for splits just because they get unlucky. This is why for estimation of predictor importance I usually set 'nvartosample' to 'all'. This gives a model with somewhat lower accuracy ensures that every predictor is sensibly included. If you run TreeBagger at the default settings, this is generally not a problem. For example, if you have two strongly correlated features and one of them is included in the 7 predictors selected at random for a split, the other is likely not included in these 7. If you want to adopt my scheme by inspecting all predictors for each split, use surrogate splits by setting 'surrogate' to 'all'. Training will take longer but you will have full information about predictor importance, irrespective of other predictors, associations among predictors. The TreeBagger doc and help have this statement at the bottom: "In addition to the optional arguments above, this method accepts all optional fitctree and fitrtree arguments with the exception of 'minparent'. Refer to the documentation for fitctree and fitrtree for more detail." Look at the doc for fitctree and fitrtree. fitensemble for the 'Bag' method implements Breiman's random forest with the same default settings as in TreeBagger. You can change the number of features to sample to whatever you like; just read the doc for templateTree. The object returned by fitensemble has a predictorImportance method which shows cumulative gains due to splits on each predictor. The TreeBagger's equivalent of that the DeltaCriterionDecisionSplit property (or something like that). In addition, TreeBagger has 3 OOBPermuted properties that are alternative measures of predictor importance.
