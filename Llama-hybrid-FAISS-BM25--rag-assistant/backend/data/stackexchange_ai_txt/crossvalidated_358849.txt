[site]: crossvalidated
[post_id]: 358849
[parent_id]: 
[tags]: 
Conditions on stationary distribution for continuous cases

Above is from my Bayesian notes, I have questions as: I know for discrete case, the stationary distribution $p(\theta)$ is defined as $$p(\theta) = A p(\theta)$$ where $A$ is the Markov Chain. But what is the definition for stationary distribution $p(\theta)$ in continuous cases? In the definition of $A(\theta, S)$, what does it actually mean? I thought (might be not correct) since $\theta \in S$, so $p(S|\theta)$ is just the probability that moves from any $\theta$ in $S$ to any elements in $S$. Thus, $p(S|\theta)$ contains the situation that maps $\theta$ to itself. If this is the case, why we need not moving term $r(\theta)I(\theta \in S)$? Or might be I misunderstood it... In the proof part, for the last step, why the second term just canceled? Might be again as the second question I did not understand the $r(\theta)$. The last line, "The above auto-regressive process converges to $N(0,1.33)$". Does this $N(0,1.33)$ just pop out from nowhere? Or might be I just do not know why we get it...
