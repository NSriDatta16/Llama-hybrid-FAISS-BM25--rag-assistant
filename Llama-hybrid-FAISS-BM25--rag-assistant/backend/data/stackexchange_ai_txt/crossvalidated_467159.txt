[site]: crossvalidated
[post_id]: 467159
[parent_id]: 467156
[tags]: 
Similar question was asked before. In case of constraints like this, there is an easy solution: just transform the outputs to fit the range. In neural networks, we use activation functions on the output layer to achieve this. For example, if you needed map the outputs to $[0, 1]$ , you would use a sigmoid activation function. Here, you can just use sigmoid(h) * 10 , or something else, as activation function on the final layer. Technically, this is the same as dividing the predicted values by 10 and using standard sigmoid activation, as you proposed. If you used unconstrained regression and "some kind" of regularization, this would still not guarantee the outputs to fit the constraint, since regression would still be able to make predictions outside the bounds, so this is not the way to go.
