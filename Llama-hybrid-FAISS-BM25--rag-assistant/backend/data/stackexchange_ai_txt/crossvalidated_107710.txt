[site]: crossvalidated
[post_id]: 107710
[parent_id]: 
[tags]: 
Down-sampling with building models (specifically random forests)

I was wondering if anyone had ever used down-sampling to build random forests with data that has unbalanced classes. Basically down-sampling samples (with replacement) x*min from the population where x is the number of classes and min is the size of the smallest class. So it is sampling min number of observations from each class. In some examples I have seen, it improves predictive power, but it seems like there is some sampling bias. This might be nullified by the fact that it is bootstrapping and building multiple trees and averaging them, but I dont fully understand it. If someone can help explain why this is not biased that would be great. I have heard that this technique is comparable with weighted random forests. I am working with a data set with 30,000,000 entries where one class has about 120 observations, so you can see why this would be helpful.
