[site]: datascience
[post_id]: 45984
[parent_id]: 45970
[tags]: 
There is a number of different ways to view variable importance for random forests. A great explanation of them Variable importance measures The most basic way is to look at how many times the variable was included in the trees in the random forest. This has the problem that it does not include how much of an effect the variable has on each tree An improvement on this is to calculate the decrease in impurity that the variable cause in a tree when adding that variable to a tree. This is the default measure for the importance() function in the RandomForest R library. This has the problem of being biased towards continuous variables and ones with high carnality. Another approach is the permutation importance. How this works is that it shuffles the variables in one column of the data and computes the decrease in accuracy. in R this is importance(type = 1) . This is quite a good measure however it needs the features to be normalized to get the best accuracy ( Shown in this paper ). Its a little slower but still quite fast. The last approach is the drop column approach. This works by dropping a feature then calculating how much the accuracy decreases from training the random forest on all the features. This is the slowest but most accurate. Interpreting the results These methods will all give a list of the feature with a measure of how good it is. This will give you an idea of how the performance of the random forest will change because of that feature. Ie you might find Age is the most important feature for prediction viewing time. You can then look at a graph of age vs view time to see how those two variable interact. With a drop column approach there is the possibility of finding that removing a feature will increase accuracy by the decrease in accuracy being an negative value. For applying to the domain of the problem that is up to you and the real world problem that you are trying to solve with this model. Hope this helps :)
