[site]: crossvalidated
[post_id]: 473408
[parent_id]: 473395
[tags]: 
First problem is that fitting the model once and comparing the metrics is not the best approach as discussed in the answer by EdM to the How to find a statistical significance difference of classification results? question (see the whole answer, as it seems to answer your question to great extent): Second, instead of simply fitting the model one time to your data, you need to see how well the modeling process works in repeated application to your data set. One way to proceed would be to work with multiple bootstrap samples, say a few hundred to a thousand, of the data. For each bootstrap sample as a training set, build KNN models with each of your distance metrics, then evaluate their performances on the entire original data set as the test set. The distribution of Brier scores for each type of model over the few hundred to a thousand bootstraps could then indicate significant differences, among the models based on different distance metrics, in terms of that proper scoring rule. With simple models, we can make some assumptions and derive the errors, for more complicated cases, as noted in the answer we can use procedures like bootstrap. Now, with deep learning models using bootstrap is problematic, because they need great computational power and time to train, where cost of the biggest models in this field is comparable to cost of a car (per single training). This is one of the reasons why there is ongoing research on models that are aware of their uncertainties, e.g. Bayesian neural networks, and many research projects look into approximating it, e.g. with using dropout in prediction phase (see blog post by Yarin Gal , but see also the critique by Ian Osband ). All those approaches are based on approximations and have their pitfalls. So the answer to your question would be that it's not that simple to get meaningful estimates for the uncertainties.
