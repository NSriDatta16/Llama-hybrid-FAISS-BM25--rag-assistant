[site]: crossvalidated
[post_id]: 210815
[parent_id]: 
[tags]: 
Understanding the E step of EM for GMM

I'm reading this chapter about EM (9.3.1) of the book "Pattern Recognition and Machine Learning". I understand the basic EM algorithm for GMM, but I'm having some problems understanding the probabilistic interpretation of the E step. Most of the following formulas make sense to me except 9.39. Can someone please explain it for me? Thanks a lot. It seems that it is in the form of $$E[z_{nk}]=\frac{\sum_{z_{nk}}z_{nk}f(n,k)}{normalizer}$$ where the numerator is a summation over two possible states of $z_{nk}$ (0 and 1), so it becomes $\pi_kN(x_n|\mu_k,\Sigma_k)$ in the second step, am I right so far? If so, what are we summing over here in the denominator? 0s and 1s of all possible $j$s (but the result doesn't quite match)? Why $f(n,k)=[\pi_kN(x_n|\mu_k,\Sigma_k)]^{z_{nk}}$ is used as an unnormalized probability? It would make more sense if the denominator is the unnormalized zeroth moment and the numerator is the unnormalized first moment, did I understand it correctly?
