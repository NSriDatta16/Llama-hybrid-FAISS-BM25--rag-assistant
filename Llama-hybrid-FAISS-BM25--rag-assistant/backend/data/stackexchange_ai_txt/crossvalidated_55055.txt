[site]: crossvalidated
[post_id]: 55055
[parent_id]: 55034
[tags]: 
Disclaimer: I'm usually wrong at things. Decision trees, by virtue of doing recursive splitting of your samples, with splits being based on a single variable, can only generate decision boundaries parallel to the axes of your co-ordinate system. So by rotating the data to directions of maximum variance/diagonalizing your covariance matrix as best you can, it might be easier to put decision boundaries between your class distributions That being said, I'm not sure why you'd do PCA (without discarding some of your eigenvectors) before using a neural network model or whatever, because the rotation alone makes no difference - the network can approximate any function through the feature space.
