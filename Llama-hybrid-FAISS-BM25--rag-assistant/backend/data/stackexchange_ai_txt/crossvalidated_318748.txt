[site]: crossvalidated
[post_id]: 318748
[parent_id]: 
[tags]: 
Deriving the KL divergence loss for VAEs

In a VAE, the encoder learns to output two vectors: $$\mathbf{\mu} \in\ \mathbb{R}^{z}$$ $$\mathbf{\sigma} \in\ \mathbb{R}^{z}$$ which are the mean and variances for the latent vector $\mathbf{z}$ , the latent vector $\mathbf{z}$ is then calculated by: $$\mathbf{z} = \mu + \sigma \epsilon$$ where: $\epsilon = N(0, \mathbf{I}_{z \times z})$ The KL divergence loss for a VAE for a single sample is defined as (referenced from this implementation and this explanation ): $$\frac{1}{2} \left[ \left(\sum_{i=1}^{z}\mu_{i}^{2} + \sum_{i=1}^{z}\sigma_{i}^{2} \right) - \sum_{i=1}^{z} \left(log(\sigma_{i}^{2}) + 1 \right) \right]$$ Though, I'm not sure how they got their results, would anyone care to explain or point me to the right resources?
