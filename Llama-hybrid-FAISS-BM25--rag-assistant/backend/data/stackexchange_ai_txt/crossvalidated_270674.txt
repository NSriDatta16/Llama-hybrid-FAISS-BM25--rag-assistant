[site]: crossvalidated
[post_id]: 270674
[parent_id]: 270482
[tags]: 
It sounds like you may not be generating the "probabilities" (aka "level-one" data) correctly. These predicted values should be cross-validated predicted values from the base learners (or sometimes people use a separate hold-out set to generate these predicted values). My guess is that you are using predictions generated exclusively from the training set, which is leading to overfitting. Here are some references which explain the construction of the level-one dataset in more detail: Scalable Ensemble Learning in H2O (Strata San Jose 2016) Slides useR! Machine Learning Tutorial: Stacking h2oEnsemble Stacking Tutorial "Scalable Ensemble Learning and Computationally Efficient Variance Estimation" (PhD Thesis, see chapter 2) Soon, we will release H2O with XGBoost support , so you should be able to ensemble XGBoost models much more easily using the Stacked Ensemble method in H2O. Or you could use H2O models for the time-being and skip the manual construction of the ensemble.
