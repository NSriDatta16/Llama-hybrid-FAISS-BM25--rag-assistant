[site]: stackoverflow
[post_id]: 2875846
[parent_id]: 2874570
[tags]: 
If possible, I suggest reorganizing the data. For example, put all those doubles into one file instead of spreading them across multiple files. If you need to run the program multiple times and the data doesn't change, you may want to create a tool that will optimize the data first. The performance issue with the files is the overhead of: {overhead}Ramping up the hard drive. {overhead}Locating the file. Positioning within the file. Reading data. {Closing a file adds very little to the performance.} In most file based systems, that use a lot of data, reading data is optimized to have a longer duration than any overhead. The requests would be cached and sorted for optimal disk access. Unfortunately, in your case, you are not reading enough data so that the overhead is now longer duration than the reading. I suggest trying to queue the reading operation of the data. Take 4 threads, each opens a file and reads the doubles, then places them into a buffer. The idea here is stagger the operations. Thread 1 opens a file. Thread 2 opens a file while thread 1 is positioning. Thread 3 opens a file while thread 2 is positioning and thread 1 is reading the data. Thread 4 opens a file, thread 3 positions, thread 2 reads, thread 1 closes. Hopefully, these threads can keep the hard drive busy enough to not slow down; continuous activity. You may be able to try this in a single thread first. If you need better performance, you may want to consider sending commands directly to disk drive (order them first).
