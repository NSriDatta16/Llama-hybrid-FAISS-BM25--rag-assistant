[site]: crossvalidated
[post_id]: 585056
[parent_id]: 468685
[tags]: 
Old post but replying for the benefit of progeny. So I believe OP's confusion arises from how positive-valued measures can be recovered from derived negative eigen values. I believe what OP is overlooking is the step where the values are mean normalized before applying SVD. "Going on, the next step is to define PCA. Its arguments are X which is vectors of images and n_pc which is the number of principal components. As a little reminder, principal components define an orthonormal basis that can extract the maximum variance in the original data. In our case, its shape is (1000, 4096) since we needed to transform the images into vectors for PCA. Then, we find the mean and subtract it from our data to center it around the origin. After that, we need to perform Singular Value Decomposition on the centered data to find those principle components called eigenfaces. " Basically, what was done to support the achieved compression (in non-technical terms) was to compute the "average face" and then subtract that average face from every individual face before computing the eigen-values/vectors via SVD. As such, these eigen-value/vectors represent the difference details from the "average face". I suspect that doing something similar to your time series example will allow you to reconstruct the series in a positive-definite manner (that is, take the "average series" and add the derived eigen values to compute the resultant values in higher-dimensional space). NOTE: I say "add" but really you are reconstructing the linear combination of the average representation with the derived eigen coefficients.
