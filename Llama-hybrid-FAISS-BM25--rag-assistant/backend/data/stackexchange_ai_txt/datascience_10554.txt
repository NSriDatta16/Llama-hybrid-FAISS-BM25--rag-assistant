[site]: datascience
[post_id]: 10554
[parent_id]: 989
[tags]: 
With such a huge dataset I think you'd be better off using a neural network, deep learning, random forest (they are surprisingly good), etc. As mentioned in earlier replies, the time taken is proportional to the third power of the number of training samples. Even the prediction time is polynomial in terms of number of test vectors. If you really must use SVM then I'd recommend using GPU speed up or reducing the training dataset size. Try with a sample (10,000 rows maybe) of the data first to see whether it's not an issue with the data format or distribution. As mentioned in other replies, linear kernels are faster.
