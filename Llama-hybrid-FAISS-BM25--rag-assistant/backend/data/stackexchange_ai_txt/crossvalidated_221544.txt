[site]: crossvalidated
[post_id]: 221544
[parent_id]: 221530
[tags]: 
Rank the 90 individuals by logit score from highest to lowest to obtain highest predicted probability of conviction. Examining the coefficients of the logistic regression will inform you which behaviors and demographics are most associated with conviction. No - if you're building a purely predictive model (vs. an explanatory model), then avoid self-selecting predictor variables based on VIF scores. Instead, use one of several regularization techniques such as the lasso, ridge regression, or elastic net to select variables. Avoid stepwise selection, since it often produces unreliable models based on biased tests at each step. See (2). See (1) - individuals can be ranked by their logit scores. Training, validation, and test (or holdout) datasets are frequently used in predictive modeling to compare different modeling methodologies (for example, logistic regression vs. decision trees). Models are built or "trained" using training data and then prediction errors for the models are computed on the validation data. Holdout data is sometimes used to assess generalization error after final models, since even with training/validation some degree of over fitting can occur when multiple models are calibrated and compared. For small datasets, I think 5 or 10-fold cross validation will serve you better. The ROC is a means of assessing logistic regression and binary classifiers by plotting true positive rate against false positive rate. Another is the area under the ROC curve, or c-statistic. In your case the ROC & c-statistics from competing models could be plotted with the validation data. Yes - one approach for modeling sparse binary data is to use one of the aforementioned regularized logistic regression techniques.
