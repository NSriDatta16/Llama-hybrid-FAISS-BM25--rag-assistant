[site]: crossvalidated
[post_id]: 596273
[parent_id]: 596260
[tags]: 
You are comparing two different regression models: logistic regression and binomial regression. In logistic regression the outcome is a binary random variable (each data point corresponds to one trial, whose outcome is either a success or a failure) and the likelihood function is: $$ \begin{aligned} \prod_{i=1}^np_i^{y_i}\left(1-p_i\right)^{1-y_i} \end{aligned} $$ In binomial regression the outcome is a binomial random variable (each data point corresponds to a pre-determined number of trials under settings $x$ ) and the likelihood function is: $$ \begin{aligned} \prod_{i=1}^n\binom{n_i}{x_i}p_i^{y_i}\left(1-p_i\right)^{n_i-y_i} \end{aligned} $$ where $p_i=\operatorname{logit}^{-1}\left(x_i\beta\right)$ . The mathematical expressions correspond exactly to your R formulas. If we group the observations according to the predictors $x$ , then the logistic and binomial regressions are equivalent: we get the same inference for the parameters $\beta$ , the same fitted values and residuals, etc. However, as you point out, the likelihood functions are equivalent up to a (known) proportionality constant which is a function of the data. x # A tibble: 2 × 5 #> term estimate std.error statistic p.value #> #> 1 (Intercept) 0.860 0.958 0.897 0.370 #> 2 x -0.956 0.690 -1.39 0.165 logLik(m1) #> 'log Lik.' -9.8958 (df=2) # Group the observations according to the predictors. X # A tibble: 2 × 5 #> term estimate std.error statistic p.value #> #> 1 (Intercept) 0.860 0.958 0.897 0.370 #> 2 X -0.956 0.690 -1.39 0.165 logLik(m2) #> 'log Lik.' -3.162398 (df=2)
