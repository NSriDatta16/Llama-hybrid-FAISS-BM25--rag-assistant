[site]: datascience
[post_id]: 69097
[parent_id]: 69036
[tags]: 
The problem you are describing is feature selection. Welcome to a particularly sticky problem. Your method of Pearson correlation certainly can help - simply drop those features with low and or insignificant correlation to your dependant variable. This is univariate selection . Since you are using a logistic regression, you can also inspect the model coefficients and odds ratios to determine the effect size and significance of each feature. But what if some combination of your independent variables produces a significant effect within your model? Say the combination of: gender, age, income, and house value for a binary response related to likelihood to purchase. This is multivariate feature selection. For logistic regression, and other models, you can use "interaction terms" to investigate the combined effect of input variables. In Data Science and Machine Learning literature this would fall under the practice of feature engineering. There are a number of other procedural and rule of thumb methods to select inputs for multivariate models. For instance, one can use a decision tree method to evaluate important features across repeat random samples of the data, or one can simply use domain and conceptual knowledge of the process being modeled. In most cases you should use a variety of techniques and choose the inputs that make both mathematical and theoretical sense.
