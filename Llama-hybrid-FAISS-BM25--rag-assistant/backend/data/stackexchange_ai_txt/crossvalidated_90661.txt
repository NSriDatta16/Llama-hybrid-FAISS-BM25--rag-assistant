[site]: crossvalidated
[post_id]: 90661
[parent_id]: 
[tags]: 
Can one use eigenvalues to choose a number of components to retain in kernel PCA?

When using Kernel PCA for dimensionality reduction, is there any simple criterion which can be used to determine the number of components to use? I am using Kernel PCA with linear kernel, which would be equivalent to normal PCA, but I am using kPCA because my data is extremely sparse but high dimensional, and the number of instances is relatively small compared to the number of dimension. So, centering the data, which PCA requires for computing co-variance, would destroy the sparsity pattern and make computation more difficult. If I were using PCA, I could plot the eigenvalues of the co-variance matrix in descending order and look for elbow or other methods discussed here . Can I use the same approach with the "centered" kernel matrix?
