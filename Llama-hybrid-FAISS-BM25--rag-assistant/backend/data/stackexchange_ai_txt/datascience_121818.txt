[site]: datascience
[post_id]: 121818
[parent_id]: 
[tags]: 
About the last decoder layer in transformer architecture

So, in the decoder layer of transfomer, suppose I have predicted 3 words till now, including the start token then the last decoder layer will produce 3 vectors of size d-model, and only the last vector will pass through embedding layer to form logits. Am I getting this right? Because its nowhere mentioned in the original paper and I'm having a hard time understanding it. What about the information that gets lost by discarding the two tokens before the last token. We could try to linearly project all the vectors into a single d-dimension vector but then the size of vectors would keep on increasing after we predict new word everytime and we'd need a new projection matrix everytime. This detail seems implicit and isnt mentioned anywhere. Can someone provide me what is actually done and the reason behind this or is this a random heuristic that seems to work (i.e. just take the final hidden state produced by the decoder)
