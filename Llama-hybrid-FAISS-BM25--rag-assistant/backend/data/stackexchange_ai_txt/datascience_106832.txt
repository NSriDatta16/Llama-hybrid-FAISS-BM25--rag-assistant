[site]: datascience
[post_id]: 106832
[parent_id]: 
[tags]: 
Why Do We Store The action In Replay Memory Deep Q-learning

According to my understanding, in Deep Q-learning, in order to train the NN, the agent stores experiences in a buffer (Replayed Memory), and each experience contains: e = When we train the model, we input the s in the target NN, and choose the Q value of action a . Then we input the s' in the prediction network and take the max Q value (then we calculate the loss using the reward r ...). My problem is that I don't understand why do we store the action a and take its Q value in the target network, if we always take the action with the max Q value. Doesn't the data of action a redundant?
