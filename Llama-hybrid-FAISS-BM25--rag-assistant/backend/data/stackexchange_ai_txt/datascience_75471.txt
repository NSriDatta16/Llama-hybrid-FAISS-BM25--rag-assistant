[site]: datascience
[post_id]: 75471
[parent_id]: 75466
[tags]: 
XGB uses the two kinds of regularization in both classification and regression; each leaf is a continuous score, these scores added together for the final prediction (of log-odds in the classification case), so penalizing the weights makes sense in either setting. See also L1 & L2 Regularization in Light GBM But yes, some hyperparameters ( scale_pos_weight ) seem to be vestigial: What does xgb's scale_pos_weight parameter do for regression? https://discuss.xgboost.ai/t/scale-pos-weight-for-regression/218/10
