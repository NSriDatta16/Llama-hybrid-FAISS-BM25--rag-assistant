[site]: crossvalidated
[post_id]: 510808
[parent_id]: 510772
[tags]: 
Just to add to the discussion. Assumption 1. Linearity This assumption says that we believe the true population model is linear in parameters (not in explanatory variables). Thus: $$ y = a_0 + a_1 x + a_2 x^2 + a_3 ln(x) +e$$ Is a linear model. However, you can also interpret it in a slighly different way. the linearity assumption says that the conditional mean is a linear function of parameters: $$ E(y|x) = = a_0 + a_1 x + a_2 x^2 + a_3 ln(x) $$ Which is a bit more flexible, because we are basically averaging out the error. This assumption basically imposes restrictions on how the error affects the model and allows us to estimate a Linear Regression model, usually via OLS. There are other important assumptions left tho. 1a) Random sampling: Your sample is representative of the population you want to study or say something about. 1b) the Var(X)>>0 and/or no multicollinearity. You want your data to have variation (which is used to identify coefficients in the model, but that variation has to be unique that that variable. For example, if $X1 = X2$ , then you cannot include both in a model because when controlling for one, the other variable does not vary. 1c) you want X to be independent of the population unobservables $e$ . Otherwise, your model and coefficients cannot be interpreted as causal relationships. ADDED: Keep in mind that once you estimate your model, the errors $\hat{e}$ are by construction linearly independent with X. However, they are not the same as the population unobservables $e$ . Now why would this allow you to estimate causal relationships? Lets consider the assumptions again. Model is linear: $$y=a_0+a_1*x +e$$ . If X is independent of e, and we can observe both, the causal effect of $x$ on $y$ could be done as follows: $$y^1=a_0+a_1*(x+1) +e$$ $$y^0=a_0+a_1*(x) +e$$ Thus the effect of a 1 unit change in $x$ can be is just $y^1-y^0$ . This is possible because we can "assume" e is constant. However, When X and e are correlated, if $X$ changes, then $e$ may change too for unknown reasons: $$y^1=a_0+a_1*(x+1) +e + \Delta e$$ $$y^0=a_0+a_1*(x) +e$$ In this case $y^1-y^0$ is not the effect of a change in X, because it also includes a change in $e$ we cannot explain. (technically we do not even know it is there) When X and $e$ are correlated in the population model, and you estimate it via OLS, $\hat{a_1}$ will be a combination of $a_1$ and the correlation between $X$ and $e$ . The rest of the assumptions do not have to do with the estimation of the coefficient in a Linear regression, but with the estimation of the standard errors of those coefficients, and the efficiency of information use. Thus they are important, but you can live without them. Assumption 2. Normality This is not a "MUST". the LR regression model in fact imposes no assumption on the errors. your errors could be poisson, uniform, chi2, etc etc etc, and you could still estimate the LR using OLS. Now, if you want to estimate the model with other method, then yes, you need to impose a distributional assumption on the errors. Then why is it added as an assumption in some texts?. One reason is as follows. When you estimate your LR using OLS, you find the following solution for $\beta s$ : $$\hat{\beta}=(X'X)^{-1} X'Y$$ $$\hat{\beta}=(X'X)^{-1} X'(X\beta+e)$$ $$\hat{\beta}=\beta+(X'X)^{-1} X'e$$ So the estimated coefficients are a function of the errors $e$ . Now, if you want to run tests on $\beta s$ , you need to know something about their distribution (is it symmetrical, or asymmetrical, or flat, or all bundle up with a few extreme values). If the errors are normal, however, $\beta s$ are also normal, so we can use the family of Normalbased distributions (t-test, F-test, Chi2, z etc), to do inference. In other words, the assumption of normality just makes life easier because it warrants the estimated coefficients are also normal. What if $e$ is not normal? When the sample is large enough...(no strict criteria that i know to decide what is large enough), the normality of the errors are no longer needed, and one relies on the Central limit theory. This basically implies that $\beta$ still distributions as normal, even if the errors $e$ do not. So, $e$ doesnt need to be normal, but its a good assumption that makes it easy to accept that $\hat{\beta}$ is normally distributed (and you can use standard tests). Assumption 3. Independence This assumption is usually taken as given. When you have crossection data, and you get a random sample of the population (truly random), it is easy to assume that those unobservables we leave in $e$ are independent of each other. When data is collected from "clusters", this may not be true. For example, if you get info from families, it is very likely that some of those "errors" are common among all family members, although they are independent across different families. When this happens, the estimation of standard errors of the $\beta s$ will be incorrect. The standard formulas assume errors are independent, but if they are indeed correlated, using standard formulas will most likely understate the true variation of the betas. In terms of the LR. Independence of the errors is a property that simplifies the estimation of standard errors, but it is not necessary, since you can "correct" for it. Assumption 4. Homoskedasticity So this assumption means variation of $e$ does not change with X. In other words, every single observation has the same amount of information for the estimation of $\beta s$ because the error variation is constant. When there is heteroskedasticity, this is no longer the case. $Var(e|X)$ may increase or decrease with X. Now, if this happens, some observations may have better information than others to estimate the coefficients. Observations with large conditional variance ( $Var(e|X)$ is high) will be less precise for estimating Betas (thus should receive less weight in your estimation), on the other hand, those with low variance will be more precise and receive more weight. Standard OLS assumes all observations are equally important for estimating $\beta s$ , so if your model is heteroskedastic, this equal weight assumption will be incorrect, and that will be reflected in the precision of your coefficients. from the analytical point of view you will need to at the very least correct standard errors (other methods for the estimation) if you want to make any time of statistical inference. HTH
