[site]: datascience
[post_id]: 26275
[parent_id]: 26247
[tags]: 
This is a common thing with neural networks and different batch sizes. The training loss is the average of losses for the minibatch. Naturally for the first few batches you'll have a higher loss and as it goes through the data the loss gets smaller. Mean while the loss for the validation set is calculated against the entire dataset.
