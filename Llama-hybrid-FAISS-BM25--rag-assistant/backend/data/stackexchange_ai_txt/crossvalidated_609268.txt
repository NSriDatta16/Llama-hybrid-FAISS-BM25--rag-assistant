[site]: crossvalidated
[post_id]: 609268
[parent_id]: 609225
[tags]: 
Here are some papers giving theoretical guarantees for Multiple Linear Layer Networks (more generally known in the literature as Deep Linear Networks) which should be of interest to you : First, the paper Deep Linear Networks with Arbitrary Loss: All Local Minima Are Global shows that, under some technical assumptions, all local minima of Deep Linear Networks are in fact global. This is good news as it guarantees that Deep Linear Networks trained with SGD will generalize "almost" as well as the Empirical Risk Minimizer (where "almost" corresponds to the how close your SGD output is to the minimizer) As for actual speed-up of the training, you can have a look at these : Exact natural gradient in deep linear networks and its application to the nonlinear case , Global Convergence of Gradient Descent for Deep Linear Residual Networks , Exponential Convergence Time of Gradient Descent for One-Dimensional Deep Linear Neural Networks and A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks . They are all quite technical, but from my understanding, the main takeaway message from all of these is that, when properly initialized, (stochastic) gradient descent on Deep Linear Networks converges extremely (read : exponentially) fast towards the global minimum, and the speed of convergence seemingly increases with the number of linear layers you add. This behavior is specific to Deep Linear Networks and does in general not hold for Non-Linear networks. In conclusion, yes, stacking linear layers is beneficial for training purposes : if you add more linear layers, the above papers indicate that SGD should converge to the global minimum quicker, and hence generalize almost as well as the global minimizer. However, as you correctly point out, the representation power of deep linear models is very limited (basically the same as a regular linear model), so these models remain of little practical relevance for solving complex regression tasks, and the main interests of the above papers is that they provide some theoretical tools which help us build a theory towards a better understanding of Deep (Non-Linear) Networks. As a quick last note, I just want to point out that Deep Linear Models are far from being completely useless : they are for instance at the core of Deep Matrix Factorization Models , which have numerous applications.
