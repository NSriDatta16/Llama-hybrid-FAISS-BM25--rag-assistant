[site]: datascience
[post_id]: 123219
[parent_id]: 123181
[tags]: 
I don't need to learn the theory of vectors, how can I understand vector databases practical function and use (as someone who wants to build an AI assistant), without learning about the theory of vectors exactly? You have some data — say, a closed internal corporate help system — that the AI model wasn't trained on. If you ask the AI "how do I blurb my squirms using my Foobarr(tm) platform", you won't get a meaningful answer. You take the text from your help articles, and run a special function on every piece of text. This function: Takes a text string on input 1 . Returns a tuple of numbers on output. Such a function is called an embedding model . It's a deterministic function 2 . For the same input it will always return the same output. Some popular examples of embedding models are ADA2 and Instructor , but there are many more. You store these tuples of numbers (called embeddings ) in a vector database, along with the key of your article (usually its URL or numeric id in the CRM). Some examples of vector databases are Pinecone and Weaviate , but there are many others. The embegging model math is wired in such a way that strings of semantically close text return arrays of numbers with high cosine similarity 3 . This is where the magic happens. The embedding model transforms texts that are close to each other semantically to tuples (fixed-length arrays) of numbers that are close to each other numerically . Semantical closeness of phrases might be a very vague concept, but numerical closeness of vectors has a certain, strict mathematical definition which computers can work with. When you get a query for your chat bot, you will first call this function on the user's prompt: "how do I blurb my squirms using my Foobarr(tm) platform" and get a tuple of numbers back. You save it in a variable prompt_embedding You issue a query to your vector database to the following effect: "Here's a vector. Give me top three records, ordered by descending cosine similarity with this vector, as long as it's more than 0.8". You would use code similar to this 4 : client.query .get("help_articles", ["content", "key"]) .with_near_vector({ "vector": prompt_embedding, "certainty": 0.8 }) .with_limit(3) .with_additional("certainty") .do() Because of the way the embedding model works, there is a high chance that returned records will correspond to the articles relevant to blurbing the squirms. You retrieve the text of the relevant articles using the key returned by the vector database 5 . You feed the text of the articles to your AI model, usually in the system portion of the prompt 6 . On Llama2, your prompt to the model will look something like this: [INST] > {{ You are a virtual assitant for Foobarr. You cover the following topics: Foobarr platform help. Use articles below to answer the question. Condense the most relevant article into no more than thirty sentences. Append its hyperlink. Append the hyperlinks to other relevant articles. Output in Markdown format. Url: http://foobarr.example.com/help/blurbing-the-squirms Content: Efficient Squirm Blurbing is our specialty! Foobarr(tm) platform will help you blurb your squirms in no time. Log in to the app using your mobile phone, point the camera to your squirm or squirms, and click "Blurb". The squirm gujjuggins will get extraburbated in the cloud… Url: http://foobarr.example.com/help/blurb-efficiency Content: What to do if gujjuggins come out underextraburbated? First thing to check is the squirm is sufficiently dimarquidated. To do this, open the main menu, click "Marquidation", then go to… Url: http://foobarr.example.com/help/squirms-and-gujjuggins Content: Gujjugings is what makes the squirms blurbable. They are tiny pieces of… }} > {{ how do I blurb my squirms using my Foobarr(tm) platform? }} [/INST] Note that the text in the articles is semantically relevant to your user's prompt. The AI model will formulate the answer based on the user's prompt, domain knowledge you just gave it, its own knowledge of things and its own internal command of human language; and give it back to the user. Some cloud products (which might be referred to as "large language models") bundle this functionality and will do it for your with a single HTTP call. They are still doing the same or similar thing under the hood, so the LLM proper is only a part of this bundle. The embedding model, the vector database and the large language model are separate things, not coupled to each other. The (ultimate) artefact of the embedding model and the vector database is the list of human-language texts relevant to your prompt, which you can feed to any LLM to give it domain knowledge. There are several practical considerations you'll have to keep in mind when using this approach: LLM's have a limited prompt size. You will have to come up with a way to shrink your domain-specific portion of the prompt so that it fits into the limit. Since the LLM doesn't have any memory, you will need to feed it the history of the conversation, complete with the domain-specific data, with every new prompt, if you want to maintain the conversation context. At some point you will need to throw away some of the history because it will not fit into the LLM any more. Deciding what to keep and what to throw away is a challenging task. If the converation takes a sharp turn (when you jump from one topic to another), most LLMs will start to hallucinate or return responses irrelevant to the prompt. Detecting this is another challenging task. Some prompts are of meta-nature (for instance, saying "Translate it for me into Russian" or "Expand the abbreviations"). Such prompts are usually prone to yielding false positive matches from the vector database, because they use generic text that has high chance to be semantically relevant to something in your articles. Embedding scoring can help you with that as well. You can use it to decide which parts of the history are still relevant to the latest prompts, so that you can wipe some of the context, or even start the whole conversation altogether. 1 Under the hood, the string of text is first converted to an array of numbers called tokens using another function called "tokenizer". Most popular embedding models come with libraries that will hide this step from you. From the user's perspective, it's a string in and a tuple of numbers out. 2 This function is doing a combination of matrix multiplications and some other freshman year level math. It has some moving pieces: In addition to the explicit parameter (a string of text), it takes a lot of numbers as implicit parameters. They usually live in a folder on your HDD, and get pre-loaded to the memory when you initialize the model library. There might be also some additional explicit parameters in the function call, which you usually hardcode in your program and never change. Differences in CPU and GPU architectures can slightly affect the precision of the math on different machines. Some embedding models (like ADA2) you can only access as a remote function in the cloud, which means that they can replace any of that on their backend any time and the same HTTP call will give you a different answer. But as long as all these moving pieces stay fixed, an embedding model is as deterministic as the sine or the cosine. 3 The linked article will tell you the exact formula how the cosine similarity is calculated. You don't have to know how it works to build the chat bot, but you have to know if that's the algorithm the model uses. The distance algorithm is mentioned in the model's description. Some models use distance algorithms other than cosine similarity, which your vector database should know of and support, if that's the case. 4 The vector database is able to execute this query efficiently, usually using the indexing algorithm known as Hierarchical Navigable Small Worlds . You don't have to know how it works internally to work with it. 5 You don't necessarily have to use the vector database for getting the text: you can just query your help CRM directly, or download it via HTTP straight from the website. Most vector databases are able to store extra data along with the vectors, letting you get the relevant text in a single call to the database. 6 ChatGPT and its friends have a special slot for it in the JSON format that they accept; Llama2 has a slot in its prompt structure format for it; for other models you might just insert it as a part of human-language prompt. Note that all models have limited prompt size; if your text is too long you will have to come up with a way to shorten it so that it fits in the prompt.
