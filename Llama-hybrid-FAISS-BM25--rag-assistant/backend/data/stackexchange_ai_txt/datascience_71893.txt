[site]: datascience
[post_id]: 71893
[parent_id]: 71649
[tags]: 
Inputs of LSTMs are vectors and it does really matter what the vectors are, in signal processing there are signal windows, in NLP, these are usually word embeddings, here, they are document representations obtained from Doc2vec. Judging from the scheme, they probably use Doc2vec to obtain sentence embeddings. It is rather unusual, but definitely possible. If the input vectors carry enough information, LSTM will certainly learn a reasonable representation.
