[site]: crossvalidated
[post_id]: 420345
[parent_id]: 880
[tags]: 
That's a good question, and that tends to hit more of what's referred to ensemble learners and model averaging (I'll provide links below): When you're in high dimensional settings, the stability of your solution (i.e., what features/variables are selected) may be lacking because individual models may choose 1 among many collinear, exchangeable variables that by-and-large carry the same signal (among one of many reasons). Below are a couple of strategies on how to address this. In bayesian model averaging for example, Hoeting, Jennifer A., et al. "Bayesian model averaging: a tutorial." Statistical science (1999): 382-401. you construct many models (say 100), and each of which is constructed with a subset of the original features. Then, each individual model determines which of the variables it saw was significant, and each model is weighed by data likelihood, giving you a nice summary of how to "judge" the effectiveness of variables in 'cross-validation" sort of way. If you know a-priori that some features are highly correlated, you can induce a sampling scheme such that they're never selected together (or if you have a block-correlation structure then you choose elements of different blocks in your variance-covariance matrix) In a machine learning type setting: look at "ensemble feature selection". This paper (one example) Neumann, Ursula, Nikita Genze, and Dominik Heider. "EFS: an ensemble feature selection tool implemented as R-package and web-application." BioData mining 10.1 (2017): 21. determines feature significance across of a variety of "importance" metrics to make the final feature selection. I would say that the machine learning route might be better b/c linear models (w/ feature selection) saturate at p = n b/c of their optimization re-formulation (see this post If p > n, the lasso selects at most n variables ). But as long as you can define and justify a good objective criterion on how you 'cross-validate' the feature selection, then you're off to a good start. Hope this helps!
