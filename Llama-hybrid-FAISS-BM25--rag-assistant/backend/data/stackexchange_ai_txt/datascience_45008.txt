[site]: datascience
[post_id]: 45008
[parent_id]: 
[tags]: 
SVM Cost function change to improve its computational efficiency

While listening to Andrew Ng's course of Machine Learning he said that the SVM's cost function term $\frac{\Theta^T\Theta}{2}$ is usually changed to $\frac{\Theta^TM\Theta}{2}$ , where matrix $M$ depends on the kernel choice. In his words, this allows that the computational efficiency of the optimization algorithm can be improved, even when the training set is large. Can someone point me to some materials that support that claim? How do we choose $M$ , and how can we change the cost function/its partial derivatives so that the gradient computation is feasible even when the training set is large? Also given that we include a kernel-specific weighing matrix, I assume that different kernels then lead to different kinds of decision boundaries? Is this correct or does matrix $M$ have no influence on the decision boundary?
