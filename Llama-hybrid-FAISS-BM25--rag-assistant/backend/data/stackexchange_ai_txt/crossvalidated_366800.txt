[site]: crossvalidated
[post_id]: 366800
[parent_id]: 366789
[tags]: 
The first statement seems to make sense because when Y is categorical, we link a decision threshold ($Y = 1$ when $Y > Y^*$) and a regression ($Y^*=X+e$). And we can tune the threshold $Y^*$ to improve the prediction accuracy. Logistic regression works exactly in this way. When you apply tree-based models, you would primarily focus on tuning the so-called "hyperparameters", such as the maximum depth and splitting criteria of the tree model. Most importantly, tree-based models work differently from regressions. In this sense, a regression tree is a type of tree model (when $Y$ is a continuous variable), which still splits the data using Xs as the splitting nodes at each level, and the $\hat{Y}$ (predicted value) will the $\bar{Y}$ (group average) of observations at an ending node.
