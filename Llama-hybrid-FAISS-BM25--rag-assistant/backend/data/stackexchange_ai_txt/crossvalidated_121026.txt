[site]: crossvalidated
[post_id]: 121026
[parent_id]: 
[tags]: 
What is the probability distribution of $1-\text{mean}(|A-B|)$ where $A$ and $B$ are independent U(0,1)?

I'm not well versed in statistics so I'm not sure if my question is worded exactly correctly but basically here's the problem I'm trying to solve: imagine you have two equal sized arrays of size n. Each array is filled with random numbers from 0 to 1. The arrays are then subtracted from each other, and the absolute value taken. The average value of this array is then calculated, and this value then subtracted from 1. This process is repeated for T trials, and the values are plotted with a histogram to visualize the distribution. I would like an equation that gives said distribution as well as the mean and standard deviation for the distribution. I have searched extensively online to try and find an answer to no avail. Below you will find some matlab code that does what I'm talking about for an array of size 8 and 100,000 trials: clc clear all n=8; T=100000; H=zeros(1,T); for i=1:T A=rand(1,n); B=rand(1,n); H(i)=mean(1-abs(A-B)); end histfit(H,100); Minimum=min(H) Maximum=max(H) Mean=mean(H) Standard_Deviation=std(H) One can observe that the mean approaches 2/3 with increasing trials, and that the standard deviation decreases with increasing array size n. For n=8 it is about 8.33. It appears to be normal but not quite. Any help greatly appreciated, thanks.
