[site]: stackoverflow
[post_id]: 3452929
[parent_id]: 3446622
[tags]: 
Your first item will likely be the most difficult, since there are essentially no good incremental SVM implementations in existence. A few months ago, I also researched online or incremental SVM algorithms. Unfortunately, the current state of implementations is quite sparse. All I found was a Matlab example , OnlineSVR (a thesis project only implementing regression support), and SVMHeavy (only binary class support). I haven't used any of them personally. They all appear to be at the "research toy" stage. I couldn't even get SVMHeavy to compile. For now, you can probably get away with doing periodic batch training to incorporate updates. I also use LibSVM, and it's quite fast, so it sould be a good substitute until a proper incremental version is implemented. I also don't think SVM's can model the concept of an "unknown" sample by default. They typically work as a series of boolean classifiers, so a sample ends up as positively being classified as something, even if that sample is drastically different from anything seen previously. A possible workaround would be to model the ranges of your features, and randomly generate samples that exist outside of these ranges, and then add these to your training set. For example, if you have an attribute called "color", which has a minimum value of 4 and a maximum value of 123, then you could add these to your training set [({'color':3},'unknown'),({'color':125},'unknown')] to give your SVM an idea of what an "unknown" color means.
