[site]: crossvalidated
[post_id]: 408024
[parent_id]: 214900
[tags]: 
This paper covers what you are looking for in very elegant detail (using soft threshold SVD). Like Geoffrey pointed out, they do this by writing their own cost function which excludes from the cost, any predictions made against the missing values. Summary : Mazumdar et al use convex relaxation techniques to provide a sequence of regularized low-rank solutions for large-scale matrix completion problems. Algorithm SOFT-IMPUTE iteratively replaces the missing elements with those obtained from a soft-thresholded SVD. Exploiting the problem structure, they show that the task can be performed with a complexity of order linear in the matrix dimensions. The algorithm is readily scalable to large matrices; for e.g it fits a rank-95 approximation to the full Netflix training set in 3.3 hours. The methods achieve good training and test errors and have superior timings when compared to other competitive state-of-the-art techniques. @article{mazumder2010spectral, title={Spectral regularization algorithms for learning large incomplete matrices}, author={Mazumder, Rahul and Hastie, Trevor and Tibshirani, Robert}, journal={Journal of machine learning research}, volume={11}, number={Aug}, pages={2287--2322}, year={2010} }
