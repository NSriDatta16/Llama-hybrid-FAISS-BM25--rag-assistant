[site]: crossvalidated
[post_id]: 498149
[parent_id]: 498146
[tags]: 
There are many fundamental issues you need to address first. Very important is whether you conceptualize this as a forced-choice classification task or as a continuous prediction task. "Classifier" denotes the former, and probability models are examples of the latter. See here . RF, since it can provide continuous outputs, is more in the prediction than the classification mode. Random forests, which require incredibly large sample sizes in order to be successful, are notorious for overfitting data especially when there are many candidate predictive features. Though random forest (RF) algorithms may build in some cross-validation to guide the algorithm, it is not correct to say that this "internal" cross-validation provides unbiased performance of RF. When you do a proper validation that includes unbiased estimation of smooth calibration curves you will often be shocked at the appalling lack of calibration accuracy from RF. If your sample size is larger than say 50,000 observations you can affort to have a large (n=20,000) holdout sample for stringent validation of RF predictions. Otherwise you might look into bootstrapping the entire RF process to obtain unbiased estimates of likely future predictive performance of RF with respect to calibration and predictive discrimination.
