[site]: crossvalidated
[post_id]: 524805
[parent_id]: 
[tags]: 
Is it possible to improve Markov Chain Monte Carlo performance by decomposing a Binomial Likelihood?

Suppose that we have sampled $y_{1},y_{2},...,y_{n}$ from a Binomial distribution $Bin(N,p)$ . Also, let's assume that $p$ is known and our goal is to infer the unknown parameter $N$ , with the use of the MCMC Metropolis-Hastings algorithm. $\underline{Model}$ Likelihood, $L(N|y_{1},y_{2},...,y_{n}) = \prod_{i=1}^{n}Bin(y_{i};N,p)$ and prior distribution denoted as $p(N)$ . $\underline{Update \ Step}$ For the MH update step we have the fraction $\frac{p(N^{cand})L(N^{cand}|y_{1},y_{2},...,y_{n})}{p(N^{old})L(N^{old}|y_{1},y_{2},...,y_{n})}$ , where $N^{cand}$ is the proposed value of the parameter $N$ . Now let's assume that for the exact data set $y_{1},y_{2},...,y_{n}$ we know that each $y_{i}$ can be decomposed as $$y_{i} = y_{i}^{(1)}+y_{i}^{(2)}$$ where those $y_{i}^{(1)}$ and $y_{i}^{(2)}$ , can be regarded independent, and follow Binomial distributions $y_{i}^{(1)}\sim Bin(N^{(1)},p)$ and $y_{i}^{(2)}\sim Bin(N^{(2)},p)$ for which is also known that $N=N^{(1)}+N^{(2)}$ . So, in this latter case instead of using the Likelihood of the form $\binom{N}{y_{i}}p^{y_{i}}(1-p)^{N-y_{i}}$ we can use the Likelihood of the form $\binom{N^{(1)}}{y_{i}^{(1)}}p^{y_{i}^{(1)}}(1-p)^{N^{(1)}-y_{i}^{(1)}}\binom{N^{(2)}}{y_{i}^{(2)}}p^{y_{i}^{(2)}}(1-p)^{N^{(2)}-y_{i}^{(2)}}$ . (I'm not bothered into placing priors on $N^{(1)}$ and $N^{(2)}$ because their sum is equal to $N$ so I place a prior directly on $N$ ) $\underline{Question:}$ My question is, if this additional information of $y_{i}^{(1)},y_{i}^{(2)}$ gives an additional improvement into the MCMC Metropolis-Hastings algorithm, or using either the first or later Likelihood representation give exactly the same results? My intuition is that it will improve things because we have additional information for the $y_{i}$ but I do not know if it true.
