[site]: crossvalidated
[post_id]: 315586
[parent_id]: 
[tags]: 
If z is uniform normal is $f(z;\theta)$ a normal distribution

I am trying to understand the description of variational auto-encoders here to quote the excerpt: Before we can say that our model is representative of our dataset, we need to make sure that for every datapoint $X$ in the dataset, there is one (or many) settings of the latent variables which causes the model to generate something very similar to $X$. Formally, say we have a vector of latent variables $z$ in a high-dimensional space $Z$ which we can easily sample according to some probability density function (PDF) $P(z)$ defined over $Z$. Then, say we have a family of deterministic functions $f(z; θ)$, parameterized by a vector $θ$ in some space $Θ$, where $f : Z × Θ → X$ . $f$ is deterministic, but if $z$ is random and $θ$ is fixed, then $f(z; θ)$ is a random variable in the space $X$ . We wish to optimize $θ$ such that we can sample $z$ from $P(z)$ and, with high probability, $f(z; θ)$ will be like the $X$’s in our dataset. To make this notion precise mathematically, we are aiming maximize the probability of each $X$ in the training set under the entire generative process, according to: $$P(X) = \int_z P(X|z; θ)P(z)dz. \tag1$$ Here, $f(z; θ)$ has been replaced by a distribution $P(X|z; θ)$, which allows us to make the dependence of $X$ on $z$ explicit by using the law of total probability. The intuition behind this framework—called “maximum likelihood”— is that if the model is likely to produce training set samples, then it is also likely to produce similar samples, and unlikely to produce dissimilar ones. In VAEs, the choice of this output distribution is often Gaussian, i.e., $P(X|z; θ) = N (X| f(z; θ), σ 2 ∗ I)$. That is, it has mean $f(z; θ)$ and covariance equal to the identity matrix $I$ times some scalar $σ$ (which is a hyperparameter). It would occur to me that if we are making a choice on any distribution it is on $z$. How can $P(X|z; θ)$ be chosen to be normal? and what is it with mean $f(z; θ)$? I am looking forward to understand the emboldened text and a little elucidation on what is being said in those lines
