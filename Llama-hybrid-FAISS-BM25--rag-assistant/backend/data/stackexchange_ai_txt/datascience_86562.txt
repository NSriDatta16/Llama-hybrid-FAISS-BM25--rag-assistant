[site]: datascience
[post_id]: 86562
[parent_id]: 76872
[tags]: 
The NSP loss has been used to improve the model's inter-sentences understanding, which is particularly observable on inter-sentences understanding datasets like SQUAD (question answering) or MNLI (sentence entailment). You can refer to the ablation studies of the original BERT's paper for more details. By removing the NSP loss and replacing it by the DOC-SENTENCES or FULL-SENTENCES strategies, Roberta's authors showed that using MLM alone * (without NSP) is enough, even better, for the model to catch inter-sentences understanding. Again, you can refer to the ablation studies of the original Roberta's paper for more details. *To be precise, Roberta is performing dynamic MLM vs static MLM for BERT.
