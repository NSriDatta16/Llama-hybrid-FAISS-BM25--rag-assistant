[site]: crossvalidated
[post_id]: 108016
[parent_id]: 108012
[tags]: 
1. Lets look at a simple example of trying to predict housing prices. Assume we have a dataset that looks like Cost | Sq Ft | N bedroom 100K 1,800 4 120K 1,300 3 220K 2,200 5 In the case of supervised learning we would know the cost (these are our y labels) and we would use our set of features (Sq ft and N bedrooms) to build a model to predict the housing cost. The formula would look like Cost ~ Sq Ft + N bedrooms Now in unsupervised learning we would not know the cost of the house but we still would know the features. Therefore, we would train a model and try to group the types of houses together that are similar. For an example of this look at k-means clustering ( http://scikit-learn.org/stable/modules/clustering.html#clustering ) This is a great, free, book which covers this very nicely ( http://web.stanford.edu/~hastie/local.ftp/Springer/OLD/ESLII_print4.pdf ) 2. Each type of learning method (may) have a set of parameters which are called model parameters. The training phase is used to find out the optimal set of parameters which generalizes you data the best. That book also gives very nice information on different learning methods are there parameters. For example, in the leaning algorithm called SVM there is a term that looks like $\exp(-\gamma|x-x^{2}|)$. In this example the $\gamma$ parameter is what we try to optimize using the training data.
