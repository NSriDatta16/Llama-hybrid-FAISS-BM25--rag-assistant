[site]: datascience
[post_id]: 53057
[parent_id]: 53056
[tags]: 
1) You don't, at least initially. Use all of the variables you have, and through proper cross validation/model tuning (in particular, tuning the number of variables you randomly select to try in each split of the tree, and maybe stuff like minimum node size and maximum size of individual trees) let the random forest find the most predictive variables for you. If you have a lot of candidate variables, then maybe try an wrapper based method like permutation importance, recursive feature elimination, etc. though for now I'd avoid this especially if you are a beginner (these methods tend to take a long time, and may not be useful/offer marginal improvement). 2) What is the purpose of this? As a means of feature selection? Random forests already kind of do this; they randomly select a subset of your variables at each split in a tree. Again, I would stick to letting the random forest do its feature selection for you for now. 3) Both. Nominal attributes are often one-hot encoded/dummy encoded (create k-1 dummy columns, k = number of categories in the nominal variable) which I encourage you to search up. Basically, you are encoding a nominal attribute into a numerical one. You can also encode nominal variables through other means (like target encoding) but again, if you are a beginner let's avoid that for now because you can severely screw up your model validation by doing this.
