[site]: datascience
[post_id]: 112693
[parent_id]: 
[tags]: 
Do we know why GAN-based data augmentation works?

Although I've seen many examples of GAN-generated synthetic data greatly improving the performance of models, I struggle to understand how this is possible. Say we are training a classifier $h$ to predict distribution $D$ based on dataset $X$ , and we use GAN $g$ to augment our dataset, creating $X' | X \subset X'$ . $g$ has the same input as $h$ ; therefore, it has the same amount of total information and can still only reliably estimate the subset of $D$ which contains $X$ . $g$ may "create" more data, but its accuracy is far from guaranteed. Why does this work? If the composition of neural networks $h(g(X))$ is better, shouldn't we theoretically be able to create a single neural network $h'$ that is as resourceful as $g$ but serves the purpose of $h$ ?
