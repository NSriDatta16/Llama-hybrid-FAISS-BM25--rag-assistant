[site]: crossvalidated
[post_id]: 301235
[parent_id]: 289498
[tags]: 
I think that the answer might be no. There is a fixed amount of information in a dataset and a theoretically optimal classifier would use all of that information. The GAN trained on the dataset does not actually create new information, it just samples from the distribution that best represents the original dataset. So, new examples generated by the GAN would not increase the net information in the dataset and therefore would not be useful for class imbalance. It might be useful for regularization (eg, dataset augmentation), but I don't think that this would be necessary with an optimally trained classifier.
