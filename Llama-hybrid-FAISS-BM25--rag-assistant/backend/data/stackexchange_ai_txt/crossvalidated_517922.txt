[site]: crossvalidated
[post_id]: 517922
[parent_id]: 516914
[tags]: 
At time of writing- Two good answers here, so I'll take a different approach and try to explain some things in "plain English." When we talk about likelihood, think of "plausibility" rather than probability. What makes something plausible? The parameter(s) that describe the process which (assumedly) underlie the data generating process. If the parameter is a good fit, observing the data should seem plausible to you. If the parameter is a terrible fit, then observing the data should be somewhat surprising. A "frequentist" would use calculus to optimize the parameter that makes observing the data most plausible (maximum likelihood estimation.) However, this is just a point estimate and we don't know how quickly it loses predictive accuracy if we were to nudge it. Bayesians find this uncertainty around parameter estimates to be troublesome. And so they use Bayes Rule to flip the relationship, no longer asking, "how plausible is the data (given the parameter)?" (question 1) But rather "How plausible is the parameter (given the data)?" (question 2.) Question 1 is often called the likelihood. Question 2 is often called the posterior. Another element we haven't addressed is the prior. It's some belief about the parameter(s) that guides them to areas, which you believe are more reasonable. For example, if you're studying the relationship between height and weight in a linear model, you know "a priori" (pre-analysis) that there shouldn't be a negative relationship between height and weight. It would make no sense that an increase in weight should be associated with a decrease in height. And the last element is the evidence- it's the probability of seeing the data, irrespective of any specific parameter value. In the likelihood, we talk about how plausible observing the data would be given some parameter describing the data generating process. The evidence however, marginalizes this parameter out via integration (continuous) or summation (discrete). If this sounds weird, think of it in the discrete case first. If you have data of children at a school, and more specifically you have two probabilities: (1) that a student is both (A) a member of the chess club and (B) a boy and (2) that a student is both (A) a member of the chess club and (B) a girl. Then if you marginalize out sex, you can isolate the probability of belonging to the chess club. In the continuous case, we have an infinite number of classes/buckets, which the parameter could take on. To make things more complicated, often the most difficult part of Bayesian Inference (getting the posterior distribution) is in computing the evidence. Integrals are hard in 1D, damn tricky in 2D, extremely difficult in 3D, and 4-plus dimensions? Forget it. Fortunately, the evidence is only used to ensure that the numerator (likelihood * prior) is scaled such that it sums to 1 (a valid probability distribution.) With this in mind, we still know a good deal about what we'd like to model given just the numerator, whether or not it integrates to 1. In fact, there are several different ways by which the posterior could be approximated- MCMC sampling, grid approximation (like multidimensional Riemann sums), etc. Anyway, all that to say, it's very often to see the posterior being "proportionate to" to the numerator. Don't let this trip you up. The Bayesian mindset is really cool; unfortunately it's often taught in two unsatisfactory ways. (A) In a stats department with the assumption that you've had 3 semesters of calc, 1 semester of linear algebra, and perhaps even more mathematical firepower- so the proofs ought to speak for themselves. Or (B) As an anecdote or chapter in a machine learning course- these sort of books will discuss priors as little more than regularizing coefficients (not entirely false) and view Bayesian Inference as just a tool for the job, which should be used when your random forest (or insert ML model) performs poorly. Neither of these approaches are really helpful for an outside looking in. I recommend Statistical Rethinking, by Richard McElreath. Lectures , PyMC3 code . Cheers!
