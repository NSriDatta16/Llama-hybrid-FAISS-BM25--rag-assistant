[site]: crossvalidated
[post_id]: 157269
[parent_id]: 59822
[tags]: 
They are different specially in the case of regression. The effect of the quadratic loss is to average errors (so it is more sensitive to outliers). To see this, if you minimize, $$ L_{2}(y) = \sum_{i}(y-x_{i})^{2} $$ you get the mean value. The effect of the L1 loss function is to , so that it leads to sparse solutions robust against outliers. Again, consider, $$ L_{1}(y) = \sum_{i}|y-x_{i}| $$ The derivative gives you $\sum_{i}\operatorname{sign}(y-x_{i}) = 0$ , which is true for the median. And the median is robust against outliers. Now, how does this translate for SVMs? The loss function affects how the kernel function is regularized (i.e. how you compare samples). This is specially critical in the case of regression, since when using the $L_{2}$ the solution is no longer sparse! (which is one of the aspects of SVMs which makes them interesting in practice). The optimization problem in that case reads, $$ \min_{w} \sum_{i=1}^{l} \xi_{i}^{2} \\ \text{subject to } y_{i}- = \xi_{i} \\ ||w|| \leq B \text{ and } i = 1,...,l $$ After deriving with respect to the primal variables and substituting, you get, $$ \min_{\alpha} -\lambda \sum_{i=1}^{l}\alpha_{i}^{2}-\sum_{i,j}\alpha_{i}\alpha_{j}\kappa(x_{i},x_{j}) + 2\sum_{i}\alpha_{i}y_{i} $$ for a detailed derivation see for example here . Notice that the first two terms can be grouped in we define as, $$ \min_{\alpha} -\sum_{i,j}\alpha_{i}\alpha_{j}\hat{\kappa}(x_{i},x_{j}) + 2\sum_{i}\alpha_{i}y_{i} $$ where $\hat{\kappa} = (\kappa + \lambda I)$ . Notice that this forces the $\alpha$ 's not to be sparse anymore.
