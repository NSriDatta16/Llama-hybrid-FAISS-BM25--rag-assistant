[site]: crossvalidated
[post_id]: 587695
[parent_id]: 
[tags]: 
How to train neural network classifier to have a low false positive ratio, when false negative ratio is almost irrelevant?

How can I optimize training for a neural network so that I can get a low false positive ratio? I'd like to have a binary classifier that recognizes some positive examples very reliably, that is with very low false positive ratio, but don't care much about the false negative ratio (even 99.9% false negatives would be OK.) Is there a special loss function that is appropriate for this scenario? Special neural network structures / training regimes? Are there some keywords I could search for, to find more information what's appropriate for such a scenario? Update : With this question I'm wondering whether mostly dropping the requirement of a reasonable false negative ratio leads to new avenues that wouldn't make sense otherwise. For instance, if there was an algorithm that would find tiny islands of easily positively classified examples in a sea of not so easily classified examples, that'd perfectly fit the question. And please note that I'm talking about a low false positive ratio, not about few false positives, which excludes the degenerate "classify everything as negative" classifier from consideration. Intuition : To give you a reason for my hunch that there is something special about that scenario: the usual simple neural network t rained as a classifier would more or less be trained for a good classification performance over all examples. But with a radial basis function network you could in principle pick out a few interesting areas where most examples are positive, discarding all the rest, though I wouldn't know how to train it like that. So there could be special network structures, special ways to train, special loss functions and whatnot that are suited to this kind of problem.
