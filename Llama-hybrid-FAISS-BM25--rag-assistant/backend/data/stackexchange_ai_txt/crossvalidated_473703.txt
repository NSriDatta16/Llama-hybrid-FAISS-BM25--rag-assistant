[site]: crossvalidated
[post_id]: 473703
[parent_id]: 473692
[tags]: 
If you are comparing multiple models then you can compare them in the exact same way you would in machine learning, by computing the predictive likelihood of a hold-out test set. The AIC (commonly used for model selection) is an approximation to the out-of-sample predictive likelihood in certain models, but in most cases its probably best to use an actual test set instead. If you only have a single model and you want to know whether it provides a good fit to a dataset, then this can be achieved by goodness-of-fit testing (frequentist) or using something like posterior p-values (Bayesian). In both cases you are essentially testing whether the observed data "looks like" typical data simulated from the fitted model, where the definition of "looks like" is done via an appropriately chosen test statistic/distance function.
