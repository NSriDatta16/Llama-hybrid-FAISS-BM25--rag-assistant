[site]: crossvalidated
[post_id]: 19997
[parent_id]: 19996
[tags]: 
A model ensemble is simply a collection of models whose output is combined (hopefully generating superior performance in the process). Obviously, to be of any interest, the base models must vary somehow, and there are several ways to do this: vary the model type (tree induction, neural network, discriminant function, etc.), vary the starting conditions of the model training (such as differing weight initializations for feedforward neural networks), vary the observations used (typically random samples of the entire training set), vary the candidate input variables (again, typically random samples of all those available), etc. There are several ways to combine the base model outputs. The simplest are averaging or voting, though these may require some calibration.
