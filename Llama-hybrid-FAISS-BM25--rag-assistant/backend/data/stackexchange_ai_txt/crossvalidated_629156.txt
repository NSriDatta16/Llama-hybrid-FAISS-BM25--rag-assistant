[site]: crossvalidated
[post_id]: 629156
[parent_id]: 300839
[tags]: 
Understanding Noisy Loss Curves in Keras (TensorFlow 2.13) IF you are using Keras then this can be the reason: TLDR: This is a smoothing artifact dues to Keras way of using running means for all its losses and metrics. This answer is the same as in https://stackoverflow.com/questions/72119572/keras-loss-value-significant-jump Why the jumpy Loss Curves? It took me quite some time to understand why there were jumps between epochs during training, and I noticed many others discussing similar issues on various forums. I searched for bugs in my own training pipeline and tried to understand the internals. In the end this is just a case of bad documentation and a smoothing artifact. The reason for these jumps is a bit subtle. Keras, a popular AI framework, calculates its metrics and losses as running means over one epoch. This can make your loss curve look quite noisy at the beginning of training. However, over time, it smoothens out. But here's the catch: in the next epoch, the loss is actually much lower than the previous epoch's average. This creates that staircase-like curve in your training plot. If you want to visualize the raw batch loss, you can use a custom callback. I normally just inherit the Keras TensorBoard callback and add the stuff I need but you can ofc just create you own custom independent callback as done here https://stackoverflow.com/questions/72119572/keras-loss-value-significant-jump import tensorflow as tf class CustomKerasTensorBoard(tf.keras.callbacks.TensorBoard): def on_epoch_begin(self, epoch, logs=None): self.previous_loss_sum = 0 super().on_epoch_begin(epoch, logs) def on_train_batch_end(self, batch, logs=None): current_loss_sum = (batch + 1) * logs["loss"] current_loss = current_loss_sum - self.previous_loss_sum self.previous_loss_sum = current_loss_sum logs["loss_raw"] = current_loss super().on_train_batch_end(batch, logs) And add it to your model.fit/model.evaluate/model.predict call. Here's a graphical representation to help you understand the concept better: The top graph shows the classification accuracy. The middle graph displays the loss. The bottom one illustrates the raw batch loss, which I've left unsmoothed. So, when you see those seemingly erratic jumps in your loss curve, remember it's part of the smoothing process, and your training might be progressing just fine. Keep calm and carry on!
