[site]: datascience
[post_id]: 72166
[parent_id]: 
[tags]: 
Finding a vector that minimize the MSE of its linear combination

I have been doing a COVID-19 related project. Here is the question: N = vector of daily new infected cases D = vector of daily deaths E[D] = estimation of daily deaths N is a n-dimensional vector, n is around 60. E[D] is another n-dimensional vector. Under certain assumptions, each entry of E[D] can be calculated as a linear combination of the entries of N. We want to find the vector N such that the E[D] derived from N has least mean squared error when compared to actual D data. I think a gradient descent algorithm is needed here. However, I am not very familiar with gradient descent. This seems to be a basic data science problem, but I am kind of lost right now. Does anyone has any idea about which algorithm should I dig into?
