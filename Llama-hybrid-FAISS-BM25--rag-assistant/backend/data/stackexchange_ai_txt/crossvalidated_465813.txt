[site]: crossvalidated
[post_id]: 465813
[parent_id]: 
[tags]: 
How to preprocess performance counter input data for anomaly detection using autoencoders

I am working with more than 250 input features that include system performance counters and SQL Server database counters to predict anomalies / system outages. I am looking to use an autoencoder network. The problem I am faced with is how to preprocess (normalize / scale) the input features that have vastly different characteristics. The maximum z-scores for many features can range from 20-125. My initial attempts with using min-max scaler appeared to be not very useful. Similar to the observation in this thread ( How to normalize input data for autoencoders - anomaly detection ), I found that the min-max scaler suppress the reconstruction error for most observations. Next, I am experimenting with using StandardScaler (z score normalization). StandardScaler poses the other challenge to effectively training the model since the input features can assume a wider range of values. I suspect it going to be a similar problem as StandardScaler when using the sklearnâ€™s RobustScaler. My questions are: Is there any guidance on dropping blatant outliers from the training data based on z-scores? Are there other approaches to preprocessing the input data in such scenarios? Meanwhile, my plan is to continue to work with StandardScaler and focus on tuning the model architecture and hyperparameters.
