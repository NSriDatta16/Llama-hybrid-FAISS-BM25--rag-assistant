[site]: crossvalidated
[post_id]: 405998
[parent_id]: 405086
[tags]: 
Thank you for bringing this interesting book to our attention. Below are my two cents. ...we will assume that counterfactual outcomes are deterministic and that we have recorded data on every subject in a very large (perhaps hypothetical) super-population. The above seems to merely be referring to the Central Limit Theorem and the Law of Large Numbers . In other words, as the sample size N increases, from a frequentist perspective, your standard error around an effect modifier (or equally causal risk ratio , risk difference , etc.) estimate is shrinking to virtually zero; or from a Bayesian perspective, your credible interval is collapsing to a single point. In other words, the posterior distribution becomes a point estimate. So in theory a deterministic value as opposed to a random variable. Is it assuming that each of the 1 billion are identical with respect to their outcomes and treatment only, but differ with respect to the covariates? Or is it assuming the individual is a summary measure of the 1 billion? My instinct is that the notion of the 1 billion is entertaining the fact we may draw many times without having a case where we have a lack of samples. I.e., small sample sizes result in more unstable estimates. I agree with your intuition. See also the side note on Pg. 9: Technically, when $i$ refers to a specific individual, such as Zeus, $Y_i^a$ is not a random variable because we are assuming that individual counterfactual outcomes are deterministic. In addition on Pg. 14 they note: However, for pedagogic reasons, we will continue to largely ignore random error until Chapter 10. Specifically, we will assume that counterfactual outcomes are deterministic So all of the slightly confusing language (around 1 observation representing 1 billion, etc.) is for simplification to ignore "random error" in these estimates and focus the attention on systematic biases (errors) due to confounding, selection, and measurement.
