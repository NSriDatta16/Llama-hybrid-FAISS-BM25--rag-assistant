[site]: crossvalidated
[post_id]: 260911
[parent_id]: 260885
[tags]: 
The diagnosis in @Andrey Kolyadin's answer is basically correct, so here is some other ideas as to what you can do. Following up from the code in the question, noting that the absurdly large standard errors of the fitted coefficients is an indication that the quadratic approximation of the log likelihood function used in computing those standard errors is very bad. Search this site for Hauck-Donner-effect. As a replacement for those standard errors we can use likelihood profiling, which computes (approximate) confidence intervals directly from the log likelihood function: library(MASS) confint(fit) Waiting for profiling to be done... 2.5 % 97.5 % (Intercept) -210.112 NA treatmentC NA 274.9591 treatmentD NA 601.8675 There were 30 warnings (use warnings() to see them) (the warnings is about fitted probabilities being numerically zero or one). Note that the NA (NotAvailable) given as lower confidence limits indicates that the procedure was unable to find a lower limit, reasonable because the lower limits are $-\infty$ ! This is because the true maximum likelihood estimators in those two cases are just $-\infty$. So extremely wide and not very useful confidence intervals, consistent with the immense standard errors. As for diagnosing such separation problems, there is the useful R package safeBinaryRegression . See my answer here Why does logistic regression become unstable when classes are well-separated? for an example of its use. One way of "solving" this problem, obtaining somewhat more reasonable point estimators and intervals (or se) is using some form of regularization, infusing into the estimation procedure some prior information that the coefficients are really finite. One way of doing that is via a bayes method, which we will illustrate using bayesglm from the arm package. library(arm) Loading required package: Matrix Loading required package: lme4 arm (Version 1.9-3, built: 2016-11-21) Working directory is C:/Users/halvorsk/Documents fit.b |z|) (Intercept) 3.536 1.235 2.863 0.004193 ** treatmentC -4.443 1.333 -3.333 0.000859 *** treatmentD -6.733 2.229 -3.021 0.002522 ** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 55.637 on 40 degrees of freedom Residual deviance: 19.060 on 38 degrees of freedom AIC: 25.06 Number of Fisher Scoring iterations: 23 which gives much more reasonable estimates and standard errors. EDIT The OP asks about the prior distributions used by this bayesian method. That is a very reasonable question! First, the R help pages obtained by typing ?bayesglm is useful here. The prior is a "weakly informative prior", t-distributions (defaul is 1 df, that is, the Cauchy distribution) with a scale parameter (known) set to 10 for the intercept and somewhat smaller for the other parameters (This is done after scaling predictor variables). For more on this see Gelman's paper http://www.stat.columbia.edu/~gelman/research/published/priors11.pdf
