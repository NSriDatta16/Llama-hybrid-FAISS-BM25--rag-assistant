[site]: crossvalidated
[post_id]: 321846
[parent_id]: 321687
[tags]: 
To add to @juampa's answer, The big difference between bagging and validation techniques is that bagging averages models (or predictions of an ensemble of models) in order to reduce the variance the prediction is subject to while resampling validation such as cross validation and out-of-bootstrap validation evaluate a number of surrogate models assuming that they are equivalent (i.e. a good surrogate) for the actual model in question which is trained on the whole data set. Bagging uses bootstrapped subsets (i.e. drawing with replacement of the original data set) of training data to generate such an ensemble but you can also use ensembles that are produced by drawing without replacement, i.e. cross validation: Beleites, C. & Salzer, R.: Assessing and improving the stability of chemometric models in small sample size situations, Anal Bioanal Chem, 390, 1261-1271 (2008). DOI: 10.1007/s00216-007-1818-6 Whether an ensemble model can be better than single models depends entirely on what the dominant "problem" of the single model is. If it is variance (overfitting, random error, unstable predictions), then ensemble prediction can help. If the problem is bias (systematic error, underfitting, stable but wrong predictions), pretty much all models of the ensemble will give the same prediction and the ensemble prediction is just as wrong. Both out-of-bootstrap and iterated/repeated cross validation allow to measure the stability of predictions by comparing predictions for the same input (test) data by a number of different surrogate models. These surrogate models differ in that they were trained on slightly different data sets, that can be described as exchanging a few training cases between any two of the surrogate models for other training cases. As for the assumption of independence @juampa mentions, there are different topics behind this: All these techniques typically assume that the splitting of the data achieves independence between the cases. I.e. if there are repeated measurements of the same case in the data, they are all in or all out of a particular training or test set. This is a crucial assumption that allows us to assume the observed performance generalizes to unknown cases of the same population the data comes from. It is typically up to you to ensure the splitting is done in a way that achieves this independence of cases: without intimate knowledge about the data it is not possible to make sure of such an independent splitting. Such a splitting into independent cases is needed both by resampling validation and ensemble models if they are to be useful for predicting independent cases. There is another independence assumption that is sometimes but not always made for validation results (or rather, during their interpretation): For some tasks such as the general comparison of algorithms, the surrogate models in a resampling validation are sometimes treated as if they were independent trials of the algorithm. Which is quite obviously not the case, as the models share most of their training data (with the exception of a single 2-fold split). This assumption is needed in order to generalize the findings on this data set to a data set of the given characteristics. On the other hand, if the task is to establish the performance of the model obtained from the data set at hand (which is e.g. actually to be used for prediction), then the surrogate models are assumed to be equivalent, if not perfectly to the model in question (the well-known slight pessimistic bias of resampling validation) then at least among themselves. This point of view means that the training data of the different surrogate models are not assumed to be independent but quite the opposite: they are assumed to be almost equal (which is what the resampling process actually produces).
