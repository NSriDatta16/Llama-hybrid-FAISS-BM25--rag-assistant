[site]: crossvalidated
[post_id]: 143474
[parent_id]: 143473
[tags]: 
The kernel trick implies mapping to a higher dimensional feature space. Typically, performing dimensionality reduction first doesn't really help much, except for some minor reductions in computation time. Dimensionality reduction might help in making a problem easier to understand, but what do visualized dimensions really mean after applying PCA? Physical interpretation is already largely lost at that point anyway. Which kernel to use depends on the problem. In general the RBF kernel tends to work well: $$\kappa(\mathbf{u},\mathbf{v}) = \exp(-\gamma \|\mathbf{u}-\mathbf{v}\|^2).$$ When using this kernel, you still have to find a suitable value for $\gamma \in \mathbb{R}^+_0$, which is typically part of the hyperparameter optimization. A specific value of $\gamma$ can be evaluated via techniques like cross-validation.
