[site]: crossvalidated
[post_id]: 453760
[parent_id]: 449395
[tags]: 
It is probably best to assess the raters' agreement per question separately. You could use any chance-adjusted index of categorical agreement for both type of question; you'd just have to decide how to treat the "unknown" option (e.g., as a third nominal option, as a third ordinal option between yes and no, or as missing data). If, for some reason, you really need to combine the reliability across the questions, then you can calculate the index per question and then average them. If all of these questions are meant to be measuring the same exact thing in 27 different ways, then you could look into a generalizability (G) study implemented as a logistic mixed effects model (i.e., nesting responses within raters and questions). But that seems unlikely and probably overkill. If you are using R, you can look at the following packages: irr , irrCAC , or agreement .
