[site]: crossvalidated
[post_id]: 551748
[parent_id]: 551581
[tags]: 
Well no, but with some modification, yes. You cannot use $y^2$ to predict $y$ , because your model will make every coefficient $0$ and simply predict $y$ as $\sqrt{y^2}$ . However, you can use other $y^2$ to predict $y$ . Let me explain, if you have a time series you can use $y_{t-1}$ to predict $y_t$ . In your case, if you know your variable depends on another one (which may not be time), you can use the target of another similar data point to the input to predict that target. In order to make it more precise, I'll add a method for doing what I explained. Take your input, apply $K$ -nearest neighbours based on the atributes and select $K$ neighbours. Then, add the targets of those $K$ neighbours as input to your model. When predicting test you just need to select the nearest training neighbours. Â¿Will this solve your problem? Probably not. Ascending linear residuals in linear models means that the hypothesis for linear models to apply are wrong, and therefore the model doesn't work. Here, there are no hypothesis for a MLP to work, but it is an indicator that the architecture doesn't have the complexity to fit the data. I don't know the trick you mention, but I do know that modifying the input with some transformation may be enough for it to work properly. Since you are using a MLP this could mean changing the architecture (or adding more data which always helps).
