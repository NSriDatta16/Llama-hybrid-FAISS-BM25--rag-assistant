[site]: datascience
[post_id]: 101862
[parent_id]: 
[tags]: 
Cosine similarity between sentence embeddings is always positive

I have a list of documents and I am looking for a) duplicates; b) documents that are very similar. To do so, I proceed as follows: Embed the documents using paraphrase-xlm-r-multilingual-v1 . Calculate the cosine similarity between the vector embeddings (code below). All the cosine similarity values I get are between 0 and 1. Why is that? Shouldn't I also have negative cos similarity values? The sentence embeddings have both positive and negative elements. num_docs = np.array(sentence_embedding).shape[0] cos_sim = np.zeros([num_docs, num_docs]) for ii in range(num_docs): for jj in range(num_docs): if ii != jj: cos_sim[ii, jj] = np.dot(sentence_embedding[ii], sentence_embedding[jj].T)/(norm(sentence_embedding[ii])*norm(sentence_embedding[jj]))
