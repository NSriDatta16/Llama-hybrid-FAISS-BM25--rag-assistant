[site]: crossvalidated
[post_id]: 284484
[parent_id]: 284472
[tags]: 
They CAN be the same, and some recent work shows that it can be beneficial (see https://arxiv.org/pdf/1608.05859.pdf ) Now two reasons why they are not the same: Word and context vocabularies can be different, for example when using positional contexts: I_-2 walked_-1 the dog_1 today_2. The is the input word, and the other four words are the context words. Thus the word matrix is $|V| \times d$ ($|V|$ is vocab size, $d$ is embedding dim), but the context matrix is $4|V| \times d$ if we use context windows of size 2. Skip-gram was conceived in the context of neural networks as a simplified neural network. The input to the network is the observed word's embedding, a $d$ dimensional vector, and there is directly fed to the network's output layer. Each neuron in the output layer has $d$ weights for each of the input words $d$ components. And so we refer this to as context vectors or context embedding, but they're really just output layer weights in the extremely simplified neural network that Skip-gram uses. Hope that helps!
