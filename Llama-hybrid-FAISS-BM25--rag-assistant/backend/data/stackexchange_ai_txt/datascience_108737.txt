[site]: datascience
[post_id]: 108737
[parent_id]: 108734
[tags]: 
In your context, a $1$ minute granularity is too high because introduces a lot of noise due to volatility and choppiness, but this is not a valid reason for algorithms to not be able to deal with such, or bigger, objects. If you choose your strategy, which is perfectly legitimate, it will reduce (as wanted) the dimensionality of your time series data, but will lose the consecutio temporum of your data (i.e., the order matters in time series analysis). Observe that, you can still obtain a time series, if you extract a feature for each, say, $30$ minutes (e.g., the average value) and if you do not treat them separately; following your example, for $12$ hours data, you will obtain a $12 \cdot \frac{60}{30} = 24$ points time series, where each point is described by several features (e.g., the average, the minimum, etc.). If, on the other hand, you dig deeper in your application domain, given a lower granularity, you can obtain smaller objects (w.r.t. the length of the original $720$ points time series), which is simply raw, non-featured data. I would try with the latter case before trying to perform feature extraction.
