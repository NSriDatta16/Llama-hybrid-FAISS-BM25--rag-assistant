[site]: crossvalidated
[post_id]: 186787
[parent_id]: 183236
[tags]: 
PCA and K-means do different things. PCA is used for dimensionality reduction / feature selection / representation learning e.g. when the feature space contains too many irrelevant or redundant features. The aim is to find the intrinsic dimensionality of the data. Here's a two dimensional example that can be generalized to higher dimensional spaces. The dataset has two features, $x$ and $y$, every circle is a data point. In the image $v1$ has a larger magnitude than $v2$. These are the Eigenvectors. The dimension of the data is reduced from two dimensions to one dimension (not much choice in this case) and this is done by projecting on the direction of the $v2$ vector (after a rotation where $v2$ becomes parallel or perpendicular to one of the axes). This is because $v2$ is orthogonal to the direction of largest variance. One way to think of it, is minimal loss of information. (There is still a loss since one coordinate axis is lost). K-means is a clustering algorithm that returns the natural grouping of data points, based on their similarity. It's a special case of Gaussian Mixture Models . In the image below the dataset has three dimensions. It can be seen from the 3D plot on the left that the $X$ dimension can be 'dropped' without losing much information. PCA is used to project the data onto two dimensions. In the figure to the left, the projection plane is also shown. Then, K-means can be used on the projected data to label the different groups, in the figure on the right, coded with different colors. PCA or other dimensionality reduction techniques are used before both unsupervised or supervised methods in machine learning. In addition to the reasons outlined by you and the ones I mentioned above, it is also used for visualization purposes (projection to 2D or 3D from higher dimensions). As to the article, I don't believe there is any connection, PCA has no information regarding the natural grouping of data and operates on the entire data, not subsets (groups). If some groups might be explained by one eigenvector ( just because that particular cluster is spread along that direction ) is just a coincidence and shouldn't be taken as a general rule. "PCA aims at compressing the T features whereas clustering aims at compressing the N data-points." Indeed, compression is an intuitive way to think about PCA. However, in K-means, to describe each point relative to it's cluster you still need at least the same amount of information (e.g. dimensions) $x_i = d( \mu_i, \delta_i) $, where $d$ is the distance and $\delta_i$ is stored instead of $x_i$. And you also need to store the $\mu_i$ to know what the delta is relative to. You can of course store $d$ and $i$ however you will be unable to retrieve the actual information in the data. Clustering adds information really. I think of it as splitting the data into natural groups (that don't have to necessarily be disjoint) without knowing what the label for each group means (well, until you look at the data within the groups).
