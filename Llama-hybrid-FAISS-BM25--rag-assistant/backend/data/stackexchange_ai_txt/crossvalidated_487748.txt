[site]: crossvalidated
[post_id]: 487748
[parent_id]: 409301
[tags]: 
With the advent of statistical learning techniques, people are talking a lot about prediction error, while in classical statistics, one is focusing on parameter estimation error. Exactly. This difference can be properly understood only if we realize and keep in mind that the scope of model like regression , first of all linear regression as your true model suggest, can be different (read here: Regression: Causation vs Prediction vs Description ). If your goal is prediction, as usual in supervised/predictive machine learning , you have to minimize the prediction error; parameters value per se not matters therefore endogeneity is not the core issue. At the other side if your goal is description or causal inference you have to focus on parameters estimation. For example in econometrics the usual focus is (or was) in causal inference (conflated with description if we follow the argument suggested in previous link), then endogeneity is treated as the main issue. In this literature prediction is treated as secondary problem, or ad hoc one in time series context (ARMA models for example). In most case is given the impression that if endogeneity go away, as consequence, the best prediction/forecasting model is achieved too. If this thing was true, the two minimization problem that you write above would be equivalent. However this is not true, infact in prediction/forecasting endogeneity are not the main problem while overfitting is (read here: Endogeneity in forecasting ) In order to understand this distinction the bias-variance tradeoff is the crucial point. Infact at the start of most machine learning books this topic is exhaustively treated and overfitting problem come as consequence. Indeed in most generalistic econometric books the bias-variance tradeoff is completely forget, for overfitting problem the same is true or, at best, it is vaguely treated. I started to study topic like those we treat here from econometrics side and when I realized this fact I remained badly surprised. The article that underscore at best this problem is probably: To Explain or to Predict â€“ Shmueli (2010). Read here ( Minimizing bias in explanatory modeling, why? (Galit Shmueli's "To Explain or to Predict") ) In other terms, if a model minimizes estimation error, does it necessarily minimize prediction error under the assumption of a linear model? No, definitely not . For prediction scope, more precisely in term of Expected Prediction Error , the "wrong model" (incorrectly specified regression) can be better then the "right one" (correctly specified regression). Obviously this fact is irrelevant if, as in causal inference, parameters is the core of analysis. In the article is given an example that involve an underspecified model. I used this argument here ( Are inconsistent estimators ever preferable? ). The proof is in the appendix of the article but the main issue are write down also in this strongly related question ( Paradox in model selection (AIC, BIC, to explain or to predict?) ). Warning: if the true model is noiseless or the amount of data we have go to infinity, therefore never in practice, the bias-variance tradeoff disappear and the two minimization problem become equivalent. This discussion is related: Minimizing bias in explanatory modeling, why? (Galit Shmueli's "To Explain or to Predict")
