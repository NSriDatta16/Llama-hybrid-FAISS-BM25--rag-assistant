[site]: crossvalidated
[post_id]: 123504
[parent_id]: 
[tags]: 
Likely mean of a multinomial distribution with dirichlet prior

I am working to create a Bayesian non-parametric estimate of the mean of a distribution given a distribution of observations. Ultimately I'd like to get to a credibility interval of the likely mean of the distribution given observed data. This particular example is of prices of orders. Normally I can use simulation to get around having a deep understanding of the math but in this case something interesting is happening: I've noticed the credibility interval for my estimated mean will always be greater than the true mean. This is due to the preponderance of "uninformative" prior data. How do I fix this so that I can have a true credibility interval like I am expecting? As an example: Say I have hypotheses that the value of an order can be anywhere between \$1 to \$10. Then some simple python for this set up might be: hypotheses = np.array([1,2,3,4,5,6,7,8,9,10]) observations = np.array([1,1000,1,1,1,1,1,1,1,1]) sampled_observations = numpy.random.dirichlet(observations) the_mean = (sampled_observations*hypotheses).sum() In this example the_mean will never be my actual value and my credible interval will never include the true mean. This is a chart of the distribution of 1,500 samples from that code: It's easy to see the credibility interval would be something like \$2.02 to \$2.06. Of course the more initial priors I entertain the worse this problem becomes for example what if I chose \$100 as my largest bucket and assigned priors in the same way? Is there a method for discounting the effect of my prior data or a better prior to choose? Background: In my real problem I've chosen to bin every penny between \$0 and \$10 and have an average price in my test data around \$2. Which means I have hundreds of bins of very small size that are skewing my data and causing a very poor estimate. Any pointers would be much appreciated. Thank you!
