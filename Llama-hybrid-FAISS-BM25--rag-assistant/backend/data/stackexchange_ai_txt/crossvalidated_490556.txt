[site]: crossvalidated
[post_id]: 490556
[parent_id]: 490458
[tags]: 
Fisher information increases The Fisher information (which is not the property of an estimator) scales with the size of the sample. See for instance here: https://en.m.wikipedia.org/wiki/Fisher_information#Discrepancy_in_definition , if the data are i.i.d. the difference between two versions is simply a factor of $n$ , the number of data points in the sample. The Fisher information of a sample of size $n$ (with i.i.d. measurements) is the Fisher information of a single measurement times $n$ . Efficient estimate precision increases You write any random variable ... and that contains Fisher information about a parameter of that population So if you talk about an efficient estimate (an estimate with a precision that equals the Fisher information) then: yes the precision will increase with increasing sample size. Similarly, any estimator whose efficiency has some non-zero minimal bound for all $n$ $$e(T_n) = \frac{1}{Var(T_n) \times n \times \mathcal{I}(\theta)} \geq e_{min} >0$$ (where $\mathcal{I}(\theta)$ is the information of a single measurement and $n\mathcal{I}(\theta)$ of $n$ measurements) will have $Var(T_n) \to 0$ for increasing $n$ . Other estimates can do anything But note that there are many non-efficient estimators/statistics that do not scale with increasing sample size. Pathological estimator A well-known example is the sample mean of a Cauchy distribution as an estimator for the location parameter, which remains the same for increasing sample size (and I believe there are also examples where the variance of the sample mean even increases for larger sample size). Oracle estimator If you do not like the example with the Cauchy distribution, because it is a pathological distribution, then you can consider this estimator $$\hat \theta_n = 42$$ This is an estimator that can be used for the parameter Î¸ of a non-pathological distribution and does not improve (increase in precision) when we increase n. (I agree that it is an example that makes little practical sense, but it indicates that maybe you need to be more precise about the definition of 'estimator'). Stupid estimator You could argue that this oracle estimator $\hat{\theta}_n = 42$ does not contain information (and in your edit you write about estimators that contain information), in that case, you can use this stupid estimator $$\hat{\theta}_n = \min\lbrace x_1, x_2, \dots, x_n \rbrace (n+1)$$ to estimate the parameter of a continuous uniform distribution between $0$ and $\theta$ . The distribution of $\min\lbrace x_1, x_2, \dots, x_n \rbrace/\theta$ follows a beta distribution $Beta(1,n)$ , and so we can easily compute the mean and variance of the estimate based on the mean and variance of the beta distribution. $$\begin{array}{rcl} E[\hat{\theta}_n] &=& \theta \\ Var[\hat{\theta}_n] &=& \theta^2 \frac{n}{(n+2)} \end{array}$$ So the variance of this unbiased estimator will grow towards $\theta^2$ for increasing sample size. Obviously, these examples are all silly non-pragmatic estimators. But, that is because of the issue that you are looking for. You are looking for estimators that do not work well with increasing sample size, and therefore you get silly estimators as examples. See also: https://en.wikipedia.org/wiki/Consistent_estimator
