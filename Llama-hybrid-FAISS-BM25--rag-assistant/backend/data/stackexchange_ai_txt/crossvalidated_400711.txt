[site]: crossvalidated
[post_id]: 400711
[parent_id]: 397556
[tags]: 
It is very difficult to compare algorithms in a very general manner. Specific situations may favor one algorithm over another, strong higher order interactions may favor tree based algorithms or neural networks over models that require us to specify these, very large datasets may favor neural networks/complex trees, having the right prior information may favor a Bayesian approach, image data (and various other types of data such as natural language) may well be the natural domain of neural networks today, in audio data a cleverly crafted hidden Markov models may beat neural networks (esp. with a limited amount of data), various small choices (e.g. data augmentation, good pre-processing affects different models differently) small datasets with features in an already well transformed form may heavily favor simple models such as traditional regression type models and so on. There's also other considerations such as explainability/interpretability, runtime (see e.g. xgboost vs. LightGBM), resource needs (memory which can be an issue with various random forest implementations, need for GPUs and so on) and what you are trying to optimize. On the last point, you may favor extremely complex models, if you care about every last decimal point of performance and are sure that what you want to predict for comes from the same data generating mechanism as your training data (this is often the case on a kaggle competition), while you may favor a simpler model, where you need to collect less data in the future and understand deviations from the data generating mechanism better for a real-life situation. A recent paper concluded that existing benchmarks are not diverse enough to truly benchmark methods - although there are of course useful sets of standard benchmarks for which people are developing tailored neural networks, which is perhaps for very specific tasks the main benchmark for NN approach that I can think of. I assume the question was asked in a more general sense, but I suspect there simply is no very general answer that some approach always works best. Another interesting recent publication looked at whether logistic regression was competitive with more modern ML methods in practical applications where different approaches were tried. On average there was not much of a difference, for which there could of course be many explanations (amongst them that LR is often a good approach, that not all approaches were tuned equally well and so on).
