[site]: crossvalidated
[post_id]: 325874
[parent_id]: 325504
[tags]: 
I did some simulations (under $H_0$: a fair coin $p=0.5$). I limited the number of flips to $n_\max$ because the raw stopping time $T$ has such a huge tail that sometimes the computer wouldn't stop in a reasonable time. Anyway it's more realistic with a limit. The experiment is: do some first $100$ flips to initialize do a z-test with $\alpha=5$%. If it is significant or you flipped more than $n_\max$, stop. otherwise flip one more time and go back to the previous step The false discovery rate (type I error) differs a lot from $\alpha$: For $n_\max=1000$ : 26% For $n_\max=10000$ : 40% However something happens because of the optional stopping theorem : when "meta-anlyszing" several of these experiments (simply merging them into one big flipping session and do a z-test), the bias on false discovery rate tends to disappear: It might sound a bit paradoxical: we have many experiments where 26% are falsely significant on average, but the global experiment still has the right type I error of 5%. And the global estimator $\hat p$ is still (asymptotically) unbiased. It can be explained by the fact that the longest experiments, having more weight, are the least favourable to rejecting $H_0$. As a conclusion, optional stopping can cause a strong bias for tests on each single experiment, but the bias tends to disappear when doing several experiments.
