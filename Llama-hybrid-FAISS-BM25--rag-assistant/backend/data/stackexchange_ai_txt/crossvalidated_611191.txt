[site]: crossvalidated
[post_id]: 611191
[parent_id]: 440832
[tags]: 
In a perfect world, you would be able to fit a half-million observations in memory and run the neural network on everything. This has the advantage of using a representative data set and letting the neural network learn from reality instead of fiction. In this imperfect world, you might be right that you do not lose much performance. If you are able to build a useful model, that counts for a lot. If you do reduce the number of non-fraud training cases to make the data fit in memory, use representative data when you evaluate your model. If you have a $95/5$ class imbalance in reality, you should have about that kind of class imbalance when you evaluate your model on unseen data. If the performance of the model trained on undersampled data only performs well in the fictional setting where the classes are rather balanced, there is limited reason to expect good performance in production. Our Demetri Pananos discusses here what to do if you fiddle with the class ratio, and the King paper discussed here is related. Depending on your task, you might be more or less interested in the fraud probabilities (I would think that would be interesting for this task), and giving an incorrect prior probability of fraud by downsampling the majority class to fit the data into memory leads to an incorrect posterior probability of fraud, though that Pananos answer I linked discusses a possible remedy. That is, depending on what you value in predictions, you might find out that your model trained on downsampled data does not perform as well as you thought.
