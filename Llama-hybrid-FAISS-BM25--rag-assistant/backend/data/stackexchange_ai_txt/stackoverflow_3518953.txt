[site]: stackoverflow
[post_id]: 3518953
[parent_id]: 3518914
[tags]: 
I should make a note that if there's a will, then there is a way . That being said, I thought about what you've asked previously and here are some simple things I came up with: simple naive checks might be user-agent filtering and checking. You can find a list of common crawler user agents here: http://www.useragentstring.com/pages/Crawlerlist/ you can always display your data in flash, though I do not recommend it. use a captcha Other than that, I'm not really sure if there's anything else you can do but I would be interested in seeing the answers as well. EDIT: Google does something interesting where if you're looking for SSNs, after the 50th page or so, they will captcha. It begs the question to see whether or not you can intelligently time the amount a user spends on your page or if you want to introduce pagination into the equation, the time a user spends on one page. Using the information that we previously assumed, it is possible to put a time limit before another HTTP request is sent. At that point, it might be beneficial to "randomly" generate a captcha. What I mean by this, is that maybe one HTTP request will go through fine, but the next one will require a captcha. You can switch those up as you please.
