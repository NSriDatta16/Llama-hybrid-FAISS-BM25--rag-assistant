[site]: crossvalidated
[post_id]: 532468
[parent_id]: 
[tags]: 
How is inpainting for self-supervised pre-training of convolutional neural networks usually done?

I read a nice blog post on self-supervised learning and computer vision , which suggests in-painting (amongst other ideas) as a possible self-supervised task for a neural network to "adapt" to a domain. E.g. if I want to do something with x-ray images, I might take a neural network trained on ImageNet and before I do supervised training on my small sample of labelled x-rays, I do some self-supervised training on - ideally, a large set of x-rays - before then doing supervised training on my task of interest. This sounded quite interesting to me and I wondered about doing this for an actual use case. The actual paper that is cited in the blog is a bit older (around the time ResNet and U-Net first came out, I think), so might not reflect current "established" standard approaches. It's cited almost 3000 times per Google Scholar, but a lot of the citing papers seem to be new model proposals and so on. So, I'm wondering what practitioners that really do this typically do. My first reaction was to use some modern neural network (e.g. EfficientNet V2) as the backbone for a U-Net (?), the input would be an image with a reasonably large region cut-out randomly and the output would be the unchanged image (with any augmentation such as rotation, flipping, random-cropping, blurring etc. applied exactly identially to input and output). The inpainting-paper added a second term to the loss: an adverserial loss by having a discriminator that tried to detect whether the inpainted image or the original image was the real image. That appears to have worked a lot better than just a L2 loss, but perhaps people have by now come up with better loss functions to avoid a GAN (I assume just cross-entropy when you consider each image channel as being (0,1) would not help that much, but perhaps there's some other clever ideas?). So, my questions (but feel free to give any other advice) revolve around: Architecture(s) to use Loss function to use/need for GAN Finally, in architecture shown in the inpainting-paper, there's a fully connected layer in the middle. I wonder whether you'd keep that layer for down-stream tasks, or not.
