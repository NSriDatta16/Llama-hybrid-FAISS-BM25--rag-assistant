[site]: crossvalidated
[post_id]: 554047
[parent_id]: 
[tags]: 
Estimate the number of ranges that overlap

I am a CS major and I am dealing with this problem about finding the join estimation for overlapping ranges. Imagine that you have two tables (sets) with ranges ex. integers ([2,10), [5, 20)) and you only want to intersect those rows of the table that have overlapping intervals. Example: Table A: [2,10), [5,20), [12,14) Table B: [12,16), [22,30) A && B = {[5,20), [12,16), [22,30)} My goal is to estimate the number of elements (cardinality) of the overlap A&&B. Usually developers are using histograms for non continuous data like integers and floats, but there has not been really an implementation for ranges in most of the programs. What would be the best way to do this?My first intuition was to somehow create two histograms for A and B and match the bins that have the same intervals and make and sum the average of the matching bins. Then I found this very interesting research paper which explains about the selectivity of ranges, but not about joins: Range Selectivity Estimation for Continuous Attributes This article gave me an idea that I can somehow do Kernel Density. I don't know much about the Kernel density, but I was thinking about getting the integrals of table A and B and use the intersection which would be the join estimation of the ranges (I might be super wrong). My third idea was to see if there is a very long range that overlaps with most of the other ranges from the other table. That way I could estimate a very high number of ranges that overlap. But it is not a very scientific approach. From a statistics standpoint, what would be the simplest way to implement this?
