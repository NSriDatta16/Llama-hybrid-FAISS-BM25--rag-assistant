[site]: crossvalidated
[post_id]: 193045
[parent_id]: 191842
[tags]: 
There is one tree created, which is definitely overfitting the data. The specified minsplit essentially creates a tree that categorizes each terminal node into either all "present" or all "absent". rpart will not prune the tree for you, but can provide cross-validation for you to select the best subtree (i.e. select the complexity parameter $\alpha$). The best tree is any subset of the initial tree; below are a few options: library(rpart.plot) prp(tree,extra=1) #Initial tree with 16 splits prp(prune(tree,cp=0.042),extra=1) #Subtree with 10 splits prp(prune(tree,cp=0.068),extra=1) #Subtree with 5 splits prp(prune(tree,cp=0.14),extra=1) #Subtree with 1 split To decide which subtree is best, we have to perform cross-validation. First we have to determine the possible $\alpha$'s that would yield a subtree (from the initial tree). Then we divide the data into 10 groups and build 10 trees with the 'leave one group out' approach using a possible $\alpha$ to prune the tree. The left out group can determine which $\alpha$ worked best. The technical details can be seen in the rpart vignette The final tree that is returned is still the initial tree. You must use the prune function using the cross-validation plot to choose the best subtree. For this dataset, I don't think CART fits the data that well. If you perform 81-fold cross-validation (i.e. leave one observation out), you'll see five splits seems like the best tree. If you're looking for a model with better prediction accuracy, perhaps you should consider building a random forest.
