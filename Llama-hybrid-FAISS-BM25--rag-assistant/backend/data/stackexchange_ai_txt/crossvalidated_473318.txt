[site]: crossvalidated
[post_id]: 473318
[parent_id]: 
[tags]: 
No difference in the distribution of data across classes

I'm having a problem in my Deep Learning model where I encounter a class imbalance and there's no virtual difference between the data for the two classes or they have an identical if not similar distribution. One way I tried to solve the class imbalance is through resampling to create an evenly distributed training set. However, my model only predicts one class. This is an NLP problem and I am not sure of using techniques like XGBoost to NLP / Deep Learning problems. I've read this paper that recommends SMOTE and from this post , am interested in Random Forest, but am not sure of its applicability to NLP. From this paper , there are some algorithms I could use and a different loss function I could provide to the algorithm, but I am using a pre-trained model and so am constrained. My main problem however, is that there may not be an observable difference between the data in the two classes. In general, can ML / DL work if there is no identifiable difference between the data across the two classes? Is there any way I can try and manually find some difference between the data for the two classes?
