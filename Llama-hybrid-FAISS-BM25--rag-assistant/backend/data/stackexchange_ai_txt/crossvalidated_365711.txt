[site]: crossvalidated
[post_id]: 365711
[parent_id]: 365604
[tags]: 
Put properly, we don't actually test if an alternative hypothesis is true. It is often described that way, but as far as basic statistics goes, that is incorrect. We actually test whether there is, or is not, enough evidence to accept some "new"/"novel"/"not-default" hypothesis H. We do this by Taking into account what we know (Bayesian style if appropriate); Choosing a test we think is applicable to the data and hypothesis we are probing, and Stipulating a point which will be deemed "significant". The significance level This last item, the "significamce level", is often a source of confusion. What we actually say is, "If the hypothesis is wrong, then how exceptional would our results be?" So, suppose we set a significance level of 0.1% (P=0.001), what we are saying is: "If our hypothesis is wrong, we just got a 1 in 1000 result by pure chance. That's so unlikely that we conclude the hypothesis is probably correct." So you can "draw the line" where you like - for some research such as particle physics, you'd want 2 separate (independent) experiments both with a significance level of 1 in some millions, before concluding the hypothesis is probably correct. For a rigged dice game, a 1 in 3 level might be enough to persuade you not to play that game :) But either way it is crucial to pick the level beforehand, otherwise you're probably just make a self serving statement using 'whatever level you like".
