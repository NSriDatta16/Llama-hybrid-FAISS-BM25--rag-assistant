[site]: crossvalidated
[post_id]: 463923
[parent_id]: 160439
[tags]: 
This is a wonderful question, because it illustrates a common misunderstanding of statistics. Contrary to the sloppy way people tend to talk about statistical testing, there is no such thing as a "statistically difference" between two values --- the values are either different, or they are the same. Statistical tests arise only when we are comparing an unknown thing to a known thing. In that context, when users of statistical tests refer to a "statistically significant difference" this is actually a (highly misleading) shorthand for statistically significant evidence of a non-zero difference ---i.e., the quantifier of "statistical significance" applies to the evidence of a non-zero difference, not the magnitude of the difference. That is, we compare an unknown variable to a known stipulated value, and we use statistical methods to see if there is sufficient evidence to reject the null hypothesis that they are the same. Your question is also not entirely clear because you refer to the predicted value as being the same as the expected value. Assuming you are using some kind of parametric model, these things are different; the expected value is a function of the unknown model parameters, and so it is unknown, whereas the predicted value is a function of parameter estimates, so it is known. If you are referring to an unknown expected value then you can conduct a statistical test to see if this is equal to (or different from) a stipulated value. If you are referring to a known predicted value then there is nothing to test --- either it is equal to the observed value or it isn't. In the latter case, what you can do is to form a prediction interval and then see if the observed value fell within the interval or not. If you construct an interval with a high coverage probability and the observed value falls outside that interval, then this suggests that there is something defective in your prediction method.
