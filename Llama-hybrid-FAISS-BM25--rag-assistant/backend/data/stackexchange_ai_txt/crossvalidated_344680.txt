[site]: crossvalidated
[post_id]: 344680
[parent_id]: 
[tags]: 
How to structure this problem in a bayesian paradigm? (Updating the posterior?)

I have some code that randomly generates a number from m_min=10 to m_max=100 (apologies if this nomenclature is unconventional) for a total of (m_max - m_min) + 1 = 91 positions = n_positions each with equal probability of being picked (i.e. a uniform distribution in the form of a probability vector). I'm running a function using each number and receiving a score, if it's better than the previous score I want to update the weights for each position. Instead of discretely updating weights, I want to also update the weights of the neighboring positions for a smooth curve. My thoughts for implementing was to do the following: (1) update the weights for position_k ; (2) fit curve to a beta-distribution (this part gets tricky); and then (3) when I draw another number from m_min to m_max , call it query_num , I will divide that by n_positions (i.e. total number of positions) to get my x for the calculating the probability of drawing that value from the beta-distribution . I believe this is a bayesian problem since I am using a uniform distribution as my prior and updating the posterior to get a beta-distribution (if this is incorrect, please inform me). I am clearly missing a fundamental step in the logic. My ultimate goal is to "update" the posterior each time the condition == True by adding mass to the regions around query_num , recompute the probabilities based on a beta distribution , and, in this case, plot the transformation. I can't use a simple stats.beta.fit because I don't have draws from the distribution only probabilities for of the x values that I am updating. import sys import matplotlib.pyplot as plt import seaborn as sns import numpy as np from scipy import stats from scipy.optimize import curve_fit # Defaults m_min = 10 m_max = 100 # Uniform probabilities n_positions = m_max - m_min + 1 positions = np.arange(m_min, m_max + 1) x = np.linspace(0,1,n_positions) # Prior probabilities probs_num = np.ones(n_positions)/n_positions # Each value is 0.01098901 # Placeholder function def func(x): # This isn't actually what I'm doing but just to get the code to work for this example return 15 Versions: Python v3.6.4, NumPy v1.14.2, SciPy v1.0.1
