[site]: crossvalidated
[post_id]: 578960
[parent_id]: 79168
[tags]: 
Like all good things, it depends. After applying PCA, you can again represent it in the same dimensional space, but, this time, the first principal component will contain the most variance, the second will contain the second most variance direction and so on. So you can eliminate the last few principal components, as they will not cause a lot of loss of data, and you can compress the data. Right? Yes. Here's the thing: the same logic applies for the parameters of each principal components themselves. Just as many of the principal components can be eliminated without losing very much information, you can eliminate many parts of each principal component without losing very much information. Why should we be efficient in one dimension but not another? Eliminate all of the unnecessary parameters. If you think about the total number of parameters needed to describe your data, sparse PCA is very often just more efficient PCA. Modern sparse PCA methods are a big reason modern video and image compression is so good and doesn't look like sh-t. Sparse PCA is also easier to interpret; each principal component can actually represent a descriptive factor of some kind (for example: personality traits in psychological studies). And finally, sparse PCA (like sparse regression) is more likely to generalize better out of sample. If I have some new data generated from the same source, sparse principal components are more likely to summarize the new data better (less is more when it comes to generalizing). Of course, that depends on your measure of efficiency. If you're using PCA to visualize multi-dimensional data in two or three dimensions, sparse PCA would be less efficient (each principal component would describe less variance than simple PCA). Tl;dr simple PCA = handful of complicated principal components, sparse PCA = slightly more simple principal components and less parameters in total.
