[site]: crossvalidated
[post_id]: 144089
[parent_id]: 130637
[tags]: 
I second the suggestion of @Aksakal: If measurement error is seen by the analyst as potentially important, it can and should be modeled explicitly as part of the data-generating process. I see several considerations that argue against the introduction of a generic correction factor based on, e.g., the age of the data set. First, age may be a very poor proxy for the degree of data deterioration. The technology of duplication, compression, and conservation, and the degree of effort and care that went into verifying correct transcription, are apparently the important factors. Some ancient texts (e.g., The Bible) have been conserved for centuries with apparently zero degradation. Your VHS example, while legitimate, is actually unusual, in that each duplication event always introduces error, and there are not easy ways to check for and correct for transcription errors -- if one uses cheap, widely available technologies for duplication and storage. I expect that one lower the degree of introduced errors substantially, through investments in more expensive systems. This last point is more general: data conservation and propagation are economic activities. The quality of transmission depends greatly on the resources deployed. These choices will in turn depend on the perceived importance of the data to whoever is doing the duplicating and transmitting. Economic considerations apply to the analyst, as well. There are always more factors you can take into account when doing your analysis. Under what conditions will data transcription errors be substantial enough, and important enough, that they are worth taking into account? My hunch is: such conditions are not common. Moreover, if potential data degradation is seen as important enough to account for it in your analysis, then it is probably important enough to make the effort to model the process explicitly, rather than insert a generic "correction" step. Finally, there is no need to develop such a generic correction factor de novo . There exists already a substantial body of statistical theory and practice for analyzing data sets for which measurement error is seen as important. In sum: it's an interesting thought. But I don't think it should spur any changes in analytic practice.
