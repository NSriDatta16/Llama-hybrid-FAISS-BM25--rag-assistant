[site]: crossvalidated
[post_id]: 402645
[parent_id]: 402618
[tags]: 
Here's a paper dedicated to this very question: Parascandolo and Virtanen (2016) . Taming the waves: sine as activation function in deep neural networks. Some key points from the paper: Sinusoidal activation functions have been largely ignored, and are considered difficult to train. They review past work that has used sinusoidal activation functions. Most of this is earlier work (before the modern 'deep learning' boom). But, there are a couple more recent papers. The periodic nature of sinusoidal activation functions can give rise to a 'rippling' cost function with bad local minima, which may make training difficult. The problem may not be so bad when the data is dominated by low-frequency components (which is expected for many real-world datasets). Learning is easier in this regime, but is sensitive to how network parameters are initialized. They show that networks with sinusoidal activation functions can perform reasonably well on a couple real-world datasets. But, after training, the networks don't really use the periodic nature of the activation function. Rather, they only use the central region near zero, where the sinusoid is nearly equivalent to the more traditional $\tanh$ activation function. They trained recurrent networks on a synthetic task where periodic structure is expected to be helpful. Networks learn faster and are more accurate using $\sin$ compared to $\tanh$ activation functions. But, the difference is bigger for vanilla RNNs than LSTMs. Here's another relevant paper: Ramachandran, Zoph, Le (2017) . Searching for Activation Functions. They performed a large-scale, automatic search over activation functions to find new variants that perform well. Some of the activation functions they discovered use sinusoidal components (but they're not pure sinusoids--they also tend to have a monotonic component). The paper doesn't discuss these variants much, except to say that they're an interesting future research direction.
