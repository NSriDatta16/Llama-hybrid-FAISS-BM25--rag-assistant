[site]: crossvalidated
[post_id]: 106236
[parent_id]: 
[tags]: 
Bayesian Perceptron - how can I generate many different perceptrons?

I am going to implement the Bayesian version of a perceptron that I read in the Statistical Mechanics of learning, by Engel-Van Den Broeck. The idea to improve the performance is to use many Gibbs perceptrons J to define a golden one using their center of mass $J^\beta$. Formally: $$J^\beta = \lim_{m\rightarrow \infty} \sqrt N \frac{\sum^M_{i=1}J^{(i)}}{\sqrt{(\sum^M_{i=1}J^{(i)})^2}}.$$ The problem is: how should I compute these many $J$? Should I just use many different initial condition? Should I use different $J$ taken during different epochs of the perceptron learning (this in particular is my thought)? Should I perturb data? Should I perturb one specific $J$ in many different random ways?
