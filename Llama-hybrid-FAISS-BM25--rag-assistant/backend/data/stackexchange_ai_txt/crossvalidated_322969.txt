[site]: crossvalidated
[post_id]: 322969
[parent_id]: 321851
[tags]: 
My favourite one is the Kraft inequality. Theorem: For any description method $C$ for finite alphabet $A = \{1,\dots, m\}$, the code word lengths $L_C(1), \dots, L_C(2)$ must satisfy the inequality $\sum_{x \in A} 2 ^{-L_C(x)} \leq 1$. This inequality relates compression with probability densities : given a code, the length of an outcome represented by that code is the negative log probability of a model identified by the code. Further, the no free lunch theorem for machine learning has a less well known sibling the no hyper compression theorem, which states that not all sequences can be compressed.
