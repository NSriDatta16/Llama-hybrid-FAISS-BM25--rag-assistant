[site]: crossvalidated
[post_id]: 56965
[parent_id]: 56950
[tags]: 
My old neural network toolbox (I mostly use kernel machines these days) used L1 regularisation to prune away redundant weights and hidden units, and also had skip-layer connections. This has the advantage that if the problem is essentially linear, the hidden units tend to get pruned and you are left with a linear model, which clearly tells you that the problem is linear. As sashkello (+1) suggests, MLPs are universal approximators, so skip layer connections won't improve results in the limit of infinite data and an infinite number of hidden units (but when do we ever approach that limit?). The real advantage is that it makes estimating good values for the weights easier if the network architecture is well matched to the problem, and you may be able to use a smaller network and obtain better generalisation performance. However, as with most neural network questions, generally the only way to find out if it will be helpful or harmful for a particular dataset is to try it and see (using a reliable performance evaluation procedure).
