[site]: crossvalidated
[post_id]: 432588
[parent_id]: 
[tags]: 
Why not to normalize the image to have zero mean in reinforcement learning?

I learned in deep learning classes, cs231n for example, that it is better to have input data to have negative values so as to have negative gradients. Although negative gradients may also be achieved deep neural net, this, to my best knowledge, is still recommended in compute vision tasks. But I've recently seen many reinforcement learning algorithms on Atari, such as OpenAI's baselines , that do not zero center the input data --- they simply divide the input images by 255. What makes this difference? Why don't they zero center the data? Which is is better in practice?
