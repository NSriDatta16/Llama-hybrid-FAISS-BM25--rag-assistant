[site]: crossvalidated
[post_id]: 455702
[parent_id]: 455481
[tags]: 
Acquisition functions are not about a specific surrogate model. They can be calculated for many of them given that the output of a surrogate model is not a single prediction $\hat{y}(x)$ , but a probability distribution $\hat{p}(x)$ . In case, of the Gaussian process regression, we assume that the output is Gaussian distribution. So it is sufficient to specify mean $\mu(x) = \hat{y}(x)$ and variance $\sigma^2(x)$ at a specific $x$ to specify this distribution. Moreover, many acquisition functions have an analytical form. For example, UCB has the form $\mu(x) + \beta \sigma(x)$ . As you note this probability distribution is a natural extension. The problem of random forest is that there is no good extension of this model that allows obtaining a probability distribution instead of a single prediction value or even obtain reliable uncertainty estimate for prediction. Moreover, the random forest model predicts only values that are inside the range of values presented in the training sample, while during Bayesian optimization we want to find points with output values that are outside this range. Finally, if you propose an acquisition function, then you should find an optimum of it. For Random Forest both uncertainty estimates and mean values are piecewise constant. Thus, to find an optimum of an acquisition function like UCB, you should be able to adopt a global optimization algorithm with zero gradients almost everywhere. Seem like a computationally expensive problem. These three issues lead to the complexity of a Random forest application in Bayesian optimization. You should start this project at your own risk with questionable benefits in the end. There should be some options for Bayesian optimization using random forests as surrogate models, but it seems that they are outside of the scope of the main research and industrial practices
