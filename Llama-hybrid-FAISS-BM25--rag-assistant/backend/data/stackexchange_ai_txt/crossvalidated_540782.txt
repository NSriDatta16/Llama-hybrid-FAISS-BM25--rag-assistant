[site]: crossvalidated
[post_id]: 540782
[parent_id]: 540767
[tags]: 
A Decision Tree essentially works by splitting the train set into partitions. A prediction is obtained by selecting the partition which your unseen data point belongs to, then the output value is calculated as the mean of all the dependent variable values in that partition. My guess is that the partitions you get never contain all 0 (or all 100) values, so when you calculate the mean you get slightly higher (or lower) values than expected. Moreover, Random Forest trains a lot of Decision Trees and then takes the mean of the predictions of each tree, so here you have even more chances that the output deviates from extreme values. You could try to train trees with only one (or a few) sample per leaf so that you are more likely to reach the extremes, but I would suggest not to do it because the model would overfit a lot. I have never heard of that Random Forest with Beta distribution approach you linked so I don't know whether it would work well. However, you could try to use a more simple linear model instead of a tree-based one. There is something called Beta Regression (try to look at this since you use R), but I don't know anything about it so I'm not sure whether it would help. Otherwise you can try to look at Generalized Linear Models. They are a general class of linear models which can fit different distributions of a target variable. Unfortunately I have never used them in practice, so I will just leave it as a suggestion. Maybe someone more expert than me can give you better advice on this topic.
