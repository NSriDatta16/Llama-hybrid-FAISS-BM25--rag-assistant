[site]: crossvalidated
[post_id]: 215049
[parent_id]: 
[tags]: 
What is the origin of the rule of thumb for the number of samples needed machine learning?

I've heard that as a rule of thumb, the number of samples needed for a machine learning algorithm to get accurate results is ten times the number of degrees of freedom. For example, classifying an 8x8 bitmap as being a digit between 0 to 9 would require (8 * 8) * 10 = 640 training examples for accuracy. If this is correct, where is the origin of this rule of thumb (sources would be appreciated). If it is not correct, what is an accurate method of determining the minimum number of training examples needed for accurate predictions? (again, citations would be appreciated) Many Thanks, Michael
