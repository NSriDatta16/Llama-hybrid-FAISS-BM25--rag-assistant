[site]: crossvalidated
[post_id]: 614333
[parent_id]: 
[tags]: 
Huge overfit on prediction model -due to data with low predictive power or can this be fixed? (Python)

I am not used to working with machine learning models, and are currently sitting with an issue i hope you can help me with. I am sitting with a multi classification problem, where i try to predict whether the bitcoin will either ’increase’, ’decrease’ or ’neutral’ (neutral is a change in price within -0.5 % to 0.5%). The data I use are based on titles and subtitles from news articles from various media sources. This is definitely a hard machine learning task and i am not expecting a high accuracy. I’m doing data preprocessing where I doing a number of NLP-tasks such as removing stopwords, stemming etc. I am creating a countvectorizer with n-grams which results in a sparse matrix, that i train the model on. I end up with 1.095 instances. I am training the models Random-forest, KNN, Logistic regression and MLP. I am using gridsearchCV on all the different models. Most of the models i train are overfitted on the training-data and are getting a very bad result on our test data. I am trying to prevent the models from overfitting, but no matter which hyperparameters i use, it seems to be overfitting. Can i lower the accuracy on the model when predicting on training data? Or can i simply conclude, that the overfitting is caused by the data and the target variable not having any correlation? Or would this rather be shown through low results on both the training and the test data?
