[site]: crossvalidated
[post_id]: 604848
[parent_id]: 604846
[tags]: 
This is a textbook case of HARKing: Hypothesizing After Results are Known ( Kerr, 1998 ). Not disclosing this step is indeed more than borderline unethical. The authors should prominently note that they did this preprocessing step, because it makes all their results less reliable. Essentially, all statistics (i.e., parameters estimated from data, like the accuracy of a classifier or many other parameters) are random variables. Evaluating statistics across multiple methods, models or samples and then cherry-picking the largest or most significant ones means that we tend to pick the ones that just randomly in this particular dataset turned out to be large. It may well be that in a similar new dataset, this statistic is lower and another one higher, simply due to sampling variability. Any estimate driven by this kind of cherry-picking will be biased, i.e., it will systematically over-estimate the true parameter value. And confidence intervals that are calculated without taking this additional "snooping" step into account will be too low, i.e., they will have lower than their nominal coverage. Correcting for this "path through the garden of forking paths" (take a look at Gelman's paper) is highly nontrivial. (Also, accuracy is a seriously flawed evaluation measure .)
