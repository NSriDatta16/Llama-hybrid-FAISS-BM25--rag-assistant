[site]: datascience
[post_id]: 39089
[parent_id]: 39068
[tags]: 
Let's consider a simple hypothetical autoencoder with only a single hidden layer. The goal is to constrain the number of neurons in the hidden layer such that we can compress the information from the input into this smaller feature set without losing significant information. This is formalized as a transformation from $\mathbb{R}^n$ to $\mathbb{R}^m$ where $m . The trivial solution with a denoising autoencoder or a simple autoencoder is to maximize the number of hidden layers such that information is not lost during the compression process. A denoising autoencoder does not protect you from that. The compressed information at the output of the hidden layer should be rich and pertinent to the input distribution. However, with a simple autoencoder it happens that the neurons simply learn to copy the input values to the output without learning a meaningful representation in the compressed layer. This is where denoising autoencoders are useful. We can add noise to the inputs, such that when an output is produced we can be more confident that the hidden layer correctly encoded the input. What you want to do when choosing the number of hidden layers for an autoencoder is to set yourself an acceptable error threshold for the reconstruction. Then find the minimum number of neurons needed to attain that threshold.
