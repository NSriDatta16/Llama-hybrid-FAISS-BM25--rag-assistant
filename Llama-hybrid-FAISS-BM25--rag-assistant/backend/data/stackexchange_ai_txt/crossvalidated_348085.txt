[site]: crossvalidated
[post_id]: 348085
[parent_id]: 
[tags]: 
Can the Vapnik-Chervonenkis inequality be generalised to non-zero-one error functions?

I learned about the VC inequality of the bound of difference between training errors and generalisation errors. The two places ( Stanford CS229 notes and Wikipedia ) where I read about the theorem both use the 0-1 loss function. Since one normally uses cross-entropy instead of accuracy as loss functions in say basic learning models like linear or logistic regressions, I wonder if the VC Inequality can be generalised to other loss functions, or does the 0-1 loss function case serve as a "universal case" and imply the inequality holds for a range of other loss functions as well?
