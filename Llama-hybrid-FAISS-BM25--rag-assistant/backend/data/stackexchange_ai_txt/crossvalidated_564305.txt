[site]: crossvalidated
[post_id]: 564305
[parent_id]: 228344
[tags]: 
Let us say that the predictor X perfectly predicts the label Y such that Y = 1 if X >= c and Y = 0 if X The classifier that gives zero classification error is the boundary placed at $X=c$ . But, that zero error boundary is not obtained by linear regression. The problem is that even though you might have a perfect seperation of the data, you will still need to decide on the exact place of the decision boundary between the classes. Linear regression will produce a fit and this fit is not precise. For instance if the fit is a logistic regression or least discriminant analysis and we choose the point where the prediction is $p=0.5$ then this point might be slightly above or below $c$ and will actually almost never be exactly equal to $c$ . However, if the variable $X$ is discrete then it is possible to obtain the classifier $X=c$ .
