[site]: datascience
[post_id]: 95144
[parent_id]: 95134
[tags]: 
How can I convert this matrix to a single vector that contains an encoded representation of the full sentence? It's achieved by applying a softmax function to the attention scores and using these probabilities to derive a weighted sum of the encoder hidden states. More specifically: In Seq2Seq , for example, with $N$ words let $h_1,...,h_N \in R^h$ be the encoder hidden states, $s_t \in R^h$ the decoder hidden states at timestep $t$ , then the attention scores are $$e^t = [s_t^Th_1,...,s_t^Th_N] \in R^N.$$ Applying a softmax function gives the attention distribution $$\alpha^t = softmax(e^t) \in R^N.$$ And, finally, using these as weights in a weighted sum results in the attention output $$a_t = \sum_{i=1}^N \alpha_i^t h_i \in R^h.$$ By doing so, you reduce the sequence of $N$ words to a single vector of dimension $h$ . This is also well explained in Stanford's CS224N: Natural Language Processing with Deep Learning - Lecture 8 (around 1h 2mins).
