[site]: datascience
[post_id]: 35742
[parent_id]: 33916
[tags]: 
I have found a solution. My suspicions were correct and there are ways to solve both issues. For the issue about only knowing the definite yesses and not knowing both yes and no there is a type of machine learning specifically for this called PU-learning . There are libraries for this built off scikit-learn . This type of machine learning learns the binary classification problem based off training data where the target is either labelled yes or not at all. It then attempts to find a decision boundary by classifying the unlabeled data as yes or no. This does not totally solve the issue of where the decision boundary should be for who will be a good person to offer a sale but I can do this by tuning my ROC curve under business constraints. The second issue was about data and feature engineering. Basically it is a bad idea to do data prep differently for the different classes. Labeling my yes cases by the purchase event and then looking back at activity is not good. I need to choose random times to get the distributions when you predict correct. If you do not your probability is inaccurate. I also need to have many timestamps per user to sample different states of that user. This means that you will get some events labelled as yes for a user and some unlabelled for the same user. One would then expect that the boundary is drawn with some of the unlabelled timestamps on either side of the decision boundary for each user. This would then suppress the problem of contamination by the features which are only good from a time independent perspective. Explicitly the method is to select all (or a subset of) sign-in events. You build your features based on that event in whatever way you see fit. The labeling is done by joining on the purchase events to look forward and see if they purchase in the near future. The definition of 'near' needs to be tuned but you can likely make an educated guess from a plot. If there is a purchase event then that sign in event is labelled yes otherwise left unlabelled. The PU-learning does the rest.
