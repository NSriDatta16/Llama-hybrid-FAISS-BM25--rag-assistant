[site]: datascience
[post_id]: 75473
[parent_id]: 73409
[tags]: 
In order to improve the accuracy, you first have to understand the current issues. Without having looked into the dataset, I assume the following problems occur: 1) Your neural network is too simple in its structure. 2) Number of images is rather small 3) Number of images per classes is not balanced Once these issues are well-understood, you have a chance of improving the accuracy. Let's discuss these issues in detail. Issue 1) (Your neural network is too simple in its structure): Your defined network is a feedforward CNN. It applies 32 different convolutions, followed by the relu function to the input image. As a result you obtain 32 so-called feature-maps. The interpretation is that each feature map is attracted to some image feature which is necessary to perform the classification task. For example one such convolution could correspond to a Prewitt-Filter, which outputs horizontal or vertical edges present in the image. Another filter map could correspond to detecting round corners, arcs and so on. The final fully connected layer than classifies the image, based on these generated features. In a deep CNN, it has been observered that the feature maps in the early layers correspond to low-level features (corners, lines, ...), while the feature maps in the later layer are attracted to high-level features (compositions of low-level features, e.g. squares, circles, faces,...) as each convolution is applied to the "image" generated by the previous layer (which is not the input image, but a feature map). Accordingly, it is essential to have multiple layers in order to achieve this hierarchical construction of features, which turned out to be very effective (and is similar to how the human brain works in some areas). Therefore, one direction to improve accuracy is to to add more layers of convolutions (and poolings). While this turned out to be already sufficient for many task in order to achieve very high accuracies, this is probably not sufficient in your case, due to problem 2) Issue 2) Number of images is rather small: Theoretically, If you use a deep CNN, there should be some weights which provide very good accuracies. This probably will not work in practice in your case mainly due to: all known optimization methods, which are used to obtain (train) the weights of the CNN, are not delivering a global optimal solution. even in the case we had the global optimal solution and even, if we had 100% accuracy on the train set, it does not imply the CNN performs good on unseen images. It is verly likely that you would obtain weights, which lead to bad test accuracies, even if you obtain the correct classifications on the training set. Mainly, the reason is that very likely, the learned weights do not correspond to plausible "explanations" for the general task (classification if traffic signs) but are some "shortcuts". Example: Let's assume all images of the stop sign have been taken using camera A and all images of a "keep right" sign have been taken using camera B. Now if the sensor if camera A has some defect, e.g. the center pixel is always black, optimizing the weights will thus likely lead to the classification of stop sign by looking at the center pixel. However,once you have an image of a stop sign in the test set taken from another camera this will lead to a misclassification. In general, if you are using a deep CNN, there are many variables that need to be optimized from the training data (among others, the kernel weights of the convolutions). In order to avoid classifications based on unplausible reasonings, it is crucial to have many training images such that no "shortcuts" are possible. In the example, it would be crucial to take images from different cameras such that hopefully the "explanations" or the features learned by the CNN are not some special characteristics of the training data but focus on a plausible model (traffic sign is an octagon with red background and text..). In case that you do not have enough training data, one option is to use transfer learning, that means we use a pre-trained CNN model (trained on another task, but with a hugh dataset), such that the features in the early layers are attractive to low-level features. Then you use the training only the adapt the high-level features to your specific classification task. This is less prone to short-cuts as there are already meaningful features in the early layers with are feed forward in the network. Another option is to use another ML model, e.g. Random Forest, which does not need so many training samples as the features are not learned. In this case, features have to be defined or learned in another way. Issue 3) There are several ways to handle class imbalances for Random Forest and Neural network. Most of them affect how you sample your data.
