[site]: crossvalidated
[post_id]: 189665
[parent_id]: 189652
[tags]: 
Let me tell you the story of how I learned the importance of normalization. I was trying to classify a handwritten digits data (it is a simple task of classifying features extracted from images of hand-written digits) with Neural Networks as an assignment for a Machine Learning course. Just like anyone else, I started with a Neural Network library/tool, fed it with the data and started playing with the parameters. I tried changing number of layers, the number of neurons and various activation functions. None of them yielded expected results (accuracy around 0.9). The Culprit? The scaling factor (s) in the activation function = $\frac{s}{1+e^{-s.x}}$-1. If the parameter s is not set, the activation function will either activate every input or nullify every input in every iteration. Which obviously led to unexpected values for model parameters. My point is, it is not easy to set s when the input x is varying over large values. As some of the other answers have already pointed it out, the "good practice" as to whether to normalize the data or not depends on the data, model, and application. By normalizing, you are actually throwing away some information about the data such as the absolute maximum and minimum values. So, there is no rule of thumb.
