[site]: crossvalidated
[post_id]: 559773
[parent_id]: 135438
[tags]: 
Besides the excellent discussion above, there are other reasons why holdout samples were not and still are not frequently used in statistics. Holding out data from discovery and model fitting is inefficient and wasteful of information, and in some cases we have analytic results that efficiently provide what you need, e.g., an estimate of likely future model performance. The simplest example is residual $\sigma^2$ in linear models where we've long had an estimate that is unbiased by overfitting. $R^{2}_{\mathrm{addj}}$ is another example. Then there is resampling which is an invention from the field of statistics. 100 repeats of 10-fold cross-validation is a very unbiased and 9/10 efficient procedure. The bootstrap is an almost unbiased and fully efficient procedure. Both of these estimate the likely future performance on observations from the same stream of observations used to build the model. This discussion touches on independent sample validation vs. rigorous internal validation a la resampling. And it is a common mistake to label estimated performance on a holdout sample as "external validation" which it's usually not. This is discussed here . Bayesian modeling thinks of this in yet another way where prior information starts the process and the parameter "estimates" (actually distributions) are trusted based on that information and there is no overfitting per se.
