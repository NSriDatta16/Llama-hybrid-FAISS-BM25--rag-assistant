[site]: crossvalidated
[post_id]: 223783
[parent_id]: 
[tags]: 
Is having more features definitely equal to having a higher chance of overfitting?

I am doing a EEG data classification problem. Currently I am using the ANOVA test to help me select K best input features (with K a parameter to tune) and feeding the selected features into a logistic regression classifier using l2 regularization with the penalizing factor tuned. As for the data, it is a [m x p] 2D matrix obtained from a specific time point of the recording, with m trials and p electrodes. When I chose K to be equal to the total number of electrodes, intuitively and indeed I got a higher decoding accuracy as compared to only selecting a subset of the electrodes(accuracy obtained by a nested 5-fold cross-validation, with the inner cv to perform a grid search of the K and the penalizing factor and the outer cv to test the classifier). Before We jump in to the conclusion that using all the electrodes is having a higher chance of overfitting, I wonder is this really the case? First since the accuracy is obatined by averaging the 5 accuracies obtained through the 5-fold cross-validation, wouldn't the overfitting phenomenon be penalized? Since I am using penalization in the logistic regression, wouldn't the overfitting of the parameter be remediated during the training process? I guess I am having the following problems because I am still not understanding the meaning of overfitting, can anyone give me a hand?
