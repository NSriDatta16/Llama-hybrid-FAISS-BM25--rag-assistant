[site]: crossvalidated
[post_id]: 106334
[parent_id]: 
[tags]: 
Cost function of neural network is non-convex?

The cost function of neural network is $J(W,b)$, and it is claimed to be non-convex . I don't quite understand why it's that way, since as I see that it's quite similar to the cost function of logistic regression, right? If it is non-convex, so the 2nd order derivative $\frac{\partial J}{\partial W} UPDATE Thanks to the answers below as well as @gung's comment, I got your point, if there's no hidden layers at all, it's convex, just like logistic regression. But if there's hidden layers, by permuting the nodes in the hidden layers as well as the weights in subsequent connections, we could have multiple solutions of the weights resulting to the same loss. Now more questions, 1) There're multiple local minima, and some of them should be of the same value, since they're corresponding to some nodes and weights permutations, right? 2) If the nodes and weights won't be permuted at all, then it's convex, right? And the minima will be the global minima. If so, the answer to 1) is, all those local minima will be of the same value, correct?
