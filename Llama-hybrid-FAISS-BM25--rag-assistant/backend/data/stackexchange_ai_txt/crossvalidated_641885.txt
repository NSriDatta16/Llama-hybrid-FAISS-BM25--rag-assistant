[site]: crossvalidated
[post_id]: 641885
[parent_id]: 
[tags]: 
simple ANN as a set of linear transformations

We cannot classify the points of the XOR problem with a single perceptron in the hidden layer. However, we can achieve this by using two perceptrons in the hidden layer and one for the output layer, without using an activation function. The input layer consists of two perceptrons. With one hidden layer and one output layer, there are two weight matrices: the first matrix $A$ is $2*2$ , and the second matrix $B$ is $1*2$ . Let's consider $A=[[1,1],[0,1]]$ and $B=[-2,2]$ . During forward propagation, we perform a linear transformation of the $X$ vector by matrix $A$ , resulting in a new vector in $RÂ²$ . This new vector then undergoes a linear transformation by matrix $B$ to produce the output. This means that all we need to do is $B(AX)$ . Here is where my problem arises: Upon researching, it becomes clear that matrix multiplication is associative, meaning $B(AX)=(BA)X$ ,also the combination of two linear transformation is a linear transformation. The combination of $A$ and $B$ results in another matrix denoted as $K$ with a shape of 1x2. Therefore, $B(AX)=KX$ . The key here is to learn $K$ ; initially, we achieve it by working on individual linear transformations, but we can make the artificial neural network (ANN) do this in one step by choosing only one perceptron in the hidden layer. However, this approach is incorrect. So, where is my problem?!!".
