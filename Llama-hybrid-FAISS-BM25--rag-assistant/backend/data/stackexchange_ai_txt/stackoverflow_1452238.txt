[site]: stackoverflow
[post_id]: 1452238
[parent_id]: 1451894
[tags]: 
Here's what I would do after I checked the robots.txt file to make sure it's fine to scrap the article and parsed the document as an XML tree: Make sure the article is not broken into many pages. If it is, 'print view', 'single page' or 'mobile view' links may help to bring it to single page . Of course, don't bother if you only want the beginning of the article. Find the main content frame. To do that, I would count the amount of information in every tag. Now, what we're looking is a node that is big but consists of many small subnodes. Now I would try to filter out any noise inside the content frame. Well, the websites I read don't put any crap there, only useful images, but you do need to kill anything that has inline javascript and any external links. Optionally, flatten that into plain text (that is, go into the tree and open all elements; block elements create a new paragraph). Guess the header . It's usually something with h1 , h2 or at least big font size, but you can simplify life by assuming that it somehow resembles the page title. Finally, find the authors (something with names and email), the copyright notice (try metadata or the word copyright ) and the site name. Assemble these somewhere together with the the link to original and state clearly it's probably fair use (or whatever legal doctrine you feel like applies to you.)
