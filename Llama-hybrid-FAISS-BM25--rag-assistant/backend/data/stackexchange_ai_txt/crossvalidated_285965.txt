[site]: crossvalidated
[post_id]: 285965
[parent_id]: 
[tags]: 
When training neural network, should we normal the loss function with respect to the number of items?

I see some implementation minimize, let's say, $\sum_{i=1}^n ||\hat{x}_i - x_i|| +\frac{1}{2}\lambda||w||^2$, I had also saw those that minimize $\frac{1}{n}\sum_{i=1}^n ||\hat{x}_i - x_i|| +\frac{1}{2}\lambda||w||^2$. Which way is preferable? I can see that they are mathematically equivalent if tuning $\lambda$ and assume we always use all the example in training. But I am not sure about batch training. A tensorflow version of this question would be, should we apply tf.reduce_mean or tf.reduce_sum when calculating the loss?
