[site]: crossvalidated
[post_id]: 542160
[parent_id]: 542117
[tags]: 
The difference is indeed significant, but it is hard to say without knowing the size of the dataset. I can think of two possible reasons why you might have such a big difference: Your dataset is not particularly big, and therefore when running on a 5-Fold validation you are only using 4/5 of the 80% of your dataset (which is 64%) which might be insufficient for your RF to perform well and/or Your dataset is not shuffled (idk how you are performing the train/test split nor the CV), and therefore issues very different performance based on the set it is training on Finally, I would suggest you try using the Out Of Bag error of Random Forests instead of the 5 fold CV. Basically this gives you an estimate of your TEST error (equivalent to a Cross Validation) by using the forecasts that your trees make on the data they were not trained on (because of the bootstrapping). This is easier to do than a 5 fold CV, it does not reduce the size of the sample, and does not take any extra time. This will give you an extra estimate that you can compare to your test and then validation set results. EDIT: I am also adding an extra information on how to cross-validate using the AUC, as it can be misleading: Appropriate way to get Cross Validated AUC
