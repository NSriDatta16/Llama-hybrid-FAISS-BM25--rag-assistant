[site]: crossvalidated
[post_id]: 432308
[parent_id]: 432300
[tags]: 
1.) Assuming my activation function is Sigmoid (say g(x)), having very large weights (x) will lead to small gradients and slow learning through Gradient Descent. If, x is close to 0, then my gradients will be very large. Can this lead to overstepping in Gradient Descent (akin to a large Learning Rate)? I am guessing this is not the case as the gradient is due to the nature of the cost function while the learning rate is a parameter we set. "If, x is close to 0, then my gradients will be very large" --> "very" is very relative here. It will be 0.25 if x=0. 2) If my activation function is ReLU, the slope will be constant for any x>0 and 0 for x You can still lose the signal with ReLUs because the gradient will be 0 for negative inputs. EDIT: Alright. I understand how the vanishing gradients occur. But how is it ever possible to get "exploding" gradients if my gradients are never >1? Consider the following neural network sketch: For example, the partial derivative of the loss of the weight $w_{1,1}^{1}$ , doesn't just rely on the derivative of the ReLU function. I.e., the partial derivative would be $$\begin{aligned} \frac{\partial l}{\partial w_{1,1}^{(1)}}=\frac{\partial l}{\partial o} \cdot \frac{\partial o}{\partial a_{1}^{(2)}} & \cdot \frac{\partial a_{1}^{(2)}}{\partial a_{1}^{(1)}} \cdot \frac{\partial a_{1}^{(1)}}{\partial w_{1,1}^{(1)}} \\ &+\frac{\partial l}{\partial o} \cdot \frac{\partial o}{\partial a_{2}^{(2)}} \cdot \frac{\partial a_{2}^{(2)}}{\partial a_{1}^{(1)}} \cdot \frac{\partial a_{1}^{(1)}}{\partial w_{1,1}^{(1)}} \end{aligned}$$
