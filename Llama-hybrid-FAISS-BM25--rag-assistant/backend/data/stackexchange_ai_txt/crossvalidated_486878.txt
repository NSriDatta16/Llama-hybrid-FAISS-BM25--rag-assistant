[site]: crossvalidated
[post_id]: 486878
[parent_id]: 
[tags]: 
Utility of the whole distribution (other than the mean) in Bayesian posterior predictive

In prediction task, when using Bayesian fashion of predictors, I think in most the cases, people just use posterior mean for each individual estimate. I wonder if there's any utility of the higher order information, e.g. variance of the posterior predictive, in any task? I can think of example where the whole distribution might be useful is that, making a false positive in the prediction is more costly than making a false negative. In this case, people might want to define some utility function, which use the whole distribution information. It would be great if people can point me more examples and references. Thanks a lot in advance! For a concrete example, suppose I perform a Bayesian linear regression, $\boldsymbol{\beta} \sim \pi(\cdot)$ , $y_i = \mathbf{x}_i^\top \boldsymbol{\beta} + \epsilon_i$ . With some particularly chosen prior, I get posterior of $\boldsymbol{\beta}$ , $\pi(\boldsymbol{\beta} | \mathbf{x}_{1:N}, y_{1:N})$ , with this posterior, I could get posterior predictive distribution for new data point $\mathbf{x}_{\text{new}}$ : $$p(y_\text{new} | \mathbf{x}_{\text{new}}, \mathbf{x}_{1 :N}, y_{1:N}) = \int p(y_\text{new} | \mathbf{x}_{\text{new}}, \boldsymbol{\beta}) \pi(\boldsymbol{\beta} | \mathbf{x}_{1:N}, y_{1:N}) d\boldsymbol{\beta} $$ So for this new data point, I get a posterior distribution. For a real example, this could be the predicted susceptability for some disease for a potential patient. I may want to make decision whether to further check the patient. I can think of two examples: 1. use the posterior mean 2. use some tail density. It's not clear to me which one is more principled to use.
