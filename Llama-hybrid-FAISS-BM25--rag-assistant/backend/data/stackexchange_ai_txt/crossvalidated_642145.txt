[site]: crossvalidated
[post_id]: 642145
[parent_id]: 431896
[tags]: 
The book, and the post above, are not quite correct. In general, bias is not approximation error, and variance is not estimation error. It is not even the case that one is a special case of the other, as is sometimes stated. The true relation is more subtle. The approximation error is in fact just one component of the bias, and, the estimation error contributes to both bias and variance. Bias = (Approximation Error) + (Estimation Bias) Variance = (Optimisation Error) + (Estimation Variance) And furthermore… Estimation error = (estimation bias) + (estimation variance) The optimisation error comes from the performance of the learning algorithm. The reason for two definitions is simply that they came from different research communities at different times. The BV is from statistics, and the AE from computational learning theory. The motivation for the AE was to define generalisation bounds as a function of model capacity and the size of the data. The primary advantage of the bias/variance terminology is that the terms can be estimated from data, which is not possible with approximation/estimation. A disadvantage is that the bias is clearly a flawed proxy for model capacity, since the estimation bias is in there too. To further complicate things, the estimation bias can be negative in some cases. In the very special case of unregularized linear regression, i.e., OLS, then the two decompositions are equivalent, but not in general. To understand fully requires more detail than is possible in this reply, but is explained in the paper linked below. “Bias/Variance is not the same as Approximation/Estimation” Transactions on Machine Learning Research, February 2024. https://openreview.net/pdf?id=4TnFbv16hK ——— Full disclosure: I’m an author.
