[site]: crossvalidated
[post_id]: 70365
[parent_id]: 70150
[tags]: 
Let me add a few points: first of all, hypothesis generation is an important part of science. And non-predictive (exploratory/descriptive) results can be published. IMHO the trouble is not per se that data exploration is used on a data set and only parts of those findings are published. The problems are not describing how much has been tried out then drawing conclusions as if the study were a validation study for some predictive model / a hypothesis testing study Science and method development are iterative processes in a far more general way than just hypothesis generation - testing - generating new hypotheses - testing .... IMHO it is a matter of professional judgment what kind of proper conduct is necessary at what stage (see example below). What I do: try to make people aware of the optimistic bias that results When I have a chance, I also show people how much of a difference that makes (feasible mostly with a lower level of the same problem, e.g. compare patient-independently validated data with internal performance estimates of hyper-parameter optimization routines, such as grid search for SVM paraters, "combined models" such as PCA-LDA, and so on. Not really feasible for the real data dredging, because so far, noone gave me the money to make a true replicate of a sensible sized study...) for papers that I'm coauthor of: insist on a discussion of the limitations of the conclusions. Make sure the conclusions are not formulated in a more general way than the study allows. Encourage co-workers to use their expert knowledge about the subject of the study and the process of data generation to decide how to treat the data instead of performing costly (in terms of the sample size you'd need to do that properly) optimization of model-"hyper"-parameters (such as what kind of pre-processing to use). in parallel: try to make people aware of how costly this optimization business is if done properly (whether this is called exploration or not is irrelevant, if done wrongly, it will have similar results like data dredging), e.g. Beleites, C. and Neugebauer, U. and Bocklitz, T. and Krafft, C. and Popp, J.: Sample size planning for classification models. Anal Chim Acta, 2013, 760, 25-33. DOI: 10.1016/j.aca.2012.11.007 accepted manuscript on arXiv: 1211.1323 Here's a study that finds this blind trying around also is often futile, e.g. J. Engel, J. Gerretzen, E. Szyma≈Ñska, J. J. Jansen, G. Downey, L. Blanchet, L.M.C. Buydens: Breaking with trends in pre-processing?, TrAC Trends in Analytical Chemistry, 2013, 50, 96-106. DOI: 10.1016/j.trac.2013.04.015 (they tried a large number of combinations of pre-processing steps and found that very few lead to better models than no pre-processing at all) Emphasise that I'm not torturing my data more than necessary: example : All preprocessing was decided exclusively using spectroscopic knowledge, and no data-driven preprocessing was performed. A follow-up paper using the same data as example for (different) theory development reads All pre-processing was decided by spectroscopic knowledge, no data-driven steps were included and no parameter optimization was performed. However, we checked that a PLS projection [45] of the spectra onto 25 latent variables as pre-processing for LR training did not lead to more than slight changes in the prediction (see supplementary figure S.2). Because meanwhile I was explicitly asked (on a conference by an editor of the journal CILS) to compare the models with PLS pre-processing. Take a practical point of view: E.g. in the astrocytoma study linked above, of course I still decided some points after looking at the data (such as what intensity threshold corresponds to measurements taken from outside the sample - which were then discarded). Other decisions I know to be uncritical (linear vs. quadratic baseline: my experience with that type of data suggests that this actually doesn't change much - which is also in perfect agreement with what Jasper Engel found on different data of similar type, so I wouldn't expect a large bias to come from deciding the type of baseline by looking at the data (the paper gives an argument why that is sensible). Based on the study we did, we can now say what should be tackled next and what should be changed. And because we are still in a comparatively early step of method development (looking at ex-vivo samples), it is not worth while to go through all the "homework" that will ultimately be needed before the method could be used in-vivo . E.g. at the present stage of the astrocytoma grading, resampling validation is a more sensible choice than external test set. I still emphasize that a truly external validation study will be needed at some point, because some performance characteristics can only be measured that way (e.g. the effects of instrument drift/proving that we can correct for these). But right now while we're still playing with ex-vivo samples and are solving other parts of the large problem (in the linked papers: how to deal with borderline cases), the gain in useful knowledge from a proper ex-vivo validation study is too low to be worth while the effort (IMHO: unless that were done in order to measure the bias due to data dredging). I once read an argument about statistical and reporting standards, and whether such should be decided to be necessary for a journal (don't remember which one) which convinced me: the idea expressed there was that there is no need for the editors to try agree on and enforce some standard (which will cause much futile discussion) because: who uses the proper techniques is usually very aware/proud of that and will (and should) therefore report in detail what was done. If a certain point (e.g. data dredging, validation not independent on patient level) is not clearly spelled out, the default assumption for reviewers/readers is that the study didn't adhere to the proper principles in that question (possibly because they didn't know better)
