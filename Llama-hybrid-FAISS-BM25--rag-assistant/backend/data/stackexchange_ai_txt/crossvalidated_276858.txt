[site]: crossvalidated
[post_id]: 276858
[parent_id]: 
[tags]: 
Binary Neural Network using only bit and integer operations

Are there neural networks that do not make use of floating point arithmetic, i.e. ones that only use binary or (small) integer values throughout their whole classification computation? I have found the following paper Binarized Neural Networks , in which all weights seem to be binary values. However, as far as I understand, during a classification it does seem to use floating point arithmetic (see Algorithm 5), where it executes a method called BatchNorm (it is a little unclear what exactly BatchNorm is or why I need it). Is my understanding here correct or does this construction indeed perform only bit operations (or operations on small integers)?
