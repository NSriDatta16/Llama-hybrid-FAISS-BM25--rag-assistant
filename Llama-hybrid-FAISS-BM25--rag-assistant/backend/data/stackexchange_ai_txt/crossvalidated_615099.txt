[site]: crossvalidated
[post_id]: 615099
[parent_id]: 
[tags]: 
Statistical comparison of two (log) probabilities

Using R, I built 2 logistic regression models (with outcome variable being depression status - present or absent) and used leave one out cross validation to obtain predicted values for the dataset. I then used the predict function to extract the linear probability for each observation in the data set. I've also calculated the "error" or residual for each row of data, that is, the absolute difference between the observed value and each model's prediction. I'd like to compare these two error values, to test which model did a better job at predicting the ground truth. For what I am testing, log probability are more appropriate than raw probability, because they "allow" for greater error when the probabilities are further from the ground truth. See, for example, the last 2 rows in the example data. As another example, consider two scenarios, in both of which the ground truth is 1 (depression is actually present). In the first scenario, model A's probability of picking the ground truth was .9 and model B's was .7. In the second scenario, model A's probability of picking the ground truth was .3 and model B's was .1. In both cases, the difference between the models is .2, but this difference matters less in scenario 2 (because both models were so far from the truth -- for a real world prediction, both 30% and 10% are bad if depression is actually present) and matters more in scenario 1 (because a result that says "70% likelihood depression is present" is much worse than "90% likelihood depression is present"). I want to use a statistical test to compare these log errors. Can I run a t-test to compare these columns (Log Model A and Log Model B)? Or would this be inappropriate? Example data: Observed Predict. A Predict. B Error A Error B Log Error A Log Error B 0 .55 .66 .55 .66 -.59 -.41 1 .59 .68 .41 .32 -.89 -1.14 0 .85 .79 .85 .79 -.17 -.24 0 .04 .02 .04 .02 -3.23 -3.72
