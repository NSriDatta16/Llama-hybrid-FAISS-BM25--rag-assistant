[site]: crossvalidated
[post_id]: 616321
[parent_id]: 616314
[tags]: 
This is definitely desirable to have, but not trivial to get. The way XGBoost, LightGBM, CatBoost et al. are created is with a single output at a leaf node in mind. When there's extra distributional parameters (e.g. shape for accelerated failure time models), these are hyperparameters. There's a few options with the way the package is, but these all involve multiple model fits (e.g. quantile regression , but it's hard to enforce that the quantiles stay ordered if it's not from a single model, fitting for different hyperparameter values and then taking the distribution to be based on how well the different hyperparameters fit the data etc.). In general, I think it's hard because of how boosting works: you fit a model, then fit another model to the residuals, again and again. While you tend to sample observations for each model you create, because of the way each model builds on the previous one, the left-out observations don't directly provide an estimate of out-of-bag error (like e.g. in random forest, which can more easily give prediction intervals). There's been data science competitions that targeted such an output (from whatever model you wanted) such as this one (see also discussions like these ). These offer sources of inspiration for how people approach this kind of problem.
