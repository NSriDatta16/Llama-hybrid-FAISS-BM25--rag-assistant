[site]: crossvalidated
[post_id]: 495311
[parent_id]: 
[tags]: 
How are activation functions calculated in quantized Neural Networks

I seem to be missing how activation functions are calculated in a fully integer quantized Neural network. I understand that when performing inference, the input tensor is scaled to the closest calculated uint8 as shown here . What I can't follow is how a relu or a sigmoid follow this scaling or how they get modified for this input. Here for example, a sigmoid roughly makes sense to get an input of between (-5,5), and the relu input for some similar range. I do not see anywhere how/if this gets scaled to match the input transformations. When I print the details of a quantized tensor I get this sort of output 'name': 'sequential/conv2d/Relu;sequential/conv2d/BiasAdd;sequential/conv2d/Conv2D;sequential/conv2d/BiasAdd/ReadVariableOp/resource', 'index': 14, 'shape': array([ 1, 26, 26, 12], dtype=int32), 'shape_signature': array([-1, 26, 26, 12], dtype=int32), 'dtype': , 'quantization': (0.007534660864621401, -128), 'quantization_parameters': {'scales': array([0.00753466], dtype=float32), 'zero_points': array([-128], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}} Which indicates that the layer that undergoes a Relu operation has a scaling factor, but I don't see how this is applied. Any thoughts? Thanks. P.
