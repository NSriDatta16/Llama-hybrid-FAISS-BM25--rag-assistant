[site]: crossvalidated
[post_id]: 547744
[parent_id]: 237152
[tags]: 
Why does this allow the training of deep networks, escaping network saturation at deep levels? We can treat a layer as a function, and adding a layer(with more parameters) leads to a new function with a larger hypothesis space. There are two methods to adding a layer, and for the generic method, we just add a layer and this would result in the spaces depicted on the left side where a larger space does not guarantee to get closer to the truth(optima, either local or global optima) than before adding it. However, if we add residual connections, it is like 'Taylor expansion' style parametrization as depicted on the right hand side. The more layers you add, the more approximate to the truth your parameters would possibly be. Thus, only if larger function classes contain the smaller ones are we guaranteed that increasing them strictly increases the expressive power of the network. For deep neural networks, if we can train the newly-added layer into an identity function f(x) = x, the new model will be as effective as the original model. As the new model may get a better solution to fit the training dataset, the added layer might make it easier to reduce training errors. At the heart of their proposed residual network (ResNet) is the idea that every additional layer should more easily contain the identity function as one of its elements. References Deep Learning - ResNet Dive into deep learning
