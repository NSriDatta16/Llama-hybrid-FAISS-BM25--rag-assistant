[site]: crossvalidated
[post_id]: 573237
[parent_id]: 
[tags]: 
Clarification on Bayesian ensembling

I was reading a paper https://arxiv.org/pdf/2007.06823.pdf and at the end of page 3 the author presents the technique called "ensembling" for the estimation of the expected outputs and the associated uncertainties. In particular he writes My question may be silly but i would like to have a confirmation: when he sums over the parameter theta_i, does that mean that the model is trained multiple time (one new training for each value of theta_i) or, since dropout randomly drop weights, it just means using multiple predictions y_i of the same value x_i obtained with the same trained model ? thanks
