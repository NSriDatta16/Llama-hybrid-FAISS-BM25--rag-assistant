[site]: datascience
[post_id]: 74214
[parent_id]: 47987
[tags]: 
The course talks about the convergence of GD, but is it really guaranteed to converge to the global min This course talked about a case of simple MSE which is a Convex function. For a convex function, "Yes" it is guaranteed. In other scenarios, it is not guaranteed. That's why we have a bunch of optimizers. In simple ML algorithms space, you will not need these. If you start Deep Learning. You will have to learn that. Andrew NG has a great Coursera course on Deep Learning too. Using Maths to get Minima/Maxima This will become next to impossible when you have 1000+ parm ( A Neural network, millions in case of CNN ). Loss function would become way too complex to achieve that task i.e. A complex function of Millions of variables.
