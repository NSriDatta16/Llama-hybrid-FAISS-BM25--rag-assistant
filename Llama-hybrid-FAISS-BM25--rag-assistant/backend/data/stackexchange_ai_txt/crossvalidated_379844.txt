[site]: crossvalidated
[post_id]: 379844
[parent_id]: 
[tags]: 
Time series seasonality: mismatch between season length and data collection frequency

I'm working with vegetation data (NDVI collected from MODIS) that is collected at 16-day intervals. I'm doing a time series analysis on a particular site to see how it changes over time. I've already lagged the data once, so, as I'm thinking of it, my data now shows the change in NDVI between each interval. Here's the plot for that: There is still seasonality, as can be seen with the ACF: I'd like to try and lag the data to remove the seasonality. Since this is vegetation data, I know that the length of a season is 365 days. However, my data is collected at 16 day intervals, and 16 doesn't divide evenly into 365 (365/16 = 22.8125). This means that I know that my season length (in terms of the data that I have) is 22.8125. However, I can't lag my data by 22.8125. If I were to just round and lag the data by 23, then it would probably work at first but become less effective as the number of data points increases. Is there any way that I can get around this and remove the seasonality?
