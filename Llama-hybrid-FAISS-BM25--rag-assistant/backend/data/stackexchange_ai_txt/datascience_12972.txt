[site]: datascience
[post_id]: 12972
[parent_id]: 9415
[tags]: 
Wow I'm surprised that no one has given this one a go. I'm very very late to the party but I'll attempt an answer just for information's sake. Before I attempt to answer your questions I'd like to say that any situation in which the number of features exceeds the number of observations by 50% there may be some instability in the results. What I'd recommend is to attempt a number of feature selection approaches and compare the results . Personally I'd start with a PCA to reduce the number of features such that n > p and go from there. But that's just me. 1) I'm not surprised by more estimates being zero at all. In the first lasso regression with this many features a large amount of these features are likely to be noise (no prediction power) meaning that any feature with a small capability of prediction would be selected in the first cut of variables. Running a secondary lasso regression might not be completely advisable (logically, can't really think of any theoretical issues at current) but it's likely that some of the stronger predictors are being uncovered on this second iteration . 2) The estimates in lasso regressions are quite difficult to interpret (apart from zero and non-zero values obviously), but what I'd recommend is to check whether you've scaled your features before running the lasso reg? As my understanding is that a lasso reg is a penalised version of the ridge regression, and the ridge regression needs the features to be scaled as otherwise features represented by larger numbers are given higher coefficients. If you have just ignore this, but if you want to interpret the estimates I'd switch to a Linear Reg/GLM not a lasso reg . Hopefully this helps.
