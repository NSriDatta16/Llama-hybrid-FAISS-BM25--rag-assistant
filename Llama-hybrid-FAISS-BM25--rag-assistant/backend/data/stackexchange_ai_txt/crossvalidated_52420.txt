[site]: crossvalidated
[post_id]: 52420
[parent_id]: 
[tags]: 
Estimated normal density is fat-tailed?

I wanted to check what is the relative error of an estimated N(0, 1) density function. By relative error I mean: $$\alpha(x) = E \bigg ( \frac{f(x|\hat{\mu}, \hat{\sigma})}{f(x|0,1)} \bigg ) - 1$$ where $f(\cdot, \mu, \sigma)$ is the normal pdf. To get an estimate of $\alpha(x)$ I simulated $10^3$ data points from a N(0, 1), estimated sample mean and variance and evaluate the relative error at 100 points $x \in (-6, 6)$. I repeated these steps $reps$ times to calculate the average relative error. This is the R code: nX = 100 reps I expected the relative error to be close to 0 for every $x$, but to my surprise it looks like the estimated normal has fatter tails that the real thing. So my questions are: 1) Is this tail behaviour happening because of Jensen inequality (since $f()$ is non-linear)? 2) Suppose I want to use another density estimator, and I want to check how well it does in terms of relative error. I thought that for a normal I could just calculate mean and variance and use the resulting density as a "gold standard" against which I could compare other density estimators. Is there a sensible way to compare how well different density estimators are doing for different values of $x$? Thanks!
