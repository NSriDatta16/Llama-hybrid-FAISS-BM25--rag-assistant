[site]: crossvalidated
[post_id]: 476695
[parent_id]: 256778
[tags]: 
I believe another difference between cosine similarity and TF-IDF is that cosine similarity is done in an embedding space, such as one created by doc2vec . Such an embedding puts words that are used in similar contexts near to each other, so you could use clustering to find similar documents. But cosine distance probably makes more sense for a couple of reasons: An embedding like doc2vec encodes information in direction and distance. Look at the examples of king - man + woman yielding queen . I'd guess that direction dominates this comparison. In high-dimensional spaces, "nearby" (distance) can begin to lose its meaning, so directional measures -- which are also by definition finite and determined a priori -- might make more sense if the "inner product space" supports it. (I threw the last part in there not totally understanding what an "inner product space" is, but it sounds cool and it is related... I just couldn't explain how.) So, given that, I'd say that the idea of "orthogonality" isn't meaningful here. Two documents are either together in a smaller wedge of the space or a larger wedge of the space and that's that: 100 degrees apart is farther apart than 90 degrees, and 80 degrees apart is closer.
