[site]: crossvalidated
[post_id]: 498846
[parent_id]: 221513
[tags]: 
I am trying hard to visualize how weight sharing combined with recurrence and combined with word embeddings behaves in a high-dimensional space. Taking the example from @Maxim and visualizing a network that suggests the next word in the sequence: "On Monday it was" when accumulated using recurrence will be a point in a high dimensional space, and thanks to word embeddings, "On Tuesday it was" will be in the same manifold. Given this accumulated weight as an input to a downstream fully connected layer with high memory capacity, it will learn to map to things like cold, snowing, etc. There may be other stored mappings like hectic, slow, obvious, etc. This may be learnt by one unit of the layer. Another unit may have learnt to map the high dimensional vector formed from the accumulated weight of "It was snowing on" to vectors like christmas, Monday, the, etc. This is about hidden-hidden weights and hidden-output weights. About input-hidden weights, although the weights are shared, the units of the layer that they lead to will be activated for different aspects of a sentence (people and places, stop words, etc), making them position (time) agnostic.
