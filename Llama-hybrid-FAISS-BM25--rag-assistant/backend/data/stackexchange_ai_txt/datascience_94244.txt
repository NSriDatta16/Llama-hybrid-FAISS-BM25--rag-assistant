[site]: datascience
[post_id]: 94244
[parent_id]: 
[tags]: 
Understanding model's learning curves

I'm trying to train a Lane Detection CNN called PINet on a proprietary dataset. Below are some of the important configuration values: Batch size: 6 Optimizer: Adam Learning rate: High of 1e-4 and Low of 1e-6 Lr scheduler: Cyclic lr scheduler in triangular mode Dropout: 0.5 weight_decay: 4e-3 Below is the summary of my dataset: The dataset contains 15200 samples in the train dataset and 2700 samples in the dev/test dataset. For train dataset: 7000/15200 samples come from a stationary scene. I.e. 7000 samples display almost exact same scene. Below you can find the learning curves I have plotted during training My questions are: As per my analysis, The model has achieved an overfit. Am I right here?? Looking at the Train/Validation loss what would you say about the model fit type?? Why my training and validation losses are almost stable (with small variation)?? What can be done to reduce the training loss even further?? What can be done to reduce the gap between Validation and training loss?? is it advisable to use dropout values of 0.6 or 0.7 and weight decay as high as 1e-2 to reduce overfitting??
