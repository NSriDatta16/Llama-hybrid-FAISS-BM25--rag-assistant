[site]: datascience
[post_id]: 43370
[parent_id]: 
[tags]: 
Is anything missing from my model evaluation procedure?

I have been building a model, can someone please review my methods and let me know if I am making a mistake? I trained a model with a support vector machine as follows : Split data into training and test sets as 10 partitions for K10 fold cross validation. Split training set into training and validation set with K5 fold. Train a parameter $C$ using validation set by choosing $C$ that gave best results from K5 fold tests. Train a model with parameter $C$ and training data from K10 fold, 10 times for each partition from the K10 fold. Take 1000 random samples of 80% of the test set partition data, and classify these random samples with the SVM. Calculate a mean and standard deviation. Repeat 10 times for each of the K10 fold partitions. Calculate a mean of all K10 partition means, and their combined standard deviation. I am in the process of repeating this entire process 10 times, and then I will compute a mean and standard deviation of all 10 experiments. For real world testing, I plan to repeat the above procedure but instead of splitting the data into train, test, and validate, I will use all data to find $C$ with K5 fold cross validation, and then test on real world data. Meaning, there will be no test set, the test set will become part of the training set, and the training set will be bigger because of this. Is this the correct way to go about it? Edit : Here is a diagram, hope its helpful. (Hyperparam = $C$ )
