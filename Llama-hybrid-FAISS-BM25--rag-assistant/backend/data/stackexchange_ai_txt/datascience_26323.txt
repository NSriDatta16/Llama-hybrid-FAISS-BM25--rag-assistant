[site]: datascience
[post_id]: 26323
[parent_id]: 
[tags]: 
Can you reduce many Naive Bayes training samples to a few big ones?

Excuse me if this is a simple question (I'm a newbie to data science). I am trying to implement a Naive Bayes classifier in Python. I have implemented a standard Naive Bayes classifier (Bernoulli) and this went very well. Suppose I have 50,000 samples where the first half is labelled '0' and the second half '1'. Then the shape of the X_train matrix will be (#number_of_unique_words/features, 50000) and shape of y_train (50000,). From how I understand Naive Bayes, I can now also join the first 25000 documents to represent one big document with label '0' and one big document with label '1'. However, this gives me very different results when I try to predict the test set. Is my understanding of Naive Bayes correct or is my implementation just wrong?
