[site]: crossvalidated
[post_id]: 93771
[parent_id]: 93477
[tags]: 
If you want to use something like plain (i.e., unregularized) logistic regression, then the argument cited by Jose stands: you need the ratio #features/#examples to be small. However, you can do much better by using regularization. Popular forms of regularization include L1 (lasso) and L2 (ridge) regularization. If you regularize your features properly, you can get away with having many more features than training examples. In this paper , for example, using "dropout regularization" allowed us to train an efficient logistic regression model with 25k training examples and 5 million features. Finally, if you have even fewer training examples (say, a few hundred examples and a million features), you may need to go even further and use generative models models like naive Bayes. As explained here , you may get away with having a number of training examples that grows logarithmically with the number of features.
