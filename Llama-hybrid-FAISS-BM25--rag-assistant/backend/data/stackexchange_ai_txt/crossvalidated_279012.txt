[site]: crossvalidated
[post_id]: 279012
[parent_id]: 
[tags]: 
Why would this simple metric be wrong as a classifier's measure quality?

I have the outputs from two classifiers for a given test set, each of the two provided by a different party, and treated as black boxes. I would like to make a comparative evaluation of them, to give an idea of which one is better on the task. What would be the chief reasons why the following function would be dead wrong, as a measure of classification quality that might be used to juxtapose the performance of the two classifiers? For all data points in the test set: apply a function that penalizes the classifier's confidence for each datum belonging to the class, with its distance from 1 (for those datums that are in the class) and penalize with its distance from 0 for those datums that are out of the class. Then we average or otherwise normalize.
