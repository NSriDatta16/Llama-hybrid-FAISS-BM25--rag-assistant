[site]: crossvalidated
[post_id]: 248115
[parent_id]: 193863
[tags]: 
The OP asks what the link function means. This is actually probably not that important, but, lets get that out of the way first, following which we will answer what the OP needs to see. 1) I did not readily find link function documentation in R. In glm Stata the documentation for a link function probably refers to the same thing. That would be beginning on page 5 of that document This continues on page 6 so it is worth reading more. Your next question is "What does it mean?" if the link function assumption was not satisfied, and that may be that you need a different link function, or to do the regression in some other way, but it is hard for me to be sure as I use neither R nor Stata, and you did not show enough work for me to know (but maybe someone else does). 2) The above, as pointed out by @whuber, is largely irrelevant. For correlation, it's square, $R^2$ would be the explained fraction within the region of fit from min to max of fitting. It is not useful as it only speaks to how well the CDF of the data and CDF regression model fit in that region. What is important to note is that admissions to hospital are likely a Poisson process. What needs to be done is to A) plot admissions every half-year to see whatever trend there is in that model, i.e., that is a density function, and not a cumulative density function. Of course one could use a kernel density smooth, but that would complicate things. B) Find a model that fits the time based data. For example, there may be more admissions at certain times of the year so that there may be some periodicity to the half-year data which can be treated in one of two ways, I) One can model the periodicity with ARIMA or II) One can reformat the data into annual admissions to avoid the problem. In either case we should treat the data using a noise function consistent $L_2$ norm for minimization, see below. Then C) some trending model needs to be fit, and the selection process for that can be complicated. In short, lots of functions should be tried but they should all be done with good theoretical reasons for choosing them, and they should fit really well, except for noise. How to find that model is also related to how one does the regression as well, which brings us to the next consideration. The next D) thing is to measure how much noise there is and how much modelling error there is. For a Poisson process, the square root of the admissions per half-year, plotted in time is an image for which the noise and model misregistration can be measured as correlated variables (and they tend to be correlated). A Poisson consistent minimized norm is ${\mathrm{Min}}_{\mathrm{fit}}= \min \left\Vert \sqrt{\mathrm{data}\left({t}_i\right)}-S\ \frac{\mathrm{pdf}\left({t}_i-{t}_A\right)}{\sqrt{\mathrm{data}\left({t}_i\right)}}\right\Vert,$ where $S$ is the scale factor between the density function found (using much elbow grease), and the data amplitude. In some cases, and likely in yours, the best model may not be a density function, in which case the minimized ($L_2$) norm simplifies to ${\mathrm{Min}}_{\mathrm{fit}}= \min \left\Vert \sqrt{\mathrm{data}\left({t}_i\right)}- \frac{f\left({t}_i-{t}_A\right)}{\sqrt{\mathrm{data}\left({t}_i\right)}}\right\Vert.$ To show what this can mean, let us cite a paper published yesterday under creative commons license. As the image below shows, a pdf was fit to time series data at one minute per data point from a Poisson process. The misregistration of the models was only 0.80% with total fit errors of 1.44%. One can also calculate an $R^2$ value and see the paper for examples, however, $R^2$ alone will not tell us how much error is from modelling, and how much is noise. Finally, how much time should elapse between binned samples will change both the noise and the misregistration so there is an optimum time interval to use for data processing.
