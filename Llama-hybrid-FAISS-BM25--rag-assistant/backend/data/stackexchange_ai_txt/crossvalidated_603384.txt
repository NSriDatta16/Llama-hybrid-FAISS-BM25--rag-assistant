[site]: crossvalidated
[post_id]: 603384
[parent_id]: 321841
[tags]: 
Variational Auto Encoders are an intersection between Auto-Encoders Neural Networks and Variational Inference. It was introduced as an application for a general purpose VI using the reparameterization trick in a paper from 2014 by Kingma and Welling . The main goal is to generate more data - by creating a more regularized latent space for auto encoders. Auto Encoders Auto Encoders (AEs) are a NN architecture used mainly to compress data / dimensionality reduction. It's made of two NNs - an Encoder $f_\phi(x)$ which encodes the original data $x$ to some latent space $z$ . And a Decoder $g_{\theta}(z)$ which decodes the latent space back to the data space. $f$ is parameterized by the NN weights $\phi$ , and $g$ is parameterized by $\theta$ . The overall structure looks like this: In order to optimize the NN, you can take, e.g., the L2 norm of the difference between the original data $x$ and the reconstructed data $\hat x = g(f(x))$ : $Loss = ||x-\hat x||^2_2$ . As usual in NN, you optimize the weights by some form of Gradient Descent (e.g., SGD, ADAM etc.). Once the weights have been optimized, the outputs of the encoder and decoder are fixed - for a given $x$ you will always get the same $z=f(x)$ , and for a given $z$ you will always get the same $x=g(z)$ . Now, you would think that this could also be used for generation of new data: sample some random points in the $z$ space, pass them through the decoder, and get new data. Turns out it doesn't work so well - the reason being is that the process of optimizing AEs only care about your actual data, and it doesn't care how it's going to store the representation of the data in the latent $z$ space. So the $z$ space becomes very "messy". Variational Inference Variational Inference (VI) is a method to find an approximation to intractable posteriors. Given Bayes formula: $$ p(z|x) = \frac{p(x|z)p(z)}{\int p(x|z)p(z)dz} $$ The numerator consists of the likelihood $p(x|z)$ and the prior $p(z)$ , both are usually known. The denominator (which is actually equal to $=p(x)$ ) is called the "evidence", and in high dimensions its usually hard to impossible to compute. Although it's only a normalizing constant (given the data is known) without it we don't really know the distribution. VI says - let's place a probability we know and can control its parameters $q_\phi(z) = q_\phi(z|x)$ (e.g., Gaussian) and "turn the knobs" of that distribution (=optimize the parameters $\phi$ ) until we reach something that looks like the true posterior. Since we don't know the true posterior, only up to a normalizing constant - a special metric is used, the KL divergence, which is a metric to measure the "statistical distance" between two distributions: $$KL(q(z)||p(z|x)) = \int q(z)\log\frac{q(z)}{p(z|x)}dz $$ By using that metric, we can ignore the evidence, and only optimize the terms we know and care about (whose negative is called the ELBO [=evidence lower bound] - hence minimizing the KL is equivalent to maximizing the ELBO): $$ = \int q(z)\log q(z)dz - \int q(z)\log p(z|x)dz = \mathbb E_{q}[\log q(z)]-\int q(z)\log \frac{p(x|z)p(z)}{p(x)}dz \\ = \mathbb E_{q}[\log q(z)] - \mathbb E_{q}[\log p(x|z)p(z)]+\log p(x) \\ ELBO = \mathbb E_{q}[\log p(x|z)p(z)] - \mathbb E_{q}[\log q(z)] $$ There are a few ways to continue here - up to the paper the main approach was Coordinate Ascent VI with the "mean field" assumption. There was also the approach of doing gradient ascent on the ELBO, by the log-derivative trick, which suffered from high variance (although later, variance reduction techniques were used. E.g., BBVI ). The main focus of the paper seems to be the new method they suggested which is the reparameterization trick (later also used in ADVI ): sample from some neutral distribution that doesn't depend on the parameters of the variational distribution $\phi$ , and transform those samples to samples from $q_\phi$ - then evaluate the gradient for those samples, and take their average (Monte Carlo estimate): $$ \epsilon \sim p'(\epsilon) \\ z = \mathcal T(\epsilon; \phi) \\ \nabla _\phi ELBO \approx \frac{1}{L}\sum_{l=1}^L \nabla _\phi [\log p(x|T(\epsilon; \phi))p(T(\epsilon; \phi))] -\log q(T(\epsilon; \phi))] $$ Variational Auto Encoders We now place distributions over $z$ and $x$ - e.g., suppose $p(x|z)$ is a Gaussian (though it can also model discrete data and be Bernoulli or Categorical). If we look at the decoder structure from before and say that now it outputs the parameters of that Gaussian $\mu_\theta, \Sigma_\theta$ [suppose for illustration's sake that the decoder is fixed and its weights $\theta$ are known], the posterior $p(z|x)$ is intractable, because of the decoder NN function (a complex non-linear function). So we can do VI to recover it. And suppose we decide to place also a Gaussian $q(z|x)=N(\mu,\Sigma)$ . But suppose we don't simply use global parameters $\phi = (vec(\mu), vec(\Sigma))$ , but instead place an encoder network that will output the mean and covariance: $\mu_\phi, \Sigma_\phi$ . The overall structure now looks like this: Suppose we also take a standard Gaussian prior over the $z$ 's: $p(z)=N(0,I)$ . In this case, we can "massage" the ELBO a little bit more: $$ ELBO = \mathbb E_{q}[\log p(x|z)p(z)] - \mathbb E_{q}[\log q(z)] = \mathbb E_{q}[\log p(x|z) + \log p(z) - \log q(z)] \\ = -KL(q_\phi(z|x)||p(z)) + \mathbb E_{q_\phi}[\log p_\theta(x|z)] $$ The reason to do so is that in this case this KL has a closed form. Assuming we have a closed form for the KL, we can also use Monte Carlo estimates (using the reparameterization trick) for the 2nd term: $$ = -KL(q_\phi(z|x)||p(z)) + \frac{1}{L}\sum_{l=1}^L \log p_\theta(x|z=\Sigma^{0.5}_\phi\cdot\epsilon_l +\mu_\phi) $$ Note that the 1st part of the objective is optimized only w.r.t. $\phi$ , and the 2nd error is optimized both w.r.t. $\phi$ (encoder) and w.r.t. $\theta$ (decoder). There are 2 ways to look at this loss: Looking at the loss terms, noticing that the 2nd term (called the reconstruction term) is very similar to a loss of AEs (and actually equivalent [w.r.t. $\arg \max$ ] to it if we assume $\Sigma_\theta=I$ , and $\hat x = \mu_\theta$ ). The 1st term acts as a sort of a regularizer, which tells the (approximated) posterior to not stretch too far from a standard Gaussian. Looking at the derivatives. For illustration's sake suppose we are actually separating the update rule into two: Suppose $\phi$ is fixed, $\nabla_\theta ELBO$ is finding the decoder by doing a sort of maximum likelihood on our data (given $z$ 's drawn from the posterior). Suppose $\theta$ is fixed, $\nabla_\phi ELBO$ is finding the encoder by approximating the posterior using VI. So, this is not strictly a VI method, as in VI you usually assume the likelihood is fixed. Here you have two moving parts: you optimize the likelihood (decoder) and optimize the posterior (encoder) simultaneously. Using VAE eventually leads to a latent space which is more ordered and tries to be more like a standard Gaussian. I've made a video about this topic on my YouTube channel (which elaborates a bit on the paper and on VI), if you want to learn more.
