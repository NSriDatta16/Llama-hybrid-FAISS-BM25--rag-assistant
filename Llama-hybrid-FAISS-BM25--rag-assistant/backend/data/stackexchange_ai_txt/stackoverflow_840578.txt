[site]: stackoverflow
[post_id]: 840578
[parent_id]: 837847
[tags]: 
I'm only going to comment on the database side: With a normal RDBMS a 50/50 read/write load for the DB will make replication "expensive" in terms of overhead. For almost all cases having a simple failover solution is less costly than implementing a replicating active/active DB setup. Both in terms of administration/maintenance and licensing cost (if applicable). Since your data is "mostly denormalized and non relational" you could take a look at HBase which is an OSS implementation of Google Bigtable , a column based key/value database system. HBase again is built on top of Hadoop which is an OSS implementation of Google GFS . Which solution to go with depends on your expected capacity growth where Hadoop is meant to scale to potentially 1000s of nodes, but should run on a lot less as well. I've managed active/active replicated DBs, single-write/many-read DBs and simple failover clusters. Going beyond a simple failover cluster opens up a new dimension of potential issues you'll never see in a failover setup. If you are going for a traditional SQL RDBMS I would suggest a relatively "big iron" server with lots of memory and make it a failover cluster. If your write ratio shrinks you could go with a failover write cluster and a farm of read-only servers. The answer lies in the details. Is your application CPU or I/O bound? Will you require terabytes of storage or only a few GB?
