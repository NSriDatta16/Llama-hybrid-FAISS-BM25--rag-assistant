[site]: crossvalidated
[post_id]: 215032
[parent_id]: 95896
[tags]: 
The original perceptron algorithm goes for a maximum fit to the training data and is therefore susceptible to over-fitting even when it fully converges. You are also right in being surprised, because when the number of training data increases, over-fitting usually decreases. However, the original perceptron can over-fit even to one single noisy example in your training data, and sometimes such adverse examples are rare and tend to appear only in a larger set. That can be one justification for you observation. Another justification can be that your algorithm did not converge on one or both of the data sets, and the resulting weight vectors are of low quality (you did not specify convergence). This is a major weakness of the original perceptron algorithm. In any case, trying the algorithm on more data sets (to see if there is a pattern) and/or using the averaged or large-margin perceptrons can help.
