[site]: datascience
[post_id]: 54693
[parent_id]: 54689
[tags]: 
If you want to know what features are important to predict some outcome, you speak about feature importance (FI). FI can differ between models, e.g. between a random forest and logistic regression. This is because the models take a different perspective on how to predict some outcome. A mathematically sound way to assess FI in binary classification would be to run a logistic regression with a lasso or ridge penalty. This is easy and the results (the log odds) can be directly interpreted. Larger positive or negative values indicate a larger positive or negative FI et vice versa. Here is a good R example: https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html See „Introduction to Statistical Learning“ Ch. 6.2 for some background and demo: https://www-bcf.usc.edu/~gareth/ISL/
