[site]: crossvalidated
[post_id]: 244272
[parent_id]: 
[tags]: 
uniform distribution: expectation of sum squared observations

I'm trying to follow ML lesson about Bayesian inference . They have a sample of n observations from a uniform distribution: $x_i = U(0, \theta)$. They claim that $E[(\sum x_i)^2] = n(n-1)\theta^2/4 + n \theta^2/3$ - why is that true? I tried by myself and got a different result, where did I go wrong?: $Var(x_i) = \theta^2/12 = E(x_i^2)-E^2(x_i);$ $E(x_i^2) = \theta^2/12 + (\theta/2)^2 = \theta^2/3$ $E[(\sum x_i)^2] = n\theta^2/3$ Where did the first term come from??
