[site]: crossvalidated
[post_id]: 91233
[parent_id]: 90890
[tags]: 
You can use Information Gain (IG) to see if features that are natural choice to predict some target class are really better than the other features you can use. In your application domain, it might be true that there are better predictive variables than yours (Say $V$). However: There are cases where IG gets inflated just because of random chance. More specifically when the training set is small. See http://en.wikipedia.org/wiki/Adjusted_mutual_information If the other features are continuous the decision tree induction process goes for binary discretization. Due to multiple comparisons, the chances to find a high value for IG are not negligible. See http://en.wikipedia.org/wiki/Multiple_comparisons_problem Similar to the point above, when there are lots of features you can use for prediction the chance to find a group of very predictive features because of overfitting are high. See this paper about ensemble of trees: http://www.pnas.org/content/99/10/6562.abstract Keep in mind that there are other splitting criteria, for example the Gini Gain (GG) used in CART ( http://en.wikipedia.org/wiki/Decision_tree_learning ). However, if you used that you might have the same results you obtained with IG because they are different parametrization of the same thing (RÃ©nyi Entropy http://en.wikipedia.org/wiki/R%C3%A9nyi_entropy ). See if statistical tests (e.g. Chi-square) leads to different results. http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.92.9930 If you are interested in the quality of your predictor variable $V$ you might use feature importance in Random Forest. This can assess its quality in a more robust way. See http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm . R version http://cran.r-project.org/web/packages/randomForest/index.html Also, there should be a reason why you consider $V$ as important. Things that are not included in the IG computation. Is $V$ easy to measure? Cheap to measure?
