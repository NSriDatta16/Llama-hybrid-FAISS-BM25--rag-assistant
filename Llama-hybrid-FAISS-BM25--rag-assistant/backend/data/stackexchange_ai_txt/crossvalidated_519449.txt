[site]: crossvalidated
[post_id]: 519449
[parent_id]: 519422
[tags]: 
In Bayesian statistics, the marginal likelihood $$m(x) = \int_\Theta f(x|\theta)\pi(\theta)\,\text d\theta$$ where $x$ is the sample $f(x|\theta)$ is the sampling density, which is proportional to the model likelihood $\pi(\theta)$ is the prior density is a misnomer in that it is not a likelihood function [as a function of the parameter], since the parameter is integrated out (i.e., the likelihood function is averaged against the prior measure), it is a density in the observations, the predictive density of the sample, it is not defined up to a multiplicative constant, it does not solely depend on sufficient statistics Other names for $m(x)$ are evidence , prior predictive , partition function . It has however several important roles: this is the normalising constant of the posterior distribution $$\pi(\theta|x) = \dfrac{f(x|\theta)\pi(\theta)}{m(x)}$$ in model comparison, this is the contribution of the data to the posterior probability of the associated model and the numerator or denominator in the Bayes factor. it is a measure of goodness-of-fit (of a model to the data $x$ ), in that $2\log m(x)$ is asymptotically the BIC ( Bayesian information criterion ) of Schwarz (1978). See also Normalizing constant in Bayes theorem Normalizing constant irrelevant in Bayes theorem? Intuition of Bayesian normalizing constant
