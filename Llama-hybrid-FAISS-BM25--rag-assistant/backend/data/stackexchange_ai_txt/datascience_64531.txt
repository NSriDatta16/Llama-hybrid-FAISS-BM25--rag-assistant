[site]: datascience
[post_id]: 64531
[parent_id]: 64498
[tags]: 
The problem of an imbalanced dataset is the problem of generative classifiers that use the prior probability for calculating the predicted label. As the labels have a lower prior they get a lower probability. There are several ways to cope with imbalanced datasets: Oversampling the minority classes ,randomly add observations from the minority classes, so the prior probability of every class will be the same. Undersampling - if you have a dataset with a lot of observations but the majority class is few times larger than the minority, choose randomly a subset of the whole dataset that includes the same number of observations for each label. Use Data Augmentation to generate synthetic data that tries to simulate the same distribution of the features in a label. Weighted classifiers - there are classifiers support weights for the labels. If you use Neural Network model, you can do Transfer Learning. Copy the weights of the model from a model with balanced data (you told that you have models with a similar feature vector), copy the network (with the weights) and replace the last layer to randomly initialised (better to use Xavier initialiser) . Then freeze all of the layer weights except for the last one and train it. It's better to keep the same proportion between the classes using (1) or (2) and it's also recommended combine with (3).
