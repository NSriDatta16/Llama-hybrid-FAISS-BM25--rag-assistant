[site]: crossvalidated
[post_id]: 628322
[parent_id]: 
[tags]: 
Null hypothesis in permutation inference for the general linear model

I'm currently studying a paper by Winkler et. al on permutation inference for the general linear model and am confused by their reasoning and notation. They first introduce a formulation for the general linear model: Y = M ψ + ϵ (Eq. 1) where Y is the N × 1 vector of observed data, M is the full-rank N × r design matrix that includes all effects of interest as well as all modelled nuisance effects, ψ is the r × 1 vector of r regression coefficients, and ϵ is the N × 1 vector of random errors. Regarding the null hypothesis, it is stated that: Our interest is to test the null hypothesis that an arbitrary combination (contrast) of some or all of these parameters is equal to zero, i.e., H0 : C′ψ = 0, where C is a r × s full-rank matrix of s contrasts, 1 ≤ s ≤ r. The general formulation (Eq. 1) in the first quote can be transformed into a partioned one: Y = X β + Z γ + ϵ (Eq. 2) where X is the matrix with regressors of interest, Z is the matrix with nuisance regressors, and β and γ are the vectors of regression coefficients. When discussing the Freedman-Lane procedure for permutation inference in the following it is stated that: The rationale for this permutation method is that, if the null hypothesis is true, then β = 0 [...] These paragraphs pose two questions to me: What does C′ in H0 : C′ψ = 0 denote mathematically speaking, given that C is a matrix? How does C′ψ = 0 imply β = 0? Is the reasoning C′ψ = 0, therefore Y = M0 + ϵ = X0 + Z0 + ϵ = ϵ valid? Thanks a lot in advance, I'd be very grateful for help or advice on how to approach/present this problem in a better way.
