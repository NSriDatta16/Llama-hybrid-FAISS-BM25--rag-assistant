[site]: datascience
[post_id]: 77739
[parent_id]: 77738
[tags]: 
Welcome to the community! There are points coming to my mind: Check the amount of data in each block and then their distribution. First experience might be due to the lack of enough data in reserved block (i.e. you literally just trained but did not validated resulting to a complete overfitting) or having a totally different distribution (i.e. the difference between blocks is NOT random). Be sure about wht you called "Cross Validation". If you splitted 49 blocks into a training and a testing set, then it is minimal version of Cross Validation. 49 blocks should be splitted to $k$ different set, each time one of them plays the test set role and then the final error is averaged over all $k$ runs. That is statistically more robust (first experience shows problem in robustness of statistical inference) Second experiment uses 100 blocks instead of 50 (where were those other 50 in first experiment?) so first, you have more data, and second, you did not split according to "blocks" (whatever they are) but merged all and randomly chose train, validation and test. This procedure reduces the danger of distributional bias within train/valid/test and also might be an indicator of the fact that statistically those "blocks" are different (their distribution) hope it helped. Please share your results so everyone can learn from it. Good Luck!
