[site]: crossvalidated
[post_id]: 214563
[parent_id]: 214518
[tags]: 
This isn't specific to autoencoders, but a number of papers suggest procedures for initializing weights such that the outputs of each layer maintain a desired distribution. The motivation is similar to that of normalizing the inputs. Of course, the right procedure depends on the activation function. Here are a few: Glorot and Bengio (2010). Understanding the difficulty of training deep feedforward neural networks. He et al. (2015). Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification. LeCun et al. (1998). Efficient BackProp.
