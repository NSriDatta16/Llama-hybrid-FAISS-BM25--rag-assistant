[site]: datascience
[post_id]: 20379
[parent_id]: 20372
[tags]: 
Referencing @JahKnows answer, I think what he may be trying to refer to is stemming not n-grams . (I would've commented but not enough reputation). Using the python package nltk it should allow you to stem the words given that you want to only get the roots of each word. From what I've worked on, n-grams are sequences of words with length n . They are still very helpful for the bag of words suggestion. The python library scikit-learn has a class CountVectorizer which lets you create this model. In addition, it also lets you set a max number of features which can act as your feature reduction. Further, if you were going this route, I would continue and use Tf-idf which will find out 'how important' a word is to a document. You can read more on the wiki . Scikit-learn has a great tutorial on working with text data which utilizes both CountVectorizer and Tf-idf : Working With Text Data . The last steps would be is to choose a multi-class classification model and feed it into a OneVsRestClassifier (all in scikit-learn) and choose which one works the best. (As a start you can try Naive Bayes, SVM , etc.) This should work well since you already have a lot (I'm assuming) of labeled data. Good luck!
