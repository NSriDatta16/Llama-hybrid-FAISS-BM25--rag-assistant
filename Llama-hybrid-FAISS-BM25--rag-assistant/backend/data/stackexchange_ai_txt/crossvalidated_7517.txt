[site]: crossvalidated
[post_id]: 7517
[parent_id]: 7511
[tags]: 
According to the comments after the question, this is a hypothesis testing situation. You have stipulated that you can accurately assess the null distribution of the individual cell counts. We need a test statistic. The nature of the problem suggests running a small kernel over the array (essentially to deconvolve the signal). A simple choice would be a 2 x 2 mean. We would then look at the maximum of those values. The Neyman-Pearson lemma says that a good test is based on setting a critical value for this statistic. The test size depends on the null distribution of the test statistic while its power depends on the intensity of the signal. You need to choose a critical value to appropriately balance the expected false positive rate with the power for the kinds of signals you are looking for. All this is routine; the only non-routine aspect is determining the null distribution of the statistic. It's the maximum of a set of fairly highly correlated linear combinations of Poisson variates. Specifically, if we index the rows and columns of the array and let $X_{i,j}$ be the Poisson variate for the cell in row $i$, $1 \le i \le m$, and column $j$, $1 \le j \le n$, then the 2 x 2 mean is the array of values $Y_{i,j} = (X_{i,j} + X_{i+1,j} + X_{i,j+1} + X_{i+1,j+1})/4$, $1 \le i \le m-1$, $1 \le j \le n-1$, and the test statistic is $t = \max\{Y_{i,j}\}$. If someone could jump in and tell us how to compute the distribution of $t$ that would be nice, but I suspect it's not an easy calculation. Would you consider a small simulation? It's easy to set up in R or Mathematica , for instance. Alternatively, use an approximation. The results of a few simulations with arrays from 9 to 1600 elements and photon intensities from 1 to 4096 per cell are consistent with two obvious approximations: one can treat all $(m-1)(n-1)$ of the $Y_{i,j}$ as independent or one can take every fourth one and treat them as independent, and then calculate the distribution of their maximum. For small intensities the upper tails of the simulated distributions of $t$ (10,000 iterations per simulation) appear to behave like the latter approximation: that is, $t$ behaves like the largest of $(m-1)(n-1)/4$ independent averages of four Poisson variates. For large intensities the tails are quite close to the former approximation: that is, $t$ behaves like the largest of $(m-1)(n-1)$ independent averages of Poisson variates. This is a histogram of $t$ for a 30 by 40 grid with cell intensity 100 (10,000 simulations). Because its 99th quantile equals 122.25, you could create a test with at most 1% false positive rate by setting the critical value to 123. An average photon count of around 135 in a single block of four cells would be readily detectable with this method. That would represent a total flux of 4*(135 - 100) = 140 photons above background. For the record, here is the Mathematica code used to generate this histogram. simulate[m_Integer, n_Integer, \[Mu]_] /; m >= 2 && n >= 2 && \[Mu] > 0 := With[ {f = PoissonDistribution[\[Mu]]}, y = ListConvolve[{{1, 1}, {1, 1}}/4, RandomInteger[f, {m, n}]] ]; With[{m = 30, n = 40, \[Mu] = 100, nTrials = 10000}, null = ParallelTable[Max[Flatten[simulate[m, n, \[Mu]]]], {i, 1, nTrials}]; Histogram[null, {1/4}, AxesLabel -> {"t", "Count"}] ]
