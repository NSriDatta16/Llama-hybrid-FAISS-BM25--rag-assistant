[site]: crossvalidated
[post_id]: 366078
[parent_id]: 
[tags]: 
How to approach preprocessing large number features for machine learning?

I used to apply supervised machine learning for maximum few dozen "normal", natural features like human interpretable ones in Boston House Prices table. I usually try to understand each of them, think about how to preprocess them by sometimes binning continuous ones, re-categorizing and encoding categorical ones etc. Now, however, I confront with very numerous (several hundred) features where this intimate one by one overview is clearly impossible. The easiest solution would be to accept the features as they are and handle them blindly en masse but in this way some evident optimization methods or even more importantly, some decisive corrections cannot be pointed out. For instance, R and/or Python Pandas are sometimes incorrectly identify numerical/categorical columns which is quite misleading if not spotted (missing data imputation must be applied very differently, and standardization is senseless in case of categorical ones, just mentioning two problems). So my question is what is an appropriate way to handle and clear/correct these numerous features when one by one examination is out of question? Am I limited to use tree-based model types which are insensitive to NAs and scaling so no need for preprocessing? (Just for being clear: this question is not about reducing the dimensionality.)
