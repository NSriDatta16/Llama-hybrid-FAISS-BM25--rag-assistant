[site]: crossvalidated
[post_id]: 164786
[parent_id]: 164774
[tags]: 
I agree with Cagdas - it seems like you may be looking at the training error, in which case you would just be overfitting to your data as you add more features. An Introduction to Statistical Learning provides a good overview of different validation techniques. Chapter 2 explains the difference between training and test error, and chapter 5 provides an overview of different validation approaches. I have found that repeated k-fold cross validation works well. Given that you are working with a largely imbalanced dataset, it would be surprising (but nice!) if you could achieve AUCs that high. There's been a lot of discussion on working with imbalanced datasets, but in practice I have found Kaggle competitions to be a good source of ideas. You may want to check out the Liberty Mutual Fire Loss discussion boards, as that competition dealt with a highly imbalanced dataset. My read was that an ensemble of undersampled models worked well.
