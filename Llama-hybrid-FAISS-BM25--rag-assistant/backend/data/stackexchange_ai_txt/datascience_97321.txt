[site]: datascience
[post_id]: 97321
[parent_id]: 97310
[tags]: 
The dynamic masking is analogous to using different image augmentations so you can reuse the same image for training repeatedly but the network actually sees a different example. Concretely, imagine we were training a network to perform in-painting. For training we have a complete image and then choose some region to occlude and ask the network to predict what the occluded part is supposed to look like. Now imagine that each epoch we reuse this image but change the location of the occlusion. There's a bit of data leakage risk here (we're asking the network to predict a region it's actually seen before), but with an appropriately large dataset that shouldn't be a problem: the network will perform better on the rest of the dataset if it learns generally useful features than if it memorizes that one image. Memorization isn't necessarily even a bad thing if the network learns how to mix and match what it memorized in clever ways (i.e. treating an image as a bag of local features). RoBERTa's dynamic masking is just the text version of that. Instead of an image, we have a chunk of text. Instead of occluding a region of pixels, we're occluding a region of text. It's a data augmentation that functionally increases the variability of the data, encouraging the network to learn more robust features.
