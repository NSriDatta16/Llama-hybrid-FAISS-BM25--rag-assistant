[site]: datascience
[post_id]: 49744
[parent_id]: 
[tags]: 
Where can I find out about the "Helvetica scenario"?

From the paper introducing GANs : It makes sense that collapsing too many $\vec{z}$ -values to a single $\vec{x}$ -value will cause problems. However, I was a bit confused as to how training $G$ for a shorter amount of time before running $D$ can fix it. But when I googled "Helvetica scenario", all I could find were references to a parody show called Look Around You . Since it seems like this is an actual thing, not just a parodyâ€”where can I find out more about it, and how it can be prevented? And why is it called "the Helvetica scenario"?
