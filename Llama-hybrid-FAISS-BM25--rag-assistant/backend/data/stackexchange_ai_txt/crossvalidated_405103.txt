[site]: crossvalidated
[post_id]: 405103
[parent_id]: 
[tags]: 
How does one use Convolutional Neural Nets (CNNs) on varying size sentences for NLP so that the final fully connected layer can remain fixed size?

I wanted to use CNNs to classify sentences. The sentences are varying length. I am going to use standard Word Embeddings (any sort of pre-trained vectors) as features for each word then concatenate everything to represent one sentence. My issue is that sentences might have different lengths which leads to different layers having different dimensions based on the # of word we have . Then that make some layer (usually my fully connected layer) screw up because it never knows what size vector its going to receive. I really do not want to pad my sentences with nonsense to fix the issue. Is there a way to solve this issue without padding sentences with irrelevant stuff so that the final fully connected classification layer works fine? I found the following paper: https://www.aclweb.org/anthology/D14-1181 and in the model section they have a part which I suspect might be what I am looking for but I am unable to understand it (i.e. I feel it fixes some layer to have a certain size which is what I need for my FC layer to work and thus my classification layer to work). We then apply a max-overtime pooling operation (Collobert et al., 2011) over the feature map and take the maximum value ^c = maxfcg as the feature corresponding to this particular filter. The idea is to capture the most important feature—one with the highest value—for each feature map. This pooling scheme naturally deals with variable sentence lengths. in particular it says: This pooling scheme naturally deals with variable sentence lengths.
