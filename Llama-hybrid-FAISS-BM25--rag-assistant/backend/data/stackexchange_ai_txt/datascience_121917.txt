[site]: datascience
[post_id]: 121917
[parent_id]: 121906
[tags]: 
The entropy criterion is used by the CART algorithm to build the DT itself, by evaluating which split is actually the best (greedily) to split on. So, it's not directly related to feature importance which, in case of DTs, is computed as the reduction in impurity brought by a feature. This is not an error, it's just by design: actually, it is an extra capability that DTs have. You can also estimate feature importance with Random Forests and Extra Trees which should provide more accurate results since they compute that from an ensemble of models. Indeed, the way it's computed is still based on impurity reduction, which you can think of being a quantification of how much a feature improves the model's performance.
