[site]: crossvalidated
[post_id]: 605838
[parent_id]: 
[tags]: 
Bayes Theorem representation in Machine Learning

I am going through a Machine Learning tutorial, where the problem statement is expressed using Bayes theorem. Particularly, $P(y|Data) = \frac{P(Data|y) P(y)}{\int P(Data)}$ The data part is particularly confusing to me as is meant to be a normalizing constant, however, it does not depend on $y$ from the above definition. Also I'm not sure what is being integrated here and over what. For example, if $Data$ comprises several random variables, say $Data = \{x_1,x_2,...,x_d\}$ , now the above expression becomes, $P(y|x_1,x_2,...,x_d) = \frac{P(x_1,x_2,...,x_d)P(y)}{\idotsint P(x_1,x_2...,x_d) dx_1 \dots dx_d}$ if I am not mistaken. However, how does this act as a normalizing constant, i.e. resulting to a valid probability distribution? I'm certain that I have something wrong here, so could someone please shed some light on this?
