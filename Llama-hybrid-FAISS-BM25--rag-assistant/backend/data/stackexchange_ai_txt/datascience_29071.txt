[site]: datascience
[post_id]: 29071
[parent_id]: 28915
[tags]: 
Questions about definitions are always fun, so let me try to offer another answer here. Firstly, let us model mathematically what you are doing. At the highest level, you are estimating some measure of "reward" $R(s)$ for each board state $s$. You are doing it by interacting with the environment and updating your internal parameters (i.e. the table of $R$ values) towards reinforcing the favorable behaviours. Consequently, by most standard definitions, your algorithm should indeed be categorized as reinforcement learning . To understand what kind of reinforcement learning you are doing and whether it is "good", we should go a bit deeper. One of the key notions in reinforcement learning is the value function $V$ (or its alter-ego, the $Q$ function), which reports the best possible total reward you may expect to gain if you play optimally from a given board state. If you can show that your algorithm is, at least in some sense, estimating $V$, you can claim a certain "goodness" guarantee, and proceed to classify it into any of the known "good" algorithm types (which all essentially either aim to estimate $V$ directly, or to act as if they estimated it implicitly). Note that when we speak about two-player games, there does not necessarily exist a unique $V$ to aim for. For example, assuming the reward 1 for winning, 0 for losing, 0.5 for a draw, and no discounting, $V(\text{empty board})$ is $0.5$ if you are playing against an optimal opponent, but it is probably close to $1$ if your opponent is random. If you play against a human, your $V$ may differ depending on the human. Everyone knows that the first move into the center is the safest one, however I myself have never won with such a move - the game always ends in a draw. I did win a couple of times by making the first move to the corner, because it confuses some opponents and they make a mistake on their first turn. This aside, assuming $V$ to denote the game against an optimal opponent is a reasonable choice. Getting back to your algorithm, the crucial step is the update of $R$, which you did not really specify. Let me assume you are simply accumulating the scores there. If it is the case, we may say that, strictly speaking, you are not doing Q-learning, simply because that's not how the $Q$ function is updated in the classical definition. It still does not mean you are not implicitly estimating the correct $V$, though, and it is rather hard to prove or disprove whether you do it or not eventually. Let me tune your algorithm a bit for clarity. Instead of adding up the final reward to $R(s)$ for each state $s$, which occurred in the game, let us track the average reward ever reached from each state. Obviously, the position where you always win, although rarely reach, should be more valued than a position where you rarely win, even if you often reach it, so we probably won't break the algorithm by this change, and the overall spirit of learning stays the same anyway. After this change, $R(s)$ becomes easy to interpret - it is the average expected reward reachable from position $s$. This average expected reward is not really the value function $V$ we are interested in estimating. Our target $V$ should tell us the best expected reward for each position, after all. Interestingly, though, when your policy is already optimal then your average reward is equal to your optimal reward (because you always do optimal moves anyway), hence it may be the case that even though you are kind-of learning the wrong metric in your algorithm, if the learning process pushes your algorithm ever so slightly towards optimal play, as you gradually improve your policy, the "average expected reward" metric itself slowly becomes "more correct" and eventually you start converging to the correct value function. This is pure handwaving, but it should illustrate the claim about it being hard to prove or disprove whether your algorithm formally learns what it should learn. Maybe it does. In any case, let us, instead of tracking the average reward for each state, change your algorithm to track the best possible reward so far. This means, you'll check all the alternative moves from each position and only update $R(s)$ if your current move resulted in an improved score down the road (in comparison to alternative options you could have taken from this state). Congratulations, now your algorithm is equivalent to the usual Q-learning method (it is the "value iteration" method, to be more precise). Finally, "is it learning or brute force" is a valid question. The word "learning" can be interpreted in at least two different ways. Firstly, learning may denote simplistic memorization . For example, if I discover that the first move to the center is good, I may write this fact down in a table and use this fact later directly. People call such memorization "learning", but this learning is really quite dumb. A second, different meaning often ascribed to "learning" is generalization . It would be the case when, besides simply writing down which moves are good, your algorithm could generalize this information to previously unseen moves. This is the "intelligent" kind of learning. Q-learning, as well as many other RL algorithms are typically formulated in terms of updates to the tables $V$ or $Q$. As such, they are inherently "dumb learning" algorithms, which do not even aim to generalize the state information. True generalization (aka "smart learning") emerges only when you start modeling the state or the policy using something with a built-in generalization ability, such as a neural network. So, to summarize. Yes, your algorithm is reinforcement learning. No, it is not Q-learning, but it becomes that with a minor change. Yes, it is more "brute force" rather than "intelligent learning", but so is the default Q-learning as well. Yes, adding generalization by modeling states with a neural network makes the algorithm "more intelligent".
