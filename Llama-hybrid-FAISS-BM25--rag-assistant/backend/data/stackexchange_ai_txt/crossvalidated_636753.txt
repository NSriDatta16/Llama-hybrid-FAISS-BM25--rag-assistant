[site]: crossvalidated
[post_id]: 636753
[parent_id]: 
[tags]: 
Mathematics behind standardizing the data points in machine learning algorithms (e.g., K-means clustering)

For K-means algorithm, among other methods using distance-based measurements to determine similarity between data points, why we have to standardize the data points with mean as 0 and standard deviation as 1? My understanding is such a way will reduce the variation of the output (to avoid extreme values). Who can provide some mathematical justification? Thank you.
