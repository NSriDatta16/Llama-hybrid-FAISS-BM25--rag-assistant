[site]: datascience
[post_id]: 9757
[parent_id]: 9736
[tags]: 
Let's assume that we are in a classification setting. For svm feature engineering is cornerstone: the sets have to be linearly separable. Otherwise the data needs to be transformed (eg using Kernels). This is not done by the algo itself and might blow out the number of features. I would say that svm performance suffers as we increase the number of dimensions faster than other methodologies (tree ensemble). This is due to the constrained optimization problem that backs svm s. Sometimes feature reduction is feasible, sometimes not and this is when we can't really pave the way for an effective use of svm svm will likely struggle with a dataset where the number of features is much larger than the number of observations. This, again, can be understood by looking at the constrained optimizatiom problem. categorical variables are not handled out of the box by the svm algorithm.
