[site]: crossvalidated
[post_id]: 636749
[parent_id]: 
[tags]: 
In this RL problem, why is the substitution $q_*(A_t)=\mathbb{E}[R_t | A_t] \to R_t $ valid within this expectation (over actions)?

The question that follows is from a machine learning textbook (Reinforcement learning Suttion and Barto page 39 link ). Given: a probability distribution over actions $x$ (a policy) at time $t$ denoted $\pi_t(x)$ the expected reward, $R_t$ , at time $t$ immediately after taking action $a_t$ denoted $q_*(a_t) = \mathbb{E}[R_t | a_t] $ . The following equality is made, $$\mathbb{E}_{A_t}[(q_*(A_t)-B_t)\frac{\partial\pi_t(x)}{\partial H_t(a)}/\pi_t(A_t)]$$ $$=\mathbb{E}_{A_t}[(R_t-B_t)\frac{\partial\pi_t(x)}{\partial H_t(a)}/\pi_t(A_t)]$$ The authors say that the substitution $q_*(A_t) \to R_t $ is valid as $q_*(A_t)=\mathbb{E}[R_t | A_t]$ . how does this fact permit the above substitution?
