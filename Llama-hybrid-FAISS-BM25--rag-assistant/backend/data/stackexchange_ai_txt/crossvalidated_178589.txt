[site]: crossvalidated
[post_id]: 178589
[parent_id]: 178584
[tags]: 
I strongly encourage you to work through the case where $x$ can take two values (e.g. 0 = dominant, 1 = recessive as in your example), as it a useful exercise for understanding the logistic model and also the MLE approach. The model in question is supposed to be $$ Y_i \sim \mathrm{Bernoulli}(p_i), $$ where $p_i = \mathrm{logistic}(a + bx_i)$, $i = 1,\ldots,n$. Suppose the observed values are $y_1, \ldots, y_n$. Homework: What is the probability of observing this particular string of $y_i$ (assuming they are all independent). Hint: split according to whether $y_i = 0$ or $1$ and similarly for $x_i$. What is the log of this? What happens to the log likelihood if $a$ gets large or small (negative)? Same question for $b$. Conclude that you can maximise the log likelihood by differentiating in $a$ and $b$ then finding where the derivative is zero (separately for $a$ and $b$). OK, for the final answer: Write $\#_{l,m} = \#\{i : y_i = l, x_i = m\}$, so $\#_{0,1}$ is the number of observations with $y_i = 0$ and $x_i = 1$ etc. You'll see the problem can be parameterised more easily in terms of $$ p = \frac{e^{a}}{1 + e^{a}} $$ and $$ r = \frac{e^{a+b}}{1 + e^{a+b}}. $$ Assuming I didn't make a mistake, we find $$ r = \frac{\#_{1,1}}{\#_{0,1} + \#_{1,1}}, $$ i.e. the fraction of cases with $y_i = 1$ when the independent variable has $x_i = 1$. Then, $$ p = \frac{\#_{1,0}}{\#_{0,0} + \#_{1,0}} $$ is the fraction when $x_i = 0$. So, $a + b = \ln(r/(1-r))$ is the log odds for $y_i = 1$ when $x_i = 1$. Finally, $a = \ln(p/(1-p))$ and $b = \ln(r(1-p)/p(1-r))$ is the log odds ratio.
