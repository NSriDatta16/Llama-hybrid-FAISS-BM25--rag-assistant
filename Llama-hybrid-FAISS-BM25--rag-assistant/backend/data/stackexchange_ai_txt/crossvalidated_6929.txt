[site]: crossvalidated
[post_id]: 6929
[parent_id]: 6927
[tags]: 
It could possibly be a faster computer. But here is one trick which may work. Generate a simultation of $Y^*$, but only conditional on $Y$, then just do OLS or LMM on the simulated $Y^*$ values. Supposing your link function is $g(.)$. this says how you get from the probability of $Y=1$ to the $Y^*$ value, and is most likely the logistic function $g(z)=log \Big(\frac{z}{1-z}\Big)$. So if you assume a bernouli sampling distribution for $Y\rightarrow Y\sim Bernoulli(p)$, and then use the jeffreys prior for the probability, you get a beta posterior for $p\sim Beta(Y_{obs}+\frac{1}{2},1-Y_{obs}+\frac{1}{2})$. Simulating from this should be like lighting, and if it isn't, then you need a faster computer. Further, the samples are independent, so no need to check any "convergence" diagnostics such as in MCMC, and you probably don't need as many samples - 100 may work fine for your case. If you have binomial $Y's$, then just replace the $1$ in the above posterior with $n_i$, the number of trials of the binomial for each $Y_i$. So you have a set of simulated values $p_{sim}$. You then apply the link function to each of these values, to get $Y_{sim}=g(p_{sim})$. Fit a LMM to $Y_{sim}$, which is probably quicker than the GLMM program. You can basically ignore the original binary values (but don't delete them!), and just work with the "simulation matrix" ($N\times S$, where $N$ is the sample size, and $S$ is the number of simulations). So in your program, I would replace the $gmler()$ function with the $lmer()$ function, and $Y$ with a single simultation, You would then create some sort of loop which applies the $lmer()$ function to each simulation, and then takes the average as the estimate of $b$. Something like $$a=\dots$$ $$b=0$$ $$do \ s=1,\dots,S$$ $$b_{est}=lmer(Y_s\dots)$$ $$b=b+\frac{1}{s}(b_{est}-b)$$ $$end$$ $$return(a*b)$$ Let me know if I need to explain anything a bit clearer
