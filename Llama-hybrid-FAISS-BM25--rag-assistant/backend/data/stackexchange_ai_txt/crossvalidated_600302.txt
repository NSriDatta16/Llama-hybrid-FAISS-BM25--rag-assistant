[site]: crossvalidated
[post_id]: 600302
[parent_id]: 
[tags]: 
Are web-scale deep learning models considered to be supervised or unsupervised?

Consider recent large-scale deep learning models, such as transformers for NLP applications, or diffusion models for text-to-image generation. These models are trained on huge, readily-available datasets, acquired by scraping the web for text documents and images. My question is: Are these models considered to be trained with supervised learning, or unsupervised learning? I believe that there are arguments both ways, as follows. [1] They are trained with supervised learning, because we tell them what they are supposed to predict, i.e. the labels, and calculate a loss on that data. E.g. with NLP, we tell them to predict the next word from a sentence in the dataset, and with diffusion, we tell them to predict the image in the dataset. [2] They are trained with unsupervised learning, because we do not need to manually label the data. The data comes "for free", just by scraping the web, and that data can then be used in its original form, without humans needing to label it. Given that we then use this data as the labels (like in [1]), we can consider this to be self-supervised learning, which is a subset of unsupervised learning. I have seen both "supervised" and "unsupervised" applied to these kinds of models, so I am confused as to whether there is an accepted consensus within the community.
