[site]: crossvalidated
[post_id]: 328702
[parent_id]: 
[tags]: 
is One-hot encoding still used in ML models?

in machine learning, using One-Hot encoded data like words for RNNs (especially when we have a large dictionary like 10M or larger?) causes the dot product with the weight matrix slow. THE QUESTION IS: Is this true? and if yes, Is one-hot encoding still used even though? and, Does word embedding algorithms use one-hot encoding to represent data in training?
