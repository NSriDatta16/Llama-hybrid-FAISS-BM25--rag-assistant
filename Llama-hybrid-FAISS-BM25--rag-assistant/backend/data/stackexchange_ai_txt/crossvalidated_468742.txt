[site]: crossvalidated
[post_id]: 468742
[parent_id]: 
[tags]: 
mathematical definitions of neural network terminology: layer, hidden unit, neuron

I think about feedforward neural networks from a mathematical perspective. There is an input array $x_0$ (often an image) that is passed through several composed functions to arrive at an output. For example, the array might be passed through a convolution function $\text{conv}_1(\cdot)$ , a ReLU function $\text{relu}(\cdot)$ , another convolution function $\text{conv}_2(\cdot)$ , another ReLU function, then a max pooling function $\text{maxpool}(\cdot)$ , etc. The whole network might look like this: \begin{align} x_1 &= \text{conv}_1(x_0) \\ x_2 &= \text{relu}(x_1) \\ x_3 &= \text{conv}_2(x_2) \\ x_4 &= \text{relu}(x_3) \\ x_5 &= \text{maxpool}(x_4) \\ &\vdots \end{align} Despite reading many sources on neural networks, I am confused as to what the terms layer , hidden unit , and neuron refer to. For example, does "layer" refer to a function, or the arrays themselves? So is the "first layer" in the network above the $\text{conv}_1(\cdot)$ function, the $\text{relu}(\text{conv}_1(\cdot))$ function, the $\text{x}_0$ array, or the $\text{x}_1$ array? I have a similar confusion for the terms "hidden unit" and "neuron".
