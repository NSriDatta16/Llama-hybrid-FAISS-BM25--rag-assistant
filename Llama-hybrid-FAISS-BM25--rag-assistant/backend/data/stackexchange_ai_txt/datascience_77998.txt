[site]: datascience
[post_id]: 77998
[parent_id]: 
[tags]: 
Why autoencoders work well for outlier detection?

Apart from the fact that they are neural networks, which usually is a reason for outperforming other algorithms, is there other reason helping auto-encoders perform well in outlier detection? I know that autoencoders work by encoding a sample into lower-dimension representation, then decoding the representation to reconstruct the sample. As outliers usually have higher reconstruction, they can be detected. However, this does not convince me why auto-encoders can beat other methods. Is it because the outliers errors are very high, so that they can be spotted easily? If so, then what makes the reconstruction errors so high? Both intuitive and/or theoretical explanations are welcomed.
