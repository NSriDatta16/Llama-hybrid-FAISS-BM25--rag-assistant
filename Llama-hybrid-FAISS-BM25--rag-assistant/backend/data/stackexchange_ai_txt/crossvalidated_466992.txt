[site]: crossvalidated
[post_id]: 466992
[parent_id]: 466922
[tags]: 
First of all, this is not empirical Bayes. In empirical Bayes you estimate the priors from the data and then apply Bayes theorem (see nice example with further references). Unless I'm missing something, here you are not doing anything to estimate the prior distribution from the data, but rather assume some prior distribution. Couldn't I instead just try out random parameters $\theta$ for each $\eta$ and see how they do on average? What do you mean by "trying" different parameters in here? "Trying" sounds like you would like to take some random values for $\theta$ and evaluate the model on them. In such case, what would you be doing is taking samples from the prior predictive distribution of your model In Bayesian data analysis (Gelman et al. 2013) the marginal likelihood is called a prior predictive distribution . This is because it presents our beliefs about the probabilities of the data before any observations are made. It is a distribution of the data computed as a weighted average over all the possible parameter values , and the weights are determined by the prior distribution. This averages over possible results from your model, but tells you nothing about the optimal parameter values. There's also practical concern: in neural network you would usually have hundreds, thousands, or even hundreds of thousands, of the real-valued parameters, so the parameter space is very infinite. If you draw few hundred, or several thousand of the possible parameter values, this would be just a negligible fraction of all the possible parameter values and their combinations. Such Monte Carlo simulation wouldn't tell you much. Also, why looking at average performance in here? Why average over all the crappy results given the random parameters? Why not look at the best performance (i.e max, rather then average)? If you looked at best performance on the randomized parameters, this would basically be a random search . What your procedure is lacking, is either maximizing the posterior probability ( maximum a posteriori estimation), or sampling from the posterior (full Bayesian estimation), otherwise you are doing pure exploration. You may also be interested in reading about Bayesian optimization, just few days ago there was nice Distill tutorial by Agnihotri and Batara on it. In Bayesian optimization you also evaluate model on random parameters, but the parameters are carefully picked so to speed up exploration and finding the optimal ones. There is also a related concept of neural networks with random weights . There are some results showing, that it is possible to find neural network architectures that perform well without training , using random parameters. In such case, comparing different neural networks with random parameters would make sense. This doesn't have to work in all cases, and is considerably harder problem, then "just" training the network, but it is at least theoretically possible. Even though, finding such architecture would need much more sophisticated algorithm then the kind of random search that you propose.
