[site]: crossvalidated
[post_id]: 192286
[parent_id]: 
[tags]: 
Bias-variance decomposition

In section 3.2 of Bishop's Pattern Recognition and Machine Learning , he discusses the bias-variance decomposition, stating that for a squared loss function, the expected loss can be decomposed into a squared bias term (which describes how far the average predictions are from the true model), a variance term (which describes the spread of the predictions around the average), and a noise term (which gives the intrinsic noise of the data). Can bias-variance decomposition be performed with loss functions other than squared loss? For a given model dataset, is there more than one model whose expected loss is the minimum over all models, and if so, does that mean that there could be different combinations of bias and variance that yield the same minimum expected loss? If a model involves regularization, is there a mathematical relationship between bias, variance, and the regularization coefficient $\lambda$? How can you calculate bias if you don't know the true model? Are there situations in which it makes more sense to minimize bias or variance rather than expected loss (the sum of squared bias and variance)?
