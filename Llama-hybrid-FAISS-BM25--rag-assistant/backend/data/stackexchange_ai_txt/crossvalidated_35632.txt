[site]: crossvalidated
[post_id]: 35632
[parent_id]: 35590
[tags]: 
What you do really depends on the goals of the analysis. I'm not certain exactly what the goals of your analysis are, but I'll go through several examples, and hopefully one of them will be applicable to your situation. Case 1 : One quantitive variable measured twice Let's say that you ran a human subject study in which you had participants take a stats test twice and you wanted to find out if the average scores on the second measurement were different from the first measurement (to determine whether learning occurred). If scores test1 and test2 are stored in data frame d, You could do this entirely using the lm() function, as in: mod The test of the intercept is the test of the difference between test1 and test2. Note that you will have no delta-R^2 for the difference between test1 and test2 -- instead, your measure of effect size should be something like cohen's d. Case 2a : One quantitative variable measured twice, one dichotomous variable, measured totally between subjects Let's say that we have the same study design, but we want to know if different rates of learning occurred for men and women. So, we have one quantitative variable (test performance) that is measured twice, and one dichotomous variable, measured once. Assuming test1, test2, and gender are all contained in data frame d, We could also test this model only using lm(), as in: mod Assuming gender is centered (i.e., coded, for example, male = -.5 and female = +.5), the intercept in this model is the test of the difference between test 1 and test 2, averaged across males and females. The coefficient for gender is the interaction between time and gender. To get the effect of gender, averaged across time, you would have to do: mod Case 2b : One quantitative variable measured twice, one quantitative variable, only measured once Let's assume that again we have one quantitative variable measured twice and one quantitative variable measured once. So, for example, let's say we had a measure of baseline interest in statistics and we wanted to determine whether people who had higher levels of baseline interest learned more from time 1 to time 2. We'd first have to center interest, as in: d$interestc Assuming that test1, test2, and interestc are all in data frame d, this question could then be tested very similarly to Case 1a: mod Once again, the intercept in this model tests whether, averaged across interest, test scores changed from time 1 to time 2. However, this interpretation only holds when interest is centered. The coefficient for interest is whether the effect of time depends on baseline interest. We could get the effect of interest, averaged across time, by averaging together test1 and test 2, as above, and testing the effect of interest on this composite variable. Case 2c : One quantitative variable measured twice, one categorical variable, only measured once Let's assume that your between-subjects variable was a category, measured only once. So, for example, let's assume that you were interested in whether people of different races (White vs Asian vs Black vs Hispanic) had different amounts of learning from time 1 to time 2. Assuming test1, test2, and race are in data frame d, you would first need to contrast code race. This could be done using planned orthogonal contrasts, dummy codes, or using effects codes, depending on specific hypotheses / questions you want to test (I recommend looking at lm.setContrasts() if you're looking for a helper function to do this). Assuming the race variable is already contrast coded, you would use lm() very similarly to the above two cases, as in: mod Assuming the race contrasts are centered, the intercept in this model is, once again, the "main effect" of time. The coefficients for the race contrasts are the interactions between those contrasts and time. To obtain the omnibus effects of race, use the following code: Anova(mod, type = 3) Case 3 : One quantitative variable measured 3 times (i.e., a three-level within-subjects manipulation) Let's assume that you added a third point of measurement to the design from case one. So, your participants took a stats test three times instead of twice. Here you have a couple choices, depending on whether you want an omnibus test of the differences between time points (sometimes you don't). For example, let's say that your main hypothesis is that test scores will linearly increase from time 1 to time 3. Assuming that test1, test2, and test3 are in data frame d, this hypothesis could be tested by first creating the following composite: d$lin Then you would test whether an intercept only model using lin as the dependent variable has an intercept that is different from 0, as in: mod This will give you your test of whether stats scores were increasing over time. You can, of course, create other types of custom difference scores, depending on your particular hypotheses. If you care about omnibus tests of significance, you need to use the Anova() function from the car package. The specific implementation is a little convoluted. Basically, you specify which variables are within-subjects and which are between-subjects using lm(). You then create the within-subjects portion of the model (i.e., specify which of test1, test2, and test3 were measured first, second, and third) and then pass that model to Anova() by creating a data frame called idata. Using my hypothetical example: mod The idesign statement tells Anova to include the time variable (composed of test1, test2, and test3) in the model. This code will give you your omnibus tests of the effects of time on test scores. Case 4 : One quantitative variable measured 3 times, one between-subjects quantitative variable This case is a simple extension of Case 3. As above, if you merely care about 1 degree of freedom tests, you can simply create a custom difference score with your within-subjects variable. So, assuming that test1, test2, test3, and interest are all in data frame d, and assuming that we are interested in the linear effects of time on test scores (and how those effects of time vary with baseline interest), you would do the following: d$lin Then, do the following (with mean-centered interest): mod If you want omnibus tests, do the following: mod Other cases: I will omit these for brevity, but they are simple extensions of what I've already described. Please note that the (univariate) omnibus tests of time where time has more than 2 levels all assume sphericity. This assumption becomes pretty untenable as you increase the number of levels. If you have quite a few points of measurement in your design (say, 4+) I strongly recommend you use something like multilevel modeling and move to a package that is specialized for this technique (such as nlme or lme4 . Hope this helps!
