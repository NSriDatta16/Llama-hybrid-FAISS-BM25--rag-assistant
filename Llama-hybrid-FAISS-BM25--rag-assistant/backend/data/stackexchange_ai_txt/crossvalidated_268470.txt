[site]: crossvalidated
[post_id]: 268470
[parent_id]: 31985
[tags]: 
Thanx for this - good summary of background literature. The 1980 Shore and Johnson article in IEEE is a good start, but @itamar's pointer to the Good monograph from 1956 is even better. The concept seems to come from Shannon's work, with Kullback & Leibler's 1951 AMS note being the origin of the current use of the term. As far as the origin of the term "cross entropy" relates to artificial neural networks, there is a term used in a paper in Science, submitted 1994, published 1995, by G. E. Hinton, P. Dayan, B. J. Frey & R. M. Neal, in which there is an early use of the term "Hemholtz Machine" - possibly the first. Url for copy In that paper, "The Wake-sleep algorithm for unsupervised neural networks", the note before equation #5 says: When there are many alternative ways of describing an input vector it is possible to design a stochastic coding scheme that takes advantage of the entropy across alternative descriptions[1]. The cost is then:" (see paper for eqn#5) The second term is then the entropy of the distribution that the recognition weights assign to the various alternative representations. Later in the paper, eqn#5 is rewritten as eqn#8, with the last term described as the Kullback-Leibler divergence between the initial probability distribution, and the posterior probability distribution. The paper states: So for two generative models which assign equal probability to d, minimizing equation #8 with respect to the generative weights will tend to favour the model whose posterior distribution is most similar to Q(.|d) (Where Q(.|d) is the initial distribution you are training your net towards.) This paper still describes the minimization process for this specific algorithm as minimizing the Kullback-Leibler divergence, but it looks like it could be where the term "entropy across alternative descriptions" was shortened to just "cross entropy". For an numerical example of cross entropy, using TensorFlow, see the posting here, it is helpful: What is cross-entropy? Note that the solution of CE = 0.47965 is derived simply by taking the natural log of the .619 probability. In the above example, the use of "one hot" encoding means that the other two initial and posterior probabilities are ignored due to multiplication by zero-valued initial probability, in the summation for cross entropy.
