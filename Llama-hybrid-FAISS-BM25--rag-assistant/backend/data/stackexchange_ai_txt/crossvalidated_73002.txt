[site]: crossvalidated
[post_id]: 73002
[parent_id]: 72992
[tags]: 
I'm glad you like my answer :-) It's not that there is no valid method of detecting collinearity in logistic regression: Since collinearity is a relationship among the independent variables, the dependent variable doesn't matter. What is problematic is figuring out how much collinearity is too much for logistic regression. David Belslely did extensive work with condition indexes. He found that indexes over 30 with substantial variance accounted for in more than one variable was indicative of collinearity that would cause severe problems in OLS regression. However, "severe" is always a judgment call. Perhaps the easiest way to see the problems of collinearity is to show that small changes in the data make big changes in the results. [this paper http://www.medicine.mcgill.ca/epidemiology/joseph/courses/epib-621/logconfound.pdf] offers examples of collinearity in logistic regression. It even shows that R detects exact collinearity, and, in fact, some cases of approximate collinearity will cause the same warning: Warning message: glm.fit: fitted probabilities numerically 0 or 1 occurred Nevertheless, we can ignore this warning and run set.seed(1234) x1 which yields -2.55, 1.97, 5.60 and 12.54 We can then slightly perturb x1 and x2, add them for a new x3 and run again: x1a this yields wildly different coefficients: 0.003, 3.012, 3.51 and -0.41 and yet, this set of independent variables does not have a high condition index: library(perturb) colldiag(m1) says the maximum condition index is 3.54. I am unaware if anyone has done any Monte Carlo studies of this; if not, it seems a good area for research
