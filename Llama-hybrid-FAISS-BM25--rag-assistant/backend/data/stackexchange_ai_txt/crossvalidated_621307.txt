[site]: crossvalidated
[post_id]: 621307
[parent_id]: 
[tags]: 
Using regression predictions to evaluate randomised trials - good idea or not?

I need to evaluate the results of an experiment using a black-box ML model. Normally, if I have a series of outcomes from the control group $ X_i $ and the test group $ Y_i $ I would just do a t-test on the sequences $ X $ and $ Y $ to decide if their means differ. However in this case $ X $ and $ Y $ don't measure the actual outcomes, but instead have an error associated with each observation (which we only have an estimate of through backtesting/cross-validation). My questions are: Is this a valid approach to estimate a treatment effect? As far as I know the ML model doesn't do anything causal, it just makes predictions. How can I roll the error in the predictions into my power calculations so we can say things like "to see a minimum effect of 5%, you need N people if you are using predictions?" Example Use Case We have a population of customers at a subscription based internet company. We would like to test an intervention to raise the average revenue per user by X%, so we sample the customer base at random until we have N customers per variation (control and test). Normally I would have no hestiation in choosing N via the following formula: $$ N = 16 * \frac{\sigma^2}{\Delta^2} $$ Where $ \sigma^2 $ is the variance of the underlying metric (in this case revenue per user) and $ \Delta $ is the minimum effect I would like to detect. So for example, suppose you observe that 5% of your visitors make a purchase and the average purchase is $25 with a standard deviation of $ 6 during a one-week experiment period. If you want to run an experiment and detect a 5% change in revenue with 80% power, a rough sample size calculation indicates you need: $$ N = 16 \frac{\sigma^2}{\Delta^2} =16 * \frac{(6^2)}{(1.25 * 0.05)^2} = 147,456 $$ users per variant. Now, instead of the values of the metric that we observe during the experiment, we want to use predicted values to determine the outcome of an experiment. The data takes so long to mature that we can't feasibly run an experiment for that long. So the numbers in my metric, $ M $ , have an associated error. The predictions come from some ML model, with covariates $ X $ , and we have no access to the covariates, just that we know that the cross-validated and backtested accuracy of the ML is Y% (say 5%). My Q is: given this situation (and if we suspend our objections to it) how do we roll up the Y% error in the ML model to the sample size calculation? E.g. for a given N what is the $ \Delta $ we can detect in the underlying metric ? Or what is the N we need to detect a given $ \Delta $ in in the underlying metric I think another way of phrasing this question is that I don't observe the metric per user, but instead oberve: $$ \hat{M_i} = M_i + \varepsilon_i $$ i.e. I observe a value with some (known) error. So, what N do I need to sample to observe a 5% $ \Delta $ in $M$ if I can only observe $ \hat{M} $ ?
