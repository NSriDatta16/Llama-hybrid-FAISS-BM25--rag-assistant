[site]: datascience
[post_id]: 97809
[parent_id]: 
[tags]: 
SVMs: where comes the 1 and -1 in hyperplane equations and where is b?

Concering Support Vector Machines (SVM): it is always mentioned that $\textbf{w}^T\textbf{x}_i - b >= 1$ , for $\textbf{x}_i$ of class 1 (i.e. $y=1$ ) and $\textbf{w}^T\textbf{x}_i - b , for $\textbf{x}_i$ of class -1 (i.e. $y=-1$ ) from the Wikipedia page I read that it is related to normalization of the dataset, so I guess all the $\textbf{x}_i$ vectors are normalized (divided by) their norm ie $\textbf{x}_i$ := $\textbf{x}_i/|| \textbf{x}_i||_2$ so they all have norm 1 and projecting i.e. doing the scalar product $\textbf{w}^T\textbf{x}_i$ would give someting lower than 1 or 1 if $\textbf{w}$ was also normalized (which maybe it is not in general and so all this is not so clear?) So I would like a detailed (don't hesitate to put all "so-called" "trivial" steps!) explanation of why this is so? Also if possible a geometric explanation/drawing. Second question is about $b$ . For a line in 2D it would represent the y-intercept (only if the w coefficient of the parameter corresponding to y in the vector w would be 1, otherwise need to divide b by it...) ... so i m not completely sure what distance it would be geometrically... can anyone draw this clearly? It's a shape that these specific points I mention seem to be always missing (or treated as "trivial" which they are not!) in any lectures I've seen...
