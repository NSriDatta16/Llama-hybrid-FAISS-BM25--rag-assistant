[site]: crossvalidated
[post_id]: 568860
[parent_id]: 568857
[tags]: 
Of course can and should are different things. Of course you can take either the upper or lower triangle of the correlation matrix and train a machine learning model that takes them as input. While opinions, first principles, and guesswork might be helpful/misleading, the should part will become apparent from the performance of the models you try on such a feature space. Using such a feature space as the correlations will average out lots of details about the original dataset. And correlation is itself translation-invariant and absolute-scale invariant, which also loses information, but z-score standardization is common (and often useful) anyway. If you are predicting some variables from the correlation matrix in the training set, you will have to compute the corresponding correlation matrix for the test set as well. My advice is to do a few tests to see how well it works. The OP has clarified that they are not computing the usual Pearson correlation matrix, but rather a cross correlation. I quickly cooked up an example with correlation in the usual sense. For brevity I omit otherwise important aspects of machine learning such a cross validation. I may return to give a cross-correlation example in the future. First import some modules and set a seed value for reproducibility. import numpy as np from sklearn.svm import SVC np.random.seed(0) Then let's define some population covariance matrices for three classes. I later assume that the mean vectors are the zero vector since the Pearson correlations are translation invariant anyway. cov1 = np.eye(3) cov2 = np.ones((3,3)) cov3 = [[1,-1,0], [-1,1,-1], [0,-1,1]] covs = [cov1, cov2, cov3] Now let's construct a $3000 \times 3$ matrix (1000 per population) whose rows are the $m=10$ samples and whose columns are the off-diagonal correlations. The sampling assumes a trivariate Gaussian distribution for each population. I apologize that the code is needlessly complicated due to stream-of-consciousness coding. sample_corrs = [] k = 1000 m = 10 for i in range(k): sample = [np.random.multivariate_normal( np.zeros(3), cov=covi, size=10 ) for covi in covs] sample_corrs += [np.corrcoef(X_i.T)[np.triu_indices(3, 1)] for X_i in sample] y = np.array([0,1,2] * k) sample_corrs = np.array(sample_corrs) Then train a support vector machine classifier (using a radial basis function as the kernel by default). model = SVC() model.fit(sample_corrs, y) And check the accuracy on the training set. >>> print(np.mean(model.predict(sample_corrs) == y)) 0.934 So it appears you can discriminate populations based on their sample correlation matrices, at least in this idealized example. I'll skip doing the cross-validation and hyperparameter tuning here since my goal here was just to show that some sort of model could be trained on a correlation matrices to predict a class. Performing dimensionality reduction via principal component analysis on the $3000 \times 3$ matrix gives the following projection in 2D.
