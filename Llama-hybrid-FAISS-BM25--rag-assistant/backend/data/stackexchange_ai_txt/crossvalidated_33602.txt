[site]: crossvalidated
[post_id]: 33602
[parent_id]: 33520
[tags]: 
In short, and from your description, you are comparing apple to oranges....in two ways. Let me address the first comparability issue briefly. The log transform does not address the outlier problem. However, it can help making heavily skewed data more symmetric, potentially improving the fit of any PCA method. In short, taking the $\log$ of your data is not a substitute for doing robust analysis and in some cases (skewed data) can well be a complement. To set aside this first confounder, for the rest of this post, I use the log transformed version of some asymmetric bi-variate data. Consider this example: library("MASS") library("copula") library("rrcov") p Now, fit the two models (ROBPCA and classic pca both on the log of the data): x2 Now, consider the axis of smallest variation found by each method (here, for convenience, i plot it on the log-transformed space but you would get the same conclusions on the original space). Visibly, ROBPCA does a better job of handling the uncontaminated part of the data (the green dots): But now, I get to my second point. --calling $H_u$ the set of all green dots and $z_i$ ($w_i$) the robust (classical) pca score wrt to the axis of least variation -- you have that (this is quiet visible in the plot above): $$\sum_{i\in H_u}(z_i)^2 But you seem to be surprised that: $$\sum_{i=1}^n(z_i)^2>\sum_{i=1}^n(w_i)^2\;\;\;(2)$$ --the way you described your testing procedure, you compute the fit assessment criterion on the whole dataset, so your evaluation criterion is a monotonous function of (2) where you should use a monotonous function of (1)-- In other words, do not expect a robust fit to have smaller sum of squared orthogonal residuals than a non robust procedure on your full dataset: the non robust estimator is already the unique minimizer of the SSOR on the full dataset.
