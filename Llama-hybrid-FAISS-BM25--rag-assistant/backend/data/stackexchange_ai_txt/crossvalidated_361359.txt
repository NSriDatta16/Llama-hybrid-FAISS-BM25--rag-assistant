[site]: crossvalidated
[post_id]: 361359
[parent_id]: 361146
[tags]: 
One of the main methods of dealing with multicollinearity is through PCA (principal components analysis), which uses a linear combination of predictors. You might want to take a look at that. Principal Component Analysis to Address Multicollinearity (Lexi V. Perez) seems to contain a simple explanation. To me it's not really clear why the Kaggle project deals with the issue multicollinearity since they want to construct a prediction model (right?) and are not interested in the (conditional) effect estimate of individual predictors. To my knowledge multicollinearity has no big effect on prediction. (Overfitting might be an issue since the variable selection could be distorted) Do you know the issues of multicollinearity? Let me illustrate one of the issues using a simple example. Assume builds a model which estimates cholesterol levels based on different predictors like age, gender, weight, height, etc... Since weight and height are highly correlated, multicollinearity would distort the effect estimates of weight and height. Say the model results in $$\mathrm{Cholesterol = \ldots + 0.5 \cdot \mathrm{Weight}+0.7 \cdot\mathrm{Height}+ \ldots}$$ Usually you would interpret the effect estimates as: 'When all other covariates are kept constant, an increase of weight by 10 increases the cholesterol levels by 5' . However the multicollinearity issue could result in a biased effect estimate which would mean that the effect estimate 0.5 is useless... (Think like: it is impossible to keep the height constant when the weight increases by 10, so that interpretation is no longer valid). Often this issue is resolved through the usage of BMI. Combining the effect of weight and height (not in a linear fashion though, but still). Then the model would result in a effect estimate of BMI, which could be interpreted.
