[site]: crossvalidated
[post_id]: 60510
[parent_id]: 
[tags]: 
Does the central limit theorem hold for the prediction error over different samples?

Given an (infinite) data population from which you repeatedly draw samples of a fixed size. On each sample you learn a classifier which you then evaluate by computing the prediction error on a large independent test set. The prediction error is defined as the average over all instances in the test set of the zero-one loss function $\mathcal{L}(y,\hat{y})$. (The zero-one loss function $\mathcal{L}(y,\hat{y})=0$ if the predicted label $\hat{y}$ of an instance equals the true label of the instance y, and is 1 otherwise.) My question is whether the central limit theorem holds in this situation, so that the distribution of the prediction error over the samples approximates a normal distribution, given a sufficient number of samples? I know that the theorem holds for a sufficiently large number of independent random variables. So I am in doubt because the individual predictions for one test set originate from the same classifier, and the classifiers vary over the samples.
