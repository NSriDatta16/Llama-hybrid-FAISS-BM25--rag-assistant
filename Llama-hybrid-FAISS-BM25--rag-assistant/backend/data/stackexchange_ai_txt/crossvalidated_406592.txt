[site]: crossvalidated
[post_id]: 406592
[parent_id]: 406587
[tags]: 
The paper says to sort by distance to third nearest neighbor. You have only two neighbors. And you are sorting by distance to all the neighbors, then pick out the second/third neighbor value. Instead I think the code should be something like this: ... nbrs = NearestNeighbors(n_neighbors=3, metric='cosine').fit(tfidf_matrix) distances, indices = nbrs.kneighbors(tfidf_matrix) distances = distances[:,2] distances = np.sort(distances, axis=0) ... You could try to use word embeddings instead of TF-IDF, it might help things a bit, since they are designed factor nicely into N-dimensional feature space. However for a large general text corpus it could be that there are many different reasonable clusterings, and that different epsilons just give rise to groupings of different nature. Do you have a preference for for many small/detailed groupings, or to find few large/wide groupings, then using that can help decide a lot. If there is no general preference, you may want to look at topic modelling instead of clustering. Such as for example Latent Dirichlet Allocation or Non-negative Matrix Factorization. Scikit-learn has an example, https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html
