[site]: crossvalidated
[post_id]: 507168
[parent_id]: 507133
[tags]: 
This looks fine: You have completely separated the training and hyperparameter tuning sets from the calibration dataset. You are tuning hyperparameters based on roc_auc , which is mostly invariant to calibration. If you had been tuning based on some other score (one that isn't based solely on rank-ordering, and especially one that depends on a decision threshold*), you might have to worry whether your hyperparameters were optimal for the uncalibrated predictions but suboptimal for the calibrated ones. (*Anyway, then you'd be encouraged to search this site for proper scoring rules, why accuracy is bad, etc.) If your data is small, you might want to consider using a k-fold cv for the calibration, nesting the hyperparameter search inside that (or vice versa), so that you make more use of all the data. However, note that in that case the calibration actually produces an ensemble model.
