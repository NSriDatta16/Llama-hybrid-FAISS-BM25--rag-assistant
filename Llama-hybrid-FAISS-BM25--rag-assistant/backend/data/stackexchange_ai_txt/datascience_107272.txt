[site]: datascience
[post_id]: 107272
[parent_id]: 
[tags]: 
Image normalization and reverse normalization: colors lost on image generation (GAN)

I'm working on a Gan. Based on different papers, I use a Tanh activation function on the last layer of the generator. Which produces [-1,1] outputs. To make this coherent, I use image normalization with cv2: cv2.normalize(image, image, alpha=-1, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F) Example: origin image: normalized image: (first image is 32,32, second is probably 128,128 it is for the sake of demonstration) We can clearly see the colours are not the same. After training the models for 10 hours. I can generate all this images, which are interesting: Here's how I generate and plot my images: latent_points = generate_latent_points(100, 20) generated = generator_model.predict(latent_points) plt.figure(figsize=(10,10)) for i in range(generated.shape[0]): plt.subplot(4, 5, i+1) image = generated[i, :, :, :] image = np.reshape((image), [32, 32, 3]) plt.imshow(image, vmin=-1, vmax=1) plt.axis('off') plt.tight_layout() My question: is this kind of result normal? is it the way I normalize images that is wrong, or should I apply some transformation after the generation?
