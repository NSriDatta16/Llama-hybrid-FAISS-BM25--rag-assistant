[site]: stackoverflow
[post_id]: 1019559
[parent_id]: 265392
[tags]: 
One huge benefit of laziness is the ability to write immutable data structures with reasonable amortized bounds. A simple example is an immutable stack (using F#): type 'a stack = | EmptyStack | StackNode of 'a * 'a stack let rec append x y = match x with | EmptyStack -> y | StackNode(hd, tl) -> StackNode(hd, append tl y) The code is reasonable, but appending two stacks x and y takes O(length of x) time in best, worst, and average cases. Appending two stacks is a monolithic operation, it touches all of the nodes in stack x. We can re-write the data structure as a lazy stack: type 'a lazyStack = | StackNode of Lazy | EmptyStack let rec append x y = match x with | StackNode(item) -> Node(lazy(let hd, tl = item.Force(); hd, append tl y)) | Empty -> y lazy works by suspending the evaluation of code in its constructor. Once evaluated using .Force() , the return value is cached and reused on every subsequent .Force() . With the lazy version, appends are an O(1) operation: it returns 1 node and suspends the actual rebuilding of the list. When you get the head of this list, it will evaluate the contents of the node, forcing it return the head and create one suspension with the remaining elements, so taking the head of the list is an O(1) operation. So, our lazy list is in a constant state of rebuilding, you don't pay the cost for rebuilding this list until you traverse through all of its elements. Using laziness, this list supports O(1) consing and appending. Interestingly, since we don't evaluate nodes until their accessed, its wholly possible to construct a list with potentially infinite elements. The data structure above does not require nodes to be recomputed on each traversal, so they are distinctly different from vanilla IEnumerables in .NET.
