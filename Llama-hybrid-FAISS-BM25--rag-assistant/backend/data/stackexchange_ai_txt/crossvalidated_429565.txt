[site]: crossvalidated
[post_id]: 429565
[parent_id]: 429551
[tags]: 
The whole point of doing Bayesian Inference, is that you stipulate your data was generated by a model with unknown parameters. An example of this would be "the samples were all drawn independently from the same normal distribution, but we don't know the mean or variance of that distribution" We then try to use the data to come up with beliefs about the likely "true values" of the parameters of the model. Once you have stipulated your model, then $Pr(X|\theta)$ can be evaluated for any given $\theta$ (in the above example, $\theta$ refers to two parameters, the mean and variance/std of your normal dist). That's what "likelihood is fixed by the model" means. We then seek to come up with a posterior. When writing out Bayes Theorem as you did in your original post, people usually leave out this "|M" part or see it as implied, but it is probably more instructive to write: $Pr(\theta| X, M) = \frac{P(X|\theta, M)P(\theta |M)}{P(X| M)}$ $Pr(X|M)$ is a bit hard to interpret. The most simple (this isn't the deepest, but it's probably the best explanation to run with, when you're still learning) interpretation, is simply that $Pr(\theta|X, M) \propto Pr(X|\theta , M)Pr(\theta | M)$ . You require however, that $Pr(\theta|X, M)$ is normalised wrt $\theta$ , and thus $$Pr(\theta|X ,M)=\frac{Pr(X|\theta , M)P(\theta|M)}{\int Pr(X|\theta, M)Pr(\theta | M)d\theta}$$ Of course, from this, you can see that $Pr(X | M) = \int Pr(X|\theta, M)Pr(\theta |M)d\theta$ - when people say "it's fixed by the data", that means that it doesn't depend on $\theta$ , all $\theta$ -dependence has been integrated out. It does however still depend on which ever model you stipulated, which is easier to see when you don't leave out the "|M" In a sense, the above formula is just a statement about conditional probability, you could replace $\theta$ in it with k or "fred" and it would remain true. It just so happens that we know how to write down a formula for $Pr(X|\theta, |M)$ , because our model stipulates it, and we also have our prior $Pr(\theta |M)$ , so only if we expand in terms of $\theta$ , do we know how to evaluate it (i.e. $Pr(fred|M)$ is not defined) If you want more of an interpretational view, $P(X|M)$ is the probability of seeing the data we have seen, given that Model M generated it, but making no claim about what those parameters were. Instead, it's the weighted sum (integral) of the probability of Model M with parameters $\theta$ having generated the data, weighted by your prior beliefs about how plausible different values of $\theta$ are. Does this help clear things up?
