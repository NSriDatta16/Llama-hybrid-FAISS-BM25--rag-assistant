[site]: crossvalidated
[post_id]: 642876
[parent_id]: 
[tags]: 
Reward function definition in MRP/MDP, reinforcement learning different notations

I started to self-taught reinforcement learning a few weeks ago. These days I've encountered a problem with the definition of the reward function. The reward function, defines and quantifies the reward the agent will get, at (s) or (s, a) or (s, a, s') [depends on the reward function we defined in the specific problem]. We usually said the MDP is a 5-element tuple, which is (, , P, r, gamma) ,where: : the state set that contain all s , s ∈ : the action set that contains all action a , a ∈ gamma : discount factor P : transition probability, which is P(s' | s, a) However, for the " r ", " R ", or "" in the tuple, I found that different textbook defined it a bit differently... In Sutton & Barto's " Reinforcement Learning: An Introduction " : r is the reward Rt or Rt+1 is the actual reward the agent will get at a certain time step is the reward set that contains all possible reward values, which is a subset of real numbers r (s, a) or r (s) is the expected value of reward at (s, a) or (s) Actually I didn't see the word "5 tuple" in the textbook, they just simply defined a "joint" probability which is p(s', r | St = s, At = a) . In David Silver's lecture on introduction of RL: Rt or Rt+1 is the actual reward the agent will get at a certain time step is the "reward function" , which is the expected value of reward at (s, a) or (s) In some other sources (University of Toronto, CSC311 lecture slide) : Rt or Rt+1 is the actual reward the agent will get at a certain time step is a distribution of possible rewards at (s, a) (for a stochastic reward process) from [chrome-extension://fcejkolobdcfbhhakbhajcflakmnhaff/pages/viewer.html?file=https%3A%2F%2Fwww.cs.toronto.edu%2F~michael%2Fteaching%2Fcsc311_w23%2Flectures%2Flec12_mz.pdf#pagemode=thumbs] In addition, from some other answers here in StackExchange website, some people explained "reward function" as it defines the actual reward/reward distribution rather than an expected value of reward the agent will get at (s, a), or (s), or (s, a, s') I understand that in difference sources, the notation maybe a bit different. For a deterministic reward process, the expected value of reward is the actual reward value itself. For a stochastic reward process, the expected reward and the actual reward the agent will get from the distribution (or reward possibility distribution) are different concepts. I was just a bit confused by all these different 'standard' of notation when people define the term 'reward function'. For the definition of reward function r (s) , or r (s, a) , or r (s, a, s') , sometimes people refers the expected reward function, which defines the expected reward (in both deterministic & stochastic). Other times people refers to just the reward function, which defines the actual reward (deterministic)/reward distribution (stochastic).. I think in the field of RL, these two kinds of definitions of 'reward function' just being used interchangeably.. This question may looks a bit "waste of time", but I just post here for if someone also encounter a similar situation as me..Thanks
