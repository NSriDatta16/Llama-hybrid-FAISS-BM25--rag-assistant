[site]: crossvalidated
[post_id]: 337475
[parent_id]: 337377
[tags]: 
You seem to think that there is a case showing the variance being larger than the MSE, but it is far from clear how you are seeing that. In machine learning, Y is modeled as being equal to some function of X, plus a random error term. That error is, as it is in this example, often represented with an epsilon, $\epsilon$. In this model, an estimator function equal to the "real" dependency of Y on X will have an MSE equal to the variance of $\epsilon$. An estimator other than the "real" dependency will have an MSE equal to the variance of $\epsilon$, plus the variance between the "real" dependency and the estimator used. Thus, the MSE of the estimator will be greater than or equal to the variance $\underline{\text{of }}\underline{\epsilon}$. It can be, and any decent estimator will be, less than the variance $\underline{\text{of Y}}$. If the MSE of an estimator were greater than the variance of Y, then ignoring X completely and just predicting that Y will be equal to the mean of Y would be a better estimator.
