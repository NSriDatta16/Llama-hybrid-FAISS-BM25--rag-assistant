[site]: crossvalidated
[post_id]: 358657
[parent_id]: 205635
[tags]: 
As I understand your questions, what you picture is basically concatenating the input, previous hidden state, and previous cell state, and passing them through one or several fully connected layer to compute the output hidden state and cell state, instead of independently computing "gated" updates that interact arithmetically with the cell state. This would basically create a regular RNN that only outputted part of the hidden state. The main reason not to do this is that the structure of LSTM's cell state computations ensures constant flow of error through long sequences . If you used weights for computing the cell state directly, you'd need to backpropagate through them at each time step! Avoiding such operations largely solves vanishing/exploding gradients that otherwise plague RNNs. Plus, the ability to retain information easily over longer time spans is a nice bonus. Intuitively, it would be much more difficult for the network to learn from scratch to preserve cell state over longer time spans. It's worth noting that the most common alternative to LSTM, the GRU , similarly computes hidden state updates without learning weights that operate directly on the hidden state itself.
