[site]: crossvalidated
[post_id]: 158840
[parent_id]: 
[tags]: 
In a Boltzmann machine, why isn't there a simple expression for the optimal edge weights in terms of correlations between variables?

Suppose I have a fully connected, fully visible Boltzmann machine (no hidden variables) with binary variables $x_i\in \{+1, -1\}$ that defines the probability distribution $$ p(\mathbf{x} ; \mathbf{W},\mathbf{b}) = \frac{1}{Z} \exp \left( \frac{1}{2} \mathbf{x}^T \mathbf{W} \mathbf{x} + \mathbf{x}^T \mathbf{b} \right) $$ and want to choose edge weights $\mathbf{W}$ and bias $\mathbf{b}$ to maximise the likelihood of observing a set of training data. If we do this by using gradient ascent on the log-likelihood function, each step of gradient ascent involves an expensive expectation estimate using MCMC (or some cheaper approximation). Conceptually the edge weights represent the "interaction strength" between variables, i.e. $w_{ij}$ represents how much $x_i$ and $x_j$ "want" to be equal. Just looking at the above we can see that when $w_{ij}$ is large and positive, $x_i$ and $x_j$ have a high probability of being equal and the when it's negative they have a higher probability of being opposite sign. What is the relationship between the empirical correlation between each $x_i$ and $x_j$ versus the optimal edge weight $w_{ij}$? It would make sense that variables that are highly positively correlated have large positive edge weights, and variables that are negatively correlated have negative edge weights. But this would imply that learning the edge weights is easy, because we could just calculate the correlations, apply some mapping and get the edge weights. Obviously that is not true or we wouldn't need the expensive algorithm. Why isn't there a simple expression for the optimal edge weights in terms of the correlations between each pair of variables?
