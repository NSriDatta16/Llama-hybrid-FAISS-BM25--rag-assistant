[site]: crossvalidated
[post_id]: 177670
[parent_id]: 66190
[tags]: 
First, make sure you shuffle your data before you separate it in to the 50 partitions. If the original data is at all ordered, this can make a huge difference in the performance of your final model. Also, make sure your 50 partitions are stratified, meaning they have roughly the same number of exemplars per class as the overall data set. Then go ahead and train your 50 models (how you handle the hyperparameter selection is a bigger problem). When you want to make a prediction, do one of the following, all variations on boosting: 1) Hard-decision boosting: Evaluate all 50 models and treat the output of each model as a vote. Whichever class gets the most votes wins. The problem with this approach is that each model gets an equal vote, regardless of the confidence of its prediction. 2) Soft-decision boosting: Evaluate all 50 models with the parameter decision.values = TRUE . This tells e1071 to return the decision value in addition to the class label. Average the decision values returned by each of the 50 models and make a prediction based on the average. The problem with this approach is that the decision values will not necessarily be scaled equally across all models. In other words, a decision value of 1.618 will not necessarily mean the same thing across all 50 models. 3) Soft-decision boosting with logistic meta-model: Train and evaluate the models with the parameter probability = TRUE . This will fit a logistic regression on top of the SVM model that scales the output to be in the range [0, 1]. This is sort of a kludge but it may help with the scaling problem of option 2. A final word of caution - you say there are only 20 features you care about but each feature can have 1000+ levels. By your use of the word "levels", I assume these features are discrete. Are they ordinal or categorical? In other words, is there an order to the levels? Is level 1 closer to level 2 than level 3? If so, you're probably good to go. If not, you might want to consider re-coding the categorical using one-hot encoding.
