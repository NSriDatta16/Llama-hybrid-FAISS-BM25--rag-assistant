[site]: datascience
[post_id]: 32066
[parent_id]: 
[tags]: 
Could someone explain to me how back-prop is done for the generator in a GAN?

I'm not very familiar with neural networks, however, I though I understood the concept of back propagation as starting from the error in the output layer. Say, we have 3 neurons in the output layer and their respective values end up being: [1 0.5 0.3] And we wished to obtain values [0 1 0] So we can compute a vector of errors between the two: [-1, +0.5, -0.3] (Not necessarily with the - operation, but you get the point) And back propagate from there. However, in a the generator of a GAN, it seems to me that the output layer has a bunch of neurons (representative to whatever size the entity we want to generate is) but the error is based only on a % of images the discriminator classified wrongfully. So how exactly do we do back-prop for the generator ? The only human readable examples of GAN I've found use frameworks and libraries like autoGrad that kind of obfuscate the problem for me :/ Is there a simple way to explain how this backprop can be done ? (Like, in code, not with an "intuitive example" or "simple 4 variable equation where each of the 4 variable packs 50 years of mathematics behind it")
