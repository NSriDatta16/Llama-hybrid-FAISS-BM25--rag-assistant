[site]: stackoverflow
[post_id]: 2466527
[parent_id]: 2466169
[tags]: 
Not sure I understood the question correctly, but I think a quicksort-like approach might help: 10 split the file into N subfiles, one for each core/cpu 20 sort each partial file using the solutions suggested in some of the answers here 30 once every file is split and sorted, get the first line from each file and put it into a temporary file 40 get the second line from each file, put them in a second temporary file 50 repeat until you have number of temp files == number of cores 60 GOTO 20 Depending on the number of passes, you should approach a perfectly sorted file. Note that this is not a perfect solution . However, even in a couple of passes it should give you a reasonably well-sorted list of the longest lines in the first temporary file (I am assuming a Gaussian distribution of the length of the lines in the original long file). ps: if the partial files are still bigger than the available memory split them again until they fit (depending on the sorting algorithm you're using for each file, tho). But in this case you'll need to double the number of passes to get a reasonable approximation ps2: I also assume you're not interested in a perfectly sorted file but more in the statistical significance of the data (i.e. how long are long lines on average, etc).
