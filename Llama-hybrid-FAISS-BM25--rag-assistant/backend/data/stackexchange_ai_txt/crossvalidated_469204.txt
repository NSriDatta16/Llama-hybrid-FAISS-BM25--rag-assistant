[site]: crossvalidated
[post_id]: 469204
[parent_id]: 
[tags]: 
How to Calculate the Amount of Floating Point Operations Needed to Train a Neural Network

Assuming a model with N parameters, arranged in an arbitrary architecture, training on M training examples for E epochs, how does one calculate within an order of magnitude or (hopefully) better the number of floating point operations needed to train that model? I an find many numbers around the web for GFLOPS needed to train, but I'm not looking for FLOPS, I want the integral of FLOPS.
