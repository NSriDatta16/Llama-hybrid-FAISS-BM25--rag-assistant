[site]: crossvalidated
[post_id]: 550159
[parent_id]: 
[tags]: 
Are numerical solutions appropriate for inference (eg, estimating variance for confidence intervals)?

For a nicely differentiable objective function, we traditionally always derive the gradients to use for e.g. estimating the variance. (1) Is it common nowadays to use numerical rather than analytical gradients this? I know it is slower (for the computer at least), of course, but would the coverages be reasonable? (2) Are there any general results on whether this is a good idea? I know that in many cases the answer would be that it depends, but what if there is a very complex objective that would be difficult to differentiate by hand - can we use numerical gradients and sleep soundly? Update: I learned just now that there is a distinction between eg autodiff and finite difference, and that the former is exact up to machine epsilon. I guess I was thinking along the lines of the state of the art (so I guess autodiff). R's numDeriv package uses numerical differentiation, but with Richardson's extrapolation. R's deriv uses autodiff.
