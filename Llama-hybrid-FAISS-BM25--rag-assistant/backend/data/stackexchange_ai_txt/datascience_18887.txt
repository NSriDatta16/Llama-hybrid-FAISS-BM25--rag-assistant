[site]: datascience
[post_id]: 18887
[parent_id]: 
[tags]: 
What did Geoffrey Hinton mean when he said this?

I was going through some lectures from the Deep Learning course that Geoffrey Hinton taught on Coursera and I came across this statement: "RNNs could potentially learn to implement lots of small programs that each capture a nugget of knowledge and run in parallel, interacting to produce very complicated effects." I have no idea what he meant nor seen/figured out any examples of this idea (and I'm sure other people are wondering too.) So if somebody would care to demonstrate, it would be much appreciated.
