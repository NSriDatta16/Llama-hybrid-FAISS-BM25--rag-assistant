[site]: datascience
[post_id]: 75185
[parent_id]: 
[tags]: 
how do deep Q network deal with varying input size?

I am conducting research with multiply agents in an environment. The main concept of my methodology is a centralized control system, which means we take the positions, as well as other information, of all agents as a collection. Say we have 5 agents in the environment, the state is S = [X_0, X_1, X_2, X_3, X_4] And the output we want is also a vector(matrix): A = [a_0, a_1, a_2, a_3, a_4] I have managed to apply this as inputs to a fully connected Deep Q Network. The input layer is 5 nodes, which contain the positions in the grid world. However, the number of agents is unknown in the environment, and if there are n = 4 or 6 agents, the fully connected neural network can't be applied to yield the action of desire. We have to build a new neural network. What are some nice ways to deal with this, so that we can have a DQN network structure dealing with varying size of input layers. So we only need to train the network once and apply it for offline training. I did some research on it, it suggests using recurrent neural network (RNN), while deep recurrent neural network (DRQN) seems to aim for a different target. I also think about "padding" an input state metric containing, e.g, 100 agents, but only first n agents observed by the environment are fulfilled with meaningful information. The rest is nullified so that they wouldn't interact with the dynamics of the environment. S = [X_0, X_1, X_2, ..., X_99] where X_n, X_n+1,...,X_99 = 0(or sth invalid, as long as it doesn't influence next state) I am wondering if such padding strategy would work, and how efficient it would be because it seems like we built an extremely large and inefficient neural network structure. Thanks in advance!
