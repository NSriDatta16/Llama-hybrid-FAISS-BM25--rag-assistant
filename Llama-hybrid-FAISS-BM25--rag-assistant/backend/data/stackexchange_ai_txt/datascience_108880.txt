[site]: datascience
[post_id]: 108880
[parent_id]: 108875
[tags]: 
Your understanding is correct: During training, the gold tokens are used as input to the decoder, not the predictions; that's why "there is no feedback". This is called "teacher forcing" and it is the usual approach to train sequence prediction models, also for LSTMs and other RNNs. During inference (text generation), the previous token predictions are fed to the decoder as input. Therefore, there is feedback. To predict the first token, the input is a special token indicating the beginning of sequence (bos).
