[site]: crossvalidated
[post_id]: 601295
[parent_id]: 
[tags]: 
Ordinal regression - 'induced Dirichlet' conditional posterior distribution

I am trying to implement the 'induced Dirichlet' prior model proposed by Michael Betancourt (from section 2.2 of his ordinal regression case study here: https://betanalpha.github.io/assets/case_studies/ordinal_regression.html#22_Surgical_Cut ) using a Gibbs sampler rather than using Stan. However, I am unsure of how the conditional posterior distribution for this model would look like. The reason why I am using a Gibbs / MH sampler as opposed to Stan is because I have found Stan to be very inefficient for models which require data augmentation (e.g. for a dataset with binary outcomes Stan takes over 15 mins to fit whereas Gibbs/MH takes about 2 minutes to get equivelent effective samples and R-hat statistics). I know that ordinal regression can be coded in Stan without data augmentation, however the models than I am using are multivariate latent class probit regression models - these can only be coded in Stan using data augmentation. To simplify this post, I am just writing it in the context of a standard ordinal regression model with one outcome as opposed to multivariate. I will start by giving the posterior distributions of all the parameters of the ordinal regression model assuming a standard uniform prior for the cutpoints, as in Albert and Chib 1993 (see https://apps.olin.wustl.edu/faculty/chib/papers/albertchib93.pdf ): For the coefficients, $\boldsymbol\beta = \left( \beta_1, \dots, \beta_P \right)$ , if the prior is $\boldsymbol\beta \sim N_P(\boldsymbol\beta^{*}, \mathbf{B}) $ , then the the conditional posterior distribution is given by: (1) $ \pi(\boldsymbol\beta | \mathbf{Z}) = N_P\left( (\mathbf{B^{-1} + X^{T} X)^{-1} ( \mathbf{ B^{-1} \boldsymbol\beta^{*} + X^{T} Z})} , ( \mathbf{ B^{-1} } + \mathbf{X^{T} X)^{-1} } \right) $ Where $\mathbf{Z}$ is continuous augmented latent data, $\mathbf{Y}$ is the observed data, and $\mathbf{X}$ is the design matrix. The conditional distribution for the latent data, $\mathbf{Z}$ , is given by: (2) $ \pi(\mathbf{Z} | \boldsymbol\beta, \mathbf{C, Y}) = N(X_{n}^{T} \beta, 1) $ which is truncated depending on the observed data $\mathbf{Y}$ such that: $ Y_{n} = k$ if $ C_{k-1} for individual $n$ . Finally, the conditional distribution for the cutpoint parameters, $\mathbf{C}$ , assuming a uniform prior, is given by: (3) $ \pi(\mathbf{C} | \boldsymbol\beta, \mathbf{Y, Z}) = \text{Uniform}\left(Max \{ Max\{Z_n : Y_n = k \}, C_{k-1} \}, Min \{ Min\{Z_n : Y_n = k + 1 \}, C_{k+1} \} \right) $ What I need to do is replace (3) with the conditional posterior distribution assuming an induced Dirichlet prior for the induced ordinal probabilities , rather than uniform prior on the cutpoints . The induced Dirichlet \textbf{ prior model } is given as follows (see case study from Michael Betancourt linked above for more detailed explanation): We assume a Dirichlet prior model on the ordinal probabilities: $(p_{1}, \ldots, p_{K}) \sim \text{Dirichlet}(\alpha_1, \ldots, \alpha_K)$ and hence an 'induced Dirichlet' prior model on the cutpoints: $(C_{1}, \ldots, C_{K}) \sim \text{Dirichlet}(\alpha_1, \ldots, \alpha_K) \cdot |J| $ where $J$ is the jacobian adjustment for the tranformation $ \mathbf{C \rightarrow p} $ Conditional on an anchor point, $\phi $ , which we will set to 0, the prior ordinal probabilities are: $ p_{k} = \Phi(\phi - c_{k - 1}) - \Phi(\phi - c_{k}) = \Phi(-c_{k - 1}) - \Phi(-c_{k}) $ where $\Phi$ is the CDF of a standard normal The prior cutpoints are given by: $ c_{1} = \phi - \Phi^{-1}(1 - p_{1}) = - \Phi^{-1}(1 - p_{1}) \\ c_{k} = \phi - \Phi^{-1}( \Phi(\phi - c_{k - 1}) - p_{k}) = - \Phi^{-1}( \Phi( - c_{k - 1}) - p_{k}) $ Now, in order to implement this prior model in a Gibbs sampler, I need to derive the conditional \textbf{posterior} distribution for the induced Dirichlet model. I think that the posterior distribution for the induced ordinal probabilities is (also Dirichlet since if the prior is Dirichlet so is the posterior) : (4) $ \pi(\mathbf{p} | \mathbf{Y}) = \text{Dirichlet}(\alpha_1 + N_1, \ldots, \alpha_K + N_K) $ where $N_k$ is the number of observations in category $k$ . Then, from the probabilities obtained by sampling from the posterior conditional distribution, I calculate the posterior cutpoints by using the mean of $(X^{T} \boldsymbol\beta ) $ as the posterior 'anchor point': $ c_{1} = \text{mean}(X^{T} \boldsymbol\beta ) - \Phi^{-1}(1 - p_{1}) \\ c_{k} = \text{mean}(X^{T} \boldsymbol\beta ) - \Phi^{-1}( \Phi( \text{mean}(X^{T} \boldsymbol\beta ) - c_{k - 1}) - p_{k}) $ My attempt so far in implementing the induced Dirichlet prior model using a Gibbs sampling algorithm has been to sample from the conditional posterior distributions (1), (2), and then (4), and then from the probabilities obtained in (4) I calculate the posterior cutpoints. However, I do not think this is right since I obtain weird estimates (e.g. the coefficients sometimes end up being > 20 even if I set an N(0,1) prior to them). Is this approach completely wrong? Do I need a Metropolis Hastings step since we are using a non-standard prior? If so what would the proposal distribution look like? I am new to writing algorithms (prior to this I only used pre-built packages which do all the algorithms for you such as WinBUGS, JAGS and Stan) so I'm not sure what the right approach is or whether this is even possible using Gibbs / MH. Any help / suggestions would be greatly appreciated.
