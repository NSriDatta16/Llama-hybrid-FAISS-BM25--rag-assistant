[site]: crossvalidated
[post_id]: 582307
[parent_id]: 582306
[tags]: 
I made up a simpler, more extreme example that strongly hints that the problem is nonlinear averaging, not some rounding issue. library(broom) dd dplyr::group_by(x) |> dplyr::summarise(pct = mean(y), oddsratio = pct/(1-pct)/overall_odds) ## x pct oddsratio ## 1 a 0.95 17.2 ## 2 b 0.1 0.101 tidy(glm(y ~ x, family = binomial, dd), exponentiate = TRUE) ## (exponentiate = TRUE returns estimates as odds ratios ## rather than log-odds diffs) ## term estimate std.error statistic p.value ## 1 (Intercept) 1.45 0.284 1.32 1.88e- 1 ## 2 xa 13.1 0.284 9.07 1.23e-19 A little bit more digging in te Grotenhuis et al 2017 explains what's going on: Likewise, if a researcher wishes to investigate obesity (BMI > 30) and, therefore, uses a dichotomy of BMI in a logistic regression analysis, then the mean to be tested against is the average of all log odds . (emphasis added). So the value that you compare against should not be the overall odds for the data set ( $p/(1-p)$ ), but the geometric mean of the odds for each group (i.e. $\exp(\textrm{mean}(\log(p_i/(1-p_i)))$ ). Continuing the example above: dds dplyr::group_by(x) |> dplyr::summarise(pct = mean(y), odds = pct/(1-pct))) overall_odds dplyr::mutate(odds_adj = odds/overall_odds)) Now the odds_adj column matches the values calculated by glm() : x pct odds odds_adj 1 a 0.95 19.0 13.1 2 b 0.1 0.111 0.0765 te Grotenhuis, Manfred, Ben Pelzer, Rob Eisinga, Rense Nieuwenhuis, Alexander Schmidt-Catran, and Ruben Konig. “When Size Matters: Advantages of Weighted Effect Coding in Observational Studies.” International Journal of Public Health 62, no. 1 (January 1, 2017): 163–67. https://doi.org/10.1007/s00038-016-0901-1 .
