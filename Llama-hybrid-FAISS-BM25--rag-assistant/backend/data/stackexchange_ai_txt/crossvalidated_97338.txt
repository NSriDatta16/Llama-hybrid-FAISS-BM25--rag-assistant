[site]: crossvalidated
[post_id]: 97338
[parent_id]: 
[tags]: 
Why do ANOVAs and GLM (negative binomial model) give different results for interaction effects?

In my study, children repeat sentences and I count the errors (DV = error rate). Children are divided into two groups (Group factor), and there are two different types of sentences (Condition factor). I am particularly interested in the Group by Condition interaction, which is theoretically important in my field (developmental psycholinguistics). When I run a by-subjects ANOVA, the "gold standard" in my field, I get a significant Group by Condition interaction. The assumptions of the ANOVA pretty much hold (normal distribution of residuals according to a QQ plot, homogeneity of variance (Levene's), Sphericity (Mauchly's)). The data themselves are not normally distributed, but I gather it's the distribution of the residuals that really matters. I'm also interested in applying GLM approaches. The raw data (i.e. both groups combined not averaged in any way) are strongly right-ward skewed and are well-approximated by a negative binomial model (I've used STATA nbvargr routine). When a negative binomial regression is run the residuals show a normalish distribution according to a QQ plot (better than the poisson and gaussian models) though not nearly as good as the ANOVA. I also get a significant interaction but it is in the OPPOSITE direction to that of the ANOVA (a negative as opposed to positive coefficient). I think this is because the ANOVA assumes an additive model whereas the negative binomial assumes a multiplicative model (as it employs logs). My question is, which analysis is "right", i.e. which best describes the underlying processes which result in the observed data? Though it has to be said that the ANOVA fit is excellent, there is a substantial loss of data as they are averaged for each cell of the ANOVA design (participant by group by condition = 50 observations per cell). From this perspective it's no wonder that the fit of the negative binomial model is not quite as good, as the model has to account for 50 times the data. In addition, the negative binomial model attempts to model the data generating process, i.e. a poisson process, whereas I don't think the same thing can be said for the ANOVA. (NB the issue of how to interpret interactions, and whether to use an additive or multiplicative model is hot issue in Psychiatry (Kendler, K. S., & Gardner, C. O. (2010). Interpretation of interactions: guide for the perplexed. The British Journal of Psychiatry, 197(3), 170â€“171. doi:10.1192/bjp.bp.110.081331)) Here's some code showing how the interaction effect is negative for the negative binomial model, but positive for the ANOVA. ** ld = error rate, DV ** limp = group (0 = control group, 1 = group of interest) ** hard = experimental condition (0 = "easy" condition, 1 = "hard" condition) ** morphemes = length of sentence in morphemes, reqd as an offset for ***negative binomial . nbreg ld limp##hard, robust cluster(id) offset(morphemes) Fitting Poisson model: [model-fitting procedure omitted for simplicity's sake] Negative binomial regression Number of obs = 3397 Dispersion = mean Wald chi2(3) = 116.39 Log pseudolikelihood = -7574.8427 Prob > chi2 = 0.0000 (Std. Err. adjusted for 34 clusters in id_num) ------------------------------------------------------------------------------ | Robust ld_morph_br | Coef. Std. Err. z P>|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- 1.limp | 2.235774 .2752258 8.12 0.000 1.696341 2.775206 1.hard | .7013264 .1065947 6.58 0.000 .4924047 .9102482 | limp#hard | 1 1 | -.3988122 .123109 -3.24 0.001 -.6401013 -.1575231 | _cons | -10.25841 .2499824 -41.04 0.000 -10.74837 -9.768454 morphemes | (offset) -------------+---------------------------------------------------------------- /lnalpha | .696452 .0583161 .5821544 .8107495 -------------+---------------------------------------------------------------- alpha | 2.006621 .1170184 1.78989 2.249593 ------------------------------------------------------------------------------ Now dataset is collapsed to obtain the mean error rate per participant for each cell of the ANOVA design . collapse (mean) ld_morph_br, by(id_num limp hard) . . anova ld_morph_br limp / id|limp hard limp#hard, repeated(hard) Number of obs = 68 R-squared = 0.9819 Root MSE = .323407 Adj R-squared = 0.9621 Source | Partial SS df MS F Prob > F ------------+---------------------------------------------------- Model | 181.662443 35 5.19035552 49.62 0.0000 | limp | 126.729506 1 126.729506 83.17 0.0000 id_num|limp | 48.7592906 32 1.52372783 ------------+---------------------------------------------------- hard | 5.62123672 1 5.62123672 53.74 0.0000 limp#hard | .552409685 1 .552409685 5.28 0.0282 | Residual | 3.3469382 32 .104591819 ------------+---------------------------------------------------- Total | 185.009381 67 2.76133405 Between-subjects error term: id_num|limp Levels: 34 (32 df) Lowest b.s.e. variable: id_num Covariance pooled over: limp (for repeated variable) Repeated variable: hard Huynh-Feldt epsilon = 1.0323 *Huynh-Feldt epsilon reset to 1.0000 Greenhouse-Geisser epsilon = 1.0000 Box's conservative epsilon = 1.0000 ------------ Prob > F ------------ Source | df F Regular H-F G-G Box ------------+---------------------------------------------------- hard | 1 53.74 0.0000 0.0000 0.0000 0.0000 limp#hard | 1 5.28 0.0282 0.0282 0.0282 0.0282 Residual | 32 ----------------------------------------------------------------- Command "regress" used to obtain coefficients . regress Source | SS df MS Number of obs = 68 -------------+------------------------------ F( 35, 32) = 49.62 Model | 181.662443 35 5.19035552 Prob > F = 0.0000 Residual | 3.3469382 32 .104591819 R-squared = 0.9819 -------------+------------------------------ Adj R-squared = 0.9621 Total | 185.009381 67 2.76133405 Root MSE = .32341 ------------------------------------------------------------------------------ ld_morph_br | Coef. Std. Err. t P>|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- 1.limp | 3.139737 .3327826 9.43 0.000 2.461881 3.817593 | id_num#limp | | [omitted for simplicity's sake] | 1.hard | .3947685 .1109275 3.56 0.001 .1688164 .6207205 | limp#hard | 1 1 | .3605257 .1568752 2.30 0.028 .0409813 .6800701 | _cons | .9226158 .2353129 3.92 0.000 .4432992 1.401932 ------------------------------------------------------------------------------ Now we have a positive coefficient
