[site]: crossvalidated
[post_id]: 545569
[parent_id]: 545559
[tags]: 
I agree with the other answers, there are a lot of things that could go into an 'optimal' train test split for time series such as ensuring that you have complete cycles of seasonality. For example, if you have monthly data and only 28 months you are probably better off doing only a 4 month test split for models that require 2 full seasonal cycles and doing seasonal tests before hand. If it isn't seasonal then you could do longer test splits. Similarly, if there are massive changes (such as COVID in this example) you will want the train split to have some of that data or else the best model will just be the most lucky one. The most stable procedure in my experience is to select a test split that allows the models to see all the information they will need and then do time series cross validation through the test split. I have also done some work with reinforcement learning where we feed the agent relevant time series features and allow it to choose the train-test split size to minimize actualized forecast error. It performed better than any static test size but worst than cross validation. So that could potentially work in general but probably not helpful for you right now. I would do a combination of both other answers with the last year held out and do time series cross validation to that split.
