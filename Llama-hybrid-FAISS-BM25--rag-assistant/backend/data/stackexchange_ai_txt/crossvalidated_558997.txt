[site]: crossvalidated
[post_id]: 558997
[parent_id]: 557769
[tags]: 
Generally transfer learning is used as an umbrella term for any procedure that could leverage pre-trained models for different tasks. Currently, it is de-facto practice in NLP and multi task learning and also Google's new architecture pathways . In strict sense, transfer learning implies using weights coming from the previous experience, given architecture is fixed. Simple weight transfer might not be sufficient in this strict setting, all the state of the previous model should be retained. Then, training is resumed with the new data. However, freezing earlier layers may not be full transfer-learning, as in changing the task but fine tuning for a closely related learning. It appears to be the "degree of how much to transfer" is controlled by freezing part of the network weights. An other justification for freezing certain parts of the weights are to prevent catastrophic forgetting, but this appears to be again an open research field, see here See also: Foundation models : Is it a new paradigm for statistics and machine learning? Transfer Learning on Autoencoders? How to resume training in neural networks properly? .
