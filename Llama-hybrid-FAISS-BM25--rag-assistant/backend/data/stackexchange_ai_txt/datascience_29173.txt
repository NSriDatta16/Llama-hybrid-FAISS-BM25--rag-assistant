[site]: datascience
[post_id]: 29173
[parent_id]: 
[tags]: 
time series forecasting - sliding window method

I have been trying to understand this sliding window technique but to no avail and really unsure as to how I would implement it. My dataset: I have hourly values for the electric load for a year (over 8700 data points) - image below. I'm going to split the dataset into a training set (1 Jan to 30 Sept) and a test set (1 Oct to 31 Dec). I'm going to use supervised learning techniques such as Regression trees and random forests (basically anything that is available to me in scikit-learn ), train them on the training set then make predictions on the test-set. I understand that I need to use historical known values as input features to input into a model. As a result, I created "Load_lagN" . Qs: Am I correct in saying that because I have created 10 lagged variables ( $Xt-1$ to $Xt-10$ ), this is the equivalent of using a sliding window of size 10? How would I then just simply train the model on the training set and make predictions on the test set without using the sliding model? With the sliding window model, does that assume only the past n values (10 in my case) are relevant? If I have split my dataset into a training and test set, how does the regression tree become trained and then make predictions in the test set (I'll stick to one-step-ahead forecasting for now) whilst implementing this sliding window technique? I've come across this method - TimeSeriesSplit() - is this what I need to use for the sliding window technique or is it only a cross validator? My understanding of the sliding window method: As you can see in the image below, I use values of 10:00 to 19:00 to predict the value at 20:00, move the window so that this new value is now included, and then predict the value for 21:00. This keeps happening until I have exhausted the training set. I then make predictions. What are your thoughts?
