[site]: datascience
[post_id]: 45928
[parent_id]: 45922
[tags]: 
If you don't care which features are included, using PCA (or something similar) can help. If you do have some information on which features influence classification or regression, you can certainly try to fit a model without dimensional reduction. PCA, which is one of the more common dimensional reduction techniques, yields vectors that are all orthogonal (as in, uncorrelated). This means that even if your features are correlated, after the dimensional reduction, your model won't struggle with collinearity. Depending on your model type, this can be crucial. A real life example could be any housing dataset, where the features describe the house and the target is the price. Many of the features will be correlated (e.g. number of bathrooms and number of bedroom or number of rooms and square footage), and so a linear regression model may get tripped up by the collinearity. Dimensional reduction will capture the variance across the features while yielding fewer columns.
