[site]: crossvalidated
[post_id]: 623978
[parent_id]: 
[tags]: 
Effective use of benchmarks for time series forecasts

I am forecasting sales of products and want to ensure that I am using benchmark models properly. Suppose I have selected a baseline model - naive seasonal. Suppose my performance metric is WMAPE. Suppose I have trained an XGBoost model on 10 products and find that, when using cross-validation on my training set: The XGBoost model has a greater overall WMAPE, but The naive seasonal forecast outperforms 2 of the products (i.e. when I calculate WMAPE at the product level). I am now confused about how to use this information to guide what "final" model I choose for future prediction/on an unseen test set. My question is: Do I: Split the products into 2 groups, retraining an XGBoost model on only 8 of the products in the training set and using the naive seasonal model for the other two? Split the products into 2 groups, repeat the cross-validation process recursively, before doing what I said in step 1? (For example: do cross-validation for XGBoost trained on the 8 products, find that naive seasonal now outperforms it on another 3rd product, then retrain XGBoost on 7 products, repeating until I have found the set of products for which XGBoost always beats naive seasonal, then doing step 1) Retrain an XGBoost model on all of the products in the training set, but only using this model's predictions for 8 of the products as my "final" prediction, and the naive seasonal model for the other two?
