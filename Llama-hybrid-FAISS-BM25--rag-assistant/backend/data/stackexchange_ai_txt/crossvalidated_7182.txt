[site]: crossvalidated
[post_id]: 7182
[parent_id]: 7175
[tags]: 
First let me tell you that I am not going to explain exactly all the measures here, but I am going to give you an idea about how to compare how good the clustering methods are (let's assume we are comparing 2 clustering methods with the same number of clusters). For example the bigger the diameter of the cluster, the worst the clustering, because the points that belong to the cluster are more scattered. The higher the average distance of each clustering, the worst the clustering method. (Let's assume that the average distance is the average of the distances from each point in the cluster to the center of the cluster.) These are the two metrics that are the most used. Check these links to understand what they stand for: inter-cluster distance (the higher the better, is the summatory of the distance between the different cluster centroids) intra-cluster distance (the lower the better, is the summatory of the distance between the cluster members to the center of the cluster) To better understanding the metrics above, check this . Then you should read the manual of the library and functions you are using to understand which measures represent each of these, or if these are not included try to find the meaning of the included. However, I would not bother and stick with the ones I stated here. Let's go on with the questions you made: Regarding scaling data: Yes you should always scale the data for clustering, otherwise the different scales of the different dimensions (variables) will have different influences in how the data are clustered, with the higher the values in the variable, the more influential that variable will be in how the clustering is done, while indeed they should all have the same influence (unless for some particular strange reason you do not want it that way). The distance functions compute all the distances from one point (instance) to another. The most common distance measure is Euclidean, so for example, let's suppose you want to measure the distance from instance 1 to instance 2 (let's assume you only have 2 instances for the sake of simplicity). Also let's assume that each instance has 3 values (x1, x2, x3) , so I1=0.3, 0.2, 0.5 and I2=0.3, 0.3, 0.4 so the Euclidean distance from I1 and I2 would be: sqrt((0.3-0.2)^2+(0.2-0.3)^2+(0.5-0.4)^2)=0.17 , hence the distance matrix will result in: i1 i2 i1 0 0.17 i2 0.17 0 Notice that the distance matrix is always symmetrical. The Euclidean distance formula is not the only one that exists. There are many other distances that can be used to calculate this matrix. Check for example in Wikipedia Manhattain Distance and how to calculate it. At the end of the Wikipedia page for Euclidean Distance (where you can also check its formula) you can check which other distances exist.
