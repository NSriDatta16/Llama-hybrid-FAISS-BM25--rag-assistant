[site]: crossvalidated
[post_id]: 342951
[parent_id]: 
[tags]: 
Neural Network Reduction - pruning

Context I am currently working on an assignment, in the reference paper given by the assignment it has mentioned a technique using to reduce network topology (the size of hidden layer). Technique Suppose we have a feed-forward network, and there's one hidden layers with weight W1 . Considering W1 might initialized bigger than we need, we want to prune it. The pruning is based on 2 principles: shut down one of two units that are too similar (two vectors in W1 with angle less than degree of 15) shut down complementary units both of them (two vectors in W1 with angle larger than degree of 165) I understand the first principle, in backpropgation, hidden units with the same weights are likely to update by the same amount, so no need to keep 2 same hidden units (also you can check this video from Andrew Ng deep learning course). Question But for the second principal, the paper itself didn't give any explanation and I cannot find any evidence to convince myself. Why do such removal?
