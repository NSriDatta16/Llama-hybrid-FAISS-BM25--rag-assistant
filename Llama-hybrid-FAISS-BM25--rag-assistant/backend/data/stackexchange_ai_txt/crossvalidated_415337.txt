[site]: crossvalidated
[post_id]: 415337
[parent_id]: 
[tags]: 
Is the use of Nested Cross Validation and train- test CV necessary or an overkill?

I have been relatively obsessed lately in the proper way of selecting a model (including tuning hyper parameters) and then assessing model performance. I have read various posts and the approach I have found to be optimal (by combining information from different sources) is the following: Randomly split the full dataset into train (80%) and test (20%) Do not consider the test step until you reach step 5 Use (nested) 10-fold cross validation within the train set for model selection and hyper parameter tuning (I think the caret package in r does that very well as it allows you to do pre processing such as scaling or imputation in each fold). In other words in each fold I split the train set into train and validation set Select the best model (best in terms of number of nodes, cost-complexity trade off etc) from the nested CV and run it on the complete train data set Apply that best model in the test set and get the model performance metric you are interested (that can act as a fully unbiased model assessment as the test set was not used at all since that step). For example if I was building a decision tree I would get the accuracy for that best model on the complete train set and on the test set and examine the difference. Now my question is, if these processing steps are indeed sufficient or not. Am I missing an essential step or should I further split any steps? One addition in particular that I was thinking is to do cross validation on the initial full dataset by using that best model and then examine if the average CV difference between train and test set accuracy stayed (bordeline) the same with the difference from step 5. I guess one of the reasons for considering this is to ensure that the selection of test set in step 1 was well curated. Thanks in advance!
