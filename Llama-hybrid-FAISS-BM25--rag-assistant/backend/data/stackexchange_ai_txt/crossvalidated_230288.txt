[site]: crossvalidated
[post_id]: 230288
[parent_id]: 230252
[tags]: 
First, note that a Bayesian model is a joint probability distribution of all unknowns modeled as random variables. Or, if you use Bayes rule in other contexts, it still requires all random variables / events to be defined in the same joint probability space. The $P$s or $p$s then refer to marginal and conditional probabilities/distributions obtained from this joint space. The issue may be just notational. In an attempt to clarify the notational issues, I first go over your discrete example, somewhat rigorously, considering everything as events. Second, I then point out notational problems in the more general case (where we are dealing with random variables and densities rather than events), which may have confused you since the notation annoyingly refers to different functions with the same letter $p$. The event case (your example) A possible probability space modeling your example situation would be the following: let the sample space be \begin{equation} \Omega = \{hh_1 , hh_2\} \times \{dd_{heads}, dd_{tails}\}, \end{equation} (I just doubled the letters to notationally separate the outcomes and the events). Let the event space be $2^\Omega$. Now, introduce notation for the events: \begin{align} h_i &= \{hh_i\} \times \{dd_{heads}, dd_{tails}\} \\ d_{foo} &= \{hh_1,hh_2\} \times \{dd_{foo}\} \end{align} There is just one probability $P$. The point is that after specifying the following $P$-probabilities: 1. the probability of event $h_1$, 2. the conditional probability of $d_{heads}$ conditional on $h_1$, and 3. the conditional probability of $d_{heads}$ conditional on $h_2$, $P$ is uniquely determined and you may thus deduce the conditional probability of $h_1$ conditional on $d_{heads}$, using the Bayes rule. All in this same probability space. Note also that your \begin{equation} P(h_1 \mid d_{heads}) \propto P(d_{heads} \mid h_1)\,P(h_1) \end{equation} does not make sense since both the LHS and the RHS are just single numbers, so the proportionality does not mean anything. You meant \begin{equation} P(h_i \mid d_{heads}) \propto P(d_{heads} \mid h_i)\,P(h_i), \end{equation} where the point is that the proportionality factor is the same with $i=1$ and $i=2$. A note on notation with densities There is a popular simplifying-but-confusing notational convention in Bayesian statistics is to just use $p(x)$ and $p(y)$ to refer to density functions (or probability mass functions in the discrete case) of $x$ and $y$ respectively. A more precise notation would i) separate the random variables and their values (say, $X$ vs. $x$) and ii) separate the different density functions by, e.g., subscripts so that $p_X(x)$ is the value of the density function of $X$ evaluated at $x$. In the popular notation, which function each $p$ refers to is left implicit. So, instead of writing \begin{equation} p(\theta\mid y) \propto p(\theta)\,p(y\mid \theta) \end{equation} it may be clearer to write \begin{equation} p_{\Theta \mid Y}(\theta \mid y) \propto p_\Theta(\theta)\,p_{Y \mid \Theta}(y \mid \theta). \end{equation}
