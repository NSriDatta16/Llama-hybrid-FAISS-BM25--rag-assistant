[site]: crossvalidated
[post_id]: 466491
[parent_id]: 466459
[tags]: 
Bayesian probability and Bayesian statistics are subjective. Bayesian probability depends on the prior knowledge of the researcher; Bayesian statistics depend on the subjective loss function of the decision-maker. So, your statement, $\hat{p}=\frac{k}{n}$ , is only true under specific loss functions and priors in the usual case. It does work for your formula, however. $\hat{p}$ is usually considered the posterior estimator. So let us consider the case of a strictly fair coin with observers, Naïve Bob and Slick Eddy. The fair coin will be tossed by Dr. Clueless Statistician and Mandrake the Magician. You are an engineer and have formally tested the fairness of the coin. Up comes Naïve Bob, and Naïve Bob has seen coins tossed before and knows that you checked to see if the coin is fair. He uses a prior of $\beta(100,100)$ when both Dr. Statistician and Mandrake toss the coin. Dr. Statistician tosses the coin first. There are 10 tosses, and five are heads. That leaves Naïve Bob with a posterior of $\beta(105,105)$ and a posterior mean of $$\hat{p}=\frac{100+5}{100+100+5+5}=\frac{1}{2}.$$ Slick Eddy was also in the audience. Slick Eddy meets Dr. Statistician before the toss and interviews him to see if he has any particular experience with tossing coins. For example, he could be a frequent player of role-playing games, and his dice could have come up with critical rolls too often. That could have caused him to practice “magic.” He concludes that Clueless has no specific coin-tossing skills and assigns a prior of $\beta(1000,1000)$ and sees the same ten coin tosses. After this is over, Mandrake the Magician comes to toss the coin. He asks both viewers to inspect the coin. He shows that there is nothing up either sleeve and asks Naïve Bob to choose Heads or Tails for a simple one-dollar gamble. If Mandrake can cause the coin to match Naïve Bob’s call ten times in a row, then Bob will pay \$1; otherwise, Mandrake will pay \$10. Given Bob’s prior, he eagerly accepts the bet with a prior of $\beta(105,105)$ . Slick Eddy, on the other hand, waits to hear what Bob calls out to form his prior. Naïve Bob calls out, “Heads.” At first, Fast Eddy considers setting his prior at $\beta(10,1)$ to reflect the odds, but then realized that Mandrake could have his own prior of $\beta(100,1)$ and is underpaying Bob for the risk. Fast Eddy decides to be conservative and uses a prior of $\beta(10,1)$ since he has no other personal knowledge of Mandrake’s coin-tossing skills. To get a point statistic from a Bayesian prior or a Bayesian posterior, you have to apply a loss function to the distribution. In the case where the posteriors created from conjugate prior distributions, you can decompose a point statistic into prior pseudo-observations and sample observations, which is what you see there. The posterior mean is $$\frac{\alpha+k}{\alpha+\beta+n}.$$ You could decompose this into $$\hat{p}|k,n=\pi\frac{a+b}{a+b+n}+\hat{p}\frac{n}{a+b+n},$$ slightly modifying your prior notation, where $a$ is pseudo-successes, $b$ is pseudo-failures, so $a+b=iss$ , and $k$ is observed successes and $n$ is the sample size. Note that using a posterior mean implies that your loss function is quadratic. Surprisingly/Unsurprisingly, the coin comes up Heads ten times in a row. Naïve Bob has a posterior of $\beta(115,105)$ and remains eager to retake the bet. Chance effects do happen. Fast Eddy, who did not bet, now has a prior of $\beta(20,1)$ with a posterior mean of $\frac{20}{21}$ . He secretly goes to Naïve Bob and offers 20-1 odds to make the same bet, but offers even more money. In each of these cases, the prior could have been acquired by observations with a $\beta(0,0)$ prior distribution. Because of this, the prior can be viewed, in some cases, as being equivalent to a certain number of pseudo-observations. The linkage is obvious in the cases where there is a conjugate prior distribution. It is not clear how many pseudo-observations exist when working outside the exponential family of distributions or when not using a conjugate prior.
