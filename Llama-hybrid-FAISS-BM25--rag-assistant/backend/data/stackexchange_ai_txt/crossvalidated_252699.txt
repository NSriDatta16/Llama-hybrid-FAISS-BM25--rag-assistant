[site]: crossvalidated
[post_id]: 252699
[parent_id]: 252693
[tags]: 
There are different ways how we can combine models (e.g. majority vote in classification) and averaging them is only one of the approaches. They are not that arbitrary as for particular algorithms, for example as described in Wikipedia article on ensemble learning , Bayesian parameter averaging (BPA) is an ensemble technique that seeks to approximate the Bayes Optimal Classifier by sampling hypotheses from the hypothesis space, and combining them using Bayes' law. [...] Bayesian model combination (BMC) is an algorithmic correction to Bayesian model averaging (BMA). Instead of sampling each model in the ensemble individually, it samples from the space of possible ensembles [...] Notice also that there is also a difference between averaging (or combining) the models and the forecasts (i.e. outcomes of the models). In first case you average the models themselves and in the second case you use each of the models to make individual forecast and then you average the forecasts. Nevertheless, I agree that sometimes all the terms are used exchangeably, what may be confusing.
