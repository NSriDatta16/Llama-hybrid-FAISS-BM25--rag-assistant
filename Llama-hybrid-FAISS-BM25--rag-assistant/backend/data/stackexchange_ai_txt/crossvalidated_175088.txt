[site]: crossvalidated
[post_id]: 175088
[parent_id]: 175079
[tags]: 
In GLM, your dependent variable is drawn from a distribution that depends on your independent variables. More specifically, the mean of the distribution from which the DV is drawn is related to a linear combination of your independent variables by the link function (actually the inverse link, $\mu_i = g^{-1}(\theta^Tx_i)$). In ordinary least squares regression, the DV is modeled as a normal distribution whose mean is related to a linear combination of the independent variables by the identity function. $$ y_i \sim \mathcal{N}(\theta^T x_i, \sigma^2). $$ In logistic regression, the DV is modeled as being drawn from a Bernoulli distribution whose mean is related to a linear combination of the independent variables by the inverse logit (expit) function. $$ y_i \sim \mathcal{Ber}(g^{-1}(\theta^T x_i), \sigma^2), $$ where $g$ is the logit function. In your case I think the issue is that you're trying to draw large numbers from a Bernoulli distribution with an unrealistic mean. If $$ y = x^2 + \epsilon, \epsilon \sim \mathcal{Ber}(0.5), $$ this does not mean that $y$ is being drawn from a Bernoulli distribution with mean $x^2$. It means that $y-x^2$ is being drawn from a Bernoulli distribution with mean 0.5. Edit: Responding to comment It is a bit more complex than that. GLM is generalized from linear models in two ways. The distribution that the data comes from can be an distribution in the exponential family, and the relationship between the mean of that distribution and the IVs can be non-linear. These two generalizations are not entirely disconnected in that certain types of distributions are intrinsically connected to certain link functions. This resource handles the subject very well, http://data.princeton.edu/wws509/notes/a2.pdf . You have a bunch of data $\{(x_i, y_i) : 1 \leq i \leq N \}$. Our assumption is that there is a family of distributions with various means, we'll call them $D_{\mu}$ from which the $y_i$ were drawn. We believe that the mean was different for the different $y_i$, so we say specifically, $y_i \sim D_{\mu_i}$. What's more we believe that $\mu_i$ has a deterministic relationship with $x_i$, so $\mu_i = f(x_i)$. What's even more , we believe that this function lives in a family that we have parameterized by $\theta$, so for some $\theta^{*}$, $\mu_i = f_{\theta^{*}}(x_i)$. OUR GOAL: Estimate $\hat{\theta} \approx \theta^{*}$, so that for future $x$ we can predict a corresponding $\hat{y} = f_{\hat{\theta}}(x)$, the mean of the distribution that the corresponding $y$ will be drawn from.
