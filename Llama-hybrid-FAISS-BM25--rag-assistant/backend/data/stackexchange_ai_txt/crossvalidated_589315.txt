[site]: crossvalidated
[post_id]: 589315
[parent_id]: 589298
[tags]: 
so you have to understand that neural networks are used as 'computation-bound' statistical models. ie they are solving the problem how accurate can I get with a budget of X compute-hours. reasoning with a mental model of modelling in under an hour will lead you astray. minibatch is a tradeoff of stochasticity and iteration speed vs accuracy. you will be able to achieve more iterations in the same time if you have a smaller minibatch - iteration speed (batch) gradient descent will get stuck in local minima. so some stochasticity is beneficial. stochastic gradient descent (ie updating after each sample) will be too noisy (ie the single sample gradient is too far from the batch gradient) somewhere in between is therefore beneficial. the law of large numbers would suggest that accuracy of the estimate only scales with square root of the number of samples - halving the error requires 4 times the samples, quartering the error requires 16 times the samples (and therefore corresponding computation time...)
