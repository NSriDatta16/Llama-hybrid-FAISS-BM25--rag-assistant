[site]: crossvalidated
[post_id]: 635078
[parent_id]: 
[tags]: 
PCA, dot product, column space

Let's assume a data matrix $X_{n\times p}$ . The idea of PCA is to find the projection $Xv$ that has the maximum variance. When thinking about column space $C(X)$ then $Xv$ is obviously a linear combination of columns of $X$ , so we seek for that particular vector $c^* \in C(X)$ that maximizes the projection variance. Now let's think about $X$ as of $n$ $p-$ dimensional points. $Xv$ can be seen as a dot product between each $p-$ dimensional point (vector; denoted as $x_i$ ) and the vector $v$ , so in the resulting vector (say, $c = Xv$ ) each entry $c_i$ is a length of projection of $x_i$ onto $v$ , multiplied by the length of $v$ (that is equal $1$ when $v$ is an eigenvector). If this is true, then obviously we get most information when we maximize variance of that vector (projection). Am I correct?
