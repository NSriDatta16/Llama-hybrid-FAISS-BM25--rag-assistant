[site]: datascience
[post_id]: 14007
[parent_id]: 
[tags]: 
When is Gradient Descent invoked on the objective function while running XGboost?

is it at the end of every tree? or only after all trees are build? I tried to think in both ways but didn't get a clear picture. Can we focus more the part "the loss function is applied between training models" I am trying to understand how is Gradient Descent is applied to minimize the objective function function. slide no 36 in http://www.saedsayad.com/docs/xgboost.pdf Data points reach to a leaf will be assigned a weight. The weight is the prediction how is the weight assigned ? how is the weight calculated ? why predict the weight again ? Can someone shed some light on this?
