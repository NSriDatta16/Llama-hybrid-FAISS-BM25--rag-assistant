[site]: crossvalidated
[post_id]: 246843
[parent_id]: 
[tags]: 
What is the intuition between neural networks and complex non-linear hypotheses?

As a relative new arrival to realm of machine learning, I was curious as to what the intuition behind the ability of neural networks to form more complex non-linear hypotheses than typical logistic regression algorithms was. I see that in order to produce a complex output for a logistic regression function with a large number of features, one must map many features to an extent that may make the algorithm very computationally expensive, which is why a neural network is a more preferable option. But why is this so? Shouldn't a neural network also be subject to the same amount of computational expenses as it is also computing a very complex non-linear hypothesis using connections between many nodes? What is it that makes a neural network so much less computationally expensive than a feature-mapped logistic regression algorithm? I am thinking that it is due to the more "grouped" process of back-propogation. In other words, the layer-wise optimization of neural networks in contrast to the parameter-wise optimization of logistic regression algorithms.
