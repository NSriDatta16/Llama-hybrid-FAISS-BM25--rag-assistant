[site]: crossvalidated
[post_id]: 628016
[parent_id]: 627008
[tags]: 
Both models are problematic and are not really used in my field (biostatistics with a focus on clinical trials) for the reasons mentioned below: Both models assume that the residual variance is the same across time, which is rarely the case (e.g. in a clinical trial where you recruit people according to some criteria that tend to constrain the distribution a little, have a baseline pre-treatment assessment and then look at values over time, residual SD or variance almost invariably goes up over time). Using previous observations as a covariate It ignores that things are usually measured with error. It (indirectly) induces a relatively rigid inflexible correlation structure that will usually not correctly capture the real correlation structures you would encounter in the wild (which is e.g. why multiple imputation with chained equations aka "MICE" often performs badly). I think (but am not totally sure) that it might cause problems with causal inference across the timepoints (i.e. if you wanted to estimate a causal effect of an intervention across two timepoints, but you condition on the second timepoint on the observed value at the first, I suspect this is problematic). Using AR(1) correlated residuals AR(1) is a terrible correlation structure for any real data I've ever worked with (usually correlation declines over time, but not as fast towards zero as AR(1) assumes). While it may not be so bad that it overestimates correlation for times that are close together, the biggest problem comes from how it treats observations that are far enough apart as effectively independent. This latter point will typically invalidate lots of things (e.g. type I error for hypothesis tests, coverage for confidence intervals etc.). What are alternatives? If you have fixed timepoints, something like mixed models for repeated measures (see here for a Bayesian version ) with an unstructured (or at least more flexible covariance structure - you might be willing to make more assumptions than that, which would result in a gain in precision on your inference). If you don't have fixed timepoints: There's Gaussian process models to capture the correlation, but these have a reputation to be hard to fit. A more mechanistic/generative model that describes underlying process through e.g. differential equations (while your measurements are only realizations from this possibly unobserved underlying process). This tends to induce reasonably complex correlation structures of a sufficiently capacity to fit a lot of data, but you may not know enough to specify/setup such a model. If we are not really talking about an inference task, but rather about prediction: Options could include various time series types of neural networks (e.g. LSTMs). Anything that reflects what you would really know when you'd make a prediction. So, models with (possibly multiple) lags can often do okay for pure prediction tasks, although you'd often want to be more flexible than a linear model (e.g. gradient boosted decision trees with multiple lags as inputs).
