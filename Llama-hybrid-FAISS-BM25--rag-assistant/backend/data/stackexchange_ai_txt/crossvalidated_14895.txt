[site]: crossvalidated
[post_id]: 14895
[parent_id]: 222
[tags]: 
I like to think of principal component scores as "basically meaningless" until you actually give them some meaning. Interpretting PC scores in terms of "reality" is a tricky business - and there can really be no unique way to do it. It depends on what you know about the particular variables that are going into the PCA, and how they relate to each other in terms of interpretations. As far as the mathematics goes, I like to interpret PC scores as the co-ordinates of each point, with respect to the principal component axes. So in the raw variables you have $\bf{}x_i$ $=(x_{1i},x_{2i},\dots,x_{pi})$ which is a "point" in p-dimensional space. In these co-ordinates, this means along the $x_{1}$ axis the point is a distance $x_{1i}$ away from the origin. Now a PCA is basically a different way to describe this "point" - with respect to its principal component axis, rather than the "raw variable" axis. So we have $\bf{}z_i$ $=(z_{1i},z_{2i},\dots,z_{pi})=\bf{}A(x_i-\overline{x})$, where $\bf{}A$ is the $p\times p$ matrix of principal component weights (i.e. eigenvectors in each row), and $\bf{}\overline{x}$ is the "centroid" of the data (or mean vector of the data points). So you can think of the eigenvectors as describing where the "straight lines" which describe the PCs are. Then the principal component scores describe where each data point lies on each straight line, relative to the "centriod" of the data. You can also think of the PC scores in combination with the weights/eigenvectors as a series of rank 1 predictions for each of the original data points, which have the form: $$\hat{x}_{ji}^{(k)}=\overline{x}_j+z_{ki}A_{kj}$$ Where $\hat{x}_{ji}^{(k)}$ is the prediction for the $i$th observation, for the $j$th variable using the $k$th PC.
