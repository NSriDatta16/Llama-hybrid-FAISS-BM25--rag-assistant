[site]: crossvalidated
[post_id]: 342963
[parent_id]: 211451
[tags]: 
For the discrete case, my idea was to simply infer the discrete distribution directly from how often each choice has been voted so far, then using this distribution to get the probability of the new vote. How about use: $$\frac{\text{times choice has been made}+1}{\text{total choices that have been made} + \text{total number of categories}}$$ This seems to have the properties you're looking for (doesn't approach zero until you have significantly more observations than categories). I think this actually corresponds to Bayesian predictive probabilities with a multinomial model, and a Dirichlet prior. The analogous thing for the continuous case would be Bayesian inference with a normal model, and conjugate priors (normal for the mean, inverse-gamma for the variance). This does indeed give you a Student's $t$ distribution for your predictive distribution. I recommend Peter Hoff "A First Course in Bayesian Statistical Methods" for more info.
