[site]: crossvalidated
[post_id]: 627666
[parent_id]: 
[tags]: 
Why are my training and validation curves suspiciously close to one another (sklearn neural network)

I am trying to graph the accuracy, error and precision scores over epoch for a neural network and am using cross validation. However, my training and validation scores are practically on top of one another (see image below for an example). There is a little variation where the blue is peaking out from behind the green, but that seems more like random noise than anything significant. I've tried running my neural network on multiple datasets and still get the same issue. This feels a little suspicious, but I am fairly sure that I've indexed my model such that it should only be fitting the training data and not the validation. Does anyone please have some insight into why this is happening? The code is below. Thanks so much in advance! P.S. Please note the dataset_dictionary method is defined elsewhere. But it's only purpose is to return the file path on my computer where I am storing the 'heart disease' dataset and also return the key of the target column. from sklearn.neural_network import MLPClassifier from sklearn.model_selection import StratifiedKFold, learning_curve from sklearn.utils.validation import column_or_1d from sklearn.metrics import accuracy_score, mean_squared_error, precision_score from sklearn.preprocessing import OneHotEncoder import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split import pandas as pd import numpy as np from copy import deepcopy class neural_network(): def get_model(hidden_layer_sizes, epochs): return MLPClassifier(alpha=0.01, #batch_size=batch_size, epsilon=1e-08, solver=['adam', 'sgd'][0], hidden_layer_sizes=hidden_layer_sizes, learning_rate='adaptive', shuffle = True, max_iter=epochs, warm_start=True, verbose=False) def plot_iterative_curve(hidden_layer_sizes, epochs, num_splits): #Complile the model with 1 epoch for the partial fit function model = neural_network.get_model(hidden_layer_sizes, 1) #Setup variables for the partial fitting class_num = np.unique(y_train) #Variables to record the training and validation data points for plotting acc_train_scores = [] acc_val_scores = [] mse_train_scores = [] mse_val_scores = [] pre_train_scores = [] pre_val_scores = [] #One epoch occurs per cross validation score calculation for e in range(epochs): skf = StratifiedKFold(n_splits=num_splits, shuffle=True) acc_train = 0 acc_val = 0 mse_train = 0 mse_val = 0 pre_train = 0 pre_val = 0 for train_index, val_index in skf.split(X_train, y_train_skffriendly): #Define the training and validation data X_t, y_t = X_train.iloc[train_index], y_train.iloc[train_index] X_v, y_v = X_train.iloc[val_index], y_train.iloc[val_index] #temp will be used to partially fit the data within the cross validation without having to change model temp = 0 temp = deepcopy(model) temp.partial_fit(X_t, y_t, classes = class_num) #Add the training and validation scores to the train and val arrays acc_train += accuracy_score(y_true=y_t, y_pred=temp.predict(X_t)) acc_val += accuracy_score(y_true=y_v, y_pred=temp.predict(X_v)) mse_train += mean_squared_error(y_true=y_t, y_pred=temp.predict(X_t)) mse_val += mean_squared_error(y_true=y_v, y_pred=temp.predict(X_v)) pre_train += precision_score(y_true=y_t, y_pred=temp.predict(X_t), average='weighted') pre_val += precision_score(y_true=y_v, y_pred=temp.predict(X_v), average='weighted') #Dividing by the number of splits is equivalent to taking the average of the train and val scores acc_train_scores.append(acc_train / num_splits) acc_val_scores.append(acc_val / num_splits) mse_train_scores.append(mse_train / num_splits) mse_val_scores.append(mse_val / num_splits) pre_train_scores.append(pre_train / num_splits) pre_val_scores.append(pre_val / num_splits) #Partially fit the main model after each epoch model = deepcopy(temp) model.partial_fit(X_train, y_train, classes=class_num) x_axis = range(1, epochs + 1) fig, ax = plt.subplots(1, 3, figsize=(14, 6)) fig.suptitle('Training and Validation vs Epoch', fontsize=20) # Plot the model accuracy vs epochs ax[0].plot(x_axis, acc_train_scores, 'b', label='Training') ax[0].plot(x_axis, acc_val_scores, 'g', label='Validation') ax[0].set_title('Accuracy', fontsize=16) ax[0].set_xlabel('Epochs', fontsize=16) ax[0].set_ylabel('Score', fontsize=16) ax[0].set_ylim(0,1) ax[0].legend() # Plot the model error vs epochs ax[1].plot(x_axis, mse_train_scores, 'b', label='Training') ax[1].plot(x_axis, mse_val_scores, 'g', label='Validation') ax[1].set_title('Mean Squared Error', fontsize=16) ax[1].set_xlabel('Epochs', fontsize=16) ax[1].set_ylim(0,1) ax[1].legend() # Plot the model precision vs epochs ax[2].plot(x_axis, pre_train_scores, 'b', label='Training') ax[2].plot(x_axis, pre_val_scores, 'g', label='Validation') ax[2].set_title('Precision', fontsize=16) ax[2].set_xlabel('Epochs', fontsize=16) ax[2].set_ylim(0,1) ax[2].legend() plt.show() path, y_val = datasect_dictionary('heart disease') X = pd.read_csv(path, header=0).dropna() y = X[y_val].to_frame() X.drop([y_val], axis=1, inplace=True) X_train, X_test, y_train_skffriendly, y_test_skffriendly = train_test_split(X, y, test_size=0.2, random_state=42) y_train = onehot(y_train_skffriendly) y_test = onehot(y_test_skffriendly) num_hidden_layers = 5 neurons_per_hidden_layer = 10 epochs = 100 hidden_layer_sizes = (num_hidden_layers, neurons_per_hidden_layer) neural_network.plot_iterative_curve(hidden_layer_sizes, epochs, 5)
