[site]: datascience
[post_id]: 113607
[parent_id]: 64278
[tags]: 
To add up to previous answers, in CNNs and in Neural Networks in general "channels" could be seen as "features". With CNNs, you typically start with an object of size $W \times H \times 3$ (if RGB image) or $W \times H \times 1$ (if grayscale image). This last dimension could be thought as features dimension. Now, typical CNN (e.g. ResNet) will reduce first two (spatial) dimensions and increase the third dimension (channels) by applying a sequence of alternating convolutions and pooling operations. Thus, effectively you trade spatial dimensions (which is hard to use for classification/regression) in order to vectorize your image (creating channels=features). Intermediate representations for which spatial dimension is still larger than 1 are typically called feature maps , and final feature map for which spatial dimensions are $1\times 1$ is sometimes called embedding of the image (though the word embedding could be used for any featuremap reshaped into a vector). Finally, this vector representation is used to make image-wise predictions (regression or classification) by applying a one (or more) dense layer.
