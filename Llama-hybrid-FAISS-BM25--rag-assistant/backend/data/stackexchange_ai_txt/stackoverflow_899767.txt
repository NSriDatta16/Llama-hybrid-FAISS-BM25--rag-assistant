[site]: stackoverflow
[post_id]: 899767
[parent_id]: 
[tags]: 
How to prevent hackers from scraping our database?

Possible Duplicate: How do you stop scripters from slamming your website hundreds of times a second? I am building a web application in RubyOnRails, which is based on a large body of data. The application makes for powerful navigation and intersection of the data, as well as a community model for adding more data. In that respect one could compare it with StackOverflow.com: a big bunch of data, structured in a fairly simple way. I intend to offer the content under a CreativeCommons license, but if the site "hits it off", I need to discourage copycats. My biggest fear is screen scraping scripters, not only leeching away the raw data, but also incurring huge usage peaks on my servers. I wonder if RubyOnRails offers any way to throttle (obviously automated) requests, e.g. to reduce their response-time at the benefit of regular users. Perhaps this requires Apache or Phusion Passenger settings? EDIT: My target is not to recognize user types, but to reduce responsiveness to overly active users, e.g. maximize the number of requests handled per IP address per unit of time (?)
