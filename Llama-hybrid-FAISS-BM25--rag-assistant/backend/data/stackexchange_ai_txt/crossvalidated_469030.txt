[site]: crossvalidated
[post_id]: 469030
[parent_id]: 
[tags]: 
Why do convolutional neural networks benefit from input translation?

Following this tutorial , it shows that a CNN accuracy on MNIST is improved even when the training set is expanded with simple operations such as one pixel translation. The same tutorial then asks The idea of convolutional layers is to behave in an invariant way across images. It may seem surprising, then, that a network can learn more when all we've done is translate the input data. Can you explain why this is actually quite reasonable? I was thinking because the pool layer (max or L2) somehow is a sort approximation and loose the precise spatial information. This answer instead says it is due to the fully connected layers: But the fully-connected layer (if there is one, or the output layer if not) still sees as its input a spatial map, which is probably, depending on the network architecture, mostly invariant to small shifts but less so to larger ones. Does anyone have a better suggestion why this happens?
