[site]: crossvalidated
[post_id]: 39294
[parent_id]: 39283
[tags]: 
Well, if your clusters are really good, your classifiers will be crap. Because they have not enough diversion in their training data. Say your clusters are perfect i.e. pure. You can't even properly train a classifier there anymore. Classifiers need positive and negative examples! Random Forest are very successful in doing the exact opposite. They take a random sample of the data, train a classifier on that, and then use all of the trained classifiers. What might work is to use clustering, and then train a classifier on every pair of clusters, at least if they disagree enough (if a class is split into two clusters, you still cannot train a classifier there!)
