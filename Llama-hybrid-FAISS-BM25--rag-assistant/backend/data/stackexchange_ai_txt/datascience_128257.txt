[site]: datascience
[post_id]: 128257
[parent_id]: 
[tags]: 
What's the purpose of using MLM when pretraining?

If BERT is a stack of transformer encoders, and the encoder already operates bidirectionally, understanding both left and right contexts and generating contextual embeddings, what is the purpose of pretraining BERT using MLM ? Does it aim to improve the contextual embeddings even better ? Could someone please provide an explanation on this ? Thanks.
