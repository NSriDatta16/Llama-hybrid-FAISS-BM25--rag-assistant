[site]: stackoverflow
[post_id]: 2823253
[parent_id]: 2823043
[tags]: 
Is it really true? and if so does anyone know why? In ancient days, when computers were still chipped out of fused silica by hand, when 8-bit microcontrollers roamed the Earth, and when your teacher was young (or your teacher's teacher was young), there was a common machine instruction called decrement and skip if zero (DSZ). Hotshot assembly programmers used this instruction to implement loops. Later machines got fancier instructions, but there were still quite a few processors on which it was cheaper to compare something with zero than to compare with anything else. (It's true even on some modern RISC machines, like PPC or SPARC, which reserve a whole register to be always zero.) So, if you rig your loops to compare with zero instead of N , what might happen? You might save a register You might get a compare instruction with a smaller binary encoding If a previous instruction happens to set a flag (likely only on x86 family machines), you might not even need an explicit compare instruction Are these differences likely to result in any measurable improvement on real programs on a modern out-of-order processor? Highly unlikely. In fact, I'd be impressed if you could show a measurable improvement even on a microbenchmark. Summary: I smack your teacher upside the head! You shouldn't be learning obsolete pseudo-facts about how to organize loops. You should be learning that the most important thing about loops is to be sure that they terminate , produce correct answers , and are easy to read . I wish your teacher would focus on the important stuff and not mythology.
