[site]: crossvalidated
[post_id]: 535568
[parent_id]: 
[tags]: 
What is the definition of margin in a space that is not linearly separable?

When texts introduce SVM they tend to first assume that data is linearly separable, i.e. there is a $w$ and $w_0$ so that $y(x) = w^\top \phi(x) + w_0$ satisfies $y(x_n)\cdot t_n > 0$ for all $n = 1,\cdots,N$ (for binary labels $t_n \in \{-1,+1\}$ ). Then they would define the margin to be the minimum distance between any data point $\phi(x_n)$ and the decision boundary, given that the decision boundary separates the labels correctly. My question is in the case of soft SVM: $$ \min_{w,w_0} \frac 12 \|w\|^2 + C \|\xi\|_1 \text{ subject to } t_ny(x_n) \geq 1 - \xi_n \text{ for } n = 1,\cdots, N $$ what is the margin defined as? Since in this case both the data and the hyperparameter $C$ you choose may potentially cause a varying amount of datapoints to be misclassified. I've also read that tuning $C$ somehow is tuning the tradeoff between classifying points correctly and maximizing margin. I understand how larger $C$ would encourage more accurate classification, but I don't see how lowering $C$ corresponds to a scheme that tends to maximize the margin. Could someone explain?
