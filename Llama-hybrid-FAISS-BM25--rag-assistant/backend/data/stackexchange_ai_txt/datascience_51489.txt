[site]: datascience
[post_id]: 51489
[parent_id]: 
[tags]: 
Transposed convolution as umpsampling in DCGAN

I read several papers and articles where it is suggested that transposed convolution with 2 strides is better than upsampling then convolution. However implementing such model with the transposed convolution resulted in heavy checkboard effect, where the whole generated image is just a pattern of squares and no learning takes place. How to properly implement it without totally messing up the generation? With the upsampling+convolution I got okay result but I want to improve my model. I am trying to generate images based on the CelebA dataset. I use keras with tf and I used the following code: model.add(Conv2DTranspose(256, 5, 2, padding='same')) model.add(LeakyReLU(alpha=0.2)) model.add(BatchNormalization(momentum=0.9)) model.add(Conv2DTranspose(128, 5, 2, padding='same')) model.add(LeakyReLU(alpha=0.2)) model.add(BatchNormalization(momentum=0.9)) model.add(Conv2DTranspose(64, 5, 2, padding='same')) model.add(LeakyReLU(alpha=0.2)) model.add(BatchNormalization(momentum=0.9)) Here I try to turn a 4x4 image into a 32x32. Later it will be turned into a 64x64 image with 1 or 3 channels depending on the image. However I get the following pattern always. Some tweaking usually leads to some other pattern but it does not really change: Checkboard effect Thank you for your answers in advance
