[site]: crossvalidated
[post_id]: 424104
[parent_id]: 
[tags]: 
How to estimate the "true" generalization accuracy in Kohavi's "A study of cross-validation and bootstrap for accuracy estimation and model selection"

I read Kohavi's classic paper Kohavi, R. (1995, August). A study of cross-validation and bootstrap for accuracy estimation and model selection. In Ijcai (Vol. 14, No. 2, pp. 1137-1145 ( https://www.ijcai.org/Proceedings/95-2/Papers/016.pdf ) which is quite influential, which is why I hope you don't mind that I ask a fairly specific question here. Overall, I find that the paper has a lot of great insights. I.e., it is intuitive to me that the performance from k-fold cross-validation is pessimistically biased, because the training folds are smaller than the training set such that the model is fit to fewer data points and hence may not generalize quite as well. While these insights are intuitive, they depend on how the "true accuracy" in this study was defined. In the paper, it says: So, the true accuracy is basically estimated using the holdout method 500 times and then averaging. However, why not just using the repeated holdout method always in the first place then? To address this issue, I am wondering if the author did the following? a) Take a dataset, say 2000 examples b) 500 times: split it into 1500 training examples and 500 test examples; compute the "true" accuracy as the average over these 500 repetitions c) Use 1500 training examples for evaluating the k-fold cross-validation method and see how the k-fold cross-validation performance (avg over test fold accuracies) compares to the accuracy from b) and deduce the bias as the difference between the two. I assume that the dataset for c) was randomly chosen from one of the 500 partitionings in b?
