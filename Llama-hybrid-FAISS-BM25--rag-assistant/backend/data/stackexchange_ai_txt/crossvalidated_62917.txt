[site]: crossvalidated
[post_id]: 62917
[parent_id]: 62916
[tags]: 
You can use the Delta method to calculate the standard error of $\hat{p}_{1}\hat{p}_{2}$ . The delta method states that an approximation of the variance of a function $g(t)$ is given by: $$ \mathrm{Var}(g(t))\approx \sum_{i=1}^{k}g'_{i}(\theta)^{2}\,\mathrm{Var}(t_{i})+2\sum_{i>j}g'_{i}(\theta)g'_{j}(\theta)\,\mathrm{Cov}(t_{i},t_{j}). $$ The approximation of the expectation of $g(t)$ on the other hand is given by: $$ \mathrm{\mathbf{E}}(g(t))\approx g(\theta) $$ So the expectation is simply the function. Your function $g(t)$ is: $g(p_{1}, p_{2})=p_{1}p_{2}$ . The expectation of $g(p_{1}, p_{2})=p_{1}p_{2}$ would simply be: $p_{1}p_{2}$ . For the variance, we need the partial derivatives of $g(p_{1}, p_{2})$ : $$ \begin{align} \frac{\partial}{\partial p_{1}}g(p_{1}p_{2}) & = p_{2} \\ \frac{\partial}{\partial p_{2}}g(p_{1}p_{2}) &= p_{1} \\ \end{align} $$ Using the function for the variance above, we get: $$ \mathrm{Var}(\hat{p}_{1}\hat{p}_{2})=\hat{p}_{2}^{2}\,\mathrm{Var}(\hat{p}_{1}) + \hat{p}_{1}^{2}\,\mathrm{Var}(\hat{p}_{2})+2\cdot \hat{p}_{1}\hat{p}_{2}\,\mathrm{Cov}(\hat{p}_{1},\hat{p}_{2}). $$ The standard error would then simply be the square root of the above expression. Once you've got the standard error, it is straight-forward to calculate a $95\%$ confidence interval for $\hat{p}_{1}\hat{p}_{2}$ : $\hat{p}_{1}\hat{p}_{2}\pm 1.96\cdot \widehat{\mathrm{SE}}(\hat{p}_{1}\hat{p}_{2})$ To calculate the standard error of $\hat{p}_{1}\hat{p}_{2}$ , you need the variance of $\hat{p}_{1}$ and $\hat{p}_{2}$ which you usually can get by the variance-covariance matrix $\Sigma$ which would be a $2\times 2$ -matrix in your case because you have two estimates. The diagonal elements in the variance-covariance matrix are the variances of $\hat{p}_{1}$ and $\hat{p}_{2}$ while the off-diagonal elements are the covariance of $\hat{p}_{1}$ and $\hat{p}_{2}$ (the matrix is symmetric). As @gung mentions in the comments, the variance-covariance matrix can be extracted by most statistical software packages. Sometimes, estimation algorithms provide the Hessian matrix (I won't go into details about that here), and the variance-covariance matrix can be estimated by the inverse of the negative Hessian (but only if you maximized the log-likelihood!; see this post ). Again, consult the documentation of your statistical software and/or the web on how to extract the Hessian and on how to calculate the inverse of a matrix. Alternatively, you can get the variances of $\hat{p}_{1}$ and $\hat{p}_{2}$ from the confidence intervals in the following way (this is valid for a $95\%$ -CI): $\mathrm{SE}(\hat{p}_{1})=(\text{upper limit} - \text{lower limit})/3.92$ . For an $100(1-\alpha)\%$ -CI, the estimated standard error is: $\mathrm{SE}(\hat{p}_{1})=(\text{upper limit} - \text{lower limit})/(2\cdot z_{1-\alpha/2})$ , where $z_{1-\alpha/2}$ is the $(1-\alpha/2)$ quantile of the standard normal distribution (for $\alpha=0.05$ , $z_{0.975}\approx 1.96$ ). Then, $\mathrm{Var}(\hat{p}_{1}) = \mathrm{SE}(\hat{p}_{1})^{2}$ . The same is true for the variance of $\hat{p}_{2}$ . We need the covariance of $\hat{p}_{1}$ and $\hat{p}_{2},$ too (see paragraph above). If $\hat{p}_{1}$ and $\hat{p}_{2}$ are independent, the covariance is zero and we can drop the term. This paper might provide additional information.
