[site]: datascience
[post_id]: 84546
[parent_id]: 
[tags]: 
Why do we usually have fully connected layers of same sizes in CNNs?

Is there any specific reason that we observe in CNNs, the fully connected layers usually have the same sizes? You can verify this for many CNNs. I'm aware that if, for instance, we have a vector of two components and we transform it into a three-dimensional vector space, the basis of that vector will contain still two components at most. Consequently, if we have, for instance, a layer with 100 neurons and the subsequent layer has 120 neurons, our outputs can still be described with at most 100 neurons, although they are transformed from 100-space to 120-space. The point is that this is linear algebra. In fully connected layers, we have nonlinearity other than linear calculations.
