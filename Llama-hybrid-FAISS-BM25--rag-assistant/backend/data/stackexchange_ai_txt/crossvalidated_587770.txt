[site]: crossvalidated
[post_id]: 587770
[parent_id]: 
[tags]: 
Theory behind this natural sequential estimator of the mean

Suppose we have i.i.d. real random variables $X_1, \cdots$ . We want to estimate the expectation $E[X_1]$ to within some desired error $\epsilon$ , but we do not know the variance. We want to use as few samples as possible. Consider the following estimator. In the $t$ -th step, we look at $X_1, \cdots, X_t$ , compute an estimate of the variance, and if the variance estimate corresponds to error $\ll \epsilon$ (say, by Chebyshev's inequality), then we return the average of $X_1, \cdots, X_t$ , and otherwise we continue with the $t+1$ -th step. It is not hard to come up with a counterexample, showing that this estimator can fail in giving $\pm \epsilon$ approximation of $E[X]$ . Still, is this something that has been studied? Can one prove some useful statistical properties of this estimator or something similar? I am thinking of properties such that under some assumptions, this estimator with indeed return an $\pm \epsilon$ approximation of $E[X]$ .
