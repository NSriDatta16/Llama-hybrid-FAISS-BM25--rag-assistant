[site]: crossvalidated
[post_id]: 622960
[parent_id]: 622957
[tags]: 
I assume you meant binary probability estimator instead of binary classifier as you didn't classify the observations. It's good that the probability estimator is well calibrated. The estimated probabilities will still have uncertainties though, since the sample size used to estimate them is not $\infty$ . For a full Bayesian solution you'd need to know these uncertainties. That would give rise to a posterior distribution for each probability. Were a full Bayesian method to be used in estimating the probabilities from the beginning (e.g., Bayesian binary logistic regression), with MCMC used to draw samples from the probabilities' posterior distributions, you could get your final answer quite easily using the following steps: Draw one posterior sample for each of $n$ probabilities Sum these to get a posterior sample of the number of positives Divide by $n$ to get a posterior sample of the fraction of positives Repeat for $m$ posterior draws where $m$ is several thousand Compute the empirical CDF or kernel density estimator from the $m$ samples of the fraction of positives to get the posterior distribution of interest If the risk model development sample is different from the sample or population to which you want to apply the probability estimates, in a logistic regression setting you would do the following. The posterior draws would be on the $\hat{\beta}$ from the logistic model used to estimate the probabilities The probability estimates would plug in different $X$ predictor values than those occurring in the training sample
