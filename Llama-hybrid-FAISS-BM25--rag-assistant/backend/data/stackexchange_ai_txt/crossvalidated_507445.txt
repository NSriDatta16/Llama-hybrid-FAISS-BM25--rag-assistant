[site]: crossvalidated
[post_id]: 507445
[parent_id]: 
[tags]: 
How to optimize a ML model as a function of the relationship between the predictions in the training set?

I am working on a proof-of-concept model for estimating the state-of-health of batteries based on logged voltage and current time series. It is an unsupervised problem in that I do not have access to a ground truth metric of the state-of-health, but I know, for example, that the decrease in state-of-health over time is related to an increase in the internal resistance of the batteries, and we hypothesize that the internal resistance can be predicted by an ML-model from certain step-change events detected in the logged voltage/current data. I would like to explore the potential for use of a gradient boosted tree (or other) ML-model for estimation of the internal battery resistance from features extracted from these events, and my idea is to, as a first step, evaluate the quality of the model as a function of how well the predictions of the resistance, from the separate events, increase with time. Hence, I would like to evaluate the quality of a model as the function of all the predictions in relation to one another, and not in relation to a ground truth. One idea I have is to make an isotonic regression model of the model predictions as a function of time (illustrated below) and calculate the RMSE from this model, and use it as a quality metric. My understanding is that if the predictions are monotonically increasing the error will be zero. But now I do not know at all if, and if so, how I could set up this type of model training, and any input on whether it is feasible would be appreciated. By the way, I generally mostly work in python with scikit-learn. Many thanks, Johan
