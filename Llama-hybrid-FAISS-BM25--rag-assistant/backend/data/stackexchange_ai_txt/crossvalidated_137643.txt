[site]: crossvalidated
[post_id]: 137643
[parent_id]: 137636
[tags]: 
Nonparametric methods don't specify something (maybe the distribution, maybe a relationship between two variables*) with a fixed finite number of parameters; they're (potentially) infinite parametric. Consider a loess curve (a form of nonparametric regression), for example; the parameters not only aren't explicit, if you attempt to count them, the number isn't even an integer. On the other hand, you never need more than $n$ parameters to define $n$ observations; presumably in at least that sense* the number of parameters grows as you increase $n$ . Consider, for example, a kernel density estimate using the usual AIMSE-optimal bandwidth selection; that has an effective number of parameters (e.g. as measured by Ye's generalized degrees of freedom) that grows as $n$ increases (but not in proportion to $n$ ). However, since the statement is referenced ( Murphy, Kevin (2012). Machine Learning: A Probabilistic Perspective. MIT ), you should probably consult that work for the complete context. * the second can also be regarded as not specifying something about the distribution, such as the functional form of a conditional mean **(though in other senses as well)
