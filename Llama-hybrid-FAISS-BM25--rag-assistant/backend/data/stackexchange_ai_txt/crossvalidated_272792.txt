[site]: crossvalidated
[post_id]: 272792
[parent_id]: 
[tags]: 
Diebold-Mariano in the context of volatility forecasting: What is the ultimate aim of this test?

Perhaps I'm missing a simple conceptual point here. But do the error statistics (RMSE, MAE) not tell which is the best forecast by presenting the lowest figures between the forecast and the actual figures? Why then do we need to compare two forecasts directly, given we've already established which is one is superior via RMSE etc? Isn't our interest in establishing which one replicates values closest to the actual figures? Secondly how would one go about interpreting the following Diebold Mariano test I've conducted. Would the below be correct? <>prob H0: Both forecasts have the same accuracy H1: The forecasts do not have the same accuracy $ $>0.05$ accept null, both forecasts have the same accuracy >Prob H0: Both forecasts have the same accuracy H1: Forecast 1 is less accurate than forecast 2 $ $>0.05$ accept null, Both Forecasts have the same accuracy H0: Both forecasts have the same accuracy H1: Forecast 2 is more accurate than Forecast 1 $ $>0.05$ accept null, Both Forecasts have the same accuracy This can then ultimately be written as something close to the following: The table shows the Diebold-Mariano test statistics for different sub-periods. It is employed to test whether the RMSE performance between the Short Moving Average of 5 days and other volatility forecasting models are statistically different from one another. A negative value indicates that the Short Moving Average of 5 days is better than the second model. i.e. Negative values of the Dieboldâ€“Mariano test show that the squared errors of the model listed first (short moving average) are lower than those of the model listed last.
