[site]: datascience
[post_id]: 108693
[parent_id]: 108685
[tags]: 
I think you are right when you say that word embeddings etc will likely deliver "bad" results since names and addresses are highly individual. With your type of problem you will probably need to compare $1:n$ , which is expensive. Since each entry needs to see each other entry only one time, you can likely reduce the effort by using itertools.combinations(lst, 2) to generate unique pairs. However, with 9 mio. entries this will still give some 40,500,000,000,000 pairs to check (including "self" as a reference). In case you don't want to use string distances , one possible solution could be to compare a single vectorized entry to all others. This could be done based on a count vectorizer with $n$ -grams on the character level. By doing so you may be able to compare "similar" names, addresses etc. Redundant text seems not to be a big issue here. from sklearn.feature_extraction.text import CountVectorizer true = ["Mister Example 10 Liversidge St Acton ACT 2601 Australia"] alt1 = ["Mr Example Liversidge St 10 2601 Acton ACT Australia"] alt2 = ["Mister Example 10 Liversidge St Acton ACT 2601 Australia and some redundant text here"] alt3 = ["Ms Catlady 11 Liversidge St Acton ACT 2601 Australia"] alt4 = ["Mr Entirely Different Other Street 123 9031 Anywhere USA"] vec = CountVectorizer(analyzer="char_wb", ngram_range=(3,3)) x = vec.fit(true) print(x.get_feature_names()) print(vec.transform(true).todense().sum()) # "truth" is the reference here print(vec.transform(alt1).todense().sum()) print(vec.transform(alt2).todense().sum()) print(vec.transform(alt3).todense().sum()) print(vec.transform(alt4).todense().sum()) # The single arguments true, alt1 etc can also be packt into a vector and transformed in one step This will give: 48 # truth as reference value 42 # minor variations to truth 48 # truth with redundant text 33 # similar address 3 # different address
