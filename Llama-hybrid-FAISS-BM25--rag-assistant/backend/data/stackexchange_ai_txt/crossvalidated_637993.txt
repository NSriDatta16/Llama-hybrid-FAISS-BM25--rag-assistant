[site]: crossvalidated
[post_id]: 637993
[parent_id]: 637483
[tags]: 
Though not standard it's common to see the bottom notation of expectation in the expression above as discussed in this post . Specifically in this part of GAN's loss function, you are essentially considering an expectation over the conditional distribution of assessment samples $\mu_D(x)$ as they vary with different $x$ drawn from the reference distribution $\mu_{\text{ref}}$ . Thus you're right that it should be considered as an expectation over a parameterized family of distributions and it's a way to express the expectation over both the randomness in choosing $x$ from the real data distribution and the randomness in generating $y$ from the discriminator network. The GAN training process involves adjusting the parameters of $\mu_D(x)$ to minimize or maximize its entire loss function aimed by the generator and the discriminator, respectively. â€‹
