[site]: crossvalidated
[post_id]: 487223
[parent_id]: 
[tags]: 
Is this the correct hypothesis function for logistic regression?

I was reading a popular article on adversarial training. https://adversarial-ml-tutorial.org/linear_models/ It says, In this case, rather than use multi-class cross entropy loss, weâ€™ll be adopting the more common approach and using the binary cross entropy, or logistic loss. In this setting, we have our hypothesis function, $h_\theta(x) = w^Tx + b$ . Is the hypothesis in logistic regression simply a linear function? I feel like this violates the definition of hypothesis , which is a function that maps to the space containing the labels. Since the labels are $\{\pm 1\}$ for binary classification, therefore it should make sense to take the hypothesis function as $h_\theta(x) = \text{sigmoid}(w^Tx + b)$ or even better, $h_\theta(x) = \text{sign}(\text{sigmoid}(w^Tx + b) - 0.5)$ . Can someone chime in on whether the choice of hypothesis is correct in the article? See Shwartz, Ben David Understanding Machine Learning
