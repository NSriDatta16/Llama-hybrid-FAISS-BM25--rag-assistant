[site]: stackoverflow
[post_id]: 4426282
[parent_id]: 4423647
[tags]: 
I think the bottleneck you have here is twofold. Depending on your OS and disc controller, the calls to f.read(2) with f being a bigish file are usually efficiently buffered -- usually . In other words, the OS will read one or two sectors (with disc sectors usually several KB) off disc into memory because this is not a lot more expensive than reading 2 bytes from that file. The extra bytes are cached efficiently in memory ready for the next call to read that file. Don't rely on that behavior -- it might be your bottleneck -- but I think there are other issues here. I am more concerned about the single byte conversions to a short and single calls to numpy. These are not cached at all. You can keep all the shorts in a Python list of ints and convert the whole list to numpy when (and if) needed. You can also make a single call struct.unpack_from to convert everything in a buffer vs one short at a time. Consider: #!/usr/bin/python import random import os import struct import numpy import ctypes def read_wopper(filename,bytes=2,endian='>h'): buf_size=1024*2 buf=ctypes.create_string_buffer(buf_size) new_buf=[] with open(filename,'rb') as f: while True: st=f.read(buf_size) l=len(st) if l==0: break fmt=endian[0]+str(l/bytes)+endian[1] new_buf+=(struct.unpack_from(fmt,st)) na=numpy.array(new_buf) return na fn='bigintfile' def createmyfile(filename): bytes=165924350 endian='>h' f=open(filename,"wb") count=0 try: for int in range(0,bytes/2): # The first 32,767 values are [0,1,2..0x7FFF] # to allow testing the read values with new_buf[value I created a file of random shorts signed ints of 165,924,350 bytes (158.24 MB) which comports to 82,962,175 signed 2 byte shorts. With this file, I ran the read_wopper function above and it ran in: real 0m15.846s user 0m12.416s sys 0m3.426s If you don't need the shorts to be numpy, this function runs in 6 seconds. All this on OS X, python 2.6.1 64 bit, 2.93 gHz Core i7, 8 GB ram. If you change buf_size=1024*2 in read_wopper to buf_size=2**16 the run time is: real 0m10.810s user 0m10.156s sys 0m0.651s So your main bottle neck, I think, is the single byte calls to unpack -- not your 2 byte reads from disc. You might want to make sure that your data files are not fragmented and if you are using OS X that your free disc space (and here ) is not fragmented. Edit I posted the full code to create then read a binary file of ints. On my iMac, I consistently get
