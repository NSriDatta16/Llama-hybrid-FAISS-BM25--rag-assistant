[site]: crossvalidated
[post_id]: 322720
[parent_id]: 322718
[tags]: 
If I understand your question correctly, you're saying that typically after training a CNN with a softmax classifier layer, people then do additional training using an SVM or GBM on the last feature layer, to squeeze out even more accuracy. Typically the network is pretrained using a standard set of neural layers, and then you pass all your images to get the feature layer embedding, after which you train an SVM or GBM from scratch on a now fixed input. However, you would like to train both at the same time. The answer is yes, it's theoretically possible. The loss function is exactly the same as for your classifier, it's just that you're using an SVM instead of a neural network layer to do the final classification part. However, this can be quite slow. Typical feature layers are on the order of 1000 dimensions. Also, your CNN feature layer changes over time since the network is learning. So that's why people
