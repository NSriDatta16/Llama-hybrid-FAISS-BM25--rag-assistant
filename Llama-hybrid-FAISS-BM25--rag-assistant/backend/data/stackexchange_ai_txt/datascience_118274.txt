[site]: datascience
[post_id]: 118274
[parent_id]: 118273
[tags]: 
From the OpenAI website , we know that ChatGPT is a fine-tuned version of GPT-3.5 ( text-davinci-002 ). On the GPT-3.5 presentation page , they mention that the number of parameters is 175B (in the footnote of the table there, we can read that there may be slight differences with the actual model used in the API, though), which matches the size of GPT-3. Therefore I understand that ChatGPT is the same size as GPT-3. The details of the architecture of GPT-3 have been made public in the paper : Also, in the paper they mention that: All models use a context window of nctx = 2048 tokens So: Number of layers: 96 Number of attention heads: 96 Dimensions of its hidden layers: 12288 Sequence length: 2048 Number of parameters: 175B Note that, as ChatGPT comes from GPT-3 and GPT-3 is an evolution of GPT-2, we also know from their papers that the architecture differs somewhat from a standard Transformer decoder. See this answer for details on the specific architectural differences.
