[site]: crossvalidated
[post_id]: 354633
[parent_id]: 333510
[tags]: 
In comments, @user18764 writes: Since words are encoded as one-hot vectors, each column of the left weight matrix W corresponds to a word-embedding (if we assume the form h=Wx). the embeddings are encoded in the transformation from x to h. The second weight matrix gives information about how the embedded word h relates to it's context vector y. The context vector is essentially the prediction of the distribution of words in the neighborhood of x. I've copied this comment as a community wiki answer because the comment is, more or less, an answer to this question. We have a dramatic gap between answers and questions. At least part of the problem is that some questions are answered in comments: if comments which answered the question were answers instead, we would have fewer unanswered questions. Please review Are we seeing a dramatic drop in answers per question? Comments that are actually answers
