[site]: crossvalidated
[post_id]: 390289
[parent_id]: 
[tags]: 
When deciding how to scale features, do the types of activation function matter?

Typically there are two types of feature scaling methods: Z-score scaling (standardization) and Min-max scaling (normalization). Standardization normalizes each column towards a mean of zero and standardization of ones. Min-max scaling ensures all the values are between 0 and 1. I was told that, when choosing the feature scaling method from the two, the distribution of each feature matters. "The scaling using mean and standard deviation assumes that the data are normally distributed, that is, most of the data are sufficiently close to the mean" " Guassian scaling will be helpful if your data are roughly normally distributed. Otherwise, some other kind of scaling might give better results ." However today I am reading this book about Deep Learning , and it mentioned: This (Z-score) is particularly suitable for activation functions such as tanh that output values on both sides of zero. Min-max scaling: This ensures all the values are between 0 and 1, that is, positive. This is the default approach if we are using sigmoid as our output activation. How so? How are the choice of feature scaling related to the activation functions? It seems to me that both Sigmoid and Tanh function have maximum gradient when the input is around 0. Is it about the output instead of the input?
