[site]: crossvalidated
[post_id]: 461098
[parent_id]: 460900
[tags]: 
So what is the derivative of the softmax with respect to L. Normally, the loss function used in neural networks is a scalar, which is why we work with partial derivatives ( $L:\mathbb R^n \mapsto \mathbb R^1$ ). If you have a vector valued input to a loss function and a vector valued output ( $L:\mathbb R^n \mapsto \mathbb R^m$ ), first-order derivatives are summarized in a Jacobian matrix. Let's assume you have a loss function that produces a vector. In this case, your derivative calculation would make sense only if $a_1$ is not used in the calculation of $y_2$ and $y_3$ . Otherwise, for each parameter (e.g. $a_1$ ), you end up with a Jacobian as opposed to a partial derivative. How to update parameters using a vector-valued derivative is unclear, as you are now trying to optimize for multiple different outcomes. This is not to say that we don't do this, in fact, multi-task learning does exactly this. As it turns out, there is a whole lot of research on this topic alone. Check out this paper for example. Other friendly and more accessible introduction to this setting are here and here .
