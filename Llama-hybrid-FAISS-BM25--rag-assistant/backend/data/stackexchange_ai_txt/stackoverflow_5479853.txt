[site]: stackoverflow
[post_id]: 5479853
[parent_id]: 5479013
[tags]: 
One thing I see that may explain both your problems is that you are doing many allocations, a lot of which appear to be temporary. For example, in your loading you: Allocate a temporary string per row Allocate a temporary string per column Copy the row to a temporary ElementSet Copy that to a RowSet Copy the RowSet to a Table Copy the Table to a TableDescriptor Copy the TableDescriptor to a Database As far as I can tell, each of these copies is a complete new copy of the object. If you only had a few 100 or 1000 records that might be fine but in your case you have 10 million records so the copies will be time consuming. Your loading times may differ due to the number of allocations done in the loading loop per row and per column. Memory fragmentation may also contribute at some point (when dealing with a large number of small allocations the default memory handler sometimes takes a long time to allocate new memory). Even if you removed all your unnecessary allocations I would still expect the 100 column case to be slightly slower than the 100,000 case due to how your are loading and parsing by line. Your information access times may be different as you are creating a full copy of a row in selectedElementSet . When you have 100 columns this will be fast but when you have 100,000 columns it will be slow. A few specific suggestions to improving your code: Reduce the number of allocations and copies you make. The ideal case would be to make one allocation for reading the file and then another allocation per record when stored. If you're going to store the data in a Database then put it there from the beginning. Don't make half-a-dozen complete copies of your data to go from a temporary object to the Database. Make use of references to the data instead of actual copies when possible. When profiling make sure you get times when running a new instance of the program. Memory use and fragmentation may have a significant impact if you test both cases in the same instance and the order in which you do the tests will matter. Edit: Code Suggestion To hopefully improve your speed in the search loop try something like: for(int z=0;z I've just changed the selectedElementSet to use a reference which should complete eliminate the row copies taking place and, in theory, it should have a noticeable impact in performance. For even more performance gain you can change shownVector to be a reference/pointer to avoid yet another copy. Edit: Answer Comment You asked where you were making copies. The following lines in your original code: ElementSet selectedElementSet; selectedElementSet = selectedTable.getRowsCols().at(n); creates a copy of the vector elements member in ElementSet . In the 100,000 column case this will be a vector containing 100,000 strings so the copy will be relatively expensive time wise. Since you don't actually need to create a new copy changing selectedElementSet to be a reference, like in my example code above, will eliminate this copy.
