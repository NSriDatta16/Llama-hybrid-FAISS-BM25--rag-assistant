[site]: crossvalidated
[post_id]: 326065
[parent_id]: 
[tags]: 
Cross Entropy vs. Sparse Cross Entropy: When to use one over the other

I am playing with convolutional neural networks using Keras+Tensorflow to classify categorical data. I have a choice of two loss functions: categorial_crossentropy and sparse_categorial_crossentropy . I have a good intuition about the categorial_crossentropy loss function, which is defined as follows: $$ J(\textbf{w}) = -\frac{1}{N} \sum_{i=1}^{N} \left[ y_i \text{log}(\hat{y}_i) + (1-y_i) \text{log}(1-\hat{y}_i) \right] $$ where, $\textbf{w}$ refer to the model parameters, e.g. weights of the neural network $y_i$ is the true label $\hat{y_i}$ is the predicted label Both labels use the one-hot encoded scheme. Questions: How does the above loss function change in sparse_categorial_crossentropy ? What is the mathematical intuition behind it? When to use one over the other?
