[site]: crossvalidated
[post_id]: 497055
[parent_id]: 497044
[tags]: 
I assume it makes sense to model your observations as i.i.d. (identically and independently distributed) - there should not be obvious reasons in the way the observations were collected and measured to think that the observations are dependent (e.g., have a time series structure) or come in groups etc. If this is so, the Central Limit Theorem says that normal distribution theory to do basic inference such as confidence intervals can be used for a large enough number of observations. How large is large enough depends on the distribution. Basically the only threat to approximate normality if you have 1000+ observations are gross outliers, either really extreme or many of them. If you don't have such a thing, normality based inference will be valid, and you will not need to simulate. Note however that if your distribution is indeed lognormal or skewed in a way that a log-transformation brings it closer to normality, you may be more efficient and precise transforming the data to logarithms and run inference on the transformed data. The CLT means that (in all likelihood) normal inference is also valid for untransformed data, but it being valid doesn't necessarily mean that it cannot be improved.
