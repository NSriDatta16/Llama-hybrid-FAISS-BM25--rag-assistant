[site]: crossvalidated
[post_id]: 216304
[parent_id]: 216289
[tags]: 
Instead of using hand-tuned features, I'd suggest using some automated form of dimensionality reduction, which will automatically extract features from your data. There are many ways to do this, ranging from the simple, linear case (e.g. PCA) to complex, nonlinear 'manifold learning' algorithms (e.g. t-SNE). Neural networks (particularly variants like autoencoders) could also work well. All of the methods I just mentioned are unsupervised, which means that they don't use labels (in your case, you'd give the images as input, but you wouldn't specify what letters they represent). Unsupervised methods work by attempting to find 'structure' in the data; what this means varies from algorithm to algorithm. There's no guarantee that the images will form clusters around the letters they represent; whether this happens or not depends on the data and how structure is defined. You'll probably find this blog post interesting. They run various dimensionality reduction methods on the MNIST dataset, which is a set of grayscale images of hand-drawn digits. If you explicitly want the extracted features to cluster (or at least be separable) in terms of the letters they represent, then it might be better to use a supervised algorithm (i.e. to train a classifier). In your case, you'd give the training algorithm a set of images and a set of labels indicating the letters they represent. The training algorithm will attempt to learn a function (the classifier) that predicts the represented letter from the image. Some classifiers will give you a set of extracted features, and others won't. For example, a k nearest neighbors classifier won't, so it won't be useful for your purposes (assuming that what you want to do is examine the features). A simple, linear, supervised method is linear discriminant analysis (LDA). It will give you a set of features that are linear combinations of your input pixels. It attempts to find the weights of these linear combinations such that, when the data are projected into the feature space, it's possible to linearly separate the data points in each class (i.e. to find hyperplanes that separate the images representing each letter). Note that it may not be possible to linearly separate the classes in this way, and LDA will try to do the best it can. A more complicated, non-linear class of methods is supervised neural networks. In this case, the network would take an image as input, and output the predicted letter the image represents (or a probability distribution over possible letters). The network would be trained to correctly classify images of letters. You can think of the units (i.e. 'neurons') in each layer of the network as representing features of the input image. Therefore, if you want to use the network for feature extraction, simply take the vector of unit activations for some layer of the network. Dimensionality reduction and supervised learning are enormous fields, but hopefully these few examples can help getting started looking for more methods.
