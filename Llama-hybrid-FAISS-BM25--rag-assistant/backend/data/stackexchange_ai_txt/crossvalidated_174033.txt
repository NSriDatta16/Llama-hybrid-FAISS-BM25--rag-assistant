[site]: crossvalidated
[post_id]: 174033
[parent_id]: 
[tags]: 
If a decision tree already has very low entropy, do we still need a random forest?

I need some help understanding the concept of random forests. As I understand, when I make a decision tree, I carefully select each node to maximize the information gain and minimize the entropy, i.e. each node should result in a higher information gain than its parent node. If that is true, then the decision tree is already the best possible learner. Why do I need to combine it with other trees that may not be as good and then take a vote? If I created the tree to maximize the information gain, then this is already the 'best' model. I would understand the need for a random forest if I created 10 decision trees by randomly selecting the nodes to split on.
