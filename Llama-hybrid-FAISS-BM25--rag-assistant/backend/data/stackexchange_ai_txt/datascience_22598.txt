[site]: datascience
[post_id]: 22598
[parent_id]: 
[tags]: 
Understanding Logistic Regression Cost function

Linear Regression cost function: $$J(\theta) = \frac{1}{2 m} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)})^2$$ where: $$h_{\theta}(x) = \theta_0 + \theta_1 x_1$$ Logistic Regression cost function $$J(\theta) = \frac{1}{m} \sum_{i=1}^m - (y^{(i)} \times log(h_{\theta}(x{(i)})) + (1-y{(i)}) \times log(1 - h_{\theta}(x{(i)})))$$ where: $$h_{\theta}(x) = g(\theta_0 + \theta_1 x_1 + \theta_2 x_2)$$ Intuitively linear regression is easy to understand as it optimizes the average squared distance between hypothesis and training data. But in case of logistic regression , I failed to understand the cost function. What does the logistic regression cost function represent?
