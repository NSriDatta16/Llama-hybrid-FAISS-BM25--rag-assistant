[site]: crossvalidated
[post_id]: 176802
[parent_id]: 176430
[tags]: 
Bagging is an ensemble method where you train model on independent samples of the training data and combine (average, vote, ...) their predictions. This generally produces more accurate predictions than the individual models. Technically bagging means that the samples are drawn with replacement and of the same size as the full data set. However the term is sometimes also applied to other sampling schemes. Bagged Logistic Regression means bagging using logistic regression for the individual models, but it is bagging in the loose sense of the word. They are really combining subsampling (ie sampling without replacement) with randomized subspaces (sampling the columns/features). In the quote ps is the fraction of the rows/items included in each sample and pc is the fraction of columns/features. They just use a more statistics flavored terminology where observations are the rows and covariates are the columns. This is close to what sklearn.linear_model.RandomizedLogisticRegression does internally. The main differences are that RandomizedLogisticRegression does not support column sampling and also it is not a predictive model. It is only used to select relevant features. Bagging does not really offer anything extra for dealing with sequencing information. You can create features that encode the sequencing information as you would with any other machine learning method, but if that is the main thing you are interested in you should look into specialized methods.
