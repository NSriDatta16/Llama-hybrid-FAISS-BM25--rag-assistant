[site]: datascience
[post_id]: 118104
[parent_id]: 
[tags]: 
Can using the mean of absolute Shapely Values for feature importance give very wrong results?

In a classification problem, suppose a model has 2 variables, A and B , and the null model (the model without any variable) predicts 50% probability for belonging to class 1 for all the instances. Now suppose that for all the data instances xi , feature A makes the model to predict the opposite value of the correct class with a large magnitude(i.e. if the true class is 1 and the null model predicts 0.5, A makes the model to predict 0.1 probability), and B makes the model to predict toward the correct class but with less magnitude (i.e. if the true class is 1 and the null model predicts 0.5, adding B makes the model to predict 0.57). This means that the feature **B is actually a better feature than A but the impact of A is stronger than B . One common way of calculating the feature importance is calculating the average of the absolute Shapley values for all the instances for feature A and B . The example that I just gave shows that this method of finding feature importance using mean absolute value would give wrong results since it would give feature A a higher importance that feature B . Am I wrong?
