[site]: crossvalidated
[post_id]: 450557
[parent_id]: 363149
[tags]: 
This is generally called "sensitivity analysis" or "stability". An excellent paper deriving bounds based on this is Stability and Generalization . The bounds of course aren't necessarily tight! If you look at Definition 19 and the follow-up Theorems and Lemmas you can see that if something is $\sigma$ -admissable then there is a bound that is linear in $\sigma$ generally. For $L_1$ it's fairly simple to show that it is $1$ -admissable (indeed, they state this for the $\epsilon$ -insensitive $L_1$ loss for SVM - example 1, bottom of page 17 in the pdf (515 is the printed page number)), while $L_2$ requires the space $\mathcal{Y}$ to be bound - if one does the math this is basically because one can derive $$\sigma \geq \frac{|y_1^2 - y_2^2 - 2y'(y_1-y_2)|}{|y_1 - y_2|} = |y_1+y_2 - 2y'|$$ . Thus, generally, one should expect the $L_1$ to actually have a better bound here. I don't think this completely addresses your question but it hopefully does give you a starting point for a more formal approach to analyzing your specifics.
