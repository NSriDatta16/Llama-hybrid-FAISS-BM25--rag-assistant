[site]: crossvalidated
[post_id]: 262469
[parent_id]: 
[tags]: 
Poisson deviance (xgboost vs gbm vs regression)

I would like to know which is the deviance expression in poisson regression using by xgboost tool (extreme gradient boosting). According to source code, the evaluation function is: struct EvalPoissonNegLogLik : public EvalEWiseBase { const char *Name() const override { return "poisson-nloglik"; } inline bst_float EvalRow(bst_float y, bst_float py) const { const bst_float eps = 1e-16f; if (py So deviance (in R) should be something like: poisson_deviance I have two questions here: 1) How translate LogGamma to R?. I found several links googling 'loggamma' and seems each language understand differents expressions for this term. 2) What to do with exposures? I know we need to set to xgbMatrix using: setinfo(xgbMatrix, "base_margin", log(exposure)) But in the code of EvalPoissonNegLogLik I never saw the offset again, so what I deducted is that the only we need is to add the log(exposure) to predictiors: poisson_deviance The deviance formula used by gradient boosting gbm R package for poisson regression is: poisson_deviance (capped py at eps too) As you can see in the last page of this document : Are gbm and xgboost using the same error for poisson regression? This expression of deviance seems different that what is used in xgboost . At last, the deviance formula in poisson regression according to B.5.3 in here should be: 2 * mean(y * log(y / py) - (y - py)) that is other different formula. I would appreciate any help to understand why both gbm and xgboost use other deviance formulation.
