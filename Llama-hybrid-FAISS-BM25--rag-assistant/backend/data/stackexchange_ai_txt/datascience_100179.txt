[site]: datascience
[post_id]: 100179
[parent_id]: 
[tags]: 
Remedy for small batch size?

I am trying to reproduce results of other people's research, but we cannot afford to do it with the same batch size as theirs, due to limited computing resources. The method they use is a simple sequence classification using BERT. They do it with batch size 48, learning rate 4e-5, optimization Adam, and epoch 1. Their result is f2 score of 0.65 and we are nowhere near there despite doing everything the same except the batch size (we do it with batch size 16). What are some ways to compensate for small batch size? I would greatly appreciate any suggestions.
