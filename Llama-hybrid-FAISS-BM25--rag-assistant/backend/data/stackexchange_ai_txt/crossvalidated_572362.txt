[site]: crossvalidated
[post_id]: 572362
[parent_id]: 571575
[tags]: 
Caveat : You don't state it explicitly but you imply that, for each participant, their trials are independent with the same probability of success. In this case, the number of successes is a binomial random variable with size = 8 and unknown probability p . Update : In the comments you explain that the independence assumption holds but the success probability may change as participants become familiar with the experiment. In this case, the trials are binomial random variables with size = 1 and unknown probability p t . You can still use binomial regression after updating the model to include a fixed effect t for the sequence of trials. We can compare conditions A and B with a binomial regression. We assume that: Each participant goes through a fixed number of trials (not necessarily the same number of trials). The success probability of each participant is a function of their group assignment + random noise. Under this model, conditions A and B have effects α and β, and the average success probabilities are a function of this effect. [Mathematically, effect = logit (probability) .] Finally, we test if there is a difference between conditions A and B with a t-test on the difference in effects, β - α . The following code illustrates how to do this analysis in R. library("broomExtra") library("lme4") set.seed(1234) n Group.1 x #> 1 A 0.4749336 #> 2 B 1.9422930 # The success probability is the inverse logit of the participant's effect. prob Let's start with a simpler model that assumes participants in each group have exactly the same probability of success. This is not true in the simulation and it's also not a reasonable assumption in general. model1 # A tibble: 2 × 7 #> term estimate std.error statistic p.value conf.low conf.high #> #> 1 (Intercept) 0.674 0.167 4.03 0.0000548 0.352 1.01 #> 2 conditionB 1.67 0.326 5.12 0.000000299 1.06 2.34 It's more appropriate — and in the simulation definitely correct — that participants effects are not fixed but random , with mean equal to the condition effect. model2 # A tibble: 3 × 9 #> effect group term estimate std.error statistic p.value conf.low conf.high #> #> 1 fixed (Inte… 0.704 0.200 3.51 4.42e-4 0.311 1.10 #> 2 fixed condi… 1.72 0.367 4.69 2.67e-6 1.00 2.44 #> 3 ran_pars id sd__(… 0.440 NA NA NA NA NA In the summary table, conditionB estimates the effect difference β - α between conditions A and B. Since this is a simulation, we know that the true difference is β - α = 2 - 0.5 = 1.5 . Its estimate from the data is conditionB = 1.72 with a 95% confidence interval [1.00, 2.44] . The t-statistic for the null hypothesis that there is no difference between the conditions is 4.69 and the corresponding p-value is 2.7e-6 . So there is strong evidence to reject the null, which of course doesn't hold in the simulation. The effects α and β are logits (log odds) of the success probabilities; the difference β - α is the log odds ratio. It's easy to convert effects to probabilities with the inverse logit transformation. # Pr{success in group A}, Pr{success in group B}, Pr{success in B} - Pr{success in A} c(plogis(alpha), plogis(beta), plogis(beta) - plogis(alpha)) #> [1] 0.6224593 0.8807971 0.2583377 # estimate of Pr{success in B} - Pr{success in A} plogis(1.72 + 0.704) - plogis(0.704) #> [1] 0.2495652
