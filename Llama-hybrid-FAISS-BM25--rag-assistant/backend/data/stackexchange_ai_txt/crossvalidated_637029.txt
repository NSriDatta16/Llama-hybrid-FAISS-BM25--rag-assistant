[site]: crossvalidated
[post_id]: 637029
[parent_id]: 232969
[tags]: 
This is true, for instance, in linear networks. Consider the following product of $w\times w$ matrices $$y=x W_1\ldots W_d$$ Now suppose our target labels are all 0's and we fit using least-squares loss. If we initialize $W_i$ with IID $\mathcal N(0, 1/w)$ , which is scale required to keep intermediate activations from exploding or shrinking to 0, trace of our loss Hessian w.r.t to $x$ is $O(d)$ by using this result . Hence we must divide learning rate by $d$ , the number of layers, to remain convergent. Here's a plot of Hessian eigenvalues as function of depth for a randomly initialized linear neural network: notebook
