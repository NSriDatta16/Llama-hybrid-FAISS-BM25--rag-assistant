[site]: crossvalidated
[post_id]: 355927
[parent_id]: 355427
[tags]: 
[...] each training sample will be provided to its own neural network [...] Actually there is only one autoencoder shared by all samples and updated by all samples. This is convention in neural network. However, the main contribution in this paper is, for each training sample passing through the network, they only update the parameters associated with the non-empty values in input/output vectors (vectors are sparse). This could be viewed as splitting the parameters matrices into different smaller matrices for each training sample. Thus they mentioned separate model for each training sample, but it actually means they update separate parameters for each sample. As a side note, the paper you referenced is a short paper, you should find the full paper version, which usually has more details.
