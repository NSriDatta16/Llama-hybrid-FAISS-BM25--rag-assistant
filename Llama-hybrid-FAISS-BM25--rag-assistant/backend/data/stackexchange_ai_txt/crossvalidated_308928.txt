[site]: crossvalidated
[post_id]: 308928
[parent_id]: 308870
[tags]: 
I assume that the samples from $D_Z$ and $D_X$ are iid. That is, for any $X_{new}$ input, your NN shouldn't care if you randomly swap rows, but it should care if you randomly swap columns. Therefore one $\mathbb{R}^{k \times p}$ input to your network should be invariant to translation along the vertical direction, but not invariant along the horizontal direction. With that said, I think you should use an ordinary vanilla neural network with $k p$ inputs, but with the weight sharing condition that $W_i^l = W_{i+p}^l$ for the first layer, where the subscript indicates the index of the input node, and the superscript $l$ indicates the index of the node in the first hidden layer. In words, the weight corresponding to the $i,j$th element of $X_{new}$ is the same as the weight corresponding to the $(i+p),j$th element of $X_{new}$ for a given node in the hidden layer. With this weight sharing condition, your NN does not treat the $j$th element of one row in $X_{new}$ differently than the $j$th element of another row, so swapping rows in your $X_{new}$ would not change the result. Also, this NN only needs to learn as many weights as a NN with only $p$ inputs, but it still makes use of all $k \times p$ inputs as a single entry. This is much simpler than the kind of weight-sharing scheme you find in a CNN.
