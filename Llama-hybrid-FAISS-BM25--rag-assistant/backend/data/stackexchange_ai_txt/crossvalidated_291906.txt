[site]: crossvalidated
[post_id]: 291906
[parent_id]: 
[tags]: 
Can reinforcement learning be "stateless"?

After learning a little bit about RL and Q-Learning, I am a bit puzzled at why it seems that for many forms of learning scenarios, there does not seem to be a "state". I understand that if we wish for a robot to navigate around a maze, each position in the maze is a state and the robot has a selection of actions {up, down, left, right}. We want to find a sequence of actions that leads the robot to the exit of the maze or to the highest reward. However, it seems that in other situations, no state is needed. For example, if two players are playing rock-paper-scissors, at each round of the game, each player throws a hand sign and receives some reward. The goal is to throw a sequence of hand signs to maximize the reward over time. It is not clear to me what the state is in this situation. Is it every unit of time? Similarly, suppose a single player has a selection of buttons he can press, and every time he presses a button, some reward is provided. Again, there does not seem to be a state involved. There is just action and reward, that's it. Does anyone know if reinforcement learning can be formulated without states? If so, what distinguishes these types of RL with the RL that requires statese?
