[site]: crossvalidated
[post_id]: 249645
[parent_id]: 
[tags]: 
What does it mean when all gradients of a neural network are 0?

I am running a convolutional neural network on image data, and returning the gradients in each step yields gradients of exactly zero. At the same time, the network is not converging, and returns high loss. What does this mean in terms of what I should do to learning rate, momentum, decay, etc.? Thanks!
