[site]: crossvalidated
[post_id]: 615725
[parent_id]: 
[tags]: 
Why can't we replace the multi-head attention layer in transformers by a single head with sigmoid function?

I know that the reason we use multi-head attention in transformers instead of only single head is to attend to different parts of the input instead of just only one part. In attention we use softmax function to give attention weights to every input token. We can't attend to different parts of the input using one attention head because of the softmax function. but what if we used sigmoid function instead of softmax? For example: If one token in a sentence wants to attend to the first token and the last one. If we used only one-head attention with softmax, It would give approximately .5 attention weights to the first and last token and that's not right so we solve this by using multi-head attention. Can't we also solve that issue by using sigmoid to give (approximately) atttention weight value of 1 to the first and last tokens?
