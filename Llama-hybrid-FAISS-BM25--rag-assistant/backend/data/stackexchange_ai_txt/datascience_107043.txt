[site]: datascience
[post_id]: 107043
[parent_id]: 
[tags]: 
Best practices to train a transformer text classifier to predict/handle unseen labels

I fine-tuned a RoBERTa sequence classifier to classify paragraphs of certain documents using labeled paragraphs only (and skipping paragraphs with no label given). The model was validated and tested on labeled paragraphs only as well. I am using an early stopping criterion to stop training if the cross validation loss is not improving for 5 epochs. Training accuracy: 0.97 Validation accuracy: 1.00 Now, during inference on the whole documents (labeled and unlabelled) paragraphs my model is predicting a label for each paragraph, which is expected behaviour of course. I was hoping to handle the misclassifications with some sort of threshold (i.e. predict as None ) if the model's confidence score is below 0.8 (or whatever). Unfortunately, the model predicts some of the unlabelled paragraphs with a pretty high confidence score (~0.99) which makes the use of any threshold impossible. Unfortunately, my dataset only consists of 200 data points (I know its almost nothing but getting more data is really hard for my task). Now my questions: Do you think my model is overfitting? (Validation and Test accuracy seems fine though) Is there a best practice to train a model on a limited set of labels knowing that at inference time the model will see previously unseen and unlabelled texts? I could try and train a model with the unlabelled paragraphs giving them an other label. But it seems like bad practice? Any other suggestions?
