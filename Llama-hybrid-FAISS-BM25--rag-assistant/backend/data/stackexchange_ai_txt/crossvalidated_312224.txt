[site]: crossvalidated
[post_id]: 312224
[parent_id]: 312217
[tags]: 
You are describing a grid approximation to the posterior, and that is a valid approach, allthough not the most popular. There are quite a few cases in which the posterior distribution can be computed analytically. Monte Carlo Markov Chains, or other approximate methods, are methods to obtain samples of the posterior distribution, that sometimes work when the analytical solution cannot be found. The analytical solutions that can be found are typically cases of "conjugate" families, and you can find more about that by googling, see for example https://en.wikipedia.org/wiki/Conjugate_prior . As a first example, if your prior on p is uniform on [0, 1] , where p is a success parameter in a simple binomial experiment, the posterior is equal to a Beta distribution. Integration, or summation, can be done explicitly in this case. If you have finitely many parameter choices, or you use a grid approximation as in your example, a simple summation may be all you need. The number of computations can explode quickly however, if you have a couple of variables and want to use a dense grid. There are several algorithms for sampling from the posterior. Hamiltonian Monte Carlo, specifically the NUTS sampler, is now popular and used in stan and PyMC3 , Metropolis Hastings is the classic. Variational Inference is a relative newcomer, not a sampling method actually but a different way of obtaining an approximation. At the moment, none of the methods, including analytical solutions, are the best, they all work well in specific cases.
