[site]: crossvalidated
[post_id]: 620164
[parent_id]: 620093
[tags]: 
As you mention, a common way to measure similarity between word embeddings is the cosine distance. Why ist that? Its because word embeddings are high dimensional, 100-1000 dimensions are common. In this dimensionality everything is sparse and distance are always huge, a "phenomenon" that is called "curse of dimensionality". Cosine distance only takes the angle into account. You could project your datapoints onto a sphere (should be the same as normalizing the length to 1) and then run a PCA for dimensionality reduction. Now euclidean distances are more comparable to those of their cosine counterparts in the original embedding. edit I missed OP's comment about this approach not working. Why is that? Don't expect those distances to be identical or similar in magnitude, rather they relation should be somewhat constant. That is, distances are scaled between both "worlds", but should have a constant relation.
