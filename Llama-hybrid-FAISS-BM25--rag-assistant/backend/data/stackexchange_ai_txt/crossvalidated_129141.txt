[site]: crossvalidated
[post_id]: 129141
[parent_id]: 100281
[tags]: 
I first would suggest a model selection criterion that accounts for model complexity AND residual error. Candidates include Akaike Information Criteria ( AIC ), Bayes information Criteria ( BIC ), and many others. I have used AIC to good effect when selecting among non-parametric models as well (Spline Smoothing). I know this is debating terminology, but non-parametric models have parameters and in many cases have more parameters than the data sample count. I have not used these models outside their class. While I can pick the best non-parametric model of a family of such models against the data, and I can pick the best parametric (linear, polynomial, analytic, et cetera) model against other parametric models, I do not know if when comparing the AIC of a non-parametric model vs. a parametric model the results will be what I am expecting. This is an interesting line of questioning. I know that random forests can be used to determine variable importance. Perhaps the Gini score, related to the Kullback-Leibler divergence, is itself an information criteria. If so then one might use a random forest as a model selector or perhaps just a model pruner. Could variable importance in an RF be used to determine whether another RF was a better indicator of variable importance or whether a linear function was. Perhaps some of these would be relevant: http://dl.acm.org/citation.cfm?id=1143865 http://www.sciencedirect.com/science/article/pii/S0167865510000954
