[site]: crossvalidated
[post_id]: 532934
[parent_id]: 
[tags]: 
Run-length encoding on the input sequence of a RNN

I have a dataset consisting of sequences of item embeddings a, b, c, ... , in which the length of consecutive runs of an item may be large (comparative to the sequence length). I want to train an RNN that produces a summary of the sequence (by taking the hidden state at the last timestep). I am currently using a max sequence length L (derived from the data), so that for sequences S of length |S| > L , I am removing the first |S| - L items. Doing this may exclude certain items from the sequences. I was wondering whether I can apply run-length encoding as a pre-processing step to effectively shorten the sequences and allow for more unique items to be included. For instance, for this sequence of length 12: a, a, a, b, b, a, a, c, a, b, b, c using run-length encoding the following compressed sequence of length 7 is obtained: (a,3), (b,2), (a,2), (c,1), (a,1), (b,2), (c,1) Essentially, in addition to feeding the item embedding ( a, b, c, ... ), an extra feature, the run length, is provided. Is there an obvious disadvantage / problem using this encoding scheme as a pre-processing stage for training an RNN?
