[site]: crossvalidated
[post_id]: 475284
[parent_id]: 475138
[tags]: 
From the contents of two popular textbooks, Casella and Berger (1990) -- Statistical Inference Efron (2006) -- Computer Age Statistical Inference I think statistical inference simply means mathematical and reasoning activities that try to make sense of data. More specifically, one may discern two approaches -- Bayesian and Frequentist, of which there are plenty of discussions on this site. I would point out that currently, most of the answers given to this question tend to have a Bayesian flavour. For example, trying to infer the underlying distribution of the data is a distinctly Bayesian activity. Frequentist inference is often more concerned with the procedure or algorithm that we apply to data, rather than the data itself. For example, one of the goals is to find the most powerful test of two hypothesis given the data. Judging by the contents of the book, it seems these activities also fall under the umbrella of statistical inference. Lastly, I also need to point out that in the age of machine learning, the term inference has taken on a new meaning which is rather different from the above. In the training of neural networks, inference is simply the opposite of training. Whereas in training, a model is "built", in inference, the model is applied for prediction (typically in new data). See, for example, this article .
