[site]: datascience
[post_id]: 123072
[parent_id]: 
[tags]: 
Feature importance score for a feature that contains mostly 0's in XGBoost

I have read that the feature importance scores are calculated based on how a split on that feature improves performance. I have a binary classification dataset and am running XGBoost classifier on it. There is 1 feature that has the value 0 for 1425 rows out of the 1431 rows. Out of the 5 rows for which it has data, 3 of them have a label 1 and 2 have the label 0. I would assume this feature, due to just how sparse it is and the fact that it has only 0's would have a very low feature importance score. However relative to other features, it has the 2nd highest score of 0.14416. In the test set, the model predictions are as follows: Class 0 Class 1 Class 0 227 23 Class 1 18 19 Could anyone please explain the intuition as to why this feature ends up getting a high importance? Does feature importance get biased for features that have high number of the same value? Some context: The target is if the response time of a server is high or low and this feature is the run time of a particular dunction. But that function turns out to run at very few hours this having 0's for most rows
