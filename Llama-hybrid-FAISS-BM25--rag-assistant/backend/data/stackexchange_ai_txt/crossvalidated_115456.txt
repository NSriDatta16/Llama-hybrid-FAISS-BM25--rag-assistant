[site]: crossvalidated
[post_id]: 115456
[parent_id]: 
[tags]: 
Linear-time approximation to kernel SVM?

Scaling kernel support vector machines to large datasets is a very challenging problem. For linear SVMs, PEGASOS is able to learn efficiently online, so training time scales linearly with the size of the dataset. However, PEGASOS works with the primal formulation of an SVM, and so it can't be extended to kernel methods. To do exact learning of a kernel SVM you must construct the entire Gram matrix, which takes quadratic time in the size of the data. Are there approximate methods for learning kernel SVMs in linear time that don't require the full Gram matrix but still have theoretical guarantees like PEGASOS? Or is this still an open research question?
