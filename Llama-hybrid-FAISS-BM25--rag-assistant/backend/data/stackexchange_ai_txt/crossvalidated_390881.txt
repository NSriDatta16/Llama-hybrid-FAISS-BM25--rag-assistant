[site]: crossvalidated
[post_id]: 390881
[parent_id]: 
[tags]: 
With limited training time - should I shrink the training set and run more epochs?

I wonder what is considered a better practice in deep learning. I have a dataset of 100K images that I want to use for training a regional-CNN for a whole week. Is it possible that my network will be better trained if I shrink the dataset, throwing away 75% of the samples (suppose that the data is shuffled and that every class has equal number of samples), just because it will run 4 times more epochs on it?
