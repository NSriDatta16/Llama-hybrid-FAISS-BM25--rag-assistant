[site]: datascience
[post_id]: 20373
[parent_id]: 
[tags]: 
What are the key differences between FPGA and GPUs for Deep Learning?

I'm trying to investigate the ways in which FPGAs differ to GPUs for the purpose of deep learning. I understand this is a complex question and not necessarily easy to answer in one go, however what I'm looking for are the key differences between the two technologies for this domain. Ideally, it would be nice to see the benefits and cons of both technologies with regards to Deep Learning. In addition, from my understanding, and please correct me if I'm wrong, is that with FPGAs, those undertaking a DL project would need someone who is able to configure the FPGA according to the type of project they want through the use of languages such as Verilog or VDHL. Furthermore, if they want to change the type of DL project they want to do, they have to reconfigure the FPGA to follow suit. Have I got this correct, or is something like Intel's Deep Learning Inference Accelerator ( https://www-ssl.intel.com/content/www/us/en/servers/accelerators/deep-learning-inference-accelerator-product-detail.html ) a lot more similar to GPUs than I thought in terms of configuration?
