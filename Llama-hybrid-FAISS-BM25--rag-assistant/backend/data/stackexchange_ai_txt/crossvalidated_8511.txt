[site]: crossvalidated
[post_id]: 8511
[parent_id]: 
[tags]: 
How to calculate pseudo-$R^2$ from R's logistic regression?

Christopher Manning's writeup on logistic regression in R shows a logistic regression in R as follows: ced.logr Some output: > summary(ced.logr) Call: glm(formula = ced.del ~ cat + follows + factor(class), family = binomial("logit")) Deviance Residuals: Min 1Q Median 3Q Max -3.24384 -1.34325 0.04954 1.01488 6.40094 Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) -1.31827 0.12221 -10.787 He then goes into some detail about how to interpret coefficients, compare different models, and so on. Quite useful. However, how much variance does the model account for? A Stata page on logistic regression says: Technically, $R^2$ cannot be computed the same way in logistic regression as it is in OLS regression. The pseudo-$R^2$, in logistic regression, is defined as $1 - \frac{L1}{L0}$, where $L0$ represents the log likelihood for the "constant-only" model and $L1$ is the log likelihood for the full model with constant and predictors. I understand this at the high level. The constant-only model would be without any of the parameters (only the intercept term). Log likelihood is a measure of how closely the parameters fit the data. In fact, Manning sort of hints that the deviance might be $-2 \log L$. Perhaps null deviance is constant-only and residual deviance is $-2 \log L$ of the model? However, I'm not crystal clear on it. Can someone verify how one actually computes the pseudo-$R^2$ in R using this example?
