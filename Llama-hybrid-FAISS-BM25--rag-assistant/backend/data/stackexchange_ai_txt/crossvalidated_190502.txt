[site]: crossvalidated
[post_id]: 190502
[parent_id]: 190326
[tags]: 
I guess what your looking for a way to quantify variable redundancy or the opposite complimentarity('new word'). A and B can either be replacable or complementary, hence removing A will make B less usefull/important as feature. E.g. knowing the lab who produced a doping analysis (B) may be be pretty useless without knowing the result(A). The problem is that C, D could be redundant also or complimentary with A and B. Simply rerunning analysis without feature A, may not strongly indicate B is redundant as C and D was also redundant. I would implement your idea as an ensemble of random forest models, where each forest has a random variable restriction. Write a wrapper around your favorite random forest algorithm and train multiple models. Go invent some nice statistics relating variable selection to OOB-CV performance and variable importance. The procedure would be pretty experimental, but it could be fun. I have experimented with 2way-variable importance: To measure the change of variable importance if another variable were permuted also. If variable importance increase, the variable pair were redundant. If variable importance decrease the to variables were complimentary. Variable complimentatity is observed for variables with a dominant interaction effect, otherwise not. If no change, then perhaps no redundancy or no complimentarity. Perhaps the redundancy were masked by other variables. I sketched the details of this implementation here Recursively dropping the remaining variables with 5% lowest variable importance and retrain does in practice what your looking for in a simple way. You need a external CV to evaluate if your model performance has increased.
