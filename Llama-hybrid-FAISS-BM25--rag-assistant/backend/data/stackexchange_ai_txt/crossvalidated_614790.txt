[site]: crossvalidated
[post_id]: 614790
[parent_id]: 
[tags]: 
Understanding Sequential Model-Based Optimization for machine learning

One of the hyperparameter tunning approaches that I came across recently is Sequential Model-Based Optimization (SMBO), which is a very smart approach that uses previous iterations in order to find the best values for the hyperparameter. I just want to make sure I understand it right. So there are several steps and factors for this algorithm, I'll talk about them and at the end I'll add a scheme to summarize it, please correct me if I'm wrong: Build the domain that has the ranges of values that we pick for each hyperparameter. Nothing special here, excpet that we can refer to this domain as a probability distribution that will change with each iteration (when new data comes along). Objective function: this is the function that we want to minimize or maximize (like loss function for example). But we don't use it that much because it's computationally costly and slow, so we use the surrogate. The surrogate "mimics" the objective function, and it evaluates the current values of the hyperparameters. Selection function: based on the surrogate function evaluations, the selection function will now pick new values for the hyperparameters to test. Now, we test those new values in the real objective function and not in the surrogate function as in step 3. we get the REAL score, and we add it to the "History" we repeat. Now the surrogate and the selection functions are updated based on the first round (the History). Here is a scheme that I prepared to sum it up more or less: Did I describe the steps correctly? is the scheme ok?
