[site]: crossvalidated
[post_id]: 586824
[parent_id]: 586821
[tags]: 
In ordinary least squares linear regression with just one feature and an intercept (so “simple” linear regression), there are a number of calculations that yield the same result. $(cor(x,y))^2$ $(cor(\hat y, y))^2$ $1 -\dfrac{\sum_{i=1}^n \left( y_i-\hat y_i \right)^2 }{\sum_{i=1}^n \left( y_i-\bar y \right)^2 }$ Consequently, any of these three can justifiably be called $R^2$ . Even moving to regression with multiple features, option #1 is not viable, but the other two remain viable calculations for any poring of truth values and predicted values. However, when you obtain the predictions by a method other than ordinary least squares, options #2 and #3 need not be equal. Your software is giving you the ability to pick which of those two calculations you want to perform. Option #2 is called the “corr” method, while option #3 is called the “traditional” method. For reasons I discuss here , it is the third calculation (equivalent to #4 in the link) that makes sense to me as a comparison of your modeling of the conditional expected value compared to a baseline model of the conditional expected value that naïvely guesses the pooled/marginal expected value $\bar y$ every time; pay particular attention to the simulation that shows how correlation-based $R^2$ can mislead you into thinking your predictions are good. If you are in a setting where #2 and #3 are equivalent, then feel free to do the easier correlation-based calculation, but, in my view, that is only because of the equivalence, not because of inherent value to the correlation-based calculations. (I can see some value to the correlation-based calculations in the sense that, a high correlation-based calculation and low (even negative) value of #3 could suggest a simple mapping between your poor predictions and good predictions, though I would wonder why I didn’t get good predictions at the beginning.)
