[site]: crossvalidated
[post_id]: 140636
[parent_id]: 139540
[tags]: 
Some mean by dimensionality reduction what is usually called data reduction . If this applies to you, then the main choices are Do you want to use unsupervised learning to reduce the number of parameters that need to be estimated so that you can avoid overfitting? If so then data reduction is for you. Do you believe that parsimony is appropriate and you want to use $Y$ to obtain a reduced model, risking more overfitting and model uncertainty because of phantom degrees of freedom? Then use traditional feature selection. This does have the advantage that results are easier to interpret, but this is somewhat because phantom degrees of freedom are being ignored.
