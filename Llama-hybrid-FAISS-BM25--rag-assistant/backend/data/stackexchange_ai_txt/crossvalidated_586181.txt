[site]: crossvalidated
[post_id]: 586181
[parent_id]: 
[tags]: 
Scoring Probabilistic Forecasts - Can we infer a standard deviation from the 84.1 quantile prediction?

I am trying to compare forecasts of a series, and have several trained estimators which are deep neural networks with arbitrary architecture. I'd like to compare the accuracy of their probabilistic forecasts for a forecast horizon > 1, for instance using the python properscoring library to calculate a CRPS , which judges the accuracy of their predicted distributions in comparison with the true observations. However computing the CRPS requires knowing the true output distribution of my estimator, which is not explicitly returned. If I trained the model using a GaussianLikelihood object, and can return arbitrary quantiles for the predictions, can I infer the predicted distribution from the prediction intervals as follows? For each t in t+1, ..., t+fh : The predicted distribution is a normal curve with a mean of yhat (the prediction) and a standard deviation of {predicted 84.1 quantile} - yhat. I could then compare the observed value to a distribution for each step in the test forecast horizon. Of course using the 84.1th quantile because in the normal curve ~68.2% of observed values are within one standard deviation from the mean, meaning 50 âˆ“ 34.1. This seems to make sense to me but a bit roundabout way to reverse engineer the normal curve. So my two questions are: Is this a sound approach? What is a more accurate way to compare probabilistic forecasts of arbitrary models, when you can access quantile predictions but not true PDFs?
