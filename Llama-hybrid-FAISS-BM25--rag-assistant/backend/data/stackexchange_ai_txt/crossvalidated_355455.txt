[site]: crossvalidated
[post_id]: 355455
[parent_id]: 355404
[tags]: 
The interpretation of logistic regression coefficients is similar in the case where you've standardized the data (subtract mean, divide by standard deviation of each feature). By standardizing, you effectively change the units to standard deviations above/below the mean. So, a one standard deviation increase in $X_1$ corresponds to a $\beta_1$ increase in the log odds. If you fit to standardized data, you can transform the coefficients back to the original units (or vice versa). If you fit a vanilla logistic regression model to standardized vs. non-standardized data, the coefficients will take different values in each case, but both models will fit equally well (or poorly). But, this is not necessarily true if you're fitting a regularized model (e.g. $\ell_1$ or $\ell_2$ penalties on the coefficients). In this case, it's common practice to standardize first, so that all features are penalized equally.
