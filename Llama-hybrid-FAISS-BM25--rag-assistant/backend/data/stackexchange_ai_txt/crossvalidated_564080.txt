[site]: crossvalidated
[post_id]: 564080
[parent_id]: 564077
[tags]: 
If one of your features X2 can be expressed by another one X1, you can drop X2 without loss of information. More precisely: if you can predict for each new measurement (X1, X2, X3, ..., Xn) the exact value of X2 from X1, X2 doesn't carry any new information and can be dropped; at least from the information-theoretical point of view. Think about it the other way around: if there was a difference, then it would make sense to create a lot of new variables that are just deterministic functions of those that are already present, and each one would improve the amount of contained information. Having said that, there is something like "feature engineering": sometimes (or should I rather say "always") it makes sense to replace a feature / several features with a transformation / several transformations of it/them. E.g., think of scaling or standardization. That could make your regression algorithm work better, but the details depend on the type of data and regression you use and are usually part of the documentation of that algorithm.
