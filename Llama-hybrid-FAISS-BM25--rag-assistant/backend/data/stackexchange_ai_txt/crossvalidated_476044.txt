[site]: crossvalidated
[post_id]: 476044
[parent_id]: 476035
[tags]: 
Of course this depends on the bandwidth and kernel you choose. To fix things, we take the rectangular Kernel (uniform on $[-1,1]$ ). Despite the kink, your function is quite nice; it's just the absolute value function which is $1$ -Lipschitz. The Nadaraya-Watson kernel nicely adapts to precisely this type of smoothness (while it does not do as well with higher order smoothness; in that case you should use higher order local linear regression). Let us conduct a quick study of the uniform Kernel Nadaraya Watson estimator at $0$ with bandwidth $h$ : $\hat{g}(0)$ is just the average of the $Y_i$ 's corresponding to $X_i$ 's that lie in the interval $[-h,h]$ . Notice that for $X_i \in [-h,h]$ we have that: $\lvert\mathbb E[Y_i \mid X_i] - g(0)\rvert \leq h$ and so the bias of $\hat{g}(0)$ is of order $O(h)$ . On the other hand, we will be averaging approximately $O(n \cdot h)$ points and so if $\text{Var}[Y_i \mid X_i]$ is bounded, we have that $\text{Var}(\hat{g}(0)) = O(1/(n \cdot h))$ . So the mean squared error (MSE) is $O(h^2) + O(1/(n \cdot h))$ . Thus as long as we choose $h$ such that $h \to 0$ and $n \cdot h \to \infty$ , the estimator will be consistent. This answers your first question. Optimizing over bandwidth will yield $h = n^{-1/3}$ and so the MSE will be of order $O(n^{-2/3})$ . What happens away from $0$ ? If you use the same bandwidth as you did at $0$ , then the rate will be the same. However you can repeat the above argument, with a more careful decomposition for the Bias, to get that the bias is of order $h^2$ (if you make some additional smoothness assumption on $p$ ). So away from $0$ , you can pick a larger bandwidth, leading to a MSE decaying as $O(n^{-4/5})$ .
