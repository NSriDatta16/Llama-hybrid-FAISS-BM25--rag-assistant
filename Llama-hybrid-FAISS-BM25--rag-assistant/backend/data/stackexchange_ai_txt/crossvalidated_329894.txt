[site]: crossvalidated
[post_id]: 329894
[parent_id]: 
[tags]: 
Regularization on weights without bias

I was learning neural network using the book "Deep Learning" by Ian Goodfellow, Yoshua Bengio and Aaron Courville. In section 7.1, it says: ...we typically choose to use a parameter norm penalty Ω that penalizes only the weights of the affine transformation at each layer and leaves the biases unregularized. The biases typically require less data to fit accurately than the weights. Each weight specifies how two variables interact. Fitting the weight well requires observing both variables in a variety of conditions. Each bias controls only a single variable... I don't understand why it says Each weight speciﬁes how two variables interact Could someone please explain it please? It would be perfect with some examples.
