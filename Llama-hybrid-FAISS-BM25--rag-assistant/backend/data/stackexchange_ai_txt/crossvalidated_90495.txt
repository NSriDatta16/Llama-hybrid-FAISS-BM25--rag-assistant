[site]: crossvalidated
[post_id]: 90495
[parent_id]: 90452
[tags]: 
The $V(s)$'s are sufficient to determine a policy precisely because you have access to the model. In particular you have access to information about the transition structure of the MDP (you know how to look ahead). Without a model you might know you want to get to $s$, but you don't know what actions to take to get there. Q values eliminate the need to estimate the underlying model by just learning what actions are good to take in a state. This is discussed in the survey by Szepesvari on page 14 where it is noted that knowing either $Q^*$ or $V^*$, $P$ and $r$ suffice for an RL agent to act optimally. Using their notation, $P$ and $r$ are essentially the model.
