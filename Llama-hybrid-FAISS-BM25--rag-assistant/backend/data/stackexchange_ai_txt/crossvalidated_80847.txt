[site]: crossvalidated
[post_id]: 80847
[parent_id]: 80824
[tags]: 
The best way is to find out why there is a multicollinearity in your explanatory variables and remove it! Maybe looking at the pairwise correlation among your explanatory variables will give you an idea. In addition to this, there are some other ways for dealing with multicollinearity including: "Do nothing"! Believe it or not this is one simple way but most of the times it is not recommended. Dropping some of the variables is another solution, but it is like putting your head in the sand! Using an unweighted or weighted average of two variables that are highly correlated, but only if it seems logical. However, this only works when the two variables are actually a replications of one variable. Centering the variables by subtracting each explanatory variable from their means. You can use a little bit more advanced method to remove multicollinearity such as Principal Component Analysis . Here, you create a new set of variable (that are a linear combination of your explanatory variables) and use this new set of variables for your modeling. These new set of variables will be constructed in such a way that they are uncorrelated and therefore there will be no multicollinearity. In other words, you are re-defining your explanatory variables, by using some statistical analysis. See e.g. page 192 of Regression Analysis by Rudolf J. Freund, William J. Wilson. Sometimes, you have some prior knowledge about your explanatory variables and you will use this knowledge to re-define some new variables even without applying the Principal Component Analysis. For example, you may decide to use $X_i/X_1$ or $X_i-X_1$, of course if they have some meaning when modeling. If your explanatory variables are like polynomials, then you can use orthogonal polynomials (such as Legendre polynomials ) as well to reduce multicollinearity.
