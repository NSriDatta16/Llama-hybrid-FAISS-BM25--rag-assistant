[site]: crossvalidated
[post_id]: 538725
[parent_id]: 
[tags]: 
The Derivation of Gaussian Process Posterior

Denote the training data and training target as $X, y$ , respectively, now we aim at applying the Gaussian process (GP) for regression. Here, we assume the prior is a GP: $$ p(f(X)) = \mathcal{GP}(m(X), K(X,X)), $$ where $m(\cdot)$ is the mean function and $K(\cdot, \cdot)$ is the covariance matrix with $K$ denoting kernel function. The likelihood is defined as $$ p(y | f, X) = \mathcal{N}(y | f(X), \sigma^2 I). $$ Then, based on the conjugate property of GP prior, the posterior can be derived as $$ p(f(X_{*}) | y, X) = \mathcal{GP}(m_{post}(X_* ), K_{post}(X_*, X_*)), $$ $$ m_{post}(X_*) = m(X_*) + K(X_*, X)(K(X,X)+\sigma^2 I)^{-1}(y - m(X)), $$ $$ K_{post}(X_*, X_*) = K(X_*, K_*) - K(X_*, X)(K(X,X)+\sigma^2 I)^{-1}K(X, X_*). $$ This is the final expression of GP regression (we call it method 1 ). Alternatively, we can arrive at the same result in another way (we call it method 2 ), as shown in the following: We define the joint probability of the function of observed&unobserved data as a GP $$ p(f, f_* | X, X_*) = \mathcal{N}\left( \left[\begin{array}{c} m(X)\\ m(X_*) \end{array}\right], \left[\begin{array}{cc} K(X,X) & K(K, K_* )\\ K(X_*, X) & K(X_*, X_*) \end{array}\right]\right), $$ where $X_*$ denotes the unobserved data. Then the conditional probability of $f_*$ is given by $$ p(f_* | f, X, X_*) = \mathcal{N}( \mathbb{E}[f_* | f, X, X_*], \mathbb{V}[f_* | f, X, X_*]), $$ $$ \mathbb{E}[f_* | f, X, X_*] = m(X_*) + K(X_*, X)(K(X,X)+\sigma^2 I)^{-1}(y - m(X)), $$ $$ \mathbb{V}[f_* | f, X, X_*] = K(X_*, K_*) - K(X_*, X)(K(X,X)+\sigma^2 I)^{-1}K(X, X_*). $$ Obviously, the results of method 1&2 are the same. I can understand method 1 because it is a formal Bayesian perspective. But what is the intuition behind method 2? It seems quite simple than method 1, but it gives the same result. Why does it work? I mean simply given a prior of the joint distribution, and the answer automatedly arises in the posterior. Could anybody give me an intuitive explanation? PS: I follow the notation of GP in the lecture of Marc Deisenroth. The GP formula in this question is at 1:07:07 of this video , in which you can fully understand the derivation of these two methods.
