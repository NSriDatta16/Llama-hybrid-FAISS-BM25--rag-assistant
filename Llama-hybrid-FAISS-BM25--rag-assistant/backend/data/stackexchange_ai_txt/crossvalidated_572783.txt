[site]: crossvalidated
[post_id]: 572783
[parent_id]: 
[tags]: 
Curse of dimensionality: How PCA improves my model?

After having read about the curse of dimensionality, I have been looking into a filtered version of the Superconductivity dataset . The number of dimensions is 81, so initially I thought that reducing the dimensions using PCA could help to improve the prediction power. (The majority of the articles I have read about the curse of dimensionality speak about dimensions greater than 10) Surprisingly, I found the opposite: # Using caret library and a loop to iterate # over the number principal components to use: for (pca_component_i in seq(5, 80, by = 5)) { fitControl On the other hand, it also looks pretty obvious behavior cause more principal components explain more variance. So, What am I misunderstanding? Is there any possible situation where reducing the dimensionality of the data lead to a increase of the prediction power?
