[site]: datascience
[post_id]: 36782
[parent_id]: 36780
[tags]: 
Yes, essentially a typical CNN consists of two parts: The convolution and pooling layers, whose goals are to extract features from the images. These are the first layers in the network. The final layer(s), which are usually Fully Connected NNs, whose goal is to classify those features. The latter do have a typical equation (i.e. $f(W^T \cdot X + b)$), where $f$ is an activation function. Usually in the context of CNNs, $f$ is a ReLU, except for the activation function of the final layer, which is selected according to the nature of the problem. The most common cases are: Sigmoid activation functions work for binary classification problems. Softmax activation functions work practically for both binary and multi-class classification problem. For regression problems, the final layer has no activation. One final note I'd like to make is that before entering the first FC layer, the output of the previous layer is flattened . By this I mean that the (typically 3) dimensions of that tensor are layed out into one large dimension. For example a tensor with a shape of $(5, 5, 32)$, when flattened would become $(5 \cdot 5 \cdot 32) = (800)$.
