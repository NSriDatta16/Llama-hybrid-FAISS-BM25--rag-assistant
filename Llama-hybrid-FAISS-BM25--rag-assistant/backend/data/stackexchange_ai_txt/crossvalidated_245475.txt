[site]: crossvalidated
[post_id]: 245475
[parent_id]: 
[tags]: 
Form of posterior when mean and variance are unknown

In "A First Course in Bayesian Methods", Hoff writes that: $p(\theta, \sigma^2|y_1,...y_n) = p(\theta|\sigma^2,y_1,...y_n)p(\sigma^2|y_1,...y_n)$ when describing joint inference for the mean and variance for normal data. How do we arrive at this starting from $p(A|B) = \frac{p(B|A)P(A)}{P(B)}$?
