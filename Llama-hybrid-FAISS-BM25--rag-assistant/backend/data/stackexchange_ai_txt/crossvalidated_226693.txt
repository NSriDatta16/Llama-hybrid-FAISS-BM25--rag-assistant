[site]: crossvalidated
[post_id]: 226693
[parent_id]: 226672
[tags]: 
It really depends on your dataset, and network architecture. One rule of thumb I have read (2) was a few thousand samples per class for the neural network to start to perform very well. In practice, people try and see. It's not rare to find studies showing decent results with a training set smaller than 1000 samples. A good way to roughly assess to what extent it could be beneficial to have more training samples is to plot the performance of the neural network based against the size of the training set, e.g. from (1): (1) Dernoncourt, Franck, Ji Young Lee, Ozlem Uzuner, and Peter Szolovits. " De-identification of Patient Notes with Recurrent Neural Networks " arXiv preprint arXiv:1606.03475 (2016). (2) Cireşan, Dan C., Ueli Meier, and Jürgen Schmidhuber. "Transfer learning for Latin and Chinese characters with deep neural networks." In The 2012 International Joint Conference on Neural Networks (IJCNN), pp. 1-6. IEEE, 2012. https://scholar.google.com/scholar?cluster=7452424507909578812&hl=en&as_sdt=0,22 ; http://people.idsia.ch/~ciresan/data/ijcnn2012_v9.pdf : For classification tasks with a few thousand samples per class , the benefit of (unsupervised or supervised) pretraining is not easy to demonstrate.
