[site]: datascience
[post_id]: 70038
[parent_id]: 
[tags]: 
Sudden drop of score in the last few episodes

I was following this tutorial about lunar lander and deep Q learning with Tensorflow 2 and I noticed something odd. The problem was actually solved at episode 476 but then the score went from 259.90 to -520.33 and stayed negative until the last episode 500. episode: 468 score 248.95 average score 188.47 epsilon 0.01 episode: 469 score 269.47 average score 192.11 epsilon 0.01 episode: 470 score 225.31 average score 195.72 epsilon 0.01 episode: 471 score 46.36 average score 194.66 epsilon 0.01 episode: 472 score 250.61 average score 198.44 epsilon 0.01 episode: 473 score 237.25 average score 198.50 epsilon 0.01 episode: 474 score 237.02 average score 202.16 epsilon 0.01 episode: 475 score 249.85 average score 204.11 epsilon 0.01 episode: 476 score 259.90 average score 204.15 epsilon 0.01 episode: 477 score -520.33 average score 198.10 epsilon 0.01 episode: 478 score -567.86 average score 190.58 epsilon 0.01 episode: 479 score -670.45 average score 185.15 epsilon 0.01 episode: 480 score -413.31 average score 178.85 epsilon 0.01 episode: 481 score -995.13 average score 166.78 epsilon 0.01 episode: 482 score -450.73 average score 159.52 epsilon 0.01 episode: 483 score -583.07 average score 151.16 epsilon 0.01 episode: 484 score -586.59 average score 143.61 epsilon 0.01 episode: 485 score -436.80 average score 136.45 epsilon 0.01 episode: 486 score -665.69 average score 127.73 epsilon 0.01 episode: 487 score -602.49 average score 119.65 epsilon 0.01 episode: 488 score -1685.73 average score 99.95 epsilon 0.01 episode: 489 score -689.85 average score 91.23 epsilon 0.01 episode: 490 score -501.80 average score 83.63 epsilon 0.01 episode: 491 score -1016.14 average score 74.79 epsilon 0.01 episode: 492 score -475.65 average score 67.81 epsilon 0.01 episode: 493 score -525.98 average score 59.80 epsilon 0.01 episode: 494 score -393.08 average score 53.57 epsilon 0.01 episode: 495 score -557.32 average score 46.85 epsilon 0.01 episode: 496 score -431.39 average score 39.88 epsilon 0.01 episode: 497 score -944.19 average score 27.72 epsilon 0.01 episode: 498 score -571.92 average score 19.96 epsilon 0.01 episode: 499 score -498.09 average score 14.63 epsilon 0.01 What happened? Is it overfitting? Is it normal? Does it mean that we have to run fewer episodes or run it again? I'm confused about how to test different models and hyperparameters. Here's the full code import gym import numpy as np import tensorflow as tf from tensorflow import keras from tensorflow.keras.optimizers import Adam from tensorflow.keras.models import load_model class ReplayBuffer(): def __init__(self, memory_size, input_dims): self.memory_size = memory_size self.memory_counter = 0 self.state_memory = np.zeros((self.memory_size, *input_dims), dtype=np.float32) self.new_state_memory = np.zeros((self.memory_size, *input_dims), dtype=np.float32) self.action_memory = np.zeros(self.memory_size, dtype=np.int32) self.reward_memory = np.zeros(self.memory_size, dtype=np.float32) self.terminal_memory = np.zeros(self.memory_size, dtype=np.int32) def store_transition(self, state, action, reward, new_state, done): index = self.memory_counter % self.memory_size self.state_memory[index] = state self.new_state_memory[index] = new_state self.reward_memory[index] = reward self.action_memory[index] = action self.terminal_memory[index] = 1 - int(done) self.memory_counter += 1 def sample_buffer(self, batch_size): max_memory = min(self.memory_counter, self.memory_size) batch = np.random.choice(max_memory, batch_size, replace=False) states = self.state_memory[batch] new_state = self.new_state_memory[batch] rewards = self.reward_memory[batch] actions = self.action_memory[batch] terminal = self.terminal_memory[batch] return states, actions, new_state, rewards, terminal def build_dqn(learning_rate, number_actions, input_dims, fully_connected1_dims, fully_connected2_dims): model = keras.Sequential([ keras.layers.Dense(fully_connected1_dims, activation='relu'), keras.layers.Dense(fully_connected2_dims, activation='relu'), keras.layers.Dense(number_actions, activation=None) ]) model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error') return model class Agent(): def __init__(self, learning_rate, gamma, number_actions, epsilon, batch_size, input_dims, epsilon_dec=1e-3, epsilon_end=0.01, memory_size=1000000, file_name='dn_model.h5'): self.action_space = [i for i in range(number_actions)] self.gamma = gamma self.epsilon = epsilon self.epsilon_dec = epsilon_dec self.epsilon_minimum = epsilon_end self.batch_size = batch_size self.model_file = file_name self.memory = ReplayBuffer(memory_size, input_dims) self.q_eval = build_dqn(learning_rate, number_actions, input_dims, 256, 256) def store_transition(self, state, action, reward, new_state, done): self.memory.store_transition(state, action, reward, new_state, done) def choose_action(self, observation): if np.random.random() self.epsilon_minimum: self.epsilon = self.epsilon - self.epsilon_dec else: self.epsilon = self.epsilon_minimum def save_model(self): self.q_eval.save(self.model_file) def load_model(self): self.q_eval = load_model(self.model_file) if __name__ == '__main__': tf.compat.v1.disable_eager_execution() environment = gym.make('LunarLander-v2') learning_rate = 0.001 number_games = 500 agent = Agent(gamma=0.99, epsilon=1.0, learning_rate=learning_rate, input_dims=environment.observation_space.shape, number_actions=environment.action_space.n, memory_size=1000000, batch_size=64, epsilon_end=0.01) scores = [] epsilon_history = [] for i in range(number_games): done = False score = 0 observation = environment.reset() while not done: action = agent.choose_action(observation) new_observation, reward, done, info = environment.step(action) score += reward agent.store_transition(observation, action, reward, new_observation, done) observation = new_observation agent.learn() if i % 100 == 0: environment.render() epsilon_history.append(agent.epsilon) scores.append(score) average_score = np.mean(scores[-100:]) print('episode: ', i, ' score %.2f' % score, 'average score %.2f' % average_score, 'epsilon %.2f' % agent.epsilon)
