[site]: datascience
[post_id]: 112772
[parent_id]: 
[tags]: 
Do best hyperparameters remain constant when data size is scaled?

Basically what the title is. The problem I currently have is that my dataset consists of 2.8 billion rows, and I have it as a Pyspark data frame. I want to use some library such as FLAML for finding hyperparameters, but I cannot do it with a Pyspark data frame. The solution I’ve thought of currently is: Keep the overall distribution of the data and scale it down to 60 or 70 million rows so it’s usable as a Pandas data frame Use FLAML to find hyperparameters for this dataset on some model, say XGBoost Now my question is if these hyperparameters will work if I trained the model with 2.8 billion rows and the same hyperparameters I found from tuning. It would be great if someone can additionally point to any research done in this regard.
