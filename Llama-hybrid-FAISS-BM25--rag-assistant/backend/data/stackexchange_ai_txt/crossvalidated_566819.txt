[site]: crossvalidated
[post_id]: 566819
[parent_id]: 
[tags]: 
Variable Length Input: How should variable-length input data be handled during the testing stage?

I have data that is sequential. Here, I am showing a toy example of my data in the following image: I need to input the data into the model as groups of samples based on the class duration. To accomplish this, I determined the class's beginning and ending samples. For instance: I feed the data sample segment from S1 to S100 with the label "Class A." Then, for class B, we feed only nine samples, and so on. The following points stand out from the data: The model's input would be of variable length. Some classes may have very few samples, while others have many, such as the third segment (Class A), which has nearly 3000 samples. Questions: How can we better feed variable length input to deep learning models? It is simple to identify the beginning and endpoints of specific class samples during training. However, how do we feed this type of data into testing? More specifically, how should we determine the start and end times of the class samples at the testing time? Which deep learning method, such as CNN, LSTM, or a combination of these methods, performs better on this type of data? As can be seen from the data, some segments are excessively large, while others are excessively small. I am curious how the GPU will manage memory allocation for these large segment samples?
