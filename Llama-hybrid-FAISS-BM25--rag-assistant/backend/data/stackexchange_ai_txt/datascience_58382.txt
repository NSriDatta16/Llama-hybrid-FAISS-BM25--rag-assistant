[site]: datascience
[post_id]: 58382
[parent_id]: 58380
[tags]: 
from keras.preprocessing import Tokenizer samples = ["grss is green and sun is hot"] tokenizer = Tokenizer(num_words=1000) tokenizer.fit_on_texts(samples) sequences = tokenizer.texts_to_sequences(samples) The Keras library uses it's tokenizer function but you have other well known libraries like nltk, gensim to convert them into sequences and pass it into your neural network. There are other ways like TF/IDF and CountVectorizer in Sklearn for more simpler algorithms. num_words takes the most 1000 frequent words and tokenizes them. Link : Keras text processing
