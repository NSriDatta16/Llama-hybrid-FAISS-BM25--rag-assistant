[site]: datascience
[post_id]: 95173
[parent_id]: 80301
[tags]: 
Is that, we have to crop all the objects in every image and do binary classification as object vs background for classifying the anchor has object or not The RPN gets the input from backbone network(VGG, Resnet etc.) as feature maps. Here the RPN itself a CNN layer so it will handle different shaped anchors to FC layer. For the loss calculation, each feature map points will mapped to original image point (reverse scaling) and calculate the IOU score for loss function. Possible the scaling will be straight forward based on the stripes and padding. From these the RPN will learn to choose the bboxes. In Faster RCNN there are different implementation methods between RPN and object classification layers. If you are familiar with TF, you can refer this implementation. https://github.com/tensorflow/models/blob/master/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
