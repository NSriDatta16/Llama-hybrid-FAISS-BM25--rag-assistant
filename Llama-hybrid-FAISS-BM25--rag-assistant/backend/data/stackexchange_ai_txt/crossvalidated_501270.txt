[site]: crossvalidated
[post_id]: 501270
[parent_id]: 
[tags]: 
T-test, Chi-squared test and hypothesis testing?

I am not an expert in the field of statistics and my questions might be stupid, please be indulgent :) I've come across many videos/articles about T-test that suggest sampling data when their dataset is not big at all. Now, when it comes to collecting data, or dealing with an extremely huge dataset, I get it. However if my dataset is 50000 or even 500000 rows, with today's computational power, what's the point ? If the data is already collected and ready to use , why not use it ? I have also read someone mention that if I use my whole dataset, then I can just compare the populations means and don't really need to do a T-test. My case : I want to know whether or not the age difference between a customer and a call center agent could affect sales performance. My data is basically 2 columns : age_difference and sale_made( yes or not ). I then divided it in 2, one dataset being the sales made, the other one being the sales not made. I want to do a T test to know if the distributions are different. My questions : Why would I just compare just the populations means without any test when using the whole dataset ? I indeed have a small difference in the means of the 2 populations, but how can I be sure that this is not just random/luck if I don't do any test that will give me a p-value ? Shouldn't a p-value be always welcome to draw some conclusions ? Why would I sample my data ? (for sample size 50, I don't reject H0 -> same distribution, for sample size 90, I dont reject H0 -> same distribution, for sample size 100 and above, I reject H0 -> different distributions ). Sample size clearly have an impact on my conclusion and more data gives more evidence that the distributions are different. Now about Chi-squared test, in the same project, I am trying to know whether the sex of the call center agent and the customer could affect the sales. I am doing a Pearson CHi squared test with scipy chi2_contingency function. I have read so many times that one should choose the significance level before doing anything else. So I decided to choose the usual alpha = 0.05, I have 1 degree of freedom, I can read in the Chi squared distribution table the corresponding threshold (3.84). My test gives a chi_squared test statistic = 3.67 so I don't reject H0. Now it may be a stupid question, but why does changing the significance level after a first test seems to be such a big deal ? if I choose alpha = 0.1, the corresponding threshold in the table = 2.71 and I now reject H0. Why couldnt I use this as an insight ? i mean, 10% of error can still be acceptable in many cases. Someone could argue that I should have chosen this significance level from the start if it's enough for my work, and that's right. But people can make mistake and do better the next time ! It's just trial and error ... After all the point is just to get useful insights from the data. So again, why does it seem to be such a big deal and such a big mistake to change the significance level ? Thank you for reading this, I hope that I didn't sound too ignorant and that someone will be able to give me some insights.
