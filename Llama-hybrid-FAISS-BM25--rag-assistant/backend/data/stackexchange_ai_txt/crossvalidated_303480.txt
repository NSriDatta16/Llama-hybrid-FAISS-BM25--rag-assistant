[site]: crossvalidated
[post_id]: 303480
[parent_id]: 
[tags]: 
Bayesian evidence

I am currently trying to solve an exercise (Information Theory, Inference, and Learning Algorithms, by David J.C. MacKay, ch 28 exercise 28.1) The exercise is as follows: Random variables $x$ come independently from a probability distribution $P(x)$. According to model $H_{0}$, $P(x)$ is a uniform distribution. \begin{equation} P(x|H_0) = \frac{1}{2} \qquad x \in (-1,1). \end{equation} According to model $H_{1}$, $P(x)$ is a nonuniform distribution with an unknown parameter $m \in$ $(-1,1)$: \begin{equation} P(x|m,H_{1}) = \frac{1}{2}(1+mx) \qquad x\in(-1,1). \end{equation} Given the data $D = \{0.3, 0.5, 0.7, 0.8, 0.9\}$, what is the evidence for $H_{0}$ and $H_{1}$? The evidence $P(D|H_{0})$ is as follows: Edit 1 (thanks to Siong Thye Goh): Let $D = \{d_{i}|i = 1,\dots,5\}$. Since we assume independence, we have \begin{equation} P(D|H_{0}) = \prod_{i=1}^{5}P(d_{i}|H_{0}) = \frac{1}{2}^{5}. \end{equation} Question 2: However, now I am stuck with $P(D|H_{1})$. \begin{equation} P(D|H_{1}) = \int_{-1}^{1}P(D|H_{1},m)\cdot P(m|H_{1})dm. \end{equation} In the book, it is stated that the evidence can be approximated by: \begin{equation} P(D|H_{1}) \approx P(D|m_{mp},H_{1})\cdot P(m_{mp}|H_{1})\sigma_{m|D}. \end{equation} Here, $m_{mp}$ defines the most probable value for the parameter $m$. $\sigma_{m|d}$ defines the posterior uncertainty in $m$, as a part of an occam factor. $P(m_{mp}|H_{1})$ = $\frac{1}{\sigma_m}$, represents the range of values of $m$ that were possible a priori. I'm guessing here that $\frac{1}{\sigma_{m}} = \frac{1}{2}$ since $m \in (-1,1)$ and that $P(d_{i}|m_{mp},H_{1}) = \frac{1}{2}(1+m_{mp}d_{i})$, so that the approximation for the evidence becomes \begin{equation} P(d_{i}|H_{1}) \approx \frac{1}{2}(1+m_{mp}d_{i})\cdot \frac{\sigma_{m|D}}{2}. \end{equation} But now: what would $\sigma_{m|D}$ be in this case (if this even is the right approach)? How do we evaluate what $m_{mp}$ is? Can this just be done via maximum likelihood, as we do in classical statistics?
