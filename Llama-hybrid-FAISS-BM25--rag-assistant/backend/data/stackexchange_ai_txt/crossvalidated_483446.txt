[site]: crossvalidated
[post_id]: 483446
[parent_id]: 482818
[tags]: 
In this particular case the strong association of your continuous predictor X with the categorical predictor Y is probably why you found this behavior. Y seems very strongly associated with outcome, based on the single-predictor and additive 2-predictor models. You can think about that additive model as showing the association of Y with outcome while also accounting for X . As you might often see in such situations, Y was still "significant" but its hazard ratio wasn't quite as large as it was when you didn't take X into account. The single-predictor association of X with outcome might simply have represented its association with Y, which wasn't taken into account in the model including only X as a predictor. On the other hand, X might actually be associated with outcome, consistent with its positive hazard ratio point estimate in the additive model, but with this amount of data the error in that point estimate is high so you can't reliably make that statement for sure. When you add an interaction you are adding a third predictor to the model. A rule of thumb for properly fitting a Cox survival model is to have about 15 events per predictor that you are considering in the model. So with 30 events you are pushing past that limit when you try to go beyond 2 predictors. You will see this type of thing often when you try to push a regression model beyond the available data: effects that are "significant" in an additive model disappear when you try to add an interaction. You shouldn't try to get around this problem by including the interaction without the main effects of X and Y, as such practice is generally not a good idea . What you saw here could also be seen in ordinary least squares: an apparent association of a predictor with outcome that might primarily be due to its association with a better predictor that was left out of the model. That's one example of what's called omitted-variable bias . Cox models can even show bias if an omitted predictor isn't associated directly with the one you're evaluating, just with outcome. That tends to have the opposite effect from what you saw: you underestimate the magnitude of the coefficient for the predictor you're evaluating, like the omitted-variable bias in logistic regression . So as you continue with this type of work it's generally best to use your knowledge of the subject matter to start modeling with as many predictors as are compatible with the number of events you have. If you are willing to use penalization methods like ridge regression, elastic net, or LASSO, you could include even more predictors.
