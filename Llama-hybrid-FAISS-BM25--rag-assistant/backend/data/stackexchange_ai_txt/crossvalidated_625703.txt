[site]: crossvalidated
[post_id]: 625703
[parent_id]: 625186
[tags]: 
In addition to Dave's excellent answer (+1), I'd like to point out that most (training) datasets can be perfectly classified by learning them by heart (exception to this rule is when points from different classes perfectly overlap, having exactly the same features, in which case they are inherently inseparable). You can take a nearest neighbour classifier or an SVM with the RBF kernel and a high $\gamma$ and achieve perfect accuracy, precision, ... whatever on the training set. Of course, this would be overfitting and the performance on unseen data is likely to be miserable. On the other hand, we need to define "miserable". If accuracy >= 95% is what you're striving for, your job is already done: Classify everything as the larger class and you'll get 96%! So I suggest that you define a cost function to quantify how expensive each kind of classification error is. From there, I'd proceed with $k$ -nearest neighbours and validation: split your data into (at least) a training and a validation set; use increasing $k$ for the nearest neighbours, train them on the training set and measure the above defined cost on the validation set. If the cost remains mostly flat as you vary $k$ , your features probably don't contain sufficient information for classification. Alternatively, you could use an RBF SVM with a decreasing $\gamma$ , but for your dataset (> 100,000 points) it would likely only be a waste of computational resources, without any real benefit over the nearest neighbours. This is, of course, a very qualitative approach, open to interpretation. Unfortunately, I am not aware of a statistical test suitable for your purpose. Regarding clustering, I'm skeptical: If your classes formed clearly distinguishable clusters, classification would most likely be easy (this is not a law of nature, but mostly holds in practice).
