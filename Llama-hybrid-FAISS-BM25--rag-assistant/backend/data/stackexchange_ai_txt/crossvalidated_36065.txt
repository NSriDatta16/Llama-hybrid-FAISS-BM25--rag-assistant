[site]: crossvalidated
[post_id]: 36065
[parent_id]: 36006
[tags]: 
The term is quite old in artifical intelligence. Turing devoted a long section on "Learning Machines" in his 1950 Computing Machinery and Intelligence paper in Mind , and qualitatively sketches out supervised learning. Rosenblatt's original paper: The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain paper from 1958 talks extensively about a "Mathematical Model of Learning". Here the perceptron was a "model of learning"; models were not "learned". The Pitts and McCullough 1943 paper - the original "neural networks" paper - wasn't really concerned with learning, more how one could construct a logical calculus (like a Hilbert or Gentzen system, but I think they refer to Russell/Whitehead) which could perform inference. I think it was the "Perceptrons" paper which introduced a numerical, as opposed to symbolic notion of learning in this tradition. Is it possible for a machine to learn how to play chess just from examples? Yes. Does it have a model for chess playing? Yes. Is it the optimal model (assuming there is one)? Almost certainly not. In plain English I've "learned chess" if I can play chess ok - or maybe pretty well. It doesn't mean I'm the optimal chess player. This is the sense in which Turing was describing "learning" when he discussed learning chess in his paper. I'm very inconsistent with what term I use. So (for instance) for learning-in-the-limit I'd say "identify", for SVM-learning I'd say "train", but for MCMC-"learning" I'd say "optimize". And e.g. I just call regression "regression".
