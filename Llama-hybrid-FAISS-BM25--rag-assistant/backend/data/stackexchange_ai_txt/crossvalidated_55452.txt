[site]: crossvalidated
[post_id]: 55452
[parent_id]: 
[tags]: 
SVM model selection for datasets with sharp corners

I'm working with an artificially generated dataset that is separated by many sharp corners. As an example, imagine an H-shape in a 3D (or higher-dimensional) space. Points within the H are positive, points outside are negative. In my application, many new points will be generated at runtime. I would like to classify them as being inside the H-shape or not. I currently do an exact check for these points using standard collision detection methods, but since this is proving expensive I would like to do much faster classification at the expense of some accuracy. For instance, 90-95% correct classifications at an order of magnitude or more faster speed than exact checking is ideal for me. I'm using an SVM with the radial basis function (RBF) kernel since my dataset is not linearly separable in the original space. I've trained it using grid-search on the parameters gamma and nu, with 3-fold cross validation, varying both the range and granularity of the search. However, the best I can do is about 80-85% accuracy, and a running time of 6-10s for 100,000 new datapoints, which is about the same time as it takes to do the exact check. Where am I going wrong? I particularly suspect that the RBF is poor at handling sharp corners (e.g. the H-shape that I described, whose boundary separates positive and negative points). Should I be using a different kernel that is more suited to these parameters? Should I use some combination of kernels? Should I do some form of region decomposition, and train separate kernels for each region? Should I be using a different ML approach (e.g. neural networks) instead?
