[site]: crossvalidated
[post_id]: 416912
[parent_id]: 
[tags]: 
Is kernalized linear regression parametric or nonparametric?

We know that for linear regression, we can predict: $$\hat{y} = w^Tx +b$$ Where $w$ is the parameter that minimizes the square loss. It is easy to prove that for the final solution using gradient descent: $$w = \sum_i^n \alpha_i x_i$$ Substituting $w$ with the right hand side above gives rise to the kernalized version of linear regression. If we have $d >> n$ where d is the number of features, kernalizing might be a good idea when the kernel function we are using is easy to compute. Let's assume we are using a kernalized linear regression with a linear kernel. At the end we will get $w$ as a linear combination of all our datapoints. This shows that the model is non-parametric. However as $n > d$ , we know that we can have at most d linearly independent vectors among the n vectors. So if we have $\alpha_i > 0$ for more than $d$ points, we know that there is another solution with only d such $\alpha$ 's greater than 0. Given that any such solution is equivalent to the solution of having a fixed length parameter vector $w$ (in case w is finite dimensional), then it seems kernalized linear regression can be both parametric and non-parametric. Is this true? (another way to look at it is that the final solution has more than d vectors, but we are sure that such solution is inherently at most d dimensional) Also, are there techniques to limit the number of $\alpha_i >0$ to exactly $d$ points or at least ensure that the vectors we get are linearly independent (thus we can have less than d such vectors if the dataset is low dimensional)? This is also related to SVMs in some sense. There the number of support vectors ( $\alpha_i >0$ ) can vary according to how complex the model is and depending on the parameters, however in kernalized linear regression the number of vectors used to calculate $w$ doesn't say anything about the model complexity.
