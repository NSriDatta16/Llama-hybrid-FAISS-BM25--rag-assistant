[site]: crossvalidated
[post_id]: 643823
[parent_id]: 
[tags]: 
Missing features in decision tree based algorithms

I have a medium-sized dataset consisting of many features, some of which can contain missing values. I want to predict a variable using an algorithm that employs decision trees (specifically XGBoost, but that should not matter for my question). My understanding is that the usual way that these algorithms deal with missing data for features is to assign missing values to one of the leaves at each split. If the fact that data is missing is uncorrelated to its value, this should make predictions worse because you end up putting data points into the wrong leaves compared to where they should be if you had the data. I understand that if you want to use all your features in every tree, you have to take a stance on which leaves missing datapoints should go to. However, if you use algorithms that build many different decision trees and have lots of features, then each tree will only use a subset of features because you limit the depths of the tree. So, to me, the best way to handle missing data if you want your final predictions to be based on, say, 100 trees would not be to build 100 trees and handle missing features in each tree but to always drop data points with missing features when making a tree and then just make many more trees so that in the prediction stage you will still have around 100 trees available for each new datapoint you want to predict. I realize that this is a lot more computationally demanding than building trees that include missing values in the splits, but at least for my use case, I think I could afford to do it. I am relatively new to machine learning but I have never seen this method of dealing with missing data discussed. Is there a problem with this procedure that I am missing?
