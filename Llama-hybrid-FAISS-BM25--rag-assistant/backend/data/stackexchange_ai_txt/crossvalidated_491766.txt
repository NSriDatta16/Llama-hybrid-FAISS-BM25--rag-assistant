[site]: crossvalidated
[post_id]: 491766
[parent_id]: 491501
[tags]: 
First of all we have to say that bias-variance tradeoff (BVT) can be seen in respect not only of parameters estimators but also about prediction. Usually BVT is used in machine learning on prediction side and more precisely about the minimization of Expected Prediction Error (EPE). In this last sense the BVT was treated and derived in the discussion that you linked above. Now you says: Who's to say that if you have an estimator $\hat{f}$ of $Y = f(X) + \epsilon$ that you couldn't find an estimator $\hat{g}$ that has not only lowers expected squared error, but has lower bias and variance than $\hat{f}$ as well? BVT do not exclude this possibility. Usually in classical statistical or econometrics textbooks the focus is mainly on unbiased estimators (or consistent one, but the difference is not crucial here). So, what BVT tell you is that even if among all unbiased estimators you find the efficient one â€¦ remain possible that some biased ones achieve a lower $MSE$ . I spoke about this possibility here ( Mean squared error of OLS smaller than Ridge? ), even if this answer was not appreciated much. In general, if your goal is prediction, EPE minimization is the core, while in explanatory models the core is bias reduction. In math term you have to minimize two related but different loss functions, the tradeoff come from that. This discussion is about that: What is the relationship between minimizing prediciton error versus parameter estimation error? Moreover what I said above is mainly related on linear models. While It seems me that in machine learning literature the concept BVT, the what that rendered it famous, is primarily related to the interpretability vs flexibility tradeoff. In general, the more flexible models have lower bias but higher variance. For less flexible models the opposite is true (lower variance and higher bias). Among the more flexible alternatives there are Neural Networks, among the less flexible there are linear regressions. Doesn't this depend completely on the expected squared error being constant? No. Among various alternative specifications (flexibility level) the test MSE (=EPE) is far from constant. Depend of the true model (true functional form), and the amount of data we have for training, we can find the flexibility level (specification) that permit us to achieve the EPE minimization. This graph taken from: An Introduction to Statistical Learning with Applications in R - James Witten Hastie Tibshirani (pag 36) gives us three examples. In the par 2.1.3 you can find a more exhaustive explanation of this last point.
