[site]: crossvalidated
[post_id]: 374667
[parent_id]: 
[tags]: 
Neural Networks Mappings( Topology)

Hey I am trying to understand how a neural net performs a kernel trick i.e separate data linearly in high dimensional space. In the example network transforms the input space (2D example) by stretching and squeezing it like: The visualisation video is also shown in https://youtu.be/PFNp8_V_Apg (original blogpost: https://towardsdatascience.com/hyper-parameters-in-action-a524bf5bf1c ). This is of two inputs and two units in one layer and then the output. Different activations will cause different decision boundaries as : I am unable to understand: 1) What does the white space at the inital stages indicate? The network output is defined for all x1,x2 but the manifold only lays in a small region. Shouldnt the start be like the end, covering all space, but only morphing the curve till they are linearly seperable? This can be seen in the above image as well. 2)What do “widening” and “stretching along the axis” mean in the regards to the network learning? I understand the networks objective but if you look at image 3 in the top image here even that can be separated by a Line.(see first video only 30 seconds) The learning of a relu transformation can be visualised as : https://youtu.be/Ji_05nOFLE0 3) Can someone provide intuition as to why this progresses the way it does? Any other intuition for me to understand the way a network learns the new topology would be appreciated.
