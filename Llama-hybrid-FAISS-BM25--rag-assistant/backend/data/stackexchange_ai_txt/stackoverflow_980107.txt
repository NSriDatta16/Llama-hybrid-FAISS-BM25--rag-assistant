[site]: stackoverflow
[post_id]: 980107
[parent_id]: 980090
[tags]: 
The best way to avoid crawlers to index some of your content is by the robots.txt file on the root of your site. Here is an example: User-agent: * Allow: / Crawl-delay: 5 User-agent: * Disallow: /cgi-bin Disallow: /css Disallow: /img Disallow: /js On the first block I'm telling the crawler he can browse all. The second block has the list of folders I want him to avoid. This is not a safe way of really protect it, since some crawlers do not respect it. If you really want to protect it, the best way should be to have a .htaccess file on those folders to force authentication.
