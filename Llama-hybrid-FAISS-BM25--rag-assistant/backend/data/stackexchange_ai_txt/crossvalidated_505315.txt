[site]: crossvalidated
[post_id]: 505315
[parent_id]: 
[tags]: 
Latent Vectors of my VAE contain the value inf and -inf?

I have implemented a VAE and trained it on images. After training and looking at the loss and reconstructions I thought the training was successful. But then I displayed my latent vectors and saw that it contains very big numbers and also the value inf and -inf. Is this normal or did my training go wrong? For the VAE I used the Keras example as a base and changed the layer architecture to my needs. My Implementation: class Sampling(layers.Layer): """Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.""" def call(self, inputs): z_mean, z_log_var = inputs batch = tf.shape(z_mean)[0] dim = tf.shape(z_mean)[1] epsilon = tf.keras.backend.random_normal(shape=(batch, dim)) return z_mean + tf.exp(0.5 * z_log_var) * epsilon latent_dim = 100 #sehr wichtig encoder_inputs = keras.Input(shape=(32, 32, 3)) x = layers.Conv2D(32, 4, strides=2, padding="same")(encoder_inputs) x = layers.LeakyReLU(alpha=0.2)(x) x = layers.Conv2D(32, 3, strides=1, padding="same")(x) x = layers.LeakyReLU(alpha=0.2)(x) x = layers.Conv2D(64, 3,strides=1, padding="same")(x) x = layers.LeakyReLU(alpha=0.2)(x) x = layers.Conv2D(128, 4,strides=2, padding="same")(x) x = layers.LeakyReLU(alpha=0.2)(x) x = layers.Conv2D(64, 3,strides=1, padding="same")(x) x = layers.LeakyReLU(alpha=0.2)(x) x = layers.Conv2D(32, 3,strides=1, padding="same")(x) x = layers.LeakyReLU(alpha=0.2)(x) x = layers.Conv2D(100, 8,strides=1, padding="valid")(x) x = layers.LeakyReLU(alpha=0.2)(x) encoder_test = x x = layers.Flatten()(x) z_mean = layers.Dense(latent_dim, name="z_mean")(x) z_log_var = layers.Dense(latent_dim, name="z_log_var")(x) z = Sampling()([z_mean, z_log_var]) encoder_t1 = keras.Model(encoder_inputs, encoder_test, name="encoder_test") encoder_t2 = keras.Model(encoder_inputs, z_mean, name="encoder_test") encoder_t3 = keras.Model(encoder_inputs, z_log_var, name="encoder_test") encoder_t4 = keras.Model(encoder_inputs, z, name="encoder_test") encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name="encoder") encoder.summary() latent_inputs = keras.Input(shape=(latent_dim,)) x = layers.Reshape((1, 1, latent_dim))(latent_inputs) x = layers.Conv2DTranspose(latent_dim, 8, strides=1, padding="valid")(x) x = layers.LeakyReLU(alpha=0.2)(x) x = layers.Conv2DTranspose(32, 3, strides=1, padding="same")(x) x = layers.LeakyReLU(alpha=0.2)(x) x = layers.Conv2DTranspose(64, 3, strides=1, padding="same")(x) x = layers.LeakyReLU(alpha=0.2)(x) x = layers.Conv2DTranspose(128, 4, strides=2, padding="same")(x) x = layers.LeakyReLU(alpha=0.2)(x) x = layers.Conv2DTranspose(64, 3, strides=1, padding="same")(x) x = layers.LeakyReLU(alpha=0.2)(x) x = layers.Conv2DTranspose(32, 3, strides=1, padding="same")(x) x = layers.LeakyReLU(alpha=0.2)(x) x = layers.Conv2DTranspose(32, 4, strides=2, padding="same")(x) x = layers.LeakyReLU(alpha=0.2)(x) decoder_outputs = layers.Conv2DTranspose(3, 3, activation="sigmoid", padding="same")(x) decoder = keras.Model(latent_inputs, decoder_outputs, name="decoder") decoder.summary() class VAE(keras.Model): def __init__(self, encoder, decoder, encoder_t1, encoder_t2, encoder_t3, encoder_t4, **kwargs): super(VAE, self).__init__(**kwargs) self.encoder = encoder self.decoder = decoder self.encoder_t1 = encoder_t1 self.encoder_t2 = encoder_t2 self.encoder_t3 = encoder_t3 self.encoder_t4 = encoder_t4 self.total_loss_tracker = keras.metrics.Mean(name="total_loss") self.reconstruction_loss_tracker = keras.metrics.Mean( name="reconstruction_loss" ) self.kl_loss_tracker = keras.metrics.Mean(name="kl_loss") def train_step(self, data): if isinstance(data, tuple): data = data[0] with tf.GradientTape() as tape: z_mean, z_log_var, z = encoder(data) reconstruction = decoder(z) reconstruction_loss = tf.reduce_mean( keras.losses.mse(data, reconstruction) #binary_crossentropy ) reconstruction_loss *= 32 * 32 kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var) kl_loss = tf.reduce_mean(kl_loss) kl_loss *= -0.5 total_loss = reconstruction_loss + kl_loss grads = tape.gradient(total_loss, self.trainable_weights) self.optimizer.apply_gradients(zip(grads, self.trainable_weights)) self.total_loss_tracker.update_state(total_loss) self.reconstruction_loss_tracker.update_state(reconstruction_loss) self.kl_loss_tracker.update_state(kl_loss) return { "loss": self.total_loss_tracker.result(), "reconstruction_loss": self.reconstruction_loss_tracker.result(), "kl_loss": self.kl_loss_tracker.result(), } def call(self, inputs): z_mean, z_log_var, z = encoder(inputs) reconstruction = decoder(z) reconstruction_loss = tf.reduce_mean( keras.losses.mse(inputs, reconstruction) ) reconstruction_loss *= 32 * 32 kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var) kl_loss = tf.reduce_mean(kl_loss) #mean kl_loss *= -0.5 total_loss = reconstruction_loss + kl_loss self.add_metric(kl_loss, name='kl_loss', aggregation='mean') self.add_metric(total_loss, name='total_loss', aggregation='mean') self.add_metric(reconstruction_loss, name='reconstruction_loss', aggregation='mean') return reconstruction
