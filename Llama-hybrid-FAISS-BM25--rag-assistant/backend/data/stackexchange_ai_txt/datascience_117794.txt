[site]: datascience
[post_id]: 117794
[parent_id]: 
[tags]: 
Epochs for new batch when online training?

I am online training a RNN with fixed batch size k on a time series. Initially I train my model with n batches and a number of e epochs. When a new batch n+1 is available, I would like to update the weights, rather than retrain the model with all n+1 batches. My question is, should I train the model on the new batch with the same number of epochs e ? Should the number of epochs effectively be 1 ? Is there any way to know what optimal number to choose? Obviously, the more epochs I use on the new batch, the more weight this new batch will have on my predictions. I would like to read more on this, so if anyone has references they are very welcome!
