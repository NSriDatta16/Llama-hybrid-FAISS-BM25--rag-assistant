[site]: crossvalidated
[post_id]: 531792
[parent_id]: 531706
[tags]: 
As you mentioned, auto-encoders can be used for dimensionality reduction. One of the nice things about them, is they are an unsupervised learning method. If you have a large volume of unlabeled data and a small volume of labeled data, you can train your auto-encoder on the large unlabeled dataset to get a robust representation of your data and then either train something lighter weight on your embedings or use transfer learning on your encoder. There are also a number of other applications of auto-encoders though. For example, auto-encoders can be used for anomaly detection. Since the auto-encoder can encode more of the dataset correctly by robustly representing common patterns than unusual ones, the reconstruction error of "normal" data tends to be lower than that of "anomalies". The reconstruction error can then be used to identify anomalous data. Another application for them can be grouping or clustering similar data. Data with similar properties will tend to have similar embedings, so two inputs with embedings that are similar are likely to have similar content. You can then apply traditional clustering methods to the pairwise embeding distances to find structure in your dataset or identify similar instances as in recommender systems. The downside is that the "similarity" between the inputs is not necessarily related to features that are important to a user (images of two clothing items may be similar because they are both red rather than because they are both dresses). If particular aspects of similarity are important, you may be better off using triplet loss or a Siamese network. However, both of those methods require some additional label information, where as once again, auto-encoders are unsupervised so they are an option even when no labels are available, or "similarity" is hard to define. I'm sure there are plenty of other applications, these are just the ones I've used them for. That is one of the fun things about unsupervised methods; you can be creative with them without getting stuck waiting for annotations or cleaning bad labels.
