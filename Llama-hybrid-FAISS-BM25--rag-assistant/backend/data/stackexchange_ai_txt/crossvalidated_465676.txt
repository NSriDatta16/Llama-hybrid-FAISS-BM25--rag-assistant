[site]: crossvalidated
[post_id]: 465676
[parent_id]: 
[tags]: 
Can the size of the training set of a neural network be smaller compared to other models?

If some of you knows what is the rule thumb one in ten rule this what I want to discuss. Neural network can, nowadays, go deeper in performances. Did you find, according to your experiences, that you can use less observations (one in five for instance) for the same amount of variables, with neural networks compared to more traditional ML models ? Besides, if I ask for this, it is in order to avoid "illusions" about the capacities of NN.
