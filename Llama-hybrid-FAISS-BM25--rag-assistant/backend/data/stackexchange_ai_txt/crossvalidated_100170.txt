[site]: crossvalidated
[post_id]: 100170
[parent_id]: 
[tags]: 
Expected error of best possible linear fit?

In my textbook, there is a statement mentioned on the topic of linear regression/machine learning, without a proof or rigorious justification, which is simply quoted as, Consider a noisy target, $ y = (w^{*})^T \textbf{x} + \epsilon $, for generating the data, where $\epsilon$ is a noise term with zero mean and $\sigma^2$ variance, independently generated for every example $(\textbf{x},y)$. The expected error of the best possible linear fit to this target is thus $\sigma^2$. Here my concerns are, How do we know what the best possible linear fit is ? Assuming in some way we estimated best linear fit, is it $y = (w^{*})^T \textbf{x}$ ? If so why can not other parameter $w_2^{*}$ be the best fitting parameter ?
