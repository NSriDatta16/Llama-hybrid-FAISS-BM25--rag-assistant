[site]: crossvalidated
[post_id]: 338462
[parent_id]: 
[tags]: 
which layer to use batch normalization in multi layer perceptron?

Hi I'm experimenting with deep neural network with 4 hidden layer and one output layer (regression layer). I found this working without regularization so I want to find out which regularization works well. However, it seems dropout and batch normalization don't work well in the test data. It makes overall score worse (Mean Squared Error in regression task). I don't get why this happen and trying to find out what I did wrong. am I supposed to apply batch normalization at all 4 hidden layer?
