[site]: crossvalidated
[post_id]: 219463
[parent_id]: 218574
[tags]: 
The big trouble is, that the alpha value is very often used mistakenly together with hypothesis testing, Sorry, you don't have it correct there. Indeed, $\alpha$ is fundamental to hypothesis testing. Explicitly, it is your chosen (maximum) type I error rate under the null hypothesis, and the basis on which the rejection region is chosen. Let me start with a basic/general discussion of hypothesis testing with a more-or-less Neyman-Pearson flavor (but which is not formally NP). Let's take as given that you want to test some hypothesis about some population characteristic, and you have a null hypothesis and an alternative hypothesis. Let's assume for now that the null is a point null but that the alternative is not. You choose* some test statistic that will tend to behave differently when the alternative is true than when the null is true. * there's theory that can help pick good ones if you know a lot about the population distribution but that's entirely beside the point here, and we rarely actually know that kind of information in any case. You then compute the distribution of the test statistic when the null is true (perhaps by making assumptions about the population distribution and computing the sampling distribution of the test statistic when the null is true, or perhaps by making exchangeability assumptions and invoking some form of resampling for that purpose - such as randomization tests or bootstrap tests) You identify a proportion $\alpha$ (or no more than $\alpha$ ) of the distribution that's more consistent with the alternative** than the null and call that your rejection region. If your test statistic falls into the rejection region you reject the null hypothesis. If the null was actually true this happens rarely (i.e. with probability $\alpha$ , whereas if the alternative is true it should happen considerably more often). ** However, it's possible to base a test purely on likelihood and take a more Fisherian-style point of view, where all the lowest-likelihood samples (under the null) compromise the rejection region. Note that we haven't mentioned p-values at all yet, though they're common in the FIsherian-style approach. [However, they also fit in with the NP approach if you recognize that $p\leq\alpha$ precisely when the test statistic is in the rejection region.] My current understanding goes like this: The alpha value is useful for the creation of confidence intervals (CIs) No. Well, $\alpha$ does come into it in the sense that you choose a coverage probability of $(1-\alpha)$ . e.g. testing the difference of means between samples Weren't you just calculating a CI? How did we jump to doing a test? What are you trying to do, produce a CI or test something? The p-value is interpreted as "there is a Can you show us someone saying exactly that? I can't quite follow what you're saying there. This, I have learned is wrong. Well, yes it's wrong, whatever it was trying to say. I have stumbled upon some papers where they 'calibrate' p-values and relate them directly to the error rate, I have no clear idea what you're saying there, but a. in general don't try to learn statistics from what people do in papers, at least not until you have a solid grounding in it. b. it's impossible to discuss a second hand report of what people do. If you want to discuss what you see in a paper, quote it and give a proper reference (so we can see more context if needed). You have many confusions here. It might have been more useful to have given explicit examples of what you've found that were directly contradictory than present your own understanding. Here, you have to state the % of true nulls With point nulls, this is usually 0. I cannot readily interpret this, can anybody help? It seems it relates to, for instance, the empirical knowledge of how often an effect is seen as effective. Am I wrong? Yes, you're wrong. The word "effective" doesn't belong there. If your null is "no effect" it relates to how often the effect is completely absent. But you have to be careful about what is being done here -- it sounds like someone is maybe taking a Bayesian approach but it's impossible to tell -- because again all we have is a somewhat muddled second hand report. It's impossible to untangle your misunderstandings from the misunderstandings of the people you're talking about. Anyhow, for the error rate, I previously had a good idea about how to calculate it by Monte Carlo methods. But now I can't recall my reasoning! It's not clear to me what you seek here. While the first half of your question was answerable enough (by explaining some of your misconceptions), if you can clarify and narrow (and add context to) the later part of your question you might like to post that as one or two new questions.
