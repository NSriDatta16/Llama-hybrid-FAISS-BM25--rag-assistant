[site]: crossvalidated
[post_id]: 489326
[parent_id]: 
[tags]: 
Why functions sampled from a linear kernel Gaussian Process are guaranteed to be a linear function?

It's well known that a linear kernel Gaussian Process regression is equivalent to Bayesian Linear Regression, because the functions sampled from a linear kernel GP is bound to be a linear function. However, I have trouble to understand why this is the case. Just in case you are not sure, you can see it for your self with this nice visualization tool . Just choose the kernel type to be linear. My confusion can be further boiled down to the following question. Let's suppose a simple covariance matrix as below, which is derived from a linear kernel. [[ 0. 0. 0.] [ 0. 25. 50.] [ 0. 50. 100.]] If you draw a sample Y from a 3D multivariate Gaussian with this covariance matrix, and plot the vector Y against X=[1,2,3], the 3 points will always be on a straight line. I just can't figure out why this is the case. I hope someone can help give an intuitive explanation.
