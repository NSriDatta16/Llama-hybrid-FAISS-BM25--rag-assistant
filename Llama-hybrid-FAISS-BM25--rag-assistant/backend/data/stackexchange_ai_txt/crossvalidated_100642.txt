[site]: crossvalidated
[post_id]: 100642
[parent_id]: 100529
[tags]: 
I do exactly what you describe as one of my modeling duties. Your "best" solution depends on several factors. For one, how are you defining "high risk" – top X % of members? For cost predictions your most straightforward solution will be to predict cost and then rank the members. Given your limited amount of data, this could be your best option, though given tons of data, it absolutely isn't if you're only truly interested in the most costly X %. In this case, coding the top X % as 1 and everyone else as 0 or -1 will tend to work better, though as you can imagine, this becomes increasingly untenable as your N decreases. I'm also in the process of implementing a case-weighted binary target, which I suspect will work better than the two options previously mentioned, but probably not as well as my monster ensemble (though that one takes a while to train even using 8 i7 cores and 32 GB RAM ;). Part of this also depends on the number of predictors you're using; you need more observations as the number of predictors increases. Are you using EHR or just claims data? Don't shut out the idea of training on FY2011–2012 and then testing on a bootstrap oversampled FY2012–2013. Or you could create 10 bootstrapped FY2011–2012 test sets and average the performance across models. I like this option for model selection. Since your final model will really be trained on both years (I'm assuming), you can, after model selection, do k -fold CV for a less biased estimate of future performance. I'm not opposed to talking to you about this problem privately.
