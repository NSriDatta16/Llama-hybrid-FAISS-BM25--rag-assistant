[site]: crossvalidated
[post_id]: 74598
[parent_id]: 74306
[tags]: 
First of all, this is a great description of your project and of the problem. And I am big fan of your home-made measurement framework, which is super cool... so why on earth does it matter what you call "averaging the integrals"? In case you are interested in some broader positioning of your work, what you would like to do is often referred to as Anomaly detection . In its simplest setting it involves comparing a value in a time-series against the standard deviation of the previous values. The rule is then if $$x[n] > \alpha SD(x[1:n-1]) => x[n]\text{ is outlier}$$ where $x[n]$ is the $n^{th}$ value in the series, $SD(x[1:n-1])$ is the standard deviation of all previous values between the $1^{st}$ and $(n-1)^{th}$ value, and $\alpha$ is some suitable parameter you pick, such as 1, or 2, depending on how sensitive you want the detector to be. You can of course adapt this formula to work only locally (on some interval of length $h$), $$x[n] > \alpha SD(x[n-h-1:n-1]) => x[n]\text{ is outlier}$$ If I understood correctly, you are looking for a way to automate the testing of your devices, that is, declare a device as good/faulty after it performed the entire test (drew the entire diagonal). In that case simply consider the above formulas as comparing $x[n]$ against the standard deviation of all values. There are also other rules you might want to consider for the purpose of classifying a device as faulty: if any deviation (delta) is greater than some multiple of the SD of all deltas if the square sum of the deviations is larger than a certain threshold if the ratio of the sum of the positive and negative deltas is not approximately equal (which might be useful if you prefer smaller errors in both directions rather than a strong bias in a single direction) Of course you can find more rules and concatenate them using boolean logic, but I think you can get very far with the three above. Last but not least, once you set it up, you will need to test the classifier (a classifier is a system/model mapping an input to a class, in your case the data of each device, to either "good", or "faulty"). Create a testing set by manually labelling the performance of each device. Then look into ROC , which basically tells you the offset between how many devices your system correctly picks up out of the returned, in relation to how many of the faulty devices it picks up.
