[site]: crossvalidated
[post_id]: 316797
[parent_id]: 316775
[tags]: 
Classic, closed form, multivariate statistical models, such as ANOVA, for modeling categorical features assume an invertible cross-products matrix. By and large, invertibility depends on the sheer number of levels in a categorical feature as well as the software used. My experience is that SAS is able to invert factors with several thousand levels whereas R has a much, much lower maximum. I know of two possible workarounds: 1) Steenburgh and Ainslie in a marketing science paper Massively Categorical Variables: Revealing the Information in Zip Codes propose a hierarchical Bayesian model for estimating parameters for the ~36,000+ levels for residential zip codes. Their approach is easily implemented in software like Stan or even Winbugs. One interesting finding from their paper is that using zip code alone is more powerfully predictive than using geo-demographic, enhanced features that vendors like Experian make available (but not cheaply). One of the downsides to HB models is that they are computationally inefficient. 2) Machine learning, divide and conquer algorithms (e.g., Wang, Chen, Schifano's A Survey of Statistical Methods and Computing for Big Data ) are extensions of iterative, approximating algorithms such as jacknifing, bootstrapping and random forests. The approach here is to plug one of the classic, statistical, multivariate models into one of these frameworks and run thousands or even millions of 'bite-sized' mini-models that enable invertibility, collecting the results of each iteration and aggregating them on the back end. All of these workarounds are iterative and approximating. Personally, my preference is for the ML, D&C solution(s) as they avoid the need for Bayesian assumptions and are more computationally efficient.
