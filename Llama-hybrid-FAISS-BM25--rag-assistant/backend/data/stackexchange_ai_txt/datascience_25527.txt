[site]: datascience
[post_id]: 25527
[parent_id]: 25520
[tags]: 
To get total error before back propagating - it is common to take an average of all the forward-pass errors. This is what's done in RNN such as LSTM. In the case of linear regression and logistic regression, The traditional Mean Squared Error Function can produce such a value. In essence, this value is represented by an average of errors: $Y(w) = 1/n{\sum_{i=1}^n Y_i(w)}$ Also, as a reminder, speaking of an actual backpropagation - from wikipedia : When used to minimize the above function, a standard (or "batch") gradient descent method would perform the following iterations: $$w:=w - {\alpha}\nabla Y(w) $$ which is basically $$w:= w - \alpha{\sum_{i=1}^n} \nabla Y_i(w)/n $$ notice the $/n$ When used with the ${\sum_{i=1}^n}$ it results in the average of all gradients := means 'becomes qual to' $\alpha$ is the learning rate
