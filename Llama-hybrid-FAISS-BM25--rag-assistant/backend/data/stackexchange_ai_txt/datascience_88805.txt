[site]: datascience
[post_id]: 88805
[parent_id]: 
[tags]: 
How to preprocess with NLP a big dataset for text classification

TL;DR I've never done nlp before and I feel like I'm not doing it in the good way. I'd like to know if I'm really doing things in a bad way since the beginning or there's still hope to fix those problems mentioned later. Some basic info I'm trying to do some binary text classification for a university task and I'm struggling at the classification because the preprocessing with NLP is not being the best. First of all, it's important to note that I need to have efficiency in mind when designing things because I'm working with very large datasets (>1M texts) that are loaded in memory. This datasets contains data related to new articles with title , summary , content , published_date , section , tags , authors ... Also, it's important to mention that as this task being part of a learning process I'm trying to create everything by myself instead of using external libraries (only for boring or complex tasks) Procedure The basic procedure for the NLP preprocessing is: Feature extraction -> str variable with title , summary and content attributes joined in the same string Lemmatization -> same str as input but with lemmatized words Stopword filtering Corpus generation -> dict object with lemmatized words as key and the index they're being inserted in the dictionary as value. After generating the corpus with all those samples, we can finally safely vectorize them (which is basically the same process as above but without the building corpus step). As you might guess, I'm not strictly following the basic bag of words (BOW) idea since I need to relieve memory consumption so it raises two problems when trying to work with AI algorithms like DecisionTreeClassifier from sci-kit. Problems Some of the problems I've observed till the moment are: Vectors generated from those texts needs to have the same dimension Does padding them with zeroes make any sense? Vectors for prediction needs also to have the same dimension as those from the training At prediction phase, those words that hasn't been added to the corpus are ignored Also, the vectorization doesn't make much sense since they are like [0, 1, 2, 3, 4, 1, 2, 3, 5, 1, 2, 3] and this is different to [1, 0, 2, 3, 4, 1, 2, 3, 5, 1, 2, 3] even though they both contain the same information
