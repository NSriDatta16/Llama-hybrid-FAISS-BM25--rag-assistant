[site]: datascience
[post_id]: 87093
[parent_id]: 
[tags]: 
Word2Vec: Why do some dimensions of an embedding have an interpretation, and why does addition/subtraction of embedding vectors work?

I'm reading about Word2Vec from this source: http://jalammar.github.io/illustrated-word2vec/ . Below is the heatmap of the embeddings for various words. In the source, it's claimed that we can get an idea on what the different dimensions "mean" (their interpretation) based on their values for different words. For example, there's a column that's dark blue for every word except WATER , so that dimension may have something to do with the word representing a person. Secondly, there's a famous example that "king" - "man" + "woman" ~= "queen", where the word in quotation means the embedding of that word. My questions are: I don't quite understand the mechanism as to how any dimension of an embedding goes on to have a tangible, interpretable meaning. I mean, the individual components of embedding vectors could've very well been completely arbitrary devoid of meaning, and the whole embedding approach still could've worked in that scenario, since we're interested in the vector as a whole. Is there an online explanation or a paper that I can look at to understand this phenomenon? Why does this addition/subtraction of vectors to give the relevant embedding vector for "queen" work so nicely? In one source , the explanation is given as follows: This works because the way that the neural network ended up learning about related frequencies of terms ended up getting encoded into the W2V matrix. Analogous relationships like the differences in relative occurrences of Man and Woman end up matching the relative occurrences of King and Queen in certain ways that the W2V captures. This seems like a broad, vague kind of an explanation. Is there any online resource or paper that explains (or better yet, proves) why this property of embedding vectors should hold?
