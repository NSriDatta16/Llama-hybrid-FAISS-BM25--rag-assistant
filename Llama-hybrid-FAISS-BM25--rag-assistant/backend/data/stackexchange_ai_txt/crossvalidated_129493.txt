[site]: crossvalidated
[post_id]: 129493
[parent_id]: 104255
[tags]: 
First an example for "How would I predict into the future using a KNN regressor ?". Problem: predict hours of sunlight tomorrow $sun_{t+1}$ from $sun_t .. sun_{t-6}$ over the last week. Training data: $sun_t$ (in one city) over the last 10 years, 3650 numbers. Denote $week_t \equiv sun_t .. sun_{t-6}$ and $tomorrow( week_t )) \equiv sun_{t+1} $ . Method: put the 3650-odd $week_t$ curves in a k-d tree with k=7. Given a new $week$, look up its say 10 nearest-neighbor weeks with their $tomorrow_0 .. tomorrow_9$ and calculate $\qquad predict( week ) \equiv $ weighted average of $tomorrow_0 .. tomorrow_9$ Tune the weights, see e.g. inverse-distance-weighted-idw-interpolation-with-python , and the distance metric for "Nearest neighbor" in 7d. "What are the advantages of using a KNN regressor ?" To others' good comments I'd add easy to code and understand, and scales up to big data. Disadvantages: sensitive to data and tuning, not much understanding . (Longish footnote on terminology: "regression" is used as a fancy word for "fitting a model to data". Most common is fitting data $X$ to a target $Y$ with a linear model: $\qquad Y_t = b_0 X_t + b_1 X_{t-1} + ... $ Also common is predicting tomorrow's say stock price $Y_{t+1}$ from prices over the last week or year: $\qquad Y_{t+1} = a_0 Y_t + a_1 Y_{t-1} + ... $ Forecasters call this an ARMA, Autoregressive moving-average_model or Autoregressive model . See also Regression analysis . So your first line "we can only build a regression function that lies within the interval of the training data" seems to be about the confusing word "regression".)
