[site]: crossvalidated
[post_id]: 395871
[parent_id]: 395078
[tags]: 
Questions Q0: The time series looks rather right-skewed and the level shift is accompanied by a scale shift. Hence, I would analyze the time series in logs rather than levels, i.e., with multiplicative rather than additive errors. In logs, it seems that an AR(1) model works quite well in each segment. See e.g. acf() and pacf() before and after the break. pacf(log(window(myts1, end = c(2018, 136)))) pacf(log(window(myts1, start = c(2018, 137)))) Q1: For a time series without breaks in the mean, you can simply use the squared (or absolute) residuals and run a test for level shifts again. Alternatively, you can run tests and breakpoint estimation based on a maximum likelihood model where the error variance is another model parameter in addition to the regression coefficients. This is Zeileis et al. (2010, doi:10.1016/j.csda.2009.12.005 ). The corresponding score-based CUSUM tests are available in strucchange as well but the breakpoint estimation is in fxregime . Finally, in the absence of regressors when looking only for changes in mean and variance the changepoint R package also provides dedicated functions. Having said that, it seems that a least-squares approach (treating the variance as a nuisance parameter) is sufficient for the time series you posted. See below. Q2: Yes. I would simply fit separate models to each segment and analyze these "as usual" Bai & Perron (2003, Journal of Applied Econometrics ) also argue that this is justified asymptotically due to the faster convergence of the breakpoint estimates (with rate $n$ rather than $\sqrt{n}$ ). Q3: I'm not fully sure what you are looking for here. If you want to run the tests sequentially to monitor incoming data, then you should adopt a formal monitoring approach. This is also discussed in Zeileis et al. (2010). Analysis code snippets: Combine log series with its lags for subsequent regression. d Testing with supF and score-based CUSUM tests: fs sc This highlights that both intercept and autocorrelation coefficient change significantly at the time point visible in the original time series. There is also some fluctuation in the variance but this is not significant at 5% level. A BIC-based dating also clearly finds this one breakpoint: bp Clearly, the mean drops but also the autocorrelation slightly. The fitted model in logs is then: plot(log(myts1), col = "lightgray", lwd = 2) lines(fitted(bp)) lines(confint(bp)) Re-fitting the model to each segments can then be done via: summary(lm(y ~ y1, data = window(d, end = c(2018, 136)))) ## Call: ## lm(formula = y ~ y1, data = window(d, end = c(2018, 136))) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.73569 -0.18457 -0.04354 0.12042 1.89052 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 3.92638 0.21656 18.13 summary(lm(y ~ y1, data = window(d, start = c(2018, 137)))) ## Call: ## lm(formula = y ~ y1, data = window(d, start = c(2018, 137))) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.43663 -0.13953 -0.03408 0.09028 0.99777 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 3.61558 0.33468 10.80
