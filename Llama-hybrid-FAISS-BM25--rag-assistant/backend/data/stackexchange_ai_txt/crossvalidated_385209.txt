[site]: crossvalidated
[post_id]: 385209
[parent_id]: 
[tags]: 
When would you prefer evidence over cross-validation?

I’ve been trying to spend time learning and revising the fundamentals of model selection and performance evaluation for neural networks and linear regression. Cross-validation seems to be the way to go when we wish to choose a model with the best predictive capability, because it is effectively an estimator for the predictive capability on new examples. Based on my experience, the only other aspect that counts aside from predictive capability is the form of the model we obtain, and it’s closeness to the true model. For me then, it’s natural to ask, why use anything else than cross-validation for model selection, especially if we wish to obtain the best predictor model? Intuitively, I would think that the evidence captures similar properties as cross-validation in model selection. It is better motivated in the Bayesian framework and with some assumptions, it is an exact amount (not an estimate like CV), however you would need to deal with your prior somehow. What would be a rule of thumb to decide between evidence and CV? Shall I prefer evidence when it’s computable and the assumptions are reasonable? Or is it something that’s more based on my viewpoint of probability (Bayesian/frequentist)?
