[site]: crossvalidated
[post_id]: 182208
[parent_id]: 181823
[tags]: 
No Free Lunch (or NFL) is often viewed as a negative result. “Over the set of all problems, any two computational search algorithms have identical performance”. Incidentally, this could also be stated as “over the set of all computational search algorithms, any two problems are equivalent”. There is no universal optimizer. However, what it really means is, if we have a non-uniform probability distribution over our data, then we can learn something about the distribution (i.e. we can do better than random guessing). This is usually the case in the real world, which is why Occam ’s razor appears to be correct in practice. In (offline supervised) machine learning, we assume the training dataset, and testing dataset are drawn the same probability distribution. This is also the case with predictive statistics. Hence, there is no contradiction.
