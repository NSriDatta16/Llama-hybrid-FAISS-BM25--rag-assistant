[site]: crossvalidated
[post_id]: 155243
[parent_id]: 154224
[tags]: 
It depends on what kernel you are using. By far the most commonly used (apart from linear) is the gaussian kernel, which has the form $$ f = exp \left ( \frac{- || x{_{1}} - x{_{2}} || ^2 }{2\sigma ^2} \right ) $$ An SVM takes this function and uses it to compare the similarity of a point ($x1$) to every other point in the training set by summing the differences as: $$ (x{_{1}}-l{_{1}})^2+(x{_{2}}-l{_{2}})^2...+(x{_{n}}-l{_{n}})^2 $$ where $x$ is your example and the values of $l$ are the landmarks. If the feature $x{_{1}}$ ranges from 0 - 50,000 while the feature $x{_{2}}$ ranges from 0 - 0.01, you can see that $x{_{1}}$ is going to dominate that sum while $x{_{2}}$ will have virtually no impact. For this reason it is necessary to scale the features before applying the kernal. If you want to learn more I recommend module 12 (Support Vector Machines) from the Stanford online course in machine learning at Coursera (free and available any time): https://www.coursera.org/course/ml
