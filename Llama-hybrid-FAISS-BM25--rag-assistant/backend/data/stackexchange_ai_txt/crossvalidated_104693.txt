[site]: crossvalidated
[post_id]: 104693
[parent_id]: 
[tags]: 
Unstable models, repeated crossvalidation, feature selection

I'm still trying to classify few (about 200) samples in a high dimensional feature space (dim=19) into 3 (very unbalanced) classes. I use an implementation of Least Squares SVM with one vs one coding with class weighting. I want to: estimate the best model parameters estimate the overall generalization performance My procedure is as followed: lets call this the inner loop: Estimate the hyperplane-parameters via a grid search one each binary model (e.g. 3 models for 3 classes). This is done on the given data via a 5 fold crossvalidation. To be sure I'm doing it right: Each model is "optimized" separatly. The cost-criterion is the balanced accuracy (see http://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification ) to deal with the unbalanced classes. I perform a 5 fold cv on each parameter from the grid search. So I have to make e.g. N 5-fold cv, where N is the "gridsize" for the gridsearch. I'm doing this because using no cv (though it would be computational much better) might lead to an biased estimator, right? Now assume I have estimated the "right" hyperplane parameters for each model. This still does not say anything about the accuracy for the 3 classes case. But I think, "local" optimization is still better than no parameter selection? Anyway, to get a good accuracy I'm doing a second optimization step with the complete model (3 submodels for each class) using a simplex algorithm. Again I use 5-fold-cv to estimate the balanced accuracy. after the last step I have a model that has "optimal" parameters. end inner loop begin outer loop now to estimate the overall performance I do another crossvalidation in an "outer loop". So I split my whole dataset into 5 folds, pass only the training set to the inner loop, where the "inner cvs" and parameter selection is made. To get the generalization performance I use the test set to calculate the balanced accuracy for each fold. Each fold I save the confusionmatrix. At the end I simply add all the confusionmatrices to get the overall performance. end outer loop Now my problems are: Is the procedure correct? It is already computationally heavily. My model's parameters and the performance in each outer cv loop is quite different. First I did not use an outer cv loop, only a hold out partition. I experienced very different performances each time I run the script. So I assume my models are unstable. Should I therefore repeat the outer loop? Say I've made 10 iterations/repetitions of the outer loop, what informations do I get? I additionally would like to include a feature selection in the inner loop. I could use sequentialfs from matlab. But I'm not sure where in the whole procedure the feature selection takes place. Since sequentialfs uses internal 10 fold cv, there would be another cv loop. That irritates me. So what is the correct order to perform all the steps? Any advice is appreciated, thank you.
