[site]: crossvalidated
[post_id]: 275753
[parent_id]: 
[tags]: 
EM and (Gaussian) Mixture Models

I have some trouble understanding the definition of the EM-algorithm. On Wikipedia they write the following mathematical expression: $$ E_{Z|X,\Theta^{(t)}}[\log L(\Theta^{(t)}; X, Z)|X=x] $$ Now, let us rename $\Theta^{(t)}$ to just $\Theta$ and let us also recall that (yet) we do not want to undergo the complete Bayesian approach, i.e. $\Theta$ is not a random variable but just a (fixed but arbitrary) constant. $L(\Theta; X, Z)$ is simply $f_{X, Z}(x,z)$, i.e. the common density of the random variables $X$ and $Z$. We will abbreviate this as $p(x,z)$. Hence, what they want to compute is $$E[\log p(X,Z)|X=x] $$ So I know that this should be equal to $E[\log p(x,Z)|X=x] = \int_{\mathcal{Z}} \log p(x,z) p(x|z) dz$ and so on but I am stuck at the very beginning: In order to write down a conditional expectation for, in fact, any random variable $Y$ conditioned on $X$ one needs to know that the random variable $Y$ is integrable (i.e. $L^1(\Omega)$). Hence, we need to know that the variable $$ \omega \mapsto \log p(X(\omega), Z(\omega))$$ is integrable. Question: Why is this true? What I have achieved so far: When I restrict myself to try to do Mixture models with the EM algorithm then this boils down to showing that for any $\Theta$, $$\int_{\mathbb{R}} \log (f(x;\Theta)) \cdot f(x;\Theta) dx Regards & Thanks in advance, FW
