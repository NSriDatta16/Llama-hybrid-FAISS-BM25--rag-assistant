[site]: crossvalidated
[post_id]: 92049
[parent_id]: 91963
[tags]: 
Note that initialising neural nets with DBNs is more of a historical anecdote nowadays. Direct supervised training with dropout regularization and piecewise linear activation functions tend to work much better in the presence of many labeled training examples. More direct answer to your question: 1) Using DBN features for linear regression or logistic regression is nothing but fine tuning the net. The difference is that you also adapt the parameters of the full hierarchy if you initialize a neural net. 2) KNN has been used on top of a neural net, see ( Salakhutdinov, Ruslan, and Geoffrey E. Hinton. "Learning a nonlinear embedding by preserving class neighbourhood structure." International Conference on Artificial Intelligence and Statistics. 2007. ) The whole method is more complicated though, as the DBN is further fine tuned for KNN usage. 3) DBNs have also been used as inputs to Gaussian processes ( Salakhutdinov, Ruslan, and Geoffrey E. Hinton. "Using Deep Belief Nets to Learn Covariance Kernels for Gaussian Processes." NIPS. 2007. ) 4) And deep networks have also been used to learn the Kernel for SVMs. ( Yichuan Tang "Deep Learning using Linear Support Vector Machines" ) Other methods, such as GBMs or RFs, seem not to be considered since they are not differentiable and thus fine tuning the feature extractor is not possible.
