[site]: crossvalidated
[post_id]: 610219
[parent_id]: 610210
[tags]: 
Running a single cross-validation loop yields an estimate of the out of sample predictive error associated with your modeling procedure, nothing more. You have 10 different models because stepwise selection is unstable, as @Dave explains. There is no reason to believe that any of your 10 models is 'right', but the mean of the cross-validation prediction error gives you an estimate of how large the prediction error will be in the future. At this point, you would run your procedure over the full dataset and use that as the final model. In general, I would advise against this, but that would be the protocol. If you want to use cross-validation to determine the $F$ -value to use as a cutoff for your modeling procedure, you need to do more. In that case, you would use a nested cross-validation scheme. In the outer loop, you would partition the data into $k$ folds and set one aside. Then you would perform another cross-validation loop on the remaining folds. In the inner loop, you would use some means to search over possible $F$ -values; for instance, you could use a grid search over a series of possible $F$ cutoffs. For each possible $F$ , there would be a average out of sample predictive accuracy score. You would take the cutoff that performed best and use it on the entire (nested) dataset to get a model. That model would be used to make predictions on the top level set that had been set aside, and from that you would get an estimate of the out of sample performance of a model that is selected in this manner. Then you would set the second fold aside, and perform the inner loop cross-validation and $F$ cutoff selection again, etc. After having done all this $k$ times, you could average those and get an average estimate of the out of sample performance of models selected in this manner. After that, you can repeat the search procedure that you had used in the inner loop on the outer loop alone (i.e., there wouldn't be an inner loop this time). That will give the model slightly more data to work with to select your final cutoff. Finally, you would fit your intended model using that cutoff on the whole dataset to get your final model, and you would have an estimate of how well a model of that type, selected in that manner, will perform out of sample. In short, the larger protocol is this: Run nested cross-validation, selecting a cutoff on the inner loop and then using it in the outer loop, to get an estimate of out of sample performance. Run cross-validation to select the cutoff to be used for the final model. Fit your model to the full dataset using the cutoff selected. To get more detail, try reading: Nested cross validation for model selection and Training on the full dataset after cross-validation? . Again, I wouldn't recommend you use stepwise selection, even in this case because the parameters will still be biased (and the constituent hypothesis tests will still be garbage), but the out of sample estimate of the predictive performance of a model fitted in this manner should be OK.
