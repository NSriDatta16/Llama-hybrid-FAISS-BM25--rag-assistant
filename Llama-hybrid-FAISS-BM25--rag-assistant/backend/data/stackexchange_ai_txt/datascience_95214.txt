[site]: datascience
[post_id]: 95214
[parent_id]: 95205
[tags]: 
Why compare them at all? The four models are tackling different problems . So it seems like you should not compare these models against themselves. If you had, for example, 3 different models to classify class A, then you could compare these models since they are all trying to solve the same problem. It is difficult to compare the models A, B, C and D because they have different training samples and inherently the cases may be more or less difficult. For example, suppose class A is "English for Beginners" and class B is "Advanced Data Science", then maybe the student's nationality maybe good for predicting class A, but the student's past courses are needed for a good prediction of B. So, maybe scoring >90% on A is easy because you have all the info, but scoring >90% on B is extremely difficult. In the end, if you have a model_A with score 87.32% and model_B with score 84.15%, who is to say that model_A is better? We cannot because the problems they solve are different. On the other hand, if you have model_A1, model_A2, and model_A3 that solve the same problem, you can compare them against the same test set. Single problem Another way to look at your problem, is as a single problem. While you have small models like model_A, model_B, model_C and model_D, they could be combined into a single model. You can take the results of each of the models and create a final output vector (e.g. [1,1,0,1] ) which means to recommend A, B and D. You then use this result to give a score to your whole system. You can still use the individual scores to see where to fine-tune, but perhaps reporting a score for the whole system is desired.
