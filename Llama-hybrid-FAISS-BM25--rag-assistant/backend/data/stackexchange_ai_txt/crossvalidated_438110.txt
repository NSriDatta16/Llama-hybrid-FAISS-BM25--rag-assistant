[site]: crossvalidated
[post_id]: 438110
[parent_id]: 332975
[tags]: 
This problem is widely recognized in the instance of training embeddings for Knowledge graph embeddings for link prediction. Even though it might be tangential I think it complements the answer already posted. In the TransE paper ( https://www.utc.fr/~bordesan/dokuwiki/_media/en/transe_nips13.pdf ) they solve it by constraining the entity embeddings norm to always sum to 1. $$ \lVert \mathbf{E_i} \rVert= 1$$ "the L2-norm of the embeddings of the entities is 1. This constraint is important for our model, as it is for previous embedding-based methods [3, 6, 2], because it prevents the training process to trivially minimize L by artificially increasing entity embeddings norms." It's also explained similarly in the paper [3] they refer to (Learning Structured Embeddings of Knowledge Bases, https://ronan.collobert.com/pub/matos/2011_knowbases_aaai.pdf ) "The normalization in step (4) helps remove scaling freedoms from our model (where, for example, E can be made smaller while Rlhs and Rrhs can be made larger and still give the same output)" Where E is one of the embedding matrices to be learned." Check out this pytorch implementation for better understanding on how this is implemented in practice, https://github.com/xjdwrj/TransE-Pytorch/blob/master/TransE.py In the forward pass method: pH_embeddings = F.normalize(pH_embeddings, 2, 1) Let me know if something is incorrect/off.
