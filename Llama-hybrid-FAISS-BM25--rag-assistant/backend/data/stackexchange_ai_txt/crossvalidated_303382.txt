[site]: crossvalidated
[post_id]: 303382
[parent_id]: 303368
[tags]: 
First off, if you are new to deep learning, a good idea is to start from scratch, like a "linear GAN", and deep learning is just replacing the linear models with neural networks. They do the same thing, just performing better on images/texts etc. Suppose you have some 1D data $X = \{x_1, \dots, x_n\}$, and you want your generator function $G$ to produce some new data different from $X$ but look like $X$ (one purpose of GAN is to create labeled data because they are often expensive), so you cannot cheat by taking the average $\bar{x}$ as that's just one data. You will need some randomness, for example, $z \sim N(0,1)$ as the seed for your next generated data. Say you decide to use $G(z) = w_1z + b_1$ as your generator, and initialize your $W, b$ randomly. Now your $G$ produces random data $x \sim N(b_1, w_1)$ different from $X$, but how to achieve the second goal (look like $X$)? The answer is to use a classifier. Say your classifier is logistic $D(x) = \sigma(w_2x + b_2)$, and you 1) sample some data from $X$; 2) generate some data from $G(z)$. You label them with $1$ for real data from $X$ and $0$ for fake data from $G(z)$, and train your logistic classifier as usual. Your discriminator works nicely now, and then you want your generator to be good, too. So you generate some data from $G$, and for the data that successfully fooled $D$, you want $G$ to generate more, and less otherwise. You tune $w_1, b_1$ to achieve this. Then you go back to train your discriminator again. The process will continue until the Nash equilibrium , where your generator works perfectly and your discriminator decides randomly. Regarding your questions, IMPO: 1) Why is there a need for the vector z? And why p(z) = N(0,1) initially? Goodfellow calls the distribution a "noise prior" in the original paper , which explains a lot. Think of $z$ as the random number seed. Since we want to generate fake data, we must give the generator some seed to start with. The choice of $N(0,1)$ is for convenience, I think. 2) Is the latent vector z learned in the training step of the generator network? No, we don't want to learn the seed. We want to learn the function that processes the seed, i.e. the parameters that define the generator network. 3) Do we ever learn the distributions p(x) and p(z)? If so, when? In the end we want that given $z \sim p(z)$, we have $G(z) \sim p(x)$. So in some sense, we do want the generator to learn $p(x)$ through training. but there is no point to learn $p(z)$. 4) According to here, the discriminator network is trained with the real samples and the fake samples in separate steps. When there is only one class of samples (either real or fake), how come you can train a "discriminator" (there is nothing to discriminate)? The link to "here" seems to be dead, but at least according to Goodfellow's original paper, you do sample two classes of examples during discriminator training (using the cross-entropy loss). As the comment points out, SGD confuses people a bit by only taking some data in each loop, but the stochastic gradient does have the same gradient in expectation as the regular GD. The discussion is beyond the range of this question, though.
