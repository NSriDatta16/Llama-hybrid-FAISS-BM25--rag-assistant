[site]: crossvalidated
[post_id]: 614650
[parent_id]: 606806
[tags]: 
This is a problematic approach. If you just want to know about general relationships between two variables, not restricting to linear (Pearson correlation) or even monotonic (Spearman correlation) relationships, you could use a value like mutual information. The R function JMI::JMI is one way to calculate mutual information between two variables. (My experience with this function is that it is slow.) That is probably the answer to the posted question: use mutual information to calculate/estimate the overall relationships between pairs of variables so you are not restricted to the particular relationships detected by, for instance, Pearson or Spearman correlation. Then, a flexible model like a random forest will figure out the nonlinear, nonmonotonic relationships in the regression. However, a flexible model like a random forest also looks at interactions between variables and their nonlinear transformations. By only considering one feature at a time in the mutual information calculations, you miss all of those. In fact, any kind of feature-by-feature screening is going to miss these interactions. In terms of information theory, two variables can be independent (zero mutual information) yet be conditionally dependent, conditional on the value of a third variable. Considering just two variables at a time will always miss that conditional dependence, yet a random forest model will be able to discover such relationships and use them to make accurate predictions (subject to the usual concerns about overfitting).
