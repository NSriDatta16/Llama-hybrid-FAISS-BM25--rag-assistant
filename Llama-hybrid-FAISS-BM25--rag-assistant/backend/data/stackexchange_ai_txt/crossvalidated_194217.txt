[site]: crossvalidated
[post_id]: 194217
[parent_id]: 194035
[tags]: 
One of the biggest disadvantages of frequentist approaches to model building has always been, as TrynnaDoStats notes in his first point, the challenges involved with inverting big closed-form solutions. Closed-form matrix inversion requires that the entire matrix be resident in RAM, a significant limitation on single CPU platforms with either large amounts of data or massively categorical features. Bayesian methods have been able to work around this challenge by simulating random draws from a specified prior. This has always been one of the biggest selling points of Bayesian solutions, although answers are obtained only at a significant cost in CPU. Andrew Ainslie and Ken Train, in a paper from about 10 years ago that I have lost the reference to, compared finite mixture (which are frequentist or closed form) with Bayesian approaches to model-building and found that across a wide range of functional forms and performance metrics, the two methods delivered essentially equivalent results. Where Bayesian solutions had an edge or possessed greater flexibility were in those instances where the information was both sparse and very high-dimensional. However, that paper was written before "divide and conquer" algorithms were developed that leverage massively parallel platforms, e.g., see Chen and Minge's paper for more about this http://dimacs.rutgers.edu/TechnicalReports/TechReports/2012/2012-01.pdf The advent of D&C approaches has meant that, even for the hairiest, sparsest, most high dimensional problems, Bayesian approaches no longer have an advantage over frequentist methods. The two methods are at parity. This relatively recent development is worth noting in any debate about the practical advantages or limitations of either method.
