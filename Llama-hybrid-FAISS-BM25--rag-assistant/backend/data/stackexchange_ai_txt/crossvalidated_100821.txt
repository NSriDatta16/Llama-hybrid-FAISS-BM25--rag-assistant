[site]: crossvalidated
[post_id]: 100821
[parent_id]: 100785
[tags]: 
Regarding the title question: Categorically, no. In your case, not enough info, hence my comment and downvote. Also, IMO, questions that conflate statistical and practical significance have been done half-to-death here, and you haven't said enough to make your question unique. Please edit ; I'll undo my downvote if I see improvement (it's locked now), and probably upvote if it's substantial. Your question addresses a common, important misconception that deserves being done the rest of the way to death, but as is, it's hard to say anything new about your situation that would make it a useful example. From a statistical viewpoint, has the intervention failed, and if not what further can be done? Again, what have you done so far? It's also quite possible that your analysis has failed, to borrow your term (IMO, "failed" is clearly too harsh in both cases). This is why I asked about your test. There's a fair amount of controversy surrounding pre-post analysis options, and random sampling or lack thereof is relevant to the choice of analytic options (see " Best practice when analysing pre-post treatment-control designs "). This is why I asked about a control group. If your choice of test can be improved, do that (obviously). In addition to checking your data (as @MattKrause wisely suggested ), check your test's assumptions. There are quite a few involved in the usual pre-post designs, and they're violated often. Normal distributions are likely to be poor models, especially for change scores and financial data. Consider nonparametric analyses. Heteroskedasticity is common, especially without random selection or with a partially stochastic intervention. Some tests are more sensitive to this than others – especially the conventional ones. Conventional ANCOVA assumes no interaction between interventions and covariates . If baseline income affects the viability of the intervention, you should probably use moderated regression instead $(\text{Final Income = Baseline Income + Intervention? + Interaction + Error}$, basically), assuming you do have a control group. If you don't, do you have more than 2 times? What other info about your individuals do you have? Exploring covariates and moderators is a good way to reduce the amount of statistical "noise" (error) your intervention's "signal" (effect) has to overwhelm for your test to "detect" it (support rejection of the null). If you can explain a lot of variance by means other than your intervention, or explain why your intervention doesn't affect everyone equally, you might get a better sense of how big your intervention's effect really is, all else being equal – which is rarely the default state of nature. I believe this was the spirit of Matt's suggestion #2. Regarding his caveat, don't be afraid to explore covariates and moderators you haven't specified in advance; just adopt an exploratory mindset and acknowledge this epistemological transition explicitly in any report you publish. The crucial point that bears repeating about statistical and practical significance is that their overlap is generally limited. Much of the practical significance of statistical significance is in what you intend to make of it. If you're seeking evidence to support further research (e.g., for a research grant), rejection of exploratory hypotheses may be enough. AFAIK, this is the only kind of practical significance that statistical significance is supposed to imply by default, and explains the choice of terminology historically: significant enough to justify more research . If you're looking for a statistical viewpoint on whether your intervention is worthwhile, you're probably asking in the wrong way. Statistical significance is not intended to answer this by itself; it only directly represents an answer to a very specific question about a null hypothesis. I suppose this amounts to another suggestion: check your null hypothesis. It usually defaults to stating that the effect observed in your sample is due entirely to sampling error (i.e., effect of intervention = 0). Are you really interested in any change whatsoever? How consistent do you need it to be to justify the intervention? These questions partly decide the appropriate null; you need to answer them. In confirmatory testing, you need to answer in advance. Since you've already run a test, any new tests of the same kind with different null hypotheses but the same sample would be exploratory. Unless you can collect another sample, it would probably be best to regard other kinds of tests as exploratory too. The strict sense of confirmatory hypothesis testing is particularly strict about the "no peeking" rule; IMO, this is a weakness of the hypothesis-testing paradigm as a whole. AFAIK, Bayesian analysis can be a little less strict about this, and might benefit you particularly if you can collect more data, because your current result could help inform your prior probability distribution. Another way to approach the issue is by focusing on effect size and your confidence interval. $2K is a change in the direction you wanted, right? If your test's results meant what I think you think they meant, then there's a better than 5% chance you'd find a negative change if you were to repeat the study, assuming the intervention had no effect. If your investment had any positive effect at all, the probability is lower than your p value. If you're invested heavily enough in the prospect of the treatment, maybe you should replicate the study. Again, you know better than I what else affects that decision. P.S. Despite my intro, I've managed to say plenty about this "half-dead" topic. Hopefully I've provided a useful summary of ideas other than those in preexisting answers, but I wouldn't be surprised if much of it isn't very useful to you personally. A big reason I wanted more info is that answering a vague question well practically necessitates covering a lot of unnecessary bases, which is kind of a waste of time. Nonetheless, if you grace us with an edit, I'll probably subsection off whatever no longer applies, and I might expand on what still does. It's evident from the incoming views that the question resonates with the audience here, so this could become a very useful question with a little more work.
