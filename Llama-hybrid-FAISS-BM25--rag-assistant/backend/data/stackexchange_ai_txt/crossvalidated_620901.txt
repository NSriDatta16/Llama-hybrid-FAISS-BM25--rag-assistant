[site]: crossvalidated
[post_id]: 620901
[parent_id]: 
[tags]: 
Using regression where the ultimate goal is classification

Some background information about what I am trying to model before asking my question: Long periods of hot temperatures and dry conditions (heatwaves) puts pressure on the components in the power distribution grid and eventually cause faults. My ultimate goal is to predict whether a power grid is in high risk, medium risk or low risk based on the prediction of number of faults by using machine learning algorithms. I have the dataset which have the features such as: temperature, humidity, power flowing throughout the grid etc. compiled daily for the summer periods of several years. As for the the dependent variable, I have the daily number of faults occured in my target grid. To illustrate, the distribution of the number of the faults in my dataset can be seen below. As far as I can imagine, there are several ways to tackle this problem. I can use a regression algorithm first to predict the number of faults and then classify the risk level by using a threshold on the predicted fault values. Also I can add to this model some measure of how confident I am in my classification based on how far I am from the threshold. I can transform my dataset's dependent variable (number of faults) into multiclasses ,using a threshold, and tackle the problem as a multiclass classification problem. I think this can overlook the severity of the number of faults in some days and worsen the performance of my model The first one seems more promising but still I have doubts about whether using regression where my ultimate goal is classification is a wise thing to do. How should I tackle this problem? Any other recommendation is well appreciated.
