[site]: crossvalidated
[post_id]: 67903
[parent_id]: 
[tags]: 
Does down-sampling change logistic regression coefficients?

If I have a dataset with a very rare positive class, and I down-sample the negative class, then perform a logistic regression, do I need to adjust the regression coefficients to reflect the fact that I changed the prevalence of the positive class? For example, let's say I have a dataset with 4 variables: Y, A, B and C. Y, A, and B are binary, C is continuous. For 11,100 observations Y=0, and for 900 Y=1: set.seed(42) n -5, 0, 1) I fit a logistic regression to predict Y, given A, B and C. dat1 However, to save time I could remove 10,200 non-Y observations, giving 900 Y=0, and 900 Y=1: require('caret') dat2 The regression coefficients from the 2 models look very similar: > coef(summary(mod1)) Estimate Std. Error z value Pr(>|z|) (Intercept) -127.67782 20.619858 -6.191983 5.941186e-10 A -257.20668 41.650386 -6.175373 6.600728e-10 B -13.20966 2.231606 -5.919353 3.232109e-09 C -127.73597 20.630541 -6.191596 5.955818e-10 > coef(summary(mod2)) Estimate Std. Error z value Pr(>|z|) (Intercept) -167.90178 59.126511 -2.83970391 0.004515542 A -246.59975 4059.733845 -0.06074284 0.951564016 B -16.93093 5.861286 -2.88860377 0.003869563 C -170.18735 59.516021 -2.85952165 0.004242805 Which leads me to believe that the down-sampling did not affect the coefficients. However, this is a single, contrived example, and I'd rather know for sure.
