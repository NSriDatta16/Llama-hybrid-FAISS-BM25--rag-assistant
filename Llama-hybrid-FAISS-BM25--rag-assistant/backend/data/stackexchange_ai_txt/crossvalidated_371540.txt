[site]: crossvalidated
[post_id]: 371540
[parent_id]: 
[tags]: 
How to choose an appropriate variational distribution?

I work in deep learning research and I am trying to learn how to use variational inference in order to approximate a posterior over the learned weights. I have looked extensively at Yarin Gal's papers as a reference ( https://arxiv.org/abs/1506.02158 , https://arxiv.org/pdf/1506.02142 ). He approximates a mixture of Gaussians for each weight: \begin{align*} q_\theta(W_1) = \prod_{q=1}^Q q_\theta(w_q), ~~~ q_\theta(w_q) = p_1 \mathcal{N}(m_q, \sigma^2 I_K) + (1-p_1) \mathcal{N}(0, \sigma^2 I_K) \end{align*} By fixing some of the parameters to some constants, sampling from this distribution is equivalent to sampling from a Bernoulli distribution. What I'm confused about is the rationale for choosing a variational distribution of this form? My gut feeling is that this was chosen specifically because it approximates Bernoulli drop-out and thus everyone gets Bayesian neural nets for free with no extra hassle. However, this leads me to the question, how does one choose an appropriate distribution given a certain problem? For instance, in my current work, I am aiming to model certain distributions that allow for clustering of the network weights, yet it makes no sense to me as how one would approach this from a variational perspective. Any insights or good references that explain how to do this?
