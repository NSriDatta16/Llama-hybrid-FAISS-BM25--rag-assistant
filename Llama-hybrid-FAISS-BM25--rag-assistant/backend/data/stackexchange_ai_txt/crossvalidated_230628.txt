[site]: crossvalidated
[post_id]: 230628
[parent_id]: 230544
[tags]: 
Let's recall the main aim of regularization is to reduce over fitting. What other techniques are currently being used to reduce over fitting: 1) Weight sharing- as done in CNN's, applying the same filters across the image. 2) Data Augmentation- Augmenting existing data and generate synthetic data with generative models 3) Large amount of training data- thanks to ImageNet etc. 4) Pre-training- For example say Use ImageNet learnt weights before training classifier on say Caltech dataset. 5) The use of RelU's in Neural Nets by itself encourages sparsity as they allow for zero activations. In fact for more complex regions in feature space use more RelU's, deactivate them for simple regions. So basically vary model complexity based on problem complexity. Use of a bunch of such techniques in addition to dropout and early stopping seems sufficient for the problems being solved today. However for novel problems with lesser data you may find other regularization techniques useful.
