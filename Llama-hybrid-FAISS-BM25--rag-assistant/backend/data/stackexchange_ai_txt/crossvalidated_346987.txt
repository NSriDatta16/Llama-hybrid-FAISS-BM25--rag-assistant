[site]: crossvalidated
[post_id]: 346987
[parent_id]: 
[tags]: 
Probabilistic programming vs "traditional" ML

I was browsing the github repo for Pymc and found this notebook: Variational Inference: Bayesian Neural Networks The author extols the virtues of bayesian/probabilistic programming but then goes on to say: Unfortunately, when it comes to traditional ML problems like classification or (non-linear) regression, Probabilistic Programming often plays second fiddle (in terms of accuracy and scalability) to more algorithmic approaches like ensemble learning (e.g. random forests or gradient boosted regression trees). Could someone please explain: If this statement is generally true Why this statement is true
