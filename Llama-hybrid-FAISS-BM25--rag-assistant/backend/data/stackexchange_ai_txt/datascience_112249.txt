[site]: datascience
[post_id]: 112249
[parent_id]: 
[tags]: 
Is It Fundamentally Correct To The Text Classification Model To Train First Without Pre-Trained Word Vectors And Then With Pre-Trained Word Vectors?

Is this solution fundamentally correct to the text classification (sentiment analysis) model to train it by these three steps: train the model without pre-trained word vectors untill reaches the minimum loss or maximum accuracy or even stopped by a callback. get the embedding layer weights and substitute the weights of the words which are represented in pre-trained word vectors and lock the embedding layer train the model again with the embedding layer which includes weights of the known pre-trained word vectors represented in it and unknown pre-trained word vectors based on the previous training. thanks for your companion
