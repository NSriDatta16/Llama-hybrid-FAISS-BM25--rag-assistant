[site]: crossvalidated
[post_id]: 636999
[parent_id]: 
[tags]: 
Reproducing results from classic dropout paper

In the classic paper "Dropout: A Simple Way to Prevent Neural Networks from Overfitting", there is a figure comparing the features learned by a one-layer autoencoder trained on MNIST with and without dropout. The features learned using dropout appear to be more meaningful. Does anyone know of a pytorch or tensorflow implementation that reproduces this result exactly or nearly exactly? Specifically, I am trying to reproduce the pair of images below. The paper claims that these are both features learned with auto-encoders having a single hidden layer of 256 ReLU units. I believe that this repo contains the original code, which I have not been able to get to run on my computer.
