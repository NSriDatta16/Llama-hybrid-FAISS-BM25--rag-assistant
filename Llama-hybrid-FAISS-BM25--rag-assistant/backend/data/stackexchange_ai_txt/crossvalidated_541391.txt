[site]: crossvalidated
[post_id]: 541391
[parent_id]: 
[tags]: 
Is standard deviation of the output layer probability vector a good proxy of model confidence?

I'm working on a multi-classification problem and I'm kinda new to neural networks, which is what I'm using. In the model I'm using now, the probability vector has to sum up to 1, which to me would mean little variation means little confidence. So if I'm on a 3-class classification problem and my output is: [0.33 0.33 0.33] There seems to be no confidence on the results, and the standard deviation is zero. On the other extreme: [1 0 0] There is a lot of confidence and the standard deviation is maxed out. Is this approach okay to sort my results by confidence? Is there a better, more standard approach? What I want to do in the end is to present all new predictions sorted by how confident the model is in its prediction. We can't deal with false positives or negatives, so each prediction has to be manually validated, and thus, it'll be better to sort said results from least confident to more confident, so the people that have to check said results can use most of their focus at the beginning. Thanks a lot
