[site]: crossvalidated
[post_id]: 471940
[parent_id]: 
[tags]: 
Can someone explain CAM loss used in U-GAT-IT paper?

I have been reading a recent paper accepted at ICLR, U-GAT-IT , which seems to produce pleasing results in the image-to-image translation tasks. There are four kinds of loss used in this paper: Adversarial loss, Identity loss, Cycle loss, and CAM loss. I could understand all losses except CAM loss. In the paper, CAM loss is defined as such: CAM loss: By exploiting the information from the auxiliary classifiers $\eta_s$ and $\eta_{D_t}$ , given an image $x \in \{X_s, X_t\}$ . $G_{s \to t}$ and $D_t$ get to know where they need to improve or what makes the most difference between two domains in the current state: \begin{align} L_{cam}^{s \to t}&=-(\mathbb{E}_{x \sim X_s}[\log(\eta_s(x))]+\mathbb{E}_{x \sim X_t}[\log(1-\eta_s(x))]), \tag{5} \\\\ L_{cam}^{D_t}&=\mathbb{E}_{x \sim X_t}[(\eta_{D_t}(x))^2]+\mathbb{E}_{x \sim X_s}[(1-\eta_{D_t}(G_{s \to t}(x)))^2]. \tag{6} \end{align} I couldn't understand the loss used in $(5)$ as shown in the picture above. My naive guess is that minimizing this loss encourages the auxiliary classifier $\eta_s$ to output higher value for the $x$ sampled from source domain $X_s$ and from target domain $X_t$ . But, I couldn't reason about the purpose of this. I get even more confused when I inspect the official implementation of this loss. fake_A2B, fake_A2B_cam_logit, _ = self.genA2B(real_A) fake_B2A, fake_B2A_cam_logit, _ = self.genB2A(real_B) fake_A2A, fake_A2A_cam_logit, _ = self.genB2A(real_A) fake_B2B, fake_B2B_cam_logit, _ = self.genA2B(real_B) G_cam_loss_A = self.BCE_loss(fake_B2A_cam_logit, torch.ones_like(fake_B2A_cam_logit).to(self.device)) + self.BCE_loss(fake_A2A_cam_logit, torch.zeros_like(fake_A2A_cam_logit).to(self.device)) G_cam_loss_B = self.BCE_loss(fake_A2B_cam_logit, torch.ones_like(fake_A2B_cam_logit).to(self.device)) + self.BCE_loss(fake_B2B_cam_logit, torch.zeros_like(fake_B2B_cam_logit).to(self.device)) Can someone help me understand CAM loss?
