[site]: crossvalidated
[post_id]: 370751
[parent_id]: 
[tags]: 
Is z-score normalization with hard caps reasonable?

I am currently trying to train an variational autoencoder that is implemented in TensorFlow. I have a training set with around 25 000 samples which I normalize by using the formula where the $i$ denotes the i-th training sample. This maps the values to area around 0. Because this is an autoencoder, the training set contains only "good" data, e.g. so that it is able to identify anomalies via the reconstruction error. For this good data, all values are close to 0 (e.g. 0.83). Now for testing, I got myself a set of around 5 000 additional samples that DO contain anomalies (e.g. very big values) that get normalized with the mean and standard deviation of the training set. The NORMALIZED value for one of those anomalies is 2484, for example. If I give this value to my autoencoder, it produces NaNs for the reconstructed input because seemingly those values become very large within the net. While this is good for detecting anomalies, I would still like to have a large numerical value instead of a NaN. The first solution that came to my mind was to restrict the range that those normalized values could take. For instance, all normalized values equal to or greater than 20 are automatically set to 20. Question: is using those hard caps a reasonable way to produce numerical values? Or is there a way to deal with such values?
