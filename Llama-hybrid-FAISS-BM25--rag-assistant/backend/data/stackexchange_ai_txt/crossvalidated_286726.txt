[site]: crossvalidated
[post_id]: 286726
[parent_id]: 286709
[tags]: 
Statistics community and machine learning community have "different" ways to control over-fitting. Many statisticians follow "The Principle of Parsimony (Occamâ€™s Razor)", which means "given a set of equally good explanations for a given phenomenon, the correct explanation is the simplest explanation". As a results, people adds variables to the model carefully (run many hypothesis testing before adding it). There are also many regression diagnostic tools to check the validity of the model. In such setting, even without testing data set, people can avoid over fitting effectively . Note that, you will not often see linear models from statistics community with thousand variables. For people in machine learning community, they check assumptions less comparing to statisticians, and you may see people use linear model on thousands or millions of variables often. They way of controlling over fitting is using the testing data . In sum, if it can fit, it can over-fit. Although linear model have high bias, it is also possible to have over fitting problem. People in different community have different ways to control the over fitting problem. And using a testing data set is one of the way. If the features / independent variables are large, and added into the model without careful validation, then testing data is needed.
