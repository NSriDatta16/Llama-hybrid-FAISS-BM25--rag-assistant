[site]: datascience
[post_id]: 62559
[parent_id]: 62493
[tags]: 
Your considerations regarding label encoding, one-hot encoding, and the likes are fairly accurate. For dealing with descriptive data, converting each description into a meaningful vector surely helps. Meaningful vectors are vectors that catch the essence of what is being said in the descriptions. Let, football = "round bouncy toy for kids to kick" basketball = "round bouncy toy for kids to dribble" spark_plug = "device for delivering electric current from an ignition system to the combustion chamber of a spark-ignition engine" Meaningful vectors will have properties such that, dist(vector(football), vector(basketball)) For vector representation learned on common English corpora, properties like the following emerge - vector('king') - vector('male') + vector('female') = vector('queen') Word2Vec is an efficient way to calculate word vectors - vectors that capture the meaning of individual words . To use Word2Vec you can either - Use vectors obtained by pre-training Word2Vec on a large public corpus (like Wikipedia). As your descriptions contain words that do not have domain-specific meanings, this might seem to be preferable. However, this method has a disadvantage. In the spirit of obtaining accurate representations of the entire English vocabulary, these learned vectors tend to have high dimensions (300 and 1000 are more common), which might make them unusable for your task. Train Word2Vec on your own vocabulary . With this method, you can set the learned-vector dimensions enough to capture word meanings but to not blow-up your feature space. For your descriptions, you can average out the vectors of its constituent words, thereby treating each description as a bag of words or you can use sophisticated techniques like Doc2Vec , which is based on Word2Vec but tries to capture the relative order of words. Gensim's implementation of Word2Vec and Doc2Vec has a fairly simple API exposed which you can quickly learn to use for your particular task.
