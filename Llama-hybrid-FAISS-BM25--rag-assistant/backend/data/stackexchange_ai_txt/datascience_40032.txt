[site]: datascience
[post_id]: 40032
[parent_id]: 39910
[tags]: 
One of the central challenges to using neural networks for decision processes is the transparency of the decision mechanism. We tend to grant more trust and credence to algorithms that we can see some of the intermediate reasoning steps for. Neural networks are, notoriously, black-boxes algorithms in which the intermediate steps are largely obscured from the developer and/or user. To that end, using a method to explain the decision mechanism of the neural network such as lime or some proxy model approach, to express the decisions made by the network via an alternative model type such as decision trees or linear models, which lend themselves better to interpretation allows us to address this property of neural networks. That said, there's no direct barrier to using neural networks as the basis of a decision system. Any function which takes data and outputs a prescriptive decision could meet this criterion. The real trick is ensuring to yourself (as the developer of the algorithm) that the decisions the network is making are in line with the decisions you want it to make and, separately, building confidence through evidence to the stakeholders of the algorithm that: The engine is making the decisions they need it to That they have a sufficient understanding of how that decision is being made to trust it to make the decision correctly going forward That the mechanism the algorithm is using is "fair"-- in that it's not going to adversely affect one group or another in ways that could have serious ethical, regulatory, or legal consequences-- and other risks posed by the implementation of the alorithm for its intended purpose are expressed. This instance comes to mind as as a specific risk for self-driving car decisions. Self-driving cars have potentially huge benefits, but it would be irresponsible (at least) to ignore the ways the AI that powers them could go wrong. It's worth saying that the last two points above are often not well-understood by stakeholders and therefore fall to the developer, as the person who should have the knowledge of risks related to the approach they are using, to proactively address.
