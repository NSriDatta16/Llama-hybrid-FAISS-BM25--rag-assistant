[site]: datascience
[post_id]: 104450
[parent_id]: 104439
[tags]: 
From your question I assume that you are familiar with at least basic concepts in RL so I won't dive into too many details. RL in general is not SGD. In RL you will encounter various optimization schemes in order to optimize an utility function. Two of the most popular families of methods used for optimizing an utility function (in RL MDP formulation) are Value methods and Policy Gradient methods. Value (or Critic) Methods Model-based value methods use Dynamic Programming (DP) to optimize an utility function. In simple wording, once optimal value functions have been found, that satisfy the Bellman Optimality Equations, can be used to obtain optimal policies. Model-free value methods use a form of Temporal Difference (TD) Learning to estimate the value function. TDs are a combination of DP and Monte Carlo (MC) methods. Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome (they bootstrap). Like MC methods, TD methods can learn directly from raw experience without a model of the taskâ€™s dynamics. A very common TD algorithm is Q-learning. It has been proved that, under the assumption of infinite visitations of every state-action pair, Q-learning converges to the optimal value function. Policy Gradient Methods (or Actor Methods) PG methods assume a parametrized policy function and use gradient ascent to optimize its parameters in order to maximize expected return: $$\theta_{h+1}=\theta_{h}+\left.\alpha_{h} \nabla_{\theta} J\right|_{\theta=\theta_{h}}$$ In this case you could possibly state that RL is following the steepest descent on the expected return. I post some great references in case you would like to delve into the details: Reinforcement Learning: An Introduction, 2nd edition Policy Gradient Methods Policy Optimization Policy Gradient Algorithms
