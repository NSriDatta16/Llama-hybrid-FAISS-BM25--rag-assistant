[site]: crossvalidated
[post_id]: 631487
[parent_id]: 
[tags]: 
Statistical Integration of Bayesian and Frequentist Approaches: Weighing Methodology

I'm uncertain about where to post this question. I'm currently working with geotechnical data (soil parameters) and aiming to obtain realistic and safer parameter values. To achieve this goal, I've started developing a Bayesian framework for my parameters using PyMC. However, for practical purposes, it might be challenging for others to adopt this approach for updating when new data becomes available, as not everyone is familiar with PyMC or Bayesian modeling. To address this, I've opted for a mixed approach. I've done the foundational work with Bayesian modeling and intend to provide it as 'documented prior knowledge' for others to use. They can leverage this knowledge to update their parameter values with new data. The rationale behind this is that while I have access to a larger sample that I could explore, colleagues working with the same material might face bureaucratic constraints, limiting them to collecting specific new data points. Inferences from a small dataset would naturally lead to high uncertainties. Therefore, I'm trying to implement a routine in our work environment where colleagues can weigh the new parameters obtained through a frequentist approach with those derived from my Bayesian model. The weighting method involves calculating the ratio between the uncertainty in my Bayesian model and the uncertainty in their observed data, represented as w = 1 - r, where r = sigma_model / sigma_sample. My main question is: Is this mixing of Bayesian-frequentist approaches and the proposed weighing methodology statistically correct? Are there any alternative methods to achieve this integration in a straightforward manner?
