[site]: crossvalidated
[post_id]: 333245
[parent_id]: 
[tags]: 
What is the meaning of initializing weights from a distribution function

I'm a machine learning newbie. I was going through various initializers that come with TFLearn, and it says that the "weights are picked up randomly from some probability distribution function" What I interpreted: If let's say we are using normal distribution function centered at some value 'x' and has std.deviation = 'y' for weight initialization of network. Then does that mean, that most of the weights in my neural network nodes will be initialized to 'x' (since normal function at 'x' is highest and hence 'x' has the highest probability of occurrence) and the probability of other values being assigned to the weights will gradually decrease (as per the std.deviation 'y') as I move to either sides of 'x' ? Is my interpretation correct ?
