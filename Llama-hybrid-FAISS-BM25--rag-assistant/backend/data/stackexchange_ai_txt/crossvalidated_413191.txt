[site]: crossvalidated
[post_id]: 413191
[parent_id]: 
[tags]: 
How do the forecast intervals from an AR model behave when the time series is inherently stationary?

I'm trying to wrap my head around two contradictory intuitions behind how forecast intervals should behave when we use an AR process to model a stationary time series: (a) On one hand, since the time series is stationary, the variance is constant and therefore the forecast intervals should stay the same throughout the forecast horizon, since they are proportional to the standard deviation. (b) One the other hand forecast intervals should widen with each forecasting step, since intuitively, the uncertainty of our forecast should increase the further away it moves from the present. To test this, I did the following: n Ok, so far so good: The model fitted by auto.arima() is pretty close to the process I specified. fcst The result I get is confusing. It supports neither (a) nor (b). The forecast intervals get larger and larger for the first few steps, then they stabilize and stay constant for the rest of the forecast horizon. Hyndman and Athanasopoulos mention this in fpp : In general, prediction intervals from ARIMA models increase as the forecast horizon increases. For stationary models (i.e., with $d = 0$ ) they will converge, so that prediction intervals for long horizons are all essentially the same. But they don't elaborate on the reasons. What is the intuition behind the fact that the fact the intervals grow for a while then converge? And what is the math behind it?
