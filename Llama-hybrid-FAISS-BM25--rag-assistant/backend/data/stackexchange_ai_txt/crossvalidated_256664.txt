[site]: crossvalidated
[post_id]: 256664
[parent_id]: 122062
[tags]: 
James Stein estimator and Ridge regression Consider $\mathbf y=\mathbf{X}\beta+\mathbf{\epsilon}$ With $\mathbf{\epsilon}\sim N(0,\sigma^2I)$ Least square solution is of the form $\hat \beta= \mathbf S^{-1}\mathbf{X}'\mathbf{y}$ where $\mathbf S= \mathbf X'\mathbf X$. $\hat \beta $ is unbiased for $\beta$ and has covriance matrix $\sigma^2 \mathbf S^{-1}$. Therefore we can write $\hat \beta \sim N(\beta, \sigma^2\mathbf S^{-1})$ Note that $\hat \beta $ are the the Maximum likelihood estimates, MLE. James Stein For simplicity for the Jame Stein we will assume $\mathbf S=\mathbf I$. James and Stein will then add a prior on the $\beta$, of the form $\beta \sim N(0,a\mathbf I)$ And will get a posterior of the form $\frac{a}{a+\sigma^2}\hat \beta=(1-\frac{\sigma^2}{a+\sigma^2})\hat \beta$, they will then estimate $\frac{1}{a+\sigma^2}$ with $\frac{p-2}{\|\hat \beta\|^2}$ and get a James Stein estimator of the form $\hat \beta=(1-\frac{p-2}{\|\hat \beta\|^2})\hat \beta$. Ridge Regression In ridge regression $\mathbf X$ is usually standadised (mean 0, vairance 1 for each column of $\mathbf X$ ) so that the regression parameters $\beta=(\beta_1,\beta_2,\ldots, \beta_p)$ are comparable. When this is $S_{ii}=1$ for $i=1,2,\ldots,p$. A ridge regression estimate of $\beta$ is defined as, $\lambda\geq0$, to be $\hat \beta (\lambda) =(\mathbf S+\lambda I)^{-1}\mathbf X'\mathbf y=(\mathbf S +\lambda\mathbf I)^{-1}\mathbf S \hat \beta$ note that $\hat \beta$ is the MLE. How was $\hat \beta (\lambda)$ derived ?? Recall $\hat \beta \sim N(\hat \beta, \sigma^2\mathbf S^{-1})$ and if we add a Bayesian prior $\beta\sim N(0,\frac{\sigma^2}{\lambda}\mathbf I)$ Then we get $\text{E}\left(\beta|\hat \beta\right)=(\mathbf S +\lambda\mathbf I)^{-1}\mathbf S \hat \beta$ Same as the ridge regression estimate $\hat \beta (\lambda)$. So the original form of the James Stein given here takes $\mathbf S=\mathbf I$ and $a=\frac{\sigma^2}{\lambda}$.
