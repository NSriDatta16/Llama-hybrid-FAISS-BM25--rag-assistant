[site]: crossvalidated
[post_id]: 31752
[parent_id]: 
[tags]: 
An event-log data-set for evaluating a filtering system?

For my research, I am working on an event filtering/matching system that considers each event to be a set of key-value pairs (e.g. "action=logon", "user=john.doe"). I would like to evaluate its validity, but I have been unable to find a public data-set with my requirements: It has to be a time series of samples with a consistent internal structure. It should be sizable - at least several millions of records/events. It has to be pre-classified to a number of categories/use-cases. A mere classification into e.g. normal/abnormal is not enough. Alternatively, I could also use a set with a domain-specific tool that can perform this classification accurately. There are several data sets (e.g. web server logs) around, but I have yet to find one that satisfies this requirement; I need to be able both to train my system and evaluate its performance using an accurate golden standard . I would like the categories to be conceptual equals e.g. various bank transaction types. I am currently using the KDD99 data-set, but it is too focused on anomaly detection; while it does have separate anomaly classes, all "normal" events are lumped into the same category... Is there a data-set somewhere that I could use? Preferably something that has already been peer-reviewed to a degree? As an alternative, is there a standard tool or other way to generate such a data-set? I have considered setting up e.g. a Web application and hammering at it with a load tester, or even using a generic simulation tool, but I believe that none of these approaches would really stand to peer review, which would compromise the evaluation itself.
