[site]: crossvalidated
[post_id]: 624227
[parent_id]: 
[tags]: 
CNN kernels similarity

I know some theory about deep neural network, cnn and back propagation in general. I am fascinated by the power of these technologies. I try to understand also the math aspects. For example the fact that cnn layers performs better than dense layers has amazed me a lot. In fact dense layer has more parameters, so it could fit more complex data, but is more difficult to train better so cnn, having less weights, has less degrees of freedom but it performs better. It is in some way counterintuitive. Thinking of CNN kernels and back propagation I don't understand how the training process can results in different kernels with no correlation between all the CNN kernels of the same layers. I imagine that random weights let the training process find a different optimal solution for each kernel, but this not ensure that some kernels weights will be highly correlated. In my understanding these could mean that the network has a low efficiency because it's "power" is used for similar jobs. Is it true or am I missing something? Are there some particular techniques that I not know? Thank you
