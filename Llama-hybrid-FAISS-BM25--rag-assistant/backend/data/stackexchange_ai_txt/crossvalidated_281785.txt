[site]: crossvalidated
[post_id]: 281785
[parent_id]: 
[tags]: 
Why would a BaggingRegressor only use a subset of samples and features during fitting?

I am working with a BaggingRegressor from sklearn and am having difficulty understanding what the purpose/effect of max_features and max_samples is on the model fit. From the description of the attributes max_samples : int or float, optional (default=1.0) The number of samples to draw from X to train each base estimator. If int, then draw max_samples samples. If float, then draw max_samples * X.shape[0] samples. max_features : int or float, optional (default=1.0) The number of features to draw from X to train each base estimator. If int, then draw max_features features. If float, then draw max_features * X.shape[1] features. The ability to change these seems a bit odd to me, especially seeing the default values. I guess I can see why from a programming standpoint they might be set to 1 by default (handle small data sets), but from an actual use case standpoint these seem like they should almost never be used, is that a fair assumption? Overall I do not see many reasons why these values should not be set to len(X.shape[0]) and len(X.shape[1]) respectively; using all samples and all features. If you used less samples You could "tune" how the OOB score is calculated. ...? If you use less predictors Try to handle cases where the number of predictors is not much larger than the number of observations. (Curse of dimensionality) Possibly simulate a random forest. Why would a model ever want to set these attributes to anything different that their respective maximums? How do these attributes effect the model fitting?
