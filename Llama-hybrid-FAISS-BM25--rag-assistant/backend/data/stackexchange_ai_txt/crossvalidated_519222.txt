[site]: crossvalidated
[post_id]: 519222
[parent_id]: 519219
[tags]: 
Is it the case with all machine learning models that, when you find the optimal parameters that minimize your objective function, you are also, almost by definition, finding the same statistically significant coefficients you would otherwise discover if you were to attack the problem from a stats perspective where the focus is on tests of statistical significance? In some cases, yes. If you're using logistic regression as a classifier, then optimizing the cost is the same as optimizing the log likelihood. Not every model is like this. Deep neural networks do not map nicely onto a statistical counterpart (though there is some active research on their statistical properties I imagine). As to variable selection via AIC or similar, that would be a form of feature selection. I think that is something to ask about on its own. As to your titular question, inference is not our main concern; its predictive capability. Besides, most ML problems (most, not all) work with data sets so large that significance becomes a straw man. The sheer size of the data would allow for high precision estimates, and since no effect is truly 0 you would find that all effects are significant.
