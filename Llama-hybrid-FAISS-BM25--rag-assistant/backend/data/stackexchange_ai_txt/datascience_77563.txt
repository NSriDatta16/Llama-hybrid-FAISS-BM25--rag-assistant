[site]: datascience
[post_id]: 77563
[parent_id]: 77545
[tags]: 
It's a trade-off Information is in the variance of data Let's say, we have a dataset with very high dimensionality, definitely, it will create a problem for any model(may refer the logic behind Curse of Dimensionality ) - We lose some variance while reducing the dimension - We helped the model learning the data. There might be other underlying reasons too ( than computation) i.e. removal of colinear and irrelevant features So, if the gain is more than the loss, then it will definitely improve the model's performance. Though it was not your question but be mindful of the fact that with Feature engineering we also create some good Feature by doing an intelligent Exploratory data analysis I made this point because with dataset not having too many Features( when not considered as very high dimensionality) , Feature engineering is more about finding new useful feature using the info from the data and some real-world knowledge about the problem. Manytime, a simple transformation using some Heuristics also works(Kaggle kernels)
