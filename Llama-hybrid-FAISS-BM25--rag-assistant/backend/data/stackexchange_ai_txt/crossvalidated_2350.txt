[site]: crossvalidated
[post_id]: 2350
[parent_id]: 
[tags]: 
Why does the random forest OOB estimate of error improve when the number of features selected are decreased?

I am applying a random forest algorithm as a classifier on a microarray dataset which are split into two known groups with 1000s of features. After the initial run I look at the importance of the features and run the tree algorithm again with the 5, 10 and 20 most important features. I find that for all features, top 10 and 20 that the OOB estimate of error rate is 1.19% where as for the top 5 features it is 0%. This seems counter-intuitive to me, so I was wondering whether you could explain whether I am missing something or I am using the wrong metric. I an using the randomForest package in R with ntree=1000, nodesize=1 and mtry=sqrt(n)
