[site]: crossvalidated
[post_id]: 314807
[parent_id]: 
[tags]: 
Clarification on posterior computation for genomic data in bayesian framework

I'm reading Trifinov et al. (2013) and I am trying to understand the designed framework. I list some excerpts to focus on the passage that is not clear to me: A lesion $M$ is a result of a random process with a given distribution $P^m(M)$ , which specifies the probability of occurrence of the lesion $M $ due to $m$ utation. [...] To model selection we assume that the genotype resulting from a disjoint set of such lesions $S = \{M_1, ⋯, M_t \}$ expresses the phenotype when a lesion in the sequence contains a driver gene. We consider two possibilities for the posterior probability that a given gene $D$ is a driver of the phenotype, given that the genotype of the set $S$ expresses the phenotype: a global posterior and a local posterior. To obtain the two posteriors we define first the likelihood that a lesion $M$ is a driver lesion given that $D$ is a driver gene as $$L(M|D) \quad \underline{deff} \quad P^m(M)·δ(D ∈ M)$$ where $δ(D ∈ M)$ is $1$ if the lesion $M$ includes the gene $D$ and $0$ otherwise, with corresponding posterior $$P(D|M)= \frac{δ(D∈M)·P^d(D)}{∑_{G∈M}P^d(G)}$$ where $P^d(D)$ is the prior probability that $D$ is a $d$ river of the phenotype. I am trying to understand how the posterior has beed computed, i.e. how to pass from the first to the second equation. I have little experience on bayesian statistics, except for the basic understanding required by the Wikipedia pages ( 1 , 2 ). For instance, following from the Wikipedia's likelihood definition, first definition translates to: $$L(M|D) = P(D|M) = P^m(M)·δ(D ∈ M)$$ So, the probability that $D$ is a driver gene given that $M$ is a driver lesion is equal to the probability of lesion $M$ due to mutation if $D$ is in $M$ , $0$ otherwise. Is it correct? Otherwise, could it be that $L(M|D)$ and $P(D|M)$ are both probabilities, in particular the first defining the probability of $M$ given $D$ , and the second defining the posterior distribution of $D$ given $M$ ? If this is the case, it unfolds to: $$P(D|M)=\frac{P(M|D)·P(D)}{P(M)}$$ $$=\frac{P^m(M)·δ(D ∈ M)·P^d(D)}{P^m(M)}$$ $$=δ(D ∈ M)·P^d(D)$$ But in this case, the sum at the denominator of the second equation is missing. How can I reduce the calculation of the posterior? What can I read to have a solid understanding of it? Edit I am now pretty sure that the derivation of the second equation should follow simply using the Bayes theorem, i.e.: $$posterior = \frac{likelihood·prior}{marginal\ likelihood}$$ with $likelihood=P^m(M)·\delta(D \in M)$ $prior=P^d(D)$ but I am unable to compute the marginal likelihood correctly. How can it be done? Trifonov, Vladimir, et al. " MutComFocal: an integrative approach to identifying recurrent and focal genomic alterations in tumor samples. " BMC systems biology 7.1 (2013): 25.
