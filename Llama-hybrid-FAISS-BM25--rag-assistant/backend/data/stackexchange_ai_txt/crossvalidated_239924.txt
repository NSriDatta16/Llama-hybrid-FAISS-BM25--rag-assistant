[site]: crossvalidated
[post_id]: 239924
[parent_id]: 239574
[tags]: 
This is all from here: http://cims.nyu.edu/~holmes/teaching/asa15/Lecture2.pdf TLDR: you're still using the spectral decomposition theorem; you just have to find the right symmetric matrix. Detailed Balance Let $P$ be your (say 2x2) transition matrix. It isn't symmetric. Let $\pi$ be the (1x2) stationary distribution vector. If the detailed balance equations hold, your Markov chain is reversible and this means that $$ \left( \begin{array}{cc} \pi_1 & 0 \\ 0 & \pi_2 \end{array} \right) P = P' \left( \begin{array}{cc} \pi_1 & 0 \\ 0 & \pi_2 \end{array} \right). $$ If we define $$ \Lambda = \left( \begin{array}{cc} \sqrt{\pi_1} & 0 \\ 0 & \sqrt{\pi_2} \end{array} \right), $$ then you can re-write the above equation as $$ \Lambda^2P = P'\Lambda^2. $$ Using the Spectral Decomposition Theorem The link above claims $V = \Lambda P \Lambda^{-1}$ is symmetric. This can be verified using the previous formula, left multiplying both sides by by $\Lambda$ and right multiplying both sides by $\Lambda^{-1}$ . By the spectral decomposition theorem, $V$ is orthogonally diagonalizable. The link calls its eigenvectors $w_j$ , and its eigenvalues $\lambda_j$ (for $j=1,2$ in this case). So $Vw_j = \lambda_j w_j$ and $w_j' V = w_j' \lambda_j$ . Plugging in the definition of $V$ , we get $$ \Lambda P \Lambda^{-1} w_j = \lambda_j w_j $$ and $$ w_j' \Lambda P \Lambda^{-1} = w_j' \lambda_j. $$ Premultiplying the former by $\Lambda^{-1}$ and the latter by $\Lambda$ we can verify the claim that $P$ has left eigenvectors $\psi_j = \Lambda w_j$ and right eigenvectors $\phi_j = \Lambda^{-1} w_j$ . These could be written in terms of matrices as $$ \left( \begin{array}{c} \psi_1' \\ \psi_2' \end{array} \right) P = \left( \begin{array}{cc} \lambda_1 & 0 \\ 0 & \lambda_2 \end{array} \right) \left( \begin{array}{c} \psi_1' \\ \psi_2' \end{array} \right) $$ $$ P \left( \begin{array}{cc}\phi_1 & \phi_2\end{array} \right) = \left( \begin{array}{cc}\phi_1 & \phi_2\end{array} \right) \left( \begin{array}{cc} \lambda_1 & 0 \\ 0 & \lambda_2 \end{array} \right). $$ Using the fact that $w_i'w_j = 0$ for $i \neq j$ and $1$ otherwise we can show that \begin{align*} P &= P \left( \begin{array}{cc}\phi_1 & \phi_2\end{array} \right) \left( \begin{array}{c} \psi_1' \\ \psi_2' \end{array} \right) \\ &= \left( \begin{array}{cc}\phi_1 & \phi_2\end{array} \right) \left( \begin{array}{cc} \lambda_1 & 0 \\ 0 & \lambda_2 \end{array} \right) \left( \begin{array}{c} \psi_1' \\ \psi_2' \end{array} \right) \\ &= \left( \begin{array}{cc}\phi_1 & \phi_2\end{array} \right) \left( \begin{array}{cc} \lambda_1 & 0 \\ 0 & \lambda_2 \end{array} \right) \left( \begin{array}{c} \phi_1' \Lambda^2 \\ \phi_2' \Lambda^2 \end{array} \right) \\ &= \sum_{k} \lambda_k \phi_k \phi_k' \Lambda^2. \end{align*}
