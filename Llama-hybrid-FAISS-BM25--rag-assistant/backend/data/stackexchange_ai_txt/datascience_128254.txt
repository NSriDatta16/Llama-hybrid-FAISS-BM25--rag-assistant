[site]: datascience
[post_id]: 128254
[parent_id]: 
[tags]: 
can decoder only large language model be fine tuned to perform well at semantic similarity search?

BERT based models are Encoder only which are well suited for text classification, and Semantic Text similarity search (If fine-tuned via sBERT). I want to know whether decoder only models like Llama2, GPT can be fine-tuned to do well on STS benchmark. If yes, does it perform better than fine-tuning encoder-only models?
