[site]: crossvalidated
[post_id]: 496102
[parent_id]: 
[tags]: 
Using log odds to identify a feature for machine learning?

I am currently working on a project to try and predict if we have a broken electrical box in a zip-code across a range of countries (This is a made up example just to get a handle on the technique). Because of the size of the dataset and the number of columns I have with other attributes, I would like to see if the zip-code itself could be considered predictive. The idea being we would only include the predictive zipcodes (with a margin of error) and reduce the number of columns dramatically. A zip-code can have many electrical boxes and the number varies with the size of the geographical distance covered. If a box has failed in zipcode it has a 1 attached to it. If it has not failed it has a 0 attached to it. In practice this means that a zipcode can have many 1s and 0s. There are approximately 50k zipcodes which when you explode it out with the categorical has_failed flag you get approx 580k records. I thought a good way to approach this would be to run a logistical regression and using the zip-code as the independent variable and the has_failed as the dependent variable. Each zipcode would then get both the log-odds and a p_value. I would then only select the zipcodes with a large effect and an acceptable p-value. Concious of the possibility of getting a Type 1 error, I applied a false discovery rate adjustment ( FDR ) to the results to try and create a volcano plot as seen here in Figure 5.4. My problem is I use the the R package biglm to process the data in chunks, the algorithm fails to converge which I suspect is down to complete seperation of the results. It could also be that i am approaching this implementation completely incorrectly Does anyone know of another robust way I could approach this? The overall aim is to have a column in my dataset that has a 1 or a 0 if the zipcode itself has been shown to be predictive based on the method above (given the caveats of interactions being missed etc). Below is some random generated data from fake data to highlight a working example of what i was attempting. library(dplyr) library(broom) library(ggplot2) # The zipcodes lose the letters at the end but thats ok for what we are doing berlin_zip % as.character() %>% sample(., 5000, replace = TRUE) has_failed % mutate(log10_fdr_p = -log10(p.adjust(p.value, method = "fdr"))) mod_coefficents #> # A tibble: 182 x 6 #> term estimate std.error statistic p.value log10_fdr_p #> #> 1 (Intercept) 0.594 0.0883 6.72 1.99e-11 8.44 #> 2 berlin_zip10117 -0.0776 0.126 -0.617 5.38e- 1 0.0483 #> 3 berlin_zip10119 -0.110 0.126 -0.873 3.83e- 1 0.0694 #> 4 berlin_zip10178 0.0729 0.147 0.495 6.20e- 1 0.0483 #> 5 berlin_zip10179 -0.00754 0.128 -0.0589 9.53e- 1 0.0117 #> 6 berlin_zip10315 -0.135 0.135 -1.00 3.16e- 1 0.102 #> 7 berlin_zip10317 -0.0737 0.133 -0.553 5.80e- 1 0.0483 #> 8 berlin_zip10318 -0.149 0.131 -1.14 2.53e- 1 0.102 #> 9 berlin_zip10319 -0.177 0.135 -1.31 1.89e- 1 0.102 #> 10 berlin_zip10365 -0.00754 0.128 -0.0589 9.53e- 1 0.0117 #> # ... with 172 more rows ggplot(mod_coefficents, aes(x=estimate, y=log10_fdr_p)) + geom_point(alpha = 0.7)
