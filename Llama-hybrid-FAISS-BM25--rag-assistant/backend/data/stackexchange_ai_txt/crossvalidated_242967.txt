[site]: crossvalidated
[post_id]: 242967
[parent_id]: 172136
[tags]: 
I would agree on Xi'an's comment All Models Are Wrong . However, these models (assumptions) enable us to have "describe data effectively". Without these (strong) assumptions, it is impossible to compute anything. Let's use Markov Chain as an example. Suppose we want to model a length $100$ sequence, $(X_1, X_2, \cdots, X_{100})$, and you have say $5000$ sequences. And let's assume all symbols in the sequence is binary. Without the Markov assumption, how can we describe the joint distribution $P(X_1, X_2, \cdots, X_{100})$? If you think about using a table, that is $2^{100}$ rows, and $2^{100}-1$ free parameters (needs to sum to $1.0$). There is no way we can do that with computer from computational perspective. On the other hand, to fit those number of parameters, we need much much more data. So, $5000$ sequences seems to be nothing... With the Markov assumption, $$P(X_1, X_2, \cdots, X_{100})=P(X_1)\prod_{n=2}^{100}P(X_n|X_{n-1})$$ We only have very few parameters: Initial distribution: 1 free parameter Transition matrix 2 free parameters Such assumption (constrain) enables us to have a joint in a traceable way. To your edit, even all the real world data does not satisfy Markov property in a reasonable degree, we still need reasonable strong assumptions. Because it is the assumption make the modeling work doable.
