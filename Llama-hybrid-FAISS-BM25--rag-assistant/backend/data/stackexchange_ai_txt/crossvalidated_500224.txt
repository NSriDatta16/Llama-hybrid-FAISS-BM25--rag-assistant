[site]: crossvalidated
[post_id]: 500224
[parent_id]: 
[tags]: 
Is there a method for evaluating predictive models that accounts for difference between predicted/actual distributions?

I am using machine learning to predict the monthly productivity (in dollars) of various groups of people. My question relates to ways of measuring the performance of my model. The distribution of earnings is irregular, with many zeroes, as well as unusual peaks and various skews depending on the characteristics of the people I look at. Most model metrics (e.g. RMSE, MAE) seem to focus on how well the mean is captured, but for me this is irrelevant. The average may be correct, but if the distribution of productivity is not accurate and realistic then the model is not useful. To try and assess accuracy of the predicted distributions I have tried making my own metric where I sum the absolute difference in proportions earning each value. This is useful, but does not seem ideal - it requires some level of binning (i.e. to the nearest $) and the choice of bin is arbitrary. Also I feel that differences in different parts of the distribution should perhaps be weighted differently. Is there an existing metric that accounts for differences between predicted and actual distributions that would be of use to me here?
