[site]: crossvalidated
[post_id]: 517074
[parent_id]: 517065
[tags]: 
When training VAE you have auxiliary loss to fit the encoder output to P(Z) - a multivariate standard normal. But in practice the resulting distribution is not exactly standard normal. In this article they have another phase. They don't insist on standard normal, instead they are willing to change their prior to other gaussian distribution that will be closer to the encoder output. So the question is now, what is the final gaussian distribution of the encoder output. I believe that this can be found by minimizing the KL divergence between the encoder output and a new gaussian. i.e. find which mu, sigma of each dimension of the latent space, minimizes the KL divergence to the encoder output. KL divergence between two normal distribution has an analytic solution and therefore can be minimized by gradient descent algorithms
