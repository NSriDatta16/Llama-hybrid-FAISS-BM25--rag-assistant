[site]: crossvalidated
[post_id]: 533666
[parent_id]: 533621
[tags]: 
For short-length text analysis with smaller datasets, I've found pretrained word embeddings useful. For example, taking the /path/to/the/myfile part of @Tim's answer , you can tokenize to [path, to, the, myfile] (in this specific case, probably dropping the common to , the , maybe trying to split long strings like myfile ), and get their respective embeddings. From there, it seems common to just average the embeddings over all words in a document; depending on your specific usecase, some other aggregation may be worth exploring. For example, if you only need a distance between URLs, you could use the word-mover distance. Common domains can probably also be found in a word embedding, but uncommon ones probably won't appear. Request parameters and anchors may also be usable, depending on how human-readable they are. The other components of Tim's answer can be used directly as categorical features (or numerical, in the case of domain length).
