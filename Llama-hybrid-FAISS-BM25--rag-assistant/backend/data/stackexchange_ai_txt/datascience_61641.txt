[site]: datascience
[post_id]: 61641
[parent_id]: 
[tags]: 
Pytorch lstm model very high loss in eval mode against train mode

I am using a Siamese network with a 2-layer lstm encoder and dropout=0.5 to classify string similarity. For each batch, I am randomly generating similar and dissimilar strings. So, the pytorch model cannot overfit to the training data. When the model is in train() mode, loss is 0.0932, but, if the model is in eval() mode, loss is 0.613. What is the reason for this behavior? Here is the total code for this model. The code works on gpu instance of colab without any modification. import torch # torch.set_default_tensor_type(torch.cuda.FloatTensor) from torch import nn from torch.autograd import Variable import numpy as np from tqdm import tqdm_notebook as tqdm import time # from google.colab import files # files.upload() chars = r' !"#&\'()*+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz' # len(chars) char_to_id = {k:v for k,v in zip(chars, range(len(chars)))} char_to_id[' '] = len(char_to_id) #char_to_id # len(char_to_id) #char_to_id[' '] class opt: embedding_dims = 20 vocab_size = 81 hidden_dims = 128 num_layers = 2 beta_1 = 0.9 learning_rate = 0.0001 clip_value = 1 lstm_bidir = True lstm_dropout = 0.5 padding_idx = char_to_id[' '] class LSTMEncoder(nn.Module): def __init__(self, opt): super(LSTMEncoder, self).__init__() self.embed_size = opt.embedding_dims self.hidden_size = opt.hidden_dims self.num_layers = opt.num_layers self.bidir = opt.lstm_bidir self.padding_idx = opt.padding_idx if self.bidir: self.direction = 2 else: self.direction = 1 self.dropout = opt.lstm_dropout self.lstm = nn.LSTM(input_size=opt.embedding_dims, hidden_size=self.hidden_size, dropout=self.dropout, num_layers=self.num_layers, bidirectional=self.bidir) def initHiddenCell(self, batch_size): rand_hidden = Variable(torch.zeros(self.direction * self.num_layers, batch_size, self.hidden_size)) rand_cell = Variable(torch.zeros(self.direction * self.num_layers, batch_size, self.hidden_size)) return rand_hidden, rand_cell def forward(self, input1, hidden, cell): # input1 = self.embedding(input1) # input_lengths = torch.as_tensor(input_lengths, dtype=torch.int64, device='cpu') # input1 = torch.nn.utils.rnn.pack_padded_sequence(input1, input_lengths, batch_first=False, enforce_sorted=False) output, (hidden, cell) = self.lstm(input1, (hidden, cell)) return output, hidden, cell class Siamese_lstm(nn.Module): def __init__(self, opt): super(Siamese_lstm, self).__init__() self.encoder = LSTMEncoder(opt) self.input_dim = int(1 * self.encoder.direction * self.encoder.hidden_size) self.classifier = nn.Sequential( nn.Linear(self.input_dim, int(self.input_dim/2)), nn.ReLU(), nn.Linear(int(self.input_dim/2), int(self.input_dim/4)), nn.ReLU(), nn.Linear(int(self.input_dim/4), 1), nn.Sigmoid() ) self.embedding = nn.Embedding(num_embeddings=opt.vocab_size, embedding_dim=opt.embedding_dims, padding_idx=opt.padding_idx, max_norm=None, scale_grad_by_freq=False, sparse=False) def forward(self, s1, s2, s1_lengths, s2_lengths): batch_size = s1.size()[1] if device.type == 'cuda': max_length = torch.cuda.LongTensor(torch.cat((s1_lengths, s2_lengths))).max().item() else: max_length = torch.LongTensor(torch.cat((s1_lengths, s2_lengths))).max().item() # init hidden, cell h1, c1 = self.encoder.initHiddenCell(batch_size) h2, c2 = self.encoder.initHiddenCell(batch_size) s1 = self.embedding(s1) s1 = torch.nn.utils.rnn.pack_padded_sequence(s1, s1_lengths, batch_first=False, enforce_sorted=False) h1 = h1.to(device) c1 = c1.to(device) v1, h1, c1 = self.encoder(s1, h1, c1) s2 = self.embedding(s2) s2 = torch.nn.utils.rnn.pack_padded_sequence(s2, s2_lengths, batch_first=False, enforce_sorted=False) h2 = h2.to(device) c2 = c2.to(device) v2, h2, c2 = self.encoder(s2, h2, c2) v1, l1 = torch.nn.utils.rnn.pad_packed_sequence(v1, batch_first=False, total_length=max_length) v2, l2 = torch.nn.utils.rnn.pad_packed_sequence(v2, batch_first=False, total_length=max_length) # print(v1) if device.type == 'cuda': batch_indices = torch.cuda.LongTensor(range(batch_size)) else: batch_indices = torch.LongTensor(range(batch_size)) v1 = v1[l1-1,batch_indices,:] v2 = v2[l2-1,batch_indices,:] # features = torch.cat((v1,v2), 1) features = abs(v1-v2) output = self.classifier(features) return output def create_similar_strings(chars, str_length = 1000, replace_frac = 0.01, delete_frac=0.01): # creating a random string str1 = np.random.choice(chars,str_length) str2 = str1 # randomly calculate the number of characters from above string to replace or delete rand_num1 = np.random.randint(0,int(replace_frac*str_length)+1) rand_num2 = np.random.randint(0,int(delete_frac*str_length)+1) # print(rand_num1, rand_num2) # calculate the positions at which to replace the characters in str2 idx = np.random.choice(range(len(str2)), rand_num1) str2 = [y if x not in idx else np.random.choice(chars, 1)[0] for x,y in enumerate(str2)] # calculate the positions at which to delete the characters in str2 idx = np.random.choice(range(len(str2)), rand_num2) str2 = [y for x,y in enumerate(str2) if x not in idx] return np.random.choice([''.join(str1), ''.join(str2)], 2, replace=False) def create_dissimilar_strings(chars, str_length = 1000, max_len_diff_frac = 0.05): # creating a random string str1 = ''.join(np.random.choice(chars,str_length)) length = np.random.randint(int(str_length*(1-max_len_diff_frac)), int(str_length*(1+max_len_diff_frac))) str2 = ''.join(np.random.choice(chars, length)) return np.random.choice([str1, str2], 2, replace=False) #str1, str2 = create_similar_strings([x for x in chars], 10, 0.1, 0.1) #str1, str2, len(str1), len(str2) #str1, str2 = create_dissimilar_strings([x for x in chars], 10, 0.2) #str1, str2, len(str1), len(str2) def random_generate_strings(chars, str_length=1000, replace_frac=0.01, delete_frac=0.01, max_len_diff_frac = 0.05): if np.random.choice([True, False],1)[0]: str1, str2 = create_similar_strings(chars, str_length, replace_frac, delete_frac) label = [1] else: str1, str2 = create_dissimilar_strings(chars, str_length, max_len_diff_frac) label = [0] return str1, str2, label def random_generate_batch(chars, batch_size=1, min_str_length=10, max_str_length=1000, replace_frac=0.01, delete_frac=0.01, max_len_diff_frac = 0.05): strings1 = ['']*batch_size strings2 = ['']*batch_size labels = [0]*batch_size for i in range(batch_size): str_length = np.random.randint(min_str_length, max_str_length) str1, str2, label = random_generate_strings(chars, str_length, replace_frac, delete_frac, max_len_diff_frac) strings1[i] = [char_to_id[x] for x in str1] strings2[i] = [char_to_id[x] for x in str2] labels[i] = label len_strs1 = [len(x) for x in strings1] len_strs2 = [len(x) for x in strings2] max_length = max([len(x) for x in strings1+strings2]) temp_list = [char_to_id[' ']]*max_length strings1 = [(x+temp_list)[0:max_length] for x in strings1] strings2 = [(x+temp_list)[0:max_length] for x in strings2] train_batch_a = torch.t(torch.LongTensor(strings1)) train_batch_b = torch.t(torch.LongTensor(strings2)) train_labels = torch.t(torch.Tensor(labels)) len_strs1 = torch.LongTensor(len_strs1) len_strs2 = torch.LongTensor(len_strs2) return train_batch_a, train_batch_b, train_labels, len_strs1, len_strs2 train_batch_a, train_batch_b, train_labels, len_strs1, len_strs2 = random_generate_batch([x for x in chars], 3, 5, 6, 0.2, 0.2, 0.05) print(train_batch_a.size(), train_batch_b.size(), train_labels.size(), len_strs1.size(), len_strs2.size()) print(train_batch_a, '\n', train_batch_b, '\n', train_labels, '\n', len_strs1, '\n', len_strs2) def train_one_step(model, optimizer, criterion, train_batch_a, train_batch_b, train_labels, len_strs1, len_strs2): model.train() output = model.forward(Variable(train_batch_a), Variable(train_batch_b), len_strs1, len_strs2) output = output.squeeze() train_labels = Variable(train_labels).squeeze() # print(output, output.size(), train_labels, train_labels.size()) loss = criterion(output, train_labels) print(loss.item()) optimizer.zero_grad() loss.backward() optimizer.step() def test_one_step(model, criterion, train_batch_a, train_batch_b, train_labels, len_strs1, len_strs2): model.eval() output = model.forward(Variable(train_batch_a), Variable(train_batch_b), len_strs1, len_strs2) output = output.squeeze() train_labels = Variable(train_labels).squeeze() # print(output, output.size(), train_labels, train_labels.size()) loss = criterion(output, train_labels) print(loss.item()) model = Siamese_lstm(opt) device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu") # device = torch.device("cpu") device model = model.to(device) # criterion = nn.MSELoss() criterion = nn.BCELoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.000001) # optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9) n = 10 for i in range(n): train_batch_a, train_batch_b, train_labels, len_strs1, len_strs2 = random_generate_batch([x for x in chars], batch_size = 3, min_str_length = 10, max_str_length = 500, replace_frac = 0.07, delete_frac = 0.07, max_len_diff_frac = 0.1) # print(train_batch_a, '\n', train_batch_b, '\n', train_labels) # print(train_batch_a.size(), '\n', train_batch_b.size(), '\n', train_labels.size()) # print(len_strs1, len_strs2) train_batch_a = train_batch_a.to(device) train_batch_b = train_batch_b.to(device) train_labels = train_labels.to(device) len_strs1 = len_strs1.to(device) len_strs2 = len_strs2.to(device) train_one_step(model, optimizer, criterion, train_batch_a, train_batch_b, train_labels, len_strs1, len_strs2) # test_one_step(model, criterion, train_batch_a, train_batch_b, train_labels, len_strs1, len_strs2) to = time.time() with torch.set_grad_enabled(False): train_batch_a, train_batch_b, train_labels, len_strs1, len_strs2 = random_generate_batch([x for x in chars], batch_size = 1000, min_str_length = 10, max_str_length = 500, replace_frac = 0.07, delete_frac = 0.07, max_len_diff_frac = 0.1) train_batch_a = train_batch_a.to(device) train_batch_b = train_batch_b.to(device) train_labels = train_labels.to(device) len_strs1 = len_strs1.to(device) len_strs2 = len_strs2.to(device) # print(len_strs1, len_strs2) test_one_step(model, criterion, train_batch_a, train_batch_b, train_labels, len_strs1, len_strs2) print(round(time.time()-to,2)) # 0.0932, 0.613 # torch.save(model.state_dict(), '/content/text_similarity_v4(gpu).pth') # model.load_state_dict(torch.load('/content/text_similarity_v4(gpu).pth')) In the test_one_step function, we only need to change the model.eval() to model.train() to observe the difference after training the model.
