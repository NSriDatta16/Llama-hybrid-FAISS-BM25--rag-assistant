[site]: crossvalidated
[post_id]: 482240
[parent_id]: 482225
[tags]: 
In this situation, you are likely to have issues with collinearity (what econometricians call "multicollinearity" which is a bit redundant). Collinearity can affect the significance (and standard errors) of your estimated model. There are a number of approaches to deal with that: choosing a subset of covariates; creating a combination of covariates; or, using everything. Choosing a Subset of Covariates You can choose a subset of covariates by only keeping the most significant covariates in the model. However, that can run into problems: it may be that none of the covariates are significant. The first approach should be to consider which variables make more theoretic sense. Often, one of the collinear covariates will have a more direct theoretical connection to the response than others. In that case, you should prefer the theoretically-justified covariate since it is more likely to perform better out-of-sample (since it has a reason to be related to the response). For example, suppose you are trying to predict weight with sex, height, and age. Obviously, adults are heavier than infants, but height should be more related to weight than age. You might even proxy for the volume of the person by instead looking at height^3. (Note that we also should probably interact height or height^3 with sex to allow for different relationships between height and weight in men vs women.) If you lack a theoretical justification to prefer a covariate, you can instead look at variance inflation factors (VIFs). Often, omitting the variable with the highest VIF can reduce the collinearity problem. Also, plotting and/or regressing the covariates with high VIFs versus each other can be informative to see how your covariates are related. Another approach is to use ridge regression or the LASSO and vary the penalty to see which collinear covariates are the largest and dominate for large penalties. Creating a Combination of Covariates Another approach to handling collinearity is to combine covariates. As mentioned before, if there is a theoretical reason for variables to be combined, try that first. Sometimes, people will do a principle components analysis (PCA) on the collinear covariates to get a combination of the covariates which explains the most of their variation. These modelers then use the first principle component as a way to combine the collinear covariates. "Using the first PC" might mean looking at the first PC and seeing what it generally implies ( e.g. "oh, about 2/3 of X1 and 1/3 of X2") to then create a new variable X.12combo . "Using the first PC" might also mean redoing the PCA and using whatever PC1 is. The first approach is sensible; the latter approach is asking for trouble since what a principle component is will vary with the data (and may not be close to prior definitions). I wish I could say the latter approach is rare, but it is common in some social sciences. There is a serious caveat with PCA: while it might seem sensible, remember that nothing in PCA is related to your response. You might find combinations of collinear covariates which explain lots of their mutual variation but none of the response variation. Finally, you might consider an approach rarely done but sensible: create an "index" by averaging all of the collinear covariates. That way, you are not depending on any one covariate and the averaging may reduce the noise from any one measurement. This is why economists, for example, often look at indices of stock returns or costs of consumer goods. In those cases, the indices are useful enough to have acquired their own meaning over time. Using Everything You could also just use all of the covariates in your model and ignore the issues of collinearity. This has serious potential for problems since you are likely to be overfitting. Overfitting is especially likely if you find that your model has wildly varying coefficient estimates versus what you would estimate in a smaller model. For example, if using X1 or X2 in a model gives you coefficients of 1.1 or 3 but using them together gives you coefficient estimates of 8 and -12. In that case, the estimation is trying to use one covariate to cancel out the noise in the other covariate. If a difference of covariates makes sense theoretically, fine. However, if you have no theoretical reason to expect that difference to be in the model, you may well find that this strongly-weighted difference will not perform well out of sample. (Perhaps using another dataset the estimates would be 5 and -9 or 3 and -7?) For More Information Obviously, this only scratches the surface of handling collinearity. For a little more explanation of some of the above, you might want to consult this Penn State course site section on regression pitfalls . I would also recommend having a more complete guide: Weisberg's Applied Linear Regression is my personal favorite. Also very useful is the classic Regression Diagnostics by Belsley, Kuh, and Welsch.
