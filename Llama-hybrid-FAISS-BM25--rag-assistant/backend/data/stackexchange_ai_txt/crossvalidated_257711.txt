[site]: crossvalidated
[post_id]: 257711
[parent_id]: 
[tags]: 
Deep Learning inputs

I've started to research ANNs and specifically Deep Learning. I'm confused however about the exact input of such nets. When reading about ANNs we keep hearing how good they can be for the classic MNIST digit classification problem and how a 28x28 grey scale image becomes a 784 input ANN. My question is - how does the 784 input nodes 'know' that say pixels 1 & 29 are on top of each other, for it to go on and find higher abstracted patterns..? No where in the input do we specify "pixel location" if you will...? When I read about ConvNets I see they use a sliding 3x3 pixel map etc as a method to get that 'location' information....is it therefore a matter of all ANNs use this and its explicity called out when discussing ConvNets...or does the standard MNIST problem solve it differently - as I don't believe some of the examples Ive seen state that ConvNets are being used for digit classification..? I'm not sure if I've phrased this correctly - hopefully someone could shed some light. Paul.
