[site]: crossvalidated
[post_id]: 390996
[parent_id]: 388400
[tags]: 
Are you looking for time estimates are for training or for testing? Time complexity $O(Mn)$ would simply appear to be a lookup operation for each of the $n$ elements, i.e. no updating of the neighbors. In that case, why $O(M^2)$ for storage? Wouldn't there just be a single codebook value for each element in the map? In my experience there are many different estimates for SOM training. If you are doing the in-depth calculations for each portion of the algorithm, I think I agree with the following (for training SOM via the batch algorithm): “… in BSOM-t the computation of the neighborhood terms needs $O(m^2)$ operations, the assignment step runs in time $O(nmd) + O(nm^2)$ and the recalculation of the weight vectors runs in time $O(nd) + O(m^2d)$ . Therefore, the complexity of BSOM is $O(m^2) + O(nmd) + O(nm2) + O(m^2d)$ , and that of BSOM-t is $Niter · (O(m^2) + O(nmd) + O(nm^2) + O(m^2d)$ .” Le Thi, H. A., & Nguyen, M. C. (2014, p.1353). Self-organizing maps by difference of convex functions optimization. Data Mining and Knowledge Discovery, 28(5–6), 1336–1365. http://doi.org/10.1007/s10618-014-0369-7 Here, BSOM is batch SOM, BSOM-t is training using BSOM for a number of iterations, m is the number of nodes in the map, n is the number of observations in the training data, d is the number of distances and is related to the size of the neighbourhood. Note that this is assuming sequential. If the map was parallelized using GPU, it could be a different story.
