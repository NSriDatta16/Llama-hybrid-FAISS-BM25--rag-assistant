[site]: crossvalidated
[post_id]: 277308
[parent_id]: 277292
[tags]: 
I'm not sure what the interviewer was after but when facing a poorly preforming model these are the things I consider and an answer I would love hearing as an interviewer (been interviewing for a couple of years now). Getting more data : This might not always help but there are few things that can help you evaluate this solution effects: Run the model with different sample sizes - if results improve with more data then its reasonable assuming getting more data will continue improving model performance. Features to sample ratio - after you selected features try understanding if you have enough samples per each feature value. See an answered question on this subject . Missing target values - elasticity might not behave similarly between different price ranges. In a situation where you samples data is biased towards a specific range there is a good chance that you won't be able to generalize(for example 90% of samples are for prices between 0-10 and the other 10% are for prices between 1000-10000). There are ways tackling this problem other than getting more data(split the model training, don't use regression). Better feature engineering : If you have enough data and you know about deep-learning then maybe this one is irrelevant. In case you don't fit the mentioned criteria, focus your efforts on this one. In user-behavior models, there are many relations that our human-intuition is better understanding than a machine trained model. As in your case where you engineered a couple of more features and improved model performance so greatly. This step is prone to errors since it usually involve logic based code(If Elses/ Mathematical formulas). Better model selection : As you suggested, maybe a non-linear model will work better. Is your data homogeneous? Do you have reasons to believe that cross features will explain the price elasticity better? (seasonality * competitor's price). Hyper parameters tuning : grid searching model's hyper parameters(+ cross validating results) is a good practice but as far as to my experience it rarely improves performance greatly (surely not from 5% to 90%). There are more things that can be done, but these points are generic enough.
