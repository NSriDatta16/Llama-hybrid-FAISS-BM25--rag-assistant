[site]: crossvalidated
[post_id]: 341559
[parent_id]: 341549
[tags]: 
However, shouldn't computing $\sum_{h} e^{-E(v^t, h; b,c,W)}$ be intractable as well? It is. That's why RBMs are mostly trained with Contrastive Divergence which only approximates Maximum Likelihood. The idea is to approximate updates to ML using the following: $\frac{\partial{\log(p(x))}}{\partial{W_{ij}}} = \mathbb{E_{data}}[v_i h_j] - \mathbb{E_{model}}[v_i h_j]$ Where expectations are taken respectively for the distributions specified by data and equilibrium distribution of Markov Chain that is formed by alternating the process of generating (using sampling) visible given hidden states and hidden states given visible. The trick is to run this Markov Chain and get the approximation for $\mathbb{E_{model}}$ (some applications even report running only one step). Hinton's original paper on Contrastive Divergence gives more details, concrete examples and mentions the previous papers on the topic (I personally recommend going through the paper, it's pretty straightforward).
