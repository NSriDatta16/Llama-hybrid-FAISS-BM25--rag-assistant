[site]: crossvalidated
[post_id]: 319306
[parent_id]: 
[tags]: 
Conveying uncertainty in accuracy measurements for machine learning models

I've noticed that depending on how I sample training and test samples I can get a range of model accuracies, but the mean of those accuracies is reasonable. Also for methods like random forests and neural networks there is inherent randomness in the model that can lead to different results. What is the best way to convey this uncertainty? I've been thinking about showing the mean accuracy and confidence intervals, but this isn's something I see other people do. Are there any standards for conveying model variance and uncertainty?
