[site]: crossvalidated
[post_id]: 386466
[parent_id]: 
[tags]: 
The distribution of a posterior predictive p-value under certain assumptions

I am wondering if anyone can check my understanding of the following passage concerning posterior predictive p-values in the textbook " Bayesian Data Analysis 3rd Edition " on page 151: In the special case that the parameters $\theta$ are known (or estimated to a very high precision) or in which the test statistic $T(y)$ is ancillary (that is, if it depends only on observed data and if its distribution is independent of the parameters of the model) with a continuous distribution, the posterior predictive p-value $P(T(y^{\text{rep}}) > T(y) \mid y)$ has a distribution that is uniform if the model is true. I suspect this is something akin to what is called the "CDF transformation" where a random variable $X$ is transformed by plugging it into its own CDF. In other words, if $X$ has a cdf $F_X( \cdot)$ , then the random variable $F_X(X)$ is uniformly distributed. And correct me if this is wrong, but another way to write this is $P(X \le x \mid \tilde{X} = x)$ where $\tilde{X}$ is just another iid copy of $X$ . So I guess if we can show that $T(y^{\text{rep}}) \mid y$ and $T(y) \mid \theta$ are distributed in the same way, that this implies $$ P(T(y^{\text{rep}}) > T(y) \mid y) = 1 - F(T(y)) $$ where $F$ is the CDF of both of those random variables. Is that right?
