[site]: datascience
[post_id]: 107207
[parent_id]: 107202
[tags]: 
The authors provide this image in their supplemental information: There, you can see their explanation. The convolutional layers encode the image into some latent space representation. The RNN operates in this latent space, generating a new latent space representation based on the previous observations. For any latent space representation, the decoder can convert it into an image. Thus the RCNN uses essentially the same procedure as the model types you mentioned (GANs, Convolutional Encoder-Decoders); there is a decoder that takes representations from the latent space to the image space.
