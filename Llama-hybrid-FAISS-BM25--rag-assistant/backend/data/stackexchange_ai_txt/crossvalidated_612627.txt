[site]: crossvalidated
[post_id]: 612627
[parent_id]: 611958
[tags]: 
Disclaimer: I assume all three models have: 1. Common preprocessing of the training set. 2. XGBoost classifier hyperparameters tuned adequately. 3. Evaluation is done on a separate test set. 4. No target leakage. Gradient boosting machines (like XGBoost) are great but often they are hard to interpret. Here we can move forward with our investigation in two ways: 1. Use simpler explainable models. 2. Use post hoc explanation methods. Both options involve some visualisation because ultimately we want to have some contrasts. (Third option in the end) Option 2: I will actually start with the post hoc explanation methods first. Use partial dependence plots ( PDPs ) from the models you think are "reasonably performant". Examine the ones corresponding to the variables having the highest importance (I would use the usual gain attribute as the average gain across all splits the feature is used corresponds the "closest" to generic feature importance for prediction purposes). These plots should show some coherent gradient/variation. Superimpose those PDPs against the same PDPs from the less performant model. Those plots should immediately tells us how our "performant models" use certain variables while our least performant model cannot find any coherent signal there. Note that it is not necessary that different models have similarly looking PDPs, the contrary is usually true. What we want to show is there one model finds meaningful variation and the other model does not. Option 1: Using a glass-box model that is directly explainable allows us to immediately ascertain how certain predictions are made or why can't they be made. As a first step: Let's remember that XGBoost ultimately uses CARTs as base learners. That means that we can visualise (some of) these trees and see what is learned. I would suggest first trying a Random Forest ( not Extra Trees ) with a very small number of trees. Again the catch is to first see how a "good ensemble learner" behaves and contrast that with the "bad ensemble learner". As a second step: Let's also remember that GBMs are closely related to GAMs. We can use the most informative features from the post hoc analysis and build a GLM or a GAM. While this is wrong as a general model-building strategy because it leads to overfitting and fails to capture interactions, the catch here is to see if we can indeed overfit any signal we might have. We can then compare how that signal can or cannot be found between our models (for example, a $\beta_{x_1}$ might be statistically significant in our "good model" but statistically insignificant and/or with a minimal effect size in our "bad model". A third option I wouldn't immediately consider but one might try is to use Boruta or a similar meta-learning variable importance algorithm. I personally find such methods not horribly insightful but then again they should allow one to quantify feature importance scores with a standardised methodology. We can then make an almost like-for-like comparison too. As above we would want to contrast the variable importance computed rather than focus on the exact magnitudes. Final comment: The above is essentially a guide to post hoc model explainability comparisons. There are no substitutes for doing proper EDA before modelling. Simply using different response variables and then saying " this sometimes works and sometimes doesn't work " is completely normal. The whole argument here is why seemingly related response variables are not having similar performance. But we have no commentary if these response variables have the same "noise" components too. Again, EDA and literature reviews are our friends here. Good luck! :)
