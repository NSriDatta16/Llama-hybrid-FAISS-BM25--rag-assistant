[site]: crossvalidated
[post_id]: 114053
[parent_id]: 114001
[tags]: 
There is a rich literature on testing dependent correlation coefficients. Here are some relevant references: Dunn, O. J., & Clark, V. (1971). Comparison of tests of the equality of dependent correlation coefficients. Journal of the American Statistical Association, 66(336), 904-908. Neill, J. J., & Dunn, O. J. (1975). Equality of dependent correlation coefficients. Biometrics, 31(2), 531-543. Steiger, J. H. (1980). Tests for comparing elements of a correlation matrix. Psychological Bulletin, 87(2), 245-251. Steiger, J. H. (1980). Testing pattern hypotheses on correlation matrices: Alternative statistics and some empirical results. Multivariate Behavioral Research, 15(3), 335-352. Olkin, I., & Finn, J. D. (1990). Testing correlated correlations. Psychological Bulletin, 108(2), 330-333. Meng, X., Rosenthal, R., & Rubin, D. B. (1992). Comparing correlated correlation coefficients. Psychological Bulletin, 111(1), 172-175. Raghunathan, T. E., Rosenthal, R., & Rubin, D. B. (1996). Comparing correlated but nonoverlapping correlations. Psychological Methods, 1(1), 178-183. Cheung, M. W.-L., & Chan, W. (2004). Testing dependent correlation coefficients via structural equation modeling. Organizational Research Methods, 7(2), 206-223. You will find equations for the covariance of overlapping correlations in these references (for raw and r-to-z transformed correlations). You can then approach this problem from a meta-analytic perspective. In essence, you have $k$ correlations (raw or r-to-z transformed) with an approximately known variance-covariance matrix that can be computed/estimated. Let $y$ denote the (column) vector with the correlations and $V$ the corresponding var-cov matrix. Then you can test the null hypothesis that all true correlations are the same (homogeneous) by comparing the test statistic $$Q = y'Py$$ against the critical value of a chi-square distribution with $k-1$ degrees of freedom, where $P = W - WX(X'WX)^{-1}X'W$, $X$ is just a column of 1s, and $W = V^{-1}$. You can also test more focused hypotheses, such as that the $i$th correlation is different from the rest. For this, let $X$ be a $k \times 2$ matrix with 1s in the first column and 0s in the second column, except for $i$th row, which gets a 1 in the second column. Then you can fit the model $$y = X\beta + e,$$ where $e \sim N(0, V)$. You can estimate $\beta = [\beta_0, \beta_1]'$ with $$b = (X'WX)^{-1} X'Wy$$ and the variance-covariance matrix of $b$ with $$Var(b) = (X'WX)^{-1}.$$ Now compute $z = b_1 / SE[b_1]$, where $b_1$ is the second element from $b$ and $SE[b_1]$ is the standard error of $b_1$ (i.e., the square root of the second diagonal element from $Var(b)$). You can compare $z$ against the critical values of a standard normal distribution (i.e., $\pm 1.96$ for $\alpha = .05$, two-sided) to test whether the $i$th correlation is different from the rest.
