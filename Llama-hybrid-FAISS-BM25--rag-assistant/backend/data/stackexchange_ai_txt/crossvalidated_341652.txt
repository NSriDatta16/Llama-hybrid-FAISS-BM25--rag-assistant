[site]: crossvalidated
[post_id]: 341652
[parent_id]: 79168
[tags]: 
To understand the advantages of sparsity in PCA, you need to make sure you know the difference between "loadings" and "variables" (to me these names are somewhat arbitrary, but that's not important). Say you have an $n\times p$ data matrix $\textbf{X}$ , where $n$ is the number of samples. The SVD of $\textbf{X}=\textbf{US}\textbf{V}^\top$ , gives you three matrices. Combining the first two $\textbf{Z} = \textbf{US}$ gives you the matrix of Principal Components. Let's say your reduced rank is $k$ , then $\textbf{Z}$ is $n\times k$ . $\textbf{Z}$ is essentially your data matrix after dimension reduction. Historically, The entries of your principal components (aka $\textbf{Z} = \textbf{US}$ ) are called variables. On the other hand, $\textbf{V}$ (which is $p\times k$ ) contains the Principal Loading Vectors and its entries are called the principal loadings. Given the properties of PCA, it's easy to show that $\textbf{Z}=\textbf{XV}$ . This means that: The principal components are derived by using the principal loadings as coefficients in a linear combination of your data matrix $\textbf{X}$ . Now that these definitions are out of the way, we'll look at sparsity. Most papers (or at least most that I've encountered), enforce sparsity on the principal loadings (aka $\textbf{V}$ ). The advantage of sparsity is that a sparse $\textbf{V}$ will tell us which variables (from the original $p$ -dimensional feature space) are worth keeping. This is called interpretability. There are also interpretations for enforcing sparsity on the entries of $\textbf{Z}$ , which I've seen people call "sparse variable PCA"", but that's far less popular and to be honest I haven't thought about it that much.
