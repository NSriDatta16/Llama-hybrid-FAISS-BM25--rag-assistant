[site]: crossvalidated
[post_id]: 340285
[parent_id]: 340258
[tags]: 
Usually between two such convolutional layers there is a pooling layer which reduces the size of each of these feature maps . So the deeper into the network you go the smaller the dimensions become. On the contrary, the number of feature maps is actually increased. This is done because we generally want a trade-off between high-resolution maps and more features. Furthermore, the deeper you go in a network the more abstract the features it can detect. Say you have a CNN for facial recognition. The first layers would detect simple geometric shapes (lines, curves, etc.), the subsequent layers would detect more high-level features from these first ones (eyes, noses etc.) while the final layers would combine these to detect the highest level features (faces) but in low-resolution. So it makes sense we would want the model to detect many of these high-level features because they have more detail, while it wouldn't make sense wanting to detect many low level features (because in these layers the model understands just lines). The above example can be seen in the image below:
