[site]: crossvalidated
[post_id]: 439472
[parent_id]: 
[tags]: 
Parameter in expectation subscript

There are a few questions/answers out there about subscripts involving a random variable ( $E_X[...]$ ) or a density ( $E_{f(X)}[...]$ ) in expectations. I like this one . But today I ran into a subscript involving a parameter of a density, and there is a point that confuses me. The book of A. A. Tsiatis defines m-estimators $\hat{\theta}_n$ as the solution of: $\sum_{i=1}^nm(Z_i, \hat{\theta}_n)=0$ where $Z_1,...,Z_n$ is an iid sample from $p_Z(z,\theta)$ , $\theta$ is $p$ -dimensional and, by definition, $E_{\theta}[m(Z, \theta)]=0^{p \times 1}$ . When I first saw that zero-expectation condition I thought: $\int m(Z, \theta)p_\theta(\theta)d\theta$ which is not weird in a Bayesian context, where parameters have distributions. But then I remembered that the likelihood in MLE is $L(\theta;z)\equiv p_Z(z,\theta)$ , so I figured the $E_{\theta}[\quad ]$ was shorthand for: $\int m(Z, \theta)p_Z(z,\theta)d\theta$ However, at the bottom of page 32 he says that this zero-expectation condition is equivalent to: $\int m(Z, \theta)p_Z(z,\theta)d\nu(z)=0 \quad \text{for all } \theta$ where $\nu(z)$ is the dominating measure. I would like to understand (1) what that subscript $\theta$ means in the expectation and (2) why the expectation is equivalent to this last integral + the "for all $\theta$ " condition.
