[site]: crossvalidated
[post_id]: 396139
[parent_id]: 
[tags]: 
Reinforcement learning based Q-learning for wireless routing

In the Q-learning method to get the optimal strategy, the update method is like the following: \begin{equation} Q(S,A) \leftarrow \ Q(S,A) + \alpha [R+\gamma~max_a(Q(s',a)) -Q(S,A)] \end{equation} If in Q-learning the state transition probabilities are fixed to 1 then during the policy exploration how do we consider the $\epsilon$ -greedy algorithm? As $\epsilon$ denotes the small probability during exploration and it must not be 1. Are the state transition probabilities and the probability during policy exploration different?
