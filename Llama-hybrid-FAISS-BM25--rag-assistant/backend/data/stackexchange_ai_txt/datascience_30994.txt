[site]: datascience
[post_id]: 30994
[parent_id]: 30989
[tags]: 
Whenever you have a convex cost function you are allowed to initialize your weights to zeros. The cost function of logistic regression and linear regression have convex cost function if you use MSE for, also RSS , linear regression and cross-entropy for logistic regression. The main idea is that for convex cost function you'll have just a single optimal point and it does not matter where you start, the starting point just changes the number of epochs to reach to that optimal point whilst for neural networks the cost function does not have just one optimal point. Take a look at here . About random initialization, you have to consider that you are not allowed to choose random weights which are too small or too big although the former was a more significant problem. If you choose random small weights you may have vanishing gradient problem which may lead to a network that does not learn. Consequently, you have to use standard initialization methods like He or Glorot , take a look at here and Understanding the difficulty of training deep feedforward neural networks . Also, take a look at the following question. How should the bias be initialized and regularized When to use (He or Glorot) normal initialization over uniform init? And what are its effects with Batch Normalization
