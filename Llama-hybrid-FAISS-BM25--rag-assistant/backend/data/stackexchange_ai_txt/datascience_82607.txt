[site]: datascience
[post_id]: 82607
[parent_id]: 82518
[tags]: 
A confusion matrix is a great way to score a classifier. There are some additional metrics that are simply summary stats from a confusion matrix. Some of these are: Accuracy - What percent of your predictions are correct. You can calculate this by the total number of true positives + true negatives divided by the number of data points (in your case, users). (TP + TN) / total predictions. Precision - What percent of the predicted positives are correct. So true positives divided by total number of predicted positives, TP / (TP + FP). Recall - What percent of the total positives did you catch? Number of true positives divided by total number of positives in the total population, TP / (TP + FN). True Negative Rate - What percent of predicted negatives are correct. TN / (TN + FP). In general, accuracy isn't used too often as it can be very misleading for skewed frequencies. Data science generally focusses on the other metrics I mentioned. There's almost always a tradeoff between precision and recall and understanding the use case lets you weigh the tradeoffs. For example, a cancer diagnostic blood test often favors recall over precision so it doesn't miss any true positives. A followup test (eg MRI) can often help distinguish the true positives from false positives, which I probably would guess is biased towards precision so that no patients undergo unnecessary surgeries. To best understand the tradeoff, sometimes a ROC curve is generated (plot of false positive rate vs recall). This wikipedia page is a great starting point: https://en.wikipedia.org/wiki/Receiver_operating_characteristic
