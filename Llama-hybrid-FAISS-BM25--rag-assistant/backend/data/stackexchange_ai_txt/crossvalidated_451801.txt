[site]: crossvalidated
[post_id]: 451801
[parent_id]: 
[tags]: 
What to do if the rate of misclassification in Bayesian model selection depends on the model parameter

I am currently having problems with a Bayesian model selection where the rate of misclassification seems to depend on the actual model parameter. I can create a simple minimal example of this effect, and I can understand why this happens, but I am not sure how to rectify this problem. Here is a minimal example: Assume two measurements $K_1 \sim Binom(v_1,N)$ and $K_2 \sim Binom(v_2,N)$ . The question I am interested in is whether $v_1 = 1-v_2$ or if these two variables are independent. Thus I have two models, one as above (with two parameters $\theta_{1,2}$ and the other is given by $(K_1+K_2) \sim Binom(\theta,2N)$ . Assuming flat priors on the parameters and equal prior likelihood for both models, I can derive the Bayes factor as $BF_{1,2}=\frac{B(N+K_1-K_2+1,\;N+K_2-K_1+1)}{B(K_1+1,\;N-K_1+1)B(K_2+1,\;N-K_2+1)}.$ However, when I simulate this only with the model where $v_1=1-v_2$ , I find that the rate of misclassification depends strongly on the actual probability I use in the simulation: library(tidyverse) library(ggplot2) N $K1 v,function(v) rbinom(1,N,v)) df $K2 v,function(v) rbinom(1,N,1-v)) df $BF K1,df$K2,N) ggplot(df,aes(x=v,y=BF)) + geom_jitter(alpha=0.01) + geom_line(data=df %>% group_by(v) %>% summarize(BF=mean(BF)), color="red") + scale_y_log10() df %>% group_by(v) %>% summarize(M=mean(BF % ggplot(aes(x=v,y=M)) + geom_line() Bayes factors: Rate of Misclassification: It is clear, why this happens. If the parameter becomes closer to $0.5$ , then these two models become indeed more similar to each other. However, is there any method to rectify this problem? Additional details: This problem appears as part of a broader analysis. In the complete analysis, I actually have two models, similar to $v_1=1-v_2$ vs. $v_1=v_2$ and I want to distinguish between those two models using MCMC. Because I am not sure these two models adequately describe my data, I also added a model where $v_1$ and $v_2$ are independent (basically to leave those examples as unclassified). However, during analysis, I found that the parameters are distributed differently for the two groups, so I have a higher rate of misclassification (as unclassified) in one of the groups vs. the other group. EDIT Some more details on why I think this happens: While in general one could just take this to be some kind of "identifiability issue", I don't think this adequately describes my problem. Of course, the more general two-parameter model can easily mimic the simpler model (they are nested), but in that case, the simpler model should be preferred because BF controls for model complexity. However, that only would explain, why the simpler model is chosen for some data produced by the more complex model (which is actually what I want, so not a problem), but not vice versa as in my example. Rather, it seems that for some parameters of the simpler model, the data itself becomes less indicative of any of the models. This also seems to be specifically related to the use of binomially distributed variables here, because in the case of the simpler model the variance increases when $v$ approaches $0.5$ . If I plot the posterior log-odds for each model, it becomes more clear what is happening here: Both models become less likely as $v$ approaches $0.5$ , but the drop is stronger for the simpler model, leading to a decrease of the BF.
