[site]: crossvalidated
[post_id]: 366606
[parent_id]: 365421
[tags]: 
Here is my own answer to my questions after I read An Introduction to Conditional Random Fields by Charles Sutton and Andrew McCallum. So this only represents a very limited and naive view of PGM, and is only limited to conditional random field (CRF) which I think represents the class of model people most care about. I appreciate it if anyone can comment on my mistakes. PGM works for "structured prediction", meaning we want a multi-label output from the model. One example is named-entity recognition in NLP tasks and image segmentation of computer vision, where we want one label for each word/pixel. However, the deep learning model should already outperforms PGM in these fields. The PGM is not so useful for tabular data, e.g. in Kaggle contest, because it is rare that we want some multiple-output from such dataset. I would say from reading about CRF, PGM is extremely weak in feature discovery. CRF is basically like a logistic regression for sequence with multiple output, and we need to generate thousands or even millions of features as rules MANUALLY , features like "word A does not want to follow word B". Therefore, CRF is more like a framework that can fit the linear coefficients of this logistic-like model and has a probabilistic interpretation which is not useful most of the time. So the general working practice of doing CRF is to generate millions of rules by hand, then throw these rules and data into a CRF library. See above. The graph is totally not useful for PGM except for inferring the conditional independence between variables. The authors of the famous models usually come up with a model, then convert it into an equivalent graph, instead of the other way around. On the other hand, the conditional independence is only useful for reducing the complexity of inference/learning, and a modeler who uses off-the-shelf library does not care about the independence.
