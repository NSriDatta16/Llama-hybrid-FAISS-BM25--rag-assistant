[site]: crossvalidated
[post_id]: 93117
[parent_id]: 
[tags]: 
Why can we use entropy to measure the quality of a language model?

I am reading the Foundations of Statistical Natural Language Processing >. It has the following statement about the relationship between information entropy and language model: ...The essential point here is that if a model captures more of the structure of a language, then the entropy of the model should be lower. In other words, we can use entropy as a measure of the quality of our models... But how about this example: Suppose we have a machine that spit $2$ characters, A and B, one by one. And the designer of the machine makes A and B has the equal probability. I am not the designer. And I try to model it through experiment. During a initial experiment, I see the machine split the following character sequence: A, B, A So I model the machine as $P(A)=\frac{2}{3}$ and $P(B)=\frac{1}{3}$. And we can calculate entropy of this model as : $$ \frac{-2}{3}\cdot\log{\frac{2}{3}}-\frac{1}{3}\cdot\log{\frac{1}{3}}= 0.918\quad\text{bit} $$ (the base is $2$ so the unit is bit) But then, the designer tell me about his design, so I refined my model with this more information. The new model looks like this: $P(A)=\frac{1}{2}$ $P(B)=\frac{1}{2}$ And the entropy of this new model is: $$ \frac{-1}{2}\cdot\log{\frac{1}{2}}-\frac{1}{2}\cdot\log{\frac{1}{2}} = 1\quad\text{bit} $$ The second model is obviously better than the first one. But the entropy increased. My point is, due to the arbitrariness of the model being tried, we cannot blindly say a smaller entropy indicates a better model. Could anyone shed some light on this?
