[site]: datascience
[post_id]: 43323
[parent_id]: 43320
[tags]: 
the lr (learning rate) parameter is the MOST influencial parameter for Adam (or any other optimization algorithm). I It does not mean step size... as that is not controlled by Adam but by the neural network and how you feed batches into it. So yes, lr is very, very important, and it is the learning rate, not the step size. UPDATE Actually as mentioned by OP, the learning rate and step size are the same thing (my bad, I confused it with the batch size). Generally, my recommendation is to try with learning rates of 0.0001, 0.001, 0.01 and 0.1. The rule of thumb is that a smaller learning rate provides a much stable learning curve... but a much slower one while a larger learning rate will converge quicker... but it will become unstable. So you will have to do a bit of exploration. I normally use 0.001 and 0.0001 but that depends a lot on the problem you are dealing with. Also, as compared with SGD, Adam includes momentum, essentially taking into account past behaviour (the further away such behaviour is in the past, the least relevant it is) that is controlled by the beta params, but I do not recommend to modify them, as normally it is not required. So, although Adam is a more stable learning method (and normally a quicker one) than SGD, the learning rate it is still the most fundamental parameter to play with.
