[site]: crossvalidated
[post_id]: 380161
[parent_id]: 324340
[tags]: 
TenaliRaman had some good points but he missed a lot of fundamental concepts as well. First it should be noted that the primary reason to use an AE-like framework is the latent space that allows us to compress the information and hopefully get independent factors out of it that represent high-level features of the data. An important point is that, while AEs can be interpreted as the nonlinear extension of PCA since "X" hidden units would span the same space as the first "X" number of principal components, an AE does not necessarily produce orthogonal components in the latent space (which would amount to a form of disentanglement). Additionally from a VAE, you can get a semblance of the data likelihood (although approximate) and also sample from it (which can be useful for various different tasks). However, if you just want likelihood, there are better (explicit, tractable) density models out there, and if you want to draw samples....well GANs or the explicit density models with exact likelihood are a better choice. The prior distribution imposed on the latent units in a VAE only contributes to model fitting due to the KL divergence term, which the [1] reference simply added a hyperparameter multiplier on that term and got a full paper out of it (most of it is fairly obvious). Essentially an "uninformative" prior is one which individually has a KL divergence close to zero and doesn't contribute much to the loss, meaning that particular unit is not used for reconstruction in the decoder. The disentanglement comes into play on a VAE naturally because, in the simplest case of multi-modal data, the KL divergence cost is lower by having a unique latent Gaussian to for each mode than if the model tries to capture multiple modes with a single Gaussian (which would diverge further from the prior as is penalized heavily by KL divergence cost) -- thus leading to disentanglement in the latent units. Therefore the VAE also lends itself naturally to most data sources because of the statistical implications associated with it. There are sparsity imposing frameworks for AE as well, but unfortunately I'm not aware of any paper out there that compares the VAE vs AE strictly on the basis of latent space representation and disentanglement. I'd really like to see something in that arena though -- since AEs are much easier to train and if they could achieve as good of disentanglement as VAEs in the latent space then they would obviously be preferred. On a related note, I've also seen some promise by ICA (and nonlinear ICA) methods, but the ones I've seen forced the latent space to be of the same dimension as the data, which is not nearly as useful as AEs for extracting high-level features.
