[site]: crossvalidated
[post_id]: 306892
[parent_id]: 304813
[tags]: 
The account of this "downhill simplex algorithm" in the original versions of Numerical Recipes is particularly lucid and helpful. I will therefore quote relevant parts of it. Here is the background: In one-dimensional minimization, it was possible to bracket a minimum... . Alas! There is no analogous procedure in multidimensional space. ... The best we can do is give our algorithm a starting guess; that is, an $N$ -vector of independent variables as the first point to try. The algorithm is then supposed to make its own way downhill through the unimaginable complexity of an $N$ -dimensional topography until it encounters an (at least local) minimum. The downhill simplex method must be started not just with a single point, but with $N+1$ points, defining an initial simplex. [You can take these points to be an initial starting point $P_0$ along with] $$P_i = P_0 + \lambda e_i\tag{10.4.1}$$ where the $e_i$ 's are $N$ unit vectors and where $\lambda$ is a constant which is your guess of the problem's characteristic length scale. ... Most steps just [move] the point of the simplex where the function is largest ("highest point") through the opposite face of the simplex to a lower point. ... Now for the issue at hand, terminating the algorithm. Note the generality of the account: the authors provide intuitive and useful advice for terminating any multidimensional optimizer and then show specifically how it applies to this particular algorithm. The first paragraph answers the question before us: Termination criteria can be delicate ... . We typically can identify one "cycle" or "step" of our multidimensional algorithm. It is then possible to terminate when the vector distance moved in that step is fractionally smaller in magnitude than some tolerance TOL . Alternatively, we could require that the decrease in the function value in the terminating step be fractionally smaller than some tolerance FTOL . ... Note well that either of the above criteria might be fooled by a single anomalous step that, for one reason or another, failed to get anywhere. Therefore, it is frequently a good idea to restart a multidimensional minimization routine at a point where it claims to have found a minimum. For this restart, you should reinitialize any ancillary input quantities. In the downhill simplex method, for example, you should reinitialize $N$ of the $N+1$ vertices of the simplex again by equation $(10.4.1)$ , with $P_0$ being one of the vertices of the claimed minimum. Restarts should never be very expensive; your algorithm did, after all, converge to the restart point once, and now you are starting the algorithm already there. [Pages 290-292.] The code accompanying this text in Numerical Recipes clarifies the meaning of "fractionally smaller": the difference between values $x$ and $y$ (either values of the argument or values of the function) is "fractionally smaller" than a threshold $T\gt 0$ when $$\frac{|x| - |y|}{f(x,y)} = 2\frac{|x|-|y|}{|x| + |y|} \lt T\tag{1}$$ with $f(x,y) = (|x|+|y|)/2$ . The left hand side of $(1)$ is sometimes known as the "relative absolute difference." In some fields it is expressed as a percent, where it is called the "relative percent error." See the Wikipedia article on Relative change and difference for more options and terminology. Reference William H. Press et al. , Numerical Recipes: The Art of Scientific Computing. Cambridge University Press (1986). Visit http://numerical.recipes/ for the latest editions.
