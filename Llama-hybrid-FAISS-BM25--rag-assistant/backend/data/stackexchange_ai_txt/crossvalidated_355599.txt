[site]: crossvalidated
[post_id]: 355599
[parent_id]: 355533
[tags]: 
I am assuming that a) you do not question why do we have TensorFlow itself, and understand its value; and b) you only question why wouldn't TF use np.array class instead of creating a new class tf.Tensor. When you write any kind of extensive framework such as TensorFlow, you end up creating many types and even type hierarchies. Creation of every single one of them could potentially be challenged, of course. However, it doesn't make a sense to do it individually. You have to consider the context of the framework. For instance, sometimes you end up creating almost exact copies of classes from the already existing package. Why not simply import the package? There could be many reasons including that maybe you don't want to drag entire package just to get a few classes from it, so you duplicate them etc. Therefore, from software engineering point of view it is almost easy to discard your question as lacking the context. However, I'll try to answer it because it touches the central data structure of TensorFlow framework. There are many ways to answer the question, I'll only pick one. Take a look at the following numpy code in Python: import numpy as np np.array([[1,2],[3,4]])**2 Here's the output it produces, an element-wise square of the 2x2 array: array([[ 1, 4], [ 9, 16]], dtype=int32) Next, look at a seemingly equivalent TensorFlow code: import tensorflow as tf t = tf.constant([[1,2],[3,4]]) t = t**2 t Let's look at its output closely and compare to numpy's: This is not the square of the array... yet. This tensor is linked to the description of what will be ultimately calculated by the computation graph that we created above. It's a very simple graph, has only a couple of nodes: input array and the square operation. Nevertheless it's a graph. Let's now evaluate it as follows: sess = tf.Session() sess.run(t) Here's the output: array([[ 1, 4], [ 9, 16]], dtype=int32) Now, this looks like an array that we expected! We can make this even more explicit with this TF code: import tensorflow as tf import numpy as np x = tf.placeholder(tf.float32, shape=(2,2)) t = x**2 t Here, we get the Tensor again, and it can't even be calculated eagerly (thanks, @Tim)! We need to plug the actual array instead of the placeholder to get a results as follows: sess = tf.Session() sess.run(t,feed_dict={x: np.array([[1,2],[3,4]])}) getting the desired output: array([[ 1., 4.], [ 9., 16.]], dtype=float32) So, this was the long way to say that Tensor class in TensorFlow is a lot more than just a numpy array. It's almost like a variable vs. value comparison in this context.
