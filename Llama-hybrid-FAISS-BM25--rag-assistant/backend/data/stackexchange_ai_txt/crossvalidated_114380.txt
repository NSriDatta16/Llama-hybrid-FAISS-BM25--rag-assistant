[site]: crossvalidated
[post_id]: 114380
[parent_id]: 
[tags]: 
Is comparing standard deviation a valid way to evaluate the usefulness of a sample?

I have data for about 1,800 email messages, and am building a tool in Excel that enables my team to create "benchmarks" on-the-fly -- they can select various characteristics of a message (for example, audience size) and see the mean performance of similar past messages. I use COUNTIF to indicate the number of similar messages being averaged, with the idea that the mean performance of one or two messages is less reliable than five or six (or more). I'm wondering if there is a meaningful way to use standard deviation to indicate whether the sample they've generated is reliable. For example, is it useful to compare the standard deviation of the open rate for the entire population to the standard deviation of the open rate for the sample they're looking at? (It has been several years since my last statistics class, and I'm just starting to piece this stuff together again.)
