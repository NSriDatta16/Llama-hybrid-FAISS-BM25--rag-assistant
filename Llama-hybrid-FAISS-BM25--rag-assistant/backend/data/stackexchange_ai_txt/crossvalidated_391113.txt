[site]: crossvalidated
[post_id]: 391113
[parent_id]: 243384
[tags]: 
This is the answer for everybody who wonders about the clean, structured math behind it (i.e. if you belong to the group of people that knows what a random variable is and that you must show or assume that a random variable has a density then this is the answer for you ;-)): First of all we need to have that the Markov Decision Process has only a finite number of $L^1$ -rewards, i.e. we need that there exists a finite set $E$ of densities, each belonging to $L^1$ variables, i.e. $\int_{\mathbb{R}}x \cdot e(x) dx for all $e \in E$ and a map $F : A \times S \to E$ such that $$p(r_t|a_t, s_t) = F(a_t, s_t)(r_t)$$ (i.e. in the automata behind the MDP, there may be infinitely many states but there are only finitely many $L^1$ -reward-distributions attached to the possibly infinite transitions between the states) Theorem 1 : Let $X \in L^1(\Omega)$ (i.e. an integrable real random variable) and let $Y$ be another random variable such that $X,Y$ have a common density then $$E[X|Y=y] = \int_\mathbb{R} x p(x|y) dx$$ Proof : Essentially proven in here by Stefan Hansen. Theorem 2 : Let $X \in L^1(\Omega)$ and let $Y,Z$ be further random variables such that $X,Y,Z$ have a common density then $$E[X|Y=y] = \int_{\mathcal{Z}} p(z|y) E[X|Y=y,Z=z] dz$$ where $\mathcal{Z}$ is the range of $Z$ . Proof : \begin{align*} E[X|Y=y] &= \int_{\mathbb{R}} x p(x|y) dx \\ &~~~~\text{(by Thm. 1)}\\ &= \int_{\mathbb{R}} x \frac{p(x,y)}{p(y)} dx \\ &= \int_{\mathbb{R}} x \frac{\int_{\mathcal{Z}} p(x,y,z) dz}{p(y)} dx \\ &= \int_{\mathcal{Z}} \int_{\mathbb{R}} x \frac{ p(x,y,z) }{p(y)} dx dz \\ &= \int_{\mathcal{Z}} \int_{\mathbb{R}} x p(x|y,z)p(z|y) dx dz \\ &= \int_{\mathcal{Z}} p(z|y) \int_{\mathbb{R}} x p(x|y,z) dx dz \\ &= \int_{\mathcal{Z}} p(z|y) E[X|Y=y,Z=z] dz \\ &~~~~\text{(by Thm. 1)} \end{align*} Put $G_t = \sum_{k=0}^\infty \gamma^k R_{t+k}$ and put $G_t^{(K)} = \sum_{k=0}^K \gamma^k R_{t+k}$ then one can show (using the fact that the MDP has only finitely many $L^1$ -rewards) that $G_t^{(K)}$ converges and that since the function $\sum_{k=0}^\infty \gamma^k |R_{t+k}|$ is still in $L^1(\Omega)$ (i.e. integrable) one can also show (by using the usual combination of the theorems of monotone convergence and then dominated convergence on the defining equations for [the factorizations of] the conditional expectation) that $$\lim_{K \to \infty} E[G_t^{(K)} | S_t=s_t] = E[G_t | S_t=s_t]$$ Now one shows that $$E[G_t^{(K)} | S_t=s_t] = E[R_{t} | S_t=s_t] + \gamma \int_S p(s_{t+1}|s_t) E[G_{t+1}^{(K-1)} | S_{t+1}=s_{t+1}] ds_{t+1}$$ using $G_t^{(K)} = R_t + \gamma G_{t+1}^{(K-1)}$ , Thm. 2 above then Thm. 1 on $E[G_{t+1}^{(K-1)}|S_{t+1}=s', S_t=s_t]$ and then using a straightforward marginalization war, one shows that $p(r_q|s_{t+1}, s_t) = p(r_q|s_{t+1})$ for all $q \geq t+1$ . Now we need to apply the limit $K \to \infty$ to both sides of the equation. In order to pull the limit into the integral over the state space $S$ we need to make some additional assumptions: Either the state space is finite (then $\int_S = \sum_S$ and the sum is finite) or all the rewards are all positive (then we use monotone convergence) or all the rewards are negative (then we put a minus sign in front of the equation and use monotone convergence again) or all the rewards are bounded (then we use dominated convergence). Then (by applying $\lim_{K \to \infty}$ to both sides of the partial / finite Bellman equation above) we obtain $$ E[G_t | S_t=s_t] = E[G_t^{(K)} | S_t=s_t] = E[R_{t} | S_t=s_t] + \gamma \int_S p(s_{t+1}|s_t) E[G_{t+1} | S_{t+1}=s_{t+1}] ds_{t+1}$$ and then the rest is usual density manipulation. REMARK: Even in very simple tasks the state space can be infinite! One example would be the 'balancing a pole'-task. The state is essentially the angle of the pole (a value in $[0, 2\pi)$ , an uncountably infinite set!) REMARK: People might comment 'dough, this proof can be shortened much more if you just use the density of $G_t$ directly and show that $p(g_{t+1}|s_{t+1}, s_t) = p(g_{t+1}|s_{t+1})$ ' ... BUT ... my questions would be: How come that you even know that $G_{t+1}$ has a density? How come that you even know that $G_{t+1}$ has a common density together with $S_{t+1}, S_t$ ? How do you infer that $p(g_{t+1}|s_{t+1}, s_t) = p(g_{t+1}|s_{t+1})$ ? This is not only the Markov property: The Markov property only tells you something about the marginal distributions but these do not necessarily determine the whole distribution, see e.g. multivariate Gaussians!
