[site]: crossvalidated
[post_id]: 593277
[parent_id]: 592911
[tags]: 
The autoencoder learns by backpropagation, a form of batched stochastic gradient descent (SGD). That means the weight updates from the batches happen sequentially. Now, the paper you refer to suggests parallelizing this by partitioning the data and running parallelized SGD for autoencoders. For this, they use a method published in a 2010 neurips paper. Thus, when they say that the autoencoder has a serial implementation, they simply mean that it doesn't use parallelized SGD, which is precisely what the authors propose to do and what their paper is all about.
