[site]: crossvalidated
[post_id]: 615448
[parent_id]: 
[tags]: 
Is model performance estimation through Cross Validation in Caret done on the training set, test set, or the whole dataset after fitting of model?

Context: -dataset with np ( 200x80) -no categorical variable. My goal is to estimate the performance of machine learning methods which include feature selection on my samples in order to use the best one for further predictions. I perform my analysis in R with package caret, with supervised ML methods (LASSO, Stepwise etc...). However, i don't understand the whole CV performance assessing in Caret: With the Caret::resample function, i get the average of MSE, R2 and MAE. These parameters could each be calculated separately for the training set or for the test set, if i wanted to. What interests me, is how the ML method performs when generalized to unseen data. My question is therefore: At each turn of the CV process, which parts (training, or test) of the dataset are used to calculate these criterions ? The most obvious thing would be that the test set is used to calculate the criterions previously mentioned, but i don't find anything that can guarantee it . I have tried to run separately the glmnet with cross validation both with the original R package called "glmnet" (glmnet::cv.glmnet() ), but also with what should be the same method included in Caret (same hyperparameters, same number of folds). However, i get different R2, MSE, MAE from both ways of running glmnet. This is what explain the origins of my question. I would rather compare each ML method in Caret in order to ensure the reliability of my comparisons (each ML method running on the same folds) than having to load each package separatly and run the ML methods individually.
