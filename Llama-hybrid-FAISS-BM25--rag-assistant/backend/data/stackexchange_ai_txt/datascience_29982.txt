[site]: datascience
[post_id]: 29982
[parent_id]: 
[tags]: 
Technique to make NN (specifically autoencoders) more robust by artifcally making input data more sparse

I believe some month ago I have read somewhere that autoencoders can respond better to sparse input data when trained with such. For example when the training data is not sparse, but many features of the test data are zero. Apparently randomly setting some features of the training data to zero can help to perform the model better on the test set. I desperatly tried to find the book/paper where I read it again, but I was not able to and I cannot remember the name for this. Does somebody know what this technique is called?
