[site]: crossvalidated
[post_id]: 289182
[parent_id]: 
[tags]: 
How do we use cross-validation to get hyper parameters for soft-margin Gaussian kernel SVM?

I was reading this How to select kernel for SVM? but as I am relatively new to the applied techniques in ML (only familiar with some theory) I may need some explaining on the answers. Specifically, my task is to solve for $C$ (the slack variable in soft-margin SVM) and $\sigma$ (the standard deviation of the Gaussian RBF kernel I am applying). Many of the comments just say (use cross-validation) but I would like a specific algorithm to do so. From my understanding, (I'm trying to do $k$-fold cross validation) we split the training set into $k$ roughly equal pieces (by shuffling or some other randomization) and then figuring out the best (in this case, HYPER, not just regular $\alpha$ and $\beta$ parameters we get from vanilla SVM) parameters. But HOW we get our best parameters (in this case $C$ and $\sigma$) is confounding my research. Currently, I believe what I am doing, after reading the comments in that question thread, is "grid-search", where I am listing orders of magnitude for $C$ and linearly spacing my variance parameter $\sigma$. But I don't like this method and am wondering if there is some way to train for our values with cross-validation that is not this brute force method. This takes up a lot of time ($O(|C\times\sigma|)$, where the cardinality operator represents how many values of each we test in our grid search) and goes through bad values, while skipping over good or optimal values. So is there any specific way to solve for an approximate optimal slack and variance parameter? Detailed descriptions of any training algorithms during this cross validation phase would be appreciated. Thanks
