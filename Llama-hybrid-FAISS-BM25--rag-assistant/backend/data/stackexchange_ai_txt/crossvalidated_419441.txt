[site]: crossvalidated
[post_id]: 419441
[parent_id]: 419433
[tags]: 
Until you're ready to train your full model on all data, you should treat your out-of-sample data sets as nonexistent when you're doing work for the in-sample data. If you have 1000 observations split into 5 sets of 200 for 5-fold CV, you pretend like one of the folds doesn't exist when you work on the remaining 800 observations. If you want to run PCA, for instance, you run PCA on the 800 points and then apply the results of that diagonalization to the out-of-sample 200 (I believe that the sklearn functions do this automatically). But you certainly don't diagonalize the covariance matrix for the entire set of 1000 until you're training your "production" model, at which point, you've already decided on a model (neural network with X layers of Y neurons, or linear regression with L2 regularization for some $\lambda$ that you optimized by cross validation, for instance). In your case, let's stick with this idea of doing 5-fold CV on 1000 observations. You have your five folds of 200 observations each: $F_1,\dots,F_5$ . The first fold you leave out if $F_1$ . Do your feature selection and model development on $F_2,\dots,F_5$ , pretending like $F_1$ doesn't exist. Let's say you select features $X_1$ , $X_4$ and $X_7$ . Train your model on those features. Now apply your model to the out-of-sample data in $F_1$ . Let's say you get an accuracy of 75%. Now do it again, leaving out $F_2$ and training on the rest. Let's say you select features $X_2$ , $X_4$ , and $X_7$ . Train your model on the 800 observations that exclude $F_2$ . Accuracy out-of-sample is 77%. Now do it again with $F_3$ left out, then again with $F_4$ left out, and again with $F_5$ left out. Your accuracy scores are, say, 80%, 65%, and 79%. At no point does the feature selection process see the out-of-sample data! That's always hidden from model development in order to simulate the real application of machine learning where it is supposed to work on data that the developer may not even know exists. For instance, Siri should be able to figure out a baby's first words, even though Siri has never been trained on other speech by the child let alone that exact piece of audio. When you do show the all five folds to your model is when you want to make a "production" model. That occurs when you are confident that you have a general model that will work well, but you still get the same sort of issue where your in-sample data are $F_1,\dots,F_5$ and your out-of-sample data are whatever happens out in the world (a baby's first words for a speech recognition tool, for instance). With all of that stated, the answer to your question: "Should i perform a feature selection on each fold (for iteration) and then train my classifier with the reduced features?" Not for each fold, but for each set of in-sample data that leaves out a fold for out-of-sample testing. Edit: I think I'm using "fold" incorrectly. What I'm calling $F_1$ etc are not the folds. They're partitions of the data. A fold would then be $F_1,\dots,F_4$ as training data and $F_5$ as validation data. So I say that the answer to your question is yes.
