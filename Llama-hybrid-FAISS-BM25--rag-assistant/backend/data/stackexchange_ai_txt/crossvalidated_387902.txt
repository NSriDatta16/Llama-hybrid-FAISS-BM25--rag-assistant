[site]: crossvalidated
[post_id]: 387902
[parent_id]: 
[tags]: 
Differences in calibration plots for machine learning models

I'm using machine learning methods in R for descriptive regression modelling of a small dataset. I have fit random forest (randomForest), unbiased random forest (cforest) and boosted regression trees (gbm) using caret with 10 fold cross-validation and 5 repeats. I've tuned the hyperparameters for each model and achieved the same cross-validation RMSE accuracy. However, only the random forest model gives a satisfactory fit for the training data. Below are the calibration plots and fit stats (the red, horizontal line in each is the observed mean corresponding to the null RMSE). I have used predict() on the caret models and am confident that these are predictions are for the training set. Predictor variable importance statistics and partial dependence plots are qualitatively similar for all three models (except that cforest does more to reduce bias). So, while these three models detected similar patterns in the data, how come two of the three fit poorly to the training data? My choice was random forest, but I haven't found an effective method for correcting for bias in the importance statistics for that model. UPDATE: The following code and plots provide examples using the iris dataset. Note that, like my data above, randomForest RMSEtrain is consistently c. 0.6 times RMSEtrain for cforest and gbm models. I think randomForest is overfitting and therefore comparing calibration plots and RMSEtrain among different models is misleading? data(iris) ## models without added noise # null model RMSEnull $results[order(mymod.rf$ results $RMSE), ]), 3) RMSEholdout results[order(mymod.rf $results$ RMSE), ] RMSEholdout $results[order(mymod.cf$ results $RMSE), ]), 3) RMSEholdout results[order(mymod.cf $results$ RMSE),] RMSEholdout $results[order(mymod.gbm$ results $RMSE), ]), 3) RMSEholdout results[order(mymod.gbm $results$ RMSE), ] RMSEholdout
