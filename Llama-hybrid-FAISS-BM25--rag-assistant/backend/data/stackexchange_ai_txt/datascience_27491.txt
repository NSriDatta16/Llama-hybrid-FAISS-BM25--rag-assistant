[site]: datascience
[post_id]: 27491
[parent_id]: 
[tags]: 
decision rules for each feature (binary classification)

I have a collection of 10 features (all numerical) and a single binary outcome variable. I need to train a binary classification model, find the best features and compute thresholds for each feature. To find the best subset of features is easy, I use logistic regression with L1 penalisation and it works nicely. However, the next step is to find thresholds for each subselected feature: if the values of a feature i , j ,... k are above/below than some numerical values, than chances to be in the class A (rather than in B) are higher. FOR EXAMPLE Consider, we found that out of 10 original features, there are L1 leaves only 3: F1, F4,F7. And if you take three unseen (new) data with some values {F1_i, F4_i, F7_i}, where F1 > 1.23 F4 2.74 Then the new data point {F1_i, F4_i, F7_i} belongs to class A. Here {1.23, 7.15, 2.74} are thresholds. I tried to explain the problem as clear as possible, but if it isn't, please let me know. QUESTION What will be the best approach to solve this problem? How to compute thresholds?
