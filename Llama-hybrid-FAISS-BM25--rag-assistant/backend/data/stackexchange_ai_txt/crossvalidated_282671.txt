[site]: crossvalidated
[post_id]: 282671
[parent_id]: 240305
[tags]: 
Some people interpret the dropout enabled neural network as an approximation of Bayesian Neural Network. And we can see this problem from the Bayesian perspective or treat such networks as stochastic artificial neural networks. Artificial neural network An artificial neural network maps some inputs/features to the output/predictions, which can be simplified as the following process: $l_0 = x, $ $l_i = nl_i(W_il_{i-1}+b_i)\hspace{1cm} \forall i \in [1, n],$ $y=l_n.$ where $nl_i$ represents the non-linear activation function in the ith layer. Stochastic artificial neural networks There are two methods to convert a traditional neural network into a stochastic artificial neural network, simulating multiple possible models $\theta$ with their corresponding probability $p(\theta)$ distribution: 1) give the network stochastic activation(depicted below on the left), 2) or stochastic weights/coefficients(on the right). The Dropout model In this awesome article: What My Deep Model Doesn't Know... Yarin Gal views it as a stochastic network: Notice that the dropout mechanism applied on $W_1$ works on the X layer and the dropout mechanism applied on $W_2$ works on the $\sigma$ layer. And the process (with n layers) can be formulated as this: $l_0 = x, $ $z_{i,j} \sim \text{Bernouilli}(p_i)\hspace{1cm} \forall i \in [1, n],$ $l_i = nl_i((l_{i-1} \cdot \text{diag} (z_i))W_i +b_i)\hspace{1cm} \forall i \in [1, n],$ $y=l_n.$ where $l_{i-1} \cdot \text{diag} (z_i)$ means that we randomly zero out some elements of the input( preceding layer ) with probability $1-p_i$ . TL;DR Then normally we apply the dropout before the activation to dropout the input elements in the preceding layer. Here is an illustration of the dropout machanism. References: Hands-on Bayesian Neural Networks - a Tutorial for Deep Learning Users What My Deep Model Doesn't Know...
