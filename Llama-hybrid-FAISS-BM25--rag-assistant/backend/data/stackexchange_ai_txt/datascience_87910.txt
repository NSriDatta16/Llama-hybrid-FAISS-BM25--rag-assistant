[site]: datascience
[post_id]: 87910
[parent_id]: 87882
[tags]: 
The other answers make sense but I would be more categorically negative about the idea: Is this approach a correct approach, or logical with respect to machine learning principles ? No, it's not. The parameters of a ML model (whether supervised or unsupervised) are estimated using a particular set of features designed as the input for the problem. Changing the input (features) changes the definition of the problem as well, so the solution (model) obtained for the first problem is unlikely to work as well on the new problem. Will it affect the model accuracy and if yes then how ? It's very likely to decrease the performance of the model. Normally the features used in the model are chosen because they are likely to "help" the model. If they are "helpful" then the model will rely on them, and therefore removing them will cause the model to fail. If I have to provide features B and C, then can I populate them with zeros and then provide it to trained model for making predictions. You sure can, but it's a bad idea. Will the action taken in step 3) affect the model accuracy, if yes and then why and how ? Same as point 2: the performance is very likely to drop. Replacing valuable indications for the model with arbitrary values is the equivalent of randomly switching blood samples in a biology lab, it causes wrong tests and wrong results. Another way to look at it: if what you propose was possible, it would mean that it's possible in general to remove one feature and obtain the same performance. So let's say we have performance P with features (A,B,C,D,E), and when we remove A we still have performance P. Then by our assumption we can also remove B and still obtain performance P, and then do it again until we obtain a model with 0 features which has performance P. This is a contradiction, so the hypothesis that it's possible to remove a feature without losing performance is false.
