[site]: datascience
[post_id]: 10724
[parent_id]: 
[tags]: 
How does PV-DBOW (doc2vec) work?

The explanation from the official doc2vec paper "Distributed Representations of Sentences and Documents" for PV-DBOW is as follows: Another way is to ignore the context words in the input, but force the model to predict words randomly sampled from the paragraph in the output. In reality, what this means is that at each iteration of stochastic gradient descent, we sample a text window, then sample a random word from the text window and form a classification task given the Paragraph Vector. According to the paper the word vectors are not stored and PV-DBOW is said to work similar to skip gram in word2vec. Skip-gram is explained in word2vec Parameter Learning . In the skip gram model the word vectors are mapped to the hidden layer. The matrix that performs this mapping is updated during the training. In PV-DBOW the dimension of the hidden layer should be the dimension of one paragraph vector. When I want to multiply the word vector of a sampled example with the paragraph vector they should have the same size. The original representation of a word is of size (vocabulary size x 1). Which mapping is performed to get the right size (paragraph dimension x 1) in the hidden layer. And how is this mapping performed when the word vectors are not stored? I assume that word and paragraph representation should have the same size in the hidden layer because of equation 26 in 2
