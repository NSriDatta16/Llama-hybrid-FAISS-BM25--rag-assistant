[site]: crossvalidated
[post_id]: 137134
[parent_id]: 137130
[tags]: 
Monte Carlo integration is one form of numerical integration which can be much more efficient than, e.g., numerical integration by approximating the integrand with polynomials. This is especially true in high dimensions, where simple numerical integration techniques require large numbers of function evaluations. To compute the normalization constant $p(D)$, we could use importance sampling , $$p(D) = \int \frac{q(\theta)}{q(\theta)} p(\theta)p(D \mid \theta) \, d\theta \approx \frac{1}{N} \sum_n w_n p(\theta_n)p(D \mid \theta_n),$$ where $w_n = 1/q(\theta_n)$ and the $\theta_n$ are sampled from $q$. Note that we only need to evaluate the joint distribution at the sampled points. For the right $q$, this estimator can be very efficient in the sense of requiring very few samples. In practice, choosing an appropriate $q$ can be difficult, but this is where MCMC can help! Annealed importance sampling (Neal, 1998) combines MCMC with importance sampling. Another reason why MCMC is useful is this: We usually aren't even that interested in the posterior density of $\theta$, but rather in summary statistics and expectations , e.g., $$\int p(\theta \mid D) f(\theta) \, d\theta.$$ Knowing $p(D)$ does not generally mean we can solve this integral, but samples are a very convenient way to estimate it. Finally, being able to evaluate $p(D \mid \theta)p(\theta)$ is a requirement for some MCMC methods, but not all of them (e.g., Murray et al., 2006 ).
