[site]: crossvalidated
[post_id]: 524769
[parent_id]: 524750
[tags]: 
Start with the Why don't linear regression assumptions matter in machine learning? thread. Assumptions for statistical models are needed for inference, while with machine learning models we care about making accurate predictions. Even in the case of linear regression, there are misconceptions about its assumptions and their importance, as you can learn from the What is a complete list of the usual assumptions for linear regression? thread. Homoskedasticity in linear regression is needed for calculating confidence intervals and running hypothesis tests, not the things you care about or even are possible to do with random forest. If you don't care about those, you can even use linear regression if the assumption is not met. The homoskedasticity assumption comes from the distributional assumption about independent and identically distributed noise $\varepsilon_i \sim \mathcal{N}(0, \sigma^2)$ . The random forest does not make any distributional assumptions. So nothing is broken and nothing needs fixing. You may care about the predictions being biased and may want to do things like gathering more data (especially for the values 70), feature engineering, hyperparameter tuning, ensembling models, etc, to improve the predictions, but it does not have anything to do with homoskedasticity.
