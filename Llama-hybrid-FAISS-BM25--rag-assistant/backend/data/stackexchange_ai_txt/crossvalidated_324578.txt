[site]: crossvalidated
[post_id]: 324578
[parent_id]: 
[tags]: 
Why can minimizing $\|w\|$ be solved by minimizing $\frac{\|w\|^2}{2}$?

I was watching the MIT open course on machine learning. In the session on SVM, the professor derived that the margin is $\frac{1}{\|w\|}$. However, the professor then said for the convenience of mathematics, to maximize $\frac{1}{\|w\|}$, which implies to minimize $\|w\|$, is minimizing $\frac{\|w\|^2}{2}$. What is the rationale behind this substitution?
