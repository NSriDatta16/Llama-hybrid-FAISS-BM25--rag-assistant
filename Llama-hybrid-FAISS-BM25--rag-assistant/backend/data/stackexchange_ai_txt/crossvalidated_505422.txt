[site]: crossvalidated
[post_id]: 505422
[parent_id]: 
[tags]: 
Feature selection with L1 Logistic regression, shall I keep negative coefficients?

I am running an L1 LG for feature selection. I have 800 features, I scaled the features, and I would like to know which features should I keep: lr_l1 = LogisticRegression(C = LR_penalty.C, class_weight='balanced',penalty='l1', random_state=42, solver='liblinear').fit(X_train, y_train) importance = lr_l1.coef_[0] If I consider to keep the features which have an ABS value of the coefficient >1e-5 ( np.abs(importance)>=1e-5 ), the result is: My question is: is this correct? In other words, should I remove only the features where the coef_ is close to zero or also those with a negative coef_ ( importance)>=1e-5 )? My final goal is to have good recall and F1 score, since I need to minimise False Negatives.
