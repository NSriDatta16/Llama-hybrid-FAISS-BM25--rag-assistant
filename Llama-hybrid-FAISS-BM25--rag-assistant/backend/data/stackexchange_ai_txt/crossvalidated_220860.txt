[site]: crossvalidated
[post_id]: 220860
[parent_id]: 
[tags]: 
Increasing the learning rate on loss function saturation

I'm currently reading about neural networks, specifically how loss functions saturation can cause problems. During my studies, I was curious if one could remedy the problem during training of neural networks by detecting saturation (such as comparing the gradients to the previous training iteration) and just increase the learning rate in a proportional manner so that learning still can be done effectively. Unfortunately, my (admittately limited) research on google has not come up with any method using this approach. Is there a reason why this is not being done?
