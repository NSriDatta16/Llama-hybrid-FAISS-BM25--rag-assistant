[site]: crossvalidated
[post_id]: 223653
[parent_id]: 222883
[tags]: 
Adding more features helps but the benefit quickly become marginal after a lot of features were added. That's one reason why tools like PCA work: a few components capture most variance in the features. Hence, adding more features after some point is almost useless. On the other hand finding the right functional for ma of the feature is always a good idea. However, if you don't have a good theory it's hard to come up with a correct function, of course. So, adding layers is helpful as form of a brute force approach. Consider a simple case: air drag of a car. Say, we didn't know the equation: $$f\sim C\rho A v^2/2$$ where $A$ - a crossectional area of a car, $\rho$ - air density, and $v$ - velocity of a car. We could figure that car measurements are important and add them as features, velocity of a car will go in too. So we keep adding features, and maybe add air pressure, temperature, length, width of a car, number of seats, etc. We'll end up with a model like $$f\sim \sum_i\beta_i x_i$$ You see how these features are not going to assemble themselves into the "true" equation unless we add all interactions and polynomials. However, if the true equation wasn't conveniently polynomial, say it had exponents or other weird transcendental functions, then we'd have no chance to emulate it with expanding feature set or widening the network. However, making the network deeper would easily get you to the equation above with just two layers. More complicated functions would need more layer, that's why deepening the number of layers could be a way to go in many problems.
