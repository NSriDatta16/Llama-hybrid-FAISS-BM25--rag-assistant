[site]: crossvalidated
[post_id]: 626496
[parent_id]: 617124
[tags]: 
I think the accepted answer is wrong. The described optimization trick using caching really only works for the first attention layer. At step $T$ you have the same $T-1$ tokens, but after the first attention layer the output embeddings for all tokens will be different, so you have to re-compute everything from there on. No caching is possible for the deeper attention layers. There is actually a different reason for separating these two matrices. As OP remarked the matrices $W_Q$ and $W_K$ are only ever used in order to compute the attention scores: $x^TW_Q^TW_Kx$ . You could easily use a single matrix $W_{QK} = W_Q^TW_K$ and compute the attention scores as $x^TW_{QK}x$ . Since the token embeddings have dimension $d_{model}$ , the matrix $W_{QK}$ will have dimension $d_{model} \times d_{model}$ . In the multi-head attention layer your queries and keys are mapped to a $d_k$ -dimensional space, where $d_k = d_{model} // n$ . Now, the $W_Q$ and $W_K$ matrices have dimension $d_{model} \times d_{k}$ , greatly reducing the number of trainable parameters of the model. Effectively we force the matrix $W_{QK}$ to be not just any matrix, but a matrix with rank $d_k$ . You might think that reducing the number of parameters would reduce the performance of the model, but having a full rank $W_{QK}$ matrix would indirectly result in the query and key embeddings having the same dimension $d_{model}$ . However, as already mentioned, queries and keys are only ever used to compute the attention score (a single number). The smaller $d_k$ -dimensional space could easily do the job, and it would prevent the model from overfitting.
