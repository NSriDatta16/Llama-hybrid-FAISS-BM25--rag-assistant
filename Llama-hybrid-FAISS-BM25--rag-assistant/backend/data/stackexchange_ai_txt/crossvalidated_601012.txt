[site]: crossvalidated
[post_id]: 601012
[parent_id]: 600851
[tags]: 
This is a partial answer to be completed, I posted it just for clarification and drawing more attention. I have consulted the ESL section. Let me first clarify the notations. The problem is 1D function estimation so the training data is $(x_i, y_i): i = 1, \ldots, n$ (as I pointed out in the comment, using $N$ to denote the number of observations is an unfortunate confusion, even in ESL the spline matrix is boldfaced. So here I will still use $n$ to denote the number of observations). The spline matrix $N$ is defined as \begin{align} N = \begin{bmatrix} N_1(x_1) & N_2(x_1) & \cdots & N_n(x_1) \\ N_1(x_2) & N_2(x_2) & \cdots & N_n(x_2) \\ \vdots & \vdots & \ddots & \vdots \\ N_1(x_n) & N_2(x_n) & \cdots & N_n(x_n) \end{bmatrix}. \tag{1} \end{align} Evidently, if $x_i = x_j$ for some $i \neq j$ , then clearly $N$ is not invertible (because then the $i$ -th row and the $j$ -th row are identical). So in order $\operatorname{rank}(S_\lambda) = n$ (a claim made by ESL, p.153), $x_1, \ldots, x_n$ must be distinct (without loss of generality, assume $x_1 ). According to the author: $N_j(x)$ are an $n$ -dimensional set of basis functions for representing this family of natural splines... By calling $\{N_1(x), \ldots, N_n(x)\}$ "basis functions" (to be fair, doing so without first proving they really formed a basis of a vector space -- which was not clearly stated in advance, is loose), they are necessarily linearly independent in the vector space $V$ of spline functions (known as "spline space", more accurately, given knots $x_1 and only cubic splines are under consideration, I think $V$ is a real vector space spanned by $\{1, x, x^2, x^3, (x - x_1)_+^3, \ldots, (x - x_n)_+^3\}$ ). As you are interested in, this statement, of course requires a mathematical proof, which the text omitted. Recall $N_j(x)$ is defined as (Equation (5.4) in ESL): \begin{align} N_1(x) = 1, N_2(x) = x, N_{k + 2}(x) = d_k(x) - d_{n - 1}(x), k = 1, \ldots, n - 2, \tag{2} \end{align} where \begin{align} d_k(x) = \frac{(x - x_k)_+^3 - (x - x_n)_+^3}{x_n - x_k}. \tag{3} \end{align} Plug $(2)$ and $(3)$ into $(1)$ , then $\det(N)$ (denoted by $D_n$ ) is \begin{align} & D_n = \begin{vmatrix} 1 & x_1 & 0 & 0 &\cdots & 0 \\ 1 & x_2 & (x_2 - x_1)^3/(x_n - x_1) & 0 & \cdots & 0 \\ \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & x_{n - 1} & (x_{n - 1} - x_1)^3/(x_n - x_1) & (x_{n - 1} - x_2)^3/(x_n - x_2) & \cdots & (x_{n - 1} - x_{n - 2})^3/(x_n - x_{n - 2}) \\ 1 & x_n & (x_n - x_1)^2 - (x_n - x_{n - 1})^2 & (x_n - x_2)^2 - (x_n - x_{n - 1})^2 & \cdots & (x_n - x_{n - 2})^2 - (x_n - x_{n - 1})^2 \end{vmatrix} \\ & ???? \neq 0. \end{align} When $n = 1, 2, 3$ , \begin{align} & D_1 = 1 \neq 0, \\ & D_2 = x_2 - x_1 \neq 0, \\ & D_3 = 2(x_2 - x_1)^2(x_3 - x_2) \neq 0, \end{align} all confirming $N$ is invertible. However, for $n \geq 4$ , it is more difficult to make a conclusion. I have calculated \begin{align} D_4 &= (x_4 - x_1)^{-1}(x_2 - x_1)(x_3 - x_2)^2(x_2 + x_3 - 2x_1)(2x_4 - x_2 - x_3) \\ &+ (x_4 - x_2)^{-1}(x_2 - x_1)(x_3 - x_2)^3[(x_1 - x_2)^2 + (x_3 - x_4)^2 - (x_4 - x_1)^2], \end{align} but it is not immediate to see if $D_4 \neq 0$ .
