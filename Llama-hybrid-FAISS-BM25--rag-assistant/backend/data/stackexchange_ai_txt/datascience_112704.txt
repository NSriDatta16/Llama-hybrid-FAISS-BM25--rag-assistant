[site]: datascience
[post_id]: 112704
[parent_id]: 
[tags]: 
Transformers - Why Self Attention calculate dot product of q and k from of same word?

As far as I understand and looked into Attention Is All You Need and Transformer model for language understanding , the Self Attention at Scaled Dot-Product Attention is calculating $query$ and $key$ of the same word, which is the diagonal of the matrix in the diagram. The $q \cdot k$ of the same word will generate the largest value, which means that a word attends to itself in a sentence . Is it correct or am I missing something? Why Self Attention does not exclude the $q \cdot k$ of the same word itself?
