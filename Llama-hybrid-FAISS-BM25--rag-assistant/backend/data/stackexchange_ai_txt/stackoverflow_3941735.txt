[site]: stackoverflow
[post_id]: 3941735
[parent_id]: 
[tags]: 
40 million page faults. How to fix this?

I have an application that loads 170 files (letâ€™s say they are text files) from disk in individual objects and kept in memory all the time. The memory is allocated once when I load those files from disk. So, there is no memory fragmentation involved. I also use FastMM to make sure my applications never leaks memory. The application compares all these files with each other to find similarities. Over-simplified we can say that we compare text strings but the algorithm is way more complex as I have to allow some differences between strings. Each file is about 300KB. Loaded in memory (the object that holds it) it takes about 0.4MB of RAM. So, the running app takes about 60MB or RAM (working set). It processes the data for about 15 minutes. The thing is that it generates over 40 million page faults. Why? I have about 2GB of free RAM. From what I know Page Faults are slow. How much they are slowing down my program? How can I optimize the program to reduce these page faults? I guess it has something to do with data locality. Does anybody know some example algorithms for this (Delphi)? Update: But looking at the number of page faults (no other application in Task Manager comes close to mine, not even by far) I guess that I could increase the speed of my application IF I manage to optimize memory layout (reduce the page faults). Delphi 7, Win 7 32 bit, RAM 4GB (3GB visible, 2GB free).
