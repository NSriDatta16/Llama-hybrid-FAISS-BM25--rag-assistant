[site]: datascience
[post_id]: 51034
[parent_id]: 
[tags]: 
Duplicate QUORA question detection:Kaggle Dataset

I have tried to use 2 BILSTMs along with the attention layer but the validation accuracy is not improving at all. Could anyone suggest an alternative to increase the accuracy? Layer structuring: >Preprocessing >GLOVE >BILSTM >ATTENTION >BILSTM Current Validation Accuracy for 10 epochs is in the range of 0.62 to 0.63 Code: ## Embeddings matrix is the set of vectors representing our words from keras.initializers import Constant # Length of the maximum sentence in the df sequence_length = 470 DROPOUT=0.1 question1 = Input(shape=(max_len,)) question2 = Input(shape=(max_len,)) q1 = Embedding(len(word_index)+1, embedding_dim, embeddings_initializer=Constant(embedding_matrix), input_length=max_len, trainable=False)(question1) q1 = Bidirectional(LSTM(300, return_sequences=True), merge_mode="sum")(q1) q2 = Embedding(len(word_index)+1, embedding_dim, embeddings_initializer=Constant(embedding_matrix), input_length=max_len, trainable=False)(question2) q2 = Bidirectional(LSTM(300, return_sequences=True), merge_mode="sum")(q2) ## Applying Attention model = dot([q1,q2], [1,1]) model = Flatten()(model) model = Dense((max_len*300))(model) model = Reshape((max_len, 300))(model) model = add([q1,model]) ##Applying BiLSTM model = Bidirectional(LSTM(300, return_sequences=True), merge_mode="sum")(model) model = Flatten()(model) model = Dense(200, activation='relu')(model) model = Dropout(DROPOUT)(model) model = BatchNormalization()(model) model = Dense(200, activation='relu')(model) model = Dropout(DROPOUT)(model) model = BatchNormalization()(model) model = Dense(200, activation='relu')(model) model = Dropout(DROPOUT)(model) model = BatchNormalization()(model) model = Dense(200, activation='relu')(model) model = Dropout(DROPOUT)(model) model = BatchNormalization()(model) is_duplicate = Dense(1, activation='sigmoid')(model) final_model = Model(inputs=[question1,question2], outputs=is_duplicate) final_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) history = final_model.fit([x1_train, x2_train], X_train.is_duplicate.values, epochs=10, verbose=2, validation_data=([x1_test, x2_test], X_test.is_duplicate.values), batch_size=64, callbacks=callbacks)
