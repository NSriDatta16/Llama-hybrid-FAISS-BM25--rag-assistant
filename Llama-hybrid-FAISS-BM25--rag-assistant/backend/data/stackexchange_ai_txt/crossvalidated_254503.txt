[site]: crossvalidated
[post_id]: 254503
[parent_id]: 253443
[tags]: 
Although this is far from an answer and more of a reference, it might be a good idea to check Steyerberg E - Epidemiology 2012 . In this article Steyerberg and colleagues explain different ways to check prediction model performance for models with binary outcomes (succes or failure). Calibration is just one of these measures. Depending on whether you want to have an accurate probability , accurate classification , or accurate reclassification you might want to use different measures of model performance. Even though this manuscript concerns models to be used in biomedical research I feel they could be applicable to other situations (yours) as well. More specific to your situation, calibration metrics are really difficult to interpret because they summarize (i.e. average) the calibration over the entire range of possible predictions. Consequently, you might have a good calibration summary score, while your predictions were off in an important range of predicted probabilities (e.g. you might have a low (=good) brier score, while the prediction for succes is off in above or below a certain predicted probability) or vice versa (a poor summary score, while predictions are well-calibrated in the critical area). I would therefore suggest you think about whether such a critical range of predicted succes probability exists in your case. If so, use the appropriate measures (e.g. reclassification indices). If not (meaning you are interested in overall calibration), use brier, or check the intercept and slopes of your calibration plot (see Steyerberg article). To conclude, any one of the calibration summary measures require your first step to plot your predicted probabilities versus the observed probability (see Outlier's answer for example how to). Next, the summary measure can be calculated, but the choice of summary measure should reflect the goal of predicting succes of failure in the first place.
