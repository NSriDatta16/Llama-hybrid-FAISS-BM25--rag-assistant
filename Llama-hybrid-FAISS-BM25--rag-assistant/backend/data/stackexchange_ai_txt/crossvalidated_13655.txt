[site]: crossvalidated
[post_id]: 13655
[parent_id]: 
[tags]: 
What does a confidence interval (vs. a credible interval) actually express?

Possible Duplicate: What, precisely, is a confidence interval? Yes, similar questions have been asked before, but many of the answers seem contradictory and don't address my issue. (Or my perception of the issue.) As mentioned many places, what most people will likely find intuitive when presented with an interval and a probability, is that it expresses how probable it is that the true value lies in this range. If told that an exit poll has a confidence interval of 60-70 with probability 0.95, a layman may (reasonably) expect that when exit polls have this result, the interval does actually include the true proportion 95 % of the time. Expressed mathematically: $P(X\in[60,70]) = 0.95$ The problem is, this seems to be the correct interpretation of credible intervals , and a common misinterpretation of confidence intervals. From http://en.wikipedia.org/wiki/Confidence_interval : A confidence interval does not predict that the true value of the parameter has a particular probability of being in the confidence interval given the data actually obtained. So what does a confidence interval mean, then? Wikipedia says: A confidence interval with a particular confidence level is intended to give the assurance that, if the statistical model is correct, then taken over all the data that might have been obtained, the procedure for constructing the interval would deliver a confidence interval that included the true value of the parameter the proportion of the time set by the confidence level. I find the wording mighty confusing, but I understand this as meaning that given each X, there is at least a 0.95 probability of getting a Y whose interval spans X: $P_X(Y : X \in I_y) \ge 0.95$ This seems to be consistent with the explanation of confidence and credible intervals given by Keith Winstein here: What's the difference between a confidence interval and a credible interval? (The probability, given a cookie jar, of picking a cookie with a chip count whose interval spans that same cookie jar is at least 70 %) If this understanding is correct, then I fail to see why confidence intervals make any practical sense at all. Each interval depends on other intervals in ways that are difficult to grasp, and does not in fact have any strong connection to the actual result of a sampling. Can someone explain why this concept is so widespread? (I realize that using Bayesian probability to obtain the credible interval may not be desirable, but that doesn't necessarily make CIs a good alternative.)
