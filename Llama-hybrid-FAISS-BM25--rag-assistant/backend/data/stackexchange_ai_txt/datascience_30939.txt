[site]: datascience
[post_id]: 30939
[parent_id]: 30921
[tags]: 
I see in the code for the MLPRegressor, that the final activation comes from a general initialisation function in the parent class: BaseMultiLayerPerceptron , and the logic for what you want is shown around Line 271 . # Output for regression if not is_classifier(self): self.out_activation_ = 'identity' # Output for multi class ... Then during a foward pass this self.out_activation_ is called ( defined here ): # For the last layer output_activation = ACTIVATIONS[self.out_activation_] activations[i + 1] = output_activation(activations[i + 1]) That ominous looking variable ACTIVATIONS is simply a dictionary ,with the keywords being the descriptions you can choose as a parameter in your MLP, each mapping an actual function. Here is the dictionary : ACTIVATIONS = {'identity': identity, 'tanh': tanh, 'logistic': logistic, 'relu': relu, 'softmax': softmax} With all of this information, you might be able to come up with a few ways of putting in your custom function. Off the top of my head, I can't see a quick way to simply provide a function. You could for example: define your function where all the other activation functions are defined add it to that ACTIVATIONS dictionary make self.out_activation_ equal to your custom function (or even a new parameter in MLPRegressor cross your fingers it doesn't break something somewhere else run it and solve the inevitable small adaptations that will be necessary in a few places I'm, afraid I have never looked at the source code of that library before, so cannot give more nuanced advice. Perhaps there is a beautifully elegant way to do it that we have both overlooked.
