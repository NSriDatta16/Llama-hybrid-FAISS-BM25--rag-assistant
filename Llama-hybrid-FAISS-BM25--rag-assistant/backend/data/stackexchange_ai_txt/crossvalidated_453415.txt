[site]: crossvalidated
[post_id]: 453415
[parent_id]: 452842
[tags]: 
Translational and rotational invariance depends on your... everything. Feedforward, convolutional, graph, CapsNets, various activation types, objective functions and other tricks on top, the way the layers you chose above are wired... Starting from CNNs, they can learn features independent of their location, but that doesn't prove they always will . A major trick behind NN layers is that they can amplify or attenuate their inputs in a 'smart' way (e.g. an edge detection model amplifies the pixels IF they are on an edge ELSE it's attentuated). If there's even the slightest amount of contrast between the background pixels and the cat pixels, it is learnable at least in principle - your first layer could signal boost vaguely cat-shaped objects and pass them onto further layers to determine which of these are actual cats. Nnnot really, but I struggle to find a metaphor that is both correct, intuitively understandable, and isn't circular. The closest would be a very odd election system.
