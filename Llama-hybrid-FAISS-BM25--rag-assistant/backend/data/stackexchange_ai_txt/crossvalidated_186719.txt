[site]: crossvalidated
[post_id]: 186719
[parent_id]: 186091
[tags]: 
In general, when you have a problem where the sample can only belong to one class among a set of classes, you set the last layer to be a soft-max layer. It allows you to interpret the outputs as probabilities. When using a soft-max layer, cross entropy generally works very well, because the logarithmic term in the cross-entropy cancels out the plateau that is present in the soft-max function, therefore speeding up the learning process (think of points far away from $0$ in the sigmoid function). In your case you have a binary classification task, therefore your output layer can be the standard sigmoid (where the output represents the probability of a test sample being a face). The loss you would use would be binary cross-entropy. With this setup you can imagine having a logistic regression at the last layer of your deep neural net. Here are a couple links for you. Hope they help. https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression http://neuralnetworksanddeeplearning.com/chap3.html https://www.quora.com/How-do-you-decide-which-loss-function-to-use-for-machine-learning
