[site]: crossvalidated
[post_id]: 142436
[parent_id]: 142425
[tags]: 
The output of an SVM classifier is actually determined as $\hat y = \mathrm{sign}(\langle w, x\rangle + b)$, where $x$ is the test point and the model is defined by $w$ (the same kind of object as $x$) and $b\in \mathbb R$. The value $\langle w, x\rangle + b$ can be treated as a confidence score for the prediction; high absolute values correspond to confident predictions. This is sometimes called the "decision function", and can be useful to compare different examples with the same model. With multiple models, however, $w$ might vary substantially in norm (particularly if your models have different regularization parameters $C$), and so the magnitude of the scores can have very different meanings for different models; you can't necessarily compare them across models. Platt scaling , as used by libsvm and other tools, converts this score (which can be any real number) to a probability (in $[0, 1]$) of being in the positive class. It does this by passing through a logistic function $\sigma(\langle w, x\rangle + b)$, with $\sigma(z) = 1 / (1 + \exp(A z + B))$. $A$ and $B$ are usually determined by cross-validation on the training set. Being probabilities, these are much more combinable/comparable/amenable to throwing into Bayesian models/whatever. These probabilities won't be completely reliable, but they're often at least sort of reasonable. Then you can use your favorite ensemble approach to make a final prediction.
