[site]: crossvalidated
[post_id]: 248614
[parent_id]: 226692
[tags]: 
Yes, I would try Doc2Vec with that. The build_vocab() method in gensim is akin to word2vec, in any case (i.e. only for Distributed Memory algorithm, not the DBOW which does not make word vectors). You can test the words similarities in DM route after training and see how they compare. Then also you can test the documents' also. Another word embedding method is supposed to be good: GLoVE. There are some good tutorials in the blogosphere for doc2vec - as well as the gensim ipython notebook you could follow to get going. My intuition is that it works better with smaller short texts like tweets than longer documents, but you can try in any case.
