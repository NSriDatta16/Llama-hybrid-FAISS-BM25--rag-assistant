[site]: crossvalidated
[post_id]: 599867
[parent_id]: 599866
[tags]: 
The sigmoid function takes the linear predictor of the logistic regression and bends it into a range, $[0,1]$ , that can interpreted as a probability. The linear predictor can take any real number, which is fine if we think about the log-odds. However, for valid probability values, the must be a transformation to restrict outputs to $[0,1]$ . Any business about a decision boundary is separate from the sigmoid function. In fact, it is totally reasonable to do a decision boundary in terms of log-odds and skip the sigmoid function altogether. Just evaluate if the prediction has positive or negative log-odds, and classify accordingly. This is equivalent to the decision rule in the OP, and it is a worthwhile exercise to consider why. As always, hard classifications need not be made, since a logistic regression explicitly predicts probability values that can be valuable on their own without considering hard classifications (at least not initially). Our Stephen Kolassaâ€™s answer here gets into some of that.
