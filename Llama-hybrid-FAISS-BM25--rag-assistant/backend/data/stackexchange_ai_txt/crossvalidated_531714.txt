[site]: crossvalidated
[post_id]: 531714
[parent_id]: 531706
[tags]: 
Yes, dimension reduction is one way to use auto-encoders. Consider a feed-forward fully-connected auto-encoder with and input layer, 1 hidden layer with $k$ units, 1 output layer and all linear activation functions. The latent space of this auto-encoder spans the first $k$ principle components of the original data. This can be useful if you want to represent the input in fewer features, but don't necessarily care about the orthogonality constraint in PCA. (More information: What're the differences between PCA and autoencoder? ) But auto-encoders allow a number of variations on this basic theme, giving you more options for how the latent space should be constructed than does PCA. Using CNN layers instead of FFNs is clearly a different kind of model compared to PCA, so it will encode a different kind of information in the latent space. Using nonlinear activation functions will also yield a different kind of latent encoding than PCA (because PCA is linear). Likewise sparse, contractive or variational auto-encoders have different goals than PCA and will give different results, which can be helpful depending on what problem you're trying to solve.
