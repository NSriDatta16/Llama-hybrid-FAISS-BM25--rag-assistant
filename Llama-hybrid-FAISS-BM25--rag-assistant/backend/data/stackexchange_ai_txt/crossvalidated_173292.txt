[site]: crossvalidated
[post_id]: 173292
[parent_id]: 173056
[tags]: 
I believe that most 'frequentists' and 'Bayesians' would rigorously define probability in the same way: via Kolmogorov's axioms and measure theory, modulo some issues about finite vs countable additivity , depending on who you're talking to. So in terms of 'symbols' I reckon you'll likely find more or less the same definition across the board. Everyone agrees on how probabilities behave . I would say the primary difference is in the interpretation of what probabilities are . My (tongue-in-cheek militant Bayesian) preferred interpretation is that probabilities are coherent representations of information about events . 'Coherent' here has a technical meaning: it means that if I represent my information about the world in terms of probabilities and then use those probabilities to size my bets on the occurrence or nonoccurrence of any given event, I am assured that I can not be made a sure loser by agents betting against me. Note that this involves no notion of 'long-run relative frequency'; indeed, I can coherently represent my information about a one-off event - like the sun exploding tomorrow - via the language of probability. On the other hand, it seems more difficult (or arguably less natural) to talk about the event "the sun will explode tomorrow" in terms of long-run relative frequency. For a deep dive on this question I'd refer you to the first chapter of Jay Kadane's excellent (and free) Principles of Uncertainty . UPDATE : I wrote a relatively informal blog post that illustrates coherence.
