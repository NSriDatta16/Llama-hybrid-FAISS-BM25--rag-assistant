[site]: datascience
[post_id]: 116650
[parent_id]: 110119
[tags]: 
In practice, nothing is preventing one from doing what you propose, masking and predicting the [CLS] or the [SEP] token. But the important question is why the model would need to learn about unmasking these tokens. My understanding is that language models like BERT are pretrained for giving them a better understanding of the language. Then they can be finetuned for any downstream task. But [CLS] and [SEP] tokens are not part of the language and we add them for our convenience. You do not need to learn about them for getting a better "understanding of the language". Learning to predict a masked [SEP] token may not bring any additional performance improvement to the models. [CLS] is used as a representative of the whole input text sequence which is then used for classification tasks usually. It should not be treated the same way as other tokens because it is serving a different purpose. Similar reasoning may be used for [SEP] tokens.
