[site]: datascience
[post_id]: 62659
[parent_id]: 62216
[tags]: 
I came across this paper by Asteris et. al. entitled "Orthogonal NMF through Subspace Exploration". You can google it as links posted here might break in the future. The basic idea boils down to if we can construct non-negative principal components of the matrix of interest, we can use these principal components to do an orthogonal NMF. Since the matrix of interest is non-negative (this is what NMF assumes) and the non-negative principal components are non-negative, then by construction the transformed matrix is also non-negative. The paper details how exactly the construction can be achieved and guarantees convergence. To answer my own question above: There are many ways to do NMF. In the case where the transforming vectors are orthogonal, i.e. Orthogonal NMF, it is identical to Non-negative PCA. Details can be found in the paper cited above.
