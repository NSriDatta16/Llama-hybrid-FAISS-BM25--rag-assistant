[site]: datascience
[post_id]: 75401
[parent_id]: 
[tags]: 
Training models from sklearn using tf.distribute.MirroredStrategy

I want to distribute the training of a simple model, such as a support vector classifier like sklearn.svm.SVC() across some or all CPUs and GPUs on a single device. I have never utilized a GPU before and I'm confused as to how this works or if using tensorflow is even the right choice for this simple task. What i think I need to do is something like this: import tensorflow as tf from sklearn.svm import SVC from sklearn import datasets from sklearn.model_selection import train_test_split strategy = tf.distribute.MirroredStrategy() with strategy.scope(): iris = datasets.load_iris() X = iris.data y = iris.target class_names = iris.target_names X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) classifier = svm.SVC(kernel='linear', C=0.01).fit(X_train, y_train) The actual dataset I'm using is $\approx 3 \times 10^8$ training examples with 11 features. Does this code do what I think it does? If not, what would be the best way to go about this task? If so, is there anything that can be improved? EDIT: After doing some more googling I discovered that sklearn does not support GPU utilization. See this post: https://stackoverflow.com/questions/41567895/will-scikit-learn-utilize-gpu I'm still not sure how I can go about utilizing a GPU for simple ML models.
