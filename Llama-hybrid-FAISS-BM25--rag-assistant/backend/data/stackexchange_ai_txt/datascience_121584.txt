[site]: datascience
[post_id]: 121584
[parent_id]: 
[tags]: 
How big is the threshold that is usually used in determining the convergence of loss values in deep learning?

In deep learning, one way to determine whether the training has converged is to observe the movement of the loss values over iterations or epochs. One can choose any $\epsilon$ threshold and any metric. If the value is less than $\epsilon$ , then the training has converged. My question is: how big is the $\epsilon$ value that is usually used? Are there examples of papers that specifically state the threshold?
