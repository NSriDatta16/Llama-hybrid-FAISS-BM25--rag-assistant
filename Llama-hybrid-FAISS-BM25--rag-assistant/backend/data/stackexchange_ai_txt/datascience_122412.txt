[site]: datascience
[post_id]: 122412
[parent_id]: 122409
[tags]: 
Note three things: The output of the encoder is not English but just what the decoder needs from the source sentence to generate the translation. Only the first decoder layer receives the target language token embeddings. The following layers receive the output of the previous layer. There are matrix multiplications before the dot products, which can project their inputs to completely different representation spaces. So, to answer your question: Neither of your interpretations is correct. The keys, values and queries are not in an "English representation space" nor in a "French representation space". Keys, vectors and queries are vectors in representation spaces that have been learned by the network during training. These representation spaces are not necessarily interpretable by a human, they were learned just to lower the loss at the task the model was trained in (i.e. to translate). As an example of what I am trying to convey, please consider Transformer models trained for multilingual machine translation. These models can receive many different languages as input and generate translations in many different languages. These models learn to represent information in a way that makes it possible to translate properly between those languages (i.e. to minimize the loss they have been trained on). The same happens in a non-multilingual machine translation Transformer. Actually, there are many scientific papers trying to understand what kind of information is encoded at the output of each layer in Transformer models.
