[site]: crossvalidated
[post_id]: 439445
[parent_id]: 
[tags]: 
Combining continuous and binary data in unsupervised learning

I am working on cluster detection applied to housing data. Each data point has some continuous features, such as house size, and some discrete ones, such as the number of garages (0 or 1). At the moment I scale each feature to the unit interval, then run PCA, and feed the PCA components to a clustering algorithm (HDBSCAN). What I'm worried about is that all the binary features will then be represented at the extremes of the unit interval. In other words, the garage feature difference between houses with and without garages (both fairly common) will be as extreme as the size feature difference between the single smallest and single largest houses in the data set. If this were supervised learning, I'd expect the learning algorithm to compensate for any problems caused by this by assigning a lower weight to the binary feature if this is a problem. As I am new to unsupervised learning, I am not certain how to reason about this. Some specific questions: Am I correct in worrying about this in the first place? Does running PCA reduce/fix any problems? Is there an established 'best practice' for cases like this (I have not been able to find one)?
