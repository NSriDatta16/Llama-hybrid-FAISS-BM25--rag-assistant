[site]: crossvalidated
[post_id]: 102681
[parent_id]: 
[tags]: 
Using machine learning to tell apart users from crawlers

Would it be possible to build anomaly detection algorithm to be able to tell apart real user from crawlers/spiders with high confidence? I have access to user tracking data where information about client session, IP address and weather client can process javascript. Based on that data I can calculate how many pages client visited and what is the entropy of time between requests. I also know which sessions converted which would be my non anomalous data I can use to train. To summarize I have 3 features I think I can use: - number of requests - *calculated entropy based on time spent on each page - can run JS Are there any other possible or better features that could be used? Is this problem solvable with machine learning? * crawlers usually don't spend too much time on the page so time between requests would be very short with similar intervals. That would give small entropy value. Real visitors would produce more random pattern.
