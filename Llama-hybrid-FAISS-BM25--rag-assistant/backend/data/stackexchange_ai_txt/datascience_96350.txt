[site]: datascience
[post_id]: 96350
[parent_id]: 96345
[tags]: 
All heads are fed the exact same input. Each head learns different weight values because: The attention heads, along with the rest of the network, are initialized randomly. The back-propagated gradients each head receives are different. This is because the result of the multi-head attention is the concatenation of each head. When back-propagating through the concatenation, the gradient is split among the heads, so that each head gets a piece of the back-propagated gradient and, therefore, the gradient information received by each head is different.
