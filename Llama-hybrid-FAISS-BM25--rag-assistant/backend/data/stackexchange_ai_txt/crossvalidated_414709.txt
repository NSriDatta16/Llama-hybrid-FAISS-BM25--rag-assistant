[site]: crossvalidated
[post_id]: 414709
[parent_id]: 347563
[tags]: 
I can provide a few suggestions if you're still looking for help (as of this writing, the question is â‰ˆ1 year old) First thing I noticed was the number of epochs you used while training epochs=20 could be a bit low, especially considering that your documents are shorter in length. This is a little bit trial-and-error as I'm not as familiar with training on shorter documents, but have you tried bumping up epochs higher, perhaps something within the 50-100 range? (maybe even 100+) This may perform better on shorter documents. Your number of documents (500) is low in count - if possible, could you gather more examples for training? As a side note. I've noticed that you perform the document list slicing first, then shuffle docs_train for training. Not sure if that's intended: Try shuffling first and then slice. Also, it depends on why exactly you'd want to arbitrarily train on a portion of your documents - If the goal is to find similar documents going forward, I'd also try training your doc2vec model on the entire set, to find similar documents within the training set. Then, when you have an out-of-sample document, you can infer the vector to find most similar sentences within what the model knows and trained on previously. In practice, I've done a "similarity search" by inferring all new documents (out-of-sample) and performing a cross join to surface up documents that are most similar, where my training was the entire set. During the call to model.infer_vector() , you could also try bumping up steps=50 (btw, steps arg is deprecated, you could switch to use epochs=N ) I've found the optimal number under my use cases was 99-250 range, but this requires some tuning and testing on your end, as that number may not be the same for you. How I've tackled this was to perform iterative inference (for epochs=10, 20, 30... 300+) and chart the performance of cosine similarity between the model-known vector vs the inferred vector. You could try tweaking the vector_size from 100 to something smaller or larger. If you think the set of vocabulary is limited, perhaps condensing vectors to smaller size may achieve better results because you are representing vocab by mapping to smaller vector size. for short corpuses, you could try word stemming, as this may help the model. Likewise, try options for keeping stopwords and punctuation. If you are inclined, you can try more hyperparam tuning/testing: set dm=0, window={3-10}, min_count={1-5}, etc... if you package up your testing code you try "grid search" by testing various doc2vec configurations and see which one comes out with the best accuracy. Anyway, I hope this helps to at least get you started on right path on what to experiment with and tweak in your code! I've had to learn a bunch just via testing and lots of reading other posts/questions&answers - In reality, the one big takeaway I've found is that your doc2vec implementation greatly depends on your use-case. For more reading, I've found this paper very useful to understand underlying mechanics for doc2vec: An Empirical Evaluation of doc2vec with Practical Insights into Document Embedding Generation
