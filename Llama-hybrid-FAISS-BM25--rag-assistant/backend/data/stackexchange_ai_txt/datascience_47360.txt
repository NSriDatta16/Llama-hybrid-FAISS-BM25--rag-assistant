[site]: datascience
[post_id]: 47360
[parent_id]: 47355
[tags]: 
You are asking two different questions: What is linear regression? Linear regression means that, given a response variable $y$ and a set of predictors $x_i$ , you are assuming (whether or not this is true is another matter) to model your response variable as $$ y^{(j)}=\sum_{i=1}^N x_i^{(j)}\beta^i + \epsilon^{(j)} $$ for each observation $y^{(j)}$ , where $\epsilon^{(j)}$ is an error term with vanishing expectation value. The purpose of the algorithm is to find the set of $\beta^i$ to minimise the errors between the above formula and the actual values of the response. May I use linear regressio to model non-linear functions? You may use the linear regression to model anything you want, this does not necessarily mean that the results will be a good fit. The mere decision to use a model makes no assumptions on whether the underlying equation is in fact reflected by the model you choose. In case of linear regression you are essentially approximating an $N$ -dimensional manifold (where all true points belong) with their projections onto a plane. Whether or not this is a good idea it depends on the data. I want to know if that is really the case, and is it always possible to do this "change" in the functional form? By using this or that other model you are not changing the functional form of the underlying variables. You are just dictating that the original relation (that you do not know) can be approximated by the model you choose. Is it still correct to measure the performance of the model using R_square metric? The $R^2$ is defined as the ratio between the residual sum of squares of your model over the residual sum of squares of the average. Basically it tells how much of the variance of the data is explained by your model compared to just taking a straight line (in correspondence of the average) passing through all your data points.
