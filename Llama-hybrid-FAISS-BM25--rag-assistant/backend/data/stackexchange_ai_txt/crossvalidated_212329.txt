[site]: crossvalidated
[post_id]: 212329
[parent_id]: 212198
[tags]: 
It is recommend to read "Neural Networks: Tricks of the trade", all reason to improve parameter tuning are precisely describe in the book. And to your specific questions: What are the benefits that we gain by doing this? As you said, most benefit from "smoother movement in parameter space" Parallel processing of multiple batches. Does using batch-size=1 affect the performance badly? No, not as significantly as you think, just like gradient update without momentum term. Does using more batch-size translate to faster convergence? More batch-size doesn't always lead to faster convergence, but instead, gradient update can be more stable.
