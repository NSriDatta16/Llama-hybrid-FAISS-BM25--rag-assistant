[site]: crossvalidated
[post_id]: 582557
[parent_id]: 
[tags]: 
Which is better: performance stability or high performance in the initial epochs but extremely unstable performance during validation?

I am working on a multiclassification problem using time series data. I am using a hybrid model (such as LSTM, CNN, attention, etc.). I tried two optimizers, ADAM with 0.001 learning rate and SGD with 0.01. The following figures illustrate the trend in training and validation performance: SGD with 0.01 Learning Rate ADAM with 0.001 Learning Rate Now it is clear from the figure that the performance of the SGD optimizer is stable, but its peak performance is 87% f1-score, and after 20 epochs, there is no growth or reduction in performance. Furthermore, we observe that the training performance does not improve significantly after 20 epochs and is consistently lower than validation; thus, is it underfitting? On the other hand, the ADAM optimizer displays an entirely different trend for validation performance. The highest validation epoch score (about 87 percent) of both optimizers is nearly identical. The question is: Which optimizer do you prefer and Why? If SGD, then how can we further improve performance? If ADM, then how can we further improve and achieve more performance?
