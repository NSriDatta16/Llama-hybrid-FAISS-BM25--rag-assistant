[site]: crossvalidated
[post_id]: 237948
[parent_id]: 
[tags]: 
How many times does f(x) need to be called before getting a True, on average?

I have a probabilistic function f(x) which returns True or False depending on its input x. The input x is an integer on the range [1,28] and is chosen uniformly at random. f(x) behaves as follows: If x == 15 or x == 20, return True with 50% probability otherwise False Else If 15 Else return False How many times does f(x) need to be called before getting a True, on average? I've simulated this in code already, and what I'm interested in is a solution using probability and statistics. I've thought about treating f(x) as a coin flip with probability of heads being 4/28 (for the range 15 However I'm unsure of how to include the 50% probability of True when x == 15 or x == 20. It should reduce the expectation. I'd appreciate any help.
