[site]: crossvalidated
[post_id]: 605302
[parent_id]: 605258
[tags]: 
Thanks to Jarle Tufto who provided an outline of the main idea in the comments. All errors are my own. We can realize that if $\sigma$ was known, the distribution of the MLE of the slope is $\frac{\sigma^2}{\sum (x_i - m)^2}$ where $m$ is the observed mean of $x$ (taken from Wiki ) This means that the difference of the two MLEs is also going to be normally distributed. We however do not know $\sigma$ , and plugging in the estimate of sigma from the first part, the resulting quantity is $t$ -distributed with $nâˆ’2$ degrees of freedom, where $n$ is the sample size of the observed data. So the prediction interval will be derived from t distribution centered at the MLE from the first batch of data and with standard error of $\sqrt{s^2 + \frac{\hat{\sigma}^2}{\sum (\bar{x}_i - \bar{m})^2}}$ where $\hat\sigma$ is the estimate of $\sigma$ from the first regression, $s$ is the standard error from the first regression and $\bar{m}$ is the mean of $\bar{x}$ . Adapting the code example from the question: x = b1pred[,1] & b1bar Shows the expected coverage behaviour.
