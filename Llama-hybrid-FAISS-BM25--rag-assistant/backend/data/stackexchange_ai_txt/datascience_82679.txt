[site]: datascience
[post_id]: 82679
[parent_id]: 82669
[tags]: 
Both can run CUDA to accelerate deep learning. Cards that end in "ti" are slightly better versions than their non-ti counterparts. NVIDIA, the corporation that makes the 1650 and 1650ti cards, also develops CUDA. Only (and all) NVIDIA graphics cards currently support CUDA because it is proprietary. However, because deep learning models are often limited by memory capacity, I recommend buying a card such as the GTX 1070, which is comparably priced but has 8GB of RAM.
