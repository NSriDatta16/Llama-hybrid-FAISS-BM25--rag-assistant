[site]: crossvalidated
[post_id]: 197692
[parent_id]: 197689
[tags]: 
Backpropagation uses a regularizer $\lambda$ to penalize magnitudes of the weights to prevent overfitting. This is a frequentist approach to applying the Occam's Razor principle. In the Bayesian formulation, this is expressed equivalently as putting a prior distribution on the weights. Your regularizer term $\lambda$ would now be expressed in terms of the prior hyperparameters. Both approaches work and equivalent in an algorithmic sense. The Bayesian formulation might require a few additional math steps to see how $\lambda$ depends on the prior. The frequentist approach would use cross-validation to determine $\lambda$ directly. Bayesian's may use cross-validation to fix a prior. Or they may not.
