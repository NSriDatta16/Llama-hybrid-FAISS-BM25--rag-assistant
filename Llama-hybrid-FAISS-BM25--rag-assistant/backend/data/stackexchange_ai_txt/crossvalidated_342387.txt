[site]: crossvalidated
[post_id]: 342387
[parent_id]: 
[tags]: 
How sampling and KL loss work in Variational Autoencoders?

I am trying to learn about Variational Autoencoders and found this very informative blog about vae's . I understood most part of vae's but cant understand how sampling and KL loss work in a vae. This is the code which confuses me def sampler(mean, log_stddev): # we sample from the standard normal a matrix of batch_size * latent_size (taking into account minibatches) std_norm = K.random_normal(shape=(K.shape(mean)[0], latent_size), mean=0, stddev=1) # sampling from Z~N(μ, σ^2) is the same as sampling from μ + σX, X~N(0,1) return mean + K.exp(log_stddev) * std_norm latent_vector = Lambda(sampler)([mean, log_stddev]) # pass latent_vector as input to decoder layers Can you give an intuitive explanaition of what is happeing in sampler and Lambda functions? Why vae needs KL loss?
