[site]: datascience
[post_id]: 93888
[parent_id]: 
[tags]: 
XGBoost incremental training for big datasets

I am trying to train an XGBoost model on a quite big dataset (tens of GB, almost a hundred). I have been trying to use some libraries such as Dask to deal with this problem, without any success due to high memory consumption (I opened a question on Stackoverflow , if you can answer, you are welcome). Due to the above failure, I decided to implement my own incremental XGBoost training, something like what has been proposed here: XGBoost incremental training The main idea is basically the following code: params = {} # your params here ith_batch = 0 n_batches = 100 model = None while ith_batch However I have three important questions: Is this method of incremental training the same as training on the whole dataset? If not, why? How should I split my batches at each iteration? Should I basically sweep the whole dataset sequentially? How do the most common distributed/parallel compute libraries (Dask-ML, SparkML ecc...) train these models without storing everything in memory? Do they rely on incremental training?
