[site]: crossvalidated
[post_id]: 273768
[parent_id]: 
[tags]: 
Fitting Parametric Equations with Implicit Parameter

This question has been imported from the python stackoverflow 32133733 . I have experimental data of the form (X,Y) and a theoretical model of the form (x(t;*params),y(t;*params)) where t is a physical (but unobservable) variable, and *params are the parameters that I want to determine. t is a continuous variable, and there is a 1:1 relationship between x and t and between y and t in the model. In a perfect world, I would know the value of T for every X and Y (for example, T is the position of a spring I cannot see) and would be able to do an extremely basic least-squares fit to find the values of *params , which are fixed and the same across experiments. I cannot guarantee that in my real data, the latent value T is monotonic, as my data is collected across multiple cycles. I'm not very experienced doing curve fitting manually, and have to use extremely crude methods without easy access to a basic built-in function. My basic approach involves: Choose some value of *params and apply it to the model Take an array of t values and put it into the model to create an array of model(*params) = (x(*params),y(*params)) Interpolate X (the data values) into model to get Y_predicted Run a least-squares (or other) comparison between Y and Y_predicted Do it again for a new set of *params Eventually, choose the best values for *params There are several obvious problems with this approach. 1) I'm not experienced enough with coding to develop a very good "do it again" other than "try everything in the solution space," of maybe "try everything in a coarse grid" and then "try everything again in a slightly finer grid in the hotspots of the coarse grid." I tried doing MCMC methods, but I never found any optimum values, largely because of problem 2 2) Steps 2-4 are super inefficient in their own right. I've tried something like (resembling pseudo-code; the actual functions are made up). There are many minor quibbles that could be made about using broadcasting on A,B, but those are less significant than the problem of needing to interpolate for every single step. People I know have recommended using some sort of Expectation Maximization algorithm, but I don't know enough about that to code one up from scratch. I'm really hoping there's some awesome scipy (or otherwise open-source) algorithm I haven't been able to find that covers my whole problem, but at this point I am not hopeful. Another suggestion has been to eliminate t in the model and fit for only x and y . However, I've found two problems with this approach. One is that I cannot always eliminate t analytically, which makes the fitting hard to perform in programs like python. Second, even when I can eliminate t , I end up with an implicit equation in x and y that is highly singular. When I keep the equations in terms of t , I only have singularities at t=0 . However, when I eliminate t , I get parts of the equations that look like (some constants taken out for simplicity) y - x + c' = y/x + (y/(x-y))^(1/3) Here, relatively small errors in x , y , can overwhelm the fitting lead to curves that don't fit very well. I can put in place some constraints ( |x|>|y| ), but these curves are not monotonic in x (and can bend back on themselves). I ended up abandoning the "eliminate t approach" when it was leading to serious singularity problems when I tried to plot the analytical functions in Mathematica - problems that went away when I switched to parametric mode. Any advice on either approach would be greatly appreciated (especially if you can point me to programming modules that may help)
