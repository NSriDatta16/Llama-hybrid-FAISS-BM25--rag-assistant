[site]: datascience
[post_id]: 36354
[parent_id]: 36347
[tags]: 
It is a 49 page long paper, so following observations are based only on a cursory reading. The optimisation is for finding best value of parameters for cost function of machine learning models. Rather than finding a fixed value of the parameters, it is assumed that the parameters come from statistical distribution and the task is to find the nature/shape of this statistical distribution. Bayes theorem tells you that if you have prior beliefs and evidence (data), you can go to posterior. They start with the assumption that prior distribution of the parameters is Gaussian. The task is to then find the posterior. Since there are multiple parameters and not single variable, Gaussian process comes into picture rather than single Gaussian distributed random variable. Bayes theorem comes into picture while going from priors to posteriors. The optimisation is solved using sampling technique like Monte Carlo sampling . Reading up about MCMC sampling method will help you see the connection.
