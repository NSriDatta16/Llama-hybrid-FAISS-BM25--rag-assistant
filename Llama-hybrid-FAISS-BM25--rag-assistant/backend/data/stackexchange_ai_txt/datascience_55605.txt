[site]: datascience
[post_id]: 55605
[parent_id]: 
[tags]: 
Using reinforce algorithm with per-action reward instead of per-trajectory reward

I've found some articles that talk about the reinforce algorithm / monte carlo method. The algorithm boils down to using this equation. The right summation over the trajectory is the reward for the trajectory. This is multiplied by the derivative of the log likelihood of the action sequence. The idea is you take a large sample of trajectories and average them with this equation to estimate the policy gradient. I want to adapt this core algorithm to use discounted rewards for actions within the trajectory instead of using a total reward for trajectories. I read somewhere that this can reduce variance and is a common optimization. Could someone please derive how the action-reward would work or cite a source.
