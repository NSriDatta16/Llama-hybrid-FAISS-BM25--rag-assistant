[site]: crossvalidated
[post_id]: 474904
[parent_id]: 
[tags]: 
Overlap-tile strategy in U-Nets

I was reading the U-Nets paper and there is a mention of some "overlap-tile strategy" in it that I am not quite familiar with. Here is the paragraph from the paper where it has been introduced: What do they mean by "only us[ing] the valid part of each convolution"? I looked this up and from what I have understood I think they mean that their convolution operations do not involve any padding. Instead of having padded-convolutons to maintain the spatial size of the feature maps, they pad the original image by mirroring the borders and forward the pre-padded image through the network, where it gets downsampled at every convolution and the final output comes out downsampled to the same spatial dimension as the original image. If this strategy really is better than padded-convolutions, why is this not being used everywhere? If not, what is it that makes it worse? Also, what is the overlap-tile strategy? And how does it allow the "seamless segmentation of arbitrarily large images"? The Figure 2 they are referring to is this, but I am finding it difficult to see what the figure is trying to depict.
