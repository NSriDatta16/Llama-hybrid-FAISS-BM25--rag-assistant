[site]: crossvalidated
[post_id]: 284974
[parent_id]: 249881
[tags]: 
My understanding is that for non-linear SVM kernels training time scales as a square of the number of training examples, i.e. if you double the training examples training time will quadruple. Add in cross validation and your 1M training examples and it is not surprisingly that training is prohibitively computationally expensive. If your training samples are unbalanced (i.e. for a classification problem a large majority of outcomes belong to one class), sub-sampling the majority class to reduce both class imbalance and therefore the total training set size might be optimal. In this case you're discarding data that may not be needed, but still producing a representative training set. If your training samples are balanced, then cross validation on a reduced-size training set really defeats the purpose of cross validation - namely validating that the train/test split for each fold is representative of the distribution of the entire 1M record training set. Rather than reduce the size of the training set, I'd reduce the number of folds - or simply not do cross validation, but instead use a proportionally large (and therefore hopefully representative) test set, e.g. 20-40% of your 1M samples. In general a Bayesian Optimisation method is much more efficient than either grid or random hyperparam optimisation as it is a guided search in hyperparam space. There are a variety of Bayesian and a few non-Bayesian hyper-parameter optimisation libraries to choose from. Have a look at: http://fastml.com/optimizing-hyperparams-with-hyperopt/ I've used hyperopt with a lot of success. SMAC looks very interesting as it incorporates cross validation and will not necessarily validate every fold if it determines that a particular candidate set of hyperparams will not score better as compared to previous folds. If you do want to use SVM with cross validation on 1M training examples then GPUs might be able to do the heavy computational lifting you require. Check out: http://mklab.iti.gr/project/GPU-LIBSVM
