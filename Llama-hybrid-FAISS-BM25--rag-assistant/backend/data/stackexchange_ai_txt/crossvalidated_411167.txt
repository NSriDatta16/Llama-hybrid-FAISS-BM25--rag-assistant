[site]: crossvalidated
[post_id]: 411167
[parent_id]: 
[tags]: 
How are the various guarantees provided to SVMs by Statistical Learning Theory affected by the Kernel Function

I never studied the field in depth, but I am very aware that state of the art performance in most ML tasks is now achieved by various flavors of neural networks. At the same time, Vladimir Vapnik, the co-founder Statistical Learning Theory (SLT), and the co-inventor of SVMs is continuing to play around with various marginal improvements to SVMs, such a privileged information ( https://www.youtube.com/watch?v=5mvfpSdWsOo ) or using location information ( https://www.youtube.com/watch?v=LEYglsxKclo ). In this interview he seems explicitly hostile to deep learning, claiming that the problems it might be solving are actually easy ( https://www.youtube.com/watch?v=STFcvzoxVw4#t=23m39s ). To my untrained eye, the biggest benefit of NNs is that they produce their own kernels from the ground up, which in the SVM framework have to be designed by hand. In some sense, SVMs push the task of intelligence once step back, to the problem of kernel design. Which brings me to my question: How are the guarantees of SLT dependent on the kernels? Obviously the right kernel can transform a problem from one requiring slack variables to one lacking them, but if theory itself cannot provide rigorous and mechanical method of producing kernels, then it seems SLT is severely incomplete as a theory of inference. Am I misunderstanding something?
