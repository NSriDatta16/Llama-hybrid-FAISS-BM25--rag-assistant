[site]: crossvalidated
[post_id]: 432710
[parent_id]: 
[tags]: 
Understanding convergence in Bayesian inference of coin tossing

When we are uncertain about the probability of head, $p_H$ , in a coin tossing, we often model it using a Beta prior as follows: $$p_H\sim \text{Beta}(a_0,b_0),$$ for some parameters $a_0,b_0$ . When we toss the coin $N$ times and when we get $N_H$ and $N_T$ number of heads and tails, respectively, the posterior we have is $$p_H\sim \text{Beta}(a_0+N_H,b_0+N_T).$$ So, the mean value of $p_H$ is $\frac{a_0+N_H}{a_0+N_H+b_0+N_T}$ with a variance $\frac{(a_0+N_H)(a_0+N_H)}{N^2(N+1)}.$ My question here is Can we say the posterior distribution converges to the "true" distribution? when the true distribution (or the true scalar value of $p_H$ ) is never going to be observed?
