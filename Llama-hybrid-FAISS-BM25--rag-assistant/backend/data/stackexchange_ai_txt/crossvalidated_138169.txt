[site]: crossvalidated
[post_id]: 138169
[parent_id]: 138061
[tags]: 
50 features is a way too much for 150 samples. Autoencoder won't do anything good, unless you have access to large amount of unlabeled data. trying to apply autoencoder to just 150 samples unlikely to improve things much. Try do the following: If you don't use early stopping, use it. Its by far simplest method of regularization and usually good one. (stop training when error on your development set stop improving) Design better features. Start from removing unuseful ones, and use separate development set to find ones that work best. Maybe start from minimal set (one or two features). Having better features can help more than anything else. Other things that might help: use simpler model with smaller number of neurons up to a single neuron (logistic regression) try simple regularization like L1 and L2 regularization try "advanced" regularization methods like dropout, dropconnect, or full bayesian learning (this can be practical for your problem size and usually work good) consider generating synthetic dataset (sometimes it is possible to apply transformations to existing data that are known to preserve class label and make more data this way) try using different classifier, like decision trees or SVM if everything fails, you need more data, perhaps you can get unlabeled data samples to use autoencoders or stacked RBM. Without knowing the nature of your classification problem, I can't recommend anything better.
