[site]: datascience
[post_id]: 77272
[parent_id]: 77234
[tags]: 
Yes , gradient boosted trees can make predictions outside the training labels' range. Here's a quick example: from sklearn.datasets import make_classification from sklearn.ensemble import GradientBoostingRegressor X, y = make_classification(random_state=42) gbm = GradientBoostingRegressor(max_depth=1, n_estimators=10, learning_rate=1, random_state=42) gbm.fit(X,y) preds = gbm.predict(X) print(preds.min(), preds.max()) outputs -0.010418732339562916 1.134566081403055 (and make_classification gives outputs just 0 and 1). Now, this is unrealistic for a number of reasons: I'm using a regression model for a classification problem, I'm using learning rate 1, depth only 1, no regularization, etc. All of these could be made more proper and we could still find an example with predictions outside the training range, but it would be harder to construct such an example. I would say that in practice, you're unlikely to get anything very far from the training range . See the (more theoretical) example in this comment of an xgboost github issue , found via this cv.se post . To be clear, decision trees, random forests, and adaptive boosting all cannot make predictions outside the training range. This is specific to gradient boosted trees.
