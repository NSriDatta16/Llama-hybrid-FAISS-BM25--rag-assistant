[site]: crossvalidated
[post_id]: 358943
[parent_id]: 
[tags]: 
Change of optimal learning rate with small changes to architecture and data

I am training variants of similar neural networks, which differ slightly in the number of filters or layers. Additionally, the data is sometimes slightly changed using different preprocessing like random cropping or translation. Due to long training time, I do not want to run a grid search on all variants of the model. However, optimizing hyperparameters just for one model biases the comparison. I could not find any paper on the influence small changes in architecture have on the optimal hyperparameters. Is anyone aware of research on this topic or has some experience?
