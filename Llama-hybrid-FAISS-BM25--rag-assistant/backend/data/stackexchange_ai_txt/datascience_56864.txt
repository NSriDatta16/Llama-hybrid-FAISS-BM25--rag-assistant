[site]: datascience
[post_id]: 56864
[parent_id]: 56676
[tags]: 
I will exclude educated designs from my answer. No it is not possible to use an out of the box machine learning (ML) approach to fully represent the maximum function for arbitrary lists with arbitrary precision. ML is a data-based method and it is clear that you will not be able to approximate a function at regions where you do not have any data points. Hence, the space of possible observations (which is infinite) cannot be covered by finite observations. My statements have a theoretical foundation with Cybekoâ€™s Universal Approximation Theorem for neural networks. I will quote the theorem from Wikipedia: In the mathematical theory of artificial neural networks, the universal approximation theorem states[1] that a feed-forward network with a single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of $\mathbb{R}^n$ , under mild assumptions on the activation function. The theorem thus states that simple neural networks can represent a wide variety of interesting functions when given appropriate parameters; however, it does not touch upon the algorithmic learnability of those parameters. The most important part is the bounded subset of $\mathbb{R}^n$ . This additional statement restricts the application of approximating the maximum function for $x\in \mathbb{R}$ . This restriction is manifesting itself in the poor fit of the model from the answer with the most upvotes. If your space of observations is compact then you might be able to approximate the maximum function with a finite data set. As the top voted answer made clear you should not reinvent the wheel!
