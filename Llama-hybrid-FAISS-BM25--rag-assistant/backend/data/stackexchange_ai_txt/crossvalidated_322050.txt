[site]: crossvalidated
[post_id]: 322050
[parent_id]: 321659
[tags]: 
@AdamO points out, quite rightly that an estimator "is not necessarily the better estimator because it has less bias. An estimator with more bias can in fact converge much faster." For instance, one might ask "How do we measure the rate of convergence for an estimator" and secondarily "How can we compare two possibly biased estimators in terms of their rate of convergence?" A normal distribution is not an especially good example of what you are trying to ask. The best measure of location for a distribution is its minimum variance unbiased estimator MVUE . Let us take estimation of parameters of the Cauchy distribution . The mean is not robust, in fact, it will diverge from the Cauchy distribution's location parameter when more data is added because of the heavy tails of that distribution. The median is robust and converges to the distribution's location. A properly trimmed mean will be more robust than the median, and converge to the distribution's location faster. Another exemplary distribution is the uniform distribution, the median will converge to the location, the mean will converge faster, and the average extreme value will converge to the uniform distribution's location faster still. How one finds which measurement converges faster to the location is to look it up. For one thing, it takes mental effort coupled with trial and error to come up with better measurements if one starts from scratch. One can confirm easily enough, once one has an answer, what works better, but, one cannot specify a general methodology for creating better measurements other than to say, it is a bit like asking how one performs magic. Intuition perhaps, after having seen it done for a number of different cases. For example, a Pareto type I distribution has harmonic mean as a best measure of location overall, but demonstrating that might require doing Monte Carlo simulations. Finally, robustness is sometimes more important than bias, for example, ridge regression introduces bias which can decrease variance and increase robustness at the cost of that bias, nor is bias for fitting necessarily inappropriate as ill-posed problems may have optimal solutions that are not optimal for the data fitting, but rather are optimized for other considerations.
