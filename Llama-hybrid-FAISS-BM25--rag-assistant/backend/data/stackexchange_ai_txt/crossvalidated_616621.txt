[site]: crossvalidated
[post_id]: 616621
[parent_id]: 616582
[tags]: 
Have a look at llamaindex or langchain for information injection in the promt of the llm: https://gpt-index.readthedocs.io/en/latest/index.html This retrieve-augment-generate (RAG) works without Finetuning/prolonged pre-training but comes with the cost of having to pay for the additional tokens in your "pre-promt" which is where you inject your new information Also note that pretraining or fine-tuning modifies the parameters $\theta$ of your neural net $f_\theta(x)$ . RAG on the other hand improves/modifies the input $x$ supplied to your model.
