[site]: crossvalidated
[post_id]: 116071
[parent_id]: 116004
[tags]: 
Apologies for being brutally honest, but the answer posted by the @zkurtz is entirely wrong. The way @zkurtz did it NEVER rejects the normality hypothesis since it is not testing the normality of the transformed data, but the normality of the normal distribution. @zkurtz' procedure is like the following puzzle: Think of a number between 2 and 10. Calculate the sum of the two digits Divide the result by 3 THE FINAL RESULT IS 3!!!! WOW!!! Of course, because you are manipulating the input in such a way that you are destroying all of its information. What you had to do in the case of x=rlnorm(5000) is to take shapiro.test(log(x)) with the usual warning for large samples. The CLT applies to averages, not to individual observations! This is a typical puzzle for undergraduate students. The logged-data actually looks right-skewed, which could be modeled using a skew-normal, for example. This is equivalent to fitting a log-skew-normal to the original data. See the following illustrative R code. library(sn) set.seed(0) x = exp(rsn(5000,10,1,5)) hist(x,50) hist(log(x),50) # Data vs. Theoretical model hist(log(x),50,probability=T) tempf = function(x) dsn(x,10,1,5) curve(tempf,8,15,col="red",add=T)
