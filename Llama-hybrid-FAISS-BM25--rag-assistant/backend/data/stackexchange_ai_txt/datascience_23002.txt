[site]: datascience
[post_id]: 23002
[parent_id]: 
[tags]: 
Classifying Sequences Where Some Sequences in Both Classes

I am building a modified RNN (specifically a GRU) to classify sequences. These sequences are of variable length, and contain "states". Each point in the sequence is categorical and they look like [A,B,C] or [A,A,C,B], etc. Classification will be 1 or 0 depending on wether or not the user in the sequence performed an action. I face an issue where some of these sequences appear in both classes. In addition, one class occurs less than 10% of the time. In training on a downsampled dataset, the RNN can usually reach 75+% accuracy, with similar Specificity and Sensitivities. In Testing, the sensitivity falls dramatically to under 20%, with the specificity rising to 95%+. This is less than desirable, and I believe the issue may stem from the sequences that appear in both classes. A sequence that is positive most of the time in training will usually be negative most of the time in testing (as the testing set is unbalanced). I assume that the net learns the distributions more than the sequences, which explains the stark contrast in results. To people with experience working on these kind of imbalances and dataset issues, is my assumption correct? What advice do you have to handle data where the same sequence can appear in both classes? As next steps, I am thinking of weighting the minority class, and training on a non-downsampled dataset to preserve the distribution of sequences in each class. I am also considering a "representative" training and testing set, where if sequence [A,B,A] appears in a 6:1 negative to positive ratio, I will downsample to the extent that I can preserve the majority. Hopefully that will lead to less imbalance. There is also the option to add more "depth" to the training data. Each point in the sequence has multiple features (usually hierarchical) that add more specifics about each state, this may decrease the number of sequences that appear in both classes. However, I am hesitant to make things more complex if there is a chance the network isn't learning properly.
