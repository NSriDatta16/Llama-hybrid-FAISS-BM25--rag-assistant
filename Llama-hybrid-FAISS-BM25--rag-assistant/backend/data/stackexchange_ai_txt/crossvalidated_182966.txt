[site]: crossvalidated
[post_id]: 182966
[parent_id]: 182936
[tags]: 
The simple answer for all your questions is: in Bayesian model you include a priori information in your model besides the data, $$ \text{posterior} \propto \text{likelihood} \times \text{prior} $$ so if you include in your model additional information, than your estimates (mean, standard deviation etc.) can possibly differ from maximum likelihood estimates. You can use noninformative priors , and in this case estimates should be the same as in maximum likelihood case. One example where Bayesian methods can outperform other methods are cases where we are dealing with small samples (see nice example here ), even in cases such as predicting future mid-air collisions in times when non such collision happened yet - in such cases out-of-data information helps us to learn from insufficient data. You ask also about posterior distribution vs. sample mean and about point estimates. In posterior distribution you have a whole distribution for your parameter of interest instead of a point value, you can take mean (or median, or possibly other statistics) of this distribution to get a point value. If you are interested only in point estimate, you can use maximum a posteriori methods and not bother with posterior distribution. Finally, neither of the methods are "better", or "worse". They are just different. By reviewing multiple questions tagged as bayesian you can learn much about their pros and cons, for example: When are Bayesian methods preferable to Frequentist? , or Why are Bayesian methods widely considered particularly "convenient"? .
