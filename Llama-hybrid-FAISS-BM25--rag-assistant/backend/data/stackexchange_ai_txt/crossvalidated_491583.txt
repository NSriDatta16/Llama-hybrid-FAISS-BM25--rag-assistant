[site]: crossvalidated
[post_id]: 491583
[parent_id]: 308044
[tags]: 
I think it is because pretraining is both helpful for generalization and optimization. When the input data similarity is significant between two domains the generalization works here because the lower layers learn the basic features shared by the domains, for instance, we often use BERT for feature extraction for downstream NLP tasks. While when the input data similarity is less the dense layers can supervise/guide the lower learnable layers to learn, functioning as optimization, for example, we can employ some high-level layers for a style transfer model. Take for one more example, we can use the last several layers in BERT for the ASR task since the input data for BERT is text but for ASR the input data is the sound wave but the result/target is the same: text. In this case, the parameters of the layers can be involved in the loss function, like the use of Gram matrix in style transfer learning.
