[site]: crossvalidated
[post_id]: 502651
[parent_id]: 502603
[tags]: 
It's not normalized twice! It's normalized once, using two steps. I say it's normalized once because all the normalization does is apply two linear transformations. We can rewrite two or more linear transformations as a single linear transformation. All we need to do is to choose a certain linear function to transform the raw pixel values to the values that the neural network expects to receive. We can show that the two-step process achieved in code is exactly the same as an equivalent operation carried out in one step. The network expects to receive images with a certain scale, but images are encoded by with values between 0 and 255. The route suggested in the documentation is two-step. For some pixel $p\in[0,255]$ , divide by 255: $q = \frac{p}{255}$ . If we wanted to emphasize that this is a "linear scale and shift" transformation, we could even write $q = \frac{p-0}{255}$ . Subtract the mean and divide by the standard deviation: $z = \frac{q - \mu}{\sigma}$ . The network expects to receive $z$ as inputs. We can do this in one step instead, because the composition of linear functions is linear . Just doing substitution and rearranging, we can show $$\begin{align} z&=\frac{q - \mu}{\sigma} \\ &=\frac{p/255 - \mu}{\sigma} \\ &=\frac{p - 255\mu}{255\sigma} \\ \end{align} $$ So in the specific case, you can achieve the exact same scaling using the transformation albumentations.Normalize(mean=[255*0.485, 255*0.456, 255*0.406], std=[255*0.229, 255*0.224, 255*0.225]) . This should make intuitive sense, because we're just rescaling the transformation to take place on the $[0,255]$ interval of pixel values, instead of a $[0,1]$ interval.
