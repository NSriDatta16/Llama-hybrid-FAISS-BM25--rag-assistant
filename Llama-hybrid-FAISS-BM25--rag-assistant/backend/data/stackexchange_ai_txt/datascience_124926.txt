[site]: datascience
[post_id]: 124926
[parent_id]: 124925
[tags]: 
From the xgboost documentation https://xgboost.readthedocs.io/en/stable/tutorials/dart.html . Vinayak and Gilad-Bachrach proposed a new method to add dropout techniques from the deep neural net community to boosted trees, and reported better results in some situations. Drop trees in order to solve the over-fitting. Trivial trees (to correct trivial errors) may be prevented. This argument falls in the sampling category: sample_type: type of sampling algorithm uniform: (default) dropped trees are selected uniformly. weighted: dropped trees are selected in proportion to weight. In essence this is lightGBMs attempt at dropout (regularization). In a NN dropout regularization randomly omits units from the hidden layers during training.
