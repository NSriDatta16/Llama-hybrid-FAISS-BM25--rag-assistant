[site]: crossvalidated
[post_id]: 240001
[parent_id]: 238846
[tags]: 
Expanding on the excellent answer by @juho-kokkala and using R here are the results. For a prior distribution for the population mean mu I used an equal mixture of two normals with mean zero, one of them very skeptical about large means. ## Posterior density for a normal data distribution and for ## a mixture of two normal priors with mixing proportions wt and 1-wt ## and means mu1 mu2 and variances v1 an ## Adapted for LearnBayes package normal.normal.mix function ## Produces a list of 3 functions. The posterior density and cum. prob. ## function can be called with a vector of posterior means and variances ## if the first argument x is a scalar mixpost 0 in trials for ## which posterior prob(mu > 0) > 0.95, and also use a loess smoother ## to estimate prob(mu > 0) as a function of the final post prob ## In sequential analyses of observations 1, 2, ..., N, the final ## posterior prob is the post prob at the final sample size if the ## prob never exceeds 0.95, otherwise it is the post prob the first ## time it exceeds 0.95 sim = postcut))] stopped[i] = postcut)) } list(mu=Mu, post=Post, stopped=stopped) } # Take prior on mu to be a mixture of two normal densities both with mean zero # One has SD so that Prob(mu > 1) = 0.1 # The second has SD so that Prob(mu > 0.25) = 0.05 prior.sd mu = 0.95], nclass=25) k = 0.95 mean(k) # 0.44 of trials stopped with post >= 0.95 mean(st) # 313 average sample size mean(mu[k] > 0) # 0.963 of trials with post >= 0.95 actually had mu > 0 mean(post[k]) # 0.961 mean posterior prob. when stopped early w 0, iter=0) # perfect calibration of post probs plot(w, type='n', # even if stopped early xlab=expression(paste('Posterior Probability ', mu > 0, ' Upon Stopping')), ylab=expression(paste('Proportion of Trials with ', mu > 0))) abline(a=0, b=1, lwd=6, col=gray(.85)) lines(w) 0 vs. posterior probability at stopping">
