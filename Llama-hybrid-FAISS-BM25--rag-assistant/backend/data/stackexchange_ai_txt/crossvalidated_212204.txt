[site]: crossvalidated
[post_id]: 212204
[parent_id]: 211582
[tags]: 
Although I don't have the exact answer to your question, a general thought I had w.r.t. adding prior information about features to neural networks was: Normally we use weight decay, which means unless the feature decreases error on the training set reduce the weight to zero. What happens if we add a reverse of weight decay term to the weight of this feature, i.e. unless the training prediction error increases keep increasing the weight of this feature, is that kind of an prior knowledge added to the system? Else does initializing the weight of this feature as a larger positive number than other features help?
