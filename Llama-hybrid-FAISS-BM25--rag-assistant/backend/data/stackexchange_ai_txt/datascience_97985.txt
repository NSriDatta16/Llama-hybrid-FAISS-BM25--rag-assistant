[site]: datascience
[post_id]: 97985
[parent_id]: 
[tags]: 
text classification - does number of features matters?

I'm working on a multi-class text classification project that aims to assign a "new bug" to his "final group assignee" To do that I was able to extract ~17000 samples and divided each sample to the below (all text features): Subject Description Comments Has images (boolean field) ==> ('yes' or 'no') Involved groups Committer groups Reporter group Assignee group Group name I've 6 classes (labels) overall My main question: Since all of my data is text, I'm kinda wondering: does the neural network expects all features (columns) to be gathered to a single one? (Since all of them will be feed via some kind of tokenizer anyway) I've tried 2 approaches/libraries ( sklearn and keras ): When using sklearn, I kept the columns and processed each one with TfidfVectorizer , but when I tried to explore keras, I couldn't (or didn't understand how to) do it. Also - for keras, all of the examples I've seen so far took 1 columns and processed it. I've very confused with how I should continue from this point on. I'll just mention, that using sklearn I was able to get ~70% score vs keras (after uniting all columns) where I got ~60%
