[site]: crossvalidated
[post_id]: 316452
[parent_id]: 316333
[tags]: 
Say that you are predicting sells of some products given their price and some other variables. Your data is noisy, since you have many different products and there are many factors that you are not able to account. You may assume that there is some kind of effect that may be approximated with linear function (better price leads to better sells). You need this model for doing future predictions of different products, sold in different part of the year, so possibly by different clients etc., so basically lot's of things may change. Surely you could use many different methods for approaching this problem, but in many cases linear regression would be something that you would start and end with. There are many reasons for this, e.g. more complicated models would possibly overfit, simple model would be more robust, this is important if you care about out-of-sample errors (and you care), by design it would give you the results that are "on average" correct, if you have products that sell very well and very bad, then it possibly wouldn't give you the exactly correct results for them, but "in total" it should give you the balanced solution, regression will work out-of-the-box for many cases, it will work even for larger sets of the data and it would be fast, it is easily interpretable, so it would be easy to explain to the management what is your model and how did it predict what it did, this is something that you cannot say about many of the machine learning models, etc. Finally, there is many different measures of "accuracy" for predictions, different models would aim at minimizing different loss functions. Regression minimizes the squared errors, but it may be so that you need to minimize something different and then it obviously would be sub-optimal.
