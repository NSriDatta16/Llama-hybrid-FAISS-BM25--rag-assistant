[site]: datascience
[post_id]: 75652
[parent_id]: 
[tags]: 
Which supervised machine learning algorithms assume normally distributed feature variables?

I want to understand the assumptions made by supervised machine learning models. I've heard it said many times that 'you need to make sure your feature variables are normally distributed for your ML models to work.' However, when I looked up the assumptions for Linear Regression, I found many conflicting viewpoints. This post and this post mention normality of error distributions. The latter even says that features do not need to be normally distributed - just the errors. Statistics solutions says that features do need multivariate normality, as do a lot of top beginner ML courses such as Machine Learning A-Z . Wikipedia says that features do not need to be normally distributed, but it has a major influence on the precision of estimates . Which algorithms assume that the feature variables are normally distributed? For ones that don't, why is it beneficial to scale your features so that they are more normally distributed?
