[site]: datascience
[post_id]: 46648
[parent_id]: 
[tags]: 
What does 'Linear regularities among words' mean?

Context : In the paper "Efficient Estimation of Word Representations in Vector Space" by T. Mikolov et al., the authors make use of the phrase: 'Linear regularities among words'. What does that mean in the context of the paper, or in a general context related to NLP? Quoting the paragraph from the paper: Somewhat surprisingly, it was found that similarity of word representations goes beyond simple syntactic regularities. Using a word offset technique where simple algebraic operations are performed on the word vectors, it was shown for example that vector(”King”) - vector(”Man”) + vector(”Woman”) results in a vector that is closest to the vector representation of the word Queen [20]. In this paper, we try to maximize accuracy of these vector operations by developing new model architectures that preserve the linear regularities among words . We design a new comprehensive test set for measuring both syntactic and semantic regularities1 , and show that many such regularities can be learned with high accuracy. Moreover, we discuss how training time and accuracy depends on the dimensionality of the word vectors and on the amount of the training data.
