[site]: stackoverflow
[post_id]: 1051357
[parent_id]: 1051347
[tags]: 
The main issue here TMO is about indexing. If you're going to search information in a huge file without a good index, you'll have to scan the whole file for the correct information which can be long. If you think you can build strong indexing mechanisms then fine, you should go with the huge file. I'd prefer to delegate this task to ext3 which should be rather good at it. edit : A thing to consider according to this wikipedia article on ext3 is that fragmentation does happen over time. So if you have a huge number of small files which take a significant percentage of the file system then you will lose performances over time. The article also validate the claim about 32k files per directory limit (assuming a wikipedia article can validate anything)
