[site]: crossvalidated
[post_id]: 574928
[parent_id]: 407033
[tags]: 
I will add a couple of points to Tim's answer, focussing on the original question, which was "My question is - is this valid? Am I missing anything here?". I think the approach can be valid, but... it depends on lots of details. Regarding the question of whether you "remove" or "dilute" the dependence when you downsample I feel this is more about semantics than anything else. If you downsample enough you will get to a point where for all practical purposes you have removed the correlation. So if you had lots and lots of data and enough signal on them, that could certainly be a pragmatic approach. And if the residuals of the model showed no "significant" correlation after the fitting, then you'd be home safe. In a way... because, nonetheless, that is sub-optimal for at least 4 reasons: By throwing away data you are throwing away useful information This introduces an arbitrary treshold via the chosen downsampling factor (which if you downsample too much, might actually limit your ability to find existing significant effects, say Depending on the sample you select out of the complete data set you could end up with different results, so by doing it you are really ignoring a component of the variability in the reported estimates (but then again, this could be negligible, and you could try to work around it by doing this many times via a resampling procedure) You might be interested in the correlation structure itself (although I assume this is not the case for you) So I would not dismiss it if it saves you the headaches I suspect might come along with properly fitting a model with the correct correlation structure at the scale you collected the data. On the other hand, you should think that if you had collected the data once every 10 minutes you would only have 1/10th of the data any way. So I guess an important factor to consider is at what sclae are you interested in describing the phenomena you are trying to model? Nowdays, with some sensors available to researchers, say, sound sensors sampling at 256kHz, one might argue that for practical purposes throwing away large amounts of data might be OK. This will always be an "it depends on the specific setting" question, so I'm not really advocating one might do that as a blind routine. But it might be a pragmatic effecient approach. Not really relevant for the point being discussed, but commenting on a point that @tim noted, I believe the reason that thinning is not recommended in an MCMC analysis is not that there is still correlation after you thin, but because this is always an inneficient procedure since you use the vales of the MC as a posterior distribution, and essentially you actualy marginalize over time (you ignore the temporal autocorrelation). Hence, removing values, even if they are correlated, is always less eficient that using all the values (this provided the chain has converged, but if it has not, no need to try to make inferenc3es out of it). Then again, I have like 61 reputation points and @tim does have thousands, so he might know stuff I don't ;) !
