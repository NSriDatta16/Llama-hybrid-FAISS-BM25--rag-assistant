[site]: crossvalidated
[post_id]: 610513
[parent_id]: 444210
[tags]: 
It depends on how you define $R^2$ , and there are several legitimate definitions, with the two below being the most reasonable to me. $$ R^2 =\left(\text{corr}\left(\hat y, y\right)\right)^2\\ R^2=1-\left(\dfrac{ \overset{N}{\underset{i=1}{\sum}}\left( y_i-\hat y_i \right)^2 }{ \overset{N}{\underset{i=1}{\sum}}\left( y_i-\bar y \right)^2 }\right) $$ $R^2 =\left(\text{corr}\left(\hat y, y\right)\right)^2$ will not care about bias. If you predict one unit too high every time, the correlation is perfect. Quoting an answer of mine from Data Science , for any real $a$ and positive $b$ , $ \left(\text{corr}\left(\hat y, y\right)\right) = \left(\text{corr}\left(a + b\hat y, y\right)\right) $ . In fact, for nonzero $b$ , $ \left(\text{corr}\left(\hat y, y\right)\right)^2 = \left(\text{corr}\left(a + b\hat y, y\right)\right)^2 $ . Concretely, if your true values are $y=(1,2,4,6,5)$ , $\hat y=(12, 13, 15, 17, 16)$ makes for terrible predictions but does have a perfect correlation with $y$ . (In this example, $a=11$ and $b=1$ .) For the second formula, the numerator is just a function of the mean squared error. Since the denominator is a function of the data and not of any particular model, regard the denominator as a constant (which is is for any given data set). Then the second formula is just a decreasing function of the mean squared error. Since, all else equal, mean squared error increases when bias magnitude increases ( $MSE = \text{bias}^2 + \text{var}$ ), you can regard that equation for $R^2$ as moving in the opposite direction of bias magnitude, yes. As bias magnitude decreases, $R^2$ increases, and as bias magnitude increases, $R^2$ decreases (assuming equal variance in both cases). This makes sense to me. Holding the variance equal, higher bias magnitude means a worse fit, which should correspond to a lower $R^2$ .
