[site]: datascience
[post_id]: 112496
[parent_id]: 
[tags]: 
logistic regression,machine learning

I just started learning ML, so I tried to implement logistic regression on my own in 2 ways (the first code and the second code), but they aren't working, I cross-checked by using sklearn (third code given below), could anyone help me in figuring out the mistake I have been doing. The link to the dataset used in this code is here dataset FIRST CODE vectorized implementation of logistic regression import numpy as np import pandas as pd import matplotlib.pyplot as plt #sigmoid function def sigmoid(z): a=1/(1+np.exp(-z)) return a #predicted y value for a given w,b ans x def y_out(x_test,w_ini,b_ini): m,n=x_test.shape a=sigmoid(np.dot(w_ini,np.transpose(x_test))+b_ini) a=a.reshape(m,1) return a #loss of a single training example def loss(x_train,y_train,w_ini,b_ini): l=0 l=-(y*np.log(y_out(x_train,w_ini,b_ini))+(1-y)*np.log(1-y_out(x_train,w_ini,b_ini))) return l #the cost function def cost(x,y,w,b): J=0 m,n=x.shape J=(1/m)*np.sum(loss(x,y,w,b)) return J #derivative terms def gradient(x_train,y_train,w_ini,b_ini): m,n=x_train.shape dw=np.zeros(n) dw=dw.reshape(len(dw),1) db=0. dw=(1/m)*(np.dot(np.transpose(x_train),y_out(x_train,w_ini,b_ini)-y_train)) db=(1/m)*(np.sum(y_out(x_train,w_ini,b_ini)-y_train)) dw=dw.reshape(1,len(dw)) return dw,db #gradient descent def optimize(x_train,y_train,w_ini,b_ini,alpha,itirations): m,n=x.shape w=w_ini b=b_ini for i in range(itirations): dw,db=gradient(x_train,y_train,w,b) w=w-alpha*(dw) b=b-alpha*(db) return w,b dataset=pd.read_csv("ex2data1.csv") x=dataset.iloc[:,:-1].values y=dataset.iloc[:,-1].values y=y.reshape(len(y),1) m,n=x.shape #initialization w=np.zeros(n) b=0. #running the algoritm w_out,b_out=optimize(x,y,w,b,1e-5,10000) #parameters after running gradient descent print(w_out,b_out) SECOND CODE, Logistic regression using loops import numpy as np import matplotlib.pyplot as plt import pandas as pd #definition of the sigmoid function def sigmoid(z): f=1/(1+(np.e)**-z) return f #cost def cost(x,y,w,b): m,n=x.shape cost=0 for i in range(m): cost_i=(-y[i]*(np.log(sigmoid(np.dot(w,x[i]))+b)))-((1-y[i])*(np.log(1-sigmoid(np.dot(w,x[i]))+b))) cost+=cost_i return cost/m #gradient(derivatives) def gradient(x,y,w,b): m,n=x.shape dj_dw=np.zeros(n)#derivative terms for each feature dj_db=0. for i in range(m): z_wb=np.dot(w,x[i])+b f_wb=sigmoid(z_wb) for j in range(n): dj_dw[j]+=(f_wb-y[i])*x[i][j] dj_db+=f_wb-y[i] dj_dw=dj_dw/m dj_db=dj_db/m return dj_dw,dj_db #gradient descent def gradient_descent(x,y,w_in,b_in,alpha,num_iters): m,n=x.shape w=w_in b=b_in for i in range(num_iters): dj_dw,dj_db=gradient(x,y,w,b) w=w-alpha*dj_dw b=b-alpha*dj_db return w,b #predicting def predict(x,w,b): m,n=x.shape p=np.zeros(m) for i in range(m): z_wb=np.dot(w,x[i])+b f_wb=sigmoid(z_wb) if(p[i]>=0.5): p[i]=1 else: p[i]=0 return p #preprocessing dataset=pd.read_csv("ex2data1.csv") x_train=dataset.iloc[:,:-1].values y_train=dataset.iloc[:,-1].values y_train=y_train.reshape(len(y_train),1) #initialization m,n=x_train.shape w=np.zeros(n) b=0. w_out,b_out=gradient_descent(x_train,y_train,w,b,1e-4,10000) print(w_out) print(b_out) #predicting y_pred=predict(x_train,w_out,b_out) THIRD CODE,logistic regression using sklearn import numpy as np import pandas as pd import matplotlib.pyplot as plt dataset=pd.read_csv("ex2data1.csv") x=dataset.iloc[:,:-1].values y=dataset.iloc[:,-1].values from sklearn.model_selection import train_test_split x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25,random_state=1) from sklearn.preprocessing import StandardScaler sc=StandardScaler() x_train=sc.fit_transform(x_train) x_test=sc.transform(x_test) from sklearn.linear_model import LogisticRegression classifier=LogisticRegression() classifier.fit(x_train,y_train) y_pred=classifier.predict(x_test) from sklearn.metrics import confusion_matrix,accuracy_score cm=confusion_matrix(y_test,y_pred) print(cm) accuracy_score(y_test,y_pred) print(classifier.coef_) print(classifier.intercept_)
