[site]: crossvalidated
[post_id]: 364200
[parent_id]: 364089
[tags]: 
Suppose that a screen $x_t$ can be encoded as a vector of $P$ pixels and each pixel can take on $R$ possible values. Also assume $A$ possible actions. (In Atari games the action space is discrete and finite -- our actions at each timestep are the buttons "UP/DOWN/LEFT/RIGHT/...".) Then there are $PR \cdot A$ possible pairs $(x_t, a_t)$, and with an upper bound of $T$ timesteps, there are $PR + PR \cdot A \cdot (1 + 2 + \dots + T-1)$ possible sequences, ie states. So we have a finite number of states and a finite number of actions (and therefore a finite MDP). Later in the paper the authors reduce the size of these inputs by using a feature-extracting function φ: Since using histories of arbitrary length as inputs to a neural network can be difficult, our Q-function instead works on fixed length representation of histories produced by a function φ. The Deep Q-learning network used in this paper predicts the Q-value of each action from the state features. So in other words, the neural net assigns a score to each action that the agent can take. After an exploration phase, the agent will always choose the action that has the highest predicted Q-value.
