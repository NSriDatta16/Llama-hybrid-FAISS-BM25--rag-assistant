[site]: datascience
[post_id]: 112768
[parent_id]: 112733
[tags]: 
It would not change much. Instead of using a random policy, you would just replace the opponent with the policy you are training (assuming it is a zero-sum game, where both agents have the same actions and aim to achieve the same objective) $\pi_{\theta}(a\mid s)$ . You probably want to update the policy of the opponent less often than the policy you are training for stability. In the case of tictactoe, chess or Go, a typical reward would be 1 if we win and -1 if we lose, that would be given at the very end of the episode.
