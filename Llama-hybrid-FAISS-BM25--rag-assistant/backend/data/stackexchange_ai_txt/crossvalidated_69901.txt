[site]: crossvalidated
[post_id]: 69901
[parent_id]: 26558
[tags]: 
I would suggest using Hadoop and RMR (a specific package for Map Reduce in R). With this strategy you can run large datasets on comodity computers with an affordable configuration (probably in two hours you come up with both Hadoop and RMR (RHadoop) installed and running). In fact, if you have more than one computer you can create a cluster, reducing the processing time. I give you some links supporting my suggestion: This link will lead you to a tutorial for installing Hadoop on a single-node cluster (one computer). This link and this link will show you how to install RMR on your Hadoop cluster. And finally, here you may find an example of logistic regression by means of RHadoop. So, my advice is to follow these guidelines as it is certainly worthy if your data is huge.
