[site]: crossvalidated
[post_id]: 604321
[parent_id]: 
[tags]: 
Several Questions About "Prioritized Experience Replay"

I have 3 questions about "Prioritized Experience Buffer" as described in the paper . what's the point of the importance sampling (IS)? I'll explain - I understand that: a. when we prioritize some samples, we change the distribution. b. by doing that, we change the excepted value and therefore the final solution the agent will converge to. but, isn't that an issue only if the first solution is better ? the first solution is based on uniform sampling, and to me it sounds like there is no reason to think this is a more "real" distribution, since these samples were collected using older policy. samples that are common in the buffer can be very rare given the current policy. even if we do want to correct the distribution, why do we need the IS? the authors start with some beta and slowly raising it to 1, therefore completely compensating for the bias. can't we just remove the IS, start with an alpha of 0.6 and slowly lower it to 0? this one is not related to IS. the authors acknowledge that updating each priority after each update to the agent is too computationally expensive and time consuming. therefore, they only update the priority for the samples they learn from. but as the agent learns, the priorities of all of the samples that were not learned from for a while becomes outdated. how come this improvement to the uniform replay buffer improve learning after all?
