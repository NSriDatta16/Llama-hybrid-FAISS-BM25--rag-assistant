[site]: crossvalidated
[post_id]: 288703
[parent_id]: 250258
[tags]: 
Have you considered simply scaling the images in the preprocessing stage? Intuitively, a human facing a scaled image will still be able to recognize the same features and objects, and there's no obvious reason why a CNN wouldnt be able to do the same thing on a scaled image. I think that scaling the images to be the same size might be easier than trying to make a convolutional network handle images of different sizes, which I think would be up there in 'original research' land. You can certainly make the conv layers of a convnet handle images of any size, without retraining. However, the output of a convnet will typically be some kind of classifier, and this will probably work less well, if you feed in inputs of different size, I would imagine. Another approach would be to just pad the images with zeros. But imagine intuitively you are looking at either a tiny photo, padded with black borders, or you can zoom in, so it subtends a reasonable arc in your visual field. Which would you do? Which is easier to see?
