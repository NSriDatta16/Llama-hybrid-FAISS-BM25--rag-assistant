[site]: crossvalidated
[post_id]: 610671
[parent_id]: 
[tags]: 
How to prove that neural network estimates posterior distribution

Let's say that I train a neural network in a classic binary classification setting where all the training data has labels in $\{-1, +1\}$ . From my understanding, if I train the network with a log-loss function and softmax output layer, the network outputs will essentially estimate $P(y \mid \mathbf{x})$ . How do I prove this convergence mathematically? In other words, if $h(\mathbf{x};y)$ is the output of the network, how do I show that the minimizing the loss brings $h(\mathbf{x};y)$ closer to $P(y \mid \mathbf{x})$ over all $\mathbf{x}$ over time? Thanks!
