[site]: stackoverflow
[post_id]: 4105155
[parent_id]: 4101924
[tags]: 
Since there are a few misconceptions flying around here, I’d like to clarify some points. The “in-place” quicksort isn’t really in-place (and quicksort is not by definition in-place). It requires additional storage in the form of stack space for the recursive step, which is in the order of O (log n ) in the best case, but O ( n ) in the worst case. Implementing a functional variant of quicksort that operates on arrays defeats the purpose. Arrays are never immutable. The “proper” functional implementation of quicksort uses immutable lists. It is of course not in-place but it’s got the same worst-case asymptotic runtime ( O ( n ^2)) and space complexity ( O ( n )) as the procedural in-place version. On average, its running time is still on par with that of the in-place variant ( O ( n log n )). Its space complexity, however, is still O ( n ). There are two obvious disadvantages of a functional quicksort implementation. In the following, let’s consider this reference implementation in Haskell (I don’t know Scala …) from the Haskell introduction : qsort [] = [] qsort (x:xs) = qsort lesser ++ [x] ++ qsort greater where lesser = (filter ( = x) xs) The first disadvantage is the choice of the pivot element , which is very inflexible. The strength of modern quicksort implementations relies heavily on a smart choice of the pivot (compare “Engineering a sort function” by Bentley et al. ). The above algorithm is poor in that regard, which degrades average performance considerably. Secondly, this algorithm uses list concatenation (instead of list construction) which is an O ( n ) operation. This doesn’t impact on the asymptotic complexity but it’s a measurable factor. A third disadvantage is somewhat hidden: unlike the “in-place” variant, this implementation continually requests memory from the heap for the cons cells of the list and potentially scatters memory all over the place. As a result, this algorithm has a very poor cache locality . I don’t know whether smart allocators in modern functional programming languages can mitigate this – but on modern machines, cache misses have become a major performance killer. What’s the conclusion? Unlike others, I wouldn’t say that quicksort is inherently imperative and that’s why it performs badly in an FP environment. Quite on the contrary, I would argue that quicksort is a perfect example of a functional algorithm: it translates seamlessly into an immutable environment, its asymptotic running time and space complexity are on par with the procedural implementation, and even its procedural implementation employs recursion. But this algorithm still performs worse when constrained to an immutable domain. The reason for this is that the algorithm has the peculiar property of benefitting from a lot of (sometimes low-level) fine-tuning that can only be efficiently performed on arrays. A naive description of the quicksort misses all these intricacies (both in the functional and in the procedural variant). After reading “Engineering a sort function” I can no longer consider quicksort an elegant algorithm. Implemented efficiently, it is a clunky mess, an engineer’s piece of work, not an artist’s (not to devalue engineering! this has its own aesthetic). But I would also like to point out that this point is particular to quicksort. Not every algorithm is amenable to the same sort of low-level tweaking. A lot of algorithms and data structures really can be expressed without performance loss in an immutable environment. And immutability can even decrease performance costs by removing the need of costly copies or cross-thread synchronizations. So, to answer the original question, “ is immutability expensive? ” – In the particular case of quicksort, there is a cost that is indeed a result of immutability. But in general, no .
