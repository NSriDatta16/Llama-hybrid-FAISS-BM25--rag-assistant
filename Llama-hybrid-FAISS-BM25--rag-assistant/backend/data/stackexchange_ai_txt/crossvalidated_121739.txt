[site]: crossvalidated
[post_id]: 121739
[parent_id]: 121576
[tags]: 
Compared to many other situations such as in NLP (natural language processing), you have 189 samples and 4 features which are not bad. Besides, the example you gave is a typical example (you should have seen many similar examples) for your samples. That is an intuitive reason why your prediction should not be "so wrong". I think the bootstrapping won't help here in this case because you don't introduce any new information into the samples. If you are able to introduce some more information and then create "virtual samples", this would be helpful. However, it seems dangerous here unless there are already proved medical evidence to justify this approach. Finally, I have the impression that here it is the variable selection procedure which makes your regression model good or bad. Carefully choose the right variable to integrate into the regression model can make the results more convincing. (Perhaps it is something you have already done as your model has fewer variables than the original file).
