[site]: crossvalidated
[post_id]: 493212
[parent_id]: 492171
[tags]: 
Let's start with this equation. $\sum_{n=k}^{\infty}{{n \choose k}\pi^k(1-\pi)^{n-k}} $ Hopefully this is self-explanatory, but as just an intuition you can see this as a brute-force way of calculating your distribution by finding the probability that n pulls came from each possible binomial function. You would then need to divide by some constant $C$ such that: $\sum_{n=k}^{\infty}{\frac{{n \choose k}\pi^k(1-\pi)^{n-k}}{C}} = 1 $ to get a PMF. So if we can figure out what C is, we've got our distribution (even if we don't know the (EV or Varience). The internet (wolframalpha) seems to be suggesting to me that C is equal to $1/\pi$ . Feel free to confirm, but I'm sure that's correct. With that constant, the model simplifies to: ${n \choose k}\pi^{k+1}(1-\pi)^{n-k} $ I ran a simulation with this equation where we have observed n=1 with a probability of .5. > pmf > > graph for(i in 1:100){ + graph[i] > plot(graph) > sum(graph) [1] 1 Hopefully this distribution "rings true." 25% of the time we observe 1 success with 50% chance of success, it was after n=1, 25% it was n=2, and then it trails of exponentially from then. The expected value of our distribution is given by: $\sum_{n=k}^{\infty}{n{n \choose k}\pi^k(1-\pi)^{n-k}} $ and variance: $ E((X-E(n)^{2}) $ Unfortunately I don't currently have the time to solve those, but I challenge someone here to do so. Edit: others have suggested a Bayesian solution for this problem. My bone to pick with those is that they assume that n HAS a distribution. Your question seems to assume that N is only distributed in as far as it is dependent on the binomial distribution(s) of $\pi$ .
