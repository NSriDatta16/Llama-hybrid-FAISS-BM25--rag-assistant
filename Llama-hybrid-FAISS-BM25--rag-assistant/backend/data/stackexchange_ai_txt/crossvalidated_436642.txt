[site]: crossvalidated
[post_id]: 436642
[parent_id]: 436617
[tags]: 
Let $X_1,\dots,X_n \mid \lambda \sim \mathcal{P}(\lambda)$ The likelihood of this model is then \begin{align*} L(\lambda ; X) &= \prod_{i=1}^{n} e^{-\lambda} \frac{\lambda^{X_i}}{X_i!} \\ &= e^{-n \lambda} \frac{\lambda^{ \sum X_i}}{ \prod X_i!} \end{align*} The prior on $\lambda$ being a $\mathcal{G}(a,b)$ , we have $$ p(\lambda ; a,b) = \frac{b^a \lambda^{a-1} e^{-b \lambda }}{\Gamma(a)} $$ From Bayes formula the posterior is \begin{align*} p( \lambda \mid X ;a,b) &\propto p(\lambda ; a,b) L(\lambda ; X)\\ &\propto \lambda^{\sum X_i + a-1} e^{-\lambda(n+b)} \end{align*} The posterior distribution of $\lambda$ is then $\mathcal{G}(\sum X_i + a,n+b)$ One interesting property of the Gamma-Poisson mixture is that the marginal distribution is Negative Binomial. That is, given a particular value of $\lambda$ , $X \mid \lambda$ will follow a Poisson distribution. But if we average over the distribution of $\lambda$ , then the marginal distribution of $X$ is Negative-Binomial. From the section 4 this pdf , if $\lambda \sim \mathcal{G}(a,b)$ the marginal distribution of $X$ is $\mathcal{NB}(a, \frac{1}{b+1})$ . Thus since in our case we have $\lambda \sim \mathcal{G}(\sum X_i + a,n+b)$ , the posterior predictive distribution of $X$ knowing $(X_1,\dots,X_n)$ is $\mathcal{NB}(\sum X_i + a, \frac{1}{1+n+b})$ And you can use that to compute the probability of getting $k$ customers : $$ \mathbb{P}\big (X_{n+1} = k \mid (X_1,\dots,X_n) \big) = \binom{k+\sum X_i + a-1}{k}\Big(1-\frac{1}{1+n+b}\Big)^{\sum X_i + a}\frac{1}{(1+n+b)^k} $$ which is the probability mass function of a Negative Binomial distribution.
