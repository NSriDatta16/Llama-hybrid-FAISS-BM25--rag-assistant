[site]: crossvalidated
[post_id]: 13429
[parent_id]: 13086
[tags]: 
There is a generalization of standard box-plots that I know of in which the lengths of the whiskers are adjusted to account for skewed data. The details are better explained in a very clear & concise white paper (Vandervieren, E., Hubert, M. (2004) "An adjusted boxplot for skewed distributions", see here ). There is an $\verb+R+$ implementation of this ($\verb+robustbase::adjbox()+$) as well as a matlab one (in a library called $\verb+libra+$). I personally find it a better alternative to data transformation (though it is also based on an ad-hoc rule, see white paper). Incidentally, I find I have something to add to whuber's example here. To the extend that we're discussing the whiskers' behaviour, we really should also consider what happens when considering contaminated data: library(robustbase) A0 In this contamination model, B1 has essentially a log-normal distribution save for 20 percent of the data that are half left, half right outliers (the break down point of adjbox is the same as that of regular boxplots, i.e. it assumes that at most 25 percent of the data can be bad). The graphs depict the classical boxplots of the transformed data (using the square root transformation) and the adjusted boxplot of the non-transformed data. Compared to adjusted boxplots, the former option masks the real outliers and labels good data as outliers. In general, it will contrive to hide any evidence of asymmetry in the data by classifying offending points as outliers. In this example, the approach of using the standard boxplot on the square root of the data finds 13 outliers (all on the right), whereas the adjusted boxplot finds 10 right and 14 left outliers. EDIT: adjusted box plots in a nutshell. In 'classical' boxplots the whiskers are placed at: $Q_1$-1.5*IQR and $Q_3$+1.5*IQR where IQR is the inter-quantile range, $Q_1$ is the 25th percentile and $Q_3$ is the 75th percentile of the data. The rule of thumb is to regard everything outside the fence as dubious data (the fence is the interval between the two whiskers). This rule of thumb is ad-hoc: the justification is that if the uncontaminated part of the data is approximately Gaussian, then less than 1% of the good data would be classified as bad using this rule. A weakness of this fence-rule, as pointed out by the OP, is that the length of the two whiskers are identical, meaning the fence-rule only makes sense if the uncontaminated part of the data has a symmetric distribution. A popular approach is to preserve the fence-rule and to adapt the data. The idea is to transform the data using some skew correcting monotonous transformation (square root or log or more generally box-cox transforms). This is somewhat messy approach: it relies on circular logic (the transformation should be chosen so as to correct the skewness of the uncontaminated part of the data, which is at this stage an un-observable) and tends to make the data harder to interpret visually. At any rate, this remains a strange procedure whereby one changes the data to preserve what is after all an ad-hoc rule. An alternative is to leave the data untouched and change the whisker rule. The adjusted boxplot allows the length of each whisker to vary according to an index measuring the skewness of the uncontaminated part of the data: $Q_1$-$\exp(M,\alpha)$1.5*IQR and $Q_3$+$\exp(M,\beta)$1.5*IQR Where $M$ is an index of skewness of the uncontaminated part of the data (i.e., just as the median is a measure of location for the uncontaminated part of the data or the MAD a measure of spread for the uncontaminated part of the data) and $\alpha$ $\beta$ are numbers chosen such that for uncontaminated skewed distributions the probability of lying outside the fence is relatively small across a large collection of skewed distributions (this is the ad-hoc part of the fence rule). For cases when the good part of the data is symmetric, $M\approx 0$ and we're back to the classical whiskers. The authors suggest using the med-couple as an estimator of $M$ (see reference inside the white paper) because of its high efficiency (though in principle any robust skew index could be used). With this choice of $M$, they then calculated the optimal $\alpha$ and $\beta$ empirically (using a large number of skewed distributions) as: $Q_1$-$\exp(-4M)$1.5*IQR and $Q_3$+$\exp(3M)$1.5*IQR, if $M\geq 0$ $Q_1$-$\exp(-3M)$1.5*IQR and $Q_3$+$\exp(4M)$1.5*IQR, if $M
