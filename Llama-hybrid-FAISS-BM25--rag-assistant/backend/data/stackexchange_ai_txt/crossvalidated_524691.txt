[site]: crossvalidated
[post_id]: 524691
[parent_id]: 
[tags]: 
What can I do whith this random effect conditional variance in lme4?

In the R package lme4, upon estimating a mixed-effects model I can retrieve the random effects and a corresponding variance using as.data.frame(ranef(model)) . Various authors use this to construct confidence intervals , some even mention "statistical significance". One example source is Ben Bolker and others' GLMM FAQ . But I cannot find a precise description of what I can do with it and how it is computed and modeled. A brief simulation I did showed such confidence intervals only contain the true value about 60-70% of the time when they should have 95% under the assumption of normality (using +/- 1.96 * conditional variance for confidence interval). Example: library(lme4) model This even generates a graphic which invites to compare the subjects' reaction times to one another, and it does use the 1.96 to compute the interval. It is interesting to know in how far this variance can be used to determine whether Subject 308 differs significantly to Subject 309 in Reaction . Since in this case the differences in Subjects are treated as a random variable I wonder in how far it is reasonable to test for different Subjects' reaction times. I know that as a realisation of a random variable this becomes more straightforward when viewed from a Bayesian perspective, but I find this in use in various sources outside a Bayesian context and want to know about its non-Bayesian relevance. EDIT: Simulation This is the simulation I ran. It assumes a model $Y_{ij} = X_{ij} \beta + Z_{j} \delta_{j} + \epsilon_{ij}$ , with $\delta$ varying over 20 groups and $Z$ being only a column of ones, making it a random intercept model. If this conditional variance ( condsd ) * 1.96 constructed a normal 95% confidence interval, I would expect each delta to be within range in ~950 of 1000 iterations, but they have around ~750 . I would expect the same for beta , which is the case. I hope this simulation is somewhat reasonable and doesn't miss the point. You can also pull out the delta and draw it only once. It changes the simulation, but doesn't reach the expectations either. So what is this variance for? library(lme4) library(dplyr) set.seed(789) m % mutate(true_value = delta, lower = condval - confint_factor * condsd, upper = condval + confint_factor * condsd) %>% mutate(true_val_in_confint = true_value >= lower & true_value $coefficients fixefs[[idx]] coefficients %>% as_tibble() %>% mutate(true_value = beta, lower = Estimate - confint_factor * `Std. Error`, upper = Estimate + confint_factor * `Std. Error`) %>% mutate(true_val_in_confint = true_value >= lower & true_value % sapply(FUN = function(x) x[['true_val_in_confint']]) %>% rowSums() betas_in_range % lapply(as.data.frame) %>% sapply(FUN = function(x) x$true_val_in_confint) %>% rowSums() print(deltas_in_range) ## [1] 772 781 793 785 788 779 805 791 800 766 803 780 781 775 791 788 788 788 778 784 print(betas_in_range) ## [1] 954 944 948
