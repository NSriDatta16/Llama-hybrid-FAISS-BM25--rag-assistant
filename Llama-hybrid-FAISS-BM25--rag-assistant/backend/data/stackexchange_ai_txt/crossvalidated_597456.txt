[site]: crossvalidated
[post_id]: 597456
[parent_id]: 
[tags]: 
Multiple linear regression machine learning can work well with high-dimensional data?

My data and problem: Data is high-dimensional data with 15474 features (columns) and 375 cases (observations, rows). Data contains 34% zero values. Output/response is multiple outputs of 2 (continuous values). Many researchers said that this high-dimensional data is unsuitable with multiple linear regression algorithms. But, I compared different machine learning models (such as linear regression, Support Vector Regression, Automatic Relevance Determination, Huber regression, random forest, and Gradient Boosting for regression) and found that multiple linear regression (Ordinary least squares) is the best one. I used techniques to avoid overfitting and data leakage while training models, such as 1) split: train, validation, and test data; 2) standardized and nested cross-validation; 3) using pipeline in Python; 4) test data was held out to finally test... So, can I ignore the term "high dimensional data" and accept the comparative results using a "try and error" approach? Or Are there any comments or explanations for my problem?
