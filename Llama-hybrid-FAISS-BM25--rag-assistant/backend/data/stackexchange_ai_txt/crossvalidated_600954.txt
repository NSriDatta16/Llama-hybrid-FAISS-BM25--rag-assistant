[site]: crossvalidated
[post_id]: 600954
[parent_id]: 
[tags]: 
Logistic regression with opposing states with same identical variables

I have a dataset with identical variables have opposing target variable (so instead of 0 is 1). > data.frame(y=c(y1[1],y2[1]),v1=c(v1[1],v1[1]),v2=c(v2[1],v2[1])) y v1 v2 1 1 -0.6264538 1.134965 2 0 -0.6264538 1.134965 Does this have effect on logistic regression estimates? I run this example in r and it shows already impact with adding only 10 (representing ony 1% of the total dataset) such opposing variables. (Duplicating in the below example the same variables with changing (inverting) the target). set.seed(1) v1 = rnorm(1000) # some continuous variables v2 = rnorm(1000) z = 1 + 2*v1 + 3*v2 # linear combination with a bias pr = 1/(1+exp(-z)) # pass through an inv-logit function y1 = rbinom(1000,1,pr) # bernoulli response variable y2 = abs(y1-1) # bernoulli response variable #fit df1 = data.frame(y=c(y1,y2[1:10]),v1=c(v1,v1[1:10]),v2=c(v2,v2[1:10])) summary(glm( y~v1+v2,data=df1,family="binomial")) # same without opposing inputs set.seed(1) x1 = rnorm(1000) # some continuous variables x2 = rnorm(1000) z = 1 + 2*x1 + 3*x2 # linear combination with a bias pr = 1/(1+exp(-z)) # pass through an inv-logit function y = rbinom(1000,1,pr) # bernoulli response variable #fit df2 = data.frame(y=y,x1=x1,x2=x2) summary(glm( y~x1+x2,data=df2,family="binomial")) Question: What is the ultimate implication to have in dataset both states of target variable (0, 1) binary for the same (continuous) explanatory variables? I have a dataset with identical variables have opposing target variable (so instead of 0 is 1). > data.frame(y=c(y1[1],y2[1]),v1=c(v1[1],v1[1]),v2=c(v2[1],v2[1])) y v1 v2 1 1 -0.6264538 1.134965 2 0 -0.6264538 1.134965 In my earlier question @Dave answered that this is ok, to have same variables observed for both states 0, 1 (target variable) as this represents just uncertainty and the this is reflected in the model being uncertain with 0.5 probability. Here below I run simulation which shows that with 1/2 cases for the same explanatory variables with both states of 1 and 0 (target variable). Here the probability predict() does not output this (doesn't converge to 50% with 1/2 such cases). But on another point. The problem I'm having is that, when I order the model explanatory variables along with model probabilities output, I do not any longer observe monotonicity in the probability, here the variable is ordered from min to max, where min is the worst outcome (should have probability close to 1) and max variable should have model output prob. close to 0. Because of the mix 0/1 of response for identical variables affects this. I see this as a data error issue that should be investigated and cases where 0 and 1 is observed for identical variables some of those (either 0 or 1) should be dropped as this leads to actual bias (break in monotonicity = where we expect give observable explanatory variable, we expect to have the correct prob>0.5 or prob set.seed(1) v1 = rnorm(1000) # some continuous variables v2 = rnorm(1000) z = 1 + 2*v1 + 3*v2 # linear combination with a bias pr = 1/(1+exp(-z)) # pass through an inv-logit function y1 = rbinom(1000,1,pr) # bernoulli response variable y2 = abs(y1-1) # bernoulli response variable sample.q
