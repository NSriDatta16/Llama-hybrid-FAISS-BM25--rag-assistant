[site]: crossvalidated
[post_id]: 513612
[parent_id]: 497661
[tags]: 
The plot shows results of bootstrap validation , and this is really explained in the post you link too. The calculation is done with Frank Harrell's rms R package, and you can read its help file by typing ?rms::calibrate in an R session (assuming that package is installed). It is discussed in detail in the book Regression Modeling Strategies in section 10.9. But the idea is simple enough, take the estimated linear predictor $X\hat{\beta}$ from a logistic regression, and estimate a new LR with that as predictor, that is, $$ P_c(Y=1 \mid X\hat{\beta})=\frac1{1+e^{-(\gamma_0+\gamma_1 X\hat{\beta})}} $$ If this is estimated on the same data , we will get $\gamma_0=0,\gamma_1=1$ . But use bootstrapping, do the reestimation on each resample, and use the out-of-bag data points for validation. That is, calculate the predicted probabilities on the data points not in the resample. That is the basis of the calibration plot. As explained in How to interpret the basics of a logistic regression calibration plot please? and Why does logistic regression produce well-calibrated models? , LR generally produces well-calibrated probabilities in the large , globally. That is because $$ \sum_i y_i = \sum_i \hat{p_i} $$ (assuming the model contains an intercept). But even then, there might be problems, for instance extreme probabilities could be to low (when large) and to low (when high). That would result in a calibration plot not following the ideal line at the diagonal $y=x$ .
