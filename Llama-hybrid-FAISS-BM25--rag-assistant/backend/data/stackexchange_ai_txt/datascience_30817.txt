[site]: datascience
[post_id]: 30817
[parent_id]: 
[tags]: 
Why would 2 sets of similar training samples take significantly longer to train?

I've built a fully-connected feed-forward neural network to recognize handwritten digits. I used MNIST and another very similar dataset (containing Arabic digits - same training set and test set count as those of MNIST). For a network with exactly the same architecture (1 hidden layer, same learning rate, same weight sampling), I tested on a sub-sample that used 100 images for training. The test accuracy was very low, but what I noticed was that for the Arabic digits, the network took significantly longer to train (double the amount of time than for English digits - 33ms and 15ms, respectively). What could be the reason for this?
