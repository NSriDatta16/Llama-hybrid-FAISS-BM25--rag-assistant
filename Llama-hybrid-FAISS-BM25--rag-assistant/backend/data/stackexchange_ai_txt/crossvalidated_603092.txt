[site]: crossvalidated
[post_id]: 603092
[parent_id]: 602247
[tags]: 
Whether you "need to" adjust depends on what you desire. Do you care about whether when you do many such "tests" repeatedly you would more frequently wrongly claim there is a difference when there isn't one because you looked at multiple metrics (=familywise type I error rate for your experiment)? If so, you should adjust (because you would otherwise get an inflated familywise type I error rate). If you instead say that on philosophical grounds you don't care about the type I error rate (some Bayesians would take that perspective appealing to the strong likelihood principle), then you don't. If you have some complicated composite decision criteria (e.g. at least 2 out of 3 metrics need to improve or the like, then it gets pretty complicated to understand the operating characteristics of such a rule, so I'll ignore that option). There's not trivial way (like the Bonferroni adjustment for p-values - NB: of course there's uniformly more powerful frequentist multiple testing adjustments) for adjusting Bayes factors in a way that would control the familywise type I error rate. You can see this, because as discussed in this paper - if we ignore prior distributions - the Bayes factor calculated from a p-value for a hypothesis test comparing two parameter values is given by $BF(p)=1/f(p|H_A)$ , where $f(p|H_A)$ is the average likelihood $f(y|\theta_A)$ averaged over the prior distribution for $\theta$ under the alternative $H_A$ . This likelihood depends also on the power/size of your experiment. I guess for a given prior and design, you can calculate $f(p|H_A)$ . If not analytically, then at least by simulation from parameter values under the alternative followed by simulating the experiment and its analysis, and treating the posterior probability that the value of interest is (greater/smaller) than the null hypothesis value. You can then approximate the likelihood for this probability (or its logit) via e.g. a kernel density estimate and see where the value obtain for your actual data lies relative to it. But, that does seem rather roundabout...
