[site]: crossvalidated
[post_id]: 311884
[parent_id]: 
[tags]: 
Stacking PCA variants

Stacking is commonly used for training autoencoders. Composing several PCA transformations doesn't make sense, just like adding more layers with linear activations doesn't make sense for neural networks. What about nonlinear generalizations of PCA? Did someone try to stack Sparse PCA, Robust PCA, or some other variant? Or maybe there's a fundamental reason that it won't work?
