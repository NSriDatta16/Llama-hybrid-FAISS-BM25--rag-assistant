[site]: crossvalidated
[post_id]: 112568
[parent_id]: 112566
[tags]: 
Generally there is a very efficient algorithm called Belief Propagation, which gives exact results when the structure of the Bayesian Network is a singly connected tree (there is only a single path between any two vertices in the undirected version of the graph). You can make use of that algorithm for an exact inference in this case. The problem is, that algorithm is somewhat complex, instead of it, you can directly use the definition of the Bayesian Network as well: $$P(fo,bp,lo,do,hb) = P(fo)P(bp)P(lo|fo)P(do|fo,bp)P(hb|do)$$ What you want to calculate is: $$\dfrac{P(fo=yes,lo=true,hb=false)}{P(lo=true,hb=false)}=\dfrac{\sum_{bp}\sum_{do}P(fo)P(bp)P(lo|fo)P(do|fo,bp)P(hb|do)}{\sum_{bp}\sum_{do}\sum_{fo}P(fo)P(bp)P(lo|fo)P(do|fo,bp)P(hb|do)} = \dfrac{P(lo=true,fo=yes)\sum_{bp}P(bp)\sum_{do}P(do|fo,bp)P(hb|do)} {P(lo=true,fo=yes)\sum_{bp}P(bp)\sum_{do}P(do|fo,bp)P(hb|do) +P(lo=true,fo=no)\sum_{bp}P(bp)\sum_{do}P(do|fo,bp)P(hb|do)} $$ This seems complex but since the variables are binary, the summations are just done on the two different values of the variable we are summing on. The variables behind the summation signs, which are not summed over has their values $lo=true$ and $hb=false$ but I did not write them in order to make the whole thing readable. In general for more complex networks, this kind of variable elimination techniques are not tractable computationally but for easier graphs, it is OK.
