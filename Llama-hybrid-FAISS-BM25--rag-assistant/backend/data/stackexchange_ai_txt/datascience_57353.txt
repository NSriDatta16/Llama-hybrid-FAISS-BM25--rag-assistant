[site]: datascience
[post_id]: 57353
[parent_id]: 
[tags]: 
Getting different precisions for same neural network with same dataset and hyperparameters in sklearn mlp classifier

I get WAY DIFFERENT results in each run despite using random state for making sure that network outputs same result for same hyper parameters, here is some sample outputs(I've printed the hyper parameters manually to show that they are the same) and I think I may missing something... {'hidden_layer_sizes': [20, 20, 30], 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.02, 'learning_rate': 'adaptive', 'iteration': 400} 0.8888888888888888 {'hidden_layer_sizes': [20, 20, 30], 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.02, 'learning_rate': 'adaptive', 'iteration': 400} 0.3333333333333333 {'hidden_layer_sizes': [20, 20, 30], 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.02, 'learning_rate': 'adaptive', 'iteration': 400} 0.4444444444444444 {'hidden_layer_sizes': [20, 20, 30], 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.02, 'learning_rate': 'adaptive', 'iteration': 400} 0.7777777777777778 here is some of the code: mlp = MLPClassifier(random_state=2, hidden_layer_sizes=(20, 20, 30), max_iter=400, alpha=0.02, activation='tanh', solver='adam', learning_rate='adaptive') mlp.fit(X_train, y_train.values.ravel())
