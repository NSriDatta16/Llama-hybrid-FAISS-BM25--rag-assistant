[site]: crossvalidated
[post_id]: 199007
[parent_id]: 198956
[tags]: 
It has already been pointed out that many of the behaviors and thought processes labeled "irrational" or "biased" by (behavioral) economists are actually highly adaptive and efficient in the real world. Nonetheless, OP's question is interesting. I think, however, that it may be profitable to refer to more fundamental, descriptive knowledge about our cognitive processes, rather than go looking for specific "biases" that correspond to those discussed in the economic literature (e.g., loss aversion, endowment effect, baserate neglect etc.). For instance, evaluability is certainly an issue in data analysis. Evaluability theory states that we overweight information that we find easy to interpret or evaluate. Consider the case of a regression coefficient. Evaluating the "real-world" consequences of a coefficient can be hard work. We need to consider the units of the independent and the dependent variable as well was the distributions of our independent and dependent variable to understand whether a coefficient has practical relevance. Evaluating the significance of a coefficient, on the other hand, is easy: I merely compare its p-value to my alpha level. Given the greater evaluability of the p-value compared the coefficient itself, it is scarcely surprising that so much is made of p-values. (Standardizing increases the evaluability of a coefficient, but it may increase ambiguity : the sense that relevant information is unavailable or withheld, because the "original" form of the data we are processing is not available to us.) A related cognitive "bias" is the concreteness principle, the tendency to overweight information that is "right there" in a decision context, and does not require retrieval from memory. (The concreteness principle also states that we are likely to use information in the format in which it is given and tend to avoid performing transformations.) Interpreting a p-value can be done by merely looking at the regression output; it does not require me to retrieve any substantive knowledge about the thing that I am modeling. I expect that many biases in the interpretation of statistical data can be traced to the general understanding that we are likely to take the easy route when solving a problem or forming a judgment (see "cognitive miser", "bounded rationality" and so on). Relatedly, doing something "with ease" usually increases the confidence with which we hold the resulting beliefs ( fluency theory ). (One might also consider the possibility that data which are easier to articulate - to ourselves or to others - are overweighted in our analyses.) I think this becomes particularly interesting when we consider possible exceptions. Some psychological research suggests, for instance, that if we believe that a problem should be difficult to solve, then we may favor approaches and solutions which are less concrete and more difficult, e.g., choose a more arcane method over a simple one.
