[site]: datascience
[post_id]: 81841
[parent_id]: 81730
[tags]: 
The convergence time is sensitive to the data you have and a random seed. Specifically, the convergence time is linear in expectation in all three cases. SGDClassifier uses the stochastic gradient descent for optimization. Since L1 loss is only subdifferential, the L1 penalty causes the algorithm to converge noticeably slower. Comparing with or without the L2 penalty, it is not clear what algorithm is faster. The loss function is differential. The L2 penalty may be faster in the underdetermined case. In the example below, I consider the gradient descent instead of the stochastic linear descent and regular regression to simplify the argument. Say, we aim to solve y = Xb + e, where we observe y and X only. We set the loss function to be f(b) = 0.5||y - Xb||^2. Without regularization, the solution is sol1 =(X^TX)^{-1}X^Ty and with L2 regularization, the solution is sol2 = (X^TX + lambda I)^{-1}X^Ty . In the latter case, we can guarantee that the matrix to invert is not close to singular, and, therefore, the faster convergence is expected. In short, on average, I would expect the following number of iterations requires from smallest to largest ON AVERAGE: L2 penalty No penalty (potentially, with a close tie with L2 penalty) L1 penalty You observe the opposite order. It should be very specific to your data or random seed.
