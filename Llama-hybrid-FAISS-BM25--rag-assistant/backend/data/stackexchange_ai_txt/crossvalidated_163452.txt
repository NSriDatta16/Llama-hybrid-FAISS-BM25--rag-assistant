[site]: crossvalidated
[post_id]: 163452
[parent_id]: 
[tags]: 
CNN: Details of Zeiler Fergus Net

I want to replicate the modified AlexNet by Zeiler and Fergus from 2013 ( Visualizing and Understanding Convolutional Networks ) but they spare some details. Hope someone here knows more about it. What is their exact learning rate schedule? They just write "We anneal the learning rate throughout training manually when the validation error plateaus". Do they use weight decay? In which layers do they "renormalize" the filters (they do not divide the input by the global standard deviation)? I do not understand their architecture completely: In the first layer: 224 -> 110 with filters of width/height 7 and stride 2. Do they add a padding of one only on one side because $110 \cdot 2 + 5 = 225$ or am I wrong? Same for 3x3 maxpooling 26 -> 13 with stride 2.
