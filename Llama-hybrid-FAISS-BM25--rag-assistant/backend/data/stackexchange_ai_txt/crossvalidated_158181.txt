[site]: crossvalidated
[post_id]: 158181
[parent_id]: 55695
[tags]: 
One method that comes to mind is perplexity , which is a common idea in information theory and NLP. Essentially it is defined as $$ \text{Perplexity} = 2^{-\sum_{i}p_i\log_2 p_i} $$ where the exponent can readily be interpreted as the entropy of your probability distribution in bits. The general idea is that if your distribution were encoded as a fair n-sided die, how many faces would it have? For instance, a fair 6-sided die has distribution $\{\frac{1}{6},\frac{1}{6},\frac{1}{6},\frac{1}{6},\frac{1}{6},\frac{1}{6}\}$ so it has perplexity $$ \text{Perplexity} = 2^{-\sum_{i=1}^{6}\frac{1}{6}\log_2 \frac{1}{6}} = 2^{\log_2 6} = 6 $$ as we would expect. Now let's consider a die which has distribution $\{\frac{1}{2},\frac{1}{2},0,0,0,0\}$. This has perplexity 2 (note that $0\log 0 = 0$ for us) so is equivalent to a 2-sided die, or an unbiased coin. Try this on a few other examples and you'll find it's a really nice measure of how far a discrete distribution is away from uniform, what you called 'biased'. Note that you don't have to use the base 2, but it is the most standard form, followed by $e$.
