[site]: datascience
[post_id]: 30680
[parent_id]: 30676
[tags]: 
During the phase where the neural network generates its prediction, it feeds the input forward through the network. For each layer, the layer's input $X$ goes first through an affine transformation $W \cdot X + b$ and then is passed through the sigmoid function $σ(W \cdot X + b)$. In order to train the network, the output $\hat y$ is then compared to the expected output (or label) $y$ through a cost function $L(y, \hat y)=L\left(y, σ(W \cdot X + b)\right)$. The goal of the whole training procedure is to minimize that cost function. In order to do that, a technique called gradient descent is performed which calculates how we should change $W$ and $b$ so that the cost reduces. Gradient Descent requires calculating the derivative of the cost function w.r.t $W$ and $b$. In order to do that we must apply the chain rule , because the derivative we need to calculate is a composition of two functions. As dictated by the chain rule we must calculate the derivative of the sigmoid function . One of the reasons that the sigmoid function is popular with neural networks, is because its derivative is easy to compute .
