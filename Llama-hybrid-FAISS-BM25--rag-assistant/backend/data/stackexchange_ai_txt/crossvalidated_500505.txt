[site]: crossvalidated
[post_id]: 500505
[parent_id]: 
[tags]: 
How to deduce the function of each layer in a neural network without being explicitly told beforehand?

In research papers regarding deep learning you commonly get an explanation of how each layer functions. For example in SRCNN, an image up scaling model ( https://arxiv.org/pdf/1501.00092.pdf ), the first layer extracts patches, the second layer learns a non-linear mapping from the low resolution patches to high resolution patches, and the 3rd layer reconstructs high resolution patches into a full high resolution image. My question is how do I deduce what each layer does? I understand the code for models well (I think) and understand what they hyperparameters are for, but I would have no idea how to generate an image like the above for another network architecture. Is there a procedure I can follow to work these things out? and/or do I just need a better mathematical understanding? Thanks. EDIT follow up question referenced in answer: Is it just that the researchers have a goal in mind for each layer before they start coding? or do you need to find out through experiments (like printing the output of each layer to see what it does exactly)?
