[site]: crossvalidated
[post_id]: 128957
[parent_id]: 
[tags]: 
Prior/Posterior predictive distributions

I have trouble understanding some notes I have from the lectures. We are in Bayesian linear regression and he explained how we can first introduce a prior probability distribution to the weights: $p(w∣α)$ where α is a hyperparameter. Then we can compute the posterior on the weights like this:$ p(w∣x,t,α,σ2) \propto p(t∣x,w,σ2)⋅$$p(w∣α)$, where x is our dataset and t our target variables. So we do Maximum A Posteriori for this posterior, and then he goes on to predictive distributions and gives us this formula: $p(t_{new}∣x_{new},t,x,α,σ2)=∫((p(t_{new}∣x_{new},w,t,x,σ2)⋅$$p(w∣α))dw$ I can't understand the above formula. Does he use the prior predictive distribution? And if yes, why is he not using the earlier result of the posterior on the weights? Shouldn't we use exactly that to find the predictive distribution?
