[site]: crossvalidated
[post_id]: 499004
[parent_id]: 
[tags]: 
What does this notation in the Deep Learning book by Goodfellow, Bengio, and Courville mean?

When they first introduce the back propagation in an algorithm listing they write ... The next line computes $\frac{\partial u^{(n)}}{\partial u^{(j)}} = \sum_{i:j\in Pa(u^{(i)})}{\frac{\partial u^{(n)}}{\partial u^{(i)}}} \frac{\partial u^{(i)}}{\partial u^{(j)}}: ...$ What is the sum over? And how it is expressed here? I know $Pa(u^{(i)})$ is parents of the node $u^{(i)}$ . But what is $i:j\in Pa(u^{(i)})$ ?
