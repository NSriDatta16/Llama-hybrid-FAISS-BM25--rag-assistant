[site]: stackoverflow
[post_id]: 5688794
[parent_id]: 5669287
[tags]: 
Well, a guy on opengl.org has pointed out that the clip space coordinates the projection produces are divided by clipPos.w to compute the normalized device coordinates. When reversing the steps from fragment over ndc to clip space coordinates, you need to reconstruct that w (which happens to be -z from the corresponding view space (camera) coordinate), and multiply the ndc coordinate with that value to compute the proper clip space coordinate (which you can turn into a view space coordinate by multiplying it with the inverse projection matrix). The following code assumes that you are processing the frame buffer in a post process. When processing it while rendering geometry, you can use gl_FragCoord.z instead of texture2D (sceneDepth, ndcPos.xy).r. Here is the code: uniform sampler2D sceneDepth; uniform mat4 projectionInverse; uniform vec2 clipPlanes; // zNear, zFar uniform vec2 windowSize; // window width, height #define ZNEAR clipPlanes.x #define ZFAR clipPlanes.y #define A (ZNEAR + ZFAR) #define B (ZNEAR - ZFAR) #define C (2.0 * ZNEAR * ZFAR) #define D (ndcPos.z * B) #define ZEYE -(C / (A + D)) void main() { vec3 ndcPos; ndcPos.xy = gl_FragCoord.xy / windowSize; ndcPos.z = texture2D (sceneDepth, ndcPos.xy).r; // or gl_FragCoord.z ndcPos -= 0.5; ndcPos *= 2.0; vec4 clipPos; clipPos.w = -ZEYE; clipPos.xyz = ndcPos * clipPos.w; vec4 eyePos = projectionInverse * clipPos; } Basically this is a GLSL version of gluUnproject.
