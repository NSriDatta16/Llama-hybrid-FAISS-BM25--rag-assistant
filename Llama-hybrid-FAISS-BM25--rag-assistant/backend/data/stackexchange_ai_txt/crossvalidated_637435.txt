[site]: crossvalidated
[post_id]: 637435
[parent_id]: 637411
[tags]: 
Note that there is more than one algorithm for obtaining a K-means clustering. The literature is to some extent confusing as some literature uses the term "K-means" for a specific algorithm (mostly Lloyd's algorithm as explained below, sometimes attributed to MacQueen), whereas other literature (which I like more) uses the term "K-means" for the general optimisation problem, for which you then can have several algorithms (which only deliver a local optimum that depends on initialisation unless the dataset is very small or initialisation is fixed and deterministic). One of the most popular and maybe the most simple K-means algorithm is Lloyd's algorithm, which alternates (i) assigning all observation to the closest mean and (ii) compute the mean from the new cluster assignments. Normally for Lloyd, if the observations don't change anymore, the means remain the same as well, and if the means remain the same, the observations are all again assigned to the same clusters. The only potential exception is if there are observations that are exactly equally close to two means, in which case their assignment is ambiguous. What happens then depends on how the assignment is exactly implemented. In any case I believe that it is hard to construct examples for any assignment rule in which after equality of the means occurred once many further steps are needed for convergence of the assignments, and I think this happens very rarely. I can conceive situations in which the means don't change at some point but they start changing again for a few steps afterwards due to assignment changes of ambiguous observations (if assignment is random in case of ambiguity, convergence may not be guaranteed). There may be other algorithms which play out differently. The Hartigan-Wong algorithm as used as default in R-function kmeans will stop if no assignment change can be found anymore that improves the objective function, in which case also centroids are no longer changed, and on the other hand if only one observation changes clusters (as is the case for a single Hartigan-Wong step), centroids will always change unless an observation is changed that is exactly the centroid for both the cluster where it originally comes from and where it goes, which once more is extremely rare and can only happen in the beginning due to weird initialisation. I won't comment on further algorithms.
