[site]: crossvalidated
[post_id]: 440550
[parent_id]: 
[tags]: 
Speed up matrix calculations with pretrained neural networks

I was surprised how few content there is on the general aspect of neural networks to speed up matrix calculations. Let me line out the problem: In a lot of algorithms, we deal with matrix inversion or matrix calculations of any kind. In general, a complete matrix inversion is not required in most of the cases. For a certain type of matrices, there are methods to speed things up, e.g. cholesky decomposition, solving Cx=b instead of performing a matrix inversion... (the list can go on for a while). These methods are implemented in a function, let's say f(x) , where x is most likely a high dimensional data object. The universal approximation theorem states that we can approximate almost any continuous function with a feed-forward network with a hidden layer. Is it possible to approximate these functions by training neural networks beforehand, and replace them with the networks? To answer this, several aspects need to be taken into account: can we learn these functions f(x) that in this case represent complex matrix calculations (most of the time implemented with time-expensive for loops over all dimensions) ? are these functions f(x) continuous? If not, can we approximate non-continuous functions with more complex architectures? how can we evaluate if a given algorithm implemented in f(x) is continuous? if we can actually approximate f(x) , are we rewarded with a speed up? What about the general mathematical limits of let's say stubborn matrix inversion? Is the mathematical limit imprinted into the network structure (we need such a complex network to approximate the function that there is no speed up anymore) ? As there is very little content on this topic, I suspect that I am missing an obvious problem? Any thoughts, literature or advice is much appreciated. DK
