[site]: datascience
[post_id]: 90317
[parent_id]: 90310
[tags]: 
Neural networks are trained with a mix of mathematical optimization and trial and error exploration: Neural networks are comprised of trainable parameters . These parameters are trained with some variant of stochastic gradient descent (SGD) . Trainable parameters include those in dense layers, convolutional layers, attention layers, LSTMs, etc. There are other aspects of neural networks that cannot be trained but are equally decisive in the performance of the network. They are known as hyperparameters . Some of these hyperparameters are the number and size of filters in convolutional layers, the number of layers, the dimensionality of embeddings, etc. To decide which hyperparameters values are optimal , you either choose them "by intuition", or explore different combinations of values and check which combination performs best (e.g. random search, grid search), or apply some sort of black-box optimization (Bayesian optimization, genetic algorithms, etc).
