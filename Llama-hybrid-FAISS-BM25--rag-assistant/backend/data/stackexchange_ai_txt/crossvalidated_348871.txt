[site]: crossvalidated
[post_id]: 348871
[parent_id]: 348622
[tags]: 
To answer the main question: how do I add seasonal & trend components to the final answer? First you need to be careful with the terminology. Making a series stationary and removing trend and seasonality from a series are not the same thing - even though they are related. More specifically - removing trend and seasonality might make a series stationary, but there is also the possibility that even after removing the trend and the seasonality, the series is still not stationary (for the variance is still no constant even after detrending and deseasonalising). Bringing back the seasonality and the trend into the forecast after removing them also depends on what transformation you used to detrend and deseasonalize the signal in the first place. If differencing was used to make the series stationary (the way it usually is in ARIMA models) then cumulative summing is how you reverse transform the data back to it's original shape. Note that when differencing is used, then the trend and seasonality are being removed implicitly not explicitly. For example if your differencing order is one: Original series: $X_1,X_2,X_3...X_n$ Differenced series: $Y_1 = X_1$ and for $n>1$ $Y_n = X_{n+1} - X_n$ Initial forecast series: $\hat{Y}_1,\hat{Y}_2,....\hat{Y}_n$ Final (summed) forecast series: $\hat{X}_1 = \hat{Y}_1$ For $n>1$ $\hat{X}_n = \hat{Y}_1+\hat{Y}_2+...+\hat{Y}_n$ Other approaches such as Facebook Prophet (GAM based) or STL() try to model the trend $T$, the seasonal component $S$ and the residuals $R$ individually and then add them back together. In such cases, you try to explicitly model the trend $\hat{T}$, the seasonality $\hat{S}$, and the residuals $\hat{R}$ and then you simply add them back together: $\hat{X}_n = \hat{T}_n +\hat{S}_n+\hat{R}_n$. A note on the use of Neural Networks based on the discussion in the comments on the OP (Full disclosure: I am more of a practitioner of time series - I do demand forecasting for retail - than an academic). From what I see, the idea of Neural Networks - especially LSTM - outperforming traditional methods like ARIMA and exponential smoothing seems to driven mainly by the current overall hype for using Neural Networks and Tensorflow to try to solve every single data science problem out there. Google (and Amazon with their DeepAR model) have been pushing for this since it brings more customers to their cloud platform. The results in the academic literature seem to be more nuanced and mixed. In theory, for an LSTM to perform better than Triple Exponential Smoothing, Seasonal ARIMA or TBATS, means that your time series has to have highly complex non linear patterns beyond the seasonal and trend components. In practice, I've found such time series to be rare, and anything that can be modeled by an LSTM can also be modeled by a simpler statistical model with considerably less computational cost and the additional benefit of being easier to interpret.
