[site]: crossvalidated
[post_id]: 414029
[parent_id]: 282987
[tags]: 
Let's first see the differences between the HMM and RNN. From this paper: A tutorial on hidden Markov models and selected applications in speech recognition we can learn that HMM should be characterized by the following three fundamental problems: Problem 1 (Likelihood): Given an HMM λ = (A,B) and an observation sequence O, determine the likelihood P(O|λ). Problem 2 (Decoding): Given an observation sequence O and an HMM λ = (A,B), discover the best hidden state sequence Q. Problem 3 (Learning): Given an observation sequence O and the set of states in the HMM, learn the HMM parameters A and B. We can compare the HMM with the RNN from that three perspectives. Likelihood Likelihood in HMM(Picture A.5) Language model in RNN In HMM we calculate the likelihood by $P(O)=\sum_Q P(O, Q) = \sum_Q P(O|Q)P(Q)$ where the $Q$ represents all the possible hidden state sequences, and the probability is the real probability in the graph. While in RNN the equivalent, as far as I know, is the inverse of the perplexity in language modeling where $\frac{1}{p(X)} = \sqrt[T]{\prod_{t=1}^T \frac{1}{p(x^t|x^{(t-1)},...,x^{(1)})}}$ and we don't sum over the hidden states and don't get the exact probability. Decoding In HMM the decoding task is computing $v_t(j) = max_{i=1}^N v_{t-1}(i)a_{ij} b_(o_t)$ and determining which sequence of variables is the underlying source of some sequence of observations using the Viterbi algorithm and the length of the result is normally equal to the observation; while in RNN the decoding is computing $P(y_1, ..., y_O|x_1, ..., x_T) = \prod_{o=1}^OP(y_o|y_1, ..., y_{o-1}, c_o)$ and the length of $Y$ is usually not equal to the observation $X$ . Decoding in HMM(Figure A.10) Decoding in RNN Learning The learning in HMM is much more complicated than that in RNN. In HMM it usually utilized the Baum-Welch algorithm(a special case of Expectation-Maximization algorithm) while in RNN it is usually the gradient descent. For your subquestions: Which sequential input problems are best suited for each? When you don't have enough data use the HMM, and when you need to calculate the exact probability HMM would also be a better suit(generative tasks modeling how the data are generated). Otherwise, you can use RNN. Does input dimensionality determine which is a better match? I don't think so, but it may take HMM more time to learn if hidden states is too big since the complexity of the algorithms (forward backward and Viterbi) is basically the square of the number of discrete states. Are problems which require "longer memory" better suited for an LSTM RNN, while problems with cyclical input patterns (stock market, weather) more easily solved by an HMM? In HMM the current state is also affected by the previous states and observations(by the parent states), and you can try Second-Order Hidden Markov Model for "longer memory". I think you can use RNN to do almost references Natural Language Processing with Deep Learning CS224N/Ling284 Hidden Markov Models
