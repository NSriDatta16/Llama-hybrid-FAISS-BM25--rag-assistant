[site]: crossvalidated
[post_id]: 393730
[parent_id]: 
[tags]: 
Asymptotic Expectation of Ratio of Sample Averages

I have two random variables: $X$ and $Y$ . I know that: \begin{equation} E[X]=E[Y]=\mu>0 \end{equation} I know that variance of both can be bounded: \begin{equation} \operatorname{Var}[X] The variables might be correlated. Suppose I create a statistic: \begin{equation} Z_n=\frac{ \overline{x}-\overline{y}}{ \overline{y}}=\frac{\frac{1}{n}\sum x_{i}-\frac{1}{n}\sum y_{i}}{\frac{1}{n}\sum y_{i}}. \end{equation} Clearly: \begin{equation} \overline{x}-\overline{y}\rightarrow0 \quad and \quad \overline{y} \rightarrow\mu \quad as \quad n\rightarrow\infty \quad \end{equation} My suspicion is that: \begin{equation} Z_{n}\rightarrow0 \quad as \quad n\rightarrow\infty \quad? \end{equation} (in some statistical sense - in probability, almost surely, etc.) I am an economist and reasonably decent at statistics, but this is stretching my abilities. It seems obvious in some sense, but I need to provide a proof for paper. My intuition is that E[x-y] is going to 0 and E[y] is always bounded from 0, and it seems as if the averages are getting tighter and tighter so that correlations between X and Y can't matter much as n rises. But, I worry that there are weird cases I need to exclude for X and Y (which are pretty simple variables in actuality). I was thinking that one way forward is to do a Taylor Expansion ( http://www.stat.cmu.edu/~hseltman/files/ratio.pdf ) and then show all of the extra terms must go to zero. But, it seems like there must be an easier way? Like, maybe this is a very obvious and simple?
