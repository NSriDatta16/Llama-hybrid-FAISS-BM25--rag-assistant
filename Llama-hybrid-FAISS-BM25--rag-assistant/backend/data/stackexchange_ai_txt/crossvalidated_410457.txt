[site]: crossvalidated
[post_id]: 410457
[parent_id]: 
[tags]: 
Bayesian Statistics in CS229

In this note ,there is a posterior distribution of parameter $\theta$ in section 3 like this: $$ \begin{align} p(\theta|S)&={p(S|\theta)p(\theta)\over p(S)}\\&={\big(\prod_{i=1}^mp(y_i|x_i,\theta)\big)p(\theta)\over\int_\theta \big(\prod_{i=1}^mp(y_i|x_i,\theta)\big)p(\theta)d\theta} \end{align} $$ I understand the first step is Bayes' theorem, but use the same theorem and chain rule for conditional probability to $p(S|\theta)$ , should we get something like this: $$ p(S|\theta)=\big(\prod_{i=1}^mp(x_i,y_i|\theta)\big)=\big(\prod_{i=1}^mp(y_i|x_i,\theta)p(x_i|\theta)\big) $$ there are extra $p(x_i|\theta)$ terms compared to their form, do I make a mistake or miss something? I also wonder do they use naive Naive Bayes (NB) assumption which mentioned in the previous note to get $p(S|\theta)$ , for let a data point in training set as $z_i = (x_i,y_i)$ , $p(S|\theta)$ become $p(z_1,z_2,...,z_m|\theta)=p(z_1|\theta)p(z_2|z_1,\theta)p(z_3|z_1,z_2,\theta)...p(z_m|z_1,z_2,...,z_{m-1},\theta)$ , which may need assume $z_i,z_j$ are conditionally independent on $\theta$ to arrive the reduced form in their note.
