[site]: crossvalidated
[post_id]: 122195
[parent_id]: 122133
[tags]: 
Imagine that your posterior is somewhat Gaussian. The expectation of the squared euclidean norm of the best of $N$ draw depends on the dimension $d$ and is asymptotically $O(1/N^{2/d})$, but the average remains $O(1/N)$. As soon as you have more than 2 dimensions, the average converges faster. You have $d=4$ so the average of the points around the mode will be a much better estimate of the mode than the point closest to the mode. If that's not immediately intuitive, imagine you're drawing from a standard multivariate normal with identity covariance and $d=1000$. The closest vector to $0$ is still going to be very far on average; in high dimensions, most of the mass of a Gaussian is away from the center. The average will be much closer as values from different draws cancel out in each dimensions. Try it in python def draw(d): x = np.random.randn(1000*d).reshape((1000,d)) return np.sum(np.mean(x,axis=0)**2) Edit: so I got nerdsniped into computing the multiplicative factor in the expectation of the minimum squared euclidean norm when drawing from a standard multivariate normal with dimension $d$. It is asymptotically equivalent to $$2\Gamma\left(1+\frac{d}{2}\right)^{\frac{2}{d}}\Gamma\left(1+\frac{2}{d}\right) n^{-2/d}$$ while the average is of course $d n^{-1}$
