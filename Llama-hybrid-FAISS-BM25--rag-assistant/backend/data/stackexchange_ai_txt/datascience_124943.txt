[site]: datascience
[post_id]: 124943
[parent_id]: 124940
[tags]: 
I'm new to this field as well, and from what I've learnt, yes, a lot of what we're doing while working with NNs is empirical. We mostly build such NNs to work with some specific type of data, and since data comes in all sizes and shapes, there is no single specific set of rules to follow for the architecture of our network or the values of the hyper-parameters such that it yields best results regardless of the nature of data you're working with. The trail and error part is necessary because you're trying to make a NN specifically for your use case. We can of course learn from existing models and architectures. If you find a model that works with data similar to yours, you could read up on its architecture and why they decided to do things in that certain way. Might give you some insights on why a certain characteristic of the model's architecture might work for the type of data you're working with. That being said, if you're feeling lost on what LR to use and why, which activation function to use and why, or why use regularization and where, then what you may be missing is a foundational understanding in these concepts. I would recommend having a look at below two resources: Andrew Ng's Deep Learning Specialization: https://www.coursera.org/specializations/deep-learning Deep Learning by Ian Goodfellow and Yoshua Bengio and Aaron Courville https://www.deeplearningbook.org/ Good luck.
