[site]: crossvalidated
[post_id]: 68465
[parent_id]: 65808
[tags]: 
When computing power, you have to state what hypothetical effect size you are trying to detect. As Peter mentioned, computing the power to detect the results you actually detected is rarely useful. Here is a page I wrote: http://graphpad.com/support/faq/why-it-is-not-helpful-to-compute-the-power-of-an-experiment-to-detect-the-difference-actually-observed-why-is-post-hoc-power-analysis-futile/ The key paragraph: If your study reached a conclusion that the difference is not statistically significant, then -- by definition-- its power to detect the effect actually observed is very low. You learn nothing new by such a calculation. It can be useful to compute the power of the study to detect a difference that would have been scientifically or clinically worth detecting. It is not worthwhile to compute the power of the study to detect the difference (or effect) actually observed. Here are five related peer-reviewed articles: SN Goodman and JA Berlin, The Use of Predicted Confidence Intervals When Planning Experiments and the Misuse of Power When Interpreting the Results , Annals Internal Medicine 121: 200-206, 1994. Hoenig JM, Heisey DM, The abuse of power , The American Statistician. February 1, 2001, 55(1): 19-24. doi:10.1198/000313001300339897. Lenth, R. V. (2001), Some Practical Guidelines for Effective Sample Size Determination , The American Statistician, 55, 187-193 M Levine and MHH Ensom, Post Hoc Power Analysis: An Idea Whose Time Has Passed , Pharmacotherapy 21:405-409, 2001. Thomas, L, Retrospective Power Analysis , Conservation Biology Vol. 11 (1997), No. 1, pages 276-280
