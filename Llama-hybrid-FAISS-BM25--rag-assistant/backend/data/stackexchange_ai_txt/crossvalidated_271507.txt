[site]: crossvalidated
[post_id]: 271507
[parent_id]: 271480
[tags]: 
You are right. For a set of training samples, there are many different functions that can explain these samples. Each learning method favors some of these functions according to its hypothesis space and its hyperparameters. For example, a neural network with drop-out chooses a different function than the same NN without drop-out. On the other hand, I would say that many machine learning algorithms are pragmatic methods, that is they try to find a model that produces suitable outputs, though their learned function are not exactly the correct function which generates the data. I would like to add another point here. The best function is not always the function that gives you zero training error since such a function is prone to overfitting. In other words, it tries to model noises in the training data or fake regularities in data due to the limited number of training samples. In fact, regularization techniques such as drop-out are introduced to prevent the learning method to favor too complex functions.
