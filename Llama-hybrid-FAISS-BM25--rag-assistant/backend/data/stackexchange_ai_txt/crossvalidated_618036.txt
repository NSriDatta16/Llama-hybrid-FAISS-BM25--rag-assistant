[site]: crossvalidated
[post_id]: 618036
[parent_id]: 
[tags]: 
Should the KL loss term for a VAE be the KL-Loss of a batch's mean mu and log sigma, or is it the mean of the kl loss for each individual input image?

I've been trying to learn about Variational Autoencoders and been looking at the Keras sample implementation ( https://github.com/keras-team/keras-io/blob/master/examples/generative/vae.py ) I'm confused about how to calculate KL-Divergence. Currently, they calculate the KL-Loss as follows: def train_step(self, data): with tf.GradientTape() as tape: z_mean, z_log_var, z = self.encoder(data) ... kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)) kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1)) As I understand it: The purpose of the kl_loss variable is to quantify the difference between a Std. Normal distribution and the distribution represented by mu(i.e. z_mean) and log sigma**2 (i.e. z_log_var). By minimizing the KL-Divergence, they're making the distribution represented by z_mean and z_log_var as similar to N(0,1) as possible. z_mean and z_log_var are not singular vectors, but are a batch of N vectors, with one pair of (mu, sigma) vectors for each sample image. I am confused over the following point: The way they're calculating the KL-divergence, they seem to be calculating the KL-divergence for each individual image's latent space representation with resp. to N(0,1) and then averaging over the batch, instead of calculating the z_mean and z_log_var for the entire batch and then getting the KL-divergence of those parameters from N(0,1) . Their KL loss function is calculated like this: kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)) kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1)) If they were getting the KL divergence for the distribution represented by the batch, I would've expected something like: mean_z_mean = tf.reduce_mean(z_mean) mean_z_log_var = tf.reduce_mean(z_log_var) kl_loss = -0.5 * (1 + mean_z_log_var - tf.square(mean_z_mean) - tf.exp(mean_z_log_var)) In other words, I expected to see the kl-loss of the mean terms, not the mean kl-loss of the individual terms. Since the kl-loss function is not linear, these two expressions for it are not equivalent. So, are they calculating the mean kl-loss of the individual terms? If so, what does it mean to calculate z_mean and z_log_var with a sample size of 1, and also how do they avoid forcing each image to have the same representation in latent space (i.e. Std. Normal)? If not why are they getting the average of the kl-losses associated with each image, instead of the kl-loss associated with the average z_mean and average z_log_var?
