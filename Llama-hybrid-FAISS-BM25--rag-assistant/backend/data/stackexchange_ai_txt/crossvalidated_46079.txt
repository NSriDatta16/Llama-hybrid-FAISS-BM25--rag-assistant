[site]: crossvalidated
[post_id]: 46079
[parent_id]: 46052
[tags]: 
I don't completely understand your question, and anyways we cannot possibly say whether this is sensible if you don't tell us what you model (application) and how. However, recognize two situations where building a model that describes errors does make sense hyperparameter optimization (e.g. cost and Î³ for SVM). The performance/error of the SVM could be modeled as function of the hyperparameters. (the hyperparameters would be @whuber's withheld variables). In analytical chemistry, the test set performance is often modeled pred. conc. = lm (true conc.). The slope (aka recovery) can be $\neq$ 1, e.g. if part of your analyte (= substance you want to measure) is masked by the sample matrix (= other stuff that makes up the physical specimen). Here's a hypothetical example, where 10% of the analyte are not detected (recovery rate 90%) : The predicted concentrations should be around the gray line (where $c_{predicted} = c_{reference~method}$). The predictions however, are systematically too low ($c_{predicted}$ below gray line), and the bias depends on the true concentration $c_{reference~method}$. A linear model regressing the predictions against the reference values models these systematic deviations (dashed line). Literature: Danzer, Analytical Chemistry: Theoretical and Metrological Fundamentals
