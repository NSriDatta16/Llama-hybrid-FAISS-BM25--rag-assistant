[site]: crossvalidated
[post_id]: 277026
[parent_id]: 277014
[tags]: 
As you correctly recognise almost always people focus on optimising against $k$ when in comes to $k$-NN applications. A standard paper on the matter is Complete Cross-Validation for Nearest Neighbor Classifiers by Mullin and Sukthankar. Having said that, this is half of the story (or one third actually if you take $k$-NN literally). The nearest-neighbour part means that we employ some notion of near , ie. we use some distance metric to quantify similarity and thus define neighbours and the notion of near in general. This choice is most probably of equal if not greater importance than $k$. To given an example of an obviously wrong metric: Assume we want to cluster cities geographically and we base their proximity on lexicographic order; Athens, Greece and Athens, Georgia are extremely close to each other while Athens, Greece and Tirana, Albania (*) are far part; obviously this metric is useless for our intended purposes. They are many possible metrics; to mention some commonly used: Euclidean distance , Chebychev distance , Mahalanobis distance , Hamming distance and Cosine similarity . We need to therefore either derive/select a distance metric based on our prior knowledge of the data or learn a good metric from our data if possible. Distance metric learning is a task on its own. Some nice first papers on the matter are: An Efficient Algorithm for Local Distance Metric Learning by Yang et al. and Distance Metric Learning for Large Margin Nearest Neighbor Classification by Weinberger et al. The vast majority of applications employ a Euclidean distance (or cosine similarity if they are an NLP application) but this might be not be the most appropriate for the data at hand. So, first think what is a reasonable metric of similarity between the data to be clustered and then focus on the $k$. (*) For the less-aware of European geography: Albania and Greece are adjacent to each other.
