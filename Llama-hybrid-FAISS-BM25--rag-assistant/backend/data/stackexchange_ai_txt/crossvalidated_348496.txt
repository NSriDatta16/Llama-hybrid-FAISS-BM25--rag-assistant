[site]: crossvalidated
[post_id]: 348496
[parent_id]: 
[tags]: 
Optimal neural net weights when using cross entropy loss

I'm trying to understand how cross-entropy works for finding the optimal weights in neural networks. According to Eli Bendersky's website and neural networks and deeplearning tutorial, we can find the optimal weights using gradient descent. What if we use the cross-entropy loss function without using gradient descent? I mean is there any way to just find the derivative of cross-entropy with respect to w_ij and then makes the result of derivative equal to zero like what we do in Winer. Is there any resource available which try to find the optimal weights of neural networks using this way?
