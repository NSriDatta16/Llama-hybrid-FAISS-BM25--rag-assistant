[site]: crossvalidated
[post_id]: 174075
[parent_id]: 174058
[tags]: 
As wikipedia tells us the coefficient of determination as 1 minus the ratio of the explained sum of squares of the model over the total sum of squares of the sample we use this model upon. Or with some hand-waving: 1 minus the explain over total variance in your modelling. Now as you correctly identify your model1 , model2 and model3 are closely related and one should be able to combine the resulting RSS from model1 and model2 and get the RSS of model3 . (The other parts of the $R^2$ are the same). So let's see what can we get out of the box: set.seed(123) N = 123 data = data.frame(x1 = runif(N), x2= runif(N )); data$y = data$x1 + data$x2 + rnorm(N); model1 = lm(y~x1, data) model2 = lm(y~x2, data) model3 = lm(y~x1+x2, data) And lets check if our intuition is correct: 1 - sum(( data$y - fitted(model1))^2) / sum((data$y - mean(data$y))^2) - summary(model1)$r.squared # -1.457168e-16 1 - sum(( data$y - fitted(model2))^2) / sum((data$y - mean(data$y))^2) - summary(model2)$r.squared # -1.110223e-16 1 - sum(( data$y - fitted(model3))^2) / sum((data$y - mean(data$y))^2) - summary(model3)$r.squared # -4.163336e-17 All these guys evaluate to numerical zeros so we are pretty confident we got out basic definition right (or at least aligning with R 's definition.) Now we mentioned we want the RSS of model1 and model2 to get as close as possible to the RSS of model3 . Let's remember that the RSS is essentially the sum of squared differences of the original values from predicted sample mean plus the predicted individual variations (ie. RSS = $\sum( y_i - \hat{y}_i)^2$ where $\hat{y}_i = \hat{\mu} + x_i\hat{\beta}$. So actually if we want the fitted values of model3 we have: $\hat{y}_{i,m3} = \hat{\mu}_{m3} + x_{1,i}\hat{\beta}_{1,m3} + x_{2,i}\hat{\beta}_{2,m3}$; where $\hat{y}_{i,m3}$ is the $i$-th predicted value from model3 , $\hat{\mu}_{m3}$ is the estimated sample mean value from model3 and $\hat{\beta}_{2,m3}$ is the estimated $\beta$ coefficient for variable x2 from model3 . Similarly for your example for model1 's fitted values we will have $\hat{y}_{i,m1} = \hat{\mu}_{m1} + x_{1,i}\hat{\beta}_{1,m1}$ and for model2 $\hat{y}_{i,m2} = \hat{\mu}_{m2} + x_{2,i}\hat{\beta}_{2,m2}$. So if we blindly say that $\hat{y}_{i,m3} = \hat{y}_{i,m1} +\hat{y}_{i,m2}$ we will clearly are about an estimated $\hat{\mu}$ off. So the obvious thing is to just add a $\hat{\mu}$. ie. 1 - sum(( data$y + mean(data$y) - fitted(model2) - fitted(model1) )^2) / sum( (data$y - mean(data$y))^2) - summary(model3)$r.squared # -3.320282e-06 # (Discrepancy? Check covariance.) Hm... This definitely close enough to suggest we are on the right track but not close enough to say we got right down to standard numerical precision. The catch here is in the covariance between the x1 and x2 ; this is the only bit information that while present in model3 is not present in model1 or model2 . We can remind ourselves that by remembering that using the simple OLS formula for the estimation of $\beta$, can be written as: \begin{align} \hat{\beta} = \frac{\hat{cov}(y,x)}{\hat{var}(x)}. \end{align} and in the same manner if you have multiple predictors, say x1 and x2 you would write: \begin{align} \hat{\beta} = \hat{cov}(y,[x_1 x_2])^{-1}\hat{var}([x_1 x_2]). \end{align} Clearly now if there is any covariance between x1 and x2 that will come into play and it will change our estimates. Is there any? cov(data[,1:2])[2] # [1] -0.0004360986 Yeap, it exists. Nothing huge, but something is there. OK, so what if we had independent variables? Well, let's see that. I will use PCA to get the principal components scores of our sample and use those as surrogate data for a new dataset. PCScores = princomp(data[,1:2])$scores cov(PCScores)[2] [1] -4.890018e-19 # Nice numerical zero newdata = data.frame(x1 = PCScores[,1], x2 = PCScores[,2]); newdata$y = newdata$x1 + newdata$x2 + rnorm(N); model1 = lm(y~x1, newdata) model2 = lm(y~x2, newdata) model3 = lm(y~x1+x2, newdata) 1 - sum(( newdata$y - fitted(model1) )^2) / sum( (newdata$y - mean(newdata$y))^2) - summary(model1)$r.squared # 8.326673e-17 1 - sum(( newdata$y - fitted(model2) )^2) / sum( (newdata$y - mean(newdata$y))^2) - summary(model2)$r.squared # -9.714451e-17 1 - sum(( newdata$y - fitted(model3) )^2) / sum( (newdata$y - mean(newdata$y))^2) - summary(model3)$r.squared # 2.775558e-17 So far so good what about our intuition about $\hat{\mu}$ though? 1 - sum(( newdata$y + mean(newdata$y) - fitted(model2) - fitted(model1) )^2) / sum( (newdata$y - mean(newdata$y))^2) - summary(model3)$r.squared # 2.775558e-17 Or mathematically: \begin{align} 1 - \frac{\sum ( y_i + \hat{\mu}_y - \hat{y}_{i,m1} - \hat{y}_{i,m2})^2}{\sum ( y_i - \hat{\mu}_y)^2} \end{align} Which is now numerically correct. :) We have replicated exactly the RSS of model3 using model1 and model2 because the explanatory variables in model1 and model2 are independent. Let me note that one might be tempted to somehow insert a covariance term in the calculation shown above. That is not really feasible because the presence of the covariance terms will render the estimates $\hat{y}_{i,m1}$ and $\hat{y}_{i,m2}$ suboptimal as they will be estimated without taking in account any covariance-based variation. In that case, yes, one will be able to an $R^2$ but it will be biased.
