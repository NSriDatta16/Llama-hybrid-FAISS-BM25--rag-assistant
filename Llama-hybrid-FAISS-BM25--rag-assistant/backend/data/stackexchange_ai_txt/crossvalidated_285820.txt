[site]: crossvalidated
[post_id]: 285820
[parent_id]: 
[tags]: 
Explain this backpropagation graph?

Can someone help explain this backpropagation graph? This is from cs231n Convolutional Neural Networks for Visual Recognition by Stanford. So for this graph, let's say the true value of the output was 1.03 and the alpha value was 1. Then the error*alpha would be (1.03-0.73)*1 = 0.3. So for the backpropagation, it would update all of the green values by multiplying the red values by 0.3 and subtracting it from the green values, not including x0, and x1 because they are the input values? Is this correct? I appreciate the help.
