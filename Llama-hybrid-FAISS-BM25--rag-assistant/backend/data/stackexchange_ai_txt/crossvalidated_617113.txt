[site]: crossvalidated
[post_id]: 617113
[parent_id]: 
[tags]: 
Conceptual questions about the proxy distribution in variational inference

I am trying to implement a variational extension of some kind of Bayesian network estimation method. The main goal is to improve speed, since the current method is pretty slow due to MCMC. My question should be pretty simple for veterans in the field: "How to obtain the optimal variational density?" "Is it fixed a priori? Or derived analytically from the data ?". But before that, I think it's important to validate my understanding on what is variational inference and why we use it. This post it's an attempt to summarize what I think I understand. Correct me if I did any mistakes during the process. DISCLAIMER: I am a frequenstist-trained biostatistician. This is my first bayesian attempt in research. What I think I understand : Assuming we are in a general framework (linear regression for example) with one vector of parameter $\beta$ and $\sigma$ , each following a given distribution. From a bayesian perspective we are looking for the posterior distribution of some parameters $\beta, \sigma$ given data $Y$ in order to do inferences. Formally: $$ p(\beta, \sigma|Y) = \frac{p(\beta)p(\sigma)p(Y|\beta, \sigma)}{p(Y)} \propto p(\beta)p(\sigma)p(Y|\beta, \sigma) $$ Since the denominator is basically an integral over all parameters $\int_{\beta}\int_{\sigma} p(Y, \beta, \sigma) d_{\beta} d_{\sigma}$ , in presence of lot of parameters, this density is often intractable. One approach to estimate this quantity it's by sampling (MCMC), but this is not the focus here. Another approach is to consider variational inference (VI). VI turns the approximation problem into an optimization one. Let's move into VI more formally and check if I understand all concepts in a good way. In VI we consider a proxy parametric distribution for parameter $q(\beta, \sigma)$ which is easy to compute while enough flexible to capture the true posterior. Traditionally, researchers try to minimize some kind of metric in order to determine the best $q(.)$ . The most common is the Kullback-Leibler divergence. This is just an asymptotic Likelihood ratio test statistic between $q(\beta, \sigma)$ and $p(\beta, \sigma|Y)$ . Formally: $$ KL(q(\beta, \sigma) || p(\beta, \sigma|Y)) = \int q(\beta, \sigma) \log(\frac{q(\beta, \sigma)}{p(\beta, \sigma|Y)})d_{\beta, \sigma} $$ From $$ p(Y) = \int_{\beta}\int_{\sigma}p(Y, \beta, \sigma) d_{\beta}d_{\sigma} $$ $$ = \int_{\beta}\int_{\sigma} q(\beta, \sigma) \frac{p(Y, \beta, \sigma)} {q(\beta, \sigma)} d_{\beta}d_{\sigma} = \int_{\beta}\int_{\sigma} q(\beta, \sigma) \frac{p(\beta, \sigma |Y) p(Y)} {q(\beta, \sigma)} d_{\beta}d_{\sigma} $$ $$ \log(P(Y)) = \log(\int_{\beta}\int_{\sigma} q(\beta, \sigma) \frac{p(\beta, \sigma |Y) p(Y)} {q(\beta, \sigma)} d_{\beta}d_{\sigma}) >= \int_{\beta}\int_{\sigma} q(\beta, \sigma) \log(\frac{p(\beta, \sigma |Y) p(Y)} {q(\beta, \sigma)}) d_{\beta}d_{\sigma}$$ The last inequality is obtained from Jensen's inequality. The right term of the inequality is the evidence lower bound (ELBO). After simple algebraic manipulations, the ELBO can be re-expressed as: $$ \int_{\beta}\int_{\sigma} q(\beta, \sigma) \log(\frac{p(\beta, \sigma |Y) p(Y)} {q(\beta, \sigma)}) d_{\beta}d_{\sigma} = \int_{\beta}\int_{\sigma} q(\beta, \sigma) (\log(p(\beta, \sigma |Y)) - \log(q(\beta, \sigma)) + \log(p(Y))) d_{\beta}d_{\sigma} \propto -KL(q(\beta,\sigma)||p(\beta, \sigma |Y)) $$ Since $p(Y)$ is constant over $q(\beta,\sigma)$ , minimizing the KL is the same as maximizing the ELBO. From the mean-field theory $q(\beta,\sigma)$ can be rewritten as $q(\beta) q(\sigma)$ . Thus, $$ ELBO = \int_{\beta}\int_{\sigma} q(\beta, \sigma) \log(\frac{p(\beta, \sigma |Y))}{q(\beta, \sigma)}) d_{\beta}d_{\sigma} = \int_{\beta}\int_{\sigma} q(\beta)q(\sigma) \log(\frac{p(\beta, \sigma |Y)}{\log(q(\beta) q(\sigma))} ) d_{\beta}d_{\sigma} $$ Rewriting the ELBO we obtain: $$ \int_{\beta}\int_{\sigma} q(\beta)q(\sigma) \log(\frac{(p(\beta)p(\sigma)P(Y|\beta, \sigma))}{q(\beta) q(\sigma)} ) d_{\beta}d_{\sigma} $$ $$ = \int_{\beta}\int_{\sigma} q(\beta)q(\sigma) \log(p(\beta))d_{\beta}d_{\sigma} + \int_{\beta}\int_{\sigma}q(\beta)q(\sigma) \log(p(\sigma))d_{\beta}d_{\sigma} + \int_{\beta}\int_{\sigma}q(\beta)q(\sigma) \log(p(Y|\beta, \sigma))d_{\beta}d_{\sigma} - \int_{\beta}\int_{\sigma}q(\beta)q(\sigma) \log(q(\beta))d_{\beta}d_{\sigma} - \int_{\beta}\int_{\sigma}q(\beta)q(\sigma) \log (q(\sigma)) d_{\beta}d_{\sigma} $$ EDIT : We can rewrite it in terms of expectations: $$ E_{q(\beta, \sigma)}\log(p(\beta)) + E_{q(\beta, \sigma)}\log(p(\sigma)) + E_{q(\beta, \sigma)}\log(p(Y| \beta, \sigma)) - E_{q(\beta, \sigma)}\log(q(\beta)) - E_{q(\beta, \sigma)}\log(q(\sigma)) $$ From my understanding we have two general approaches to optimize the ELBO. Analytically , where each optimal $q(z)^*$ are defined for every particular problem. Following Blei et al., 2017 for the jth parameter $q_j(z_j)^* \propto \exp{E_{-j}(log(p(Z,X))}$ . This quantity is the same as integrating out over the jth parameter keeping all the other parameters constant. But practically speaking I am not sure what does that mean. Approximately , where there is no context-specific optimal $q(z)^*$ but we use approximate distribution instead, such as Gaussian or any distribution closed to the prior. I know that in PyMC3 or Stan, Gaussian Mean-field approxiamtions are used. If I translate it correctly it's the same as replacing $q(z_j) \sim N(z_j, \mu_s, \sigma_s)$ , where $\mu_s, \sigma_s$ are the corresponding variational parameters. I know that other kind of approximations are available depending on the context. My questions Now this is the difficult part for me. I am not sure which method is better in which context. Is the Gaussian mean-field can be extended to other kind of parametric family ? I am thinking about Laplace distribution for example. Also, I will appreciate if a high-level explanation of the algorithm needed to obtain the variational parameters can be provided. Some authors talk about "EM-like" algorithms. But I am not sure what is done at each iteration.
