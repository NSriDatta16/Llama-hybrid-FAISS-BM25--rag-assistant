[site]: crossvalidated
[post_id]: 507922
[parent_id]: 507910
[tags]: 
We have to start step by step. We assume that there are random variables $X_1, X_2, ..., X_n, X$ , $Y_1, Y_2, ..., Y_n, Y$ and $\text{Eps}_1, ..., \text{Eps}_n, \text{Eps}$ and that the pairs $(X, Y)$ and $(X_i, Y_i)$ are iid (identically, independently distributed). We also assume that there is a function $f$ such that for all $i$ , $$Y_i = f(X_i) + \text{Eps}_i$$ and such that $$Y = f(X) + \text{Eps}$$ In reality, we assume that there is one single $\omega$ in the complicated original probability space $\Omega$ such that if we are given a real dataset with actual values $x_i, y_i$ then $X_i(\omega) = x_i$ and $Y_i(\omega) = y_i$ and we define $\epsilon_i = \text{Eps}_i(\omega)$ . CAUTION: Many applied statisticians do either not know or do not care about the difference between a random variable (i.e. a function) and one single value of that random variable. This can sometimes lead to confusion! In our setup, $X_i, Y_i$ and $E_i$ are random variables and $x_i, y_i$ and $\epsilon_i$ are concrete values. This is important because e.g. for concrete values we cannot compute an expectation, expressions like $E[y|x]$ or so do not make sense: First of all there is no $y$ and no $x$ and secondly, if there were, it would not be random variables but single values. The problem is that the function $f$ is hidden from us (we only know the $x_i, y_i$ and try to recover it as good as possible from that). Hence we try different approximations $\hat{f}$ and evaluate which one is the best. Let's assume that we have decided for one particular $\hat{f}$ . That could be one very particular decision tree, one very particular neural network with fixed weights, etc. Then we reason like this: If $\hat{f}$ was a good representative for $f$ then it should somehow be able to reproduce/explain the data $x_i, y_i$ that we observed. So what we do is we compute $$E[Y_i - \hat{f}(X_i)]$$ for an arbitrary but fixed $i$ (this expression does not change when we change $i$ because of the idd assumption above). However, this would lead to only measuring how well $f$ does on one single training example in the end. However, since $(X,Y)$ is also iid, we may also choose $(X, Y)$ instead of $(X_i, Y_i)$ in this expression. i.e. we will measure $$E[Y - \hat{f}(X)]$$ However, there is a problem with this expression: if $\hat{f}(X)$ sometimes overshoots (i.e. is bigger than $Y$ ) and sometimes smaller but is never actually close to $Y$ then we get a value close to $0$ but the function we have selected is totally shitty. Hence, we need to make this number in the expectation positive. We could evaluate $$E[|Y - \hat{f}(X)|]$$ but for technical reasons we sometimes rather use $$E[(Y - \hat{f}(X))^2]$$ It turns out that this expression can be written as $$E[(Y - \hat{f}(X))^2] = \text{Var}(\text{Eps}) + \text{Var}(\hat{f}(X)) + E[f(X) - \hat{f}(X)]$$ There is two things that you mix up right now (I guess): The Bias-Variance-Dilemma and 'checking how good $\hat{f}$ reproduces the data'. 1: Checking how good $\hat{f}$ reproduces the data In order to check how good $\hat{f}$ reproduces the data we try to approximize $E[(Y - \hat{f}(X))^2]$ : By the law of large numbers we know that $$\frac{1}{n} \sum_{i=1}^n (Y_i - \hat{f}(X_i))^2$$ converges in expectation against the expression above. In very simple words, this means that if $n$ is large enough then for 'almost all' (and we hope that this includes the $\omega$ above) elements in $\omega' \in \Omega$ , $$\frac{1}{n} \sum_{i=1}^n (Y_i - \hat{f}(X_i))^2 ~~ \text{evaluated at $\omega'$}$$ is very close to $$E[(Y-\hat{f}(X))^2]$$ So when we insert $\omega$ then we get that $$\frac{1}{n} \sum_{i=1}^n (Y_i - \hat{f}(X_i))^2(\omega) = \frac{1}{n} \sum_{i=1}^n (Y_i(\omega) - \hat{f}(X_i(\omega)))^2(\omega) = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{f}(x_i))^2$$ is very close to what we want to estimate, namely $$E[(Y-\hat{f}(X))^2]$$ Hence: If the training set is good enough, then evaluating the RMSE of $\hat{f}$ on that training set will be very close to the (much more complicated) expression $E[(Y-\hat{f}(X))^2]$ that describes how goof $\hat{f}$ resembles $f$ . 2: Choosing the right $\hat{f}$ So which $\hat{f}$ should we choose in order to make $E[(Y-\hat{f}(X))^2]$ small? Which weights should we put on a neural network, which splits should we select in a decision tree? And first and foremost: which class of models should we use? Simple linear regression? Random forests? Neural networks? The second point is about that: selecting the right 'class' of models (not necessarily the single instance of the model). If the class is linear regression then $\text{Var}(\hat{f}(X))$ will be small and $E[f(X) - \hat{f}(X)]$ will potentially be big (the actual function is wiggly but $\hat{f}$ is just linear then they will probably do different things often). If the function class is more complex like neural networks, random forests, gradient boosting (you name it), then it is able to resemble $f$ better, i.e. $E[f(X) - \hat{f}(X)]$ will be small but then $\text{Var}(\hat{f}(X))$ will be big. The key is to find the right class (not too complicated and not too simple) so that the sum of both, $E[f(X) - \hat{f}(X)] + \text{Var}(\hat{f}(X))$ is small (then also $E[(Y-\hat{f}(X))^2]$ becomes small because of the decomposition above). However, while point 1 gives a rather practical guide on how to find the right $\hat{f}$ (just choose the one with the lowest RMSE), the second point is rather a motivation on why we need to use regularization whenever we choose for more complex models but it does not say anything about how to do it. There are theoretical results about it involving the Vapnik dimension and shattering coefficients and so forth but that would lead too far for this post. NOTE: It seems tempting to choose an $\hat{f}$ with RMSE=0. Why would that be pretty bad usually? (Hint: train and test set) NOTE2: It is not like 'when choosing cross validation instead of splitting the data only once then the variance goes up/down'! It is about the quality of the estimate for the variance. When we split the dataset more often (in the extreme case: leave-one-out CV) then we can estimate the variances better / more reliably. They do not go up/down! Whether they are low/high depends on the model class and the model, not on the way how we evaluate it!
