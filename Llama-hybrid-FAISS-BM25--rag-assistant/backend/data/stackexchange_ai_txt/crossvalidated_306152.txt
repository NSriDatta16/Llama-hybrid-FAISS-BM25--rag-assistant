[site]: crossvalidated
[post_id]: 306152
[parent_id]: 280665
[tags]: 
We've been through this before -- you're getting too mathematical about a dead horse. See Ron Kohavi's (Stanford-Univ) classic paper on CV and the bias-variance dilemma here . When you're done reading this, you won't want to perform LOOCV, and will likely be attracted to 10-fold CV and/or bootstrap-bias CV. You also have to think about large datasets, for which LOOCV is way too computationally expensive. At present, LOOCV is not really an option in most groups' workflows/pipelines. What precisely is this "stability" condition? Does it apply to models/algorithms, datasets, or both to some extent? In the universe of all cost functions and in the universe of all feature sets, I would not assume there is an overall "stability" index, because it would not be inadmissible, and would be too prone to breaking down under an infinitely large set of conditions. Fundamentally, $k=n$ is appropriate when the d.f. and/or # parameters is so large that more training data are needed. Bias will also be greater for $k=n$, since more data are used, and variance would be artificially zero, since the training datasets are too similar to one another. You would also be learning more noise in the data when $k=n$. LREG as a classifier would work when the data are linearly separable, but on average its bias would be too high, since many datasets are not linearly separable. Is there an intuitive way to think about this stability? Not in my view -- since there is no general rule on stability. What are other examples of stable and unstable models/algorithms or datasets? This is open-ended and too broad, since an infinitely large number of responses can be contrived, which would not be helpful. Is it relatively safe to assume that most models/algorithms or datasets are "stable" and therefore that $K$ should generally be chosen as high as is computationally feasible? No. No. Relying only on $k$ assumes that you believe the data. An example is Random Forests, for which there really is no $k$. While roughly 37% of the data will be used for testing (on average, 37% of objects are not selected when sampling with replacement), there are e.g. 5,000 different datasets (bootstraps) each of which are split into training/testing differently. Your example pulled from papers assumed that each dataset used was a true realization of the data -- which is an erroneous assumption. Given bootstrapping, the rule of stability surrounding $k$ is admissible, since the data sample used for a straightforward CV approach involving $k$ is not a true realization of the universe of all data from which the sample was obtained.
