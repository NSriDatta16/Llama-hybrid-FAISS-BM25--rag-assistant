[site]: crossvalidated
[post_id]: 608919
[parent_id]: 533395
[tags]: 
When overfitting is not an issue, we rarely care about the internal vs. external validity of the $R^2$ statistic. Machine learning often confronts analyses where overfitting is a prominent issue. ML advocates calculating an $R^2$ - like statistic using split-sample validation, cross-validation, or a number of resampling based techniques. However, calling these statistics the $R^2$ is a mistake. It's something different and requires considerably more assumptions to replicate the findings, such as the specific sample-splitting technique, the number of iterations, the seed used to generate the findings, software versions, etc. One also must consider assessing the stability of these estimates since the MCMC error needs to be ameliorated to produce generalizable findings. These are computer-era considerations which the $R^2$ long preceeded.
