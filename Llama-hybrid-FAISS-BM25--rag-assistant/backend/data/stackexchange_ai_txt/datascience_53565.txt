[site]: datascience
[post_id]: 53565
[parent_id]: 53009
[tags]: 
Neural networks have a tons of hyperparameters you can tune to improve the result : Test other activation functions for the hidden layer : relu or sigmoid for example Add some training examples Tune the learning rate : sometimes the learning rate is too high for a good convergence Change the optimization strategy : SGD, Adam, genetic algorithm ...
