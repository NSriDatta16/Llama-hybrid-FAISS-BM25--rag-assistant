[site]: crossvalidated
[post_id]: 560193
[parent_id]: 208867
[tags]: 
Logistic regression models are typically trained by minimizing the negative log likelihood. Maximum-likelihood models are known to asymptotically identify the solution minimizing the Kullback-Leibler divergence $D_{KL}(P, Q)$ between the empirical data distribution $P$ and the distribution $Q$ described by the model, see wiki . Minimizing the KL divergence between the two distributions implies optimizing for calibration.
