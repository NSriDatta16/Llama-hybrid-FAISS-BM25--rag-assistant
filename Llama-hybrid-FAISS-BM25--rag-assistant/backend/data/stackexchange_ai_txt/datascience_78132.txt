[site]: datascience
[post_id]: 78132
[parent_id]: 
[tags]: 
Back propagation through a simple convolutional neural network

Hi I am working on a simple convolution neural network (image attached below). The input image is 5x5, the kernel is 2x2 and it undergoes a ReLU activation function. After ReLU it gets max pooled by a 2x2 pool, these then are flattened and headed off into the fully connected layer. Once through the fully connected layer the outputs are converts into Softmax probabilities. I've propagated froward through the network and am now working on back propagation steps. I have taken the derivative of cross entropy and softmax, and calculated the weights in the fully connected layer. Where I get confused is how to preform back propagation through Max pooling and then ultimately find the derivatives of the weights in the convolution layer. What I've found online is that you need to find the derivative of the loss with respect to flattened layer, but I am unsure on how you do that. If I could get some help with an explanation, ideally with equations it would be awesome. Cross posted in stack overflow ( https://stackoverflow.com/questions/63022091/back-propagation-through-a-simple-convolutional-neural-network )
