[site]: crossvalidated
[post_id]: 397936
[parent_id]: 377278
[tags]: 
For unregularized generalized linear models it's usually not a good idea to one-hot encode and not remove one of the variables because of colinearity. In that case your one-hot encoded design matrix doesn't have full rank, and you cannot invert $X^TX$ (the moment matrix) in case you need it (e.g., depending on your optimization method for getting the model coefficients). In most applications, and if you regularize your model, this is not an issue. Also, most algorithmic approaches like random forests etc. have no issues with one-hot encoded design matrices (versus dummies). In other words, I'd say in 99% of ML applications, one-hot is just fine and there is no need to bother about using dummies vs one-hot encoded design matrices.
