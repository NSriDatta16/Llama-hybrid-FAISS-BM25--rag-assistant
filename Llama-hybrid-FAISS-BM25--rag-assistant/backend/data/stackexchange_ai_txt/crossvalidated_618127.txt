[site]: crossvalidated
[post_id]: 618127
[parent_id]: 
[tags]: 
Choice of base distribution in normalizing flows for product distribution

I'm currently trying to implement a normalizing flow (NF) to efficiently sample from a product distribution $p=fg$ . Contrary to most examples I've come across, I actually know how to sample analytically from both $f$ and $g$ , but sampling from their product requires a large tabulation which quickly become intractable in practice. Using neural spline flows (NSF) I can get a relatively decent fit with a trainable Gaussian base distribution $q_0$ . However, I have been wondering if using a more informed, problem-specific prior could improve performance. For instance, it would make sense to me to start from $f$ , $g$ , or a learnable mixture $w f + (1-w)g$ of the two as a base, in which case the NF would already have access to some parts of the product. Intuitively, this seems like it would greatly simplify the sequence of diffeomorphisms the NF has to learn in order to warp $q_0$ to $p$ . I have tried implementing this, but to my surprise this approach performs worse than a base Gaussian and I am trying to understand why. My hypothesis is that this has something to do with how the NF's warps are initialized which can be arbitrarily far from identity maps. Since NSF's splines are parameterized by the output of neural network, it doesn't seem feasible to have an initialization scheme that would encourage a heavy reliance on the prior early on into training. Is my intuition correct regarding the base distribution? Are there any papers that have investigated different base distributions for NFs? Or perhaps different initialization schemes for NFs?
