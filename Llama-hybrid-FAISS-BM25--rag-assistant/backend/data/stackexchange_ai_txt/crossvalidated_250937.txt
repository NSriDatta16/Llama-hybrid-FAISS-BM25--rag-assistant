[site]: crossvalidated
[post_id]: 250937
[parent_id]: 
[tags]: 
Which loss function is correct for logistic regression?

I read about two versions of the loss function for logistic regression, which of them is correct and why? From Machine Learning , Zhou Z.H (in Chinese), with $\beta = (w, b)\text{ and }\beta^Tx=w^Tx +b$ : $$l(\beta) = \sum\limits_{i=1}^{m}\Big(-y_i\beta^Tx_i+\ln(1+e^{\beta^Tx_i})\Big) \tag 1$$ From my college course, with $z_i = y_if(x_i)=y_i(w^Tx_i + b)$ : $$L(z_i)=\log(1+e^{-z_i}) \tag 2$$ I know that the first one is an accumulation of all samples and the second one is for a single sample, but I am more curious about the difference in the form of two loss functions. Somehow I have a feeling that they are equivalent.
