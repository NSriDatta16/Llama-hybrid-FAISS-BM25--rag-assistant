[site]: crossvalidated
[post_id]: 313273
[parent_id]: 
[tags]: 
Fitting a Multivariate ARMA-GARCH model

I am considering a multivariate time series . I denote the general term of this multivariate time series by $Y_t = \left[y_t^{(1)}, \ldots, y_t^{(k)}\right]^T \in \mathbb{R}^{k \times 1}$. For some background, each univariate series $\left(y_t^{(i)}\right)_t$ corresponds to the daily returns on the price of a security. If $i \ne j$, $\left(y_t^{(i)}\right)_t$ and $\left(y_t^{(j)}\right)_t$ correspond to the daily returns on the price of distinct securities. I have checked that the multivariate time series is stationary, and I want to fit a VARMA-GARCH model to it . I plan to use Maximum Likelihood Estimation to do so. I have conducted some research into the literature, and I have found a paper by Tsay and Wang On Diagnostic Checking of Vector ARMA-GARCH Models with Gaussian and Student-t Innovations (2013) ( see here for the article ). The authors write the model as follows : $Y_t = \sum_{i=1}^{p} \Phi_i Y_{t-i} + a_t + \sum_{j=1}^{q} \Theta_j a_{t-j}$, $~ ~ ~ ~ a_t = \Sigma_t^{\frac{1}{2}} \epsilon_t ~ ~ ~$ where $\Sigma_t = A_0 + \sum_{i=1}^{r} A_i a_{t-i} a_{t-i}^T A_i^T + \sum_{k=1}^{s} B_k \Sigma_{t-k} B_k^T$. The $\epsilon_t$'s are a sequence of i.i.d. $k$-variate random vectors with mean 0 and covariance matrix the identity $I_k$. For instance we can take that $\forall t,~ \epsilon_t \sim \mathcal{N}_k \left(0, I_k\right)$ and they are i.i.d . The parameters to estimate are $\phi = vec(\Phi_1, \ldots, \Phi_p, \Theta_1, \dots, \Theta_q)$ and $\delta = vec(A_0, \ldots, A_r, B_1, \dots, B_s$ where $vec$ is the operation of vectorisation of a matrix. I regroup all these parameters into $\lambda$, $\lambda = \left[\phi^T, \delta^T\right]^T$. If I know the orders p and q of the ARMA part as well as the orders r and s of the GARCH part, to determine $\hat{\lambda}_{MLE}$, I "just" have to write the likelihood, take the derivative with respect to $\lambda$ etc. as usual. I have the likelihood : $L_n(\lambda) = \frac{1}{n} \sum_{t=1}^{n} l_t(\lambda)~ ~ ~$ with $l_t(\lambda) = -\frac{1}{2} \log \det\left(\Sigma_t\right) - \frac{1}{2} a_t^T \Sigma_t^{-1} a_t$ where $\Sigma_t$ is defined as in above and $a_t$ is defined by $a_t = \left[I_k + \sum_{i=1}^{q} \Theta_i L^i \right]^{-1} \left[I_k - \sum_{i=1}^p \Phi_i L^i\right] Y_t$ where $L$ is the difference operator : $L Y_t = Y_{t-1}$. My question is - thanks for reading up to here - what is an explicit form I can use for matrix $\left[I_k + \sum_{i=1}^{q} \Theta_i L^i \right]^{-1}$ when I want to numerically solve $\frac{dL_n}{d\lambda}\left(\lambda\right) = 0$ ? I think I am missing something/not looking at this MLE from the right angle and I am completely stuck. I welcome any suggestion as to how to proceed !
