[site]: crossvalidated
[post_id]: 234204
[parent_id]: 234181
[tags]: 
So the advantage of coding directly is that you can "see that it works", the disadvantage is that if you haven't done the maths on the side you might get confused (as you apparently are now) so allow me to do some maths: You have an initial data matrix which we will call $M$ (the notations in the code are particularly unhelpful). This is your Xt which is of size $100\times 50$. Now $M$ can be written as: $ M = U\Lambda V^T$ this is the SVD decomposition (such a decomposition always exists), $U$ is of size $100\times 50$, $\Lambda$ is a diagonal matrix of size $50\times 50$ and $V$ a squared, orthonormal ($V^TV =I)$, matrix of size $50\times 50$. To connect with the R code, res contains the results from the decomposition with res$x being $U\Lambda $ and res$rotation being $V$. You can check this in your code by doing res which should give you something extremely small. Now PCA essentially amounts to truncating a bunch of singular values (just ignoring them). That is, you write a reconstruction $\hat M = U \hat \Lambda V^T$ where $\hat \Lambda$ is the same $\Lambda$ but with a zero diagonal apart from the first few largest singular values (corresponding to the number of components you're considering). So in your case, keeping only three elements $\hat\Lambda = \left(\begin{array}{ccccc} a & & & &\\ & b& & &\\ & & c & &\\ & & & 0 &\\ & & & & \ddots\end{array}\right)$ but multiplying this matrix to $V^T$, means really you should only care about the first three lines of $V^T$ in the reconstruction (the rest will be multiplied by zero), and similarly, the first three columns of $U$ the PCA uses slightly different notations but the principle is exactly the same, the res$x is, as said before, $U\Lambda$ so taking res$x[,1:pc.use] exactly amounts to $U\hat\Lambda$. And similarly, the first three rows of $V^T$ are the transpose of the first three columns of $V$ whence the t(res$rotation[,1:pc.use]) the $\hat M$ from above is then just your Xt_reconstructed : Xt_reconstructed I'm hoping by now all the operations make sense. What I'd stress given your question is that you're not reconstructing the original data ($M\neq \hat M$, obviously, unless you're taking all the components which is silly). There may be confusion due to this. What you're doing is reconstructing in the original space. That's it. But effectively all you care about is that you can now store a "pretty good" approximation to $M$ by just storing the first three columns of $U\Lambda$ (your res$x[,1:pc.use] ) and the first three columns of $V$ (your res$rotation[,1:pc.use] ), then whenever you actually need to consider what it looks like (in order, for example, to visualize it) then you compute the product and you end up with a representation which has the same dimension than the original object (and therefore is comparable to it) but has effectively less information in it. More info about the SVD: https://en.wikipedia.org/wiki/Singular_value_decomposition More info about PCA versus SVD: Relationship between SVD and PCA. How to use SVD to perform PCA?
