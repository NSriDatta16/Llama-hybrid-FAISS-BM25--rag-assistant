[site]: crossvalidated
[post_id]: 469507
[parent_id]: 
[tags]: 
understanding 0 kl-divergence loss in beta variational autoencoders

I've been reading this paper "Understanding disentangling in beta-VAE" and there was one part I was confused about. https://arxiv.org/abs/1804.03599 (paper link) On Figure 4, it shows that if the KL-divergence loss between the latent posterior distribution and the latent prior distribution is close to, or equal to 0, varying the values in that latent dimension doesn't produce any changes in the generated images. Why is that? My understanding is, if the KL-divergence loss is 0, we can assume that that latent dimension's distribution is an isotropic unit gaussian distribution, so it has 0 mean and 1 variance. Why does the fact that it's isotropic unit gaussian mean the generated images don't change?
