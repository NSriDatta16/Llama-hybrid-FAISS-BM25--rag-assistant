[site]: crossvalidated
[post_id]: 418469
[parent_id]: 
[tags]: 
Are boosted machine learning methods robust against low probable feature combinations when predicting?

I would like to use machine learning methods in the potential outcome framework, that is, simulating outcome for all observations under different values of a specific predictor, while keeping all other features fixed. The problem with this approach is that some feature combinations are simply unlikely to happen. For example, for healthcare-associated infections, it is known the great importance of central vascular catheters in driving up the risk. But if I would like to simulate the individual risk change if all the patients in a hospital would have the catheter compared to not having it, I have to keep in account that is very unlikely that, say, psychiatry ward patients would ever get a vascular catheterization. Causal estimation methodologies use techniques like propensity score or observations reweighting to account for this phenomenon. Formally, using a linear model dummy example, given a variable of interest X which I set artificially to 1 for all observations, and Z a set of covariates, for each observation $i$ , it be would somthing like: $$(Yi|do:X=1)=\beta_0 + \beta_x(X=1)p(X=1|Z_i) + \beta_zZ_i$$ That is, when $p(X=1|Z_i)$ is zero, the effect of the of the X disappears (NB: I'm not an expert of counterfactual methods). Normally one would have to compute $p(X=1|Z_i)$ separately. Since ML boosting techniques, like xgboost, work on observation reweighting, I was wondering if such models already keep in account the plausibility of feature values coexistence, by giving more weight to highly predictive combinations of predictors values and less weight to unseen (because unlikely) ones.
