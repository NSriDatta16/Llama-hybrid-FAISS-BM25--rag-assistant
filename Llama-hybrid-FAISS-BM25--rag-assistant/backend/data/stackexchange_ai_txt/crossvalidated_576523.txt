[site]: crossvalidated
[post_id]: 576523
[parent_id]: 576463
[tags]: 
After some searching, I find some explanations: karpathy's explanation and a disscusion in the pytorch forum . @morganmcg1 the purpose of L2 regularization is to "spread out" the weights in dot products, ensuring that more "independent measurements" (dimensions of the input) get used more equally, instead of any one feature dominating the computation. This only makes sense for matrix multiply layers, which the embeddings and the layernorm parameters are not. L2 regularization is covered eg in CS231n.
