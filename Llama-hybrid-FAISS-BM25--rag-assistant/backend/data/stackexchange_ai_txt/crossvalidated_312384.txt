[site]: crossvalidated
[post_id]: 312384
[parent_id]: 310288
[tags]: 
Finally I have realized a work around. After I played with strides, delation and input tensor modification; in the end I used brute force and combine each of the pairs in the input using itertools.combinations to concatenate embedding layers for each input. This improved my prediction by 1.5 percent points over the simple dense layers model (I tested over hundred combinations of those). Assuming you have an input pandas dataframe named features containing integers (label were tokenized) I create the model like this: embedding_output_dim = 8 em, inputs = {}, {} for name in features: vocabulary = features[name].max()+1 #for lookups max = nunique() inputs[name] = Input(shape=(1,)) em[name] = Embedding(vocabulary,embedding_output_dim)(inputs[name]) conv,concatem = [], [] i = 0 for a,b in list(itertools.combinations([key for key in em], 2)): print(a,b,i) concatem.append(concatenate([em[a],em[b]],axis=1)) conv.append(Conv1D(filters=32, kernel_size=2, activation='relu')(concatem[i])) i += 1 convmerge = concatenate([con for con in conv],axis=1) convmerge = Conv1D(filters=64, kernel_size=2, activation='relu')(convmerge) #possibly add further Conv1D and MaxPooling1D flatten = Flatten()(convmerge) d = Dense(1, kernel_initializer='normal', activation='sigmoid')(flatten) model = Model(inputs=[inputs[name] for name in features], outputs=d) For training your model you would use original features dataframe. batch_size = 32 history = model.fit(x = [features[name] for name in features], y = np.array(y), batch_size = batch_size, epochs = 1, validation_split=.1) Don't be scared by the number of parameters. In case one of your input variables have very high vocabulary size (e.g. Age goes up to 100, bude Income can go to milions (of course you can scale it down)), then embedding parameters can grow a lot, though the training might still be quite quick since training of the embedding layers is not that resource consumming.
