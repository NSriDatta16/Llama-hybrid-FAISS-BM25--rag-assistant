[site]: datascience
[post_id]: 87274
[parent_id]: 
[tags]: 
Pytorch Luong global attention: what is the shape of the alignment vector supposed to be?

I am looking at the Luong paper on Attention models and global attention. I understand how the alignment vector is computed from a dot product of the encoder hidden state and the decoder hidden state. So that all makes sense. My question is, what should the dimensions of the alignment score tensor be? If I have my data in batches, I basically compute a single score for each timestep in the hidden state, right. So should the alignment vector be of dimension [sequence length, 1], or something like that? I would then softmax this alignment vector and multiply it by every element in the batch to compute the context vector, right. Again, my key question is what the dimensions of the alignment score vector or tensor should be. Thanks.
