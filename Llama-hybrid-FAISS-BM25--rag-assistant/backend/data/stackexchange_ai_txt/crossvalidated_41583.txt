[site]: crossvalidated
[post_id]: 41583
[parent_id]: 41055
[tags]: 
1) Do power analysis to find out how many students are needed to find a desired effect size. 2) Do realise you are doing 10 different tests, so you have to account for inflated type I error (False Discovery Date), and a simple, say alpha of 0.05 per test is not a good choice here. Question for point 1 and 2 to ask yourself: is it more important to find a test if it works (power), or is it more important to prevent from getting false positives? You should probably find a right balance. 3) You could do simple t-tests between (t being time). t1 and t2, t2 and t3 and so on. Problem is that, if I understand the situation correctly, effects found could also originate from the test before the test measured. That is: when testing whether t4 is better than "baseline" t3, it might be there is an effect of t1 or t2 causing t4 to be higher. Perhaps a nice and easy way to find out which test seemed to have helped, is to graph all test scores together. That is, on the X-axis the different tests (test1,test2,test3), and on the Y-axis the average scores. You should see an upwards trend (like you predict), but in the mean time can see if there might be tests that perform better than expected. If you add a linear regression line, you can then easily see which scores are above it. You can also see if there is some "non-linear" relationship in the sense that the first 3 tests could be great, next 3 bad, while the last are better again.
