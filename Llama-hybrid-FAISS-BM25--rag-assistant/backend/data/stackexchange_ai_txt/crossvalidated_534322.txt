[site]: crossvalidated
[post_id]: 534322
[parent_id]: 533575
[tags]: 
Your notation for this problem is excessive (and adding to the apparent difficulty of the problem), so I'm going to cut it down to bare bones. Your problem can be described by the hierarchical model: $$X|N \sim \text{Bin}(N,\theta) \quad \quad \quad N \sim p,$$ where $\theta$ is known and $p$ is an unknown mass function over the positive integers. Since the value $\theta$ is known, but the mass function $p$ is not, the sampling distribution for a single observation can be obtained from the law of total probability . If we denote the mass function $p$ by the sequence of probability values $p_1,p_2,p_3,...$ with $p_n \equiv p(n)$ then we have: $$\begin{align} \mathbb{P}(X=x|p) &= \sum_{n=0}^\infty \mathbb{P}(X=x|N=n) \mathbb{P}(N=n) \\[6pt] &= \sum_{n=x}^\infty \text{Bin}(x|n, \theta) p(n) \\[6pt] &= \sum_{n=x}^\infty {n \choose x} \theta^x (1-\theta)^{n-x} p(n) \\[6pt] &= \frac{\theta^x}{x!} \sum_{n=x}^\infty \frac{n!}{(n-x)!} (1-\theta)^{n-x} p(n) \\[6pt] &= \frac{\theta^x}{x!} \sum_{n=0}^\infty (n+x)! (1-\theta)^n p(n+x) \\[6pt] &= \frac{\theta^x}{x!} \sum_{n=0}^\infty (n+x)! (1-\theta)^n p_{n+x}. \\[6pt] \end{align}$$ Now, in your problem, you observe IID outcomes of the observable values $X_1,...,X_n$ and you want to use these to make an inference about the unknown mass function $p$ . You have not specified any structure for $p$ , so at the moment it is just an arbitrary distribution on the positive integers. Given IID observations $x_1,...,x_n$ you get the likelihood function: $$L_\mathbf{x}(p_1,p_2,p_3,...) = \text{const} \times \prod_{i=1}^n \sum_{n=0}^\infty (n+x_i)! (1-\theta)^n p_{n+x_i},$$ which gives the log-likelihood function: $$\ell_\mathbf{x}(p_1,p_2,p_3,...) = \text{const} + \sum_{i=1}^n \log \Bigg( \sum_{n=0}^\infty (n+x_i)! (1-\theta)^n p_{n+x_i} \Bigg).$$ The resulting inference depends on whether you want to use classical estimation methods (e.g., MLE) or Bayesian inference. The equations for the MLE are complicated, but Bayesian analysis is high-dimensional, so I leave it to you to decide which you think is the best approach. If you compute the MLE (with some difficulty) you will clearly get $\hat{p}_1 = \cdots = \hat{p}_{x_{(1)}-1} = 0$ , which reflects that fact that your best estimate of $p$ is concentrated entirely on values of $N$ that can give at least one of the observed outcomes. Beyond this, I will leave it to you to decide on your inference method and undertake the required computations. Now, obviously you can simplify this problem substantially if you are willing to assume some simple parametric form for $p$ (e.g., assuming it is Poisson). If you do this then problem becomes an inference on parameters in this parametric form, and those will give you much simpler equations for the MLE. The same basic method will apply, where you determine the sampling distribution using the law of total probability and then form your likelihood function accordingly.
