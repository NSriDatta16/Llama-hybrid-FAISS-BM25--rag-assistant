[site]: crossvalidated
[post_id]: 336219
[parent_id]: 333865
[tags]: 
Inductive reasoning refers broadly to the general process of making inferences about unknowns from knowns, and usually involves some attempt to measure the evidential support for an uncertain proposition, based on what is known. It is distinguished from deductive reasoning insofar as the truth or falsity of the uncertain proposition is not logically determined by what is known. Statistical inference is a formalised form of inductive reasoning where observations are quantified as data and the rules of probability and statistical theory are applied to this data to make inferences. Statistical theory is (in part) a normative theory telling you how you should undertake inductive reasoning. Formal application of this theory requires quantification of data and some technical knowledge, but the theory also gives you broad principles that can be applied in a qualitative way to less formalised contexts. In practice, inductive reasoning is ubiquitous, and statistical theory is time-consuming and hard, so human beings (even expert statisticians) apply the formal apparatus of statistical theory in only a tiny fraction of induction problems they encounter. Since you are engaging in a talk within the context of formal statistical theory, it probably makes more sense to refer to overfitting within this context (i.e., say “statistical inference” rather than “inductive reasoning”). Overfitting refers to the use of analysis that fails to adequately allow for randomness, and therefore fits the data too closely. If you were to refer to overfitting in the context of statistical inference, you might want to describe this in terms that put it in context of that theory, with an appropriate level of formality (e.g., describing overfitting that occurs with an excessive number of parameters in a model, failure to use a train-test split, etc.). It is also possible to talk about overfitting within the broader context of inductive reasoning, outside of formal statistical inference. In this broader context, overfitting similarly refers to an overactive pattern-recognition that fails to adequately allow for randomness, and therefore reasons too closely to observation. This general cognitive tendency has been referred to as apophenia and has been studied extensively. There are a number of studies in psychology that test the ability of humans to recognise sequences of random numbers. These show that human beings tend to have an overactive sense of pattern-finding, and tend to perceive patterns even in complete randomness (e.g., they tend to underestimate the likely number of runs, sample correlations, etc., in randomly generated data). This discussion is outside the scope of formal discussion of machine learning, but it is an interesting little tid-bit of information that relates to overfitting in that context. So, as to what words you should use, I think that depends on the level of generality at which you want to discuss this phenomenon. ( Side note: I agree with the comment by Jacom Socolar that “overfitting” is not the most important aspect of machine learning and statistics. It is very important, but I would say that being able to fit a model in the first place is far more important.)
