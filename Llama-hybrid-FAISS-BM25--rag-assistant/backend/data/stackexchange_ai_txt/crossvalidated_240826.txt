[site]: crossvalidated
[post_id]: 240826
[parent_id]: 231026
[tags]: 
Your suggestion is reasonable. If your groups can be assumed independent, the joint likelihood of respondents in all groups is just the product of the individual group likelihoods. In symbols, if you have response vectors $\mathbf{Y}_1,\mathbf{Y}_2,\ldots \mathbf{Y}_k$ for the $k$ groups, you're assuming that each $\mathbf{Y}_i$ is a vector of correlated binary random variables following an autologistic distribution, call it $g_i(\mathbf{Y};\mathbf{\alpha_i},\beta)$. Here $g_i(\cdot)$ is the autologistic pmf for the $i$th group (each group could have its own graph), $\mathbf{\alpha}_i$ is the vector of individual-specific parameters for group $i$, and $\beta$ is an association parameter held in common between groups. If the groups are independent the joint log-likelihood is then $$l(\mathbf{\alpha}_1,\mathbf{\alpha}_2,\ldots,\mathbf{\alpha}_k,\beta) = \sum_{i=1}^k \log g_i(\mathbf{Y}_i;\mathbf{\alpha_i},\beta).$$ The $g_i(\cdot)$ functions are the actual pmfs, containing the partition functions. For a single parameter setting, computing all the partition functions means evaluating the un-normalized pmf $\sum_{i=1}^k2^{n_i}$ times, which should be possible (as you mentioned) for your case as long as $k$ isn't very large. You could feed it to any continuous optimizer (e.g. optim or nlm in R), but it would be nice to double-check whether or not the thing is convex first to know whether you need to worry about getting stuck in local optima. Finally, some miscellaneous notes. You could alternatively think of your data (all groups) as one big graph with $k$ disconnected subgraphs. The likelihood should work out the same. I've written the above as if the $\mathbf{\alpha}_1,\ldots\mathbf{\alpha}_k$ are distinct parameter vectors, but it's possible you meant that the individual-specific part of your model includes a single vector $\mathbf{\alpha}$ and covariate vectors $\mathbf{x}_i$. This is the autologistic regression model. I strongly recommend that you use the $\{-1,1\}$ coding, rather than $\{0,1\}$ values for your responses. It may seem a trivial change but it greatly influences the interpretation of the parameters. The centered model you mentioned does not do a great job at fixing things. Better just to use the standard model with plus/minus variable coding. I have a paper under review explaining this point but as of now I'm not aware of any publicly-available reference about it.
