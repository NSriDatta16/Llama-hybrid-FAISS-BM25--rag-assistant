[site]: crossvalidated
[post_id]: 417535
[parent_id]: 
[tags]: 
how to interpret the sharp decline in loss in seq2seq models

I have a seq2seq model. I have applied this data over 20_newsgroup data set. My problem is that I face with exploding gradient so when I print the weight they are nan . Then I decide to finish the training early before it gets nan. this is the plotting of loss of training vs validation. I have three questions here: I have a sharp decline in the loss, can it be a sign it got stuck in local minima? if not what could be a good justification of this behaviour? based on the diagram what would you suggest to experience a more stable model? which optimizer doing better? with sgd = SGD(lr=0.01, momentum=0.) and batch_size = 64 In case you think the structure of model can be useful to drive conclusion: This is the structure of my data: inputs = Input(shape=(SEQUENCE_LEN, VOCAB_SIZE), name="input") encoded = Bidirectional(LSTM(LATENT_SIZE, kernel_initializer="glorot_normal",), merge_mode="sum", name="encoder_lstm")(inputs) # encoded = Lambda(score_cooccurance, name='modified_layer')(encoded) decoded = RepeatVector(SEQUENCE_LEN, name="repeater")(encoded) decoded = LSTM(VOCAB_SIZE, return_sequences=True)(decoded) autoencoder = Model(inputs, decoded) # sgd = SGD(lr=0.01, momentum=0.9, clipnorm=1.0) sgd = SGD(lr=0.04, momentum=0.9, clipnorm=1.0, nesterov=True) # sgd = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False) autoencoder.compile(optimizer=sgd, loss='categorical_crossentropy') autoencoder.summary() checkpoint = ModelCheckpoint(filepath='checkpoint/50/{epoch}.hdf5') history = autoencoder.fit_generator(train_gen, steps_per_epoch=num_train_steps, epochs=NUM_EPOCHS, callbacks=[checkpoint]) and this is the way I have prepared data: sent_wids = np.zeros((len(parsed_sentences),SEQUENCE_LEN),'int32') sample_seq_weights = np.zeros((len(parsed_sentences),SEQUENCE_LEN),'float') for index_sentence in range(len(parsed_sentences)): temp_sentence = parsed_sentences[index_sentence] temp_words = nltk.word_tokenize(temp_sentence) for index_word in range(SEQUENCE_LEN): if index_word This is some statistics about my data (20_news_group data set): number of sentences: 58280 distribution of sentence lengths (number of words) min:1, max:141, mean:47.863, 25quart:32.000, med:50.000, 75quart:64.000 vocab size (full): 114490 I have also feed the model with paragraphs, and these paragraphs were prepared by concatenating 15 sentences together: def readingfiles(): paras = [] splitLen = 15 dirname = './Data/20news/' documents = getlistoffiles(dirname) for file in documents: with open(file, encoding='latin1') as f: input = f.read().split('\n') at = 1 for lines in range(0, len(input), splitLen): # First, get the list slice outputData = input[lines:lines + splitLen] outputData = [x.replace('\n','').replace('\t','') for x in outputData if x] at += 1 if len(outputData)>0: paras.append(outputData) return paras Update1 I find out why it faces exploding weight though I don't know how can I fix it! So the problem was not related to data issue, as I checked it with amazon data set and the same thing happened(nan weights). When I changed the one_hot encoding to embedding and use mse as a loss the model behaved well without facing nan weights . So in summary, the same model with one_hot encoding and categorical cross-entropy for loss face with exploding weights. The same model with word embedding technique and mse loss it behaves normally. Update 2 So finally I have done this: I tried the same model with also categorical cross entropy and one hot encoding . (Before going through that, I should mention that as you see in the code the vocab_size is set to 2000 , however, the whole vocab_size available is more than 100000 . So I had OOV problem). So I decided to try with the same model and same one-hot encoding but with less data so that I am not forced to decrease vocab_size . In this scenario (fewer data so I could have the whole data in my sentences, so not facing with OOV problem), the model behaved normally. So, per my hardware limitations, I can not afford to have one_hot encoding with a large number of data which my system crashes. What I have done as the solution is, I trained a word embedding with embed_dime=2000 over my data set separately and then I exactly followed the approach proposed here . with this I can have the matrice with size (latent_size, Embed_size) in the encoder and embed_size is also 2000 which is not small. Though I'm not sure if it is a stable approach or not, Or I could do anything else to get one_hot encoding works on my data?
