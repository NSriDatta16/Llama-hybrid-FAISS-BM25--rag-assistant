[site]: datascience
[post_id]: 56436
[parent_id]: 56432
[tags]: 
As far as I know, overfit a small sample of training data doesn't help predict performance. Normally, overfit a batch of training data is a debugging method. It's often very hard to detect logical errors in a neural network. One easy way to test is if our neural network can overfit a small batch of training data, we can reasonably assume that it has no critical bug and start training. The reason is that a neural network can even fit a random labeling of training data , if it cannot fit a small batch, it's almost guaranteed that there's a bug in your network. Keep in mind that this is only one test to perform, there might be some other subtle bugs that prevent you from getting a good result.
