[site]: crossvalidated
[post_id]: 195010
[parent_id]: 
[tags]: 
How to estimate ratio of variances?

Suppose we have two time series generated by \begin{align} x_t &= \sigma_t\,e_t \\ y_t &= c\,\sigma_t\, f_t \end{align} where $e_t$ and $f_t$ are IIDs with mean zero and unit variance. We don't know much about $\sigma_t$ (although perhaps we can assume it's slowly changing). Given a series of observations of $x$ and $y$ I would like to estimate $c$. For example, if both $e$ and $f$ are normal, and we make no assumptions about $\sigma$ one can compute the angle corresponding to the point $(x_t,y_t/a)$. If $a=c$ then this angle should be uniformly distributed on $(-\pi,\pi]$. One can then use Kolmogorov-Smirnov to judge how good an estimate $a$ is. Here is an R implementation of this idea: set.seed(123) n In the normal case is the above approach optimal? What approaches exist if $e$ and $f$ are not normal? Finally, it seems that if $\sigma_t$ is slowly-varying there may be additional attacks on this (e.g., sampling consecutive $x$s and $y$s and estimating variances).
