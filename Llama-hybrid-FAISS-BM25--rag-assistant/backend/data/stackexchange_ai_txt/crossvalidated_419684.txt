[site]: crossvalidated
[post_id]: 419684
[parent_id]: 
[tags]: 
Why is the autoencoder decoder usually the reverse architecture as the encoder?

The majority of autoencoder architectures I've seen have a similar architecture, mainly that the decoder is just the reverse of the encoder. If the goal of the autoencoder is low-dimensional feature learning, why isn't the decoder simple? An example would be a linear transformation $FW$ where $F$ is an $n$ observation by $f$ feature matrix (i.e. the bottleneck) and $W$ is a learned weight matrix that maps $F$ to the original input feature size. In the case of a deep autoencoder with multiple hidden layers, the decoder in the above example would have low capacity compared to the encoder. My intuition is the following: If the decoder is simple, then the autoencoder is forced to learn higher quality features in the bottleneck to compensate. Conversely, if the decoder has high representational capacity, it can map a poorly learned bottleneck to the output reconstructions effectively. The reconstruction error might be lower in this case, but that doesn't necessarily mean the learned features are actually better. In my own application (feature learning on graphs), I have found that a simple decoder results in better learned features than a decoder that just mirrors the encoder. In this paper, the authors design a graph autoencoder with a very simple decoder as $\hat{A} = \sigma(ZZ^T)$ where $\hat{A}$ is the reconstructed graph adjacency matrix, $Z$ is the learned feature matrix and $\sigma$ is some non-linear transformation like a ReLU. I've been looking around for an answer to this question on-and-off for a while but I haven't found any explanations or theoretical results as to why a higher capacity decoder is preferable to a low capacity one (or vice-versa). If anyone can provide an explanation or point me in the right direction I'd be grateful.
