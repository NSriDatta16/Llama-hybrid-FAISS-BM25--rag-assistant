[site]: crossvalidated
[post_id]: 570605
[parent_id]: 
[tags]: 
Understanding the Jeffreys-Zellner-Siow (JZS) prior in Bayesian t-tests

I am currently working on a lecture on Bayesian hypothesis testing, following the paper by Rouder et al, 2009 . On Page 231 they present the formula for the relevant Bayes factor based on the Jeffreys-Zellner-Siow (JZS) prior in Equation 1. Here is the formula: $$B_{01} = \frac{ \left( 1 + \frac{t^2}{\nu}\right)^{-(\nu+1)/2} }{ \int_0^{\infty} (1 + N g)^{-1/2} \left(1+\frac{t^2}{(1 + N g) \nu} \right)^{-(\nu+1)/2} (2 \pi)^{-1/2} g^{-3/2} e^{-1/(2g)} \mathrm{d}g } $$ where $t$ is the usual t-statistic value, $N$ is the sample size, $\nu = N-1$ is the degrees of freedom, and $g$ is not defined in the paper as far as I can tell. The authors also note that To our knowledge, Equation 1 is novel. The derivation is straightforward and tedious and not particularly informative. However, I would like to understand at least the basic intuition behind how they arrived at this formula to present the derivation in a simplified form to my students. I do understand the basic idea, namely that the Bayes factor $B_{01}$ is the ratio of the following marginal likelihoods: $$M_0 = \int_0^{\infty} f_0(\mathbf{y} | \sigma^2) p_0(\sigma^2) \mathrm{d}\sigma^2$$ and $$M_1 = \int_{-\infty}^{\infty}\int_0^{\infty} f_1(\mathbf{y} | \mu, \sigma^2) p_1(\mu, \sigma^2) \mathrm{d}\sigma^2 \mathrm{d}\mu$$ (see Page 229 of the paper). My problem, probably due to my ignorance, is that it's unclear to me from the paper what the likelihoods $f_0$ , $f_1$ and the prior distributions $p_0$ , $p_1$ are in this particular case . Is the likelihood $f_0$ a Normal or a t-distribution density? Is $f_1$ a non-central t density? In the denominator ( $M_1$ ), what is the prior $p(\mu, \sigma^2)$ ? And what is $g$ in the integral in the denominator of $B_{01}$ ? For my purposes it would be completely sufficient if I could tell my students, "Look, $B_{01} = \frac{M_0}{M_1}$ , and then these marginal likelihoods are obtained by integrating the product of the likelihoods $f_0$ and $f_1$ and the priors $p_0$ and $p_1$ , respectively. The likelihoods are such-and-such distributions for this and that reason. The priors are such-and-such distributions for another set of reasons." I don't need the exact steps leading to Equation 1, but I am hoping that if I plug in the correct likelihood and prior distributions in the marginal likelihood integrals $M_0$ and $M_1$ then at least I could reproduce Equation 1 with Mathematica.
