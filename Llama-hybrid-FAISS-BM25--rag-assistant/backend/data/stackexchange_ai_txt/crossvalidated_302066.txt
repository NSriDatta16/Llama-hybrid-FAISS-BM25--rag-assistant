[site]: crossvalidated
[post_id]: 302066
[parent_id]: 
[tags]: 
Model training: difference between inference, marginalisation, and estimation

I am working through the lecture slides of Carl Rasmussen's Probablistic Machine Learning course. In slide 6 of the first lecture it lists a number of ways that one can learn the parameters (A, C) and latent variables (x) of a model, e.g.: They are: inference estimation sampling marginalisation I am struggling to find exact definitions of all of these. Inference I think means evaluating the posterior probability distributions of all the variables. While sampling I think means approximate inference with MCMC. Is this correct? Would estimation be methods like MAP and MLE? What does marginalisation mean here? I thought marginalisation is a part of inference.
