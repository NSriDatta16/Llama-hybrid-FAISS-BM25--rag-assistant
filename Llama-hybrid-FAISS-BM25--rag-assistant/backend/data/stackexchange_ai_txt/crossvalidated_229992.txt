[site]: crossvalidated
[post_id]: 229992
[parent_id]: 
[tags]: 
Does a Local Minima Exists at K=2 in the Gap Statistic for the G-Means Algorithm

Currently, I am attempting to use the G-Means algorithm to attempt to solve the ill-defined image segmentation problem as it is a very algorithmically cheap solution to the ill-defined high-dimensional clustering problem. The specifics about this particular implementation I'm using can be found at this blog post on The Data Science Lab . I'm finding that for if I use a small maximum K value (it's easy to end up with a ridiculous amount of clusters for image data), the algorithm tends to select K=2 as the optimal clustering, when from qualitative analysis of the image it would seem like there are 3-4 feature clusters. Looking at the math again, there seems to be an unwanted local minimum for the gap statistic at K=2, which would affect clustering if the maximum K is small enough. For the purpose of illustrating the algorithm to the reader, I will define the G-Means algorithm's gap statistic by the code I am using. The gap statistic function is defined as such: def get_alpha(k,ndim,previous_alpha): if k The G-Means algorithm iterates over K from K=1 to some hyper-parameter K=Max(K), calculating the gap statistic for each K except for K=1 (since the inertia of the previous K is undefined for K=1). It stores the alpha values from the gap_statistic calculation for the next iteration. The inertia is easily obtained from sci-kit learn 's implementation of the KMeans algorithm. It seems to me that making alpha an increasing function, along with the fact that the change in inertia from K=1 to K=2 will intuitively be incredibly large for reasonably clustered data, leads to an unwanted local minima in the gap statistic at K=2. Is this intuition wrong? Is there a proof for this minimum (based upon the definition of inertia), and what would be a way to implement the gap statistic for a low Maximum K and not have this local minimum affect the clustering results? Thank you!
