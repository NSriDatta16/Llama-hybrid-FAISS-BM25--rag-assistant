[site]: crossvalidated
[post_id]: 418336
[parent_id]: 418306
[tags]: 
The formal definition of a Markov Chain is a: discrete-time stochastic process satisfying the Markov property From my understanding this means that any finite graph could be considered a Markov Chain, with the vertices being the states and the edges representing the probability of transitioning from one state to another. The only assumption that you need to be aware of is that of the Markov property , which says that transitions to future states depend only in the present and not in past states. When traversing your graph (i.e. through a random walk), if your jumps from one node to the other depend solely on the edge weights (which usually is the case), then the assumption holds. However, if during your traversal you also consider the previous steps, then the Markov Chain won't give a proper representation of the graph.
