[site]: crossvalidated
[post_id]: 573225
[parent_id]: 573189
[tags]: 
Regression is a very flexible approach to hypothesis testing as it actually does lots more than compute p-values. Regression estimates parameters and, as a side effect, this allows us to test hypotheses about those parameters. Estimation is usually more helpful though. For example, a logistic regression will estimate time effects (as the log odds of the probability to click). So you will learn not only if the time effects are statistically different but also how much different and which time is most effective. However, if you are interested only in testing the null hypothesis of no difference in clicks at the three times, then you can use the chi squared test for independence. Summarise your data into a 2x3 contingency table of counts that has one row for "click" or "not click" and one column for "9:30", "12:00" and "19:00". The null hypothesis of independence means that the rows/columns have the same distribution as the row/column marginals. So in effect, no difference between the distribution of clicks/no clicks at each time point. Update after you provided a summary of your data. You don't need the -1 (no intercept) trick to estimate the time effects, either on the logit or on the probability scale. Fit regression with an intercept and the default treatment contrast. The first level, Time = 0930, is selected as the reference level. library("broom") library("tidyverse") dat However, we don't need to know the reference level. Or to understand contrasts really (though that always helps). We can easily get estimates for the time effects, on the logit or probability scale, using the emmeans package. emmeans(fit, ~Time) # logit scale #> Time emmean SE df asymp.LCL asymp.UCL #> 0930 -4.55 0.0321 Inf -4.61 -4.49 #> 1200 -4.46 0.0307 Inf -4.52 -4.40 #> 1930 -4.36 0.0293 Inf -4.42 -4.30 #> #> Results are given on the logit (not the response) scale. #> Confidence level used: 0.95 emmeans(fit, ~Time, type = "response") # probability scale #> Time prob SE df asymp.LCL asymp.UCL #> 0930 0.0105 0.000332 Inf 0.00983 0.0111 #> 1200 0.0115 0.000349 Inf 0.01082 0.0122 #> 1930 0.0126 0.000365 Inf 0.01194 0.0134 #> #> Confidence level used: 0.95 #> Intervals are back-transformed from the logit scale Finally, note that we did all this math just to get the confidence intervals. The estimated probabilities for each levels are just Clicks / Emails otherwise. dat %>% mutate(Clicks / Emails) #> # A tibble: 3 Ã— 4 #> Time Clicks Emails `Clicks/Emails` #> #> 1 0930 981 93799 0.0105 #> 2 1200 1073 93446 0.0115 #> 3 1930 1182 93542 0.0126
