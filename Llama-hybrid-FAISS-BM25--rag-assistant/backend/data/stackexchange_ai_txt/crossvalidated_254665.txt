[site]: crossvalidated
[post_id]: 254665
[parent_id]: 211492
[tags]: 
[Answer to your first question] Apparently these effects are not fully understood yet (Deep Learning, Goodfellow et al). It is hypothesised that pretraining will initialise the network in a region which would cause it to approach a different local minima which would otherwise be inaccessible (due to random initialisation). The set of weights learned with pretraining is speculated to have a regularising effect (i.e. improved test error rates). The second reason, applicable to unsupervised learning, is that features learned in modelling the input are "somehow" useful for a discriminative task like classification.
