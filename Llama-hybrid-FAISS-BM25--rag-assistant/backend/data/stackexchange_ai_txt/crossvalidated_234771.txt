[site]: crossvalidated
[post_id]: 234771
[parent_id]: 234646
[tags]: 
But I have some question. Is this approach have normal idea? Yes, though instead of a sliding window people tend to give the entire sentence, since recurrent neural networks can handle sentences of variable size. And how to pass words into input layer which is not in my vocabulary? typically they are mappped to some UNK (unknown) token, a.k.a. OOV (out of vocabulary), the embeddings of which is one of the initialized, then jointly learnt during the training phase. If you have some better idea for implementing this task please describe this. You could add character embeddings at the bottom of your network, as well as some sequence optimization leader at the top of network. Example: https://arxiv.org/abs/1606.03475
