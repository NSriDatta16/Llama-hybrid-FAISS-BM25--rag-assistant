[site]: crossvalidated
[post_id]: 55229
[parent_id]: 
[tags]: 
Why do Bayesian methods achieve sparsity via normal likelihood and Gamma hyperprior?

In the view of $\ell_1$ regularization, a natural way to incorporate sparsity into a Bayesian model is to add a Laplacian prior in my opinion. But instead of this, a usual fashion to handle sparsity in Bayesian model is the method proposed in the paper Sparse Bayesian Learning and the Relevance Vector Machine . What is the difference between these two in practice? Are there comparative studies of them? Thank you.
