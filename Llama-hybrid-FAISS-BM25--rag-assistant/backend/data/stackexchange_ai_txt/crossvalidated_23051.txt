[site]: crossvalidated
[post_id]: 23051
[parent_id]: 22852
[tags]: 
So $X_{n-1}$ is the target of inference. Then the correct Bayesian procedure is to calculate this posterior: $$p(X_{n-1}|Y_0\dots Y_{n-1}\mu_0\sigma_0N_0DI)$$ (Note I have added the symbol $I$ to explicitly indicate the assumptions and model structure being used). Now because we also know that the other $X_i$ are important, but not present in the above posterior, then they must have been integrated out (via the law of total probability): $$\int \dots \int p(X_{n-1}X_{n-2}\dots X_0|Y_0\dots Y_{n-1}\mu_0\sigma_0N_0DI)dX_{0}\dots dX_{n-2}$$ Now I will suppress the $\mu_0\sigma_0N_0DI$ from the probabilities for brevity, but they are still there hiding. Now the above is not directly calculable using the information given, but we can use the rules of probability theory to manipulate this into something we do know how to calculate. We can use bayes theorem to get $$p(X_{n-1}|Y_0\dots Y_{n-1})$$ $$=\frac{\int \dots \int p(X_{n-1}\dots X_0)p(Y_{n-1}\dots Y_0|X_{n-1}\dots X_0)dX_{0}\dots dX_{n-2}}{\int \dots \int p(X_{n-1}\dots X_0)p(Y_{n-1}\dots Y_0|X_{n-1}\dots X_0) dX_{0}\dots dX_{n-1}}$$ Because we have a ratio of integrals with the same integrand, we can eliminate constants which don't depend on $X_0,\dots,X_{n-1}$. This makes the equations slightly smaller and easier to write. Now we can use the "markovian" property of brownian motion to simplify the joint prior. $$p(X_{n-1}\dots X_0)=p(X_{n-1}|X_{n-2})p(X_{n-2}|X_{n-3})\dots p(X_1|X_0)p(X_0)$$ Now for brownian motion we have $p(X_i|X_{i-1})\sim N(X_{i-1},D)$ (remembering that conditional on $X_{i-1}$ means it is just a constant, not a random variable) and combined with your prior for $X_0$ we have: $$p(X_{n-1}\dots X_0)\propto\exp\left(-\frac{1}{2D}\sum_{i=1}^{n-1}(X_i-X_{i-1})^2-\frac{(X_0-\mu_0)^2}{2\sigma_0^2}\right)$$ Now for the likelihood I will make an assumption which I think you have implicitly made. This is that the $Y_i$ are conditionally independent, given $X_i$. I think this is what you mean by "noise is independent from observation to observation". This allows us to factor the likelihood as: $$p(Y_{n-1}\dots Y_0|X_{n-1}\dots X_0)=\prod_{i=0}^{n-1}p(Y_{i}|X_{i})\propto\exp\left(-\frac{1}{2N_0}\sum_{i=0}^{n-1}(Y_i-X_i)^2\right)$$ Hence the "integrand" is given by the product of the prior and the likelihood, stripped of constants which don't depend on $X_i$. This is given by: $$p(X_{n-1}\dots X_0)p(Y_{n-1}\dots Y_0|X_{n-1}\dots X_0)$$ $$\propto\exp\left(-\frac{1}{2N_0}\sum_{i=0}^{n-1}(Y_i-X_i)^2-\frac{1}{2D}\sum_{i=1}^{n-1}(X_i-X_{i-1})^2-\frac{1}{2\sigma_0^2}(X_0-\mu_0)^2\right)$$ $$\propto\exp\left(-\frac{X_0^2-2X_0\mu_0}{2\sigma_0^2}-\sum_{i=0}^{n-1}\frac{X_i^2-2X_iY_i}{2N_0}-\sum_{i=1}^{n-1}\frac{(X_i-X_{i-1})^2}{2D}\right)$$ Now we "simply" integrate this over $X_0,\dots,X_{n-2}$, noting that we only need to keep the terms which depend in $X_0,\dots,X_{n-1}$. This will be a straight-forward but tedious process if we were to directly integrate out from this equation. An easier way to obtain the integral is to evaluate the posterior for $n=2$ and $n=3$ cases which will establish a recursive relationship between the posterior $p(X_t|Y_0,\dots,Y_t)$ and $p(X_{t+1}|Y_0,\dots,Y_{t+1})$ $$\bf{}n=2\text{ case}$$ The integrand we require is given by: $$p(X_{1}X_0)p(Y_{1}Y_0|X_{1}X_0)$$ $$\propto\exp\left(-\frac{X_0^2-2X_0\mu_0}{2\sigma_0^2}-\frac{X_1^2+X_0^2-2X_1Y_1-2X_0Y_0}{2N_0}-\frac{(X_1-X_0)^2}{2D}\right)$$ $$=\exp\left(-a_0X_0^2+b_0X_0-\frac{X_1^2-2X_1Y_1}{2N_0}-\frac{X_1^2}{2D}\right)$$ Where $a_0=\frac{1}{2\sigma_0^2}+\frac{1}{2N_0}+\frac{1}{2D}=\frac{1}{2S}+\frac{1}{2D}$ and $b_0=\frac{\mu_0}{\sigma_0^2}+\frac{Y_0}{N_0}+\frac{X_1}{D}=\frac{M}{S}+\frac{X_1}{D}$. I have used $M$ and $S$ from your definition in the question. Now because $-a_0X_0^2+b_0X_0=-a_0\left(X_0-\frac{b_0}{2a_0}\right)^2+\frac{b_0^2}{4a_0}$ we can consider $\hat{X}_0=\frac{b_0}{2a_0}$ as an estimate for $X_0$ with variance equal to $v_0=\frac{1}{2a_0}$. Using these relations we have $\frac{b_0^2}{4a_0}=\frac{\hat{X}_0^2}{2v_0}$. Note that it has the same weighted average form as your $M$ for $n=1$ case. Using this we have a simple gaussian integral, which basically replaces $X_0$ by its estimate. To see this note that $-a_0\hat{X}_0^2+b_0\hat{X}_0=\frac{b_0^2}{4a_0}$, and we have: $$\int p(X_{1}X_0)p(Y_{1}Y_0|X_{1}X_0)dX_0\propto\sqrt{\frac{\pi}{a_0}}\exp\left(\frac{b_0^2}{4a_0}-\frac{X_1^2-2X_1Y_1}{2N_0}-\frac{X_1^2}{2D}\right)$$ $$\propto\exp\left(-a_1X_1^2+b_1X_1\right)$$ where $a_1=\frac{1}{2N_0}+\frac{1}{2(D+S)}$ and $b_1=\frac{M}{D+S}+\frac{Y_1}{N_0}$. As before, this gives rise to the estimate $M_1=\frac{b_1}{2a_1}$ with variance $S_1=\frac{1}{2a_1}$. Additionally the form for the above expression is a gaussian kernel (i.e. a gaussian pdf stripped of normalising constants). So the posterior for $n=2$ is given by: $$p(X_1|Y_0Y_1\mu_0\sigma_0N_0DI)=\frac{1}{\sqrt{2\pi S_1}}\exp\left(-\frac{1}{2S_1}\left[X_1-M_1\right]^2\right)$$ Where $M_1=\frac{MN_0+Y_1(D+S)}{N_0+D+S}$ is the estimate and $S_1=\frac{N_0(D+S)}{N_0+D+S}$. Notice here that this is the same form as for $n=1$ except with $M$ as the new "prior estimate" and $S+D$ as the new "prior variance". Note that $D$ in the prior variance accounts for the uncertainty in the inference path $X_0\to X_1$ and $S$ accounts for uncertainty in the inference path $\mu_0,Y_0\to X_0$. $$\bf{}n=3\text{ case}$$ The integration over $X_0$ is the same, however the integration over $X_1$ is different, because now $X_2$ can be used to help estimate $X_1$ - this is due to the additional term $\frac{(X_2-X_1)^2}{2D}$. So we now have (note the similar form to the $n=2$ case): $$\int p(X_2X_1X_0)p(Y_2Y_1Y_0|X_2X_1X_0)dX_0$$ $$\propto\exp\left(\frac{b_0^2}{4a_0}-\frac{X_1^2-2X_1Y_1}{2N_0}-\frac{X_1^2}{2D} -\frac{X_2^2-2X_2Y_2}{2N_0}-\frac{(X_2-X_1)^2}{2D}\right)$$ $$\propto\exp\left(-a_1^{'}X_1^2+b_1^{'}X_1-\frac{X_2^2-2X_2Y_2}{2N_0}-\frac{X_2^2}{2D}\right)$$ Where $a_1^{'}=\frac{1}{2S_1}+\frac{1}{2D}$ and $b_1^{'}=\frac{M_1}{S_1}+\frac{X_2}{D}$ We are now ready to integrate out $X_1$ and we get: $$\int p(X_2X_1X_0)p(Y_2Y_1Y_0|X_2X_1X_0)dX_0dX_1$$ $$\propto\exp\left(\frac{[b_1^{'}]^2}{4a_1^{'}}-\frac{X_2^2-2X_2Y_2}{2N_0}-\frac{X_2^2}{2D}\right)\propto\exp\left(-a_2X_2^2+b_2X_2\right)$$ Where $a_2=\frac{1}{2N_0}+\frac{1}{2(D+S_1)}$ and $b_2=\frac{M_1}{S_1+D}+\frac{Y_2}{N_0}$. As before these give a gaussian kernel: $$p(X_2|Y_0Y_1Y_2\mu_0\sigma_0N_0DI)=\frac{1}{\sqrt{2\pi S_2}}\exp\left(-\frac{1}{2S_2}\left[X_2-M_2\right]^2\right)$$ Where $M_2=\frac{M_1N_0+Y_2(D+S_1)}{N_0+D+S_1}$ is the estimate and $S_2=\frac{N_0(D+S_1)}{N_0+D+S_1}$. Notice here that this is the same form as for $n=2$ except with $M_1$ as the new "prior estimate" and $S_1+D$ as the new "prior variance". Note that $D$ in the prior variance accounts for the uncertainty in the inference path $X_1\to X_2$ and $S_1$ accounts for uncertainty in the inference path $\mu_0,Y_0,Y_1\to X_1$. $$\bf{}\text{General case}$$ this has an obvious general case via mathematical induction on the previous two cases. The result can be stated recursively. Set $M_0=\frac{Y_0\sigma_0^2+\mu_0N_0}{\sigma^2_0+N_0}$ and $S_0=\frac{\sigma^2_0N_0}{\sigma^2_0+N_0}$. Then, given the posterior mean and variance at point after observing $Y_0,\dots,Y_t$ are $M_t$ and $S_t$ respectively the posterior mean and variance given $Y_0,\dots,Y_{t+1}$ is given by: $$M_{t+1}=\frac{M_tN_0+Y_{t+1}(D+S_t)}{N_0+D+S_t}$$ $$S_{t+1}=\frac{N_0(D+S_t)}{N_0+D+S_t}$$ All posterior distributions are normal. Hence for the most recent part of the chain the posterior mean is given by: $$M_{n-1}=\frac{M_{n-2}N_0+Y_{n-1}(D+S_{n-2})}{N_0+D+S_{n-2}}$$ $$S_{n-1}=\frac{N_0(D+S_{n-2})}{N_0+D+S_{n-2}}$$ You would then recursively "build up" this estimate starting from $M_0$ and $S_0$
