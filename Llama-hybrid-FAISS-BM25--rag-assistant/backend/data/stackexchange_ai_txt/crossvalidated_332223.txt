[site]: crossvalidated
[post_id]: 332223
[parent_id]: 331749
[tags]: 
It sounds like you are randomly throwing statistical tests at the problem. Instead, I would recommend trying to understand what you are actually trying to test, what assumptions you could make and then figure out how to get the statistic you are after. You have only tried the classifiers on 2 datasets. Are you trying to estimate the probability that your classifier will perform better than the other classifiers on another dataset of the same size, randomly sampled from the same underlying population? Are you assuming that that the first 2 datasets were also of the same size and randomly sampled from the population? Do you also assume that your classifier scores on these datasets are normally distributed for each classifier? These could be unreasonable assumptions to make, depending on your application. But assuming these are reasonable assumptions, you could easily run a Monte-Carlo simulation of your problem to estimate the probability you are after. No statistics is better than bad statistics. Edit to place relevant comments into the answer: Since your datasets are not similar, it may actually be impossible to prove that your algorithm is better than the others in general because it may be false due to the "no free lunch" theorem - e.g. see https://en.wikipedia.org/wiki/No_free_lunch_theorem#Implications_for_the_scientific_method . What you may actually be after is proving that your algorithm is better on particular kinds of datasets, even if that range may be very broad. But that can be very difficult to define formally and thus difficult to prove, and you would rarely see such proofs in ML literature. What may be a good idea instead is to prove that your algorithm does better than others on a particular benchmark universe. If you have a sizeable dataset, you could do it by satisfying the assumptions I or jbowman listed by constructing your test datasets by sampling from this universe. It would then be easy to show that your algorithm does better on that universe. Ideally, such a benchmark universe would be widely used as a benchmark, so you would be able to compare against the results of the authors of the other algorithms. One common problem with such comparisons is that the author would carefully tune their own algorithm, but not spend as much effort on tuning the other algorithms, which would result in inflated metrics of the author's algorithm relative to the other algorithms. If that is the case here, you may also need to try to address that. Edit 2: This paper, which you might have already seen, appears to deal with the problem that you are having: Demšar, Janez. "Statistical comparisons of classifiers over multiple data sets." Journal of Machine learning research (2006) . But again, I would advise against blindly applying their methodology as you need to understand their assumptions and caveats of their research. For example, in the conclusion they state: We have observed the behaviour of the proposed statistics on several real-world classifiers and data sets. We varied the differences between the classifiers by biasing the selection of data sets, and measured the likelihood of rejection of the null-hypothesis and the replicability of the test. We have indeed found that the non-parametric tests are more likely to reject the null-hypothesis, which hints at the presence of outliers or violations of assumptions of the parametric tests and confirms our theoretical misgivings about them. The empirical analysis also shows that replicability of the tests might be a problem, thus the actual experiments should be conducted on as many data sets as possible. In the empirical study we provided no analysis of Type 1/Type 2 error rates. The main reason for this is that the correct result—rejection or non-rejection of the null-hypothesis—is not well defined and depends upon the kind of difference between the algorithms we intend to measure. Besides, conducting the experiments in which we knew the true hypotheses would require artificial data sets and classifiers with the prescribed probabilities and distributions of errors. For this we would need to make some assumptions about the real-world distributions; these assumptions are, however, exactly what we were testing in the first place. There is an alternative opinion among statisticians that significance tests should not be performed at all since they are often misused, either due to misinterpretation or by putting too much stress on their results (Cohen, 1994; Schmidt, 1996; Harlow and Mulaik, 1997). Our stance is that statistical tests provide certain reassurance about the validity and non-randomness of the published results. For that to be true, they should be performed correctly and the resulting conclusions should be drawn cautiously. On the other hand, statistical tests should not be the deciding factor for or against publishing the work. Other merits of the proposed algorithm that are beyond the grasp of statistical testing should also be considered and possibly even favoured over pure improvements in predictive power.
