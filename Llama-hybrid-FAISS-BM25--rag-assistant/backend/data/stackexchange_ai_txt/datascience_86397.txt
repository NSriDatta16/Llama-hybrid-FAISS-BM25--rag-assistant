[site]: datascience
[post_id]: 86397
[parent_id]: 86395
[tags]: 
For most cases, probably. For all cases, no. Especially if you are training on small data with very aggressive regularization in place, you may need a very long time until the desired performance level is achieved. For instance, for some popular text generation networks called Transformers trained on small datasets, it is necessary to use very aggressive regularization techniques and train during a very very large number of iterations (see this Twitter thread where they describe how to train a Transformer model on the PTB and Wikitext-103 datasets).
