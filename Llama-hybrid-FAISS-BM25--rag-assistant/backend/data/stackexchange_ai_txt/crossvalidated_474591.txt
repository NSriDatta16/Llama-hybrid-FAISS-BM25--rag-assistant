[site]: crossvalidated
[post_id]: 474591
[parent_id]: 
[tags]: 
Uniformly distributed VAE samples

I am currently working on a VAE to generate images (for simplicity MNIST). If I understand the theory correctly, the latent variables follow a gaussian normal distribution in the dimensions of the latent space. To generate images we take a random sample $z$ from a standard normal distribution and feed this to the decoder. Looking at images that are generated from VAEs (like in this tensorflow tutorial ) the distribution of the different generated classes does not match the distribution of classes of training images (in this example 7/16 images are instances of "9"): If I look at any of the various visualizations of the latent space ( e.g. here ) it seems like the projections center around 0, so sampling from a gaussian distribution will put more weight on centered projections right? Does this cause the described bias? For my use-case it is important to sample the trained classes uniformly distributed . I know about the Conditional VAE but I don't know the labels at the time of sampling. My questions are: As the distribution for the latent variables is parameterized by the mean and variance generated by the encoder why do we assume this follows a standard normal distribution and use this to generate samples? Also, why do the latent space visualization then not look more like 2-dimensional standard normal distributions? Is there a way to sample the latent variables for the different classes uniformly distributed (e.g. generate about the same times "1"s than "5"s)? Can the latent space be estimated or known so that a uniform distribution or a gaussian distribution with higher variance could be used to generate $z$ values for the image generation?
