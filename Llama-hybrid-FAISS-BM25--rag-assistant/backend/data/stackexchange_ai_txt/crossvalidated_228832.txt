[site]: crossvalidated
[post_id]: 228832
[parent_id]: 89484
[tags]: 
The standard errors of the model coefficients are the square roots of the diagonal entries of the covariance matrix. Consider the following: Design matrix: $\textbf{X = }\begin{bmatrix} 1 & x_{1,1} & \ldots & x_{1,p} \\ 1 & x_{2,1} & \ldots & x_{2,p} \\ \vdots & \vdots & \ddots & \vdots \\ 1 & x_{n,1} & \ldots & x_{n,p} \end{bmatrix}$ , where $x_{i,j}$ is the value of the $j$th predictor for the $i$th observations. (NOTE: This assumes a model with an intercept.) $\textbf{V = } \begin{bmatrix} \hat{\pi}_{1}(1 - \hat{\pi}_{1}) & 0 & \ldots & 0 \\ 0 & \hat{\pi}_{2}(1 - \hat{\pi}_{2}) & \ldots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \ldots & \hat{\pi}_{n}(1 - \hat{\pi}_{n}) \end{bmatrix}$ , where $\hat{\pi}_{i}$ represents the predicted probability of class membership for observation $i$. The covariance matrix can be written as: $\textbf{(X}^{T}\textbf{V}\textbf{X)}^{-1}$ This can be implemented with the following code: import numpy as np from sklearn import linear_model # Initiate logistic regression object logit = linear_model.LogisticRegression() # Fit model. Let X_train = matrix of predictors, y_train = matrix of variable. # NOTE: Do not include a column for the intercept when fitting the model. resLogit = logit.fit(X_train, y_train) # Calculate matrix of predicted class probabilities. # Check resLogit.classes_ to make sure that sklearn ordered your classes as expected predProbs = resLogit.predict_proba(X_train) # Design matrix -- add column of 1's at the beginning of your X_train matrix X_design = np.hstack([np.ones((X_train.shape[0], 1)), X_train]) # Initiate matrix of 0's, fill diagonal with each predicted observation's variance V = np.diagflat(np.product(predProbs, axis=1)) # Covariance matrix # Note that the @-operater does matrix multiplication in Python 3.5+, so if you're running # Python 3.5+, you can replace the covLogit-line below with the more readable: # covLogit = np.linalg.inv(X_design.T @ V @ X_design) covLogit = np.linalg.inv(np.dot(np.dot(X_design.T, V), X_design)) print("Covariance matrix: ", covLogit) # Standard errors print("Standard errors: ", np.sqrt(np.diag(covLogit))) # Wald statistic (coefficient / s.e.) ^ 2 logitParams = np.insert(resLogit.coef_, 0, resLogit.intercept_) print("Wald statistics: ", (logitParams / np.sqrt(np.diag(covLogit))) ** 2) All that being said, statsmodels will probably be a better package to use if you want access to a LOT of "out-the-box" diagnostics.
