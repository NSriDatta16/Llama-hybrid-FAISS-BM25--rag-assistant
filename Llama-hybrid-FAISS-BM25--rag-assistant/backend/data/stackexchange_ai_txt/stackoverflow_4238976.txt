[site]: stackoverflow
[post_id]: 4238976
[parent_id]: 
[tags]: 
Calculating the actual average value

I've got a relatively little (~100 values) set of integers: each of them represents how much time (in millisecond) a test I ran lasted. The trivial algorithm to calculate the average is to sum up all the n values and divide the result by n , but this doesn't take into account that some ridiculously high/low value must be wrong and should get discarded. What algorithms are available to estimate the actual average value?
