[site]: crossvalidated
[post_id]: 363333
[parent_id]: 363048
[tags]: 
An RNN and any of its more sophisticated versions has a hidden state. The hidden state at time $t$ is a function of the hidden state at time $t-1$ and the input at time $t$. This hidden state at time $0$ is typically initialized to $0$. The fundamental reason why RNNs are "unrolled" is because all previous inputs and hidden states are used in order to compute the gradients wrt the final output of the RNN. One issue with this is that the memory required to hold all these activations and gradients is linear in the length of the sequence. Therefore, very long sequences require prohibitive amounts of memory. In order to get around this issue, long sequences are typically truncated. For example, if you have 99 words in a phrase and want to predict the 100th, you may only use the last 9. The idea of stateful RNNs is that you can do better than simply initializing the hidden state at time $0$ to $0$. Indeed, if we are feeding in the 90th word at time $0$, then we should initialize the hidden state as computed by feeding in words $0$ through $89$ in an rolled fashion instead. So we're saying that we're only going to train with shorter sequences, but initialize the hidden state to the same value it would've been computed if we trained with longer sequences. Obviously, we can't backpropagate through the initial value of the hidden state, since deliberately truncating the backpropagation was the whole point. So your stateful RNN with length 1 would not properly learn relationships across times.
