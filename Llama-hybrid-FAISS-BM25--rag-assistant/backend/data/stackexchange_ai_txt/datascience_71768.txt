[site]: datascience
[post_id]: 71768
[parent_id]: 
[tags]: 
Combining convolution operations

Reading an article about 1x1 convolution , I found this: It should be noted that a two step convolution operation can always be combined into one , but in this case [GoogLeNet] and in most other deep learning networks, convolutions are followed by non-linear activation and hence convolutions are no longer linear operators and cannot be combined. What do they mean saying "two step convolution operation can always be combined into one"?
