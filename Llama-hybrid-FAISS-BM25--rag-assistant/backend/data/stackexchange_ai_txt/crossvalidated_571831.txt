[site]: crossvalidated
[post_id]: 571831
[parent_id]: 571798
[tags]: 
When there are that many unknowns, generally you'd say "I don't know the probability". For example, your local book-maker will not give you odds on this event with the tree, and your local insurance agent will not sell you insurance against it. In order to produce a proabability you could take one of at least two approaches: Get the person to look out of the window lots of times, at various angles, times of day, etc, record when they saw a tree and when they didn't, and come up with a frequency. Use this as a probability. You do not even necessarily need to know how many of the trials occured during the day, and how many at night. You don't know what factors affect whether a tree is seen or not. You just know that you've measured the event you're interested in. Consider all the variables, make lots of measurements, define a more precise model to describe what is going on, put probability distributions on each of the parameters of your model (such as "time of day", "angle of glance"), and derive a probability for the event from what your model tells you are the times/angles/etc that produce tree-sightings. Very loosely speaking, the former approach might be used by an office manager for a question like "what is the probability that someone in my office has COVID-19?", where you really can't do a lot of careful research and modelling, but perhaps you do have access to the self-reported results of various kinds of tests, or failing that to government estimates of COVID-19 prevalance in your population as a whole. The scond approach might be used by "scientists"[*] for a question like "what is the probability that a person with COVID-19, on walking into a crowded supermarket, will infect at least one other person?", which is the sort of thing a committed epidemiologist might try to tackle. Doesn't necessarily mean all epidemiologists would come up with the same answer, of course, since they might make decisions about what to ignore, what to include in the model, and how to model it, which means they get to a different number. You can't generally put a probability on, "my theory of physics/biology/shopping is completely wrong and therefore everything which follows from it is bunk", since you have neither a good model nor good observed frequency for that. It's best not to think in terms of "every conceivable event has a probability, and my task is to calculate it". Rather, the actual physical world has observed events, and any probabilistic model you make of the world generates probabilities, and any relation between the two is down to whether your model is any good or not. The reason we're 100% confident that a uniform selection from 1-4 has probability of 0.25 of giving the number 3, is that this is a mathematical theorem following immediately from the definition. We are sure of mathematical definitions. No real-world events are even described in the sentence whose truth we are sure of: it's just a straight application of the definition of "uniform discrete probability distribution". The fact that we're 100% sure of mathematics (which itself arguably is a matter of opinion, but you say you are and I believe you) doesn't help us put a number on how sure we are of optics, or the medical theory of hallucinations, or that those trees won't blow down in the night and therefore the probability of spotting them tomorrow is very different from what it is today. Statisticians working for insurance companies, however, might actually have quite good data on the nationwide incidence of trees blowing down in the night. The reason they could plausibly care is that if you have a tree near your house, they might want to have an opinion on whether they should instruct you to remove it, or at least charge you higher premiums on your buildings insurance than they charge for buildings far from trees. So, any particular factor is subject to study, but to produce a probability you always have to decide at some point to ignore everything you haven't studied. [*] The same ones from the dread-inducing journalistic expression, "scientists say that...", which with high probability means that all scientific theory and common-sense nuance will be omitted from the remainder of the article.
