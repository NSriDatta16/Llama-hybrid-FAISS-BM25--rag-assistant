[site]: crossvalidated
[post_id]: 282482
[parent_id]: 
[tags]: 
Classification model has bad results on new data

I am working on a classification problem where I try to predict for each individual to which of two classes (0 or 1) they belong. The data set I have shows about 99% of individuals are in class 0, and 1% in class 1. The model I use is obtained by logistic regression, with a cut-off probability determined by ROC analysis. It performs decently on labeled test data (around 75% true positive and 25% false positive rate). However, I also applied the obtained model on another unlabeled data set, and there many individuals get predicted to be in class 1. An inspection of the data shows that some risk factors such as age provide a higher risk in this set, which is the reason why the predicted probabilities are larger. Is there anything I can do to cut down on the number of individuals predicted to be in class 1 (since it is unlikely so many are actually at such high risk)? I have tried to use a model based on features that are similar between the training data and the test set I'm trying to predict, but of course cutting down on the predictors we lose predictive power. Any suggestions?
