[site]: datascience
[post_id]: 23717
[parent_id]: 20421
[tags]: 
I am sorry, but your description is lacking, you might want to follow this procedure when classifying a multi-label classifier. Step 1: Make assumptions Let's try and simplify the problem with the most basic multi-label classifier assumption: the labels are independent. The way to tackle this problem is to classify $\{l_1,\dots,l_k\}$ labels by training $k$ classifiers independently, where classifier $C_i(x)=1$ iff $l_i\in Labels(x)$. Step 2: Feature engineering We have no idea from your question what features are reasonable for your problem. But applying logistic regression on $x_1$ and $x_2$ would not yield good results if their relation to the label is not linear. You could add any number of "engineered feature" such as $x_1*x_2,\frac{x_1}{x_y},\sqrt{(x_1-c_1)^2+(x_2-c_2)^2},\dots$. And bring prior knowledge to the model, for example, if $x_1$ and $x_2$ are longitude and latitude, What country / region do they represent ? climate ? earth-quake rate, etc.... Step 3: Removing outliers and dont-cares The best way is to plot your model or apply clustering algorithms, and see what data points pull the model to undesired places/ Step 4: decide on an objective function This is a multi-label classification problem, so your input of "0.0006" accuracy doesn't tell me much. One way to measure multi-label performance, is the Jaccaard index $$J(y_{true}, y_{pred})=\frac{|y_{pred}\cap y_{true}|}{|y_{pred}\cup y_{true}|}$$ There are plenty of other ways, and you may want to weigh the labels differently. Step 5: Train the classifier Now you can train your logistic regression / NN model. There are several packages (e.g. MEKA ) that specialize on multi-label models.
