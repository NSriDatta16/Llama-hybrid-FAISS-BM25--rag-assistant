[site]: crossvalidated
[post_id]: 364353
[parent_id]: 283452
[tags]: 
Most neural-network based autoencoders I am familiar with do not make a hard decision on which input features will be kept and which will be thrown away. Think of a typical Fully Connected autoencoder. All of the inputs will be connected to all nodes of the hidden layer representation. If you wanted to try and define some sort of feature importance in the fully connected case, you could look at the sum of the absolute value of the weights going out of each feature node, but this relies on all of your input features having a comparable scale. If, for example, one feature is height of person in feet and another feature person's income in USD, these will likely have very different weights regardless of importance (and your neural network will likely not train well either). You could try to encourage disparities in weights through weight regularization. In this case, I'd use L1 before I'd try L2. See When will L1 regularization work better than L2 and vice versa? for more details. Defining feature importance will be much more difficult in the case of a convolutional neural network (or pretty much any novel, non-Fully Connected based architecture).
