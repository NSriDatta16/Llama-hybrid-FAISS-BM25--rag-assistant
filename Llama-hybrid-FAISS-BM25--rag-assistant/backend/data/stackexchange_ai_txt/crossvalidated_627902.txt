[site]: crossvalidated
[post_id]: 627902
[parent_id]: 627683
[tags]: 
Benchmark with models of different size A common practice for developing machine learning models is to not only look at the best model you have, but also measure and visualize the relationship of model performance (on some validation set) with respect to model size to observe any trends visible. If a slightly smaller models than the largest one show weaker performance, then that suggests that further increases are likely to fit data better; but if your performance has reached a plateau, this indicates that the data might just be fundamentally hard to model. The same applies to other aspects of model structure complexity - if simpler models reach similar results as complex models, that's an indication that what you get might be a fundamental limitation of data, but if more complex models perform better, then that's an indication that perhaps even the most complex model you tried is "too weak" to properly model the data and going even further could improve things. Benchmark against expert human performance Many tasks try to automate something which can also be performed by human judgement. Having a validation dataset processed by expert humans provides a benchmark of what is definitely possible to extract from that data (because you observed a "system" doing that), so a sufficiently powerful model should be able to perform at least as well. If the gap between model performance and human performance is large, that indicates that a more powerful model (or, potentially, more training data or more external world knowledge) might get better results, but if the gap is insignificant then that suggests that perhaps the problem fundamentally can't be modeled better. But, as you might notice from my wording, there generally is no certainty. There is an active (and important!) niche of research on trying to provide formal guarantees, but (as far as I know) it has not yet resulted in generally applicable techniques to determine these limits for practical problems before actually trying to solve the particular task.
