[site]: crossvalidated
[post_id]: 604929
[parent_id]: 
[tags]: 
Clarification Question from Berger's Statistical Decision Theory (Chapter 1, Exercise 12 a))

I have a CS background with intro stats courses, and currently, I am working through Berger's Statistical Decision Theory (the theoretical questions). I'd like to ask a clarification question: this is question 12 from Chapter 1: Consider a decision problem and decision rule $\delta$ for which $$\mathbb{E}[(L(\theta, \delta(X)) - R(\theta, \delta))^2] \le k Imagine a sequence of repetitions of the decision problem, where the parameters $\theta_i$ can change and the data $X^{(i)}$ (from the density $f(x|\theta_i)$ ) are independent. Show that, if $R(\theta, \delta) for all $\theta$ , then $$\lim_{N \rightarrow \infty} \frac{1}{N} \sum_{i=1}^{N}L(\theta_i, \delta(X^{(i)})) \le \bar{R}$$ with probability one for any sequence $(\theta_1, \theta_2, \ldots).$ Now, the book defines $$R(\theta, \delta) = \mathbb{E}_{\theta}^X[L(\theta, \delta(X))]$$ , which is the expectation over $X$ for a specific value $\theta$ . So I see that this makes the first expression mean that variance of $X$ for a specific value $\theta$ is finite and less than equal to some value $k$ . It's also clear to me that I will need to use the Law of Large Numbers. But I am confused by $\bar{R}$ . It is not defined in the book, but obviously, the bar generally means the 'average' (or mean, or expected value). So my question is what would be that average over? As $R(\theta,\delta)$ is already defined as the expectation over X, my only guess is that it could be the expectation over the $\theta$ 's, but that's precisely the expression on the left hand side of the second expression.
