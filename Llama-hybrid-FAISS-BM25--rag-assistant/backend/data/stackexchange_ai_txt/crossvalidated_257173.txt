[site]: crossvalidated
[post_id]: 257173
[parent_id]: 173060
[tags]: 
People seem to fixate on a big qualifier in Big Data. However, the size is only one of the components of this term (domain). It's not enough that your data set was big to call your problem (domain) a big data, you also need it be difficult to understand and analyze and even process. Some call this feature unstructured , but it's not just the structure it's also unclear relationship between different pieces and elements of data. Consider the data sets that high energy physicists are working in places such as CERN . They've been working with petabytes size data for years before the Big Data term was coined. Yet even now they don't call this big data as far as I know. Why? Because the data is rather regular, they know what to do with it. They may not be able to explain every observation yet, so they work on new models etc. Now we call Big Data the problems that deal with data sets that have sizes that could be generated in a few seconds from LHC in CERN. The reason is that these data sets are usually of data elements coming from multitude of sources with different formats, unclear relationships between the data and uncertain value to the business. It could be just 1TB but it's so difficult to process all the audio, vidio, texts, speech etc. So, in terms of complexity and resources required this trumps the petabytes of CERN's data. We don't even know if there's discernible useful information in our data sets. Hence, Big Data problem solving involves parsing, extracting data elements of unknown value, then linking them to each other. "Parsing" an image can be a big a problem on its own. Say, you're looking for CCTV footage from the streets of the city trying to see whether people are getting angrier and whether it impacts the road accidents involving pedestrians. There's a ton of video, you find the faces, try to gauge their moods by expressions, then link this to the number of accidents data sets, police reports etc., all while controlling for weather (precitipotation, temperature) and traffic congestions... You need the storage and analytical tools that support these large data sets of different kinds, and can efficiently link the data to each other. Big Data is a complex analysis problem where the complexity stems from both the sheer size and the complexity of structure and information encoding in it.
