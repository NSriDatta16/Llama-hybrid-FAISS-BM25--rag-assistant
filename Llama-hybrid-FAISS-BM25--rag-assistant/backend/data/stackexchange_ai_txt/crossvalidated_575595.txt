[site]: crossvalidated
[post_id]: 575595
[parent_id]: 
[tags]: 
How to force splits in decision tree to be distributed uniformelly in case of no dependency on feature?

I have targets ordered by a feature. I want to find a single split that minimizes a squared deviation (RMSE). For example, I have 100 values (targets) and it might be the case that, if I split them as 30 + 70, I get minimal squared deviation (as compared with all the other splits like 10+90 or 83+17). Here I assume that 30 targets "on the left" get one prediction (average of those 30 targets) and 70 targets on the right get another prediction (average over 70 values). To test my split function, I have generated 100 values using a single normal distribution and for those values I found an optimal split given by its location (for example location 15 means 15 values on the left and 85 values on the right). When I do it many times I find that split locations are distributed non uniformly. For example very small locations (3, 4, 5) or very large locations (97, 98) are much more frequent than locations close to the middle (48, 50, 52). In other words, good RMSEs are more likely to be found for the splits close to the two edges of the whole interval. So, it looks to me as an inherent undesirable bias of the method. In the case of no pattern in the data (no dependency on the feature, as in my test), I would expect splits to be distributed uniformly between 1 and 100. Otherwise, when I do have a dependency on the feature, mixed with large noise, I still would get close to the edges of the intervals and I do not want it. By the way, my explanation of the effect is that root means squares deviations corresponding to the splits close to the edges are less correlated with each other and therefore the chances that we find minimal root mean square there is larger.
