[site]: crossvalidated
[post_id]: 614855
[parent_id]: 614851
[tags]: 
You can assign a integer number for every category (plus to be safe a category for "other" = anything new, where you maybe want to group rare categories). There's already a lot of other answers out there on how one could also represent categorical features in terms of dimensionality reduction and ideas like target encoding/random effects/embeddings/ . I'd especially consider ideas like target encoding and training a neural network with an embedding layer and then taking the embeddings for the categorical features as a feature for LightGBM. If you have a cold-start problem (i.e. some categories will initially have no data), then Bayesian target encoding can be helpful, e.g. if you are predicting a logit-proportion, then having e.g. a Beta(0.5, 0.5) prior for each category and providing LightGBM not just with the mean or median of the distribution, but also inter-quartile range, or the 90th and 10th percentile can tell the model about the uncertainty about the new category (we e.g. used that idea for predicting drug approvals , where there's constantly new drug classes etc.). I can only, again, recommend to look at what people tend to do in Kaggle competitions, where LightGBM is widely used and high-cardinality categorical data (e.g. users, products, shops, locations is common). Besides browsing the forums for solutions to competitions, there's Kaggle competition GM Thakur's book , the Kaggle book , the book on the fast.ai course and an excellent How to Win a Data Science Competition: Learn from Top Kagglers course on coursera.org (as of May 2023 inaccessible, if you have not already enrolled, due to the association of the course with Moscow university).
