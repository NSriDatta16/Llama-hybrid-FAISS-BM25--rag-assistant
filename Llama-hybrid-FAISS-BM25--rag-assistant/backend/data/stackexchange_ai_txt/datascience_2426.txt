[site]: datascience
[post_id]: 2426
[parent_id]: 
[tags]: 
Validity of data

I have a data set that is pivoted in to the following format: [key] [id] [0] [1] [5] [10] [15] [60] [120] [180],.. [365] So key could be [Products] [1000] [15,000] [4000]... etc Where products is the category of item being reviewed and key is the identifier for the product; the only fields (0, 1,... 180,.. [365]) are individual daily samples identify how many of "x" product were logged as either sold, in-stock etc. What I need to do is perform some kind of analysis on an entire slew of products and their inventory levels. i.e. each import of data I need to make sure the incoming data is accurate or predictably accurate and that some human did not typo a stock level. The problem is, using a simple average or rolling average can introduce significant variance and smoothing out the average renders my analysis less reliable. Ideally this analysis would trigger an alarm that someone would have to investigate. Is there a better and more accurate way of performing this analysis? Thanks!
