[site]: crossvalidated
[post_id]: 282913
[parent_id]: 
[tags]: 
LFW face pair-matching performance evaluation, why retrain model on view2?

I am trying to understand how performance evaluation works in LFW(Labeled Faces in the Wild) dataset http://vis-www.cs.umass.edu/lfw/ . I am interested in task: pair-matching. However, as I dig deeper, I found myself confused. Here is a brief summary on evaluating pair-matching performance in LFW dataset: LFW dataset is divided into View1 and View2. View1 is for development of algorithms, you can use it to select model, tune parameters and choose features. View2 is for reporting accuracy of your model produced by View1. View1 description: For development purposes, we recommend using the below training/testing split, which was generated randomly and independently of the splits for 10-fold cross validation, to avoid unfairly overfitting to the sets above during development. For instance, these sets may be viewed as a model selection set and a validation set. See the tech report below for more details. pairsDevTrain.txt , pairsDevTest.txt View2 description: As a benchmark for comparison, we suggest reporting performance as 10-fold cross validation using splits we have randomly generated. I also found an example of carrying out the experiment with PCA for face pair-matching in the LFW 2008 paper . Eigenfaces for pair matching . We computed eigenvectors from the training set of View 1 and determined the threshold value for classifying pairs as matched or mismatched that gave the best performance on the test set of View 1. For each run of View 2, the training set was used to compute the eigenvectors, and pairs were classified using the threshold on Euclidian distance from View 1. State of the art pair matching . To determine the current best performance on pair matching, we ran an implementation of the current state of the art recognition system of Nowak and Jurie [14].11 The Nowak algorithm gives a similarity score to each pair, and View 1 was used to determine the threshold value for classifying pairs as matched or mismatched. For each of the 10 folds of View 2 of the database, we trained on 9 of the sets and computed similarity measures for the held out test set, and classified pairs using the threshold My questions are: How to do training with View1 data using 10-fold cross validation? The data is already split into pairsDevTrain.txt and pairsDevTest.txt. Does it mean that I need to merge these two file and then do a standard 10-fold cross validation to train my model? Why is 10-fold cross validation required in View2? Since model and parameter is all determined using data in View1, why not just use all View2 data to report performance. Since 10-fold cross validation is required in View2, there must be a training process. Why retrain another model? It is worth mentioning here, both in View1 and View2. train and test data don't share common identity, i.e. person1 appear in train, will not appear in test. 10-fold cross validation is recommended for both View1 and View2. 10-fold splits are given for View2 but not View1. Is there a reason why? Thank you beforehand for helping me understand the performance evaluation for LFW.
