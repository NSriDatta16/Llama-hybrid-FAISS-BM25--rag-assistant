[site]: crossvalidated
[post_id]: 603044
[parent_id]: 602560
[tags]: 
Yes, absolutely! XGBoost or any other gradient boosting machine implementation (LightGBM, CatBooost, etc.) would work fine. Approximating, a purely linear function by recursively partitioning our sample space using tree-based learners can require training a large number of base learners (i.e. doing a lot of "iterations") but aside from that it is nothing "too exotic". seanv507's suggestion with LightGBM 's linear treee base learner will likely require fewer iterations (i.e. using a smaller number of base learners) as we will have a model at each leaf that is linear instead of constant (i.e. it should capture that linear dependency more naturally/efficiently). Finally, to state the obvious, maybe using a different learning methodology (e.g. Elastic Net) might be more suitable if have known linear patterns dominating our underlaying data-generating mechanism.
