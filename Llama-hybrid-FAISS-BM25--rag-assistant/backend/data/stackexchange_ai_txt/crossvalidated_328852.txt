[site]: crossvalidated
[post_id]: 328852
[parent_id]: 328630
[tags]: 
So I decided to run nested cross-validation using the specialized mlr package in R to see what's actually coming from the modelling approach. Code (it takes a few minutes to run on an ordinary notebook) library(mlr) daf = read.csv("https://pastebin.com/raw/p1cCCYBR", sep = " ", header = FALSE) tsk = list( tsk1110 = makeRegrTask(id = "tsk1110", data = daf, target = colnames(daf)[1]), tsk500 = makeRegrTask(id = "tsk500", data = daf[, c(1,sample(ncol(daf)-1, 500)+1)], target = colnames(daf)[1]), tsk100 = makeRegrTask(id = "tsk100", data = daf[, c(1,sample(ncol(daf)-1, 100)+1)], target = colnames(daf)[1]), tsk50 = makeRegrTask(id = "tsk50", data = daf[, c(1,sample(ncol(daf)-1, 50)+1)], target = colnames(daf)[1]), tsk10 = makeRegrTask(id = "tsk10", data = daf[, c(1,sample(ncol(daf)-1, 10)+1)], target = colnames(daf)[1]) ) rdesc = makeResampleDesc("CV", iters = 10) msrs = list(mse, rsq) configureMlr(on.par.without.desc = "quiet") bm3 = benchmark(learners = list( makeLearner("regr.cvglmnet", alpha = 0, lambda = c(0, exp(seq(-10, 10, length.out = 150))), makeLearner("regr.glmnet", alpha = 0, lambda = c(0, exp(seq(-10, 10, length.out = 150))), s = 151) ), tasks = tsk, resamplings = rdesc, measures = msrs) Results getBMRAggrPerformances(bm3, as.df = TRUE) # task.id learner.id mse.test.mean rsq.test.mean #1 tsk10 regr.cvglmnet 1.0308055 -0.224534550 #2 tsk10 regr.glmnet 1.3685799 -0.669473387 #3 tsk100 regr.cvglmnet 0.7996823 0.031731316 #4 tsk100 regr.glmnet 1.3092522 -0.656879104 #5 tsk1110 regr.cvglmnet 0.8236786 0.009315037 #6 tsk1110 regr.glmnet 0.6866745 0.117540454 #7 tsk50 regr.cvglmnet 1.0348319 -0.188568886 #8 tsk50 regr.glmnet 2.5468091 -2.423461744 #9 tsk500 regr.cvglmnet 0.7210185 0.173851634 #10 tsk500 regr.glmnet 0.6171841 0.296530437 They do basically the same across tasks. So, what about the optimal lambdas? sapply(lapply(getBMRModels(bm3, task.ids = "tsk1110")[[1]][[1]], "[[", 2), "[[", "lambda.min") # [1] 4.539993e-05 4.539993e-05 2.442908e-01 1.398738e+00 4.539993e-05 # [6] 0.000000e+00 4.539993e-05 3.195187e-01 2.793841e-01 4.539993e-05 Notice the lambdas are already transformed. Some fold even picked the minimal lambda $\lambda = 0$. I fiddled a bit more with glmnet and discovered neither there the minimal lambda is picked. Check: EDIT: After comments by amoeba, it became clear the regularization path is an important step in the glmnet estimation, so the code now reflects it. This way, most discrepancies vanished. cvfit = cv.glmnet(x = x, y = y, alpha = 0, lambda = exp(seq(-10, 10, length.out = 150))) plot(cvfit) Conclusion So, basically, $\lambda>0$ really improves the fit ( edit: but not by much! ). How is it possible and what does it say about my dataset? Am I missing something obvious or is it indeed counter-intuitive? We are likely nearer the true distribution of the data setting $\lambda$ to a small value larger than zero. There's nothing counter-intuitive about it though. Edit: Keep in mind, though, the ridge regularization path makes use of previous parameter estimates when we call glmnet , but this is beyond my expertise. If we set a really low lambda in isolation, it'll likely degrade performance. EDIT: The lambda selection does say something more about your data. As larger lambdas decrease performance, it means there are preferential, i.e. larger, coefficients in your model, as large lambdas shrink all coefficients towards zero. Though $\lambda\neq0$ means that the effective degrees of freedom in your model is smaller than the apparent degrees of freedom, $p$. How can there be any qualitative difference between p=100 and p=1000 given that both are larger than n? $p=1000$ invariably contains at least the same of information or even more than $p=100$. Comments It seems you are getting a tiny minimum for some non-zero lambda (I am looking at your figure), but the curve is still really really flat to the left of it. So my main question remains as to why λ→0 does not noticeably overfit. I don't see an answer here yet. Do you expect this to be a general phenomenon? I.e. for any data with n≪p, lambda=0 will perform [almost] as good as optimal lambda? Or is it something special about these data? If you look above in the comments, you'll see that many people did not even believe me that it's possible. I think you're conflating validation performance with test performance, and such comparison is not warranted. Edit: notice though when we set lambda to 0 after running the whole regularization path performance doesn't degrade as such, therefore the regularization path is key to understand what's going on! Also, I don't quite understand your last line. Look at the cv.glmnet output for p=100. It will have very different shape. So what affects this shape (asymptote on the left vs. no asymptote) when p=100 or p=1000? Let's compare the regularization paths for both: fit1000 = glmnet(x, y, alpha = 0, lambda = exp(seq(-10,10, length.out = 1001))) fit100 = glmnet(x[, sample(1000, 100)], y, alpha = 0, lambda = exp(seq(-10,10, length.out = 1001))) plot(fit1000, "lambda") x11() plot(fit100, "lambda") It becomes clear $p=1000$ affords larger coefficients at increasing $\lambda$, even though it has smaller coefficients for asymptotically-OLS ridge, at the left of both plots. So, basically, $p=100$ overfits at the left of the graph, and that probably explains the difference in behavior between them. It's harder for $p=1000$ to overfit because, even though Ridge shrinks coefficients to zero, they are never reach zero. This mean that the predictive power of the model is shared between many more components, making it easier to predict around the mean instead of being carried away by noise.
