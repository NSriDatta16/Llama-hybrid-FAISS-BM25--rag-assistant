[site]: crossvalidated
[post_id]: 411649
[parent_id]: 396918
[tags]: 
Recently there were two papers commenting on the self-attention heads: Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned from University of Edinburgh Are Sixteen Heads Really Better than One? from CMU Both of them come to the same conclusion that most of the heads are just noise and can be relatively easily pruned out. The Edinburgh guys also spotted some regularities in what the heads do: they mostly attend to adjacent or more distant neighboring words (after all the Transformer is not explicitly aware of the mutual position of the words and need to learn it), some tend to capture some syntactic relations (verb to adverbial modifier, subject to verb, etc.), some to tend to attend to rare words.. There is also a paper attempting to extract constituency syntax from self-attentions.
