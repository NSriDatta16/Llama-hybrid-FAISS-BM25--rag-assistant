[site]: crossvalidated
[post_id]: 410191
[parent_id]: 
[tags]: 
Reinforcement Learning in Real Life/Practical Terms

In every day life, it seems that we all have various habits and actions that we perform. For example, we wake up and check our email/facebook etc. on our phones. We don't look at are current state right now, and consider the values of all the possible future trajectories. We basically choose the action that maximizes our "reward" at our current state. Question. Is it practical to randomly initialize actions $a \in A$ , states $s \in S$ and a policy $\pi(a|s)$ and update this according to some algorithm (e.g. REINFORCE, exploration, etc.) to achieve some desired goal in your life? This could be done, for example, by uniformly sampling a random number in the interval $(0,1)$ and acting according to a policy $\pi(a|s)$ . For example, suppose your goal is to get married, get a new house etc. What would be appropriate reward functions in this case? Reward is usually defined as the immediate reward plus the discounted cumulative reward. But is this the right definition for practical problems like marriage/dating, buying a house, etc.? Would you define reward as $R_{\text{total}} = R_{\text{marriage}}+R_{\text{buying house}}$ in our example, where each of those individual rewards are typical immediate plus discounted rewards and try to maximize that? Or would it be better to maximize $R_{\text{marriage}}$ and $R_{\text{buying house}}$ individually?
