[site]: datascience
[post_id]: 120195
[parent_id]: 75599
[tags]: 
Very var in general - they capture different things The crucial part of causal diagrams is identifying a graph encoding predictive relationships between variables that agree with their conditional independence statements. See here for an introduction to causal graphs. By contrast, a standard machine learning algorithm will simply use all other variables to best predict a target variable with no regard for conditional independencies. It may be the case that the feature importance would carry some information about causal dependencies between features and target (for example if variables that are not direct causes are not helpful for predicting the target). However, this need not be the case and there is no way of knowing in general. As for any relationships among features, the feature importance map is not a suitable indicator for causality or anything else really - it only measures predictive importance of the features with respect to the target. Illustrative examples 1. Assume that all features are direct or indirect causes of the target (e.g. genetic predispositions for a disease) The machine learning model may correctly identify that indirect causes are not needed for prediction, discard them, and assign importance to direct causes in proportion to their causal influence. In this case, the feature importance could be a good proxy for the causal influence of the features on the target. 2. Assume that all features are effects of the target (e.g. symptoms of a disease) The machine learning model may predict well, but any relationship it would find would be in the opposite direction from the actual causal direction. In this case, the feature importance would give a very wrong idea about the causal relationships between features and target.
