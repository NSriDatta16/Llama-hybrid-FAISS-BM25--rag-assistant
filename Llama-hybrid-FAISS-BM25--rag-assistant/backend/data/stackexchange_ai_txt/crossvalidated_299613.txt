[site]: crossvalidated
[post_id]: 299613
[parent_id]: 299593
[tags]: 
It's true that KNN, when run on bogus data, can appear to find patterns which don't exist. However, you could say the same for soft-margin SVM, or logistic regression, or nearly any classification algorithm. Therefore it's necessary to perform cross-validation on classification models to ensure that the model is not overfitting to random noise. About your example with $k = 5$. I think the better way to look a it, is instead of the classification of your datapoint being determined with just a single vote, it's now determined by 5 votes, which on average, may reflect reality better than just one vote (this depends heavily on the dataset though). Suppose 999 students elected a class president, and the final vote came down to 500 vs 499. Sure, it all came down to the last person to vote, but that doesn't mean the vote couldn't have just been replaced by one random student voting.
