[site]: datascience
[post_id]: 93488
[parent_id]: 70229
[tags]: 
According to the paper 'Global Optimality in Neural Network Training' [Haeffele, B.D. and Vidal, R., 2017] ( https://openaccess.thecvf.com/content_cvpr_2017/papers/Haeffele_Global_Optimality_in_CVPR_2017_paper.pdf ) a global optimum for a neural network with 1 hidden layer is guaranteed but only under certain conditions. The layer must be big enough and the activation/regularization functions must be of a certain type. For example, ReLU activations satisfy the conditions but sigmoid activations do not.
