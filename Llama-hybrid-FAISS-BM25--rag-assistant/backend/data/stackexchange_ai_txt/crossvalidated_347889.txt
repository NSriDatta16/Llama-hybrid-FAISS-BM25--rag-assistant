[site]: crossvalidated
[post_id]: 347889
[parent_id]: 201452
[tags]: 
I will focus on the part: I don't understand how this would affect the backprop calculation. First of all you've probably already noticed that the only difference between the resulting loss values is that the average loss is scaled down with respect to the sum by the factor of $\frac{1}{B}$ , i.e. that $L_{SUM} = B \cdot L_{AVG}$ , where $B$ is the batch size. We can easily prove that the same relation is true for a derivative of any variable wrt. the loss functions ( $\frac{d L_{SUM}}{{dx}} = B \frac{d L_{AVG}}{{dx}}$ ) by looking at the definition of derivative: $$ \frac{dL}{{dx}} = \mathop {\lim }\limits_{\Delta \to 0} \frac{{L\left( {x + \Delta } \right) - L\left( x \right)}}{\Delta } $$ Now, we would like to multiply the value of the function and see how it affects the derivative: $$ \frac{d (c \cdot L)}{{dx}} = \mathop {\lim }\limits_{\Delta \to 0} \frac{{c \cdot L\left( {x + \Delta } \right) - c \cdot L\left( x \right)}}{\Delta } $$ When we factor out the constant and move it before the limit we should see that we come up with the original derivative definition multiplied by a constant, which is exactly what we wanted to prove: $$ \frac{d (c \cdot L)}{{dx}} = c \cdot \mathop {\lim }\limits_{\Delta \to 0} \frac{{L\left( {x + \Delta } \right) - L\left( x \right)}}{\Delta } = c \cdot \frac{d L}{{dx}} $$ In SGD we would update the weights using their gradient multiplied by the learning rate $\lambda$ and we can clearly see that we can choose this parameter in such way that the final weights updates would equal. The first update rule: $$ W := W + \lambda_1 \frac{dL_{SUM}}{dW} $$ and the second update rule (imagine that $\lambda_1 = \frac{\lambda_2}{B}$ ): $$ W := W + \lambda_1 \frac{dL_{AVG}}{dW} = W + \frac{\lambda_2}{B} \frac{dL_{SUM}}{dW} $$ The excellent finding by dontloo may suggest that using the sum might be a little bit more appropriate approach. To justify the average which seems to be more popular I'd add that using the sum might probably cause some problems with weight regularization. Tuning the scaling factor for the regularizers for different batch sizes may be just as annoying as tuning the learning rate.
