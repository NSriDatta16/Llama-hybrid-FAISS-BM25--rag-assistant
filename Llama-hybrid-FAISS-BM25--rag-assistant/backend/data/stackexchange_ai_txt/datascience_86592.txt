[site]: datascience
[post_id]: 86592
[parent_id]: 86584
[tags]: 
First, it is impossible to say without further information about the nature of your data, the training conducted, etc. That being said, in general there is no guarantee that a more complex would outperform a simpler model in time series forecasting. In fact, this was a controversy in the earlier M-forecasting competitions, where simpler Exponential Smoothing methods outperformed the more complex ARIMA and Neural Networks (in recent years though machine learning methods clearly reign supreme). In all cases, you should compare performance against very simple benchmarks such as the $naive/ persistence$ method. Regarding the evaluation: $R^2$ : This is used as an in-sample performance measure. I would argue that if more complex models such Gradient Boosting Machines achieve lower in-sample $R^2$ than a simpler linear model, this could indicate underfitting and you should increase the capacity of your model (increase number of trees, tune learning rate, etc.) $MAE, RMSE, MAPE$ : These are all used as out-of-sample performance metrics. If the more complex models outperform the simple linear model in-sample, but fail to generalize in a hold-out set, this could mean that your model is overfitting and you should include some form of regularization such as weight penalty or early stopping.
