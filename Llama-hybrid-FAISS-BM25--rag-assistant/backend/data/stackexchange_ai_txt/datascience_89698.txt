[site]: datascience
[post_id]: 89698
[parent_id]: 89696
[tags]: 
About the question whether to scale only a subset of features, I would tell you to do it over all the features (at least the continuous numeric ones) since the goal of data-scaling is to put these data on the same "reference scale" to be fairly compared. Nevertheless, having mixed data types (continuous numerical, categorical...) for your classification problem looks more appropriate for scale-invariant algorithms like the ones based on decision trees . More precisely, you can have a look at XGBoost , where the author explains in this link that you do not actually have to re-scale your data. Actually, in a recent real use case at my company, we tried re-scaling data VS not re-scaling it applying XGB, and we had better results with the second option.
