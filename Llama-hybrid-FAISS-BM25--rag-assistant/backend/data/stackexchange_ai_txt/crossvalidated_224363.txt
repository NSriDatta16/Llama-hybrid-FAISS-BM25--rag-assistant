[site]: crossvalidated
[post_id]: 224363
[parent_id]: 224005
[tags]: 
With respect to optimization (because you tagged your question with the optimization tag), SPD matrices are extremely important for one simple reason - an SPD Hessian guarantees that the search direction is a descent direction. Consider the derivation of Newton's method for unconstrained optimization. First, we form the Taylor expansion of $f(x + \Delta x)$: $$f(x + \Delta x)\approx f(x) + \Delta x^T \nabla f(x)+ \frac{1}{2} \Delta x^T \nabla^2 f(x) \Delta x$$ Next, we take the derivative with respect to $\Delta x$: $$f'(x + \Delta x)\approx \nabla f(x) + \nabla^2 f(x) \Delta x$$ Finally, set the derivative equal to 0 and solve for $\Delta x$: $$\Delta x = -\nabla^2 f(x)^{-1} \nabla f(x)$$ Assuming $\nabla^2 f(x)$ is SPD, it is easy to see that $\Delta x$ is a descent direction because: $$\nabla f(x)^T \Delta x = -\nabla f(x)^T \nabla^2 f(x)^{-1} \nabla f(x) When using Newton's method, non-SPD Hessian matrices are typically "nudged" to be SPD. There's a neat algorithm called modified Cholesky that will detect a non-SPD Hessian, "nudge" it appropriately in the right direction and factorize the result, all for (essentially) the same cost as a Cholesky factorization. Quasi-Newton methods avoid this problem by forcing the approximate Hessian to be SPD. As an aside, symmetric indefinite systems are receiving a lot of attention these days. They come up in the context of interior point methods for constrained optimization.
