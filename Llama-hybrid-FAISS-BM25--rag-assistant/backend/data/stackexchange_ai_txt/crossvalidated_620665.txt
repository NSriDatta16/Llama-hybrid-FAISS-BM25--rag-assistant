[site]: crossvalidated
[post_id]: 620665
[parent_id]: 
[tags]: 
MSE for ARIMA and UR in linear models

I'm trying to assess the predictive power of some time series models (LASSO, ARIMA, UR) on time series data, but I have a problem. I am conducting simulations with $N$ data points, $P$ potential predictors ( $P>=10$ ), where the true variables are $x_1, x_5, x_9, x_{10}$ . I am performing cross-validation using the sliding window technique, with a window size of $W$ and a horizon $H$ . The model for which I am generating the data is given by $$y_t=a_1+a_2x_1+a_3x_5+a_4x_9+a_5x_{10}+u_t,$$ where $u_t$ is a normally distributed random variable with mean 0 and variance 1. Now, I create the sliding windows, make predictions, and obtain results where LASSO always has a very small MSE compared to the MSE of ARIMA (Adjusted with auto.arima in R) and UR (for example, 0.6, 420, 900, respectively). How can I explain this tremendous difference? I have done this for different sizes of $N$ and for different potential predictors, and in the case of LASSO, the MSE mostly decreases, while for ARIMA and UR, it tends to increase. Is there a statistical explanation for this, or should I review my simulations? What I have thought is that when I fit ARIMA and UR to the simulated data, since these data come from a different functional form than the one used to generate them, these models may not be suitable for fitting such data. However, I would like to know if someone can provide a justification for why this is happening. Thank you in advance, and if anything is unclear in my question, I would be happy to clarify.
