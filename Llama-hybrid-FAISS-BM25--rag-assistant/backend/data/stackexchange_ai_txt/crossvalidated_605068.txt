[site]: crossvalidated
[post_id]: 605068
[parent_id]: 
[tags]: 
Correction of biased probabilities

I have a classifier that outputs biased predictions, that I want to correct for in a mathematically sound way. I define bias as the difference between the true class distribution and the average predicted probability per class. I cannot add this bias directly to the outputted probabilities, because then the probabilities would not (necessarily) be in the range [0, 1]. Therefore, for the binary case, I transform both the bias and the predictions via the logit function , add them and then transform them back using the expit function . The procedure looks somewhat like this true_class_distribution = np.array([y_true == c for c in clf.classes_]) # e.g. [0.34, 0.33, 0.33] proba_pred_avg = y_pred_proba.mean(axis=0) # e.g. [0.09, 0.42, 0.48] bias = y_proba_pred_avg - true_class_distribution # [-0.25, 0.09, 0.15] predictions_proba = expit(logit(y_pred_proba) - logit(bias)) This works for the binary case, but for the multiclass case (such as the one shown above) this fails, as the probabilities are not restricted to [0, 1]. For instance, say that the classifier now predicts a new instance with probability [0.1, 0.89, 0.01] would be transformed to [0.36, 0.84, 0.007] which adds up to 1.2, so that the values can not be interpreted as proper probabilities. How can I apply a mathematically sound bias correction? Some options I have thought of: Divide by the sum of the probabilities per sample (i.e. predictions_proba = predictions_proba / predictions_proba.sum(axis=1) ). It works - and since at this stage all the predictions are already restricted to [0, 1], but I'm not sure whether it makes sense. Use the softmax function which will always give values which add up to 1 and are restricted to [0, 1]. This seems to alter the probabilities much strong and seems more ad-hoc to me than option 1. Use the softmax functions after the adding of the bias to the predicted probabilities directly, skipping the expit/logit step. I'm not sure how to pick between these options (or other options that I haven't thought of). I'm also open to the idea that the logit/expit step isnot a good idea for the binary case.
