[site]: crossvalidated
[post_id]: 550561
[parent_id]: 273230
[tags]: 
I have a similar question. But now I think the increase of performance at certain random seed could be real, not overfit . I do the following experiment to prove it. In one of my modeling projects, I split the data into train/validation/test by 70%/15%/15% . Then similar to questioner's method, I keep a set of parameters of xgboost model same , only change the random seed. For each random seed, train the model with early stop, i.e. stop when performance stops increasing on valid dataset. The performance metric here is MAP (mean average precision). In the end , there were about 400 random seeds corresponding about 400 models with different validation data performance. At last, I apply all the models to test dataset, to computer test data MAP. The following is relation between validation MAP and test MAP. The shade is 95% confidence interval. They are significantly correlated tested by Pearson correlation test (correlation=0.356, p=4.13e-14). It should not be this case if it just overfit. This means if I tune random seed to select the best model on validation data, the model will likely (on average) perform better on test data. The result is surprising. My explanation is as following. The random seed most likely effects the resulted xgboost model through the sampling of train data in the fitting process (I use colsample_bytree and subsample). So a random seed will determine a 'path' of trees that focus on different part (e.g. subset of columns) of training data. I use the term 'path' as a metaphor here as one tree in boost algorithm will effect the subsequence trees . A better model with better 'path' of different focuses on training data could be generalized if it reflects the intrinsic property of the data.
