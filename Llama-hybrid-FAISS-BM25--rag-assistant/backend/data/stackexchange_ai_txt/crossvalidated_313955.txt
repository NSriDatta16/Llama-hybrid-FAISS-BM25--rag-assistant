[site]: crossvalidated
[post_id]: 313955
[parent_id]: 
[tags]: 
How to train a Bayesian network with Bernoulli switch variable?

I model my problem as a simple V-structured Bayesian network. There is an $outcome$ variable, the binary $switch$ variable, and some environment features $X$ . All the variables are observed during training but $outcome$ and $switch$ are not known during inference. I am interested in estimating the marginal $P(outcome \mid X)$ . From the domain knowledge, $$P(outcome \mid switch = 0, X) = 0,$$ so, the model can be decomposed as $$P(outcome \mid X) = P(switch = 1 \mid X) \cdot P(outcome \mid switch = 1, X).$$ Fitting $P(switch = 1 \mid X)$ is straightforward: I just use logistic regression. But how to fit $P(outcome \mid switch = 1, X)$ ? My current approach is to filter the training data to retain only examples for which $[switch=1]$ as dictated by the form of this probability. However, it does not seem sensible when it comes to inference. There will be some $[switch=0]\ $ examples that have non-zero $P(switch = 1 \mid X)$ prediction, which can be amplified by $P(outcome \mid switch = 1, X)$ as we did not train that model on similar examples. Should I instead train $P(outcome \mid switch = 1, X)$ on the $[switch=0]$ examples as well, just weighting them proportionally to the $P(switch = 1 \mid X)$ prediction? Or should I reformulate my model completely? Will appreciate any references.
