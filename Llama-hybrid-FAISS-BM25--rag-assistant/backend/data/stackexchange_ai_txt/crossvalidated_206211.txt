[site]: crossvalidated
[post_id]: 206211
[parent_id]: 114672
[tags]: 
For sources, I'd suggest Missing Data Problem in Machine Learning, by Benjamin M. Merlin (Thesis) , or Statistical Analysis with Missing Data, by Roderick and Rubin. What you are explaining in your answer is similar to the augmented model. There are multiple approaches to deal with missing data, and as a quick look into it, the easiest ones to implement are Multiple Mean Imputation : This works if the data is M issing A t R andom, i.e., the fact that the data is missing as no impact on the dependent variable, outside the fact that you do not know the value of the variable. The idea is to assign the mean of the variable to missing values, and add a random error to avoid having a big spike in the distribution. Do this multiple times to get different training sets, and average the models. Another technique that works in this context is matrix reconstruction. Reduced Models : If the data is N ot M issing A t R andom, you get into troubles with mean imputation. The idea of reduced models is to use multiple models for the different patterns of training data. In your case, you would train four models, $[x_1,x_2],[x_1,?],[?,x_2],[?,?]$. Note that when training $[?,x_2]$ you can use data that is valid from the $[x_1,x_2]$ pattern. The question of if you should will come from testing the resulting models. If you do not include other pattern and the dependent variable is dependent on the pattern, not including other data will improve the result. If the independent variable are missing based on their value, but the dependent variable does not depend on the "missingness", including more data should improve the results. Augmented Models : This is the model you finally choose to implement: Set unknown values to $0$ and complement the model with $[0,1]$ variable to represent missingness of features. This works for some models, such as linear regression, where $0$ has a special meaning. This would probably work less well using trees or SVMs, but should still work using NeuralNets. This provides a way to compensate the intercept for different pattern types. This should work well in your case, but when a lot of features have missing values, and there are a lot of patterns with similarities, two patterns may need compensation in different ways, and some additional work may be required to properly identify patterns that are similar.
