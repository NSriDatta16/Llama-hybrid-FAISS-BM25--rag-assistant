[site]: crossvalidated
[post_id]: 547024
[parent_id]: 
[tags]: 
What are pitfalls of clustering variables by absolute correlation for variable selection?

I have a 1,500,000-row dataset with about 100 binary flags and 1,100 continuous variables. Many of the continuous variables have a very high correlation with each other ( |spearman correlation| > 0.8 ). The end-goal is to build an xgboost model predicting an outcome with about 5% incidence among the overall population. When selecting variables, my primary goal is to simply reduce the amount of time and memory that it takes to build the model, primarily through eliminating multicollinearity. If I were building a linear model and wanted coefficients, this would also improve the precision of the model coefficients. The process is as follows: Calculate spearman correlation matrix of independent variables. Calculate spearman correlation between independent variables and dependent variable. Define "distance" between variables as $1-\|x\|$ , where $x$ is the spearman correlation between them. Perform agglomerative clustering between all the variables, using "complete" linkage, so that variables with low correlations between each other do not end up in the same cluster. I used roughly 200 clusters here, since that was reasonable for the resources I had. Rank variables in each cluster, and select the one with the highest correlation coefficient with the dependent variable. I can imagine models in which this could reduce performance noticeably over keeping all the variables, but as a matter of practicality, I cannot imagine too many methods that would be able to reliably account for this prior to modeling. Is there something that I am overlooking here?
