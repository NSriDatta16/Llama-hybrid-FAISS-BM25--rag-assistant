[site]: crossvalidated
[post_id]: 4977
[parent_id]: 4961
[tags]: 
Put in simple terms, regularization is about benefiting the solutions you'd expect to get. As you mention, for example you can benefit "simple" solutions, for some definition of simplicity. If your problem has rules, one definition can be fewer rules. But this is problem-dependent. You're asking the right question, however. For example in Support Vector Machines this "simplicity" comes from breaking ties in the direction of "maximum margin". This margin is something that can be clearly defined in terms of the problem. There is a very good geometric derivation in the SVM article in Wikipedia . It turns out that the regularization term is, arguably at least, the "secret sauce" of SVMs. How do you do regularization? In general that comes with the method you use, if you use SVMs you're doing L2 regularization, if your using LASSO you're doing L1 regularization (see what hairybeast is saying). However, if you're developing your own method, you need to know how to tell desirable solutions from non-desirable ones, and have a function that quantifies this. In the end you'll have a cost term and a regularization term, and you want to optimize the sum of both.
