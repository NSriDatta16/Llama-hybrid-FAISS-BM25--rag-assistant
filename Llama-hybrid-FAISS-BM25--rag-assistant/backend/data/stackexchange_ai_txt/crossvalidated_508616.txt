[site]: crossvalidated
[post_id]: 508616
[parent_id]: 499696
[tags]: 
The fact that cross entropy has an interpretation as a probability distribution or as a the Kullback-Leibler divergence between's the model estimated probability distribution and the empirical distribution of labels does not mean that it is optimal for all cases. As you point out, classification with focal loss actually works even better. The original paper ( Focal Loss for Dense Object Detection ) explains the motivation behind it in great detail, and why it is expected to yield better performance. Maybe most relevant for the motivation of your question, it has already been proven that focal loss leads to better calibrated networks (see Calibrating Deep Neural Networks using Focal Loss ). And well calibrated classifiers can actually have a better performance ( Properties and benefits of calibrated classifiers ). Notice that MLE has problems in situations like imbalanced datasets. That has motivated the development of resampling strategies to compensate for it. Focal loss can be seen as a more principled way to weight samples. Calibration is about how correct your predictions are, on average. As an example, say you want to predict whether tomorrow is going to rain or not. If your perfectly calibrated model tells you it is going to rain with 70% chance, it means that, every time you get such a score from your estimator, 70% of the times it will actually rain. This is specially relevant for decision making. This way you can estimate what your average outcomes (gain/losses) will be.
