[site]: datascience
[post_id]: 92488
[parent_id]: 92476
[tags]: 
There is not necessarily a difference between the two, but the most important reason we minimize a cost function instead of maximizing a profit function is because of the optimization method. Neural networks are optimized using gradient descent, in which we use the derivative of the cost function to calculate the gradients and use the gradient to adjust the parameters. Assuming that you would use the accuracy as the profit function, it is not differentiable, meaning that you cannot calculate the derivate and therefore cannot use gradient descent (or in this case gradient ascent since we want to maximize a function). We then specify a cost function that is differentiable and minimize this as the cost function generally goes down as the profit function increases. You can use gradient ascent to maximize any arbitrary profit function as long as it differentiable.
