[site]: crossvalidated
[post_id]: 625968
[parent_id]: 
[tags]: 
Model Building Process Without Feature Names/Domain Knowledge

I am working with a dataset of $\approx 10^7$ samples and $\approx 120$ features. All samples have a binary classification $0 - 1$ . I am attempting to build a model that minimizes out-of-sample error with not much importance placed on inference. (More of a machine learning than classic statistics framework I guess). Unfortunately, none of the features are named. To prevent against overfitting, I'm motivated to perform some sort of feature selection. When solely focused on accuracy, is there a good approach? The only thing that comes to mind is sequential feature selection using a greedy algorithm, and finding when validation accuracy begins to decrease. Any other thoughts? Furthermore, what is the correct model iteration process? Do I compare models like LDA/QDA vs decision trees, then perform feature selection once I've selected a model? Finally, where would something like hyperparameter tuning fit into this process?
