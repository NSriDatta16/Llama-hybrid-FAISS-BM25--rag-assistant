[site]: crossvalidated
[post_id]: 405300
[parent_id]: 267925
[tags]: 
I would just like to add that another way to take into account prior knowledge is to use either adaptive ridge regression or adaptive LASSO regression, where the lambdas with which you penalize your variables is then given by lambda*adaptive_penalty_weights where adaptive_penalty_weights=1/(abs(betas_prior)+small_epsilon)^2 (typically renormalized to sum to the nr of observations n ) and where betas_prior is your prior belief of what your coefficients should be (often they are set to OLS or NNLS or regular ridge regression estimates). In Bayesian terms, adaptive LASSO regression corresponds to assuming a Laplacian prior (exponential if nonnegativity constraints are imposed), whereas adaptive ridge regression corresponds to assuming a Gaussian prior (truncated Gaussian with nonnegativity constraints). In glmnet there is the penalty.factor argument to fit such models. Glmnet also allows nonnegativity constraints to be set on particular coefficients using the lower.limits argument (as that takes a vector as input).
