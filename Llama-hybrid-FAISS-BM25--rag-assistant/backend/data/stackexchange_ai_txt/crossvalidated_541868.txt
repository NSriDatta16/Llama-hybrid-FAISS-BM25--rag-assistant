[site]: crossvalidated
[post_id]: 541868
[parent_id]: 
[tags]: 
In a sparse reward problem, is it possible to remove reward shaping once the RL agent trains long enough to consistently reach the final reward?

I'm new to machine learning, and primarily looking at it from the perspective of it's applications to control theory. In the application found in this paper, a RL agent attempts to land a spacecraft near a target. The optimization environment is very sparse, with a large reward gained if the spacecraft can successfully land, along with small penalties for fuel use along the way. In order to overcome the sparsity and allow the RL agent to successfully land, the authors employ reward shaping and eventually achieve a successful agent. From this point, the authors discuss improving the reward shaping function to increase fuel efficiency. However, couldn't they just continue training this same agent without any shaping, thus allowing the fuel efficiency rewards alone to shape training? I'm very new to the field, so I suspect I'm missing some fundamental or practical issue with this approach.
