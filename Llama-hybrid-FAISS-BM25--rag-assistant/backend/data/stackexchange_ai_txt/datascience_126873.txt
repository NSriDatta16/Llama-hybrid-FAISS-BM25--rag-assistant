[site]: datascience
[post_id]: 126873
[parent_id]: 126861
[tags]: 
I'd say that it's correct. BERT pre-training doesn't use labels, as it uses two self-supervised objectives: masked language model (mask a word in the middle of a sentence, and guess what it is) next sentence prediction (guess which sentence comes next in the corpus) I don't think there is a "right" or "wrong" way to do this here. You want to have a pre-trained BERT model, and this could come from an unknown dataset, or a dataset that you have. The point of a model like BERT is that it's able to handle large amounts of data, so it's likely that you will get some "similar" data in your pre-training and training. As long as you test your end-model properly and check that it's robust, you should be grand
