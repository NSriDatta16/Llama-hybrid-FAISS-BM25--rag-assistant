[site]: crossvalidated
[post_id]: 131277
[parent_id]: 131268
[tags]: 
Your questions cover a lot of ground, most likely too much for a single question. Nevertheless, I will try my best to answer them in general and point to some resources in order to help you in reformulating some of the questions as clearly as possible, as people suggested in comments. What is the order in which the steps of data pre-processing should be done? To prevent repeating myself, let me refer you to my earlier relevant answers: on data cleaning , on reproducible research and on data analysis workflows . Multiple imputation Reading the following JSS paper might help you answer some, if not most, of your sub-questions on multiple imputation (MI): van Buuren, S., & Groothuis-Oudshoorn, K. (2011). mice: Multivariate imputation by chained equations in R. Journal of Statistical Software, 45(3). Retrieved from http://www.jstatsoft.org/v45/i03/paper . Keep in mind that, in addition to general information, it focuses on the MICE approach, so reading additional information on alternative approaches/methods might be beneficial. For example, this vignette paper, focused on the EMB approach-based Amelia II R package: http://cran.r-project.org/web/packages/Amelia/vignettes/amelia.pdf . Is "expectation-maximization" a better method for imputing missing variables? I don't think this question makes sense, as any other question, asking about relative terms ("better") without explicitly specifying criteria for comparison or assessment. However, the description of the EM approach is available in the excellent book The Elements of Statistical Learning (section 8.5). Variables redundancy This is a rather large area and it is beyond the scope of my answer, but I will just mention that this question is, at least partially, related to the topics of feature selection , feature extraction and dimensionality reduction . and 6. No answer. and 8. Skewness and data transformation I would like to refer you to the sources that I cite in this answer , especially recommend the first one.
