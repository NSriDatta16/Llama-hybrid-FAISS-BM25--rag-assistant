[site]: crossvalidated
[post_id]: 324643
[parent_id]: 
[tags]: 
Aggregating information and Bayesian information

Consider a binary random variable $\theta\in\{0,1\}$. I refer to this variable as "the state". Further assume that each possible state happens with equal probability (i.e., equal to $1/2$). I cannot observe this state, however I have access to a sample $\{r_i(\theta)\}_{i=1}^n$. This sample constitutes the realization of a random variable $R_i(\theta)$, which is distributed according to the following equation: $$R_i(\theta)=\begin{cases} p^A\cdot u_A-c+\varepsilon_i &\text{if} \quad \theta=1\\ (1-p^A)\cdot u_B-c+\varepsilon_i &\text{if} \quad \theta=0\\ \end{cases}$$ where $\varepsilon_i\sim\mathcal{N}(0,1/\rho_{\epsilon^2})$ and all the other variables are deterministic. Notice that in some sense I would like to have no noise term and observe the "true" realisations but I instead only have access to noisy realizations of $R_i$, which I denote $r_i$. My question is then the following: Can I use my sample $\{r_i\}_{i=1}^n$ in order to infer the true state of the world $\theta$? In some Bayesian sense, I was thinking about updating in the following way: $$\mathbb{P}(\theta=1|\text{observing sample } r_i(\theta))=\frac{\mathbb{P}(L(r_i|\theta=1)) \mathbb{P}(\theta=1)}{L(r_i|\theta=1) \mathbb{P}(\theta=1)+L(r_i|\theta=0) \mathbb{P}(\theta=0)}$$ where $L(r_i|\theta=1)$ is some sort of "likelihood" of observing the sample $r_i(\theta)$ given the state of the world. I am being very loose here, but essentially I am interested in computing the probability of this underlying state . What do you think? Do you have any suggestions on how to compute $\mathbb{P}(\theta=1|\text{observing sample } r_i(\theta))$?
