[site]: datascience
[post_id]: 20296
[parent_id]: 
[tags]: 
Cross-entropy loss explanation

Suppose I build a neural network for classification. The last layer is a dense layer with Softmax activation. I have five different classes to classify. Suppose for a single training example, the true label is [1 0 0 0 0] while the predictions be [0.1 0.5 0.1 0.1 0.2] . How would I calculate the cross entropy loss for this example?
