[site]: crossvalidated
[post_id]: 250284
[parent_id]: 
[tags]: 
Normalization of eigenvector $\alpha$ of kernel PCA by eigenvalue $\lambda$

Could someone please help me understand why when we project a new sample in kernel PCA to an eigenvector $\alpha$, we normalize it by dividing the eigenvector $\alpha$ with its eigenvalue $\lambda$? I understand that we need to normalize the eigenvector $V$ and hence the eigenvector $\alpha$ following the criteria described in Eq 9 of the following original paper: Scholkopf et al. 1997, Kernel Principal Component Analysis . However, I couldn't figure out why we divide by $\lambda$ in the normalization (instead of e.g. $(\|\boldsymbol\alpha^{k}\|\sqrt\lambda)$ - see below for derivation)? Eq.9 requires $$1 = \lambda_k(\boldsymbol\alpha^{k*}\cdot\boldsymbol\alpha^{k*})$$ where $\boldsymbol\alpha^{k*}$ is the normalized $\boldsymbol\alpha^k$. Since $\boldsymbol\alpha^k\cdot\boldsymbol\alpha^k = \|\boldsymbol\alpha^k\|^2$, $$\boldsymbol\alpha^{k*} = \frac{\boldsymbol\alpha^{k}}{\|\boldsymbol\alpha^{k}\|\sqrt{\lambda_k}}$$ Eq. 10 (projection of a test point $\boldsymbol\phi(\mathbf{x})$) becomes $$(\mathbf{V}^{k*}\cdot\boldsymbol\phi(\mathbf{x})) = \sum_{i=1}^{l}\alpha_i^{k*} (\boldsymbol\phi(\mathbf{x}_i)\cdot\boldsymbol\phi(\mathbf{x})) = \frac{1}{\|\boldsymbol\alpha^{k}\|\sqrt{\lambda_k}}\sum_{i=1}^{l} \alpha_i^{k}(\boldsymbol\phi(\mathbf{x}_i)\cdot\boldsymbol\phi(\mathbf{x}))$$ $$= \frac{1}{\sqrt{\lambda_k}}\sum_{i=1}^{l} \alpha_i^{k}(\boldsymbol\phi(\mathbf{x}_i)\cdot\boldsymbol\phi(\mathbf{x}))$$ where $\mathbf{V}^{k*}$ is the normalized $\mathbf{V}^{k}$ and $\|\boldsymbol\alpha^k\| = 1$. But, I know that the above normalization is incorrect because instead of dividing by $\sqrt{\lambda_k}$, we should simply divide by $\lambda_k$. So, I am not sure where I am making mistakes here. For a nice python implementation of kernel PCA and the normalization of the eigenvector, please refer to the following site: http://sebastianraschka.com/Articles/2014_kernel_pca.html You can go straight to def project_x(x_new, X, gamma, alphas, lambdas): to see that we need to normalize the $\alpha$ by dividing it with $\lambda$ in order to get the correct projection. Update: The above normalization is correct. The question was there because I misunderstood Sebastian's code.
