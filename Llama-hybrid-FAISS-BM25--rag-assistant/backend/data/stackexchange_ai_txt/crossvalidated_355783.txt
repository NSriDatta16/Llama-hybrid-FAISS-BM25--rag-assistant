[site]: crossvalidated
[post_id]: 355783
[parent_id]: 
[tags]: 
Preventing logloss from over-optimizing on some points (or groups of points)

I've got a neural network classification model that does ok in terms of binary logloss. However I watched the training quite closely, outputting the logloss of each batch (batches are 100 samples) and noticed that while overall the logloss goes down, the network is "sacrificing" performance on some points (or groups of points). For instance if there were just two batches, early in the training the overall logloss is 0.5 and the performance of both batches is close to equal. After a few epochs the overall logloss drops to 0.4 but one of the batches is doing considerably worse than the other one. Is there a way to regularise this? I would prefer decent all-round performance as opposed to amazing performance on one group of points and worse performance on others. I've done a lot of literature reading and I wonder if a max-margin classifier might be helpful.
