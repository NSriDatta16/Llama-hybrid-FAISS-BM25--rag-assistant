[site]: datascience
[post_id]: 75477
[parent_id]: 74966
[tags]: 
If you want to train an AI agent, then you should use RL, but only if your agent has access to control of the game. It should be able to play the game (i.e. interact with the environment) to be able to optimize its policy. It doesn't matter if the reward is delayed, but interaction is crucial. So, basically two elements: interaction with the environment and reward gaining are needed for any RL model. The model has to be able to explore and exploit its policy in the process of improvement, that's why it has to receive feedback from its actions according to the current best policy, not some random games. If you want to just classify some game data (units in your case) collected from the game, you should try different classification algorithms, but not RL. I won't be an AI agent, it will be just a classification model, so it won't be able to play the game. One other option of learning how to play the game without actually playing for your agent would be to collect the data after each game, but not only the data about the units (i.e. states of the game environment), but also the possible actions in each state and actions taken in those states. Then you could hard code some heuristic search algorithm to look for the best possible action in each state of the game. But then again you won't be able to test it unless your agent can interact with the game. And it probably won't be as effective as RL trained agent. So, I'd advise finding a way for an agent to have access to the game and then to try the RL algorithms.
