[site]: datascience
[post_id]: 45117
[parent_id]: 
[tags]: 
Replicating RNN within PyTorch

I tried to create a manual RNN and followed the official PyTorch example , which tries to classify a name to a language. I should note that it does indeed work. I'm not using the final logsoftmax, since I use nn.CrossEntropyLoss() and that should apply that automatically (it gives exactly the same results). class ManualRNN(nn.Module): def __init__(self, batchSize, inputSize, outputSize): super(ManualRNN, self).__init__() self.batchSize = batchSize self.inputSize = inputSize self.hiddenSize = 64 self.hidden = nn.Linear(self.hiddenSize + self.inputSize, self.hiddenSize) #I tried using both, the first one is from the example suite #self.linear = nn.Linear(self.hiddenSize + self.inputSize, outputSize) self.linear = nn.Linear(self.hiddenSize, outputSize) self.tanh = nn.Tanh() def forward(self, x, hidden): x = x.view(1, -1) hidden = hidden.view(1, -1) combined = torch.cat( (x, hidden), dim=1) newHidden = self.tanh(self.hidden(combined)) #output = self.linear(combined) output = self.linear(newHidden) return output, newHidden def InitHidden(self): return torch.zeros(1, self.hiddenSize) My goal is to tie this out vs the internal RNN module, so I used tanh() for the hidden layer, since that's what the documentation shows for nn.RNN . I also tried making the input for the final linear layer only dependent on the hidden layer which seems to be what nn.RNN is doing, but I might be wrong - in either case I tried both. Now I attempted to tie this out vs the nn.RNN module (or should I say cell?): class TorchRNN(nn.Module): def __init__(self, inputSize, hiddenSize, outputSize): super(TorchRNN, self).__init__() self.inputSize = inputSize #number of possible letters, i.e. 31 self.hiddenSize = hiddenSize #64 self.outputSize = outputSize #18 (number of classes) #this should be applying tanh() at each time step self.rnn = nn.RNN(input_size=self.inputSize, hidden_size=self.hiddenSize, batch_first=True) #note that here I am only using hiddenSize, because that's the output from nn.RNN ...? #that's why in the ManualRNN example above I tried the same self.linear = nn.Linear(self.hiddenSize, outputSize) def forward(self, x, hidden): output, hidden = self.rnn(x, hidden) output = self.linear(hidden) return output def InitHidden(self, batchSize): return torch.zeros(1, batchSize, self.hiddenSize) I made sure to use only names with 5 letters, and using the final hidden layer as input, since I didn't want to further confuse the situation with pad_packings. With enough iterations both, the ManualRNN and TorchRNN models perform similarly well. However, they don't tie out. There shouldn't be any randomness and I checked that the inputs are being fed in the same order. I am using the following to establish seeds (even though I am not using cuda on this machine): np.random.seed(666) torch.manual_seed(666) torch.cuda.manual_seed_all(666) torch.backends.cudnn.deterministic = True What am I doing wrong? My understanding is that the example from the website isn't actually replicating the nn.RNN class properly. nn.RNN outputs seem to be hidden_for_each_k and last_hidden . The last element of hidden_for_each_k is the same as last_hidden . Consequently it seems that the only inputs available for the linear layer are last_hidden - which is not the same as in the tutorial, hence I changed the input for the linear layer in the ManualRNN to accept only the hidden layer. However, even that doesn't tie out. The values are completely different for basically any point in time. So in short: Is there a way to go from "manual RNN" to "nn.RNN" so that the results tie out exactly? Any way you could show this to me would be appreciated (it doesn't have to be the same example).
