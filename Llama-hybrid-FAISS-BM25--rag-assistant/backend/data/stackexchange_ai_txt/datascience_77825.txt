[site]: datascience
[post_id]: 77825
[parent_id]: 
[tags]: 
Calculating the entropy of a neural network

I am looking to calculate the information contained in a neural network. I am also looking to calculate the maximum information contained by any neural network in a certain amount of bits. These two measures should be comparable (as in I can compare whether my current neural network has reached the max or is less than the max and by how much). Information is relative, so I define it relative to the real a priori distribution of the data that the neural network is trying to estimate. I have come across Von Neumann entropy which can be applied to a matrix, but because it is not additive I can't apply it to a series of weight matrices (assuming the weight matrices encode all the information of a neural network). I found three other papers Entropy-Constrained raining of Deep Neural Networks , Entropy and mutual information in models of deep neural networks and Deep Learning and the Information Bottleneck Principle . The second contains a link to this github repo, but this method requires the activation functions and weight matrices to be known which is not the case for finding the max entropy of any neural network in n bits. How can I calculate the amount of information contain in/entropy of a neural network? And how can I calculate the same measure for any neural network in n bits?
