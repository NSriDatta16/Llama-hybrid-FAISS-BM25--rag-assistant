[site]: crossvalidated
[post_id]: 610184
[parent_id]: 
[tags]: 
Reduce dimensions of matrices with different shapes: Methods?

BACKGROUND : I have huge (>10000x10000px) pathology slides images with different sizes. Here is an example: You can find this specific example here . This images are pieces of tissue samples (biopsies) that are scanned with a high resolution device. Pathologists use them to look at the morphology of the cells and tell if a cell is malignant (cancer) or bening, and identify other types of structures (blood vessels, muscular cells, etc). This images have a very high resolution but, as you may see in the image, not all the regions have information in it (there are regions without tissue here and there). Hence, there are several tools to process these large images and most of them use "tiling" which consist on create patches of regions and process them individually. Like: From this paper One solution to process this kind of images is the one used in this git . This tool performed feature extraction of every non-blank patches and create a vector of 1024 features for every patch. Finally all patches are concatenated in a matrix for every image with shape 1024xN (being N the number of patches). OBJECTIVE : I would like to use this feature matrices together with some clinical data associated to each image and build and ML classifier that uses both, the clinical data and the feature matrix as input to make a prediction. Hence, I am looking for a way to merge this two datasets. However, since the number of patches is different between images, I would like to find a method that reduce the shape of the matrix ( 1024xN ) to the same shape for all the features extracted ( 1024xC , being C a common dimension for all the output matrices). PRELIMINARY TRY 1 : Here, I am tempted to select some N patches randomly and use them but I would like to know if there is any method to select top-K N patches. PRELIMINARY TRY 2 :I was looking at this answer but I think JLT could not be the solution since it reduce the number of features (here the 1024 ) and not the number of observations (here N ). However, I see that this method also returns a projection matrix but I am unsure if this recapitulates the information in the initial one. PRELIMINARY TRY 3 : This git solve the problem but I would like to compare this method to a ML model (random forest etc.) TLDR : Hence, I am looking for a method that could select the top-K most informative (Â¿? not sure if is the correct word here) observations. Here "informative" would mean the ones with less correlation with the rest of the patches, but it could be also any method that summarises the information differently. Any ideas how could this be done? EDIT: Add more context on what means "informative" in this case EDIT2: Add some background on the type of source of data I am using and structured the post for better understanding EDIT 3: Added a bounty for the task and add a possible solution using Deep Learning. However, I would like to use ML tabular models for comparison.
