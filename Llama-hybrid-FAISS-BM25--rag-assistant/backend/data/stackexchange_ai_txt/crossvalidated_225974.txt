[site]: crossvalidated
[post_id]: 225974
[parent_id]: 225820
[tags]: 
According to bias-variance tradeoff concept, does training on a larger set compared to a smaller sample add more variance and reduces bias from the model, and does this become a factor to consider in my original question? Citing Hastie et al. (2001) section 7.3 "The Bias-Variance Decomposition" (as of 2nd edition, 2009), if we assume $$ Y = f(X) + \varepsilon $$ with $\mathbb{E}(\varepsilon)=0$ and $\text{Var}(\varepsilon)=\sigma_{\varepsilon}^2$, the expected prediction error of a regression fit $\hat f(X)$ at an input point $X=x_0$ using squared error loss will be $$ \begin{aligned} \text{Err}(x_0) &= \mathbb{E}[((Y-\hat f(x_0))^2|X=x_0] \\ &= [\mathbb{E}\hat f(x_0)-f(x_0)]^2 + \mathbb{E}[\hat f(x_0)-\mathbb{E}\hat f(x_0)]^2 + \sigma_{\varepsilon}^2 \\ &= \text{Bias}^2(\hat f(x_0)) + \text{Var}(\hat f(x_0)) + \sigma_{\varepsilon}^2 \\ &= \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}. \end{aligned} $$ For a given model (a functional form), changing the sample size will only affect $\text{Var}(\hat f(x_0))$; namely, increasing the sample will diminish it. Meanwhile, $\text{Bias}^2(\hat f(x_0))$ will stay the same as the functional form $\hat f(\cdot)$ is fixed. (Clearly, the irreducible error also stays the same.) So in your original question, you reduce the expected squared error by reestimating the chosen model on the full sample as compared to having estimated it on just the training sample. References: Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. The Elements of Statistical Learning . Vol. 1. Springer, Berlin: Springer series in statistics, 2001.
