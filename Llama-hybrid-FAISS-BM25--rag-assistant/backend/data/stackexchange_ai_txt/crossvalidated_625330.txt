[site]: crossvalidated
[post_id]: 625330
[parent_id]: 625318
[tags]: 
XGBoost applies a learning rate : the value (in log-odds) in a leaf is scaled by this learning rate compared to the tree-building mechanism. If you set the learning rate to 1, you will recover predicted probabilities closer to the empirical ones. There is further shrinkage from the regularization parameters. And if this weren't binary classification, you'd have to account for the base score. Finally, the nonlinearity of the sigmoid will cause some slight mismatch between the average log-odds and average outcome. (All of this is washed out by having more iterations.)
