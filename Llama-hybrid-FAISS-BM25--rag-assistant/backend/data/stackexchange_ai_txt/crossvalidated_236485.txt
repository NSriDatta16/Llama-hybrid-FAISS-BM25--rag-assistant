[site]: crossvalidated
[post_id]: 236485
[parent_id]: 
[tags]: 
Is gradient checking useless in high dimensional setting?

Is gradient checking (finite difference for numerical gradient to check if analytical gradient is correct) useless in high dimensional setting (say 100K parameters in a deep neural network)? Here is my reason. First, computing numerical gradient is extremely slow. I experimented with R numderiv library, it will not give me any results in couple of hours with even 10K paramters. Second, if we get norm between two very long vectors $\|a-b\|$, it will still be reasonable big (say around $1 \times 10^{-3}$) Am I right?
