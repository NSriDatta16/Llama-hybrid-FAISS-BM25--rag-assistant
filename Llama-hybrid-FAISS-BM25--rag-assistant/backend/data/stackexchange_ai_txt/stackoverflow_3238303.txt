[site]: stackoverflow
[post_id]: 3238303
[parent_id]: 3238118
[tags]: 
You may want to elaborate on the crawling process. I'm guessing it's a recursive crawl, where for each crawled page, you crawl all links on it, and repeat crawling all links on all those pages too. If that's the case, you may want to do two things: Create some sort of a depth limit, on each recursion you increment the counter and stop crawling if limit is reached Detect circular linking, if you have a PAGE_A with a link to PAGE_B, and PAGE_B has a link to PAGE_A you'll be crawling until you run out of memory. Other than that, you should look into using the standard timeout facility of the module you're using, if that's LWP::UserAgent you do LWP::UserAgent->new(timeout => 60)
