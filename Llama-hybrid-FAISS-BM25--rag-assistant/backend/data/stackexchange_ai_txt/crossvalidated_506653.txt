[site]: crossvalidated
[post_id]: 506653
[parent_id]: 43471
[tags]: 
The answer provided by Christoph Hanck compares a Bayesian prediction interval for a future experimental result with a frequentist point estimate of a population-level parameter. A more appropriate comparison would be to compare a Bayesian prediction interval with a frequentist prediction interval . In my examples below I compare Bayesian posterior intervals with frequentist confidence intervals. Ultimately, the choice of using a Bayesian or frequentist approach comes down to how you choose to define probability. See my post here on interpretation and why one would choose a frequentist interpretation, Bayesian vs frequentist interpretations of probability . The following is taken from my manuscript on confidence distributions - Johnson, Geoffrey S. "Decision Making in Drug Development via Confidence Distributions" Researchgate.net (2021) . In short, objective Bayesian and frequentist inference will differ the most when the data distribution is skewed, the sample size is small, and inference is performed near the boundary of the parameter space. Below I will begin with an example where Bayesian and frequentist inference agree perfectly. I will then provide another example where they differ. Under $H_0$ : $\theta=\theta_0$ the likelihood ratio test statistic -2log $\lambda(\boldsymbol{X},\theta_0)$ follows an asymptotic $\chi^2_1$ distribution (Wilks 1938). If an upper-tailed test is inverted for all values of $\theta$ in the parameter space, the resulting distribution function of one-sided p-values is called a confidence distribution function. That is, the one-sided p-value testing $H_0$ : $\theta\le\theta_0$ , \begin{eqnarray}\label{eq} H(\theta_0,\boldsymbol{x})= \left\{ \begin{array}{cc} \big[1-F_{\chi^2_1}\big(-2\text{log}\lambda(\boldsymbol{x},\theta_0)\big)\big]/2 & \text{if } \theta_0 \le \hat{\theta}_{mle} \\ & \\ \big[1+F_{\chi^2_1}\big(-2\text{log}\lambda(\boldsymbol{x},\theta_0)\big)\big]/2 & \text{if } \theta_0 > \hat{\theta}_{mle}, \end{array} \right. \end{eqnarray} as a function of $\theta_0$ and the observed data $\boldsymbol{x}$ is the corresponding confidence distribution function, where $\hat{\theta}_{mle}$ is the maximum likelihood estimate of $\theta$ and $F_{\chi^2_1}(\cdot)$ is the cumulative distribution function of a $\chi^2_1$ random variable. Typically the naught subscript is dropped and $\boldsymbol{x}$ is suppressed to emphasize that $H(\theta)$ is a function over the entire parameter space. This recipe of viewing the p-value as a function of $\theta$ given the data produces a confidence distribution function for any hypothesis test. The confidence distribution can also be depicted by its density defined as $h(\theta)=dH(\theta)/d\theta$ . Consider the setting where $X_1,...,X_n\sim\text{Exp}(\theta)$ with likelihood function $L(\theta)=\theta^{-n} e^{-\sum{x_i}/\theta}$ . Then $supL(\theta)$ yields $\hat{\theta}_{mle}=\bar{x}$ as the maximum likelihood estimate for $\theta$ , the likelihood ratio test statistic is $-2\text{log}\lambda({\boldsymbol{x},\theta_0})\equiv-2\text{log}\big(L({\theta}_0)/L(\hat{\theta}_{mle}) \big)$ , and the corresponding confidence distribution function is defined as above. The histogram above, supported by $\bar{x}$ , depicts the plug-in estimated sampling distribution for the maximum likelihood estimator (MLE) of the mean for exponentially distributed data with $n=5$ and $\hat{\theta}_{mle}=1.5$ . Replacing the unknown fixed true $\theta$ with $\hat{\theta}_{mle}=1.5$ , this displays the estimated sampling behavior of the MLE for all other replicated experiments, a $\text{Gamma}(5,0.3)$ distribution. The Bayesian posterior depicted by the thin blue curve resulting from a vague conjugate prior or an improper 1/Î¸ prior is a transformation of the likelihood and is supported on the parameter space, an $\text{Inverse Gamma}(5,7.5)$ distribution. The bold black curve is also data dependent and supported on the parameter space, but represents confidence intervals of all levels from inverting the likelihood ratio test. It is a transformation of the sampling behavior of the test statistic under the null onto the parameter space, a ``distribution" of p-values. Each value of $\theta$ takes its turn playing the role of null hypothesis and hypothesis testing (akin to proof by contradiction) is used to infer the unknown fixed true $\theta$ . The area under this curve to the right of the reference line is the p-value or significance level when testing the hypothesis $H_0$ : $\theta \ge 2.35$ . This probability forms the level of confidence that $\theta$ is greater than or equal to 2.35. Similarly, the area to the left of the reference line is the p-value when testing the hypothesis $H_0$ : $\theta \le 2.35$ . One can also identify the two-sided equal-tailed $100(1-\alpha)\%$ confidence interval by finding the complement of those values of $\theta$ in each tail with $\alpha$ /2 significance. The dotted curve shows the exact likelihood ratio confidence density formed by noting that $\bar{X}\sim$ Gamma $(n,\theta/n)$ and inverting its cumulative distribution function. This confidence density agrees perfectly with the posterior distribution. A confidence density similar to that based on the likelihood ratio test can be produced by inverting a Wald test with a log link. When a normalized likelihood approaches a normal distribution with increasing sample size, Bayesian and frequentist inference are asymptotically equivalent. In the example above the posterior mean agrees with the maximum likelihood estimate. This is not always the case. Take, for example, estimation and inference on a non-linear monotonic transformation of $\theta$ . For an example where Bayesian and frequentist inference differ, consider the setting where $X_1,...,X_n\sim\text{Bernoulli}(\theta)$ with likelihood function $L(\theta)=\theta^{\sum x_i}(1-\theta)^{n-\sum x_i}$ . The conjugate Bayesian posterior is a $\text{Beta}(a+\sum x_i, b+n-\sum x_i)$ where $a$ and $b$ are the prior parameters. If a vague conjugate prior is used and $19$ events are witnessed in a sample of size $n=20$ , the Bayesian posterior becomes $\text{Beta}(a+19, b+20-19)$ . This produces a posterior mean point estimate of $\frac{a+19}{a+19+b+20-19}$ . Below are two posterior density estimates with 95% credible intervals based on vague conjugate priors, one with $a=1$ and $b=1$ , and another with $a=0.1$ and $b=0.1$ . Also plotted are confidence curves, one-sided p-values calculated using the cumulative distribution function for $\sum X_i\sim$ $\text{Bin}(n=20,\theta)$ , as well as the resulting 95% confidence interval. The Bayesian and frequentist point and interval estimates are similar but different. I suspect this is due at least in part to the parameter space being continuous and the sample space for $\sum X_i$ being discrete. In terms of a willingness to bet, the Bayesian bets according to his beliefs while the frequentist bets according to the long-run probability of his testing procedure. This betting is best imagined in terms of a betting market. The question becomes, what would the market decide on, personal belief or long-run performance? Most gamblers would agree that long-run performance is the best bet. If there is no relevant historical data the frequentist would be willing to bet $\$0.95$ and expect $\$1$ in return if his $95\%$ confidence interval contains the true $\theta$ based on the long-run characteristics of the test above, whereas the Bayesian would be willing to bet more than $\$0.95$ for the same interval based on his beliefs that all $\theta$ 's were equally likely (or concentrated near 0 and 1) until $19$ events were witnessed in a sample of $n=20$ . The frequentist would gladly "buy this bet in the market" at $\$0.95$ and "sell it" (play the bookie) to the Bayesian to make a risk-free profit regarless of whether the frequentist or Bayesian interval covers the true $\theta$ . To the frequentist at no point was $\theta$ randomly selected from a $\text{Beta}(a, b)$ distribution and then imagined instead to have been seleted from a $\text{Beta}(a+\sum x_i, b+n-\sum x_i)$ . The Bayesian prior represents subjective belief. It can also be used to incorporate historical data. Under the frequentist paradigm, historical data can be incorporated via a fixed-effect meta-analysis. (1)
