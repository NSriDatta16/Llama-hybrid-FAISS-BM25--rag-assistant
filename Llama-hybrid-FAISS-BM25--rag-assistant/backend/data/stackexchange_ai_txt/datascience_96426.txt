[site]: datascience
[post_id]: 96426
[parent_id]: 12554
[tags]: 
Adding to the answer of @dalloliogm, I tried to modify his diamond_xx dataframe by simply swapping x and xx via diamonds_xx , and here is the result: > evaluate_model(diamonds_xx) [1] "Correlation matrix" carat depth table xx y z x carat 1.00000000 0.02822431 0.1816175 0.97509423 0.95172220 0.95338738 0.97509423 depth 0.02822431 1.00000000 -0.2957785 -0.02528925 -0.02934067 0.09492388 -0.02528925 table 0.18161755 -0.29577852 1.0000000 0.19534428 0.18376015 0.15092869 0.19534428 xx 0.97509423 -0.02528925 0.1953443 1.00000000 0.97470148 0.97077180 1.00000000 y 0.95172220 -0.02934067 0.1837601 0.97470148 1.00000000 0.95200572 0.97470148 z 0.95338738 0.09492388 0.1509287 0.97077180 0.95200572 1.00000000 0.97077180 x 0.97509423 -0.02528925 0.1953443 1.00000000 0.97470148 0.97077180 1.00000000 [1] "running model" [13:41:45] WARNING: amalgamation/../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior. [1] "Importance matrix" Feature Gain Cover Frequency 1: xx 0.37595393 0.54788335 0.19607102 2: carat 0.19699834 0.18015576 0.04873442 3: depth 0.15358272 0.08780079 0.27767284 4: y 0.11645935 0.06527969 0.18813751 5: table 0.09447860 0.05037063 0.17151492 6: z 0.06252706 0.06850978 0.11786929 So as you can see, the x was discarded in the importance matrix and been replaced by xx . So as long as your dataset keeps the original order, adding new highly correlated features will not alter your result. Worth noticing if you need to analyze those features of importance.
