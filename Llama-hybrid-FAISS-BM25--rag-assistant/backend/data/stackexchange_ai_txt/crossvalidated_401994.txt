[site]: crossvalidated
[post_id]: 401994
[parent_id]: 
[tags]: 
Is the out-of-bag (OOB) error of a random forest classifier overly optimistic if hyperparameters were learned via CV on the same dataset?

I am training a random forest classifier in a setting with such a low sample size that I cannot afford setting aside validation and test sets. I train the hyperparameters via cross-validation and utilize the out-of-bag error of the final model (retrained on the whole dataset with the hyperparameters learned via CV) as an estimator for the generalization error when the model is applied to new data. Is this a sound procedure or is the OOB error overly optimistic, since it is based on the data which has also been used for training the hyperparameters?
