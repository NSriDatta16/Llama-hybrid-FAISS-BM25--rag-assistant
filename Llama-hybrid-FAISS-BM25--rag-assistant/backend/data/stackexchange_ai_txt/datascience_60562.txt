[site]: datascience
[post_id]: 60562
[parent_id]: 60527
[tags]: 
I assume that you are referring to the fact that training a decision tree (e.g. with the C4.5 algorithm or a variant) involves the selection of the features used as conditions in the nodes. Typically at each iteration the algorithm selects the most discriminative feature by ranking all the features using for instance their information gain . This is indeed very close to a feature selection process: ranking features by their informative power w.r.t the label is a very common way to do feature selection the decision tree can end up selecting only a subset of the available features, exactly as if feature selection had been applied. SVM works very differently and (in general) doesn't have a similar process of selecting particular features. But it's true that in a loose sense any classifier does some kind of feature selection: it measures how relevant each feature is in order to predict the correct class. However it's important to understand that a real feature selection process consists in removing features before training, i.e. the learning algorithm doesn't get the option to use the features which have been removed. This differs a lot from the above cases because: if the feature selection removes too many features, the model might not be able to perform as well as with all the original features if the feature selection correctly removes redundant and/or useless information, the model is likely to perform better than if all all the features were provided. This is because the algorithm can get confused by the abundance of information, either because it's harder to find what is relevant among many options or because it doesn't have enough instances to properly assess the relevance of all the features.
