[site]: crossvalidated
[post_id]: 112913
[parent_id]: 
[tags]: 
How to deal with categorical features.

Recently I am playing in the famous Big-Data website Kaggle . There is a Display Advertising Challenge. In this competition, you are giving a training file which include huge records. the records is constitute of 13 integer features and 26 categorical features and a label that indicates an ad is clicked(1) or not(0). Then give you a record without label, you should predict the probability it takes the value 1 given the 13 integer features and 26 categorical features. I use stochastic Linear Logistic Regression to deal with integer features. the function I use is $$\frac{1}{1+e^{-\theta^Tx}}$$ But I don't know which model is suitable for the categorical features. I think although these features is hashed into 32Bit numbers. but they don't have any numerical meaning. I think the number is used to indicate the two features is equal or not. I don't know how to deal with these features. my naive way is to count the times of features when the label is $1$(clicked). and divide the whole number when the label is $1$. then we get the probability of the feature. and then use this probability to predict the probability of clicked.(if the feature appear more in the train data, the high probability it is.) When the number of different feature in one category become very large. this way seem very poor. Could you tell me some book or idea or somethings else to help me take the first step. thanks very much.
