[site]: crossvalidated
[post_id]: 202819
[parent_id]: 202793
[tags]: 
The misunderstanding here is that GMM exploits both moment conditions simultaneously. As there are more ($=2$) moment conditions than unknown parameters ($=1$), there is no value that uniquely solves both moment equations $$ E(X)-1/\lambda=0 $$ and $$ E(X^2)-2/\lambda^2=0 $$ GMM therefore minimizes the weighted squared difference between the empirical version of the moments and the functions of the parameters, weighted by some suitable (positive definite) weighting matrix. Thus, let $\bar{X}$ the sample average and $\bar{X^2}=\frac{1}{n}\sum_iX_i^2$. Next, let $$ m(\lambda)=\begin{pmatrix}\bar{X}-1/\lambda\\\bar{X^2}-2/\lambda^2\end{pmatrix} $$ Taking the identity matrix as the weighting matrix for simplicity (see below for a more efficient alternative), the GMM minimization problem becomes $$ \min_\lambda m(\lambda)'m(\lambda) $$ A bit of algebra will give you a FOC $$ \bar{X}\lambda^3+(4\bar{X^2}-1)\lambda^2-8=0 $$ In R, you could solve that as follows: polyroot(c(-8,0,4*xbar2-1,xbar)) Let us try some data: n The admissible (positive) solution seems to do the trick (note the default is $\lambda=1$) > polyroot(c(-8,0,4*xbar2-1,xbar)) [1] 0.9993932-0i -1.1701779-0i -6.8476177+0i It seems worth emphasizing, however, that GMM is not efficient here, as the MLE $1/\bar{X}$ already is. Consider this little Monte Carlo simulation: reps The following plot shows that ML is not only much simpler, but more efficient: A more efficient GMM estimator is obtained by employing an efficient weighting matrix, i.e., one that converges to the inverse of the variance matrix of the moment conditions: optGMM Including this in the simulation (for $n=1000$ now) gives The variance of this distribution is only marginally higher than that of the MLE in the simulation.
