[site]: crossvalidated
[post_id]: 503339
[parent_id]: 
[tags]: 
Why fit on the gradient rather than minimize the loss directly when using Gradient Boosting?

In Friedman's paper on Gradient Boosting, he states the motivation for the gradient boosting algorithm is that it provides a framework of boosting for arbitrary loss functions. He then steps through a motivating example of gradient boosting using the MAE or L1 loss with regression trees as follows: To add more motivation, he states the following: There are two things I don't understand: Why is using least squares to fit the regression tree on the gradient of the L1 loss function faster than directly minimizing the L1 loss directly? It seems straightforward to iterate through your data to find the best splitting point and variable using L1 loss rather than the traditional L2 loss. I don't see how "Squared-error loss is updated much more rapidly" in this case. Also I don't think inducing/fitting a tree on the gradient of L1 loss is equivalent to fitting the tree directly with the L1 loss function. For example, if I have the data set ${(-1, -1), (0,0), (1, 1000)}$ with $F_0(x) = 0$ . Then on my first step if I use the GBM algorithm, I'm fitting onto the signed vectors (-1,0,1). My tree split would then be $R_1 = (x and $R_2 = (x >= 0)$ where the values of $R_1(x) = -1$ and $R_2(x) = 500$ . The MAE in this case would be 500. However if I fit the tree by directly minimizing the L1 loss, I would make the split $R_1 = (x and $R_2 = (x > 0)$ with values $R_1(x) = -0.5$ and $R_2(x) = 1000$ with a MAE of 1. Fitting the tree by directly minimizing the L1 loss rather than fitting on the gradients seems strictly better in this example. Thus I'm struggling to understand the benefits of doing the "intermediate" step of fitting on the gradient rather than utilizing the "alternative approach" of fitting a tree to directly minimize the loss function of your choice (L1 loss in this particular example). EDIT: To address my first question, it is more efficient to calculate the L2 loss because it first involves taking the average for each region in the tree vs. taking the median. Taking the average should have complexity $O(n)$ vs $O(n * lg(n))$ for the median b/c you have to sort the data in each region to calculate the median. So to follow up on my uncertainty: Is this the sole motivation for fitting to the gradients (with L2 error) rather than optimizing for the arbitrary loss function?
