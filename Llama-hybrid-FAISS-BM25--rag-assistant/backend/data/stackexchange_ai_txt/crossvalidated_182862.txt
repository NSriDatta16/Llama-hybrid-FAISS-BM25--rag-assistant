[site]: crossvalidated
[post_id]: 182862
[parent_id]: 
[tags]: 
Prior pdf decay in Recursive Bayesian Estimation

I'm doing Recursive Bayesian Estimation numerically. I have a state vector, x, that I'm trying to estimate by regularly taking noisy measurements, z. I use Posterior = Likelihood x Prior / Normalization, or: $$P(x_n|z_{1:n}) = \frac{P(z_n|x_n)P(x_n|z_{1:n-1})}{P(z_n|z_{1:n-1})}$$ I can also use the Chapman-Kolmogorov equation to get $$P(x_n|z_{1:n-1})=\int{P(x_n|x_{n-1})P(x_{n-1}|z_{1:n-1})dx_{n-1}}$$ Together these 2 equations give me a way of recursively calculating the posterior at the current step, n, given all the information preceding step n. However, it seems that every time step is given equal weight because if you unroll all the recursive equations, you just get a huge chain of products of various probabilities all multiplied together and not weighted in any way at all. Physically, this is silly because more current information should be weighted more heavily. So my question is, on every time step, can I raise the prior pdf to an arbitrary fractional power, k, between 0 and 1 to weight it differently? As follows: $$P(x_n|z_{1:n}) = \frac{P(z_n|x_n)P(x_n|z_{1:n-1})^k}{Normalization}$$ I think that doing this on every time step will mean that the older prior distributions are sequentially raised to the k for as many timesteps as they have been used. In other words, with 0 For example, if I use k= 0, the posterior is identical to the likelihood, i.e. my recursive Bayesian filter has no memory at all and completely ignores the prior distribution (I realize this would be dumb to do, but this is just the limit that k goes to 0). On the other hand, k=1 gives the usual equations without any modification. The interesting region is with k a little less than 1, so that the prior pdf experiences slight exponential decay. I realize that you cannot just take a pdf and raise it to a power and expect that it is still a probability distribution. However, numerically, my likelihood and prior are just 2D matrices that are multiplied elementwise, and there is a normalization factor that is calculated at each time step that takes care to make it a pdf again. My question is if raising to the kth power will have the desired effect of placing more emphasis on the recent pdfs and less on older ones. Also, if there is a better way of incorporating memory into the recursive Bayesian estimation equations please let me know. Thanks.
