[site]: crossvalidated
[post_id]: 504140
[parent_id]: 
[tags]: 
Is the so-called "maximum likelihood" problem for linear regression really a "conditional maximum likelihood" problem?

I am reading the highly praised “Mathematics for Machine Learning” by Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. And in their development of the linear regression model, they write, This “conditional PDF”... $p(y_1, \ldots, y_N|x_1, \ldots, x_N, \theta)$ is not a conditional PDF at all. It is merely carrying the parameters $x_1, \ldots, x_N, \theta$ . The only random variable is $y$ , which is induced by a zero-mean Gaussian noise, $$y = \theta^Tx + \epsilon$$ Random = Deterministic + Random The same snippet shows that $x_1, \ldots, x_N$ are not random variables, but merely some vector in $\mathbb{R}^D$ . So this makes the maximum likelihood estimation problem an unconditional one. Which is fine. However, in other texts, they do use conditional PDF, because the author assumes a probability density on each pair of data $(x,y) \sim P$ (as opposed to $y \sim P$ as above). This leads to an alternative development of the maximum likehood problem, for instance, these notes by CMU Lecture 6: The Method of Maximum Likelihood for Simple Linear Regression : Here, this joint PDF is indeed a conditional one. Here $X$ is a random variable. And the calculation follows similarly as in the above reference texts. So which is it? Is linear regression a maximum likelihood problem or a conditional maximum likelihood problem? How do you reconcile these two separate and distinct framing of the problem?
