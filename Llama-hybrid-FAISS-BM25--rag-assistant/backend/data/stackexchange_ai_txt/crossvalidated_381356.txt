[site]: crossvalidated
[post_id]: 381356
[parent_id]: 
[tags]: 
Some questions about exponential families

Regarding the book The Bayesian Choice I understand most of chapter three on exponential families, but there are two parts I have trouble understanding. The first is Consider $$f(x|\theta)=h(x)\exp(\theta \cdot x - \psi(\theta))$$ a generic distribution from an exponential family, then a proposition is that a conjugate family for $\theta$ is given by $$\pi(\theta|\mu , \lambda)=K(\mu,\lambda)\exp(\theta \cdot \mu - \lambda \psi(\theta))$$ where $K$ represents the normalising constant and the corresponding posterior is $\pi(\theta|\mu+x,\lambda+1)$ (under certain conditions). I don't really understand this or how it is applied. For example consider a $Poisson(\lambda)$ density $$f(x|\lambda)=\frac{1}{x!}\exp(x\ln\lambda-\lambda)$$ Our sufficient statistic is $x$ , and we can write in the form above either as $\psi(\lambda)=\lambda$ or using natural parameter form $\psi(n)=\exp(n)$ as our natural parameter is $n=\log \lambda$ But I don't understand the proposition above, how it is applied or used. Second, I don't understand the following related proposition from the same page If $\Theta$ is an open set in $\mathbb{R^{k}}$ and $\theta$ has prior $$\pi_{\lambda,x_{0}}(\theta)\propto\exp(\theta \cdot x_{0}-\lambda \psi(\theta))$$ with $x_{0}$ in $X$ then $$\text{E}[\nabla \psi (\theta)]=\frac{x_{0}}{\lambda}$$ and for $n$ iid observations the conditional posterior expectation over all is $\frac{x_{0}+n \bar x}{\lambda+n}$ Looking for any help understanding these or examples of how they are used/why they are important etc.
