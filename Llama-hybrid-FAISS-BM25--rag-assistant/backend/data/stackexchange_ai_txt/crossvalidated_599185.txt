[site]: crossvalidated
[post_id]: 599185
[parent_id]: 
[tags]: 
Lost in Fisher information notation

I believe I understand the gist of what Fisher Information is, but I want to be rigorous and I am confused by notation. Also I believe I am masquerading mistakes by abusing the notation (I have studied statistics by myself from different sources, no teacher to point me my mistakes). I will try to state my understanding with the notation I think is right and rigorous and hope someone can help me figure out where/if I am doing it wrong. I will label my statements to organize the discussion. Let's go! (a) Imagine I have observed the nature and collected the data $D=\{x^{(i)},y^{(i)}\}_{i=1}^{n}$ , which I assume comes from a distribution (probability mass function) parametrized by an unknown $\theta$ , i.e. $D \sim p(y|x;\theta)$ . As a shortcut notation, $p(\cdot; \theta) = p_{\theta}$ . (b) Let the scalar field $q_w: \mathbb{R}^p \to \mathbb{R}$ be a model with $p$ parameters $\{w^{(i)}\}_{i=1}^{p}$ that learned $p_\theta$ within some margin of error. This model predictions are called $\hat y$ : \begin{align} \hat{y}:&= \underset{y}{\arg\max} ~q_w(y|x) \tag{b.1}\\ \frac {\partial \hat{y}}{\partial w} &= \nabla_w q_w(y|x) \tag{b.2} \end{align} (c) $\nabla_w q_w(y|x)$ is a vector field $\mathbb{R}^{n \times p} \to \mathbb{R}^{n \times p}$ . (d) The Fisher Information of the model $q_w$ w.r.t its parameter $w$ , $F(w)$ is: \begin{align} F(w)&= \mathbb{E}_{x\sim p_\theta} \left[ \mathbb{E}_{y\sim q_w} \left[\frac{\partial^2}{\partial w^2} \log q_w(y|x) \right ] \right ] \tag{d.1}\\ F(w)_{i,j}&= \mathbb{E}_{x\sim p_\theta} \left[ \mathbb{E}_{y\sim q_w} \left[\frac{\partial^2}{\partial w_i \partial w_j} \log q_w(y|x) \right ] \right ] \tag{d.2}\\ F(w)_{i,j} &= \mathbb{E}_{x\sim p_\theta} \left[ \mathbb{E}_{y\sim q_w} \left[ \frac{\partial \nabla_{w_i}}{\partial w_j} \log q_w(y|x) \right ] \right ] \tag{d.3}\\ F(w)_{i,j} &= \mathbb{E}_{x\sim p_\theta} \left[ \mathbb{E}_{y\sim q_w} \left[ \nabla_{w} \log q_w(y|x) \cdot \nabla_{w} \log q_w(y|x)^{\intercal} \right ] \right ] \tag{d.4}\\ F(w)_{i,j} &= \frac{1}{n}\sum_{x} \left[ \frac{1}{n}\sum_{\hat y} \left[ \nabla_{w} \log q_w(y|x) \cdot \nabla_{w} \log q_w(y|x)^{\intercal} \right ] \right ] \tag{d.5} \end{align} Observations about $F$ : (e) $F$ is the hessian of the Kullback-Leibler divergence $KL[p_{\theta}\|q_{w}]$ which is the same as the hessian of the cross-entropy $H[p_{\theta}, q_w]$ : \begin{align} F(w)=\nabla^2_w KL[p\|q] = \nabla^2_w H[p,q] \tag{e} \end{align} (f) $F$ measures the curvature of $\log q_w(\hat{y} ~|x)$ for a certain parameter $w$ ; i.e. if $w$ is the result of a loss minimization, $F$ indicates if the $w$ found is in a sharp minima region or a flat minima region. (g) $F^{-1}$ is equivalent to the covariance matrix $\Sigma$ . If the Fisher Information is high, the expected error is low and vice-versa. (h) Let $q_1$ and $q_2$ be two models of $p_{\theta}$ . If for a certain loss function $\mathscr{L}$ , $\mathscr{L}(q_1)=\mathscr{L}(q_2)$ , and $F_1 , $q_2$ is a better than $q_2$ because we can expect it to be more robust than $q_1$ , i.e. it's loss function is less sensitive to small changes in the models weights, therefore, the model is less likely to be overfitting the data. (i) We can approximate the Fisher Information in these ways: (i.1) By using $y \sim p_{\theta}$ instead of $y \sim q_{w}$ in (d.5): \begin{align} F(w) \approx F^\prime = \frac{1}{n}\sum_{x \sim p_{\theta}} \left[ \frac{1}{n}\sum_{y\sim p_{\theta}} \left[ \nabla_{w} \log q_w(y|x) \cdot \nabla_{w} \log q_w(y|x)^{\intercal} \right ] \right ] \end{align} (i.2) extending (i.1) and approximate $F$ to the inverse of the diagonal of the covariance matrix, i.e we consider only the variance for each $\{w^{(i)}\}_{i=1}^{p}$ : \begin{align} F(w) \approx F^{\prime\prime} = \frac{1}{n}\sum_x \text{diag}((\nabla_{w} \log q_w(y|x))^2) \end{align} My questions: Is there any error in what was stated above in terms of understanding or notation? I am particularly in doubt of the $\frac{1}{n^2}$ term in (d.5). Is there a better notation to state what I said? What makes a notation stylistic better? Is (d.3) equal to (d.4) or is it an approximation? I know it is common to approximate $F$ to the diagonal of the Hessian (as I pointed out in i.2) for faster calculation and I am not used to calculus with matrices. By the way, is there a good reference to learn Differential equations with matrices?
