[site]: crossvalidated
[post_id]: 504239
[parent_id]: 504206
[tags]: 
As I said at the beginning in the comments some detail more would be useful for avoid ambiguities and misconceptions. Anyway I propose here a quite general basic answer. Starting to define the linear regression model as $y = Xâ€™\beta + \epsilon$ In a widely shared definition (for example from Bruce Hansen) heteroskedasticity is: $V[\epsilon|X=x] = E[\epsilon^2|X=x] = \sigma^2(x)$ (a function of $X$ ) while homoskedasticity mean that $V[\epsilon|X=x] = E[\epsilon^2|X=x] = \sigma^2$ (a positive constant) Therefore if regression error ( $\epsilon$ ) and regressors ( $X$ ) are independent we cannot have heteroskedasticity, then homoskedasticity must hold. In your (@bgst) plot residuals are showed as function of $X$ , so heteroskedasticity (defined as above) appear evident. Indeed graph like that is frequently used for show how heteroskedasticity appear. However heteroskedasticity can be defined differently. For example: $V[\epsilon|Z=z] = E[\epsilon^2|Z=z] = \sigma^2(z)$ where $Z$ is a set of variable not necessarily shared with the regressors set. So even if regression error ( $\epsilon$ ) and regressors ( $X$ ) are independent, heteroskedasticity can appear. Moreover heteroschedaticity sometimes is presented as a property of error matrix , then as temporal or spatial dependence among errors. In time series framework a GARCH process is a common example of heteroskedasticity and it imply some dependency among errors; more precisely imply correlation among some squared error terms. The last case can be included in the definition $V[\epsilon|Z=z] = \sigma^2(z)$ where $Z$ represent past value of squared errors. The message in that: the concept of heteroskedasticity have much to do with that of stochastic dependance, however context/definitions/details matters a lot.
