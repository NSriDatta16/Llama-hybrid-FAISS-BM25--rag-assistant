[site]: stackoverflow
[post_id]: 5085333
[parent_id]: 
[tags]: 
Rule of thumb for determining web server scale up and down?

What's a good rule of thumb for determining whether to scale up or down the number of cloud based web servers I have running? Are there any other metrics besides request execution time, processor utilization, available memory, and requests per second that should be monitored for this purpose? Should the weighted average, standard deviation or some other calculation be used for determining scale up or down? And finally, are there any particular values that are best for determining when to add or reduce server instances?
