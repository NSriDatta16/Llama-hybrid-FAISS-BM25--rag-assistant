[site]: datascience
[post_id]: 52899
[parent_id]: 
[tags]: 
Character-level seq2seq with LSTM in Keras for language declension

I am trying to create sequence to sequence mapping of words on Croatian language, to automatize declension ( https://en.wikipedia.org/wiki/Declension ). English language is fairly simple in that regard, but most world languages do have extensive declension rules. Analogy in English language would be car -> cars, or bus -> buses. Since Croatian language has 14 forms for each noun (7 for singular, and 7 for plural), and there are a lot specific rules with exceptions, deep learning seems like a good strategy to try out. I have created a training dataset with about 2000 entries, and I would like to train a model that is capable of generating 13 other forms from the initial word. I followed several tutorials and examples, but I can not make it work, since model is returning floating-point values, instead of 0 and 1, which I need to revert one-hot encoded vectors for each word. I am guessing that some of the parameters in model are not correct, but after a couple of days trying, I was not able to make it work. My code is here: https://github.com/matkosoric/deklinacije Initial step would be to create mapping for just one form, and then others, and maybe eventually combine outputs. What I am doing in my Jupyter Notebook is chartacter-level encoding, to get vectors like this: [[1, 17, 14, 3, 15, 6, 7], [1, 17, 3, 10, 2, 19, 2, 8, 1], [1, 17, 3, 7, 6, 7, 4], [1, 17, 3, 5, 4, 12, 11], [1, 22, 5, 2, 9, 1, 7, 9, 1], [1, 14, 1], ... Then I add zero-padding, and then I do the one-hot encoding of the characters. Finnaly, this is the code for model: model = Sequential() model.add(LSTM(28, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True, activation='softmax')) #, return_sequences=True # model.add(Dropout(0.2)) model.add(LSTM(28, activation='softmax', return_sequences=True, recurrent_activation="sigmoid")) #, return_sequences=False model.add(Dense(28, activation='sigmoid')) model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy']) # binary_crossentropy | categorical_crossentropy history = model.fit(X_train, Y_train, epochs=20, batch_size=128, validation_data = (X_val,Y_val) ) After training and using model for prediction for test set, I get something like this, which can not be converted back to meaningful character sequence: [0.5826852 0.42405558 0.42048743 0.41398 0.425908 0.4192267 0.4133946 0.41803166 0.4172448 0.42309627 0.4156795 0.419433 0.416399 0.41344917 0.4158144 0.4196655 0.42249662 0.42083743 0.42371705 0.41620597 0.41855162 0.42147878 0.41726097 0.41544187 0.42190933 0.42433727 0.42017445 0.409134 ][0.5842657 0.4228464 0.41903406 0.41235432 0.42476133 0.41782877 0.41161612 0.4165 0.415777 0.42182064 0.41404086 0.41801238 I am guessing that Dense layer at the end should have number 2 as a parameter, since I am expecting zero or one values, but it is not compiling because of return_sequences=True in the LSTM layer before.
