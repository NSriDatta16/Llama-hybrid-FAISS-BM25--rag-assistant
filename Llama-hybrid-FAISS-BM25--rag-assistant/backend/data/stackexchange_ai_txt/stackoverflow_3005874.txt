[site]: stackoverflow
[post_id]: 3005874
[parent_id]: 
[tags]: 
DB optimization to use it as a queue

We have a table called worktable which has some columns( key (primary key), ptime , aname , status , content ). We have something called producer which puts in rows in this table and we have consumer which does an order-by on the key column and fetches the first row which has status 'pending'. The consumer does some processing on this row: updates status to "processing" does some processing using content deletes the row We are facing contention issues when we try to run multiple consumers (probably due to the order-by which does a full table scan). Using advanced queues would be our next step but before we go there we want to check what the max throughput is that we can achieve with multiple consumers and producer on the table. What are the optimizations we can do to get the best numbers possible? Can we do an in-memory processing where a consumer fetches 1000 rows at a time processes and deletes? will that improve? What are other possibilities? partitioning of table? parallelization? Index organized tables?...
