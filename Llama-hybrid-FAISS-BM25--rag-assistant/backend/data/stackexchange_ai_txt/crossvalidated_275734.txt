[site]: crossvalidated
[post_id]: 275734
[parent_id]: 275641
[tags]: 
As explained in the comments, to know a pdf up to a normalizing constant means that for a random variable $X$ with pdf $f(x)$, we know that $$f(x) \propto g(x) \Rightarrow f(x) = c g(x), $$ where $c = \int g(x) dx$ is unknown. Consider the standard normal distribution which has pdf $$f(x) = \dfrac{1}{\sqrt{2\pi}} e^{\frac{-x^2}{2}}\,. $$ Now suppose we are in a situation, where we can only identify that $$f(x) \propto e^{\frac{-x^2}{2}} \,,$$ and thus $g(x) = e^{\frac{-x^2}{2}}$ and the normalizing constant $c = \frac{1}{\sqrt{2\pi}}$ is not known. If we visualize this and plot $f(x)$ and $g(x)$, we get the following graph The dotted line is the true pdf unknown to us, and the solid line is the known $g(x)$. The shape of the density remains the same, but the function is stretched or squeezed in the vertical direction depending on the unknown normalizing constant. In Bayesian models, often we know the posterior only upto a normalizing constant, but we want to learn about different characteristics of the posterior distribution: like mean, mode, or quantile of the distribution. Well, clearly the mode of $g(x)$ corresponds to the mode of $f(x)$, so the normalizing constant is not needed. However, to calculate the mean for the distribution $$E(X) = \int xf(x) dx = \int cx g(x) = c \int xg(x)dx\,. $$ Even if you can solve $\int xg(x)dx$, $c$ is unknown, and you can't find the mean of $X$. Here is where MCMC can be used since it is able to draw samples from the distribution of $X$, without requiring the normalizing constant $c$.
