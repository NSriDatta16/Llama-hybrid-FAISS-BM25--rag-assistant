[site]: datascience
[post_id]: 65776
[parent_id]: 65712
[tags]: 
The number of units in an RNN layer determines the amount of "memory" of that layer. Higher number of units make the model (potentially) able to remember longer sequences, and to explain current values based on information that dates more back in time. If on the other side the number of units is shorter, the model at that layer won't be able to recover information way back in the past. This is tru for any Recurren architecture (simple recurrent, LSTM, GRU). If you increase the number of units of a GRU layer and you see that there's no improvement in the quality of your model, that probably means that your time series data have "shorter memory", i.e. that you don't need to go that far back in time in order to explain current values. Of course I didn't take a look at your data, this is just the most likely reason IMHO.
