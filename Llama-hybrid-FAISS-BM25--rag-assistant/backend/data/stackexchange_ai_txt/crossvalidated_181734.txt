[site]: crossvalidated
[post_id]: 181734
[parent_id]: 
[tags]: 
Fully grown decision trees in random forests

Several sources suggest it's ok to fully grow the decision trees in a RF (e.g., Leo Breiman's article and Elements of Statistical Learning , p. 596). I don't understand the following. Suppose that due to noise, a single data point x of class A ended up somewhere deep inside class B (in terms of its position in the space of features). Every tree that includes x will have a leaf that contains just x alone (because it's fully grown, so it won't stop until each node can precisely identify the class on the training data set; and because x is so isolated from other points of class A, that it can't ever be joined with them in a contiguous region of space carved by a tree). Roughly two-thirds of the trees will include x. Therefore, it seems the majority vote would always be to classify any points close x as if they belong to class A - even though this is clearly overfitting. A similar argument can be made for any noise that caused a few points of one class to end up in the region that should be assigned to another class. How is it, then, that fully grown trees inside a RF aren't causing major overfitting?
