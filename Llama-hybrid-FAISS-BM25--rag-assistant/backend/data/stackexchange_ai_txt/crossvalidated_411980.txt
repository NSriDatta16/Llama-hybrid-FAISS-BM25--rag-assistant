[site]: crossvalidated
[post_id]: 411980
[parent_id]: 411341
[tags]: 
Since you ask "What other approaches should be considered?" we'll have to describe various methods. First, Regression by leaps and bounds is a technique for considering all possible regressions. In some sense it is a "brute force" model selection method with some optimization to allow a large number of related linear models to be fit quickly. Based on your comment: I fit all possible regressions with zero up to N variables Leaps and bounds may be exactly what you are looking for, or it may not. Second, there are other model selection techniques are based on the idea of finding the single best model are forward selection, backwards selection, etc. However, I won't discuss these because they are strictly inferior to feature selection via the LASSO . Essentially, a LASSO is a linear model with an l1 regularization penalty. The l1 regularization penalty encourages individual parameters to be exactly zero if they do not contribute significantly to reducing loss; parameters which are exactly zero can be considered to be "removed" from the model. Least angle regression in particular is a very efficient algorithm for the LASSO, and is the most efficient way I know of for doing variable selection on a very large number of variables. Third and finally, if you have a variety of different models and want to form an ensemble model by taking a weighted average, you have two choices. You can either come up with some a priori scheme for weighting them, such as equal weights or some function of their goodness-of-fit, or you fit a new separate model which picks optimal values for those weights. This second strategy is called stacking. The easiest and most common way to stack models is to treat the outputs (predictions) of all other models as the inputs of a linear regression model and fit it to the dependent variable. $$ y = \beta_0 + \beta_1 h_1(X) + ... + \beta_N h_N(X)$$ This regression will set a coefficient $\beta_i$ close to zero if $h_i(X)$ is uncorrelated with $y$ , and a large coefficient when $h_i(X)$ is highly correlated with $y$ . In this way, it not only selects weights for you, but it chooses the optimal weights in a certain sense. l1/l2 regularization can be a used to make the stacking model more robust, and it's not uncommon to remove the intercept term $\beta_0$ so that we give a true average over the constituent models in the ensemble. If you suspect that your constituent models are of very different quality I would definitely recommend stacking over any a priori weighting scheme. On the other hand, if you suspect your models are very close in quality, a simple arithmetic mean is known to work well, as demonstrated by random forest.
