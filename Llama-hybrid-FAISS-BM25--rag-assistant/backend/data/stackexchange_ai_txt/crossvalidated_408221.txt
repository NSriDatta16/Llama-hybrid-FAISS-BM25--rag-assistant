[site]: crossvalidated
[post_id]: 408221
[parent_id]: 408121
[tags]: 
From a frequentist perspective , there are some clear disadvantages of a sequential analyses. That is, if we are concerned with preserving type I errors, we need to recognize that we are doing multiple comparisons: if I do 3 analyses of the data, then I have three non-independent chances to make a type I error and need to adjust my inference as such. There's a variety of methods for accounting for this, but in short, for a fixed sample size and significance level, all of them end up reducing power compared to waiting until all the data comes in. So if you're looking at the power/subjects ratio, you can't beat a fixed analysis, although as you point out, often that's not necessarily the most important metric. Theoretically , from a Bayesian perspective, there's nothing wrong with using a sequential analysis. Since Bayesian decision theory generally does not worry about type I errors, there's nothing wrong with multiple peeks. However, in practice, it's a lot more of a gray area. Derived prior distributions don't really capture our knowledge before seeing the data, but we can hand wave this issue away by saying that the likelihood will typically dominate the prior, so this isn't an issue. But if we do a sequential analysis, we may be analyzing the data when we have very little data. Suddenly, miss-specification of the prior becomes a really big issue! EDIT: @FrankHarell brings up the point that if you have a valid prior, you should do a sequential analysis. The point I would like to make is that "Valid" priors (i.e. a distribution that perfectly matches the desired uncertainty) are extremely hard to come by. Pragmatic priors (i.e. a distribution that improves the performance of our model) are much easier to find. Despite the fact that priors are typically not "valid", we still have some faith in our Bayesian analyses, since the likelihood usually swamps the prior anyways. With a sequential analysis, early on in a study the likelihood may not swamp the prior, so we need to handle with extra care! As a toy example, suppose we had a sequential analysis where we wanted to compare $\mu_1$ and $\mu_2$ and we (mistakenly) put a prior on $\sigma$ (shared between both groups) that puts almost all the probability below 1. If we observe a single pair of data points where $x_1 = 0$ and $x_2 = 4$ , we should now be very convinced that $\mu_1 and stop the sequential analysis. Note that our inference on $\sigma$ is only from the prior! On the other hand, if we had waited until we had 100 data pairs, we at least have the chance to let the data tell us that our strong prior on $\sigma$ was not justified. To be clear, I think sequential analyses are a very good idea. But there are downsides.
