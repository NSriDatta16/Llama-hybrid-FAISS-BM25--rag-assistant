[site]: crossvalidated
[post_id]: 410205
[parent_id]: 
[tags]: 
Understanding K-fold Cross Validation

I have a doubt regarding the cross validation approach and train-validation-test approach. I was told that I can split a dataset into 3 parts: Train: we train the model. Validation: we validate and adjust model parameters. Test: never seen before data. We get an unbiased final estimate. So far, we have split into three subsets. Until here everything is okay. Attached is a picture: Then I came across the K-fold cross validation approach and what I don’t understand is how I can relate the Test subset from the above approach. Meaning, in 5-fold cross validation we split the data into 5 and in each iteration the non-validation subset is used as the train subset and the validation is used as test set. But, in terms of the above mentioned example, where is the validation part in k-fold cross validation? We either have validation or test subset. I would like to cite this information from https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7 Training Dataset Training Dataset: The sample of data used to fit the model. The actual dataset that we use to train the model (weights and biases in the case of Neural Network). The model sees and learns from this data. Validation Dataset Validation Dataset: The sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. The evaluation becomes more biased as skill on the validation dataset is incorporated into the model configuration. The validation set is used to evaluate a given model, but this is for frequent evaluation. We as machine learning engineers use this data to fine-tune the model hyperparameters. Hence the model occasionally sees this data, but never does it “Learn” from this. We(mostly humans, at-least as of 2017 ) use the validation set results and update higher level hyperparameters. So the validation set in a way affects a model, but indirectly. Test Dataset Test Dataset: The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset. The Test dataset provides the gold standard used to evaluate the model. It is only used once a model is completely trained(using the train and validation sets). The test set is generally what is used to evaluate competing models (For example on many Kaggle competitions, the validation set is released initially along with the training set and the actual test set is only released when the competition is about to close, and it is the result of the the model on the Test set that decides the winner) . Many a times the validation set is used as the test set, but it is not good practice . The test set is generally well curated. It contains carefully sampled data that spans the various classes that the model would face, when used in the real world. I Would like to say this: Taking this into account, we still need the TEST split in order to have a good assement of our model. Otherwise we’re only training and adjusting parameters but never take the model to the battle field Thank you! Check this splitting:
