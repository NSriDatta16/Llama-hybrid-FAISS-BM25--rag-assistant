[site]: crossvalidated
[post_id]: 384814
[parent_id]: 
[tags]: 
How sensitive is $L_p$ regression to initialisation?

Consider that I wish to solve a linear regression in the $L_p$ framework That is, the optimisation problem that I wish to solve is $$ \mathbf{w} = \text{argmin}_{\mathbf{w}} ||\mathbf{w}^T\mathbf{x} - \mathbf{y}||_2^2 + \lambda ||\mathbf{w}||_p $$ From a frequentist perspective, $ \lambda $ could be most easily chosen using a grid search cross-validation approach. $P$ is a preset constant, for the simplicity of the discussion, I wish to scope my question to $ p \leq 1 $ , as $ p = 2 $ has closed form solution. The problem with this approach is that CV comparison only makes sense if the minimisation problem is actually solved. As I know, this is not the case, rather a local optima is obtained, and the number of local optimas increase as $ p \rightarrow 0 $ . In scikit-learn, the coordinate ascent algorithm is chosen to solve the $ L_1 $ problem. I assume it is possible to extend this to the $ L_p $ framework. As I know, coordinate ascent will give a locally optimal solution. This means that trying different initialisations for $ \mathbf{w} $ would lead to different local optima. For example analogously, with neural networks, there are the Xavier and He initialisations which sometimes lead to superior solutions. For this problem, is there any standard way to initialise $ \mathbf{w} $ , and a reason why that should be used?
