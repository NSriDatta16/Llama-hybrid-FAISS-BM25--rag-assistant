[site]: datascience
[post_id]: 19101
[parent_id]: 19099
[tags]: 
I agree with the comments on your question that you should look into a course, maybe Andrew Ng's Machine Learning on Coursera , which is a highly regarded, free introductory course. This is a basic question about fundamentals of machine learning. As such I am not covering the maths in this answer, you can get that from many places, including that course. where and how the values for Bias and Weight are determined? Weights and biases are the learnable parameters of your model. As well as neural networks, they appear with the same names in related models such as linear regression. Most machine learning algorithms include some learnable parameters like this. The values of these parameters before learning starts are initialised randomly (this stops them all converging to a single value). Then when presented with data during training, they are adjusted towards values that have correct output. Do we have to provide these values or does the TensorFlow library calculates these values automatically based on the training data set? You do not need to provide values before training, although you may want to decide things such as how many parameters there should be (in neural networks that is controlled by the size of each layer). TensorFlow calculates the values automatically, during training. When you have an already-trained model and want to re-use it, then you will want to set the values directly e.g. by loading them from file. The specific code that handles changes to weights and biases from the tutorial is this: train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy) and this: sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys}) The first line defines how the weights and values will be changed. You can read this almost literally as "define a training function that uses the gradient descent optimizer to reduce the cross entropy of the supplied data". The second line invokes that function with a specific piece of data. Each time this second line is run, the weight and bias values are adjusted so that neural network outputs $y$ values a little bit closer to the correct association for each $x$ value.
