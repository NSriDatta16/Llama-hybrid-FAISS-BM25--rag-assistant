[site]: crossvalidated
[post_id]: 508538
[parent_id]: 508530
[tags]: 
According to the documentation for numpy.random.geometric , The geometric distribution models the number of trials that must be run in order to achieve success. It is therefore supported on the positive integers, k = 1, 2, .... this means that z-1 should used as the number of repetitions of the current state, before moving to the other state. I thus wonder at the use of Y[i, t+1: t+z-1] when z=1 and at the replacement by Y[i, t+1: t+z] which seems to always include a repetition of the current state, while it should not when z=1 . Since the Markov chain is a sequence of 0 and 1 , as eg 0100100010111010111001 updating the Markov chain one position at a time or updating the uninterrupted blocks of 0 and 1 all at once are equivalent. As noted in the question, when at a state 0 at time t, the number of subsequent 0 till the next 1 is a indeed Geometric random variable $\mathcal Geo(1-\alpha)$ [when defined as taking values in $\mathbb N$ , i.e. zero is a possible value]. Hence generating using Geometric batch lengths rather than going one time step at a time is equivalent from a probability perspective (if not producing the same output when implemented on a computer). I ran both ways of simulating the Markov chain in R and did not spot any discrepancy in the ergodic convergence of the chain average to the stationary mean $\frac{1-\alpha}{1-\alpha+1-\beta}$ (equal to $1/3$ with the chosen values): T=1e5 a=.5;b=.75 x=1:T for(t in 2:T) x[t]=(runif(1)>ifelse(x[t-1],1-b,a)) plot(cumsum(x)/(1:T),type="l",col="gold") t=1 while(t
