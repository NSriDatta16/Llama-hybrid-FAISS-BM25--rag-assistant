[site]: datascience
[post_id]: 109892
[parent_id]: 38696
[tags]: 
Normalization Normalization is a data preparation technique that is frequently used in machine learning. The process of transforming the columns in a dataset to the same scale is referred to as normalization. Every dataset does not need to be normalized for machine learning. It is only required when the ranges of characteristics are different. When you don’t know the distribution of your data or when you know it’s not Gaussian , normalization is a smart approach to apply. Normalization is useful when your data has variable scales and the technique you’re employing , such as k-nearest neighbors and artificial neural networks, doesn’t make assumptions about the distribution of your data. Four common normalization techniques may be useful: scaling to a range clipping log scaling z-score Refer to this summary table from Google Developer's Data Preparation and Feature Engineering for Machine Learning for choosing the right Normalization Technique. Normalization Technique Formula When to Use Linear Scaling $x^′= \frac{(x−x_{min})}{(x_{max}−x_{min})}$ When the feature is more-or-less uniformly distributed across a fixed range. Clipping if $x$ > max, then $x' = max$ . if $x , then $x' = min$ When the feature contains some extreme outliers. Log Scaling $x' = log(x)$ When the feature conforms to the power law. Z-score $x' = (x - μ) / σ$ When the feature distribution does not contain extreme outliers. For Detailed Explanation check here
