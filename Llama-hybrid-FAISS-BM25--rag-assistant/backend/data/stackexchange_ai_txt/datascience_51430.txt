[site]: datascience
[post_id]: 51430
[parent_id]: 
[tags]: 
Intuition behind the loss function in Deep Q learning?

I'm currently following a tutorial but I got stuck at the deep Q learning model. According to my understanding of neural networks they predict an approximate function for the inputs given with the help of the loss value, but in the deep Q case, the author of the tutorial said the loss is calculated as Q_target - Q. He told the Q_target is the Q value from the previous iteration. My doubt is if we're using Q value from old iteration to calc lot loss how is the network converging to predict a bellman like equation. Upon asking this doubt he changed his stand, he told that they use the target networks. Each different tutorial I'm trying to refer has a different notion for the Q_target and the loss function. That's why I'm confused. If we're using the "previous iteration's Q value - current Q value" as loss won't our network try to predict the model that predicts values that are closer to the previous iteration's Q values? And moreover for each and every epoch while training for the same state the Q_target will vary each time, so will the network predict a stable model? Can anyone help me by telling me the actual loss function?
