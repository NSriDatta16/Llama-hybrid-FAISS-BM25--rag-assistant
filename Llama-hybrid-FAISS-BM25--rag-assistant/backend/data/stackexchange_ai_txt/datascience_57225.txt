[site]: datascience
[post_id]: 57225
[parent_id]: 
[tags]: 
Choosing weights on random forest for imbalanced data with the aim to minimize false positives

I am currently dealing with a binary classification task on imbalanced data with the following distribution: y_train: 4981 positive / 863894 negative samples y_test: 128 positive / 128309 negative samples The goal is to aim for a high precision (as little false negatives as possible). How do I go on about choosing the weights for the random forest? I tried to balance out the y_train ratio by assigning weight 1 to "negative" and 173 to "positive", but that still caused all the samples to be assigned to negative. At this point, should I already consider this a problem with the features used, or should I try to assign higher weights with GridSearch ( roc_auc as scoring parameter) and set the decision-threshold higher first?
