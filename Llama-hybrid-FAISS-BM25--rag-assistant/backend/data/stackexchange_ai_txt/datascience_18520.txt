[site]: datascience
[post_id]: 18520
[parent_id]: 18518
[tags]: 
There are many factors which are underlying the 'importaces' of featues obtained from random forest. For instance, features with more number of categories (unique values if its a numerical feature) would be more likely to get find splits; making it more important feature. Having all features with same number of categories is not a common scenario. So, even though the features identified are likey to be the best predictors (in logistic regressions etc.), one need to take the inherent biases in the randome forest algorithm into consideration while utilizing the 'important' features. Therefore whether it would make sense to use the important features from random forest model into logistic regression would depend on if any and how much the importances are biased. Source for the information and further useful details on the biases in estimation of feature importances by random forest algorithm: Bias in random forest variable importance measures: Illustrations, sources and a solution Bias in random forest variable importance measures: Illustrations, sources and a solution
