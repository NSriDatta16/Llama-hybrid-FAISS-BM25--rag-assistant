[site]: crossvalidated
[post_id]: 301185
[parent_id]: 
[tags]: 
Model selection in linear regression

I got some homework feedback recently, and I don't understand what I did wrong. I had to select 2,5 and 7 feature models, and couldn't perform stepwise regression (since it had to be done using PySpark, and I didn't see an implementation for that at the time). I ended up constructing these models using Pearson's correlation (i.e. a model with the 2, 5 and 7 most correlated with the target variable). These are my results: //The features were normalized prior to constructing the model, but the target variable was not. 2: Explained variance: 1198100424.86 Mean Absolute Error: 5596.52622937 Mean Squared Error: 243471938.055 Root Mean Squared Error: 15603.5873457 R^2: 0.831111138209 5: Explained variance: 1193222675.73 Mean Absolute Error: 5830.54480402 Mean Squared Error: 245313823.722 Root Mean Squared Error: 15662.4973654 R^2: 0.82983347978 7: Explained variance: 1191909326.29 Mean Absolute Error: 5915.68243845 Mean Squared Error: 242263942.387 Root Mean Squared Error: 15564.8303038 R^2: 0.831949086989 cross validation was not a requirement here - since it's not a project but just a simple HW assignment (to practice ML models in PySpark). We were asked to compare the models - in my opinion, there's no real difference between these models. If I'd had to choose one I would choose the one with the 2 features since it has less complexity. However, I was told that it's common practice to choose a metric and "run" with it even if the differences are minuscule. Additionally, I was told that I had to add a discussion in my HW regarding the fact the MSE gets bigger the more variables I add. Am I wrong for thinking these models perform the same? Is the MSE really that different between the models that it requires special attention?
