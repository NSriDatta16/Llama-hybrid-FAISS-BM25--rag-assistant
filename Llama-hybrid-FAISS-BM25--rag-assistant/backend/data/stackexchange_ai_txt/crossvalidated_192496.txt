[site]: crossvalidated
[post_id]: 192496
[parent_id]: 192493
[tags]: 
Each data point (e.g., tablet: 8 ) is actually the number remembered correctly out of a finite total possible words ( 10 ). Numbers like these are binomial. They cannot be normal, and although they might be 'normal enough' in some circumstances, they won't be in yours. A common analysis with data like these would be to run a logistic regression. It is possible to do that with paired data, but it requires some fancier machinery. If you're new to statistics, you might appreciate something simpler. A nonparametric test like the Wilcoxon signed rank test might be appropriate. In essence, you are testing if one number tends to be higher than the other, but ignoring by how much. You are only using the ordinal information in the data. Here is an example, coded in R : d = read.table(text="8 8 8 6 10 6 4 10 10 6 4 6 10 9 10 9 9 10 9 7 10 10 10 9 9 10 9 10 10 6 4 8 9 10 10 10 10 10 3 9") d = as.data.frame(t(as.matrix(d))) names(d) = c("tablet", "book") rownames(d) = NULL with(d, wilcox.test(tablet, book, paired=TRUE)) # Wilcoxon signed rank test with continuity correction # # data: tablet and book # V = 20, p-value = 0.04246 # alternative hypothesis: true location shift is not equal to 0 # # Warning messages: # 1: In wilcox.test.default(tablet, book, paired = TRUE) : # cannot compute exact p-value with ties # 2: In wilcox.test.default(tablet, book, paired = TRUE) : # cannot compute exact p-value with zeroes The results are significant, but it complains about the ties in the data. If that bothers you, you could try a permutation test. The idea is just to shuffle the rows independently of each other on every iteration for lots of iterations to determine the null distribution of the Wilcoxon signed rank test statistic. Then we can compare your observed test statistic to that distribution. d2 = as.matrix(d) v.vect = vector(length=1000) nd = matrix(NA, nrow=20, ncol=2) set.seed(7998) for(i in 1:1000){ for(j in 1:20){ nd[j,] = sample(as.vector(d2[j,]), size=2) } v.vect[i] = wilcox.test(nd[,1], nd[,2], paired=TRUE)$statistic } summary(v.vect) # Min. 1st Qu. Median Mean 3rd Qu. Max. # 6.00 40.00 52.00 51.84 63.00 94.00 2*mean(v.vect It is still significant. In the original dataset, people remembered more words from the book than the tablet 55% of the time, more from the tablet 15% of the time, and equal from both 30% of the time. That is rather unlikely to occur when words from books and tablets are equally memorable. I don't know about the book request. @StatsStudent's suggestion seems reasonable. On the other hand, there may be some issues with this study. Namely, I gather the words and paragraphs differ between the conditions, which is a problem. It might just be that the words and paragraphs used in the book are more memorable. What the exhibitor could have done is have 10 people test with one set of words in the book and the other 10 on the tablet, and switch the words for the last 10 people. Then the words and the conditions would be counterbalanced, and would not be confounded. That makes it possible to make a claim about the conditions without worrying that the effect is really due to the words.
