[site]: datascience
[post_id]: 16609
[parent_id]: 
[tags]: 
Benefits of stochastic gradient descent besides speed/overhead and their optimization

Say I am training a neural network and can fit all my data into memory. Are there any benefits to using mini batches with SGD in this case? Or is batch training with the full gradient always superior when possible? Also, it seems like many of the more modern optimization algorithms (RMSProp, Adam, etc.) were designed with SGD in mind. Are these methods still superior to standard gradient descent (with momentum) with the full gradient available?
