[site]: crossvalidated
[post_id]: 511212
[parent_id]: 
[tags]: 
Bayesian Prediction Simple Explanation

I have read through other questions on the site and I feel that none provide a great answer to this question. Simply put - could anyone explain, common approaches for generating predictions on new unseen data with a Bayesian model. For example, suppose you have a simple linear regression: $$ y = \alpha + \beta * x $$ Which under a probabilistic framework is given by: $$ y \sim N(\mu = \alpha + \beta *x, \sigma = \epsilon) $$ We set prior distributions for the parameters $ \alpha, \beta$ and $\epsilon$ and with our training data $X_{train}$ we return samples from posterior to obtain posterior distributions across these parameters. My question is, given new data $X_{new}$ , how are predictions then made on that data? For example, would you: For each new data point in $X_{new}$ , take the posterior samples from the 'training' process, and calculate $Y_{new}$ using combinations of coefficients from the posterior samples (for a given data point in $X_{new}$ ) - to generate a vector of predictions for each data point in $X_{new}$ ?
