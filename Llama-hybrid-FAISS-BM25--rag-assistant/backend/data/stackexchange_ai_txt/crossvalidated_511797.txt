[site]: crossvalidated
[post_id]: 511797
[parent_id]: 511777
[tags]: 
I don't think an entire data presentation is needed to give some intuition behind this phenomenon. While we would expect that a logistic regression and OLS model will, on average, produce parameter estimates (slopes or log-odds ratios) that are similar sign, it's entirely possible they will disagree for a given dataset and analysis. The most likely issue is the influence of observations in the tails. Supposing we have an X that follows a standard normal density, and a modest positive trend in the risk of outcome for greater values of X. The influence of a single observation at (X=5, Y=0) will be far, far greater in a logistic regression model than in a linear regression model. That is because the influence of such an observation in a logistic model can be arbitrarily high. If the fitted value in the logistic model is 0.9, then the Pearson residual will be $(0-0.9)^2/\left( 0.9*(1-0.9) \right)\approx 9$ compared to the linear model where the residual is $0.81$ . The other issue is overadjustment. Due to non-collapsibility of the odds ratio, if (as you say) "several hundreds" of variables are input to the linear model, and suppose several of those variables are unrelated to the outcome, the tendency will be for the OR of effect to attenuate with larger adjustment. Then it becomes likely that the OR will spontaneously flip effect due to random perturbations along the lines of what's pointed out above - again, in multivariate adjustment, it can be difficult to understand the contribution of a single variable, so using the DF-beta diagnostic is a prudent choice.
