[site]: datascience
[post_id]: 110486
[parent_id]: 110481
[tags]: 
There is not an optimal answear to this question, however let me shade some lights on how I have previously used these methods. I work with large amount of features and with different type of models. As you may already know there are linear and non linear models. Linear models perform well when feature selection is done on the most basic level. However models like random forest or even xgboost give you the opporunity to let more features in. I have used both at the same time as steps. Apply Kbest. If I still have a lot of features, apply VIF to reduce even more. From experience, when using Boosted models, you don't really need VIF since these type of models know how to deal with multi-colinearity. I apply VIF only when I use linear regression models. It really depends on what model you are going to feed those feautures.
