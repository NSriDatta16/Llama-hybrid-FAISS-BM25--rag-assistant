[site]: datascience
[post_id]: 80494
[parent_id]: 80488
[tags]: 
So the question asks how to represent a series of words a uniform vector representation, which is not dependent on sequence. The idea you suggested is definitely not a bad idea, you should try that out. You should also try Doc2Vec, which works on the same principle as Word2Vec but this time it will output a vector which represents the meaning of a section of text that is longer than 1 word. The main problem with this sort of representation is that you lose sequential information of the text and you treat words in sentences as a “bag of words”. If you happy to make that assumption, then continue using your approach of Doc2Vec. Otherwise, you might better off using a sequential model architecture, such as RNN/LSTM, etc. Here you can input each word at a given time step initially as a one hot encoded vector and then you add an embedding layer before it goes into the sequential model to transform the one hot encoding into a word embedding.
