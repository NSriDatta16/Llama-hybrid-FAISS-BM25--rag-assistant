[site]: datascience
[post_id]: 69603
[parent_id]: 
[tags]: 
Logistic Regression vs SVM

Following Andrew Ng's machine learning course, he explains how we can modify logistic regression to obtain SVM algorithm. First he replaces (sort of approximating) cross entropy loss with hinge loss as shown in the image below: Then he removes the $\frac{1}{m}$ coefficient and divides the whole cost function by the regularization parameter $\lambda$ , which leads to parameter $C$ behind the sigma ( $C=\frac{1}{\lambda}$ ). Resulting cost function is shown in image below which is the minimization objective in SVM: Later he shows that by choosing a very large value for parameter $C$ , SVM will be a large margin classifier . Large value for parameter $C$ is the same as choosing a small value for parameter $\lambda$ in logistic regression. So, is logistic regression with a small value for $\lambda$ also a large margin classifier?
