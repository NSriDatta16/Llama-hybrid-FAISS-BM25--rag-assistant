[site]: crossvalidated
[post_id]: 612420
[parent_id]: 612418
[tags]: 
Some of the trouble with this question is that many of the "metrics typically associated to measure the performance of predictive modelling" are applied inappropriately, such as assessing the classification accuracy with no regard for how those classifications are created and if a software-default threshold (usually $1/2$ ) is appropriate. That said, I have seen plenty of papers (published in the elite journals of their respective fields) where the ultimate goal is causal inference on the coefficients, yet the logistic regression models have standard metrics calculated like McFadden's $R^2$ or classification accuracy. For instance, Sundaram & Yermack (2007) report classification accuracy in their table 6 despite the main purpose of running those logistic regressions being the coefficient inference. (My take is that they made a mistake in doing so, because one of their models reports classification accuracy worse than would be achieved by predicting the majority class every time.) On the other hand, I recently saw another paper that had $R^2_{adj} all over the place. That their regression models were, arguably, doing worse than doing no modeling at all, helped me form a rationale for being skeptical of their results (there were other issues with the statistics). Thus, to explictly address your question , it can be helpful to give some sense of model performance, no matter how modest, even if prediction is not the main goal of the work. While a reviewer/professor/customer might not require it and oblige you to report it, I still believe it to be valuable information. In such a case, it is typically acceptable to editors and referees to have rather modest performance in terms of the performance metrics. For instance, I have seen papers in top journals with adjusted $R^2$ around $0.05$ , maybe even lower. Even that Sundaram & Yermack paper has rather pedestrian performance when accuracy scores are compared to na√Øvely predicting the majority category every time . (Most of their $R^2$ -style scores, defined as I do in that link, are greater than zero (one is less than zero), but they do not scream out, "This model gets an $\text{A}$ ," the way that a classification accuracy of $97\%$ might.) REFERENCE Sundaram, Rangarajan K., and David L. Yermack. "Pay me later: Inside debt and its role in managerial compensation." The Journal of Finance 62.4 (2007): 1551-1588.
