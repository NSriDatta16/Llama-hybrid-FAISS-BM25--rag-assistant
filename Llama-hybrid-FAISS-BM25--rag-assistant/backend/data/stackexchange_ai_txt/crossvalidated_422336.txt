[site]: crossvalidated
[post_id]: 422336
[parent_id]: 
[tags]: 
Orthonormal regularizer to encourage diverse or non-redundant model parameters in neural networks

I was recently reading the paper Nian, F., Chen, X., Yang, S., & Lv, G. (2019). Facial Attribute Recognition With Feature Decoupling and Graph Convolutional Networks. IEEE Access, 7, 85500-85512. ( https://ieeexplore.ieee.org/abstract/document/8747501/ ) and can't quite follow their method for encouraging orthonormal (or even orthogonal) features by constraining the model weights. I inserted the relevant section below: I am not quite sure how that approach is ensuring the the model parameters in $\theta_F$ are going to be orthogonal if that constraint is added to the loss function. I.e., suppose we have model parameters $$ \theta_{f} = \begin{bmatrix} a & c \\ b & d \end{bmatrix}. $$ Then $$ \theta_{f}^\top \theta_{f} = \begin{bmatrix} a \cdot a + b \cdot b & a \cdot b + c \cdot d\\ b \cdot a + d \cdot c & c \cdot c + d \cdot d \end{bmatrix}. $$ And the whole expression would be $$ \lambda \bigg|\bigg| \begin{bmatrix} a \cdot a + b \cdot b & a \cdot b + c \cdot d\\ b \cdot a + d \cdot c & c \cdot c + d \cdot d \end{bmatrix}. - \begin{bmatrix} 1 & 0\\ 0 & 1 \end{bmatrix}\bigg|\bigg|^{2}_{F}.$$ Now, I can see that if the covariance matrix instead of $\theta_{f}^\top \theta_{f}$ was used how this would enforce an orthogonal constraint. But with the dot product, I am a bit lost. Am I missing something?
