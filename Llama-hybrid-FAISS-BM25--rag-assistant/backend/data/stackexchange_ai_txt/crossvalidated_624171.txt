[site]: crossvalidated
[post_id]: 624171
[parent_id]: 
[tags]: 
Unscrambling the pixels of a GoPro

I'm curious if there are any algorithms that solve this type of problem: Suppose you have a camera that captures what a human would see, a GoPro for instance. But suppose I uploaded video from it to my computer, and I see that all of the pixels are scrambled, every pixel teleports to a uniformly random location. If a pixel gets moved in one frame of the video, it should move to the same coordinates in every frame of the video. And suppose I have quite a lot of video of varying environments, but mostly it's the type of things that humans would see on a day-to-day basis. How would you go about unscrambling this? My intuition says that you should find colors that are similar in the image and gradually pull them together. For example, suppose I'm looking at a grassy plain and the sky. Pulling together the blue with the other blue and the green with the other green should get the pixels more aligned. Or maybe you could pay special attention to colors that are rare. For instance, suppose my camera is pointed at the sun. Pulling together those pure white pixels should help me get closer to the answer. Sidenote, suppose this works, the image could still be flipped or rotated, squashed or stretched when unscrambled, it might be a challenge to get past that stage, would be very curious about possible insights about that. I ask because the human brain operates on the "neurons that fire together wire together" principle. If you take a nerve from one finger and put it on another one, the signal unscrambles itself eventually and you still get the correct touch sensation. I feel like it might be possible that ideas behind an algorithm that would solve this might be able to do something like create specialized regions in deep learning networks, similar to how humans have specialized areas in their brains. Maybe not, but just a hunch. Thanks for reading! :)
