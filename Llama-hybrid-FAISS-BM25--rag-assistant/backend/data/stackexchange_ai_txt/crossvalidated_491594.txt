[site]: crossvalidated
[post_id]: 491594
[parent_id]: 
[tags]: 
Notation predictive posterior distribution and $x^*$, $y^*$

I often see the posterior predictive distribution in ML defined as follows: $$p(y^* \mid x^*, X, Y) = \int p(y^* \mid x^*, \omega)p(\omega, X, Y) d\omega$$ where $\omega$ are all parameters, $x^*$ is a new input point and $X, Y$ is the training dataset. What confuses me is the lower case $y^*$ and $x^*$ , because I am not sure whether it is a random variable and where it comes from. Without knowing a lot about Bayesian statistics, I would first define the posterior $P(W \mid X, Y)$ (with $W$ being the parameters). Then use the law of total probability to obtain $$P(Y \mid X) = \int P(Y \mid X, W)P(W)dW$$ Next, when I get a new point $x^*$ , I would set $P(Y= y^* \mid X = x^*)$ . Are $y^*$ and $x^*$ as random variables necessary?
