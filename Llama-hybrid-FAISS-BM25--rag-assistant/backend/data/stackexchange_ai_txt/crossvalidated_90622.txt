[site]: crossvalidated
[post_id]: 90622
[parent_id]: 
[tags]: 
Regression model where output is a probability

I am trying to fit a model, and have the suspicion that what I am doing is not quite right. The data tracks what proportion of people made a decision, and what factors were active when they made their decision, i.e. something like this: 1, 0, 1, 0, 23% 1, 1, 0, 1, 41% etc... I also know how big each group is. The goal is to predict the percentage, based on the binary input. My initial thought was, the model cannot be a straight linear combinations, if only because the output is bounded. It is not exactly a logistic regression either, because the output is not a label, but the average for each group. My first take has been to transform the output in a fashion similar to logistic regression, into $\log \left(\frac{p}{1-p}\right)$ , and fit a linear regression. This has given me some decent results, but I have a nagging feeling this is not quite right. Besides the output transformation, I am also concerned that treating the input as numbers, when they really represent binary values, is probably not the best way to go. So my question is, if this is not the right way to go, are there models that address this specific type of situation? What should I be looking for? Edit for clarification From the comments / responses it seems my description of the data was a bit lacking, so here is a bit more, as well as why I am uncertain about using logistic regression. I'll illustrate on similar data. Suppose products had a set of binary characteristics, and various products were presented to customers, and the result (buy/no buy) was recorded. Then the raw dataset would look like: F1, F2, F3, …, Fn, Buy/No Buy 1, 1, 0, …, 0, 1 1, 1, 1, …, 1, 0 where each row is a specific product, and what the customer did. Now I can aggregate these by identical products, which will have the same characteristics, and simply record the proportion of buys, as well as the number of customers that were presented with that choice. This is essentially what I have. I could disaggregate back into the original data set, and run a logistic regression on it, but the groups themselves are very large, and of very different sizes. On top of that, I have 2 problems. First, I could reconstruct synthetic groups that have the same proportion as the original (e.g., if 4% purchased, construct 4 Buy, 96 No Buy rows), but the the Buy/No Buy ratios are pretty small, which would mean reconstructing large groups to properly approximate. Second, the groups have very different size, and I believe the group composition in the complete sample should have similar composition to the original one, which would mean creating potentially very large groups. Which is why I was essentially wondering if there was a way to work off the much smaller data set directly, without having to reconstruct an artificial gigantic data set. My current approach has been to use a gradient descent approach, weighting observations by the group sizes, but I was wondering if there was a smarter way to handle this!
