[site]: datascience
[post_id]: 94198
[parent_id]: 
[tags]: 
Best Method for Using Experts to Score Qualitative Data

TL;DR: Looking for recommendations or resources on the best method to have subject matter experts score qualitative data to train ML model. Problem: I am working on a problem in the biochemistry domain. I have a set of ~500 protocol results that need to be given a quantitative score. Historically the biochemists just review each protocol and judge if the result is satisfactory or not. Data: The value being evaluated is the quality of a SNP allelic discrimination plot (sorry for the jargon, it actually not that important to the question). Each allele is represented as a cluster of samples. A good result has dense clusters with good separation between. A poor result has clusters that spread out. Note for this particular application, all data points in the plot are known, so no clustering algorithms are needed (e.g. k-means). PoC Model: For a proof of concept model, I gave each protocol result is my data set a binary pass/fail value (note I'm a non-expert on passable SNP plots, so my scoring was a bit capricious). For feature engineering, I calculated various cluster separation metrics (Davies-Bouldin, Calinski-Harabasz, Silhouette) and did Box-Cox transforms (lambdas all near 0) to get normalized distributions. All three metrics are highly correlated, so I applied PCA and used the 1st PC (95% variability explained). I then fit a logistic regression curve, using the 1st PC as the variable and my manual pass/fail scoring values as the response. I was able to achieve 90% accuracy on test data, which is promising. I then verified on a new data set and reviewed it with the science team, and the results showed promise. Now I have 4 real biochemists that are willing to score/grade my data set to use as the true expert scores. Question: So my actual question. So what would be the best method for the subject matter experts to score the quality of the SNP plots? I have scoured the interwebs for guidance on this problem, but haven't had much luck. Mulling over the issues, I have 3 ideas so far. Binary Score: Simply have them score pass/fail for each plot. Limits me to classification and logistic regression models. Numeric Score: Score 1-10. Values >5 are passing. The big issue here is score "drift" as each reviewer goes through the data set. An 8 at the beginning of scoring may match a 6 at the end of the scoring process. End users really want the model to create a score, so looking at maybe doing regression models (although I think I sold them on using the logistic regression model probability of a "good SNP" as a score.) Rank Score: Force rank each protocol relative to the others for a 1-500 score. Would also include the binary score to find the subgroup of rankings that pass. Implementation would be to show a pair of plots side-by-side and have the review pick the best. Use sorting algo in the background (merge sort?) to keep iterations down to O(n * log(n)). Not sure the best tool to implement this - maybe Dash+Plotly?
