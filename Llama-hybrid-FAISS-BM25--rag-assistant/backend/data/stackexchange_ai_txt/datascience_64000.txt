[site]: datascience
[post_id]: 64000
[parent_id]: 63953
[tags]: 
If you had two different functions $z_1$ and $z_2$ , I would agree with Piotr: you could have two parallel networks, one for each part of the input, that join together at the end. This would minimize (though not eliminate) information from $x_{i,1}$ bleeding into the fitting of $z_2$ . But with $z_1=z_2=z$ , we'd like to enforce that those two parallel networks are the same. The idea of using the same weights for different parts of the input remind of convolutions, and I think that will work here: let $x_{i,1},x_{i,2}$ be the columns of a $3\times 2$ input, apply 1-dimensional width-1 convolutional layers instead of dense ones, and add a "Sum Pooling" layer at the end. (Without writing something custom, you could manage that in Keras with average pooling and manually deal with the factor of 2.)
