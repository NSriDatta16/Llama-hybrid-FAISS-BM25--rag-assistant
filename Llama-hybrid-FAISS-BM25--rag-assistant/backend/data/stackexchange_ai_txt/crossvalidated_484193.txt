[site]: crossvalidated
[post_id]: 484193
[parent_id]: 484184
[tags]: 
Finding iteratively the best analytical model that fits data that has an error term is acceptable within the constraints nicely explained in the article you quote . But perhaps what you are asking is what is the effectiveness of such model when you use it to predict out-of-sample data that was not used to generate the model. If it is reasonable to assume that the data generating mechanism used to calculate the model and the mechanism that generates the new data are the same, there is nothing wrong with using the model you obtained. But you may have some justifiable scepticism about this assertion which goes to the essence of frequentist statistics. As you develop the model, you obtain the parameters that best fit the data. To get a better model you add more data. But that does not help if you add data points that you do not know whether they belong to the same data-generating mechanism used to develop the model. Here the issue is one of belief about how likely it is for the new data point(s) to belong to the same mechanism. This takes you directly to Bayesian analysis by which you determine the probability distribution of the parameters of the model and see how this distribution changes as you add more data. For an introductory explanation of Bayesian analysis see here . For a nice explanation of Bayesian regression see here .
