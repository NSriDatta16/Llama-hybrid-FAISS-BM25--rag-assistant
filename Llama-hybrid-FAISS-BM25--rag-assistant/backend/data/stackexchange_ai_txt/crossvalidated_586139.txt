[site]: crossvalidated
[post_id]: 586139
[parent_id]: 145768
[tags]: 
AlexNet also uses a competitive normalization step immediately after the ReLU step of layers C1 and C3, called local response normalization (LRN): the most strongly activated neurons inhibit other neurons located at the same position in neighboring feature maps (such competitive activation has been observed in biological neurons). This encourages different feature maps to specialize, pushing them apart and forcing them to explore a wider range of features, ultimately improving generalization. Equation 14-2 shows how to apply LRN. Equation 14-2. Local response normalization (LRN) In this equation: $b_i$ is the normalized output of the neuron located in feature map $i$ , at some row u and column v (note that in this equation we consider only neurons located at this row and column, so u and v are not shown). $a_i$ is the activation of that neuron after the ReLU step, but before normalization. k, α, β, and r are hyperparameters. k is called the bias, and r is called the depth radius. $f_n$ is the number of feature maps. For example, if r = 2 and a neuron has a strong activation, it will inhibit the activation of the neurons located in the feature maps immediately above and below its own. In AlexNet, the hyperparameters are set as follows: r = 5, α = 0.0001, β = 0.75, and k = 2. This step can be implemented using the tf.nn.local_response_normalization() function (which you can wrap in a Lambda layer if you want to use it in a Keras model). A variant of AlexNet called ZF Net12 was developed by Matthew Zeiler and Rob Fergus and won the 2013 ILSVRC challenge. It is essentially AlexNet with a few tweaked hyperparameters (number of feature maps, kernel size, stride, etc.). GoogLeNet[![enter image description here][2]][2] The answer is from this book Hands-On Machine Learning with Scikit-Learn, Keras, and Tensorflow: Concepts, Tools, and Techniques to Build Intelligent Systems
