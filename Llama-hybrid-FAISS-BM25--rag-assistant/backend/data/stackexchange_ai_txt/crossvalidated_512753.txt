[site]: crossvalidated
[post_id]: 512753
[parent_id]: 512750
[tags]: 
Dichotomozing and binning continuous variables is a poor poor idea. Not only are the splits more or less arbitrary, but splitting continuous variables leads to residual confounding and decreases the degrees of freedom in the model (maybe not a problem in your particular example, you likely have oodles of data). This can lead to poorer measures of model performance (like calibration). Additionally, splitting the predictors can fool the analyst into believing noise is signal. Here is an example. Below, I've simulated binary data and a continuous predictor. I've binned the predictor into 5 bins and computed the means in each bin. When I simulated that data, I made sure that as the predictor increased then so too would $E(y \vert x)$ . However, when I plot the binned data, I see a different story. It would appear the data have a non-linear effect -- something that would fail to generalize to new data. You seem to be vexed by apparent confusion in the interpretation of model coefficients. Without sounding flippant, I would assume it is part of your job to explain to stakeholders what these coefficients mean, rather than leaving them to do it. If the insight you mention is hidden, it might be worthwhile to elucidate it. Alternatively, you can eschew model coefficients completely as their effect is somewhat obfuscated and depends on the baseline risk. A log odds ratio of 2.0 be associated with a massive risk difference if the comparitor's risk is moderate, or it can be associated with a paltry risk difference if the outcome is certain (near 0 or 1). Consequently, I usually report probabilities of various scenarios to try to communicate how interventions might affect risk. For example, I might say something like "If customers had one more item in their basket on average, we might expect a p% change in probability of sign up from x% to x+p%". This might even be a good opportunity to do some visualizing. Because you can not realistically intervene and change predictors in an enormous way, I find this approach sufficient. All in all, needlessly categorizing hurts rather than helps. If you're having trouble explaining model coefficients, my advice would be to take a different approach and report model probability estimates under various scenarios. Skip coefficients entirely.
