[site]: crossvalidated
[post_id]: 255390
[parent_id]: 255097
[tags]: 
As in comment by @Batman, sequential MNIST is explained in section 4.3 of your link: https://arxiv.org/abs/1610.09038 . We evaluated Professor Forcing on the task of sequentially generating the pixels in MNIST digits. As far as I am aware, sequential MNIST always implies the model does not get to see/generate the whole image at once (like for example a normal 2d-ConvNet would), but only one pixel at a time sequentially . So sequential MNIST should have the same meaning also in other non-generative contexts. Also in section 4.3. they explain permuted mnist: Applying a fixed random permutation to the pixels makes the problem even harder but IRNNs on the permuted pixels are still better than LSTMs on the non-permuted pixels. The problem should be harder after permuting the pixels in all images with the same permutation, because you have to learn more long-range patterns: Distinctive shapes, like the horizontal bar of the 7 are typically more spread apart in the input after permutation compared to before.
