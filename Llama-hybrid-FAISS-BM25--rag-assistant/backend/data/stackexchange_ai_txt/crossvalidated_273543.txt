[site]: crossvalidated
[post_id]: 273543
[parent_id]: 273473
[tags]: 
Each $X_i$ comes from either one of the two lognormals with probabilities $p$ and $1-p$. Let $Z_j$ be the number of $X_i$'s in the sum $S_j=\sum_{i=10j-9}^{10j}X_i$ that comes from the first lognormal. Clearly $Z_j \sim \mbox{bin}(10,p)$. Conditional on $Z_j$, each $S_j$ is a sum of $Z_j$ lognormals with parameters $\mu_1,\sigma^2$ and $10-Z_j$ lognormals with parameters $\mu_2,\sigma^2$, with pdf $$ f_{S|Z=z}(s;\mu_1,\mu_2,\sigma^2), \tag{1} $$ given by a complicated convolution integral. The unconditional distribution of each $S_j$ is the 11-component mixture $$ f_S(s;\mu_1,\mu_2,\sigma^2,p)=\sum_{z=0}^{10}{10 \choose z}p^z(1-p)^{10-z}f_{S|Z=z}(s;\mu_1,\mu_2,\sigma^2). \tag{2} $$ If $\sigma^2$ is sufficiently small or $\mu_1$ and $\mu_2$ sufficiently different this is going to be a 11-modal distribution with modes located near $10e^{\mu_1+\sigma^2},9e^{\mu_1+\sigma^2}+e^{\mu_2+\sigma^2},\dots,10e^{\mu_2+\sigma^2}$. This suggest that all five parameters in principle are identifiable given enough data. Perhaps you can approximate the convolution in (1) by a single moment-matched lognormal as discussed here , use (2) to compute the likelihood and then compute approximate maximum likelihood estimates by maximising the resulting log likelihood numerically. Or you could do approximate Bayesian inference using this approximate likelihood function. This option would allow using informative priors on some of the parameters which might be necessary in practice if there is too much overlap between each component of (2).
