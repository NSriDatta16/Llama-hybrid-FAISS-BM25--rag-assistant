[site]: datascience
[post_id]: 86791
[parent_id]: 86778
[tags]: 
The principle in supervised ML is quite simple: the "method" which is going to be used to predict the response variable must be fully determined from the training set and only from the training set. In other words, anything which doesn't belong to the training set cannot be used. As a consequence, feature engineering, i.e. choosing how to prepare/represent/normalize features must be done using only the training set. This includes any feature selection/extraction step. Note that once the final data preparation process is fully determined, it can and should be applied exactly the same way on the test set or in production. This means that for instance normalization does not involve recomputing any parameter, it uses the ones calculated on the training set. See also a few related questions: Data normalization before or after train-test split? Why you shouldn't upsample before cross validation Using PCA as features for production
