[site]: crossvalidated
[post_id]: 266217
[parent_id]: 266152
[tags]: 
Much depends on how strict you interpret the definition of perceptron. Typically the perceptron refers to the precursor of what is called a neuron in neural networks: it is a linear binary classification function that only allows as output a 1 or a 0. According to this definition, the answer to your question would be 'no' since convolutional networks require more than a binary output and the max-pooling layer is not linear. Much confusion comes from the name of the successor to perceptrons: multi-layer perceptrons. MLP's no longer have linear activation functions and are no longer restricted to binary outputs. MLP's are also different from convolutional networks however in the sense that all layers are fully-connected. Update: Can MLP's be used to construct a fully convolutional network? You could argue that you could use MLP's with one layer to construct parts of the convolutional network, but by definition MLP's have multiple layers, so again the answer would be 'no'.
