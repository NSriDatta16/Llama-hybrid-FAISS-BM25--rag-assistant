[site]: datascience
[post_id]: 75003
[parent_id]: 
[tags]: 
Disparity between training and testing errors with deep learning: the bias-variance tradeoff and model selection

I am developing a convolutional neural network and have a dataset with 13,000 datapoints that is split 80%/10%/10% train/validation/test. In tuning the model architecture, I found the following, after averaging results over several runs with different random seeds: 3 conv layers: training MAE = 0.255, val MAE = 0.340 4 conv layers: training MAE = 0.232, val MAE = 0.337 5 conv layers: training MAE = 0.172, val MAE = 0.328. Normally, I'd pick the model with the best validation MAE (the trends are the same for the testing MAE, for what it's worth). However, the architecture with the best validation MAE also has the largest difference between training and validation MAE. Why is what I'd normally think of as overfitting giving better results? Would you also go with 5 convolutional layers here, or are there concerns with a large difference in training and validation/testing performance? On what I imagine is a related note, I am familiar with the article "Reconciling modern machine-learning practice and the classical biasâ€“variance trade-off" in PNAS , which has the thought-provoking image below. Is this something that's actually observed in practice -- that you can have minimal training error but good out-of-sample, generalizable performance, as shown in subpanel B?
