[site]: crossvalidated
[post_id]: 565094
[parent_id]: 
[tags]: 
Understanding Mixed-Effect Models

I am trying to get my head around exactly how mixed-effect models produce the estimates they produce. Say you have 20 students from 100 classrooms each (200 students altogether) and you're trying to regress minutes spent studying onto GPA. My understanding of the general concept is that the model will estimate "fixed-effects," that is, what the intercept and slope would be if you ran one big regression and ignored the clustering by classroom. Then, it produces "random-effects" that are essentially classroom-specific deviations for the intercept and slope parameters. What I don't understand is how these 40 estimates (20 random intercepts, 20 random slopes) are produced. If they are just assumed to be drawn from a random normal with mean 0, how does that improve anything? Take classroom 1. Compared to classrooms 2 through 39, it has an above-average intercept and a below-average slope. But if the random-effects (deviations from grand-intercept and grand-slope) are both drawn from normals with mean 0, what's preventing the random effects from estimating a below-average intercept and an above-average slope? How does the model fitting find some "medium-ground" (partial pooling) between the grand-intercept/slope and the intercept/slope it would produce if it just ran 20 independent regressions? How does the shrinkage produced by the added mean 0 noise aid inference?
