[site]: crossvalidated
[post_id]: 629309
[parent_id]: 468064
[tags]: 
To supplement Gung's correct answer, I show a very direct example using R with some simulated data in order to directly see what is going on, but first I want to explain roughly what the intercept in logistic regression represents. In a Gaussian regression, the intercept is usually considered a conditional mean of the response variable, and when only estimating the intercept, we just get the raw, uncorrected mean of the response. We can think of logistic regression in a similar way, but instead consider the intercept a conditional log odds , or when transformed, the conditional probability or odds ratio of the total probability of a response variable. When this intercept is not disaggregated by additional slope terms, the intercept is simply the raw log odds of the response, which can also be transformed into the raw probability or odds ratio of the response. On to the simulation. First we can simulate binomial data with rbinom with $P(x) = .5$ . Then plotting it shows this is the case, as the bins have roughly equal height. #### Sim Data #### set.seed(123) y Fitting the data with an intercept-only model: #### Fit Model #### fit We can obtain both the estimated probability and odds ratio of the intercept with the following code: #### Exponentiate Intercept Term #### plogis(coef(fit)) # probability of event exp(coef(fit)) # odds ratio of event As expected the probability is roughly around what we estimated with our rbinom function, with some uncertainty added into the random simulation: > plogis(coef(fit)) (Intercept) 0.487 We should then expect that the odds ratio is roughly around 1, as the chances of $1$ or $0$ are roughly the same, and this is confirmed by our odds ratio estimate when exponentiated (though again, it is slightly off due to uncertainty in random simulation): > exp(coef(fit)) (Intercept) 0.9493177 Extending that idea further...if we have additional slope terms, this estimate simply becomes the probability of the event after controlling for whatever predictors you include in your model. So while the interpretation changes slightly with the inclusion of additional terms, the overall message is the same.
