[site]: crossvalidated
[post_id]: 313052
[parent_id]: 
[tags]: 
Calculate entropy of sample

I have a bayesian model that gives me a sample of a distribution for some parameter x - a list of 100 continuous values like so: [1.32, 1.38, 1.44, ..., 1.28] . If I know that a sample is very disperse, I can request additional information to be inputted by hand - so I can ask "Is x lower than 1.34? Additionally, for some parameters, there are only 1 or 2 possible values, and the model outputs 1 or 2 gaussians around those values (see the image below). If for all parameters I had a fixed number of possible values, I could easily compute the entropy, by treating them as categorical values. Unfortunately, that is not the case. I could very easily make up some heuristic rule, but I'd like to know how to quantify how much entropy does a parameter contain. If I knew that, I would be able to easily minimize the amount of queries to the data entry team. My current solution is to run kernel density estimation, and for sum the information per kernel. How can I calculate entropy in a more reasonable way?
