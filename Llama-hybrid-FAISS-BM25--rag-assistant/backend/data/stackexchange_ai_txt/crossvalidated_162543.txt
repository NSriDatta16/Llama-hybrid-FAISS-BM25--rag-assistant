[site]: crossvalidated
[post_id]: 162543
[parent_id]: 140652
[tags]: 
An Echo State Network is an instance of the more general concept of Reservoir Computing . The basic idea behind the ESN is to get the benefits of a RNN (process a sequence of inputs that are dependent on each other, i.e. time dependencies like a signal) but without the problems of training a traditional RNN like the vanishing gradient problem . ESNs achieve this by having a relatively large reservoir of sparsely connected neurons using a sigmoidal transfer function (relative to input size, something like 100-1000 units). The connections in the reservoir are assigned once and are completely random; the reservoir weights do not get trained. Input neurons are connected to the reservoir and feed the input activations into the reservoir - these too are assigned untrained random weights. The only weights that are trained are the output weights which connect the reservoir to the output neurons. In training, the inputs will be fed to the reservoir and a teacher output will be applied to the output units. The reservoir states are captured over time and stored. Once all of the training inputs have been applied, a simple application of linear regression can be used between the captured reservoir states and the target outputs. These output weights can then be incorporated into the existing network and used for novel inputs. The idea is that the sparse random connections in the reservoir allow previous states to "echo" even after they have passed, so that if the network receives a novel input that is similar to something it trained on, the dynamics in the reservoir will start to follow the activation trajectory appropriate for the input and in that way can provide a matching signal to what it trained on, and if it is well-trained it will be able to generalize from what it has already seen, following activation trajectories that would make sense given the input signal driving the reservoir. The advantage of this approach is in the incredibly simple training procedure since most of the weights are assigned only once and at random. Yet they are able to capture complex dynamics over time and are able to model properties of dynamical systems. By far the most helpful papers I have found on ESNs are: A tutorial on training RNNs by Herbert Jaeger (curator of the Scholarpedia page on ESNs) A Practical Guide to Applying Echo State Networks by Mantas Lukoševičius They both have easy to understand explanations to go along with the formalism and outstanding advice for creating an implementation with guidance for choosing appropriate parameter values. UPDATE: The Deep Learning book from Goodfellow, Bengio, and Courville has a slightly more detailed but still nice high-level discussion of Echo State Networks. Section 10.7 discusses the vanishing (and exploding) gradient problem and the difficulties of learning long-term dependencies. Section 10.8 is all about Echo State Networks. It specifically goes into detail about why it's crucial to select reservoir weights that have an appropriate spectral radius value - it works together with the nonlinear activation units to encourage stability while still propagating information through time.
