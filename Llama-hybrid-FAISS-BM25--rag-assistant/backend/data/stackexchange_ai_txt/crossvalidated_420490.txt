[site]: crossvalidated
[post_id]: 420490
[parent_id]: 420186
[tags]: 
I think a good way to do this is via Bayesian Structural Time Series (BSTS). I found out about this approach via these 2 sites ( 1 , 2 ). I would still be interested in other approaches. Here is the example done with the bsts package in R. I use a time series component and a regression component. The regression component incorporates the prior information. A stack and slab prior is used on the regression component. Below is a plot of the time-series features. I intentionally made C unlike the other features, and the response variable (D), in order to test the feature selection ability of BSTS. The plot below shows that the BSTS model correctly observes that feature C isn't useful for predicting D. Actual (Blue) vs Predicted (Red) Code is below: library(ggplot2, bsts, ggplotly, data.table) # generate some data set.seed(1) n = 20 train_size = 10 A = seq(1,n) + rnorm(n) B = seq(1,n) + rnorm(n) # this variable is not like the others C = rnorm(n) + 5*sin(seq(1,n)) D = seq(1,n) + rnorm(n) X = data.table(A, B, C, D) # transform the data for ggplot long_data = melt(X) m[, t := seq_len(.N), by = variable] g1 = ggplot(data=m, aes(x=t, y=value, colour=variable)) + geom_line() + labs(title="Evolution of Parameters over Time") ggplotly(g1) #break the data into training/testing data train_ind = seq(1,train_size) train_X = X[train_ind,] test_X = X[-train_ind,] ss $D) model4 D, col="red")
