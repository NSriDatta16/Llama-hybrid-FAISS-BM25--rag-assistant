[site]: crossvalidated
[post_id]: 478014
[parent_id]: 476590
[tags]: 
Previous answers already touched upon the difference between statistics and reduced form econometrics, in that the latter places more emphasis on causal inference based on observational data. This difference is very clear is you compare the techniques for "panel data" by econometricians with those used for "longitudinal data" by statisticians, despite the data structure being exactly the same. There is an additional layer of difference between statistics and structural econometrics. Econometric models and methods arise from the need to test economic theory. One starts with an economic model, then consider how it can be taken to data, rather than applying statistical models/methods in an ad hoc way. Two standard examples: 1. CAPM and Fama-French-MacBeth The classical Capital Asset Pricing Model (due to Markowitz and Sharpe ) says that, if investors have mean-variance preferences, then asset price obey the relationship $$ E[R - r] = Cov(R, M) $$ where the RHS is covariance of return $R$ with the market $M$ , and LHS is expected excess return of asset. Empirically, taking this relationship to data means fitting a linear model---regressing $R-r$ on $M$ . Later Fama and French introduced additional covariates (the Fama-French factors) in the CAPM regression. In this particular case, the appropriate econometric model turns out to be the linear model. 2. Generalized Method of Moments In a more contemporary model of asset prices (by now also basic), one arrives at the equilibrium relationship (called an asset pricing equation in economics) $$ E[u'(c_t) R_t|\mathcal{I}_t] = 0 $$ where $c_t$ is consumption, $R_t$ is asset return, and $u$ is the preference (utility function) of the agent. A natural econometric question is now to estimate the parameters of $u$ from data. This led Hansen to introduce GMM, which makes the above moment condition, and others, a testable statistical hypothesis. (GMM contains the instrumental variables (IV) as a special case.)
