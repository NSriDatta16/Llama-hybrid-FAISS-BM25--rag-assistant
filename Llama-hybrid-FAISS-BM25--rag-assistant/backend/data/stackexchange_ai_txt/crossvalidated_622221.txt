[site]: crossvalidated
[post_id]: 622221
[parent_id]: 
[tags]: 
In what sense do Bayesian models simulate observation?

Could you verify that i understand the author correctly here? New to Bayes. [...] These assumptions [the prior and the likelihood] also allow us to simulate the observations that the model implies. They allow this, because likelihood functions work in both directions. Given a realized observation, the likelihood function says how plausible the observation is. And given only the parameters, the likelihood defines a distribution of possible observations that we can sample from, to simulate observation. In this way, Bayesian models are always generative, capable of simulating predictions. Many non-Bayesian models are also generative, but many are not. -- McElreath R., Statistical Rethinking^2 Let's say i trained some model of P(A|B) . Please let me know if i'm correct in interpreting the following statements. Given a realized observation, the likelihood function says how plausible the observation is. This just refers to the "forward" P(B|A) that i chose to use when conditioning the prior on every data point? Given only the parameters, the likelihood defines a distribution of possible observations that we can sample from, to simulate observation : Now, turning it around, if i sample from the posterior (via something like the PPD ), the factor that would have mostly determined the shape of whatever i trained and am now sampling from would be the likelihood P(B|A) . So in this sense, i have tap of synthetic data which represents the target P(A|B) as accurately as my conditioning up-to-now has been?
