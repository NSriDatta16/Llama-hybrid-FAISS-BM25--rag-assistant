[site]: crossvalidated
[post_id]: 635634
[parent_id]: 635419
[tags]: 
Preamble: The mode of a distribution is the value that is most commonly observed. A bit more formally, it is the value with the highest probability of occurrence. Now, in the context of the posterior class probabilities as here, "mode" refers to the class that has the highest probability for a given data-point $x$ , i.e. it is the class that the SVM classifies assumes it has the highest probability of being the correct class. (and hopefully that class indeed is the correct class) Importantly, the SVM hinge loss is minimized when the margin between the two classes is as large as possible. And that margin is in terms of "scores", not probabilities. In that sense, the minimizer does not faithfully reflect the posterior probabilities. We use the logistic function to get them - the logistic function turns the decision function's scores into probabilities. (And that's why SVMs, as they are not inherently probabilistic models, do not directly output probabilities and while SVMs might "accurate" in their class assignments (i.e. finding the mode) their probabilities are often not well-calibrated .)
