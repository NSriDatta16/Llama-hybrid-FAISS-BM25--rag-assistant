[site]: crossvalidated
[post_id]: 629468
[parent_id]: 629372
[tags]: 
Stationarity is a property of a random process. In regression modeling, the covariates/predictors are considered fixed and non-random. That is not to say that they do not have random properties in the context of experimental design, but in traditional regression modeling, such as logistic regression, the modeling strategy is to model the conditional distribution of the response as a function of the predictors - hence why they are called predictors . You should be aware this is a separate discussion from confounding and models for dependent data (like GEE or GLMMs). (Note: stationarity , such as for an error term, does not mean a model for independent data is correct, rather it means that very simple models for dependent data can be used). Furthermore, when I say models for "dependent data" I refer to the conditional distribution of the response, after adjustment for predictors. You can easily think of many modeling applications where the "X" variable is a non-stationary process, and the naive inference ranges from clear, to nuanced, to completely ambiguous. For instance, you might look at a time series of blood pressure predicting the use of a blood pressure lowering medication. In this case, medication use is associated with a higher blood pressure specifically due to confounding by indication. On one hand, it is correct to say people using this medication tend to have higher blood pressure than those who do not. On the other hand, it is completely incorrect to say the blood pressure medication is ineffective.
