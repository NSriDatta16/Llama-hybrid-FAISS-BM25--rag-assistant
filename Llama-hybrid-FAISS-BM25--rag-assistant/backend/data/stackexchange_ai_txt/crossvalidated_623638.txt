[site]: crossvalidated
[post_id]: 623638
[parent_id]: 623560
[tags]: 
Suggest to look at "Optimal Training of Mean Variance Estimation Neural Networks" by Sluijterman, Cator, and Heskes ( https://arxiv.org/abs/2302.08875 ) . Just got posted last week on exactly this problem and strategies to avoid it. Tl;Dr: Use a warmup period to train mean first, and once mean converged, then train variance Use different subnetworks and regularization parameters for each parameter of the distribution. That way network can actually learn different feature mappings for each output neuron; if you simply use a fully connected DNN with just the last layer as 2 output neurons , you don't leave enough freedom for network to explore the functional space independently between mean and variance.
