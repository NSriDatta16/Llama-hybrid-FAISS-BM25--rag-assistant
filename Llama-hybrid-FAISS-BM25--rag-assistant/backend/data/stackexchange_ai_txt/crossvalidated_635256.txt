[site]: crossvalidated
[post_id]: 635256
[parent_id]: 635183
[tags]: 
Is there hidden cost function for hidden layer in the neural network? Nearly all neural networks have loss functions only at the final layer. All weights and biases are updated by using backpropagation , which computes the loss on the final layer and uses the gradient of the loss wrt the weights and biases to adjust them. The exceptions to this are explicit about having auxiliary loss functions sprinkled throughout, but this is a very specialized case. There are several examples in What is auxiliary loss as mentioned in PSPNet paper . These include: " Going Deeper with Convolutions " (2014) by Christian Szegedy et al. " Pyramid Scene Parsing Network " (2017) by Hengshuang Zhao et al.
