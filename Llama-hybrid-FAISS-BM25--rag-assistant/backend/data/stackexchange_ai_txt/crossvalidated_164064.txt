[site]: crossvalidated
[post_id]: 164064
[parent_id]: 
[tags]: 
Fixed Effects vs Lagged DV vs. First Differences Regression

What are the differences between using unit fixed effects, unit fixed effects and time fixed effects, lagged DV, or first differences to analyze a time series with 4-5 time periods and 35-50 units per time period (depending on the subset)? In particular, how will they perform if both an $X$ covariate and the dependent variable $Y$ have consistent time trends, for example if both trend downwards over the time period under observation. My current understanding is that if the dependent variable has a consistent time trend, for example if $Y_{it}= .9 Y_{it-1}+ \epsilon_{it}$ then using a lagged DV instead of a first difference model will bias estimates of the effect of $X$ on $Y$ towards zero, particularly if $Y$ and thus $Y_{t-1}$ are measured more precisely than $X$ and $X$ also has a time trend, because the time trend in $Y$ could be entirely a function of $X$, but with measurement error the regression will partition the explained variance across both $X$ and the lagged DV. In terms of the fixed effects vs. first differences, I found this video series ( Part 1 Part 2 , Part 3 ) helpful for understanding which is better at handling different types of serial correlation. In particular, it says in part 2 that if the error (i.e. the residuals) $u_{i}$ are serially correlated then first differences will perform better, but that if $\Delta u_{i}$ are serially correlated then fixed effects is the way to go. How can I test for serial correlation? This answer points to dwtest in R , but it's unclear to me how to test for serial correlation in fixed effects model using this function because it will not allow me to pass it a formula with fixed effects (I get the error Error in chol2inv(qr.R(qr(X))) : element (51, 51) is zero, so the inverse cannot be computed ). Finally, part 3 of the video series says that if an $X$ covariate is non-stationary and unit root then the first difference model will preform better. My understanding is that an $X$ covariate that consistently trends downwards is unit root. Is that correct? Is there a more formal test I can do to determine whether or not an $X$ covariate is unit root?
