[site]: crossvalidated
[post_id]: 409054
[parent_id]: 
[tags]: 
How do Bayesians verify their methods using Monte Carlo simulation methods?

Background : I have a PhD in social psychology, where theoretical statistics and math were barely covered in my quantitative coursework. Through undergrad and grad school, I was taught (much like many of you also in the social sciences, probably) through the "classical" frequentist framework. Now, I also love R and using simulation methods to verify that methods work makes way more sense to me than mathematical proofs (again: background in a quantitative social science, not theoretical statistics). Frequentist methods and simulation methods together make a ton of sense to me. Because frequentists see probability as long run odds (e.g., If I do this an arbitrarily large number of times, and it happens 50% of the time, then there is a 50% probability). We can simulate this long run with Monte Carlo methods! Complications : Since undergrad, I have been very aware of Bayesian methods, and there has always been people in my life calling me to the Bayesian side, saying that the results were easier to interpret, that we get probability for a hypothesis instead of the data given a hypothesis, etc. I was really into this and took a Bayesian class, read some Bayesian books and papers, and now am pretty familiar with Stan and its associated R packages. Enter Mayo : After thinking "Bayesian is probably the way of the future" for a while, I read Deborah Mayo's Statistical Inference as Severe Testing . She says she doesn't pick a side at the beginning of the book, but she does: She is a frequentist, and a lot of the book is defending frequentist methodologies. I don't want to necessarily get into a discussion of whether or not we think the way she sees evidence is valid, but this got me thinking: Is Bayes really all that it is advertised? I mean, the Bayes crowd is so fractured itself that I don't even know the "right" way to analyze data in a Bayesian framework often. Usually, I would just use rstanarm and present point estimates and credible intervals... which often line up closely with frequentist estimates and confidence intervals. I might do model comparisons, but I'm always afraid of describing Bayes factors as posterior probability comparisons, etc. More Thinking : What I kept thinking through Mayo's book was: There is a way we can use computers to make sure our frequentist methods work, because probability is what we see in the long run and we can simulate that. Bayesians can't even agree on what probability really is, it seems, depending on the Bayesian school (default, subjective, etc.). Which leads me to my question: Question : How do Bayesians verify that their methods define uncertainty properly (i.e., calculate valid credible intervals and posterior distributions) using Monte Carlo simulation methods, if probability is not defined as rates in the long run? Example : I create a data generator. This is just going to simulate from a Bernoulli distribution with a probability of .5: set.seed(1839) p Now, let's say I want to make sure that confidence intervals in a logistic regression are actually valid. I can simulate a regression a large number of times and make sure that the actual population value falls in the 95% confidence interval 95% of the time. It's an intercept-only model, so I just want to make sure that I'm estimating p correctly: set.seed(1839) iter min(conf) }) mean(results) This takes a minutes to run, but we end up with the mean(results) call giving us 0.9416 . This is about 95%, and I'm confident in saying that the glm command is describing uncertainty in a valid way. I'm sure it would have gotten closer to right on the nose at 95% if I upped iter and wanted to wait here at my laptop longer. On the other hand, let's fit a Bayesian model for the same thing: library(rstanarm) set.seed(1839) dat In part, this gives me: Estimates: mean sd 2.5% 25% 50% 75% 97.5% (Intercept) -0.1 0.2 -0.5 -0.2 -0.1 0.0 0.3 mean_PPD 0.5 0.1 0.3 0.4 0.5 0.5 0.6 log-posterior -73.0 0.7 -75.1 -73.1 -72.7 -72.5 -72.5 Since Bayesians don't define probability as what we see in the long run, how can I use simulation methods to verify than stan_glm is accurately capturing uncertainty? That is, how could I trust that these credible intervals are valid, using simulation methods? And right now, I'm not even defining a priorâ€”how does the inclusion of priors come into play here, since that will affect our measures of uncertainty? When I was trying to write a beta regression with a hurdle model component in Stan from scratch once, I had someone recommend to me: "Simulate data. Do it a bunch of times, and the true estimates should be in the credible interval about 95% of the time." But to me, that goes against the very thing that Bayesians believe in! That relies on frequentist understandings of probability! So how would a Bayesian convince me that the credible interval I'm getting from the summary() call to my model is accurately describing uncertainty, using simulation methods? Purpose of Question : This is a trivial example, but many times clients provide me with difficult problems. And I try things that I am unfamiliar with, so I often run a simulation study to make sure that what I am doing is valid. If I were to write a custom model in Stan, how would I know that what I am doing is legit? How could I use simulation methods to verify that what I'm doing in Stan is actually going to tell me what I want to know?
