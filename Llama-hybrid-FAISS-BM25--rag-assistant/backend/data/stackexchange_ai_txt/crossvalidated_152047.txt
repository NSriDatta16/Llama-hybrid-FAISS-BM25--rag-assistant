[site]: crossvalidated
[post_id]: 152047
[parent_id]: 152002
[tags]: 
A sequential mixture model would look something like: $\boldsymbol Y_i \sim \mathcal N_d(\boldsymbol \mu_{c_i}, \boldsymbol\Sigma_{c_i}) \ \ i=1,...,n$ $\boldsymbol c_i \sim \text{Multinom}(\boldsymbol p), \ \ i=1,...,n$ Where $\boldsymbol p$ is of length $k$, then: $(\boldsymbol \mu_{c}, \boldsymbol\Sigma_{c}) \sim \text{Normal-Inverse-Wishart}(m,S,\boldsymbol \Psi, v), \ \ c=1,...,k$ Now the important thing for introducing dependency will be making $\boldsymbol \Psi$ have a suitable structure. E.g. precision matrix of an AR1 process is tri-diagonal (e.g. see here ). Generally the IW prior will capture an arbitrary dependency structure if $\boldsymbol \Psi$ is only diagonal, but this might help it along. This is different than a Hidden Markov Model. The difference is that mixture models suppose the whole series is sampled at once from a particular type of process, but you don't know which process. HMMs suppose that at each time point $\boldsymbol Y_t$ is sampled from a different distribution, and the distribution changes over time via a Markov chain. Looking at the samples of data the HMM might be interesting, as it looks as though there are bursts of similar behaviour, followed by changes. When you say you used a GMM previously, was this multivariate (the whole time series at once) or univariate (pooling observations across series and time)?
