[site]: crossvalidated
[post_id]: 493336
[parent_id]: 
[tags]: 
Seq2Seq Machine Translation Question

I'm reading through Pytorch's NLP from Scratch: Translation with a Sequence to Sequence Network and Attention , and I am a bit confused on the Preparing Training Data section, particularly: def indexesFromSentence(lang, sentence): return [lang.word2index[word] for word in sentence.split(' ')] def tensorFromSentence(lang, sentence): indexes = indexesFromSentence(lang, sentence) indexes.append(EOS_token) return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1) def tensorsFromPair(pair): input_tensor = tensorFromSentence(input_lang, pair[0]) target_tensor = tensorFromSentence(output_lang, pair[1]) return (input_tensor, target_tensor) Why do they add an EOS token to the end but not a SOS in the beginning?
