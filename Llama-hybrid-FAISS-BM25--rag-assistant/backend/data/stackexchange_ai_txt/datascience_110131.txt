[site]: datascience
[post_id]: 110131
[parent_id]: 110124
[tags]: 
A common complaint about accuracy is that it fails when the classes are imbalanced. For instance, if you get an accuracy of $98\%$ , that sounds like a high $\text{A}$ in school, so you might be pretty happy with your performance. However, if the class ratio is $99:1$ , then you’re doing worse than you would by always guessing the majority class. However, accuracy has issues when the classes are naturally balanced, too. In many applications, there are different costs associated with the different mistakes. Accuracy takes away from your ability to play the odds. The typical threshold for a (binary) model that outputs probability values (logistic regression, neural nets, and others) is $0.5$ . Accuracy makes a $0.49$ and $0.51$ appear to be different categories while $0.51$ and $0.99$ are the same. I’d be a lot more comfortable making a huge decision based on a probability of $0.99$ than on $0.51!$ Accuracy masks this. In fact, any threshold-based metric like sensitivity, specificity, $F_1$ , positive predictive value, or negative predictive value masks the differences between $0.51$ and $0.99$ . Consequently, statisticians advocate for direct evaluation of the probability outputs of models, using metrics such as log loss (often called crossentropy in machine learning circles and sometimes negative log likelihood) and Brier score (pretty much mean squared error, with an unsurprising generalization in the multiclass setting). Vanderbilt’s Frank Harrell, the founder and former head of the Department of Biostatistics at their medical school, as well as a frequent user of the statistics Stack , has two good blog posts about the idea of predicting tendencies instead of categories and measuring success by evaluating the probability outputs of models. Damage Caused by Classification Accuracy and Other Discontinuous Improper Accuracy Scoring Rules Classification vs. Prediction
