[site]: datascience
[post_id]: 11256
[parent_id]: 11255
[tags]: 
This is the log-likelihood: $\log P(x; w) \equiv \log \prod_i P(x_i | w) = \sum_i \log P(x_i | w)$, where $P(x_i | w) \equiv \left\{ \begin{array}{rl}\sigma(x_i), & y_i =1 \\ 1 - \sigma(x_i), &y_i = 0\end{array} \right.$ Why the log-likelihood? When you have a probabilistic model, such as logistic regression, it's one way (the MLE ) of finding the parameters that fit best. Recall that in logistic regression we are, contrary to the name, trying to classify rather than regress, and the MSE is a regression loss; it seeks to minimize the distance from a point, while we wish to penalize being in the wrong subspace (the parts that don't correspond to the correct class). If you squint a bit, you can see that the negative log-likelihood minimizes the cross entropy .
