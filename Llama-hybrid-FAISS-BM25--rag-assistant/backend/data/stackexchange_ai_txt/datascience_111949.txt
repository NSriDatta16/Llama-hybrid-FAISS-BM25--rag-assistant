[site]: datascience
[post_id]: 111949
[parent_id]: 111948
[tags]: 
One efficient way is to use the roberta base squad 2 model, using your text as context and then ask questions. It should work well and the model can be downloaded directly. git lfs install git clone https://huggingface.co/deepset/roberta-base-squad2 Here is an extract of code to use it: from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline model_name = "deepset/roberta-base-squad2" # a) Get predictions nlp = pipeline('question-answering', model=model_name, tokenizer=model_name) QA_input = { 'question': 'Why is model conversion important?', 'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.' } res = nlp(QA_input) # b) Load model & tokenizer model = AutoModelForQuestionAnswering.from_pretrained(model_name) tokenizer = AutoTokenizer.from_pretrained(model_name) You can also test the Robert squad 2 model using this link: https://huggingface.co/deepset/roberta-base-squad2?context=Francisco+is+in+the+west+USA.&question=Where+is+San+Francisco%3F You can also fine-tune your model on your data: https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial2_Finetune_a_model_on_your_data.ipynb
