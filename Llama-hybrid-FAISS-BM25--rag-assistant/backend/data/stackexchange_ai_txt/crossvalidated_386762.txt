[site]: crossvalidated
[post_id]: 386762
[parent_id]: 386758
[tags]: 
The point of hold out validation set is that you want part of your data to be left out from training so that you can test out the performance of your model on unseen data. Therefore, you need your validation set to have the same distribution as your training data. Thus, random selection is always the way to go since the data you use are obtained from somewhere else and you don't know how the data points are sorted, and you don't want the original sorting to affect which data goes into training and which goes into validation. However, sometimes you do want to use the tail of the data. One obvious case is time series predictions. The data you have should usually be sorted by time. Thus, it makes a lot of sense to use the first X% as training data and use the remaining (100-X)% as validation since those are the future time points. If you did the random validation set split, you will be in trouble. For example, if values at time point 1,3, 4, 5 are in training and 2 is in validation, the model will be able to use time point 3, 4, 5 to infer the value at time point 2, which is completely not acceptable as you are leaking information. How you select your validation set may vary depends on the objective and the modeling goal. However the principle is to make sure: The training and validation set are from the same distribution/population. Your training data does not leak information to your validation data.
