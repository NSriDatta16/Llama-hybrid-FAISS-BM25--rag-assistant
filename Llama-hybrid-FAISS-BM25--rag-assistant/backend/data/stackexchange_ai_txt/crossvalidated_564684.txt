[site]: crossvalidated
[post_id]: 564684
[parent_id]: 
[tags]: 
Are there any hyperparameter search approaches which fine-tune rather than train from scratch?

Whenever I hear about any hyperparameter search approach for training neural networks, the models are always trained from scratch. I'm wondering if there is any validity in starting from a converged training instead of random weights as an initialization point for all hyperparameters. While obviously the results will be less accurate, shouldn't the reduction in training time be sufficient to compensate?
