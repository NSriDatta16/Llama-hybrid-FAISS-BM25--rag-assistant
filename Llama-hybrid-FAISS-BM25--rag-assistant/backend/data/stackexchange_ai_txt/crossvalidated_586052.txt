[site]: crossvalidated
[post_id]: 586052
[parent_id]: 
[tags]: 
Why do we call word embeddings "vectors" ? Shouldn't we call them word coordinates?

Word embeddings are often referred to as vectors. For instance, "Word embedding models are dense vector representations of words learned from a corpus of natural language" (Rosado, 2020). Would it be more precise to talk about word coordinates, or word scalars ? My rough understanding of embeddings is that they are representations of where the word resides in a conceptual space: a sparse coocurence matrix is transformed using a dimension reduction technique, transposing each word into a k -dimension Euclidean space. Based on that assumption, the "vector" associated with each word would therefore be a set of coordinates. A list of embeddings would therefore be the list of coordinates where the elements reside in the k dimension space. The elements are geometrical points in that space, having no volume, no direction and no magnitude, they would be scalars . It is possible that I am indeed confused, and that embeddings really are geometrical vectors, and would like to read an explanation if that is the case. An alternate explanation is that we are talking about vectors in a computational sense (a list of numbers), not a geometrical sense. In that case, I believe that making that clarification would help. Rozado, D. (2020). Wide range screening of algorithmic bias in word embedding models using large sentiment lexicons reveals underreported bias types. PloS one, 15(4), e0231189.
