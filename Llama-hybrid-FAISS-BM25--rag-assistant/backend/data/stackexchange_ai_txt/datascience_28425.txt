[site]: datascience
[post_id]: 28425
[parent_id]: 28420
[tags]: 
I think @TitoOrt gets at the bulk of the confusion here, you're trying to explain how to get back the training parameters. When it seems like the point the authors are making is that you can use X-Fold (for avoiding confusion with my example KNN) to find a hyperparameter. Let me better explain with K-Nearest Neighbor: Say you were trying to find the best K to use with the Iris set, here are error rates for K {2, 3, 4, 5}: from sklearn import cluster, datasets, mixture from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import train_test_split data = sklearn.datasets.load_iris() X = data["data"] Y = data["target"] accuracy = [] for n in range(2, 6): # 70% holdout for training x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.7) model = KNeighborsClassifier(n_neighbors=n) model.fit(x_train, y_train) score = model.score(x_test, y_test) accuracy.append((n, score)) # accuracy, number of clusters and the accuracy of the model # [(2, 0.96190476190476193), # (3, 0.92380952380952386), # (4, 0.97142857142857142), # (5, 0.94285714285714284)] However this brings up a really easy criticism, how do I know we didn't just get lucky in picking the data to pass into the model when K was set at 4? (Thankfully for this example, that's probably the seeing as there is only suppose to be 3 classes). So in order to overcome this we use X-Fold to average out the bias that comes from sampling data, and thus better trust the accuracy that we get from picking K. accuracy = [] for n in range(2, 6): k_accuracy = [] # splitting and running this loop 10 times kf = KFold(n_splits=10) for train_index, test_index in kf.split(X): model = KNeighborsClassifier(n_neighbors=n) model.fit(X[train_index], Y[train_index]) score = model.score(X[test_index], Y[test_index]) k_accuracy.append(score) # get the averaging of each 10 kfold runs score = sum(k_accuracy) / 10 accuracy.append((n, score)) # accuracy # [(2, 0.93333333333333335), # (3, 0.94666666666666688), # (4, 0.92666666666666675), # (5, 0.93333333333333335)] First of all we've eliminated the K=4 option, and the actual K=3 emerges as the winner. With more folds our accuracy can be better trusted as well. Let's connect this back to your original question though. Each radius attached to each cluster will be different every time you run this algorithm because we're fitting to the data. So yes you are right that you will never be able to exactly replicate the radius (parameter) choices that were made to get a certain accuracy, but that is not the goal of cross-validation. The more important aspect of folding was to pick the best K. Once we know that we can train to our hearts content.
