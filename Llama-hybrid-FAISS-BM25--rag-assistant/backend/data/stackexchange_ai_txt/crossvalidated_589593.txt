[site]: crossvalidated
[post_id]: 589593
[parent_id]: 
[tags]: 
Why do graph convolutional neural networks use normalized adjacency matrices?

It seems that it is common to perform something like the following operation (like in Kipf & Welling, " Semi-Supervised Classification with Graph Convolutional Networks " 2017) to preprocess the adjacency matrix in graph convolutional networks $$ \hat{A} = D^{-\frac{1}{2}} A D^{-\frac{1}{2}} $$ where $D$ is the degree matrix and $A$ is the adjacency matrix . Then, it seems like we do $$ \hat{A} X W $$ where $X$ is the node data and $W$ is a weight matrix to create the new graph. What is the reasoning behind preprocessing the adjacency matrix using the degree matrix? Why can't we just do $AXW$ ?
