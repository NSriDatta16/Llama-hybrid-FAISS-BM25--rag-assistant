[site]: crossvalidated
[post_id]: 292560
[parent_id]: 241854
[tags]: 
I have an answer for "Why regularize towards a value? Does this change the interpretation of $\beta$?" Transfer learning is a type of Machine Learning where knowledge from the source domain when performing a task is transfered to the target domain when performing the same task i.e. the task remains the same but datasets in the two domains differ. One way to perform transfer learning is parameter sharing. The high level intuition is that target domain model parameters should be very close to source domain model parameters while still allowing for some uncertainty. Mathematically this intuition is captured by penalizing the deviation of the parameters i.e., $\lambda\|W_{target}−W_{source}\|^2_2$ , where, $λ$ is the penalization parameter and W's are a vector of model parameters. I have used this approach to perform transfer learning for conditional random fields , look at Eq. 4 and related text. I had a similar question for Ridge regression posted here on the interpretability of the closed form solution.
