[site]: crossvalidated
[post_id]: 318494
[parent_id]: 318463
[tags]: 
The answer to this question is actually very simple. With theoretical justification behind the machine learning model we at least can prove that when some more or less realistic conditions are met, there are some guarantees of optimality for the solution. Without it, we don't have any guarantees whatsoever. Sure, you can say "let's just check what works and use it for the particular problem", but this isn't feasible since there is a infinite number of ways how you could solve any machine learning problem. Say that you want to predict some $Y$, given some $X$. How do you know that $X + 42$ is not an optimal way to solve it? What about $X + 42.5$? Or, $\sqrt{X - 42}$? Maybe just return $42$ as your prediction? Or if $X$ is odd use $X+42$ and otherwise return $0$? Sure, all those suggestions sound absurd, but how can you be sure, without any theory, that one of them wouldn't be optimal? With an infinite number of possible solutions, even the simplest problem becomes unsolvable. Theory limits your "search space" of the feasible models for some class of the problems (you know which models are worth considering and which not).
