[site]: crossvalidated
[post_id]: 25836
[parent_id]: 
[tags]: 
Minimax estimator for the mean of a Poisson distribution

I recently took a course on Bayesian statistics based on The Bayesian Choice by C. Robert (aka Xi'an) . I couldn't solve one of the exercises regarding minimax estimators and was hoping that someone here can give me a clue as to have it can be solved. This is not homework; this is me wanting to fully understand the material covered in the book. The exercise is labelled 2.32 (and is found on page 90 in the second edition of the book). It amounts to the following problem. Problem: Show that when $X\sim \mathrm{Poisson}(\lambda)$ the estimator $\hat{\lambda}=x$ is minimax. Here $\lambda$ denotes the mean of the Poisson distribution. Presumably the use of the quadratic loss is implicit. A hint is given: Hint: Notice that $\hat{\lambda}$ is a generalized Bayes estimator for $\pi(\lambda)=1/\lambda$ and use a sequence of $\mathrm{Gamma}(\alpha,\beta)$ priors. This points me to Lemma 2.4.15 (p. 72) which states that if there exists a sequence $(\pi_n)$ of proper priors such that the generalized Bayes estimator $\delta_0$ satisfies $R(\theta,\delta_0)\leq \lim_{n\rightarrow\infty}r(\pi_n) All other results that I can find deal with Bayes estimators of proper priors and are thus of no use to me here (?). Now, what's been giving me a headache for some time is that $R(\lambda,\hat{\lambda})=E(X-\lambda)^2=\lambda$ for all $\lambda\in\mathbb{R}_+$. Thus $r(\pi_n)=E^{\pi_n}\lambda$. I don't see how one can construct a sequence $(\pi_n)$ such that $\lambda\leq \lim r(\pi_n) I think that the reason that I'm stuck is that I've been staring myself blind at Lemma 2.4.15. Is there some other way of showing that this estimator is minimax (or isn't it?)?
