[site]: datascience
[post_id]: 57007
[parent_id]: 57005
[tags]: 
Softmax isn't a continuous mathematical function such as logistic(sigmoid), tanh or ReLU. It is used to map outputs of the last layer of a Neural Network into a probability distribution, i.e., summation of softmax squashed layer's outputs will be 1 (unity). Unlike other activation functions, softmax takes a list/array of input and maps them into probability distribution. Example : softmax([ 2.0 , 1.0 , 0.1 ]) will return [ 0.7 , 0.2 , 0.1 ] And 0.7 + 0.2 + 0.1 = 1, so if we pass softmax list with only one element, its probability of occurrence will be 1(unity), i.e., softmax([ any_value ]) will return 1. Therefore, softmax is helpful only when multiple outputs have to be squashed. function softmax(list){ // list is an array of outputs of last layer neurons of a Neural Network // numerators is an array of Math.exp() applied to each element of list array var numerators = list.map(function(e){ return Math.exp(e); }); // denominator is summation of each element of numerators array var denominator = numerators.reduce(function(p, c){ return p + c; }); // returning numerators array after dividing each element by denominator return numerators.map(function(e){ return e / denominator; }); } Softmax is helpful in classification problems .
