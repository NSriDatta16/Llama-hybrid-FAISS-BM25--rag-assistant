[site]: datascience
[post_id]: 69478
[parent_id]: 60631
[tags]: 
I would say trees are "differently" robust in this sense. A tree model will never predict a target value outside the range of those in the training set; so never a negative value for a count, or more infections than the population, etc. (Some tree-based models might, e.g. gradient boosting, but not a single tree or a random forest.) But sometimes that's detrimental, too. In your bikes example, maybe city population is another variable; your model will quickly become useless as the city grows, while a linear model may cope with the concept drift better. Finally, again in your bike example: because the tree has no reason to make rules about winter when temp>12, as @SvanBalen says, it will essentially be making up an answer if you ask it about a hot winter. In your tree's case, hot winters are treated as summers; another tree might split first on season, never considering temperature in the winter branch, so that this alternative tree will treat hot winters as winters. It seems better to try to track the independent variables' concept drift and interdependencies to recognize when the model hasn't seen enough useful training data to make accurate predictions.
