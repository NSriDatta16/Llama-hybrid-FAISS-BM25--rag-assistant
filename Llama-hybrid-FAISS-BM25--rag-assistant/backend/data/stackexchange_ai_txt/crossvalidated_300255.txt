[site]: crossvalidated
[post_id]: 300255
[parent_id]: 300220
[tags]: 
The problem would be simple when the dataset does contain enough information to easily separate the classes. This would be a situation where it is very difficult to overfit. If the data does not contain enough information to separate the classes, you should expect overfit with some models. The number of parameters alone does not necessarily imply overfitting either. Random forests for example can have very many parameters but do not overfit proportionally to this. As a sanity check, try one nearest neighbor classification. That's easy to implement and horribly overfitting. If this doesn't overfit, not much else will.
