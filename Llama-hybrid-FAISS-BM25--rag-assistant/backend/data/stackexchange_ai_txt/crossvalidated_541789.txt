[site]: crossvalidated
[post_id]: 541789
[parent_id]: 541754
[tags]: 
You have several mistakes. You're missing the minus sign due to $-\hat y_i$ term If you have $n$ samples, $x_1$ and $x_2$ should also have indices Derivative wrt $b_3$ can't be $0$ because it's incorporated in the loss term. From what I see, $\frac{\partial \hat y}{\partial b_3}=1$ Your theta vector is incomplete, I believe it should be something like below: $$\theta=[w_1,w_2,w_3,w_4,w_5,w_6,b_1,b_2,b_3]$$ You're sometimes multiplying $x_1$ with $w_3$ in the derivative expressions, which shouldn't happen at all due to the loss expression. Similarly, you're using $b_2$ in the first neuron's expression etc. All the lines have errors like this. Edit : Your differentiation and implementation is correct after you applied the changes mentioned above. The only mistake you've done is in your initialization. You've already mentioned the following: #theta = runif(9) #when using random weights it appears to converge If you initialize all the weights as equal, the first and second neuron in the first layer behave as if they're the same. So, your neural network acts as if there is only one neuron in the first layer, and is not capable of discriminating the samples anymore. You need symmetry breaking , which can be achieved by either random initialization or mechanisms similar to dropout.
