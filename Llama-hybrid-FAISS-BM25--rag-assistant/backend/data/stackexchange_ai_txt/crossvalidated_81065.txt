[site]: crossvalidated
[post_id]: 81065
[parent_id]: 80858
[tags]: 
On looking at those lecture notes, the sample size is only two - so you can hardly do better without knowing anything about f than simply averaging the two values. The notions of "bias" and "variance" can be defined only relative to some kind of model structure. This is clearly encapsulated by the $ E_x[.] $ and $ E_D[.] $ operators in the lecture notes. Note though, that they are a tad clumsy in that $ E_x [E_D [.]] $ should really be written as $ E_x [E_{D|x}]] $ as D and x are related by the model. These basically describe "how did you choose the C values" ($ E_x $) and "given the choice of X values how did you choose the Y values" ($ E_{D|x} $). Generally speaking, the latter encspsulates the model assumptions with the former generally assumed known. Now, it seems like you are talking about a problem with "no noise" - now if you know that $ y=f (x) $ exactly - then any estimator that doesn't interpolate the observed data is necessarily wrong. The only source of "randomness" is which particular "X" values and "Y" values are observed. This has a similar flavour to design based inference for sample surveys. The notion of bias in this context depends on the "sample space" for the X values, the "sampling distribution" for the X values, and the function $ f(x) $. I use quotes as it is entirely reasonable to consider degenerate cases where the X values are not "random" but fixed at prespecified values (as is the case for prediction of a new Y value not used to fit the model). Now basically you can't get any further with the bias of an estimated function $ h $ unless you impose some conditions on what the function $ f$ might look like. In fact, without the prescence of any noise this is "merely" a transformation of a random variable. If you propose/assume a "sampling distribution " for the $ X $ values, call this cdf $ G_X(x)=Pr (X\leq x) $, then the corresponding distribution for the response is $ G_Y (y)=Pr (Y\leq y)=Pr (f (X)\leq y)=\int I\{f (x)\leq y\} dG_X (x)$. For 1-to-1 continuous, differentiable functions you can simplify this further to state that the pdf for "y" must have the form $$ g_Y (y)=g_X (f^{-1}(y))|\frac {\partial f^{-1}(y)}{\partial y}|$$ where $ f^{-1}(.)$ is the inverse transformation of $ f (.) $. So for a linear function $ f (x)=a +bx $ (inverse function of $ f^{-1}(y)=b^{-1} (y-a) $ ) combined with a uniform $[-1, 1] $ pdf for $ X $ gives $$ g_Y (y)=\frac {I[-1 \leq b^{-1}(y-a) \leq 1]}{2|b|}$$ That is, Y is uniform $[a-b, a+b] $ if $ b> 0 $ and uniform $[a+b, a-b] $ otherwise. Now we can show that the mle for $ a, b $ is given by the same OLS "saturated" fit , namely $\hat {b}=\frac {y_1-y_2}{x_1-x_2} $ and $\hat {a}=\frac {y_2x_1-y_1x_2}{x_1-x_2}$. In fact these must be the exact values for $ a $ and $ b $ provided the linear function is correct - regardless of the sampling distribution for X. Another way of saying this is that there can be only one noiseless linear relationship between two or more X-Y pairs. This also leads to an extremely aggressive predictive distribution degenerate at $ \hat {y }=\hat {a}+\hat {b} x $ with zero margin for error (after observing x). The aggressive prediction comes from the "no noise" assumption. As a final remark, the observed data provides no information on what that relationship is for "X-Y" pairs that are not fully observed. This can only come from other pieces of information - such as assumptions about smoothness, and continuity of $ f (.) $. This makes calculating bias impossible in a general sense, because your answer will depend on some arbitrary unknown function. You have to assume something about what it could be in order to calculate the bias of an estimator for $ f(.) $ (eg $ f (.) $ has a third order derivative, no singularities, is analytic, etc). But these choices cannot be disentangled from the choices resulting from standard model checking (eg add a quadratic term if a plot of the residuals shows curvature). This clouds the practical use of "bias" in a rigorous and completely general fashion, as the observed data set is analysed to decide model structure. Different data sets get analysed in different ways, adding a "human element" to a bias calculation (and variance too) that is difficult to both automate (making Monte Carlo infeasible) and write down a formula for what happens. Having said that, the notion of bias is still useful as part of a check of model assumptions - but is generally better thought of in terms of complexity and stability of the model IMO. Bias is also useful as a conceptual tool to aid understanding of general model fitting issuues and the tension between explaing the observed data and predicting unobserved data.
