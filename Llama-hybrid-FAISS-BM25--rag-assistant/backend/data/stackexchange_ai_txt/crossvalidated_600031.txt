[site]: crossvalidated
[post_id]: 600031
[parent_id]: 600000
[tags]: 
This is a hard question because there are different Bayesian philosophies, different ideas of what a prior and posterior actually mean, and I'd even say there are also different versions of frequentism. There are various issues here. Why do people still compute classical confidence intervals, and why isn't a Bayesian analysis more popular? How to interpret the results of the Bayesian analysis? Given that people do a Bayesian analysis, why aren't weakly informative priors more popular? (I don't think it makes sense to ask why non-informative priors aren't more popular given that the analysis is Bayesian, because I think they are really quite popular indeed, arguably too popular, see https://statmodeling.stat.columbia.edu/2015/05/01/general-problem-noninformatively-derived-bayesian-probabilities-tend-strong/ and the answer by @Tim.) Ad (1): I believe that the requirement to specify a prior is a major issue that many have with a Bayesian approach. One reason for this can be convenience (thinking about a prior is additional work), another can be the idea that results are supposed to be objective rather than influenced by the subjective choice of a prior. A third reason is that, even if the requirement to specify a prior is accepted, in many situations existing information is of such a kind that it is very hard to translate this into a prior, and often there are various conceivable ways of doing this, and choosing one in particular is hard to justify. Note that I'm not saying that all these are good reasons, although I believe that particularly the last reason often makes a lot of sense, and in principle comprehensive sensitivity analysis would be required, exploring the implications of the choices of different priors that may all seem realistic. Regarding subjectivity vs. objectivity, the thing is that there is also subjective impact in setting up a sampling model as frequentists do, and choices such as the confidence level. There is no way to determine these objectively from the data, and therefore the idea that only a Bayesian approach is affected by subjective choices is wrong. One may also argue that there are advantages in acknowledging necessary "non-objective" aspects of model choice rather than hiding them. For example here we argue that the ability to take into account multiple perspectives and context dependence is an advantage of an approach that requires non-objective input. On the other hand, requiring additional subjective input (the Bayesian approach requires a prior on top of the other choices) isn't advantageous if it is unclear how to choose it and how to use it in an advantageous way. A prior helps if it is clear how the information encoded in the prior can improve the analysis; otherwise it is a much harder sell. Ad (2): In the question it is stated that "credible Intervals in the Bayesian setting seem to have more advantageous interpretations compared to Confidence Intervals". I'm not so sure, and the interpretation of credible intervals depends on the specific school of Bayesian thought, and often it is ignored that there is more than one. For starters, many Bayesians believe that true frequentist distributions and true parameters do not exist, in which case an interpretation in terms of the probability that the "true parameter" is in the credible interval doesn't make much sense. There can be a long discussion about this, and some Bayesians may say that if they talk about a "true parameter" they mean something else than a true objectively existing frequentist parameter, but anyway, the Bayesian marketing claim that, as opposed to confidence intervals, "credible intervals give the users what they really want", namely probabilities regarding the true parameter, is highly problematic and not very convincing in my view. If you indeed want posterior probabilities about true frequentist parameters, a Bayesian approach will have to be based on a frequentist probability concept for the sampling distribution, and it is "philosophically" difficult then to integrate this with a non-frequentist prior, at least as long as we're not in an "empirical Bayes" situation in which there is data generating process with repetition that can be interpreted convincingly as generating the parameters. In any case, credible intervals and posterior probabilities in general are conditional on the specification of the prior, and if the prior is meaningless, so is the posterior. Therefore any prior choice needs meaningful justification and interpretation if the resulting posterior (and not, for example, only the resulting point estimator) is meant to be interpreted in terms of quantifying the "real" uncertainty. This applies to non-informative priors as well - one needs to argue why there is no information that allows a more precise choice, because otherwise the resulting quantification of uncertainty is not in line with what we actually know (which is the aim of Bayesian analysis in the first place). Ad (3): Non-informative priors are actually quite popular because there are default choices (no subjective freedom!) and because users believe (in my view wrongly) that they do not need to put effort into the specification. If you choose a weakly informative prior, of course again you have to choose and justify how exactly to do it, and this makes them less popular. In fact default choices are controversial, and arguably any supposedly non-informative choice actually implicitly also encodes some information. So I don't think that they give "best of both worlds", rather the opposite, if anything (although there are situations in which they can be well motivated). The idea is that weakly informative priors encode a certain minimum amount of key information that people can easily agree on, but leave the data lots of power to determine the inference. This may often be reasonable, but doesn't solve all the problems either, see above.
