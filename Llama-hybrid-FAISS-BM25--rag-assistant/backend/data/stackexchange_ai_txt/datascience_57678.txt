[site]: datascience
[post_id]: 57678
[parent_id]: 57367
[tags]: 
Never tried this, but I think it will work: Let's say you are using a neural network. Continue to use your normal y values (1, 0, etc), but make sure to hold in memory the respective probabilities for each classification (they all should be greater than 1/number of classes). Then, once the algorithm has calculated the loss for the sample, multiply the loss by the probability for that sample, making sure to do this before any backpropagation. I think you would have to do this at a fairly low level. Tensorflow should be able to get you there, though I'm less sure if something like Keras would.
