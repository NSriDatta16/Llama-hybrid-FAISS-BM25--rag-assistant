[site]: crossvalidated
[post_id]: 552378
[parent_id]: 306695
[tags]: 
I believe I may be comepletley missing the point, since I am relativley new to the subject but i think that the math doesnt 'give' us soft alignments between similar words, but more so enables the possibility of it. I assume that the neural network "Learns" to make a soft alignment out of this particular construction since semnatic resemblence can in fact be derived from word embeddings, and moreover the ability to "focus", provide more "attention" on words of similar meaning in the question and passage truly does help the process of text comprehension. the math "enables" soft alignment since it provides a way to use information from word embeddings (since the word embeddings are used in the equation) pass them through a neural network (i.e let the neural network learn the way to maximize dot product of similar semantic values) and translate that resemblence to a probability distrubtion (using the softmax to "rank" the similarities of passage words to question words). Thus the math provides the tools to perform the soft alignment, and the soft alignment is performed since it has turned out to be valuable for reading comprehension, the training found it through it being the best way available through the existing neural architecture
