[site]: datascience
[post_id]: 81891
[parent_id]: 
[tags]: 
Is window based sequencing a good idea to obtain more training data for LSTMs?

I am trying to do an unsupervised autoencoder based outlier detection for time series using LSTMs. Here, there are multiple time series, and an entire series is to be considered as an outlier. However, I only have around 25-30 time series instances to work on (though each series comprises ~10k points). I wanted to know whether creating sliding windows for each time series to generate more data is an okay idea for training the autoencoder more accurately. Also, in that case, how does one merge the results to selectively identify which of the original 25-30 time series is an outlier? I am relatively new at working with LSTMs and would really appreciate suggestions on whether this idea is feasible.
