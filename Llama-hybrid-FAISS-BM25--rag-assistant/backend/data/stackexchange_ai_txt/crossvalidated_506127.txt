[site]: crossvalidated
[post_id]: 506127
[parent_id]: 
[tags]: 
Trimmed, weighted mean

The trimmed mean (or truncated mean) is a robust version of the mean, designed to be robust to outliers. I am wondering what is the right trimmed version of a weighted average. If I have a sample with data points $x_1,\dots,x_n$ , and I want to trim one point from each end (i.e., be robust to a single outlier), the trimmed mean is $$\text{trimmed mean} = {1 \over n-2} \sum_{i=2}^{n-1} x_{(i)},$$ where $x_{(i)}$ is the $i$ th largest value from the sample (i.e., put $x_1,\dots,x_n$ in increasing order, then call them $x_{(1)},\dots,x_{(n)}$ ). This is robust to a single outlier (a single value that gets wildly corrupted), and also robust to small errors in any single value (small errors in a single value cause only small changes in the trimmed mean). This makes sense to me. Suppose instead of computing a mean, I want to compute a weighted mean, but I want to do it in a way that is robust. In other words, I have data points $x_1,\dots,x_n$ with weights $w_1,\dots,w_n$ . The weighted mean is $$\text{weighted mean} = \sum_{i=1}^n w_i x_i.$$ This is not robust to outliers; a single value that is corrupted by a large amount can cause an unbounded error in the weighted mean. What is the robust version of the trimmed mean, for weighted means? The obvious scheme would be to do the same as for the ordinary trimmed mean, and discard the smallest and largest $x$ -value: $$\text{candidate} = {\sum_{i=2}^{n-1} w_{(i)} x_{(i)} \over \sum_{i=2}^{n-1} w_{(i)}},$$ where $x_{(i)}$ is as defined above and $w_{(i)}$ is the weight associated to $x_{(i)}$ . This too is robust to a single wildly corrupted value. However, it smells a little bit dubious to me. For instance, if I have the values $0,1,100,199,200$ in my sample, with weights $1,1,1,1000,1$ , then this would give me $(1 \times 1 + 1 \times 100 + 1000 \times 199)/1002$ , and small changes or errors in the value 199 would have a fairly large influence on the final mean. Perhaps that is just a consequence of having a large weight, but it makes me wonder whether I have the right notion of weighted, trimmed mean. I wonder if perhaps instead I should be looking at both the values and the weights when deciding which value to trim. For example, I could sort $w_1x_1,\dots,w_nx_n$ in increasing order, letting the result be $w_{[1]}x_{[1]},\dots,w_{[n]}x_{[n]}$ , and then use $$\text{alternative} = {\sum_{i=2}^{n-1} w_{[i]}x_{[i]} \over \sum_{i=2}^{n-1} w_{[i]}}.$$ This too is robust to a single outlier (a single value that is wildly corrupted cannot cause unbounded error in the final estimate), but seems like it might better limit the influence of small errors in any one data point. However it too smells a bit dubious to me for reasons I am not sure how to articulate. Is there a "right" generalization of the trimmed mean, to weighted means? Or how should I think about the options and their tradeoffs? I have difficulty articulating the goals or requirements clearly. I suspect there are at least three that one might hope for: breakdown point (a change or error to any one data point cannot cause unbounded change to the estimate), local sensitivity (a small change or error to any one data point will have limited influence over the estimate, to the extent possible), and efficiency (low variance in the estimate); but I'm not sure whether I'm thinking clearly about the goals.
