[site]: crossvalidated
[post_id]: 434689
[parent_id]: 
[tags]: 
Why is $\frac1n\sum_{i=0}^{n-1}\sum_{j=0}^i1_{\{\:X_i\:=\:Y_j\:\}}f(Y_j)$ an equivalent representation for the usual Metropolis-Hastings estimator?

At the beginning of section 2 of the paper A Vanilla Rao-Blackwellization of Metropolis-Hastings Algorithms , the usual Metorpolis-Hastings estimator of $\int f$ given by the ergodic average $\frac1n\sum_{i=0}^{n-1}f(X_i)$ may equivalently be written as $\frac1n\sum_{i=0}^{n-1}\sum_{j=0}^i1_{\left\{\:X_i\:=\:Y_j\:\right\}}f(Y_j)$ , where $(X_n)_{n\in\mathbb N_0}$ is the chain generated by the algorithm, $(Y_n)_{n\in\mathbb N}$ is the corresponding sequence of proposals and $Y_0:=X_0$ . However, while the idea behind this equivalent representation is clear to me, I don't understand why it holds. Couldn't it be the case that, for a particular outcome $\omega$ , $X_i(\omega)=Y_j(\omega)=Y_k(\omega)$ and hence $f(Y_j(\omega))$ is mistakenly counted (at least) twice compared to the ergodic mean? EDIT : Let's try to figure out if we can prove $$\operatorname P[\exists 1\le i as suggested in Taylor's answer . It would be sufficient to show that, given $1\le m , $$\operatorname P\left[Y_m=Y_n\right]=0\tag2.$$ In order for this to make sense, we technically need to assume that $\Delta:=\left\{(x,x):x\in E\right\}\in\mathcal E^{\otimes2}$ , where $(E,\mathcal E)$ denotes the state space. We know that $$Z_k:=(X_{k-1},Y_k)\;\;\;\text{for }k\in\mathbb N$$ is a time-homogeneous Markov chain with transition kernel $$\kappa_{\text{aug}}((x,y),A\times B):=(1-\alpha(x,y))\delta_x(A)Q(x,B)+\delta_y(A)\alpha(x,y)Q(y,B)$$ for $x,y\in E$ and $A,B\in\mathcal E$ , where $\alpha$ denotes the acceptance function of the algorithm.. Thus, $$(Z_m,Z_n)\sim\mathcal L(Z_m)\otimes\kappa_{\text{aug}}\tag3$$ and $$\mathcal L(Z_m)\sim\mathcal L(X_{m-1})\otimes Q\tag4,$$ where $Q$ denotes the proposal kernel. Assume $Q$ and the target distribution $\mu$ have a density $q$ and $p$ with respect to a common reference measure $\lambda$ . For simplicity, let's focus on the case $n-m=1$ . Then, \begin{equation}\begin{split}&\operatorname P\left[Y_{n-1}=Y_n\right]=\operatorname P\left[(Y_{n-1},Y_n)\in\Delta\right]\\&\;\;\;\;=\operatorname P\left[X_{m-1}\in{\rm d}x_1\right]\int Q(x_1,{\rm d}y_1)\int\kappa_{\text{aug}}((x_1,y_1),{\rm d}(x_2,y_2))1_\Delta((y_1,y_2))\end{split}\tag5\end{equation} and \begin{equation}\begin{split}&\int\kappa_{\text{aug}}((x_1,y_1),{\rm d}(x_2,y_2))1_\Delta((y_1,y_2))\\&\;\;\;\;=(1-\alpha(x_1,y_1))\int Q(x_1,{\rm d}y_2)1_\Delta((y_1,y_2))+\alpha(x_1,y_1)\int Q(y_1,{\rm d}y_2)1_\Delta((y_1,y_2))\end{split}\tag6\end{equation} for all $x_1,y_1\in E$ . How can we show that $(5)$ (or maybe already $(6)$ ) vanishes?
