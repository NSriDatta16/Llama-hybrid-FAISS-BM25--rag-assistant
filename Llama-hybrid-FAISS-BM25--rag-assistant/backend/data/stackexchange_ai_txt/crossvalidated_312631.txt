[site]: crossvalidated
[post_id]: 312631
[parent_id]: 194035
[tags]: 
Conceptually : I don't know. I believe Bayesian statistics is the most logical way to think but I coudn't justify why. The advantage of frequentist is that it is easier for most people at elementary level. But for me it was strange. It took years until I could really clarify intellectually what a confidence interval is. But when I started facing practical situations, frequentist ideas appeared to be simple and highly relevant. Empirically The most important question I try to focus on nowadays is more about practical efficiency: personal work time, precision, and computation speed. Personal work time: For basic questions, I actually almost never use Bayesian methods: I use basic frequentist tools and will always prefer a t-test over a Bayesian equivalent that would just give me a headache. When I want to know if I'm significantly better at tictactoe than my girlfriend, I do a chi-squared :-). Actually, even in serious work as a computer scientist, frequentist basic tools are just invaluable to investigate problems and avoid false conclusions due to random. Precision: In machine learning where prediction matters more than analysis, there is not an absolute boundary between Bayesian and frequentist. MLE is a frequentist approcah: just an estimator. But regularized MLE (MAP) is a partially Bayesian approach : you find the mode of the posterior and you don't care for the rest of the posterior. I don't know of a frequentist justification of why use regularization. Practically, regularization is sometimes just inevitable because the raw MLE estimate is so overfitted that 0 would be a better predictor. If regularization is agreed to be a truly Bayesian method, then this alone justifies that Bayes can learn with less data. Computation speed: frequentist methods are most often computationally faster and simpler to implement. And somehow regularization provides a cheap way to introduce a bit of Bayes in them. It might be because Bayesian methods are still not as optimized as they could. For example, some LDA implementations are fast nowadays. But they required very hard work. For entropy estimations, the first advanced methods were Bayesian. They worked great but soon frequentist methods were discovered and take much less computation time... For computation time frequentist methods are generally clearly superior. It is not absurd, if your are a Bayesian, to think of frequentist methods as approximations of Bayesian methods.
