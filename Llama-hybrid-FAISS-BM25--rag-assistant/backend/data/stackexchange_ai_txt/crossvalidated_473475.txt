[site]: crossvalidated
[post_id]: 473475
[parent_id]: 473219
[tags]: 
First, "efficient representation" is a very vague term that you should avoid. For instance, one-hot encoding is way more efficient in terms of memory. It takes way, way more memory to encode 300 real-values than a single integer from 1 to $|V|$ . On the other hand, one-hot representation of words are not efficient statistically, because words are distributed according to heavy tailed distributions. That is, if you take a random word in your corpus, it will likely be a rare words. Conversely, frequent words account for a lot of the total number of occurences. The consequence is clear on the following task. You want to classify text. If you use logistic regression on the sum of one-hot vectors (bag-of-word representation), the parameters associated with rare words will be fit based on very little data and the classifier might be unreliable. On the other hand, if you use the sum of word embeddings (bag-of-vector) there is no component that is estimated with little data so it should be more reliable.
