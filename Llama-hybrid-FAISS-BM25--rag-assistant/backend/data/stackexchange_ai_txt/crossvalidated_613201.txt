[site]: crossvalidated
[post_id]: 613201
[parent_id]: 613193
[tags]: 
That depends on what exactly you want to evaluate. If you want to evaluate how well the model predicts $h$ steps ahead, instead of a single split into training and test you could use rolling or expanding windows (see e.g. section 5.10 of Hyndman & Athanasopoulos "Forecasting: Principles and Practice"). You would apply a loss function on the prediction errors (or more generally, the predictions and the actual values) and would obtain the distribution of losses over the test cases. You could then take some summary metric of the loss distribution such as mean loss. If you want to evaluate how well the model predicts for several different horizons $h$ , you could do the same for each horizon and then construct a weighted average of mean losses (or some other combination) depending on how important the accuracy of the predictions for each horizon is. E.g. if one step ahead is twice as important as two steps ahead, you could either reflect it in the loss function (double the loss for one step ahead relative to two steps ahead) or by weighting the mean losses of the different horizons accordingly (double weight vs. unit weight).
