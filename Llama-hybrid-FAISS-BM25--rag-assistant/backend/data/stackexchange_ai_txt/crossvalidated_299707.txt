[site]: crossvalidated
[post_id]: 299707
[parent_id]: 
[tags]: 
training error remains constant but testing error decreasing

I am training a SVM and apply the sklearn learning_curve function on order to get the training and testing error curves. However, although the testing error is decreasing finely, the training error is not growing, as it would be expected to do. My dataset contains around 25,000 samples, distributed in 9 classes and there are 6 attributes. Here is the code : dataset=np.loadtxt("data",delimiter=",") X=dataset[:, 0:6] Y=dataset[:,6] train_sizes, train_scores, test_scores=learning_curve(SVC(),X,Y,train_sizes=np.arange(500,15000,500)) train_scores_mean = np.mean(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) plt.figure(figsize=(8, 8)) plt.subplots_adjust() plt.title("Erreur d'entrainement et de test" ) plt.plot(train_sizes,1-train_scores_mean,label="train error") plt.plot(train_sizes,1-test_scores_mean,label="test error") plt.legend() plt.show() and the graph is joined. Could someone help me to understand why the training error is not growing ? Thanks ] 1
