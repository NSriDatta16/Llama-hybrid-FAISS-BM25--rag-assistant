[site]: datascience
[post_id]: 117893
[parent_id]: 
[tags]: 
Can a Simple Neural Network Predict a 0 or 1 Output by Looking Only at the Last Input?

I wrote a simple neural network that functions similarly to many of the C# examples I've seen online. It uses weights and biases and can be trained using backpropagation. It works well for predicting XOR. But I want to know if it can look at a series of inputs and always return the last input as the output. For example, I want it to predict like this: Input: [ 1, 2, 0] -> Output: [0] Input: [1000, 2000, 0] -> Output: [0] Input: [ 1, 2, 1] -> Output: [1] Input: [1000, 2000, 1] -> Output: [1] (My actual data is much more randomized. Notice that the last input becomes the output.) I just wanted to see if I could teach the neural network to ignore all inputs except the last one. Ideally, all weights except those attached to the last input would be 0. At least, that was my hope. (I think this is obvious, but I don't want to set the weights manually. I want the network to "learn" the weight.) Can someone recommend how something like this could be accomplished? I'm currently using 3 input neurons, and one ReLU hidden layer with 3 neurons. I'm using sigmoid in my output layer with 1 neuron. I'm also generating weights in the range of -.5 to .5. I've experimented with different combinations of functions and randomized weights without success. Thank you.
