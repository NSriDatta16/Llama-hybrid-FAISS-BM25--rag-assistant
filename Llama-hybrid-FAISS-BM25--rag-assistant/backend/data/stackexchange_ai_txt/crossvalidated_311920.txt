[site]: crossvalidated
[post_id]: 311920
[parent_id]: 
[tags]: 
How do I prove the following equality?

Let $\Phi = \begin{bmatrix} 1 & \varphi_1(x_1) & \varphi_2(x_1) & \dots & \varphi_{M-1}(x_1) \\ 1 & \varphi_1(x_2) & \varphi_2(x_2) & \dots & \varphi_{M-1}(x_2) \\ \vdots \\ 1 & \varphi_1(x_N) & \varphi_2(x_N) & \dots & \varphi_{M-1}(x_N) \end{bmatrix}=\begin{bmatrix}\phi(x_1)^T\\ \vdots\\ \phi(x_N)^T\end{bmatrix}$ where $\varphi_m(x)$ is the $m^{th}$ basis function for regression $y = 1 + \sum_{m=1}^{M-1}w_m\varphi_m(x)$ How can I prove that $\sum_{n=1}^Nk(x,x^n)=\sum_{n=1}^N\phi(x)^T(\Phi^T\Phi)^{-1}\phi(x_n)=1$ where $k$ is the equivalent kernel for Bayesian linear regression. $w_m$ are the regression coefficients whose MLE solutions are already included in the equivalent kernel. [Update] Okay. Looks like I have done a lousy job describing my question. My apologies. My question is really simple and it does not need more context than what I have already described. All I want to prove is given a simple design matrix of say for example a ${M-1}^{th}$ order polynomial regression $\Phi=$ $\begin{bmatrix} 1 & x_1 & x_1^2 & \dots & x_1^{M-1} \\ 1 & x_2 & x_2^2 & \dots & x_2^{M-1} \\ \vdots \\ 1 & x_N & x_N^2 & \dots & x_N^{M-1} \end{bmatrix}$ which can be written in a more compact form $\Phi=\begin{bmatrix}\phi(x_1)^T\\ \vdots\\ \phi(x_N)^T\end{bmatrix}$ where $\phi(x_n)^T$ is the $n^{th}$ row of $\Phi$, $\phi(x_n)^T=[1,x_n,...,x_n^{M-1}]$. So each row of $\Phi$ is a mapping of an input x to an M-dimensional feature space which is spanned by the columns of $\Phi$. Also given a new $x^*$ with its corresponding mapping to the M-dimensional space, $\phi(x^*)^T=[1,x^*,...,{x^*}^{M-1}]$, prove that the following matrix operation yields a scalar 1 . $\sum_{n=1}^N\phi(x^*)^T(\Phi^T\Phi)^{-1}\phi(x_n)=1$
