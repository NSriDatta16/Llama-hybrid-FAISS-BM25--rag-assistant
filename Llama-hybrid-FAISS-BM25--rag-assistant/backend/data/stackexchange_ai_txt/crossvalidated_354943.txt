[site]: crossvalidated
[post_id]: 354943
[parent_id]: 354046
[tags]: 
Perhaps this question is more difficult than I thought, or would be better asked on math.stackexchange.com. But since it arose in the context of SVM I will leave it here. There are some papers discussing coordinate descent and constrained optimization with the following key words. Linear and box constraints and blocks of primal variables here Coupled constraints, Compact convex sets and Polyhedra here A cryptic MIT paper While the mathematics are beyond me, I will proceed to show visually and experimentally that for the following toy, convex function, coordinate descent works for box, linear and highly non linear constraints. Consider the function to be minimized: $$f(x_1,x_2) = .01x_1^2 + 0.03x_2^2$$ $$\nabla f = [2 \times 0.01 x_1, 2 \times .03 x_2]$$ Since the gradient has only terms in $x_1$ and $x_2$ respectively, we can evaluate it at each step $k$ holding the other variable constant. Checking the constraint at each step allows to Do nothing if $x_i^{(k+1)}$ has a value outside the constraint boundary Update $x_i^{(k+1)} = x_i^{(k)} - \alpha \ \nabla_i \ f(x_1, x_2)$ otherwise A few examples 1) Box and 2) linear constraints $c1: \ x_1 \leq -1 \ \ c2: \ x_2 \leq -1.5 $ $c: \ x_1 + x_2 \leq -2$ 3) Non linear constraint $c: \ (x_1 + 1.5)^3 + (x_2 + 1.5)^3 \leq 2$ Next steps Perhaps a function which is convex, but whose gradient has cross terms $x_1x_2$ will prove more difficult or even impossible to optimize using CD
