[site]: crossvalidated
[post_id]: 177376
[parent_id]: 177273
[tags]: 
The comments to this thread, including the great Kleist reference, led me down a slightly different path. One of Simonsohn, et al's (and others, going back at least to Neyman) critiques of current practice is the obsession with alpha and Type I errors -- aka "p-hacking." One antidote to this ongoing discussion is a recent Andrew Gelman post citing a Carl Morris article underscoring the importance of Type II errors and statistical power in driving confidence in one's findings. http://andrewgelman.com/wp-content/uploads/2015/07/morris_example.pdf In addition, there is Regina King's recent article in Science that does a good job of decomposing the controversy. http://www.nature.com/news/scientific-method-statistical-errors-1.14700 However, the original question concerned the stability of model results, not p-hacking or confidence. In my opinion, "stability" is an under-researched area in statistics (I would be interested in other's thoughts about that). I wanted to use this answer to develop some operational and applied heuristics for quantifying this otherwise vague notion. Some of the quickest and easiest diagnostics wrt model instability is to decompose any collinearity in the model's structure since, as is well known, collinearity can lead to inflated standard errors which can contribute to unstable results. One of the myths about collinearity is that it's a product of small sample sizes, as is common with psychological research. Another myth is that it can be diagnosed with pairwise correlations. Neither is true. Collinearity is just as likely to occur in models with huge amounts of data where every relationship is statistically significant as it is in small datasets. Some of the best diagnostic evidence for underlying collinearity are partial or semi-partial correlations, not pairwise measures. Other diagnostics include "wrong" signed parameters, VIFs and eigenvalue decompositions that go back to Belsey, Kuh and Wallace's 80s book Regression Diagnostics . A completely different and much more CPU intensive approach would be to leverage Monte Carlo simulation of your data -- Bayesian methods also work here. Based on the random draws, a range of parameters and outcomes across differing data "landscapes" can be generated which would provide empirical evidence towards understanding the stability of key model results. This could be done based on a coefficient of variation of the resulting simulated metrics, suggesting differential model strengths and weaknesses as a function of the magnitude of the variability. "Tornado"-type visualizations of this variability would highlight the range of significance in outcomes across all of the draws. This would help delineate those areas or combinations of model inputs and data that drive instability. This may seem a bit vague as a solution to some but it is only intended as a heuristic. All of this work should only serve to deepen one's understanding of the model. I'm very interested in hearing the suggestions of others.
