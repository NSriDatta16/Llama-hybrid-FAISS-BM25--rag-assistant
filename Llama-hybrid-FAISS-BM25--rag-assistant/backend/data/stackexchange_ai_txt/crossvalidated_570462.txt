[site]: crossvalidated
[post_id]: 570462
[parent_id]: 570456
[tags]: 
This depends a lot on the range of possible objective functions. For arbitrary objective functions, there is no method better than enumeration of all the $2^{len(X)}$ possible filters. For well-behaved functions $A$ it might work to first generalise from boolean filters to a differentiable version of the problem, where the filter assigns a real weight in $[0.0, 0.1]$ to each row. In that case, you can try to solve by gradient descent. Alternatively, if there are reasons to stick with a discrete filter, the variety of function $A$ that you are dealing with might leave simulated annealing as a reasonable optimization strategy. Or even genetic programming, if there is a way of deriving a "child" filter from two "parent" filters, that makes sense in your domain. There's a no free lunch theorem for all machine learning - with no domain knowledge, no learning.
