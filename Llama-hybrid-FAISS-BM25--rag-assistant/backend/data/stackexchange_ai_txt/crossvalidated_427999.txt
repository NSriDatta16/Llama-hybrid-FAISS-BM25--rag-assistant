[site]: crossvalidated
[post_id]: 427999
[parent_id]: 202697
[tags]: 
Now, I am not particularly interested in the classification task per se, bu in testing a hypothesis about the predictors. I have two sets of predictors, say A and B, and if my theory is right then set A should play a role in the classification task, but set B should not play a role in the classification task. What exactly would it prove? Imagine that instead of using only the neural network, you used neural network and XGBoost, and one of the algorithm would work better on set A, while the other, on set B, what would that prove? That something "should not play a role in the classification task" is not a valid hypothesis to test. Trying to answer this would give you the wrong answer, to the wrong question . Moreover, machine learning algorithms are in many cases sensitive to the choice of hyperparameters. How would you tune the hyperparameters? Would you tune them in terms of model performance on set A, or on set B? This would give you biased result. You could choose to tune based on overall performance, but how would you measure it? If you would sum the error metrics from A and B, then this does not prevent one of the sets to having greater influence then the other (e.g. better parameter choice of the set with worse initial error would lead to greater improvement in performance, so will have greater weight on overall score).
