[site]: crossvalidated
[post_id]: 254579
[parent_id]: 254574
[tags]: 
SVMs, like many other linear models, are based on empirical risk minimization, which leads us to an optimization of this sort: $$\min_w\sum\ell(x_i, y_i, w)+\lambda\cdot r(w)$$ Where $\ell$ is a loss function (the Hinge-loss in SVMs) and $r$ is a regularization function. The SVM is a squared $\ell_2$-regularized linear model, i.e. $r(w) = \|w\|^2_2$. This guards against huge coefficients, as one would say in regression terms, as the coefficient magnitudes are themselves penalized in the optimization. Besides that, the regularization allows for a unique solution in the $p> n$ situation, so the 1st statement is true. The problem when $p \gg n$ is that the bias introduced by regularization can be so large towards the training data the model heavily underperforms. It doesn't mean SVMs can't be used in that scenario (they are usually employed in gene expression data, for example, where $p$ can be thousands times larger than $n$). So I see no contradiction in statements. The 2nd statement is more likely a warning against overfitting.
