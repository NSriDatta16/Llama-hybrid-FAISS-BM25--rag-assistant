[site]: crossvalidated
[post_id]: 77023
[parent_id]: 77018
[tags]: 
A random forest is not considered a boosting type of algorithm. As explained in your boosting link: ...most boosting algorithms consist of iteratively learning weak classifiers with respect to a distribution and adding them to a final strong classifier. When they are added, they are typically weighted in some way that is usually related to the weak learners' accuracy. After a weak learner is added, the data is reweighted... An example of this iterative process is adaboost, whereby weaker results are boosted or reweighted over many iterations to have the learner focus more on areas it got wrong, and less on those observations that were correct. A random forest, in contrast, is an ensemble bagging or averaging method that aims to reduce the variance of individual trees by randomly selecting (and thus de-correlating) many trees from the dataset, and averaging them.
