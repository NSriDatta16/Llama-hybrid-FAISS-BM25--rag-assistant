[site]: crossvalidated
[post_id]: 330578
[parent_id]: 295809
[tags]: 
If I understood your question properly, then what you have thought of vanishing gradient in RNN is wrong. In RNN we known compute the current timestep value based on previous output values . The main reason we compute gradients of the weights is to see how much of influences does it make on the network's output. In RNN we take in previous output to compute current value. The above image is from wildml where Denny Britz has written elegantly. The red lines in the image is where we calculate the gradients. So while we compute chain rule to find the gradients, if the previous time step computation we take keeps on increasing then the gradient values will get small. This is known as Vanishing Gradient and a problem in RNN. We use LSTM, GRU to solve these problems.
