[site]: crossvalidated
[post_id]: 299340
[parent_id]: 
[tags]: 
Uncertainty in Binary Classification of New Data (via Random Forest)

We trained a binary classification RF and validated it with a test set of about N=300 entries. Here are the performance statistics: We now would like to classify a production set of a couple of million items. If we just do that, the output of that will be p TRUE and 1-p FALSE where p lies within [0,1] . However , because we have a certain FP- and FN-Rate, this result/ratio comes with some uncertainty , right? For that reason, we would like to quantify this uncertainty, e.g. with a 95% confidence interval, based on our performance measures. E.g. something like: With 95% probability, 20-25% of items in the production set are TRUE , 75-80% are FALSE . These numbers probably don't even add up, the range for TRUE would be sufficient. Also, all these performance measures displayed above come with their own confidence intervals, maybe these could be considered too in that uncertainty calculation? Bonus-Question : We used the caret package in R. Is there some sort of a function (in caret or in another R package) to do this automatically?
