[site]: stackoverflow
[post_id]: 693417
[parent_id]: 691922
[tags]: 
Here's an outline for a clustering algorithm that doesn't have the K-means requirement of finding a centroid. Determine the distance between all objects. Record the n most separate objects. [ finds roots of our clusters, time O(n^2) ] Assign each of these n random points to n new distinct clusters. For every other object: [ assign objects to clusters, time O(n^2) ] For each cluster: Calculate the average distance from a cluster to that object by averaging the distance of each object in the cluster to the object. Assign the object to the closest cluster. This algorithm will certainly cluster the objects. But its runtime is O(n^2) . Plus it is guided by those first n points chosen. Can anyone improve upon this (better runtime perf, less dependent upon initial choices)? I would love to see your ideas.
