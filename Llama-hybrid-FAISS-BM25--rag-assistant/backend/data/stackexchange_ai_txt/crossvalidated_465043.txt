[site]: crossvalidated
[post_id]: 465043
[parent_id]: 
[tags]: 
AIC for logistic regression

On page 231 of The Elements of Statistical Learning AIC is defined as follows in (7.30) Given a set of models $f_\alpha(x)$ indexed by a tuning parameter $\alpha$ , denote by $\overline{err}(\alpha)$ and $d(\alpha)$ the training error and number parameters for each model. Then for a set of models we define $$AIC(\alpha) = \overline{err}(\alpha) + 2 \cdot \frac{d(\alpha)}{N}\hat{\sigma_\epsilon}^2$$ Where $\overline{err}$ , the training error, is $\frac{1}{N}\sum_{i=1}^NL(y_i,\hat{f}(x_i))$ . On the same page it is stated (7.29) that For the logistic regression model, using the binomial log-likelihood, we have $$AIC = -\frac{2}{N} \cdot \text{loglik} + 2 \cdot \frac{d}{N}$$ where " $\text{loglik}$ " is the maximised log-likelihood. The book also mentions that $\hat{\sigma_\epsilon}^2$ is an estimate of the noise variance, obtained from the mean-squared error of a low-bias model. It is not clear to me how the first equation leads to the second in the case of logistic regression? In particular what happens to the $\hat{\sigma_\epsilon}^2$ term? Edit I found in a later example in the book (on page 241) the authors use AIC in an example and say For misclassification error we used $\hat{\sigma_{\epsilon}}^2=[N/(N-d)] \cdot \overline{err}(\alpha)$ for the least restrictive model ... This doesn't answer my question as it doesn't link the two aforementioned expressions of AIC, but it does seem to indicate that $\hat{\sigma_{\epsilon}}^2$ is not simply set to $1$ as stated in Demetri's answer.
