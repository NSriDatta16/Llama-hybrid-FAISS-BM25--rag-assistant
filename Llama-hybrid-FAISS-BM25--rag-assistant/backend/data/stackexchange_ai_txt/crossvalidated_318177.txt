[site]: crossvalidated
[post_id]: 318177
[parent_id]: 318140
[tags]: 
The likelihood is a two variables function $L(\theta,x)$. For fixed $\theta$ , this function can be seen as a function of $x$, and this is a distribution: the distribution of $x$ for this fixed $\theta$. For fixed $x$ , this function can be seen as a function of $\theta$, and this should not be thought as a distribution. Most often it is not formally a distribution since it does no sum to 1. Sometimes it may formally be a distribution and sum to 1 (see Xi'an answer), but it can be thought as an "accident". Actually, when using the word "likelihood" we most often implicitly mean that we look at it as a function of $\theta$ for fixed $x$, thus it makes sense saying the likelihood is not a distribution. Bayesian inference is a useful way however to understand that in spite of not being a distribution it is related to a distribution. In Bayesian inference the distribution of $\theta$ for fixed $x$, known as the posterior, is : $$p(\theta|x)=p(\theta)L(\theta,x)c(x)$$ where: $p(\theta)$ is the prior $L(\theta,x)$ is the likelihood $c(x)$ is a normalization constant (that is not very important) The likelihood can be thought as the distribution of $\theta$ for fixed $x$ (actually proportional to it) for the special case where the prior is uniform (constant). Generally, the likelihood is not the posterior, but the function by which you multiply the prior to get the the posterior. It plays a key role in defining a distribution (the posterior) while not being a distribution.
