[site]: crossvalidated
[post_id]: 48641
[parent_id]: 48512
[tags]: 
You cannot turn a sample of 2000 into 6000, no matter how many times you measure it. So fundamentally I think you cannot do what you are trying to do within the constraints you have set out. Worse, I think even fitting three separate regressions and reporting them separately is dangerous and potentially misleading too, if it is thought these analyses are somehow independent checks on eachother. I see two options: analyse the data in one go with a mixed effects model with subject as a random effect with 2000 values and measurement time a fixed effect with 3 values, allowing for interactions between measurement time and your 14 explanatory variables. Then you will need to write your own code to "decompose" the $R^2$ (although this decomposition is not something I'd recommend in any event). combine the data in some other way before conducting a single regression eg by taking the average measurement (out of the three observations) for each of your 2000 points. Then you could continue to use the relaimpo package. The second approach would only make sense if you expect the system to be fundamentally stable, and all you are trying to do in your multiple measurements is reduce measurement error. Some careful thinking about all the sources of randomness would be necessary before you did any inference from this. The first approach would be my preferred one, but I would recommend at least looking for some alternative way of exploring the relative contributions of explanatory variables other than decomposing the $R^2$ (not sure what - the problem might be intractable). In either case, I have all sorts of philosophical problems with the idea of explanatory variables being allocated a share of $R^2$ in either case (it would seem to be highly vulnerable to slightly different selection of explanatory variables when there is any collinearity), but it seems to be an approach that is used at least in some quarters.
