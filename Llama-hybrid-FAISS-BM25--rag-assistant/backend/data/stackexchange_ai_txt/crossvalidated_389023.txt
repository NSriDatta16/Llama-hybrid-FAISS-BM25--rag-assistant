[site]: crossvalidated
[post_id]: 389023
[parent_id]: 71489
[tags]: 
I find it hard to agree that FDA is LDA for two-classes as @ttnphns suggested. I recommend two very informative and beautiful lectures on this topic by Professor Ali Ghodsi: LDA & QDA . In addition, page 108 of the book The Elements of Statistical Learning ( pdf ) has a description of LDA consistent with the lecture. FDA To me, LDA and QDA are similar as they are both classification techniques with Gaussian assumptions. A major difference between the two is that LDA assumes the feature covariance matrices of both classes are the same, which results in a linear decision boundary. In contrast, QDA is less strict and allows different feature covariance matrices for different classes, which leads to a quadratic decision boundary. See the following figure from scikit-learn for an idea how the quadratic decision boundary looks. Some comments on the subplots : Top row: when the covariance matrices are indeed the same in the data, LDA and QDA lead to the same decision boundaries. Bottom row: when the covariance matrices are different, LDA leads to bad performance as its assumption becomes invalid, while QDA performs classification much better. On the other hand, FDA is a very different species, having nothing to do with Gaussion assumption. What FDA tries to do is to find a linear transformation to maximize between-class mean distance while minimizing within-class variance . The 2nd lecture explains this idea beautifully. In contrast to LDA/QDA, FDA doesn't do classification, although the features obtained after transformation found by FDA could be used for classification, e.g. using LDA/QDA, or SVM or others.
