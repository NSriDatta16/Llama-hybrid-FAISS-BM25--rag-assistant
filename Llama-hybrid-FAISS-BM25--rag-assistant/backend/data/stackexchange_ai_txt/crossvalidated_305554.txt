[site]: crossvalidated
[post_id]: 305554
[parent_id]: 305306
[tags]: 
As you said in the comments, (at least in the training set) there are no conflicting patterns. Thus, it is a little unusual to talk about cross entropy when there is no confusion (entropy) in the data. A more natural choice could have been using a hinge loss (as SVMs do). On the other hand, Logistic Regression (LR) may also yield good results even if you do not expect a distribution on $P(y|X)$. In fact, results from LR may be useful if you need some sort of "confidence" on your classification. It is easier to interpret a probability than a distance to a hyperplane in a large dimensional space (in the case of the hinge loss). Or you may desire to expand your results beyond binary classification and LR can do this naturally. Without being familiar with your work it is impossible for us Internet strangers to guess for what reasons you chose cross entropy for your problem. Aside from that (and perhaps more importantly), the qualification (and thesis/dissertation) committees also evaluate your mastery of the concepts used in your work. Thus, to obtain a Msc or Phd you need more than merely good results based on experimentation and/or trial and error. You need to understand your results and have an intuition or guiding principle on why you did what you did. In this setting there are many questions without a unique correct answer. It is more like a discussion than an exam. There are many possibilities on why this professor asked this question, but the important part is that you should be able to explain your design decisions and argue why you chose a certain method over other viable options.
