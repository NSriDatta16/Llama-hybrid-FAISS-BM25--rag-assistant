[site]: datascience
[post_id]: 120829
[parent_id]: 120826
[tags]: 
How are you using the word as a dimension? For many NLP tasks, you typically want to generate an embedding to represent the word, where the embedding dimension is going to be much smaller than the total vocabulary size. This would look something like the following for OpenAI embeddings or HuggingFace GTR-T5 embeddings: encoder = "text-embedding-ada-002" def get_openai_embedding(text: str, model: str=encoder) -> list[float]: result = openai.Embedding.create( model=encoder, input=text ) return result["data"][0]["embedding"] def get_t5_embedding(text: str, model: str=encoder) -> list[float]: model = SentenceTransformer('sentence-transformers/gtr-t5-xxl') embeddings = model.encode(text) return embeddings def get_embedding(text: str, model: str=encoder) -> list[float]: if encode_with_openai: return get_openai_embedding(text, model) else: return get_t5_embedding(text, model) Also, if it's feasible, you might want to investigate vector databases instead of SQL - you generate an embedding for a query, and pass the embedding to the vector dB which will return the most similar documents. I've used PineCone and Weaviate but there are many others. OpenAI has some example notebooks here: https://github.com/openai/openai-cookbook/tree/main/examples/vector_databases
