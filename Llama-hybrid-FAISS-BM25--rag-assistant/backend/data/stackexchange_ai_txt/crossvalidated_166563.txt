[site]: crossvalidated
[post_id]: 166563
[parent_id]: 
[tags]: 
What's the current thinking on selecting model complexity in the statistical community?

I was watching a recent presentation by a neural networks researcher who recommended using a model more complex than would be suggested by the data, and regularizing the life out of it. He said this yielded better results in machine learning tasks than matching the complexity to the data, as is the tradition. I realize this is a bit hand-wavey since it comes from a high-level presentation, so does anybody have something concrete to add; relevant papers or a mathematical argument?
