[site]: crossvalidated
[post_id]: 225514
[parent_id]: 
[tags]: 
Family of flexible parametric mappings $f_\theta:(0,1) \rightarrow \mathbb{R}$?

For the purpose of reparameterizing a model (mostly with the goal of improving MCMC efficiency), I am looking for a family of flexible parametric mappings $f_\theta:(0,1) \rightarrow \mathbb{R}$ such that $f$ is able to express (approximations of) common nonlinear transforms (logarithmic, exponential, powers, etc.), including linear, for different values of $\theta$ ($\theta \in \mathbb{R}^k$, for some low $k$). For motivation, the goal of the reparameterization is to make the underlying pdf more regular and less non-stationary -- e.g., such that the optimal length scale of a MCMC algorithm such as Metropolis-Hastings could remain the same across the parameter space. Ideally, the "perfect" transformation family could transform any pdf $g$ (given enough regularity assumptions) into something close to a Gaussian. The idea I am tentatively working with is to use a parametrized Beta CDF to nonlinearly map $(0,1) \rightarrow (0,1)$, and then subsequently apply a logit transform: $$ f_{\alpha,\beta}(u) = \text{logit}\left[\text{BetaCDF}(u;\alpha,\beta)\right] $$ with $\text{logit}(p) = \log\left(\frac{p}{1-p}\right)$. The Beta CDF reparametrization has been shown to be quite useful in recent machine learning work (e.g., Snoek et al., 2014 ). Ideally, I would like the transformation, its inverse and the derivative to be computable without too much hassle (e.g., the Beta CDF is not analytical, but it is included in many common statistical packages, so it's okay). Any better idea? Reference Snoek, J., Swersky, K., Zemel, R. S., & Adams, R. P. (2014, February). Input Warping for Bayesian Optimization of Non-Stationary Functions. In ICML (pp. 1674-1682).
