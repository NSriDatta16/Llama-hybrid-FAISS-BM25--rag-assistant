[site]: crossvalidated
[post_id]: 268892
[parent_id]: 268876
[tags]: 
Firstly, each time you randomly sample 75% of your data, you are selecting a different set of observations. You are therefore training a model on a different dataset each time, which is why your model coefficients are different each time. This is normal. They will vary more greatly if you have a small dataset to begin with. If you have a very large dataset, there will be very little variation between random samples. The purpose of splitting the model is not directly to estimate model coefficients, but to examine the ability of the specified model trained on the training data to accurately predict frost events (or estimate the probability of frost events) in the testing data. This is most commonly used to ensure your specified model is not over-fitting the data. You would typically use the AUC statistic to check for this in logistic regression (see What does AUC stand for and what is it? ). By 'specified model', I mean the choice of predictor variables, interactions, non-linear forms, etc. If you split only once, an implicit choice has been made to ignore the variation induced by randomly splitting 75/25. If you don't want to ignore this, then you can split repeatedly using cross-validation - see https://en.wikipedia.org/wiki/Cross-validation_(statistics) . Cross-validation can be done a number of ways. "Repeated random sub-sampling validation" is the method that you have already intuitively considered! You would then want to average the values of the AUC (or whichever model-checking statistic you are using, but NOT the values of the coefficients) over these repeated splits to get your overall estimate of model validity. Incidentally, you'll find that if you repeat this random sampling process enough times, the average of the model coefficients will approach the values you get if you train a model using the whole dataset. This suggests that if you are a-priori happy with your specified model (eg you are happy there is no over-fitting), then you should just use the complete dataset in the first place without splitting.
