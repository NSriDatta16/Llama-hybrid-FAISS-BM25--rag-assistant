[site]: crossvalidated
[post_id]: 595328
[parent_id]: 595317
[tags]: 
Multilevel logistic models are also referred to as logistic GLMMs. Even just fitting the darn things is difficult, so an adequate answer to this question is likely not for the faint of heart. At the most fundamental level, in addition to sample size and design requirements, we require that the observations are conditionally, mutually independent of each other. Conditional, mutual independence encapsulates the issue of correct model specification in the form of fixed effects (no unadjusted predictors/omitted variable bias), and correct identification of correlation structure (in the form of random intercepts or slopes). If these conditions are not met, no guarantee can be made that model parameters, standard errors, confidence intervals, or p-values are correct - in some cases this means bias, and in others, errors are too small or too large. So never mind assumptions tests. Even simple descriptive statistics and plots are difficult to conceptualize with GLMMs. You don't, for instance, have access to any readily recognizable "residual" to inspect, not even Studentized residuals. For a Gaussian response, when we say "conditional, mutual independence", we immediately imply residuals because mixtures of normal are normal. The same is not true of binary outcome models. The logistic curve exhibits a unique characteristic that it's not collapsible. In other words, you do not have free reign to discover the "right" model through extensive diagnostics; you will tend to "discover" a model that's more complex than the truth, leading to positively biased ORs and SEs. This is true of logistic fixed effect models too. I know of one useful diagnostic tool: Whereas a Gaussian response might consider a variogram to explore correlation between observations as a function of some value (e.g. time or y-hat), binary responses can be explored with lorelograms. https://www.jstor.org/stable/2669612 More importantly, you might need to consider the particular design, and simulate data according to a few potential conditions, and practice fitting your GLMM on 1,000 replicates of datasets under those conditions, then ensure that proper fit (unbiased estimates, powered inference, appropriate coverage of uncertainty intervals) is achieved.
