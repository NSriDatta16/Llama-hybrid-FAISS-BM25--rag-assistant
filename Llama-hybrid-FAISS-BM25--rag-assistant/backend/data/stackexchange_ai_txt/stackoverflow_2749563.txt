[site]: stackoverflow
[post_id]: 2749563
[parent_id]: 2749441
[tags]: 
file_get_contents() is the most optimized way to read files in PHP, however - since you're reading files in memory you're always limited to the amount of memory available . You can issue a ini_set('memory_limit', -1) if you have the right permissions but you'll still be limited by the amount of memory available on your system, this is common to all programming languages. The only solution is to read the file in chunks , for that you can use file_get_contents() with the fourth and fifth arguments ( $offset and $maxlen - specified in bytes ): string file_get_contents(string $filename[, bool $use_include_path = false[, resource $context[, int $offset = -1[, int $maxlen = -1]]]]) Here is an example where I use this technique to serve large download files: public function Download($path, $speed = null) { if (is_file($path) === true) { set_time_limit(0); while (ob_get_level() > 0) { ob_end_clean(); } $size = sprintf('%u', filesize($path)); $speed = (is_int($speed) === true) ? $size : intval($speed) * 1024; header('Expires: 0'); header('Pragma: public'); header('Cache-Control: must-revalidate, post-check=0, pre-check=0'); header('Content-Type: application/octet-stream'); header('Content-Length: ' . $size); header('Content-Disposition: attachment; filename="' . basename($path) . '"'); header('Content-Transfer-Encoding: binary'); for ($i = 0; $i HTTP->Flush(file_get_contents($path, false, null, $i, $speed)); ph()->HTTP->Sleep(1); } exit(); } return false; } Another option is the use the less optimized fopen() , feof() , fgets() and fclose() functions, specially if you care about getting whole lines at once , here is another example I provided in another StackOverflow question for importing large SQL queries into the database : function SplitSQL($file, $delimiter = ';') { set_time_limit(0); if (is_file($file) === true) { $file = fopen($file, 'r'); if (is_resource($file) === true) { $query = array(); while (feof($file) === false) { $query[] = fgets($file); if (preg_match('~' . preg_quote($delimiter, '~') . '\s*$~iS', end($query)) === 1) { $query = trim(implode('', $query)); if (mysql_query($query) === false) { echo ' ERROR: ' . $query . ' ' . "\n"; } else { echo ' SUCCESS: ' . $query . ' ' . "\n"; } while (ob_get_level() > 0) { ob_end_flush(); } flush(); } if (is_string($query) === true) { $query = array(); } } return fclose($file); } } return false; } Which technique you use will really depend on what you're trying to do (as you can see with the SQL import function and the download function), but you'll always have to read the data in chunks .
