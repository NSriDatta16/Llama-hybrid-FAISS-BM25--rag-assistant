[site]: datascience
[post_id]: 23096
[parent_id]: 11852
[tags]: 
Just some elaborations, RBM is trained to maximize probability of visible units $p(\mathbf{v})$ defined by the model using maximum likelihood , not the reconstruction error, this is true. But the maximum likelihood estimation is performed using Gradient Descent . Another pitfall that analytical expressions for gradients of log-likelihood w.r.t. model parameters are typically intractable (because of exponentially many terms in respective sums), therefore gradients themselves are approximated using MCMC-based algorithm called Contrastive Divergence (or its variants). TL;DR RBM is trained using GD/SGD where gradients are approximated using Contrastive Divergence and the function to optimize is log-likelihood of the training data.
