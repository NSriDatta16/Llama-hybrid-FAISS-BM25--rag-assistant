[site]: datascience
[post_id]: 73632
[parent_id]: 73613
[tags]: 
In short: Yes. Conceptually, bagging and boosting are model-agnostic techniques, meaning that they work regardless of the learner. Bagging essentially is the following: create multiple predictors (they can even be hard-coded!) gather predictions from the learners and come up with a prediction Boosting can be seen as: train a predictor find where the predictor makes mistakes put more emphasis on these mistakes repeat until satisfactory Regarding the specific Sklearn implementations, here are the base learners that you can use: AdaBoostClassifier() The documentation says Support for sample weighting is required, as well as proper classes_ and n_classes_ attributes . This means that you can use all models that can give weight to your samples as part of the learning process (KNN, SVM, etc.) BaggingClassifier() This is a simple bagging strategy, so all estimators can be used here. GradientBoostingClassifier() This requires that your learners are differentiable so that the gradient can be computed. Generally, this technique is specific for tree learning.
