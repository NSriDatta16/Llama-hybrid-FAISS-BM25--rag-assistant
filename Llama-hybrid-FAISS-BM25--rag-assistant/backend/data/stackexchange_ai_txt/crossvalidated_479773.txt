[site]: crossvalidated
[post_id]: 479773
[parent_id]: 295016
[tags]: 
HPO makes a huge difference. Over the past few years, many new architecture modifications have been proposed for deep learning. Often, authors show that their new modification performs slightly better than previous techniques. While many of these new methods do indeed improve the performance of deep learning models slightly, often, similar performance improvements can be achieved using proper HPO. In the paper of Isensee et al. this is illustrated by their "No new U-net". Isensee et al. shows here that using a basic U-net that is properly tuned outperforms all previously reported architecture modifications, achieving highest performance on the BRATS dataset. This in my opinion shows how important HPO is for achieving good performance. If you'd like to read up on how to optimize hyperparameters in a deep learning model in an easy practical way I would suggest reading the paper of Smith - A disciplined approach to neural network hyper-parameters: part1 - learning rate, batch size, momentum, and weight decay.
