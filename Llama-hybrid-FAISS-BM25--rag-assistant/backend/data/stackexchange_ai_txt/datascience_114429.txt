[site]: datascience
[post_id]: 114429
[parent_id]: 
[tags]: 
Neural net patience moving average

To my understanding, when we use patience = 8 when training a neural net, if there is no improvement on the loss (usually the validation set loss) for 8 epochs, then we would stop the training and take the model from 8 epochs ago as our "trained model". An issue I see with this is that maybe the model trained 8 epochs ago was just lucky and happened to perform well on the validation set. A way I could see around this would be to use a moving average of the loss (eg a moving average of 3 epochs). So every epoch we would calculate the average loss (using the loss from this epoch and the loss from the previous 2 epochs). Then with a patience = 8, we would stop training when the average loss doesnt improve for 8 epochs. To choose a model from the 3 models that make up the best moving average, we could just take the one that actually scored lowest. I believe this would mean we avoid fluke drops in the loss and so avoid models that "got lucky". Do you think this would help much/at all or is this a waste of resources?
