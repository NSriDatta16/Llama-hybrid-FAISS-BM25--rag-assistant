[site]: crossvalidated
[post_id]: 370180
[parent_id]: 370179
[tags]: 
I thought a regression loss function such as mean squared error or mean absolute error must be used instead, which have a value of zero when labels and predictions are the same. That's exactly the misconception you have. You think that in order for a loss function to be used in a model like an autoencoder, it must have a value of zero when predictions equal to true labels. That's simply wrong since in most of the machine learning models (including autoencoders) we are trying to minimize a loss/cost function. And we are doing this with the assumption that the loss function we are using when reaches its minimum point, implies that the predictions and true labels are the same . That's the condition for using a function as a loss function in a model trained based on minimzing loss function. Note that the value of loss function at this minimum point may not be zero at all, however we don't care about this as long as it implies in that point predictions and true labels are the same. Now let's verify this is the case for binary crossentropy: we need to show that when we reach the minimum point of binary crossentropy it implies that $y = p$ , i.e. predictions equal to true labels. To find the minimum point, we take the derivative with respect to $p$ and set it equal to zero (note that in the following calculations I have assumed that the $log$ is natural logarithm function to make calculations a little easier): $$\begin{align}&\frac{\partial BCE(y,p)}{\partial p} = 0\\ &\implies -y.\dfrac{1}{p} - (1-y).\dfrac{-1}{1-p} = 0\\ &\implies -y.(1-p) + (1-y).p = 0\\ &\implies -y + y.p + p - y.p = 0\\ &\implies p - y = 0\\ &\implies y = p \end{align}$$
