[site]: crossvalidated
[post_id]: 217384
[parent_id]: 217383
[tags]: 
PCA is not a feature selection method, but it can be used for that purpose. PCA is simply a feature transformation, where it first chooses linear subspaces that are orthogonal to each other (eigenvectors), then projects your original data on these subspaces and returns you the projected vectors. That's why you see different values. As for feature selection, what is usually done is to utilize the fact that the eigenvectors are ordered in decreasing variance, which is quantized by the eigenvalues. So when you select the first K (which is or ) components, you still hold the axes that explain a lot (90% or 99%) of the variance. Therefore, instead of your original N dimensions, you can continue your task with a smaller set of K dimensions (combinations of input features) without much information loss.
