[site]: crossvalidated
[post_id]: 467163
[parent_id]: 
[tags]: 
Is it normal to have inconsistent values of accuracy when running a classification algorithm?

I ran several algorithms on several datasets (i.e. SVM, KNN, Decision Tree, Naive Bayes, Logistic Regression and MLP). Due to it some randomization process in creating the training sets, the accuracy produced from each algorithm is not the same. I did not train each algorithm using the whole training set, however, I randomly chose a specific number of training samples. Thus, in each run of the same algorithm, different training sets were created. Is this normal? If it is normal, how Iâ€™m going to report the results if I run it today is different from yesterday? Another problem, if I have recorded the previous results and then changed some experimental setup, the results have also changed which made my recording for the previous results are useless.
