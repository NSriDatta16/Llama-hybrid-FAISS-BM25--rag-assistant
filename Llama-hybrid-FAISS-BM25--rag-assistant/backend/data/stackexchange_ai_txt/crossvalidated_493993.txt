[site]: crossvalidated
[post_id]: 493993
[parent_id]: 
[tags]: 
Relationship between minimizing mutual information and maximizing non-Gaussianity in Independent Component Analysis (ICA)

In independent component analysis (ICA), we assume that the observe data $\bf{x}$ from $n$ channels come from linear mixing of $n$ independent sources $\bf{s}$ : $$ \bf{x}=\bf{As} $$ And we try to find a matrix $\bf{W}$ so that the components in: $$ \bf{u}=\bf{Wx} $$ are statistically independent. This means that, in theory, we should minimize the mutual information between the components in $\bf{u}$ . However, some papers about ICA instead suggest that we can also do ICA by maximizing the non-Gaussianity of the components in $\bf{u}$ (for example, Hyvarinen & Oja, Neural Network, 2000 ). While I understand that by Central Limit Theorem, distribution of sum of independent non-Gaussian random variables becomes more Gaussian, is there a formal mathematical proof that "maximizing the non-Gaussianity" is equivalent to "minimizing the mutual information"?
