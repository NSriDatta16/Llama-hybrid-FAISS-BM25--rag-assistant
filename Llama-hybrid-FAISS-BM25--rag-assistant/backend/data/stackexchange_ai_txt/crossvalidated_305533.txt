[site]: crossvalidated
[post_id]: 305533
[parent_id]: 
[tags]: 
Formal equation for variance retention in PCA

In lecture 8 of his Coursera machine learning course Andrew Ng states that $$\frac{ \frac{1}{m} \sum_{i=1}^m {|| x^{(i)}-{x^{(i)}_{\text {approx}}} ||}^2}{\frac{1}{m} \sum_{i=1}^m {|| x^{(i)} ||}^2}\le 0.05$$ means "95 % of the variance was retained". Can anyone explain to me why the equation means that? I understand that you can reduce $\frac{1}{m}$ in the equation, but afterwards I have no idea. Edit: Agenda: $x^{(i)}$ : Original data points of the variables before the Principal Component Analysis $x^{(i)}_{approx}$ : The data points after the principal component analysis The numerator as a whole stands for the "Average squared projection error". The denominator as a whole stands for the "Total Variation of the data". So you can also rephrase the equation as $$\frac{\text{Average squared projection error}}{\text{Total variation of the data}}\le 0.05$$ Edit: You can also have a look at Slide 24 of this Powerpoint presentation:
