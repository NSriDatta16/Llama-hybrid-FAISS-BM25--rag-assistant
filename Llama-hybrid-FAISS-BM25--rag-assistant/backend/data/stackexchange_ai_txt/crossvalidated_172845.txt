[site]: crossvalidated
[post_id]: 172845
[parent_id]: 172842
[tags]: 
When pre-processing data, you are generally trying to achieve the following: A. Removal of errors from your data. If your outliers are due to data recording errors, for example, you would want to fix this in the pre-processing stage. The various rules for identifying outliers should be treated as providing initial guesses, requiring further investigation. B. Creating variables where you have a reasonable expectation that different values of the predictor variables may by correlated with the outcome variable. This is the bit that requires domain-specific knowledge, and good variables are often constructed using ratios, differences, averages of variables, etc. C. Modifying the data to avoid restrictive assumptions of whatever model we are fitting. The super-cool thing about tree-based methods, like random forests, is that they require much less effort in the type C pre-processing. In particular, normalizing, removing non-error-outliers, discarding variables, and log transformations, are not generally required. But, the cost of tree-based methods is that they are data hungry, so with smaller samples (e.g., less than 10,000 case), my experience is that often a glm is going to do a better job, but this brings in the type C processing, which after 25 years of building models I still find a challenge.
