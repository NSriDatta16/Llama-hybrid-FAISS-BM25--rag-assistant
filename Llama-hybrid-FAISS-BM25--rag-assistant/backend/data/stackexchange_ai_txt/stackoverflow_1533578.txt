[site]: stackoverflow
[post_id]: 1533578
[parent_id]: 1533554
[tags]: 
Have you considered using a robots.txt file to minimize unwanted traffic from automated spidering tools? You can have multiple Disallow lines for each user agent (ie, for each spider). Here is an example of a longer robots.txt file: User-agent: * Disallow: /images/ Disallow: /cgi-bin/ User-agent: Googlebot-Image Disallow: / Here is an example that disallows everything except google User-agent: * Disallow: / User-agent: Googlebot allow: / A word of warning: This method isn't guaranteed to stop disallowed agents from going through your site, it just asks them nicely in a standardized way that most of these tools understand.
