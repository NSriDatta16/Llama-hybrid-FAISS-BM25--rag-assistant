[site]: crossvalidated
[post_id]: 160536
[parent_id]: 
[tags]: 
Interpretation of cross validation results when comparing models

I'm trying to solve a bio-medical image segmentation problem using a binary classifier and then a spatial smoothing (assuming continuous regions). I have: Training set of 10 3D scans, a total of ~30 million voxels to classify 20-30 continuous features for each voxel The classes are unbalanced, ~90% healthy in each scan. I'm using 10-fold CV to compare many different methods (SVM with different kernels, ensemble learning with different weak learners and different learning methods, etc), I'm using the same stratified partition with all of the methods. Since it's a lot of data, I want to do that in three stages. First use ~5% of the data (sampling from all 10 scans) to score these many methods and eliminate most of them which perform poorly. Secondly, use ~35% of the data to compare the better methods. Third, use the rest of the data to fine tune the best model. The segmentation task of each fold is scored using the dice score (same as f1) of the non-healthy region. For each method I compute the Mean,Median,Max & Std of the dice scores of all folds. I also add all the confusion matrices to a grand confusion matrix and compute the dice score. Sometimes, a method can't converge or produce an error for a specific fold, so it's removed from the aggregated results. I'm still in the first stage, the mean scores are anywhere between 0 and ~0.75. For any specific method with mean score higher than .5 the std of the score is lower than 0.1 and many times much lower ( I'm assuming that low variance/std suggest a more stable model that will better generalize and a more predictable score. Higher Mean/Median/Max scores are better of course. I'm trying to figure out how to choose the better methods? Which stats should I look at? I think computing confidence intervals and comparing by the lower bound of the score should be appropriate but I have no idea how to do that.
