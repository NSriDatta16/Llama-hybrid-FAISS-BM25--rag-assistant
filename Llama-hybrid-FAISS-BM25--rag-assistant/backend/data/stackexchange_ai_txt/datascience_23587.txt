[site]: datascience
[post_id]: 23587
[parent_id]: 23577
[tags]: 
If your model has dependent variables, then it's hard to interpret the coefficients down the road. For example, say that your model has 3 variables: profits , costs and revenue . You trained your linear regression model to predict revenue (silly, I know). You could get the following coefficients: $c_{cost}=-1,c_{profits}=1,c_{revenue}=0$ Someone looking at these results, could conclude that revenue is not predictive of revenue . This is kind of a trivial example because the linear dependency is very clear ( revenue=profits-costs ). But consider it a motivation for not adding dependent variables to your model. Correlation is a measurement of pairwise linear dependency. It's more of a question of what type are your variables. If your variables are continuous, Dimensionality reduction algorithms like PCA should account for the correlation.
