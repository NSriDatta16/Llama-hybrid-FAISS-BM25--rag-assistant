[site]: datascience
[post_id]: 112106
[parent_id]: 
[tags]: 
Different results between hyperparameter optimisation and actual training/val values

If I want to do a hyperparameter optimisation on a dataset using e.g. hyperband or random search, I note that some of the models being randomly chosen seem to have rather good R2 scores, MSE etc. I then get excited and create a neural network of the same description and train it on the same thing, same number of epochs, all seemingly same way etc. and I get quite a bit worse values. Is there any good reason for this behaviour (e.g. are randomly chosen weights to start off that important for the final output? (???)) (and if this is true what causes might there be for this that explains it as "expected behaviour"?), or is it entirely unexpected and I should be thinking I've done something wrong? (and if this is true, what causes might there be?) Thanks! Edit: code layout #build hypermodel class MyHyperModel(kt.HyperModel): def build_model(self, hp): model = Sequential() ... #do hyperband search tuner = kt.tuners.Hyperband( MyHyperModel().build_model, objective='loss', ...) tuner.search(x=X_train, y=y_train, epochs=...) #get best best_model = tuner.get_best_models(num_models=1) #build best model and fit - same data best_model.fit(x=X_train, y=y_train, epochs=...)
