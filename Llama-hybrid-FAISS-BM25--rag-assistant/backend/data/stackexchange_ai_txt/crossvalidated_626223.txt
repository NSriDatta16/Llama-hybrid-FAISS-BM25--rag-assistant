[site]: crossvalidated
[post_id]: 626223
[parent_id]: 625522
[tags]: 
This is about a modeling assumption differing from reality. Let me respond point-by-point. it has been assumed that the subwords are independent of each other The modeling assumption that subwords are independently generated is not a fact about reality. It is a choice that Kudo made in defining the SentencePiece segmentation model; in fact, it is quite common in segmentation. As they say, "All models are wrong, but some are useful." $P(t_i, t_j) = P(t_i)P(t_j)$ , thus the value of formula (3) is 0 That's not quite true. The mutual information calculation compares the empirical distributions of the subwords on a corpus. Acknowledging that the modeling assumption is not true, the mutual information asks how frequently the two subwords co-occur, compared to our prior belief according to the model about their co-occurrence . This divergence between the true and predicted values is what makes the mutual information nonzero. Many tutorials define $\log \frac{P(t_k)}{P(t_i)P(t_j)}$ as the mutual information between two subwords, but I think this approach is a bit inappropriate. To call the quantity $\frac{P(t_k)}{P(t_i)P(t_j)}$ (point wise) mutual information is completely correct and in line with the standard usage of MI applied to bigrams in NLP for at least 40 years. This is because the subword $t_k$ yields exactly the same string as $t_it_j$ , so the numerator is equal to $P(t_i, t_j)$ .
