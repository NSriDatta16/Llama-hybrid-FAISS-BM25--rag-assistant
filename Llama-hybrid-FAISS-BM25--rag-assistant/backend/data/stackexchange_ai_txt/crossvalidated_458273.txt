[site]: crossvalidated
[post_id]: 458273
[parent_id]: 
[tags]: 
How to conclude the best parameter configuration of an optimisation algorithm?

Suppose that one is given an algorithm for solving a specific optimisation problem. This algorithm has various parameters (say, e.g., $p_1$ , $p_2$ and $p_3$ ) that have to be fine-tuned to the algorithm achieve better results. $p_1$ can be set to be equal to 1, 2 or 3. $p_2$ can be set to be equal to 1 or 2. And $p_3$ can be set to be equal to 1, 2, 3, 4. The idea is to found the best parameter configuration (e.g., $p_1=1$ , $p_2=2$ and $p_3=4$ ) that is able to improve the efficacy and effectiveness of the algorithm. Thus, it was randomly chosen a set of optimisation functions, from all the most used benchmark function, and the algorithm was tested with all the combinations of different parameters values. Besides that, for each combination, ten tests were executed and the execution time and the algorithm output value is saved (lower is better -- minimisation problem). In total, my data set of test results have $(3×2×4)×10=240$ samples of two elements (execution time and algorithm output). In order to conclude that, on average, for any optimisation problem the best parameter configuration is, e.g., $p_1=1$ , $p_2=2$ and $p_3=4$ , what kind of analysis should I conduct? An ANOVA?
