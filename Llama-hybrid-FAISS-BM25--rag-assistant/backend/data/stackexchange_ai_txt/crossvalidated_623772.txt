[site]: crossvalidated
[post_id]: 623772
[parent_id]: 
[tags]: 
Measuring the epistemic uncertainty mutual information

I possess training data denoted as $D$ , a given test input denoted as $x$ , and a model that is parameterized by a random variable $\theta$ . This model is a neural network containing dropout layers, making it parameterized by the random variable $\theta". My objective is to assess the model's epistemic uncertainty by quantifying the mutual information between the model parameters and the possible model outputs, represented as $y$ : $$ I(y,\theta | x,D) = H_y(y|x,D) - H_\theta(y|x,D,\theta). $$ Here, the entropy is labeled to indicate the specific random variable being considered. Given that $D$ remains constant and $\theta$ depends on $D$ , can we assert that $H_\theta(y|x,D,\theta) = \int p(y|x,D,\theta) p(\theta|D) d\theta = \mathbb{E}_{\theta|D} p(y|x,D,\theta)$ ? In essence, I aim to evaluate how uncertain the model's predictions are by examining the relationship between its parameters and the potential outcomes, considering the inherent variability introduced by dropout layers.
