[site]: datascience
[post_id]: 20304
[parent_id]: 
[tags]: 
Why do we pick random features in random forest

I understand that random forest is a stylized version of bagging of trees. We choose randomly data points as well as random features for constructing random forest. But if we use just plain version of bagging by choosing only data points randomly then we have trees which have trained on more number of features unlike the random forest in the stylized version. Since learning with more features every individual tree has more information about the data points and so are more 'intelligent' in some sense than the individual trees in the random forest. So why does the random forest using stylized version of bagging performs better than the random forest using plain implementation of bagging? I understand that the random forest using the stylized version gives a model lower variance but since each of its trees is trained on some of the features shouldn't it make the model a bit high biased?
