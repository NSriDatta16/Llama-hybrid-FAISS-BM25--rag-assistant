[site]: crossvalidated
[post_id]: 545896
[parent_id]: 545890
[tags]: 
Including my original answer at the bottom below the line. I may have confused you by including too many of the subutilies about conditional probability. First, I would suggest that if you are learning about these things for the first time that you focus on learning about the frequentist approach first and wait till you have mastered that before trying to learn about the Bayesian approach. The frequentist approach is a little easier to learn, and you won't be able to understand the differences between the two until you learn that. At this point it is easier to start with simple rules about what is and what is not conditional probability before learning about the subtleties and situations where the distinction can become a bit blurred. In the frequentist world all parameters are viewed as fixed so the simplest distinction between conditional and unconditional probability is that when a probability has all terms on the right side of the conditioning bar | are parameters (usually represented by greek letters) then it is just probability (also called marginal probability or unconditional probability, but only when it is important to distinguish it as being different from conditional probability). When any of the terms on the right are random variables then it is a conditional probability. $P(Y=y|\theta)$ probability $P(Y=y|X=x,\theta)$ conditional probability. For conditional probability we always need the term being conditioned on written to the right of the conditioning bar, but for probability (i.e. unconditional probability) there is actually quite a bit of variety in how you will see the notation in a book. Sometimes as $P(Y=y|\theta)$ will be written as $P(Y=y;\theta)$ and other times as $P(Y=y)$ , but all mean the same thing, really just depends on the situation which notation is best. A PMF (or PDF) is just a probability. It can either be a marginal probability or a conditional probability, but whichever it is for a given random variable $X$ there are not two different versions: one marginal and one conditional. Either you want the probability of $X$ unconditional and you have a PMF or you want it conditional on something and they you would use a different PMF, (but in either case there is only one PMF) unconditional PMF: $P(X=x|\theta)$ there is not a 'conditional version' of this You could consider a different situation where you want to condition on say a variable $Y$ then $P(X=x|Y=y,\theta)$ is a conditional PMF $P(X=x|\theta)$ and $P(X=x|Y=y,\theta)$ are not two different versions of the PMF; They are just two different PMFs for two different situations. The likelihood function is always equal to the PMF/PDF, but with a different interpretation. It is not a conditional probability. And although the functions look the same we do view it differently and we don't really view it as a probability at all. Just a function of the parameter. Again, to simplify things you should really just be focused now on learning about PMFs and PDF and how conditional probability works and hold off on learning about likelihood functions, which are a more advanced topic. I think there are two reasons for your confusion and it related to two somewhat different things. One having to do with the term conditional probability and one having to do with the likelihood function. Let's look at the first, so forget for the moment this having anything to do with likelihood functions. As a start you need to know that not everyone uses the distinction between a 'conditional probability' and a 'probability' in the same way. Let's look at the binomial probability mass function (PMF) $P(X=x|\theta)=$ $n\choose x$$\theta^x(1-\theta)^{n-x}$ The question is do we call $P(X=x|\theta)$ a conditional probability or not? The answer is that depends on who you ask. It sure looks like a conditional probability because it has a conditioning bar $|$ , so I would call that a conditional probability (specifically, conditional on $\theta$ ). But many others would not, the reason being that when that formula is used to calculate a probability you are always using a fixed value for $\theta$ as in your example where $\theta=0.7$ . $P(X=x|0.7)=$ $n\choose x$$0.7^x(1-0.7)^{n-x}$ and some people prefer to only use the term 'conditional probability' when you are conditioning on a random variable. It is common practice (especially among non-Bayesians) when a probability depends on only a fixed value not to call it a conditional probability. And to instead only call something a conditional probability when you are conditioning on something that is random. So, if for example we had a second random variable $Y$ and we calculate the probability of some value of $x$ conditional on some value of $y$ as $P(X=x|Y=y)$ then that would be called a conditional probability because $Y$ is random. Note also, that when people write a PMF they do not always include the $|\theta$ part, so you will sometimes see it as $P(X=x)=$ $n\choose x$$\theta^x(1-\theta)^{n-x}$ Note that I did not include $n$ in my notation above for $P(X=x|\theta)$ , so did not write $P(X=x|n,\theta)$ . (lots of times authors will not include $n$ in the notation. Again because when you use this formula $n$ is always a fixed value so the same logic applies with some calling that something being conditioned on and other not. Now let's look at the likelihood part of your question. You use the PMF when you know the value of $\theta$ and want to calculate a probability, as you did to find the probability of $X$ being 5 when $\theta=0.7$ . You only use the likelihood function when you do not know the value of $\theta$ and you are trying to estimate it from some data you have observed. So if you did not know the value of $\theta$ and flipped a coin say 100 times and got heads 23 times and wanted to then estimate $\theta$ , now is where you use the likelihood function. And yes we construct the likelihood function from the PMF, but we use different notation because we have a different purpose here (to estimate $\theta$ not to use a known value of $\theta$ to calculate a probability). The likelihood function is $L(\theta|x)=$$n\choose x$$\theta^x(1-\theta)^{n-x}$ where $x$ is the data, so to use this likelihood you plug in the value of $x$ (and $n$ ) $L(\theta|x)=$$100\choose 23$$\theta^{23}(1-\theta)^{100-23}$ and then you maximize the function with respect to $\theta$ to estimate $\theta$ which is very easy to show gives an estimate 23/100. A few things about the likelihood function. It is not explicitly a Bayesian inference function. (used in both Bayesian and non-Bayesian inference). We flip the ordering the $\theta$ and $x$ in the conditioning notation (with $\theta$ on the left side of the conditioning bar and $x$ on the right) because we are thinking of this as a function of $\theta$ conditional on $x$ with $\theta$ being the value we do not know and $x$ being the value we do know. (opposite of the situation where we know the value of $\theta$ and we use the PMF to estimate a probability of $x$ being some value) while the likelihood is always equal to the PMF (or PDF) technically it is any function that is proportional with respect to the parameter $\theta$ to the PMF. $L(\theta|x)\propto\theta^x(1-\theta)^{n-x}$ this is because $n\choose x$ does not contain $\theta$ so if you maximize this function with respect to $\theta$ you get the same answer. There is quite a bit more to explain if you wanted to know how a Bayesian inference would use the likelihood function, but it does not appear to me that really is your questions, and additional details about it would not be helpful. As a last thought here. My perspective is that ALL probabilities are conditional. There is always something that any probability depends on.
