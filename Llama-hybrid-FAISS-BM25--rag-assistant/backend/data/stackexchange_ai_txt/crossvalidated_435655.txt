[site]: crossvalidated
[post_id]: 435655
[parent_id]: 
[tags]: 
weak learning of 3-piece classifiers using decision stumps

I have a question about Example 10.1 in Shalev-Shwartz and Ben-David's "Understanding Machine Learning." The example means to illustrate weak learning of 3-piece classifiers $\mathcal H$ using decision stumps $\mathcal G$ , where $\mathcal H=\{h_{\theta_1, \theta_2, b}:\theta_1, \theta_2\in \mathbb R, \theta_1 with $$ h_{\theta_1, \theta_2, b}(x) =\left\{\begin{array}{ll} -b, & \theta_1 \le x \le\theta_2\\ +b, & \textrm{otherwise.}\end{array}\right.$$ An example of $h_{\theta_1, \theta_2, b}(x)$ is as follows: On the other hand, $\mathcal G=\{x\mapsto \mathrm{sign}(x-\theta)\cdot b:\theta\in \mathbb R, b\in \{+1, -1\}\}.$ What I don't understand is that the book claims that for every distribution $\mathcal D$ that is consistent with $\mathcal H$ , there exists a decision stump $g$ with $L_{\mathcal D}(g)\le 1/3,$ where $L_{\mathcal D}(g)=P_{\mathcal D}\{x: g(x)\ne \hat h_{\theta_1, \theta_2, b}(x)\}$ denotes the true/population error and $\hat h_{\theta_1, \theta_2, b}$ is the ground truth. And the reasoning is that for any pair of the three regions of $\mathbb R$ partitioned by $\theta_1$ and $\theta_2$ , namely $\{x:x and $\{x:x>\theta_2\}$ there exists a decision stump that agrees with the labeling of these two components. Suppose that $P_{\mathcal D}(\{x:x \theta_2\})=0.4$ and $P_{\mathcal D}(\{x:\theta_1\le x \le\theta_2\})=0.2$ . I don't see how we can find a decision stump $g$ which disagrees with the correct labeling $\hat h_{\theta_1, \theta_2, b}(x)$ only on the interval $\{x:\theta_1\le x\le \theta_2\}$ , so that the true error $L_{\mathcal D}(g)\le1/3$ ? Any decision stump would at least disagree with part of $\{x:x or $\{x:x>\theta_2\}$ , wouldn't it? I'd appreciate it, if someone can point out where I missed.
