[site]: crossvalidated
[post_id]: 637965
[parent_id]: 637962
[tags]: 
First, LASSO lets you pick a value for small based on cross validation. Second, LASSO does that adjustment. Simply dropping a variable will result in a big bias. Statology offers a nice example using the mtcars data. set.seed(1234) #Start of statology code y The coefficients through LASSO were (Intercept) 477.905338 mpg -3.102593 wt 19.252832 drat . qsec -18.534966 Through lm : Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 473.779 105.213 4.503 0.000116 *** xmpg -2.877 2.381 -1.209 0.237319 xwt 26.037 13.514 1.927 0.064600 . xdrat 4.819 15.952 0.302 0.764910 xqsec -20.751 3.993 -5.197 1.79e-05 *** Note that 1) The coef for drat is the smallest in terms of absolute value of t. 2) Since the mean of drat is 3.60, if you just drop drat you will be deducting an average of $4.81\cdot3.60 = 17.4$ from each predicted value. 3) LASSO adjusts the other coefficients by fairly substantial amounts, but it does not adjust the intercept by much. Of course, you could run x2 which gives (Intercept) x2mpg x2wt x2qsec 494.57 -2.70 25.04 -20.97 which is quite different from the LASSO results.
