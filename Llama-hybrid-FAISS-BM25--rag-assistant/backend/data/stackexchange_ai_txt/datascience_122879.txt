[site]: datascience
[post_id]: 122879
[parent_id]: 122863
[tags]: 
If L is close to the end of the model, then it can maybe be thought of as analogous to the conventional practice of replacing the last linear layer or freezing the first L - k layers and only finetuning on the last k . Although the latter doesn't reset the parameters, if your finetuning task is significantly different from your pretraining task, the resulting performance could end up being similar as later layers tend to encode more task-specific information. I would expect this to use less data as well, as the conventional practice of layer freezing or linear classifier replacement tends to be done in low-resource settings. I think the keyword you're looking for is layer reinitialization . RIFLE looks at iterative reinitialization of FC layers during finetuning, to address the issue of FC layers converging too quickly and thus being biased towards pretraining features. The Impact of Reinitialization on Generalization in Convolutional Neural Networks evaulates several methods for repeatedly reinitializing + retraining subsets of parameters during training. They find that for the same number of iterations, such methods can achieve better test accuracy than training only once. Both of these are not on transformer models. This paper uses reinitialization as a method to study the transferability of parameters in pretrained transformers. They reinitialize the parameters for layers that they aren't looking to probe, and study how performance changes during finetuning . They find earlier layers to be most transferrable. All of these papers are slightly different to what you're looking for. Particularly, many of these settings don't freeze the non-reinitialized parameters. However, I hope this gives you a good starting point into the existing literature.
