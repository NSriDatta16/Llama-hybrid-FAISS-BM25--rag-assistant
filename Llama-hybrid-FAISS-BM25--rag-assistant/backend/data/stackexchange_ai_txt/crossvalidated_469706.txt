[site]: crossvalidated
[post_id]: 469706
[parent_id]: 
[tags]: 
log(1 - softmax(X))?

Let $\vec X$ be a vector. The $\vec V = \mathrm{logsoftmax}(\vec{X})$ function is defined as: $$v_i = \ln\left(\frac{e^{x_i}}{\sum_i e^{x_i}}\right)$$ This is provided in machine learning numerical packages for numerical stability. Is there a numerically stable implementation of: $$\ln\left(1 - \frac{e^{x_i}}{\sum_i e^{x_i}}\right)$$ provided in standard packages (e.g. PyTorch, or scipy, etc.)? What's a good way of computing this? For example, if $e^{x_i}$ represents the (non-normalized) probaiblity of a label, then this is the log of the probability of an incorrect label.
