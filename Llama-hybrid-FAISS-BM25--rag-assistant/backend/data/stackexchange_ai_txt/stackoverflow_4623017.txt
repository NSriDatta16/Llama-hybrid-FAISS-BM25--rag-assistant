[site]: stackoverflow
[post_id]: 4623017
[parent_id]: 4622931
[tags]: 
CSV file parsing is I/O bound, determined by how fast you can read the data off the disk. The fastest that could ever go is around 50 to 60 MB per second for a consumer level hard drive. Sounds like this LumenWorks is close to that limit. You'll only ever get this kind of throughput though on a nice clean unfragmented disk with one large file. So that the disk reader head is just pumping data without having to move a lot, just track-to-track moves. Moving the head is the slow part, usually around 16 milliseconds average. There's lots of head movement when you're reading 3000 files. Just opening a file takes about 50 milliseconds. At least do a comparable test to find the bottleneck. Use a good text editor and copy/paste to make one giant file as well. Run a disk defragger first, Defraggler is a decent free one. As far as code improvements, watch out for strings. They can generate a lot of garbage and have poor CPU cache locality. Threads can't make I/O bound code faster. The only possible improvement is one thread that reads the file, another that does the conversion so that reading and converting is overlapped. Having more than one thread doing the reading is pointless, they'll just take turns waiting for the disk. And watch out for the file system cache. The second time you run a test on the same file, you'll get the data from memory, not the disk. That's fast but won't tell you how it will perform in production.
