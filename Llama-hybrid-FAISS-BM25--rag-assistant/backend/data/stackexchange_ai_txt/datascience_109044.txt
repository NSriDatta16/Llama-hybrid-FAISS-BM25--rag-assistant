[site]: datascience
[post_id]: 109044
[parent_id]: 109042
[tags]: 
This approach strikes me as bad practice, since it immediately torpedoes the hope of having any independent test data in which to have an unbiased measure of algorithm performance. By combining all the data and performing t-SNE, you are generating t-SNE features that have explanatory power in both the training and the testing data . No matter how you split the data after that, there can be no truly independent test data, since all of the data was used to define the features in the first place. I'm not at all surprised to see an apparent improvement in performance statistics from this approach, since it is a biased method that has "peeked" at the test data, and will likely be overoptimistic. You should never perform feature selection/dimensionality reduction before splitting into train/test datasets, or else you are contaminating the process with the test data which should only be used at the very end after the model is built. Using the test data for anything other than testing (in this case, dimensionality reduction) will introduce bias into your evaluation.
