[site]: crossvalidated
[post_id]: 56259
[parent_id]: 
[tags]: 
Baum-welch algorithm: probabilities after each step

In an effort to understand machine learning, at least to some degree, I've been implementing the various algorithms to solve the three problems in a Hidden Markov Model. I've been using Rabiner's tutorial paper as a guide. I have had some trouble making the Baum-Welch algorithm work - namely, the probability after re-estimation is occasionally lower than before. The paper states that the result of a Baum-Welch step is either a critical point, or more likely than the previous step. This means my implementation is incorrect. For the purposes of debugging, I would like to know if this property is true for each variable (e.g. the initial distribution, transition matrix and emission matrix) in isolation, or only when they all are applied. I tried to follow the proof from the original Baum paper , but it was beyond my mathematical ability!
