[site]: datascience
[post_id]: 51792
[parent_id]: 51714
[tags]: 
Note that $\frac{\partial L}{\partial \theta}$ is different from $\frac{\partial \theta}{\partial L}$ . What you tried to describe seems to be $\frac{\partial L}{\partial \theta}$ where $\theta$ is a variable. If $\theta$ is high dimensional, sometimes we just use the $\nabla$ notation. Gradient descent is $$\theta_{n+1}=\theta_n-\gamma \nabla L(\theta_n)$$ Not everything is differentiable and gradient might not be well defined for some optimization problem. In the event that there are constraints, $L$ might need to take the role of Langragian rather than the objective function. Gradient descent is just a means to find the parameters for a model. Gradient based approach seems to be the norm for now but things can change. What you proposed to tattoo is just "gradient" or "slope". Not objection but just want to let you know what you are doing.
