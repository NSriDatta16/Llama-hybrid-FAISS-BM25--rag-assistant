[site]: crossvalidated
[post_id]: 354205
[parent_id]: 354202
[tags]: 
As I understand the theory of NN activation function are needed to bound the value between 0 and 1. No. There are various activation functions, only some of them are bounded (e.g. tanh, sigmoid). One of the most popular activation functions is ReLU which has unbounded positive end. Bounded activation functions are useful for classification and recurrent neural networks, neither of which is your case. As you are performing regression, just leave the outputs as they are. A neural network with linear output unit should handle it well. What you might need to scale are the inputs . Neural networks often exhibit better behavior when the inputs are scaled to zero mean and unit variance.
