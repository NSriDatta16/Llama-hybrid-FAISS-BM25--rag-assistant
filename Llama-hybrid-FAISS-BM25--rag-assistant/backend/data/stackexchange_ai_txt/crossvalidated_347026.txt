[site]: crossvalidated
[post_id]: 347026
[parent_id]: 319666
[tags]: 
The quantity that AIC / AICc estimates is the expected out-of-sample log-likelihood (see Burnham & Anderson 2004, Sec. 2.2), $$ \mathbf E_y \mathbf E_x [ \log g(x | \hat \theta (y)) ], $$ (multiplied with $-2$). This formula means, you obtain maximum likelihood parameter estimates from one sample, $\hat \theta (y)$, then compute the logarithm of the likelihood $g$ of these parameters on another independent sample $x$ (from the same source), and then average across infinitely many realizations of both samples, $\mathbf E_y \mathbf E_x$. Akaike's main result is that $\log g(x | \hat \theta (x)) - K$ is an asymptotically unbiased estimator of the quantity given above, where $K$ is the number of parameters. Equivalently, $$ \mathrm{AIC} = - 2 \log g(x | \hat \theta (x)) + 2 K $$ is an asymptotically unbiased estimator of $$ - 2 ~\mathbf E_y \mathbf E_x [ \log g(x | \hat \theta (y)) ]. $$ If the sample size is small compared to the number of parameters $K$, a better correction term (AICc; see Burnham & Anderson 2004, Sec. 7.7.6, and McQuarrie & Tsai 1998) may be necessary which is more complicated than $2K$, and it is different for different models (likelihood functions $g$). If you use cross-validation with $$ - 2 \log g(x_\mathrm{test} | \hat \theta (x_\mathrm{train})) $$ as the error measure, you are estimating (almost) the same thing. The differences between AIC / AICc and cross-validation are: – AIC / AICc formulas are based on an approximation for large samples (they are only asymptotically correct). – For cross-validation, in order to have test data $x_\mathrm{test}$ the training (estimation) data $x_\mathrm{train}$ must be smaller than the original sample $x$. Moreover, the cross-validated measure depends on two random samples instead of just one, so it may be noisier. Answer a: AIC / AICc and cross-validation are alternative methods to do the same thing (if log-likelihood is the error measure). It therefore does not make sense to use them together. Answer b: Since AIC / AICc and cross-validation with -2 log likelihood estimate (almost) the same quantity, it should be OK to apply the same scale to evaluate differences between models.
