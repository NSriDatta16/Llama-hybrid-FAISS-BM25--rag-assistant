[site]: crossvalidated
[post_id]: 309236
[parent_id]: 
[tags]: 
Trying to write Nesterov Optimization - Gradient Descent

Problem: Im unsure if I understood Nesterov Optimization Im writing about Nesterov Optimization, but the notation im using seems different from the references below. I have done it using some books as guides. Would someone please clarify? Let $\epsilon$ be the learning rate, $w$ each weight of the neural network, $\alpha$ the momentum and $E$ a loss function and considering the weights and gradients are calcualted as an unidimensional vector, the weight updates is done as below : $n_0 = 0 $ $n_t = \alpha * n_{t-1} + \epsilon \frac{\partial E}{\partial w_t}$ And the update for each weight done as the formula below: $\Delta_{w(t)} = \alpha_{n{t-1}} - {1 - \alpha} n_t$ QUESTIONS What exactly is $n$ and $t$ ? References: https://blogs.princeton.edu/imabandit/2014/03/06/nesterovs-accelerated-gradient-descent-for-smooth-and-strongly-convex-optimization/ http://ruder.io/optimizing-gradient-descent/ What's the difference between momentum based gradient descent, and Nesterov's accelerated gradient descent? http://neuralnetworksanddeeplearning.com/chap2.html https://brilliant.org/wiki/backpropagation/
