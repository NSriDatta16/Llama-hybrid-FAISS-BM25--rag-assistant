[site]: crossvalidated
[post_id]: 634918
[parent_id]: 
[tags]: 
Expected steps until collision of a simultaneous walk in two random functional graphs

Let $f$ be a function $f : \{0, 1, 2, \dots, n - 1\} \to \{0, 1, 2, \dots, n - 1\}$ . Now let $u \in \{0, 1, 2, \dots, n - 1\}$ . $u$ is our initial value. Consider the sequence $$ u, f(u), f(f(u)), \dots, f^k(u) $$ where the superscript $f^k$ means composing $f$ $k$ times with itself. $k$ is minimal such that $f^k(u) = f^i(u)$ for some $i . I.e. when starting at $u$ , we need to apply $f$ $k$ times to get a "collision". For a given function $f$ and a starting point $u$ , we call this minimal number of applications until collision $k_{f,u}$ . Note that by the pigeonhole principle a collision must always occur since we consider a function on a finite set (in particular, it holds that $k \le n$ ). If one pictures a function from $\{0, 1, 2, \dots, n - 1\}$ to $\{0, 1, 2, \dots, n - 1\}$ as a graph with $n$ nodes labeled $0$ to $n-1$ and a directed edge from node $a$ to node $b$ if and only if $f(a) = b$ , then repeatedly applying the function means "walking" through that graph, always taking the unique edge going out from a node. A collision occurs when we visit a node that we've already visited before. Let $R_n$ be the set of functions $\{0, 1, 2, \dots, n - 1\} \to \{0, 1, 2, \dots, n - 1\}$ . If one picks an element $f \in R_n$ uniformly at random and an initial value uniformly at random, the expected value of $k$ as above (the number of steps until a collision occurs) is around $\sqrt n$ (see the section on Random Number Generators in Knuth's 2nd vol. of Art of Computer Programming). This is also related to the Birthday Paradox. Now consider picking two elements from $R_n$ independently and uniformly at random, say $f$ and $g$ . Also pick respective starting values independently and uniformly at random, say $u$ and $v$ . Let $k_{f, u}$ be the number of required applications of $f$ with starting value $u$ until a collision occurs, and $k_{g, v}$ the same thing for $g$ and $v$ . The quantity of interest is the expected value of $\min(k_{f, u}, k_{g, v})$ . Let's call this value $M_n$ . I also considered the following variation: Pick $f \in R_n$ uniformly at random and $u$ and $v$ uniformly and independently from $\{0, 1, 2, \dots n\}$ . Then let $L_n$ be the expected value of $\min(k_{f, u}, k_{f, v})$ . I wrote a program to estimate $M_n$ by averaging $\min(k_{f, u}, k_{g, v})$ for many uniform random choices of $f, g, u$ and $v$ . Similarly, I estimated $L_n$ by averaging $\min(k_{f, u}, k_{f, v})$ for many uniform random choices of $f, u$ and $v$ . I found that $L_n$ was noticeably higher than $M_n$ . My question is: Why $L_n$ greater than $M_n$ ? I tried to find an explanation, but failed up to this point. It is perhaps since $k_{f, u}$ and $k_{f, v}$ are not independent anymore, since the same function $f$ is used. But I could not get beyond that vague intuition. And how can I calculate the difference in expectation between these?
