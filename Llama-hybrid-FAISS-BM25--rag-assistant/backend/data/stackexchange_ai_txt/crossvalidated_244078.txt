[site]: crossvalidated
[post_id]: 244078
[parent_id]: 244046
[tags]: 
If you use the random forest variable importance, you should make sure that the hyper-parameter are tuned. Results will not be meaningful if you perform variable importance with a model that over/underfits. Assuming you have done this there are two other options apart from the rf variable importance that I can think of to perform this feature selection task: Wrapper approach: train a model with the full set of variables and step-wise remove variables with the lowest variable importance score. Stop until the prediction error does not decrease anymore (backwards feature elimination). This can be also done the other way around, i.e. start with one variable and step-wise add a the one with the highest importance score. Filter Approach: There exists many different heuristics to perform feature selection called filter methods. Some of them called multivariate filters also take into account correlation among the variables and remove redundant variables, e.g. Minimum redundancy Maximum Relevance (mRMR) or Conditional Mutual Information Criterium (CMIM). The first option treats the number of variables essentially as a hyper-parameter. The second option will be faster, but often less reliable. Performing PCA would also be an option for highly correlated variables assuming that you just care about the prediction results and not about interpretation.
