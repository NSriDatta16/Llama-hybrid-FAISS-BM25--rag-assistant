[site]: crossvalidated
[post_id]: 441215
[parent_id]: 
[tags]: 
Marginalization not understood

In the book "Pattern recognition and machine learning" by Christopher M. Bishop, at page 374 The joint distribution corresponding to this graph is again obtained from our general formula (8.5) to give $$ p(a,b,c) = p(a) p(c|a) p(b|c) $$ First of all, suppose that none of the variables are observed. Again, we can test to see if $a$ and $b$ are independent by marginalizing over $c$ to give $$ p(a,b) = p(a) \sum\limits_c p(c|a) p(b|c) = p(a) p(b|a)$$ I did not understand this last passage. How does it go from $$ p(a) \sum\limits_c p(c|a) p(b|c) $$ to $$ p(a) p(b|a)$$ ? I have tried to explicit the conditional distribution: $$ \sum\limits_c p(c|a) p(b|c) = \sum\limits_c \dfrac{p(c,a)}{p(a)} \dfrac{p(b,c)}{p(c)} $$ I have also tried to use the Bayes theorem $$ \sum\limits_c p(c|a) p(b|c) = \sum\limits_c p(a|c) \dfrac{p(c)}{p(a)} p(b|c) $$ but nothing
