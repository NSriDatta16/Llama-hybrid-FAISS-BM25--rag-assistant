[site]: datascience
[post_id]: 18487
[parent_id]: 
[tags]: 
Choosing the right parameters for SARSA and Q-Learning & Comparing Models

What is the correct way to fine-tune a model's (SARSA(0), SARSA(lambda), Q(0), Q(lambda)) parameters, and how can one compare the models? I read that typically one compares the number of actions needed to achieve the goal, after X episodes. However, my model is slightly different than maze/grid worlds, so I'm not sure on how to do it. Essentially its a time model, with X-days, and each day an action is selected, the reward on each day depends on the action selected, and the end-state reward depends on the actions selected in the other 249 days. It is an optimization model for a regulatory situation, which implies revising the whole reward every 250 days, based on the performance (which is tied to actions) of the other 249 days. With that being said, I cannot measure the performance based on the number of actions needed to reach the terminal state, as it will always be reached with the same number of actions. The models are being trained in a simulation, the optimal policy will be achieved once per model. The state-space is quite large actually, which is one of my concerns about how many epochs (episodes) I should have, its a 250*8*7*3000 space-action (3-state combination, 250,8,7 and 3000 possible actions per state). Ideally one would compare all possible models - as there are quite a few - with a smaller number of episodes, and the selected ones would then run with the full amount of episodes, for time-reasons. Any suggestions on how to compare the models? Based on average rewards with the optimal policy's actions?
