[site]: crossvalidated
[post_id]: 321704
[parent_id]: 321687
[tags]: 
Bagging is useful when you work with classifiers that can achieve high accuracy because they tend to overfit (for example decision trees and neural networks) but because of that, show high variance, specially in the small data domain. Then you generate bootstraps samples of your data, train classifiers on each of them, and then use the aggregate to classify so that you can average out errors due to overfitting. Cross validation is about evaluating how well your model does, but you have limited data. Again, you have the problem that your estimate might not be robust enough. Both share the problem that when used, one makes the assumption that the estimates for each iteration (the classifier that results from each bootstrap iteration and the error of the classifier resulting from each cross-validation step, resp.) are statistically independent. Which is not true. See for example A Study of CrossValidation and Bootstrap for Accuracy Estimation and Model Selection and Bagging Predictors for further details. So yes, they might be helpful in your case.
