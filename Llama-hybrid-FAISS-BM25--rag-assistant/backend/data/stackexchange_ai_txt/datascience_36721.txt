[site]: datascience
[post_id]: 36721
[parent_id]: 36707
[tags]: 
You can refer to this answer here for a very detailed explanation and derivation of the backpropagation algorithm. In general, backpropagation is based on the idea that it is possible to attribute an amount of blame to each parameter in your model for the resulting error. So this is what we do, first we will set all the parameters in our neural network (weights and biases) randomly. Then we will pass examples through the model, and at the output we will compute some loss function. Then using backpropagation we will tune our model parameters proportionally to their contribution to the incurred error. Here are some other related answers that explain the complexities of backpropagation: Derivative of the sigmoid Quick run through of the equations dictating backpropagation Gradient descent, backpropagation and the partial derivative Computing the backpropagation Backpropagation in CNN Backpropagation with different activation functions
