[site]: crossvalidated
[post_id]: 585134
[parent_id]: 585129
[tags]: 
I think it's a pretty well-known thing that gradient boosted decision trees have a tendency towards poor calibration. However, what you get looks pretty extreme. Part of the answer is right there in the XGBoost documentation , where it says:"If you care about predicting the right probability [...] in such a case, you cannot re-balance the dataset" (i.e. you should not use scale_pos_weight ). I suspect if you avoid using it, you could get a predictions that can be calibrated a bit more reasonably. In terms of other models, I would usually expect some form of logistic regression to be reasonably well calibrated out-of-the-box.
