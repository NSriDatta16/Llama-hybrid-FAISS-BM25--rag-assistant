[site]: crossvalidated
[post_id]: 484261
[parent_id]: 250505
[tags]: 
Yes, the innovations need not be normal, not at all. The underlying mathematical fact that gives rise to the asymptotic null distribution of the DF statistic is Functional Central Limit Theorem , or Invariance Principle . The FCLT, if you'd like, is an infinite dimensional generalization of the CLT. The CLT holds for dependent, non-normal sequences, and similar statement can be made about the FCLT. (Conversely, FCLT implies CLT, as the finite dimensional distribution of the Brownian motion is normal. So any general condition that gives you a FCLT immediately implies a CLT.) Functional Central Limit Theorem Given a sequence of random variables $u_i$ , $i = 1, 2, \cdots$ . Consider the sequence of random functions $\phi_n$ , $n = 1, 2, \cdots$ , defined by $$ \phi_n(t) = \frac{1}{\sqrt{n}}\sum_{i = 1}^{[nt]} u_i, \; t \in [0,1]. $$ Each $\phi_n$ is a stochastic process on $[0,1]$ with sample paths in the Skorohod space $D[0,1]$ . The generic form of FCLT provides sufficient conditions under which $\{ \phi_n \}$ converges weakly on $D[0,1]$ to (a scalar multiple of) the standard Brownian motion $B$ . Sufficient conditions, which are more general than those from Phillips and Perron (1987) quoted above, were known prior, if not in the time series literature. See, for example, McLiesh (1975) : The strong mixing condition (iv) of Phillips and Perron implies McLiesh's mixingale condition under some conditions. Condition (ii) of Phillips and Perron requiring uniform bound on $2 + \epsilon$ moments of $\{ u_i\}$ is relaxed in McLeish to the uniform integrability of $\{ u_i^2 \}$ . Condition (iii) of Phillips and Perron is actually not quite correct/sufficient as intended. For a further milestone in the time series literature in this direction, see Elliott, Rothenberg, and Stock (1996) , where they apply a Neyman-Pearson-like approach to benchmark the asymptotic power envelope of unit root tests. The normality assumption is long gone by then. DF Statistic It follows immediately from the FCLT and the Continuous Mapping Theorem that the DF $\tau$ -statistic $\tau$ has the asymptotic distribution $$ \tau \stackrel{d}{\rightarrow} \frac{\frac12 (B(1)^2 - 1)}{ \sqrt{ \int_0^1 B(t)^2 dt} }. $$ The 5th-percentile of this distribution is the critical value for DF test with nominal size of 5%. Simulating $\tau$ with an i.i.d. normal error term and another error term that follows, say, a time series specification would lead to the same distribution as sample size gets large. Comment I am going to disagree with @mlofton's comment that Generally speaking, no one would ever use Phillips result because simulating analytically from the derived distribution is not terribly practical. People generally use the DF tables and those have nothing to do with asymptotic representation. They use the normality of the error term and allow the practitioner to obtain DF statistics for sample sizes as low as 20... It's a major contribution of Phillips to point out that an "assumption free" asymptotic distribution is possible. This is one of the reasons, along with contemporary developments in economic theory, that convinced empirical practitioners (in particular, macro-econometricians) that unit root tests belonged to their everyday toolbox. A statistic (more specifically, a null distribution) that requires normality of the data generating process is not useful at all---e.g. suppose the $t$ -statistic is only valid if data is i.i.d. normal. That was the limitation of the early unit root literature.
