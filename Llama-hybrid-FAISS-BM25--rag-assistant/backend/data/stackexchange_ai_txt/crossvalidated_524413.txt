[site]: crossvalidated
[post_id]: 524413
[parent_id]: 
[tags]: 
Is standard deviation suitable for error bars in case of time-dependent measures?

I'm a software developer so my knowledge of statistics is pretty limited. I will rephrase the question if it sounds confusing. I'm trying to visualize the data for a networking test in which I performed repeated measures of trasmission time with a varying parameter (package length). For every different value of the parameter I performed 15 measures and then recorded the minimum, maximum, average and standard deviation value for the batch. I am asked to plot said values in function of the parameter, so I chose a XY scatter chart, but I'm really skeptical of plotting std dev as a separate data series, since it's 2 orders of magnitude smaller than the other three data series. I think a 'clever' way of showing all the information on the graph would be to add std dev error bars to the mean data points, but since I'm no expert in statistics, I'm not sure whether that would make sense or could be misinterpreted. From a quick read on Wikipedia, it doesn't seem to be a good idea, since I'm not taking all samples (all possible -infinite- test results at that time) into account, but then again I am not performing measures on a fixed set of data, and my measure is more similar to a stochastic process than to repeated measures of the same quantity (there's an implicit time dependency). Thank you in advance! :)
