[site]: crossvalidated
[post_id]: 380362
[parent_id]: 194035
[tags]: 
One type of problem in which a particular Frequentist based approach has essentially dominated any Bayesian is that of prediction in the M-open case. What does M-open mean? M-open implies that the true model that generates the data does not appear in the set of models we are considering. For example, if the true mean of $y$ is quadratic as a function of $x$ , yet we only consider models with the mean a linear function of $x$ , we are in the M-open case. In other words, model miss-specification results in an M-open case. In most cases, this is a huge problem for Bayesian analyses; pretty much all theory that I know about relies on the model being correctly specified. Of course, as critical statisticians, we should think that our model is always misspecified. This is quite an issue; most of our theory is based on the model being correct, yet we know it never is. Basically, we're just crossing our fingers hoping that our model is not too incorrect. Why do Frequentist methods handle this better? Not all do. For example, if we use standard MLE tools for creating the standard errors or building prediction intervals, we're not better off than using Bayesian methods. However, there is one particular Frequentist tool that is very specifically intended for exactly this purpose: cross validation. Here, in order to estimate how well our model will predict on new data, we simply leave of some of the data when fitting the model and measure how well our model predicts the unseen data. Note that this method is completely ambivalent to model miss-specification, it merely provides a method for us to estimate for how well a model will predict on new data, regardless of whether the model is "correct" or not. I don't think it's too hard to argue that this really changes the approach to predictive modeling that's hard to justify from a Bayesian perspective (prior is supposed to represent prior knowledge before seeing data, likelihood function is the model, etc.) to one that's very easy to justify from a Frequentist perspective (we chose the model + regularization parameters that, over repeated sampling, leads to the best out of sample errors). This has completely revolutionized how predictive inference is done. I don't think any statistician would (or at least, should) seriously consider a predictive model that wasn't built or checked with cross-validation, when it's available (i.e., we can reasonable assume observations are independent, not trying to account for sampling bias, etc.).
