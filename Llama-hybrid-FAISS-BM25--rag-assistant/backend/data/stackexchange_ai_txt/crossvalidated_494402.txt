[site]: crossvalidated
[post_id]: 494402
[parent_id]: 491270
[tags]: 
After learning more about this topic and communicating with the authors of the paper it turns out that the solution in my case was that neither downsampling nor splitting the image in to four pieces, but simple using the other image size. This is possible due to the fact that the weights in a fully convolutional neural network (FCNN), which is what I was trying to implement, are not dependent on the input image size. A FCNN is a CNN which does not contain any dense (fully connected) layers and are common in tasks such image segmentation. Each convolutional layer contains a number of filters/kernels which are moved across the image performing convolutions producing a feature map that is fed to the next layer in the network. Pooling layers and upsampling layers for example work in a similar way. The filters are arrays of a fixed size where each element is a weight. So due to the way the convolutions are performed the weights are (more or less) independent of the size of the input and since there are not dense layers present (which are not independent of input size) this makes it possible to have different input sizes for training and testing for instance. To achieve this in Keras for example you can set the input shape to (None, None,3) which will avoid any error due to input size.
