[site]: datascience
[post_id]: 102351
[parent_id]: 
[tags]: 
Should deterministic models be trained splitting into train, test datasets?

I'm studying the difference between GLM models (OLS, Logistic Regression, Zero Inflated, etc.), which are deterministic, since we can infer the parameters exactly, and some CART models (Random Forest, LightGBM, CatBoost, etc.) that are based on stochastic prediction. What I've heard is that for stochastic models we should split into train and test to avoid over-fitting, fact that does not happen in deterministic models, because they use Linear Programming for finding the best parameters. I've like to start some discussion about it. My opinion is that it's true. Deterministic models are just equations solved, and it should not over-fit the data at all, and it differs from stochastic models based on randomness to make predictions. But what I found was every course saying to split every datasets, independent if its deterministic or not.
