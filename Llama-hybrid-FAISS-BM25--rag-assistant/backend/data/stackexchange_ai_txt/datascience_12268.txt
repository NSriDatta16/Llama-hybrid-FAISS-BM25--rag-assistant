[site]: datascience
[post_id]: 12268
[parent_id]: 12265
[tags]: 
(1) Data quality . The single best way to improve your accuracy . Garbage in garbage out . You already said your data is suspect. Some data was mis-classified; data only has single label, when multiple labels are possible. This is the biggie - this will improve your accuracy more than any other technique: improve the quality of your training data. One way to go about this. Recruit expert labellers (these could be workers in your own company who do this task, or they could be external people, e.g., mturkers who have been trained to do this labelling task). In general, the more labellers, the better. At a minimum I would have 3 labellers, label all n emails in your train/test set, using either majority label wins, or require all-three consensus depending on your confidence needs. You will want to measure your inter-rater reliability to ensure the integrity of your data using Cohen's kappa or similar. Importantly, this data will become your gold standard data . That is to say the average agreement upon labels provided by your original labellers is the ground truth to which your algorithm attempts to approach. Obviously, this approach to data quality is costly in terms of resources (time to label all the emails; cost of hiring mturkers, etc.). But depending upon your specific needs it needn't be... basically it depends upon the needed level of "expertise" to do the task. Sometimes, it is safe to consider you (the researcher/developer) as the "expert" (e.g., you yourself said that some emails were mis-classified... how do you know that? You must have some sort of expert knowledge.). However, using a single labeller for the task makes your data less reliable due to individual processing differences introducing a bias into the data. (2) Including stop words . Yes, you read that correct including stop words. Grammatical function words (e.g., the, and, of, etc.) can often be highly indicative of specific text types or genres. It is premature to eliminate stop words without first ensuring that they are an uninformative feature. Similarly, try including punctuation , try including capitalization features. Time and time again, I see people blindly pre-processing text without empirically determining whether these features are significant predictors of class . (3) Capture spelling variation . As the OP stated, the original texts have many "spelling mistakes." You can improve accuracy by reducing this variance - basically map variants to their canonical form before extracting your word features (e.g., "colour" -> "color"). I often rely on external corpora to rapidly create these spelling maps. (4) Balance your training data - The OP mentions that his classes are severely imbalanced. Try over- and/or under-sampling to balance your classes. (5) Describe your problem . It is also important to understand what the underlying purpose of your classifier. If your classifier is intended to run on historical data, perhaps to flag incorrectly human-classified emails, then you would probably want to keep auto-response and correspondence thread info. On the other hand, if the goal is to classify new customer emails so that they get properly routed, for example, then you probably want to remove these correspondence features from your training and test data.
