[site]: crossvalidated
[post_id]: 278265
[parent_id]: 276457
[tags]: 
I like this question because it touches a common source of confusion: the overloaded use of the term model . In this contest, model refers to a the mathematical formulation of a solution (a method $ \hat f (x, \theta)$) rather than a specific instance of a model where every parameter ($\theta$) has been set. I have also seen the k trained model instances generated in a k-fold cross validation exercise moved to production and used as an ensemble instead of training one, single model on the whole calibration set . This is a particularly concerning topic, because as the number of folds declines , depending on the performance of the learning method with respect to the training set size, might result on k-fold model instances being biased - not only in their estimate of the generalization error but, in this specific context, biased in their estimation of the target variable. Edit 1 If I understand what you said correctly, k fold validation is more used to compare modeling algorithms, but a specific model itself. What I am not quite understand is the bias you talked about. Would you please comment on that? Thanks! Your understanding is correct and the comparison takes place on the generalization error, which we estimate calculating the error, or cost, on each validation set and then (very likely) averaging it. As every estimator, our estimator of the generalization error suffers of bias and variance, which we have to trade-off. An Introduction to Statistical Learning covers this topic in detail. Some other treads explain this thoroughly: this and this for example.
