[site]: crossvalidated
[post_id]: 117941
[parent_id]: 117923
[tags]: 
Linear regression excels when it comes to taking into account different kinds of predictors. Unless the number of predictors gets to be large relative to the number of observations, you should be fine. (The "small sample adjustment" for inflating standard errors in OLS is $n/(n-k)$, which only gets you into trouble when $k$ is large relative to $n$.) However, the bigger problem you're going to run into is that for time series data, the errors are likely to be correlated over time, which violates the independence assumption of OLS. To "fix" OLS, you should look into the Newey-West estimator , although your best bet may be to make a full-on autoregressive model with nine AR terms and a covariate, and let a time-series program do the heavy lifting. For a good introduction to time series analysis I recommend Bowerman et al's book: http://www.amazon.com/Forecasting-Time-Series-Regression-CD-ROM/dp/0534409776 For a more in-depth technical discussion with all the matrix math you'll ever need, I recommend the book by Box, Jenkins, and Reinsel: http://www.amazon.com/Time-Analysis-Forecasting-George-Box/dp/0470272848 On the software side of things, I know Stata has "arima" and "newey" commands, and there are also dedicated time-series programs such as EViews. I'm almost certain there are R packages as well, but I am not familiar with them.
