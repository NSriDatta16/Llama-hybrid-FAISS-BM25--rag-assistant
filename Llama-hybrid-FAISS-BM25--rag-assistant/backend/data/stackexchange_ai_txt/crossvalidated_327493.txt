[site]: crossvalidated
[post_id]: 327493
[parent_id]: 327486
[tags]: 
Of course, in practice this would be pretty inefficient since you would have to do a forward pass for every sample experience in your batch. In fact if you were using $\epsilon$-greedy, you would need to find $a' = \text{argmax}_{\hat{a}} Q(s_{t+1}, \hat{a})$ in any case most of the time in order to find the greedy action, so this has more or less the same cost as Q-learning. Is your idea, to use experience replay and re-generate $a'$ used in $Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha [R_{t+1} + \gamma Q(s_{t+1}, a') - Q(s_t, a_t)]$ partially off-line (due to experience replay delay), but still on-policy? I think the answer is a cautious "maybe yes, but it loses some advantages of on-policy". The value of $a_t$ is from the old policy, so you may be updating a record that you no longer care much about. However, that doesn't stop it being on-policy, and is more generally a problem with experience replay. In some cases it is a benefit, because re-visiting older experience might discover that actually the value of an older specific state/action pair is still relevant once you get closer to convergence. More worryingly, you have no guarantee that you have or will ever visit $(s_{t+1},a')$ in practice. As such it might be an on-policy action selection, but it shares bootstrapping problems with off-policy, which will result in higher variance and problems converging when used with function approximation. This combination of the two points could make the action choice $a'$, one that was never actually taken under any policy, and one that you would not naturally be considering under the current policy - for instance $s_{t+1}$ could be unreachable under the current policy (although for $\epsilon$-greedy that would never be the case). I think that at least some researchers would call that an off-policy update.
