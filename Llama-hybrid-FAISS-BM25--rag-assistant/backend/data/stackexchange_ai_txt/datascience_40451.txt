[site]: datascience
[post_id]: 40451
[parent_id]: 
[tags]: 
Approximation of function with neural networks

I am looking for the relevant techniques to approximate a function on $[0,1]$ with a neural network when the function has a huge amplitude? Here is an example with a simple neural network. For test = 1 everything works well. For test = 2 then the algo "converges" towards a constant. Why? import numpy as np import tensorflow as tf import matplotlib.pyplot as plt import matplotlib sample_size = 5000000 test = 1 if test == 1: learning_rate = 1e-2 alpha = 2. elif test == 2: learning_rate = 5e-10 alpha = 2000000. xs = np.random.normal(0.,1.,sample_size).reshape(-1,1) ys = alpha * xs * xs xs_test = np.arange(-3.,3.,0.01).reshape(-1,1) ys_test = alpha * xs_test * xs_test matplotlib.interactive(True) fig = plt.figure() ax = fig.gca() ax.plot(xs_test, ys_test, 'r') fig.canvas.draw() fig.canvas.flush_events() x = tf.placeholder(tf.float32, shape=(None,1)) y = tf.placeholder(tf.float32, shape=(None,1)) batch_size = 50 nb_batches = int(np.ceil(sample_size/batch_size)) with tf.name_scope('nn'): hidden = tf.layers.dense(x, 50, name="hidden", activation=tf.nn.elu) outcome = tf.layers.dense(hidden, 1, name="output") with tf.name_scope('loss'): loss = tf.reduce_mean(tf.square(outcome - y)) with tf.name_scope('train'): optimizer = tf.train.GradientDescentOptimizer(learning_rate) training_op = optimizer.minimize(loss) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for i in range(nb_batches): indices = slice(i*batch_size, (i+1)*batch_size) mini_batch_x = xs[indices] mini_batch_y = ys[indices] if i % 100 == 0: print('loop: ', i, 'loss:', loss.eval(feed_dict = {x: xs_test, y: ys_test})) ax.clear() ax.plot(xs_test, ys_test, 'r') ax.plot(xs_test, outcome.eval(feed_dict = {x: xs_test}), 'b.') fig.canvas.draw() fig.canvas.flush_events() sess.run(training_op, feed_dict={x: mini_batch_x, y: mini_batch_y})
