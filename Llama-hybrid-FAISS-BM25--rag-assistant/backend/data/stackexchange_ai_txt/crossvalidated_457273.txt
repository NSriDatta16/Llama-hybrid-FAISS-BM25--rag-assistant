[site]: crossvalidated
[post_id]: 457273
[parent_id]: 457269
[tags]: 
The values in the embedding layer are not probabilities. They are a high-dimensional representation of the word. The way that Word2Vec is trained indeed focuses on having words that tend to co-occur be closer together in the embedding space, because the objective is equivalent, loosely, to minimizing the negative log-likelihood (NLL) of context words given a center word (skip-gram) or minimizing the NLL of a word given its context (CBOW). However, the probabilities in the objective and the numbers in the embedding are not the same. The values of the embedding vector are instead trained via some optimization algorithm (i.e. SGD) based on this probabilistic formulation. I'm a bit confused by your second question about "the rest of the skip-gram part" -- without more context to the image I'm not sure I understand what you're asking.
