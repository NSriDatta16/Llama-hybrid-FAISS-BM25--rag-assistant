[site]: crossvalidated
[post_id]: 318531
[parent_id]: 
[tags]: 
Bayesian updating with discrete priors + possibly unknown classes

I'm following along with some lecture notes on Bayesian updating with discrete priors. They give an example problem to illustrate some of these concepts, which I briefly restate here: Someone tells you they have three types of coins (A, B, and C) each with different probabilities of HEADS: Coin Type A has p = 0.5, Coin Type B has p = 0.6, and Coin Type C has p = 0.9. Determine the posterior probability for each coin type, given that the outcome of a single toss is HEADS. Actually, the authors give you priors as well: The coin being tossed is drawn at random from a desk drawer containing two A's, two B's, and one C. The solution is quite simple: Multiply the likelihoods (in this case, {0.5, 0.6, 0.9}) by the priors {0.4, 0.4, 0.2}, then normalize. In fact, pretty much every set of notes I've read on the subject contains the following expression: $$posterior \,\propto\, likelihood \, \times \, prior $$ To test out some of these ideas, I wrote a quick simulation to see how the posteriors behave after observing $N$ outcomes of coin tosses. In order to make this problem more like a realistic scenario (and less trivial), I'd now like to consider a modified problem: There is now a fourth category of coin, D, whose probability of heads P(HEADS|D) is unknown . The formula quoted above now seems to break, so my question is, How does a Bayesian proceed on the modified problem? One approach I've considered is parameter estimation: After each new coin toss, use all the observed data to estimate the population parameter of a binomial distribution. Then successively test each of four hypotheses: $\mathcal{H}_1 =$ Coin is type A $\mathcal{H}_2 =$ Coin is type B $\mathcal{H}_3 =$ Coin is type C $\mathcal{H}_4 =$ Coin is neither type A, B, nor C But this seems like such a radically different approach, and awfully frequentist. I'm looking for a method that is a simple modification of the method above, or, failing that, a different method that doesn't altogether abandon Bayesian concepts like priors, posteriors, and likelihoods. Thanks for reading
