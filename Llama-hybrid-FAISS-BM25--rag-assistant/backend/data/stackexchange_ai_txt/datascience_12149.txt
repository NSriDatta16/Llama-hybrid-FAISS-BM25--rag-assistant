[site]: datascience
[post_id]: 12149
[parent_id]: 
[tags]: 
How do I find the correct decay rate when the data are not helping?

If I am making nonsense, I beg some mercy. I am only 17 and is not exactly a college freshmen. In Nate's Silvers methodology of predicting election result, he explained that each survey has a decay rate of 200 days. That means, if a new data weights full, data that was published 200 days ago only weights the half of the new day. I mean, it makes senses. A poll of Hillary v Trump D-5 of election is more accurate compared to Hillary v Trump D-200. So I implemented the same thing with soccer result. I have like 12 years of data at my disposal, and using these data, I want to predict the soccer result--specifically to its score (ie.2-5,5-7,3-8) not just who wins and who loses. I treated this as a classification problem. So my idea was to have a decay rate when evaluating the match. I mean, older matches are pretty much useless right? I used Bayesian Theorem to predict the result. To get the prior and like hood probability, I used Gaussian and Poisson distribution. The reason I used Poisson distribution because a research against Bundesliga showed that using Poisson distribution yields best match prediction. The reason I used Gaussian distribution because it turns out that goal distribution is pretty much log-Gaussian (you have to log transform the data first to make it Gaussian.) The plan was to train the decay rate using the train data, and later pick the best model using validation data. I decided to create a variable called decayrate = X, where the value of match occurs in a season is 1/X of the value of the match occurs in the next season. For ex: I'm predicting a match from 2013/2014 with a decayrate = 1.5. A match from 2012/2013 weights full. A match from 2011/2012 weights = 1/1.5, A march from 2010/2011 weights 1/(1.5^2), and so on. To find the correct decay rate, I find the logloss of each method when pitted against every decay rate from 1.0 ~ 2.0, multiple of 0.05. The decay rate with the lowest logloss is the optimal decay rate. As an experiment, I randomly pick 25% match from the pool of data as a train data 10 times--not as a retarded attempt to do cross validation--but to check if the optimal decay rate changes when pitted against different train data. With Gaussian distribution, the optimal decay rate is always either 1.20 or 1.25--that's close enough. The problem is when I used Poisson distribution, the optimal decay always changes as I throw in another random training. It can be 1, it can be 2, or it can 1.3. There's no pattern. So my question is. 1) Is there anything I can do when the data is not working with me? 2) Is there a technical jargon for this? I googled time series but no result. I googled decay rate data science , but what I got was exponential decay rate used for physics.
