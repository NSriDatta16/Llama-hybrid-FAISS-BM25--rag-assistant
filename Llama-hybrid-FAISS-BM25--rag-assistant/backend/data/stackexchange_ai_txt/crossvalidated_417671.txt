[site]: crossvalidated
[post_id]: 417671
[parent_id]: 417181
[tags]: 
Computing the softmax over the entire vocabulary with tens of thousands of words is computationally expensive. With negative sampling, you do not update most of the rows in the output matrix, so you eventually end up with a less accurate estimate of the probability distribution. However, in word2vec you don't care about modeling the distribution of a language model, the only thing you need is to collect training signal that is strong enough to be propagated to the embeddings.
