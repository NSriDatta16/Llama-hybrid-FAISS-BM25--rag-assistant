[site]: crossvalidated
[post_id]: 230659
[parent_id]: 230653
[tags]: 
Your comment regarding the epoch is true. So, if you use too few epoches, you may underfit and using too many epoches can result in overfitting. As you know you can always increase the training accuracy arbitrarily by increasing model complexity and increasing the number of epoch steps. One way to try and alleviate this problem could be through early stopping. In pseudocode: Split data into training, validation and test sets. At every epoch or every N epoch: evaluate network error on validation dataset. if the validation error is lower than previous best, save network to epoch. The final model is the one with the best performance on validation set. This is very similar to the classical cross validation techniques you use in machine learning approaches. Regarding convergence, you usually say the network has converged to some local minima if your error metric and weights are relatively constant over several iterations.
