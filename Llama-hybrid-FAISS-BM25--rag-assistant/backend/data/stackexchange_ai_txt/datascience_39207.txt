[site]: datascience
[post_id]: 39207
[parent_id]: 30693
[tags]: 
One important difference between isolation forest and other types of decision trees is that it selects features at random and splits the data at random, so it won't produce a nice feature importance list; and the outliers are those that end up isolated with fewer splits or who end up in terminal nodes with few observations. I guess if you dig into the code you might be able to rank the features according to levels of the splits that use them and the number of observations in the nodes, but AFAIK neither the scikit-learn implementation, nor Zelazny's R implementation, have such a thing built-in (although the R one has some functions for individual nodes: https://github.com/Zelazny7/isofor/blob/master/R/interpret.R ). Nevertheless, after you identify the outliers from your sample, you can use that as labeled data for a classification problem, and from there fit a model such as xgboost or random forest that could give you the feature importances.
