[site]: datascience
[post_id]: 70106
[parent_id]: 
[tags]: 
What are some modern algorithms\analysis that use Taylor Series or Lagrange multipliers?

What are some modern machine learning algorithms\analysis that use Taylor Series or Lagrange multipliers? I spent a good chunk of my weekend reading about SQP, aka SLSQP. SciPy has an implementation, but when I looked at the source code I realized it was all in Fortran. Is there is a python implementation somewhere? I found this textbook excerpt: https://www.math.uh.edu/~rohop/fall_06/Chapter4.pdf . This resource was pretty good at explaining 1) the Taylor Series approximation 2) the Lagrange equation (multiplier variables on the constraint function). I started getting lost when the book went into approximating the Hessian of the Lagrange function. I have no idea how to actually implement this algorithm in python. I know that XGBoost uses a quadratic (Taylor Series) term in the approximation of the loss before making a split. What some other examples? Is it worth learning about or should I stick to learning LP\MIP\GA\POS? Taylor Series approximation of the optimal point x at point k: $ f(x) = f(k) + f^{'}(k)*(x-k) + 0.5*(x-k)^Tf^{''}(k)(x-k)$ And the Lagrange equation where the gradients of the objective and the constraints are equation to zero: $\nabla L = \nabla f(x) + \lambda * \nabla h(x) =0$ .
