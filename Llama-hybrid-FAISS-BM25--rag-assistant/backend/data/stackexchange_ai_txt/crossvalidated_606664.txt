[site]: crossvalidated
[post_id]: 606664
[parent_id]: 606628
[tags]: 
Preamble: We often do a TFIDF transform before feeding that matrix to the SVD. In addition: $U$ is term matrix (holding left singular vectors) and $V$ is the (potentially truncated) document matrix (holding right singular vectors), both are orthonormal. To your main question: In the example shown in Wikipedia for a document approximation ( $\hat{d} = \Sigma^{-1}_k U^T_kd$ ), we multiply by the inverse of $\Sigma$ to whiten our data when projecting them to the new document-space; the elements in the diagonal matrix $\Sigma$ can be though as "standard deviations" so by multiplying with the inverse of them we ensure that the projected scores will have unit variance. Note that with LSA, contrary to standard PCA, the input data in LSA example are not usually preprocessed in any way; nevertheless by construction the components in $U$ still define an orthonormal basis (for $XX^T$ ). Examining now to the Medium.com article linked: Our (query) document approximation is given as $\hat{d} = U^Td$ . $\Sigma V^T$ is the projection of our 9 documents in the rescaled document-space. We note that in this document-space the projections are rescaled by $\Sigma$ . Therefore when projecting a new document $d_i$ in that space (the one from $\Sigma V^T$ ) it makes sense to use $U^T$ only because we want to have projections in their original scale, we don't want to whiten them. As I see it, the author tried (and succeeded) to match gensim output/conventions. Both sources are correct, just they differ on where they expect $\hat{d}$ to reside.
