[site]: datascience
[post_id]: 88658
[parent_id]: 88520
[tags]: 
All current pre-trained Transformers (BERT, RoBERTa, ...) can take two sentences as an input, simply by concatenating the sentences and separating them by a special token. You can then finetune the model on the paraphrase dataset. The Microsoft Research Parallel Corpus is part of the GLUE benchmark for sentence representation evaluation. It has a public leaderboard, so you can have a look and get an idea, how the best models look like.
