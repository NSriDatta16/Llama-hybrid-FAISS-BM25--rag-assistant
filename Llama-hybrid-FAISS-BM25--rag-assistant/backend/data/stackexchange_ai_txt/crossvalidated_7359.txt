[site]: crossvalidated
[post_id]: 7359
[parent_id]: 7357
[tags]: 
The reason that the $R^2$ values are not matching is because randomForest is reporting variation explained as opposed to variance explained. I think this is a common misunderstanding about $R^2$ that is perpetuated in textbooks. I even mentioned this on another thread the other day. If you want an example, see the (otherwise quite good) textbook Seber and Lee, Linear Regression Analysis , 2nd. ed. A general definition for $R^2$ is $$ R^2 = 1 - \frac{\sum_i (y_i - \hat{y}_i)^2}{\sum_i (y_i - \bar{y})^2} . $$ That is, we compute the mean-squared error, divide it by the variance of the original observations and then subtract this from one. (Note that if your predictions are really bad, this value can go negative.) Now, what happens with linear regression ( with an intercept term! ) is that the average value of the $\hat{y}_i$'s matches $\bar{y}$. Furthermore, the residual vector $y - \hat{y}$ is orthogonal to the vector of fitted values $\hat{y}$. When you put these two things together, then the definition reduces to the one that is more commonly encountered, i.e., $$ R^2_{\mathrm{LR}} = \mathrm{Corr}(y,\hat{y})^2 . $$ (I've used the subscripts $\mathrm{LR}$ in $R^2_{\mathrm{LR}}$ to indicate linear regression .) The randomForest call is using the first definition, so if you do > y 1 - sum((y-predicted)^2)/sum((y-mean(y))^2) you'll see that the answers match.
