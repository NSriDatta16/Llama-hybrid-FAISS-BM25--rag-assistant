[site]: crossvalidated
[post_id]: 218695
[parent_id]: 217593
[tags]: 
Short version: Take a Gaussian centered at the previous estimate, with std. dev. equal to the CI. Long version: Let $\phi_0$ be the true value of the parameter, and let $\hat \phi$ the estimate that you have. Assume an a priori uniform prior $P(\phi)=ct$. You want to know the distribution of $\phi_0$ given that an estimate $\hat \phi$ has already been obtained: $$ P(\phi_0|\hat\phi)=\frac{P(\hat\phi|\phi_0)P(\phi_0)}{P(\hat \phi)}\\ =P(\hat\phi|\phi_0) \frac{ct}{P(\hat \phi)} $$ Now the only dependence on $\phi_0$ is in the term $P(\hat\phi|\phi_0)$, the rest is a normalization constant. Assuming the $\hat\phi$ is a maximum likelihood estimator (or some other consistent estimator), we can use the following facts: As the number of observations increases, the MLE is asymptotically Gaussian, It is asymptotically unbiased (centered at the true value $\phi_0$), It fluctuates around $\phi_0$ with variance equal to the inverse Fisher Information of the prior observations, and that is what I would have used as CI (squared). Another way to put it: The Bayesian posterior and the distribution of a consistent and efficient estimator become asymptotically the same.
