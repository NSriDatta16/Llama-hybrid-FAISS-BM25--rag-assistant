[site]: datascience
[post_id]: 42024
[parent_id]: 41704
[tags]: 
If you use logistic regression, you can try the following: Oversample the minority class by passing class_weight="auto" to the LogisticRegression constructor. You may also want to set intercept_scaling=1e3 (or some other large value). See the docstring for details. Edit: As per sklearn version 0.17 'class_weight="balanced"'. Change the intercept of the model. The class_weight should have made sure that you got the intercept (log-odds prior) for a 50/50 split, which can be turned into one for a 90/10 split by prior = .9 lr.intercept_ += np.log(prior / (1 - prior)) - np.log((1 - prior) / prior) This mathematical trick is common in epidemiology (or so I've been told), where often you have a set of n_positive cases and a very small prior probability of disease, but getting a control group of actual size prior / (1 - prior) * n_positive is prohibitively expensive. Similar tricks can be played with other probability models by multiplying the prior into their outputs, rather than folding it into the model directly. Naive Bayes (not a good probability model, but I'll mention it anyway) actually takes an optional class_prior argument.
