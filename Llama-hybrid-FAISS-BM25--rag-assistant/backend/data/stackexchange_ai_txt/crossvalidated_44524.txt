[site]: crossvalidated
[post_id]: 44524
[parent_id]: 44518
[tags]: 
It probably has to do with the characteristics of your problem! What sorts of features are you using for your classification (i.e., are they binary, continuous-valued, etc.), and what's the dimensionality of your problem? I don't have a reference for this off the top of my head, but it's a well-accepted observation that SVMs tend to perform well in high-dimensioned classification problems, which is why they are so ubiquitous in text classification, where problems frequently have over hundreds of thousands of dimensions. As an algorithm, $k$NN has always interested me, because it's a very general approach to classification that can be adapted to the characteristics of a particular problem. For example, the value of $k$ can (and should) be optimized using a hold-out training data set, and can differ drastically from problem to problem. What's more, there are any number of distance metrics that can be used to determine how data points relate to one another. In my own work, I've used mutual information as a distance metric for protein-protein interaction text classification, which out-performed other distance metrics and an SVM-based approach. This specific flavor of $k$NN, however, hasn't been as successful on other problems, though!
