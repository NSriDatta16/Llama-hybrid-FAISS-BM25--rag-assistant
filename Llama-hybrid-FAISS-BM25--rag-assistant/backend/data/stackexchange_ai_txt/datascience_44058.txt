[site]: datascience
[post_id]: 44058
[parent_id]: 44055
[tags]: 
The problem is that the correlation matrix has to be done with numerical values. So what you have to do is to transform the texts into numerical vectors. There are several ways of doing this, there are libraries like gensim that can make implementation easier. Bag-of-words A vector of the most frequently used words across all texts is created. Then for each text sample the occurrence of each word in that sample is counted. "John likes to watch movies Mary likes movies too" Would give {"John":1,"likes":2,"to":1,"watch":1,"movies":2,"Mary":1,"too":1, ... other words in corpus} And then the values of this could be used to calculate the correlation matrix [1,2,1,1,2,1,1, ...] This method can also be improved by using TF-IDF . Word embeddings and averaging Word embeddings are when a word is mapped to vectors that try to encode the information in a word. These can trained from scratch for the task at hand, but there are also pretrained word embeddings that can input a word and output a embedding vector. To get a single vector from an entire text the average word embedding can be used. Doc2Vec Doc2Vec expands on word embeddings and is speficially used for embedding entire chunks of texts.
