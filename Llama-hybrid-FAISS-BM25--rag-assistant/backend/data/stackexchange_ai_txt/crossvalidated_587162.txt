[site]: crossvalidated
[post_id]: 587162
[parent_id]: 
[tags]: 
Proper use of multiple test correction

So in this publcition https://www.nature.com/articles/s41586-019-1787-x which invovles a huge dataset of neuronal spiking data they have included neurons in a downstream analysis based on whether or not they pass 6 tests. So the data is time series and basically the test are simple hypothesis tests from the neurons rate from a time window before an event and then a time window after that event or another. So you get a set of rates X and Y and test using wilcoxin or hmanwhittney-U or rank-sum to determine if for that neuron there is a significant difference in rates between events A and B across the recording. For non neuro people we can consider each neuron generating samples for 6 different questions essentially. But I think the way they've done multiple test correction is funny, they do 6 tests per neuron so they use the Bonferonni i.e. 0.05/6. But they are doing this across like 42000 neurons each subject to these six tests. Also I would consider each event they are testing around as different families, so correcting across 6 different tests to me makes no sense. To me they should calculate all the pvalues across all neurons then for each of the 6 tests use a FDR correction, Benajamini-Hochberg would be my go to. Am I correct in my feelings?
