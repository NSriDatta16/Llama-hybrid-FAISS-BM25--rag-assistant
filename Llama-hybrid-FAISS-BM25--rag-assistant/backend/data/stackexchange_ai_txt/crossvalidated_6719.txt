[site]: crossvalidated
[post_id]: 6719
[parent_id]: 6707
[tags]: 
Systematic and unsystematic are rather ambiguous terms. One of the possible explanations is given by @probabilityislogic. Another may be given here. Since the context you gave is time series, I think this might be related to Wold's theorem . Unfortunately wikipedia text captures the essence, but does not go into the details of which part is systematic and non systematic. I did not manage to find appropriate link to refer to, so I will try give some explanation based on the book I have. This subject is also discussed in this book . I will not give precise and rigorous definitions, since they involve Hilbert spaces and other graduate mathematics stuff, which I think is not really necessary to get the point across. Each covariance-stationary process $\{X_t,t\in \mathbb{Z}\}$ can be uniquely decomposed into two stationary proceses: $X_t=M_t+N_t$, singular $M_t$ and regular $N_t$. Singular and regular processes are defined via their prediction properties. In stationary process theory the prediction of process $X_t$ at time $t$ is formed from linear span of its history $(X_s,s $$E(\hat{X}_t-X_t)^2$$ is zero. Such processes sometimes are called deterministic, and in your context can be also called systematic. The most simple example of such process is $X_t=\eta$ for all $t$ and $\eta$ some random variable. Then the linear prediction of $X_t$ based on its history will always be $\eta$. The error of such prediction as defined above would be zero. Regular stationary processes on the other hand cannot be predicted without error from their history. It can be shown that the stationary process $N_t$ is regular if and only if it admits $MA(\infty)$ decomposition. This means that there exists white-noise sequence $(\varepsilon_t)$ such that $$N_t=\sum_{t=0}^{\infty}c_n\varepsilon_{t-n}.$$ where coefficients $c_n$ are such, that the equality holds. These processes sometimes are called non-deterministic, or probably non-systematic in your case.
