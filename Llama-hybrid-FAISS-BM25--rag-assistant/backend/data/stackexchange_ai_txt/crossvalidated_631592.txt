[site]: crossvalidated
[post_id]: 631592
[parent_id]: 
[tags]: 
Maximum Entropy distribution of a ticking clock

Say I have a clock that emits "ticks". An ideal clock looks like a dirac comb . It has: perfect periodicity of ticks (there is a precise fixed time interval between any two consecutive ticks) perfect tick sharpness (infinitely high infinitely thin spikes at each tick, integrating over a tick will increase the state by 1, whereas any interval without a tick integrates to 0) Now in reality, if you were to try to realize such a clock, it's going to take infinite energy. $^{1.}$ If I relax the condition of perfect periodicity, I believe the maximum entropy distribution of that turns out to be the exponential distribution: I get sharp ticks with an average ticking rate $\lambda$ but no guarantee for the spacing. In order to actually extract time from this, I need to add up multiple ticks, which ends up giving me some average time interval that increases in sharpness as I add up more ticks. It therefore turns into the Gamma distribution. $^{2.}$ If I instead relax the condition of perfect sharpness, but guarantee that there is one tick within a time interval, arbitrarily setting that to "one revolution", I believe the maximum entropy distribution of that is the VonMises distribution. I can then directly control sharpness by that distribution's parameter $\kappa$ and in the limit $\kappa \to \infty$ , I recover the dirac comb. $^{3.}$ What if I relax both conditions? What is my maximum entropy distribution if I neither have perfect sharpness nor perfect periodicity, but I can say something about the expected value of either? Presumably, the solution will degenerate into the gamma distribution if $\kappa \to 0$ and into the (periodically extended) VonMises distribution in the limit $\alpha,\beta \to \infty, \frac{\alpha}{\beta}\to\tau . I think conceptually it's equivalent to a scaled VonMises distribution with arbitrary periodicity $\tau$ like $$\frac{e^{\kappa \cos\left(2 \pi \frac{t}{\tau} - \mu\right)}}{\tau I_0\left(\kappa\right)}$$ , where $\tau$ is itself distributed according to a gamma distribution $$\frac{\beta^\alpha}{\Gamma\left(\alpha\right)}x^{\alpha-1}e^{-\beta x}$$ . It's like directional statistics on a circle of unknown, gamma distributed radius. But I'm not sure simply doing that, i.e. taking the compound distribution of this modified VonMises distribution and a Gamma Distribution, is the actual maximum entropy solution for this problem. Footnotes $^{1.}$ If that doesn't make intuitive sense, it's not really important for this question, but it comes down to any physical "ticking" system being in some way equivalent to a driven harmonic oscillator and if you want extreme sharpness and extreme consistency, you need extreme energies. For a more mathematical take, in particular, the absolute of the Fourier transform of a single dirac distribution $\delta_{x_0}\left[x\right]$ , i.e. the distribution's power spectrum, is just a constant function which obviously can't be integrated over all of $\mathbb{R}$ . And having infinitely many such spikes is surely not improving on this. $^{2.}$ This is equivalent to using radioactive decay as a means to keep time. You get an arbitrarily good time estimate by taking arbitrarily many ticks as your time scale. It's basically a monte carlo take on keeping time. The faster the decay rate, and the more ticks you take together to count as one unit of time, the more consistent this process becomes. $^{3.}$ Technically, you can still think of this in terms of infinitely sharp ticks, but you confine the uncertainty of when a tick occurs within a fixed known interval. It's like, given a periodically fluctuating input signal, when do you decide that a tick has occurred? Intuitively, by setting some threshold at which a detector says so. If you set it too stringently, you might miss a tick, if you set it too broadly, you might pick up additional ticks that were just noise, and the VonMises distribution ought to describe that signal with higher $\kappa$ representing a higher SNR
