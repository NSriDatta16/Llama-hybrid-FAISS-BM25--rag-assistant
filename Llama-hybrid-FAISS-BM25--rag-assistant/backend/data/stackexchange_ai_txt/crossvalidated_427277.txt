[site]: crossvalidated
[post_id]: 427277
[parent_id]: 426939
[tags]: 
There are two terms in the ELBO: $$E_{z \sim q}[\log P(x|z)] - \text{KL}(q(z)||p(z))$$ We estimate the first term by sampling a single $z$ and computing $\log P(x|z)$ . Since the VAE models $x|z \sim \mathcal{N}(\mu, \sigma^2)$ , where $\mu = f(z;\theta)$ for some decoder neural network $f$ , and the log of the gaussian density is $-(\mu - x)^2$ (up to some constant factors and scaling), therefore this squared "reconstruction" loss is correct. And we should sample from that output to calculate the reconstruction loss. Not quite. You should sample from that output if you want to sample from the distribution modeled by the VAE, but we have shown here that the "reconstruction loss" between $\mu$ and $x$ is the correct loss to use.
