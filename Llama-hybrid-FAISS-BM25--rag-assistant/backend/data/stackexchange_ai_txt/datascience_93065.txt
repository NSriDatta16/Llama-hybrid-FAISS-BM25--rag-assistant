[site]: datascience
[post_id]: 93065
[parent_id]: 93062
[tags]: 
Translation as a pre-processing step is usually sufficient for many tasks (e.g. sentiment classification), but naturally undesirable for other tasks e.g. grading someone in written Dutch fluency . Hence, for these tasks, the objective is: Be able to train a language model for your specific language However, you want to be able to do this with minimal resources Thus, people in research view this as few-shot learning . One of the most common approaches to this is to exploit meta-learning, i.e. use models that have been pre-trained using many tasks (languages in this case) and they have been trained so that only a few gradient steps and little data is required for them to be applicable to a new task (Dutch in your example). An example of one such model you could finetune (transfer learning) is called multiliingual BERT... this was back in 2019, so I expect since many other multilingual language models have emerged - these can be good initialisation points for any bespoke model you want to train for some new language...
