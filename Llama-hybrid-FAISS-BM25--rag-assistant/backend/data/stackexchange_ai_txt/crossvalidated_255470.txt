[site]: crossvalidated
[post_id]: 255470
[parent_id]: 255447
[tags]: 
Decision trees and Random forests are known to be useful to do feature selection. If you train your data on any of those at the end you'll be able to sort features by importante. The nature of those methods works by kind of splitting data based on the gain of information when a particular feature is used to split data. If you split the data with a good feature you'll will gain more information than if you split data with a bad feature. Check this reference using R on the examples: http://freakonometrics.hypotheses.org/19835 The thing with lasso is that l1 regularization can produce 0 coefficients so people often refers to that property as a 'builtin feature selector'. If the coefficient is 0 it means that this feature it's not relevant to describe the data. It's also a popular way to do feature selection.
