[site]: crossvalidated
[post_id]: 85523
[parent_id]: 18178
[tags]: 
You need to define what you mean by "accuracy". What you would like to know, please pardon me for putting words in your mouth, is how well your model fits the training data, and more importantly, how well this model "generalizes" to samples not in your training data. Although ROC curves can be useful in analyzing the tradeoff between precision and recall for various values of the threshold, I suggest adding mean-squared-error, or the Brier score to your toolbox. It's easy to compute, and you can immediately get a feel for whether feature changes affect the fit of the model, when applied to training data. Since overfit is possible in this case, your job isn't done here. To evaluate generalization performance, or how well you do on data you haven't seen, it isn't enough to look at your performance on the training samples. Of course your model is good at those, because they the values you used to determine the coefficients to your logistic. You need to set aside some samples for test data. Your MSE performance on this set should set your generalization expectations according to the Hoeffding inequality. Your maximum generalization error will depend on the number of features in your model as well as the number of samples used to compute the test statistic. Be mindful that you'll need to steal some of your training samples for test samples. I recommend 10-fold cross-validation, where you shuffle, choose 90% for training, 10% for testing, and then measure, repeat, and then average all the measurements.
