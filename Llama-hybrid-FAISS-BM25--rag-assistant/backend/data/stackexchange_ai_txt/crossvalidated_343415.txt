[site]: crossvalidated
[post_id]: 343415
[parent_id]: 
[tags]: 
GVIFs increase after removing variables

I have a qualitative dependent variable (Y), a dicothomic cathegorical variable (X1), a cathegorical variable (around 60 non-homogeneous groups, X2), two cathegorical variables (10-20 groups each, X3 and X4) and the date of the observation (ranging 3 years, quantitative X5). I have obtained as well the month-year (X6) of the observation. N= 300000 observations. I want to see significant differences in Y between X2 groups over time while controlling X1 and X3 (partial correlation, but since Y is qualitative I used a glm). I have used R to create a glm (logistic regression), but I get strange GVIFs ( GVIFs are higher when they should be lower ) Models I have tried: 1) Y ~ X6 + X1 + X2 * X5 + X3 + X4 car package gvifs: GVIF Df GVIF^(1/(2*Df)) X6 1.379939e+03 35 1.108805 X4 2.349995e+00 23 1.018748 X1 1.016412e+00 1 1.008173 X3 5.616355e+00 7 1.131182 X2 7.019190e+209 60 56.068516 X5 1.290364e+03 1 35.921643 X2:X5 7.006197e+209 60 56.067650 GVIFs are appalling and unaceptable, but since I'm used cathgorical variables I put more attention to corrected GVIFs (which are still very high). I don't need the date of the observation (X5) as a predictor and I believe it's correlated with the month (X6) so I try the following model. 2) Y ~ X6 + X1 + X2 + X2 : X5 + X3 + X4 This is the model I like the most: I can draw conclussions for changes (trend) in each group over time (X2:X5 in R glm notation) while controlling X3 and X4 and paying some attention to monthly changes (X1). I wouldn't want to have X5 instead of X1 since the monthly changes for the whole group over time don't seem to be linear. GVIF Df GVIF^(1/(2*Df)) X6 NaN 35 NaN X4 NaN 23 NaN X1 NaN 1 NaN X3 NaN 7 NaN X2 Inf 60 Inf X2:X5 Inf 61 Inf vifs obtained via rms package are also higher for the interactions than when using the previous model (up 4-5 times higher for them) I don't understand this. I thought by removing a variable with colinearity issues my VIFs would drop.
