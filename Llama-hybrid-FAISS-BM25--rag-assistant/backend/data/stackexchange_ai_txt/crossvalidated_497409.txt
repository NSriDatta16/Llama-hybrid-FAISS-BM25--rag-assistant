[site]: crossvalidated
[post_id]: 497409
[parent_id]: 497406
[tags]: 
PCA doesn't predict anything so I think you may be committing a category error here. PCA is an optimal low-rank approximation method. You can use it to reduce the number of dimensions in a dataset amongst other things. Sometimes people apply PCA to a dataset prior to using other algorithms in order to reduce collinearity between variables because the PCA transformed variables are guaranteed to be orthogonal to each other and that sometimes helps. Retaining fewer than all the dimensions can also be useful as a way of ignoring the effect of dimensions which don't account for much variance. The closest thing I can think of that to some extent fits your description is a binary classifier use case. You could fit PCA to a training set and use the resultant linear space to evaluate whether vectors from the test fit the space. You'd do this as follows: Fit PCA to your training set. Transform vectors from your test set using the fitted model, then inverse transform the transformed vectors to get an approximatino of your test vectors according to the model. Evaluate your true test vectors versus your approximations. If an approximation is good enough then classify the vector as from the same distribution as the training set.
