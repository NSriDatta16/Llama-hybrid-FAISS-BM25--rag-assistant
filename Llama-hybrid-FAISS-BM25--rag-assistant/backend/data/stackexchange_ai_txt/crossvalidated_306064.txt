[site]: crossvalidated
[post_id]: 306064
[parent_id]: 295358
[tags]: 
Yes, you are right, your output of xgb.cv may indicate over-fitting. You should change parameters lambda and alpha which control L2 and L1 regularization and/or use early stopping. This will make your model more general, less fitted to training dataset and thus more suited to predict on test dataset. Since RMSE on test set is still decreasing, you could use more iterations. You could read about these techniques here: high-level description of regularization in xgboost , early stopping with examples in Python , Elements of Statistical Learning - although this position does not cover xgboost implementation there is a chapter about regularization in boosted trees.
