[site]: crossvalidated
[post_id]: 294873
[parent_id]: 
[tags]: 
What is the significance of the Delta matrix in Neural Network Backpropagation?

I'm currently taking Andrew Ng's Machine Learning course on Coursera, and I feel as though I'm missing some key insight into Backpropagation. Particularly, I'm stuck on this algorithm slide: First, when we set capital Delta in the line right above the loop from i=1 to m, what does this represent? My current understanding is that $\Delta$ is a matrix of weights, where index l is a given layer of the network, and indices i and j together represent a single weight from node j in layer l to node i in layer l+1 . Or is this i in relation to the number example we are currently training on in the for-loop? Furthermore, what does this matrix look like, for say a 3 layers with 3 nodes each? I'm having a hard time visualizing this information in a matrix. Next, I see that after Forwardpropagation we simply subtract the real values $y^{(i)}$ from the predicted values $a^{(L)}$ to determine the "first" layer of error (really the error on the output layer). Following that, I'm awfully confused. How are the proceeding layers deltas being computed? We use this function below: My intuition is that we multiply the errors by their corresponding weights to calculate how much each should contribute to the error of a node in the next layer, but I don't understand where the $g^{'}(z^{i})$ comes in-- also, why g prime? Furthermore, what is the significance of this update rule on $\Delta$ and why are we simply adding these up and then setting $D^{(l)}_{ij}$ to the final sum? Furthermore, how is this all of a sudden equivalent to the partial derivative of the cost function J with respect to the corresponding theta weight? I've been struggling with this for a few days now, and I must be missing something pretty substantial. Any clarification would be really appreciated.
