[site]: crossvalidated
[post_id]: 78715
[parent_id]: 
[tags]: 
Scaling hinge loss in SVM

Recall the functional of primal SVM problem: $\|w\|^2 + C \sum_i \xi_i \to \min$. Suppose I have 1000 training objects, and want to find optimal $C$ by 2-fold cross-validation. As a result of cross-validation I'll get $C$ that is optimal for certain scale of $\sum_i \xi_i$. When I use this $C$ to train SVM on full training sample, there will be 1000 summands in $\sum_i \xi_i$ instead of 500, so the scale of this penalty component will be different. The question is: will my $C$ be optimal for sample with 1000 objects if it was trained on sample with 500 objects? Why shouldn't we scale the penalties: $\|w\|^2 + \frac{C}{L} \sum_i \xi_i \to \min$, where $L$ is the size of sample?
