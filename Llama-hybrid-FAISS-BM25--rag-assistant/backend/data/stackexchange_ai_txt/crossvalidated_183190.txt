[site]: crossvalidated
[post_id]: 183190
[parent_id]: 
[tags]: 
Parameters go to zero for lasso regularization function

From the book Pattern Recognition and Machine Learning by Bishop I saw this picture which shows the contours of a regularization function. The advantage of this regularization function (right graph) is supposed to be that one of the parameters equals zero at some optimal solution(in this case w2 is zero at w*). I can see from the picture why this is true in this case, what I do not get is wy the the contours of the unregularized error function (blue lines) can not shift downwards such that the two shapes are tangent to each other on the side of the red contour such that none of the two parameters is equal to zero.
