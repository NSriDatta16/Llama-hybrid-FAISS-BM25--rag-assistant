[site]: datascience
[post_id]: 104165
[parent_id]: 104087
[tags]: 
We can use the below code to fetch the embeddings for each word words_embeddings = {w:embeddings[idx] for w, idx in tokenizer.word_index.items()} res_vectors = np.empty((0, 2), float) words = [] for k,v in words_embeddings.items(): print(k,"-->", v) words.append(k) res_vectors = np.append(res_vectors, [v], axis=0) Since each word is represented as a 2D vector, I have not reduced the dimensionality of the vector. With the below code we can get the word representations. import matplotlib.pyplot as plt plt.figure(figsize=(13,7)) plt.scatter(res_vectors[:,0],res_vectors[:,1],linewidths=10,color='blue') plt.xlabel("PC1",size=15) plt.ylabel("PC2",size=15) plt.title("Word Embedding Space",size=20) for i, word in enumerate(words): plt.annotate(word,xy=(res_vectors[i,0],res_vectors[i,1])) To get better results, try to increase the vector dimensions of each word. If we use a 100-dimensional vector for a word. We can make use of PCA as below. from sklearn.decomposition import PCA pca = PCA(n_components=2) res_vectors = pca.fit_transform(res_vectors) print(res_vectors) Word representations in this case. We can get the phrase embeddings as below: doc_vecs = np.empty((0,2), float) for i, doc in enumerate(padded_docs): vec = np.empty((0,2), float) for token in doc: vec = np.append(vec, [embeddings[token]], axis=0) vec = vec.mean(axis=0) print(docs[i], "-->", vec) doc_vecs = np.append(doc_vecs, [vec], axis=0) Phrase Representation: import matplotlib.pyplot as plt plt.figure(figsize=(13,7)) plt.scatter(doc_vecs [:,0],doc_vecs [:,1],linewidths=10,color='blue') plt.xlabel("PC1",size=15) plt.ylabel("PC2",size=15) plt.title("Phrase Embedding Space",size=20) for i, doc in enumerate(docs): plt.annotate(doc,xy=(doc_vecs [i,0],doc_vecs [i,1]))
