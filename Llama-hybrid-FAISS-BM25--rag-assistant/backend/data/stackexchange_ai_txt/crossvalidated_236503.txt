[site]: crossvalidated
[post_id]: 236503
[parent_id]: 
[tags]: 
Random Forest signal lagged?

I would like to expand the original question posted here First steps learning to predict financial timeseries using machine learning with another question of my own: Let's fit an overly basic RF algorithm in R following the example: getSymbols("GOOG") fit 0, 1,-1) ret = diff(log(GOOG$GOOG.Close)) pnl = ret * sig I'm not lagging the signal sig because I already lagged the explanatory variable. Is that correct or I should lag the signal again such as: sig1 = lag(sig,1) pnl = ret * sig1 What is the correct procedure ? Thank you!
