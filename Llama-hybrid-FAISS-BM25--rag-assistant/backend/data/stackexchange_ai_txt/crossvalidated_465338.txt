[site]: crossvalidated
[post_id]: 465338
[parent_id]: 465232
[tags]: 
The easier way to think of this is as matrices: a two-layer linear-activation neural network (with bias units) can be written $$ f(x) = W (V x + b) + c = (W V) x + (W b + c) .$$ Using your notations, the first layer has its weights $V$ of shape $M \times D$ (and bias $b$ of shape $M$ ), and the second layer has weights $W$ of shape $K \times M$ (and bias of shape $K$ ). This is equivalent to a single layer, with linear activations, whose weights are $W V$ of shape $K \times D$ and bias $W b + c$ . Because $c$ can be anything, we can ignore the effect of $W b$ there, and get any bias we want for the equivalent one-layer network. $W V$ can be any $K \times D$ matrix whose rank is at most $\min(M, K, D)$ . So, if $M = K$ or $M = D$ , the one-layer network can be any possible matrix. If $M$ is smaller than both $K$ and $D$ , then it limits the set of possibilities. So, in your example, if $D then $M$ doesn't actually limit the final outputs. I don't think Bishop's intended meaning was wrong, but he wrote it in a slightly confusing way: "if the number of hidden units is smaller than either the number of input or output units" should mean " $M ", not the perhaps more natural reading of " $M or $M ."
