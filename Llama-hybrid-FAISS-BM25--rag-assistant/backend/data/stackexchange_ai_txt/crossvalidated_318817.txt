[site]: crossvalidated
[post_id]: 318817
[parent_id]: 271701
[tags]: 
As answered by the others, the primary reason is that it would not work well during backpropagation. However, adding to what the others wrote, it is important to note that differentiability everywhere is not a necessary condition for backpropagation in neural networks, as one may use subderivatives as well. For example, see the ReLU activation function, which is also non-differentiable at 0 ( https://en.wikipedia.org/wiki/Rectifier_(neural_networks) )
