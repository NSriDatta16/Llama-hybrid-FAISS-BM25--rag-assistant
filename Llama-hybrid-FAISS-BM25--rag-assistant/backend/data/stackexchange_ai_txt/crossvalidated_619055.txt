[site]: crossvalidated
[post_id]: 619055
[parent_id]: 
[tags]: 
How to use validation curves within the framework of nested cross validation

I am using nested cross validation to tune and validate a random forest classifier before doing a final evaluation by fitting the model on my entire dataset. I would like to use validation curves to show how changes in my hyperparameters effect my score. My question is whether it is correct to plot learning curves after my final evaluation on the entire train set or whether I should be doing this within my nested cross validation somehow? My steps are as follows: Step 1: Split data into train and test X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) Step 2: Carry out nested cross validation inner_cv = KFold(n_splits=5, shuffle=True, random_state=1) grid_search = GridSearchCV(pipe, param_distributions=param_grid,scoring='accuracy', cv=inner_cv, refit=True, return_train_score=True) outer_cv=KFold(n_splits=10, shuffle=True, random_state=1) scores = cross_val_score(grid_search, X_train, y_train, scoring='accuracy', cv=outer_cv) Step 3: Carry out a final evaluation of the model eval_grid=GridSearchCV(pipe, param_grid, cv=inner_cv, scoring='accuracy', refit=True, return_train_score=True) eval_grid.fit(X_train, y_train)
