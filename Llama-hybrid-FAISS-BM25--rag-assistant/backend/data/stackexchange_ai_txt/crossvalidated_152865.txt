[site]: crossvalidated
[post_id]: 152865
[parent_id]: 152808
[tags]: 
If you remove duplicates, you need to add weights to your data instead, otherwise the result may change (except for single-linkage clustering, I guess). If your data set has few duplicates, this will likely cost you some runtime. If your data set has lots of duplicates, it can accelerate the processing a lot to merge them and use weights instead . If you have on average 10 duplicates of each object, and an algorithm with quadratic runtime, the speedup can be 100 fold. That is substantial, and well worth the effort to merge duplicates.
