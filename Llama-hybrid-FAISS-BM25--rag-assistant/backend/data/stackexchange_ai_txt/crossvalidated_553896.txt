[site]: crossvalidated
[post_id]: 553896
[parent_id]: 553164
[tags]: 
In Bayes by Backprop , the setup is to have a prior over the parameters $\theta$ of the network, then to approximate the posterior as a gaussian and backpropagate to maximize the variational lower bound, using the same reparameterization trick. Then the end result is that you can compute uncertainty intervals for the outputs of your neural net. I think this is basically the procedure described in appendix F. However, the authors only fit a few regression and classification models, and didn't try a "BBB VAE".
