[site]: crossvalidated
[post_id]: 511236
[parent_id]: 
[tags]: 
Measuring uncertainty of model prediction by repeat measurements

Say I’ve trained some single value regression ML model (a neural network or something). I have trained this ML model with simulation data. I see that this neural network is good at predicting data in my training and test data set but they are both simulated data. Now I want to use this network on real data. Suppose I can take multiple trials of my real data which all have the same target value. But I don’t know what this target is. If I use my predictor to predict the value of these trials and then create a distribution of predictions can I place an uncertainty on my measurement? More generally is there a name for this technique? Intuitively, it should seem like I should be able to since if the values are close together then I know my predictor is doing a good job and if they are far then something is going wrong but I’m not sure how to make this rigorous.
