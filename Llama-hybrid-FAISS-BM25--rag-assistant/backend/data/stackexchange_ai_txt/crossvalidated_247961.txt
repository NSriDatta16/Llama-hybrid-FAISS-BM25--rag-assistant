[site]: crossvalidated
[post_id]: 247961
[parent_id]: 
[tags]: 
What are the advantages of normalizing flow over VAEs with deep latent gaussian models for inference?

I am reading the normalizing flow paper and am a bit confused. It seems that being able to model complex (correlated?) posterior is one of the advantages of the proposed approach (Section 2.3, last paragraph). But for a deep latent Gaussian model with diagonal covariance, i.e. $$z^{(1)}\sim N(\mu^{(1)}(x),\mathrm{diag}(\sigma^{(1)}(x))),\\ z^{(2)}\sim N(\mu^{(2)}(z^{(1)}),\mathrm{diag}(\sigma^{(2)}(z^{(1)}))),$$ $P(z^{(2)}|x)$ can also be arbitrarily complex. So why is normalizing flow considered to be more expressive? What did I get wrong?
