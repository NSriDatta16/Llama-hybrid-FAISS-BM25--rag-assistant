[site]: crossvalidated
[post_id]: 450648
[parent_id]: 444954
[tags]: 
I gather that you are interested in $\text{Var}(Y|X_1=x_1)$ when you have $\text{P}(Y|X_1=x_1,X_2)$ The determination of the variance of random variable from its conditional dependence on an additional random variable is described by the LoTV, the Law of Total Variance , also known as Eve's Law due to the way the Es and Vs line up in the expression. In your case, where $X_1$ is specified but $X_2$ is not, Eve's Law gives you $$ \text{Var}(Y|X_1=x_1)=\text{E}[\text{Var}[Y|X_1=x_1,X_2]] + \text{Var}[\text{E}[Y|X_1=x_1,X_2]). $$ As it is quite common to be confused over what is being averaged over, so I'll rewrite the same law to try to make it explicit $$ \text{Var}_{Y|X1}(Y|X_1=x_1)=\text{E}_{X_2}[\text{Var}_{Y|x_1,X_2}[Y|X_1=x_1,X_2]] + \text{Var}_{X_2}[\text{E}_{Y|x_1,X_2}[Y|X_1=x_1,X_2]). $$ where references to $\text{P}(Y|X_1,X_2)$ is really just $\text{P}(\epsilon)\sim N(0,\sigma^2)$ since once you've specified $X_1$ and $X_2$ , the only moving part left is the irreducible noise. Also, I've simplified $\text{P}(X_2|X_1=x_1)=P(X_2)$ as I assume that the predictors are independent. (Is that the case?) The first term is computed in two phases. First, you need to find the variance of the fully conditional model. That is, you'll find the variance of the underlying distribution of $Y$ when $X_1$ and $X_2$ are specified. This variance should just be your irreducible noise $\sigma^2$ , which is presumably not a function of the predictors $X_1$ and $X_2$ . (If you don't satisfy this Gauss-Markov assumption, then the linear model itself must change.) When you finish the first phase, you need to calculate the expectation of that variance over a credible distribution of $X_2$ values. If, as we posit with the Gauss-Markov assumptions, you benefit from homoskedasticity w.r.t. $X_2$ , then $\text{E}[\sigma^2|X_1,X_2]=\sigma^2(X_1)=\sigma^2_{\epsilon}$ . (David uses this result and all associated assumptions in his answer.) The second term is also evaluated in two phases. First you find how $\text{E}[Y|X_1, X_2]$ depends on $X_2$ , then you find the variance of that function respect to the underyling density $\text{P}(X_2),$ which is very likely a significant contributor to variance without violating assumptions. This is what has you worried! And rightly! The crux of the problem is finding some trustworthy $\text{P}(X_2).$ (Perhaps $\text{P}(X_2|X_1)?)$ You don't have $X_2$ and you are loath to impute it, but I hope you are comfortable selecting (and getting general consensus from your colleagues for) a prior $P(X_2)=\pi(X_2)$ which can be used to finish your calculation. This is a classic problem in Bayesian inference: to find $\text{Var}(Y|X_1=x_1)$ when you have $\text{P}(Y|X_1=x_1,X_2)$ and $X_2$ remains in play. This problem is ripe for numerical evaluation, especially if $X_1$ and $X_2$ covary. That is, you can calculate the variance by sampling from the posterior of $\text{P}(Y|X_1=x_1)$ . You can sample from the posterior by sampling from the prior $\text{P}(X_2=x_2)$ and computing $\text{P}(Y|X_1=x_1,X_2)P(X_2=x_2).$ While you may find many papers that use conditional variance, I don't think you'll find a landmark paper about conditional variance. It is sufficient to find a section of a textbook about that. As an aside, the popularity of the name Eve's Law has led to the renaming of its mate LoTE Law of Total Expectation as Adam's Law.
