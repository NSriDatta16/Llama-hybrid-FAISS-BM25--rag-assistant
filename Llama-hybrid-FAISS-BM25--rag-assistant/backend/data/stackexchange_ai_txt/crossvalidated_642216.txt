[site]: crossvalidated
[post_id]: 642216
[parent_id]: 641831
[tags]: 
The only way to get statistical properties that work fully "as advertised" is to completely pre-specify a model. When a model is "tweaked" to better fit the data, Bayesian uncertainty intervals of frequentist compatibility (confidence) intervals will be too narrow because they don't select model uncertainty. P-values will be too small. On the other hand, sticking with an ill-fitting model can be a disaster. That's why I say in Regression Modeling Strategies that using the data to select the model is almost as bad as not doing so. The best solution to my mind is to pre-specify flexible models that contain parameters for things that matter that you don't know. Bayesian priors are the only coherent way to handle much of this. For example you hope a relationship is linear but have a skeptical prior on the amount of nonlinearity as shown here . Faraway in his paper The Cost of Data Analysis showed that the largest impact of model uncertainty comes from how the response variable is transformed. You are using a very parametric model. You can solve many problems by using Y-transformation-invariant semiparametric models . A detailed case study of using semiparametric ordinal models for continuous Y may be found here . Comparing a very limited number of models using AIC or its modifications can be OK, but comparing 5 models results in too low a probability that AIC will select the "right" model.
