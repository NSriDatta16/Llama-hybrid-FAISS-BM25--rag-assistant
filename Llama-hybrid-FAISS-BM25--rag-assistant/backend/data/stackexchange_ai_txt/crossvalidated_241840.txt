[site]: crossvalidated
[post_id]: 241840
[parent_id]: 241798
[tags]: 
I think the problem is that you're plotting the projected data onto the projected basis (provided by PCA). That's confusing. The two ways to think about this are: 1) Original data (original basis) plotted against the new principle components - this will help you see that the principle components provide the best two orthogonal lines which minimize the squared perpendicular distance to your data. In fact, your first PC explains 85% of the variance in your data, so I'd probably just draw the first one only. 2) Projected data (new basis) and standard $\mathbb{R}^2$ - this will help you see how your projected data looks in it's "natural state". That is, if you treated the new basis as the "standard basis" and plotted the projected data onto it. See below for more detail on PCA and some R code to generate these plots. The idea is to find the best k dimensional representation of the data, where usually k is less than the number of dimensions you are starting with. In this case, you start with 2 dimensions, so ideally we'd like to find a single dimension to reduce it to. Which is why plotting it on two dimensions might not provide much insight compared to plotting it on the first principle component How do we know it's the best k dimensions? Because we know that the dimension that preserves the most variance in the data is the one orthogonal to your data - that's where the perpendicular part comes in. The next best dimension that preserves the most variance in your data is orthogonal to that first one. And so on. Each new dimension is orthogonal to the others. So perhaps you should plot your projected data onto the first principle component only, as I showed above. To help illustrate some of the concepts, here is some R code: X = c(-0.7142,-1.7142,-1.7142,-0.7142,1.2857,1.2857,2.2857) Y = c(1.2857,0.2857,-0.7142,0.2857,0.2857,-0.7142,-0.7142) covariance = cov(data.frame(X,Y)) # grab the eigenvalues lambda1 = eigen(covariance)$values[1] lambda2 = eigen(covariance)$values[2] # first principle component captures 85% of the total variance # this suggests that you can express your data in one dimension lambda1/(lambda1 + lambda2) # 0.8461777 # grab the eigenvectors v1 = eigen(covariance)$vectors[,1] # x = -0.9795803 y = 0.2010532 v2 = eigen(covariance)$vectors[,2] # x = -0.2010532 y = -0.9795803 # note that the new basis is orthogonal v1 %*% v2 # 0 r = seq(-4,4,0.1) # plot projection of original data into new basis dim1 = t(v1)%*%t(as.matrix(data.frame(X,Y))) dim2 = t(v2)%*%t(as.matrix(data.frame(X,Y))) projection = cbind(t(dim1),t(dim2)) plot(projection,xlim=c(-3,3),ylim=c(-3,3),main='projected data') abline(h=0,v=0,col='red') # correlation between projected data is 0 cov(projection) # plot original data and 2 principle components plot(data.frame(X,Y),xlim=c(-3,3),ylim=c(-3,3),pch=8,main='original data, 2 PCs') lines(y=v1[2]/v1[1]*r,x=r,type='l',col='red') lines(y=v2[2]/v2[1]*r,x=r,col='red')
