[site]: crossvalidated
[post_id]: 167875
[parent_id]: 
[tags]: 
Using SVM when kernel is simple and sample size is large

Consider SVM classification: $y_i \in \{+1,-1\} $ are labels, $\mathbf{x}_i$ are covariates ($i=1\ldots N$). Let $K(\cdot,\cdot)$ be the kernel function, whose corresponding feature mapping is $h(\mathbf{x})$ with dimension $p$. SVM would provide solution as $\hat{f}(\mathbf{x})= \sum_{i=1}^N{\alpha_i K(\mathbf{x}_i,\mathbf{x})}+b $. With $\mathbf{\alpha} \ \mbox{and}\ b$, there are in total $N+1$ parameter to be optimized. Now consider the case where $N$ is very big, but the kernel is simple (eg. low order polynomial) leading to comparatively small $p$. Would it be simpler if we just forget about the kernel and optimize: $$ \min_{\beta,b} \sum_{i=1}^N{ \big( 1-y_i(h(\mathbf{x}_i)^T\beta+b) \big) _+ } +\lambda||\beta ||^2$$
