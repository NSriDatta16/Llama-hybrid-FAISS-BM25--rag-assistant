[site]: crossvalidated
[post_id]: 590464
[parent_id]: 590311
[tags]: 
You can certainly use logistic regression to estimate the three treatment effects and then test the one-sided hypothesis that A is more effective that B and C. One benefit of this approach is that it's straightforward to include covariates (if you have any). In your case, however, the summary table clearly suggests there is no difference between the drugs: there are 23 successes in 35 patients treated with A and also 23 successes in another 35 patients treated with B or C. The success rates is about 66% for all three drugs. Nevertheless, let's back this up by fitting a logistic regression to the data and then looking at the contrast which compares the effectiveness of A to the average effectiveness of B and C. library("contrast") aggregated $treatment, aggregated$ successes), rep(aggregated $treatment, aggregated$ failures) ), success = c( rep(1L, sum(aggregated $successes)), rep(0L, sum(aggregated$ failures)) ) ) # We can fit the model to the aggregate data by concatenating counts of 0s and 1s. # But the `contrast` function doesn't compute the degrees of freedom correctly. m0 glm model parameter contrast #> #> Contrast S.E. Lower Upper t df Pr(>|t|) #> 1 -0.01313936 0.5060498 -1.023219 0.9969403 -0.03 67 0.9794 (Lower,Upper) is the two-sided confidence interval for the contrast (comparison) between A and the average of B and C. However, since the t statistic is negative ( t = -0.03), we know that the one-sided confidence interval includes 0 as well: the data is consistent with the null hypothesis that A is as effective as B and C. # Two-sided confidence interval -0.01313936 + qt(.975, 67) * 0.5060498 * c(-1, 1) #> [1] -1.0232190 0.9969403 # One-sided confidence interval -0.01313936 + qt(.95, 67) * 0.5060498 * c(-1, Inf) #> [1] -0.857188 Inf
