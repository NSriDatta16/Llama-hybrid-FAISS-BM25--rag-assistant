[site]: stackoverflow
[post_id]: 2767217
[parent_id]: 2759845
[tags]: 
Regarding the argument about not setting a pointer to null after deleting it so that future deletes "expose errors"... If you're really, really worried about this then a better approach, one that is guaranteed to work, is to leverage assert(): ... assert(ptr && "You're deleting this pointer twice, look for a bug?"); delete ptr; ptr = 0; ... This requires some extra typing, and one extra check during debug builds, but it is certain to give you what you want: notice when ptr is deleted 'twice'. The alternative given in the comment discussion, not setting the pointer to null so you'll get a crash, is simply not guaranteed to be successful. Worse, unlike the above, it can cause a crash (or much worse!) on a user if one of these "bugs" gets through to the shelf. Finally, this version lets you continue to run the program to see what actually happens. I realize this does not answer the question asked, but I was worried that someone reading the comments might come to the conclusion that it is considered 'good practice' to NOT set pointers to 0 if it is possible they get sent to free() or delete twice. In those few cases when it is possible it is NEVER a good practice to use Undefined Behavior as a debugging tool. Nobody that's ever had to hunt down a bug that was ultimately caused by deleting an invalid pointer would propose this. These kinds of errors take hours to hunt down and nearly alway effect the program in a totally unexpected way that is hard to impossible to track back to the original problem.
