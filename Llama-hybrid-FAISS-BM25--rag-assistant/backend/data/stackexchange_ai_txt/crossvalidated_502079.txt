[site]: crossvalidated
[post_id]: 502079
[parent_id]: 
[tags]: 
Robust standard errors under overlapping observations: confusing simulation results from alternative methods

I have a time series of $h$ -step-ahead forecasts $\hat{y}_{t+h|t}$ for $t=1,\dots,T-h$ with $h>1$ . I also have the corresponding realized values of their targets $y_{t+h}$ and the corresponding forecast errors $e_{t+h}:=\hat{y}_{t+h}-y_{t+h}$ . As $h>1$ , my time series consists of overlapping observations, and the effective sample size is below $T-h$ . I am interested in testing the hypothesis that the mean of the process that is generating the forecast errors is zero, $H_0\colon\ \mathbb{E}(e_{t+h})=0$ . The general idea I am going to follow is simply a $t$ -test. However, I am facing some challenges in the implementation. What I have tried so far are the following approaches. (I gave them names for easy reference, though better names are likely possible.) Liberal: Ignore the fact that the observations are overlapping and use a simple $t$ -test: $$ t=\frac{\bar{e}}{\widehat{\text{s.e.}}(e)} $$ with $\bar{e}=\frac{1}{T-h}\sum_{\tau=1}^{T-h}e_\tau$ and $$ \widehat{\text{s.e.}}(e)=\sqrt{\frac{\left(\sum_{\tau=1}^{T-h}(e_\tau-\bar{e})^2\right)/(T-h-1)}{T-h}}. $$ Probably not a smart choice, producing overly optimistic (too small) standard errors and thus inflated $t$ -statistics. Conservative: Use a simple $t$ -test with a crude, conservative correction accounting for the effective sample size being below $T-h$ . When constructing the $t$ -statistic as above, use $$ \widehat{\text{s.e.}}(e)=\sqrt{\frac{\left(\sum_{\tau=1}^{T-h}(e_\tau-\bar{e})^2\right)/(T-h-1)}{(T-h)/h}} $$ as $(T-h)/h$ is the lower bound of the effective sample size. (The upper bound is $T-h$ and the true effective sample size is somewhere in between.) Probably not a smart choice, producing overly pessimistic (too large) standard errors and thus shrunken $t$ -statistics. Nonoverlapping: Use a subset of observations that is not overlapping. E.g., use $\tilde{e}:=(e_1,e_{1+h},,e_{1+2h},\dots)$ where the series stops once the index hits the end of the sample. Construct the vanilla $t$ -statistic based on $\tilde{e}$ instead of $e$ . Probably a decent choice if $T$ is large and $h/T$ is small. In other cases, the failure to use all the data will be costly as the power of the test will suffer. HAC robust: Use all observations and account for the fact they are overlapping by employing autocorrelation-robust standard errors. For the latter, I choose either (a) Andrew's quadratic spectral (QS) estimator or (b) Newey-West estimator based on Bartlett kernel. In R, there are at least two implementation alternatives: function lrvar from sandwich package and function getLongRunVar from cointReg package. There is an option for prewhitening in lrvar and for demeaning in getLongRunVar . I try both with and without prewhitening for lrvar but only try with demeaning (not without it) for getLongRunVar . It seems lrvar yields an estimate of the long-run variance for the mean of the series rather than the individual elements, so the estimate is considerably smaller than in other cases. But that is not a problem; I can apply the estimate directly when constructing the $t$ -statistic, as I no longer need to divide it by the sample size when calculating $\widehat{\text{s.e.}}(e)$ . I have tried a simulation to see how the different methods perform under known conditions. Below is a typical result from a simulation of $1000$ instances with $T=100$ and $h=10,20,\dots,90$ (in rows) with $e$ being $10$ -long cumulative sums of i.i.d. Normal(0,1) random variables. The numbers are means of $\widehat{\text{s.e.}}(e)$ across the simulation runs. naive_y manual_y nonover_y And_pw_y NW_pw_y And_nonpw_y NW_nonpw_y And_nonpw_y2 NW_nonpw_y2 h=10 0.31 0.94 0.98 1.40 1.42 0.76 0.70 0.75 0.69 h=20 0.42 1.70 1.85 3.03 3.04 1.15 1.00 1.16 0.99 h=30 0.49 2.26 2.77 5.29 5.40 1.32 1.17 1.40 1.13 h=40 0.53 2.60 3.51 7.37 7.32 1.36 1.23 1.50 1.18 h=50 0.53 2.70 4.06 6.94 7.06 1.25 1.16 1.41 1.14 h=60 0.53 2.65 NaN 10.06 10.35 1.12 1.12 1.27 1.07 h=70 0.55 2.54 NaN 12.04 12.46 1.00 1.04 1.13 1.00 h=80 0.55 2.25 NaN 17.71 18.61 0.82 0.91 0.94 0.84 h=90 0.56 1.77 NaN 3.96 3.91 0.62 0.74 0.70 0.68 where naive_y is method 1 (liberal), manual_y is method 2 (conservative), nonover_y is method 3 (nonoverlapping), And_pw_y is method 4 Andrews QS with prewhitening via lrvar , NW_pw_y is method 4 Newey-West with prewhitening via lrvar , And_nonpw_y is method 4 Andrews QS without prewhitening via lrvar , NW_nonpw_y is method 4 Newey-West without prewhitening via lrvar , And_nonpw_y2 is method 4 Andrews QS without prewhitening via getLongRunVar , NW_nonpw_y2 is method 4 Newey-West without prewhitening via getLongRunVar . As you can see, the results are quite different across the methods. This is unsurprising for liberal vs. conservative, but kind of surprising for the rest. Even worse, they are quite different within method 4 depending both on specification (prewhitening or none) and even implementation. Questions: Could you offer any insight into what method is the most appropriate (either from mine or maybe yet another one)? Does the conservative method truly provide a conservative estimate? Could there be situations when it is not conservative enough? (I have not been able to come up with a counterexample so far, but perhaps you will be.) Is it normal that the different specifications and implementations within method 4 give different answers, especially when some of them even exceed the conservative limit by a large factor? At this stage I am a bit confused and still need time to digest the findings and appreciate the differences between the methods. I would not be surprised if I am doing everything wrong, especially if I made a mistake early on e.g. by focusing on a wrong variable.
