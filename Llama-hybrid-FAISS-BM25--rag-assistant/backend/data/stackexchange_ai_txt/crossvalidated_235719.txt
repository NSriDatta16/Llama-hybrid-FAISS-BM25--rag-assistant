[site]: crossvalidated
[post_id]: 235719
[parent_id]: 
[tags]: 
Symmetry in supervised learning models

I am training a bunch of supervised models on a binary classification problem. My dataset is comprised of some positive examples (p) and their symmetric negative examples (n), i.e. rows created by reverting or taking the complement value of same of the features for each of the examples in p. The resulting trained model is not exactly symmetric, i.e. the model does not give exactly symmetric probabilities for a row in p and its symmetric in n. In particular, logistic regression is quite close to a symmetric model. On the other hand, random forest, extreme gradient boosting and neural networks are far from that. Do you have any intuitive explanations for this?
