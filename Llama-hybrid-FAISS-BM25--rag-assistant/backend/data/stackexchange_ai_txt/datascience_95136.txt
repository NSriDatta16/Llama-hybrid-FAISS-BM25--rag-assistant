[site]: datascience
[post_id]: 95136
[parent_id]: 95134
[tags]: 
A standard way of obtaining a sentence representation with attention models is using BERT or any other of its derivations, like RoBERTa . In these models, the sentence tokens passed as input to the model are prefixed with a special token [CLS] . The output of the model at that first position is the sentence representation. To use these models, you may use sentence-transformers library, e.g.: from sentence_transformers import SentenceTransformer model = SentenceTransformer('paraphrase-distilroberta-base-v1') sentences = ['This framework generates embeddings for each input sentence', 'Sentences are passed as a list of string.', 'The quick brown fox jumps over the lazy dog.'] sentence_embeddings = model.encode(sentences) for sentence, embedding in zip(sentences, sentence_embeddings): print("Sentence:", sentence) print("Embedding:", embedding) print("")
