[site]: datascience
[post_id]: 41203
[parent_id]: 41170
[tags]: 
I will try to answer to your questions one by one. Unfortunately I am not familiar with R, but I will provide some python links whenever relevant - even if you won't use the code you will be able to find there more details on the suggested methods. I suggest you use a Decision Tree Regressor for the clustering. It gives you the ability to set what the target variable is and you can directly select as optimization criterion the minimization of Mean Square Error. You can select as stopping criterion a maximum number of levels according to the number of clusters/MSE that you find acceptable. The following picture summarizes the logic behind tree-based hierarchical clustering and also shows that you usually have to do some pruning (some samples might be too different to be assigned to any cluster). The picture comes from this link, where hierarchical clustering is explained; the optimisation criterion there is distance as it fits better with the type of data they are using, but of course you can use the one that fits to your own data. In python, this can been done using scikit-learn.DecisionTreeRegressor and you can even plot the tree using export_graphviz function (you can read some interesting information about the advantages and limitations of Decision Trees here ). For better understanding, the tree below shows an example of clustering results using a Decision Tree Regressor. I have highlighted the end-clusters with red. Yes, it is advisable to do some kind of data scaling before proceeding with the clustering. This way, you ensure that the clustering results are not biased by the scale of the variables (e.g. a variable that takes large values might influence certain clustering algorithm results more that one with smaller values). Keep in mind though that many clustering algorithms that come from analysis toolkits anyway scale data automatically before clustering (at least in python). There are 2 alternatives for scaling: standardisation and normalisation; you need to choose which one of those fits better to the current problem and clustering algorithm. Standardization transforms the data in order to have zero mean and unit variance. Normalisation rescales the data into the range [0,1]. You can read more about standardisation and normalisation here , while here you can find some very good examples about their use. As far as I know there is no trustworthy "rule of thumb" regarding the number of features for each problem, this is something that depends a lot on the nature of the problem and most importantly on the nature of the features. It is always important though to make sure that the input variables are uncorrelated with each other. PCA is a good way to ensure that - it actually applies some orthogonal transformation to your set of features in order to produce a set of linearly non-correlated variables based on the initial set of features - the new features are ordered based on the amount of information they contain. @Shubham summarised very well the logic behind pca, so I don't have much to add.
