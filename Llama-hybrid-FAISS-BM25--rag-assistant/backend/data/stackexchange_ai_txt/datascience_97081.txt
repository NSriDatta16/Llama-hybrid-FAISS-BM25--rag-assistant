[site]: datascience
[post_id]: 97081
[parent_id]: 97077
[tags]: 
Neural Network is nothing but an accumulation of multiple Logistic Regressors. What is the Equation of Logistic Regression? hypothesis = $W^T * x$ + b activation = Sigmoid (hypothesis) Now, what does z1 tell us it tells us that first take the dot product between the input (x) and the weight matrix then apply the non-linearity. you have 3 input examples. $ $ . and a weight matrix, [[ $w^{1}_{11} , w^{1}_{12} , w^{1}_{13} , w^{1}_{14}$ ] [ $w^{1}_{21} , w^{1}_{22} , w^{1}_{23} , w^{1}_{24}$ ] [ $w^{1}_{31} , w^{1}_{32} , w^{1}_{33} , w^{1}_{34}$ ]], Now you're multiplying the x with the transpose of W, i.e. $W^{t} * x$ Take the transpose of the Matrix W, [[ $w^{1}_{11} , w^{1}_{21} , w^{1}_{31}$ ] [ $w^{1}_{12} , w^{1}_{22} , w^{1}_{32}$ ] [ $w^{1}_{13} , w^{1}_{23} , w^{1}_{33}$ ] [ $w^{1}_{14} , w^{1}_{24} , w^{1}_{34}$ ]] So, the input dimensions are (3, 1) Now the transpose matrix dimensions are (4, 3) and $W^T*x$ dimensions are (4, 3) * (3, 1) after multiplication the dimensions would be (4, 1). Now, Z1 is computed in this way. The $W^1$ is telling the layer number means for which hidden layer this weight is initialized in your case your weight matrix is for hidden layer number 1. And the subscript $W_{11}$ or $W_{12}$ ...So on, represents the corresponding matrix multiplication indices $W_{1j}$ this first number in the subscript tells us the input number like in this case $x_1$ . and the second subscript associated with the $W_{i2}$ tells us the neuron number in the corresponding hidden layer like in this case it is 2. Now, lets compute $Z_1$ , $Z_1 = (w^{1}_{11} * x_1 + w^{1}_{21} * x_2 + w^{1}_{31} * x_2)$ + bias $ = sigmoid(Z_1)$ The rest are computed in the same and now you have the weight matrix and input vector you can compute the rest by yourself.
