[site]: datascience
[post_id]: 29866
[parent_id]: 29600
[tags]: 
I am working in the same topic right now. I am using the following algorithm: 1) extract plain content from the news, for example using dragnet . 2) tokenize each text and represent them with vectors with the bag of words technique. A simple way to perform this is using TfidfVectorizer from sklearn. 3) Clusterize them using some classification technique like k-NN (k nearest neighbors). You will find the k-NN sklearn implementation very helpfull. The key to perform the task is using the TfidfVectorizer which weigths more the tokens that only appear in a few notices, and so I can recognize similar news that talk about same topics.
