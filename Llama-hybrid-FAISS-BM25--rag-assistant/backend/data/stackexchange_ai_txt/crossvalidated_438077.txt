[site]: crossvalidated
[post_id]: 438077
[parent_id]: 
[tags]: 
Question about state-value function in RL - doesn't first sum always equal 1 by definition?

I'm going through the textbook Reinforcement Learning by Sutton and Barto ( available online ), and equation 4.4 shows "the state-value function $v_\pi$ for an arbitrary policy $\pi$ ": $v_\pi = \sum\limits_a \pi(a|s) \sum\limits_{s',r}p(s',r|s,a) [r + \gamma v_\pi(s')] $ "where $\pi(a|s)$ is the probability of taking action $a$ in state $s$ under policy $\pi$ ." My question is: isn't the first sum -- $\sum\limits_a \pi(a|s)$ -- always equal to 1? Like, the probabilities of taking some action given a particular state, across all actions available in that state, must sum to 1 (100%) right? Even "no action" would be an "action" in this context, if I understand correctly, such that the probabilities would always sum to 1. So it seems to me either I must be wrong and that doesn't always sum to 1, or if I'm right, I don't understand the purpose of adding the extra term to the equation, if it just multiplies the second sum by 1 always. Any helpful explanation would be much appreciated.
