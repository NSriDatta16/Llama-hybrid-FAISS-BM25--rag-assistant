[site]: datascience
[post_id]: 6556
[parent_id]: 977
[tags]: 
Stability-Plasticity Dilemma, Learning Rates, and Forgetting Algorithms: First, let me say that this is a really great question and is the type of thought provoking stuff that really improves one's understanding of ML algorithms. Does this "problem" have a name that it can be referred to? This is generally referred to as "stability". What's funny is that stability is actually a useful concept in regular clustering i.e. not online. The "stability" of the algorithm is often chosen as a selection criterion for whether the right number of clusters have been selected. More specifically, the online clustering stability issue that you have described is referred to as the stability-plasticity dilemma . Are there "standard" solutions to this and ... First, the big picture answer is that many online clustering algorithm are surprisingly stable when they have been well trained with a large cohort of initial data. However, its still a problem if you want to really nail down the cluster identities of points while allowing the algorithm to react to new data. The trickiness of you point is briefly addressed in Introduction to Machine Learning By Ethem Alpaydin. On page 319 he derives the online k-means algorithm through the application of stochastic gradient descent, but mentions that the stability-plasticity dilemma arises when choosing a value for the learning rate. A small learning rate results in stability, but the system looses adaptability where as a larger learning rate gains adaptability, but looses cluster stability. I believe the best path forward is to choose an implementation of online clustering which allows you to control the stochastic gradient descent algorithm and then choose the learning rate so that you maximize stability and adaptability as best as you can using a sound cross-validation procedure. Another method that I've seen employed is some sort of forgetting algorithm e.g. forgetting older points as the data stream matures. This allows for a fairly stable system on fast time scales and allows for evolution on slower time scales. Adaptive Resonance Theory was created to try to solve the stability-plasticity dilemma . You might find this article interesting. I'm not well-versed enough in R to suggest an algorithm, but I suggest you look for a mini-batch k-means algorithm that allows you to control the learning rate in its stochastic gradient descent algorithm. I hope this helps!
