[site]: datascience
[post_id]: 103364
[parent_id]: 103193
[tags]: 
The predict method produces the class prediction, which will be either 0 or 1 here. With an imbalanced dataset, it's not unusual for the class predictions to be nearly or entirely the majority class, because the cutoff probability is 50%. You can check predict_proba for the estimated probabilities of each class. xgboost is an ensemble of trees, so the output of just one of those trees doesn't tell you much. You would need to add the leaf values for every tree to find the aggregate prediction for a given sample, and even then for classification with log-loss objective that aggregate will be a log-odds measurement, not a hard class. The cutoff probability of 50% is a log-odds of 0, so you can tell the class prediction by the sign of the aggregate; so, while you haven't checked the rest of the trees, the negative value in the leaf does support the class prediction of 0.
