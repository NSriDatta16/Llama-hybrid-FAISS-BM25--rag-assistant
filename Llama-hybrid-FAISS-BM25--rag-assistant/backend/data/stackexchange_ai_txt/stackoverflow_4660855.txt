[site]: stackoverflow
[post_id]: 4660855
[parent_id]: 4636435
[tags]: 
Brian, In this case, you can either use HBase or Hive or just raw map-reduce jobs. 1. HBase is a column-oriented database. HBase best suits for a column based computations. For example, average employee salary(assuming salary is a column). And with it's powerful scalability feature, we can add nodes on the fly. 2. Hive is like traditional databases which supports SQL like queries. Internally queries will be converted into map-reduce problems. We can use this in case of row based computations. 3. Final option, where we can write our own map-reduce functionality. Using "sqoop", we can migrate data from relational databases to HDFS(Hadoop File System). Then we can write map-reduce problems that directly deal with underlying flat files. Mentioned some of the possible options. Let me know if you need additional details about above mentioned options.
