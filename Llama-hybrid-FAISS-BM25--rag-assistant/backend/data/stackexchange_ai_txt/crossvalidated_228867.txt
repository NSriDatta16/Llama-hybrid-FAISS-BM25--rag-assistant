[site]: crossvalidated
[post_id]: 228867
[parent_id]: 228856
[tags]: 
Training error is used in estimating model parameters. Think about linear regression: if our model is $Y = X\beta + \varepsilon$ we estimate $\beta$ by minimizing $\vert \vert Y - Xv\vert \vert_2^2$ over $v \in \mathbb R^p$. This is just minimizing the training loss. Why does this make sense here? It's because we are considering models (i.e. different values of $v$) that all have the same complexity, so we just pick the one with the smallest loss (or in this case largest likelihood if we're assuming normal errors). Once we're comparing models with different complexities we need to consider out-of-sample performance. Suppose in linear regression we are debating adding a quadratic term $\beta_2 x^2$ to the model $y = \beta_0 + \beta_1x$. The model $y = \beta_0 + \beta_1x$ is a special case of $y = \beta_0 + \beta_1x + \beta_2 x^2$, which has greater complexity, so it is no longer fair to compare the losses. If you're familiar with AIC and BIC, when two models have the same complexity we are just comparing likelihoods (i.e. losses), but when the dimensions of the models (i.e. complexities) differ, we then need to also take this into account. Now let's think about machine learning methods like SVM: once we've fixed our tuning parameters, we are considering a bunch of models with the same flexibility so we can just choose the one that fits the data best. It is only when we are comparing models with different complexities (i.e. when we tune the cost and kernel parameters, and maybe choose the kernel) that we need to consider out of sample performance. As far as how to "use" it, I don't really pay attention to the training error per se, but it can appear in certain calculations that I do pay attention to. For example, in linear regression the sum of the squared residuals, a value that is quite useful, is just the total training loss if we're using squared loss.
