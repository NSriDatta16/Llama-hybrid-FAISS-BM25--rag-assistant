[site]: crossvalidated
[post_id]: 132966
[parent_id]: 
[tags]: 
What is the definition of generalization error and its justification?

I was trying to understand rigorously what the goal of machine learning is. One could frame that one of the central goals of machine learning is to obtain the best possible function ever. But what does best mean? I have been taught that one way to address this is by defining the expected error or generalization error. Wikipedia as an article on it: https://en.wikipedia.org/wiki/Generalization_error however, why is this a good definition of generalization error? Is the definition that wikipedia is talking about the following formula? $$ I[f] = \int_{x,y} V(y,f(x)) d\rho(x,y) = \mathbb{E}_{x,y}[V(y,f(x))]$$ Where $V(f(x),y)$ is the cost function. i.e. const incurred if you say f(x) but the answer was y. I was wondering, why is this a good definition of generalization? I have my own thoughts and wanted to share them but I also wanted to see what other people thought. These are my thoughts: So one can interpret probabilities as the true (long run) frequencies of the data we get, over every single possible data that we might get. So the data will be generated in pairs $(x,y) \in X \times Y$ and x might be attached to different values of y's. However, if we knew the distribution, then we would get the truth about how often a specific x is attached to a specific y, for all possible pairs. Therefore, the best way to measure the quality of our function f, is to consider this long term trend of frequencies i.e. every possible y that a specific x could get. This is important because we probably want to output the y that is most often attached to a specific x. Therefore, this function $I[f]$, if we knew the distribution, captures the correct notion of "weighted-average" cost that f will have in a prediction, because it considers the true long-term frequencies of all possible datas. Is that kind of the correct philosophical justification for why we define Generalization error the way we do? Is there something in my thoughts that I missed that is important in justifying this definition of generalization error? Are there alternative definitions? Why are they good? Are they used less often than the one suggested? Is that the official definition of generalization error? Sorry if this question is very basic, but after indulging so long in machine learning, I decide to step back a second (and get back at my foundations) and really make sure that the goal of what I was doing was not lost and wikipedia seems to treat it not very well (I mean, it doesn't even have an equation for generalization error! Or even candidate equations!), that I thought it was important to get it straight, as well as I could.
