[site]: crossvalidated
[post_id]: 632672
[parent_id]: 
[tags]: 
Derive ELBO for Mixture of Gaussian

I am working through "Variational Inference: A Review for Statisticians" by Blei et al. (see https://arxiv.org/abs/1601.00670 ) and they illustrate Variational Inference using a Bayesian mixture of Gaussians. However, I am lost when it comes to computing the expectations in the evidence lower bound (ELBO) which are supposed to have a closed form solution. Here is my derivation and the introduction so far: Consider $K$ mixture components and $n$ real-valued data points $x_{1: n}$ . The latent variables are $K$ real-valued mean parameters $\mu=\mu_{1: K}$ and $n$ latent-class assignments $\mathbf{c}=c_{1: n}$ . The assignment $c_i$ indicates which latent cluster $x_i$ comes from. In detail, $c_i$ is an indicator $K$ -vector, all zeros except for a one in the position corresponding to $x_i$ 's cluster. There is a fixed hyperparameter $\sigma^2$ , the variance of the normal prior on the $\mu_k$ 's. We assume the observation variance is one and take a uniform prior over the mixture components. The full hierarchical model is $$ \begin{aligned} \mu_k & \sim \text{Normal}\left(0, \sigma^2\right), & k & =1, \ldots, K, \\ c_i & \sim \operatorname{Categorical}(1 / K, \ldots, 1 / K), & i & =1, \ldots, n, \\ x_i &\sim \text{Normal}\left(c_i^{\top} \mu, 1\right) & i & =1, \ldots, n . \end{aligned} $$ The joint density of the latent and observed variables is $$ p(\boldsymbol{\mu}, \mathbf{c}, \mathbf{x})=p(\boldsymbol{\mu}) \prod_{i=1}^n p\left(c_i\right) p\left(x_i \mid c_i, \boldsymbol{\mu}\right) $$ The mean-field variational family is $$ q(\mathbf{z})=q(\mu, \mathbf{c}) = \prod_{k=1}^K q\left(\mu_k ; m_k, s_k^2\right) \prod_{i=1}^n q\left(c_i ; \varphi_i\right) $$ where $\varphi_i$ are the categorical parameters for approximating the posterior cluster assignment of the $i$ th data point $m_k$ , $s_k^2$ are the Gaussian parameters for approximating the posterior of the $k$ th mixture component. We combine the joint density of the latent and observed variables and the mean-field family to form the ELBO for the mixture of Gaussians. $$ \begin{aligned} \text{ELBO}(q) &= \mathbb{E}_{z \sim q(z)} \left[ \log p(x, z)\right] - \mathbb{E}_{z \sim q(z)} \left[ \log q(z)\right] \\ &= \mathbb{E}_{z \sim q(z)} \left[ \log p(x \mid z)\right] + \mathbb{E}_{z \sim q(z)} \left[ \log p(z)\right] - \mathbb{E}_{z \sim q(z)} \left[ \log q(z)\right] \end{aligned} $$ Inserting $q(\mathbf{z})=q(\mu, \mathbf{c})=q(\mu) q(\mathbf{c})$ , and $p(\mathbf{z})=p(\mu, \mathbf{c})=p(\mu)p(\mathbf{c})$ yields $$ \begin{aligned} \text{ELBO}(q) &= \mathbb{E} \left[ \log p(x \mid z)\right] \\ &+ \mathbb{E} \left[ \log p(\mathbf{c})\right] + \mathbb{E} \left[ \log p(\mu)\right]\\ & - \mathbb{E} \left[ \log q(\mathbf{c})\right] - \mathbb{E}\left[ \log q(\mu)\right] \end{aligned} $$ Here the ELBO is a function of the variational parameters $\mathrm{m}, \mathrm{s}^2$ , and $\varphi$ , $$ \begin{aligned} \operatorname{ELBO}\left(\mathbf{m}, \mathbf{s}^2, \varphi \right) = & \sum_{i=1}^n \mathbb{E} \left[\log p (x_i \mid c_i, \boldsymbol{\mu}) ; \varphi_i, \mathbf{m}, \mathbf{s}^2\right] \\ & + \sum_{i=1}^n \mathbb{E}\left[\log p\left(c_i\right) ; \varphi_i\right] + \sum_{k=1}^K \mathbb{E}\left[\log p\left(\mu_k\right) ; m_k, s_k^2\right] \\ & -\sum_{i=1}^n \mathbb{E}\left[\log q\left(c_i ; \varphi_i\right)\right]-\sum_{k=1}^K \mathbb{E}\left[\log q\left(\mu_k ; m_k, s_k^2\right)\right] \end{aligned} $$ Now I would like to understand for example how to evaluate the expectation for the prior of the Mixture Components, this is what I have so far $$ \begin{aligned} \mathbb{E}\left[\log p\left(\mu_k\right) ; m_k, s_k^2\right] &= \int q(\mu_k) \log p(\mu_k) d \mu_k \\ &= \int \text{Normal}(\mu_k \mid m_k, s_k^2) \log \text{Normal}(\mu_k \mid 0, \sigma^2) d \mu_k \\ &= \int \frac{1}{\sqrt{2 \pi s_k^2}} \exp \left( - \frac{(\mu_k-m_k)^2}{2 s_k^2} \right) \left[ - \log \sqrt{2 \pi \sigma^2} - \frac{\mu_k^2}{2 \sigma^2} \right] d \mu_k \\ \end{aligned} $$ but how to deal with that integral?
