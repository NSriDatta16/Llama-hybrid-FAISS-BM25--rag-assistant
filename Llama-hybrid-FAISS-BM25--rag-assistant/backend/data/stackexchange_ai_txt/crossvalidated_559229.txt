[site]: crossvalidated
[post_id]: 559229
[parent_id]: 559222
[tags]: 
I imagine that with 1 million data points, the prior information is more or less negligible, especially if the model has no complex structure (e.g. hierarchical structure). The likelihood should -- assuming the model is simple -- overwhelm the prior. Why remove features at all? Do you have some other constraint to satisfy? Given the number of observations vs the number of features, I don't see the need to select any. Additionally, MCMC in 30 dimensions is not really I problem (I routinely fit models with 10x that number of parameters). The bottleneck would be the likelihood evaluation. If you're going to use MAP, you might as well just fit a logistic regression with an l2 or l1 penalty. The choice of l1/l2 corresponds to a particular parameterization of a model with laplacian/normal priors, so you'd be saving yourself time and trouble. I say this because inference is not your main objective so uncertainty estimation is moot.
