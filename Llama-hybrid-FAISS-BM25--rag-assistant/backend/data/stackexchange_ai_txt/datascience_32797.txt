[site]: datascience
[post_id]: 32797
[parent_id]: 
[tags]: 
Dealing with long sequence labeling

I am dealing with a problem in which I have to label the inputs (in a sequence format) to 5 distinct classes. The input would be like: X = {x_1,_x_2,...,X_500} and the output should be something like: Y = {Y_1,Y_2,...,Y_500} But the problem is that too many of the labels are from the first class so a sample output would have many first class labels and only a few (5 or 6 samples) related to other classes. The classifier tends to learn to classify everything to first class and yet get higher accuracy which is not correct. What are your suggestions? Edit: Loss function being used is cross entropy loss, the architecture itself is a bilstm which is applied after an embedding layer, to be more precise: InputLayer -> Embedding -> BidirectionalLSTM -> NN -> Softmax The input is in character format (indexes of characters) and the output for each character is of 5 distinct classes while the problem is: Most of characters belong to first class. PS: I hope this would leave enough information.
