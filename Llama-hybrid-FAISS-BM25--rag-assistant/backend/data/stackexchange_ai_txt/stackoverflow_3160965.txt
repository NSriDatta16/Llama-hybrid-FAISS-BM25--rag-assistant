[site]: stackoverflow
[post_id]: 3160965
[parent_id]: 3159976
[tags]: 
You said you're already familiar with neural networks, but since there are many different types of neural networks of differing complexity (convolutional, hebbian, kohonen maps, etc.), I'll go over a simple Feed-forward neural network again, just to make sure we're on the same page. A basic neural network consists of the following things Neurons Input Neuron(s) Hidden Neurons (optional) Output Neuron(s) Links between Neurons (sometimes called synapses in analogy to biology) An activation function The Neurons have an activation value . When you evaluate a network, the input nodes' activation is set to the actual input. The links from the input nodes lead to nodes closer to the output, usually to one or more layers of hidden nodes. At each neuron, the input activation is processed using an activation function . Different activation functions can be used, and sometimes they even vary within the neurons of a single network. The activation function processes the activation of the neuron into it's output. The early experiments usually used a simple threshold function (i.e. activation > 0.5 ? 1 : 0), nowadays a Sigmoid function is often used. The output of the activation function is then propagated over the links to the next nodes. Each link has an associated weight it applies to its input. Finally, the output of the network is extracted from the activation of the output neuron(s). I've put together a very simple (and very verbose...) example here . It's written in Ruby and computes AND, which is about as simple as it gets. A much trickier question is how to actually create a network that does something useful. The trivial network of the example was created manually, but that is infeasible with more complex problems. There are two approaches I am aware of, with the most common being backpropagation . Less used is neuroevolution , where the weights of the links are determined using a genetic algorithm.
