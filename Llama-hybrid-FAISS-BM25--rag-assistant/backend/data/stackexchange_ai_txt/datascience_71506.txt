[site]: datascience
[post_id]: 71506
[parent_id]: 61358
[tags]: 
RELU can only solve part of the gradient vanishing problem of RNN because the gradient vanishing problem is not only caused by activation function. equal to see above function, the hidden state derivative will depend on both activation and Ws, if Ws's max eigen value check the chain multiplication section, the Ws will be multiplied multiple times
