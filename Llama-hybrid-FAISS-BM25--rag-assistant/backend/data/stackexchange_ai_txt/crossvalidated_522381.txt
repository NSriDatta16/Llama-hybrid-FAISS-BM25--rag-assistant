[site]: crossvalidated
[post_id]: 522381
[parent_id]: 
[tags]: 
Information Matrix for Conditional Likelihood

I am studying the MLE theory on my own and I am confused by the difference between the fisher information matrix for the full sample and for one observation, when it comes to conditional likelihood. However, I think that I have understood the i.i.d. case. I start with a description of the i.i.d. case. IID Observations Assume that we have $T$ i.i.d. random variables with density $f(x_t;\boldsymbol{\theta})$ and the usual regularity conditions hold. The MLE maximizes the log-likelihood-function: \begin{align} \ln {\cal L}(\boldsymbol{\theta})= \ln\left(\prod_{t=1}^Tf(x_t;\boldsymbol{\theta})\right)=\sum_{t=1}^T\ln(f(x_t;\boldsymbol{\theta}))=\sum_{t=1}^T\ell_t (\boldsymbol{\theta}) \end{align} The FOC's are given by: \begin{align} \frac{\partial \ln {\cal L}(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}}=\frac{\partial}{\partial \boldsymbol{\theta}}\left[\sum_{t=1}^T\ell_t (\boldsymbol{\theta})\right]=\sum_{t=1}^T\frac{\partial \ell_t (\boldsymbol{\theta})}{\partial \boldsymbol{\theta}}=\boldsymbol{0} \end{align} In most books the following discussion seems to be based on $\frac{\partial \ell_t (\boldsymbol{\theta})}{\partial \boldsymbol{\theta}}$ , rather than $\frac{\partial \ln {\cal L}(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}}$ . I understand the derivation of the following results: The expected value of the score vector of the $t$ -th observation is $\boldsymbol{0}$ , i.e. $E\left(\frac{\partial \ell_t (\boldsymbol{\theta})}{\partial \boldsymbol{\theta}}\right)=\boldsymbol{0}$ . We can express the information matrix for the $t$ -th observation as \begin{align} \boldsymbol{{\cal I}}_t(\boldsymbol{\theta})=E\left(\frac{\partial \ell_t(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \frac{\partial \ell_t(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}'}\right)=-E\left( \frac{\partial ^2 \ell_t (\boldsymbol{\theta})}{\partial\boldsymbol{\theta}\partial\boldsymbol{\theta}'} \right) \end{align} and since we have i.i.d. observations, the result is the same for all $t$ . So I write for simplification $\boldsymbol{{\cal I}}_t(\boldsymbol{\theta})=\boldsymbol{{\cal I}}(\boldsymbol{\theta})$ . I also know that for $T\rightarrow \infty$ , the following results hold: \begin{align} \sqrt{T}(\boldsymbol{\hat{\theta}}-\boldsymbol{\theta}_0)\overset{L}{\rightarrow}N(\boldsymbol{0};\boldsymbol{{\cal I}}(\boldsymbol{\theta_0})^{-1}) \end{align} Or equivalent: \begin{align} \boldsymbol{\hat{\theta}}\overset{L}{\rightarrow}N(\boldsymbol{\theta}_0;\boldsymbol{{\cal I}}_T(\boldsymbol{\theta_0})^{-1}) \end{align} Where $\boldsymbol{\theta}_0$ is the true parameter vector, $\boldsymbol{\hat{\theta}}$ is the MLE and $\boldsymbol{{\cal I}}_T(\boldsymbol{\theta_0})$ is the information matrix of the full sample evaluated at $\boldsymbol{\theta}_0$ . Since the random variables are i.i.d., it holds that $\boldsymbol{{\cal I}}_T(\boldsymbol{\theta})=T\cdot\boldsymbol{{\cal I}}(\boldsymbol{\theta})$ . Conditional Case Now I want to extend these results to the case where we have to work with the conditional likelihood (for example, time series models). Assume that we observe the variables $\left\{x_t\right\}_{t=1}^T$ which are dependent. Suppose that the conditional density of the $t$ -th observation, conditioned on $\Omega_{t-1}=\left\{x_{t-1},\dots,x_1\right\}$ , is given by $f(x_t\vert \Omega_{t-1};\boldsymbol{\theta})$ . The conditional log-likelihood function is given by: \begin{align} \ln {\cal L}(\boldsymbol{\theta})= \ln\left(\prod_{t=1}^Tf(x_t \vert \Omega_{t-1};\boldsymbol{\theta})\right)=\sum_{t=1}^T\ln(f(x_t \vert \Omega_{t-1};\boldsymbol{\theta}))=\sum_{t=1}^T\ell_t (\boldsymbol{\theta}) \end{align} I tried to derive the same statements as above and I am not sure whether they are true or not: The expected value of the score vector of the $t$ -th observation conditioned on $\Omega_{t-1}$ is $\boldsymbol{0}$ , i.e. $E\left(\frac{\partial \ell_t (\boldsymbol{\theta})}{\partial \boldsymbol{\theta}}\vert \Omega_{t-1}\right)=\boldsymbol{0}$ . We can express the information matrix for the $t$ -th observation as \begin{align} \boldsymbol{{\cal I}}_t(\boldsymbol{\theta})=E\left(\frac{\partial \ell_t(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \frac{\partial \ell_t(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}'} \vert \Omega_{t-1}\right)=-E\left( \frac{\partial ^2 \ell_t (\boldsymbol{\theta})}{\partial\boldsymbol{\theta}\partial\boldsymbol{\theta}'} \vert \Omega_{t-1} \right) \end{align} However, since we are conditioning on $\Omega_{t-1}$ , I think that $\boldsymbol{{\cal I}}_i(\boldsymbol{\theta}) \neq \boldsymbol{{\cal I}}_j(\boldsymbol{\theta})$ for $i \neq j$ . Now my final question: Are these two statements correct ? And how do I calculate the equivalent of $\boldsymbol{{\cal I}}(\boldsymbol{\theta})$ and especially $\boldsymbol{{\cal I}}_T(\boldsymbol{\theta})$ , so that I can derive the asymptotic distributions ?
