[site]: crossvalidated
[post_id]: 436765
[parent_id]: 436737
[tags]: 
It's quite hard to say without knowing more details. It's likely that by updating with such a large learning rate, you've sent the weights off to some region where they are not amenable to further optimization. If you're familiar with how important correct weight initialization is for training neural networks (indeed, with very careful initialization, it's possible to train 10000 layer networks !) -- then it shouldn't come as a surprise that by violently disturbing the weights, you may have messed up the initialization so much that it's not able to recover.
