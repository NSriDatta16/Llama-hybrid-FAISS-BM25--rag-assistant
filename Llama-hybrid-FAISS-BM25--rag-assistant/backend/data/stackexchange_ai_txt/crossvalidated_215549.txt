[site]: crossvalidated
[post_id]: 215549
[parent_id]: 
[tags]: 
look up table as a special case of a the linear function approximation (Reinforcement learning)

In reinforcement learning, where the state space is discrete and relatively small, a form of learning algorithm commonly used is the Q learning. This involves a look up table $Q(s,a)$ (Or you think of this as a matrix with the states on the row, and actions on the column). The entries are updated as the agent continues to learn. In the case of continuous state space, or a overly large state space, a look up table is not feasible anymore. And we turn to Q values with linear function approximation. In this case, Q values are now approximated by a linear combination of features. I was watching a lecture by David Silver on Reinforcement Learning, he mentioned that the look up table is just a special case of the linear function approximation. (The slide I am referring to begins at 28:43.) This never occurred to me, and he showed a little 'proof' that was not so clear to me. Anyone who could give some insights into the matter? Originally, I just accepted (without proof) that look up table and linear function approximation are just two independent things. It never occurred to me that the two are related.
