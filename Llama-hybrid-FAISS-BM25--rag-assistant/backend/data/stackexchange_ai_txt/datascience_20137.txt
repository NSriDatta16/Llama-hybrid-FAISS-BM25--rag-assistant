[site]: datascience
[post_id]: 20137
[parent_id]: 20132
[tags]: 
In general, it’s better NOT to binarize them if the decision tree algorithm that you are using supports categorical features, as variables with fewer levels are less likely to be used in splits. Note however that not all implementations in all software are able to use categorical features without one-hot encoding – for example, most R implementations of decision trees and tree ensembles support categorical features natively, whereas scikit-learn’s and spark’s do not. Here’s a blog post with a comparison of categorical as-is vs. one-hot encoded for random forests: https://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/
