[site]: crossvalidated
[post_id]: 161137
[parent_id]: 161109
[tags]: 
In regular Q learning, Q is a table that has a row for every state and a column for every action. It seems that this approach adds another dimension to this table for the actions of every additional agent. You are right, this is not very practical. $\pi$ is a probability distribution over the available actions. Multiplying the probabilities of actions for all your friends gives you the probability of that collective choice of actions â€” this will add up to 1 when summed over all combinations of actions. In effect the Nash probability calculates a weighted average of actions in the current state, where each action value is weighted by its probability under a choice of policy. The policy is chosen in such a way as to maximize this weighted average. The paper also minimizes over the actions of foes. So, you choose foe actions that leave your friends with the smallest maximum Q, and then choose the friend actions that give you that Q value. Maybe I misunderstand, but it seems like there would be a unique Q value that satisfies this condition, and the sum has only a single non-zero term.
