[site]: datascience
[post_id]: 84438
[parent_id]: 84395
[tags]: 
In the Video I believe, in the video when it said that you have 5 models trained on 5 different datasets , it is a bit incorrect. You have one model trained on 5 datasets. Hence you have 5 trained models. Then it suggested to pick a model based on voting etc. This is how Ensemble models work but Cross-validation is not for the process of Ensembling the Models Why K-Fold CV Key goal of K-Fold CV is to provide a reliable estimate of test error with the available train data . In a simple split approach, we might just be lucky that the validation set contains more easy examples leading to an over-optimistic evaluation of the model . Or we might be unlucky when the validation set contains more difficult examples and the performance of the model is underestimated. It does not rely on only one estimate of the model error, but rather on a number(K) of estimates. Most important point to keep in mind is that you are still working on your train dataset. With this approach, you are better assured that the score of training is the best(reliability) you can have before checking it on testing data. Hence, you can have more trust in the Model configuration(Hyperparameter) Since, this is still the training data, you should train the Model with the identified hyperparameters on the whole dataset. However, if we have multiple ML algorithms say random forest, neural network, SVM inside the CV loop then we select the algorithm with the highest accuracy I don't think we can have multiple models inside one K-Fold. If we mean repeating the k-fold on multiple models in a simple loop. Then we might pick the model with the highest score if "score" is the only criterion of evaluation.
