[site]: datascience
[post_id]: 107676
[parent_id]: 107630
[tags]: 
Suppose you train your model on pictures of dogs and ducks. You then test the model on a test set of pictures of dogs and ducks and it classifies them all perfectly correctly, 100%. That cannot exclude the possibility that there exists a picture of a dog not in your test set that your model will say is a duck (we'll neglect the possibility that any images in your train/test set are wrongly labelled). Your confidence in machine learning is given by how well your model performs on the test set. If it does manage to classify all 100 of your dog/duck test pictures, you only know that the performance is >99.5%. You can't say it is 100% when tested on the population of possible pictures of dogs and ducks, past, present and yet-to-be-photographed. Unless you have a known finite set of subjects that you've already confirmed you can classify correctly 100% of the time, then I don't see how any system can know if it is working "perfectly" when given new data. The only way to really get perfection in this sense is to define a dog as anything your model says is a dog, and a duck is anything your model says is a duck.
