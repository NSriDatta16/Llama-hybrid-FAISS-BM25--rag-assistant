[site]: crossvalidated
[post_id]: 83488
[parent_id]: 83216
[tags]: 
It depends on your model & the specificity of your data. For instance, fitting an unpruned decision tree will always lead to overfitting with even just a few variables. The same goes with parametric models, where a large number of parameters can lead to overfitting, even if there is a lot of data. Eitherway, you sould systematically try to tune the complexity of your model to assure the best generalisation error. I don't think it's as much a problem of "large" weights than "unconstrained" weights. Adding a regularisation term to regression basically forces your coefficients to a region near zero (or some other predefined prior value(s) ). The bayesian interpretation of regularisation makes it even more obvious : the regularisation parameter (for ridge regression, say) governs the standard deviation of the prior given to the coefficients. High regularisation means a higher "chance" for small coefficients, and constrains their value, thus reducing the freedom of your model and thus overfitting. Depends on your model. If you've got huge amounts of data and 2 variables and are doing linear regression, then probably not. If you're fitting polynomials to 100 data points, then yeah. If you're unsure how complex your model is w.r.t. your training data, then just try with small amounts of regularisation and see if generalisation error improves (using a validation set or X-validation).
