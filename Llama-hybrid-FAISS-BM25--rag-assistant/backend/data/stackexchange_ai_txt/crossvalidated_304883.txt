[site]: crossvalidated
[post_id]: 304883
[parent_id]: 
[tags]: 
Is it incorrect to interpolate between known probabilities for binned data?

I have 7 bins of binary data according to a variable x as follows: x1: N=1000, success = 324, failure=676 x2: N=1000, success = 444, failure= 556 .... .... x7: N=...., success = ..., failure = ... I only have 7 x values (7 bins of data) and I want to fit a probability distribution to this data based on the fractions of successes (approximated as the probability of success) for each value of x. For example say if I know the probabilities of success for x1 = 1.0, x2 = 2.0, and x3 = 3.0 but based on this I want to estimate the probability of x = 1.5 or x=2.5 having a successful outcome. I thought logistic regression since the outcome of each experiment is a binary varibale... but since the x values are not evenly spread (0.8,1.0,1.5,2.0,3.0,5.0,8.0) the logistic regression model is as good as random according to the sklearn AUC and the Brier score is higher than it should be for a well calibrated model. It is evident from the actual experiment I am investigating that the probabilities of success between, say x1 and x2 should increase in a roughly linear fashion as a function of x. That is, larger x values give higher probability of success. Is it extremely crude to just interpolate the probabilities between the points (interpolate between p(x1) and p(x2) and so on)? Or is it wise to do a linear regression with probabilities? Or else how should one fit a probability distribution in an experiment like this?
