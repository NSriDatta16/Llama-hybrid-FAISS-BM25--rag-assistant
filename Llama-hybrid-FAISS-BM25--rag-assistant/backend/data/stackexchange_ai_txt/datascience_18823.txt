[site]: datascience
[post_id]: 18823
[parent_id]: 16342
[tags]: 
scale_pos_weight is used for binary classification as you stated. It is a more generalized solution to handle imbalanced classes. A good approach when assigning a value to scale_pos_weight is: sum(negative instances) / sum(positive instances) For your specific case, there is another option in order to weight individual data points and take their weights into account while working with the booster, and let the optimization happen regarding their weights so that each point is represented equally. You just need to simply use: xgboost.DMatrix(..., weight = *weight array for individual weights*) You can define the weights as you like and by doing so, you can even handle imbalances within classes as well as imbalances across different classes.
