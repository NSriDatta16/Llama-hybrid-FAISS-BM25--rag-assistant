[site]: crossvalidated
[post_id]: 396389
[parent_id]: 396366
[tags]: 
GLMs are fit via maximum likelihood so if you want to view it as a minimization, you'd have the negative (log) likelihood as your loss. For some likelihoods (like a Gaussian linear model) this is equivalent to minimizing an $L_p$ norm but it doesn't have to be. A standard example is logistic regression where the log likelihood for $n$ observations is $$ \ell(\beta\mid y, x) = \sum_{i=1}^n y_i \log g^{-1}(x_i^T\beta) + (1-y_i)\log(1 - g^{-1}(x_i^T\beta)) $$ for $g = \text{logit}$ being the link function.
