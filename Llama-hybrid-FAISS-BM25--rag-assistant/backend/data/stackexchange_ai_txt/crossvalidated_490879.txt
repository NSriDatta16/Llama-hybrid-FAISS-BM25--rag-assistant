[site]: crossvalidated
[post_id]: 490879
[parent_id]: 490781
[tags]: 
Convergence of the posterior due to convergence of the likelihood One way to look at 'convergence' is in a frequentist way, for increasing sample size the posterior will, with increasing probability, be high for the true parameter and low for the false parameter. For this we can use the Bayes factor $$\frac{P(\theta_1\vert x)}{P(\theta_0\vert x)} = \frac{P(x \vert \theta_1)}{P(x \vert \theta_0)} \frac{P(\theta_1)}{P(\theta_0)} $$ where $\theta_0$ is the true parameter value and $\theta_1$ is any other alternative value. (maybe it is a bit strange to speak about the true parameter in a Bayesian context, but maybe the same is true for speaking about converging of the posterior, which is maybe more like a frequentist property of the posterior) Assume that the likelihood ratio ${P(x \vert \theta_1)}/{P(x \vert \theta_0)}$ will converge to 0 in probability for all values $\theta_1$ that do not have a likelihood function that is the same as the likelihood function for the true parameter value $\theta_0$ . (we will show that later) So if ${P(x \vert \theta_1)}/{P(x \vert \theta_0)}$ converges, and if $P(\theta_0)$ is nonzero, then you will have that ${P(\theta_1\vert x)}/{P(\theta_0\vert x)}$ converges. And this implies that $P(x \vert \theta)$ converges to / concentrates in the point $\theta_0$ . What are the necessary conditions for a model's posterior to converge to a point mass in the limit of infinite observations? So you need two conditions: The likelihood function of two different parameters must be different. $P(\theta)$ is non-zero for the correct $\theta$ . (you can argue similarly for densities $f(\theta)$ as prior) Intuitive: If your prior gives zero density/probability to the true $\theta$ then the posterior will never give a non-zero density/probability to the true $\theta$ , no matter how large sample you take. Convergence of the likelihood ratio to zero The likelihood ratio of a sample of size $n$ converges to zero (when $\theta_1$ is not the true parameter). $$ \frac{P(x_1, x_2, \dots , x_n \vert \theta_1)}{P(x_1, x_2, \dots , x_n \vert \theta_0)} \quad \xrightarrow{P} \quad 0$$ or for the negative log-likelihood ratio $$-\Lambda_{\theta_1,n} = - \log \left( \frac{P(x_1, x_2, \dots , x_n \vert \theta_1)}{P(x_1, x_2, \dots , x_n \vert \theta_0)} \right) \quad \xrightarrow{P} \quad \infty$$ We can show this by using the law of large numbers (and we need to assume that the measurements are independent). If we assume that the measurements are independent then we can view the log-likelihood for a sample of size $n$ as the sum of the values of the log-likelihood for single measurements $$\Lambda_{\theta_1,n} = \log \left( \frac{P(x_1, x_2, \dots , x_n \vert \theta_1)}{P(x_1, x_2, \dots , x_n \vert \theta_0)} \right) = \log \left( \prod_{i=1}^n \frac{P(x_i \vert \theta_1)}{P(x_i \vert \theta_0)} \right) = \sum_{i=1}^n \log \left( \frac{P(x_i \vert \theta_1)}{P(x_i \vert \theta_0)} \right)$$ Note that the expectation value of the negative log-likelihood $$E\left[- \log \left( \frac{P_{x \vert \theta_1}(x \vert \theta_1)}{P_{x \vert \theta_0}(x \vert \theta_0)} \right)\right] = -\sum_{ x \in \chi} P_{x \vert \theta_0}(x \vert \theta_0) \log \left( \frac{P_{x \vert \theta_1}(x \vert \theta_1)}{P_{x \vert \theta_0}(x \vert \theta_0)} \right) \geq 0$$ resembles the Kullback-Leibler divergence , which is positive as can be shown by Gibbs' inequality , and equality to zero occurs iff $P(x \vert \theta_1) = P(x \vert \theta_0)$ : So if this expectation is positive then by the law of large numbers, $-{\Lambda_{\theta_1,n}}/{n}$ convergences to some positive constant $c$ $$\lim_{n \to \infty} P\left( \left| -\frac{\Lambda_{\theta_1,n}}{n}-c \right| > \epsilon \right) = 0$$ which implies that $-{\Lambda_{\theta_1,n}}$ will converge to infinity. For any $K>0$ $$\lim_{n \to \infty} P\left( {-\Lambda_{\theta_1,n}}
