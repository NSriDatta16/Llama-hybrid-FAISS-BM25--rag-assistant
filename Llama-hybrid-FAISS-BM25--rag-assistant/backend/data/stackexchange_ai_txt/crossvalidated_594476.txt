[site]: crossvalidated
[post_id]: 594476
[parent_id]: 
[tags]: 
VIF after LASSO?

In have been working on a project along with other colleagues in which the dataset has almost 100 variables. The earlier approach was to use LASSO for subset selection and then use VIF to remove the "remaining multicollinear variables". A threshold of 5 (for VIF) was being used for the same. It seems like their endeavor is to eradicate multicollinearity at least to the best of their ability. But is that approach correct? After using LASSO some odd 11 features were selected. Also, a penalty of 0.01 was used by 'manual' hyperparameter tuning (loosely speaking) - i.e., accuracy for 0.1, 0.01, 0.001,... were taken into consideration. As a result, there were no 'obvious' correlated features. I personally don't think using VIF after LASSO makes any sense as LASSO itself removes multicollinearity very substantially. The task is propensity modeling and both interpretability and the performance of the model is important for us. We have been using Logistic Regression. Is there any other way through which we could address the two-pronged problem of feature selection and removing multicollinearity?
