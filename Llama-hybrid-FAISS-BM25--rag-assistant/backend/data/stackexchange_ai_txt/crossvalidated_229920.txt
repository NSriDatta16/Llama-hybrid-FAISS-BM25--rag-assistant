[site]: crossvalidated
[post_id]: 229920
[parent_id]: 229756
[tags]: 
The usual assumption of constant variance are not usually reasonable for percentage data (they might be okay in some situations). [Linearity might also matter if you're trying to infer anything but the means at those two particular times, or the change in mean there.] In any case, if you're prepared to assume the population standard deviations are about the same at the two time periods then one easy way to get a standard error of the slope would be to fit a least squares regression line to the entire six values (i.e. don't average them first). [i.e. Set up a y column by stacking the six y-values, and an x-column by stacking the times (three 1 values on top of three 2 values).] There are a variety of equivalent available formulas for the standard error of the slope, but the following approach will probably be the easiest for you to implement. In simple linear regression, $$\widehat{\text{Var}}(\hat{\beta})=\frac{(1-R^2)}{n-2}\frac{\text{Var}(y)}{\text{Var}(x)}$$ The standard error of the slope is the square root of that. [In Excel, you can get the slope using the SLOPE function, and the $R^2$ value using the RSQ function between the y and x values. The ordinary VAR functions are applied to the y and x columns and then COUNT to (say) the y-values (here assuming there are no missing values in the x-column) to get the $n$.] In the simple case you have here the slope and its standard error can be calculated a bit more simply, but the approach here is more general.
