[site]: crossvalidated
[post_id]: 302619
[parent_id]: 
[tags]: 
Gradient with respect to outputs for recurrent neural network

In Goodfellow et al.'s Deep Learning , the authors derive the gradients with respect to the parameters of a recurrent neural network with discrete outputs and a hyperbolic tangent activation function. The network is diagrammed in the figure below: As a first step to doing backpropagation, they calculate the derivative of the loss function $L$ with respect to the output $\mathbf{o}^{(t)}$. On page 374, they write: The gradient $(\nabla_{\mathbf{o}^{(t)}}L)_i$ on the outputs at time step $t$, for all $i$, $t$, is as follows: $$(\nabla_{\mathbf{o}^{(t)}}L)_i = \frac{\partial L}{\partial o_i^{(t)}} = \frac{\partial L}{\partial L^{(t)}}\frac{\partial L^{(t)}}{\partial o_i^{(t)}} = \hat{y}_i^{(t)} - \mathbf{1}_{i, y^{(t)}}. \tag{10.18}$$ Here, $L$ is the total loss for a given sequence of $\mathbf{x}$ values paired with a sequence of $\mathbf{y}$ values, which is a sum of the losses over all the time steps: $$\begin{align} &\phantom{=} L(\{\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(\tau)}\}, \{\mathbf{y}^{(1)}, \ldots, \mathbf{y}^{(\tau)}\}) \tag{10.12}\\ &=\sum_t L^{(t)}\tag{10.13}\\ &=-\sum_t \log p_{\mbox{model}}(y^{(t)} | \{\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(t)}\}) \tag{10.14} \end{align}$$ Moreover, $\hat{\mathbf{y}}^{(t)} = \mbox{softmax}(\mathbf{o}^{(t)})$, and according to the appendix on notation, $\mathbf{1}_\mbox{condition}$ is $1$ if the condition is true, $0$ otherwise. I see that $\frac{\partial L}{\partial L^{(t)}} = 1$ because the derivative is zero for all the terms in the sum in equation (10.13) except the one that we're interested in, in which case it is $1$. Questions: How did they get $\frac{\partial L^{(t)}}{\partial o_i^{(t)}}$? How do you express $\log p_{\mbox{model}}(y^{(t)} | \{\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(t)}\})$ mathematically as a function of $\mathbf{o}^{(t)}$ so you can calculate its derivative? How do you interpret the notation $\mathbf{1}_{i, y^{(t)}}$ since $i, y^{(t)}$ is not really a condition?
