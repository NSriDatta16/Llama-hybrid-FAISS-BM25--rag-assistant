[site]: datascience
[post_id]: 108752
[parent_id]: 108740
[tags]: 
Because these are the pieces of information needed to accomplish the loss' tasks, that is, both masked language modeling (i.e. predicting the masked tokens) and next sentence prediction (i.e. predict whether the second segment followed the first segment in the original text). These are the specific reasons: Token embeddings are needed to identify the word/subword being processed as input, as well as the token being masked. Positional embeddings are needed because without them, the Transformer cannot distinguish the same token in different positions (unlike recurrent networks like LSTMs). For more details, you can refer to this answer . Sentence embeddings are needed for the secondary task of the loss: next sentence prediction. They are needed to easily tell apart the different parts of the input. For more details, you can refer to this answer . Also, note that a normal Transformer architecture already adds token embeddings and positional embeddings. The reason why these embeddings are added up instead of e.g. concatenated can be found in this answer . Adding them up you basically are learning the optimal way of combining them, instead of fixing it a priori.
