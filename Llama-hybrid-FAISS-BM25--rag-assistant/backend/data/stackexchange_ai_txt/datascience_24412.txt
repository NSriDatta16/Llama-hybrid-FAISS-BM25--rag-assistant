[site]: datascience
[post_id]: 24412
[parent_id]: 
[tags]: 
Autoencoders versus Word2Vec?

I'm wondering if there has been some work done about using autoencoder versus using word2vec to produce word embeddings. Autoencoder could learn to map contexts words with themselves while word2vec usually maps one word with its context or a context with its word. Did you see some work comparing these two approaches ?
