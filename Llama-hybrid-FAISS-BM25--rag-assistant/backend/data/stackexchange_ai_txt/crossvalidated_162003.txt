[site]: crossvalidated
[post_id]: 162003
[parent_id]: 160381
[tags]: 
Your question seems to have a mix of concepts, so I'm a little unclear on what specifically your question is. I hope my answer can get close to the mark. Jim is correct that if your approach is to: come up with a probability model for the response data, $y$, given some parameters $\theta$ and the predictors $x$ ( $Pr(y|\theta, x)$ aka the likelihood) and find the maximum of the likelihood function for $\theta$ given the data, then this technique is still called maximum likelihood, no matter what your probability model is. Alternatively, you could: come up with a probability model for the data given some parameters $\theta$, $Pr(y|\theta, x)$, and come up with a reasonable prior probability for $\theta$, $Pr(\theta)$, and solve for the probability of the parameters given the data and your prior, $Pr(\theta|y, x)$ (aka the posterior ), perhaps using MCMC methods then the technique you would be using would be called Bayesian inference, no matter what you probability model is. If you are asking specifically about what to call the methods wherein you fit a regression model that includes heteroskedascity that is perhaps related to your $x$ values, well, there are different names for this depending on your underlying probability model and your domain specific jargon. For example, one probability model that has built in heteroskedascity is the Poisson model. The Poisson distribution has only one parameter, $\lambda$ that describes both the mean and the variance. Fitting a regression model where $y$ can be described by a Poisson distribution (integer counts) is called Poisson regression, and the heteroskedascity comes "baked in". Poisson regression is in the class of models described as generalized linear models . You could also have a probability model that is more familiar to you if you're coming from the world of least squares, where $x$ describes the mean of $y$ with Normally distributed errors, but you relax the homoskedastic assumption on using generalized least squares , where once again we are in the class of models described as generalized linear models. Basically, if you can write down the probability model for your data, there's a way to fit it (or attempt to fit it). Changing assumptions about the error structure may put you in some other class of models, and so you might have to figure out how to describe it in terms that are familiar to your audience. But in general if you are maximizing the likelihood for that probability model, it's still called maximum likelihood, and if you are coming up with prior and solving for the posterior it's called Bayesian inference. People tend to align more strongly with one camp or another, so, depending on your audience, you may want to avoid mixing terms such as maximizing the likelihood and posterior distribution. Just a heads up, conditional likelihood is something different from joint or marginal likelihood as well, but that is only a term you have used and it wasn't clear that it was pertinent to your question, so I haven't addressed it.
