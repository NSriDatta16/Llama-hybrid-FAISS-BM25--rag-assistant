[site]: crossvalidated
[post_id]: 549629
[parent_id]: 549609
[tags]: 
In principle, applying the strategy you outline is possible and may sometimes also lead to useful insights. However, the main drawback is that you don't exploit all information you have about the data, in particular you ignore the censoring information when learning the tree. Hence, this will usually lead to suboptimal partitions/clusterings of the data. Instead you should at least incorporate the censoring information and employ a splitting criterion that leverages this. One option to do so is to use the ctree() function from the partykit package which supports survival trees with basic Kaplan-Meier fits in each of the resulting partitions of the tree. See also: Hothorn, Hornik, Zeileis (2006). "Unbiased Recursive Partitioning: A Conditional Inference Framework." Journal of Computational and Graphical Statistics , 15 (3), 651-674. doi:10.1198/106186006X133933 . Replication material is also available in vignette("ctree", package = "partykit") . Moreover, it would be possible to fit model-based survival trees (e.g., fitting a Weibull or Cox proportional hazards model) in each of the partitions and employing a suitable splitting criterion. See vignette("mob", package = "partykit") for a worked example. Finally, you could even fit a survival treatment model ( Surv(time, status) ~ trt ) and partition the data based on that using the model4you package. The basic survival tree in partykit can be constructed as follows: ## packages and data library("partykit") library("survival") ## all categorical variables should be factors veteran If you want to create a factor with the predicted node IDs, e.g., for further subsequent grouped analyses, you can easily do so: ## factor coding groups based on node IDs veteran$node
