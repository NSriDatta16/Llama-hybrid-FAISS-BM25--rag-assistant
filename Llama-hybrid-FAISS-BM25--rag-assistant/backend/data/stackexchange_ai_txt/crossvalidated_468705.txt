[site]: crossvalidated
[post_id]: 468705
[parent_id]: 467701
[tags]: 
One way to understand the solution to the problem - the answers by carlo, whuber and comments already say a lot of this - is to re-express the logit expression as $\exp(\beta_1 (\gamma+X))\over 1+\exp(\beta_1(\gamma+X))$ , where $\gamma={\beta_0\over \beta_1}$ . Doing so, you can maximize the likelihood $$ \max_{\beta_1,\gamma} E\left [\mathbf{1}(X>c)\beta_1(\gamma+X)-\log[1+\exp(\beta_1(\gamma+X))] \right ] $$ Taking first order conditions with respect to $\gamma$ , you get: $$ \beta_1 E\left[\mathbf{1}(X>c)-{\exp(\beta_1(\gamma+X))\over 1+\exp(\beta_1(\gamma+X))} \right ] = 0 $$ That is, conditional on the value for $\beta_1$ , you'll set $\gamma$ so that the prediction errors of the logit function equal zero on average. For particular distributions of $X$ and values for $c$ , the exact minimum will be $\gamma=c$ . For other cases, this error minimization might choose different values for $\gamma$ as a way of minimizing the error for most observations. Now, note that if $\beta_1\rightarrow \infty$ , $$ {\exp(\beta_1(\gamma+X))\over 1+\exp(\beta_1(\gamma+X))} \rightarrow \begin{cases} 1\ &if\ \gamma+X>0\\ 1/2\ &if\ \gamma+X=0\\ 0\ &if\ \gamma+X Then, if $\beta_1$ is picked to be high enough, the logit function will look very close to an indicator function stating that $X>-\gamma$ . In such a case, the way to solve the first order condition for $\gamma$ when $\beta_1$ gets very high will be to set $\gamma\rightarrow -c$ . All I have leftover here is how the likelihood function solves for $\beta_1$ . For this, the first order condition with respect to $\beta_1$ will be: $$ E\left\{(\gamma+X)\left [\mathbf{1}(X>c)-{\exp(\beta_1(\gamma+X))\over 1+\exp(\beta_1(\gamma+X))} \right ] \right \} = 0 $$ Given that the term in square brackets has mean zero (from the first order condition with respect to $\gamma$ ), this FOC states that the "prediction error" from the logit function is uncorrelated with $\gamma+X$ . Once again, if we let $\beta_1$ diverge to $\infty$ , we can set the term in brackets to be arbitrarily close to zero, which will lead this expectation to be zero. If you add white noise $W|X\sim F_W(W)$ that is independent of $X$ , the first order conditions become $$ \beta_1 E_X\left[1-F_W(c-X)-{\exp(\beta_1(\gamma+X))\over 1+\exp(\beta_1(\gamma+X))} \right ] = 0 \\ E_X\left\{(\gamma+X)\left [1-F_W(c-X)-{\exp(\beta_1(\gamma+X))\over 1+\exp(\beta_1(\gamma+X))} \right ] \right \} = 0 $$ Once again, the details of the approximation will depend on the distribution of $X$ , the distribution of $W$ and the value of $c$ . For $W\sim N(0,\sigma^2)$ , the logit function can be very similar to $1-F_W(c-X)$ for the right values of $\beta_1,\gamma$ . For other thicker tailed functions $F_W$ , or bi-modal functions $F_W$ , results might become more sensitive to the values of $c$ , distribution of $X$ and distribution of $W$ .
