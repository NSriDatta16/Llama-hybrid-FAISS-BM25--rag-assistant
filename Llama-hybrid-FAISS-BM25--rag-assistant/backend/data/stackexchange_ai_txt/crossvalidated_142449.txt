[site]: crossvalidated
[post_id]: 142449
[parent_id]: 142250
[tags]: 
Two things. Let's start with the fundamental one, which is the reason for your question. The definition of the posterior, which is a consequence of Bayes' Law, is: $$ P(\theta | x) = \frac{P(x|\theta)P(\theta)}{P(x)}. $$ When our goal is to find the $\theta$ for which $P(\theta|x)$ is maximized, the denominator is irrelevant because it is positive and does not depend on $\theta$. But if we want to explicit calculate the posterior distribution, for instance in the context of Bayesian inference, we can not leave it out. A second (much smaller) way in which your answer differs from the given answer is that your answer was not piecewise. When we multiply piecewise functions it is important to pay attention to the branches. When you go to compute the marginal distribution $P(x)$ this will be very important as well.
