[site]: datascience
[post_id]: 121977
[parent_id]: 
[tags]: 
object detection neural network's bounding box error converging instantly during training

I tried to make an SSD neural network for object detection. For now, i'm using 600 (700x700) training examples, i'm planning of using 1000 (I only have one class) or more if needed. However, there seems to be something really wrong with the mean absolute error of the coordinates of the predicted bounding boxes that I'm using for the loss : Here is the code for training clsLoss = nn.CrossEntropyLoss(reduction='none') bboxLoss = torch.nn.L1Loss(reduction='none') batch_size = 32 train_iter, _ = datasetting.loadData(batch_size) device, net = try_gpu(), model.TinySSD(num_classes=1) trainer = torch.optim.SGD(net.parameters(), lr=3e-4, weight_decay=5e-4) num_epochs, timer = 11, Timer() animator = Animator(xlabel='epoch', xlim=[1, num_epochs], legend=['class error', 'bbox mae']) net = net.to(device) for epoch in range(num_epochs): metric = Accumulator(4) net.train() for features, target in train_iter: timer.start() trainer.zero_grad() X, Y = features.to(device), target.to(device) anchors, cls_preds, bbox_preds = net(X) bbox_labels, bbox_masks, cls_labels = anchor.multiboxTarget(anchors, Y) l = calcLoss(cls_preds, cls_labels, bbox_preds, bbox_labels, bbox_masks) l.mean().backward() trainer.step() metric.add(clsEval(cls_preds, cls_labels), cls_labels.numel(), bboxEval(bbox_preds, bbox_labels, bbox_masks), bbox_labels.numel()) cls_err, bbox_mae = 1 - metric[0] / metric[1], metric[2] / metric[3] animator.add(epoch + 1, (cls_err, bbox_mae)) print(f'class err {cls_err:.2e}, bbox mae {bbox_mae:.2e}') print(f'{len(train_iter.dataset) / timer.stop():.1f} examples/sec on ' f'{str(device)}') And the structure of my neural network (note that I've added a convolutional layer but it doesn't solve my issue sadly) def downSampleBlk(in_channels, out_channels): blk = [] for _ in range(2): blk.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)) blk.append(nn.BatchNorm2d(out_channels)) blk.append(nn.ReLU()) in_channels = out_channels blk.append(nn.MaxPool2d(2)) return nn.Sequential(*blk) def base_net(): blk = [] num_filters = [3, 16, 32, 64] for i in range(len(num_filters) - 1): blk.append(downSampleBlk(num_filters[i], num_filters[i+1])) return nn.Sequential(*blk) def getBlk(i): if i == 0: blk = base_net() elif i == 1: blk = downSampleBlk(64, 128) elif i == 5: blk = nn.AdaptiveMaxPool2d((1,1)) else: blk = downSampleBlk(128, 128) return blk I've tried to play with the learning rate, the epoch, the batch size and I even added a convolutional layer to experiment but nothing has worked so far. It of course outputs completely wrong predictions no matter the image I test it on. Does anyone have any idea of what could be the culprit ?
