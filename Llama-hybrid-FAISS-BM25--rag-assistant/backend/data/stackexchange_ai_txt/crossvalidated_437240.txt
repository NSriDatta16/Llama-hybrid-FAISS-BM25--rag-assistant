[site]: crossvalidated
[post_id]: 437240
[parent_id]: 437231
[tags]: 
I find out what the problem is. I used Cross Entropy loss which is combined of softmax and negative log likelihood. When I use softmax with Negative log likelihood, the accuracies of two models are similar. I add my results: # CNN with softmax and NLL loss cuda:0 tensor(-0.1002, device='cuda:0', grad_fn= ) tensor(-0.7444, device='cuda:0', grad_fn= ) tensor(-0.7638, device='cuda:0', grad_fn= ) tensor(-0.7655, device='cuda:0', grad_fn= ) tensor(-0.9515, device='cuda:0', grad_fn= ) tensor(-0.9763, device='cuda:0', grad_fn= ) tensor(-0.9706, device='cuda:0', grad_fn= ) tensor(-0.9834, device='cuda:0', grad_fn= ) tensor(-0.9815, device='cuda:0', grad_fn= ) tensor(-0.9782, device='cuda:0', grad_fn= ) # results: CNN Accuracy: 98.83814239501953% CNNwithSoftmax Accuracy: 98.4775619506836% I controlled everything except loss and softmax (i.e. same training and test data, same epochs, etc.)
