[site]: crossvalidated
[post_id]: 353819
[parent_id]: 
[tags]: 
Performing Word Embeddings with domain-specific data

I am new to word-embeddings and have only worked with older approaches like bag of words/tf-idf. Unlike td-idf or bag of words, I have to first train a model to perform the embeddings. If working with domain-specific documents, can I train word2vec on those documents and then perform word-embedding with the trained model instead of an easily available, pre-trained model? If not, what are suggestions to embed domain-specific data for classification, since similar data may not be readily available?
