[site]: crossvalidated
[post_id]: 186732
[parent_id]: 186708
[tags]: 
In a sense you are trying to learn the rules of a black box (your device), by observing its behavior. Their are a couple ways to approach this. Decision Trees: Decision trees, at least to me, seem like the most natural model for your problem, especially if you want to be able to interpret the rules that are learned. You could interpret your problem as being a classification problem with the number of classes equal to 4 (each one of the bins being a separate class). I am assuming that a single point cannot belong to 2 or more bins. To train a decision tree, first sample a large amount of points and their respective "classes", then feed this into a decision tree learner of your liking. Shallow Neural Networks You mentioned in your answer that deep neural nets are too slow for your application, but it could be possible to train a shallow net to predict the classes using the same set-up as mentioned in the decision tree paragraph. The net might be able to deal better with features that are noisy than the decision tree, but it has a setback that the rules that it learns are also black box. If you set the last layer of this shallow net as a softmax with size 4, you can interpret the outputs as probabilities of each point belonging to that specific bin. Probabilities might or might not be useful depending on your application. These two seem like they would work in your scenario. Please let me know if this would work. Edit : The first comment posted below clarified the problem, and my solution was too long to post in the comments. The problem you stated is very interesting. To minimize the final amount of classes this is the approach I would recommend. I will write the approach in terms of probability, you can substitute any probabilistic model that you please. To start off let us say that state $x = \{x_0,x_1,x_2...x_n\}$ where $x_i$ represents a single data point. We can write the probability of a single data point $x_i$ belonging to a class conditioned on the rest of the points as $$P(x_i \in b_j | \bigcap_{n/i}x_n)$$ So an example of this with 4 data-points and 2 bins could be $$P(x_0 \in b_0|x_1,x_2,x_3)$$ Now comes the part of fitting each part of $x$ into a category with the constraint of having a maximum amount of data-points put into a bin. Under the assumption that each bin is independent of every other bin (although this may be false, is is "good enough" in this case). Therefore we can write the joint probability of all the bins as $$P(b_0,b_1,...b_m) = \prod_{j}^{m}P(b_j)=\prod_{j}^{m}\prod_{i \in b_j}P(x_i \in b_j | \bigcap_{n/i}x_n)$$ The maximum of this function under the constraints will be the optimal binning under your constraint and under your model. I'm not sure what exact algorithm one could employ to shift around data-points in bin's to maximize the joint probability, but here is my naive approach. I would first take the outputs for each data-point in my model and place them in the bin which maximizes their probability. Then look at the bins that are "over-flowing". Select the data-point that has the smallest probability in that overflowing bin, and find the next maximum probability in that points set of probabilities, of a bin that is open. Move that point to that bin. Continue this until your constraints are met. With this approach I would recommend using a neural network approach with a soft-max layer so you could interpret the outputs as probabilities (as mentioned in the "Shallow Neural Network" paragraph). Hope this helps.
