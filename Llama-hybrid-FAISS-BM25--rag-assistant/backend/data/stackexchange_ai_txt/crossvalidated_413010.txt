[site]: crossvalidated
[post_id]: 413010
[parent_id]: 369169
[tags]: 
After thinking about this for a little longer I found the following answer in Puterman, Markov Decision Processes, chapter 6: Policies can be divided by two 'features' into four categories: are they Markovian (yes/no) are they stationary (yes/no) If a policy is non Markovian then it indeed depends on the whole history and we then have two write expressions like $\pi(a_t|r_{t-1},s_{t-1},a_{t-1}, ..., s_0)$ . Stationary refers to whether or not the policy depends on the time $t$ , i.e. a olicy could consist of multiple small policies $\pi = (\pi_0, \pi_1, ...)$ and at time $t$ , $\pi$ involves $\pi_t$ . Both seem to make policies much more general, i.e. allowing access to the whole past should make the policy better, right? Also, making decisions based on the time should generally improve the quality of the policy, right? Turns out: No. Under suitable conditions, the optimal policy is a deterministic, Markovian, stationary one, i.e. given a 'general', non Markovian, non stationary policy, you will always find a better one that actually is stationary and Markovian (and even deterministic). Does that mean we can eliminate history dependent and/or non-stationary policies from our heads and from the theory? No! Why? Because you need these policies as 'intermediate steps' on your way towards the best policy. Example 1: n-armed Bandits. Given a single state $s$ and actions $A_1, ..., A_N$ (that are the 'arms' of the bandit that you can pull) and attached rewards that only depend on the actions and are normally distributed like $\mathcal{N}(\mu_j, \sigma_j)$ for $j=1,...,N$ , the best policiy is obviously to always pull the arm $A_j$ such that $\mu_j$ is maximal. However: given that the agent does not know ad hoc which mean is the best, it needs to figure that out using trial and error and one of the most used strategies actually is to approximate the means for each action $A_j$ empirically, i.e. sum the rewards that you received by pulling $A_j$ and divide by the amount of pulls. Example 2: RNNs. Another strategy how to solve RL problems is using neural networks. Without going into the details of how RNNs work they have a so-called "hidden state". From a practical perspective, 'hidden' is the wrong word, because you can always see it as the programmer of that neural net, so lets call it 'internal'. This internal state is being updated after an episode and is being dragged along when playing, for example, one session of a game. Hence, RNNs descrivbe non Markovian policies!
