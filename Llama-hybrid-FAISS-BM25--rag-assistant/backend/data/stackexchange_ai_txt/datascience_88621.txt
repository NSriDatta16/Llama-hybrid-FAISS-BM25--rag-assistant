[site]: datascience
[post_id]: 88621
[parent_id]: 88602
[tags]: 
NO. It's not Underfitting( Yay!!! ). But yes, there are some scopes of improvement such as:- I can see too many dropout layers. Try to oust them with BatchNormalization. That way you can have a more generalized model along with leveraging Try to reduce no. of Dense layers and increase no. of Conv1D layers. It will have two effects. First, your training time will decrease significantly and your model will most likely perform better. Use Cross-Validation( Stratified K-Fold preferably ) as it will provide a better insight into your model's generalizability. Use ReduceLROnPlateau callback as you can see your model isn't learning anything substantial from in the epochs you have attached in the question. It may be possible that your model is going through a plateau phase. ( NOT SURE ABOUT THIS ) Try to reduce your epoch. Do point out if I have suggested something absurd. Thank You
