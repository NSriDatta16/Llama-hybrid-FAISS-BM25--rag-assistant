[site]: crossvalidated
[post_id]: 485798
[parent_id]: 471213
[tags]: 
A salient candidate is entropy . For a random experiment $X$ with possible outcomes $\omega_1,\dots,\omega_n$ and the corresponding probabilities $P(\omega_1),\dots,P(\omega_n)$ , entropy is defined as $$ H(X) =-\sum_{i=1}^n P(\omega_i) \ln \left( P(\omega_i) \right). $$ It measures the average level of "information", "surprise", or "uncertainty" inherent in the experiment's possible outcomes. The fact that Wikipedia's entry on entropy defines entropy w.r.t. a random variable rather than the underlying experiment can be considered a failure. According to @whuber, This is an unfortunate case where Wikipedia fails us. Entropy most fundamentally is a property of a discrete probability measure. Everything else is derived from that concept.
