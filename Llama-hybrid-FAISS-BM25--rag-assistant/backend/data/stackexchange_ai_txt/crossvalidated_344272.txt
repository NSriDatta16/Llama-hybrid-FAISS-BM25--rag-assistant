[site]: crossvalidated
[post_id]: 344272
[parent_id]: 344259
[tags]: 
If you want to have a more detailed description you can read the Wikipedia article that answers your questions: https://en.wikipedia.org/wiki/Random_forest and this post: this is a nice post for a more detailed explanation: https://www.quora.com/What-is-the-out-of-bag-error-in-random-forests-What-does-it-mean-Whats-a-typical-value-if-any-Why-would-it-be-higher-or-lower-than-a-typical-value But in short and just to start you up: 1) the random forest algorithm generates a set of decision trees 2) each tree is trained on a different, randomly chosen subset of the data (ca. 2/3) and using a different, randomly chosen subset of the features. so each decision tree in the random forest has a different test and training sets (answers your second question) 3) for each element X_i of the data you can then compute an out of bag error (OOB error). briefly you consider all the trees in the RF that are trained on a set not containing X_i. you compute an OOB error counting the percentage of those trees that fail classifying X_i. averaging this error on all data you get the OOB. Accuracy = 1-OOB
