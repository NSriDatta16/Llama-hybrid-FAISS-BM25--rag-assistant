[site]: crossvalidated
[post_id]: 408398
[parent_id]: 
[tags]: 
Why aren't auto-encoders also considered generative models?

Auto-encoders (AEs) are composed of an encoder and a decoder (often represented by a neural network). The encoder produces a vector representation $z$ of its input $x$ (e.g. an image). The decoder attempts to output an object similar to $x$ , $x'$ , given $z$ . Variational auto-encoders (VAEs) are auto-encoders that model the hidden (or latent) space, which is the output space of the encoder, and the output space of the decoder as a probability distribution (e.g. a Gaussian). Hence, we can sample different $z$ from this hidden space, for each $x$ , and, similarly, we can sample different $x'$ from the output space of the decoder (given the sampled $z$ ). For this reason, VAEs are considered generative models. The difference between AEs and VAEs is that VAEs are considered generative models, whereas standard AEs are not. Why is that the case? Is it only because they are usually not formalised as generative models? More precisely, why are VAEs generative models (according to the definition of a generative model ) and AEs are not? For example, a denoising AE can attempt to generate an original denoised image, so, in this sense, it is a generative model. Could AEs be formulated as generative models?
