[site]: crossvalidated
[post_id]: 486807
[parent_id]: 
[tags]: 
Dealing with Different Incident Rates in Training vs Test Sets

I am training a binary classification model with about 8000 observations in the training set and 500 in the test set (sets are dictated to me so I can't modify the split). In the training set the split between 1/0 is about 2/3=0 and 1/3=1. In the test set the split is 50/50. I notice when I test my model it is predicting things at about the same ratio as the training set distribution(2/3=0,1/3=1). I'm wondering if this is just because the model isn't optimized well yet, or if there's some fundamental problem with having these differences between test and training data distributions. And if so, if there are good ways to deal with a problem like that? Edit: Some more information. I'm attempting to train a neural network on it, and the distribution of the actual population we would eventually use it for is unknown.
