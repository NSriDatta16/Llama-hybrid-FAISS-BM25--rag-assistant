[site]: crossvalidated
[post_id]: 74401
[parent_id]: 74394
[tags]: 
It seems like you are mixing a couple of things up. First of all, cross-validation is used to get an accurate idea of the generalization error when certain tuning parameters are used. You can use svm-train in k-fold cross-validation mode using the -v k flag. In this mode, svm-train does not output a model -- just a cross-validated estimate of the generalization performance. grid.py is basically a wrapper around svm-train in cross-validation mode. It allows you to easily assess the best parameter tuple out of a given set of options via cross-validation. It is essentially a loop over the specified parameter tuples which performs cross-validation. a. Is the -v 10 option of cross validation can replace the testing step? Not entirely. Cross-validation is indeed used to get an estimate of the generalization performance of a model, but when performing cross-validation the entire training set is never used to construct a single model. The typical steps are (i) find optimal tuning parameters using cross-validation, (ii) train a model using these optimal parameters on the full training set and (iii) test this model on the test set. b. The result given by the steps above is suspiciously high (96%), and so I'm wondering if I am doing something wrong? Don't worry, be happy. Such classification accuracies are quite feasible for a wide range of problems. c. Could the use of grid.py for parameter selection before the train + cross validation damage the results (as if I were testing on data I've already trained)? grid.py does cross-validation for you. There is no point to perform cross-validation again after you ran grid.py .
