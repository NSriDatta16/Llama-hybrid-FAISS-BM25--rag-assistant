[site]: datascience
[post_id]: 107300
[parent_id]: 107278
[tags]: 
The feature selection step is there to guard against model overfitting . The feature selection step may decide that all the variables in the dataset are relevant, or it may decide to remove some. If no feature selection step is performed then no variables are removed and the resulting model may be well-fitted but may be (and likely is) overfitted. The main concern with overfitted models is their poor performance on out-of-sample or validation data. That is the main reason given by Wikipedia and matches my experience. If you have extra variables in the model then irrelevant data will corrupt your models output and make its predictions less accurate. You could take a more philosophical perspective and say that you want to base your modelling on some consistent and reasonable foundation of statistical reasoning. Whichever system you choose (classical, Bayesian, etc), it is likely to encode some form of Occam's razor and therefore have some mechanism of feature selection built in. That is it say, when comparing two models with the same quality of fit it will prefer the one with the fewest variables selected. In some cases, the ability of humans to interpret the model is important, as fractalnature points out. Sometimes you want a model so simple that a human can apply it themselves with a calculator. The introduction to the paper Supersparse linear integer models for optimized medical scoring systems provides a nice example of this.
