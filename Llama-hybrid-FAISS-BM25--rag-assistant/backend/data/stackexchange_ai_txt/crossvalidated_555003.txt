[site]: crossvalidated
[post_id]: 555003
[parent_id]: 
[tags]: 
VAE divergence is positive in minimization of variational inference?

I have been going through the minimization of Variational inference and have a good understanding of all the steps taken: However, there is a part that relies on KL >= 0: I have derived the proof that D(P||Q) getting the same results as in Why KL divergence is non-negative? . Although, I don't see how this result applies to D(q(z)||p(z|x)) >= 0, and am having a hard time deriving the proof to apply this fact in this, is it possible to derive KL >= 0 for this case?
