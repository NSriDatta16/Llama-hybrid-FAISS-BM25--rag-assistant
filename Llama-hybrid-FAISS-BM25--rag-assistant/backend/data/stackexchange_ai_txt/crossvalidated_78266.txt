[site]: crossvalidated
[post_id]: 78266
[parent_id]: 78259
[tags]: 
It is possible that the additional units are over-fitting the data. Formal analysis of neural networks is limited to broad statements because they're exceedingly difficult to manipulate analytically. An experiment to test this for a particular dataset, is to perform nested k-fold cross-validation. Select a $n$ observations of the data to perform nested k-fold cross-validation. Each subfold is fixed on the number of hidden units, but the training parameters are tuned for learning rate, momentum, weight decay and (if you're using drop-out, then) drop-out probability. Repeat this as a function of $n$. Plot the accuracy metric as a function of hidden units, for each $n$. I would expect for small $n$, the argmax to be a smaller number of hidden units. For large $n$ it might approach a higher number of hidden units.
