[site]: crossvalidated
[post_id]: 38633
[parent_id]: 38627
[tags]: 
It looks like you are analyzing time series. Try to use a distance function designed for time series, that e.g. allows slight differences in the exact beginning (i.e. at spikes at slightly different time points). And that scales better with dimensionality... Euclidean distance is not a very good choice for high-dimensional data, and this probably also kills k-means for you (it also expects you to use Euclidean distance!). Similarly, dimension-reduction techniques will often not work well, because you have too many dimensions for too few objects. PCA for example has d^2 degrees of freedom - with fewer objects than say 3*d*d you cannot expect good results, but it will overfit the dimensionality reduction. Just an example why Euclidean (or actually any LP norm) will not work for you. Consider the vectors: 0 0 0 1 1 0 0 0 0 0 ... 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 ... 0 1 1 0 0 0 0 These two vectors are about as different as they get for euclidean distance. A time-warping distance function however should recognize that with a shift of 2 it can align them perfectly. Once you have found a distance measure that work for your dataset (note, this depends heavily on the dataset. Measures that work with one may fail badly on the others. Sometimes you need to go into the frequency domain and use completely different measures, too!), you can try MDS and/or Clustering. There are various clustering algorithms available that are "density" based (where density is defined via distance, usually!)
