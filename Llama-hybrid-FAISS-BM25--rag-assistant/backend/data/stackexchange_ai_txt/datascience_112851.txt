[site]: datascience
[post_id]: 112851
[parent_id]: 112704
[tags]: 
It is not necessarily the case. The matrics $K$ and $Q$ can be very different. The intuition is that these two projections allow the model to search for a particular piece of information in the hidden states. From that perspective, there is no need to exclude the query state. If you look at analyses of what the attention heads in trained models do, you can see that most of the patterns are not diagonal as you would expect. Some of them look diagonal, some really are, but it is often a shifted diagonal. Example from Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned (Voita et al., ACL 2019): Another example from From Balustrades to Pierre Vinken: Looking for Syntax in Transformer Self-Attentions (Mareƒçek & Rosa, 2019)
