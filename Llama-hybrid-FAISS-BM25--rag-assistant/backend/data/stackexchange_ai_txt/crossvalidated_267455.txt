[site]: crossvalidated
[post_id]: 267455
[parent_id]: 267442
[tags]: 
First, it is important to notice that finding the optimal subset of features is NP-hard (see Wrappers for feature selection ). Hence, you look for computationally efficient, sub-optimal approaches. There are many, many possibilities. The two most basic approaches are particular cases of greedy search: start with all (resp. none) of the features, and remove (add) the feature which leads to a least (biggest) decline (improvement) in the recognition rate (this so called wrapper methods are discussed in the first reference). Correlation methods (for example, based on mutual information, see references on the corrensponding wikipedia page). More recently it random forest have proven to be efficient for this task (in addition to having a very good performance in many tasks). See Variable selection using Random Forests for a description of a couple of basic heuristics based on the ranking of features provided by the random forest. See this blog entry for an intuitive explanation of the feature importance measure provided by the random forest algorithm.
