[site]: crossvalidated
[post_id]: 639438
[parent_id]: 638886
[tags]: 
I want to compare the performance of two versions of a widget using a 2-sample comparison test. H_0 = both versions perform the same, H_1 = widget.b performs worse than widget.a. Your hypotheses might be ill defined. They currently represent a boundary case where the null and alternative hypotheses do not complement to an entire continuous region of parameter space. A more common hypothesis definition is $H_0$ : Widget B performs no worse than Widget A ( $y_\text A \leqslant y_\text B$ ) vs. $H_1$ : Widget B performs worse than Widget A ( $y_\text A > y_\text B$ ). In this setting, we aim to find evidence that A outperforms B. It is very important to define what performance means and how it is measured. Is $y$ continuous, nonnegative, strictly positive, integer, or a duration? Response variables of different types require different modeling methods even if the hypotheses are about means. I have good reason to suspect there is inherent variability in the performance metric that is being measured. I want to include multiple replicates for each specimen to try to address this. As Sextus Empiricus pointed out, you better clarify what 'widgets', 'subjects', 'specimens', 'replicates', 'repeat measures', and 'inherent variability' mean. Having variability in the response variable is natural and necessary to conduct statistical tests and analysis. Without variability, no statistical analysis is necessary. Question 1 - how many replicates. I'm not sure how to decide how many repeat measures (replicates) I should make with each specimen. Is 2 enough? Is 3? 10? What process (with reputable source) can I use to determine how many repeat measures I should include? It appears that you are talking about sample-size determination. Power analysis is used to find the required sample size at given values of significance level, power, and effect size in a specific test method. The sample size to obtain ideally should be neither more nor less than this calculated requirement, as it would deflate and inflate p values, respectively. Question 2 - how to treat the replicates in the comparison test How does one go about incorporating replicates into the sample data for comparison testing? Measuring the outcome multiple times on each participant can provide data to estimate within-participant variation, which makes the effect estimation more precise, standard errors smaller, and statistical tests more powerful. With longitudinal data, we need to use mixed-effect modeling. Many different experimental design can emerge: (1) one participant measured only once, (2) one participant measured multiple times but on only one version of one of the two widget, (3) one participant exposed to multiple versions of the same widget, (4) one participant exposed to both widgets, ect. With mixed-effect modeling, no need to average the responses before estimation. I did a pilot study using 10 specimens of widget.b (I expect the variance to be the same between widget.a and widget.b) with 2 repeat tests on each of 3 test subjects. As expected, most of the variance (92%) comes from the repeat measurement (i.e., it's inherent to the measurement), with only 8% of the variance attributable to the differences in specimens. This makes sense to me as the characteristics that define the performance of the widget in this test do not vary much from widget to widget. In your pilot study, you had 60 rows from three participants, each of whom had 20 measurements, two per widget version. Since the ANOVA table is not built from a mixed-effect model, it may decompose the variation incorrectly and deliver a wrong impression. Repeat measures are cheap, but tested components cannot be placed back into inventory (i.e., it's a non-destructive test, but inventory value is reduced by testing). I have many version b widgets available and I can sample as many as I need (within reason - more than around 60 would be difficult just due to time constraints). Version a widgets are expensive and have a long lead time. I have up to 10 available to test, but would prefer to consume less if possible. It is okay to include many versions of a widget; in fact, when using fixed-effect estimators, both the number of participants and the number of repeated measurements on each participant, which are likely done across multiple widget versions, need to be large to get consistent estimates. In a real experiment, you will likely model both widget versions and participant identities as random effects. My power and sample size calculations, based on the sample mean and sample s.dev from my pilot study, indicate I >90% power with n=4 and >97% power with n=5. Your power analysis look suspicious. A sample size that small does not look right. Comparing means does not mean that t , z , or ANOVA are the best choice. You need to choose the correct model before power analysis, and that depends on the response variable type and experimental design.
