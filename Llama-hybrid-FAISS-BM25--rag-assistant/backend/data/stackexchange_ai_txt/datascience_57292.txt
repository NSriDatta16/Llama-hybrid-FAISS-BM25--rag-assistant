[site]: datascience
[post_id]: 57292
[parent_id]: 57171
[tags]: 
Looking at the training epochs, it seems to me you set a patience parameter that is too short. Please consider removing early stopping at all, for a model trained on 1500 observations only. Early stopping comes useful for particularly heavy models, but in this you shouldn't need it. I think the size of each mini-batch is very small. That would make gradient descend very noisy, please consider increasing its size, or using full batch training as well. Additionally, I think you have implemented a Network that is too big. Your input is very small, therefore you don't need to expand its signal on layers of size 64. There are too many nodes that are trying to "learn" not many things, IMHO. A good architecture could be: model = Sequential() model.add(Dense(6, input_dim=6, activation='relu')) model.add(Dense(6, activation='relu')) model.add(Dense(1, activation=None)) That would make your model faster to train, and ensure that each node is learning relevant features of your data. I would also change the output layer. Since you want to predict an outcome, you need an output node with no activation (i.e. linear activation). That is mandatory for regression tasks with unbounded output. Additional things you can try are: change dropout levels (but for such a small network it might not be needed at all), try regularization techniques, such as Batchnorm, L1 - L2 regularization, different weights initialization... you name it, try alternative activation functions. Also, for medium-to-small size datasets, it is possible for other ML algorithms such as Random Forests or SVMs to beat Neural Networks in performance.
