[site]: datascience
[post_id]: 61153
[parent_id]: 61142
[tags]: 
train_test_split has the problem that the sets can be unbalanced, so if I am unlucky I train my model only with positive or negative examples. --> can't that be solved by using stratify=True? --> yes, that's what stratify=True is for. However you still only train on the data of your training set and test with the data from the test set train_test_split doesn't split the sets the same way all the time so the results aren't comparable --> setting random_state=0 solves the problem? --> it does...keep in mind that random_state= will work fine as long as you keep the seed aka random-state the same for 3. and 4. let me make sure you understand the difference between CV and TTS. TTS splits your data once, trains on the now "fixed" training set and tests on the "fixed" test set. However this introduces a kind of bias to your evaluation because you are not training or testing on all observations. By going for a CV you make sure that all observations are used for testing and training. This reduces the bias of setting a "fixed" training and test set. Now for 3.: When in doubt use CV. HOWEVER...sometimes this doesn't make sense. Think about time series data and how cv would work then and what the result would be. That being said...now think about how TTS with stratify or random would work and what those would mean. CV usually is a good way to go however you always have to think about if the evaluation method fits your problem. TTS has its merit for time series as well as for performance reasons using large datasets. And 4.: The picture visualizes TTS and CV. First you have CV which splits your training data so that each small data set is used as test data. The "Testing" set is then used to measure generalization of your model. This is called the holdout method The diagram that says "Data Permitting" demonstrates TTS with an additional "Testing" set for measuring generalization. You train on "Training", validate the model on "Validation" and as with the holdout method you keep a "Testing" set for measuring how well your model generalizes on unknown data. Last but not least...5.: If you get good results and generalization by using TTS...go for it. If you get problems with over-/underfitting or generalization try CV and see if it helps. Of course as with everything in Data Science and Machine Learning there are multiple levels of complexity that you can add on top of TTS and CV as well. For example you could read about K-Fold or LOOCV etc. I don't think that is necessary though.
