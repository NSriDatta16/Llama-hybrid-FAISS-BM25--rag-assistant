[site]: datascience
[post_id]: 102526
[parent_id]: 102509
[tags]: 
I am not sure what you mean by "gradient descent in its entirety" but I assume you mean using the entire data in each epoch. In general, you have 3 different ways to "batch" your data. You can update the variables after training each sample, which is Stochastic Gradient Descent. If you do the update after training some number of data (smaller than the size of your entire data) - it is called "mini-batch gradient descent" regardless the batch size. And finally you can use your entire dataset before the update, and this is called "batch gradient descent"; but usually, as the entire dataset is almost never used for training (it might be impossible to fit the entire dataset in the memory), so mini-batch gradient descent is called batch gradient descent. If you use your entire dataset for training, update is made after going through all the data you have. This is called one "epoch". Update is done at the end of the epoch in this case. As for the initialization, you are correct to point out there is a problem. If the function we minimize was convex , it would not matter what we choose for initial values, as gradient descent would get us to the minimum no matter what. But as the dimensions of the model increase, it is extremely unlikely that we have a convex loss function. And in this case, initialization of the weight depends on the activation functions used in the model. As discussed in here , weights can be initialized such that w 's are chosen to be random but small numbers, whereas b 's are set to zero. For a more detailed discussion of initialization of neural networks, and a brief summary of the Xavier initialization , check this article, where you can also test various initialization methods against MNIST data to see the difference.
