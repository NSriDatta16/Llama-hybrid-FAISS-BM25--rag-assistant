[site]: datascience
[post_id]: 123938
[parent_id]: 
[tags]: 
Is n-Step SARSA Off-Policy?

To keep things simple, let us consider the case of an episodic Finite MDP with n=2 and no discounts. Copying the equation for n-step SARSA from Reinforcement Learning 2nd edition, page 146: The n-step update rule would be: Translating this to our case for n=2, the 2-step update rule would be: $$Q_{t+2}(S_t,A_t) =Q_{t+1}(S_t,A_t) +\alpha[R_{t+1} + R_{t+2} + Q_{t+1}(S_{t+2},A_{t+2}) -Q_{t+1}(S_t,A_t) ] $$ Let us simulate the algorithm: t=0 : Take action $A_0$ based on $Q_1$ , get reward $R_1$ . Pick the next action $A_1$ based on $Q_1$ t=1: Take action $A_1(Q_1)$ , get reward $R_2$ . Pick the next action $A_2$ , based on $Q_1$ . We then update $Q_1 -> Q_2$ t=2: Take action $A_2(Q_1)$ , get reward $R_3$ . Pick the next action $A_3$ , based on $Q_2$ . We then update $Q_2 -> Q_3$ t=3: Take action $A_3(Q_2)$ , get reward $R_4$ . Pick the next action $A_4$ , based on $Q_3$ . We then update $Q_3 -> Q_4$ â€¦. $Q_3(S_1,A_1) =Q_2(S_1,A_1) +\alpha.[R_2 + R_3 + Q_2(S_3,A_3) -Q_2(S_1,A_1) ]$ Consider above equation. We are constructing a boot-strapped estimate for $Q_2(S_1,A_1)$ . This is the value that would accrue if we took action $A_1$ in $S_1$ and then followed policy encoded in $Q_2$ after that. $R_2$ consequence of playing $A_1$ . Note that since we are learning action value function, $A_1$ need not be "on-policy". $R_3$ consequence of playing $A_2$ which is picked based on the policy encoded by $Q_1$ not $Q_2$ . This seems off-policy . $Q_2(S_3,A_3)$ - $A_3$ is picked based on policy encoded by $Q_2$ It seems to be that we need to do an off-policy correction for $R_3$ which results from $A_2$ based on $Q_1$ . Would love feedback on this.
