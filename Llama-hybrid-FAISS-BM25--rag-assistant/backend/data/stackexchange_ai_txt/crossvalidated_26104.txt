[site]: crossvalidated
[post_id]: 26104
[parent_id]: 20793
[tags]: 
I'll go by parts and suggest you to not follow what you quoted in the introduction: it is wrong to prove gaussianity by tests that gives a certain value (or range of values) if the data is gaussian (if A implies B, B does not necesarly implies A). If I understand correctly, the answer is no: there are several distributions with finite variance that are non-gaussian (e.g. white noise in time series). Furthermore, the family of elliptically symmetric distributions are, by definition, of the form \begin{equation*}p(\mathbf{x})=\frac{1}{\alpha|\Sigma|^{1/2}}f(-\frac{1}{2}\mathbf{x}^T\Sigma^{-1}\mathbf{x})\end{equation*} where you can represent $\mathbf{x}$ as a random variable or vector (as you like, but I putted it sugestively as a random vector). $f(\cdot)$ in principle can be any function such that $\int p(\mathbf{x})=1$, where the special case $f(\cdot)=exp(\cdot)$ is a gaussian distribution. In general, then, the family of elliptically symmetric distributions are non-gaussian, where the only exception is the function $f(\cdot)=exp(\cdot)$. If gaussianity is a requirement, you have to test for non-gaussianity . There are several ways of testing non-gaussianity, where the most intuitive is the search for higher-order cummulants in your data, because the gaussian distribution is the only one that has a finite number of non-zero cummulants (this is a theorem known as Marcinkiewicz's theorem). However, this is not recommended because (a) it is computationally expensive and (b) you'll be never sure! However, one way of measuring (and therefore testing for) non-gaussianity that has been particularly useful in Independant Component Analysis (an application where you need to measure the degree of non-gaussianity of samples) is negentropy. For an introduction on these measures, see these notes by Hyvärinen on the subject which is an extract of the paper by Hyvärinen & Oja (2000) on Independant Component Analysis. If you are interested, search for his papers on efficient ways of calculating negentropy. It really depends on how many samples we are talking about... I didn't understand: conditions for what?
