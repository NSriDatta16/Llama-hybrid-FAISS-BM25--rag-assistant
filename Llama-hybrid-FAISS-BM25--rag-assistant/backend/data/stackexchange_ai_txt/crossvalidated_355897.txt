[site]: crossvalidated
[post_id]: 355897
[parent_id]: 306703
[tags]: 
The most widely-used method for predicting the probability of a categorical output is to use softmax activation in the final layer and cross-entropy loss for training . This is because you're modeling the probability that each word in your dictionary comes next. Full softmax can be expensive, though, so sampled softmax and its variants can speed things up. Another option is to observe that it's generally expensive to work with words , because there are so many of them: you'll have a large embedding layer, and a large projection layer -- that's a lot of data to store. Instead, working with characters can work very well -- there are relatively fewer English characters, so full softmax works fine, and LSTMs can essentially "learn" English from nothing. To generate the next word, perhaps because you want your network write a new sonnet, take the output of the network as a probability vector. Then sample from a multinomial distribution. The sampled element is used as the next input to the network, and so on. This works because the output of the softmax layer is a probability vector, and you can write your sonnet word-by-word, or character-by-character. Using ReLU as the output layer doesn't make much sense. Your outcome is categorical so you need a probability distribution over the categorical output. ReLUs output unbounded values, so it's not a probability distribution. This article is a good place to start: Andrej Karpathy, " The Unreasonable Effectiveness of Recurrent Neural Networks "
