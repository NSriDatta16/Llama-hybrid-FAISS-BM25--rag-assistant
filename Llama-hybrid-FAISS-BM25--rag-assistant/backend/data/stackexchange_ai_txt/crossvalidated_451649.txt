[site]: crossvalidated
[post_id]: 451649
[parent_id]: 451641
[tags]: 
It's not using a literal logistic regression at each boosting stage, but there are two strong connections. First, the model is forming predictions by passing the output of the fit regression trees through the (inverse) link function used in logistic regression: $$ P(y \mid X) = \frac{1}{1 + \exp(T_0(X) + T_1(X) + \cdots + T_N(X))} $$ Second, the regression trees are being fit to the gradient of the loss function used in logistic regression (a.k.a. the log-loss). This means that the boosting stages are constructed to incrementally minimize the log-loss of the predictions, much like the iterative algorithms for fitting a logistic regression. This should be all true regardless of the particulars of the boosting algorithm used, none of the above is particular to XGBoost.
