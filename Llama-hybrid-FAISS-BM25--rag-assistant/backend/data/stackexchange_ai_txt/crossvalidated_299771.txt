[site]: crossvalidated
[post_id]: 299771
[parent_id]: 181934
[tags]: 
Actually, the general formula of sequential Bayesian updating is: $$ P(\theta \mid D_{a}, D_{b}) \propto P(D_b \mid \theta, D_a) P(\theta \mid D_a). \,\,\,(*) $$ However, for most machine learning models, $D_a$ and $D_b$ are conditionally independent given $\theta$, i.e., $$ P(D_a \mid \theta) P(D_b \mid \theta) = P(D_a, D_b \mid \theta), $$ then, $P(D_b \mid \theta, D_a)$ in $(*)$ naturally equals to $P(D_b \mid \theta), $ so the $(*)$ becomes: $$ (1)\,\,\,\,\,\,P(\theta \mid D_{a}, D_{b}) \propto P(D_b \mid \theta) P(\theta \mid D_a), $$ which is exactly what Murphy's ML book talks about.
