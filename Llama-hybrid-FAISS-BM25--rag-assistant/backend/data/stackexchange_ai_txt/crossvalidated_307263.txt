[site]: crossvalidated
[post_id]: 307263
[parent_id]: 
[tags]: 
What approach to use for debt collection modeling with real world data?

I work in a bank and am tasked with designing the debt collection data science dept. from the ground up. Basically, we need to implement strategies that maximize our cash flow in perpetuity. I have a clear view of the objective function (see below), however I am unsure regarding how to model each component and how to optimize the global function. Objective: choose $(s_a,o_a)$ such as to maximize $y$, where $$ y = E(NPV) = \sum_{c=1}^{C} \sum_{a=1}^{A} \Big [p(locate|x_{c},h_{c},s_{c,a})*p(accept|x_{c},h_{c},s_{c,a},o_{c,a})*PV(o_{c,a},t_{c,a})*surv(x_{c},h_{a},o_{c,a})*(1-vcost(x_{c},s_{c,a}))-fcost(x_{c},s_{c,a}) \Big ] $$ $E(NPV)$: expected net present value $c$: delinquent loan contract $C$: total number of contracts $a$: repayment agreement $A$: total number of agreements in the life of a contract $p(locate|...)$: probability of successfully contacting a client, for further negotiation (many times they don't answer calls or reply messages) $p(accept|...)$: probability of a client accepting an offer (see below) $PV$: present value $surv$: "survival", i.e. percentage of PV repaid in an agreement before the client defaults again $vcost$: variable costs (depends on success, e.g. agent commissions, agreement maintenance...) $fcost$: fixed costs (does not depend on success, e.g. legal expenses, communication costs, personnel...) $x_c$: characteristics of the client/contract (e.g. debt ammount, product, days since default, demographics...) $h_c$: history of interactions with the client (e.g. previous contacts, offers accepted/refused, repayments...) $h_a$: repayment history of the agreement (if currently active) $s_{c,a}$: strategy (e.g. communication channel, communication time, negotiation script...) $o_{c,a}$: offer (terms of a repayment agreement, e.g. discount, interest rate, number of terms...) $t_{c,a}$: date of an agreement (one contract can have more than one agreement in its life) Problems/Questions/What am I thinking: We have a catalog of possible strategies, but none have been tested with statistical rigor. I want to allot part of our assets to continuously run hypothesis testing and estimation, to generate both empirical insights and enough data to train our statistical models. How can I know technically which hypotheses to prioritize? Which tools can help me document the tests and conduct meta-analyses with the objective of accumulating knowledge over time? I can use both frequentist and bayesian approaches where necessary. How can I breakdown the objective function into statistical learning components? For example, from the objective function I can see that I'll need a model for client location, a model for offer acceptance propensity, and possibly a survival model. I'm also thinking of modeling it hierarchically, first clustering all the contracts with the most robust data to provide a general recommendation, and then "individualization" models that can be more sparse to take into account triggers such as promisses that a client made, change in employment status etc. What technique can I use to combine the statistical learning components with historical information about the strategies/offers and business rules/constraints to optimize the global model? Monte Carlo? We are constrained by the capacity of our service providers and implementation speed, i.e. each strategy can only be applied to a maximum number of contracts. How can I take these constraints into consideration when optimizing the global model? We have over 15 years worth of real world data, mainly characteristics of clients/contracts ($x_c$), offers accepted (part of $h_c$), and history of repayments (part of $h_c$). Not all variables were available in each moment in time, and we don't have historical info regarding offers refused, collection effort (i.e. if we don't even try contacting a client once, most probably he won't repay), located clients who never accepted offers, or what strategies were being applied in each moment in time. We will have some of these infos in the future, though. How can I extract the maximum ammount of information from all these historical data, taking into account different timelines, concepts, strategies and available variables? How avoid introducing past biases when using this information to train the new models? How to combine historical (incomplete and less trustable) with current information? Again, I can use both frequentist and bayesian approaches where necessary. There are some characteristics of clients/contracts that may change over time, such as socioeconomic status (e.g. employment status, income...). This causes contracts to be classified in different clusters in each moment in time. How can I most effectively take this "classification fluctuation/history" into account, both when training or applying the clusterization/prediction models? What technique to use to clusterize in the first place?
