[site]: crossvalidated
[post_id]: 503115
[parent_id]: 502760
[tags]: 
A test can never make sure that the model is correct. What tests do is that they check the compatibility of the data with the model, i.e., could the model realistically have produced such data? You are right that a non-rejection does not prove the model - regardless of what the "explicit goal" was. You are also right that technically one can therefore object to a procedure that implies that a certain model assumption is fulfilled just because a test of the model did not reject the model assumption. The statement that "the Type I error is only 5%" refers to what I call the "model-world", not the real world. It says that if the model is correct, the rejection probability is 5%. This is a technically correct statement regardless of what the practical situation is. It however does not give you a guarantee in practice; you should not believe something like that you'd now be 95% sure that the model holds. Given that models never hold precisely in reality, the aim of a misspecification test (like testing homogeneity of variances) is not quite what people normally say it is. Actually this is done not because it could make sure that the model holds with large probability (it can't, as explained above). Rather it is done to indicate that a potential violation of the assumption is not strong and strikingly clear . True, the test cannot confirm the model, but it can make sure that the data are not obviously far away from it. The hope is that, although the model may be violated in a certain way, it is not violated in such a way to invalidate the conclusions you draw from it. The aim is, rather more modestly than making sure that the model assumptions hold, to do something with the data that is not obviously contradicted by them. Whether this is a good approach is controversial and unfortunately depends on all kinds of details (what exact misspecification test you run, what is your alternative action in case the misspecification test rejects the assumption, how exactly is the assumption violated that you are testing). There is some literature that investigates testing homogeneity of variances and then conditionally on it to run a standard two-sample test or ANOVA, and the results are not very encouraging for this kind of "combined procedure" - in the specific setup treated by BruceET's answer I'd agree with the advice given there -, although it does help in some other situations. See this arxiv paper for a recent literature overview and discussion: https://arxiv.org/abs/1908.02218 Unfortunately the problem does not go away by not running formal misspecification tests and instead looking at the data and deciding whether an assumption looks OK. The problems with this are the same in principle, but because of the subjective nature they cannot be formally analysed, so there is no literature quantifying problems with this approach. This doesn't mean that they don't exist. Visual checking certainly has its role to play, but I have met people who think that formal misspecification tests should not be used because of the problems that you have alluded to and that are elaborated in the paper linked above and the literature cited there, however looking at graphs and deciding what to do based on them is fine. Not so - there is really no escape. We have to live with the fact that we are working with model assumptions that ultimately we have no way to verify.
