[site]: datascience
[post_id]: 116878
[parent_id]: 115965
[tags]: 
After some work I haven't come up with a real solution. I assume that this is not a good loss to train the encoder. I got better results whit this one: def encoder_loss(self, image, reconstructed_image): return self.mse(image, reconstructed_image) Where the image(s) will be encoded and then decoded (like an autoencoder). Another way is calculate the loss on the latent vectors: def encoder_loss(self, random_noise, encoded_image): return self.mse(random_noise, encoded_image) On my tests this one works but is not as good as the first one.
