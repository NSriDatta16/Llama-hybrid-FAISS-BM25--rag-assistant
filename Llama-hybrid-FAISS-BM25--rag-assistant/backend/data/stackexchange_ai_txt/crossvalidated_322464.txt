[site]: crossvalidated
[post_id]: 322464
[parent_id]: 
[tags]: 
What is the difference between classical frequentist methods and likelihood methods?

You may assume that I'm familiar with the material in Casella and Berger. This question is identical to What is the difference between Fisherian vs frequentist statistics? ; however, the question was closed for ambiguity and has no answers. I speak of "likelihood methods" in context of the text In All Likelihood: Statistical Modelling and Inference Using Likelihood by Pawitan . Truth be told, I've only skimmed this text a few times. Here is the description of the text provided in Amazon [emphasis added]: Based on a course in the theory of statistics this text concentrates on what can be achieved using the likelihood/Fisherian method of taking account of uncertainty when studying a statistical problem . It takes the concept ot the likelihood as providing the best methods for unifying the demands of statistical modelling and the theory of inference. Every likelihood concept is illustrated by realistic examples, which are not compromised by computational problems. Examples range from a simile comparison of two accident rates, to complex studies that require generalised linear or semiparametric modelling. The emphasis is that the likelihood is not simply a device to produce an estimate, but an important tool for modelling. The book generally takes an informal approach, where most important results are established using heuristic arguments and motivated with realistic examples. With the currently available computing power, examples are not contrived to allow a closed analytical solution, and the book can concentrate on the statistical aspects of the data modelling. In addition to classical likelihood theory, the book covers many modern topics such as generalized linear models and mixed models, non parametric smoothing, robustness, the EM algorithm and empirical likelihood. Also, the review by "MathsEngineer" states [emphases added]: As a practicing data analyst, I frequently find the standard "classical" statistical techniques either useless, cumbersome, or convoluted for all but the most straightforward applications . However, before running across the book, I thought my only other option would be to utilize Bayesian methods (which are infinitely more flexible and elegant) to perform inference with complex, non-normal/non-linear models. Unfortunately, solving Bayesian models relies on two rather unappealing concepts: a prior probability (so that all your calculations still yield probabilities),and Markov Chain Monte Carlo (a computationally intensive, sometimes non-convergent, algorithm that is only asymptotically correct wrt the true Bayesian posterior probability distribution.) Reading Dr. Pawitan's book introduced me to a very satisfying "third way" as he calls it. Instead of force-fitting all uncertainty into a probability, the "likelihood" approach recognizes two types of uncertainty, which is both novel in statistics and extremely refreshing once you understand why two types are necessary . The first, which I would call "well calibrated" uncertainty, is analogous to a confidence interval for the mean of a normal sample. With this type, we know how often we would be wrong under repeated sampling from this population, so we have a good idea how well our method brackets the true mean i.e., well calibrated. As I skim through the index, a lot of it looks like the statistics material in Casella and Berger (which has much more of a frequentist than Bayesian emphasis), as well as some linear models (with matrices). Thus, I don't see how this is so different from frequentist "classical" statistics. I do understand the philosophical differences between frequentist and Bayesian statistics, but this "likelihood" approach I'm not as familiar with. Could someone elaborate on this?
