[site]: crossvalidated
[post_id]: 104189
[parent_id]: 104107
[tags]: 
The problem is discretization error. density divides the range of data by default into n=512 intervals. The outlier causes the intervals to be at least an order of magnitude greater than otherwise. With very large amounts of data (it seems to take somewhere more than $10^5$ for this problem to become really apparent with the default value of n and this order-of-magnitude outlier), the cumulative discretization error takes over. The general lesson--which is by no means confined to just density estimation nor to R --is that when routine tools are pushed towards their limits, we need to understand how they do their calculations and we should remember to pay attention to all the parameters that control their accuracy. I haven't reverse-engineered the source code to pin this down definitively, but I will supply some empirical evidence using random data sets. trial
