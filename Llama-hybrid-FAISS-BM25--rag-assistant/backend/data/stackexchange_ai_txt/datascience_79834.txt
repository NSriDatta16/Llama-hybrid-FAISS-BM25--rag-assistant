[site]: datascience
[post_id]: 79834
[parent_id]: 79649
[tags]: 
Neural network algorithms are stochastic. This means they make use of randomness, such as initializing to random weights, and in turn the same network trained on the same data can produce different results.The random initialization allows the network to learn a good approximation for the function being learned. The most common form of randomness used in neural networks is the random initialization of the network weights. Although randomness can be used in other areas, here is just a short list: Randomness in Initialization, such as weights. Randomness in Regularization, such as dropout Randomness in Layers, such as word embedding. Randomness in Optimization, such as stochastic optimization. The Solutions. The are two main solutions. Solution #1: Repeat Your Experiment The traditional and practical way to address this problem is to run your network many times (30+) and use statistics to summarize the performance of your model, and compare your model to other models. I strongly recommend this approach, but it is not always possible due to the very long training times of some models. Solution #2: Seed the Random Number Generator Alternately, another solution is to use a fixed seed for the random number generator. from numpy.random import seed seed(1)
