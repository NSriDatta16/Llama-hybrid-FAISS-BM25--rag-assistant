[site]: datascience
[post_id]: 23652
[parent_id]: 
[tags]: 
Can TF turn a given graph into a recursive one?

I have a complex graph that takes an input $x_t$, it does a bunch of convolutions, tensor shape manipulations etc... until it produces $y_t$. I want now to try a different approach, where I feed $y_t$ at some point into the graph (I concatenate it to some features after a convolution), while the net is processing $x_{t+1}$. So the net should do the following ($y_0$ is a fixed initial input): $(x_0,y_0)\rightarrow y_1$ $(x_1,y_1)\rightarrow y_2$ etc... I have been looking at the standard RNN implementations in TensorFlow (GRU, LSTM, etc), but I don't quite understand how I can do this, because it seems that LSTM and the other RNN units have a fixed graph (like, a fully connected layer) and I cannot use the one that I need instead. Do I need to subclass tf.nn.rnn_cell.RNNCell ? That seems problematic too because it seems to admit only 2D tensors as input. EDIT: I just discovered this awesome article and I'm going to document myself about tf.scan .
