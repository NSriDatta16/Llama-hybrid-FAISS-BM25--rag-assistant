[site]: crossvalidated
[post_id]: 245022
[parent_id]: 
[tags]: 
Testing whether a classification model is better than another predictor

I hope this has not been asked before. I have got data(my prediction variables) of 3 populations of individuals, A,B and C. These individuals can be described as: A is the part of the risk group showing symptoms, B is the part of the risk group not showing symptoms, and C is not a risk group and does not show symptoms. I have now created two logistic regression models for the questions: "Does an individual belong to the risk group?", i.e. "classify (A and B) vs C" and "Is an individual showing symptoms", i.e. "classify A vs (B and C)". Both models are better than chance level on the test set and therefore i want to answer the following question: can i predict the symptoms better than just classification to the risk group? Is there a good statistical test for that? Both models are tested on the same data. I am thinking that i should just run both models on the task A vs (B and C) and use a one-sided McNemors test for that. But the models are trained on different class prior class probabilities, so this would not give the right result. Another alternative would be a DeLong test on the AUC. Any other options?
