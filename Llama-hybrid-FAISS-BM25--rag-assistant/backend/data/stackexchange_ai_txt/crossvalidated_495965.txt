[site]: crossvalidated
[post_id]: 495965
[parent_id]: 495937
[tags]: 
(Proper) scoring rules assess probabilistic predictions, i.e., full continuous or discrete predictive distributions in the numerical case, and predictive class membership probabilities in the (possibly multiclass) classification case. Specifically in the numerical case, you might be predicting tomorrow's temperatures, or sales. Your predictive distribution will be a probability density. For instance, your probabilistic temperature forecast might be "a normal distribution with mean 20°C and standard deviation 10°C", and your probabilistic sales forecast might be "a Poisson distribution with mean 3.7 units". You can then assess the actually observed temperature or sales against these probabilistic predictions using proper scoring rules, like the log score . As a benchmark , we use the simplest reasonable model. If our complicated model cannot even beat this simple model, we have nothing to show. In your two examples, you evaluate point predictions, and the benchmarks, i.e., the simplest models, used are: For classification accuracy, the simplest reasonable model is to assign every new instance to the majority class of the training data. For numerical predictions assessed through the RMSE, the simplest model is to predict the mean of the training data. ( If you wanted to minimize the Mean Absolute Error, you would instead use the median of the training data. (Note in both cases how the error measure influences what the "best" point prediction is.) So, what is the simplest reasonable model for probabilistic predictions? It's what is called the climatological model: we issue a probabilistic prediction that is simply the distribution observed in the training data. For a classification task, the predicted probabilities would be the incidence of the classes in the training sample. For a numerical prediction, this would be the simple historical histogram or a density estimate (possibly smoothed). Of course, this nomenclature comes from meteorology: your weather forecast should be at least as good as the climatological one, i.e., the multi-year average (e.g., Mason, 2004 ). For references, I often recommend Tilmann Gneiting's papers. Gneiting & Katzfuss (2014) is a good overview of probabilistic predictions and proper scoring rules. Gneiting, Balabdaoui & Raftery (2007) have a nice little example comparing the climatological forecaster to more skillful colleagues. Gneiting also has a number of papers in journals like JASA and JRSS , but these are naturally more mathematical. For the discrete case (predicting counts), you may want to look at Czado, Gneiting & Held (2009) , and I have published an application to (count) sales forecasting in Kolassa (2016) .
