[site]: crossvalidated
[post_id]: 326788
[parent_id]: 
[tags]: 
When to choose SARSA vs. Q Learning

SARSA and Q Learning are both reinforcement learning algorithms that work in a similar way. The most striking difference is that SARSA is on policy while Q Learning is off policy. The update rules are as follows: Q Learning: $$Q(s_t,a_t)←Q(s_t,a_t)+α[r_{t+1}+γ\max_{a'}Q(s_{t+1},a')−Q(s_t,a_t)]$$ SARSA: $$Q(s_t,a_t)←Q(s_t,a_t)+α[r_{t+1}+γQ(s_{t+1},a_{t+1})−Q(s_t,a_t)]$$ where $s_t,\,a_t$ and $r_t$ are state, action and reward at time step $t$ and $\gamma$ is a discount factor. They mostly look the same except that in SARSA we take actual action and in Q Learning we take the action with highest reward. Are there any theoretical or practical settings in which one should the prefer one over the other? I can see that taking the maximum in Q Learning can be costly and even more so in continuous action spaces. But is there anything else?
