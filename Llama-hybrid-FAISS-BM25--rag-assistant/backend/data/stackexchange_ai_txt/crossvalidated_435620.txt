[site]: crossvalidated
[post_id]: 435620
[parent_id]: 164267
[tags]: 
First of all we do not stack linear functions into each other in order to obtain a nonlinear function. There is a clear reason why NNs might never work like that: Stacking linear functions into each other would yield again a linear function. What makes NNs nonlinear is the activation function that comes behind the linear function! However, in principal you are right: We simply stack a lot of logistic regressions (not linear ones though!) into each other and ... tadaa: we get something good out of it... is that fair? Turns out that (from a theoretical point of view) it actually is fair. Even worse: Using the celebrated and well known Theorem of Stone-Weierstrass we simply prove that neural networks with just one hidden layer and no output function at the final node is enough to approximize any continuous functions (and beleive me, continuous functions can be ugly beasts, see the "devils staircase": https://en.wikipedia.org/wiki/Cantor_distribution ) on intervals of the form $[a,b]$ (NNs with just one hidden layer and no output function at the final node are exactly functions of the form $x \mapsto = b + a_1\phi_1(x) + ... + a_l\phi_l(x)$ where $l$ is the size of the hidden layer, i.e. polynomials in logistic functions and they form an algebra by definition!). I.e. 'by construction', NNs are very expressive. Why do we use deep NNs then? The reason is that the SW-theorem above only guarantees that there is a sufficiently large layer size so that we can come close to our (hopefully continuous) target function. However, the layer size needed may be so large that no computer could ever handle weight matrices of that size. NNs with more hidden layers seem to be a good compromise between 'accuracy' and computability. I do not know of any theoretical results that points into the direction of 'how much' the expresiveness of NNs grows when putting in more hidden layers in comparison to just increasing the size of the single hidden layer but maybe there are some resources on the web... Can we truly understand deep NNs? Example questions: Why exactly does the NN predict this case to be TRUE while it predicts this other, similar case to be FALSE? Why exactly does it rate this customer more valuable than the other one? I do not really believe so. It comes with the complicatedness of the model that you cannot explain it reasonably well anymore... I only hear that this is still an active area of research but I do not know any resources... What makes NNs so unique among all models? The true reason why we use NNs so much these days is because of the following two reasons: They come with a natural 'streaming' property. We can pimp them to the max in many directions. By 1. I mean that given a training set $T$ , a NN $f$ that was trained on this set $T$ and some new training samples $T'$ , we can easily include these training samples into the NN by just continuing the gradient descent/backprop algorithm while only selecting batches from $T'$ for the training. The whole area of reinforcement learning (used to win games like Tic Tac Toe, Pong , Chess, Go, many different Atari games with just one model , etc) is based on this property. People have tried to infuse this streaming property to other models (for example Gradient Boosting) but it does not come that naturally and is not as computationally cheap as in the NN setup. By 2. I mean that people have trained NNs to do the weirdest things but in principle they just used the same framework: stacking smooth functions into each other and then let the computer (i.e. PyTorch/Tensorflow) do the dirty math for you like computing the derivative of the loss function w.r.t. the weights. One example would be this paper where people have used the RL approach and also pimped the architecture of the NN to learn the complex language of chemical substances by teaching it how to operate on a memory stack (!). Try to do that with gradient boosting ;-) The reason why they must do that is that the language of chemicals is at least as 'hard to learn' as the bracket language (i.e. every opening bracket has a closing one later on in the word) because the SMILES language that peopple use in order to describe molecules contains the symbols '(' and ')'. From theoretical computer science (Chomsky hierarchy) one knows that one cannot describe this language with a regular automata but one needs a push down automata (i.e. an automata with a stack memory). That was the motivation for them (I guess) to teach this weird thing to the NN.
