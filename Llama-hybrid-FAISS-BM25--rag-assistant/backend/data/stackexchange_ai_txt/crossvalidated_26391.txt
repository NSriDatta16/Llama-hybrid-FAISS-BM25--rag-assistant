[site]: crossvalidated
[post_id]: 26391
[parent_id]: 25789
[tags]: 
It all depends on how to plan to make predictions after you've finished pre-processing there data. For example, if you expect consecutive observations of a time series $y_t$ to be linearly related to previous measurements: $$ y_{t} = \beta_0 + \beta_1 y_{t-1} + \cdots $$ then you'd better choose a linear transformation. Or, if you expect consecutive measurements to be multiplicatively related to each other: $$ y_{t} = \eta_{0} \cdot y_{t-1}^{\eta_{1}} \cdot \cdot \cdot $$ then a log transformation (and subsequent linear model) would accomplish this since $$ \log(y_{t}) = \log(\eta_0) + \eta_{1} \log(y_{t-1}) + \cdot \cdot \cdot $$ You may investigate various transformations to see which one provides the best predictive power. But, the "best predictor" will depend on your measure of prediction accuracy. Regarding dimension reduction, your example only seems to involve 2 variables - dimension reduction often refers to the situation where you have a large number of predictors and want to lower the number in a principled way. If you have that situation, you may use standard techniques (e.g. PCA) for dimension reduction, which are not related to the fact that this is a time series. If you mean dimension reduction in the sense of reducing the number of timepoints, the only types of transformations that will accomplish this will be things like averaging over timepoints (e.g. to calculate weekly averages from daily data). It would be hard to justify doing something like this unless it is drastically easier to fit weekly data than daily data (for example, for rainfall data, it is easier to predict the average rainfall over the course of a week than it is to predict the daily rainfall values).
