[site]: crossvalidated
[post_id]: 626387
[parent_id]: 626376
[tags]: 
I don't think there is the perfect measure to do so. Still, here are two options from explainable ML that do not need to refit : Permutation feature importance The average loss (or some other meaningful performance measure) is calculated on the reference data, typically the validation data to get rid of model optimism. Then, for each feature, its data column is randomly shuffled and the increase in average loss is calculated. If the increase is small, it means the feature is unimportant. The measure was introduced by Leo Breiman for random forests and then later generalized. The algorithm Screenshot from our ETH Zurich lecture ( $\hat m$ is the fitted model) The process can be repeated a couple of times to get more robust results (and standard errors). It also works for feature groups (shuffling multiple feature columns together). One can also study relative increases in average loss. It's easy to implement. The only annoying thing is to specify the loss. Major drawback: Especially when features are collinear/dependent, shuffling creates uncommon or even impossible values. So the model is forced to extrapolate, which may lead to strange results. It can be calculated by R packages {iml}, {DALEX}, {hstats} (my new one dedicated to Friedman's model-agnostic interaction statistics). With {hstats}: library(shapviz) # to get a cool dataset library(ggplot2) library(hstats) head(miami) fit Shuffling living area increases the average unit Gamma deviance by almost 300%. (The error bars are created by shuffling four times by default.) References: Breiman, Leo. 2001. “Random Forests.” Machine Learning 45 (1): 5–32. https://doi.org/10.1023/A:1010933404324 . https://christophm.github.io/interpretable-ml-book/feature-importance.html SHAP importance The other frequently used model-agnostic method is based on SHAP values. The SHAP value of feature $j$ and observation $i$ equals the fair additive contribution of the feature to the prediction. Taking the average absolute SHAP value over a set of observations is a very natural measure of importance. A value of 0.2, e.g., means that the features increases or decreases the prediction on average by 0.2. The problem with SHAP values: While their game theoretic pendants (Shapley value) are perfect in all desireable aspects, their translation to statistics is not. Model-agnostic algorithms are either slow or imprecise, and like with partial dependence plots (and permutation importance), the model needs to be applied on sometimes unnatural/impossible feature combinations.
