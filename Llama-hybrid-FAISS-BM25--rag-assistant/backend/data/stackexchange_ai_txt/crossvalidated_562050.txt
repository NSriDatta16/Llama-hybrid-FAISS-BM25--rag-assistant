[site]: crossvalidated
[post_id]: 562050
[parent_id]: 
[tags]: 
Bias introduced when using weak shuffling

I have batch learning problem (in this particular case a neural network) where I am training my data in batches, and then repeating for a number of epochs. In Stochastic Gradient Descent, we minimise bias by randomising the data at every epoch. That is, before the data is batched, we shuffle the data. So if we consider X = [1,2,3,4,5,6,7,8] , we shuffle, X = [2,5,1,3,8,6,4,7] and then batch (here using batch_size=2 ), arriving at X = [[2,5], [1,3], [8,6], [4,7]] . Due to some other constraints, I can't shuffle my data like this. The best I can do is shuffle at batch level. That is, batch, X = [[1,2], [3,4], [5,6], [7,8]] and then shuffle the batches, X = [[3,4], [7,8], [1,2], [5,6]] . My intuition is that for dataset sizes $m$ such that $m >> \mathrm{batch\_size}$ the bias induced should be minimal. Is there a way to estimate the bias that this method of shuffling introduces to the model? Is there an experimentally sound way for choosing the appropriate sizes for $m$ and $batch\_size$ ?
