[site]: crossvalidated
[post_id]: 621062
[parent_id]: 
[tags]: 
Why is linear regression taught differently from what I have learned?

I took a machine learning course using the book "Learning from Data: A Short Course" by Hsuan-Tien Lin, Malik Magdon-Ismail, and Yaser Abu-Mostafa (LFD) You are given a set of examples $\{x_n\}$ , a set of labels $\{y_n\}$ , the linear regression model is the function $\widehat{y}(x) = w^Tx$ where $w$ is a set of learned parameters obtained from minimizing the sum of the MSE error between $y_n$ and $\widehat y(x_n)$ . The end (LFD, page 84). However, when I open up other textbooks (virtually any ML/statistics textbook), they all say something like: We assume the labels $y$ were generated via $y = w^T x + \epsilon$ where $\epsilon$ is some iid Gaussian error. But why? This is very strange and unwieldy, no? In the way that is taught from LFD, there is no need to assume that the label are generated according to an additive iid Gaussian noise. This assumption can never be verified in practice. Can someone make sense of what I am seeing? Is LFD taking on a new approach to teaching about linear regression? After all, all I care about is drawing a linear relationship between the label and my data. I don't care if the label are generated in some specific way ( $\approx0\%$ chance of being true in practice).
