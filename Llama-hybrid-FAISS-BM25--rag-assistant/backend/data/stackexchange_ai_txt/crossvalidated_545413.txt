[site]: crossvalidated
[post_id]: 545413
[parent_id]: 535720
[tags]: 
The sublayers refer to the self/cross multi-head attention layers, as well as the position-wise feedfoward networks. Your code is mostly correct, but: your pseudocode accidentally overwrites the value of the original x . The layer norm is applied after the residual addition. there's no ReLU in the transformer (other than within the position-wise feed-forward networks) So it should be x2 = SubLayer(x) x2 = torch.nn.dropout(x2, p=0.1) x = nn.LayerNorm(x2 + x) You can find a good writeup at The Annotated Transformer .
