[site]: crossvalidated
[post_id]: 330409
[parent_id]: 328550
[tags]: 
First, it's worth recognizing that you cannot typically change sensitivity independently of specificity, and vice versa. This is the point of a ROC curve. Given the nature of the data generating process, and your specific data and model, you will always be stuck with some tradeoff between sensitivity and specificity. You would of course prefer to have 100% sensitivity and 100% specificity at the same time, but typically you can't. You can get better sensitivity, but at the expense of worse specificity, or better specificity, but at the expense of worst sensitivity. The ROC curve shows you the set of tradeoffs you are forced to choose between. (A couple of notes: 1. you can sometimes appear to gain on one dimension without loosing anything on the other because there is a gap in your dataset, but this is mostly illusory; 2. technically a ROC curve is sensitivity as a function of 1-specificity, plotting sensitivity vs specificity itself would be a reflected ROC curve.) At any rate, how could the apparent sensitivity and specificity change with prevalence? This is an issue where it helps to simulate and play with some data to see how this can work out in practice. Let's imagine that a model is fit to a fairly large dataset that has a particular prevalence, and a threshold is set on the x-axis 1 . Later, the performance of this test is computed with samples that have substantially different prevalences (and thus different x-values). The result is that the same model, using the same threshold will perform differently when applied to datasets with differing prevalences. library(caret) # we'll use these packages library(binom) # we'll use this function to convert log odds to probabilities lo2p = function(lo){ exp(lo)/(1+exp(lo)) } ##### training dataset for original model set.seed(734) # these make the examples exactly reproducible Nt = 1000 xt = rnorm(Nt, mean=5, sd=1) # this is the distribution of X lo = -1.386 + .308*xt # this is the data generating process pt = lo2p(lo) yt = rbinom(Nt, size=1, prob=pt) mt = glm(yt~xt, family=binomial) summary(mt) # ... # Coefficients: # Estimate Std. Error z value Pr(>|z|) # (Intercept) -1.16736 0.32794 -3.560 0.000371 *** # xt 0.24980 0.06429 3.886 0.000102 *** # ... # Null deviance: 1384.5 on 999 degrees of freedom # Residual deviance: 1369.1 on 998 degrees of freedom # AIC: 1373.1 ## determine threshold # prob(Y) = 50%, where log odds = 0, so: -coef(mt)[1]/coef(mt)[2] # 4.673159 threshold = 4.7 # a simple round number classt = ifelse(xt>threshold, 1, 0) tabt = table(classt, yt)[2:1,2:1] confusionMatrix(tabt) # yt # classt 1 0 # 1 346 279 # 0 175 200 # # Accuracy : 0.546 # ... # Sensitivity : 0.6641 # Specificity : 0.4175 # Pos Pred Value : 0.5536 # Neg Pred Value : 0.5333 # Prevalence : 0.5210 ##### high prevalence dataset from hospital set.seed(4528) Nh = 500 xh = rnorm(Nh, mean=6, sd=1) # a different distribution of X lo = -1.386 + .308*xh # but the same data generating process ph = lo2p(lo) yh = rbinom(Nh, size=1, prob=ph) classh = ifelse(xh>threshold, 1, 0) # the same threshold is used tabh = table(classh, yh)[2:1,2:1] confusionMatrix(tabh) # yh # classh 1 0 # 1 284 163 # 0 20 33 # # Accuracy : 0.634 # ... # Sensitivity : 0.9342 # Specificity : 0.1684 # Pos Pred Value : 0.6353 # Neg Pred Value : 0.6226 # Prevalence : 0.6080 ##### low prevalence dataset from outpatients set.seed(1027) Nl = 500 xl = rnorm(Nl, mean=3, sd=1) lo = -1.386 + .308*xl pl = lo2p(lo) yl = rbinom(Nl, size=1, prob=pl) classl = ifelse(xl>threshold, 1, 0) tabl = table(classl, yl)[2:1,2:1] confusionMatrix(tabl) # yl # classl 1 0 # 1 9 14 # 0 190 287 # # Accuracy : 0.592 # ... # Sensitivity : 0.04523 # Specificity : 0.95349 # Pos Pred Value : 0.39130 # Neg Pred Value : 0.60168 # Prevalence : 0.39800 ##### sensitivities binom.confint(346, 521, method="e") # method x n mean lower upper # 1 exact 346 521 0.6641075 0.6217484 0.704592 binom.confint(284, 304, method="e") # method x n mean lower upper # 1 exact 284 304 0.9342105 0.90022 0.9593543 binom.confint( 9, 199, method="e") # method x n mean lower upper # 1 exact 9 199 0.04522613 0.02088589 0.08411464 ##### specificities binom.confint(200, 479, method="e") # method x n mean lower upper # 1 exact 200 479 0.4175365 0.3729575 0.4631398 binom.confint( 33, 196, method="e") # method x n mean lower upper # 1 exact 33 196 0.1683673 0.1188206 0.2282441 binom.confint(287, 301, method="e") # method x n mean lower upper # 1 exact 287 301 0.9534884 0.9231921 0.9743417 Here are the sensitivities and specificities as a function of prevalences, with exact 95% confidence intervals: So what's going on here? Consider that a prototypical logistic regression might look something like the figure below. Note that all of the 'action' is taking place in the interval [4, 6] on the x-axis. Data below that will have very low prevalence, and the model will show poor discrimination and sensitivity. Data above that interval will have very high prevalence, but the model will again not discriminate well and will have poor specificity. To help understand how this could happen, consider the testing of Alanine transaminase to determine if the patient's liver is failing 2 . The idea is that the liver normally uses ALT, but that if the liver has stopped functioning ALT will be dumped into the blood stream. So if the level of ALT in a patient's bloodstream is above some threshold, that implies the liver is failing. If you draw a sample with a high prevalence of liver failure, you will be drawing a sample with high levels of ALT in the blood. Thus, you will have more patients above the threshold. Not everyone with high blood levels of ALT will have liver failureâ€”for some patients there will be some other cause. But those with liver failure should be being caught. This leads to higher sensitivity. Likewise, not all patients with normal levels of ALT have healthy livers, but a sample with low prevalence will have lower levels of ALT, and more patients will pass the test. Those whose livers aren't failing, but who have normal levels of ALT will be missed. This leads to lower sensitivity, but higher specificity. More generally, the whole idea of a medical test is that something or other is a correlate of a disease state that you might like to have direct measures of, but can't. Getting a measure of the correlate gives you insight into the disease state. A (potential) test where this isn't true would be of no value and wouldn't be used. Thus in practice, higher prevalence samples should have a distribution of the correlate with more abnormal values leading to higher sensitivity, and vice versa. (Note that the correlate does not have to be a cause of the disease; in the ALT example, it is an effect, in other examples, both the disease and the correlate could be effects of a common cause, etc.) 1. This is actually quite common in medicine. Consider that cholesterol should be Are 0-1 thresholds always equivalent to x-axis thresholds? , and Why are the number of false positives independent of sample size, if we use p-values to compare two independent datasets? 2. Please be aware that I am not a physician, and this example may well be badly botched. Ask an actual physician if you want accurate information about liver function, tests thereof, and related matters.
