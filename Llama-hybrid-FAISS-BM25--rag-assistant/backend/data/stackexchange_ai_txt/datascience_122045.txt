[site]: datascience
[post_id]: 122045
[parent_id]: 122040
[tags]: 
You seem to have mixed up two similar names. The normal (Gaussian) distribution, which is a “bell curve” A normalized distribution that has been centered and scaled by subtracting the mean and the dividing by the standard deviation These are not the same. Few regression techniques make distribution assumptions about the features, and neural networks are among those that kind of do not. Gaussian distributions of features are not important for neural networks. If you plot the distributions of pixels in MNIST images, I suspect there would be a fair amount of non-normality despite deep learning models being near-perfect on MNIST (though I have not done this plotting, at least not in a long time). In fact, neural networks can handle categorical features, which are guaranteed not to be normal. Also, you do not theoretically need to do feature normalization in deep learning. Where this becomes useful has to do with the fact that computers need to calculate network optima in reasonable amounts of time, and feature scaling helps with the numerical optimization that must be performed, since neural networks do not have the kind of closed-form mathematical solutions for the optima like an ordinary least squares linear regression has.
