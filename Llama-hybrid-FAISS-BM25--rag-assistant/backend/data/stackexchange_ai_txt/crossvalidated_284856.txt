[site]: crossvalidated
[post_id]: 284856
[parent_id]: 
[tags]: 
Significance test for cross-entropy of 2 classifiers

Firstly, I have seen this post and my question is different. I have trained 2 probabilistic classifiers on a task to correctly predict one out of $O$ output labels ($O$ is around 40 labels). I then tested their performance on a test data set which contains $N$ samples. I have a strange result where one classifier has an accuracy of 0.4 and average cross-entropy loss of $\approx 2.4$ and the other has an accuracy of 0.38 and average cross entropy loss of $\approx 1.8$. I can run accuracy based significance test like the McNemar Test but I want to run a test on the average cross-entropy loss. What is the correct way to show 1.8 is significantly better/different from $2.4$?
