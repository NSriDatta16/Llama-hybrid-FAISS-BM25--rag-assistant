[site]: crossvalidated
[post_id]: 239676
[parent_id]: 238433
[tags]: 
This answer is based on a general knowledge of hyperparameters (most specific with experience on SVM hyperparameters) and not any specific knowledge on gradient boosting or the xgboost implementation in particular. Hyperparameters are not in general independent, that is one can fix all but one of them, search for the best choice for the remaining hyperparameters, and repeat the process. The OP is not doing this, but he/she is assuming that limiting the range of one number of interactions, and searching for the others will result in the same solution one would get without the limit on the interactions. This is not likely to be true given my experience. If search time is an issue I would reduce the number of folds in the grid search. The larger the dataset, lower is the variance on the CV estimation, so you could probably get the same or very similar hyperparameters with 3-fold, and I would go as low as 2-fold. I have some experiments that 2-fold is OK for SVM, with datasets above 1k.
