[site]: crossvalidated
[post_id]: 125937
[parent_id]: 
[tags]: 
What does it mean to compute eigenvectors of a covariance matrix if the data were not centered first?

Say $\mathbf{X} \in \mathbb{R}^{n \times p}$ and $\boldsymbol{\Sigma} = \frac{1}{n}\mathbf{X}'\mathbf{X}$. The eigenvector decomposition of $\boldsymbol{\Sigma}$ gives $\boldsymbol{\Sigma} = \mathbf{P}\boldsymbol{\Lambda}\mathbf{P}'$ where $\mathbf{P}$ are the eigenvectors and $\boldsymbol{\Lambda}$ is a diagonal matrix with eigenvalues on the diagonal. The singular value decomposition of $\mathbf{X}$ gives $\mathbf{X}=\mathbf{U}\mathbf{S}\mathbf{V}'$, where $\mathbf{U}$ contains the left eigenvectors of $\mathbf{X}$, $\mathbf{V}$ contains the right eigenvectors of $\mathbf{X}$, and $\mathbf{S}$ is a diagonal matrix with singular values on the diagonal. The eigenvalues of $\boldsymbol{\Sigma}$ will equal the squares of the singular values of $\mathbf{X}$ only if $\mathbf{X}$ was centered (the mean of $\mathbf{X}$ was removed). If I plot the cumulative variance explained by the principal components, the plot will (obviously) be different depending on whether $\mathbf{X}$ was centered first. I'm trying to understand what the eigenvectors mean if I don't center $\mathbf{X}$ prior to doing the decomposition. I'm working on an naive algorithm that involves performingPCA online. I need to recompute eigenvectors after observing each new data point, and my results are much worse if I recenter the data set as I observe new data points. However, if I pretend to have seen all the data at first, center the data, and compute the eigenvectors of the entire centered data set, my (offline) performance is much better than if I had not centered my data. Could someone please provide some intuition as to what centering the data set does and how this might be affecting my algorithm performance? Also, what is the difference in the singular values computed before and after centering the data?
