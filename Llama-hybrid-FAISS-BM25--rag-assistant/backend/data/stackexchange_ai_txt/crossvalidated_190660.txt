[site]: crossvalidated
[post_id]: 190660
[parent_id]: 
[tags]: 
Applying an uncertainty to a prediction

I am estimating/predicting the numbers of students who will pass on an exam at a school this year. My method is very simple: Each student give me a guess of their own grade one month before the test. This way, the estimated mean of the passing percentage turns out to be 95 %. Now, I would like to give a measure of uncertainty (like P10, P50, P90) in addition to this value. Last year the test was done at several schools in my area. It turns out that the percentages of students passing the test seem to be Weibull distributed with a mean of 92 % and a standard deviation of 3 %. How wrong would it be to apply this standard deviation - and the distribution - to my estimate of 95 %? Is it completely, utterly wrong, or could this be okay to some pragmatic extent in the absence of other alternatives? For an estimate of the mean on my school this year I would trust my own prediction (95 %) more than the one from several schools last year (92 %) because there are some parameters like size and quality of lectures that would vary. Could the standard deviation still be of some use? I guess it would not if we are strict, but I want to be pragmatic as well. Any response is highly appreciated.
