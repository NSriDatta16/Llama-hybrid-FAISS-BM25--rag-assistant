[site]: crossvalidated
[post_id]: 518402
[parent_id]: 513754
[tags]: 
Note first that there are many different R-squared estimators. For an overview, see the references below. Disclaimer: I am the author of the third one. However, all of them indeed require that $p>N$ , as otherwise, it is impossible to fit the regression model, in the sense that no unique regression coefficients exist. If you for example try to fit a model with $11$ predictors and $10$ samples, using the lm function in R, it will not provides estimates for two predictors and also no standard errors for any coefficient. While you technically can get an R-squared from such a model, it is guaranteed to be $1$ and thus meaningless, as will any adjustment of this value.See also the comment by whuber. You can of course obtain a prediction function for the $p>N$ setting using a different technique, for example, ridge regression. However, all the adjusted R-squared formulas only work for $R^2$ estimates as obtained by ordinary least squared regression. You could fall back to predictive $R^2$ for this case but this estimate something else than adjusted- $R^2$ , see my answer to another question: https://stats.stackexchange.com/a/518400/30495 References $[1]$ : Shieh G (2008): Improved shrinkage estimation of squared multiple correlation coefficient and squared cross-validity coefficient. Organizational Research Methods, 11(2): 387-407 ( link ) $[2]$ : Yin P, Fan X (2001): Estimating $R^2$ shrinkage in multiple regression: A comparison of different analytical methods. The Journal of Experimental Education, 69(2): 203-224 ( link ) $[3]$ : Karch J (2020): Improving on adjusted R-squared. Collabra: Psychology (2020) 6 (1): 45. ( link )
