[site]: crossvalidated
[post_id]: 335837
[parent_id]: 333556
[tags]: 
TLDR – the problem is underdefined Aim The details provided in the question can only lead to a relative balance of errors. Simply rearrange the equation and $ P(TypeII Error) ≈ 3P(TypeI Error)/240000$. This is not what is wanted or needed, rather it is to know what threshold for mean that would be used as a cut off. I.e. what effect size minimises the cost function of your situation. The cost of a typeI and typeII error have been defined, but not of the test to determine these. Why does this matter? The effect size that you can reliably measure depends on the acceptable type I error and the acceptable type II error. Below is a typical sample size calculator ( Box 1 in https://academic.oup.com/ndt/article/25/5/1388/1842407 , open access) (OP indicated a generic answer was preferred so I have used a formula for normal distributions as this s probably more widely used, the principles are the same for other distributions types) $$n = 2*((a + b)^2δ^2)/(μ_1−μ_2)$$where a is the z multiplier to achieve the desired $\alpha$ level (1.96 for $\alpha$ = 0.05), while b is the equivalent for $\beta$. $\sigma$ is the standard deviation of the test result and $(\mu_1-\mu_2)$ is the effect sizem, we can simplify this to the threshold since an unbiased average would be 0 so we can drop $\mu_2$). $n$ in this specific example is the number of repeated trials to be carried out on an individual die. Rearrange this to solve for the effect size: $$ \mu_1 = 2*((a + b)^2δ^2)/(n)$$ What does this equation tell us? The issue of type I and type II error trade-off only applies when sample size and effect size are set to a constant. You can push both $alpha$ and $beta$ down ever lower and detect the same effect size by increasing the number of repetitions. But repeating a test increases the cost. Since your costs link type I and II errors you can replace $a+b$ with $Z_\alpha + Z_{3\alpha/240000\alpha}$ Then you will need to either: 1) define an upper limit on what you are willing to pay for testing the dice, this will then leave you with two variables to solve for (effect size and alpha) 2) solve for repetitions, alpha and effect size simultaneously You either need to define even more limits up front or use some post-hoc decision criteria to decide which balance of the possible outcomes is best suited. If this were a reflection of a real world example, you would want to do a more detailed cost/benefit assessment (non-comprehensive list). The cost of repetitions for your measure of fairness (this will guide your sample number, if it is say £1 per 10 reps, then it will cost 33x the cost of the dice to do 1000 reps). The cost of failing to detect a bias (you give this as £240000 in you example for legal expenses, but in the real world the loss of earnings due to reputation damage and many others may be worth throwing in) The cost of rejecting a fair dice (given as £3 in this case, in more generic cases there may be other issues to consider, including revenue boost due to enhanced reputation) If you want estimates of absolute costs and risks you would need to define absolute values for how the scenario would be used in the real world. E.g. how many dice do you envisage deploying in the real world and how many times will each be used? A useful reference: https://www.graphpad.com/guides/prism/7/statistics/stat_sample_size_for_which_values_o.htm?toc=0&printWindow
