[site]: crossvalidated
[post_id]: 228344
[parent_id]: 
[tags]: 
Why a linear regression cannot obtain a zero classification error on a predictor that perfectly classifies the target?

Let us say that the predictor X perfectly predicts the label Y such that Y = 1 if X >= c and Y = 0 if X This question was asked in Machine Learning course on Coursera and according to them, zero classification rate isn't possible. I can't see why is this the case. Of course, I am assuming that this condition is true forever and not just for the training set (no such mention in the question) Edit:- Let me break the problem down so that you know what I am thinking - According to the question, there is a variable X and a variable Y such that if X >= C (some fixed value) then Y = 1 and if X Now, according to the definition of classification error - classification error = (no. of misclassified objects)/(total no. of objects) By this definition, it means that Y was supposed to be 1 when it was classified as 0 (or the other way around). This means that there exists a value C such that X >= C but the hypothesis h(x) classified it as 0. (Am I right till here?) But according to the question, this can never be the case. Now, here is the part where I am getting confused - is the question only talking about training data? Or is the question actually stating the property of the entire population? If the question is talking only about training data, then the answer is understandable. However, if the property of the predictor is true throughout the population, then I am unable to understand it. Please let me know if you need more details.
