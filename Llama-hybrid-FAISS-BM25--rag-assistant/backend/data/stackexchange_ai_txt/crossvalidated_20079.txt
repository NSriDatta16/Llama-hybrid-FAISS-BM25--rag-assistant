[site]: crossvalidated
[post_id]: 20079
[parent_id]: 
[tags]: 
The effect of skewed continuous predictors in a binary logistic regression model

I am analyzing data with a binary outcome and a variety of continuous and categorical (including dichotomous) predictor variables. My approach is to perform a binary logistic regression and to treat any predictor with more than 20 unique values as continuous. Several arguments against categorization, especially well-documented on Frank Harrell's site are a good reason to not categorize. However, at a recent meeting where I discussed my analyses approach, a faculty suggested that I would get more accurate risk estimates for the data if I categorized the variables which have a skewed distribution and outliers. Their logic was that the tail of the skewed distribution and the outliers in that tail will have a detrimental effect on the risk estimates generated by the logistic regression and that categorization will address this by erasing the effect of the tail and the skew. I have several predictor variables that definitely have skewed distributions and some outliers. Is the claim that a variable with a skewed distribution (with outliers) is more likely to produce inaccurate risk estimates compared to the categorized version of the same variable, true? How do skews and outliers in the tail affect logistic regression estimates?
