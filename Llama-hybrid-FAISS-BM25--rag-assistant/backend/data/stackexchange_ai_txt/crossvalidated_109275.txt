[site]: crossvalidated
[post_id]: 109275
[parent_id]: 109274
[tags]: 
If I understand your question correctly, you have multiple response variables (y, not x), and you want to know if it makes more sense to use multivariate multiple regression on the responses directly, or to extract the first principal component of the y's first, and then regress on that. As others have said, you are likely to get a much more rigorous answer on https://stats.stackexchange.com/ , and I encourage you to try that, but hopefully this will help a little bit. IMO the answer is: use mlm. There are two main reasons. First, you are likely to loose information about structure if you run PCA first, and second, the goodness-of-fit tests available for an mlm model are more robust than those available for a simple lm model (which is what you would end up with in the PCA approach). To illustrate the first problem (in R), consider the following highly contrived dataset: set.seed(1) # for reproducible example df In this dataset, we have two responses (y1 and y2), and three predictors (x1, x2, and x3). Both y1 and y2 depend strongly on x3 (by design); y1 also depends on x1 but not x2, and y2 depends on x2 but not x1. Because both y1 and y2 depend on x3, they are highly correlated. If we run mlm on this dataset, we see the dependencies clearly. # multivariate multiple regression fit.mlm |t|) # (Intercept) 1.3142 16.1056 0.082 0.935 # x1 0.5874 0.1849 3.177 0.002 ** # x2 -0.1303 0.1847 -0.705 0.482 # x3 3.0592 0.1857 16.470 |t|) # (Intercept) 17.3293 14.8825 1.164 0.2471 # x1 -0.2371 0.1709 -1.387 0.1685 # x2 0.4432 0.1707 2.597 0.0109 * # x3 3.0696 0.1716 17.884 So from this we can see clearly that x1 is important in explaining the variation in y1, and x2 is important in explaining the variation in y2. But look what happens when we run PCA first: # principal components analysis pc |t|) # (Intercept) -242.6575 15.4029 -15.754 This result suggests that neither x1 nor x2 are important, which is false by design! In other words, we have lost the structural information about y1 and y2. The second reason, regarding robustness of the goodness-of-fit tests is complex (there is a pretty good treatment here , while the classic paper on this is here ). To motivate the treatment, consider two extreme cases: In the first case, suppose the y`s are completely uncorrelated (orthogonal). In that case the principal components will just be the original y's, and PCA will contribute nothing. In the second case, suppose the y's are perfectly correlated (colinear). In that case, the y's in a sense represent replicate measurements for each combination of the x's. The goodness of fit tests for mlm take into account these addition degrees of freedom, whereas an approach that uses PCA first essentially discards them. So in this case as well, you are better off with mlm.
