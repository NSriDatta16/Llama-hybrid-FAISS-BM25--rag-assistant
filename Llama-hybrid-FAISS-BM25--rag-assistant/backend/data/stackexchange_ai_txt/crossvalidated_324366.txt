[site]: crossvalidated
[post_id]: 324366
[parent_id]: 
[tags]: 
Clarification on the Math of Logistic Regression

I am trying to get a better understanding of the math behind logistic regression. Logistic regression is looking to give a prediction based on data likelihood. From my understanding it is making a posterior prediction. Suppose we have a two-class model. So this becomes the equation \begin{align} p(C_1|x) &= \frac{p(x|C_1)p(C_1)}{p(x|C_1)p(C_1)+p(x|C_2)p(C_2)}\\ &=\frac{1}{1+\exp(-a)} = \sigma(a) \end{align} where $a= \ln \frac{p(x|C_1)p(C_1)}{p(x|C_2)p(C_2)}$ My questions are what does $a= \ln \frac{p(x|C_1)p(C_1)}{p(x|C_2)p(C_2)}$ intuitively mean? Does it imply that the $\theta x$ of our model, is trying to estimate the ratio in $a$? And why in the first step is the equation equivalent to the same as the naive bayes classifier; how does logistic regressor distinguish itself from the naive bayes if the background math is the same
