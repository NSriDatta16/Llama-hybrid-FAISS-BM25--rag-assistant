[site]: datascience
[post_id]: 26001
[parent_id]: 
[tags]: 
Accelerate deep learning model training on several GPUs

I have a deep learning model that can be trained in one GPU, however, is very slow. Is there a way to accelerate the training by parallelizing it across several GPUs? How would be the training process? I can use any framework (pytorch, tensorflow, etc). And I know they accept multiple GPU and how to do it. I am more interested in how to spread, conceptually, a single model between GPUs to gain performance. I do mostly NLP models with RNN but I am also interested in CNN.
