[site]: crossvalidated
[post_id]: 636804
[parent_id]: 636802
[tags]: 
I don't think this is a proof, but just from playing around with Bayes' rule here's what I got: $$ P(\theta|x_{1}, x_{2}) = \underbrace{P(x_{1}, x_{2}|\theta)}_{\text{Likelihood}} \, \underbrace{P(\theta)}_{\text{Prior}} \, c^{-1}_{1} $$ where $c_{1} = P(x_{1},x_{2})$ is the normalization constant. But also, $$ P(\theta|x_{1}) = P(x_{1}|\theta) \, P(\theta) \, c^{-1}_{2} \quad \Rightarrow \quad P(\theta) = c_{2} \, \frac{1}{P(x_{1}|\theta)} \, P(\theta|x_{1}) $$ where $P(\theta|x_{1})$ is the posterior of $\theta$ after only updating with $x_{1}$ . Now you can plug this into the first equation: $$ P(\theta|x_{1}, x_{2}) = \frac{c_{2}}{c_{1}} \, \frac{P(x_{1},x_{2}|\theta)}{P(x_{1}|\theta)} \, P(\theta|x_{1}) $$ But the second term is just conditioning the joint likelihood: $\frac{P(x_{1},x_{2}|\theta)}{P(x_{1}|\theta)} = P(x_{2}|x_{1},\theta)$ . So overall, $$ P(\theta|x_{1}, x_{2}) = c^{-1}_{3} \, P(x_{2}|x_{1},\theta) \, P(\theta|x_1) $$ So the Bayesian updating in the second step (with $x_{2}$ ) is just Bayes' rule applied to a prior that is already conditioned on $x_{1}$ , i.e. the posterior from the first updating.
