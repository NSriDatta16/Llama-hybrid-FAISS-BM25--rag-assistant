[site]: datascience
[post_id]: 68163
[parent_id]: 68162
[tags]: 
What helps the model more, keeping all features or removing correlated ones? There is some theory about it but in the end Machine Learning is try and error. You should give it a try with all features and then doing a feature selection to see if you are able to improve your model. What works for some models doesnÂ´t necessarily have to work for the rest of the models. If you want to select which features help your model you could do otherwise, instead of having all and removing, starting with one and adding features and only keeping them in case that it boosts the performance of the model. There are cases when you add a feature and the performance of the model drops. There are a lot of ways in which we can think of feature selection, but most feature selection methods can be divided into three major buckets. From this source Filter based: We specify some metric and based on that filter features. An example of such a metric could be correlation/chi-square. Wrapper-based: Wrapper methods consider the selection of a set of features as a search problem. Example: Recursive Feature Elimination Embedded: Embedded methods use algorithms that have built-in feature selection methods. For instance, Lasso and RF have their own feature selection methods.
