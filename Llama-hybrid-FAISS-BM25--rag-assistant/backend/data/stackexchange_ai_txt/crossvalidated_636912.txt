[site]: crossvalidated
[post_id]: 636912
[parent_id]: 
[tags]: 
question on the model number for the validation test dataset

I'm doing the project related to the glioma research. It would be a good strategy to have the training, validation, and testing but it is not fit in the data number I got.((12 IDH mutant (180 MRI) and 7 Wild type (105 MRI) for training Total dataset number : 285 MRI) that had all multiparametric imaging (15 different mir images) ) SO I TRED WITH THE SPLITNG DATASET INTO THE TRAINING AND TESTING SET (70: 30). Then the dataset goes to 10 mut 5 wild for the training and 2 mut and 2 wild for test dataset. And i was wondering whether the number of the test data is not optimal for the validation purpose? Since my data set had the imbalance dataset, the optimal way to do the feature selection is to perform different methods of feature selections among the supervised methods of wrappers , filters and embedded? I saw the review articles related to my search on using the radiomic with the machine learning, but the methods are vary that I don't know whether the methods used in each article will be fit for me or not. In this case how would you plan for running the machine learning? Go over each features selection methods and try with different method for the classification until get a optimal result? Finally, I followed the few methods with the using the balancing tech according to the aritlce (http://when%20is%20resampling%20beneficial%20for%20feature%20selection%20with%20imbalanced%20wide%20data/?), but did not got the improved performance on classifying the minor class. Therefore, does result is coming simply from having the small number of dataset for the validation set?
