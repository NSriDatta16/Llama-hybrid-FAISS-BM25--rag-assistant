[site]: crossvalidated
[post_id]: 475016
[parent_id]: 475010
[tags]: 
I'll tell you what I know/have heard about it. There's probably other analysis out there I haven't seen. I think multi-head attention was introduced in the transformer network paper . Their justification/explanation is: Another paper on a modified Transformer architecture questions this understanding though:
