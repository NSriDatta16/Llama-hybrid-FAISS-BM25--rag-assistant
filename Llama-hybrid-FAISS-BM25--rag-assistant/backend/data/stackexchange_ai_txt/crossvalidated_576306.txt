[site]: crossvalidated
[post_id]: 576306
[parent_id]: 576156
[tags]: 
Your question indicates a number of common errors in regression modeling. Omission of the intercept in order to have fewer parameters and less overfitting Particularly in an imbalanced problem where you want the model to be skeptical about membership in the minority class, the intercept is important. I understand the idea of model parsimony in hopes of avoiding overfitting, but the intercept plays a unique role in the model by affecting the outcome in the same way no matter what the features are. No other parameter in the model can do this, so if you're going to do away with a parameter, that isn't the one you want to lose unless you have an excellent reason for doing so (such as knowing that the outcome is zero when the features all are zero). Pure linearity You don't state anything about this in the question, but you are allowed to have nonlinear features like splines in your generalized linear model. By allowing the probability to increase, decrease, and then increase again (for instance) as a feature increases, you allow for better predictions than you would get from forcing the straight-line fit that I suspect you are using. Parsimony In one of his videos, the same Frank Harrell as in the comments mentions that parsimony is the enemy of predictive accuracy. My answer here gets into some of why that might be the case and links to additional material (by Frank Harrell). If you're worried about overfitting but want to have a lot of variables, interactions, and nonlinear features (e.g., splines), use some kind of penalized estimation, such as a ridge penalty. This fights overfitting without getting rid of any of the parameters. All of these and more are discussed in Frank Harrell's Regression Modeling Strategies textbook. Finally, by having a good model that considers all of these issues, you can, if you must, apply a threshold to make dichotomous decisions based on the known costs of mistakes, and that is how you should pick your threshold. I would argue that, if you don't know the costs of your mistakes, you have no business making classifications and should only give predicted probabilities. In the extreme, where it is unacceptable to mistake a $0$ for a $1$ or a $1$ for a $0$ , the best dichotomous decision might be to classify every instance as one class so that you never make the unacceptable mistake. Additionally, don't be surprised when you find yourself unable to make accurate dichotomous decisions for instances whose predicted probabilities are near the threshold. The best decision in such a situation might be no decision. This is related to what Kolassa means when he writes that "a cost optimal decision might also well include more than one threshold!" If I may pontificate at the end of my answer, your post suggests that you face a common issue for data scientists to face. You don't have to knuckle under to every bad idea someone has. You're the expert; act like it. This doesn't mean being an arrogant punk, but you're allowed to explain why someone is wrong. This is scary the first time you have to do it, and it is when you are most valuable as a statistician or regression expert. (It also get less scary as you do it more and more.)
