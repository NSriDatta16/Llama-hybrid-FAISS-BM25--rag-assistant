[site]: crossvalidated
[post_id]: 479851
[parent_id]: 479359
[tags]: 
No, using cross validation does not give the green light to run exhaustive searches over arbitrarily many hyperparameters. The usual goal of hyperparameter tuning (a.k.a. model selection) is to maximize generalization performance. Cross validation can be used for model selection because it provides a means to estimate generalization performance. However, because only finite data are available, it's possible for the model selection algorithm to overfit the validation set. That is, a particular choice of model or hyperparameters may happen to yield good performance on the validation set, but generalize poorly to unseen data from the underlying distribution. Overfitting the validation set may result in selecting models that either overfit or underfit the training data. Overfitting during model selection is more likely when the validation set is smaller, or when searching over many models or hyperparameters. Obviously, this issue can be mitigated by increasing the amount of validation data (which includes using cross validation as opposed to simple holdout/split sample validation). Otherwise, if no further data are available, various mitigation strategies can be employed including regularization, early stopping, fully Bayesian approaches, and ensemble methods. For more information, see: Cawley, G. C., & Talbot, N. L. (2010). On over-fitting in model selection and subsequent selection bias in performance evaluation. The Journal of Machine Learning Research, 11, 2079-2107.
