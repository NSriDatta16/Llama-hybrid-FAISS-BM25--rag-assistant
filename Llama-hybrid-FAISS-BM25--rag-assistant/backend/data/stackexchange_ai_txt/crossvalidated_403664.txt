[site]: crossvalidated
[post_id]: 403664
[parent_id]: 50210
[tags]: 
Though I agree the with the theoretical explanations posted here , in practice, having a too large number of trees is a waste of computational power and makes the model objects uncomfortably heavy for working with them (especially if you use to constantly save and load .RDS objects). Because of that, I think if we want models to be adequate we have to find somehow the minimum necessary number of trees that allow for a stable performance (and then "let the asymptotic behavior of LLN do the rest"). Perhaps if you are a very experienced statistician or if you are always working on similar problems you can use some rule of thumb (say 1000 or 10000 trees). But if your work requires you to adapt to a variety of modelling tasks, you'll end up needing some calibration method that allows you for finding an adequate and inexpensive number of trees. For this purpose, you could just download the source code of the method from here and then rewrite it to create a custom method that adapts to your needs. Feel free to use the following example: customRF $mtry, ntree=param$ ntree...), predict = function(modelFit, newdata, submodels = NULL) if(!is.null(newdata)) predict(modelFit, newdata) else predict(modelFit), prob = function(modelFit, newdata, submodels = NULL) if(!is.null(newdata)) predict(modelFit, newdata, type = "prob") else predict(modelFit, type = "prob"), predictors = function(x, ...) { ## After doing some testing, it looks like randomForest ## will only try to split on plain main effects (instead ## of interactions or terms like I(x^2). varIndex $forest$ bestvar))) varIndex 0] varsUsed $forest$ ncat)[varIndex] varsUsed }, varImp = function(object, ...){ varImp $type == "regression") varImp y) if(all(retainNames %in% colnames(varImp))) { varImp $classes, tags = c("Random Forest", "Ensemble Model", "Bagging", "Implicit Feature Selection"), sort = function(x) x[order(x[,1]),], oob = function(x) { out type == "regression") c("RMSE", "Rsquared") else c("Accuracy", "Kappa") out }) After defining this custom method you only have to call it from train(method=customRF) and both mtry and ntree will be calibrated depending on your trainControl() specifications.
