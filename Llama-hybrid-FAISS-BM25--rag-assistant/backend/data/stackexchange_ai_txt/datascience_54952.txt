[site]: datascience
[post_id]: 54952
[parent_id]: 48920
[tags]: 
The name "Graph Convolutional Neural Network" is a bit misleading, as no "traditional" convolutions (like in the context of CNNs) take place at all. You are correct that it doesn't really make sense to perform convolutions on the adjacency matrix of a graph. The thing that GCNNs have in common with CNNs is that there is a concept of a "local neighbourhood" which is exploited. In CNNs, this neighbourhood consists of the surrounding pixels of each pixel. A linear transformation (discrete convolution) is applied to the pixels in the neighbourhood, before being passed through a nonlinearity. In GCNNs, however, the neighbourhood for each node is given by the set of other nodes that share edges with the node. The representations of each node are averaged with the representations of the nodes in their neighbourhoods before having a linear transformation applied and subsequent nonlinearity. The method for performing the local averaging can be done using some matrix arithmetical tricks with the adjacency matrix, which is perhaps what you got confused about. In the simplified version, they remove the nonlinearity step which vastly simplifies the computation of the hidden representations and the linear transformation step.
