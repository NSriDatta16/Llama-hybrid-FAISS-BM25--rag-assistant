[site]: crossvalidated
[post_id]: 572125
[parent_id]: 571593
[tags]: 
If I try to reproduce the data I get this as the sample: Because these are many cycles (1000), the fitting of this with a wave function requires a very small margin of error in the parameter $u$ . When a small difference is made, then this has already a large influence on the position after 1000 cycles. Below you see this with an example where every 20 time-ticks a sample of 5 is taken. In the first image with the green curve the frequency is changed by a factor 5% and this has little effect for small times $t$ on the left but a large effect on the right. For a few special ratios in the frequency, like 1.25, which is the red curve below in the second image, some of the points in the wave function coincide and you get a relatively higher likelihood. This makes that you get multiple peaks in the likelihood function. The consequence is a likelihood and prior that is very sharp. Your proposal function has a much wider range and so you are gonna sample mostly in the region with non-zero probability. What you need is a first good estimate of the $u$ and the bandwidth of the prior. Then you can get a better function that proposes new samples within the range. Is still have to make the MCMC part for this question, but how do you compute the prior? I can not find this easily in your code. You have this function function ret = acceptance(u, u_new) but it seems to compare exp(u_new - u) , which is the likelihood ratio but not the ratio of the posterior. It might possibly be easier to use the variable $X(t)-X(t-1)$ (and since you observe batches of 5 consecutive times you get batches of 4 of those differences). Then you get that a small change in $u$ has not such a large effect on the likelihood. It may also be more realistic in cases that are not so steady. With your current likelihood function you are saying that the ground truth $e^{iu4\pi/\lambda t}$ has no variability after hundreds of cycles and that the process is very steady. So currently you assume that the error distribution is the same after hundreds of seconds as after 1 second. R-code ###### ###### set.seed(1) ### Parameters for sampling ### ### Nt = N*S numbers are the ground truth ### every N numbers there are M numbers sampled N = 100 M = 5 S = 10 Nt = N*S ### Parameters for signal dt = 0.1 u = 1 lambda = 1 ### Parameters for noise SNRdb = 20 SNR = 10^(SNRdb/10) sigma = 1/sqrt(SNR) ### create variables t = c(1:Nt)*dt ### time variable Z = exp(1i*4*pi/lambda*u*t) ### signal sigma = sqrt(sum(abs(Z)^2)/(Nt * SNR)) epsilon = 1/sqrt(2) * (rnorm(Nt, 0, sigma) + 1i * rnorm(Nt, 0, sigma)) ### noise X = Z + epsilon selection = rep(N*c(1:S),each = M) + rep(1:M,S) - M ### id's of S times M consequitve id's with gaps of N plot(X) ### view the variable loglikelihood = function(uf,t,X) { x_fit = exp(1i*4*pi/lambda*uf*t) residual = x_fit-X error = sum(Im(residual)^2 + Re(residual)^2) return(error) } loglikelihood = Vectorize(loglikelihood, "uf") posterior = function(uf,t,X) { prior = dnorm(uf,1.3,10^3) exp(-1*loglikelihood(uf,t,X))*prior } posterior = Vectorize(posterior, "uf") u = seq(0.7,1.3,0.0001) plot(u,loglikelihood(u,t[selection],X[selection]), log = "", type = "l") u = seq(0.7,1.3,0.00001) plot(u,posterior(u,t[selection],X[selection]), log = "", type = "l", ylab = "posterior(u,X) [not scaled to integrate to 1]")
