[site]: datascience
[post_id]: 54124
[parent_id]: 54120
[tags]: 
Okay, so what I understand is you just have a list of words and want to get word vectors for those. You are correct that you cannot train a word2vec model as it requires a corpus. But what you can do is use a pre-trained model (word2vec or glove). I suggest you use word2vec as gensim has a pretty simple implementation. You can download Googleâ€™s pre-trained model here . And then you can use the following code to get word_embed for a given word_list . import gensim model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True) vocab = model.vocab.keys() word_embed = {} for word in word_list: if word in vocab: word_embed.append(model[word]) Also, you'll have to apply some pre-processing to your word list so that you can get maximum matches from the pre-trained embeddings (like removing the etc.) And if a word is still not found in the pre-trained embeddings you can either initialize it randomly or take an average of the embeddings.
