[site]: crossvalidated
[post_id]: 265044
[parent_id]: 265037
[tags]: 
I think it's still very much an open question of which distance metrics to use for word2vec when defining "similar" words. Cosine similarity is quite nice because it implicitly assumes our word vectors are normalized so that they all sit on the unit ball, in which case it's a natural distance (the angle) between any two. As well, words that are similar tend to have vectors be close to each-other, especially in length, which means that their magnitudes are comparable, and so again cosine distance becomes natural. In reality this is much more complex, because word2vec does not explicitely require that the embedding vectors all have length 1. Indeed there is work that shows that there is important information hidden in the lengths of vectors, so that L2 distance can be used. See here for example: https://arxiv.org/pdf/1508.02297v1.pdf
