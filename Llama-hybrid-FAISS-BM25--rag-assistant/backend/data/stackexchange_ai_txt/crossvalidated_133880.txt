[site]: crossvalidated
[post_id]: 133880
[parent_id]: 133876
[tags]: 
You say that you have the same intuition (that the coefficients shouldn't change) for a linear regression, so I'm going to answer your question in that setting, because linear regression is a bit easier to visualize. I think the thing that is causing your mistaken intuition here is that you're imagining that duplicating data points doesn't change the scatterplot. But it does! It might not be easily visible, depending on the kind of scatterplot you use, but that's why we have things like jitter . The phenomenon of different scatterplots looking the same because the datapoints are directly on top of each other is a problem (called "overplotting"), because it makes fundamentally different data sets look the same. The regression algorithm doesn't "see" the scatterplot itself, it sees the underlying data points. And when you replicate some of those data points, it makes them more strongly represented in the underlying data, so the regression algorithm treats them as more important to "get right." (Basically, you can think of each occurrence of a data point as pulling the regression line towards it with the same force--so if you have two data points at a given spot, they will pull the line towards them twice as hard.) I'll give a visual example, using jitter to make clear what a regression actually "sees." First, here's a dataset consisting of one ascending and one descending sequence: It's symmetrical, so the line of best fit would just be flat through 5.5. But what happens if I replicate the descending sequence 10 times, and use jitter to make the scatterplot look visually like it looks like to the underlying algorithm? Now it looks a lot more like the ascending sequence is just outliers, right? And the trend line is clearly decreasing. (It works exactly the same way with logistic regression, it's just harder to make it clear from a plot.)
