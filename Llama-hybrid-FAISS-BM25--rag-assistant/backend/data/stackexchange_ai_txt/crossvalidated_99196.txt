[site]: crossvalidated
[post_id]: 99196
[parent_id]: 99193
[tags]: 
Well, random forest uses bagging which is specifically designed to reduce problems with overfitting. Ensemble methods like bagging and CV are both ways to avoid overfitting. Cross-validation can be used in random forest modelling various ways - e.g. to find the optimal number of trees - but I don't know anywhere it has to be used. For example, to measure the out-of-sample performance I think you can use the out-of-bag error. I suppose the resulting question is ' can overfitting - while reduced in scope - still be a problem if you don't use cross-validation '? I'm not 100% certain of the answer to that, but searching around $^{[1]}$ it looks like overfitting might still be a potential issue (BMA and bagging are both forms of model averaging, the problem could easily carry over to bagging and so perhaps to random forests). In that case, some other approach - such as cross-validation - might be needed. (Cross validation isn't the only other way to reduce/avoid overfitting of course, which may have been the underlying point of the question.) [1] Domingos, P., (2000) "Bayesian Averaging of Classifiers and the Overfitting Problem" Proceedings of the Seventeenth International Conference on Machine Learning , pp.223-230
