[site]: crossvalidated
[post_id]: 43656
[parent_id]: 43648
[tags]: 
Let's take a step back. Why would you want to select input features and why would you not take as many hidden nodes as you can? One reason is speed. The more input features and hidden nodes you take, the longer training takes. I think that this is nowadays negligible due to fast computers. If this is the case for you, I can give you tips on how to speed up MLPs. Another reason is to control overfitting. However, the neural network commmunity has come up with more efficient ways to control overfitting, where the most popular is weight decay. Let's say you optimise the loss $L$ with your neural net, then you will instead optimise $L + \lambda \sum_i w_i^2$ where the $w_i$ are all your weights (not biases) in your network. The $\lambda$ should be tuned to give best performance on a held out validation set. Other ways of doing that is to use dropout or early stopping. Searching for the latter term here will give you good answers. My advice is to use all input features, a big number of hidden nodes and use only weight decay. If you code everything yourself, use dropout instead of weight decay since it's much better.
