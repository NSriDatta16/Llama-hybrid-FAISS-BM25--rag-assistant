[site]: crossvalidated
[post_id]: 319614
[parent_id]: 319604
[tags]: 
What you assume in a linear regression model is that the error term is a white noise process and, therefore, it must be stationary. There is no assumption that either the independent or dependant variables are stationary. However, consider the following simple linear regression model for time series data: $$Y_t = a + b X_t + \varepsilon_t$$ If $Y_t$ is stationary but $X_t$ is not, then if you rearrange the equation: $$Y_t - \varepsilon_t = a + bX_t$$ Then, the left-hand side is stationary, but the right-hand side is not, so the model can't be correct. If, instead, both variables are not stationary, then: $$Y_t - bX_t = a + \varepsilon_t$$ The right-hand side is stationary, but the left-hand side may or may not be. If it's not, then the model is wrong. It's possible for it to be stationary, as in a cointegration model for example, but it need not be. Violating the assumption about the stationarity of the error process can lead to all sorts of problems, like spurious regressions where what appears to be a significant coefficient is frequently really not at all significant.
