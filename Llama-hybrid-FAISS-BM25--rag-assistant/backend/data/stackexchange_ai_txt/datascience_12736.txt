[site]: datascience
[post_id]: 12736
[parent_id]: 12695
[tags]: 
If the content/information is lengthy, I'd suggest you to use some NLP tasks for starters. I would suggest you to use some basic NLP based preprocessing because it makes our model perform better. So, the basic feature extraction can be used for this. Example, using Porter Stemmer , Lemmatizer to clean the data or removing stop words and then using ngrams for features seem to be a basic idea and a good start. There are various vectorizers which can be used to extract features the documents. For example, TfidfVectorizer calculates the frequency of a word in a document and also frequency across documents. This can be more useful than a naive Bag of words approach. Then, on top of this there are various classifiers which can be used like OneVsRestClassifier or others . A simple approach could be selecting the input and target first. Select the parameters which are to be passed as input and the desired output. Then, decide to clean the input or not based on some NLP APIs(you can use nltk ). Then decide on a classifier. You can then predict the values. Test on validation set and try various classifiers for starters. As for terminology, I can think of Multiclass Classification only now.
