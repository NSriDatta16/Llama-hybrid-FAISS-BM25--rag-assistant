[site]: crossvalidated
[post_id]: 112480
[parent_id]: 112451
[tags]: 
It is possible to say something without using (much) math, but for actual statistical applications of maximum likelihood you need mathematics. Maximum likelihood estimation is related to what philosophers call inference to the best explanation , or abduction . We use this all the time! Note, I do not say that maximum likelihood is abduction, that term is much wider, and some cases of Bayesian estimation (with an empirical prior) can probably also be seen as abduction. Some examples (the first taken from http://plato.stanford.edu/entries/abduction/#Aca ). See also https://en.wikipedia.org/wiki/Abductive_reasoning (In computer science "abduction" is also used in the context of non-probabilistic models.) "You happen to know that Tim and Harry have recently had a terrible row that ended their friendship. Now someone tells you that she just saw Tim and Harry jogging together. The best explanation for this that you can think of is that they made up. You conclude that they are friends again." This because that conclusion makes the observation you try to explain more probable than under the alternative, that they are still not talking. You work in a kindergarten, and one day a child starts to walk in a strange way, and saying he broke his legs. You examine and find nothing wrong. Then you can reasonably infer that one of his parents broke their legs, since children then often actuate as described, so that is an "inference to the best explanation" and an instance of (informal) maximum likelihood. (and, of course, that explanation might be wrong, it is only probable, not sure. Abduction/maximum likelihood cannot give sure conclusions). Abduction is about finding pattern in data, and then searching for possible theories that can possibly make those patterns probable. Then choosing the possible explanation, which makes the observed pattern maximally probable, is just maximum likelihood! The prime example of abduction in science is evolution . There is no one single observation that implies evolution, but evolution makes observed patterns more probable than other explanations. Another typical example is medical diagnosis? Which possible medical condition makes the observed pattern of symptoms the most probable? Again, this is also maximum likelihood! (Or, in this case, maybe bayesian estimation is a better fit, we must take into account the prior probability of the various possible explanations). But that is a technicality, in this case we can have empirical priors which can be seen as a natural part of the statistical model, and what we call model , what we call prior is some arbitrary(*) statistical convention. To get back to the original question about layman term explanation of MLE, here is one simple example: When my daughters where 6 and 7 years old, I asked them this. We made two urns (two shoe-boxes), in one we put 2 black balls, 8 red, in the other the numbers where switched. Then we mixed the urns, and we draw one urn randomly. Then we took at random one ball from that urn. It was red. Then I asked : From which urn do you think that red ball was drawn? After about one seconds thinking, they answered (in choir): From the one with 8 red balls! Then I asked: Why do you think so? And anew, after about one second (in choir again): "Because then it is easier to draw a red ball!". That is, easier=more probable . That was maximum likelihood (it is an easy exercise to write up the probability model), and it is "inference to the best explanation", that is, abduction. (*) Why do I say "arbitrary?" To continue the medical diagnosis problem, say the patient is a man with some difficult to diagnose condition the physician didn't see earlier. Then, say, in the talk with the patient it arises that he visited someplace in tropical Africa short time ago. That is a new piece of data, but its effect in the typical models (used in this kind of situation, be it formal or informal) will be to change the prior of the difficult possible explanations, as tropical diseases like malaria now will get higher prior probability. So the new data enters the analysis in the prior .
