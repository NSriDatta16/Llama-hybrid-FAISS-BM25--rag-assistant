[site]: crossvalidated
[post_id]: 612181
[parent_id]: 612177
[tags]: 
This is about something that the ML community has taken to calling “soft labels”. Think of the original zero or one labels as a probability distribution. These place all the probability mass on one outcome or the other. By smoothing the labels, we can ascribe fractional certainty to the outcomes, and the model can fit to these smoothed values instead of 1.0 and 0.0. An observed benefit is that it avoids the saturation problems attested in the sigmoid and tanh functions. Aside from the empirical benefit to label smoothing, sometimes you want to explicitly model the prior uncertainty in the label. If you have noisy labels for your data, or if your data are aggregations of multiple trials with different outcomes, then there is inherent uncertainty in what the correct label for a given instance is. You can interpret the number as a probability, leveraging whatever philosophical stance you take toward the meanings of those.
