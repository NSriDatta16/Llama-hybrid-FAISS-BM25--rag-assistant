[site]: crossvalidated
[post_id]: 309266
[parent_id]: 309244
[tags]: 
You can use backpropagation with non-differentiable operations, but the gradients won't propagate through them and hence you won't be able to train weights that are before those operations. Sometimes this is a problem, sometimes it is not. In the proposed case, you would not be able to perform the optimization of the generator loss, as the non-differentiable operation $h$ prevents from using backpropagation in the Generator's weights. However, why would you have such a setup instead of a normal $D(G(x))$? If you have cluster labels, you can use conditional GANs by supplying those labels to both Generator and Discriminator. P.S.: one option for having the clustering in your GAN setup would be to implement a differentiable clustering approach by leveraging internal representation separation (see this paper)
