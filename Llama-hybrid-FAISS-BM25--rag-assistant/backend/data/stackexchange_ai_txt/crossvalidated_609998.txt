[site]: crossvalidated
[post_id]: 609998
[parent_id]: 609970
[tags]: 
They are distinct notions. Point #2 refers to the usual kind of loss function. The first example almost anyone who studies statistics or data analysis of any kind sees is the square loss in ordinary least squares linear regression: add up the squared residuals. $$ L(y,\hat y)=\overset{n}{\underset{i=1}{\sum}}\left( y_i - \hat y_i \right)^2 $$ Another viable loss function is to add up the absolute residuals. $$ L(y,\hat y)=\overset{n}{\underset{i=1}{\sum}}\left\vert y_i - \hat y_i \right\vert $$ Each of these can be expressed in terms of $p$ -norms. $$ L(y,\hat y)=\overset{n}{\underset{i=1}{\sum}}\left( y_i - \hat y_i \right)^2=\vert\vert y-\hat y\vert\vert_2^2\\ L(y,\hat y)=\overset{n}{\underset{i=1}{\sum}}\left\vert y_i - \hat y_i \right\vert = \vert\vert y-\hat y\vert\vert_1 $$ Consequently, it is reasonable to refer to these as $\ell_2$ and $\ell_1$ loss, respectively. The penalization from the regularization in point #1 is separate. First, there is not necessarily a need to include penalization, so it might be that you just find the regression parameters that lead to predictions $\hat y_i$ giving the minimal $\ell_p$ loss, and this is exactly what ordinary least squares estimation does for $\ell_2$ loss. However, there are various reasons why the best loss value might not be desirable. Regularization is a way of sacrificing the training loss value in order to improve some other facet of performance, a major example being to sacrifice the in-sample fit of a machine learning model to quell overfitting and improve out-of-sample performance. You can mix-and-match loss functions and regularization to your heart's content. For instance, ridge regression uses square loss with an added penalty term that involves the $\ell_2$ norm of the regression parameter vector. $$ L_{\text{ridge}}=\vert\vert y-\hat y\vert\vert^2_2 + \lambda\vert\vert\hat\beta\vert\vert_2 $$ LASSO regression uses square loss with a penalty term that uses the $\ell_1$ norm of the parameter vector. $$ L_{\text{LASSO}}=\vert\vert y-\hat y\vert\vert^2_2 + \lambda\vert\vert\hat\beta\vert\vert_1 $$ Elastic net uses both types of penalty. $$ L_{\text{Elastic Net}}=\vert\vert y-\hat y\vert\vert^2_2 + \lambda_1\vert\vert\hat\beta\vert\vert_1+ \lambda_2\vert\vert\hat\beta\vert\vert_2 $$ Finally, while I do not see this approach discussed much, you could use $\ell_1$ loss with either penalty or even both. $$ L_{\text{Other}}=\vert\vert y-\hat y\vert\vert_1 + \lambda_1\vert\vert\hat\beta\vert\vert_1+ \lambda_2\vert\vert\hat\beta\vert\vert_2 $$ (The $\lambda$ parameters control how much of a penalty there is for having large coefficients in the parameter vector. It is common to tune these using cross validation.) Getting to other types of models, nothing stops you from using $\ell_1$ or $\ell_2$ penalization (or both) with, say, logistic regression and its associated "log loss". $$ L_{\text{Log}}=-\overset{n}{\underset{i=1}{\sum}}\left( y_i\log(\hat y_1) + (1 - y_1)\log(1 - \hat y_1) \right)\\ L_{\text{Penalized Log}}=-\overset{n}{\underset{i=1}{\sum}}\left( y_i\log(\hat y_1) + (1 - y_1)\log(1 - \hat y_1) \right)+ \lambda_1\vert\vert\hat\beta\vert\vert_1+ \lambda_2\vert\vert\hat\beta\vert\vert_2 $$
