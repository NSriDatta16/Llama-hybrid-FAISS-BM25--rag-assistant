[site]: crossvalidated
[post_id]: 386181
[parent_id]: 386012
[tags]: 
To answer your question, it seems the same to me whether you scramble columns in the set you train on or before you pass the test set: Effectively if you scramble before training, the model expects things in scrambled order, and then the canonical order looks scrambled to it. Or if you train on the canonical order, then the scrambled order looks scrambled. I wouldn't take either of these approaches to decide feature importance, because when I misplace a column, I don't just misplace it in isolation; I displace the one where it has to go. Maybe you could devise some crazy Hamming distance scheme to figure out how much to weight the inaccuracies of models training on scrambled datasets, but this seems like it would require a lot of models and be computationally expensive, and it shouldn't be necessary. Generally you can just train a random forest on your data and then look to see how many nodes chose particular features as the one to split on. Sklearn's random forest has _feature_importances , for example. As you mention, each node already takes a random subset of features to decide splits, so in a way you already get your scramble. If you're concerned about duplication of information across features, and because you have an absurd number of features, consider feature reduction techniques. You want $N$ , the number of samples, to be much much greater than $p$ , because curse of dimensionality: You can't really fill the super-high-dimensional 1000-dimensional space with examples; its volume is just too large. Your results will be pretty unpredictable nonsense unless the problem is constrained enough that models can find a somewhat stable function (or decision surface or whatever).
