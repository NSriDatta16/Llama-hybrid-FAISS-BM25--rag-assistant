[site]: crossvalidated
[post_id]: 113395
[parent_id]: 
[tags]: 
How to derive the gradient formula for the Maximum Likelihood in RBM?

I am learning RBM (restricted Boltzmann machine) for deep learning. The log-likelihood of RBM is given as : $$\ln(L(\theta|v))=\ln(p(v|\theta))=\ln\frac{1}{Z}\sum_h e^{-E(v,h)}=\ln\sum_h e^{E(v,h)}-\ln\sum_{v,h}e^{-E(v,h)}$$ and its gradient w.r.t. the parameter is: $$\frac{\partial L(\theta|v)}{\partial\theta}=-\sum_h p(h|v)\frac{\partial E(v,h)}{\partial\theta}+\sum_{v,h}p(v,h)\frac{E(v,h)}{\partial\theta}$$ I don't understand how is the gradient derived from the log-likelihood.
