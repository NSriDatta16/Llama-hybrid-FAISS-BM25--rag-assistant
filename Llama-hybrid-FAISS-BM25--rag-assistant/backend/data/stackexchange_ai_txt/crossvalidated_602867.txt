[site]: crossvalidated
[post_id]: 602867
[parent_id]: 602811
[tags]: 
Both terms are used as near-synonyms nowadays, but there is a slight difference. Transfer learning is a machine learning paradigm where you adapt a model trained for one task (in the case of BERT, it is masked language modeling) to perform another task (e.g., question answering) Finetuning means training a neural network (or only a sub-network) on a smaller dataset than the main training procedure, typically with a smaller learning rate and often using a different training objective. This is how transfer learning is done in the BERT paper. There are instances of transfer learning without fine-tuning and vice versa. A popular way of transfer learning with Transformer is Adapters , which adds small subnetworks, but does not involve finetuning. Finetuning on artificially noised data is a common training step when training machine translation models (so they can handle typos). However, it is not transfer learning because the task is still the same.
