[site]: crossvalidated
[post_id]: 279826
[parent_id]: 279802
[tags]: 
Posterior inference is a Bayesian approach to quantifying uncertainty in your parameter estimates. Boostrapping is a non-Bayesian approach to the same question. I won't re-hash Bayesian inference in this answer, but I suggest you google it. Posterior inference works by positing a distribution over possible parameter values (i.e., we model our parameters as being randomly chosen from some prior distribution). Then, we develop the sampling model (i.e., what is the statistical model for our data) that goes with our parameters. We combine these with Bayes' Theorem to get the posterior probability distribution for our parameters given our observed data. Bootstrap has many flavors, but it essentially treats your sample as a proxy for your population and proceeds to calculate sampling errors under this assumption. The end result is an estimate of the distribution of estimation errors about the "true" value. So, two very different ways of attacking the same thing. I would challenge the author's statement the posterior is always fast, although there are certainly cases where one approach beats the other. However, they are calculating different things as well...Bayes gives credible regions whilst boostrap tries to estimate confidence regions. Read up on the difference between these concepts to see that they are different.
