[site]: crossvalidated
[post_id]: 461931
[parent_id]: 461925
[tags]: 
Firstly, I don't feel qualified to comment on the details of the COVID-19 models, so my comments are more about the general principles of using outcomes from one model for subsequent models. Your intuition is certainly right that using point estimates ignores uncertainty and can thus lead to very wrong results, particularly, if unlikely but possible parameter values have very important implications E.g. if some parameter values conceivable based on the data, but away from the point estimate, would imply extreme outcomes that are extremely unlikely with the point estimates, then this would be important to know. In principle, when using outputs of previous experiments / analyses / trials etc. into subsequent analyses, one of the most logical approach is using a Bayesian approach. I.e. you take the posterior distribution of one analysis as the prior for the next analysis. In particular, if a estimate +- standard error description is appropriate (or is appropriate after some distribution e.g. for the logit of a proportion or for log viral load or ...) for the first analysis, if the knowledge about the other parameters in a model and the other parameters int he other models is independent (for example, it is likely a problem if the same data are used in both models), and if there is no differences/generalization issues, then a Normal(mean=estimate, SD=standard error) distribution would be appropriate for the parameter of the next analysis. Obviously, any of the above can be relaxed. E.g. allowing non-normal distributions can be quite easy in some cases (e.g. exploiting conjugate distributions, for a proportion one might assume a Beta(0.5, 0.5) prior and get a Beta(0.5+number of events, 0.5 + number of non-events) posterior). Using a joint model that can deal with the same data going into multiple of your chained models. When there's a question of whether data from one setting generalizes (e.g. laboratory testing versus what happens out in the real-world), there methods that attempt to deal with prior-data conflict or one can try to explicitly elicit the uncertainty from experts (and adjust the distribution accordingly). However, the technical/statistical/implementation difficulty/potential for errors increases the more complicated we have to make things. However, there are a lot of tools (like the R and Python interfaces to Stan - rstan and pystan ) that allow hand-crafted specification of almost arbitrarily complex models.
