[site]: crossvalidated
[post_id]: 46194
[parent_id]: 
[tags]: 
kernelized l1 norm and the representer theorem

I'm trying to derive a kernel-ized $l_1$ penalty for logistic regression. I have been looking at the slides Learning with Sparsity Inducing Norms along with the slides on Regularization and Variable Selection via the Elastic Net . On page 20 of reference 2 they add a $l_1$ penalty of the form $\lambda_1\sum_{i=1}^n|\alpha_i|$ with little explanation. Given we want a $l_1$ norm of the form $\sum_{i=1}^D|w_i|$, I think we can use the representer theorem to help things along. On page 36 of 1 , "Regularization and representer theorem", they say that for the $l_2$ norm the minimized function w.r.t weights $w$ is of the form $w = \sum_{j=1}^n\alpha_j\Phi(x_j)$ and then say that this is also equal to $\sum_{j=1}^n\alpha_jk(.,x_j)$. I tried looking at the reference for this equality but found it totally incomprehensible. As best as I can figure, the $l_1$ norm should be something like $||\tilde{w}||_1 = \sum_i|\tilde{w}_i| \le |\sum_i\sum_{j=1}^N\alpha_j\Phi(x_j)|^{(i)}$ $|\sum_i\sum_{j=1}^N\alpha_j\Phi(x_j)|^{(i)} = |\sum_i\sum_{j=1}^N\alpha_jk(.,x_j)|^{(i)} = |\alpha^TK1|$ where $1$ is the column vector of ones. My question is two-fold: Why is $f = \sum_{j=1}^n\alpha_j\Phi(x_j) = \sum_{j=1}^n\alpha_jk(.,x_j)$? What should the $l_1$ norm look like?
