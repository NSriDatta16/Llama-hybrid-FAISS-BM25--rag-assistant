[site]: crossvalidated
[post_id]: 411799
[parent_id]: 
[tags]: 
Derivation for the expression for $ \mathbf{x-\mu} $ in a multivariate Gaussian distribution

The multivariate normal distribution is given by $$\text{N}(\textbf{x}|\mathbf{\mu}, \mathbf{\Sigma}) = (2\pi)^{-D/2} |\mathbf{\Sigma}|^{-1/2} \exp \Bigg( - \frac{1}{2}{(\mathbf{x - \mu})}^\text{T} \mathbf{\Sigma}^{-1}(\mathbf{x-\mu}) \Bigg).$$ Let the eigenvector equation for the covariance matrix be $$\mathbf{\Sigma} \mathbf{u}_i = \lambda_{i} \mathbf{u}_i.$$ Equation 2.60 of Bishop's book on Pattern Recognition and machine learning reads $$\mathbf{x} - \boldsymbol{\mu} = \sum_{j=1}^{D} \mathbf{u}_j^\text{T} (\mathbf{x} - \boldsymbol{\mu}) \mathbf{u}_j.$$ Bishop's book states that the above equation can be derived from the eigenvector expansion of the covariance matrix together with the completeness of the set of eigenvectors. My questions are What is completeness of the set of eigenvectors? How does that lead to the derivation of the above equation?
