[site]: datascience
[post_id]: 11070
[parent_id]: 11057
[tags]: 
It depends on how you combine results. Many ensemble techniques will either: train the same classifier, but on different parts of the data train different classifiers on the full data vary both: classifier and data subsets In either situation, you afterwards have to combine the results; usually with some form of majority voting. So if 2 classifiers return "A", 1 classifier returns "B", then the outcome is "A". To get a good outcome, every member needs to be better than random; to improve over the individual results they must not be too similar. You could use clustering for the first approach (to get different parts of the data). But the problem is that these parts are not independent, and too biased. You would usually want each classifier to know "a little bit of everything". By withholding some part of the data you prevent them from overfitting the same way. For this, random is usually best. If you do clustering, there is a chance you get 1 classifier that thinks everything is "A", 1 that thinks everything is "B", and 1 that thinks everything is "C". You even encourage them to overfit! So you always get the result 1 A, 1 B, 1 C = no majority.
