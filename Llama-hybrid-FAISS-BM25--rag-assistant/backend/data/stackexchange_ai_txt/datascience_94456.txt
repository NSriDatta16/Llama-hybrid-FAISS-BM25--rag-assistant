[site]: datascience
[post_id]: 94456
[parent_id]: 
[tags]: 
Why does this paper claim to have found a minimal width of $d_{in}+1$?

Why does this paper (click the link) claim to have found a minimal width of $d_{in}+1$ in the abstract? I mean, if you read the main result, it seems like they only find a universal approximator with width $d_{in} + d_{out}$ . What am I missing or did they make a mistake? I do not completely understand all the math but I do think I get the gist of it, and also $d_{in}+1$ is not even mentioned when showing a bound of the difference between the function approximated by the neural network and the actual function...
