[site]: crossvalidated
[post_id]: 546819
[parent_id]: 
[tags]: 
How does torch.nn.Embedding or tf.keras.layers.Embedding compare to a dense layer?

Embedding layers are a common choice to map some high-dimensional, discrete input to real-valued (computationally represented using floating point) numbers in a much smaller number of dimensions. Some common usages are word embeddings, character embeddings, byte embeddings, categorical embeddings, or entity embeddings. Common implementations such as torch.nn.Embedding and tf.keras.layers.Embedding accept an integer as an input and yield a floating-point vector as an output. Is an embedding layer comparable to a dense layer? How?
