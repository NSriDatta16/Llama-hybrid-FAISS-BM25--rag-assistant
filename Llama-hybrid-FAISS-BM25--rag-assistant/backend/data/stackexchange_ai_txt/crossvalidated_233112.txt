[site]: crossvalidated
[post_id]: 233112
[parent_id]: 3931
[tags]: 
Suppose that you have a random phenomenon. Suppose again that you only get one $N=1$ sample, or realization, $x$. Without further assumptions, your "only" reasonable choice for a sample average is $\overline{m}=x$. If you do not subtract $1$ from your denominator, the (uncorrect) sample variance would be $$ V=\frac{\sum_N (x_n - \overline{m} )^2}{N}$$, or: $$\overline{V}=\frac{(x-\overline{m})^2}{1} = 0\,.$$ Oddly, the variance would be null with only one sample. And having a second sample $y$ would risk to increase your variance, if $x\neq y$. This makes no sense. Intuitively, an infinite variance would be a sounder result, and you can recovered it only by "dividing by $N-1=0$". Estimating a mean is fitting a polynomial with degree $0$ to the data, having one degree of freedom (dof). This Bessel's correction applies to higher degrees of freedom models too: of course you can fit perfectly $d+1$ points with a $d$ degree polynomial, with $d+1$ dofs. The illusion of a zero-squared-error can only be counterbalanced by dividing by the number of points minus the number of dofs. This issue is particularly sensitive when dealing with very small experimental datasets .
