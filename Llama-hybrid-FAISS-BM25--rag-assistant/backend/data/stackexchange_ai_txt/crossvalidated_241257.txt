[site]: crossvalidated
[post_id]: 241257
[parent_id]: 241196
[tags]: 
Inverse reinforcement learning (IRL) can be seen an instance of supervised learning. The data is the demonstrations and the target is the reward function. So the 'learning' task is just to look for the mapping from the space of demonstrations to reward functions, under the constraints of the specification of the MDP. Concrete example: Lets use Bayesian IRL to illustrate. Given some MDP without reward function $(\mathcal{S}, \mathcal{A}, T, \gamma)$, and a set of demonstrations $\Xi = (\xi_1, \ldots, \xi_M)$ where each demo trajectory $\xi_i = ((s^i_1,a^i_1), \ldots, (s^i_H,a^i_H))$ is a set of state-action pairs. The BIRL task is to find, $$ \Pr(R \mid \Xi) $$ which is easily expanded as $\Pr(R \mid \Xi) \propto \Pr(\Xi \mid R) \Pr(R)$ by Bayes rule. The 'data' ($\Xi$) are also often assumed to be iid. From this formulation it is obvious that its a supervised learning problem. The devil is only in the details of computing the likelihood. Important : IRL seeks the reward functions that 'explains' the demonstrations. Do not confuse this with Apprenticeship learning (AL) where the primary interest is a policy which can generate the seen demonstrations (although this is often but not necessarily obtained via the reward). Additionally, there is behavior cloning which is also closely related. Given some examples of a behavior, behavior cloning simple try to reproduce it. This could mean generating behavior that 'matches' the statistics of the observed behavior. It is obvious to see how this is supervised learning. E.g. given some demonstrations, train a neural net to generate 'similar' behaviors given 'similar' situations. P.S. Forgive my hand-wavy nature with the vocabulary.
