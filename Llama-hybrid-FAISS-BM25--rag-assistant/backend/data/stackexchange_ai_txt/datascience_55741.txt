[site]: datascience
[post_id]: 55741
[parent_id]: 54984
[tags]: 
Your question is relatively devoid of details, so all I can really suggest is to use a standard convolutional network classifier. The problem you seem to be having is that most such models output a one-hot vector, i.e., categorical distribution, corresponding to a single label: $p=(p_1,\ldots,p_n)$ , where $\sum_i p_i = 1$ and $p_i$ is the probability of assigning label $i$ (and only label $i$ ) to the input. Usually this is computed via $p = \text{SoftMax}(f_\theta(x))$ for some input image $x$ and neural network $f_\theta$ . To change this to handle multiple labels, let $q=(q_1,\ldots,q_m)$ , where $q_i\in[0,1]$ is the probability of assigning label $i$ to the input. You can think of each $q_\ell$ as parameterizing a Bernoulli distribution over a single label. You can compute $q$ via, for instance, $q = \sigma_E(f_\theta(x))$ , where $\sigma_E$ is the elementwise sigmoid function. You can then assign labels to $x$ by, for example, thresholding each $q_j$ , i.e., assign label $k$ to $x$ if $x_k \geq T$ , where $T$ is a hyper-parameter that can be, say, 0.5. In other words, take the classifiers on Github and replace the softmax with an elementwise sigmoid, change $n$ to $m$ , and use your training data; that's it. Hopefully that's what you meant.
