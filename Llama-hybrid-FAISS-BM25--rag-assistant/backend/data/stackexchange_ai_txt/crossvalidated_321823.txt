[site]: crossvalidated
[post_id]: 321823
[parent_id]: 
[tags]: 
Can a stationary var(1) process have no variance?

Consider as an example the following Var(1) process: $y_t=Gy_{t-1}+\varepsilon_{t+1}$, where $\varepsilon_{t+1}$ is white noise and the matrix $G$ has all eigenvalues strictly smaller than 1. Then by standard results the process is stationary, and since $\sum_iG^i$ is absolutely summable, it can be represented as a MA($\infty$): $y_t=\sum_{i=0}^{+\infty}G^i \varepsilon_{t-i}$. Now, all the books I checked (e.g. Lutkepohl proposition C.10, Brockwell Davis chapter 11, Hamilton prop 10.2) state that, without any further condition beyond absolute summability, we get that the autocovariance is: $\Gamma(k)=\sum_i^{+\infty} G^{k+i}(G')^{i}$ However, consider the matrix $G=0.9\left(\begin{array}{ccc}0 & 1 & 0.5 \\ 0 & 0 & 0.5\\ 1& 0 & 0\end{array}\right)$ The eigenvalues of $G$ have all modulus smaller than 1 (they are 0.9, 0.9(0.5$\pm$0.5i)), hence the process is stationary and can be represented as a moving average. Yet, the expression $\Gamma(k)=\sum_i^{+\infty} G^{k+i}(G')^{i}$ does not converge, because $GG'$ has an eigenvalue bigger than 1 (barely, around 1.05). As a further evidence, for example for $k=0$ we would have, if it converged: $\Gamma(0)=(I-GG')^{-1}$ since its a geometric series. But this cannot be the limit because it has negative terms (calculation approximate because done with the pc): $(I-GG')^{-1}=\left(\begin{array}{ccc}-15 & -4 & 0 \\ -4 & 0.25 & 0\\ 0& 0 & 5\end{array}\right)$ So it seems that some further condition is needed to ensure that the process has a finite variance, or at least that the variance can be calculated with that sum. Indeed, this makes sense, because in the one dimensional case $\sum_i g^i$ and $\sum_i g^{2i}$ both converge iff $|g| Am I right, or is there something I am missing? Do someone know a reference where some of these conditions are discussed? For example, if we assume $G$ symmetric then the matrix to be summed is $G^2$, and the eigenvalues of $G^2$ will all be smaller than one if and only if the eigenvalues of $G$ are, so in this case it works.
