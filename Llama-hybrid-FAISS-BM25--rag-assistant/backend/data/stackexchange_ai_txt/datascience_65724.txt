[site]: datascience
[post_id]: 65724
[parent_id]: 20535
[tags]: 
The algorithm (or at least a version of it, as implemented in the Coursera RL capstone project ) is as follows: Create a Replay "Buffer" that stores the last #buffer_size S.A.R.S. (State, Action, Reward, New State) experiences. Run your agent, and let it accumulate experiences in the replay-buffer until it (the buffer) has at least #batch_size experiences. You can select actions according to a certain policy (e.g. soft-max for discrete action space, Gaussian for continuous, etc.) over your $\hat{Q}(s,a ; \theta)$ function estimator. Once it reaches #batch_size , or more: make a copy of the function estimator ( $\hat{Q}(s,a; \theta)$ ) at the current time, i.e. a copy of the weights $\theta$ - which you "freeze" and don't update, and use to calculate the "true" states $\hat{Q}(s', a'; \theta)$ . Run for num_replay updates: sample #batch_size experiences from the replay buffer. Use the sampled experiences to preform a batched update to your function estimator (e.g. in Q-Learning where $\hat{Q}(s,a) =$ Neural network - update the weights of the network). Use the frozen weights as the "true" action-values function, but continue to improve the non-frozen function. do this until you reach a terminal state. don't forget to constantly append the new experiences to the Replay Buffer Run for as many episodes as you need. What I mean by "true": each experience can be thought of as a "supervised" learning duo, where you have a true value function $Q(s,a)$ and a function estimator $\hat{Q}(s,a)$ . Your aim is to reduce the Value-Error, e.g. $\sum(Q(s,a) - \hat{Q}(s,a))^2$ . Since you probably don't have access to the true action-values, you instead use a bootstrapped improved version of the last estimator, taking into account the new experience and reward given. In Q-learning, the "true" action value is $Q(s,a) = R_{t+1} + \gamma \max_{a'}\hat{Q}(s', a'; \theta)$ where $R$ is the reward and $\gamma$ is the discount factor. Here's an excerpt of the code: def agent_step(self, reward, state): action = self.policy(state) terminal = 0 self.replay_buffer.append(self.last_state, self.last_action, reward, terminal, state) if self.replay_buffer.size() > self.replay_buffer.minibatch_size: current_q = deepcopy(self.network) for _ in range(self.num_replay): experiences = self.replay_buffer.sample() optimize_network(experiences, self.discount, self.optimizer, self.network, current_q, self.tau) self.last_state = state self.last_action = action return action
