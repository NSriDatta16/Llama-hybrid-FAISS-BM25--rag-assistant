[site]: datascience
[post_id]: 121498
[parent_id]: 121493
[tags]: 
Yes training for large number of epochs will lead to overfitting . This is because after a point the model starts learning noise from the training set. After a certain number of epochs, majority of what has to be learnt is already learnt and if you continue past that point, the noise present in the dataset starts affecting the model. Based on your question, you think 50 epochs is not that many but how many epochs to set also depends on your dataset and what model you are using. If you have a large enough dataset, 50 epochs can be too much. Similarly if you have a small dataset, 50 epochs might not be enough. On the same note, if you have a neural network with a lot of parameters, (example gpt2 or 3) you don't need that many epochs as the model is large and complex enough to learn from the data in just a few epochs. But if you have a relatively smaller neural network then you might need to increase the epochs so that the model can have sufficient iterations to learn from the data. I would advise using learning curves to visualize how your model is performing for certain number of epochs. sklearn has a library learning_curves I think, for that purpose.
