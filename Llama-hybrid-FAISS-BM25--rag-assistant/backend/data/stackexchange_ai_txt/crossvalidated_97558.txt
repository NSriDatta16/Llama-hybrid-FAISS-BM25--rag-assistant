[site]: crossvalidated
[post_id]: 97558
[parent_id]: 97555
[tags]: 
SMOTE isn't really about changing f-measure or accuracy... it's about the trade-off between precision vs. recall. By using SMOTE you can increase recall at the cost of precision, if that's something you want. Just look at Figure 2 in the SMOTE paper about how SMOTE affects classifier performance. Undersampling the minority class gets you less data, and most classifiers' performance suffers with less data. An alternative, if your classifier allows it, is to reweight the data, giving a higher weight to the minority class and lower weight to the majority class. So why use something like SMOTE? Usually if the class you're interested in is rare, like finding defaults if predicting a credit score, a classifier giving 0-1 scores will say everyone doesn't default. Often in practice, one would rather have a classifier that returns the vast majority of the defaults, even if precision is less than 50%, as these can be examined by a human, or you can direct deeper, more expensive, data collection efforts towards these cases. If you use a classifier with a more continuous score, you can just lower the threshold to get more recall - i.e. for a logistic regression, start treating $X^T w > -2$ as positive, but this usually results in getting lower f-measure, since it's not the "fulcrum point" of where the model is being trained. By reweighting the proportion of the classes, you make your model be trained at the precision/recall tradeoff you prefer, which means you end up with both being slightly better than if you just lowered the threshold.
