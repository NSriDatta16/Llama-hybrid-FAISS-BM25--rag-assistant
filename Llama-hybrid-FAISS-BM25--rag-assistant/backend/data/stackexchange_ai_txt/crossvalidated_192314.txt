[site]: crossvalidated
[post_id]: 192314
[parent_id]: 192275
[tags]: 
The reason there is such a fuss about them now is because of 'big data' in particular image and speech data (high dimensional and large number of samples, cf imagenet) . Essentially the issue is computational rather than mathematical :neural networks using (gradient descent) learn in O(n) whereas most other statistical methods are of higher order and cannot be applied efficiently to such large datasets. Furthermore GPU devices allow fast computation of the simple matrix algebra underlying neural nets. The third computational aspect is to do with convolutional neural networks (which basically mirror adaptive nonlinear filters): in standard image processing, you manually develop a set of (non linear) image features (feature engineering) that you believe best captures the problem, then pass those into a classifier. And iterate (adding new features..) this manual intervention is very time consuming and may not yield very good results. With convolutional Nns this manual step is removed and you have a fully automated pipeline that you can leave the computer to optimise over (much as a chess computer would work as opposed to person)... So then the magic is developing scripts running gpus on loads of machines to evaluate different combinations of filters /filter sizes etc.
