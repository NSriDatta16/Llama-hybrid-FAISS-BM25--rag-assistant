[site]: crossvalidated
[post_id]: 187273
[parent_id]: 
[tags]: 
deterministic time trend vs stationarity

Sorry for the newbie inquiry but I'm having a little trouble making sense of stationarity and how a the presence of a time trend impacts this. I'm working on a model for operating margins and as a first step I want to determine if the original series is stationary before proceeding. I first fitted a simple linear trend line to the data and the time regressor, while small in magnitude, registered as highly significant. I was always under the impression that this implied a non constant mean, thus non-stationary and may require a transform or differencing. I decided to regress the first differenced time series on the lag of the original time series and found the regressor of the lagged value to be negative and highly significant (t-stat greater than 9). This is where I got a little confused as these two seem to contradict my understanding of the subject. I thought a rejection of the null: g =0 (Dickey Fuller test) indicated no unit root, thus mean reverting and stationary. This seems to conflict with my initial assessment based on the deterministic time trend component. Thanks in advance!
