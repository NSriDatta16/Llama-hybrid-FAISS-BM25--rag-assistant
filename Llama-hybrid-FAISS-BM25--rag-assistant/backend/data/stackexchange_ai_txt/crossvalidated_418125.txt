[site]: crossvalidated
[post_id]: 418125
[parent_id]: 417649
[tags]: 
Yes you can do this: this is closely related to so-called permutation tests (you'd be doing a permutation test under the null hypothesis indistinguishable classes). But: there are types of overfitting that you can only detect if the permutation test incorporates the correct assumptions about data structure. Consider the following situation: Your input data has clusters of similar rows, which truly belong to the same class. But you do not know this, and model each row as independent of all other rows. The model overfits: it learns the clusters "by hard", i.e. while it has good predictive ability for unknown rows of known clusters, it's predictions for rows of unknown clusters is bad. The nested cross validation won't detect this: you'd need to implement splitting-by-cluster in order to detect this overfitting. But you cannot do that as you do not know of the clusters. Neither will the permutation test: that can also detect the overoptimism in the cross validation results only if the random labels are random by cluster (which you still don't know...) Some more thoughts: there are situations where you should get worse than guessing performance in the permutation test of a cross validation: this happens if the classifier in the absence of useful information guesses more-or-less the majority class, but as the row to be tested was removed, the test row on average belongs to the minority class. Permutation tests are useful to detect data leakage in the cross validaton that happens while the overall data structure (clustering) is modeled correctly. Examples for such leaks would be preprocessing that includes information from the test data.
