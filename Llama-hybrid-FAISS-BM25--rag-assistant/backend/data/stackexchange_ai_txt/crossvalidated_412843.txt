[site]: crossvalidated
[post_id]: 412843
[parent_id]: 
[tags]: 
Distribution $f$ that minimizes $JSD(f||q) + JSD(f||p)$

What can we say about the distribution $f^*$ that is the solution to the following optimization problem: $$\min_f JSD(f||p)+JSD(f||q) ,$$ where $p,q$ are given distributions over some set, and $JSD$ is the Jensen-Shannon Divergence. Intuition is that $f$ would tend to be something that approximates both probabilities, "some sort of average". I tried opening it up to expectations of $KL$ -divergences, and opening them up to one big integral, and deriving it (with the lagrangian constraint $\int_{x\in X} f(x) = 1 $ ) but it didn't get me far. We can assume $Supp(p) \cap Supp(q) \neq \emptyset $ . Context: I'm trying to understand what happens when I train the GAN formulation with one Generator and 2 discriminators (each with its own datasets, corresponding to $p$ and $q$ ). The generator step of the training scheme solves essentially the above optimization problem. In the one-discriminator case it's easy: $\min _fJSD(f||p) \iff f^*=p$ .
