[site]: crossvalidated
[post_id]: 305309
[parent_id]: 305306
[tags]: 
Your teacher needs to open a basic book on machine learning. The cross entropy loss function is equivalent to minimizing the KL divergence between your empirical and predicted distributions. While you can think of your predicted distribution as a probability, it's much more straightforward to think of it as a "confidence" in your prediction. If your teacher wants you to map your predicted distribution to 0s and 1s, then you would use ROC analysis to do so, since there is no universal way of mapping continuous values to discrete ones.
