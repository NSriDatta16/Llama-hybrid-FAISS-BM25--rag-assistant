[site]: crossvalidated
[post_id]: 495675
[parent_id]: 495658
[tags]: 
This is probably a comment rather than an answer and I'm not sure I'm getting it right. Let's try to compare the output of a linear model on two nearly colinear variables before and after PCA: set.seed(1234) x1 Linear regression on raw data: summary(lm(y ~ x1 + x2)) ... Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) -0.9351 0.7503 -1.246 0.253 x1 1428.6673 3681.8475 0.388 0.710 x2 -1427.5288 3681.8604 -0.388 0.710 Residual standard error: 1.094 on 7 degrees of freedom Multiple R-squared: 0.9281, Adjusted R-squared: 0.9076 F-statistic: 45.18 on 2 and 7 DF, p-value: 9.963e-05 Now on the principal components: pca $x[,1] + pca$ x[,2]) summary(pca_lm) ... Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 5.382e+00 3.458e-01 15.562 1.09e-06 *** pca $x[, 1] 8.086e-01 8.514e-02 9.498 3.00e-05 *** pca$ x[, 2] 2.020e+03 5.207e+03 0.388 0.71 --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 1.094 on 7 degrees of freedom Multiple R-squared: 0.9281, Adjusted R-squared: 0.9076 F-statistic: 45.18 on 2 and 7 DF, p-value: 9.963e-05 Looking at adjusted R-squared, the quality of the two models is the same - as expected. Project the coefficients from model with principal components to the original scale (am I doing this right?) (pca_lm $coefficients[1] + pca_lm$ coefficients[2:3]) %*% pca$rotation PC1 PC2 [1,] 1436.278 -1427.529 These are similar to the coefficients from the model on raw variables. So, in summary, there is no advantage in passing by principal components.
