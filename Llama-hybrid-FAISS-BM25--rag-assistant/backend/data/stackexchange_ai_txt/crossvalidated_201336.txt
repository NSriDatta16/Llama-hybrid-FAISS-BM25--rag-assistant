[site]: crossvalidated
[post_id]: 201336
[parent_id]: 200495
[tags]: 
Is it common practice in RL to have only one reward function awarded when a task is fulfilled in the end? This isn't quite the correct definition of a reward function. An MDP has a single reward function, $R(s,a,s'): S \times A \times S \mapsto \mathbb{R}$, where $S, A$ are the sets of states, actions in the problem. You'll sometimes see versions with fewer arguments, say $R(s,a)$ or $R(s)$. $R$ returns rewards for every state transition. Many of them, or even all but one, can be zero. Or, other intermediate states can include positive or negative rewards. Both are possible, and dependent on the particular application. This the definition you'll find at the start of most reinforcement learning papers, e.g. this one on reward shaping , the related study of how one can alter the reward function without affecting the optimal policy.
