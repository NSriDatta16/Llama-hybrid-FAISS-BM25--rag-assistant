[site]: crossvalidated
[post_id]: 442101
[parent_id]: 442074
[tags]: 
In general, there are two main issues with finite differences: They can be expensive: it can require a lot of extra function evaluations. Technical detail: knowing the sparsity pattern can help. They are a bit inexact. You loose roughly half your precision. Many algorithms require exact gradients to work properly. So the rule of thumb is: whenever you can provide gradients, you should do so. If this is not possible then finite differencing is a good fallback method. It may be better to use a proper Derivative Free Optimization (DFO) method. For many gradient descent applications (such as neural networks) we know the analytical form of the gradients. Clever implementations can further speed up gradient calculations.
