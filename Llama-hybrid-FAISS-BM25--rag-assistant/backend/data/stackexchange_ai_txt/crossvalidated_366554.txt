[site]: crossvalidated
[post_id]: 366554
[parent_id]: 
[tags]: 
Is there utility in comparing different machine learning algorithms against ground truth data?

I am at the beginning of my (interdisciplinary) PhD studies intersecting Economics, Machine Learning and Statistics. One of my two supervisors with a background in AI, argues that for the field of computer science, it would be interesting to compare the performance of different machine learning algorithms on behavioural data (in my case migration data) and to determine the impact of removing specific attributes or entire data sets on the performance. In doing so, it would be possible (since I have very granular data from residents' registration offices) to determine the significance of different data sources and errors other studies made when not considering them. During my literature research, I got the feeling that there is a large number of scientific papers pursuing these research questions. As a consequence, it seems to me there is not a great demand for research in this area. Is comparing the performance of algorithms an already-well-researched topic? Is it possible to determine the importance of features in this way? Am I likely to be able to say something novel about migration by interrogating the data with these tools? Background info: In short, my goal is to utilize different machine learning techniques to learn the probability of migrating for individuals given sociodemographic characteristics, housing characteristics, environmental variables and so on. In a second step, I will use this model to simulate and forecast migration in the future (most likely using agent-based modelling).
