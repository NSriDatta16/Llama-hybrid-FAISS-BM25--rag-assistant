[site]: datascience
[post_id]: 82412
[parent_id]: 
[tags]: 
Which one is better method and why? Manually Handcrafted sound features vs spectrogram + convolution

I am working on classifying different sounds ( not speech or words exactly something like ambulance alarm, police alarm, cough sounds etc) I read few paper which suggested to extract dsp features such as MFccs, skewness, kurtosis, log energy, entropy, zcr etc ( using total around 20 features ) from each segmented sound event. I am currently using all these features and training xgboost, 3 layer DNN with ReLU, I am getting good accuracy. I also read recent deep learning papers where they just used spectrogram images and feed it to convolution network which is capturing both temporal and spatial features. ( Using spectrogram one feature only ) I am looking for some explanation which one is better method for classifying sounds and why? Any reference paper for comparison would be additional help. Thank you!
