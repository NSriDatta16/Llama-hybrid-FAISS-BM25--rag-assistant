[site]: crossvalidated
[post_id]: 515594
[parent_id]: 
[tags]: 
Network topology of dropout

So dropout is a popular way to regularize neural networks by randomly removing nodes in the network. There are similar methods that remove edges, as well as skip connections which introduce connections that skip layers of the network. I am wondering whether there has been any topological explanation of why these methods work. The high-level explanation is that dropout introduces randomness and thus reduces overfitting, and that it smooths the loss landscape. But I am looking for something that rigorously shows the effect of new/deleted linkages on the loss function. More specifically, backpropagation can quantify the effect of a single node/edge on the loss, so is there a way to use that knowledge to describe the effect of dropout?
