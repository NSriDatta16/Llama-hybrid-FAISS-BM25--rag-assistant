[site]: datascience
[post_id]: 122710
[parent_id]: 
[tags]: 
Dropout and BatchNorm decrease speed of learning

Experimenting with the cifar10 dataset and faced with strange behavior when Dropout and BatchNorm don't help at all. As I get: Dropout - freezing some of the weights which helps us to prevent overfitting BatchNorm - make training faster and more stable through normalization Everything seems reasonable to use by default in NN. A course that I'm passing says the same. Experiments showed the opposite. Experiments: Base NN (Conv2d and max pool) - showed the best result of growing accuracy and decreasing loss. Base NN + Droupout - 2nd place Base NN + Droupout + BatchNorm - 3th place The question is why? Maybe I'm doing smt wrong? I'm kinda newbie in the Deep learning NN itself: nn.Sequential( nn.Conv2d(3, 16, 3, padding=1), # nn.Dropout2d(0.2), # nn.BatchNorm2d(16), nn.ReLU(), nn.MaxPool2d(2,2, padding=1), nn.Conv2d(16, 32, 3, padding=1), # nn.Dropout2d(0.2), # nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2,2, padding=1), nn.Conv2d(32, 64, 3, padding=1), # nn.Dropout2d(0.2), # nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2,2, padding=1), nn.Flatten(), nn.Linear(1600, 500), nn.ReLU(), nn.Linear(500, n_classes) )
