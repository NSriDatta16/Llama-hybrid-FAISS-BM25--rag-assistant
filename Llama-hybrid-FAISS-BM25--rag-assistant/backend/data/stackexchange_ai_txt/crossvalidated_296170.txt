[site]: crossvalidated
[post_id]: 296170
[parent_id]: 296164
[tags]: 
Firstly, unlike PCA, "the sum of variances of all PLS components is normally less than 100%".(a good explanation by @amoeba) Also, since the aim of PLS models is usually to build a predictive model, instead of explained variance based decisions, using predicted residual error sum of square(PRESS) values obtained by cross-validation is probably the most common practice while selecting number of components. It is basically a plot of number of components vs. PRESS values where one usually chooses number of components corresponding to the first minima. Read more about it here . If you are only interested in dimension reduction ability of PLS, then you are interested in using the X scores of PLS obtained by, say, first h number of components. If, for some reason, you want to use these scores as an input to another regression method, I suggest you to cross-validate the entire modeling steps. There is also a way of using PCA for regression which is called Principle Components Regression (PCR) in which the obtained scores are used as X matrix for the following OLS with same Y. However, I have never seen it performing better than PLSR. Furthermore, there is this article by the author of famous SIMPLS algorithm of PLS, namely PLS fits closer than PCR. Finally, on your main question, if what you meant by step-wise regression for "which components to use" rather than "how many first h components to use", I do not see any reason to use that approach. The first h component approach is almost always better. Edit: Honestly, I don't know much about rolling regression and time series. However, the reason to choose first h number of components should apply to all cases and is about the theory of PLS. Basically, the first component is the direction for X that maximizes the covariance with Y that follows a set of rules such as the orthogonality of weights which are used to obtain X scores. The calculation of next components is then accomplished by removing covariance that are covered by previous components and finding a new direction that aims to cover remaining covariance as much as possible while obeying the same certain criterions. Selecting, for example, 1st and 3rd components and leaving the 2nd out would be like skipping the 2nd digit while simply multiplying two 3 digit numbers. It is arguable that "digit" may not come out useful, but it is wise to keep it. Edit2: to @Frank Harrell , I totally disagree with you. See the quotation directly from the article: Geladi, Paul, and Bruce R. Kowalski. "Partial least-squares regression: a tutorial." Analytica chimica acta 185 (1986): 1-17. There are certain cases, i.e. when there is no local minima, selecting many number of components may lead to modeling of noise thus overfitting. However, since one can not choose number of components arbitrarily or one can not include all components, (because doing so makes PLS equivalent to OLS) CV for number of components is the way to go. PLS utilizing Y matrix makes no exception to that. Ridge, LASSO, PLSR, PCR all needs CV for selecting alpha, lambda and alpha, number of latent variables and principle components, respectively.
