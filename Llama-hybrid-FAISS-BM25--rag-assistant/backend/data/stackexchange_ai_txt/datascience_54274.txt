[site]: datascience
[post_id]: 54274
[parent_id]: 20373
[tags]: 
I'll address the example you linked to first. The link in your answer redirects, it seems that an equivalent product from the same company is Myrtle . The only product on that webpage is Nervana . All of Intel's AI products (including their Xenon processors, containing no AI acceleration) are supported by their libraries . That permits running your project on a range (most, but not all) of their hardware. Attempting to use Wayback or search for the old webpage turns up nothing useful. Myrtle has significantly accelerated the time-to-market for FPGA designs by automating the translation of AI network designs using compiler technology targeting the Intel® Arria® 10 FPGA. Trained, high level algorithms, specified in a wide range of network descriptor formats - Torch, Caffe etc. are compiled and mathematically optimised, directly to VHDL to produce efficient, low latency designs. In cases where proprietary non-standard, deep learning layers are used these first need to be implemented. Myrtle.ai sets new deep learning standard; trains ResNet network 4.5x faster, using 8x fewer GPUs and at a quarter of the cost than previous benchmark winner. Currently it's being implemented on the Intel Arria 10 GX FPGA : The Intel Nervana NNP is a purpose built architecture, an application specific integrated circuit (ASIC) that is custom-designed and optimized for deep learning. There are two different versions: The Intel® Nervana™ Neural Network Processor- T 1000 put a large amount of HBM memory and local SRAM much closer to where compute actually happens. This means more of the model parameters can be stored on-die to save significant power for an increase in performance. The Intel® Nervana™ Neural Network Processor features high-speed on- and off-chip interconnects enabling multiple processors to connect card to card and chassis to chassis, acting almost as one efficient chip and scale to accommodate larger models for deeper insights. The Intel® Nervana™ Neural Network Processor- I 1000 is a discrete accelerator designed specifically for the growing complexity and scale of inference applications. It is built on Intel’s 10nm process technology with Ice Lake cores to support general operations as well as neural network acceleration. This is the Nervana: The FPGA has optimized libraries available that compile direct to VHDL, if you want your own libraries you'll be writing your own code. It is somewhat analogous to wanting a custom BIOS for a GPU, you can wait for the manufacturer, look for a third party offering, or write your own - not simple, not extremely difficult. The FPGA is highly configurable, though that one isn't particularly powerful. The ASIC also has optimized libraries, the same ones (with a compiler switch) so there's no difficulty switching from one to another; or running your code slower on a GPU or CPU. The ASIC isn't configurable, after years of development on the FPGA they decided to implement the results in dedicated hardware; gone is the reconfiguration but there's an increase in speed and a decrease in price (and TDP). ... would need someone who is able to configure the FPGA according to the type of project they want through the use of languages such as Verilog or VDHL. You could , much as you could write your own BIOS for your GPU, like some cryptominers do. It's not something you have to do, but there are easy tools and brain surgery tools. The manufacturer offers downloadable updates from time to time. Furthermore, if they want to change the type of DL project they want to do, they have to reconfigure the FPGA to follow suit. Maybe, probably not. Just change the program and recompile is probably good enough, unless you want to squeeze another few percent more performance out of it; and you know more than the manufacturer about the device and optimized AI architecture. Now to your main question, which seems to be the first thing that you asked: I'm trying to investigate the ways in which FPGAs differ to GPUs for the purpose of deep learning. I understand this is a complex question and ... First, a similarity: They are a means to solve AI problems, they have a certain amount of potential (both capability and processing speed); better ones cost more but because GPUs are mass produced (comparatively, FPGAs are mass produced also) they will be cheaper and there will be more free tech support (Forums). A cheap FPGA (or ASIC) won't be fast, and a fast one won't be cheap - if you think GPUs are expensive then that will be your cheaper route, unless you have a new algorithm, a breakthrough, then make your own ASIC and sell your own card for the greatest profit. Assuming that you're the average person an off the shelf solution is likely better priced. The " biggest challenge " is Bayesian optimization, recurrent network and reinforcement learning or genetic algorithms for parameter tweaking; something that GPUs don't do well. Intel has its Loihi neuromorphic research chip to represent the next direction in AI. While training is done the conventional way the inference is ran on that tiny device, much faster than previous generation devices. Neuromorphic computing is one of the newest directions for AI, an FPGA or GPU is a stone-age tool. Here's what BrainChip says: BrainChip is the leading provider of neuromorphic computing solutions, a type of artificial intelligence that is inspired by the biology of the human neuron - spiking neural networks. Our spiking neural network technology can learn autonomously, evolve and associate information just like the human brain. We have recently unveiled the Akida Neuromorphic System-on-Chip. The Akida™ NSoC is both low cost and low power, making it ideal for edge applications such as advanced driver assistance systems (ADAS), autonomous vehicles, drones, vision-guided robotics, surveillance and machine vision systems. Its scalability allows users to connect many Akida devices together to perform complex neural network training and inference for cybersecurity and financial technology (FinTech) applications. Notice that it also comes full circle back to FPGA. If you could write VHDL and have considerable experience you could have developed that but the FPGA probably isn't an Arria 10. A GPU is the least expensive and quite a flexible solution for the average person, it's also not a one trick pony . Something a few years old, like the Intel Deep Learning Inference Accelerator shown below, costs U$10K.
