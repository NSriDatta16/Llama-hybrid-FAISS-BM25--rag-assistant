[site]: crossvalidated
[post_id]: 177697
[parent_id]: 177677
[tags]: 
If you're referring to Bayesian regression with Gaussian likelihood, the posterior distribution of a Gaussian process is Gaussian: $$p(f(x)\mid X_n,Y_n) = \mathcal{N}\big(\mu_n(x),\sigma_n^2(x)\big)\,,$$ where $X_n$ are the data locations and $Y_n$ are the data values, and $\mu_n$ and $\sigma_n^2$ computed with Bayesian inference : $$\mu_n(x) = \mathbf{k}_n(x)^\top C_n^{-1}Y_n \text{ and } \sigma_n^2(x) = k(x,x) - \mathbf{k}_n(x)^\top C_n^{-1} \mathbf{k}_n(x)\,,$$ where $\mathbf{k}_n(x) = [k(x_t, x)]_{x_t \in X_n}$ is the kernel vector between $x$ and $X_n$, and $C_n = K_n + \eta^2 I$ with $\eta^2$ the standard deviation of the observation noise and $K_n=[k(x_t,x_{t'})]_{x_t,x_{t'} \in X_n}$ the kernel matrix (see the second chapter of Rasmussen and Williams' book ). Therefore a ~95% confidence interval for $x$ is simply $\mu_n(x) \pm 2 \sigma_n(x)$.
