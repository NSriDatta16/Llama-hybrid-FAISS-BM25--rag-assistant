[site]: crossvalidated
[post_id]: 425025
[parent_id]: 424969
[tags]: 
the latent space representation of an autoencoder is linear in that if I average out two representations and propogate back to input space, I will get a real looking object with the features of the two. That doesn't make the latent space "linear", and I've never heard the word linear used in that way. There's no official term for this property of latent spaces we want to describe but maybe we can use the word "interpolable" for now. Let's see what some other people have to say about what makes an interpolable latent space: Bojanowski et al. in Optimizing the Latent Space of Generative Networks say: First, the generator translates linear interpolations in the [latent] space into semantic interpolations in the image space. In other words, a linear interpolation in the [latent] space will generate a smooth interpolation of visually-appealing images Now back to your question: What proof is there to show the space is indeed linear? What is the intution behind in? Since "smooth" and "visually-appealing" are not mathematically well-defined (at least not as used by Bojanowski), "interpolable" is not a well-defined mathematical concept, so there cannot be any proof.
