[site]: crossvalidated
[post_id]: 235471
[parent_id]: 
[tags]: 
Recently started learning ML. Several questions on a case with overfitting

I finished university as master of economics, but have worked for 4+ years as IT-analyst (implementing ERP-systems and other stuff). Recently I decided that I want to do something different and choose data analysis (and big data analysis in prospect). I have studied econometrics, statistics, calculus, theory of probability and other topics which I just need to remember. And had some opportunities to analyse data while working. So currently now I'm studying python and machine learning. I'm starting with Andrew Ng's ML-Class at coursera, but at first I wanted to try something in practice. So I have found this one . Train dataset: ~ 32k lines. ID and ten independent variables (Age, Workclass, Education, Marital.Status, Occupation, Relationship, Race, Sex, Hours.Per.Week, Native.Country) and Income.Group. Test dataset: ~16k without Income.Group. The task is to predict Income.Group. And the only way to check the quality of prediction is to upload file with ID from test dataset and predicted Income.Group. You get accuracy score after uploading. I followed the description of the process and made some changes, here is the result . Some basic analysis at first, then filling missing values with mode (only categorical variables have missing values, so using mode is reasonable), combining categories with less than 5% and transforming data with labeling. I noticed at least two errors. There was a question. What is the percentage of males which have income Is far as I can see from this table: Income.Group 50K All Sex Female 9592 1179 10771 Male 15128 6662 21790 All 24720 7841 32561 The answer is 15128/21790 = 69,4%. But the correct answer is supposed to be 46.5%. Or maybe I'm wrong? And in a couple of cases the code didn't work and I had to look for alternatives. At first I tried Decision Trees for prediction, as in example. Accuracy score was ~81% for train data. But uploading the result yielded the accuracy of ~66%. I tried to change parameters - it didn't change the situation. I tried Random Forest, tried GridSearchCV to optimise it. Even did cross-validation (though I read that Random Forests don't need it) to see the variation. The result remained roughly the same: 0,81-0,84 accuracy for training dataset and 0,66-0,68 for test dataset. So I have several questions, hope someone will help me with them: About transforming. Is simple labeling (as I did) good enough? Or feature engineering is much better? About speed of GridSearchCV. I tried it with 5 parameters with 3-5 values each on my PC. It took several hours, is it normal? So accuracy for train dataset is much higher than accuracy for test dataset. Did I do something fundamentally wrong? Could the problem be with checking itself (there were errors in the example, so there also could be a mistale)? What can be done for improving the score after optimisation with GridSearchCV? I realise that I didn't do a serious analysis, so maybe it could give some insights...
