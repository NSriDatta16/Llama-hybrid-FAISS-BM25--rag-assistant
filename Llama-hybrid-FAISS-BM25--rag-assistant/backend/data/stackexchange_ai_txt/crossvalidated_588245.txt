[site]: crossvalidated
[post_id]: 588245
[parent_id]: 588229
[tags]: 
There are at least three points why you should perform a power analysis before the experiment: If your t-test does not reach statistical significance then you can neither reject the $H_0$ nor the $H_1$ . You simply are not any wiser then before the test. Only if you know, that your sample was reasonably sized the non-significant p value has a meaning. If your t-test is significant, even though your sample size was far to small, something is wrong about your data. There are studies that come up with really unreasonable effects that are not plausible through the experimental group, yet statistically significant (Like in his great talk "Andrew Gelman: Crimes against Data" at minute 7 here ). Either (bad) luck or some effect that was not planned mixes with your data. It happens and it is the most likely reason for largely undersized studies to become significant. So you are in a bad spot if the test is significant and you are in a bad spot if the test is not significant. Your users are probably a worthy ressource and there is a lot of possible things to try and test. (I assume that. My backgroud is medicine and there this point is easier to make.) Should you really claim the next 10,000 users for your experiment and thus block them from any other tests? It might be valid to just claim that 10,000 users should be large enough for everything -- but if so, it should get you thinking why your power analysis shows the opposite. All of this is for Frequentist significance testing. The Bayesian in me opposes but citing the t test I assume you really want p values.
