[site]: datascience
[post_id]: 109806
[parent_id]: 109800
[tags]: 
The difference between the traditional bag of words representation and the word embedding representation is that: bag of words: every index of a vector represents a specific word. Since there must be an index for every possible word, the dimension of every vector must indeed be the full vocabulary size. embedding: words and sentences are represented in an (usually pre-trained) embedding space. The dimension of this space is predefined and arbitrary, and there's no way to know directly what every index represents. Indirectly, it can be proved that indexes can represent quite precise semantic concepts. Anyway, in both cases the features values (which can vary) don't carry the meaning, it's always the fixed indexes which represent a particular semantic concept. In your example, say the word "Hi" has index 1234: the fact that this specific index contains 6 or 4 allows the model to recognize a similarity between these 2 sentences. Note that in an embedding representation it's also the indexes which carry the concept. For example maybe "Hi" would have a important value for the dimension related to "salutations" and this would allow the model to find a similarity with worlds like "hello", "dear X", etc.
