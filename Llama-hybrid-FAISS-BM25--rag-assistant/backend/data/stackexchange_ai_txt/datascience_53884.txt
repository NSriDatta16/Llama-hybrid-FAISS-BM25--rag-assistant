[site]: datascience
[post_id]: 53884
[parent_id]: 53870
[tags]: 
Gradient Descent is an optimization method used to optimize the parameters of a model using the gradient of an objective function ( loss function in NN ). It optimizes the parameters until the value of the loss function is the minimum ( of we've reached the minima of the loss function ). It is often referred to as back propagation in terms of Neural Networks. All the below methods are variants of Gradient Descent. You can learn more from this video . Batch Gradient Descent: The samples from the whole dataset are used to optimize the parameters i.e to compute the gradients for a single update. For a dataset of 100 samples, updates occur only once. Stochastic Gradient Descent: Stochastic GD computes the gradients for each and every sample in the dataset and hence makes an update for every sample in the dataset. For a dataset of 100 samples, updates occur 100 times. Mini Batch Gradient Descent: This is meant to capture the good aspects of Batch and Stochastic GD. Instead of a single sample ( Stochastic GD ) or the whole dataset ( Batch GD ), we take small batches or chunks of the dataset and update the parameters accordingly. For a dataset of 100 samples, if the batch size is 5 meaning we have 20 batches. Hence, updates occur 20 times. All the above methods use gradient descent for optimization. The main difference is that on how much samples are the gradients calculated. Gradients are averaged in Mini-Batch and Batch GD. You can refer to these blogs/posts: Batch gradient descent versus stochastic gradient descent Gradient Descent Algorithm and Its Variants
