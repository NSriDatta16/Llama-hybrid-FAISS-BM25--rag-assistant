[site]: crossvalidated
[post_id]: 200071
[parent_id]: 111654
[tags]: 
In short: Regular AdaBoost (decision stumps, not decision trees as weak learners) usually outperform linear SVMs. Of course this is dependent on your data, data normalization, etc, so this is certainly not a general rule. The reasoning should be fairly clear. AdaBoost can learn non-linear decision boundaries, which is almost always helpful, especially if your data can not be linearly separated. If it can, it is all just a question of empircal results and not much can be said about AdaBoost vs. SVM except that the SVM builds upon Statistical Learning Theory (SLT, http://videolectures.net/mlss09uk_shawe_taylor_lt/?q=machine%20learning%20summer%20school ). I am not entirely sure that can be said for AdaBoost. Given that the SVM maximizes the margin between the point clouds it should, given linearly separable data, outperform AdaBoost since we would expect better generalization from the SVM than from AdaBoost at this point. When the SVM uses a proper non-linear kernel that fits the data better and generalizes well, it usually outperforms AdaBoost. AdaBoost is commonly used because it is very fast. Viola & Jones have used it 15 years ago to realize fast object detection, especially face recognition: https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf In addition, please see my comments on your question.
