[site]: crossvalidated
[post_id]: 213948
[parent_id]: 
[tags]: 
Confused about PCA transformation vectors

I'm trying to get the intuition of how PCA works. So far I understood that: I start from the input matrix $X = [X_{1},...,X_{p}]$ where each $X_{i}$ is composed by $n$ elements that are the $n$ observations for those features ($X_{i}$, in fact) and therefore $X$ is a ($n$ x $p$) matrix. In order to transform my starting problem into a lower-dimensional one I must define a transformation and thus transformation vectors like: $w_{(k)} = (w_{1},...,w_{p})_{(k)}$ that are $p$-dimensional vectors. I must compute the 1st PCA vector by means of: $w_{1} = argmax _{||w||=1} {1 \over m} \sum_{i=1}^m [(x_{i}w^2)]$. The other PCA vectors, in general, will be computed as: $w_{k} = argmax _{||w||=1} {1 \over m} \sum_{i=1}^m [(x_{i}-\sum_{j=1}^{k-1}x_{i}w_{j}w_{j}^T)w]^2$ (in brief: for the k-th PCA vector I have to subtract all other components to the data input matrix in order to pick the feature with the higher variance). My questions now are: are those $w_{(k)}$ vectors composed by just all zeros except for that element that corresponds to the feature I want to consider? I mean: since I know that $w_{(k)}$ are unit vectors (lenght one) then they have to be composed by all zeros except for one component that will be 1. Is this 1 used to pick from the associated $x_{i}$ the feature I want to consider? Are $w_{(k)}$ vector of dimension $(1$ x $p)$?
