[site]: crossvalidated
[post_id]: 455936
[parent_id]: 455901
[tags]: 
Not only neural networks can have more weights, then samples, but there are some preliminary results showing that so called overparametrized neural networks (ones that have more parameters then samples) can overperform smaller networks . Below you can see figure by Belkin et al (2019) illustrate the phenomenon observed in some experiments, where the test error first falls with growing number of hidden units, then starts overfitting when the number of hidden units approaches the number of samples, but after surpassing the interpolation threshold (at this point the network is able to memorize the training dataset), but then it starts falling again with increasing complexity of the network.
