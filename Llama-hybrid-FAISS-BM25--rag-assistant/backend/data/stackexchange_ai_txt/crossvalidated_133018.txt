[site]: crossvalidated
[post_id]: 133018
[parent_id]: 82105
[tags]: 
McFadden's $R^2$ is defined as $1 - LL_{mod} / LL_0$ , where $LL_{mod}$ is the log likelihood value for the fitted model and $LL_0$ is the log likelihood for the null model which includes only an intercept as predictor (so that every individual is predicted the same probability of 'success'). For a logistic regression model the log likelihood value is always negative (because the likelihood contribution from each observation is a probability between 0 and 1). If your model doesn't really predict the outcome better than the null model, $LL_{mod}$ will not be much larger than $LL_0$ , and so $LL_{mod} / LL_0 \approx 1$ , and McFadden's pseudo- $R^2$ is close to 0 (your model has no predictive value). Conversely if your model was really good, those individuals with a success (1) outcome would have a fitted probability close to 1, and vice versa for those with a failure (0) outcome. In this case if you go through the likelihood calculation the likelihood contribution from each individual for your model will be close to zero, such that $LL_{mod}$ is close to zero, and McFadden's pseudo- $R^2$ squared is close to 1, indicating very good predictive ability. As to what can be considered a good value, my personal view is that like that similar questions in statistics (e.g. what constitutes a large correlation?), is that can never be a definitive answer. Last year I wrote a blog post about McFadden's $R^2$ in logistic regression, which has some further simulation illustrations.
