[site]: crossvalidated
[post_id]: 16410
[parent_id]: 
[tags]: 
What makes the formula for fitting logistic regression models in Hastie et al "maximum likelihood"?

I am learning logistic regression from The elements of statistical learning: data mining, inference, and prediction, by Trevor Hastie, Robert Tibshirani, Jerome H. Friedman. Suppose $G$ is a random variable representing response taking class label in $\{1,\ldots,K \}$, and $X$ is a random vector representing predictor taking values in $\mathbb{R}^n$. After modeling $P(G = k|X = x; \theta), k=1,\ldots,K, x \in \mathbb{R}^n$ by logistic model in section 4.4 Logistic Regression , the parameter $\theta$ is estimated from samples $\{(x_i,k_i), i=1,\ldots,N \}$ in section 4.4.1 Fitting Logistic Regression Models : Logistic regression models are usually ﬁt by maximum likelihood, using the conditional likelihood of $G$ given $X$. Since $P(G|X)$ completely specifies the conditional distribution, the multinomial distribution is appropriate. The log likelihood for $N$ observations is $$ℓ(θ) = \sum_{i=1}^N \log P(G = k_i|X = x_i; \theta) $$ My question is how the estimation method is maximum likelihood. Generally speaking, MLE is to estimate parameter $\theta$ of a single distribution $p(z;\theta)$ from its i.i.d. samples $\{z_i, i=1,\ldots,N \}$ by maximizing $\log p(x_1,\ldots, x_N; \theta) = \sum_{i=1}^N \log p(x_i; \theta) $, the log of joint distribution of the samples. The estimation for logistic regression as in the quote seems not using MLE, because: $P(G = k|X = x_i; \theta), i=1,\ldots,N $ are $N$ different distributions, instead of just one distribution as for MLE; in $ℓ(θ) = \sum_{i=1}^N \log P(G = k|X = x_i; \theta) = \log \prod_{i=1}^N P(G = k|X = x_i; \theta)$, I wonder how to explain that "${ (k_i|x_i), i=1,\ldots,N }$" are independent and therefore $\prod_{i=1}^N P(G = k_i|X = x_i; \theta)$ is their joint distribution, as I don't know how to understand $(k_i|x_i)$ which seems not make sense. My guess answer is that $\{(x_i,k_i), i=1,\ldots,N \}$ are assumed to be i.i.d. samples of the single distribution for $(X,G)$. So MLE for the distribution of $(X,G)$ is to maximize $$ \log P((x_1,k_1),\ldots,(x_N,k_N)) = \log \prod_{i=1}^N P(x_i,k_i) $$ $$ = \log \prod_{i=1}^N P(k_i | x_i) P(x_i) = \log \prod_{i=1}^N P(k_i | x_i) + \log \prod_{i=1}^N P(x_i) $$ $P(k_i | x_i)$ is parameterized by $\theta$ as $P(k_i | x_i;\theta$, while $P(x_i)$ is assumed to be independent of $\theta$, so to solve for $\theta$ by MLE for the distribution of $(X,G)$ is equivalent to maximize $\log \prod_{i=1}^N P(k_i | x_i; \theta)$. I wonder if this is a reasonable explanation, and what yours are? Thanks and regards!
