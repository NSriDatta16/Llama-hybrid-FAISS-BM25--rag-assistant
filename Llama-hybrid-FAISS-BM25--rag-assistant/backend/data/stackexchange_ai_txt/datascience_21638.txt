[site]: datascience
[post_id]: 21638
[parent_id]: 
[tags]: 
Bayesian combination of multi-dimensional experts?

I have what seems to me to be a slightly complex version of a decision tree problem, that can't figure out how to model, and I'm trying to avoid the "just dump it into an NN" solution. I have a bunch of expert opinions (from either people or algorithms) where the features dimensions overlap. Both features (inputs) and outputs (decision) are categorical, but I also have a certainty in the decision (let's say 0-1). For example, let's say that the input features are a,b,c,d,e and there is one output (decisions) from among v,w,x,y,z: Features Decision certainty a,b,c w 0.5 b,c,d,e z 0.4 a,c,e,f v 0.75 ... Without the certainty, this would obviously just be a trivial categorical decision tree problem. However, the certainty both simplifies and complexifies the problem. It simplifies it bcs if the features->decision data is inconsistent (which it is!), you'd have no output at all, so the certainty saves me from that failure mode, but on the otherhand, I'm not sure what to correctly do with the certainty. The certainties suggest Bayesian combination, and if the data was unidimensional this would be trivial as well. So I'm kind of caught half way between a decision tree and a bayesian model. An obvious cop out is just to dump it into an NN (or even just an NB or regression), using the certainties either as outputs on the categories, or doing something dumb like replicating the I/O pairings per the (un)certainty. Thanks in advance for any suggestions.
