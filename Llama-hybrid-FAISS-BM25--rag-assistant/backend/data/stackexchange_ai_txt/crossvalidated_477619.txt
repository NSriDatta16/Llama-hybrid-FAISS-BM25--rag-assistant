[site]: crossvalidated
[post_id]: 477619
[parent_id]: 
[tags]: 
In Reinforcement Learning (DQN), is there a way to constrain/penalise the model so that it doesn't take a different action very often?

The RL model I am building is one form of DQN. Its internal network is a regular deep NN. In the application I am looking at, there is a cost for taking a different action (compared to the previous action). That means, even when the model achieves a positive reward r when going from t to t+1, this r would be reduced by the cost if the action taken is different from the last action. Currently the model inputs are the environment data. Is there any way to penalise the model for choosing to take different actions often? Or any other form of constraints that I could apply to the model?
