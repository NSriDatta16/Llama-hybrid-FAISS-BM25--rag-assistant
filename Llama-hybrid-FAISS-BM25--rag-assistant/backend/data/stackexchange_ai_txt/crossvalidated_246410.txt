[site]: crossvalidated
[post_id]: 246410
[parent_id]: 157398
[tags]: 
It probably doesn't matter whether you use $\frac{1}{2}$ or $\frac{1}{n}$ for MSE because the denominator value of 2 and $n$ will never change for the dataset being evaluated. The scale of both methods will differ due to the magnitude of what's calculated, but nevertheless, you'll be dividing by a constant that never changes. If you compare MSE across datasets, then you might go with $\frac{1}{n}$, since that will scale with sample size -- however, within the algorithm being fitted, the artificial neural network (ANN) just needs a reference point to gauge how bad/good the fit is. FYI-The same equation (i.e., $\frac{1}{2}$), is used for MSE in the neural network chapter of Friedman and Tibshirani (Statistical Learning, Springer). Recall, however, MSE is for continuous function approximation using an ANN and cross-entropy is used for classification problems for ANNs. Since you're reading Bishop, what you won't pull away from reading is that a key issue with ANNs is that they like input features to have range [-1,1] with no correlation between features. If there is correlation between features, then an ANN will spend time learning the correlation -- which is what you don't want an algorithm to do. Therefore, run PCA to decorrelate the features first and then input the top 10 orthogonal PC's into the ANN. Last, there is another primary issue with ANNs regarding input samples, which is related to redundancy. That is, many of your records may be the same, and inputting the same (similar) records into an ANN does not help. One of the only groups I know who have developed methods to collapse features and redundant samples simultaneously before input to an ANN is Jurik Research's ( DDR ) Finally, look at Ripley's text on ANNs, since the primary focus will always be how you regularized in order to minimize over-fitting and maximize generalization.
