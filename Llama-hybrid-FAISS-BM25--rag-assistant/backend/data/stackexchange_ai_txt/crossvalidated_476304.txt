[site]: crossvalidated
[post_id]: 476304
[parent_id]: 475973
[tags]: 
This topic is probably most known as Bayesian consistency or Bayesian asymptotics, an area that, broadly speaking, studies the following class of problems: Suppose that there is a "true" data generating density (for instance data are generated i.i.d from a parametric distribution) and we assume a prior over this density (for instance by assuming a prior over the parameters of the data generating density), can we show that with enough samples the posterior will converge to the true data generating density? The basic result in this field is called Doob's theorem, which basically says that this will happen almost for all parameters in the support of the prior, so that if one assumes a parametric model and the prior is not "too stupid", than this convergence will occur. For a detailed reference on Doob's theorem see https://arxiv.org/abs/1801.03122 On the other hand, if the model is nonparametrico, stuff becomes more and more difficult, mainly because the prior null-set (I.e. with prior mass equal zero) can actually be huge. A nice (but complex book) that summarizes the last 30 years of research in this field is "Fundamentals of Bayesian Nonparametric Inference" by Ghosal and van see Vaarr
