[site]: crossvalidated
[post_id]: 205150
[parent_id]: 
[tags]: 
How do bottleneck architectures work in neural networks?

We define a bottleneck architecture as the type found in the ResNet paper where [two 3x3 conv layers] are replaced by [one 1x1 conv, one 3x3 conv, and another 1x1 conv layer]. I understand that the 1x1 conv layers are used as a form of dimension reduction (and restoration), which is explained in another post . However, I am unclear about why this structure as effective as the original layout. Some good explanations might include: What stride length is used and at what layers? What are example input and output dimensions of each module? How are the 56x56 feature maps represented in the diagram above? Do the 64-d refer to the number of filters, why does this differ from the 256-d filters? How many weights or FLOPs are used at each layer? Any discussion is greatly appreciated!
