[site]: crossvalidated
[post_id]: 549620
[parent_id]: 549501
[tags]: 
Frequentist methods have a concept called a confidence procedure. A confidence interval is an example of such a thing. It is the procedure that we have confidence in, not so much the specific interval or point estimate. If you were to perform an experiment very many times over the sample space, you would get many different estimates. The estimators could be used to form a distribution of estimates. That is called the sampling distribution of the estimator. That distribution holds a predictable relationship with the population. The standard error, as well as the sample mean, sample standard deviation, and so forth, are optimal procedures. It begs the question of optimal at what? Most of these estimators are the best unbiased estimator of the population parameter. Usually, the definition of best is that it minimizes some form of loss function. It answers the question of "what is the best estimator of the true value in the population, under the restriction that my estimator will be unbiased and minimize loss." If you would repeatedly take samples from the population, then what you would find is that the sample estimate, for each sample, would approximately average to the population parameter. However, without seeing any other samples, it is the best estimate. If you had many samples then you could perform meta-analysis if you felt it necessary on the estimates collected up to that time. The standard error is the best estimate from the data provided of the standard deviation of the sampling distribution of the estimator of interest. A way to think about this is that every sample has signal and noise. The goal of each procedure is to capture as much signal as is possible, subject to whatever constraints and rules you have in your optimization process, and to discard as much noise as is possible. The sample standard error of the mean, or the sample mean, or the sample median, or the sample estimator of something are the best estimate of the population parameter for a given sample. There is no Bayesian equivalent estimator because if a statistician uses a Bayesian procedure, then it just updates the posterior from the new sample without creating two estimates of the parameter of interest. A sampling distribution is really an artifact of the procedure, such as attempting to find the population median. It isn't so much a property of the population as a property of trying to find a parameter of the population. It is a point estimate of how wide the sample estimates will be rather than how wide the population is. Because the sampling distribution depends on how wide the population is there is a linkage between the descriptive statistics of the population and the descriptive statistics of the sampling process itself. It is a bit dangerous to think in terms of precision. Imagine a school with an even number of students and the school is large. You flip a fair coin to put students in either group A or group B. You notice that SE(A)>SE(B). What aspect of tossing a coin made group B's estimate more precise? Nothing of course. It isn't more precise, it is just different. Both are estimates of the sampling error, one just happens to be larger than the other. The question of the usefulness of the standard error is another matter. If you observe the left big toe of five randomly chosen people from around the world, then you will have less precision in your estimate than if you estimated from a sample size of 30 million people. The standard error will tell you that you have about 2449 times less precision. Is that useful?
