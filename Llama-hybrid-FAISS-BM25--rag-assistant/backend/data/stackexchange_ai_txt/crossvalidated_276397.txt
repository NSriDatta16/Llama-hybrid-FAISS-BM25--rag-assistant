[site]: crossvalidated
[post_id]: 276397
[parent_id]: 276394
[tags]: 
Your last sentence is correct, you only maintain a single set of weights. Let's assume you're using matrices in a two layer network and want to update your weights after every sample. Your inputs will be a vector (a single sample of inputs). Multiply by the first layer's matrix of weights and you have a vector of hidden layer nodes values, to which you apply your activation function. You need to save this until you backpropagate. Then multiply by your second matrix of weights and you have a vector that is your output. When you backpropagate you calculate the derivative of the error and subtract from the second set of weights, then calculate the error on your saved vector of hidden layer node values, take the derivative and subtract from the first set of weights. Now assume you want to do batch (or mini-batch) training. Your inputs now include many samples, so instead of a vector you have a matrix of input values (one row per sample). Multiply this by the first layer of weights and you have a matrix of node values, which you save until you backpropagate. Multiply by the second layer of weights and you have a matrix of outputs. Calculate the derivative of the error on the entire matrix and divide by the number of rows. In essence, you're updating the weights with the average of all the errors from all the samples in your batch. Do the same thing for the matrix of hidden layer node values to update the first layer of weights with the average of the node value error derivatives. In either case you always have a single matrix for each layer of weights. What changes is whether you have a vector or a matrix for the inputs, hidden layer node values, and outputs.
