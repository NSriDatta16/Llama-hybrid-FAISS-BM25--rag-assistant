[site]: datascience
[post_id]: 72597
[parent_id]: 
[tags]: 
Multi-classification: low precision due to imbalanced classes in test data - what to do?

I built a multi-classification model with 3 result classes (XGBoost using R's caret-package): A, B and C. I undersampled my training data - so every class is equally abundant for training. The following numbers relate to test data. My recall numbers are pretty good: rec(A) = 71%; rec(B) = 83%; rec(C) = 78% For a balanced distribution also my precision is pretty good: prec(A) = 76%; prec(B) = 74%; prec(C) = 79%. Unfortunately, in real life class C is much more abundant than classes A and B (76% to 14 % to 10%). Therefore, when applying test data with realistically imbalanced result classe I obtain the following precision values: prec2(A) = 50%; prec2(B) = 43%; prec2(C) = 96%. Apparently, just by chance, whenever I estimate class A or B the chances are pretty high that it is in fact class C. Ideally, I would like to have high precision also when facing imbalanced data. What would you do when you were in my shoes? I thought of the following things: whenever classes A and B are estimated run the data through other models in order to verify the estimate (ensemble) change the training metric; by default caret uses accuracy - I could try using a macro-f1 score keep the class imbalances in the training data and train on a precision metric
