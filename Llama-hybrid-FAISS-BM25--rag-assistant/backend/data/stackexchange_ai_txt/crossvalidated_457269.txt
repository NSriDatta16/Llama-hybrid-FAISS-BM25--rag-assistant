[site]: crossvalidated
[post_id]: 457269
[parent_id]: 
[tags]: 
What are the probabilities in the embedding layer of a Word2Vec?

I am trying to understand how a Word2Vec is being trained. I understand that it can be trained using a CBOW and SkipGram. I am however lost as to what the probabilities are in the embedding layer. Also what I do not understand from this image is, the input is "cat", let's say it is one-hot encoded by [1 0 0 0]. Where is the rest of this SkipGram part? Or is this what is in the embedding layer?
