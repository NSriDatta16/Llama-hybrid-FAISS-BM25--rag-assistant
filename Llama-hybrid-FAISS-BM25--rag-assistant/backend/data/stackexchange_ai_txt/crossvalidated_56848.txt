[site]: crossvalidated
[post_id]: 56848
[parent_id]: 
[tags]: 
Understanding the construction of Dirichlet process

I'm trying to understand the construction process of DP, however, with little background in measure theory, the original papers are hard to read, but I believe the ideas behind these papers can be followed. Let $y_1, y_2, \ldots, y_n$ be i.i.d. with distribution function $F$, which is characterized by $k$ unknown probabilities $\theta_1, \ldots, \theta_k$, i.e., $y_i$s are discrete, taking a finite number $k$ of possible values. Bayesian inference based on these observations often put a Dirichlet prior on the parameters of $F$. Posterior and predictive distribution can then be calculated. That finite mixture models is not hard to understand, but... When the $y_i$s can take infinitely many possible values ($k \rightarrow \infty$), then the Bayesian nonparametric models can deal with this situation but I don't understand why. Here are my questions: $k \rightarrow \infty$ means that $y_i$ can take any point mass on $\mathcal{R}$, so it is a continuous r.v. isn't it? In the finite case, $F$ is indexed by $1, \ldots, k$, while in the infinite case, $F$ should be indexed by partitions $(T_1, \ldots, T_k)$ (Ferguson, 1972), how can I get the intuition of the latter case from the former?
