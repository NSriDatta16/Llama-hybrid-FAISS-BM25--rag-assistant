[site]: datascience
[post_id]: 109200
[parent_id]: 
[tags]: 
Proof that multihead works better than single head in transformer

According to this post, the purpose of the multihead is to have 'gradient splitting' across heads, which is achieved by random initialization of weight matrices for Q, K and V in each head. But how can we prove this can solve the problems in using single head? Specifically, how can the splitting of gradients ensures within each output attention vector for each word it wouldn't overemphasize (the attention) of itself?
