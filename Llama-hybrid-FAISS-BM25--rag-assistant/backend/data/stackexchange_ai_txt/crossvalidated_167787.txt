[site]: crossvalidated
[post_id]: 167787
[parent_id]: 
[tags]: 
Cross-entropy cost function in neural network

I'm looking at the cross-entropy cost function found in this tutorial : $$C = -\frac{1}{n} \sum_x [y \ln a+(1−y)\ln(1−a)]$$ What exactly are we summing over? It is, of course, over $x$ , but $y$ and $a$ don't change with $x$ . All of the $x$ 's are inputs into the one $a$ . $a$ is even defined in the paragraph above the equation as a function of the sum of all $w$ 's and $x$ 's. Also, $n$ is defined as the number of inputs into this particular neuron, correct? It is worded as "the total number of items of training data" . Edit: Am I correct in thinking that $$C= -\frac{1}{n} \sum_x [y \ln a+(1−y)\ln(1−a)]$$ would be the cost function for the entire network, whereas $$C = [y \ln a+(1−y)\ln(1−a)]$$ would be the cost for the individual neuron? Shouldn't the sum be over each output neuron?
