[site]: crossvalidated
[post_id]: 154461
[parent_id]: 105236
[tags]: 
Background: The Meaning and Importance of the ARL This quality control application contemplates obtaining a sequence of random variables $X_0, X_1, \ldots, X_n$ representing measurements of process samples. Control limits, constituting an interval $\mathcal{I} = [l,u]$ have been established. A "good" measurement is one for which $X_i \in \mathcal{I}$ : it is within the control limits. Otherwise it is "bad." The process is "in control" when the $X_i$ act like independent identically distributed variables. Otherwise, the process is "out of control" (OOC). The objective is to identify, as soon as practicable, when the process might be OOC, using a procedure that is unlikely to identify an in-control process as OOC. After each sample $i=0, 1, \ldots$ is taken, a binary decision will be made: either the process is in control or it is out of control (OOC). The OOC decision is made the first time a sequence of $k$ consecutive bad measurements are observed. The value of $k$ will be determined by balancing the costs of making the two kinds of decision errors. (That is, letting the process continue when it is not in control and stopping it when it is in control.) One measure of how well this decision procedure works is its Average Run Length (ARL): this is the expected number of samples that will be taken before an OOC decision is incorrectly made. Ideally, the ARL will be large. Solution To analyze the ARL, preparatory to choosing what value of $k$ to use, let $p$ be the common chance that $X_i$ is bad when the process is in control. How long the process is expected to run depends on how many bad values $j$ have just been observed in a row. Let $e_k(k-j)$ be the expected length conditional upon having observed exactly $j$ bad values in a row. (That is, if the next $k-j$ values are bad, the process will stop.) When $j=k$ , the process stops immediately with an OOC decision, leading to the initial condition $$e_k(0) = 0.$$ For $j \lt k$ , an OOC decision is impossible, so at least one more sample will be obtained. That contributes $1$ to the expectation no matter what. With probability $p$ the next sample will be bad (extending the run to $j+1$ ). With probability $1-p$ it will be good (starting the run over at $j=0$ ). Thus $$e_k(k-j) = pe_k(k-j-1) + (1-p)e_k(k) + 1.$$ The unique solution to this recursive system is $$e_k(i) = \frac{1 - p^i}{(1-p)p^i},\ i = 0, 1, \ldots, k.$$ The solution can be found in many ways, all requiring some sophistication, but it is readily verified by plugging it in to the recurrence relation and checking that it's true for $i=1, 2, \ldots, k$ and checking the initial condition $e_k(0) = (1-p^0)/((1-p)p^0) = 0$ . Intuitive Explanation This result can intuitively be understood in a less-rigorous fashion. The expected amount of time needed to observe $i+1$ bad results in a row must equal the expected amount of time needed to observe $i$ bad results in a row, plus $1$ to account for observing the very next result, all multiplied by the expected number of attempts needed for the last of those $i+1$ results to be bad. The latter has a chance $p$ of occurring. The expected waiting time for it to show up is $1/p$ . Thus $$e_k(i+1) = \frac{1}{p}\left(1 + e_k(i) \right).$$ Starting with $e_k(0) = 0$ it's easy to obtain $e_k(1) = 1/p$ , $e_k(2) = (1 + 1/p)/p = 1/p + 1/p^2$ , and generally $$e_k(i) = \frac{1}{p} + \frac{1}{p^2} + \cdots + \frac{1}{p^i},\ 0 \lt i \le k.$$ These geometric series sum to the formula originally derived for $e_k(i)$ .
