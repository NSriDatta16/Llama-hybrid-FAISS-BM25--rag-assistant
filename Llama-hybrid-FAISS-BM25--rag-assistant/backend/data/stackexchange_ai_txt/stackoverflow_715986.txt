[site]: stackoverflow
[post_id]: 715986
[parent_id]: 715840
[tags]: 
Very interesting train of thought! First, what is bitrot? The Software Rot article on wikipedia collects a few points: Environment change: changes in the runtime Unused code: changes in the usage patterns Rarely updated code: changes through maintenance Refactoring: a way to stem bitrot By Moore's Law , delta(CPU)/delta(t) is a constant factor two every 18 to 24 months. Since the environment contains more than the CPU, I would assume that this forms only a very weak lower bound on actual change in the environment. Unit: OPS/$/s, change in Operations Per Second per dollar over time delta(users)/delta(t) is harder to quantify, but evidence in the frequency of occurrences of the words "Age of Knowledge" in the news, I'd say that users' expectations grow exponentially too. By looking at the development of $/flops basic economy tells us that supply is growing faster than demand, giving Moore's Law as upper bound of user change. I'll use function points ("amount of business functionality an information system provides to a user") as a measure of requirements. Unit: FP/s, change in required Function Points over time delta(maintenance)/delta(t) depends totally on the organisation and is usually quite high immediately before a release, when quick fixes are pushed through and when integrating big changes. Changes to various measures like SLOC , Cyclomatic Complexity or implemented function points over time can be used as a stand-in here. Another possibility would be bug-churn in the ticketing system, if available. I'll stay with implemented function points over time. Unit = FP/s, change in implemented Function Points over time delta(refactoring)/delta(t) can be measured as time spent not implementing new features. Unit = 1, time spent refactoring over time So bitrot would be d(env) d(users) d(maint) d(t) bitrot(t) = -------- * ---------- * ---------- * ---------------- d(t) d(t) d(t) d(refactoring) d(env) * d(users) * d(maint) = ------------------------------ d(t)² * d(refactoring) with a combined unit of OPS/$/s * FP/s * FP/s = (OPS*FP²) / ($*s³) . This is of course only a very forced pseudo-mathematical notation of what the Wikipedia article already said: bitrot arises from changes in the environment, changes in the users' requirements and changes to the code, while it is mitigated by spending time on refactoring. Every organisation will have to decide for itself how to measure those changes, I only give very general bounds.
