[site]: crossvalidated
[post_id]: 436668
[parent_id]: 
[tags]: 
What are typical magnitudes of gradients in neural networks?

We've all heard about the vanishing and exploding gradient problems. But I'm having a hard time finding any concrete numbers like "this gradient is x which is way too small/large". I'm wondering if there are any well established, scientifically researched rules of thumb, that are more than just gut feelings, to classify gradients into categories like "way too small", "just about right", and "way too large"? I know that the answer might highly depend on the network architecture. But maybe there are some rules like "for CNNs, this is considered too small/large" or "for RNNs this is considered too small/large"? As an example, have a look at this question on SO where the OP has the premise of their gradients being too small. Can this premise be verified?
