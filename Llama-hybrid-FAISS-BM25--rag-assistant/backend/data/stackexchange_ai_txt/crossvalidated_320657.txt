[site]: crossvalidated
[post_id]: 320657
[parent_id]: 312206
[tags]: 
Some time ago I tried this idea on 20 newsgroups data. I used GloVe embeddings from the authors site (Wikipedia ones). Aggregating word embeddings using TF-IDF doesn't give good results. It is actually worse than just using TF-IDF features. See results in this notebook ( Accuracy on tfidf data vs Accuracy on weighted embedded words ). I also made plots of truncated SVD/PCA of the encoded documents - it seems like aggregated embeddings just make everything close to everything. To illustrate this I tried to find closest words for document encodings in Word embeddings space - it seems like they just lie close to common words (see Closest $10$ words to mean-aggregated texts ). That being said, This notebook is just a toy example and it only suggests that the simplest approach won't work for this data . For instance I didn't try to filter out common words based on some threshold. Also maybe it would make more sense to first extract summaries from the documents first (for example TextRank sort of retrieves most informative paragraphs based partly on TF-IDF score of their words). If you want to try more elaborate techniques, I think that Gensim covers much of this stuff (for example extractive summarization via TextRank and similar algorithms).
