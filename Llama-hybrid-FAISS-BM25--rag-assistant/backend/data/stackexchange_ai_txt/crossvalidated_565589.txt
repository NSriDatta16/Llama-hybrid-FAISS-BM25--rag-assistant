[site]: crossvalidated
[post_id]: 565589
[parent_id]: 
[tags]: 
Trainable weights in attention mechanism

I am wondering what are the trainable weights inside an attention-powered transformer. I figure the feed-forward layer contain trainable weights and the token embeddings, but what other parts contain trainable weights? i.e. when you store a trained model, what parts are you actually storing I also wonder what weights inside the attention-mechanism must be temporary stored, or being held in memory, during a training iteration until the backpropagated gradient descent comes back?
