[site]: crossvalidated
[post_id]: 639793
[parent_id]: 588618
[tags]: 
Your question seems theoretical, but I struggled to find the answer to the corresponding practical question of how to actually run inference for a single unseen node. It's well possible that many others will be shunted by Google; when a solution hit me, I knew I had to post it somewhere. If you expect to train a model on graph structure and use it on a new node, you should have a mechanical way of imposing graph structure on that node. You can do this by link prediction (a cheap shortcut is to find the nearest-neighbor in the feature vector space and copy its edges) or, if you have the flexibility, by restating your problem so the graph structure of existing nodes doesn't have to be recomputed. For example, you can try to work around the need for a nearest neighbors graph (for example, in latitude-longitude space) by binning each component and connecting two nodes if they're in the same lat-lon rectangle. If you're able to do this, you can (1) take your entire training graph to the inference side and (2) replace the least-connected node (so to minimize distortions) with your new node. This sounds silly when I first explain it to people, but the point of using GNNs for prediction is making use of graph structure, and graph sizes need to be kept constant to reuse a model that has input size assumptions. An alternative (2') is to add a blank unconnected or fully-connected node to the graph so you don't have to choose some node to delete, but I don't have enough of a mental model of how either choice distorts the model's "understanding" of the graph structure. Now, I said "input size", but an annoying "usability" issue with torch_geometric (and possibly other packages) is that they take grapjs in edge list format, so you can't "reuse nodes" as above unless they have the same exact edges of your marginal node. It took me a couple of weeks until I figured this out: class M_GraphSAGE(torch.nn.Module): def __init__(self, **sage_kwargs): super(M_GraphSAGE, self).__init__() self.model = torch_geometric.nn.models.GraphSAGE(**sage_kwargs).to(device) def forward(self, x, M): edge_index = M.nonzero().T return self.model(x, edge_index) model = M_GraphSAGE(...) This version takes adjacency matrices M as an input, which fixes only the number of nodes. This, of course, makes for slower training, but lets you make small changes to the graph at inference time -- although the extent to which the changes to the edge list are also small shouldn't be taken for granted. All of this amounts to a bunch of small hacks, and there might be much better methods informed by a deeper understanding of the theory. But there's a surprising paucity of discussions and writing on the meat-and-potatoes issues like using GNNs on new instances. I've been told by academics that GNNs are essentially pass√©, but I've been finding great value in the conceptual distinction between features of individuals and between-individuals relations. (I suspect the success of tree-based models is that they essentially learn bundles of cross-feature commonalities so that predictive power can be squeezed from relational information that's been forced into tabular shape; IIRC that classical Leo Breiman paper on the Two Cultures almost makes my point. But this is an outrageously speculative digression.)
