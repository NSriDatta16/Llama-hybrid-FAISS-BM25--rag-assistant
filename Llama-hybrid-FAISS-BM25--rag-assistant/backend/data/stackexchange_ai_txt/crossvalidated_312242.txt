[site]: crossvalidated
[post_id]: 312242
[parent_id]: 312212
[tags]: 
With flat priors and a large sample size (and many times without both of these things), (a) Bayesian and frequentist point-estimates will be virtually the same, and (b) the credible and confidence intervals will virtually cover the same range, leading to the same "hypothesis test" decision of "reject" or "fail to reject" the null. (Tim's comment shows this). However, I will push back on this sentence: 'in large samples and for uninformative priors the results are the same, period' There is no situation where the "results are the same ." Period. Whether or not the confidence/credible interval includes zero is not the totality of the "results." It is one aspect of the results, and it is many times a trivial aspect. Let's say you estimate a regression coefficient of b = 0.5. Frequentist p -values will tell you the probability of observing your data—or more extreme data—given that the null hypothesis is true. However, using MCMC, you can sample from the posterior to calculate the probability that the coefficient is greater than zero; that is, you have the probability of an alternative hypothesis, given the data. Using Bayesian estimation, you can also compare probabilities—given the data—of different hypotheses (i.e., "Bayes factors"). You cannot do this in the frequentist paradigm. As Tim mentioned, the mode of the posterior will be virtually the same thing as the point-estimate in a maximum likelihood, frequentist paradigm. However, in Bayesian estimation, we can look at the median or mean of the posterior, as well, as often our posteriors are not (and should not) be symmetric. The argument you want to make, however, is a straw-person argument, as it does not make sense to select uniform priors. For example, nobody would choose a flat prior from $-\infty$ to $+\infty$ for a variance, because variances cannot be negative. Similarly, if all of our variables are standardized, we would expect there to be very few cases where coefficients are +1. There are certain situations where—even if you have uniform priors and large sample sizes—maximum likelihood approaches will not converge. In this case, giving even a weakly-informative prior will allow you to get some type of estimation, instead of your maximum likelihood procedure just saying, "Could not converge." I have run into this with multilevel models. In short: Having virtually the same point estimates and 95% credible/confidence interval upper and lower bounds does not mean that you have "the same results." We might make certain inductive, big-picture conclusions in the exact same way, but the totality of the results are not the same. I think the wealth of information you get from MCMC posteriors (i.e., you can estimate how uncertain you are about variances of coefficients!) is often overlooked when comparing frequentist and Bayesian methods.
