[site]: datascience
[post_id]: 104261
[parent_id]: 
[tags]: 
Feature engineering using XGBoost regressor

If I want to train a regression model through tree based algorithms like XGBoost. Suppose that there have 5 features x1, x2, x3, x4, x5 and a target y. And some experts said x2 minus x3 is highly correlate to y. Should I put x2-x3 in the model as the sixth feature, or XGBoost will automatically learn it by just put x1~x5 in model. As I know, a linear mode can learn a formula from features, and how about tree based methods? If tree based can also do the same thing, does the size of data matter?
