[site]: crossvalidated
[post_id]: 295784
[parent_id]: 
[tags]: 
Connection between loss and likelihood function

Simple question: Can we generally think of the loss function as the negative of the likelihood function? For instance with regards to logistic regression, the likelihood function in a binary setting is $\sum_i y^{i}\log(h(x^i))+\log(1-y^i)(1-h(x^i))$ while the loss function is $- \Big[\sum_i y^{i}\log(h(x^i))+\log(1-y^i)(1-h(x^i))\Big]$ However, in Maximum-A-Posteriori (MAP) tasks I have seen that the loss function is derived by maximizing the posterior, i.e. the loss function being the differentiation of the likelihood function times the prior.
