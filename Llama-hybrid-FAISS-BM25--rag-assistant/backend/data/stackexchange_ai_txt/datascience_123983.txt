[site]: datascience
[post_id]: 123983
[parent_id]: 
[tags]: 
How to work with multiple feature types on autoencoder?

This is my first post here. I am working on an adversarial autoencoder that receives different features, encodes them, and decodes them. For instance, suppose you have a dataset from a large survey with the following specs: Some (> 10) binary features (dummy variables) Some (also > 10) categorical features (may be ordinal or nominal, e.g., schooling, neighborhood index, etc.). Some (also > 10) numeric features (may be integers or float, e.g., number of children, income, height, etc.) My question is: Are there any good practices to work with features that have multiple data types? So far, I have tried the following: Using MSE and ignoring the problem ( somewhat ok ) One-hot encoding of all the categorical features and use MSE ( not that good ) One-hot encoding of all the categorical features and change the output layer of the decoder to: Spit sigmoid dense layer when the feature is binary, then use binary cross-entropy in the loss function. Spit softmax $k$ dense layer when the feature is categorical ( $k$ categories) and then use categorical cross-entropy in the loss function. Spit a dense layer and use MSE as a loss function when the feature is numeric. Then, I combine all this to create the new loss function. ( results are medium , close to MSE, but MSE still performs better) I could also use sparse categorical cross-entropy for the discrete variables, but I don't know whether there is a better route. Thanks in advance!
