[site]: crossvalidated
[post_id]: 615214
[parent_id]: 
[tags]: 
Closed form expression for the gradient of a fully connected neural network with respect to its parameters

A real-valued feedforward/fully connected neural network with activation function $\sigma : x\in \mathbb R \mapsto \max \{0,x\}\equiv \text{ReLU}(x)$ can formally be seen as a function $f_\theta :\mathcal X\subseteq \mathbb R^{d}\to\mathbb R$ defined by the expression $$f_{\theta} (x) = A^{(L)}\circ \sigma \circ A^{(L-1)}\circ\ldots\circ \sigma \circ A^{(1)}(x)$$ Where for all $1\le k\le L $ , we define $A^{(k)}(x) := W_k x + b_k$ for $W_k,b_k$ respectively weight matrices and bias vectors of appropriate dimensions, and the activation $\sigma$ is applied elementwise. We can aggregate the weights and biases $\big((W_k,b_k)\big)_{l=1}^L $ in a big column vector which we denote $\theta \cong\big((W_k,b_k)\big)_{l=1}^L \in \mathbb R^{p}$ where $p$ is the total number of parameters, and therefore denote by $f_\theta$ the neural network realized by this set of parameters. One can refer to this paper for a more precise description of this formal representation of deep neural networks as functions. What I want to ask is the following : for given $x\in\mathcal X$ , is there a closed form expression for $\nabla_\theta f_\theta (x) $ ? I'm aware that I didn't give the explicit representation $\theta$ so the question is a bit ill-posed, but the point is that I am looking for a closed form expression for the derivative of $f_\theta $ with respect to each parameter in $\theta$ , i.e. each entry of $W_k$ and $b_k$ for $1\le k\le L$ . I know that in practice, people just use backpropagation so that there is no need to ever compute the whole gradient of $f_\theta (x) $ , however I happen to need it for some theoretical investigations. It is possible to go through the algebra carefully and work it out, but it is extremely tedious and I haven't been succesful in deriving a general formula so far. Is this closed-form expression available somewhere in the literature by any chance ? Since Neural Networks have been around for a while, I would guess (hope) that the answer is yes, but haven't been able to find anything. While I'm at it, I would also be interested in a closed-form expression for the Hessian $\nabla^2_\theta f_\theta(x) $ . Thanks in advance.
