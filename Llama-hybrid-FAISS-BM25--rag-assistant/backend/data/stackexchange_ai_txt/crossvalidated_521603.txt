[site]: crossvalidated
[post_id]: 521603
[parent_id]: 521593
[tags]: 
Some comments. I'm assuming that you mean the t-test for two independent samples, although most arguments also hold for the one-sample test (mostly in a straighter way). (1) "I know that the t-test theoretically doesn't work on non-normal data." This is not true. The t-test is based on theory that requires normality, meaning that it does work (in a certain sense of the word) for normal data. No theory says that the t-test is bad for non-normal data in any generality (it is known to be bad with specific non-normal distributions, particularly where variances don't exist). In fact, in a number of non-normal situations the t-test can be theoretically justified on Central Limit Theorem grounds if the sample size is large enough. (2) Generally the appropriateness of the t-test (as of any statistical method) cannot be assessed based on the data alone, but also depends on issues such as whether the data are randomly sampled, whether the hypotheses are appropriate for the research question of interest, whether there are hidden dependencies (that cannot always be diagnosed from the data), what is known about the reliability and validity of measurements etc. (3) Note that the standard null hypothesis and alternative of the t-test are defined in terms of the normal assumption. These need to be adapted if the underlying distributions are non-normal. This may seem trivial at first (one may still say that the H0 is that means are equal and the alternative that they differ - as long as means exist which in general isn't guaranteed), however there's a catch. In a real situation it can easily happen that not only variances differ between the two groups, but also the distributional shapes. For normal distributions, the mean, median, and center of symmetry are the same. For general distributions this does not normally hold. Meaning that testing whether means are the same (against them being different) may be a different problem with a different result than testing whether medians are the same. Imagine for example that you have in one group a mixture distribution $0.99{\cal N}(0,1)+0.01{\cal N}(100,1)$ and in the other group ${\cal N}(1,1)$ . These two have the same mean but are very different, and the median of the second group will be much higher than the median of the first. In order to check whether the t-test "works" you will need to decide first whether in such a situation you think that the "correct" result is "reject" (because the second group dominates 99% of the first group) or "not reject" (because the means are the same). This decision is a personal one, depending on the situation, and cannot be made by the data, nor is there a "mathematically correct" answer to this. It depends on this decision what it even means to say that "the t-test works". (4) Some hardcore Bayesians may say that significance tests are never appropriate. (5) Violations of the independence and identical within-group distribution assumption are often much worse than non-normality, so these need to be checked, too. Unfortunately, this is not always possible, for example if you don't know in which order the observations were made, or if dependence or non-identity is irregular so that plots and standard tests cannot pick it up. One reason more that it is essential to know how the data were obtained. (6) Even if we're willing to assume i.i.d. within groups, using any automatic data-dependent rule in order to decide whether to run a t-test or, say, a Mann-Whitney test is problematic, because this in itself violates the assumptions (conditioning on the decision rule accepting normality, the distribution of the data is different from unconditionally, meaning that if the data were i.i.d. distributed normally before, they are no longer afterwards). A number of authors who have investigated the combination of a normality test and then t-test conditionally on not rejecting normality recommend against such a practice. See Example 1 and Sec. 5.3 for an overview in this paper: https://arxiv.org/abs/1908.02218 I. Shamsudheen and C. Hennig: "Should we test the model assumptions before running a model-based test?" These authors argue that preliminary normality testing may have its advantages and is somewhat too pessimistically portrayed in the literature. They don't recommend a fixed rule though. (7) You may probably want to define whether the test "works" in terms of type I and type II error probabilities. Note that in any case the transition between satisfactory and unsatisfactory results is gradual and continuous, meaning that specifying a sharp cutoff between "works" and "does not work" will be somewhat arbitrary (even if there were no other issues).
