[site]: crossvalidated
[post_id]: 349127
[parent_id]: 348937
[tags]: 
The likelihood function is defined independently from $-$ or prior to $-$ the statistical paradigm that is used for inference, as a function, $L(\theta;x)$ (or $L(\theta|x)$ ), of the parameter $\theta$ , function that depends on $-$ or is indexed by $-$ the observation(s) $x$ available for this inference. And also implicitly depending on the family of probability models chosen to represent the variability or randomness in the data. For a given value of the pair $(\theta,x)$ , the value of this function is exactly identical to the value of the density of the model at $x$ when indexed with the parameter $\theta$ . Which is often crudely translated as the "probability of the data". To quote more authoritative and historical sources than an earlier answer on this forum, "We may discuss the probability of occurrence of quantities which can be observed . . . in relation to any hypotheses which may be suggested to explain these observations. We can know nothing of the probability of hypotheses . . . [We] may ascertain the likelihood of hypotheses . . . by calculation from observations: . . . to speak of the likelihood . . . of an observable quantity has no meaning." R.A. Fisher, On the ``probable error’’ of a coefficient of correlation deduced from a small sample . Metron 1, 1921, p.25 and "What we can find from a sample is the likelihood of any particular value of r, if we define the likelihood as a quantity proportional to the probability that, from a population having the particular value of r, a sample having the observed value of r, should be obtained." R.A. Fisher, On the ``probable error’’ of a coefficient of correlation deduced from a small sample . Metron 1, 1921, p.24 which mentions a proportionality that Jeffreys (and I) find superfluous: "..likelihood, a convenient term introduced by Professor R.A. Fisher, though in his usage it is sometimes multiplied by a constant factor. This is the probability of the observations given the original information and the hypothesis under discussion." H. Jeffreys, Theory of Probability , 1939, p.28 To quote but one sentence from the excellent historical entry to the topic by John Aldrich ( Statistical Science , 1997): "Fisher (1921, p. 24) redrafted what he had written in 1912 about inverse probability, distinguishing between the mathematical operations that can be performed on probability densities and likelihoods: likelihood is not a ‘‘differential element,’’ it cannot be integrated." J. Aldrich, R. A. Fisher and the Making of Maximum Likelihood 1912 – 1922 , 1997 , p.9 When adopting a Bayesian approach, the likelihood function does not change in shape or in nature. It keeps being the density at $x$ indexed by $\theta$ . The additional feature is that, since $\theta$ is also endowed with a probabilistic model, the prior distribution, the density at $x$ indexed by $\theta$ can also be interpreted as a conditional density, conditional on a realisation of $\theta$ : in a Bayesian modelling, one realisation of $\theta$ is produced from the prior, with density $\pi(\cdot)$ , then a realisation of $X$ , $x$ , is produced from the distribution with density $L(\theta|\cdot)$ , indexed by $\theta$ . In other words, and with respect to the proper dominating measure, the pair $(\theta,x)$ has joint density $$\pi(\theta) \times L(\theta|x)$$ from which one derives the posterior density of $\theta$ , that is, the conditional density of $\theta$ , conditional on a realisation of $x$ as $$\pi(\theta|x) \propto \pi(\theta) \times L(\theta|x)$$ also expressed as $$\text{posterior} \propto \text{prior} \times \text{likelihood}$$ found since Jeffreys (1939) . Note: I find the distinction made in the introduction of the Wikipedia page about likelihood functions between frequentist and Bayesian likelihoods confusing and unnecessary, or just plain wrong as the large majority of current Bayesian statisticians does not use likelihood as a substitute for posterior probability. Similarly, the "difference" pointed out in the Wikipedia page about Bayes Theorem sounds more confusing than anything else, as this theorem is a probability statement about a change of conditioning, independent from the paradigm or from the meaning of a probability statement. ( In my opinion , it is more a definition than a theorem!)
