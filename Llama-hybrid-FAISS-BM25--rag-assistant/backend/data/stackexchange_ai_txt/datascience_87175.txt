[site]: datascience
[post_id]: 87175
[parent_id]: 77323
[tags]: 
Building embeddings is the first step in graph processing. We mainly use to apply mathematics on something like plane or hyperplane. Almost all mathematical methods can work with linear space, especially linear algebra which we use in neural networks and computer data processing. So, in fact, embedding is just one of the first steps we need to do in order to apply some math on graph data. We can embed node features to a lower dimension, we can also embed an entire graph to some space we know how to work with. Then apply functions, transformations, whatever we want. When you are building an embedding, your task is to represent a graph in linear space so that it will keep all the characteristics it has in non-linear "graph" space. In this article you can read more about different approaches to graph embedding. I will just cite one phrase from there. Machine learning algorithms are tuned for continuous data, hence why embedding is always to a continuous vector space.
