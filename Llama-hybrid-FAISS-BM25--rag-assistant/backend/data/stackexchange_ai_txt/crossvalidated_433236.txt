[site]: crossvalidated
[post_id]: 433236
[parent_id]: 
[tags]: 
Predicting a new observation: marginal mean, estimated marginal mean, or fixed effects estimator?

I'm interesting in making predictions using a random-effects model on new data that occur in new groups. Which estimator is most appropriate? I fit a Poisson random effects model on some fake data: $$y_i \sim Poisson(\exp(\beta_0 + b_i)), \quad i = 1, \ldots, 100$$ $$b_i \sim N(0, \sigma_b^2)$$ library(lme4) set.seed(666) y In the fake data simulation, $\beta_0 = 0$ and $\sigma_b^2 = 1$ . > fit Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [glmerMod] Family: poisson ( log ) Formula: y ~ 1 + (1 | group) Data: df AIC BIC logLik deviance df.resid 338.3075 343.5179 -167.1538 334.3075 98 Random effects: Groups Name Std.Dev. group (Intercept) 0.9156 Number of obs: 100, groups: group, 100 Fixed Effects: (Intercept) -0.02414 I can think of four ways to estimate the next observation: (e1) Ordinary marginal mean: Just take the average of $y$ . Ignores the model. (e2) The "unconditional" fixed effect estimate: I set the estimated random effects to $0$ and use $\exp(\hat\beta_0)$ from the model fit. (lme4 permits this if you set re.form = NA in the predict() function) (e3) Instead of setting the random effects to $0$ , I take the average of the observed random effects. The estimator is $\exp(\hat\beta_0 + \frac{1}{100}\sum_{i=1}^{100} \hat{b}_i)$ . (e4) The estimated marginal mean: $\frac{1}{100} \sum_{i=1}^{100} \exp(\hat\beta_0 + \hat{b}_i)$ . Okay, those are easy enough to compare in R: mean(y) #"ordinary" marginal mean #> 1.46 exp(fit@beta) #The "Set random effects to 0" estimator #> 0.97 exp(fit@beta + apply(ranef(fit)$group, 2, mean)) #ave. random effect #> 1.07 mean(predict(fit, df, type = "response")) # Estimated Marginal Mean #> 1.47 There's quite a bit of difference between these predictions. I can write a small simulation that compares them. Let's say the best predictor is the one with lowest MSE. l1 Is this correct? Is the estimated marginal mean the 'right' estimator of a new observation in a new group? I'm struggling to understand when, if ever, these unique estimators should be used when dealing with a random effects model. Edit : My sim was not correct! I took @dimitris' estimator and @mavery's simulation oracle, and it seems like the new estimator is actually the best predictor of a new $y$ from a new group: ynew $group, 2, mean)) e4 sigma$group ^ 2) l1[i] Which makes me wonder, why does lme4 use unconditional "population level" estimators (via reform = NA , as in $\exp(\hat\beta_0$ )), instead of calculating $\exp(\hat\beta_0 + \hat\sigma_b^2/2)$ when a new level is encountered??? Aside, the ordinary marginal mean and the estimated marginal mean seems to be about as good as @dimitis' estimator.
