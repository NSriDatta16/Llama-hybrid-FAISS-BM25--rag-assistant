[site]: datascience
[post_id]: 66019
[parent_id]: 66009
[tags]: 
Sadly, I don't think that Multilingual BERT is the magic bullet that you hoped for. As you can see in Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT (Wu and Dredze, 2019) , the mBERT was not trained with any explicit cross-lingual task (for example, predicting a sentence from one language given a sentence from another language). Rather, it was trained using sentences from Wikipedia in multiple languages, forcing the network to account for multiple languages but not to make the connections between them. In other words, the model is trained with predicting a masked instance of 'cat' as 'cat' given the rest of the (unmasked) sentence, and predicting a foreign word meaning 'cat' in a masked space given a sentence in that language. This setup is does not push the model towards making the connection. You might want to have a look in Facebook's LASER , which was explicitly trained to match sentences from different languages. P.S The fact that the sentences do not have the same representation does not mean that the mBERT cannot be used for zero-shot transfer learning across languages. Again, please see Wu and Dredze
