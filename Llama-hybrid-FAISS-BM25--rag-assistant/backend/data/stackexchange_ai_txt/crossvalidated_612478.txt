[site]: crossvalidated
[post_id]: 612478
[parent_id]: 533581
[tags]: 
A typical use of oversampling or other artificial balancing of the categories is to make it so the minority category has a better chance of having a prediction above the threshold to transform continuous model predictions into discrete categorical predictions. However, when the categories are imbalanced, it might be that the majority category is always more likely. Consequently, to get predictions that are aligned with the reality of how frequently the categories really occur, those artificially inflated high predictions have to be toned down. So the strategy is: Artificially inflate the probability of membership in the minority category so thresholded predictions are more likely to be above the threshold. Calibrate these inflated predictions so the final predictions of a pipeline are related to the true probabilities of event occurrence. That is, we do not want a predicted probability of $0.6$ to correspond to the event happening $20%$ of the time, as this would mean that the predicted probability is not telling the truth. At best, this strikes me as inefficient. $^{\dagger}$ At worst, it misleads aspiring machine learning modelers into deemphasizing the rich information available in the probability predictions and to obsess over a threshold of $0.5$ just because that is the software default. Even if there is considerable information available in full probability predictions, at the very least, it is possible to change the threshold to something more reasonable for the task if you must use a threshold (such as in an automated software system that either does or does not ring an alarm). $^{\dagger}$ There are interesting edge cases where such an approach of oversampling and then adjusting the outputs can be a good idea. There is a nice example in the comments related to computational efficiency and another one linked here (by the same member as the comment).
