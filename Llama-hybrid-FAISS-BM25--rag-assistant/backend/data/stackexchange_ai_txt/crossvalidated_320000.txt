[site]: crossvalidated
[post_id]: 320000
[parent_id]: 
[tags]: 
Meaning of the linear transformation in sigmoid output for Bernoulli parameter estimation

This is found in 'Deep learning' book, chap.6, ~p.178 ( http://www.deeplearningbook.org/contents/mlp.html ), which discusses the problem of predicting the output class of a binary variable In this section of the book $y$ is the value of a binary variable, $z:=w^Th + b$ seems to be an affine transformation of $h$, while $h$ is a function of $x$. So at this point basically there is no prescribed relation between $z$ (and by extension $x$) and $y$. We just have that based on the assumption $P(y) = \sigma((2y-1)z)$ (which is advertised as being a Bernoulli distribution), still no prescribed relation between the input and the output. Then I came across the following puzzling sentence: Saturation occurs only when the model already has the right answer, when $y=1$ and $z$ is very positive, or $y= 0$ and $z$ is very negative which by plugging in the designed distribution yield $P(y=1) \approx 1$ and $P(y=0) \approx 0$... How is that saying anything about the model ? Also why the paragraph says nothing about the meaning of the transformation for $z$ ? Thanks
