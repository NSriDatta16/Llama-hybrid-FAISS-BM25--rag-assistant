[site]: crossvalidated
[post_id]: 258272
[parent_id]: 
[tags]: 
What is the loss function in neural networks?

I'm trying to understand or visualise what a cost function looks like and how exactly we know what it is. The job of an ANN is to learn from labeled data by weight updating, so that we can make good future predictions given new data (using those weights). However, the weights are updated using back-propagation (using gradient descent). How should I mentally view the loss function? The loss means the error of the output for all possible inputs, but where do we actually derive it from?
