[site]: datascience
[post_id]: 94652
[parent_id]: 
[tags]: 
Back propagation to find the value of z

I hope all of you are doing well. I am a high school student, studying machine learning for my interest.I am studying the Backpropagation Algorithm in recent days. I got stuck in a problem: After the forward pass, the value of the terminating variable (green) is 1400 (I found it) but I can't find the value of the derivative at z, If backpropagation is run, beginning with the terminating variable in the graph. To my knowledge, I have to set the weight and biases in such a way that the value of z will be minimum. I need to define a function through gradient ( mainly partial derivative). Initial values of w and b are randomly chosen. Epsilon (e) is the learning rate. It determines the gradientâ€™s influence. w and b are matrix representations of the weights and biases. Derivative of C in w or b can be calculated using partial derivatives of C in the individual weights or biases. The termination condition is met once the cost function is minimized. Or, I will define some name to the intermediate node, Like w1,w2; then, I will get f(x,y) = a linear combination of W1,w2 with automatic differentiation. I stuck on creating the function for it as It looks like a new graph rather than a neural network graph where the algorithm is used. If someone explains to me the way to approach with the solution, I hope I can solve this type of problem easily. Thank you so much.
