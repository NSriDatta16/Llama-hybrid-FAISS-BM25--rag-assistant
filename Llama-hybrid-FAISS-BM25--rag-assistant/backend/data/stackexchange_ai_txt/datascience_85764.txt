[site]: datascience
[post_id]: 85764
[parent_id]: 85763
[tags]: 
The output layer does not have to be 1D (excl. batch size) but even if it is, it does not necessary mean you cannot transform it to a n dimensional space. Consider an autoencoder used to reconstruct an image: In the simplest case we could flatten a image (e.g. 24 x 24 pixels) and learn a network to predict the 24 x 24 pixels (output a 1D image). These pixels can then be transformed back to an 2D image ( https://www.tensorflow.org/tutorials/generative/autoencoder ). So in other words, even if your network outputs a 1D shape, nothing prevent you from reconstructing it to a higher space. We can achieve similar results as stated in the point above, by using an encoding network (convolutional + pooling layers) followed by a decoding layer (transposed convolutional + up sampling layers). In this case you can effectively generate a 2D image directly ( https://www.tensorflow.org/tutorials/generative/cvae ). You can also look at image segmentation networks for inspiration of how higher dimensions outputs can be generated.
