[site]: crossvalidated
[post_id]: 518895
[parent_id]: 
[tags]: 
Why is the vanishing and/or exploding gradient a problem for Recurrent Neural Networks despite the weight matrixes being same across time steps?

The backpropagation through time for RNNs is supposed to cause issues because it can result in vanishing and/or exploding gradient problem. While this problem is still there for your vanilla feedforward networks as well, the general consensus is that RNNs are more prone towards vanishing and exploding gradients. The gradient has to be backpropagated through all the timepoints to adjust the weight matrices and this is known to be the reason for RNNs being more prone for vanishing and exploding gradients. Now, as per theory, there are three different types of weight matrices in RNNs. Weights from each input (input at each time steps) to each RNN cell - Wxh Weights from one hidden state to its subsequent hidden state - Whh Weights from hidden states (RNN cells) to output - Why As per theory, these matrices (Wxh, Whh and Why) are constant across all the time steps. That is, the values within these three weight matrices are all same regardless of the timesteps. So if we assume that each matrices have a shape of 5 5, then there are 5 5 3 = 75 weights that need to be updated through backpropagation. If we assume that there are 10 time steps, it is not as if we have 75 10 =750 weights that needs to be computed. Instead, it's only 75 weights because the weights remain constant across timesteps. Now, that brings to the question regarding the gradient. The partial derivative of the cost with respect to each value in the weight would thus not be requiring backpropagation through different time steps because the values are gonna be constant across time-steps. So why is this vanishing gradient or exploding gradient a big problem for RNNs when the weight matrices are anyway constant across all the time steps?
