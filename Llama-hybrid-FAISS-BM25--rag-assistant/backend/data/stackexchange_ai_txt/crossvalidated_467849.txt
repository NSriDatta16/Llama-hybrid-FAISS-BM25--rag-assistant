[site]: crossvalidated
[post_id]: 467849
[parent_id]: 
[tags]: 
Neural networks for domain/style transformation?

Is there a neural network that can take data from one distribution and transform it so that it looks as if it were from another distribution, given those two distributions are closely related in some way? I have an experiment where I take simultaneous images of something in visible and infrared, so I'm wondering if there is a type of network where I can feed the visible data set and get a reconstruction of the infrared data set or vice versa. Right now I've trained a network to accept one data set as an input and minimize the reconstruction loss compared to the other data set but the results aren't great. Is there a standard approach to this problem? I have read about neural style transfer and style gans but I'm not sure it solves the same problem. I'm trying to get the corresponding sample from a distribution given a sample from the other distribution rather than apply an arbitrary style to it.
