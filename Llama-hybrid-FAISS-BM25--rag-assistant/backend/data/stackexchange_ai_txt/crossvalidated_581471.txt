[site]: crossvalidated
[post_id]: 581471
[parent_id]: 
[tags]: 
Autoencoders: Where does the encoder end and the decoder begin?

Consider a simple Autoencoder neural net: from torch import nn class AE(nn.Module): def __init__(self, x_dim, z_dim, h_dim=42): super().__init__() self.encoder = nn.Sequential( nn.Linear(x_dim, h_dim), nn.ReLU(), nn.Linear(h_dim, z_dim) ) self.decoder = nn.Sequential( nn.Linear(z_dim, h_dim), nn.ReLU(), nn.Linear(h_dim, x_dim), ) def forward(self, x): z = self.encoder(x) x = self.decoder(z) return x In popular literature, it is generally implied that the output of AE.encoder is solely responsible for encoding whereas AE.decoder is solely responsible for the decoding. Why is that? If we consider that encoding is a more complex task than decoding, there is no no actual guarantee that the network wont use the first three layers for encoding and only the last for decoding (or vice versa). This might especially be the case if we consider asymmetrical autoencoder architectures.
