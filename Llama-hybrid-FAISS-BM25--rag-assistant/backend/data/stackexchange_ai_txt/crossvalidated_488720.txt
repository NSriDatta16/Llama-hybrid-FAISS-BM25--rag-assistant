[site]: crossvalidated
[post_id]: 488720
[parent_id]: 
[tags]: 
Conceptual explanation of taking derivative of sigmoid function in neural network

In a neural network, we have a bunch of inputs and corresponding weights + a bias which are represented by a multivariable equation. Now we squash this whole equation with a sigmoid function. How does taking the derivative of this sigmoid function help us to determine the optimized weights? I don't understand this conceptually.
