[site]: datascience
[post_id]: 124901
[parent_id]: 124898
[tags]: 
I'm not sure what you have against majority voting. Clearly you have an ensemble of weak classifiers. Of the many ways to combine them, voting is a nice one, easily explained to stake holders. You didn't tell us how your cost of FN relates to your cost of FP . The cost function makes a difference when designing each classifier, and when designing an ensemble output for them. Only A, B : We return A Ã— B I don't know what your class imbalance or {A, B} probabilities typically look like, but I'm guessing they're not too far from 90% ? If you tend to have low probabilities, say closer to 10%, I would be fine with instead comparing the sum A + B. It is like voting, like computing "only A" or "only B", but more powerful because when the model shows strong confidence in the A measure or the B measure that can be enough to tip the scales. (Unlike the product, that sum is clearly not a "probability". But it can be a useful score, a figure-of-merit correlated with the target classification.) You explain that you want to put C on the same footing as A and B. That is, you want to convert it to a probability. So you're looking for a technique like Platt scaling or perhaps isotonic regression. You have a hypothesis. Three of them, in fact. So you have some opinions about the structure of the underlying generative process that's producing your data, and you know there are cases where each detector has weak performance. I recommend you put together a simulated generative process, so you know the parameters and the ground-truth Y labels. And see how models that do voting or Platt scaling interact with the simulation. You have already done several training runs. Augment the training dataset with a categorical column indicating the "winner" model, {A, B, C}, for that example. If e.g. both A and B said "true", pick the one that reported higher confidence. Train a new "arbiter" model D which, given an input example, predicts one of {A, B, C} as the winner. Rather than "voting", use "arbitration" to decide how your ensemble of models classifies a novel example. In this way D learns the shape of your data together with the strengths / weaknesses of the models, and uses each model only in the part of the problem space where it shines. Or let decision nodes found by XGBoost do that work for you. Include some A, B, C features for XGBoost to look at, in addition to the raw input features. Include an A indicator variable (boolean) if you wish, but be sure to also include the continuous raw A number, as that is more informative.
