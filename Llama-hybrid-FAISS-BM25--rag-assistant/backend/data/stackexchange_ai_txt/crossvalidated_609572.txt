[site]: crossvalidated
[post_id]: 609572
[parent_id]: 
[tags]: 
Why is less than 0.1 considered too low learning rate in this example

I watched this (with time marker to important moment https://youtu.be/TCH_1BHY58I?t=3040 ) video of Andrej Karpathy explaining some language model, and in ~50:40, he explains how to pick good initial learning rate. He does 1000 consecutive training steps while gradualy increasing learning rate. Then he plots LR versus loss (or in second version the exponent of LR versus loss). He concludes, that lr 0.1 makes the training explode (this part is straightforward). I dont undersand why lower rate on this chart is proven to be too small. This (allegedly too small) learning rate is actually quickly lowering the loss. Can someone explain this?
