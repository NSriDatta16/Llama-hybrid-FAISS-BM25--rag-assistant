[site]: crossvalidated
[post_id]: 532671
[parent_id]: 532670
[tags]: 
To answer the titular question, a key characteristic of a loss function is that the loss is minimized at the target values $y$ . In other words, if you're estimating a quantity, the least loss should be assigned to the estimates that are exactly correct. Using $\mathcal{L}_\text{subtract}$ and divisive losses are not loss functions because they do not involve the target variables. In other words, you can achieve the absolute minimum value of 0 without learning anything about what you want to model. Consider this loss function $L(\hat{y}) = (\hat{y} - c)^2$ for model outputs $\hat{y}$ , a constant $c$ and model targets $y$ . This function is clearly convex in $\hat{y},c$ , but it has nothing to do with the model targets $y$ . If $c$ is a fixed target and our model outputs $\hat{y}$ , then we achieve a minimum at $\hat{y}=c$ , and this is true no matter what the targets $y$ happen to be. My suggestion is to use $\mathcal{L}_\text{subtract}$ as a regularization to augment a typical loss function. So if you're using a binomial cross entropy loss, you could write down a total loss as $$ \text{total loss} = \text{BCE}(y,\hat{y}) + \lambda \mathcal{L}_\text{subtract}$$ for $\lambda >0$ some tuning parameter controlling how much deviation can occur from one layer to the next. Likewise, you can use an alternative loss in place of $\text{BCE}$ to model other types of data. This seems related to, but not exactly the same as " Regularizing RNNs by Stabilizing Activations " by David Krueger & Roland Memisevic. Perhaps their work and bibliography is of interest.
