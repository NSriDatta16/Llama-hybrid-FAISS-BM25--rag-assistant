[site]: crossvalidated
[post_id]: 539038
[parent_id]: 
[tags]: 
Understanding emission probability in HMM definition

This is rather basic question. I was going through Speech and Language Processing by Jurafsky and Martin. In the book, they define a Hidden Markov Model (HMM) as follows: An HMM is specified by the following components: $Q = q_1q_2 ...q_N$ : a set of N states $A = a_{11} ...a_{i j} ...a_{NN}$ : a transition probability matrix $A$ , each > $a_{ij}$ representing the probability of moving from state $i$ to state $j$ , s.t. $\sum_{j=1}^Na_{ij}=1 \quad ∀i$ $O = o_1o_2 ...o_T$ : a sequence of $T$ observations, each one drawn from a vocabulary $V = v_1, v_2,..., v_V$ $B = b_i(o_t)$ : a sequence of observation likelihoods , also called emission probabilities , each expressing the probability of an observation $o_t$ being generated from a state $q_i$ $π = π_1,π_2,...,π_N$ : an initial probability distribution over states. $π_i$ is the probability that the Markov chain will start in state $i$ . Some states $j$ may have $π_j = 0$ , meaning that they cannot be initial states. Also, $\sum_{i=1}^n\pi_i=1$ My doubt is shouldn't emission probabilities $B$ sum to 1? That is, shouldnt it be the case that $\sum_{i=1}^n b_i(o_t)=1$ (or maybe $\sum_{t=1}^{n_t} b_i(o_t)=1$ ). If not, why? If yes, why doesn't the book specify either of these?
