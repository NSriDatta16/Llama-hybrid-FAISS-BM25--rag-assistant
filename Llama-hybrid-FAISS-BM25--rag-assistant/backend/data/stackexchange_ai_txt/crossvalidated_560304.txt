[site]: crossvalidated
[post_id]: 560304
[parent_id]: 560285
[tags]: 
Random search means that you explore the potential hyperparameter values by picking the random combinations of hyperparameters. Marsaglia (1972) invented and algorithm for sampling points uniformly at random in a sphere , this may or may not be how you would like to sample the hyperparameters. There are many different algorithms for generating pseudo-random numbers from different distributions. Random search does not imply using any specific algorithm. Notice that for many problems you may want to choose some special algorithm, for example you can have discrete-values parameter (e.g. $k$ in $k$ NN), in such a case, you wouldn't use an algorithm that samples from a continuous uniform distribution. Are there any mathematical theorems that state the obvious [...] random search will become highly ineffective at optimizing a loss function? Well, as you said it is pretty obvious, isn't it? Say that you lost your wallet when traveling back from work using public transportation (people did this before the pandemic). Would you search for the wallet by visiting the random GPS coordinates within your city? This would be a highly inefficient way, but this is what random search does. It works because machine learning algorithms are often quite forgiving for picking not exactly the best values of hyperparameters. You wouldn't be able to check all the combinations of possible values of the hyperparameters, so random search helps you to pick some of them. Smarter way would be to use an algorithm that picks the points by doing educated guesses on what makes sense, like Bayesian optimization , or successive halving that was recently added to scikit-learn . On the other hand, there are empirical results showing that random search works quite well and it can beat more clever algorithms if you just make it run twice as long. As about high-dimensional data, this is just the curse of dimensionality . Why people use random search? Because it is trivial to implement and works quite well. Hyperparameter optimization is hard because we're optimizing a complicated, multi-dimensional, non-convex, and noisy function (random initialization of parameters, random sampling when using cross-validation, etc). There is no single algorithm that is known to beat all the others, so people often default to random search because it is good enough.
