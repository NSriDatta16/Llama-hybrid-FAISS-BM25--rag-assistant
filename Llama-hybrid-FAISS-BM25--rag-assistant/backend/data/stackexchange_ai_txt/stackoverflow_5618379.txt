[site]: stackoverflow
[post_id]: 5618379
[parent_id]: 5618033
[tags]: 
There are two issues here. Are you replicating the data in the Client and the Server (if so, why?) or does one, the other, or a database agent hold the Model? How does each agent access (its/the) model? If the model is only held by one agent (Client, Server, Database), then the other agents need a way to remotely query the model (e.g., object enumerators, getters and setters for various fields) operating on abstract model entities (e.g, model element identifiers, which might be implemented as integers as you have done). Regardless of who holds the model (one or all), each model can be implemented naturally. THe normal implementation has each object simply refer to other objects using normal object references, as if you had coded this without any thought of sharing between agents, and unlike what you did. You can associate an objectid with each object, as you have, but your application code doesn't need to use it; it is only necessary when referencing a remote copy of of the model. Whether this objectid is associated with each object as a special field, a hash table, or is computed on the fly is just an implementation detail. One way to handle this is to compute the objectid on-the-fly. You can do this if there is a canonical spanning tree over the entire model. In this case, the objectid is "just" the path from root of the spanning tree to the location of object. If you don't have a spanning tree or it is too expensive to compute, you can assign objectids as objects are created. The real problem with a duplicated, distributed model as you have suggested you have, is keeping it up to date with both agents updating it. How do you prevent one from creating an object (an assigning an objectid) at the same time as the other, but the objects being created are different with the same objectid, or the same with with different Objectids? You'll need remote locking and signalling to keep the models in sync (this is the same problem as "cache coherency" for multiple CPUs; just think of each object as acting like a cache line). The way it is generally solved is to designate who holds the master copy (perhaps of the entire model, perhaps of individual objects within the model) and then issue queries, reads, reads-with-intent-to-modify, or writes to ensure that the "unique" entire model gets updated.
