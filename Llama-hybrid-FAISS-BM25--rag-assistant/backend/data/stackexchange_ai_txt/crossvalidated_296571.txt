[site]: crossvalidated
[post_id]: 296571
[parent_id]: 27589
[tags]: 
There are several reasons: In many situations constructing test statistics or confidence intervals is quite difficult, because normal approximations – even after using an appropriate link function – to work with $\pm \text{SE}$ are often not working too well for sparse data situations. By using Bayesian inference with uninformative priors implemented via MCMC you get around this (for caveats see below). The large sample properties are usually completely identical to some corresponding frequentist approach. There is often considerable reluctance to agree on any priors, no matter how much we actually know, due to a fear of being accused of “not being objective”. By using uninformative priors (“no priors”) one can pretend that there is no such issue, which will avoid criticism from some reviewers. Now as to the downsides of just using uninformative priors, starting with what I think is the most important and then heading for some of the also quite important technical aspects: The interpretation of what you get is, quite honestly, much the same as for frequentist inference. You cannot just re-label your frequentist maximum likelihood inference as Bayesian maximum a-posteriori inference and claim that this absolves you of any worries about multiple comparisons, multiple looks at the data and lets you interpret all statements in terms of the probability that some hypothesis is true. Sure, type I errors and so on are frequentist concepts, but we should as scientists care about making false claims and we know that doing the above causes problems. A lot of these issues go away (or at least are a lot less of a problem), if you embed things in a hierarchical model / do something empirical Bayes, but that usually boils down to implicitly generating priors via the analysis procedure by including the basis for your prior in your model (and an alternative to that is to explicitly formulate priors). These considerations are frequently ignored, in my opinion mostly to conduct Bayesian p-hacking (i.e. introduce multiplicity, but ignore it) with the fig-leaf of an excuse that this is no problem when you use Bayesian methods (omitting all the conditions that would have to be fulfilled). On the more “technical” side, uninformative priors are problematic, because you are not guaranteed a proper posterior. Many people have fitted Bayesian models with uninformative priors and not realized that the posterior is not proper. As a result MCMC samples were generated that were essentially meaningless. The last point is an argument for preferring rather vague (or slightly more weakly-informative) priors that ensure a proper posterior. Admittedly, it can sometimes be hard to sample from these, too, and it may be hard to notice that the whole posterior has not been explored. However, Bayesian methods with vague (but proper) priors have in many fields been shown to have really good small sample properties from a frequentist perspective and you could certainly see that as an argument for using those, while with somewhat more data there will be hardly any difference versus methods with uninformative priors.
