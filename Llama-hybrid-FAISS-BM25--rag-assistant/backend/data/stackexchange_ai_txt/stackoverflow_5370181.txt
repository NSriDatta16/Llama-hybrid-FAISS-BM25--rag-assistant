[site]: stackoverflow
[post_id]: 5370181
[parent_id]: 5364977
[tags]: 
Pseudocode, assuming that your bytes are octets and that no zero termination is required: Conversion from ASCII to UTF-16 Given an ASCII input string of length n (in bytes) stored sequentially in memory at address p . Allocate 2 Ã— n bytes of memory; let the start address of that memory be q . While n is larger than zero: Check whether the byte at p is a valid ASCII character. If you don't use checksumming, the most significant bit has to be zero, otherwise it has to be the correct checksum. Issue an error if the byte is not valid. Zero-extend the byte at p to the 16-bit word at q . How this is done depends on the instruction set; e.g., x86 has MOVZX . You may also pay attention to the correct endianness. Increment p by 1. Increment q by 2. Decrement n by 1. Lossless conversion from UTF-16 to ASCII Given an UTF-16 input string of length n (in code units) stored sequentially in memory at address p . Allocate n bytes of memory; let the start address of that memory be q . While n is larger than zero: Check whether the 16-bit word at p represents a valid ASCII character. The nine most significant bits have to be zero, otherwise the character is not representable in ASCII. Issue an error if the word is not valid. Move the least significant byte of the 16-bit word at p to the byte at q . If required, add a checksum to the byte at q . Increment p by 2. Increment q by 1. Decrement n by 1.
