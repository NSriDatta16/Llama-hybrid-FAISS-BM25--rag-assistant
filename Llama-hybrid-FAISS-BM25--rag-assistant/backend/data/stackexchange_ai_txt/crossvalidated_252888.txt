[site]: crossvalidated
[post_id]: 252888
[parent_id]: 4700
[tags]: 
In econometrics, the terms are typically applied in generalized linear models, where the model is of the form $$y_{it} = g(x_{it} \beta + \alpha_i + u_{it}). $$ Random effects: When $\alpha_i \perp u_{it}$, Fixed effects: When $\alpha_i \not \perp u_{it}$. In linear models , the presence of a random effect does not result in inconsistency of the OLS estimator. However, using a random effects estimator (like feasible generalized least squares) will result in a more efficient estimator. In non-linear models , such as probit, tobit, ..., the presence of a random effect will, in general, result in an inconsistent estimator. Using a random effects estimator will then restore consistency. For both linear and non-linear models, fixed effects results in a bias. However, in linear models there are transformations that can be used (such as first differences or demeaning), where OLS on the transformed data will result in consistent estimates. For non-linear models, there are a few exceptions where transformations exist, fixed effects logit being one example. Example: Random effects probit. Suppose $$ y^*_{it} = x_{it} \beta + \alpha_i + u_{it}, \quad \alpha_i \sim \mathcal{N}(0,\sigma_\alpha^2), u_{it} \sim \mathcal{N}(0,1). $$ and the observed outcome is $$ y_{it} = \mathbb{1}(y^*_{it} > 0). $$ The Pooled maximum likelihood estimator minimizes the sample average of $$ \hat{\beta} = \arg \min_\beta N^{-1} \sum_{i=1}^N \log \prod_{t=1}^T [G(x_{it}\beta)]^{y_{it}} [1 - G(x_{it}\beta)] ^{1-y_{it}}. $$ Of course, here the log and the product simplify, but for pedagogical reasons, this makes the equation more comparable to the random effects estimator, which has the form $$ \hat{\beta} = \arg \min_\beta N^{-1} \sum_{i=1}^N \log \int \prod_{t=1}^T [G(x_{it}\beta + \sigma_\alpha a)]^{y_{it}} [1 - G(x_{it}\beta + \sigma_\alpha a )] ^{1-y_{it}} \phi(a) \mathrm{d}a. $$ We can for example approximate the integral by randomization by taking $R$ draws of random normals and evaluating the likelihood for each. $$ \hat{\beta} = \arg \min_\beta N^{-1} \sum_{i=1}^N \log R^{-1} \sum_{r=1}^R \prod_{t=1}^T [G(x_{it}\beta + \sigma_\alpha a_r)]^{y_{it}} [1 - G(x_{it}\beta + \sigma_\alpha a )] ^{1-y_{it}},\quad a_r \sim \mathcal{N}(0,1). $$ The intuition is the following: we don't know what type, $\alpha_i$, each observation is. Instead, we evaluate the product of likelihoods over time for a sequence of draws. The most likely type for observation $i$ will have the highest likelihood in all periods and will therefore dominate the likelihood contribution for that $T$-sequence of observations.
