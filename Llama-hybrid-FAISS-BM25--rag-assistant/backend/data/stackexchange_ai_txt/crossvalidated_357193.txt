[site]: crossvalidated
[post_id]: 357193
[parent_id]: 355240
[tags]: 
Suppose I want to use a (deep) neural network to output a probability that is normalised to sum to 1 within each of a potentially arbitrary number of groups. Softmax activation has this property, and is pretty much the default choice for problems of this nature. Softmax generalizes the inverse logistic function to $k \ge 2$ categorical outcomes. It is defined as $$ f(\mathbf{x})=\frac{\exp(x_i)}{\sum_{i=1}^k \exp(x_i)} $$ and is clearly bounded between 0 and 1, and the sum of the elements of $f(\mathbf{x})$ must equal 1.
