[site]: crossvalidated
[post_id]: 611659
[parent_id]: 
[tags]: 
Is it a bad practice to learn hyperparameters from the training data set?

I am working on a project where I am evaluating different machine learning models to be used as scoring functions during in-silico docking. It is a regression problem where the 3D structure data of a protein bound to a ligand is used to predict the binding affinity of the complex. I have a training set of ~3600 examples and a test set of 209 examples. The test set is chosen to be representative and diverse (more details on how exactly this works can be found in [1]). The test and training set are disjoint. The goal of the test set is to evaluate the performance of the model on a diverse set of unseen examples. I am testing out many different regression models (MARS, kNN regression, support vector regression and random forest regression). I used a grid search with 10-fold CV to tune hyperparameters on the training set, and with the optimal hyperparameters, I trained the model on the same training set. Following this, I tested and compared model performance on the testing set. How good or bad of a practice is it to tune hyperparameters and train the model on the same set? What is the consequence of doing this? References: [1] Ashtawy HM, Mahapatra NR. A Comparative Assessment of Predictive Accuracies of Conventional and Machine Learning Scoring Functions for Protein-Ligand Binding Affinity Prediction. IEEE/ACM Trans Comput Biol Bioinform. 2015 Mar-Apr;12(2):335-47. doi: 10.1109/TCBB.2014.2351824. PMID: 26357221.
