[site]: crossvalidated
[post_id]: 24625
[parent_id]: 
[tags]: 
Full posterior vs Bayesian Information Criterion for selecting number of HMM states

So I'm looking into methods in selecting the best number of hidden states for a hidden markov model, given I don't know what how many states "generated" my data. One method I've seen a lot is to learn lots of models with different numbers of states and then perform BIC on them. Another method is too place a dirichlet process (which I know nothing about) over the number of states and learn the posterior of the states. So, why is obtaining the posterior of states worth the expense and time compared to using the BIC? Do I gain more information this way? Just trying to figure out if its worth, trying to build a HMM with a DP in my work
