[site]: crossvalidated
[post_id]: 144202
[parent_id]: 144199
[tags]: 
If you oversample your sample you would have the same data. Say your sample is: $1,2,3$ and you sample $2N$ values out of it and get: $1,1,2,2,3,3$ - now if you compute some statistic on it (say a mean), then you'll get the same result for both since they contain the same observations appearing with the same probability. Thing that could change is p -value because of sample size, but this would be p -cheating. If you want to approximate the unknown distribution of your data, then one thing that could be done is to use bootstrap , i.e. sample with replacement $N$ out of $N$ cases $R$ times and use this data to approximate the distribution of your sample. If there is linear relation and you are interested in learning about the distribution of the "variation" around it, then you could use bootstrap in different fashion: fit the linear model and then sample the residuals, so to approximate the distribution of residuals. You could also conduct a simulation that generates a sample that resembles your data. For example, to generate correlated variables given some covariance matrix you can sample from Multivariate Normal distribution - for learning more on this check this thread . If the sample size is a problem, then consider using a statistical method that is robust to small sample size - for example Bayesian simulation-based statistical methods often work quite well with small samples.
