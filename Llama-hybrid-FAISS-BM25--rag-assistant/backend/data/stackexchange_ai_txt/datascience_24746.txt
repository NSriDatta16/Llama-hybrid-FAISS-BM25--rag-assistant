[site]: datascience
[post_id]: 24746
[parent_id]: 18848
[tags]: 
You're on the right track. Look at calculating a few more features, both in time and frequency domain. As long as number of samples >> number of features, you aren't likely to overfit. Is there any literature on a similar problem? If so, that always provides a great starting point. Try a boosted tree classifier, like xgboost or LightGBM. They tend to be easier to tune hyperparameters, and provide good results with default parameters. Both Random Forest and boosted tree classifiers can return feature importance, so you can see which features are relevant to the problem. You can also try removing features to check for any covariance. Most importantly though, if your results are unexpectedly poor, ensure your problem is properly defined. Manually check through your results to make sure there aren't any bugs in your pipeline.
