[site]: datascience
[post_id]: 123803
[parent_id]: 123797
[tags]: 
Do you expect the size of the dataset to be highly correlated with the classification into clusters? If so, then it could be a straightforward approach to simply count the number of columns and use that as a feature to a K-means clustering algorithm. If you think that the size is not related to the cluster that a particular data point should be assigned to, then I would recommend something like PCA to reduce the number of dimensions of each data point: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html pca = PCA(n_components=2) pca.fit(X) You might end up having to fit each data point separately, but it would be a fast/simple approach to start. You could also combine PCA with counting the number of original dimensions potentially.
