[site]: crossvalidated
[post_id]: 536136
[parent_id]: 43471
[tags]: 
An important area where the two approaches will yield conflicting assessments is the context of multiplicity. Since p-values involve the probability of getting more extreme results than the results observed if a null hypothesis is true, having more looks at the data will increase the p-value. For Bayes on the other hand, more looks at the data just result in more rapid updating of evidence, and previous evidence assessments are now obsolete and can be completely ignored. Bayesian measures are study time-respecting while frequentist $\alpha$ probability is non-directional. Two classes of examples are (1) sequential testing where frequentist approaches are well developed but are conservative and (2) situations in which there is no way to use a frequentist approach to even address the problem of interest. In a sequential study, using Bayes one may look at the data infinitely often without changing the definition or reliability of posterior probabilities. The frequentist approach becomes increasingly conservative as the number of looks increases. In a study in which there are multiple endpoints, the frequentist approach has a great deal of difficulty even putting together an overall evidentiary measure, while the Bayesian approach has no difficulty. For example suppose that one is developing a migraine headache drug and the outcomes are sleep problems, pain, nausea, light sensitivity, and sound sensitivity. One may reasonably claim the drug to be a success if there is a high posterior probability that the drug improved any 3 of the 5 patient outcomes. The only frequentist methods that have been proposed are closed testing procedures that seek evidence for any or all of the 5 endpoints being benefited by drug. Another major category where Bayes disagrees with frequentist is the frequent case where the frequentist result in incorrect from a frequentist standpoint . This occurs quite generally when the log likelihood has a very non-Gaussian shape, for example with binary logistic regression with an imbalanced Y. While the uncommonly used profile likelihood interval yields fairly accurate confidence interval coverage probabilities, the most commonly used approaches such as the Wald method and various bootstrap intervals do not. You will see inaccurate tail non-coverage probabilities in at least one of the two tails. Bayesian highest posterior density or credible intervals are exact on the other hand, for all sample sizes.
