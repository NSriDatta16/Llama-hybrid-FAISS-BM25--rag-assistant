[site]: crossvalidated
[post_id]: 615653
[parent_id]: 615594
[tags]: 
Can someone please explain why the scaled chi-square difference test favors the model with more parameters (i.e., Model A) while the AIC, BIC, and Sample-Size adjusted BIC favor the model with fewer parameters (i.e., Model B)? The LRT is a test statistic, information criteria are not. The former are used to conduct null-hypothesis significance tests, testing an exact null hypothesis without adjusting for model parsimony (in terms of how many parameters are estimated). The latter adjust for parsimony, and are used to descriptively compare models in a way that balances parsimony and fit. They are not used for the same purpose, so they cannot really contradict each other. Which fit statistic(s) should I rely on and report in this situation? Report them all. The LRT is only "statistically significant" relative to an arbitrary p value. You reported only p value is .038), so that must be your alpha level: a common convention but rather liberal (1 out of 20 true null hypotheses will be rejected). AIC is lower for the model that estimates fewer free parameters. It is designed to select the model that is most likely to predict new data. BIC is also lower for the model that estimates fewer free parameters. It is a poor approximation of a Bayes factor (making many simplifying assumptions), and asymptotically selects the true data-generating model when it is among the competitors. When the true model is not among those considered, then its behavior is less understood. I prefer AIC because it is about out-of-sample prediction and doesn't assume any model is a perfect representation of real data-generating processes, but it is up to you to transparently report your own values. Here is some reading you might find helpful. Vrieze, S. I. (2012). Model selection and psychological theory: A discussion of the differences between the Akaike information criterion (AIC) and the Bayesian information criterion (BIC). Psychological Methods, 17 (2), 228â€“243. https://doi.org/10.1037/a0027127 Bollen, K. A., Harden, J. J., Ray, S., & Zavisca, J. (2014). BIC and alternative Bayesian information criteria in the selection of structural equation models. Structural Equation Modeling, 21 (1), 1-19. https://doi.org/10.1080/10705511.2014.856691 Another observation: Your scaling factor is basically 1 (within rounding to 2 decimals), so the scaling factor in each model is only making a negligible adjustment. That implies your data do not have enough excess kurtosis to cause a problem for your SE s and test statistics. If you use standard MLE, you don't need the more complex formulas for a chi-squared difference test: https://www.statmodel.com/chidiff.shtml
