[site]: crossvalidated
[post_id]: 635144
[parent_id]: 633757
[tags]: 
Modelling Annotators' Reliability and Agreement Note that I am using "annotator" as a synonym to "rater", and "observer". Reliability and agreement are synonyms with certainty and trust. Thus antonyms are uncertainty or variance. Other keywords of interest for searching: crowdsource aggregation, crowdsource truth inference or truth discovery. To your first question , "the best way" is highly dependent on the specific problem and your data sample is not enough to answer that. As I will note throughout my answer, there are different ways to model things, and in the end you'll have to make an informed decision that best fit your case. Model Each Question's Space of Possible Answers First, look at each question in the questionnaire and specify the space of possible answers for each. This is important as it denotes the possible answers for each question and will inform you what kind of probability distribution is proper to use. If there were a finite set of possible answers, then a categorical distribution is an appropriate distribution to use. If there are other constraints across the possible values that another probability distribution assumes, then that distribution is a better fit in that case. In the case of only two values such as 'Y'/'N', a Bernoulli distribution serves well. Rarely will you ask people to give an unconstrained real (continuous) number as an answer, but in the case you do, then your space must model the answer as a real number. Typically surveys use a Likert scale or other discretization of the real line to a set of finite bins. Model Answer Agreement Independent of Annotators' Other Answers After you have the answer spaces, you can model a distribution over that answer space informed by any prior information, such as a prior distribution or model choice, and the observed answers. You get to chose how to balance their role in informing the resulting distribution. Most people use the empirical distribution / process which has no prior knowledge, and may fit your case. However the empirical process provides asymptotic convergence to truth, which involves infinite samples, not three samples (answers per question) as you have in your example. So, you may also consider Bayesian modeling approaches that involve a prior distribution to incorporate more variance depending on your prior belief of agreement (certainty) or disagreement (uncertainty) amongst answers to that question. In this case, you can look at each question's answers independent of the annotators' answers to other questions, thus disregarding the reliability of individual annotators. This approach is simpler, and probably more often used, although correctness of this approach is up to the data design(er) and analyzer of this data. How can I tell where the most disagreement comes from? Using this approach, the question with the most uncertainty ( entropy ) or variance is the question that is the source of the most disagreement between all questionnaires. One way, assuming categorical answers, is to model the answers using the empirical distribution, which results in a categorical. The categorical's entropy may then be analytically computed, as follows $$H(X) = -\sum_{x \in \mathscr{X}} P(X=x) \log P(X=x)$$ where the $\mathscr{X}$ is the space of possible answers and the probability of the modeled categorical random variable $X$ is simply informed by the probability vector found from the empirical distribution of frequencies. Note that to get a better estimate of the population of annotator's responses you may need more annotators. In your example, the possible probability vectors is severely limited by the total of 3 answers. So $[\frac{1}{3}, \frac{1}{3}, \frac{1}{3}, ...]$ is the most uncertain probability vector possible where the rest of the possible answers, if greater than 3, have probability zero. Due to this, it is often a good idea to have enough annotators to enable a fine grained enough probability vector to approximate the true distribution of answers. Model Annotator Reliability per Annotator As mentioned earlier, sometimes we are able to model the individual annotator's agreement over all their answers to other annotators, or with respect to ground truth questions for which we assume to know the correct answer to. This is useful when you have possibly unreliable annotators taking your survey or annotating your data, whether due to maliciousness or due differences in expertise on the annotation task. This is only able to be done if the unique annotators are labeled and their annotations are associated with their label, which can still be anonymized. In your case, each instance of a questionnaire serves as an annotator identifier. However , if you allow people to retake the questionnaire, then you must provide them with a consistent indicator label across their separate questionnaires, otherwise you will incorrectly treat their responses as coming from different people. By modelling each annotator's reliability overall, you can then incorporate the certainty in their annotations into each question they answer and thus have a better model of the probability distribution of pooled answers. One approach is the 1979 David & Skene EM Algorithm [1] . EM stands for Expectation Maximization . How can I tell where the most disagreement comes from? Using this approach, you can still find the most disagreed upon question, although it may be different than when modeling only agreement of answers per question in isolation. In this case though, you are able to find the annotator(s) who disagrees the most with all other annotators. You can also better characterize the agreement. For example, in a group of three annotators, say they all mostly agree for some questions, but you find two questions to have one annotator strongly disagree. With this approach, you can find those questions as well as figure out if the disagreement is due to the same annotator disagreeing from the majority each time, or due different annotators for the different questions. The Model Complexity Can Grow with Multiple Tasks When the annotation task consists of multiple tasks that a single annotator may have different expertise within each, it can be important to model the annotator's reliability in each task rather than overall, as we discussed above. This is more complex and nuanced, but at times is desirable and doable if the questions have either tags of which tasks they relate to in a binary yes/no fashion or a weighting of their relation to a task, or some other structural relationship. I leave this to further reading in the following surveys, if desired. There have been more approaches to modeling individual annotators and their agreement on problems to better understand the information involved in the annotations and to go beyond mere majority or plurality vote. Here are some surveys/reviews: Paun et al. 2018. [2] "Comparing Bayesian Models of Annotation " Zheng et al. 2017. [3] "Truth Inference in Crowdsourcing" Miller et al. 2005. [4] "Eliciting Informative Feedback" References = Closed Access. = Open Access. Dawid, Alexander Philip and Skene, A. M. "Maximum Likelihood Estimation of Observer Error-Rates Using the EM Algorithm". 1979. Journal of the Royal Statistical Society: Series C (Applied Statistics). Vol 28. pg 20--28. ISSN: 1467-9876. DOI: 10.2307/2346806 . https://crowdsourcing-class.org/readings/downloads/ml/EM.pdf Royal Statistical Society listing: https://rss.onlinelibrary.wiley.com/doi/abs/10.2307/2346806 JSTOR: https://www.jstor.org/stable/2346806?origin=crossref Paun, Silviu and Carpenter, Bob and Chamberlain, J. D. and Hovy, Dirk and Kruschwitz, Udo and Poesio, Massimo. "Comparing Bayesian Models of Annotation". 2018. Transactions of the Association for Computational Linguistics. Vol 6. pg. 571--585. MIT Press Direct: https://doi.org/10.1162/tacl_a_00040 Zheng, Yudian and Li, Guoliang and Li, Yuanbing and Shan, Caihua and Cheng, Reynold. "Truth Inference in Crowdsourcing: Is the Problem Solved?" 2017. Proc. VLDB Endow. Vol. 10, No. 5. pg. 541--552. ISSN 2150-8097. DOI 10.14778/3055540.3055547 . https://hub.hku.hk/bitstream/10722/243527/1/content.pdf?accept=1 ACM: https://dl.acm.org/doi/abs/10.14778/3055540.3055547 Miller, Nolan and Resnick, Paul and Zeckhauser, Richard. "Eliciting Informative Feedback: The Peer-Prediction Method". 2005. Management Science. Vol. 51, No. 9. pg. 1359--1373 https://nmiller.web.illinois.edu/documents/research/elicit.pdf https://pubsonline.informs.org/doi/abs/10.1287/mnsc.1050.0379 Further Reading https://en.wikipedia.org/wiki/Inter-rater_reliability https://en.wikipedia.org/wiki/Categorical_distribution https://en.wikipedia.org/wiki/Bernoulli_distribution https://en.wikipedia.org/wiki/Likert_scale https://en.wikipedia.org/wiki/Empirical_process https://en.wikipedia.org/wiki/Entropy_(information_theory) https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm
