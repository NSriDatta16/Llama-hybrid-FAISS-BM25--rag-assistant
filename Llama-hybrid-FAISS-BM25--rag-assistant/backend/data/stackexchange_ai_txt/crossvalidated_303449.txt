[site]: crossvalidated
[post_id]: 303449
[parent_id]: 303384
[tags]: 
Let's look at the definitions: $\sigma (\mathbf {z} )_{j}={\frac {e^{z_{j}}}{\sum _{k=1}^{K}e^{z_{k}}}} $ Softmax with temperature $T$: $\sigma (\mathbf {z} )_{j}={\frac {e^{ \frac{z_{j}}{T}}}{\sum _{k=1}^{K}e^{\frac{z_{k}}{T}}}} $ In context of machine learning softmax is used, as you remarked, to define probability distributions ($\mathbf{x} \in \mathbb{R}^D$): $ P(y=j\mid \mathbf {x} )={\frac {e^{\mathbf {x} ^{\mathsf {T}}\mathbf {w} _{j}}}{\sum _{k=1}^{K}e^{\mathbf {x} ^{\mathsf {T}}\mathbf {w} _{k}}}} $ Where $\mathbf{w_j}$'s are trainable parameters. Temperature is just a multiplicative constant, so you can treat it as degree of freedom of $\mathbf{w_j}$'s (if $W = (\mathbf{w_j})_{j I'm not familiar with physical interpretation, but softmax is related to maximum entropy principle, see A Brief Maxent Tutorial
