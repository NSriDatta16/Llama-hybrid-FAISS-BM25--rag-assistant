[site]: crossvalidated
[post_id]: 240063
[parent_id]: 239898
[tags]: 
I beg to differ in this question with @AmiTavory's opinion as well as with the Elements of Statistical Learning. Coming from an applied field with very low sample sizes, I have the experience that also unsupervised pre-processing steps can introduce severe bias. In my field that would be most frequently PCA for dimensionality reduction before a classifier is trained. While I cannot show the data here, I've seen PCA + (cross validated LDA) vs. cross validated (PCA + LDA) underestimating the error rate by about an order of magnitude . (This is usually an indicator that that the PCA is not stable.) As for the "unfair advantage" argumentation of the Elements, if variance of taining + test cases is examined, we end up with features that work well with both the training and test cases. Thus, we create a self-fulfilling prophecy here which is the cause of the overoptimistic bias. This bias is low if you have reasonably comfortable sample sizes. So I recommend an approach that is slightly more conservative than the Elements: preprocessing calculations that consider more than one case need to be included in the validation: i.e. they are calculated on the respective training set only (and then applied to the test data) preprocessing steps that consider each case on its own (I'm spectroscopist: examples would be baseline correction and intensity normalization, which is a row-wise normalization) may be pulled out of the cross validation as long as they are before the first step that calculates for multiple cases. That being said, also cross valiation is only a short-cut for doing a proper validation study. Thus, you may argue with practicality: You could check whether the pre-processing in question yields stable results (you can do that e.g. by cross validation). If you find it perfectly stable already with lower sample sizes, IMHO you may argue that not much bias will be introduced by pulling it out of the cross validation. However, to cite a previous supervisor: Calculation time is no scientific argument. I often go for a "sneak preview" of few folds and few iterations for the cross validation to make sure all code (including the summary/graphs of the results) and then leave it over night or over weekend or so on the server for a more fine-grained cross validation.
