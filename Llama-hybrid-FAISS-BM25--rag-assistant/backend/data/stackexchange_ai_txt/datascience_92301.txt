[site]: datascience
[post_id]: 92301
[parent_id]: 92253
[tags]: 
You have a mining problem which is basically a search problem. In search problem you have TP, FP, FN but not TN (you have it but it is trivial, imagine search in google. Basically bilions of webpages are TN in every search. right?) I also assume that you are not interested in ranking results as you did not mention it in the question. In this case you can easily go with Precision and recall: TP: Graphs which are right and you could find. FP: Graphs which are not right but your model found. FN: Graphs which are right but your model did not find. Then you simply calculate: Precision: $$\frac{TP}{TP+FP}$$ Recall: $$\frac{TP}{TP+FN}$$ And as a combined score, you may calculate F1: $$\frac{2TP}{2TP+FP+FN}$$ About Accuracy When the volume of search space is huge, as I said above, accuray is not meaning much. According to the formulation: $$ACC=\frac{TP+TN}{TP+TN+FP+FN}$$ when $TN$ is very large then the whole fraction approaches zero and does not inform much. But if your search space is relatively small, you can calculate accuracy with the formulation above. However it was just a theoretical justification and still your best metrics are precision and recall and their related metrics. Update For Your Specific Question Your set of top 1% results may have different sizes due to different size of your model output (If it is always 4200 please write in the comments but in "mining" questions usually the size of output is variant). In your case TP and FP are pretty straight-forward and meaningful. About FN, if your test set is also ranked, then you can evaluate your results according to top 1% of your ground-truth which includes 90 samples and go on as you already did, or you can fix the size of ground-truth set equal to your 1% (i.e. take 42 top test results) and again calculate same scores. The first one is more stable and significant for comparing different models. A more precise metric which is related to the ranked results is called NDCG . If the test set is not ranked then go this way: $|correctly\_found\_graphs|$ : Number of graphs that you correctly found. $|wrongly\_found\_graphs|$ : Number of graphs you found but they are not in ground-truth $|all\_found\_graphs|$ : here 42 $|all\_correct\_graphs|$ : here 900 Then calculate Precision@k (here precision@42 ) metric which calculates the precision only on the top k results of a search output: $Precision@42 = \frac{|correctly\_found\_graphs|}{|all\_found\_graphs|}=\frac{38}{42}$ This is a well-established information retrieval metric. $Recall = \frac{|correctly\_found\_graphs|}{|all\_correct\_graphs|}=\frac{38}{900}$ How can I improve the recall? As long as size of your top 1% results is less than ground-truth, your recall is limited to an upper-bound i.e. even if all 42 all relevant, still your recall is 42/900. Knowing this fact you can use recall but I do not see the point. Recall makes more sense if you take all 4200 outputs into account. Your calculation of True Negative in the comments is also wrong. TN in your case is the number of all graphs that your model should not have returned and it didn't i.e. 900-42 Moreover, all samples are positive in my test dataset. Not single sample is negatives See the problem from an Information Retrieval point of view. In this approach, any graph which is not in 900 set is a negative sample. You can comment in case it is still unclear.
