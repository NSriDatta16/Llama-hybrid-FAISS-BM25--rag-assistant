[site]: crossvalidated
[post_id]: 465666
[parent_id]: 414317
[tags]: 
Here's an attempt at an information theoretic explanation. It relies on the principle that, when encoding samples from a distribution, the shortest code is the one designed based on the true underlying distribution of the samples. Using what you know about the true distribution, you can do the best possible job allocating short codes to common samples and long codes to rare samples. Your friend, who doesn't know the true distribution and accidentally uses long codes for common words, ends up wasting their bandwidth. To understand this answer, you also need to know that the optimal code uses a word of length $\log_2 P(x|\theta)$ bits to encode $x$ . This is exactly true for discrete distributions with probabilities of the form $2^{-k}$ . It is kinda-sorta-mostly-true-especially-when-you-have-many-samples for other, more complicated distributions. To see how this applies, notice the expectation of the score can be approximated by a Monte Carlo algorithm: sample $x_1, ... x_n$ from $P(X|\theta)$ and evaluate $\frac{d}{d\theta}\frac{1}{n}\sum_i \log (P(x_i|\theta))$ . All I did was turn the integral into a Monte Carlo approximation. In the limit as $n\rightarrow \infty$ , this whole thing converges to $0$ , and we want intuition for why it does that. But, squinting at this, it is the derivative of the average code length per sample (up to a constant, since it's a natural log and not a base-2 log). Since we're using the ground-truth $\theta$ , the code can't get any more efficient. Any change in $\theta$ leads to a more verbose encoding. So the average code length $\lim_{n\rightarrow \infty}\sum_i \log (P(x_i|\theta))$ is optimal. What's the only thing you remember from calculus? The derivative at the optimum is zero.
