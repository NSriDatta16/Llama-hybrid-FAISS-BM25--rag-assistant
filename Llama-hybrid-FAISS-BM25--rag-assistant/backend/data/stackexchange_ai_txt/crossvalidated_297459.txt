[site]: crossvalidated
[post_id]: 297459
[parent_id]: 297380
[tags]: 
The global minimum may as well as be useless, so we don't really care if we find it or not. The reason is that, for deep networks , not only the time to find it becomes exponentially longer as the network size increases, but also the global minimum often corresponds to overfitting the training set. Thus the generalization ability of the DNN (which is what we really care about) would suffer. Also, often we prefer flatter minima corresponding to a higher value of the loss function, than sharper minima corresponding to a lower value of the loss function, because the second one will deal very badly with uncertainty in the inputs. This is becoming increasingly clear with the development of Bayesian Deep Learning. Robust Optimization beats Determinist Optimization very often, when applied to real world problems where uncertainty is important. Finally, it's a fact that DNNs just kick the ass of methods such as XGBoost at image classification and NLP. A company which must make a profit out of image classification will correctly select them as models to be deployed in production ( and invest a significant amount of money on feature engineering, data pipeline, etc. but I digress). This doesn't mean that they dominate all the ML environment: for example, they do worse than XGBoost on structured data (see the last winners of Kaggle competitions) and they seem to not still do as well as particle filters on time series modelling. However, some very recent innovations on RNNs may modify this situation.
