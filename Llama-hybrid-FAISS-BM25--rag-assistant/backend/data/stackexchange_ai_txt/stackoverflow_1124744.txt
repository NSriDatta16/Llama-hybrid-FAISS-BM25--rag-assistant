[site]: stackoverflow
[post_id]: 1124744
[parent_id]: 460022
[tags]: 
One key distinction is how big files tend to be on average. Big files (1000 lines +) tend to have many independent changes that are trivially automatically mergeable. So the fact that someone else is actively changing a file you are about to start work on is probably uninteresting, so it is ok if the system makes that hard to discover. So you tend to end up a VC strategy that has a lot of branches, and easy merges. New functionality is written in new branches. If you are working with the smaller, highly-cohesive files typical of an OO design, in a language like Java, such accidental conflicts are a lot rarer. Even as an artificial example, it is pretty hard to come up with two sets of changes that can be made to a class (and corresponding JUnit test cases) that can sensibly be made in isolation and then automatically weaved back together by a text merge tool. If you do a lot of refactoring (renaming and splitting files) then that stresses out the merge tool even more. So you tend to be best off with a VC strategy that has an identifiable and usable stable trunk and minimal branches. In the first approach, new functionality is written in new branches, and merged when complete. In the second, it is written in new files, and enabled when complete. Of course, if you do the second, you definitely need strong protection from the baseline becoming unusable for any length of time (i.e. continuous integration and a strong automatically-run test suite).
