[site]: crossvalidated
[post_id]: 415086
[parent_id]: 
[tags]: 
Is this scaling algorithm viable?

I just came up with my own random number scaling algorithm (and I'm sure someone else has come up with it before me), and I wanted to see if any of you can find holes in it. The idea is to take a string of random binary data (such as 01011101100010011001011110100111001) from a truly random source ( http://qrng.anu.edu.au/ in this case), and use that to create a string of random numbers between 0 and 71. The simple way is to take 7-bit chunks, convert them to decimal, and throw out anything 72 and over. But since random bits aren't free to produce, I want to be more responsible and throw out as little data as possible. The Algorithm Ok, I don't even know if algorithm is the right word. I think of it as a crawler. Since any binary, 7-digit number starting with 11, 101 or 1001 will be 72 or higher, I crawl through the bits one at a time and if I see that pattern at the beginning of the number, I throw away what I've gotten so far, and continue crawling. So to crawl 01011101100010011001011110100111001 it looks like this: 0-1-0-1-1-1-0 = 46, Use that number! 1-1 STOP! Throw out those 2 bits and continue crawling 0-0-0-1-0-0-1 = 9, use it! 1-0-0-1 STOP! Throw out all 4 and continue 0-1-1-1-1-0-1 = 61 0-0-1-1-1-0-0 = 28 The last bit I have to throw out (or save for later). I have found this gives me one usable number per 8.7 bits on average, instead of the 11.5 bits when I use the plain 7-bit chunk method. Can anyone find any holes in my reasoning that would make this method less random (i.e. more predictable)?
