[site]: crossvalidated
[post_id]: 531981
[parent_id]: 
[tags]: 
What does interpolating the training set actually mean?

I just read this article: Understanding Deep Learning (Still) Requires Rethinking Generalization In section 6.1 I stumbled upon the following sentence Specifically, in the overparameterized regime where the model capacity greatly exceeds the training set size, fitting all the training examples (i.e., interpolating the training set), including noisy ones, is not necessarily at odds with generalization. I do not fully understand the term "interpolating" in the context of fitting training data. Why do we speak of "interpolation" in this context? What does the term exactly mean here? Is there any other term that can be used instead? In my understanding interpolation means the prediction within the training domain for some novel input that was not part of the training set.
