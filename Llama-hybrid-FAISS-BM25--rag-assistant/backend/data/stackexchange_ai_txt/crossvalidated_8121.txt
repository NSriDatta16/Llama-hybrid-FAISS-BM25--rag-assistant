[site]: crossvalidated
[post_id]: 8121
[parent_id]: 8103
[tags]: 
If I understand, you have a classification problem where you observe $(X_1,Y_1),\dots,(X_n,Y_n)$. Denoting by $\mathcal{X}$ the space where $X$ takes values, you assume the existance of $\mathcal{X}_{10}$ the smallest closed subset of $\mathcal{X}$ such that: $\mathcal{X}\setminus \mathcal{X}_{10}$ can be partinioned into $\mathcal{X}_{1\setminus 0}\cup \mathcal{X}_{0\setminus 1}$ with $P_{0}(X\in \mathcal{X}_{1\setminus 0})=0$, $P_{1}(X\in \mathcal{X}_{0\setminus 1})=0$ (for $i=0,1$ $P_i$ is the distribution of $(X|Y=i)$) I think there are two problems that may arise from your question: How do you justify a decomposition of the distribution to be able to solve the problem only on $\mathcal{X}_{10}$ (Theoretical problem) How do you estimate $\mathcal{X}_{1\setminus 0}$, $\mathcal{X}_{0\setminus 1}$ and$\mathcal{X}_{0 1}$ from data (practical estimation problem) Theoretical problem In this case (and under suitables conditions) you can decompose $P_{1}$ and $P_0$ using the Radon-Nikodym theorem (This version ) into: $$P_{1}=P_{1,\mathcal{X}_{1\setminus 0}}+P_{1,\mathcal{X}_{0 1}}$$ $$P_{0}=P_{0,\mathcal{X}_{0\setminus 1}}+P_{0,\mathcal{X}_{0 1}}$$ with $P_{0,\mathcal{X}_{0 1}}\sim P_{1,\mathcal{X}_{0 1}}$ (mutually absolutly continuous) $P_{1,\mathcal{X}_{0 1}} \bot P_{1,\mathcal{X}_{1\setminus 0}} \bot P_{0,\mathcal{X}_{0\setminus 1}}$ (mutually singular) Note that distributions in the decomposition are not probabilities, and you need to renormalize (i.e. compute $P_{i}(X\in A)$ when $A$ is one of the three above mentionned sets). This renormalization is also required from a practical point of view. Once you have done the renormalization you can construct a classification rule on $\mathcal{X}_{0 1}$ using renormalized version of $P_1$ and $P_0$ 's restrictions. You can extend this classification rule in the obvious way to the whole $\mathcal{X}$. Estimation problem If you want to "learn" the above mentionned rule from the data, you need to estimate the corresponding sets I guess there is a large litterature in "support" estimation but in view of the classification problem at the end, it is not a big deal since the places where you might be wrong in terms of support estimation are does where you don't have much "mass" and hence does that are not really important for the classification errors at the end... at least from a bayesian perspective of which error is important... (see my answer here )
