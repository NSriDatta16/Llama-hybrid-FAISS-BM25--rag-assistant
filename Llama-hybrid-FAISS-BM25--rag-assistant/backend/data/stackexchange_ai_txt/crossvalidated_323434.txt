[site]: crossvalidated
[post_id]: 323434
[parent_id]: 323014
[tags]: 
You don't quiet have your LSTM setup right. Some thoughts: Use keras.preprocessing.text.text_to_word_sequence to turn your texts into sequences of word ids. Use keras.preprocessing.sequence.pad_sequences to truncate/pad all your sequences to something like 32 or 64 words. Use an embedding layer after your input layer to map the sequences of word ids to a sequence of word vectors. Use the sequences of word vectors as input to to the LSTM Try a GRU instead of an LSTM (it's a little bit simpler) After the decoding layer, the output will be a sequence of word vectors. Use nearest neighbors to lookup the nearest word in your vocabulary for each "word" in the output. You can learn the embeddings while training the network, or you can use a pre-trained embedding layer.
