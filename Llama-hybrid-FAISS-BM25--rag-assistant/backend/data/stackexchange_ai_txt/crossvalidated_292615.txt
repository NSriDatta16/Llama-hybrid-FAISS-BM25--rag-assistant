[site]: crossvalidated
[post_id]: 292615
[parent_id]: 
[tags]: 
Best practices for feature selection?

I have datasets that range from ~2000-9000 columns of predictor variables. I'm often charged with primarily classification - but sometimes regression tasks. I know that I don't need this many variables for my models to be effective but I can't anticipate which ones in a reliable way. I'm looking for ideas on the general best practices that would cut this down to around 50-150 variables which from my experience, seems fairly effective in determining the outcome. Currently I'm using lasso or random forest to whittle down the number of variables before running a final model. I want less variables so theres less noise, simply don't need that many, and to make it easier to deploy to production.
