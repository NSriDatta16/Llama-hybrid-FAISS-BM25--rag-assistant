[site]: crossvalidated
[post_id]: 377893
[parent_id]: 377865
[tags]: 
The random forests is a collection of multiple decision trees which are trained independently of one another . So there is no notion of sequentially dependent training (which is the case in boosting algorithms ). As a result of this, as mentioned in another answer, it is possible to do parallel training of the trees. You might like to know where the "random" in random forest comes from: there are two ways with which randomness is injected into the process of learning the trees. First is the random selection of data points used for training each of the trees, and second is the random selection of features used in building each tree. As a single decision tree usually tends to overfit on the data, the injection of randomness in this way results in having a bunch of trees where each one of them have a good accuracy (and possibly overfit) on a different subset of the available training data. Therefore, when we take the average of the predictions made by all the trees, we would observe a reduction in overfitting (compared to the case of training one single decision tree on all the available data ). To better understand this, here is a rough sketch of the training process assuming all the data points are stored in a set denoted by $M$ and the number of trees in the forest is $N$ : $i = 0$ Take a boostrap sample of $M$ (i.e. sampling with replacement and with the same size as $M$ ) which is denoted by $S_i$ . Train $i$ -th tree, denoted as $T_i$ , using $S_i$ as input data. the training process is the same as training a decision tree except with the difference that at each node in the tree only a random selection of features is used for the split in that node. $i = i + 1$ if $i go to step 2, otherwise all the trees have been trained, so random forest training is finished. Note that I described the algorithm as a sequential algorithm, but since training of the trees is not dependent on each other, you can also do this in parallel. Now for prediction step, first make a prediction for every tree (i.e. $T_1$ , $T_2$ , ..., $T_N$ ) in the forest and then: If it is used for a regression task, take the average of predictions as the final prediction of the random forest. If it is used for a classification task, use soft voting strategy: take the average of the probabilities predicted by the trees for each class, then declare the class with the highest average probability as the final prediction of random forest. Further, it is worth mentioning that it is possible to train the trees in a sequentially dependent manner and that's exactly what gradient boosted trees algorithm does, which is a totally different method from random forests.
