[site]: crossvalidated
[post_id]: 444494
[parent_id]: 442796
[tags]: 
Let's begin with a weighted average, which slightly modifies the formula for an ordinary average: $$\bar{x}^w=\frac{\sum_i w_i x_i}{\sum w_i}$$ An unweighted average would correspond to using $w_i=1$ (though any other constant would do as well). Why would we do that? Imagine, for example, that each value occurred multiple times ("We have 15 ones, 23 twos, 19 threes, 8 fours and 1 six"); then we could use weights to reflect the multiplicity of each value ( $w_1=15$ , etc). Then the weighted average is a faster way to calculate the average you'd get if you wrote "1" fifteen times and "2" twenty three times (etc) and calculated the ordinary average. For another possible example, imagine that each observation was itself an average. Each average is not equally informative -- the ones based on larger samples should carry more weight (other things being equal). (In that case if we set each observation's weight to the underlying sample size, we get the overall average of all the data that would comprise the component averages.) There are many other reasons one might weight observations differently, though (e.g. if the observation values are not all equally precise). In somewhat similar fashion, we can modify the estimator in ordinary regression to incorporate weights to the observations. It will reproduce a weighted average when the regression is intercept only. The usual multiple regression estimator is $\hat{\beta}=(X^\top X)^{-1}X^\top y$ . The weighted regression estimator is $\hat{\beta}=(X^\top W X)^{-1}X^\top W y$ , where $W$ is a diagonal matrix, with weights on the diagonal, $W_{ii} = w_i$ . Weighted logistic regression works similarly, but without a closed form solution as you get with weighted linear regression.
