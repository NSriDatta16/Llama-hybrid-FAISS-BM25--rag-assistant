[site]: crossvalidated
[post_id]: 397037
[parent_id]: 396987
[tags]: 
There is nothing strange about the effect you observe: As long as the loss is not zero, the gradient will not be zero, and thus the gradient descent algorithm will keep updating the weights. How much they change in each iteration depends on the step size (learning rate) and the gradient. Unless you decrease the learning rate over time, your weights may oscillate around a local minimum forever. Looking at the weights with a naked eye is not a very good criterion for checking convergence. However, there are various established strategies for deciding when to stop the learning algorithm. A common choices include: Training for a fixed number of epochs. Reaching a threshold value for training loss. Early stopping: Monitoring the loss on a hold-out dataset and stop once it starts increasing. A paper by Soudry et al. [1] studies the properties of convergence of weights w.r.t. convergence of the training loss in logistic regression models. They observe that the convergence of weights is much slower that the convergence of the loss. They suggest that it is beneficial to keep training even when the loss has seemingly converged and recommend using accuracy on the validation set to be a good criterion for terminating the training. Note that their paper does not discuss explicitly deep neural networks which are much more complicated to analyze, however their earlier empirical results show similar effects in neural nets, e.g. [2]. [1]: Soudry, D., Hoffer, E., Nacson, M.S., Gunasekar, S. and Srebro, N., 2018. The implicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1), pp.2822-2878. [2]: Hoffer, E., Hubara, I. and Soudry, D., 2017. Train longer, generalize better: closing the generalization gap in large batch training of neural networks. In Advances in Neural Information Processing Systems (pp. 1731-1741).
