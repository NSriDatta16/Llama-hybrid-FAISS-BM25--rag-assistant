[site]: crossvalidated
[post_id]: 38767
[parent_id]: 38749
[tags]: 
That Wikipedia article is a mess. No wonder it has been tagged as "cleanup" for more than two years. If you want to learn about clustering , do not approach it from the learning side . To the machine learning side, unsupervised learning is the ugly duckling they resort to when they don't have any labeled training data. But they do not really like or understand it. Because actually it is doing something very different. Note that most of the clustering work is done outside of the machine learning community (but in the knowledge discovery community), and they would not call it unsupervised learning. In learning, you have an objective. You want e.g. to be able to predict the value of future observations. The clear objective in particular helps with evaluation, but also narrows down the search space a lot. In cluster analysis, you don't have a strict objective. It is an explorative method, with unfortunately a very big search space, so you need a lot of heuristics and assumptions. You want to explore your data, and learn something new - discover some new structure in this case. If you had a clustering method that gives your the structure you already know, it actually failed the objective to some kind. Yet, this is how cluster analysis is often approached and evaluated: can it discover the structure that I already knew? Dimensionality reduction is a technique one would prefer to be able to avoid (as it means dropping some of your data), but higher data complexity usually means much worse processing time. And if there is redundancy in your data, it is very reasonable to reduce the dimensionality first. Reducing the number of dimensions makes data tractable that was not tractable before. It also helps with finding an appropriate distance function, because the popular distances such as Euclidean distance don't work well in high-dimensional data due to what is known as the "curse of dimensionality". With increasing dimensionality, the distances in your dataset concentrate and become more similar. As most clustering algorithms are based on distances, they fail to find clusters then, as the difference between objects blurs. There are various aspects involved (I remember having just seen an article discussing like ~9 different views of it!), but naively you can see it as a consequence of the central limit theorem. Given enough dimensions, the distances become normally distributed around some mean, and the variance is largely the amount of noise you have across the dimensions. If there is too much noise, the distances are only determined by the noise (not the signal) and your algorithms fail. BSS I can't tell you much about. It has not been on my radar much. I remember having seen the basic idea in a purely audio analysis domain (isolating 4 voices from an 5-channel audio signal, I remember something about needing n+1 microphones to isolate n voices based on the delay alone?) It definitely is not the essential technique of unsupervised learning...
