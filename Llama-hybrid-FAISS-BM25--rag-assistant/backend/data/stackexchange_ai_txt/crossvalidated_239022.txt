[site]: crossvalidated
[post_id]: 239022
[parent_id]: 238822
[tags]: 
Usually, n-gram language model use a fixed vocabulary that you decide on ahead of time. In the smoothing, you do use one for the count of all the unobserved words. It's possible to encounter a word that you have never seen before like in your example when you trained on English but now are evaluating on a Spanish sentence. The out of vocabulary words can be replaced with an unknown word token that has some small probability. This is consistent with the assumption that based on your English training data you are unlikely to see any Spanish text. Another thing people do is to define the vocabulary equal to all the words in the training data that occur at least twice. The words that occur only once are replaced with an unknown word token. This way you can get some probability estimates for how often you will encounter an unknown word.
