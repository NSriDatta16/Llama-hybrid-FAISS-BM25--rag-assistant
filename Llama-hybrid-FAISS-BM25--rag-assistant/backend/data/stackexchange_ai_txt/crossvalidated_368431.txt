[site]: crossvalidated
[post_id]: 368431
[parent_id]: 
[tags]: 
Can it be over fitting when validation loss and validation accuracy is both increasing?

Training a simple neural network over a very sparse matrix (Has 2400 features and 18000 train rows) for a binary classification problem. At the end of 1st epoch validation loss started to increase, whereas validation accuracy is also increasing. Can i call this over fitting? I'm thinking of stopping the training after 6th epoch. My criteria would be: stop if the accuracy is decreasing. Is there something really wrong going on? ps: I have perfectly balanced binary classification dataset and a random classifier would result around %50 accuracy.
