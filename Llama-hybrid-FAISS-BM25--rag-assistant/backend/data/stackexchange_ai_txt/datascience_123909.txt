[site]: datascience
[post_id]: 123909
[parent_id]: 
[tags]: 
Neural regression predictions all around the mean of target

I have a transformer regression model and some data about last users transactions (categorical and numerical). My target has exponential distribution with mean aroud 10e4 and also zero-inflated, so I have written my own implementation of tweedie loss, which is actually improving over training #loss function is somethin like this tweedie_loss = -1*(y*torch.pow(preds, 1-p)/(1-p)) + torch.pow(preds, 2-p)/(2-p) Training is going smoothly and my loss function is improving (until it hits the "plato" but it`s okay) But on evaluation model predicts pretty much same values around the mean (actually during training mean of predictions constanly growing up until it reaches mean of the target). Even if I trying to overfit it on data Im getting same results. I also tried to log-transform my data and fit it with different loss (rmse, smoothl1 etc...) but still getting nowhere. Problem is definitely not in the model implementation because Im using exact same code for another task and it works flawlessly. So I think my question is the following: do I have too complex target for the model or is there something wrong in my approach to solving the problem?
