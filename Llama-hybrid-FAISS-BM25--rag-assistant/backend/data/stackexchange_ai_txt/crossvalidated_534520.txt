[site]: crossvalidated
[post_id]: 534520
[parent_id]: 534514
[tags]: 
What anomaly detection algorithms do, is they learn the distribution of your data, and then, at prediction time, they cut off the most dissimilar observations and classify them as anomalies. To make the decision, they usually have a hyperparameter that controls how aggressive they ought to be when classifying observations as anomalies. For example, in scikit-learn the anomaly detection algorithms have the contamination (or nu for OneClassSVM) hyperparameter that tells the algorithm what is the fraction of the outliers in your data. To learn more about the parameters, consult the documentation of the software you're using and the papers describing the methods. To set the contamination parameter, you can make an educated guess about the possible fraction of the outliers that you expect to see in the data. Depending on how costly would it be for you to make false positive or false negative classifications, you can set it to the upper or lower bound of the expected fraction of outliers. The bigger problem however is that if you don't have any negative examples, you have no way of validating the performance of your algorithm. You really should have at least some samples of outliers in the test set.
