[site]: crossvalidated
[post_id]: 631219
[parent_id]: 
[tags]: 
Conflicting results from convergence measures for MCMC

I have a Gibbs sampling algorithm, for which I would like to estimate burn-in time. The model isn't hugely complex, and I run sampling for 1000 iterations. One approach I took was tracking the running mean/standard deviation with increasing samples. This showed me that approximately 100 samples were needed before the mean/standard deviation converged/stabilised, suggesting a burn in of 100 samples. I then turned to some more formal measures, such as the Geweke diagnostic and the Gelman-Rubin diagnostic. Both of those seem to tell me that the even without dropping any burn-in observations, we have convergence. This leaves me with two questions. Firstly, how can MCMC converge instantaneously so that I don't need any burn-in? Secondly, how can I reconcile the observations when tracking running mean/standard deviation with the results from the more formal convergence metrics? And a side question - with Geweke's diagnostic, I find that some high burn-in values sometimes have a smaller z-score than lower burn-in values. For example, burning the first 500 samples yields worse convergence than say burning the first 200 samples. Why might this be the case?
