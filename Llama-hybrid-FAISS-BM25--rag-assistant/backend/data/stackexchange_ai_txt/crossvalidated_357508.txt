[site]: crossvalidated
[post_id]: 357508
[parent_id]: 
[tags]: 
Why does Hindsight experience replay help addressing the problem of sparse rewards?

My question is about Hindsight experience replay which has been proposed for problem with sparse rewards. While the idea of this paper seems intuitive, I don't know why it can help in addressing the above issue. The following paragraph is from this paper. Any idea why it helps? For example, in a game where at each time, the reward is -1 if state st is not equal to a goal g and +1 otherwise (st =g). Then, consider a state sequence s1, . . . , sT and a goal g which is not equal to any of s1, . . . , sT which implies that the agent received a reward of −1 at every timestep. The pivotal idea behind our approach is to re-examine this trajectory with a different goal — while this trajectory may not help us learn how to achieve the state g, it definitely tells us something about how to achieve the state sT . This information can be harvested by using an off-policy RL algorithm and experience replay where we replace g in the replay buffer by sT . In addition we can still replay with the original goal g left intact in the replay buffer. With this modification at least half of the replayed trajectories contain rewards different from −1 and learning becomes much simpler.
