[site]: crossvalidated
[post_id]: 234681
[parent_id]: 
[tags]: 
Interpretation of Weighted Mean of Relative Errors

I run a simulator and compare the results against the real experiment results. The results show the resource usage of the system every minute. So it's a time series data. I first computed the point-wise relative error: RelativeError = |Experiment - Simulation| / (Experiment) Now I want to aggregate the relative error over the time space. I use the weighted mean of the relative errors with the experiment results as the weights, because the data points with the higher resource usage values in experiment results are the main concerns in my simulation. WeightedMean(Experiment, RelativeError) = Sum (Exp_i * RelativeError_i) / Sum (Exp_i) But then, something weird occurs: (Exp_i * RelativeError_i) = Exp_i * |Exp_i - Sim_i| / (Exp_i) = |Exp_i - Sim_i| So the weighte mean becomes: WeightedMean(Experiment, RelativeError) = Sum(|Exp_i - Sim_i|) / Sum(Exp_i) It is summation of absolute errors divided by the summation of experiment results, which is hard to understand as a meaningful metric. Can any one give an idea on it? Is this a wrong way to use the weighted mean?
