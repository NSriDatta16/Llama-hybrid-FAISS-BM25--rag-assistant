[site]: datascience
[post_id]: 122157
[parent_id]: 
[tags]: 
Difference between Word2Vec and contextual embedding

am trying to understand the difference between word embedding and contextual embedding. below is my understanding, please add if you find any corrections. word embedding algorithm has global vocabulary (dictionary) of words. when we are performing word2vec then input corpus(unique words) map with the global dictionary and it will return the embeddings. contextual embedding are used to learn sequence-level semantics by considering the sequence of all words in the documents. but i don't understand where we considered context in word embedding.
