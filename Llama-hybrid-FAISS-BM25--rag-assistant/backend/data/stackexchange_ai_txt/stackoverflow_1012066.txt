[site]: stackoverflow
[post_id]: 1012066
[parent_id]: 1010652
[tags]: 
It depends on how many images you expect to handle, and what you have to do with them. I have an application that needs to temporarily store between 100K and several million images a day. I write them in 2gb contiguous blocks to the filesystem, storing the image identifier, filename, beginning position and length in a database. For retrieval I just keep the indices in memory, the files open read only and seek back and forth to fetch them. I could find no faster way to do it. It is far faster than finding and loading an individual file. Windows can get pretty slow once you've dumped that many individual files into the filesystem. I suppose it could contribute to security, since the images would be somewhat difficult to retrieve without the index data. For scalability, it would not take long to put a web service in front of it and distribute it across several machines.
