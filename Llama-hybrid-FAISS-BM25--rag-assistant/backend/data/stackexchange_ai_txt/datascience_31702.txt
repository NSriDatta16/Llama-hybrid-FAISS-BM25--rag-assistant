[site]: datascience
[post_id]: 31702
[parent_id]: 31700
[tags]: 
3D plot 3 features indicates that your data is 3 dimensional. Thus you can use a 3D plot. The following code will plot 3 dimensional data. $x$ is a numpy matrix with the 3 features as columns, and the rows are the instances. Then $y$ is the cluster label that you obtain from k-means. import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D import numpy as np from numpy import where # Plots 2 features, with an output, shows the decision boundary def plot3D(x, y): fig = plt.figure() ax = fig.add_subplot(111, projection='3d') pos = where(y == 1) neg = where(y == 0) color = ['r', 'b', 'y', 'k', 'g', 'c', 'm'] for i in range(30): ax.scatter(x[i, 0], x[i, 1],x[i, 2], marker='o', c=color[int(y[i])-1]) #ax.scatter(x[:,1], x[:,2], y) ax.set_xlabel('X Label') ax.set_ylabel('Y Label') ax.set_zlabel('Z Label') axes = plt.axis() plt.show() plot3D(X, cluster_labels) This will give you the following plot 2D plot Alternatively you can project the data into two dimensions. You can do this naively by collapsing any of the three dimensions. For example this will only show the first 2 features, the third would be projected onto the plane of the first and second feature. plt.scatter(X[:,0], X[:,1], c=cluster_labels) plt.show() You can also plot the 2nd and 3rd features, where the first feature is projected as plt.scatter(X[:,1], X[:,2], c=cluster_labels) plt.show() Projecting data naively can lead to problems so instead you can use a feature embedding method. Here I will give an example for 4 different methods: Isomap, MDS, spectral embedding and TSNE (my favorite). This is continuous data that I have access to but you can easily do the same for clustered data. Just set the labels $y$ as your determined clusters. from sklearn.datasets import load_boston from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt %matplotlib inline from sklearn.manifold import TSNE, SpectralEmbedding, Isomap, MDS boston = load_boston() X = boston.data Y = boston.target X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, shuffle= True) # Embed the features into 2 features using TSNE X_embedded_iso = Isomap(n_components=2).fit_transform(X) X_embedded_mds = MDS(n_components=2, max_iter=100, n_init=1).fit_transform(X) X_embedded_tsne = TSNE(n_components=2).fit_transform(X) X_embedded_spec = SpectralEmbedding(n_components=2).fit_transform(X) print('Description of the dataset: \n') print('Input shape : ', X_train.shape) print('Target shape: ', y_train.shape) print('Embed the features into 2 features using Spectral Embedding: ', X_embedded_spec.shape) print('Embed the features into 2 features using TSNE: ', X_embedded_tsne.shape) fig = plt.figure(figsize=(12,5),facecolor='w') plt.subplot(1, 2, 1) plt.scatter(X_embedded_iso[:,0], X_embedded_iso[:,1], c = Y, cmap = 'hot') plt.title('2D embedding using Isomap \n The color of the points is the price') plt.xlabel('Feature 1') plt.ylabel('Feature 2') plt.colorbar() plt.tight_layout() plt.subplot(1, 2, 2) plt.scatter(X_embedded_mds[:,0], X_embedded_mds[:,1], c = Y, cmap = 'hot') plt.title('2D embedding using MDS \n The color of the points is the price') plt.xlabel('Feature 1') plt.ylabel('Feature 2') plt.colorbar() plt.show() plt.tight_layout() fig = plt.figure(figsize=(12,5),facecolor='w') plt.subplot(1, 2, 1) plt.scatter(X_embedded_spec[:,0], X_embedded_spec[:,1], c = Y, cmap = 'hot') plt.title('2D embedding using Spectral Embedding \n The color of the points is the price') plt.xlabel('Feature 1') plt.ylabel('Feature 2') plt.colorbar() plt.tight_layout() plt.subplot(1, 2, 2) plt.scatter(X_embedded_tsne[:,0], X_embedded_tsne[:,1], c = Y, cmap = 'hot') plt.title('2D embedding using TSNE \n The color of the points is the price') plt.xlabel('Feature 1') plt.ylabel('Feature 2') plt.colorbar() plt.show() plt.tight_layout()
