[site]: crossvalidated
[post_id]: 355661
[parent_id]: 
[tags]: 
SVM: Why alpha for non support vector is zero and why most vectors have zero alpha?

In the optimization problem in SVM to compute the margin, we use Lagrange multipliers to insert the constraint: $$L(w,b,\alpha)= \frac{1}{\lambda}|w| - \sum \alpha (y_i(w*x_i+b) -1)$$ Now we want to compute the $\alpha$ . It is stated that $\alpha$ for all non-support vectors is 0. How is this statement derived from the above equation? How can that be proved? UPDATE : If we solve the dual of an SVM using the KKT conditions we have: $$w_i = \frac{1}{\lambda} \Sigma_{i=1}^{N}\alpha_i y_i x_i$$ One of the main goals of using the dual, is that $w$ can be computed more efficiently because of the above equation and the fact, that most $i\in[N]$ , $\alpha_i = 0$ , and therefore we just have to just focus on $\{x_i,y_i:\alpha_i \neq 0\}$ , which is a small part. My question is: Why are most of $\alpha_i, i\in [N]$ = 0? Geometrically it is said, that $\alpha_i\neq 0$ for exactly the data points which lie on the hyperplanes $\{x:w^Tx - b = 1 \}$ or $\{x:w^Tx - b = -1 \}$ . I am not sure that this is true. If this is the case, how can that be proven?
