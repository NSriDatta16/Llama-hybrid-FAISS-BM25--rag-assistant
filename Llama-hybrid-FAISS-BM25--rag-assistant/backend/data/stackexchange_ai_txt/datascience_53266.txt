[site]: datascience
[post_id]: 53266
[parent_id]: 53258
[tags]: 
Like all hidden layers in a neural network, an embedding layer can be thought of as a feature extractor with parameters that are automatically learned during training. So, like any other layer, the parameters are adjusted during training by the backpropagation algorithm . The specific equations used to compute error gradients and update weights are likely to depend on the optimizer used to train the network. The classic optimizer is stochastic gradient descent , but more sophisticated optimizers like Adagrad, RMSProp, and ADAM are commonly used nowadays.
