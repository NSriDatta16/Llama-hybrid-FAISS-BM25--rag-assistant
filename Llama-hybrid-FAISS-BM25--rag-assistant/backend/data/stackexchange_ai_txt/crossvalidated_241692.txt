[site]: crossvalidated
[post_id]: 241692
[parent_id]: 241653
[tags]: 
The variance is a kind of average squared difference from the mean. When you have the entire population of values, that's in fact exactly its definition. However with samples, you don't know the population mean, so you must use some estimate -- using the sample mean in the variance formula actually minimizes the average squared difference from the mean, making the sum of squares smaller than if you used the true population mean. So when you use the sample mean to calculate an estimate of the variance, it's too small. As it turns out, the resulting variance estimate is on average $\frac{n-1}{n}$ what it should be, so if you multiply by $\frac{n}{n-1}$ you make it the right size on average. So a common thing to do with sample variances is to cancel out the two terms in $n$ (the denominator of the average squared distance with the numerator of the adjustment) so that you end up dividing by $n-1$ instead of $n$ when you calculate the average. This use of $n-1$ on the denominator instead of $n$ is known as Bessel's correction . This is named after Bessel because it was first used by Gauss in 1823 (see Stigler's Law ). [Both definitions are perfectly valid ways to define a sample variance (we use biased estimators all the time). Indeed, if you're interested in estimating standard deviation (which is often the case) both the $n$ and $n-1$ divisor will yield biased estimates.]
