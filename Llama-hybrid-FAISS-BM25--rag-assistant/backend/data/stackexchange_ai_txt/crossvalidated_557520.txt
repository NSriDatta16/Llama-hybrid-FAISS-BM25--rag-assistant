[site]: crossvalidated
[post_id]: 557520
[parent_id]: 556085
[tags]: 
This clearly looks like an attempt by an instructor to introduce some intuition behind linear regression and iterative optimisation to computer science students not familiar with derivatives or without a mathematical background in general. If it was up to me I would do it in a slightly different way - start with some "goodness of fit" measure, then, since this is a simple linear regression with one covariate, perform a grid search over the intercept and slope, and calculate the selected goodness of fit measures for every point on a grid computationally using a loop. This would give students some intuition and they would even feel being able to get an approximate answer themselves. After this step we then can mention that for some goodness of fit measures such problem can be solved precisely without the need to perform any iterations, but that doesn't change the goal or intuition behind looking for the best-fitting solution and minimising residuals. Sticking with the iterative procedure, however, does require a starting point with a random line. This is similar to how some neural network optimisations start with initiation of random weights. Still, I feel, that calculating the goodness of fit to every subsequent iteration would help to clarify things further. Without it it might seem unclear why the line needs to move at all, and how is it getting any better by moving towards the points.
