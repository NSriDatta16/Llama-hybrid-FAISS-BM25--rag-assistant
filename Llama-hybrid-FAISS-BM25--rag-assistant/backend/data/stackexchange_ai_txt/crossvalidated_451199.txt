[site]: crossvalidated
[post_id]: 451199
[parent_id]: 451020
[tags]: 
You are right that for the different time points in a time series the i.i.d. assumption usually is inappropriate. However I believe that in all instances in which a Gaussian mixture is used for time series data in a sensible way, it is used so that there is no i.i.d. assumption for the points in the same time series. One possibility (as used in a paper linked in the comments to the original question) is that a full time series is represented by a single Gaussian vector allowing for a flexible covariance matrix structure, whereas different time series are assumed independent and the whole thing is used for clustering a data set of various time series. Another option is a Hidden Markov model, in which the cluster membership of points in time is governed by a Markov process allowing for dependence.
