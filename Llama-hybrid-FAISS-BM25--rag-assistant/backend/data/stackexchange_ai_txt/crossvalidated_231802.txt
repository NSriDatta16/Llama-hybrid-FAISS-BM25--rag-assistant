[site]: crossvalidated
[post_id]: 231802
[parent_id]: 
[tags]: 
What is the bias from choosing best train/test set from many options called?

I am confident that the below methodology creates a bias in that it indirectly uses the test set as part of the training procedure, however, I am having trouble explaining this to my colleagues. I would greatly appreciate any relevant terminology or reading material to help me articulate the problem. Here is an example scenario: I have a dataset split into three chunks A, B, and C. I use two chunks to train my model, and one to test. I try all permutations of test/train splits (ie using each AB, AC, BC as training), and get different results each time. I report the performance from the best of the splits as the performance of my model on this data. In reality, this value is an over-optimistic evaluation of my model's performance because I have essentially looked at all the data and chosen the best way to split it. This is cheating because I have looked at my test data before evaluating. The actual performance of my model would be the average performance of all split permutations. What is the bias I have introduced called? What is this (common) mistake called? Where/how can I find a more complete description of this problem?
