[site]: crossvalidated
[post_id]: 629209
[parent_id]: 248959
[tags]: 
"Vanilla" neural networks (fully connected layers, trained via backpropagation) should converge to similar results because the linear part of the neural layer should adjust for scaling and bias. However I don't have a proof of this. Also some activation functions have restricted outputs, like [0,1] for sigmoid and [-1,1] for tanh.
