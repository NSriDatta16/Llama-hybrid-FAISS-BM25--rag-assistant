[site]: crossvalidated
[post_id]: 420776
[parent_id]: 286386
[tags]: 
Stochastic gradient descent is an optimization algorithm. It is a variant of gradient descent. It is used to find minima or maxima of functions. The difference between SGD and vanilla gradient descent is that SGD works on samples of the objective function, while vanilla gradient descent works on the exact objective function. In statistical learning, for example, you want to find a parameter vector which maximizes the likelihood function of the data. The parameters are assumed static. Kalman filter is a type of online Bayesian learning. It can be used to learn states that varies with time in a nonstationary or stationary way. For this, Kalman filter assumes a model which describes the dynamics of the states over time. States can be any variable you want, including time-varying parameters of a statistical model. In a dynamic linear regression model, you must assume a model of how the parameters of linear regression are varying over time. A very simple model is to assume that the parameters vary as a random walk. At any time, a prior probability distribution synthesizes knowledge about the states (in your case, parameters of a model). With observation of data, you use Bayes rule to update to a posterior distribution. In the Kalman filter, both prior and posterior are Gaussians. Kalman filter has been shown to be optimal in the sense that it minimizes the mean squared error of the real unobserved state and its prediction. A closely related method is recursive least squares, which is a particular case of the Kalman filter. In summary, Kalman filter is an online algorithm and SGD may be used online. Kalman filter assumes a dynamic model of your parameters, while SGD assumes the parameters do not vary over time. SGD will not be optimal in a dynamic setting, specially because it relies on the stepsize parameter, which must be set by the modeler and must follow some theoretical conditions to converge. The Kalman filter also has an equivalent of the stepsize parameter, which is called "the Kalman gain", which automatically adapts to the data.
