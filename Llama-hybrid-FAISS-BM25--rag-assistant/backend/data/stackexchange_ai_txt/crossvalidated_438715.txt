[site]: crossvalidated
[post_id]: 438715
[parent_id]: 438673
[tags]: 
This depends heavily on the task, but typically, the size of networks used in RL is significantly smaller than in other applications. 2 to 3 fully connected layers is typical for Atari and Mujoco environments. Somewhere from 16 to 256 neurons per layer is common. Some exceptions I can think of are the large scale experiments run by OpenAI and Deepmind with Dota / Go / Starcraft, where they used pretty massive neural networks and a correspondingly ridiculous amount of compute power. Or is it art/trial&error? It's definitely trial and error -- RL is famously finicky when it comes to hyperparameters. I recall seeing a paper where the authors specified the one of the layers had precisely 87 neurons, the next had precisely ?? neurons, and so on. Which just goes to how precisely things had to be tuned to get the system to work.
