[site]: crossvalidated
[post_id]: 383389
[parent_id]: 
[tags]: 
Neural Network Parallel Architecture

I am a beginner to Machine Learning. While working on a personal project with VAEs, I had an idea. I will first give some background. Sometimes I have seen that it is common practice, when creating neural networks, to have multiple layers in parallel like so (just a badly made image in MS paint): Many neural networks have this kind of structure, where instead of a layer submitting an output to a single layer in front of it, it submits it to a series of layers (or sub neural network, which is the same as a series of layers) in front if it, usually designed for specific purposes. For example, lets say, in the picture, that the top layer track handled language recognition and the bottom layer track handled accent recognition, and they both fed their results into Layer 6 at the end to do further calculation. When training these kinds of networks, is it more effective to train each "parallel branch" as its own network individually and clip them together at the end for the final network, or to backpropagate the whole joined network in one shot? I have already done the math for it, but I am wondering if I should actually implement a system to allow for me to create asynchronous neural networks like these rather than just network chaining, and was wondering if someone with more experience than me could help me out. Thank you. For some context, I am creating a neural network library to help me with my work, and I am wondering if I should allow "forking" the network into two parallel tracks, or if I should force that to require training sub networks on their own, and just clip the networks together at the end. Thank you.
