[site]: datascience
[post_id]: 69405
[parent_id]: 12706
[tags]: 
I thought about three cases: Binary classification tasks: you can do it with one output node + Sigmoid + Binary Crossentropy loss. Multiple independent classifications: when an observation can be classified in two or more than two classes, and each class is independent from the others. In this case, even if you have many output nodes, the final activation function must be Sigmoid. LSTM and GRU layers contain Sigmoid and TanH gates. They have the right shape to let the RNN determine how much of past information must be forgotten/discarded/passed forward. (It's possible to change them with ReLU's, but there is no evidence this improves the inner performance of Recurrent layers.)
