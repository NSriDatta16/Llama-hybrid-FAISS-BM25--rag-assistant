[site]: datascience
[post_id]: 64483
[parent_id]: 64418
[tags]: 
You basically don't want to have a loss like that in any case. The objective of Machine Learning is define a loss function that is convex to find the minimum by approximating it with the given data. Not having a convex loss function means that you will have multiple minima and won't be able to determine if the one you're in is a global minimum or just a local one so you won't know if it's optimal or not. The only kind of Machine Learning without convex function is Deep Learning . And you see how complicated it is to work with it and having "good results" without understanding what is happening under the hood. Deep Learning models are (most of the time) stochastically with batch gradient descent, that helps getting out of local minima, but that's not even easy. We can't even prove mathematically that these systems are well-optimized. So basically, if you want a good model, you can avoid creating such a monster loss :)
