[site]: crossvalidated
[post_id]: 181692
[parent_id]: 
[tags]: 
Regulating precision in dimension reduction

I am having a problem trying to minimize the distortion when compressing/decompressing a set of parameters. Let me try to explain. I have a space of parameters $q_1$,$q_2$,...,$q_n$. Since these parameters have some level of correlation, I am using PCA to compress these variables, so that I have a smaller set of uncorrelated variables $r_1$,...,$r_m$, with $m When I try to reconstruct the original variables, naturally I will have some error added in the approximation. Let's call the set of original variables $Q$, and the reconstructed set $Q'$ In my problem I have a function $D(Q,Q')$ that measures the distortion between my original set and the reconstructed one. Ideally, I would like to minimize this distortion, but this function is not linear ($Q$ is composed of rotational data), and it is cumbersome to compute it. However, I can make some assumptions about my parameters. Some of them are more important than the others. For example, I know that when calculating the distortion, any approximation error added to $q_1$ has a bigger effect on the distortion than $q_2$. This means that while I don't want to completely ignore $q_2$, I would $q_1$ to have a higher precision when performing the inverse PCA. Is there a way for me to regulate the precision, or add specific weight to a dimension when performing dimension reduction?
