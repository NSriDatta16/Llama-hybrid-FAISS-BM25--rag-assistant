[site]: datascience
[post_id]: 115452
[parent_id]: 115446
[tags]: 
It is know that all optimization algorithms show more or less the same performance when it is averaged over all possible objective functions. This is know as the no free lunch theorem . You will have to do some experimentation. If you want to plug things in as you write in your post, you might want to use models that leverage feature importance, such as the random forest and XGBoost, as they both have the feature_importances_ property.
