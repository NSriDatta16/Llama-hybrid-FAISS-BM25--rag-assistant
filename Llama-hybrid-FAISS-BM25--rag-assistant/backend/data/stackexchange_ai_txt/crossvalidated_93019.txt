[site]: crossvalidated
[post_id]: 93019
[parent_id]: 93016
[tags]: 
There are cases where the multiclass approach would better; imagine that you have classes A1, A2 (both A) and B1 (only B class) and want to discriminate between A and B. Also consider that when looking at the predictors, B lies between A1 and A2; this already works in 1D or 2D. Some methods like linear discriminant analysis or logistic regression will fail miserably. That's because in a basic sense they will try to find a separating hyperplane, which will not work because A1 and A2 would lie on different sides (or all three classes would lie on the same). The same algorithms combined with a softmax extension to handle multiclass work fine when predicting A1, A2 and B1 individually. So, the answer is that it will depend on your data and your algorithm. I feel that if the subclasses are very distinct it makes more sense to give the model this additional structure, but there's no general law for all algorithms. But in a optimal situation more information is always better than less information. However, if the subclasses are so similar that the additional information does not help it just adds noise.
