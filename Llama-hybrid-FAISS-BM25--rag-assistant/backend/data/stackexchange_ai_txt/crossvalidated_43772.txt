[site]: crossvalidated
[post_id]: 43772
[parent_id]: 43768
[tags]: 
First of all, since your correlation matrix is a square matrix you may want to do Principle Component Analysis (PCA), I guess doing SVD on a square matrix is same as doing PCA, so you should be fine. I assume your original aim is to reduce data dimensionality. With whatever information you have provided, I see that you are working with only 6 features and for most practical purposes you can keep working with original 6 features without worrying about omitting some of them. If your aim is to map original data onto new space where features are non-correlated, then PCA can help. The eigenvectors that you have obtained above denote new axes (they are orthogonal to each other) and corresponding eigenvalues are indicators of how much variance in the data is accounted by each axes (once you have accounted for previous axes). You can do the following: You can set a threshold, say 80%, select appropriate number of eigenvalues so that 80% of the energy (or variance) is accounted for. In your case most likely first 4 (out of 6) would suffice. Then you project your data onto first 4 new axes (defined by first 4 eigenvectors) and then take your problem forward from that point. PS: A good resource to learn more about LDA/LSI is Chapter 18 of freely available book Introduction to Information Retrieval .
