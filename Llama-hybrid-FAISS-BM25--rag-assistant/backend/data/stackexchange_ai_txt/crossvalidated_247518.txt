[site]: crossvalidated
[post_id]: 247518
[parent_id]: 
[tags]: 
Autoencoder not able to generate same input and output value

I am doing some work with Theano based auto-encoder, giving input as samples from mixture of Gaussians, one hidden layer. I expected output to be same as input, but I am not achieving it. I have been inspired by this tutorial for implemenation. Is autoencoder with only one hidden layer is also sufficient to recover exact replica of output ? My code looks like below : ` def train(self, n_epochs=100, mini_batch_size=1, learning_rate=0.01): index = T.lscalar() x=T.matrix('x') params = [self.W, self.b1, self.b2] hidden = self.activation_function(T.dot(x, self.W)+self.b1) output = T.dot(hidden,T.transpose(self.W))+self.b2 output = self.output_function(output) # Use mean square error L = T.sum((x - output) ** 2) cost = L.mean() updates=[] #Return gradient with respect to W, b1, b2. gparams = T.grad(cost,params) #Create a list of 2 tuples for updates. for param, gparam in zip(params, gparams): updates.append((param, param-learning_rate*gparam)) #Train given a mini-batch of the data. train = th.function(inputs=[index], outputs=cost, updates=updates, givens={x:self.X[index:index+mini_batch_size,:]}) import time start_time = time.clock() acc_cost = [] for epoch in xrange(n_epochs): #print "Epoch:", epoch for row in xrange(0,self.m, mini_batch_size): cost = train(row) acc_cost.append(cost) plt.plot(range(n_epochs), acc_cost) plt.ylabel("cost") plt.xlabel("epochs") plt.show() # Format input data for plotable format norm_data = self.X.get_value() plot_var1 = [] plot_var1.append(norm_data[:,0]) plot_var2 = [] plot_var2.append(norm_data[:,1]) plt.plot(plot_var1, plot_var2, 'ro') # Hidden output x=T.dmatrix('x') hidden = self.activation_function(T.dot(x,self.W)+self.b1) transformed_data = th.function(inputs=[x], outputs=[hidden]) hidden_data = transformed_data(self.X.get_value()) #print "hidden_output ", hidden_data[0] # final output y=T.dmatrix('y') W = T.transpose(self.W) output = self.activation_function(T.dot(y,W) + self.b2) transformed_data = th.function(inputs=[y], outputs=[output]) output_data = transformed_data(hidden_data[0])[0] print "decoded_output ", output_data # Format output data for plotable format plot_var1 = [] plot_var1.append(output_data[:,0]) plot_var2 = [] plot_var2.append(output_data[:,1]) plt.plot(plot_var1, plot_var2, 'bo') plt.show() ' My learning curve looks like and my plot for input(red colour) and output value(blue) colour . My concept that I should expect same input/output is wrong or what ?
