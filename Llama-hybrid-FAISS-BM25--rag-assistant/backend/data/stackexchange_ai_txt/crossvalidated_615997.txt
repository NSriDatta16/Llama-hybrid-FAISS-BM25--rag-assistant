[site]: crossvalidated
[post_id]: 615997
[parent_id]: 
[tags]: 
Too large KL-divergence in training

I tried to train bayesian network using ELBO loss function. \begin{align*} \mathcal{F}(D, \theta) = KL[q(w|\theta)||P(w)] - \mathbb{E}_{q(w|\theta)}[\log p(D|w)] \end{align*} My question is, if model gets larger, is there any possibility to KL-divergence becomes larger so that dominate likelihood cost term? For large network, $w$ has many components, and their KL-divergence will be added as $\log$ is additive. But, likelihood loss does not change even we have larger network. So, when model gets larger, KL-divergence term in ELBO will be dominated. So I think model will be trained so that it just only regularize, but not do indeed train to increase accuracy. Indeed, in my implementation, my model has 2 hidden layer as bayesian. But it cannot be trained well. When I debug, cross entropy loss is up to $200$ , but my KL-divergence is up to $30000$ . So my network is only at first trained to increase accuracy, but after some epochs, its accuracy becomes lower. Is there any wrong in my opinion or my code implementation? def forward(self, x): # part of Bayesian Linear layer # Sample w, b w_eps = torch.randn((self.in_features, self.out_features)) b_eps = torch.randn(self.out_features) w_std = torch.exp(0.5 * self.w_logvar) b_std = torch.exp(0.5 * self.b_logvar) w = self.w_mu + w_std * w_eps b = self.b_mu + b_std * b_eps # Evaluate KL-divergence loss self.posterior = (-math.log(math.sqrt(2*math.pi))-torch.log(w_std+1e-8) -(((w-self.w_mu)**2) / (2 * w_std**2+1e-8))).sum() \ + (-math.log(math.sqrt(2*math.pi))-torch.log(b_std+1e-8) -(((b-self.b_mu)**2) / (2 * b_std**2+1e-8))).sum() self.prior = (-math.log(math.sqrt(2*math.pi))-(0.5 * w**2)).sum() \ + (-math.log(math.sqrt(2*math.pi))-(0.5 * b**2)).sum() self.kl_loss = self.posterior - self.prior return torch.mm(x,w) + b class BayesianNet(nn.Module): """ 2 layer Fully connected Network Linear -> ReLU -> Linear -> ReLU -> Linear """ def __init__(self, mode, input_size, hidden_size, output_size): super(BayesianNet, self).__init__() self.mode = mode self.flatten = nn.Flatten() # 2D to 1D array self.hidden1 = BayesianLinear(in_features=input_size, out_features=hidden_size[0], mixture_prior=mixture_prior, prior_init=prior_init) self.hidden2 = BayesianLinear(in_features=hidden_size[0], out_features=hidden_size[1], mixture_prior=mixture_prior, prior_init=prior_init) self.out = BayesianLinear(in_features=hidden_size[1], out_features=output_size, mixture_prior=mixture_prior, prior_init=prior_init) if mode == "Classification": self.last = nn.CrossEntropyLoss(reduction='sum') elif mode == "Regression": self.last = nn.MSELoss(reduction='sum') def forward(self, x): x = self.flatten(x) x = F.relu(self.hidden1(x)) x = F.relu(self.hidden2(x)) x = self.out(x) return x def kl_loss(self): return self.hidden1.kl_loss + self.hidden2.kl_loss + self.out.kl_loss def loss(self, x, y, samples=1): kl_loss = torch.zeros(samples) nll = torch.zeros(samples) for i in range(samples): pred = self.forward(x) kl_loss[i] = self.kl_loss() / 600 # I run MNIST with batch size 100, so total 600 batch exist nll[i] = self.last(pred, y) return pred, (kl_loss.mean() + nll.mean())/x.shape[0]
