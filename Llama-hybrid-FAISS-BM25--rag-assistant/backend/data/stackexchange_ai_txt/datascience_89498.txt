[site]: datascience
[post_id]: 89498
[parent_id]: 
[tags]: 
Classifying videos with varying length using ConvLSTM2D in tensorflow

I have a collection of videos, where I would like to extract a frame for every second, and then feed them through a ConvLSTM2D for binary classification. I was under the impression that a LSTM could take varying input sizes, but after many hours of googling it seems like I either need to: Use padding and masking Use ragged tensors Actually use varying input length, but use batch size of 1 I'm not sure how to proceed from here, since I cant find any resources for padding and masking a sequence of images. Ragged tensors are confusing, and I cant find any examples for a sequence of images. When trying to use a batch size of 1, tensorflow still complains that the inputs are not the same size when using model.fit . The length of the video is actually important, so thats the reason I'm using a variable amount of images, but I could possibly extract a fixed amount of frames. Any code examples or suggestions appreciated
