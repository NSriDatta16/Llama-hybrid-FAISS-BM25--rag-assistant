[site]: crossvalidated
[post_id]: 205534
[parent_id]: 205532
[tags]: 
Here's one possible explanation, though I don't know for sure it's what's happening here: Remember that a random forest is composed of trees, which are composed of splits, and that each of those splits see a random subset of the input features. In sklearn, the default is that each tree sees the square root of the total number of features. So, when you only input HoG features, each tree sees some random subset of the HoG features, and can do its thing pretty well. When you input both edge features and HoG features, each of your trees' splits is going to get to look at some combination of HoG and edge features. If the edge features are just less useful than HoG, as it seems they might be, then each of these splits is going to have fewer chances at finding that one delicious cut of HoG to work with â€“ so each split is going to just be a little worse. You could try increasing max_features to combat this.
