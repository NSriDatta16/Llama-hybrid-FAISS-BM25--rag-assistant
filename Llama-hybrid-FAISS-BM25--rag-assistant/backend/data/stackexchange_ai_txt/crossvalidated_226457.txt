[site]: crossvalidated
[post_id]: 226457
[parent_id]: 179836
[tags]: 
Bagging would help stabilize predictions, so it's worth a try, but you'll have to ensure models in the ensemble don't end all looking the same (high correlation between models is bad for bagging). So you could employ another concept, attribute bagging (otherwise called "Random subspace method") is useful in that scenario, I myself employed it. It's an ensemble method analogous to bagging, where you subset features instead of samples (Random Forests employ both at once, for example). Check this paper [1] that details the procedure and the properties. From personal experience, AB can work very well, even without bagging in $p \gg n$ problems. [1] Bryll, R., Gutierrez-Osuna, R., & Quek, F. (2003). Attribute bagging: improving accuracy of classifier ensembles by using random feature subsets. Pattern recognition, 36(6), 1291-1302.
