[site]: crossvalidated
[post_id]: 615022
[parent_id]: 615008
[tags]: 
Note that too much focus on a binary "reject/not reject" decision by a test is problematic. In fact p=0.06 and p=0.04 are very similar regarding their message about the data, so a p-value switching from, say, 0.04 to 0.06 when leaving out an observation doesn't mean that something big and worrying has happened. p-values slightly below 0.05 (and p-values in general) should be interpreted with some care anyway. Note that criticism of overuse and overinterpretation of hypothesis tests and p-values is all over the place these days, and too strong interpretation of being above or below some threshold is one of the major issues. The p-value computed on all the data gives you information about the evidence in all the data regarding your null hypothesis. If you leave out a data point (which normally would be good information in case your measurements are valid), you lose information, and obviously as a consequence the p-value will be different, and less useful than computing the p-value on all data. Cross-validation (CV) is not normally used with tests. The idea of CV is to leave data points out so that it can be assessed how well these points can be predicted by some kind of prediction method, where the predictions don't involve the predicted observations. As tests do not predict observations, this idea is not applicable to tests. (Although it is not necessarily wrong to take an "experimental" interest in what happens to the p-value if you leave observations out - however it's a nonstandard thing, so don't expect that there is a standard interpretation for whatever the outcome of this exercise is.) This also means that you won't be in any trouble if you ignore the results of the CV on p-values, as hardly anybody else would do such a thing in the first place. In case the null hypothesis is wrong (which some people argue is always the case), the smaller the sample size, the weaker the power. This means that you should expect (slightly) larger p-values if you leave observations out. Consequently, I'd expect the mean p-value from your CV to be slightly larger than the p-value from all data, but the latter one is a valid p-value whereas the former one isn't (as averaging p-values doesn't correspond to a well defined probability). Averaging p-values is problematic anyway, as the meaning of p-values is not linear. E.g., 0.12 is arguably much more similar to 0.1 than 0.005 is to 0.025. Averaging treats the p-values implicitly as linear.
