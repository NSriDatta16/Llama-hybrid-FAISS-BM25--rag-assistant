[site]: crossvalidated
[post_id]: 447715
[parent_id]: 447684
[tags]: 
It depends on how those predictions (whatfor) are made. Say you do a study on the influence of alcoholic products on health. Say you measure the number of occurances of cardiovascular disease as dependent variable and you have, among other beverages, separately white wine consumption and red wine consumption as independent variables. For a lot of people, the case is that they drink both if they drink any (the consumption of red and white wine correlates). If there is an effect of wine then it is a bit interchangeable to put this effect on white or red. So there might be in some studies (if they have red and white wine correlated) not much clarity about whether it is red or white. However, on the one hand, for most predictions it doesn't really matter. If you would erroneously put the effect on the one variable then you are still predicting indirectly ok because this wrong variable correlates with the correct variable. On the other hand, if you are making the predictions for cases where this correlation is absent (e.g. a group/person that is drinking only white wine or only red wine) then you may predict badly. The estimate/prediction may have bad external validity. See also the example in my answer to Does LASSO suffer from the same problems stepwise regression does? It shows fitting of the curve allong with the estimated coefficients. Some of those coefficients are estimated badly but because the wrongly estimated components are so close (correlating) to the correct components this matters very little for the fit of the curve. How does inclusion of these two in a multiple regression affect the predictions In this question Is ridge regression useless in high dimensions ($n \ll p$)? How can OLS fail to overfit? you can read how additional parameters work as some sort of additional noise, but also can actually work regularizing.
