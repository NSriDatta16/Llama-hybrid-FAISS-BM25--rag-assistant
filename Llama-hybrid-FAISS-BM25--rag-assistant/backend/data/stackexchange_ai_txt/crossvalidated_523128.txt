[site]: crossvalidated
[post_id]: 523128
[parent_id]: 
[tags]: 
When does this Deep Neural Network inequality become an equality?

Let $\text{DNN}_k(x)$ be some fully connected DNN with $k$ hidden layers. Let $(x,y)$ be some data points and $\ell$ be a loss function. Then $\forall \ k \in \{1,2,,..\} $ , we have that $$\min \ell(\text{DNN}_0(x), y ) \leq \min \ell(\text{DNN}_k(x), y )$$ My question is: $$\textbf{When does this above inequality become an equality?}$$ I have been looking at this for a while and I am not sure what to do. I am thinking about something with linear transformations? Basically, it is saying that with no activation functions, adding more hidden layers does not train a better model. Anyone know how to do this? Thanks!
