[site]: crossvalidated
[post_id]: 507827
[parent_id]: 
[tags]: 
Does Hard-SVM maximize the margin by shrinking the norm of $w$ a normal to the hyperplane?

I've been reading about Hard-SVM a little and I've ran into the analysis presented in "Understanding Machine learning" by Shai-Shalev Schwartz, there on pages 168-169 he presents a short proof of the following: The distance between a point $x$ and the hyperplane defined by $(w,b)$ where $||w||=1$ is $|\langle w,x\rangle+b|$ So That explains why we are interested in $w\in\mathbb R^d$ with norm of 1, otherwise we would have to revise the formula for the margin from $min_{i\in[m]} (|\langle w,x\rangle+b|)$ to $min_{i\in[m]} (\frac{|\langle w,x\rangle+b|}{||w||})$ Then he also presents the following quadratic program: $(w_0,b_0)=argmin_{(w,b)} ||w||^2$ s.t. $\forall i\in[m]: y_i(\langle w,x_i\rangle+b)\geq 1$ output: $\hat w = \frac{w_0}{||w_0||}, \hat b=\frac{b_0}{||w_0||}$ I'm a little confused by the fact that we minimize the norm of $w$ and then always return a $w$ with norm 1. Why would we do that? I guess it has something to do with how the margin was defined (for $w$ with norm 1), but doesn't that defeat the purpose of minimizing the norm of $w$ ? Reference (pages 203-204 in this pdf version) Thanks in advance.
