[site]: datascience
[post_id]: 37839
[parent_id]: 37835
[tags]: 
It means that you randomly select 40% of the neurons and set their weights to zero for the forward and backward passes i.e. for one iteration. Have a look here for some of the reasons and benefits . Have a look here for all the details of standard Dropout . Important is to notice that the remaining weights are commonly scaled by the value p , as to keep the expected mean value of the weights to be roughly consistent over many iterations. Different deep learning frameworks scale the weights at different different points, but the reason it the same. From the relevant Keras documentation : Dropout consists in randomly setting a fraction rate of input units to 0 at each update during training time, which helps prevent overfitting.
