[site]: crossvalidated
[post_id]: 563429
[parent_id]: 
[tags]: 
What pretrained word embeddings does the Universal sentence encoder use for Deep Averaging Network?

The paper for Universal sentence encoder Universal sentence encoder paper ! is pretty straightforward, and so is the paper for Deep averaging network Deep averaging network paper ! but I'm confused about one thing in Universal sentence encode, what's the embeddings they've used for the Deep averaging network! seems like they never talked about it on the paper! was just wondering what pre-trained embeddings they have used for the Deep Averaging network,pre-trained embeddings they have used for their transformer, or what embeddings the Universal Sentence Encoder-Lite demo is using for DAN? Does anyone know what embeddings they might be using?
