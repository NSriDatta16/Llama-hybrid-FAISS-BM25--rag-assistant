[site]: crossvalidated
[post_id]: 195557
[parent_id]: 195550
[tags]: 
Don't be alarmed. Logistic Regression (LR) can very much be a classification scheme. LR minimizes the following loss: $$ \mathop {\min }\limits_{{\bf{w}},b} \sum\limits_{i = 1}^n {\log \left( {1 + \exp \left( { - {y_i}{f_{{\bf{w}},b}}({x_i})} \right)} \right) + \lambda {{\left\| {\bf{w}} \right\|}^2}} $$ where the $x_i$ and $y_i$ are the feature vector and target vector for example $i$ from your training set. This function originates from the joint likelihood over all training examples, which explains its probabalistic nature even though we use it for classification. In the equation $\mathbf{w}$ is your weight vector and $b$ your bias. I trust that you know what ${{f_{w,b}}({x_i})}$ is. The last term in the minimization problem is the regularization term, which, among other things, controls the generalization of the model. Assuming all your $\mathbf{x}$ are normalized, for example by deviding by the magnitude of $\mathbf{x}$, it is quite easy to see which variables are more important: those wich are larger c.f. the others or (on the negative side) smaller c.f. the others. They influence the loss the most. If you are keen on finding the variables which really are important and in the process don't mind kicking a few out, you can $\ell_1$ regularize your loss function: $$ \mathop {\min }\limits_{{\bf{w}},b} \sum\limits_{i = 1}^n {\log \left( {1 + \exp \left( { - {y_i}{f_{{\bf{w}},b}}({x_i})} \right)} \right) + \lambda \left| {\bf{w}} \right|} $$ The derivatives or the regularizer are quite straightforward, so I will not mention them here. Using this form of regularization and an appropriate $\lambda$ will enforce the less important elements in $\mathbf{w}$ to become zero and the others not. I hope this helps. Ask if you have any further questions.
