[site]: crossvalidated
[post_id]: 436490
[parent_id]: 436327
[tags]: 
Cross validation can be applied as long as the model is predictive (i.e. $\mathbf x \mapsto y$ ), regardless of how that model works internally . This general applicability is one of the strong advantages of cross validation. And it also implies that there is nothing special in the cross validation of a k-nearest neighbour model compared to, say, a logistic regression model. $k$ NN produces predictions by looking at the $k$ nearest neighbours of a case $\mathbf x$ to predict its $y$ , so that's fine. In particular, the $k$ NN model basically consists of its training cases - but that's the cross validation procedure doesn't care about at all. We may describe cross validation as: loop over splits $i$ { split data into $\mathbf Z_i$ (train) and $\mathbf W_i$ (test) $model$ := training_function ( $\mathbf Z_i$ , further parameters) $predictions_i$ := prediction_function ( $model$ , $\mathbf W_i$ ) } calculate figure of merit from all $predictions$ and reference values training_function and prediction_function are just the same training and prediction functions that are used to produce a production model. Now, the $k$ NN training_function would just store the training cases and the chosen $k$ in $model$ , and the $k$ NN prediction_function would then look up the $k$ (number taken from $model$ ) closest training cases (also taken from $model$ ) to the submitted case(s) and calculate the prediction from their $y$ values. So the only thing that is a bit peculiar with $k$ NN compared to many other models is that the trainings function may not be doing much "proper work" (though that is really true only for the brute force approach, there are ways to put the training data into structures that allow easier prediction and those do "proper work" during training).
