[site]: crossvalidated
[post_id]: 383138
[parent_id]: 
[tags]: 
gradient descent for logistic regression

I'm implementing (for learning purposes) a logistic regression model. I've followed this guide . Now, the author is taking the derivative of $l$ , the cost function with respect to some $\beta_j$ : $$\frac{\partial l}{\partial \beta_j} =\sum_{i = 1}^m y_i x_{ij} - \sum_{i = 1}^m \frac{e^{\beta_0 + \beta \cdot x_i}}{1 + e^{\beta_0 + \beta \cdot x_i}} \cdot x_{ij} \\ = \sum_{i=1}^m x_{ij} (y_i - p(x_i;\beta_0, \beta))$$ my question is regarding $\beta_0$ . Basically the derivative is different for $\beta_0$ . Is that reasonable, for simplicity, to ommit $\beta_0$ and instead extending $x_i$ to a larger dimension where the first term is a constant of $1$ ?
