[site]: datascience
[post_id]: 126539
[parent_id]: 126538
[tags]: 
According to Karpathy: https://youtu.be/kCc8FmEb1nY?t=5729 it's more common to do pre-norm now vs. the post-norm formulation in the original paper. https://arxiv.org/pdf/2002.04745.pdf talks about some of the reasoning, mainly about stability of gradients during training. The earliest mention I could find of prenormalization is from here : " Different to Vaswani et al. (2017) we apply layer normalization before the self-attention and FFN blocks instead of after, as we find it leads to more effective training. Sub-blocks are surrounded by a residual connection (He et al., 2015)." It's possible that the Annotated Transformer was created after prenorm became the norm.
