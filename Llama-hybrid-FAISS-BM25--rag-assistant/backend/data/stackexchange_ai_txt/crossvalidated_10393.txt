[site]: crossvalidated
[post_id]: 10393
[parent_id]: 10387
[tags]: 
Regarding the " Real Values " The " Real Values " are better called " confidences " or (from my pov the most common term) " scores ". Such scores are often normalized so that they sum up to 1 for all classes. They represent a measure how, well, confident the model is that the presented example belongs to a certain class. They are highly dependent on the general strategy and the properties of the algorithm. For example in KNN the score for a class i is calculated by averaging the distance to those examples which both belong to the k-nearest-neighbors and have class i. Then the score is sum-normalized across all classes. Regarding your question I suppose with "converting into confidences" you actually mean "probability estimates". E.g. if an example has probability 0.3 for class "1", then 30% of all examples with similar values should belong to class "1" and 70% should not. As far as I know, his task is called "calibration". For this purpose some general methods exist (e.g. binning the scores and mapping them to the class-fraction of the corresponding bin) and some classifier-dependent (like e.g. Platt Scaling which has been invented for SVMs). A good point to start is: Bianca Zadrozny, Charles Elkan: Transforming Classifier Scores into Accurate Multiclass Probability Estimates EDIT after Question-Edit: @Thomas wrote: Does it seem right the answers with the highest real values be the ones that are most confidently categorized in the True group? Yes, in general this is correct (with the same argument as above). I suggest to create a ROC - plot to see if this also applies to mlpy - package. I suggest ROCR for this purpose.
