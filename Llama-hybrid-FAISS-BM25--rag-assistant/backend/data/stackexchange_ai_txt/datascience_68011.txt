[site]: datascience
[post_id]: 68011
[parent_id]: 67962
[tags]: 
The reason why the seq2seq models are not just stacks of layers is that the decoder cannot know in advance how long the output will be (at the inference time) and the next actions of the decoder depend on its previous actions. This property of the decoder is called autoregressivity. The decoder needs to keep track of two things: what is on the input (left branch of your diagram) and it did in the previous steps (right branch of the diagram). Formulating MT as a stack of layers is an active research area, mostly because it offers a significant speedup, but usually at the expense of translation quality. This approach also does not work with LSTMs, but only with Transformers because the self-attentive layers in the Transformer directly allow arbitrary reordering of input states which is a crucial feature for MT because different languages have different word order.
