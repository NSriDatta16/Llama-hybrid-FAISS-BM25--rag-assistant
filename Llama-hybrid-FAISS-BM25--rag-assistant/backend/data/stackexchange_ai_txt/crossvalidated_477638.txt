[site]: crossvalidated
[post_id]: 477638
[parent_id]: 477619
[tags]: 
As I understand your description, the previous action taken influences the distribution of reward values possible from the current time step. That means effectively the previous action should be part of the current state in order to maintain the Markov property - which is theoretically required so that the agent can correctly assign expected returns to action values. Add the previous action taken to the state features - for a neural network approximator in DQN, I would expect that to be a one-hot-encoded feature, i.e. multiple new input fields. This will correctly model the MDP as you need it to describe your problem. Everything else should then fall into place then with no need to change how the DQN solver works.
