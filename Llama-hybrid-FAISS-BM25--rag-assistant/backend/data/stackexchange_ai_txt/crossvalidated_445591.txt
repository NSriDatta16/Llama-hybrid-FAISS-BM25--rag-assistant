[site]: crossvalidated
[post_id]: 445591
[parent_id]: 445513
[tags]: 
Your understanding is correct. Word embeddings, i.e., vectors you retrieve from a lookup table are always non-contextual, not matter in what this is happening. (It is slightly different in ELMo which uses a character-based network to get a word embedding, but it also does consider any context). However, when people say contextual embeddings, they don't mean the vectors from the look-up table, they mean the hiden states of the pre-trained model. As you said these states are contextualized, but it is kind of confusing to call them word embeddings.
