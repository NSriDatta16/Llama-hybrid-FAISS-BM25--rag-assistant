[site]: crossvalidated
[post_id]: 350836
[parent_id]: 
[tags]: 
Alternate (?) definition of sample variance

The variance of a sample can be defined as $$s^2 = \frac{1}{2}\frac{1}{n(n-1)}\sum_{i}\sum_{j\ne i}\left(x_i - x_j\right)^2$$ Apart from the factor of $1/2$, this can be paraphrased verbally as The variance is the average of the squared distances between pairs of distinct data points Mathematically, this is equivalent to the "usual" definition of variance, $s^2 = \frac{1}{n-1}\sum \left(x_i -\bar{x} \right)^2$. Conceptually, however, it seems (to me) quite different, in two respects: This definition makes no reference to the mean value $\bar{x}$; we are not measuring how far apart points are from the mean , but rather how far away points are from one another . The factor of $n-1$ in the denominator -- which is well-known to be a source of confusion for students (see, e.g., Intuitive explanation for dividing by $n-1$ when calculating standard deviation? ) -- appears naturally because there are $n(n-1)$ ordered pairs of distinct data points $(x_i, x_j), i\ne j$. No need for either hand-waving justifications about "cushioning" the sample variance, or for complicated calculations of estimator bias. Again, just to be clear, I understand why the standard definition of $s^2$ includes a denominator of $n-1$, and I understand that the double-sum definition above is mathematically equivalent to the standard definition. What I would like to know: Are there contexts (pedagogical or otherwise) in which the alternate definition of variance (as a double-sum over pairs of data points) is more commonly referred to? Are there any textbooks, for example, that take this as the primary definition?
