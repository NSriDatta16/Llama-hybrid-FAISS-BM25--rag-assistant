[site]: crossvalidated
[post_id]: 363657
[parent_id]: 363646
[tags]: 
The explanation comes in the next paragraph. Instead of normalizing by the mean of activations in a single batch, they normalize with respect to the whole training data ($\mathcal{X}$). The way they achieve this is by using moving averages that get updated for each mini-batch during training. The reason why the bias term becomes redundant is due to learned scale and shift variables $\gamma^{(k)}$, and $\beta^{(k)}$. Without these, the normalization would squash everything which could limit what a non-linearity activation function can represent. They give the example of the sigmoid activation function which would be constraint to the linear part only and the tails would be squashed.
