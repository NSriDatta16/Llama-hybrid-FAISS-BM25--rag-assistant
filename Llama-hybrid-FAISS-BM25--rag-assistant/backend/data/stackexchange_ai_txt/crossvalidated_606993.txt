[site]: crossvalidated
[post_id]: 606993
[parent_id]: 606975
[tags]: 
Both (sufficiently large) neural networks and XGBoost are not interpretable on their own (they are not "intrinsically interpretable") and are typically seen as part of the same "not interpretable" category. Both can be interpreted post-hoc using various methods such as feature importance, SHAP, PDP, saliency maps , counterfactual explanations , etc. As Stephan Kolassa already mentioned, the "more interpretable" traditional models neural networks are often compared with are much simpler models like logistic regression or simple (non-boosted, non-ensemble) decision trees. (These often perform surprisingly well when compared with modern neural networks in a proper evaluation. Even in image processing, logistic regression based on interpretable hand-crafted features can very well outperform modern CNNs .) In a separate branch of research, there is a push to make neural networks intrinsically interpretable .
