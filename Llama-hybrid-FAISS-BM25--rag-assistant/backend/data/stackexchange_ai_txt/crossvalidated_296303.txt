[site]: crossvalidated
[post_id]: 296303
[parent_id]: 
[tags]: 
A classifier based on PCA done within classes

Suppose that you want to classify if an unlabeled length $n$ vector $X$ belongs to a class $i=1...I$. Method A: You have labeled data, so you compute the per class mean $\mu_i$ and covariance matrices $C_i$, as well as the PCA of the $C_i$'s so that you can write a subspace, $V_{i,d}$, from the $d$ top eigenvectors of $C_i$. You can then project $X$ onto a given $\mu_i+V_{i,d}$ subspace: $\rm{Proj}_{i,d}(X)$. Fix $d$. A possible classifier is then $\rm{argmin}_{i=1...I} \lVert X- \rm{Proj}_{i,d}(X) \rVert$. Method B Another classifier is $\rm{argmin}_{i=1...I} \lVert X- \mu_i \rVert$ Method C Nearest Neighbor search I am interested in comparing these 3 methods, in terms of predictive performance, and I am not interested in minimizing the amount of computation. My questions: A is like B but ignores the contribution to the distance from the lowest $n-d$ eigen-directions, correct? There is something to gain by doing A over B if you know there is some important linear discriminatory variables between the classes, and some unimportant nonlinear non-discriminatory variables between the classes (hoping to cut the later out with PCA)? When can B perform worse than A, in the opposite scenario I describe in 2? When will C beat A and B? What is method A called and is it well known, or a result of/related to something well known? Can this be related to m-ary hypothesis testing, or some linear discriminant?
