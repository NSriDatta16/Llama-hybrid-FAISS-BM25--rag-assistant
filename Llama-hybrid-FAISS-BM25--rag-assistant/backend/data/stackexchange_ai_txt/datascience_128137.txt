[site]: datascience
[post_id]: 128137
[parent_id]: 
[tags]: 
ValueError with matmul Single Class Sep. mismatch in its core dimension 0

Single Class Separation Problem Creating custom NMF function with custom weights updates (nmf_updateH) and (nmf_updateW). The goal is to compute Single Class Separation (NMF). **Effort to update H is working ok. Effort to update W on specific condition is causing a : Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 131 is different from 60). This is happening on following code section for the W calculation, W = W * ((X / (W @ H)) @ H.T) / (ones_matrix @ H.T): 119 W = nmf_updateW(X, activation, H, num_components) 121 WH = W @ H 122 error = np.sum(X * np.log((X + eps) / (WH + eps)) - X + WH) Cell In[75], line 9, in nmf_updateW(X, W, H, num_components) 7 F, T = X.shape 8 ones_matrix = np.ones((F, T)) ----> 9 W = W * ((X / (W @ H)) @ H.T) / (ones_matrix @ H.T) Test Case Works, but Production Code Fails Here is a test case on the function in error, namely, nmf_updateW(). I setup the conditions that match the overall production code run. The ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 131 is different from 60) and the matrix that need to work together are in the function nmf_updateW() at W = W * ((X / (W @ H)) @ H.T) / (ones_matrix @ H.T) : W = W * ((X / (W @ H)) @ H.T) / (ones_matrix @ H.T) #W = (513,60) * ((X / (513,60 @ H (131, 129)) @ (129, 131)) The code works for other passes but fails on this scenario: def nmf_updateH(X, W, H, num_components, eps=1e-20): F, T = X.shape ones_matrix = np.ones((F, T)) H = H * (W.T @ (X / (W @ H + eps))) / (W.T @ ones_matrix) return H def nmf_updateW(X, W, H, num_components): F, T = X.shape ones_matrix = np.ones((F, T)) W = W * ((X / (W @ H)) @ H.T) / (ones_matrix @ H.T) return W, ones_matrix F, T = S.shape W = np.random.rand(F, num_components) H = np.random.rand(num_components, T) H = np.random.rand(num_components, S.shape[1])[0:513,:] W = np.concatenate((W_S_model, W_N_model), axis=1)[0:513,:] print('----') print('WS', W_S_model.shape) print('WN', W_N_model.shape) print('W',W.shape) print('H',H.shape) print('signal', S.shape) _W, ones_matrix = nmf_updateW(S, W, H, num_components) print('ones_matrix', ones_matrix.shape) print(((S / (W @ H)) @ H.T) / (ones_matrix @ H.T)) print('_W',_W.shape) Machine Learning Code def nmf_updateH(X, W, H, num_components, eps=1e-20): F, T = X.shape ones_matrix = np.ones((F, T)) H = H * (W.T @ (X / (W @ H + eps))) / (W.T @ ones_matrix) return H def nmf_updateW(X, W, H, num_components): F, T = X.shape ones_matrix = np.ones((F, T)) W = W * ((X / (W @ H)) @ H.T) / (ones_matrix @ H.T) return W def nmf(X, num_components, provide=False, activation=0, max_iterations=1000, tolerance=1e-5, eps=1e-20): if not check_non_negativity(X): raise ValueError("matrix X must be non-negative.") F, T = X.shape W = np.random.rand(F, num_components) H = np.random.rand(num_components, T) if (provide): H = np.random.rand(num_components, X.shape[1])[0:513,:] nmf_prev_error = np.inf for i in range(max_iterations): #x, w, h = : 0.5 * np.square(no.linalg.morm(x-w@h)) H = nmf_updateH(X, W, H, num_components, eps) W = nmf_updateW(X, W, H, num_components) if (provide): W = nmf_updateW(X, activation, H, num_components) WH = W @ H error = np.sum(X * np.log((X + eps) / (WH + eps)) - X + WH) if np.abs(nmf_prev_error - error) ```
