[site]: datascience
[post_id]: 123214
[parent_id]: 123208
[tags]: 
No, the magnitude of gradient vectors is not arbitrary. As you pointed out, the gradient of a function at a particular point indicates the direction of steepest ascent. For some functions, like the Mean Squared Error (MSE) , there's typically a single global minimum , and a larger magnitude does suggest that we are far from this minimum. However, in the context of deep learning , where loss landscapes can have multiple peaks and valleys, "vanilla" gradient descent is less commonly used. Instead, augmented or modified variants of gradient descent, such as Stochastic Gradient Descent (SGD) , Mini-batch Gradient Descent , and methods with Adaptive Learning Rates , are employed. You can also find some interesting points here .
