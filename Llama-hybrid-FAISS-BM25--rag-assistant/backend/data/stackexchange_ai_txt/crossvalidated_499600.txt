[site]: crossvalidated
[post_id]: 499600
[parent_id]: 
[tags]: 
Bayes by backprop unbiased monte carlo gradients

I am currently trying to understand a paper on bayesian neural networks whereby the authors use a bayes by backprop approach to learn weight uncertainties in the neural networks. I am trying to understand the derivation for proposition 1 in the paper. Particularly, I am not sure how $$\frac{\partial}{\partial\theta}\int f(\boldsymbol{w},\theta)q(\epsilon)d\epsilon = E_{q(\epsilon)}[\frac{\partial f(w,\theta)}{\partial w} \frac{\partial w}{\partial \theta} + \frac{\partial f(w,\theta)}{\partial \theta}]$$ I am not sure why there is an additional $\frac{\partial f(w,\theta)}{\partial \theta}$ inside the expectation ? since I thought $\frac{\partial f(w,\theta)}{\partial w} \frac{\partial w}{\partial \theta} = \frac{\partial f(w,\theta)}{\partial w}$
