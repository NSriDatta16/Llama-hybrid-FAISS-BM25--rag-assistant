[site]: crossvalidated
[post_id]: 449648
[parent_id]: 247006
[tags]: 
This is a heuristic for approximating skewness of the long-tail in non-negative valued distributions. It's used to signal changes in the tails. Without a minimum specified, you'll find it relevant to latency, time-to-resolve, and any log-normal distributions. With only non-negative values, and an assumed minimum of zero, the observed maximum is also the range of the sample set. Long tail metrics are generally interested in the long right tail . The bigger the difference between maximum and average, the longer the right tail. That's not to say that ”in the middle” is preferred, better, or relevant. Many distribution have a mean that will be observed with a long tail. It's the change in the metric that is of interest. Typically you may see these heuristics as a percentage (avg/max) because the scaling is more standardized. But as others have pointed out it's reckless on it's own to conclude anything. The value is in observing the change of it as a time series. It's also cheap. Skew heuristics based on maximum and average require the least memory - 3 longs of storage for online calculation (sum, count, max) and are not constrained to any predefined observation range. For your continuing adventure consider these three distribution methods: Histogram : Requires bin definition in advance or at least a threshold beyond which you don't care. Requires a counter (long) for each bin. Provides observations of entire distribution. Can yield robust statistics like median, mode, percentiles, etc. Graphs nicely. Alerts nicely. Online calculations. Preferred for SLO of API latency in systems like Prometheus. Moments : Key statistics, like average, standard deviation, skewness, kurtosis, count, and let's lump in none-robust extreme like maximum and minimum as well. Used in heuristics to estimate some part of the distribution. Yields simple and mostly consistent but biased estimators. No apriori observations, like range or distribution shape, are required. Online calculations. These are cheaper and faster computationally, with a smaller memory space needed. Likelihood : Given an assumed distribution shape, calculate estimates of the parameters of the distribution by iterating through the observed data to maximize a likelihood function. These maximum likelihood estimators have higher probability of being close to the quantities to be estimated and are more often unbiased. Often uses moments as a starting estimate of parameters, or even to guess an unknown distribution. Definitely not an online calculation, and not generally a real-time possibility. For large enough data sets, might not even be tractable.
