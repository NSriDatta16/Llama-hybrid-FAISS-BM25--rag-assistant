[site]: crossvalidated
[post_id]: 282492
[parent_id]: 282482
[tags]: 
When working with data that is not stable it can either be that the X is coming from different distributions or f(X) (being the function you are trying estimate) is non-stationary. In the first case there are different solutions. First one use a classifier that captures allot of local structure. Thereby data that does not look like your test data will have lower influence and will not harm this as much. I recoment using a gradient boosting algorithm e.g. xgboost. Other than that you can weight the samples by how much they look like the test data in the training. This is often less critical for the model performance. The second case where the label generating function changes is way more difficult. If you know a variable changes effect over time, then removing it seems like a solution. But do not it just because the variables changes overtime only if the dynamics changes. Otherwise i recoment using classifiers that does not rely on a few variables for their prediction. So for RandomForest lower the m_try/number_of_features and for the xgboost lower the colsample allot. This generally make the classifier more robust. So what case do you think your in for this problem? In general I have had the best luck with tree models when data distributions changes overtime. My intuition is that tree models perform 0-order interpolation making large outliers having lower effect than if for example using linear models.
