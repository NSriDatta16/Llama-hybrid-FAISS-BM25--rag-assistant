[site]: datascience
[post_id]: 128419
[parent_id]: 
[tags]: 
Intuition and technical explanation behind "3D-aware Feature Attention" in Sync Dreamer? (Multiview Generating Diffusion Model)

I am looking to understand the whole block on how the Sync Dreamer paper has constructed the "3D aware feature attention" which is outlined on page 5. To enforce consistency among multiple generated views, it is desirable for the network to perceive the corresponding features in 3D space when generating the current image. To achieve this, we first construct a 3D volume with V 3 vertices and then project the vertices onto all the target views to obtain the features. The features from each target view are concatenated to form a spatial feature volume. Next, a 3D CNN is applied to the feature volume to capture and process spatial relationships. In order to denoise n-th target view, we construct a view frustum that is pixel-wise aligned with this view, whose features are obtained by interpolating the features from the spatial volume. Finally, on every intermediate feature map of the current view in the UNet, we apply a new depth-wise attention layer to extract features from the pixel-wise aligned view-frustum feature volume along the depth dimension. I don't really understand much that's going on here and have many doubts: What does it mean to project 3D vertices onto a set of target views? Can you get the 2D plane for a target view purely based on the azimuth angle and elevation? How do you restrict this 2D plane in size during projection? What does it mean to have a view-frustum "aligned pixel-wise with the target view"? What exactly is this depth-wise attention layer? I need some visualization intuition.
