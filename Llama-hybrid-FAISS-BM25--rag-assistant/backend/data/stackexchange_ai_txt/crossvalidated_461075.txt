[site]: crossvalidated
[post_id]: 461075
[parent_id]: 460510
[tags]: 
1. To understand the answer to your first question, we first need to understand what mutual information is and how it is different from conditional mutual information . For random variables $X$ and $Y$ with distributions $P_{X}$ and $P_{Y}$ respectively, mutual information $I(X; Y)$ is a measure of independence between $P_{X}$ and $P_{Y}$ , defined as the KL-divergence between the joint distribution $P_{(X, Y)}$ and the product of marginals $P_{X} \otimes P_{Y}$ as follows: $$ I(X; Y) = D_{KL}(P_{(X, Y)} || P_{X} \otimes P_{Y}). $$ If $P_{(X, Y)} = P_{X} \otimes P_{Y}$ then then $X$ and $Y$ are independent and hence $I(X; Y) = 0$ and the same goes from the other direction. In other words, $I(X; Y)$ measures the amount of information about $X$ we get after observing $Y$ and vice-versa. Now let $Z$ be some other random variable with distribution $P_{Z}$ . Suppose we have observed $Z = z$ . Conditionally on this new information , the mutual information between $X$ and $Y$ may have changed. For example, if $Z = (X, Y)$ , then observing $X$ does not provide any new information about $Y$ , since all of it is already contained in the random variable $Z$ . To account for this new information, we can compute a refined measure of independence between the distributions of $X$ and $Y$ . In particular, we can let $P_{X \mid Z=z}$ and $P_{Y \mid Z = z}$ denote the distribution of $X$ and $Y$ conditionally on $Z = z$ and let $$ I(X;Y \mid Z = z) = D_{KL}(P_{(X, Y) \mid Z = z} || P_{X \mid Z=z} \otimes P_{Y \mid Z=z}). $$ With the above notation at hand, we can now say what the difference between $I(X;Y \mid Z)$ and $I(X;Y \mid Z = z)$ is. The former is formally a random-variable , while the latter is a number . The conditional mutual information is defined to be the expected value of $I(X;Y \mid Z)$ -- that is, what is the average measure of independence between $X$ and $Y$ after observing $Z$ , which is typically denoted by the following rather unfortunate, overloaded notation: $$ I(X;Y\mid Z) = \mathbb{E}_{z \sim P_{Z}} I(X;Y \mid Z = z). $$ 2. Using the above notation $Z = \mathbf{H}$ , hence the expectation is taken with respect to $\mathbf{H} \sim P_{\mathbf{H}}$ . That is, the design matrix $\mathbf{H}$ is assumed to be random and follows distribution $P_{\mathbf{H}}$ . 3. $I(\mathbf{x}; \mathbf{y})$ and $I(\mathbf{x}; (\mathbf{y}, \mathbf{H}))$ have very different meanings! To appreciate the difference, consider a setting where $r = t$ and suppose that $\mathbf{H}$ is always invertible (that is, $P_{\mathbf{H}}$ is supported on a set of invertible square $t \times t$ matrices). Further, let the additive noise term $\mathbf{n}$ be always $0$ . Then \begin{align} \tag{*} \mathbf{y} &= \mathbf{Hx},\\ \mathbf{x} &= \mathbf{H^{-1}y}. \end{align} Hence, using the formulas you wrote down we have $$ I(\mathbf{x}, (\mathbf{y}, \mathbf{H})) = I(\mathbf{x}, \mathbf{y} \mid \mathbf{H}) $$ which means that $I(\mathbf{x}, (\mathbf{y}, \mathbf{H}))$ is large , because knowing $\mathbf{H}$ and one of $\mathbf{x}, \mathbf{y}$ the other one can be computed exactly using Equation (*) above. On the other hand, $$I(\mathbf{x};\mathbf{y})$$ is going to be small , because without the knowledge of $\mathbf{H}$ , $\mathbf{x}$ says nothing about $\mathbf{y}$ and vice-versa.
