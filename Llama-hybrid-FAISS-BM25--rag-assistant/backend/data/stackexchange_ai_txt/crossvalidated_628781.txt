[site]: crossvalidated
[post_id]: 628781
[parent_id]: 628338
[tags]: 
This is an interesting question. I guess the transformer might help in pretraining with a different (unsupervised task, like predicting masked timesteps). Then (if this pretraining was done on a huge amount of unlabelled data), I could imagine that the classification head on top of these pretrained transformers might perform better in your classification task than training a fully connected neural network for classification directly. The benefit would be that the transformer learn about features of your data, although you do not get additional labelled data for the classification task. Only experiments will tell you for sure.
