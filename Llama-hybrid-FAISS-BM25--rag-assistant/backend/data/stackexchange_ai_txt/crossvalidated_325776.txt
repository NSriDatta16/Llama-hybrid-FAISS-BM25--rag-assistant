[site]: crossvalidated
[post_id]: 325776
[parent_id]: 
[tags]: 
Does the universal approximation theorem for neural networks hold for any activation function?

Does the universal approximation theorem for neural networks hold for any activation function (sigmoid, ReLU, Softmax, etc...) or is it limited to sigmoid functions? Update: As shimao points out in the comments, it doesn't hold for absolutely any function. So for which class of activation functions does it hold?
