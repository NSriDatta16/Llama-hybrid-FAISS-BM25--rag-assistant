[site]: datascience
[post_id]: 33799
[parent_id]: 19802
[tags]: 
I faced this problem (and still face it today) for many years. I really thing that, if you don't provide detailed requirements, you can't expect a serious answer. I explain myself with examples of my work: I regularly try multiple variations of the same model to find what parameters work best. It takes several day to train one single model which produces some output that is later used for evaluation. To do so, I make a simple NumPy dump of the model since it is easy to share it between servers, or colleagues. You should avoid pickle since it stores much more (instances of class, libraries...) than just the parameters learned by your model. Importing the model on another machine might not work if the python environment slightly differs. When pushing a model in production, I need 1) a version of the model that I can load fast in case of a server breakdown (typically a binary format, storing only what is necessary such as weights of a neural network) and 2) a way to keep the model in-RAM to quickly deal with the API requests. For two different purposes, I need three different formats. Then, more generally speaking, the choice of the format depends on the tools you use. For example, if you work with TensorFlow, you might be interested in their TensorFlow Serving system
