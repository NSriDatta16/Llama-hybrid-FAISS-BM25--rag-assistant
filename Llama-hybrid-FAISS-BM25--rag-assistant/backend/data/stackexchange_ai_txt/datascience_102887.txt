[site]: datascience
[post_id]: 102887
[parent_id]: 102558
[tags]: 
Character-level models are only rarely better than subwords, not even in situations where you would naturally expect it (cf. recent papers When is Char Better Than Subword: A Systematic Study of Segmentation Algorithms for Neural Machine Translation , Towards Reasonably-Sized Character-Level Transformer NMT by Finetuning Subword Systems ). The biggest gains can come from data handling, not likely from modeling improvements. The SoTA in low-resource MT is using pre-trained models such as mBART or MASS , even in cases when the pre-training was done in different languages. In this case, you need to use the tokenization of the pre-trained model which is probably suboptimal, but the benefit of pre-training is usually bigger. It would be very helpful if you can generate synthetic data by romanizing existing parallel corpora. Also, if you can monolingual data, iterative back-translation is going to help a lot.
