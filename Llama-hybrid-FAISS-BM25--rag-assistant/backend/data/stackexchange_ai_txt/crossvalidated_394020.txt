[site]: crossvalidated
[post_id]: 394020
[parent_id]: 
[tags]: 
Is it better to interpret PCA components using the eigenvectors or the rescaled loadings?

I have a dataset to which I am applying PCA, and looking to each PCA component. Initially I was using the eigenvectors as a way to understand what each component "means". When using the eigenvectors I can see the coefficients of the linear transformation that leads to that PCA component so, interpretation seemed quite obvious (greater values mean those features are more important in that pca component). Looking to this thread: Loadings vs eigenvectors in PCA: when to use one or another? I understand the differences between loadings and eigenvectors, but... Does it make any difference when interpreting each PCA component? For example, when using the eigenvectors, if I sum all the squared values, the result will be 1, so by selecting the features whose squared eigenvectors are (for example) above 0.8 (aka 80% of the variance of that component I think) I can get the most extreme features contributing to that component. If I use the loadings, I incorporate the overall explained variance, but I loose the previous mathematical property to better interpret the component, right? For example, in this other thread: How to interpret PCA loadings? the person asking calls them "loadings" but as the best answer states, they are actually eigenvectors, and you can get a nice interpretation from there. Overall, it seems that we should use eigenvectors to understand what each component means in PCA, but after seeing so many people using loadings and rescaled loadings I start to doubt this and would like to ask the community if there is anything I'm missing here.
