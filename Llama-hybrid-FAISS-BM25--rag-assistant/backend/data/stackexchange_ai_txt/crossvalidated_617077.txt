[site]: crossvalidated
[post_id]: 617077
[parent_id]: 
[tags]: 
Confidence intervals for binary classification

I'm doing binary classification in Python with an SVM classifier, and I implemented stratified repeated cross validation to have more robust results. I would like to calculate confidence intervals for the mean accuracy, but using normal distribution assumption (if my code is correct) I obtain unrealistically narrow CIs. I know that another approach is to use bootstrapping, but I have some confusion and I am unsure if I can use bootstrapping together with cross-validation. Below is a toy example with the dataset breast_cancer where the accuracy is relatively stable from one run to another. However I have data with smaller sample size, where the accuracy can range a lot. from sklearn.datasets import load_breast_cancer import numpy as np from sklearn.model_selection import StratifiedKFold from sklearn.svm import SVC import scipy.stats as stats from sklearn.metrics import accuracy_score df = load_breast_cancer(as_frame=True) X = df['data'] y = df['target'] n_runs = 5 accuracies = [] for i in range(n_runs): # Split the data into training and testing sets using KFold skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=None) for train_index, test_index in skf.split(X, y): X_train, X_test = X.iloc[train_index], X.iloc[test_index] y_train, y_test = y[train_index], y[test_index] svclassifier = SVC(kernel='linear', random_state=None) svclassifier.fit(X_train, y_train) y_pred = svclassifier.predict(X_test) accuracy = accuracy_score(y_test, y_pred) accuracies.append(accuracy) mean_accuracy = np.mean(accuracies) std_accuracy = np.std(accuracies) n_samples = len(accuracies) z_score = stats.norm.ppf(0.975) margin_of_error = z_score * sqrt((mean_accuracy*(1-mean_accuracy))/n_samples) accuracy_CI = (mean_accuracy - margin_of_error, mean_accuracy + margin_of_error) EDIT: I found a way to compute the confidence interval, calculating the difference between the mean_accuracy and each value of accuracy obtained from each run of the classifier. That should be a bootstrap approach. Is that correct? result_tot=[] for i in accuracies: result=i-mean_accuracy result_tot.append(result) pct_05 = np.percentile(result_tot, 2.5) pct_95 = np.percentile(result_tot, 97.5) print("0.05 percentile:", mean_accuracy+pct_05) print("0.95 percentile:", mean_accuracy+pct_95) ```
