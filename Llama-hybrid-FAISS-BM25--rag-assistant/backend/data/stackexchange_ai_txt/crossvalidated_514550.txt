[site]: crossvalidated
[post_id]: 514550
[parent_id]: 333078
[tags]: 
I have also pondered this question and my tentative answer is a very practical one. Please take this with a pinch of salt. Suppose you have a model that has no parameters. For instance, your model could be a curve that predicts the growth of some scalar quantity over time, and you have chosen this particular curve because it is prescribed by some available domain knowledge. Since your model has no parameters, there is no need for a test set. To evaluate how well your model does, you can apply your model on the entire dataset. By applying, I mean you can check how well your chosen curve goes through observed data and use some criterion (e.g. likelihood) to quantify the goodness of the fit. Now, in practice, our model will have some parameters. The Bayesian methodology sets the goal of calculating the marginal log-likelihood which involves integrating out all the model parameters. Marginal log-likelihood quantifies how well the model explains the data (or should I say how well the data support the model?). By integrating out, we are left with no parameters to tune/optimise. I will risk saying that this seems to me very similar to the case where we had a model with no parameters , the similarity being that we do not need to adapt any parameters to the observed dataset. There is a nice quote that I have read in this forum which states that "optimisation is the root of all evil in statistics" (I think it originates from user DikranMarsupial). In my understanding, this quote says that once you have stated your model assumptions, all you have to do is "turn the Bayesian crank". In other words, as long as you can be Bayesian (i.e. integrate out the parameters), then you have no reason to worry about overfitting, as you are considering all possible settings of your parameters (with density dictated by the prior) according to your model assumptions. If instead you need to optimise a parameter, it is difficult to tell whether you are over-adapting it to the particular data you observe (overfitting) or not. One practical way of testing overfitting is of course holding out a test set which is not used when optimising the parameter. In the presence of competing rival models, that effectively challange your assumptions, you can compare them using marginal log-likelihood in order to find the most likely one. Of course, somebody naughty may posit a model which perfectly replicates the observed data (trivially, it could be the data itself). In such a case, I am not sure how I would defend myself. In real life, however, it would be hard to motivate this contrived model in a setting such as physics where explaination based models are required...
