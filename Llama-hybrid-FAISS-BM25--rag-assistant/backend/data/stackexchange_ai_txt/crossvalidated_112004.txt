[site]: crossvalidated
[post_id]: 112004
[parent_id]: 
[tags]: 
Is NN saturation always bad?

I am trying to analyse the effect of hidden unit saturation (outputting mostly 0 and 1 for sigmoid, and not much in-between) on the neural network training performance, and I feel a bit stuck, theory-wise: who says saturation is necessarily a bad thing? The idea is that a saturated unit can not differentiate much between patterns. However, that is only true if the unit outputs the same value for all signals it receives. What if the unit just turns binary? Of course, a collection of binary units has less information capacity than a collection of continuous non-linear functions... But perhaps binary (saturated) units still have a place and should not necessarily be eliminated? What are your thoughts on the dangers of saturation? And... are there any standard cures besides regularisation?
