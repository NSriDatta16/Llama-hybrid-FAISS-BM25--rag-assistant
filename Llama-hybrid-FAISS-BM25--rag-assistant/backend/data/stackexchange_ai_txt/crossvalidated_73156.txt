[site]: crossvalidated
[post_id]: 73156
[parent_id]: 73032
[tags]: 
Usually, the decision is whether to use linear or an RBF (aka Gaussian) kernel. There are two main factors to consider: Solving the optimisation problem for a linear kernel is much faster, see e.g. LIBLINEAR. Typically, the best possible predictive performance is better for a nonlinear kernel (or at least as good as the linear one). It's been shown that the linear kernel is a degenerate version of RBF , hence the linear kernel is never more accurate than a properly tuned RBF kernel. Quoting the abstract from the paper I linked: The analysis also indicates that if complete model selection using the Gaussian kernel has been conducted, there is no need to consider linear SVM. A basic rule of thumb is briefly covered in NTU's practical guide to support vector classification (Appendix C). If the number of features is large, one may not need to map data to a higher dimensional space. That is, the nonlinear mapping does not improve the performance. Using the linear kernel is good enough, and one only searches for the parameter C. Your conclusion is more or less right but you have the argument backwards. In practice, the linear kernel tends to perform very well when the number of features is large (e.g. there is no need to map to an even higher dimensional feature space). A typical example of this is document classification, with thousands of dimensions in input space. In those cases, nonlinear kernels are not necessarily significantly more accurate than the linear one. This basically means nonlinear kernels lose their appeal: they require way more resources to train with little to no gain in predictive performance, so why bother. TL;DR Always try linear first since it is way faster to train (AND test). If the accuracy suffices, pat yourself on the back for a job well done and move on to the next problem. If not, try a nonlinear kernel.
