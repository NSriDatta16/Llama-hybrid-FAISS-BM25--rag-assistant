[site]: crossvalidated
[post_id]: 371251
[parent_id]: 280665
[tags]: 
I will give my answer in context of the paragraph you cite: With K=N, the cross-validation estimator is approximately unbiased for the true (expected) prediction error, but can have high variance because the N "training sets" are so similar to one another. The CV estimator of the true (expected) prediction error is based on a training set example, so here, the expectation is over training set samples, when I understand that correctly. So, what this paragraph regarding "high variance" then says is that there is a "high" difference between expected error and the error estimated by CV (which is here, the average over folds). This makes sense because the model is fit to a particular training set and because all training folds are so similar within leave-one-out. However, while the training folds are very similar within a CV round, the estimate probably differs by a lot if we swap training samples for CV. In k-fold CV, since we "diversify" the training folds, we have some averaging affect, and across k-folds, the estimates then vary less. Or in other words, the leave-one-out CV estimator is basically almost like a holdout method were you don't rotate folds and base your error estimate on one validation set. Again, over training examples, there will be a high variance compared to estimates from k-fold, where you average over folds by already training somewhat diverse models within k-fold round (in other words, if you swap training sets, the estimates of the error via k-fold probably won't vary that much). EDIT: When I read some answers here on cross-validated and the internet in general, I think there seems some confusion to which estimator we are referring. I think some people refer to a model having high variance (with is ML talk for the loss having a dominating variance component) vs high variance of the k-fold CV estimator. And, another set of answers refer to variance as the sample variance regarding the folds when someone says "k-fold has high variance". So, I suggest to be specific, because the answers are different in either case.
