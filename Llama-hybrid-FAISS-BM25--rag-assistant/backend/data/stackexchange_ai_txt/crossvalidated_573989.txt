[site]: crossvalidated
[post_id]: 573989
[parent_id]: 573981
[tags]: 
Gradient descent is about following the derivatives (gradients). Recall that the derivative rule for calculating the derivative of a function $f$ times a constant $c$ is just $$ \frac{\partial}{\partial x} c f(x) = c f'(x) $$ So you only need to multiply the gradient by the weight. The weighted average is equivalent to using a regular average but with different observations repeated a number of times that is proportional to the sampling weights. In batch gradient descent, you would use weighted average instead of regular average, so it would be the same as if you used regular batch gradient descent, just the batches may vary in size because of using different weights. The changes in size are not a problem because we are averaging the result. That said, there is interesting empirical research by Byrd and Lipton (2018) showing that neural networks trained with SGD learn to ignore the sampling weights if they are trained long enough. It is an open question when exactly it happens, but worth keeping in mind that it can happen.
