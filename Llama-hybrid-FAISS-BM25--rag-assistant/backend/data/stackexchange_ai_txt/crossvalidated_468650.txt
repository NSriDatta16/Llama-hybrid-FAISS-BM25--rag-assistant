[site]: crossvalidated
[post_id]: 468650
[parent_id]: 468537
[tags]: 
The key idea here is that it's possible to avoid a full retraining of the model as new items and users show up in the data, instead you can incrementally learn new vectors for them. Imagine the following: You've trained your recommender system on a big batch of data and deployed it into production. Later on a new item $i_0$ is added, that wasn't around when you did your initial training. They're describing a technique to learn an embedding for $i_0$ without having to retrain your entire model. In order for this technique to work, some users must have interacted with item $i_0$ , otherwise as you mentioned there's nothing to learn. The same approach can work if a new user appears in the data. Again the new user must interact with some items before you can learn a useful representation for them. The big advantage here is computational efficiency, in order to compute the vector for the $i_0$ you only need to look at the vectors for the users who interacted with it. In a real world setting you might use this technique to keep the model up to date (doing this partial retraining say hourly or daily), and the doing a full retrain once a week or something.
