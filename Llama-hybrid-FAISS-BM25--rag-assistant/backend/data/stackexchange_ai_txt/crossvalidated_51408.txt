[site]: crossvalidated
[post_id]: 51408
[parent_id]: 
[tags]: 
How to measure the "average" autocorrelation of a time-series signal with itself

I have a short time series signal (say around 30 samples), and I would like to check whether or not it's oscillating. One approach I came up with was to measure the autocorrelation of the signal with itself. Given a random input signal, I would expect an output of 0. Given an oscillating signal I would expect some value consistently greater than 0. I have run the autocorrelation across all possible offsets of the signal (by "wrapping around" the index and assuming it's periodic), but when I take the mean of the autocorrelations across all possible offsets, I get (almost exactly) 0 no matter what the input signal looks like. This is likely "correct" (although not what I expected), but I can't seem to wrap my head around why that would be. Is there a better or more correct way of examining the autocorrelations to measure whether or not a signal tends to "agree with a shifted version of itself"? What am I missing?
