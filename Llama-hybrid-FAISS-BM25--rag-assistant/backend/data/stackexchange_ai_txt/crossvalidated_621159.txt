[site]: crossvalidated
[post_id]: 621159
[parent_id]: 
[tags]: 
Interpreting Bayesian u values (marginal p value)

I am going through Bayesian Modelling and Computation Chapter 2 which covers Exploratory Analysis of Bayesian Models. I am having some trouble wrapping my head around Bayesian u-values: They define the bayesian p value $p_{B}$ as the probability that simulated test statistic $T_{sim}$ is less or equal to the observed test statistic $T_{obs}$ : $$ \begin{eqnarray} p_{B} = p(T_{sim} \leq T_{obs} \mid Y ) \end{eqnarray} $$ They then introduce Bayesian u values: we evaluate how many of the simulations are below (or above) the observed data "per observation". For a well-calibrated model, all observations should be equally well predicted, that is the expected number of predictions above or below should be the same. I wasn't sure about what is meant by per observation, so I looked into some additional resources, namely the arviz.plot_bpv documentation which defines the u values $p_i$ as: $$ \begin{eqnarray} p_{i} = p(Y_{i}^{sim} \leq Y_{i}^{obs} \mid Y ) \end{eqnarray} $$ I think my confusion lies in what is meant by "per observation". How do you index into the distribution to get the ith observation (ie $Y_{i}^{sim}$ and $Y_{i}^{obs}$ . Does this imply we draw an equal amount of samples for each distribution, order them, and compare the values at the same index. Would appreciate any help building some intuition about bayesian u values (marginal p value)!
