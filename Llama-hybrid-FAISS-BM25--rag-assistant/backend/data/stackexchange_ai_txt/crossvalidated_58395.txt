[site]: crossvalidated
[post_id]: 58395
[parent_id]: 58371
[tags]: 
In ordinary least squares (e.g., lm(y~x)) you are allowing for variability (uncertainty) around y values, given an x value. If you flip the regression around (lm(x~)) you minimize the errors around x. In both cases, the errors are assume to be fairly homogenous. If you know the amount of variance around each observation of your response variable, and that variance is not constant when ordered by x, then you would want to use weighted least squares. You can weight the y values by factors of 1/(variance). In the case where you are concerned that both x and y have uncertainty, and that the uncertainty is not the same between the two, then you don't want to simply minimize residuals (address uncertainty) in perpendicular to one of your axes. Ideally, you would minimize uncertainty that is perpendicular to the fitted trend line. To do this, you could use PCA regression (also known as orthogonal regression, or total least squares. There are R packages for PCA regression , and there have previously been posts on this topic on this web site , which have then also been discussed elsewhere . Furthermore, I think (i.e., I may be wrong...) you can still do a weighted version of this regression, making use of your knowledge of the variances.
