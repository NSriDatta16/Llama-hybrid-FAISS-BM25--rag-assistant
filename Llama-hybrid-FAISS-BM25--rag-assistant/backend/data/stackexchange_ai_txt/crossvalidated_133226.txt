[site]: crossvalidated
[post_id]: 133226
[parent_id]: 133197
[tags]: 
In Information Theory the typical way to quantify how "close" one distribution to another is to use KL-divergence Let's try to illustrate it with a highly skewed long-tail dataset - delays of plane arrivals in the Houston airport (from hflights package). Let $\hat \theta$ be the mean estimator. First, we find the sampling distribution of $\hat \theta$, and then the bootstrap distribution of $\hat \theta$ Here's the dataset: The true mean is 7.09 min. First, we do a certain number of samples to get the sampling distribution of $\hat \theta$, then we take one sample and take many bootstrap samples from it. For example, let's take a look at two distributions with the sample size 100 and 5000 repetitions. We see visually that these distributions are quite apart, and the KL divergence is 0.48. But when we increase the sample size to 1000, they start to converge (KL divergence is 0.11) And when the sample size is 5000, they are very close (KL divergence is 0.01) This, of course, depends on which bootstrap sample you get, but I believe you can see that the KL divergence goes down as we increase the sample size, and thus bootstrap distribution of $\hat \theta$ approaches sample distribution $\hat \theta$ in terms of KL Divergence. To be sure, you can try to do several bootstraps and take the average of the KL divergence. Here's the R code of this experiment: https://gist.github.com/alexeygrigorev/0b97794aea78eee9d794
