[site]: datascience
[post_id]: 104179
[parent_id]: 
[tags]: 
Is the Transformer decoder an autoregressive model?

I have been trying to find an answer to these questions, but I only find conflicting information. Is the transformer as a whole autoregressive or not? And what about the decoder? I understand that the decoder during inference proceeds autoregressively, but I am not sure about during training time. Here are posts saying that the Transformer is not autoregressive: Minimal working example or tutorial showing how to use Pytorch's nn.TransformerDecoder for batch text generation in training and inference modes? Here are some saying that it is: What would be the target input for Transformer Decoder during test phase? https://www.tensorflow.org/text/tutorials/transformer https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0 https://huggingface.co/transformers/summary.html#seq-to-seq-models
