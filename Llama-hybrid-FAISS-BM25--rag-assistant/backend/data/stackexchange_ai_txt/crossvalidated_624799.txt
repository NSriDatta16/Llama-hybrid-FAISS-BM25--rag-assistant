[site]: crossvalidated
[post_id]: 624799
[parent_id]: 
[tags]: 
How do additional loss terms impact the parameter count in a Deep Neural Network?

I've been working on training DNNs for various tasks and recently started incorporating additional loss terms into my models to improve convergence and performance. I'm curious to know if I need to add parameters (e.g. layers) when I add loss terms encoding ``soft'' constraints or prior knowledge. Can someone provide some references about this? More practically, can I first search for the best the number of layer and then the best combination of loss term?
