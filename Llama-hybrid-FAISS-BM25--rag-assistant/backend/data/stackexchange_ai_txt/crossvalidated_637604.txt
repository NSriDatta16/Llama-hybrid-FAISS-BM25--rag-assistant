[site]: crossvalidated
[post_id]: 637604
[parent_id]: 637591
[tags]: 
First question: Is my reasoning reasonable? Am I missing something? Do I need to consider model effects as random since I’m somewhat assuming that there is a “mean slope” that governs the distribution of the model effect? Yes. Modeling ensemble members as random effects and both model type and data year as fixed effects is okay. However, it is also possible to have model type as random effects if we consider that the model types used in the data are a sample of all possible model specifications. Whether to use model type as random or fixed effects depends on the plausibility of the assumption that the effect of model types are independent from all other fixed-effect (regular) predictors in the formula (i.e. time in your case). If there are tens of model types (to ensure a reliable variance-component estimate) and they are reasonably argued to be uncorrelated with data year (effects of model types do not vary across data years), then we can have random effects of model types. It is not reliable to model data year as random effects if there are only three unique values of years: 1979, 1980, and 1981. If both model type and data year are essentially discrete categories, it is difficult (though possible) to model both random and fixed effects of them at the same time. Second question: Is this formula correct? No. You indicated that there are 36 years. This means we may need many polynomial terms to represent time effects accurately to meet the linearity assumption of the model. If we model (1) fixed effects of time , (2) fixed effects of model , (3) random effects of member , we should use a formula lmer(hgt ~ (time + I(time^2) + I(time^3) + ...) * factor(model) + (1 | member), data = data) . We should not nest effects of member in model using (1| model/member) , as they cross: Different categories of member appear within the same model while different categories of model appear within the same member . If we model random effects of model , the formula should be lmer(hgt ~ time + I(time^2) + I(time^3) + ... + (1 | member) + （1 | factor(model)), data = data) . The specification hgt ~ time * model + (1 + time | model:member) suggested by Ben Bolker is also attractive. But if we need multiple polynomial terms to represent time effects, allowing random effects of these many time terms correlated with the random intercept and each other can make the estimation unstable. Another reasonable specification is lmer(hgt ~ (time + I(time^2) + I(time^3) + ... ) * factor(model) + (1 | member:model), data = data) so there is only one random-intercept term but no random slopes, making the model more parsimonious and stable to estimate. Third question: How can I get an average slope and its uncertainty? It is unclear why you want “an average time effect” and what it refers to. You will get one single coefficient of time because the data sample shows it is integer, a continuous variable. If there are only three unique values of time , it is better to treat it as a discrete categorical variable, using factor(time) . You will get two distinct coefficients of time in this way, corresponding to time == 1980 and time == 1981 respectively. Since it is a linear model, the coefficients directly show average marginal effects: How much hgt changes moving from time == 1979 to time == 1980 and from time == 1979 to time == 1981 . If there are 36 unique values of time , it is beneficial to examine its effects as a discrete categorical variable, using factor(time) , so we can determine the shape of the curve to which polynomial terms of time have to fit. Changing the contrast suggested by Ben Bolker works but is unfamiliar to most audience. Presenting coefficients under such special contrasts requires deliberate explanation and may confuse reviewers. Presenting the contrasts not through coefficients but through prediction after fitting, through emmeans and marginaleffects , is much easier to understand. Fourth question: Is this serious? What could be causing the issue? Yes. (Perfect) collinearity can be the reason for singularity (taking the inverse of Hessian to get vcov() is as if dividing by zero). In your case, it is due to an oversaturated specification in the model matrix instead of the original data: too complex a model specification than what the data allows for accurate estimation. As Ben explains, (time | model/member) makes the fixed effect of model redundant, analogue to the collinearity issue. You should also check the confidence intervals to see if there are any boundary issues (confidence limits too close to theoretical boundaries), such as the lower limit of a standard deviation of random effects very close to zero. Sometimes we also need to center and scale predictors to achieve convergence, such as using I((time - 1990)/10) instead of time .
