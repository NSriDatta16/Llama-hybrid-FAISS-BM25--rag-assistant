[site]: datascience
[post_id]: 126623
[parent_id]: 
[tags]: 
Difference in the value of evaluation-metric in xgb.train() and predict in R

I have trained a xgboost classifier with a custom metric (f1_xgb), that is, the F1 score. Here the important aspect is that I evaluated the classifier on the test set by setting: `dtrain After the training, I used the predict() function on the SAME set ( x_test ). I would expect the same F1 score. Instead, the result was different (I used the F1 function from the MLmetrics package: f1 $pred, y_true = data$ obs, positive = lev[2]) c(F1 = f1_val) } ). However, the strangest aspect was a pattern involving the two F1 values as I adjusted the early_stopping parameter during the training. Here is the custom metric for the evaluation of the classfier: f1_xgb 0.5) & (labels == 1)) fp 0.5) & (labels == 0)) fn First attempt: no early_stopping, 1000 iterations: set.seed(20) xgb_tuned3 After all the iterations I get test-f1:0.181818. validation_probabilties3 0.5) f1_df As I said, the F1 value for the predicted values is higher than test-f1 . Second attempt: early_stopping = 200 set.seed(20) xgb_tuned3 validation_probabilties3 validation_prediction3 0.5) > f1_df f1_model f1_model F1 0.1818181818182 Here the F1 score is again higher, but is the SAME as the test-f1 value that I obtained in the first attempt ( test-f1:0.181818 )! Third attempt: early_stopping = 50 > set.seed(20) > xgb_tuned3 The evaluation metric value here is 0 > validation_probabilties3 validation_prediction3 0.5) > > f1_df f1_model f1_model F1 0.0952380952381 As expected at this point, the F1 score for the predicted values is the same I obtained during the attempt 2 training phase. I cannot really understand what is going on. Am I missing something obvious here?
