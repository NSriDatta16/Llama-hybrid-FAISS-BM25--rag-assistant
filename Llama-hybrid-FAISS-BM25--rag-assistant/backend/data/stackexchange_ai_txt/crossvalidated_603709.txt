[site]: crossvalidated
[post_id]: 603709
[parent_id]: 454320
[tags]: 
Depending on what the purpose of the explanation is, gradients make sense (or do not make sense). To explain a decision, e.g., what parts of the inputs are relevant (as done in GradCAM) one needs the final decision and it makes a lot of sense to go backwards from the output (class). For other purposes, gradients are not required, for example, if you wish to understand what information on the input is relevant for classification. To this end, one can pick a particular layer and apply a decoder on the activations that aims at reconstructing the original input. This allows to see what aspects of the inputs can be reconstructed well and which cannot. See ["Explaining Classifiers by Constructing Familiar Concepts", Schneider et al, Machine Learning, 2022] (Disclaimer, I am one of the authors)
