[site]: crossvalidated
[post_id]: 328667
[parent_id]: 
[tags]: 
Why this Way of iteration in RNNs?

One of the things that make them super-slow in training, is that they often use two nested loops to iterate on data through all time steps for every single iteration, calculating loss and cost at the end, and then back-propagate that error to get the gradients to update all weight matrices. I mean, We can't get rid of iterating for some epochs to fit the model more and more. but Isn't there any known method to just feeding the whole sequence at once and iterate for some epochs while training? I know this is impossible if we are talking about something like language models where the next input character is based on the probability made by the softmax activation from the previous time-step.
