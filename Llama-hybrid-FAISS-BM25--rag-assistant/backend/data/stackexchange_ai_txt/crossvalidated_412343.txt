[site]: crossvalidated
[post_id]: 412343
[parent_id]: 
[tags]: 
Time series data validation error is significantly lower than training error

I have a time series dataset that covers daily observations (closing price) for several stocks, and I would like to build models to forecast the closing prices for the future 7 days using their historical closing prices (up to 21 days) as the features. I splitted the data into training (first 9 months with shuffling) and validation set (last 3 months without shuffling) and built a MLP model on the training set. However, it is surprising that the training error (either MSE or MAE) is always significantly higher than the validation set. This is also the case for very naive model such as moving average. Is this normal in time-series forecast? What could be the reasons? Thanks! Edit: If I randomly split the data into 80% training and 20% validation, the errors are comparable, with validation error slightly higher, which makes the above issue even stranger. Why would different splitting methods yeild to different error patterns?
