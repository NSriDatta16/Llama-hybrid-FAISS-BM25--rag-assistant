[site]: crossvalidated
[post_id]: 161227
[parent_id]: 
[tags]: 
Regression Tree Predictions

For various regression tree algorithms (e.g. GBM, Random Forest, Extra Trees), is there any sensible way to get predictions for new data when the independent variables for the new cases are much larger than anything seen in training AND there is a highly positive or negative relationship between the independent variables and the response? For example, here's what I'd like to do: x = [1, 2, 3]; y = [2.1, 3.9, 5.8] # y is just about 2 times x model = random.forest(y ~ x) # run the model predict(model, [98, 99, 100]) # predict y where x is much larger now # Results in something like : [5.1, 5.1, 5.1] # when I'd expect closer to : [196, 198, 200] I understand that the prediction returned is just going to be something close to the mean value over a set of leaf nodes in the trees (so I'm not looking for an explanation on why this is happening), but this implies that regression tree algorithms are incapable of making good predictions for new feature data far outside the domain seen in training. Is this true, or are there common ways to account for this and get more reliable out-of-sample predictions for these types of models?
