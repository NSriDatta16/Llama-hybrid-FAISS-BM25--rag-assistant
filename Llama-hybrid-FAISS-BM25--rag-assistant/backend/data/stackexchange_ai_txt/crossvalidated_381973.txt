[site]: crossvalidated
[post_id]: 381973
[parent_id]: 381952
[tags]: 
Yes, with a Bayesian analysis you can get sensible posteriors and concentrate around sensible values to the point that the combination of information in prior and likelihood allow it. In this sense the Bayesian analysis can deal with this kind of situation a lot better than a frequentist analysis. However, it cannot get around the fundamental non-identifiability of the parameters in a model and the posteriors will still reflect the lack of identifiability of the parameters. To use an example, let's use a simpler model that is just $\log Z_i \sim N(\theta_1/\theta_2, \sigma^2)$ and let's assume that we have a huge amount of data. We'll have hardly any uncertainty around the ratio $\theta_1/\theta_2$ , but lots of values for each parameter are still getting support in the likelihood. Thus, we just get still a wide marginal posterior distribution and the joint distribution does have a very strong correlation between the two parameters. This is illustrated with example code below (using the re-parameterization $Y_i := \log Z_i$ and $\beta_j := \log \theta_j$ ). Obviously, the marginal posterior would be wider, if our prior for the parameters had been wider and the posterior correlation less strong, if we had less data (then the parameter identifiability problem would be less obvious). I would expect something similar to happen in your example - whether that's a problem or not is a different matter. library(rstan) library(bayesplot) y sigma; } model { beta0 ~ normal(0,1); beta1 ~ normal(0,1); sigma ~ normal(0,1); y ~ normal(exp(beta1-beta0), sigma); } " stanfit
