[site]: crossvalidated
[post_id]: 177223
[parent_id]: 141529
[tags]: 
The best way to determine the optimal number of hidden units is to perform cross-validation on your final supervised task using the new representation of your data. In other words, trial and error while monitoring the performance of your reconstructions (loss) on the unsupervised task and final error on the supervised task. I believe Hinton refers to the log-loss (aka cross-entropy) which is often used during training also in autoencoders (instead of the mean squared error). Optimizing the log-loss is interpretable to minimizing the description length -- aka compression, which is one way of interpreting RBMs, autoencoders (undercomplete) and other dimensionality reduction algorithms. "The principle of maximum pseudo-likelihood is based on optimizing a product of one-dimensional conditional densities under a log loss" See these slides: http://www.cs.toronto.edu/~asamir/cifar/cifarss-marlin.pdf
