[site]: crossvalidated
[post_id]: 199417
[parent_id]: 199397
[tags]: 
Let $Y$ be the binary response variable and $X$ the vector of predictors with density $f$ (which would either be continuous, discrete or a combination of both). Note that $$ \frac{P(Y = 1 \mid X = x)}{P(Y = 0 \mid X = x)} = \frac{P(Y = 1) f_{X \mid Y=1}(x)}{P(Y = 0) f_{X \mid Y=0}(x)} $$ and so $$ \log \left ( \frac{P(Y = 1 \mid X = x)}{P(Y = 0 \mid X = x)} \right ) = \log \left ( \frac{P(Y = 1)}{P(Y = 0)} \right ) + \log \left ( \frac{f_{X \mid Y=1}(x)}{f_{X \mid Y=0}(x)} \right ) . $$ This means that under a logistic regression model the logarithm of the prior odds of the event $\{ Y = 1 \}$ appears as an additive constant in the conditional log odds. What you might consider then is an intercept adjustment where you subtract off the logit of the empirical odds and add the logit of the prior odds. But, assuming that the prior probability is accurate this doesn't expect to have much of an effect on the model. This type of adjustment is made primarily after some sampling procedure that artificially alters the proportion of events in the data.
