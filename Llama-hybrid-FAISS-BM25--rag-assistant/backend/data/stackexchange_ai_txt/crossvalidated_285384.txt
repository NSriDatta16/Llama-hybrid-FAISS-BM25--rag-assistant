[site]: crossvalidated
[post_id]: 285384
[parent_id]: 
[tags]: 
Problems of small training dataset vs. large test dataset

I am currently working on a problem where my training dataset is very limited in size (few thousands of rows). Any model I develop on this needs to predict the outcome on a dataset millions of rows in size. I wanted to know what might be the possible problems that one might face when the training dataset is a few thousands of rows and test dataset is millions of rows? Because of the size disparity, more often than not, the output (classification/regression) will have a different distribution than in the original dataset. Is this a problem? What are the common pitfalls I need to keep in mind while in this scenario? If you want to answer specific to an algorithm - go at it with 'Random Forests'
