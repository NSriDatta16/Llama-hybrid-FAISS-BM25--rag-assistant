[site]: stackoverflow
[post_id]: 1506316
[parent_id]: 1505075
[tags]: 
I've generally looked at it not from a fastest perspective , but rather from a memory utilization perspective. All of the implementations have been fast enough for the usage scenarios I've used them in (typical enterprise integration). However, where I've fallen down, and sometimes spectacularly, is not taking into account the general size of the XML I'm working with. If you think about it up front you can save yourself some grief. XML tends to bloat when loaded into memory, at least with a DOM reader like XmlDocument or XPathDocument . Something like 10:1? The exact amount is hard to quantify, but if it's 1MB on disk it will be 10MB in memory, or more, for example. A process using any reader that loads the whole document into memory in its entirety ( XmlDocument / XPathDocument ) can suffer from large object heap fragmentation, which can ultimately lead to OutOfMemoryException s (even with available memory) resulting in an unavailable service/process. Since objects that are greater than 85K in size end up on the large object heap, and you've got a 10:1 size explosion with a DOM reader, you can see it doesn't take much before your XML documents are being allocated from the large object heap. XmlDocument is very easy to use. Its only real drawback is that it loads the whole XML document into memory to process. Its seductively simple to use. XmlReader is a stream based reader so will keep your process memory utilization generally flatter but is more difficult to use. XPathDocument tends to be a faster, read-only version of XmlDocument, but still suffers from memory 'bloat'.
