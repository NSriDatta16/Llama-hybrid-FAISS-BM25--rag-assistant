[site]: stackoverflow
[post_id]: 4084004
[parent_id]: 4037927
[tags]: 
Since nobody else has chimed in, I'm going to assume that my evolved theory of what happened was correct. Specifically, this was due to a lock of as-yet undiagnosed origin on the ModelingNodeState or ModelingNodeStateLog table causing a statement timeout, not a connection timeout. This distinction was obfuscated by LINQ-to-SQL's usually handy management of connections. This agrees with all observed symptoms: Sproc calls with deliberately bad credentials in the connection string were rejected immediately at the connection level Sproc calls with correct credentials in the connection string were allowed to login (SQL Server logs showed normal login/logout), but timed out executing the statement agains the locked table LINQ's hiding of the underlying SQL connection management meant the captured stack trace could not be used to distinguish between a connection and a statement timeout, and I erroneously assumed it was a connection timeout. We have no idea how/why/if there was an all-day lock on either or both of these tables, but an uncommitted statement somewhere could have done it, as this began after a night of deploys involving DBA work, migration scripts, etc. This would explain things starting suddenly (after the script began), lasting all day, then suddenly resolving when the offending script committed or was rolled back, perhaps when the app running it was shut down at the end of the work day. Lessons learned: Don't assume you know what kind of timeout it is just because you saw the word "timeout" in an exception or stack trace As I thought about it, I realized that this whole aspect of the app could be allowed to fail utterly without impacting critical app functionality. I'm going to rewrite it so that a recurrence of this issue will not bring the whole app down as it does today. Thanks for the community feedback that steered me to the (let's hope) correct answer!
