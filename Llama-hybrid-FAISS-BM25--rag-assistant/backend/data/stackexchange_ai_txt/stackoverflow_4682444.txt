[site]: stackoverflow
[post_id]: 4682444
[parent_id]: 4682358
[tags]: 
I worked at a startup that basically answered this question with software. Part of the answer is organizing your network so that multicast works, and part of it is reliant on intelligently forward-caching content to local servers so that not every file transfer has to go all the way across your network. On top of the multicast, you should look at forward error correction. FEC allows you to transmit large files with a small "insurance policy" in the form of correction bytes that will ensure that if there is a problem with the transmission (up to a critical level), the entire file need not be retransmitted. We used a third party platform called Fazzt for this. FEC is actually everywhere in world, on our CDs, mobile phones, but I digress. I think the real takeaway is that you need to intelligently manage requests...you can treat requests for multi-gig files the same way you do ICMP packets - the economics really come into play.
