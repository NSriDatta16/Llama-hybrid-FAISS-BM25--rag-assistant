[site]: crossvalidated
[post_id]: 314526
[parent_id]: 
[tags]: 
Is it pointless to use Bagging with nearest neighbor classifiers?

In page 485 of the book [1], it is noted that " it is pointless to bag nearest-neighbor classifiers because their output changes very little if the training data is perturbed by sampling ". This is strange to me because I think the KNN method has high variance when $K$ is small (such as for nearest neighbor method where $K$ is equal to one), and that makes it perfect for bagging. What is wrong with this intuition? [1] Witten, Ian H., et al. Data Mining: Practical machine learning tools and techniques. Morgan Kaufmann, 2016.
