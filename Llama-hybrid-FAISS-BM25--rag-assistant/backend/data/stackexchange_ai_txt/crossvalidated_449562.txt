[site]: crossvalidated
[post_id]: 449562
[parent_id]: 449147
[tags]: 
Although you could use a common ID for each matched pair, as suggested in a comment on the question, I would recommend that you use (inverse) propensity-score weighting rather than propensity-score matching to deal any bias resulting from how the method was chosen in each case. When you match you are necessarily throwing away all the information included in the discarded cases and decreasing the number of degrees of freedom for your ultimate statistical tests. If your original logistic multiple regression model was well specified, close to the "true" model, then you wouldn't need to correct at all. The propensity adjustment provides an additional control in case your model wasn't actually well specified. You can correct for the bias that you fear by including all cases but weighting each case inversely to its propensity score. Also, for generating the propensity score, it can be best to use a rich modeling approach that allows for nonlinearities and for interactions among covariates, like gradient boosting, as provided by the twang package in R. (I don't know what the equivalent is in SPSS.) That can provide superior performance to, say, determining propensity scores via a logistic regression without interactions. The threads listed as "Related" on this page and links from them provide further guidance. I've found answers from @Noah particularly helpful on issues regarding propensity scores.
