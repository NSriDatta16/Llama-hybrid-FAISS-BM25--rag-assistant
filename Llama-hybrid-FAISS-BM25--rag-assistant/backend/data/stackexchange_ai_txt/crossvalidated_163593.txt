[site]: crossvalidated
[post_id]: 163593
[parent_id]: 
[tags]: 
Binning a continuous feature for a SVM

I am using a support vector machine (SVM) for binary classification. One of my features is continuous: each item has an attribute $x$ that is a real number. For various reasons (e.g., because I suspect the relationship between $x$ and the class of the item is not a simple linear relationship), I suspect the SVM will be more effective if I first bin $x$ into one of a few bins, so it becomes categorical. Is there a method of binning that works best for SVMs? Is the performance of the SVM sensitive to the choice of binning method? For instance, consider the following two options: Option A: Introduce five zero-or-one features $y_1,\dots,y_5$ corresponding to five bins, where $y_1=1$ if $x$ is in the low 20% percentile of range of values, $y_2=1$ if $x$ is in the 20-40% percentile, $y_3=1$ if $x$ is in the 40-60% percentile, etc. Option B: Introduce five zero-or-one features $y_1,\dots,y_5$ corresponding to five bins, where $y_1=1$ if $x$ is in the low 20% percentile of range of values, $y_2=1$ if $x$ is in the low 40% percentile, $y_3=1$ if $x$ is in the low 60% percentile, etc. (Thus Option A is more like an estimate of the pdf, and Option B is more like an estimate of the cdf.) Does one of these two options tend to work better with SVMs? For instance, I'm thinking that Option B might interact better with regularization (and parsimony). For instance, if just testing whether $x$ is in its low 60% is enough to get most of the predictive power out of $x$, then with Option B the SVM might put weight on $y_3$ and not any of the other $y_i$'s (with the corresponding regularization bonus), whereas Option A has no similar sparse option available to it. But I'm speculating wildly. Does it make a difference, in practice, and if so, how should I choose what binning method to use?
