[site]: crossvalidated
[post_id]: 516920
[parent_id]: 
[tags]: 
Incorporating per-class-accuracy penalties into Deep Learning

Typically when training Neural Networks for image classification, many people use SGD with weight-decay as a penalty term. The loss that is minimized corresponds to the misclassification and state of the art architectures can achieve very high overall accuracies. What might happen however is that some classes have a very low per-class-accuracy/recall/precision while others get classified very well. Are there approaches to mitigate this problem by incorporating a penalty term such that all classes must reach a certain accuracy, i.e. the results cannot be too imbalanced? Would greatly appreciate if someone could point me to a couple papers! Thanks!
