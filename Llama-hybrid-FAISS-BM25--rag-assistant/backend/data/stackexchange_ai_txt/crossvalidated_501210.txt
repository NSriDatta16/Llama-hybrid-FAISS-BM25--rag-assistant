[site]: crossvalidated
[post_id]: 501210
[parent_id]: 501039
[tags]: 
Almost yes . There are a few caveats regarding directly interpreting the scaled SHAP values, as the percentage contributions of our final classification prediction for a single observation. Raw SHAP values for classification tasks are often shown as additive contribution in the log-odds domain. That means that it is not a linear scale we are dealing with and scaling them will not massively simplify that. (Some implementation transform them to probabilities but that's not always the case.) SHAP values' baseline is always relative based on the average of all predictions; i.e. the contribution (SHAP value) of feature X regards the difference between the actual prediction and the mean prediction. Therefore they do not fully explain how a singe prediction came to be. (This may vary a bit based on the implementation but worth checking.) Depending on the learner used if the features A , B and C are on different scales, their coefficient values in the log-odds domain might not be directly comparable. (Usually not a big problem because often the features are binned when it comes to feature importance and/or we pre-process the data but it can happen.) SHAP (and Shapley) values are approximations of the model's behaviour. They are not guarantee to account perfectly on how a model works. (Obvious point but sometimes forgotten.) The above being noted, what you describe (summing the absolute SHAP values for an individual prediction and normalising them to get percentage of contribution) is reasonable. It is actually how we calculate overall feature importances with SHAP values. In this more general case, we sum the absolute values per feature across all our observations (and potentially normalise them afterwards). These overall feature contributions will be against our mean prediction of course; as we are examining relative importance for a classifier's overall output this is perfectly fine. For a single point it can be argued that any of the caveats mentioned above partially invalidates our proposed interpretation but realistically the contribution of each feature would "thereabouts" in terms of percentages for the prediction of a single observation.
