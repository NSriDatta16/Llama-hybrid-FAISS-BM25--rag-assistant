[site]: datascience
[post_id]: 25452
[parent_id]: 25420
[tags]: 
Here is just a guess, but according to me, the linearSVC might perfoms better than SVM with linear kernel because of regularization. Because linearSVC is based on liblinear rather than libsvm, it has more flexibility and it gives you the possibility to use regularization with your SVM (default is L2-Ridge regularization). Because you have more features than observations, it exists multiple solutions to your classification model. Some of them are more "robust" than other ones. L2 regularization will help you reduce the ampltitude of your model coefficients and maybe lead to a more stable solution. More on LinearSVC in the documentation : http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html More on Ridge regression benefits when working with a lot of features (and so multicollinearity) : https://stats.stackexchange.com/questions/118712/why-does-ridge-estimate-become-better-than-ols-by-adding-a-constant-to-the-diago
