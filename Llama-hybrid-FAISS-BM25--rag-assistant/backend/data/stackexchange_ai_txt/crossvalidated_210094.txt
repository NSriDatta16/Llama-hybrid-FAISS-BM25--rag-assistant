[site]: crossvalidated
[post_id]: 210094
[parent_id]: 
[tags]: 
Method for Estimating Autocorrelation in Severely Gappy Data

My question is somewhat long and boils down to "Does the following work?" I'm working on a project that involves timing analysis of astrophysical data sets that have large chunks of data missing due to occultation of the instrument (X-ray satellite telescope). An important method of analysis in the field involves computing the power density spectrum (PDS) of a given flux data series. However, the usual methods for computing the PDS involve computing estimators that are severely distorted by the gaps in the data, either through the DFT (periodogram) or autocorrelation, which is subsequently Fourier transformed (Wienerâ€“Khinchin theorem). There exist alternative spectral estimation techniques (Lomb-Scargle, etc) that try to approach the problem from another angle to avoid the issues gaps present, but I'm wondering if it's possible to directly eliminate the effect of gaps in computing the autocorrelation. The idea I had is to take a time series along with the time stamps for it and use the fact that the discrete autocorrelation is, for a given lag, the average correlation between points in the dataset at that lag. This is easily seen in the expression below. The correlations at a given lag are summed and then the result is divided by the number of terms in the sum, to give each lag the same weight despite a different number of terms in the each sum. Now, to modify this for gappy data, all one has to do is ignore terms in the sum that have one or both terms missing and modify the corresponding weight by dividing by (n-k-z), where z is the number of missing terms in the sum at that particular lag. At this point, I don't think there will be distortions introduced when the autocorrelation is Fourier transformed, since the effect that gaps introduce into the autocorrelation is eliminated. The correlation structure of the data series is maintained, information that's missing is excluded, and the result is scaled to give the best estimate given the information available. For discrete data, this will result in an autocorrelation vector that contains more points than the data itself, but I think this should be equivalent to zero-padding data in that the additional points do not represent additional information, just interpolation based on the available information. The result will be over-resolved, but not distorted. So, as asked at the outset, does this work?
