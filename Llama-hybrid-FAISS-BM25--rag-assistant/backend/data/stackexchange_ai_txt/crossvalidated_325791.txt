[site]: crossvalidated
[post_id]: 325791
[parent_id]: 325779
[tags]: 
I guess it depends on what you mean by "best fitting." I would point out two things. First, your sample variance will always be finite, regardless of how much sampling that you do. It is invalid as a tool if the shape parameter is less than two, but the sample variance will always be finite. Second, it may be better to think of variance as undefined rather than infinite. If you think of variance as a property that is either present or absent, rather than infinite or finite, it may make thinking about it simpler. So, let us consider the simplest possible test, which would be a Bayesian test using a conjugate prior. This may not be the best test, but it is pretty simple. It also has nice properties if you use an honest, prior density. Let's define our likelihood as $$L(\alpha|x_1,\dots,x_n)\propto\frac{\alpha^n\beta^{na}}{m^{\alpha+1}},$$ where $$\min(x_i)>\beta$$ and $$m=\prod_{i=1}^nx_i.$$ If you use a Gamma distribution as your prior distribution, then your prior density is: $$\pi(\alpha|a,b)=\frac{\alpha^{a-1}\exp\left(\frac{-\alpha}{b}\right)}{\Gamma(a)b^a},\text{ where }\alpha>0$$ and $a$ and $b$ are hyperparameters. The posterior density is also a gamma distribution with parameters $a'$ and $b'$ where $$a'=a+n$$ where n is the sample size and $$b'=\frac{1}{\frac{1}{b}+\ln(m)-n\ln(\beta)}$$ and $$b From: Arnold, Barry C., and Press, S. James. ”Bayesian Estimation and Prediction for Pareto Data”, Journal of the American Statistical Association 1989, 84, 1079-1084 quoted in https://www.johndcook.com/CompendiumOfConjugatePriors.pdf I would use wolframalpha.com if you do not have access to a math package as the cumulative posterior density is a generalized, regularized, incomplete gamma function. I would set $a=1$ and either set $b$ equal to the magnitude you think the observations have to the minimum value or at least small enough to meet the regularity condition required above. I would then integrate up to $\alpha EDIT Is the some level, either of the stability of the observed variance or of its value that implies you should reject your model? As compared to infinity, how low is to low? If you perform the above Bayesian method and end up with undefined variance, your question is "should I reject the model?" The answer is "no." In and of itself, the answer is no. The stability would trouble me on subsamples, but what I would do is test an alternate model. Bayesian methods allow you formally test which model has greater or lesser probability of being the near the data generating function. I have had a similar experience with sample variance in another set that definitely has no defined variance. The variance was wild until an extreme value was observed and it was so extreme that it stabilized the sample variance from that point onward. The one observation was so extreme that every wild swing ignoring it was so small that it had the appearance of stabilizing the sample variance. I had calculated the sample variance out of curiousity and I had a similar reaction that you had. I knew as a matter of mathematics a variance didn't exist, yet it hit a date and stabilized. Once I removed the offending observations, there were actually a few of them, then the sample variance was all over the place, but far smaller. The offending observations, which I kept for parameter estimation, were over 1500 times the interquartile range. That meant that those handful of observations were over 1500 times the maximum value of the middle 25,000,000 observations in my data. You could have things going on in your data, such as multiple Pareto distributions or some other effect that you have not accounted for. Also draw out the gamma distribution from the posterior density. If there is a significant reason to doubt the value of the parameter, it will show up in a very wide posterior density. The posterior accounts for all uncertainty in the data, presuming your likelihood function is correct. There may be a sizable mass for the parameter in the region that has a variance. If you have a mixture distribution, or it does not follow a Pareto distribution, then there isn't a simple or easy solution. It requires a lot of domain knowledge to even begin talking about what things may be happening in the data. How you model it depends on the domain knowledge and not at all on the statistics.
