[site]: crossvalidated
[post_id]: 118450
[parent_id]: 
[tags]: 
Can a perceptron be modified so as to converge with non-linearly separable data?

Normally, a perceptron will converge provided data are linearly separable. Now if we select a small number of examples at random and flip their labels to make the dataset non-separable. How can we modify the perception that, when run multiple times over the dataset, will ensure it converges to a solution? I know the pocket algorithm by S. I. Gallant provides an answer to this question, but I'm not sure how exactly it works or if there's any better solution. I'm a beginner in machine learning, so it would be great if someone could share a detailed explanation.
