[site]: crossvalidated
[post_id]: 259850
[parent_id]: 255423
[tags]: 
Random Forest is already designed to be robust to features that are not informative of the response. By using backwards elimination you are not doing the algorithm any favors, but you are making more decisions. Each of these decisions has a chance to be influenced by noise, and hence be wrong. As you make more decisions, you build up risk of componding your errors. Instead, just let the forest do its job. Give it all your features and trust the algorithm to make good decisions. If you really, really, really need to remove features for some outside concern (efficiency or implementability), then when all is said and done, you can use the feature importances to find which features were not used by the algorithm, remove them in one go, and then retrain for a final model.
