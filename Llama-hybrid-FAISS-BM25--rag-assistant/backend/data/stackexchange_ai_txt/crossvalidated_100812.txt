[site]: crossvalidated
[post_id]: 100812
[parent_id]: 100800
[tags]: 
Your question is tricky to answer because it covers almost all of machine learning! In some cases, it's possible to solve for the optimal values directly, using a known formula. Suppose you want to model some data with a line: $$Y=AX$$ but don't know the slope . The least-squares algorithm can calculate the slopes $A$ directly, without actually searching for them. In other cases, algorithms have parameters or hyper-parameters that need to be determined empirically. For example, Support Vector Machines have two parameters: $c$ and $\gamma$, and the appropriate values for each depend on problem. There are many different ways to perform hyperparameter optimization . The simplest is grid search: pick a (regularly-spaced) grid of possible values for each parameter, then test them all. It is often possible to do lot better than a grid search, however, and some of these approaches do use genetic algorithms or related concepts (e.g., gradient descent or simulated annealing ). Some algorithms try to quickly converge on the "best" region of the parameter space, and then spend their time finding the precise location of that optima. Others, like Bayesian Global Optimization , try to produce a reasonable "map" of the entire parameter space. There are whole books, conferences, and journals on the topic of optimization. Take a look around and feel free to come back with more specific question.
