[site]: crossvalidated
[post_id]: 241506
[parent_id]: 
[tags]: 
What is the difference between Markov chains and Markov processes?

What is the difference between Markov chains and Markov processes? I'm reading conflicting information: sometimes the definition is based on whether the state space is discrete or continuous, and sometimes it is based on whether the time is discrete of continuous. Slide 20 of this document : A Markov process is called a Markov chain if the state space is discrete, i.e. is finite or countable space is discrete, i.e., is finite or countable. http://www.win.tue.nl/~iadan/que/h3.pdf : A Markov process is the continuous-time version of a Markov chain. Or one can use Markov chain and Markov process synonymously, precising whether the time parameter is continuous or discrete as well as whether the state space is continuous or discrete. Update 2017-03-04: the same question was asked on https://www.quora.com/Can-I-use-the-words-Markov-process-and-Markov-chain-interchangeably
