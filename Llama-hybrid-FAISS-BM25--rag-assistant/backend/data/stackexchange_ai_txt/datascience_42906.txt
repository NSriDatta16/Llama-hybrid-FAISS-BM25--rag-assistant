[site]: datascience
[post_id]: 42906
[parent_id]: 42857
[tags]: 
As Qusai said in his answer, these are not methods to build models, these are selection mechanisms. There are others as well, like feature engineering (for instance dimensionality reduction can be seen as feature engineering). Starting from your example, and assuming that X is your full data: from sklearn.linear_model import LinearRegression lr = LinearRegression() lr.fit(X, y) y_pred = lr.predict(X_test) This is the all-in. From this, backward elimination is removing one of the columns and keeping the n-1 with the best result. Then forward is starting from 0 and adding 1 feature, so just one column at a time, comparing all potential ones. The bidirectional is to try adding or removing at each step (because non linear models may sometimes be better without one feature, but better with two of them). Each time, compare the score, and more precisely, compare the cross-validated score. In a way, only step 5 is what you ALWAYS do.
