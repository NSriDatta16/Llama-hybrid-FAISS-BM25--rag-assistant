[site]: crossvalidated
[post_id]: 238982
[parent_id]: 238831
[tags]: 
Firstly, even a single hidden layer neural network with as many hidden units as needed, can approximate any function (the so called universal approximation property of neural networks http://deeplearning.cs.cmu.edu/pdfs/Kornick_et_al.pdf ). However, it is more difficult (if not impossible, depending on the complexity of the function) to train such a large network. Deep learning tries to ease (not solve) this by adding more layers, i.e., increasing the capacity to learn more complex functions without increasing the complexity during learning. So, for all practical purposes, let's assume that each neural network has a finite capacity (which is a function of number of parameters in the model). That being said, what you have described above is model averaging with equal weights to each model, called as Committee methods. A more useful way of doing model averaging is weighted averaging using BIC Elements of Statistical Learning , Sec. 8.8. It can shown (same reference) the full regression has smaller error than any single model, so combining models never makes things worse, at the population level. Note that this model averaging can be on any regression, need not be neural networks.
