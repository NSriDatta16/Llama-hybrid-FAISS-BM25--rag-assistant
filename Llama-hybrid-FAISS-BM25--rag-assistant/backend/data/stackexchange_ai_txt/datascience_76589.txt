[site]: datascience
[post_id]: 76589
[parent_id]: 32644
[tags]: 
The memory usage of the Random Forest depends on the size of a single tree and number of trees. To control the memory size of RF you can: limit the number of trees (the dependency is almost linear), limit the size of a single tree The default hyper-parameters for Random Forest from scikit-learn are set to build a full tree, which can be very deep in the case of complex datasets. I was running the experiment where I train RF on Adult Income dataset (~ 3.8MB of data; 32,561 rows and 15 cols). The full tree on this dataset has a depth of about 42 (on average, depends on a bagged sample). The size of the single tree saved to hard drive is about 0.5 MB. And saving the whole Random Forest is about 50 MB. When I limit the max_depth of the single tree to 6 then the size of single tree saved to disk was ~ 0.01 MB and whole Random Forest saved to disk take ~ 0.75 MB, which was 66 times less than RF will full trees! The interesting part was that I decrease the memory consumption while the performance was up by 4% :) The size of single tree in the forest can be also controlled with other parameters: min_samples_split , min_samples_leaf , min_weight_fraction_leaf , max_features , max_leaf_nodes . To me, ' max_depth is the most intuitive.
