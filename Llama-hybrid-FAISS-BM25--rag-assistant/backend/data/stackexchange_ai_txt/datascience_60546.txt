[site]: datascience
[post_id]: 60546
[parent_id]: 
[tags]: 
What is weighted cross entropy loss?

I'm reading Cliche, Mathieu (2017) . In his paper, he describes using cross entropy loss, weighted by the inverse frequency of the true classes to counteract an imbalanced dataset (this is stated on page 5, first paragraph). Mathematically, what does this mean? I know that cross entropy loss is defined as: $$ -\sum_iy_i\log(\hat{y}_i) $$ where $i$ is a particular node in the output layer, $y$ is the true distribution and $\hat{y}$ is our distribution in the output layer. How is this weighted by the inverse class frequency?
