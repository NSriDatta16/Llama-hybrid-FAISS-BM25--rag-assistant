[site]: crossvalidated
[post_id]: 635357
[parent_id]: 635295
[tags]: 
I don't follow your attempts so far, but reproducing the standard errors is not that complicated. They are the square roots on the diagonal of the coefficient's covariance matrix $(\textbf{X}^\textbf{T}\textbf{VX})^{-1}$ . $\textbf{X}$ is the design matrix and $\textbf{V}$ a square matrix with on its diagonal $\hat\pi_i(1-\hat\pi_i)$ , the variance of the predicted response for observation $i$ . Let's run through the calculation: ## Convert your data to long format, one observation per row long > (Intercept) storage > 0.2632444 0.1741129 These numbers match the summary exactly. You can easily turn this standard error into a Wald confidence interval for any given $\alpha$ : alpha (Intercept) 0.7592902 0.4532458 1.271985 > storage 1.4895944 1.0589193 2.095430 There are two things to keep in mind: An actual model implementation will almost never use this particular calculation. GLMs are usually fit iteratively, and during iteration certain implementations keep track of (partial second) derivatives to determine convergence. This so-called Hessian matrix is the inverse of the variance-covariance. I believe R's glm uses a QR decomposition though. You can retrieve that matrix as vcov(fit) , the square roots of its diagonals are the standard errors of the summary object (they are the same to high numerical precision but not calculated as above). The confidence intervals from questionr are profiled from the likelihood, not based on standard error estimates. This is also what happens by default if you call the stats::confint method on a GLM object for which questionr::odds.ratio is just a wrapper, the function doing the actual work is MASS:::profile.glm . See e.g. here for some more discussion on the difference between these approaches - profile likelihood is generally considered more reliable but will require numerical optimization.
