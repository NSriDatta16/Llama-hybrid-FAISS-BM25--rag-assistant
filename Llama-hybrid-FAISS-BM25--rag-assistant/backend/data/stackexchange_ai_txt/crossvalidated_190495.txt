[site]: crossvalidated
[post_id]: 190495
[parent_id]: 135958
[tags]: 
Short answer : Use the best model settings selected from cross validation to train a final model on your entire training set. Long answer : Cross validation is used for two purposes: Model selection Model evaluation Model selection is when you are comparing competing models, this could be in the form of different architectures, i.e. SVM vs kNN vs random forest, or one algorithm with different hyper-parameters, i.e. SVM with linear kernel function, but 5 different values of cost. You can even use it to compare multiple architectures with multiple hyper-parameter values, i.e. linear SVM with 5 values of c vs kNN with 3 values of k. Typically you will evaluate each competing model under cross validation, and then select the model setup with the highest cross validation score (the average of its scores on each of the k folds) to be your final model. Say I was comparing the 5 SVMs and the 3 kNN models mentioned in the last paragraph. I discover that an SVM with c=0.01 had the highest CV score overall, this setup will form my final model. However, as you say, I have k implementations of this model as it's been formed on k folds. To obtain a single final model I fit a model using these settings on all the training data. Model evaluation comes after model selection, and is used to provide an unbiased estimate of how well your final model will perform on unseen data. You may think that the cross-validation score calculated in the model selection phase does this, but it will be biased as you have selected these settings owing to their strong CV score. You'll probably need to use nested cross-validation for this, see the excellent answer by cbeleites here for a better explanation than I can provide.
