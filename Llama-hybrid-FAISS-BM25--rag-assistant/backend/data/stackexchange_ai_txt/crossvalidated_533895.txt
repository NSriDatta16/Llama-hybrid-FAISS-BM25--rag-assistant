[site]: crossvalidated
[post_id]: 533895
[parent_id]: 533866
[tags]: 
We do often seek the unbiased estimator with the smallest variability. But sometimes convenience (or habit) leads to use of estimators that are not "optimal" according to some criterion. In the case of estimating the mean $\mu$ of a normal population where $\sigma$ is known, sample average $A= \bar X$ is used instead of the sample median $H.$ Both are unbiased: $E(A) = E(H) = \mu.$ But $Var(A) so $A$ is more widely used. Suppose we have a sample of size $n=9$ from $\mathsf{Norm}(\mu=10,\sigma=1).$ An easy formula gives $Var(A) = 1/n = 1/9.$ There is no such easy formula for $Var(H),$ but one can show that $Var(H) > Var(A).$ One way to get a rough estimate is by simulation: set.seed(2021) h = replicate(10^5, median(rnorm(9,10,1))) mean(h); var(h) [1] 10.00027 # aprx E(H) = 10 [1] 0.1668142 # aprx Var(H) > 1/9 set.seed(2021) a = replicate(10^5, mean(rnorm(9,10,1))) mean(a); var(a) [1] 10.0009 # aprx E(A) = 10 [1] 0.1118979 # aprx Var(A) = 1/9 By contrast, the sample variance $S^2 = V_{n-1} = \frac{1}{n-1}\sum_{i=1}^n (X_i - \bar X)^2$ is unbiased with $E(S^2) = \sigma^2.$ However, even though the estimate $V_n = \frac{1}{n}\sum_{i=1}^n (X_i - \bar X)^2$ is biased, some still argue that it is better because its mean squared error $MSE(V_n) = E[(V_n-\sigma^2)^2]$ is smaller than for $V_{n-1}.$ set.seed(2021) v.8 = replicate(10^5, var(rnorm(9,10,1))) mean(v.8); var(v.8); mean((v.8-1)^2) [1] 1.001419 # aprx E(V.8) = 1 [1] 0.2528645 # aprx Var(V.8) [1] 0.252864 # aprx MSE(V.8) = Var(V.8) set.seed(2021) v.9 = replicate(10^5, (8/9)*var(rnorm(9,10,1))) mean(v.9); var(v.9); mean((v.9-1)^2) [1] 0.8901506 # aprx E(V.9) Perhaps more remarkable is that the estimator $V_{n+1} = \frac{1}{n+1}\sum_{i=1}^n (X_i-\bar X)^2.$ has even smaller MSE. set.seed(2021) v.10 = replicate(10^5, (8/10)*var(rnorm(9,10,1))) mean(v.10); var(v.10); mean((v.10-1)^2) [1] 0.8011355 # more serious downward bias [1] 0.1618333 [1] 0.2013788 # aprx MSE(V.10) In practice, $V_{n+1}$ is not much used. Some statisticians feel MSE puts too much emphasis on far deviations. And many would object to such large underestimates of $\sigma^2.$ The basic lesson is that there are frequently trade-offs to be made in choosing an 'ideal' estimator, and it is not always clear whether unbiasedness or small MSE is more important.
