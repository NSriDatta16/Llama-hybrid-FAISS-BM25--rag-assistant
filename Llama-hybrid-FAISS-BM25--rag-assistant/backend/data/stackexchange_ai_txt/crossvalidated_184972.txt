[site]: crossvalidated
[post_id]: 184972
[parent_id]: 182734
[tags]: 
Good answer so far, though there are a couple of things nobody around here mentioned, here's my 0.02$ I'll just answer in the form of a story, should make things more fun and clear. No tldr here. In the process you should be able to understand what the difference is. There are multiple reasons why DNNs sparked when they did (stars had to align, like all things similar, it's just the matter of right place, right time etc). One reason is the availability of data, lots of data (labeled data). If you want to be able to generalize and learn something like 'generic priors' or 'universal priors' (aka the basic building blocks that can be re-used between tasks / applications) then you need lots of data. And wild data, might I add, not sterile data-sets carefully recorded in the lab with controlled lighting and all. Mechanical Turk made that (labeling) possible. Second, the possibility to train larger networks faster using GPUs made experimentation faster. ReLU units made things computationally faster as well and provided their regularization since you needed to use more units in one layer to be able to compress the same information since layers now were more sparse, so it also went nice with dropout. Also, they helped with an important problem that happens when you stack multiple layers. More about that later. Various multiple tricks that improved performance. Like using mini-batches (which is in fact detrimental for final error) or convolutions (which actually don't capture as much variance as local receptive fields) but are computationally faster. In the meantime people were debating if they liked em more skinny or more chubby, smaller or taller, with or without freckles, etc. Optimization was like does it fizz or does it bang so research was moving towards more complex methods of training like conjugate gradient and newtons method, finally they all realized there's no free lunch. Networks were burping. What slowed things down was the vanishing gradient problem. People went like: whoa, that's far out, man! In a nutshell it means that it was hard to adjust the error on layers closer to the inputs. As you add more layers on the cake, gets too wobbly. You couldn't back-propagate meaningful error back to the first layers. The more layers, the worse it got. Bummer. Some people figured out that using the cross-entropy as a loss function (well, again, classification and image recognition) provides some sort of regularization and helps against the network getting saturated and in turn the gradient wasn't able to hide that well. What also made things possible was the per-layer pre-training using unsupervised methods. Basically, you take a bunch of auto-encoders and learn increasingly less abstract representations as you increase the compression ratio. The weights from these networks were used to initialize the supervised version. This solved the vanishing gradient problem in another way: you're already starting supervised training from a much better start position. So all the other networks got up and started to revolt. But the networks needed supervision anyway, otherwise it was impossible to keep the big data still. Now, for the last part that finally sort of leads to your answer which is too complex to put in a nutshell: why more layers and not just one. Because we can! and because context and invariant feature descriptors. and pools. Here's an example: you have a data set of images, how are you going to train a plan NN using that data? Well, naively, you take let's say each row and you concatenate it into one long vector and that's your input. What do you learn? Well, some fuzzy nonsense functions that might not look like anything, because of the many many types of variances that the objects in the image contain and you are not able to distinguish between relevant and irrelevant things. And at some point the network needs to forget to be able to re-learn new stuff. So there's the capacity issue. This is more non-linear dynamics, but the intuition is that you need to increase the number of neurons to be able to include more information in your network. So the point is that if you just input the image as one piece, adding extra layers does not do too much for you since you're not able to learn abstractions , which is very important. Doing things holistically thus does not work that well, unless you're doing simpler things with the network like focusing on a specific type of object, so you limit yourself to one class and you pick on some global properties as a classification goal. So what's there to do? Look at the edge of your screen and try to read this text. Problem? As stupid as it sounds, you need to look at what you're reading. Otherwise it's too fuzzy / there's not enough resolution / granularity. Let's call the focus area the receptive field. Networks need to be able to focus too. Basically instead of using the whole image as input, you move a sliding window along the image and then you use that as input to the network (a bit less stochastic than what humans do). Now you also have a chance to capture correlations between pixels and hence objects and you also can distinguish between sleepy cat sitting on a sofa and an upside-down cat bungee jumping. Neat, faith in humanity restored. The network can learn local abstractions in an image on multiple levels. The network learns filters, initially simple ones and then builds up on those to learn more complex filters. So, to sum things up: receptive fields / convolutions, unsupervised initialization, rectified linear units, dropout or other regularization methods. If you're very serious about this I recommend you take a look at Schmidhuber's Deep Learning in Neural Networks: An Overview here's the url for the preprint http://arxiv.org/abs/1404.7828 And remember: big learning, deep data. Word.
