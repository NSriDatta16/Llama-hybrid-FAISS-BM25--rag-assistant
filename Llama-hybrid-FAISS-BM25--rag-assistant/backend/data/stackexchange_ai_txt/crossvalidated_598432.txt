[site]: crossvalidated
[post_id]: 598432
[parent_id]: 598430
[tags]: 
Ordinary least squares regression is a special case of least squares regression. With least squares regression we try to find a fit $\hat{y}_i({\bf{x}}_i,\boldsymbol{\beta})$ to datapoints $y_i$ by minimising the sum of (weighted) squared residuals. $$\text{given data $\bf{x}_i$ and $y_i$, and weights $w_i$ find $\boldsymbol{\beta}$ that minimises:} \quad L = \sum_{i = 1}^n w_i [y_i-\hat{y}_i({\bf{x}}_i,\boldsymbol{\beta})]^2$$ OLS is the special case when the weights are equal $w_i = 1$ and the model is a linear combination $$\hat{y}_i({\bf{x}}_i,\boldsymbol{\beta}) = \beta_1 f_1({\bf x}) + \beta_2 f_2({\bf x}) + \dots +\beta_p f_p({\bf x}). $$ OLS is by definition using a linear model. But not all methods that use linear models are OLS. For instance think of GLM, quantile regression, lasso/ridge or Bayesian modelling, which can use a linear model but with a different cost function.
