[site]: crossvalidated
[post_id]: 25127
[parent_id]: 562
[tags]: 
Here's a slightly out of left-field answer, that only touches the "best-practices around combining multiple models" part of your question. This is basically exactly my honours thesis, except that I'm dealing with complex, highly non-linear models that exhibit chaos and noise - climate models. This isn't likely to be broadly applicable to many fields, but might be useful in ecology or econometrics. Until fairly recently in the climate modelling community, models were largely just smashed together in an unweighted average (usually after bias correction involving removing the model mean for part or all of the sample period). This is basically what the IPCC did for the 4th assessment report (4AR), and previous reports. This is more or less an example of the " truth plus error " school of ensemble combination, where it is tacitly or explicitly assumed that observational series (eg. global temperature, local precipitation, etc) is true, and that if you take enough samples (eg. model runs), the noise in the model runs will cancel (see (1)). More recently, methods for combining models based on performance weighting have been used. Because climate models are so noisy, and have so many variables and parameters, the only ways of assessing the performance (that I know of) are by taking the covariance, or by taking the MSE between the model output and the observed time series. Models can then be combined by weighting the mean based on that measure. There's a good overview of this in (2). One assumption behind this method of combining simulations is assumption that the models are all reasonably independent - if some were highly dependent, they would bias the mean. This assumption was reasonably fair for the dataset used for 4AR ( CMIP3 , as this data set was made up of few model runs from many modelling groups (on the other hand, code is shared in the modelling community, so there may still be some interdependence. For an interesting look at this, see (3)). The dataset for the next assessment report, CMIP5 , does not have this somewhat fortuitous attribute - some modelling teams will be submitting a few runs, while some will be submitting hundreds. Ensembles coming from different teams may be produced by initial condition peturbation, or by changes to the model physics and parametrisation. Also, this super ensemble isn't sampled in any systematic way - it's just who ever brings data is accepted (within reason). This is known in the field as an " ensemble of opportunity ". There's a fair chance that using an unweighted mean on such an ensemble is going to git you some major bias toward the models with more runs (since even though there are hundreds of runs, there are likely a much smaller number of truly independent runs). My supervisor has a paper in review at the moment describing a process of model combination involving performance AND independence weighting . There is a conference paper abstract available (4), I'll post the link to the paper when it's published (slow process, don't hold your breath). Basically, this paper describes a process that involves taking the covariance of model errors (model-obs), and weighting down models that have high covariance with all other models, (ie. models with highly dependent errors). The model error variance is computed as well, and used as the performance-weighting component. It's also worth noting that climate modelling is obviously hugely impacted by the vagaries of numerical modelling in general. There's a thing called a "laugh test" - if you end up with a model run that implies that global mean temperatures will be +20°C by 2050, you just throw it out, because it's clearly not physically relevant. Obviously this kind of test is fairly subjective. I haven't required it yet, but I expect to in the near future. That's my understanding of the state model combination in my field at the moment. Obviously I'm still learning, so if I hit on anything special, I'll come back and update this answer. (1) Tebaldi, C. & Knutti, R., 2007. The use of the multi-model ensemble in probabilistic climate projections. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 365(1857), pp.2053–2075. (2) Knutti, R. et al., 2010. IPCC Expert Meeting on Assessing and Combining Multi Model Climate Projections. (3) Masson, D. & Knutti, R., 2011. Climate model genealogy. Geophys. Res. Lett, 38(8), p.L08703. (4) Abramowitz, G. & Bishop, C., 2010. Defining and weighting for model dependence in ensemble prediction. In AGU Fall Meeting Abstracts. p. 07.
