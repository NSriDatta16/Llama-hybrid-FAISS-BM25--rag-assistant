[site]: crossvalidated
[post_id]: 486680
[parent_id]: 484982
[tags]: 
The answer depends on whether you are restricting yourself to the class of linear models , which I will define as something with the form: \begin{align} y_i &\sim \mu_i \\ g(\mu_i) &= X_i\beta. \end{align} Further, let's denote the sample size by $n$ and the number of predictors/variables by $p$ . Case 1: Linear model If you have a large sample, then simple, un-regularized regression will converge to the true values of $\beta$ if $p$ remains small (say 40). This naturally begs the question: what counts as a large sample? Well, it depends. If there's no severe collinearity and all the variables have decent representation (for example, we don't have binary variables with only one 1 and all other 0), then a few thousands would be considered large. However, when you do have samples of this size, then typically statisticians would consider modeling possible non-linearity in the data. For example, one could include interaction terms or polynomial terms, which could increase your number of variables massively if a large number of these are considered. One could then use LASSO or better still, Elastic Net, to regularize the model, since LASSO is simply a special case of Elastic Net. Note that neither the LASSO nor the Elastic Net (EN) has the oracle property, which means there's no guarantee that the estimated $\beta$ converges to their true values with infinite sample size (although adaptive LASSO does). If interpretation is important, as opposed to prediction, then this may put some off using these techniques. Moreover, it may be possible that some interaction effects are retained while the main effects are excluded, which can further hamper interpretation, although one can impose constraints to prevent that. However, in case where the sample size is not large or when you want to consider a large number of possible non-linearities (i.e. you have large $p$ ), then the lack of the oracle property is arguably irrelevant, and I would argue that the EN is a reasonable choice. By "reasonable" I mean a reasonable choice over alternatives such as best-subset/stepwise regression, which are simply coarser forms of regularization. On the other hand, there are an infinite number of ways one could regularize a linear model. There is simply no one method which is the "best" in all cases. Case 2: Non-linear model Because of possible non-linearities, one could consider non-linear approaches such as SVM/SVR or random forest. One can use approaches such as permutation or dropping the variables to investigate the importance of the variables concerned. See here for some intuition. Overall Note that whether in the linear or the non-linear model case, whether a variable is important in the prediction of the outcome depends critically on the target population. These methods all suppose that the target population is the same as the source population, i.e. the population from which you derived the sample. A variable that is unimportant in the sample can turn out to be hugely important in the target. This kind of information will require domain knowledge. It also implies that ranking variable importance in terms of some derived statistics will always have some serious limitations.
