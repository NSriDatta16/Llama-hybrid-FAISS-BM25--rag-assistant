[site]: crossvalidated
[post_id]: 362888
[parent_id]: 
[tags]: 
CNN and kernel sizes: is upsampling useful?

I am playing with Deep Recurrent Q-Network in Reinforcement learning. The architecture I am currently using is similar to the one presented in "Human-level control through deep reinforcement learning" (Volodymyr Mnih, Koray Kavukcuoglu, David Silver et al.) Nature volume 518, pages 529â€“533 (26 February 2015): The input to the neural network consists of an 84x84x3 image produced by the preprocessing map. The first hidden layer convolves 32 filters of 8x8 with stride 4 with the input image and applies a rectifier nonlinearity. The second hidden layer convolves 64 filters of 4x4 with stride 2, again followed by a rectifier nonlinearity. This is followed by a third convolutional layer that convolves 64 filters of 3x3 with stride 1 and then a fourth conv layer with 512 filters of 7x7 with stride 1. This is the convolutive part of the network: it works! However I thought that this is largely oversized with respect to my problem, a simple grid game where the state is represented by a RGB 11x11 matrix. In fact, the preprocessing function actually upsample the matrix in order to match the input shape of the model. What's the point in resizing the grid from 11x11 to 84x84 (and thus having to manage a way larger set of weights)? However any manual attempts to define a simpler architecture that I tried is a failure, there's no learning at all! For example, I tried the following convolutive module (input shape: 11x11x3): 32 filters, 4x4, stride 2 64 filters, 2x2, stride 1 512 filters, 3x3, stride 1 I've read many similar questions, but wasn't able to get a hint about this kind of problem (I'm a beginner, so no hyperparameters experience!). Could you offer any insights?
