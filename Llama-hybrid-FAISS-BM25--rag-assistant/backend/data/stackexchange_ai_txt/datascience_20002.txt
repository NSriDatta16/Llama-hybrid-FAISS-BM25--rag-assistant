[site]: datascience
[post_id]: 20002
[parent_id]: 19869
[tags]: 
TL;DR : The first matrix represents the input vector in one hot format The second matrix represents the synaptic weights from the input layer neurons to the hidden layer neurons Longer Version : "what the feature matrix exactly is" It seems you have not understood the representation correctly. That matrix is not a feature matrix but a weight matrix for the neural network. Consider the image given below. Especially notice the left top corner where the Input Layer matrix is multiplied with the Weight matrix. Now look at the top right. This matrix multiplication InputLayer dot-producted with Weights Transpose is just a handy way to represent the neural network at the top right. So, to answer your question, the equation you have posted is just the mathematical representation for the neural network which is used in the Word2Vec algorithm. The first part, [0 0 0 1 0 ... 0] represents the input word as a one hot vector and the other matrix represents the weight for the connection of each of the input layer neurons to the hidden layer neurons. As Word2Vec trains, it backpropogates into these weights and changes them to give better representations of words as vectors. Once training is complete, you use only this weight matrix, take [0 0 1 0 0 ... 0] for say 'dog' and multiply it with the improved weight matrix to get the vector representation of 'dog' in a dimension = no of hidden layer neurons. In the diagram you have presented, the number of hidden layer neurons is 3 So the right hand side is basically the word vector. Image Credits : http://www.datasciencecentral.com/profiles/blogs/matrix-multiplication-in-neural-networks
