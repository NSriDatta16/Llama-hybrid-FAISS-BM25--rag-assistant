[site]: datascience
[post_id]: 96714
[parent_id]: 96557
[tags]: 
If you want one value per word rather than a vector, you could run the CBOW algorithm (or skipgram, the other w2v model) with an embedding side of 1. But it is unlikely to work well since the semantic meaning of a word can't easily be reduced a single value. That said, you can feed vector inputs into logistic regression though, e.g. single word embeddings or several concatenated together.
