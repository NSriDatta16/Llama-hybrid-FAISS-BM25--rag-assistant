[site]: crossvalidated
[post_id]: 78849
[parent_id]: 
[tags]: 
Measure for Separability

I have a (binary) classification problem where after merging single training data points (that can be tracked back to the same source) into aggregates, test accuracy (on single data points again) increases significantly. By merging I mean adding and averaging feature vectors. A SVM classifier/linear kernel is used. (The training data is noisy since it is semi-automatically generated). So there is the weird situation that making training and test data more 'dissimilar' (aggregates vs. single data points) increases performance. (Moreover, for plain linear regression, adding together feature vectors shouldn't make a difference, I guess). I'm trying to find out what might be the reason for that. One hypothesis is that for large margin classifiers like SVM's separability is crucial, and that by aggregating, there is less overlap between classes caused by noise. It would be nice to have a measure indicating the amount of 'overlap' or 'separability'. What are standard measures here? I plan to do some plot of the first two PCA dimensions and see how things look, but some 'credible number' would be good in addition.
