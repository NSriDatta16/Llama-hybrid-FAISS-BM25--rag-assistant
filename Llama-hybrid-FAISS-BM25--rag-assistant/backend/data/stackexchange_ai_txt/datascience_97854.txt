[site]: datascience
[post_id]: 97854
[parent_id]: 97811
[tags]: 
If you are using the embeddings in a downstream task, better performance is often achieved by averaging or summing them (the factor of 2 makes no difference). Concatenating them is fine also as it preserves that same information, i.e. the downstream model can effectively average them (of course it preserves more info, but it is unknown if that is beneficial). Averaging/summing is obviously more memory efficient than concatenating. An intuition for why averaging is useful is in this paper , which relates mainly to word2vec but both models learn similar statistics (which is ultimately what these embedding capture). This paper examines the effect of adding the embeddings and offers a different explanation.
