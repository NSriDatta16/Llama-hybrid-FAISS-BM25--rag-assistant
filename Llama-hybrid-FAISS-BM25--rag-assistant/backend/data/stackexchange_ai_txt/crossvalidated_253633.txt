[site]: crossvalidated
[post_id]: 253633
[parent_id]: 253532
[tags]: 
Weight representation: In a standard neural network, each connection has a scalar weight value. In a Bayesian version of a neural network, each connection has a distribution of weight values. More generally, for any node or neuron in a standard neural network, there is a "fan" of incoming connections, each of which has a single scalar weight value. In a Bayesian neural network, there is instead a joint distribution of weights on the fan-in connections. Learning: In a standard neural network, learning changes the vector of weight values. There are various learning algorithms, usually motivated by increasing consistency or decreasing a cost function (e.g., error reduction in backprop). In a Bayesian neural network, learning changes the distribution of weight values. Learning in a Bayesian network works by applying Bayes rule: The previous distribution across the weights is the prior, the current activation of the nodes is the data, and then learning adjusts the weight distribution according to Bayes rule. Perhaps the simplest version of a Bayesian neural network is a "Kalman filter." It's just a linear node that adjusts its weight distribution with Bayes rule. Here's an article that gives an fairly introductory description, though it's couched in the language of associative learning in psychology: Kruschke, J. K. (2008). Bayesian approaches to associative learning: From passive to active learning. Learning & Behavior, 36(3), 210-226.
