[site]: stackoverflow
[post_id]: 2569372
[parent_id]: 2567483
[tags]: 
If you are interested in a different way of doing this, you could do the following. This way is theoretically more sound, however not as straightforward. By mentioning mean and std, it seems as if you refer to data that you assume to be distributed in some way. E.g., the data you observer is Gaussian distributed. You can then use the Symmetrised Kullback-Leibler_divergence as a distance measure between those distributions. You can then use something like k-nearest neighbour to classify. For two probability densities p and q, you have KL(p, q) = 0 only if p and q are the same. However, KL is not symmetric - so in order to have a proper distance measure, you can use distance(p1, p2) = KL(p1, p2) + KL(p1, p2) For Gaussians, KL(p1, p2) = { (μ1 - μ2)^2 + σ1^2 - σ2^2 } / (2.σ2^2) + ln(σ2/σ1). (I stole that from here , where you can also find a deviation :) Long story short: Given a training set D of (mean, std, class) tuples and a new p = (mean, std) pair, find that q in D for which distance(d, p) is minimal and return that class. To me that feels better as the SVM approach with several kernels, since the way of classifying is not so arbitrary.
