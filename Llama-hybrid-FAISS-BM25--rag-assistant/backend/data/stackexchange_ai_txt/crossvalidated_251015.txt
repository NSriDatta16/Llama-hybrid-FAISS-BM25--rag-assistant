[site]: crossvalidated
[post_id]: 251015
[parent_id]: 249881
[tags]: 
My experience with SVM does not include 1M datasets. I work usually up to 50K datasets. So caveat emptor. 1) there is no way to decouple gamma from C. I answered your other question on that Are the kernel parameters and the regularization parameter correlated in SVM? 2) There is no epsilon for classification. This is a hyperparameter for regression only 3) on datasets up to 50K I found that PSO work better than simplex, and SA. I hav not tried CMA or GA. There is no downhill or hill climbing unless you use some approximations to the leave one out error there is no closed expression for the gradient. 4) You don't need to use a low variance CV - I found that 2-fold is good enough. If you cannot afford the computational time of the 2-fold, than you could use a subsample, but in my experience that yields worse results. 5) IN MY EXPERIENCE, the error surface for the hyperparametrs is somewhat smooth - not convex, but smooth - there are no deep and narrow regions of low error that are worth spending a lot of computational time searching for them. Provided you are not selecting hyperparameters in a bad region of the error surface, there is no point in probing the surface too much. On that note, I would suggest a 5x5 grid search followed by another 5x5 grid around the minimum of the first grid - this should be enough. You end up probing the error surface 50 times, and you probably get a result as good as any black box optimization with the same limit on the number of probing. And it is much easily to parallelize.
