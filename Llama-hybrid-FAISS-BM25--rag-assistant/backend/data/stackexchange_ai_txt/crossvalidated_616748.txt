[site]: crossvalidated
[post_id]: 616748
[parent_id]: 616697
[tags]: 
tl;dr A parametric bootstrap (#3 below), or a cheesy parametric bootstrap (#2 below), are probably the most straightforward approaches. The components of uncertainty are (1) the sampling variance of the fixed effect components $\mathbf V$ (for a fixed-effect model matrix $\mathbf X$ , the corresponding variances of the predictions are $\textrm{Diag}(\mathbf X \mathbf V \mathbf X^\top))$ ; (2) the variances of the random effects (for previously unmeasured levels, if $\boldsymbol \Sigma$ is the covariance matrix of the random effects, then $\textrm{Diag}(\mathbf Z \boldsymbol \Sigma \mathbf Z^\top)$ are the corresponding variances of the random-effects components of the prediction. Combining these components is easy, because they're Gaussian. Notes so far: you can construct $\mathbf X$ with model.matrix() ; you could use lFormula() and extract the reTrms$Zt component to get $\mathbf Z^\top$ (or, see the machinery in vignette("lmer", package = "lme4") and use model.matrix() + Matrix::fac2sparse + Matrix::KhatriRao to make your own). $\mathbf V$ comes from vcov() ; $\boldsymbol \Sigma$ is crossprod(getME(., "Lambda")) . Once you go back from the link to the response scale (exponentiating the random effects, which are on the log scale) we end up with a log-Normal with the specified mean and variance. The hard part is combining the Gamma distribution. What we have now is a generalized logNormal-Gamma distribution (ugh). If you just wanted the variance (and were prepared, e.g., to construct confidence intervals based on a Gaussian distribution), you could get the variance of the generalized distribution by combining the Gamma and log-Normal variance (the formula for the variance of a generalized distribution in terms of the mean and variance of its components can be derived from generating functions, and is in Pielou's Mathematical Ecology book, which I don't have with me right now ... it may appear elsewhere on the web but I couldn't find it in a quick search). The next better approach, which would be pretty quick: draw MVN samples (using MASS::mvrnorm or one of the other alternatives in the R ecosystem) from the distribution with the combined FE sampling variance + RE variance, exponentiate them, and draw Gamma samples based on those mean values. This is basically the method described in section 7.5.3 of Bolker (2008), which Lande et al. 2003 call "population prediction intervals". This is pretty good, but ignores uncertainty in the RE covariance matrix. You might be able to implement this with simulate() â€” "True" parametric bootstrapping would simulate, re-estimate parameters, and then do the computation above including the RE variance but not the FE sampling variance (that part would be handled by the sampling/re-fitting). Or you could go all the way Bayesian ... These four choices are essentially ordered by computational effort (the actual Bayesian computation might not be slower than the parametric bootstrap, but might take more effort to set up). Second thoughts after starting an implementation. Not sure that we actually want $\boldsymbol \Sigma$ from the model, which is the covariance matrix of the random effects in the fitted model. Instead, we want to create a new standalone model block and push the relevant elements of the theta vector into it ...
