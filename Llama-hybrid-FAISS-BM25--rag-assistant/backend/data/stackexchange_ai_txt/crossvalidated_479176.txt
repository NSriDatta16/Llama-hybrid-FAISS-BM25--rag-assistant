[site]: crossvalidated
[post_id]: 479176
[parent_id]: 
[tags]: 
How does kernel density estimation work?

There is an inbuilt "adaptive Kernel density" anomaly detection routine provided in a data streaming library ( https://docs.microsoft.com/en-us/stream-analytics-query/anomalydetection-spikeanddip-azure-stream-analytics ). It returns a p_value for each data point given the history and is designed to detect sudden jumps. I've been trying to find online resources into how it works but can't find good ones. The best resource I've found so far is this paper: https://cis.temple.edu/~latecki/Papers/mldm07.pdf and it suggests some kind of distribution for the observed value is formed based on history and "convolution" with a kernel function which is a multi-dimensional probability density function (Gaussian is most common). Per equation (3) in the paper, it seems very much like this is a mixture of Gaussians. My question is, how this compares vis-Ã -vis just doing a one sample t-test for the current observation versus the the history? Seems like the one-sample t-test would be appropriate for gaussian white noise. Does this kernel method improve on that for other kinds of time series? How so? It certainly seems less efficient since the documentation says that its linear in the number of history points, so surely the added complexity must be providing some advantage. And is it possible to quantify the advantage (given the generative process for the time series and in terms of statistical power)?
