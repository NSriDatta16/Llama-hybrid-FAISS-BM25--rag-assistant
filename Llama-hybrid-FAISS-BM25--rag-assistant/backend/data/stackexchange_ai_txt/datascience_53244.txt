[site]: datascience
[post_id]: 53244
[parent_id]: 
[tags]: 
Why is my Siamese network always predicting 1?

There is no change to loss and the accuracy stays the same. left_input = Input((3,224,224)) right_input = Input((3,224,224)) encoded_l = encoder2(left_input) encoded_r = encoder2(right_input) # Getting the L1 Distance between the 2 encodings L1_layer = L.Lambda(lambda tensor:K.abs(tensor[0] - tensor[1])) L1_distance = L1_layer([encoded_l, encoded_r]) prediction = L.Dense(1,activation='sigmoid')(L1_distance) siamese_net = Model(inputs=[left_input,right_input],outputs=prediction) optimizer = Adam(0.0005, decay=2.5e-4) siamese_net.compile(loss="binary_crossentropy",optimizer=optimizer,metrics=['accuracy']) siamese_net.fit([Xl_train.reshape((-1,3,224,224)),Xr_train.reshape((-1,3,224,224))], to_categorical(y_train), batch_size=32, epochs=50, verbose=1, validation_data=([Xl_test.reshape((-1,3,224,224)),Xr_test.reshape((-1,3,224,224))],to_categorical(y_test)), shuffle=True) I only have positive pairs set so I had to make the negative pairs set by using np.roll() on one set of members in the pair to produce a new combination. So it isn't predicting 1 because of the imbalance problem(since both positive and negative samples have the same number of pairs). I have tried shuffling the dataset, using softmax with 2 nodes in the final layer. I tried it on SVM with rbf kernel. While the accuracy was pretty bad, it was still at least predicting 0. And here's the code for the encoder- def encoder(): model=Sequential() model.add(Convolution2D(128, (7, 7), activation='relu')) model.add(Dropout(0.2)) model.add(Convolution2D(64, (5, 5), activation='relu')) model.add(Dropout(0.2)) model.add(Convolution2D(64, (5, 5), activation='relu')) model.add(Dropout(0.2)) model.add(Convolution2D(16, (5, 5), activation='relu')) model.add(Dropout(0.2)) model.add(Convolution2D(1, (5, 5), activation='relu')) model.add(Flatten()) model.build((None,3, 224, 224)) #print(model.summary()) return model Layer (type) Output Shape Param # ================================================================= conv2d_1 (Conv2D) (None, 128, 218, 218) 18944 _________________________________________________________________ dropout_1 (Dropout) (None, 128, 218, 218) 0 _________________________________________________________________ conv2d_2 (Conv2D) (None, 64, 214, 214) 204864 _________________________________________________________________ dropout_2 (Dropout) (None, 64, 214, 214) 0 _________________________________________________________________ conv2d_3 (Conv2D) (None, 64, 210, 210) 102464 _________________________________________________________________ dropout_3 (Dropout) (None, 64, 210, 210) 0 _________________________________________________________________ conv2d_4 (Conv2D) (None, 16, 206, 206) 25616 _________________________________________________________________ dropout_4 (Dropout) (None, 16, 206, 206) 0 _________________________________________________________________ conv2d_5 (Conv2D) (None, 1, 202, 202) 401 _________________________________________________________________ flatten_1 (Flatten) (None, 40804) 0 ================================================================= Total params: 352,289 Trainable params: 352,289 Non-trainable params: 0 EDIT 1- I added- kernel_initializer='random_uniform',bias_initializer='zeros' to all the layers so that it would stop predicting everything as 0. The training losses are decreasing now but the validation loss and accuracy aren't being changed. Edit 2- Resolved it. It was an issue with my dataset.
