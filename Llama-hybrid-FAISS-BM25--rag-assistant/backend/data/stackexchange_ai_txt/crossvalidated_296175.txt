[site]: crossvalidated
[post_id]: 296175
[parent_id]: 296172
[tags]: 
You can applying reinforcement learning by simply changing the loss function. If the output of your network is 0.8/0.1/0.1 then treat this as if you are betting \$0.8 on home, \$0.1 on a draw, and \$0.1 on away. If I understand odds correctly then a 50:1 odds means that you get \$51 back if you bet \$1. Therefore, if you bet \$0.1 then the money you get back is $(0.1\times 51 \times \text{[result]})$; where [result] is 0 if this option didn't win and 1 if the option did win. Suppose in your example the odds are 2:1 for home, 3:1 for a draw, and 50:1 for away. And suppose that the result was that "away" won. The profit would be: $\text{profit}=(0.8 \times 3 \times 0) + (0.1 \times 4 \times 0) + (0.1 \times 51 \times 1) - 1$ I would recommend adding a fourth option of betting on nothing, then if your network output was 0.8/0.1/0.08/0.02 for home/draw/away/nothing the profit is: $\text{profit}=(0.8 \times 3 \times 0) + (0.1 \times 4 \times 0) + (0.08 \times 51 \times 1) + (0.02) - 1$ The loss function is just the negative of the profit $\text{Loss}=1- (0.8 \times 3 \times 0) - (0.1 \times 4 \times 0) - (0.08 \times 51 \times 1) - (0.02)$ Take derivatives of this loss function to train your network by gradient descent as your normally would. Edit: To get the partial derivatives of this loss function lets first define it symbolically. We have the odds offered for home, draw, and away; call these $o_1, o_2, o_3$. There is also the odds for the "none" option I mentioned, call this $o_4$. If you bet £1 on "none" you always get back £1, therefore the odds for the "none" option is always $o_4=1$ You must have collected data on the results. Call the home/draw/away results $r_1, r_2, r_3$, the result is $1$ if it happened or $0$ if it didn't. If away wins the game then $r_3=1$ and $r_2=r_1=0$. The result for "none" is always $r_4=1$. Lastly, suppose your model outputs $z_1, z_2, z_3, z_4$, these figures are then put through the softmax function to give the amounts of money gambled $x_1,x_2, x_3, x_4$. Where $x_j = \frac{e^{z_j}}{\sum_{i=1}^4 e^{z_i}}$ The loss is defined as $$L=1-\sum_{i=1}^4 x_i o_i r_i$$ You want to get the partial derivative $\frac{\partial L}{\partial z_j}$. Once you have this the rest of the derivatives with respect to the model parameters works like a normal neural net. Apply the chain rule: $$\frac{\partial L}{\partial z_j} = \frac{\partial L}{\partial x_j} \times \frac{\partial x_j}{\partial z_j}$$ You can see easily that $\frac{\partial L}{\partial x_j}$ = $-o_jr_j$ To get $\frac{\partial x_j}{\partial z_j}$ look at the softmax function. $\frac{\partial x_j}{\partial z_j} = \frac{e^{z_j}((\sum_{k=1}^4 e^{z_k})-e^{z_j})}{(\sum_{k=1}^4 e^{z_k})^2}$ Therefore $\frac{\partial L}{\partial z_j} = -o_jr_j \frac{e^{z_j}((\sum_{k=1}^4 e^{z_k})-e^{z_j})}{(\sum_{k=1}^4 e^{z_k})^2}$ You can proceed to get the derivatives with respect to weights in the model using the normal approach for neural networks. Notice how the derivative and the loss depends on $o_jr_j$ only instead of $o_j$ and $r_j$ separately. Consider storing your data as $o_jr_j$ rather than storing odds and results separately.
