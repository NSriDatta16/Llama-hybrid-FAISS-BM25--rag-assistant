[site]: crossvalidated
[post_id]: 316936
[parent_id]: 
[tags]: 
Linear Regression : Proving least squares model

My question is exactly similar to an earlier question Conditional on Gaussian, need clarification But I wasn't satisfied with the answer. I'll copy the problem statement : I'm reading Andrew Ng's notes on machine learning, and on page 12 of this document , he makes a step in his proof that I'm trying to decipher: Let $x=(1,x_1,x_2,⋯,x_n)^T$ , a vector of variables, and $θ=(θ_0,θ_1,θ_2,⋯,θ_n)^T$ , a vector of linear coefficients of those variables. Let's define $y$ as $$y_{i} = \theta^T x_{i} + \epsilon_i$$ where $\epsilon_i ∼ N(0, \sigma^2)$ , that is $$p(\epsilon_i) = \frac{1}{\sqrt{2\pi\sigma}}\text{exp}\bigg(-\frac{\epsilon_i^2}{2\sigma^2}\bigg)$$ Next line says the following about conditional probability of y given x and coefficients θ, which are treated as deterministic: $$p(y_i|x_i;θ) = \frac{1}{\sqrt{2\pi\sigma}}\text{exp}\bigg(-\frac{(y_i - \theta^T x_i)^2}{2\sigma^2}\bigg)$$ Can someone help me see how we get this conditional distribution? Now I understand that since $\epsilon_i$ is drawn from a gaussian, y will itself be a random variable drawn from a gaussian. But it $y_i$ is a random variable with mean $\theta^T x_i$ , so does it mean we have m (number of training samples) different gaussians and we draw 1 data point from each of m different gaussians and do a maximum liklihood estimation for that single data set for each of m gaussians, because this seems sort of unintuitive. And also this only proves that $y_i$ is a gaussian not $y_i|x_i$ . I dont understand why this has to hold?
