[site]: crossvalidated
[post_id]: 242225
[parent_id]: 
[tags]: 
Why are neural networks easily fooled?

I've read some papers about manually contriving images to "fool" a neural network (see below). Is this because that the networks only model the conditional probability $p(y|x)$? If a network can model the joint probability $p(y,x)$, will such cases still occur? My guess is such artificially generated images are different from the training data, so they are of low probability $p(x)$. Hence $p(y,x)$ should be low even if $p(y|x)$ can be high for such images. Update I've tried some generative models, it turned out not being helpful, so I guess probably this is a consequence of MLE? I mean in the case that KL divergence is used as the loss function, the value of $p_{\theta}(x)$ where $p_{data}(x)$ is small doesn't affect the loss. So for a contrived image that doesn't match $p_{data}$, the value of $p_{\theta}$ can be arbitrary. Update I found a blog by Andrej Karpathy that shows These results are not specific to images, ConvNets, and they are also not a “flaw” in Deep Learning. EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images
