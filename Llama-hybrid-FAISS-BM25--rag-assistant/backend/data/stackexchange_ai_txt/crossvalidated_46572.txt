[site]: crossvalidated
[post_id]: 46572
[parent_id]: 38016
[tags]: 
To further the idea about using all subsets or best subsets tools for finding a "Best" fitting model, The book "How to Lie with Statistics" by Darrell Huff tells a story about Readers Digest publishing a comparison of the chemicals in cigarette smoke. The point of their article was to show that there was no real difference between the different brands, but one brand was lowest in some of the chemicals (but by so little that the difference was meaningless) and that brand started a big advertisement campaign based on being the "lowest" or "best" according to Readers Digest. All subsets or best subsets regressions are similar, the real message from the graph you show is not "here is the Best" but really that there is no one best model. From a statistical view (using adjusted r-squared) the majority of your model are pretty much the same (the few at the bottom are inferior to those above, but the rest are all similar). Your wanting to find a "Best" model from that table is like the cigarette company saying that their product was the best when the purpose was to show that they were all similar. Here is something to try, randomly delete one point from the dataset and rerun the analysis, do you get the same "Best" model? or does it change? repeat a few times deleting a different point each time to see how the "Best" model changes. Are you really comfortable claiming a model is "Best" when that small of a change in the data gives a different "Best"? Also look at how much different the coefficients are between the different models, how do you interpret those changes? It is better to understand the question and the science behind the data and use that information to help decide on a "Best" model. Consider 2 models that are very similar the only difference is that one model includes $x_1$ and the other includes $x_2$ instead. The model with $x_1$ fits slightly better (adj r-squared of 0.49 vs. 0.48) however to measure $x_1$ requires surgery and waiting 2 weeks for lab results while measuring $x_2$ takes 5 minutes and a Sphygmomanometer. Would it really be worth the extra time, expense, and risk to get that extra 0.01 in the adjuster r-squared, or would the better model be the quicker, cheaper, safer model? What makes sense from the science standpoint? In your example above do you really think that increasing spending on the military will improve olympic performance? or is this a case of that variable acting as a surrogate for other spending variables that would have more direct affect? Other things to consider include taking several good models and combining them (Model Averaging), or rather than having each variable be either all in or all out adding some form of penalty (Ridge regression, LASSO, elasticnet,...).
