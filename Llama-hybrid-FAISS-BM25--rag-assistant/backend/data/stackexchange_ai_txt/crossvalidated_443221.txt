[site]: crossvalidated
[post_id]: 443221
[parent_id]: 
[tags]: 
Evaluating model that is in production without access to new, labeled data

I have a deep learning image classification model that is in production. I am trying to get a sense of model drift by calculating distribution metrics (e.g. mean predicted probability per label) over time, since I can't easily or cost-effectively obtain new, labeled data (i.e. images). Recently, we re-trained the model using the same data as the previous versions. The difference was that we started using tensorflow 2 and added a momentum hyper-parameter. When we took a stratified random sample for each of the labels, we found that the new model's prediction distributions significantly differed from those of previous versions. The model was initially deployed to production based on some "quick and dirty" evaluation. In that sense, I do not have the most objective measure of the baseline performance at the time of deployment. Therefore, hypothetically, the new model could be outperforming the previous version, but I have no way of easily confirming that. I don't really care why the model is behaving differently, at least until I get to the point where I know whether the model is better or worse than its predecessors. I would love to hear what others have done to evaluate the performance of their models in such situations. What course of action would you follow once you've identified possible model drift when you do not have access to new, labeled data? I am leaning toward biting the bullet and performing some manual labeling so to have a more robust comparison of the previous version of the model against the new one.
