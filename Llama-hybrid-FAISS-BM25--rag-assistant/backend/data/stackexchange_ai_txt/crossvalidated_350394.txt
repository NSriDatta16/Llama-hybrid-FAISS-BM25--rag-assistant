[site]: crossvalidated
[post_id]: 350394
[parent_id]: 127395
[tags]: 
There might be two reasons for which you would want to reduce the number of features: Predictive Power: Random forest model accuracy does not really get impacted by the multicollinearity much. You can have a look at this . It actually selects random samples of the training data and also subsets of features while running each of the decision trees. So whichever feature gives it more decrease in impurity, it will pick that. That way, be it large number of predictors or correlated predictors the model accuracy should not be affected. Interpretability : If you want to interpret the model output using the features and their impact, in that case you might suffer because of the multicollinearity. If two predictors are correlated and they are important, the tree will choose one of them and you might lose the other one if you have small number of trees. So for that you might wanna reduce features. Methods : I would suggest you to use the inbuilt importance function in randomForest . This is calculating the importance of each feature based on Gini Importance or Mean Decrease in Impurity (MDI). ``` fit This will give something like below, and you can exclude features with lesser importance. You can also check other methods like Permutation Importance or Mean Decrease in Accuracy (MDA) Information Gain / Entropy Gain Ratio All these are really useful when the dependent is categorical. In case your dependent variable is continuous, you can follow the classical approach , which leads to the correlation calculation between each feature and the target.
