[site]: crossvalidated
[post_id]: 411495
[parent_id]: 411459
[tags]: 
The problem seems to be underspecified. As described, there might not even be chance involved and you do not mention any underlying distributions. Furthermore, it is not clear what kind of overall performance criteria are important. So here are some very generic/general ideas. You want to decouple the ranking from the scores, due to outliers. Furthermore the scores for different datasets might have very different characteristics. I think this makes a lot of sense. So instead of using the scores themselves, you might want to use their ranks. I.e. for each dataset $j, j=1,\ldots, m$ you assign to method $i, i=1\ldots,n$ the rank $r_{ij}$ , where $r_{ij}=1$ if the score $f(M_i, D_j)$ is lowest, 2 when it is second best and so on. To obtain an overall ranking, you need to decide what is important to you. For example: Average case performance: then the overall rank of method $i$ is $R_i=\frac{1}{m}\sum_j r_{ij}$ Worst case performance: then the rank is $R_i=\max_j r_{ij}$ . Of course there are obvious and endless variations (such as weighted ranks, squared deviations, ...).
