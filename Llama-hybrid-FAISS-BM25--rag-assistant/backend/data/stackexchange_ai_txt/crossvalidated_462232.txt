[site]: crossvalidated
[post_id]: 462232
[parent_id]: 462120
[tags]: 
There was a paper on using F-measure as a training objective. I tried for training a model for very imbalanced classes and didn't work well (oversampling worked better). The idea is simple: for each class, you just compute a soft precision and soft recall and to the harmonic mean. Your suggestions 1 and 2 sound like a reasonable thing to do. Regarding the bigger / deeper model: This depends on how you are able to fit your training data. If your model is already overfitting, a deeper model probably would not help, otherwise, it might. Attention or other changes in architecture indeed can help, but it very much depends on how your current architecture looks like. You certainly cannot say that attention would be a universal solution for data sparsity.
