[site]: crossvalidated
[post_id]: 450491
[parent_id]: 198810
[tags]: 
Its right that cosine-similarity between frequency vectors cannot be negative as word-counts cannot be negative, but with word-embeddings (such as glove) you can have negative values. A simplified view of Word-embedding construction is as follows: You assign each word to a random vector in R^d. Next run an optimizer that tries to nudge two similar-vectors v1 and v2 close to each other or drive two dissimilar vectors v3 and v4 further apart (as per some distance, say cosine). You run this optimization for enough iterations and at the end, you have word-embeddings with the sole criterion that similar words have closer vectors and dissimilar vectors are farther apart. The end result might leave you with some dimension-values being negative and some pairs having negative cosine similarity -- simply because the optimization process did not care about this criterion. It may have nudged some vectors well into the negative-values. The dimensions of the vectors dont correspond to word-counts, they are just some arbitrary latent concepts that admit values in -inf to +inf.
