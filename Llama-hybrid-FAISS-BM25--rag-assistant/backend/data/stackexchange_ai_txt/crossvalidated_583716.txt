[site]: crossvalidated
[post_id]: 583716
[parent_id]: 583715
[tags]: 
Wikipedia calls it $KL(P||Q)$ "the relative entropy from $Q$ to $P$ ", so this also could be interpreted as $P$ being the "reference distribution". The cross-entropy can be understood as the average Huffman encoding length when the distribution is thought to be $q$ but is actually $p$ , i.e. the "real thing" is $p$ . So, it makes kind of sense to consider $p$ as the distribution that is the "baseline".
