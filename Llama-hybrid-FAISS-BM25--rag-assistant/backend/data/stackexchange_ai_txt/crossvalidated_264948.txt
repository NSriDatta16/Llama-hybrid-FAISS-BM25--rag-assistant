[site]: crossvalidated
[post_id]: 264948
[parent_id]: 264643
[tags]: 
I wasn't able to access the paper you refer to but perhaps a few of the following insights might help answer your question: Both cross entropy and MSE are frequently used for the stopping criterion when training autoencoders (not sure why the authors refer to cross entropy when they use MSE though) When training an auto-encoder, the goal is effectively to reproduce the output y from input x as close as possible, therefore the stopping criterion they use does appear to make sense Depending on the context, it is not always strictly necessary to use a validation set when doing unsupervised training. The risk is of course that overfitting occurs and that the autoencoder doesn't perform well on unseen training data. If the authors don't present any other unseen training data to the autoencoder afterwards, there is no need for a validation set. Another possibility is that if the autoencoder is subsequently used as a part of a larger model afterwards, one might also judge the performance of the autoencoder on the outcome of the entire algorithm. The disadvantage with this approach is that you don't know the impact of the autoencoder on the performance of the entire model for unseen training data.
