[site]: crossvalidated
[post_id]: 355358
[parent_id]: 355329
[tags]: 
Data sparsity is mostly a computational problem. Think of a recommender system that recommends thousands of products to hundreds of thousands of users, if you stored the data about user-product interaction in a matrix, it would be a huge amount of data consisting of lots of zeros (most users are interested just in a selected subset of products). More wisely, you would store such data using a sparse matrix representation (that records only the non-zeros). This would help with a storage, but still you would need an algorithm that can interact with such sparsely represented data and in many cases this is not offered off-the-shelf. Moreover, since most of the values are zeros, for most samples, they bring relatively little information and if you train your model on such data, then you end up with a huge number of parameters that are useless most of the time. Models with huge number of parameters are problematic on their own, because to estimate the parameters you need huge amounts of data and the optimization algorithms that work well in such settings. So in cases where sparse data is common, like language data where the distribution of words is very skewed, we usually use some kind of dimensionality reduction, like embeddings (this is what word2vec does).
