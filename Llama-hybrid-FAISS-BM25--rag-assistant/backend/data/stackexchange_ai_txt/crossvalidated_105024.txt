[site]: crossvalidated
[post_id]: 105024
[parent_id]: 104871
[tags]: 
Increasing the number of dependent variables is a problem for any prediction algorithm. This includes regression models, NNs, SVMs, etc. http://en.wikipedia.org/wiki/Curse_of_dimensionality Huge input vectors are harder to classify the larger they get. NNs are not immune to this. However, NNs are advantageous in that by design they're networks. They lend themselves to distributed environments well. If your problem can be subdivided and executed simultaneously on a large number of compute nodes then an ANN optimized for running in a distributed environment is certainly an attractive option. Parallelization of regression models is an ongoing area of research. http://www.bing.com/search?q=logistic+regression+parallelization&pc=MOZI&form=MOZSBR It's not something I'm deeply familiar with. Apache Mahout apparently allows you to run the same regression model with different parameters simultaneously, but with a very large model (the ubiquitous millions of input vectors all with thousands of independent variables) you're still limited in how you can break each model down and take advantage of modern distributed computing options.
