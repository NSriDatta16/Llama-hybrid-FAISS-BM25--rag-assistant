[site]: crossvalidated
[post_id]: 94254
[parent_id]: 94003
[tags]: 
Although it is possible to select the Gaussian kernel and achieve separability in the feature space, this might not be the best strategy for minimizing the expected loss (i.e. the true risk as opposed to the empirical risk). Consider an example of labeled points in $\mathbb{R}^3$ where negative points lie in the ($\ell_2$) unit ball and positive points lie outside the ball of radius 2. However, suppose there also are a few outliers in the training sample: a few positive points lie in the "negative" region inside the unit ball. Now, if we use a polynomial kernel that includes the $\ell_2$ norm of the data points as a new dimension, then we can almost linearly separate the data in the feature space. There are a few outliers of course, so we will still have some training error using this kernel. However, if the outliers correspond to fundamental noise in our problem, then it might be the case that the Bayes optimal decision rule in fact is the hypothesis that classifies points as positive if there norm is at least two and negative otherwise. Indeed, if the outliers arise because for some points $x$ the label is not deterministic in the sense that $P(Y = 1 \mid X = x) \in (0, 1)$ rather than being $0$ or $1$, then the noise is fundamental to the problem and we should avoid fitting to it. We could instead go with a Gaussian kernel which makes the data linearly separable, but this would amount to overfitting and hurt the true risk of our hypothesis. This example shows that there are cases when one does want to use a kernel, the data may still not be linearly separable in the new feature space, and so we still need the soft-margin SVM formulation.
