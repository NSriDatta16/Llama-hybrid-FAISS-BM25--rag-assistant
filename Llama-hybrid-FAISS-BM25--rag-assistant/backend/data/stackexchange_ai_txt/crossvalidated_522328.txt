[site]: crossvalidated
[post_id]: 522328
[parent_id]: 
[tags]: 
gradient descent in neural network

Given that almost all the activation functions in neural networks are increasing, by the gradient descent rule, all parameters should be updated in the same direction (negative direction). Then how come we may have very large parameters?
