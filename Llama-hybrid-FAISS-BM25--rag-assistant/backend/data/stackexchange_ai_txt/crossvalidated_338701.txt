[site]: crossvalidated
[post_id]: 338701
[parent_id]: 338675
[tags]: 
If you have $n$ samples, then you can use Bayesian updating , to update your priors sequentially, by starting with a prior $\pi(\Theta)$, then you can use Bayes theorem to estimate the posterior $$ \pi_1(\Theta) = \pi (\Theta | X_1) \propto p(X_1 | \Theta)\; \pi(\Theta) $$ to update your knowledge given $X_2$ sample, you take $$ \pi_2(\Theta) = \pi (\Theta | X_1) \propto p(X_2 | X_1, \Theta)\; \pi_1(\Theta) $$ etc., using the general $$ \pi_{n+1}(\Theta) = \pi (\Theta | X_{n+1}) \propto p(X_{n+1} | X_1, \dots, X_n, \Theta)\; \pi_n(\Theta) $$ what would be equivalent to updating all at once $$ \pi(\Theta|X_1,\dots,X_n) \propto p(X_1,\dots,X_n|\Theta) \; \pi(\Theta) $$ To give an example, consider a beta-binomial model , where the posterior distribution is $$ \alpha,\beta|x \sim \mathcal{B}(\alpha + x, \,\beta + n-x) $$ so if you had two samples of sizes $n_1,n_2$ and observed $x_1,x_2$ successes, then first you'd update the prior parameters $\alpha,\beta$ to $\alpha+x_1$ and $\beta+n_1-x_1$, and then, using second sample, to $(\alpha+x_1)+x_2$ and $(\beta+n_1-x_1)+n_2-x_2$, what would be equivalent to updating all-at-once: $\alpha + (x_1 + x_2)$ and $\beta + (n_1 + n_2) - (x_1 + x_2)$. This directly translates to your case, where you have the "external" $X_1,\dots,X_n$ samples and want to use them to generate posterior for the "internal" sample $X_{n+1}$. Basically, this is the general idea behind Bayesian approach, where you can use your initial, or previous, knowledge and include it into your model as a prior, that is updated given new data. What follows, is that using posterior obtained using $X_1,\dots,X_n$ to update given $X_{n+1}$ is a perfectly valid way to go, if for some reasons you need to proceed sequentially (e.g. the data comes sequentially), then this is a valid way to go, it doesn't make much sense to obtain the $n$ independent "prior" estimates of $\Theta$ given $n$ samples, since you can proceed all-at-once, in fact, you shouldn't make $n$ independent estimates using the same prior in each case, since if then you somehow aggregated the results, then in the final result you'd include your prior $n$ times, you shouldn't use the same data to "estimate" the prior (so $X_{n+1}$ really needs to be new data), since then the same information would be used twice and you would end up with the result that is overconfident (while the point estimates wouldn't change, the posterior distributions and interval estimates would be too narrow). On another hand, if what you are asking is if using frequentist approach on the external data, to estimate the parameters for the priors, is a valid way to go, then the answer is still: yes, we often use external data, or previous results to create informative priors. However a Bayesian would still argue, that there is no reason for using frequentist approach in here. You can go all the way using Bayesian approach: starting with some prior, updating it using the external data, then using the posterior as a prior for a model using the "internal" data.
