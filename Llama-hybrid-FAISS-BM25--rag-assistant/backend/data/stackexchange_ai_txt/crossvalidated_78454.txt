[site]: crossvalidated
[post_id]: 78454
[parent_id]: 78391
[tags]: 
To get you started, the Elements of Statistical Learning have a nice discussion about regularization, and also sound discussions of different models To judge whether a particular regularization is a good idea, you need to take into account you data as well. E.g. for the LASSO, does it make sense for your data to assume that you have noise-only variates that should be excluded and information-carrying variates that should be included (as opposed to: do you expect that the separating information will be spread over all or nearly all variates) Also, Ripley's Pattern Recognition and Neural Networks discusses the artifical neural nets in R package nnet . This package has a function multinom which uses an ANN without hidden layser to fit (multinomial) logistic regression models. I think the NN book possibly together with the MASS book should get you started on the connection between ANN and GLM in this case. There is a close relationship between LDA and logistic regression: the posterior probabilities of a binary LDA is actually a logistic function. Whether non-linear is better than linear obviously depends on the structure of your data: if the classes are linearily separable, then a non-linear classifier won't help (but may be worse due to overfitting). Whether e.g. LDA is better than, say, logistic regression will depend on how much the assumptions of the LDA are violated. However, the Elements of Statistical Learning suggest that in practice, there is rarely much of a difference. And, of course, regularization or not is independent of whether you use LDA or GLM/logistic regression.
