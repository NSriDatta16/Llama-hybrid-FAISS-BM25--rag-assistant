[site]: crossvalidated
[post_id]: 294332
[parent_id]: 
[tags]: 
Backpropagation with zero weight initialization

Consider the following neural network: Input layer is 10 neurons Layer 1 is a fully connected layer with 10 output neurons Layer 2 is a ReLU activation layer Layer 3 is a fully connected layer with a single output The label is the sign of the output. Suppose that we have data which is realizable by this architecture, and for which the labels are balanced (have same number of 0's and 1's), while initializing the SGD with the all zeroes weight vectors. Which of the following statement is correct? SGD is guaranteed to converge on convex problems, hence it will always converge to the correct weights. The SGD process will not change the weights at all. For some data sets, the SGD process will converge to the correct weights. This is a non-convex problem, hence the weight will be changed by SGD, but might converge to a local minimum. So far, I've managed to eliminate the first answer, I'm left with 3 answers which I consider, however, the last answer seems the most logical to me although I'm not an expert on neural networks and therefore I'm asking here this question.
