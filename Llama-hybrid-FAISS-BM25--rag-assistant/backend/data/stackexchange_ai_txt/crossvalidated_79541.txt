[site]: crossvalidated
[post_id]: 79541
[parent_id]: 79366
[tags]: 
If the number of independent variables is not very large, you can just do “all subsets” regression in which all possible models are fit. The model the model with the highest F statistic or proportion of explained variation (PVE) (note: the concept was established with linear regression but can be applied to logistic regression as well) is selected. But this often results that we will choose the full model. So we need to penalize models with many variables that don’t ﬁt much better than models with fewer variables with Akaike Information Criterion (AIC). Lower AIC values usually indicate a better model that we will finally select. If the number of independent variables is large. The strategy is, select the best model with only one variable, then select another variable so that the best model with two variables is obtained, then select the 3rd variable...so on and so forth. The selection stops once AIC increases. Usually the complexity is around O(n^2) rather than O(2^n) in all subsets regression.
