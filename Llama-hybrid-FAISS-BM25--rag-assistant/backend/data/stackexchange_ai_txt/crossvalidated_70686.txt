[site]: crossvalidated
[post_id]: 70686
[parent_id]: 70490
[tags]: 
Not an actual answer, but an example to show that things are not so nice, and that extra hypotheses are needed to make this result true. Define $X_n$ as a mixture between a uniform $U\left( \left[ -{1\over n} ; {1\over n} \right] \right)$ and a normal $\mathcal N({n \over n-1}, {1\over n})$, the uniform component being chosen with probability $1\over n$, and the normal with probability $1 -{1\over n} = {n-1 \over n}$. You have $E(X_n) = 1$ and its variance converges to $0$ when $n$ goes to infinity, as $$E\left(X_n^2\right) = {1\over 3 n^2} \times {1\over n} + \left(\left({n \over n-1}\right)^2+{1\over n}\right)\times{n-1 \over n},$$ if I am not mistaking. Now define $f(x) = 1/x$ (and $f(0) = 0$ or whatever). The random variables $f(X_n)$ are well defined but does not have an expected value, as $$ \int_{-{1\over n}}^{1\over n} {1\over x} \mathrm dx$$ is not defined, no matter how big $n$ is. My conclusion is that you clearly need hypotheses on either the global behavior of $f$ or – more likely, more elegantly – on the speed at which the density of $X_n$ decays when you’re far from the expected value. I am sure that such hypotheses can be found in the classic literature (and even in textbooks), unfortunately my training was not in statistics and I still struggle with the literature myself... anyway I hope this helped. PS. Isn’t this example a counter-example to Nick’s answer? Who’s wrong then?
