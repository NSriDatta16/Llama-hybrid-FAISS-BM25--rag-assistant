[site]: crossvalidated
[post_id]: 376358
[parent_id]: 376224
[tags]: 
For the inputs, it is equivalent to scaling the initial weight matrix. Neural networks can be sensitive to such things, but that is a reason why it is good practice to try different variances for the weight scale, rely on batch normalisation (or its cousins layer and weight normalisation). For the one hot targets, it changes the loss function a little. Instead of just maximising the likelihood, it mixes in a term to follow the likelihood as if the label was wrong. I don't have a proper justification for this, but maybe it make gradients smoother. I have never encountered both "tricks" before, and I am highly skeptical the author know what he/she is doing. While it makes sense for the outputs, as it keeps a $\log$ from exploding, I don't see why it should be done for the targets and for the inputs. On a side, the transformation of the inputs does not place the inputs in the $[0.01, 0.99]$ , but in $[0.01, 1.]$ .
