[site]: crossvalidated
[post_id]: 493990
[parent_id]: 
[tags]: 
L2 regularization and its intuition

I am reading about L2 Regularization. As far as I know we add a thing to the loss function that: $$J(w) = LOSS + \lambda w^T w$$ In the book Deep Learning by Goodfellow et al., they stated "minimizing J(w) results in a choice of weights that make a tradeoff between fitting the training data and being small". $w^T w$ . How is this related to "being small"? Why the weights now tend towards zero rather than any other values?
