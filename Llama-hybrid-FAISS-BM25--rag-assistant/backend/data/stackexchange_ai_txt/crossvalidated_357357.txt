[site]: crossvalidated
[post_id]: 357357
[parent_id]: 
[tags]: 
How to normalize input data for autoencoders - anomaly detection

I'm building an autoencoder to identify anomalies on numerical data. The input features have different scales (i.e. some take values from 0 to 5, while others can be much much higher) and most of them are positively skewed. The features are too many to look manually and transform them to follow a normal distribution. The long-tails of the features are where I'm expecting to find anomalies. I started by firstly scaling the data (zero mean, unit variance) and then applying a min-max scaling. Based on the reconstruction error of the features, it seems that most anomalies refer to features with smaller skewness. This is reasonable (given the min-max scaling) since the features with long tails are compressed within a small range and only a few values are close to '1'. As a result, those features' reconstruction errors are usually small. My sense is that I should not apply the min-max scaling in order to be able to compare the reconstruction errors of features with different scales. So, here are my questions: Is it acceptable for NNs to accept inputs outside the range [0, 1]? The scaled data can take values up to 120 with the vast majority being within the [-1, 0] range. The output activation function is 'linear'. Should I possibly apply any other transformation like (X-median(X))/IQR(X) ? Should I blindly (given that it's not possible to review them all manually) apply any function on the input features to make them more 'normal' (like sqrt, log, box-cox)? Given all these possibilities, is there any way to compare the results of these models since the reconstruction errors cannot be directly compared?
