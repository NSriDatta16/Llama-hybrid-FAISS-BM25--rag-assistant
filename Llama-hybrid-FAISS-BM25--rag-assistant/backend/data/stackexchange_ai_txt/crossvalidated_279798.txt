[site]: crossvalidated
[post_id]: 279798
[parent_id]: 274815
[tags]: 
I only see this today but still I think I should chip in given that I'm kind of an expert and that at least two answers (nr 3 and 20 (thanks for referring to my work Xi'an!)) mention my work on SafeBayes - in particular G. and van Ommen, "Inconsistency of Bayesian Inference for Misspecified Linear Models, and a Proposal for Repairing It" (2014). And I'd also like to add something to comment 2: 2 says: (an advantage of Bayes under misspecification is ...) "Well, Bayesian approaches regularize. That is something, to help against overfitting - whether or not your model is misspecified. Of course, that just leads to the related question about arguments for Bayesian inference against regularized classical approaches (lasso etc)" This is true, but it is crucial to add that Bayesian approaches may not regularize enough if the model is wrong. This is the main point of the work with Van Ommen - we see there that standard Bayes overfits rather terribly in some regression context with wrong-but-very-useful-models. Not as bad as MLE, but still way too much to be useful. There's a whole strand of work in (frequentist and game-theoretic) theoretical machine learning where they use methods similar to Bayes, but with a much smaller 'learning rate' - making the prior more and the data less important, thus regularizing more. These methods are designed to work well in worst-case situations (misspecification and even worse, adversarial data) - the SafeBayes approach is designed to 'learn the optimal learning rate' from the data itself - and this optimal learining rate, i.e. the optimal amount of regularization, in effect depends on geometrical aspects of model and underlying distribution (i.e. is the model convex or not). Relatedly, there is a folk theorem (mentioned by several above) saying that Bayes will have the posterior concentrate on the distribution closest in KL divergence to the 'truth'. But this only holds under very stringent conditions - MUCH more stringent than the conditions needed for convergence in the well-specified case. If you're dealing with standard low dimensional parametric models and data are i.i.d. according to some distribution (not in the model) then the posterior will indeed concentrate around the point in the model that is closest to the truth in KL divergence. Now if you're dealing with large nonparametric models and the model is correct, then (essentially) your posterior will still concentrate around the true distribution given enough data, as long as your prior puts sufficient mass in small KL balls around the true distribution. This is the weak condition that is needed for convergence in the nonparametric case if the model is correct. But if your model is nonparametric yet incorrect, then the posterior may simply not concentrate around the closest KL point, even if your prior puts mass close to 1 (!) there - your posterior may remain confused for ever, concentrating on ever-different distributions as time proceeds but never around the best one. In my papers I have several examples of this happening. THe papers that do show convergence under misspecification (e.g. Kleijn and van der Vaart) require a lot of additional conditions, e.g. the model must be convex, or the prior must obey certain (complicated) properties. This is what I mean by 'stringent' conditions. In practice we're often dealing with parametric yet very high dimensional models (think Bayesian ridge regression etc.). Then if the model is wrong, eventually your posterior will concentrate on the best KL-distribution in the model but a mini-version of the nonparametric inconsistency still holds: it may take orders of magnitude more data before convergence happens - again, my paper with Van Ommen gives examples. The SafeBayes approach modifies standard bayes in a way that guarantees convergence in nonparametric models under (essentially) the same conditions as in the well-specified case, i.e. sufficient prior mass near the KL-optimal distribution in the model (G. and Mehta, 2014). Then there's the question of whether Bayes even has justification under misspecification. IMHO (and as also mentioned by several people above), the standard justifications of Bayes (admissibility, Savage, De Finetti, Cox etc) do not hold here (because if you realize your model is misspecified, your probabilities do not represent your true beliefs!). HOWEVER many Bayes methods can also be interpreted as 'minimum description length (MDL) methods' - MDL is an information-theoretic method which equates 'learning from data' with 'trying to compress the data as much as possible'. This data compression interpretation of (some) Bayesian methods remains valid under misspecification. So there is still some underlying interpretation that holds up under misspecification - nevertheless, there are problems, as my paper with van Ommen (and the confidence interval/credible set problem mentioned in the original post) show. And then a final remark about the original post: you mention the 'admissibility' justification of Bayes (going back to Wald's complete class thm of the 1940s/50s). Whether or not this is truly a justification of Bayes really depends very much on one's precise definition of 'Bayesian inference' (which differs from researcher to researcher...). The reason is that these admissibility results allow the possibility that one uses a prior that depends on aspects of the problem such as sample size, and loss function of interest etc. Most 'real' Bayesians would not want to change their prior if the amount of data they have to process changes, or if the loss function of interest is suddenly changed. For example, with strictly convex loss functions, minimax estimators are also admissible - though not usually thought of as Bayesian! The reason is that for each fixed sample size, they are equivalent to Bayes with a particular prior, but the prior is different for each sample size. Hope this is useful!
