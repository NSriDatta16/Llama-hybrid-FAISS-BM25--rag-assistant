[site]: datascience
[post_id]: 122499
[parent_id]: 122498
[tags]: 
ChatGPT uses byte-pair encoding (BPE) as tokenization strategy. This approach was first proposed in the scientific article Neural Machine Translation of Rare Words with Subword Units . BPE uses subword-level tokens. To define the list of subwords, the BPE algorithm uses a text corpus and tries to find the most reusable word parts. Also, the BPE vocabulary keeps the individual characters as tokens, which grants it the ability to represent any word that is written in the same scripts it was trained on. The canonical implementation of BPE is this Python package created and maintained by the first author of the scientific article. However, OpenAI created their own implementation, called Tiktoken , which is faster. Other popular tokenization strategies include "wordpieces" (which is very similar to BPE)(see this answer for details) and the unigram vocabulary (see this answer for details).
