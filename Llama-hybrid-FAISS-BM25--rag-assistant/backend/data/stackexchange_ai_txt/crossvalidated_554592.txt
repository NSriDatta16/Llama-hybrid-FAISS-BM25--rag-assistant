[site]: crossvalidated
[post_id]: 554592
[parent_id]: 
[tags]: 
Intuition behind random strides in CNNs

I recently attended a lecture on CNNs and was given a brief overview on the topic of dropout. I understood the logic behind the regularization and silencing the firing of neurons to prevent overfitting. Is there an intuitive connection behind dropout and if we used disjoint/random strides for the filters in a convolutional layer? I can visualise that disjoint and random strides while performing convolution can reduce the correlation between different neurons. Additionally, is there any body of text that approaches this relation/ expands on a relation if it exists?
