[site]: stackoverflow
[post_id]: 3534653
[parent_id]: 3534313
[tags]: 
Well there is not just one thing to do to be honest. I would suggest what I have done in the past to combat this same issue. use a browser detection script there are a tone of classes out there for detecting browsers. Then check the browser against a db of known browsers. Then if the browser is in your list allow the call to the service if not use a "best guess" script. By this I mean something like this: Generic ip lookup class So what you are doing is in the event that a browser type is not in your list it will not use your paid services DB instead it uses this class which can get as close as possible. This way you get the best of both worlds bots are not racking up hits on your ip service and if a user does slip past your browser check for some reason they will most likely get a correct location and thus appearing as normal on your site. This is a little jumpy I know I just hope you get what I am trying to say here. The real answer is that there is no easy answer or 100% right answer to this issue, I have done many sites with the same situation and have went insane trying to figure it out and this is as close to perfect as I have come. Since 99% of most ligit crawlers will have a value like so: $_SERVER['HTTP_USER_AGENT'] = 'Googlebot', 'Yammybot', 'Openbot', 'Yahoo'... etc. A simple browser check will do but it is the shady ones that may respond with IE6 or something. I really hope this helps like I said there is no real answer here at least not that I have found to be 100%, it's kind of like finding out if a user is on a hand-held these days you can get 99% there but never 100% and it always works out that the client uses that 1% that doesn't work lol.
