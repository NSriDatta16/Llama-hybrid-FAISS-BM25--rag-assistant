[site]: crossvalidated
[post_id]: 204479
[parent_id]: 204471
[tags]: 
Let me start by denying the premise. Robert Geary probably didn't overstate the case when he said (in 1947) " ...normality is a myth; there never was, and never will be, a normal distribution. " -- the normal distribution is a model*, an approximation that is sometimes more-or-less useful. $\:$ *(about which, see George Box , though I prefer his " ...how wrong does it have to be to not be useful " version). That some phenomena are approximately normal may be no vast surprise, since sums of independent [or even not-too-strongly-correlated effects] should, if there a lot of them and none has a variance that is substantial compared to the variance of the sum of the rest that we might see the distribution tend to look more normal. The central limit theorem (which is about the convergence to a normal distribution of a standardized sample mean as $n$ goes to infinity under some mild conditions) at least suggests that we might see a tendency toward that normality with sufficiently large but finite sample sizes. Of course if standardized means are approximately normal, standardized sums will be; this is the reason for the "sum of many effects" reasoning. So if there are a lot of little contributions to the variation, and they're not highly correlated, you might tend to see it. The Berry-Esseen theorem gives us a statement about it (convergence toward normal distributions) actually happening with standardized sample means for iid data (under slightly more stringent conditions than for the CLT, since it requires that the third absolute moment be finite), as well as telling us about how rapidly it happens. Subsequent versions of the theorem deal with non-identically distributed components in the sum , though the upper bounds on the deviation from normality are less tight. Less formally, the behavior of convolutions with reasonably nice distributions gives us additional (though closely related) reasons to suspect it might tend to be a fair approximation in finite samples in many cases. Convolution acts as a kind of "smearing" operator that people who use kernel density estimation across a variety of kernels will be familiar with; once you standardize the result (so the variance remains constant each time you do such an operation), there's clear a progression toward increasingly symmetric hill shapes as you repeatedly smooth (and it doesn't much matter if you change the kernel each time). Terry Tao gives some nice discussion of versions of the Central limit theorem and the Berry-Esseen theorem here , and along the way mentions an approach to a non-independent version of Berry-Esseen. So there's at least one class of situations where we might expect to see it, and formal reasons to think it really will tend to happen in those situations. However, at best any sense that the result of "sums of many effects" will be normal is an approximation. In many cases it's quite a reasonable approximation (and in additional cases even though the approximation of the distribution isn't close, some procedures that assume normality aren't especially sensitive to the distribution of the individual values, at least in large samples). There are many other circumstances where effects don't "add" and there we may expect other things to happen; for example, in a lot of financial data effects tend to be multiplicative (effects will move amounts in percentage terms, like interest and inflation and exchange rates for example). There we don't expect normality, but we might sometimes observe a rough approximation to normality on the log scale. In other situations neither can be appropriate, even in a rough sense. For example, inter-event times are generally not going to be well approximated by either normality or normality of logs; there's no "sums" nor "products" of effects to argue for here. There are numerous other phenomena that we can make some argument for a particular kind of "law" in particular circumstances, such as the limiting distributions in extreme values (Fisher-Tippett-Gnedenko or Pickands-Balkema-de Haan theorems, for example).
