[site]: crossvalidated
[post_id]: 568819
[parent_id]: 568816
[tags]: 
Why would you do that? As you noticed, there already exists a solution by respected authors including Trevor Hastie and Bradley Efron who rigorously studied and empirically validated the solution. If you are planning to introduce an alternative solution, you should probably start with equally rigorous mathematical proofs and an empirical evaluation vs their solution. It sounds like an interesting research paper, but if you want to use it as an applied method, re-inventing the wheel is probably not the best idea. Your proposed method considers the random seed as the only source of randomness. It does not take into account the randomness in the data due to sampling. If you are intending to use it for calculating confidence intervals it would underestimate the intervals because of that. Same argument would apply to varying the random seed only for neural networks. You need more than this, for example, use random samples of the training data ( bootstrap ) for each in the models in ensemble used for estimating the variability, and for neural network-specific approaches things like Monte Carlo dropout, or injecting randomness to the weights, etc.
