[site]: crossvalidated
[post_id]: 135959
[parent_id]: 135955
[tags]: 
If you first estimate $\theta$ by $\hat\theta$, a direct estimate of the $\alpha$-quantile is $F_{\hat\theta}^{-1}(\alpha)$. This is a convergent and biased estimator, whose asymptotic variance can be derived by the delta-method. Your first solution is validated by the Glivenkoâ€“Cantelli theorem, namely the fact that the empirical cdf converges to the true cdf:$$\hat{F}_n(x)=\frac{1}{n}\sum_{i=1}^n \mathbb{I}_{X_i\le x} \stackrel{n\to\infty}{\longrightarrow}F_{\hat\theta}(x)$$Once again, $\hat{F}_n^{-1}(\alpha)$ is a convergent and biased estimator, which variance can be estimated by boostrap. Your second method uses an average of estimators validated by your first method, hence it is equally valid and equally biased. However, for a given computing budget, i.e., a pre-determined total number of simulations, you have to run an experiment to compare both methods. For instance, running a toy experiment aiming at estimating the normal 80% quantile (equal to 0.8416212) shows the difference between both your approaches. #method 1 R=10^3 N=10^4 x=matrix(rnorm(R*N),ncol=R) kant=apply(x,1,quantile,prob=.80) leads to > sd(kant) [1] 0.04513716 > mean(kant) [1] 0.8404416 while #method2 R=10^3 M=N=10^2 kant=rep(0,R) for (r in 1:R){ x=matrix(rnorm(M*N),ncol=M) kant[r]=mean(apply(x,1,quantile,prob=.80)) } leads to > sd(kant) [1] 0.01375016 > mean(kant) [1] 0.8285708 hence to a smaller variance but to a larger bias. (This experiment does not account for the variability in replacing $\theta$ by $\hat\theta$.)
