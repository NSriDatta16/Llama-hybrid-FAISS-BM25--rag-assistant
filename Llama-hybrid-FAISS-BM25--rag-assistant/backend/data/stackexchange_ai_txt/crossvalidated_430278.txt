[site]: crossvalidated
[post_id]: 430278
[parent_id]: 
[tags]: 
How to Select the Largest Y Values for {X,Y} Pairs, for a Pareto-Distributed Dataset, for a Meaningful Fit?

First, apologies for the inelegant question. Second, on to the question: Background Information: I study impact craters, and the size-frequency distribution (number vs diameter) of impact craters tends to follow a power-law, such that I have many more small craters than large craters. I have measured the depths of those craters. The crater depth vs diameter (y vs x) has been measured across the solar system and tends to follow a power-law, too. Simple craters (craters that are small and bowl-shaped) have a different power-law fit than complex craters (craters that are larger and have more complicated features like wall terraces and central peaks). One way to determine the simple-to-complex transition on a given solar system body is to measure where those two power-law fits intersect, since the complex one is always shallower than the simple. State of the Literature: In the crater community, there is no standard method to determine which simple and which complex craters are used to fit the depth vs diameter data. Meaning, some people fit all crater data so have artificially steep fits based on ability to measure (we can more easily measure/identify shallow small craters than shallow deep craters). Some people try to identify the morphologically best preserved craters and just fit all of those data, but there's still a spread of values (though I think that's probably the best technique right now). Some people try to skim the deepest craters at a couple different diameters and fit that, but it's prone to outlier effects and can throw off the fit. (Each of the above being for simple craters separately from complex craters, where the differentiation there is often eyeballed from the plot.) Important: The fit values, and the intersection, are geophysically important to get right. They help validate hydrocode models, give you the strength of your target (the body's crust), and other things. What I've Tried: In my dissertation, I used the N deepest craters for a SQRT(2)Â· D binning scheme, where I chose N based on where the fit parameters started to reach a steady-state. I don't like that because at the large end, you suffer from many fewer craters and might include everything whereas at the small end you include just a tiny bit of the data. More recently, I was thinking perhaps some sort of Monte Carlo approach and hope for a plateau in model parameters that might indicate the best, but that could also be thrown off by too many shallow craters. Ideas? So I'm curious, per my title, if I have the depth vs diameter data, is there a statistically meaningful way to select some number of craters that are deepest - while rejecting outliers - to provide a statistically meaningful and robust fit. And, is there a meaningful way to quantify the uncertainty in that process (beyond the standard error of the fit itself), and quantify the uncertainty in the process on that final intersection point for simple vs complex craters? I've attached an example plot of craters from Pluto. The break from simple to complex is roughly at 10 km. The reason the data thin between about 8 and 20 km is that I only did every-few craters at that point just to get a decent idea of the distribution.
