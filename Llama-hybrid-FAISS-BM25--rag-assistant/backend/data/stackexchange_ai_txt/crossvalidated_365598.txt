[site]: crossvalidated
[post_id]: 365598
[parent_id]: 
[tags]: 
Why is this the correct formula to update the NN weights in Q-learning?

I'm trying to implement Q-learning to train an AI bot to play Pokemon battles. Since there is a large state space (corresponding to all possible states a battle can have in between moves), I can't use tables, so I'm using (nonlinear) function approximation. I found this formula to update the weights in a NN for Q, but I don't understand why it works: $$ \theta_i \leftarrow \theta_i + \alpha \left( r + \beta \max_{a'} \hat{Q}(s', a') - \hat{Q}(s, a) \right) \frac{\partial \hat{Q}(s, a)}{\partial \theta_i} $$ It seems related to backpropogation but I'm not sure how it's supposed to work. For example, there isn't an error function we're trying to minimize; the only feedback we get from the environment is the reward, and I don't see how updating the weights in this way is supposed to result in a closer approximation to the actual $Q(s, a)$. Can someone please translate this intuitively, or provide a proof of convergence? Thanks.
