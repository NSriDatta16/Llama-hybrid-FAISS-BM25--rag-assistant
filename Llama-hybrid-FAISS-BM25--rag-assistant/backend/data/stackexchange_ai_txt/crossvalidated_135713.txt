[site]: crossvalidated
[post_id]: 135713
[parent_id]: 135665
[tags]: 
Simulations are an excellent way to check whether you can obtain useful estimates from a model. You would do this by generating/simulating fake data that follows the distribution implied by your model. Then go ahead and fit your model to that data. This is an ideal case: your model is, in fact, true. So if the fit is noisy or inaccurate, then you know there is a problem either with the estimation procedure or the model itself. Similarly, you can simulate data using the "wrong" data generating process and use that fake data to assess how your estimates are affected by violating model assumptions. This is often called sensitivity analysis . These points are similar to items 2 and 8 in Tim's answer, and also a somewhat more ad-hoc version of the procedure in whuber's answer. Simulations are also used to perform predictive model checking as advocated by Andrew Gelman and others. This amounts to plugging your predictor data back into the model and then simulating fake response data from the implied distribution, to see if your simulated data is close enough (by whatever criterion you're using) to the real thing. Note that this is not the same thing as just computing fitted values. In a regression model, for instance, the fitted values are conditional averages; to run a predictive check on a regression model, you would have to draw once from the Gaussian distribution centered at each fitted value.
