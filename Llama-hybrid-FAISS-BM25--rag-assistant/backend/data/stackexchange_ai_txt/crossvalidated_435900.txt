[site]: crossvalidated
[post_id]: 435900
[parent_id]: 435318
[tags]: 
After doing the multi-head attention, you have 12 heads context vectors of dimension 768 and you need to project them back to the model dimension, this gives you another 12 × 768 × 768 + 768 parameters. In addition, there is layer normalization with 2 × 768 parameters.
