[site]: stackoverflow
[post_id]: 3022318
[parent_id]: 3022273
[tags]: 
If it is a big file chances are that it is written to disk as a contiguous part and "streaming" the data would be faster than parallel reads as this would start moving the heads back and forth. To know what is fastest you need intimate knowledge of your target production environment, because on high end storage the data will likely be distributed over multiple disks and parallel reads might be faster. Best approach is i think is to read it with large chunks into memory. Making it available as a ByteArrayInputStream to do the parsing. Quite likely you will peg the CPU during parsing and handling of the data. Maybe parallel map-reduce could help here spread the load over all cores.
