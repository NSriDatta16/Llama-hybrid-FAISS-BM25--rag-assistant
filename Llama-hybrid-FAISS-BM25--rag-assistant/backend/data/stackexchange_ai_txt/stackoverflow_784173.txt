[site]: stackoverflow
[post_id]: 784173
[parent_id]: 
[tags]: 
What are the performance characteristics of sqlite with very large database files?

2020 update , about 11 years after the question was posted and later closed, preventing newer answers. Almost everything written here is obsolete. Once upon a time sqlite was limited to the memory capacity or to 2 GB of storage (32 bits) or other popular numbers... well, that was a long time ago. Official limitations are listed here . Practically sqlite is likely to work as long as there is storage available . It works well with dataset larger than memory, it was originally created when memory was thin and it was a very important point from the start. There is absolutely no issue with storing 100 GB of data. It could probably store a TB just fine but eventually that's the point where you need to question whether SQLite is the best tool for the job and you probably want features from a full fledged database (remote clients, concurrent writes, read-only replicas, sharding, etc...). Original: I know that sqlite doesn't perform well with extremely large database files even when they are supported (there used to be a comment on the sqlite website stating that if you need file sizes above 1GB you may want to consider using an enterprise rdbms. Can't find it anymore, might be related to an older version of sqlite). However, for my purposes I'd like to get an idea of how bad it really is before I consider other solutions. I'm talking about sqlite data files in the multi-gigabyte range, from 2GB onwards. Anyone have any experience with this? Any tips/ideas?
