[site]: crossvalidated
[post_id]: 348005
[parent_id]: 347920
[tags]: 
It sounds as if you conducted only a single iteration of training-set and test-set crossvalidation. This will render your accuracy levels highly unreliable -- especially for the decision tree model. The accuracy of the random forest model, drawing on many decision trees, should be a little more reliable. But still, rather than using one such iteration, you should use many in order to obtain stable estimates of model predictive accuracy. One prominent author typically recommends at least 10,000 such iterations, and though I suspect that that many are not necessary, you'll find other authors who assert that the term "crossvalidation" doesn't even apply to the use of training and test sets unless there are multiple iterations. Also note that there are other ways to split data besides 50-50 splits. You could look into k-fold procedures in which one might use more than half the data (perhaps 70-90%) for the more "demanding" task of building each model, and the smaller portion for testing it. I would think this would be especially relevant to a situation like yours, where you have so many cells (each reflecting combinations of ordinal and/or nominal variables) that need to be populated in order to establish a basis for prediction. But 45 predictors (with only 1,962 observations) will make that difficult, so you may want to reduce the number of predictors via data reduction or other approaches.
