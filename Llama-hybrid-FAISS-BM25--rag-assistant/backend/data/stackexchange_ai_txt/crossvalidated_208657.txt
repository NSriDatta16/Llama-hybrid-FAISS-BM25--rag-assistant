[site]: crossvalidated
[post_id]: 208657
[parent_id]: 208259
[tags]: 
Yes, there is. We are modeling using $y = \theta x$. consider a simple example: $(1,2), (3,3)$ Using naive, we are taking the average of $\frac{y}{x}$ for all data points as $\theta$ (see below for mathematical explantion). We have: $$\theta_{naive} = (1/2)(2+1) = 1.5$$ Using LR, we have $$ \theta_{LR} = 1.1$$ The training SSE with naive is $(2-1.5)^2 + (3-4.5)^2 = 2.5$ while with LR it is $(2-1.1)^2 + (3- 3.3)^2 = 0.9$. Why is this? We can compare how the two $\theta$ differ mathematically The naive predictor: $$\hat{y} = \frac{1}{n} \sum^n_{i=1} \frac{y_i}{x_i}x = (\sum^n_{i=1} \frac{1}{n} \frac{y_i}{x_i})x$$ $$ \theta_{naive} = \sum^n_{i=1} \frac{1}{n} \frac{y_i}{x_i} $$ Linear regression: $$ \theta_{LR} = (X^T X)^{-1} X^T Y= \frac{\sum^n_{i=1} x_i y_i}{ \sum^n_{i=1} x_i^2 } = \frac{\sum^n_{i=1} x_i^2 \frac{y_i}{x_i}}{ \sum^n_{i=1} x_i^2 } = \sum^n_{i=1} \frac{x_i^2}{ \sum^n_{j=1} x_j^2} \frac{y_i}{x_i} $$ So, they differ in how much they weigh each $\frac{y}{x}$. Naive weighs all $\frac{y}{x}$ equally (using $\frac{1}{n}$), whereas linear regression weighs it relative to $x$ (using $\frac{x_i^2}{ \sum^n_{j=1} x_j^2}$). It makes sense that we should weigh the slopes of points with larger $x$ values more: consider if we had (1,1) and (1,0). $\theta = 0.5$ minimizes error. If we had (1,1) and (10000,0), $\theta$ should decrease, not stay the same, despite the slopes staying the same.
