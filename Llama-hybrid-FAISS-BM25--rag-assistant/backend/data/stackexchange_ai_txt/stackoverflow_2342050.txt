[site]: stackoverflow
[post_id]: 2342050
[parent_id]: 2341998
[tags]: 
I need help answering the following question: How can my team ensure that our effort isn't wasted in the long run by unmaintained tests and/or failure to write new ones as business requirements roll on? Make sure that your build process executes the tests on every build and fails the build if there are failures. Do you use Continuous Integration? Hudson is a great tool for this. It can keep a graph of # of tests, # of failures, test coverage, etc., for every build over the lifetime of your project. This will help you keep an easy eye on when your coverage % is declining. As you mentioned it can be pretty hard to retrofit unit testing into an existing project, let alone TDD. I wish you luck on this effort! Update: I also want to point out that 100% test coverage isn't really a great goal, it has diminishing returns as you try to go from ~80% or ~90%. To get those last few percentage points you need to start simulating every possible branch in your code. Your team will start spending time simulating scenarios that either can't happen in real life ("this stream won't actually throw an IOException when I close it but I need to get this branch covered!") or has no real value in your testing. I caught someone on my team verifying that if (foo == null) throw new NullPointerException(...); as the first line of a method actually threw an exception when the value was null . Much better to spend your time testing the code that actually matters than becoming obsessive-compulsive about making every last line show up as green in Emma or Cobertura.
