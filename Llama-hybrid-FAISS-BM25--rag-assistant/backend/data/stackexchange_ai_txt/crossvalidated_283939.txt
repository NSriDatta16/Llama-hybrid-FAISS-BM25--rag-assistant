[site]: crossvalidated
[post_id]: 283939
[parent_id]: 283932
[tags]: 
You can use an unbiased estimate for variance if you want. Why not? However a little bias is often less important than people seem to think. If you really do need something to be unbiased, make sure it's the thing that matters for your problem, not some other arbitrary choice that could make the thing that you care about even more biased than the MLE is. Ideally we want to resample from the true distribution. Because we cannot do that, we use bootstrap. We bootstrap using something close to the true distribution. But which kind of closeness is more important in a particular simulation? (i.e. make sure you solve the right problem) Bias in particular is tricky. If it's so essential to have unbiased estimators, why is variance the thing you want to be unbiased? Why not standard deviation or even precision? Perhaps they're more important to have unbiased for your problem -- or maybe you need some complicated function of the mean and standard deviation to be unbiased, for example. You won't get that by unbiasing the variance. [Note that using the MLE yields ML for variance, standard deviation and precision at the same time, while you can only be unbiased for one -- the others must then be biased.] if we use MLE for the variance, we will likely resample from a distribution with smaller variance than the true variance In small samples, yes, you'll sample a little more often from below the true variance than above it. On the other hand if you use the unbiased version, on average you'll simulate from a version that's further away from the true variance (I might be happy to take a little bias if I miss the target by less)*. This is why it's important to be clear why bias was more important than say mean square error (or indeed some other criterion). * The ML estimate won't minimize MSE either, but it's closer to it than the unbiased one is. Note that we can use simulation to investigate the suitability of various choices in some particular circumstance. For example, we can investigate the effect on coverage of a confidence interval on replacing an MLE with some other estimator.
