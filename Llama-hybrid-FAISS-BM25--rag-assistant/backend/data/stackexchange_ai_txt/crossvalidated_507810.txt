[site]: crossvalidated
[post_id]: 507810
[parent_id]: 507796
[tags]: 
The decision boundary is 'the' space where the final prediction is exactly the threshold (usually $0.5$ if it is a balanced problem). So this is exactly the critical area where the model switches from a positive to a negative prediction and vice versa. In the case of logistic regression it looks quite nonlinear (because of all of these exponential functions) but let us simply calculate: Given a single data row $x$ (consisting of all the features) the prediction for $x$ being $C_1$ is $$P[Y=C_1|X=x] = \sigma(\text{linear function}(x))$$ Since $P[Y=C_2|X=x] = 1-P[Y=C_1|X=x]$ the sets $$\{x : P[Y=C_1|X=x]=0.5\}$$ and $$\{x : P[Y=C_2|X=x]=0.5\}$$ are actually equal and this set is called the decision boundary. So where is $P[Y=C_1|X=x] = \sigma(\text{linear function}(x))$ exactly $0.5$ ? Since $\sigma$ is strictly monotonously increasing ( $\sigma' > 0$ ), it is injective and $\sigma(0) = 0.5$ meaning that $0$ is the only value where the sigma function attains $0.5$ . So $$\sigma(\text{linear function}(x)) = 0.5 \iff \text{linear function}(x)=0$$ Let's say we have two features $x_1$ and $x_2$ and the linear function looks like $ax_1 + bx_2 + c$ then we see that the decision boundary is exactly those $(x_1, x_2)$ where $$ax_1 + bx_2 + c = 0 \iff x_1 = -(bx_2 + c)/a$$ and that is the condition for a line (first parameter = second parameter multiplied by a weight). NOTE: The way I way(s) keep this in my head is that the decision boundary is actually linear and the switch from $C_1$ to $C_2$ is 'hard'. I.e. if we replace the sigmoid function with something like $1_{x >= 0.5}$ , the function that is $1$ when the input is $>=0.5$ and $0$ otherwise then we also get kind of a logistic regression model. The sigmoid function makes this transition a bit more soft (that is also important for the computation of the gradient when we want to train this model!). Another interpretation is this: Let us recall that computing the linear function yields a real scalar. What $\sigma$ does is that is 'squashes' that (from the interval $(-\infty, \infty)$ ) to the interval $(0, 1)$ . Hence, it is also sometimes called the 'squashing' function.
