[site]: crossvalidated
[post_id]: 80938
[parent_id]: 79547
[tags]: 
My problem is, we should know Ptarget(θ) before we doing this Metropolis process, right? Yes. The whole purpose of MCMC is to sample from the (known) target distribution, because handling it with other methods is difficult. For example, the target distribution might be multi-dimensional and maybe you only need the marginal distribution of one variable, and integrating the target distribution is unfeasible or very difficult to do (specially for hierarchical models, for example, in which every unknown parameter depends on other unknown parameters and so on). Then what is this target distribution, does it have to do with my prior belief of θ? As @Zhubarb answered, by bayes theorem, if we call $p(\theta)$ your prior belief on $\theta$, then your target distribution, a.k.a. the posterior distribution, is $$p(\theta |\textrm{Data})=\frac{p(\textrm{Data}|\theta)p(\theta)}{P(\textrm{Data})}$$ So yeah, your prior belief has to do with your target distribution: in fact, it is a function of it. If I've already known it (the target distribution), why bother doing this Metropolis sampling, can't we just use grid approximation? Yeah, you could just use a grid approximation if you know the posterior. This might seem easy to do in one-dimensional problems, but in multi-dimensional problems it's a mess. For example: how would you go on choosing your grid when you have a 10-dimensional parameter vector $\theta$? Where is the maximum or minimum of the distribution? Not in all settings you'll have an easy target distribution to play with and is in these kind of settings that MCMC is very useful, because it allows you to draw samples from the target distribution.
