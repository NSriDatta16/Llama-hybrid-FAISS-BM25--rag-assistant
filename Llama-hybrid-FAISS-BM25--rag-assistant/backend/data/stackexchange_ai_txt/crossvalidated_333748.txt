[site]: crossvalidated
[post_id]: 333748
[parent_id]: 333746
[tags]: 
I think it would keep your code simplest if you use the Keras Embedding to handle things for you: model = Sequential() model.add(Embedding(max_features, 256, mask_zero=True, ...) It's not documented very well, but Embedding seems to learn a word2vec-style embedding, based on an input of token numbers. In the example, above, for each token it takes as input a 1-wide token number vector, and will output a 256-wide embedded vector. Note also the mask_zero , which says that Embedding -- and (many) other layers -- should ignore token number 0. So left-pad your input token vector with 0's. (Of course, you don't want to try to use 0 as an actual token number.) The downside is that the Embedding expects a maximum token number ( max_features ) and it's only going to learn an embedding based on your training set. So you can't really do something like Transfer Learning with a pre-done, large-scale word2vec embedding.
