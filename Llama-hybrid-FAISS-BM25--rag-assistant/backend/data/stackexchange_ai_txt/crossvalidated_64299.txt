[site]: crossvalidated
[post_id]: 64299
[parent_id]: 
[tags]: 
Implementing kernel logistic regression using IRWLS

I am referring to [1] for implementing Kernel Logistic Regression using IRWLS. In logistic regression, the form of the regularized negative log-likelihood we aim to minimize is the following: $L(w) = - \sum_{i = 1}^{l} t_{i} \log \mu_{i} + (1 - t_{i}) \log(1 - \mu_{i}) + \lambda ||w||^{2},$ where $\mu_{i} = P(y_{i} = 1 \mid x_{i}) = {\exp(w^{T} x_{i})}/[1 + \exp(w^{T} x_{i})]$ , and $t_{i} \in \{0, 1\}$ is the actual "label" associated to the sample (everything as in usual logistic regression; we can see the bias term as an additional 1 in $x$ ). When using IRWLS (Newton-Raphson), learning $w$ should boil down to the following iterative procedure: $$w^\text{new} = w^\text{old} - (L''(w))^{-1} L'(w) = (X^{T} W X + \lambda I)^{-1} X^{T} W z,$$ with $z = X w^\text{old} + W^{-1}(t - \mu)$ , and $W$ is a diagonal matrix where $W_{ii} = \mu_{i} (1 - \mu_{i})$ . Everything to here seems OK for me. In the kernelized version, $w^{T} x$ becomes $\sum_{i = 1}^{l} \alpha_{i} K(x_{i}, x)$ , and [1] provides the following procedure for iteratively finding $\alpha$ : $$\alpha^{(k)} = (KW + \lambda I)^{-1} \phi(X)^{T} W z$$ where $z = (\phi(X) \alpha^{(k - 1)} + W^{-1} (t - \mu))$ . My questions are the following, in decreasing order of importance (to me): How can I find the bias term $b$ ? Which are good starting values for $W$ and $\alpha$ ? Can $(K W + \lambda I)$ turn out to be non-invertible? [1] Zhu, J. et al. - Kernel Logistic Regression and the Import Vector Machine - NIPS'01
