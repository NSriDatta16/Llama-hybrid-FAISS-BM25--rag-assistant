[site]: crossvalidated
[post_id]: 370009
[parent_id]: 369998
[tags]: 
My intuition about the topic: In a statistical parametric setting, which is in Desion Theory bounds, we'd like to estime, say, $\theta \in \Theta$ in the best way possible, by choosing a function of the sample data (a statistic) before we met the data. Let 'best' be measured by a loss function $l: t \times \theta \to \!R^+$ , where $t$ is the estimate for $\theta$ . So $l(t, \theta)$ is high for poor values of $t$ and zero for $t=\theta$ . Now we want to compare statistics $T_1$ and $T_2$ . One is clearly the winner if the loss function is less or equal than the other for all samples. If that's not the case, we cannot say which one is better. In other words, $T_1$ may be better than $T_2$ in some subset of sample space, but $T_2$ may be better than $T_1$ in another subset. To remove the dependency on sample space, we may take the average value over it. That's the risk function . Now, suppose $T_1$ is better than $T_2$ in average. $T_2$ may be better than $T_1$ in certain circumstances yet! $T_2$ mat be better than $T_1$ for some values of $\theta \in \Theta_1 \subset \Theta$ , yet worse in average! To further remove the dependency on $\Theta$ is to set a priori over $\Theta$ - this is the Bayesian approach. Here, we set 'importance' over $\Theta$ : more reasonable values of $\theta$ are more important, since they're more likely to be found in reallity. In a nutshell, I think we consider the risk function the way it is because it removes dependency on information, in a setting where we are making decision before we gather it. In complementation of @shadowtalker's answer, it's important to notice that sometimes (like @shadowtalker suggested on heavy-tailed distributions) expectation is not sufficient to summarise a random variable (though being a good "descriptor"). In those cases, we may need variance, skewness or kurtosis. Also, other central measure of tendendy as median are very useful in nonparametric statistcis for instance. Alhough historicaly theory was developed first for pararmetric statistics, where expected value has more appeal.
