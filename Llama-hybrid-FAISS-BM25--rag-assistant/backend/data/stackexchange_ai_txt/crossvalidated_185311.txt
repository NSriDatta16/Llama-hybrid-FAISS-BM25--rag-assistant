[site]: crossvalidated
[post_id]: 185311
[parent_id]: 
[tags]: 
Why does sampling from the posterior predictive distribution $p(x_{new} \mid x_1, \ldots x_n)$ work without having to average out the integral?

In a Bayesian model, the posterior predictive distribution is usually written as: $$ p(x_{new} \mid x_1, \ldots x_n) = \int_{-\infty}^{\infty} p(x_{new}\mid \mu) \ p(\mu \mid x_1, \ldots x_n)d\mu $$ for a mean parameter $\mu$. Then, inside most books, such as this link: Sampling MCMC It is claimed that it is often easier to sample from $p(x_{new} \mid x_1, \ldots x_n)$ using Monte Carlo methods. Commonly, the algorithm is to: for $j=1 \ldots J$: 1) Sample $\mu^{\ j}$ from $p(\mu \mid x_1, \ldots x_n)$ then 2) Sample $x^{\ * j}$ from $p(x_{new} \mid \mu^{\ j})$. Then, $x^{\ * 1}, \ldots, x^{\ * J}$ will be an iid sample from $p(x_{new} \mid x_1, \ldots x_n)$. What confuses me is the validity of this technique. My understanding is that Monte Carlo approaches will approximate the integral, so in this case, why do the $x^{\ * j}$'s each constitute a sample from $p(x_{new} \mid x_1, \ldots x_n)$? Why isn't is the case that the average of all those samples instead will be distributed as $p(x_{new} \mid x_1, \ldots x_n)$? I am under the assumption that I am creating a finite partition to approximate the integral above. Am I missing something? Thanks!
