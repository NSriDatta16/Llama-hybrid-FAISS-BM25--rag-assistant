[site]: crossvalidated
[post_id]: 237240
[parent_id]: 237226
[tags]: 
Simple intuition: Imagine we're estimating the probability that a thumbtack lands heads. Experiment 1 flips a thumbtack 5 times. Experiment 2 flips a thumbtack 100 times. Let's say from experiment 1, $\hat{b}_1 = .8$ and from experiment 2 $\hat{b}_2 = .6$ Which estimate is more precise? For an overall estimate based upon both experiments, should we calculate $\frac{1}{2} .8 + \frac{1}{2} .6$ (i.e. take the simple arithmetic mean) or is that in some sense sub-optimal? What weighting would correspond with a single experiment that flipped the thumbtack 105 times? Linear algebra based argument: Let's split the sample in two. $$ X = \left[ \begin{array}{c} X_1 \\ X_2 \end{array} \right]\quad \quad \mathbf{y} = \left[ \begin{array}{c} \mathbf{y}_1 \\ \mathbf{y}_2 \end{array} \right] $$ Now let's write the ordinary least squares (OLS) estimate $\hat{\mathbf{b}}$ for the full sample as a weighted sum of the OLS estimates for the subsamples. $$ \begin{align*} \hat{\mathbf{b}} &= (X'X)^{-1}X'y \\ &= \left[\begin{array}{c} X_1'X_1 + X_2'X_2 \end{array} \right]^{-1}\left[X_1'\mathbf{y}_1 + X_2'\mathbf{y}_2 \right] \\ &= \left[\begin{array}{c} X_1'X_1 + X_2'X_2 \end{array} \right]^{-1}\left[X_1'X_1(X_1'X_1)^{-1} X_1'\mathbf{y}_1 + X_2'X_2(X_2'X_2)^{-1} X_2'\mathbf{y}_2 \right] \\ &= \left[\begin{array}{c} X_1'X_1 + X_2'X_2 \end{array} \right]^{-1}\left[X_1'X_1\hat{\mathbf{b}}_1 + X_2'X_2\hat{\mathbf{b}}_2 \right] \\ &= \left( \left[\begin{array}{c} X_1'X_1 + X_2'X_2 \end{array} \right]^{-1}X_1'X_1\right) \hat{\mathbf{b}}_1 + \left( \left[\begin{array}{c} X_1'X_1 + X_2'X_2 \end{array} \right]^{-1} X_2'X_2\right)\hat{\mathbf{b}}_2 \end{align*} $$ Now define $\hat{M}_{1} = \frac{X_1'X_1}{n_1} \quad \hat{M}_{2} = \frac{X_2'X_2}{n_2} \quad \quad \hat{M} = \frac{X'X}{n} = \frac{n_1}{n_1 + n_2} M_{1} + \frac{n_2}{n_1 + n_2} M_{2}$ where $n_1$ is size of subsample $X_1$ etc... And the idea is that $M = \mathrm{E}\left[\mathbf{x}\mathbf{x}' \right] $. Then: $$\hat{\mathbf{b}} = \left( \frac{n_1}{n_1 + n_2}\right)\left(\hat{M}^{-1}\hat{M}_1 \right) \hat{\mathbf{b}}_1 + \left( \frac{n_2}{n_1 + n_2}\right)\left(\hat{M}^{-1}\hat{M}_2 \right) \hat{\mathbf{b}}_2 $$ $\hat{\mathbf{b}}$ from OLS is in some sense a sophisticated weighted-average of the subsample estimates. How does this compare to the simple arithmetic average of $\hat{\mathbf{b}}_1$ and $\hat{\mathbf{b}}_2$? Which way of combining $\hat{\mathbf{b}}_1$ and $\hat{\mathbf{b}}_2$ is optimal in a best linear unbiased estimator sense? Note also that $\left( \frac{n_1}{n_1 + n_2}\right)\left(\hat{M}^{-1}\hat{M}_1 \right)$ is a matrix, not a scalar. And for $\hat{M} \approx \hat{M}_1$ you'll probably have something in some sense close to $\frac{n_1}{n_1+n_2}$ as the weight on $\hat{\mathbf{b}}_1$.
