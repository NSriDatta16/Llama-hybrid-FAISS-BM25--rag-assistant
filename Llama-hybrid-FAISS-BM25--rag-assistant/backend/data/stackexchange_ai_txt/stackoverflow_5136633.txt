[site]: stackoverflow
[post_id]: 5136633
[parent_id]: 
[tags]: 
AVFoundation - Retiming CMSampleBufferRef Video Output

First time asking a question here. I'm hoping the post is clear and sample code is formatted correctly. I'm experimenting with AVFoundation and time lapse photography. My intent is to grab every Nth frame from the video camera of an iOS device (my iPod touch, version 4) and write each of those frames out to a file to create a timelapse. I'm using AVCaptureVideoDataOutput, AVAssetWriter and AVAssetWriterInput. The problem is, if I use the CMSampleBufferRef passed to captureOutput:idOutputSampleBuffer:fromConnection: , the playback of each frame is the length of time between original input frames. A frame rate of say 1fps. I'm looking to get 30fps. I've tried using CMSampleBufferCreateCopyWithNewTiming() , but then after 13 frames are written to the file, the captureOutput:idOutputSampleBuffer:fromConnection: stops being called. The interface is active and I can tap a button to stop the capture and save it to the photo library for playback. It appears to play back as I want it, 30fps, but it only has those 13 frames. How can I accomplish my goal of 30fps playback? How can I tell where the app is getting lost and why? I've placed a flag called useNativeTime so I can test both cases. When set to YES, I get all frames I'm interested in as the callback doesn't 'get lost'. When I set that flag to NO, I only ever get 13 frames processed and am never returned to that method again. As mentioned above, in both cases I can playback the video. Thanks for any help. Here is where I'm trying to do the retiming. - (void)captureOutput:(AVCaptureOutput *)captureOutput didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer fromConnection:(AVCaptureConnection *)connection { BOOL useNativeTime = NO; BOOL appendSuccessFlag = NO; //NSLog(@"in captureOutpput sample buffer method"); if( !CMSampleBufferDataIsReady(sampleBuffer) ) { NSLog( @"sample buffer is not ready. Skipping sample" ); //CMSampleBufferInvalidate(sampleBuffer); return; } if (! [inputWriterBuffer isReadyForMoreMediaData]) { NSLog(@"Not ready for data."); } else { // Write every first frame of n frames (30 native from camera). intervalFrames++; if (intervalFrames > 30) { intervalFrames = 1; } else if (intervalFrames != 1) { //CMSampleBufferInvalidate(sampleBuffer); return; } // Need to initialize start session time. if (writtenFrames My setup routine. - (IBAction) recordingStartStop: (id) sender { NSError * error; if (self.isRecording) { NSLog(@"~~~~~~~~~ STOPPING RECORDING ~~~~~~~~~"); self.isRecording = NO; [recordingStarStop setTitle: @"Record" forState: UIControlStateNormal]; //[self.captureSession stopRunning]; [inputWriterBuffer markAsFinished]; [outputWriter endSessionAtSourceTime:imageSourceTime]; [outputWriter finishWriting]; // Blocks until file is completely written, or an error occurs. NSLog(@"finished CMtime"); CMTimeShow(imageSourceTime); // Really, I should loop through the outputs and close all of them or target specific ones. // Since I'm only recording video right now, I feel safe doing this. [self.captureSession removeOutput: [[self.captureSession outputs] objectAtIndex: 0]]; [videoOutput release]; [inputWriterBuffer release]; [outputWriter release]; videoOutput = nil; inputWriterBuffer = nil; outputWriter = nil; NSLog(@"~~~~~~~~~ STOPPED RECORDING ~~~~~~~~~"); NSLog(@"Calling UIVideoAtPathIsCompatibleWithSavedPhotosAlbum."); NSLog(@"filePath: %@", [projectPaths movieFilePath]); if (UIVideoAtPathIsCompatibleWithSavedPhotosAlbum([projectPaths movieFilePath])) { NSLog(@"Calling UISaveVideoAtPathToSavedPhotosAlbum."); UISaveVideoAtPathToSavedPhotosAlbum ([projectPaths movieFilePath], self, @selector(video:didFinishSavingWithError: contextInfo:), nil); } NSLog(@"~~~~~~~~~ WROTE RECORDING to PhotosAlbum ~~~~~~~~~"); } else { NSLog(@"~~~~~~~~~ STARTING RECORDING ~~~~~~~~~"); projectPaths = [[ProjectPaths alloc] initWithProjectFolder: @"TestProject"]; intervalFrames = 30; videoOutput = [[AVCaptureVideoDataOutput alloc] init]; NSMutableDictionary * cameraVideoSettings = [[[NSMutableDictionary alloc] init] autorelease]; NSString* key = (NSString*)kCVPixelBufferPixelFormatTypeKey; NSNumber* value = [NSNumber numberWithUnsignedInt: kCVPixelFormatType_32BGRA]; //kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange]; [cameraVideoSettings setValue: value forKey: key]; [videoOutput setVideoSettings: cameraVideoSettings]; [videoOutput setMinFrameDuration: CMTimeMake(20, 600)]; //CMTimeMake(1, 30)]; // 30fps [videoOutput setAlwaysDiscardsLateVideoFrames: YES]; queue = dispatch_queue_create("cameraQueue", NULL); [videoOutput setSampleBufferDelegate: self queue: queue]; dispatch_release(queue); NSMutableDictionary *outputSettings = [[[NSMutableDictionary alloc] init] autorelease]; [outputSettings setValue: AVVideoCodecH264 forKey: AVVideoCodecKey]; [outputSettings setValue: [NSNumber numberWithInt: 1280] forKey: AVVideoWidthKey]; // currently assuming [outputSettings setValue: [NSNumber numberWithInt: 720] forKey: AVVideoHeightKey]; NSMutableDictionary *compressionSettings = [[[NSMutableDictionary alloc] init] autorelease]; [compressionSettings setValue: AVVideoProfileLevelH264Main30 forKey: AVVideoProfileLevelKey]; //[compressionSettings setValue: [NSNumber numberWithDouble:1024.0*1024.0] forKey: AVVideoAverageBitRateKey]; [outputSettings setValue: compressionSettings forKey: AVVideoCompressionPropertiesKey]; inputWriterBuffer = [AVAssetWriterInput assetWriterInputWithMediaType: AVMediaTypeVideo outputSettings: outputSettings]; [inputWriterBuffer retain]; inputWriterBuffer.expectsMediaDataInRealTime = YES; outputWriter = [AVAssetWriter assetWriterWithURL: [projectPaths movieURLPath] fileType: AVFileTypeQuickTimeMovie error: &error]; [outputWriter retain]; if (error) NSLog(@"error for outputWriter = [AVAssetWriter assetWriterWithURL:fileType:error:"); if ([outputWriter canAddInput: inputWriterBuffer]) [outputWriter addInput: inputWriterBuffer]; else NSLog(@"can not add input"); if (![outputWriter canApplyOutputSettings: outputSettings forMediaType:AVMediaTypeVideo]) NSLog(@"ouptutSettings are NOT supported"); if ([captureSession canAddOutput: videoOutput]) [self.captureSession addOutput: videoOutput]; else NSLog(@"could not addOutput: videoOutput to captureSession"); //[self.captureSession startRunning]; self.isRecording = YES; [recordingStarStop setTitle: @"Stop" forState: UIControlStateNormal]; writtenFrames = 0; imageSourceTime = kCMTimeZero; [outputWriter startWriting]; //[outputWriter startSessionAtSourceTime: imageSourceTime]; NSLog(@"~~~~~~~~~ STARTED RECORDING ~~~~~~~~~"); NSLog (@"recording to fileURL: %@", [projectPaths movieURLPath]); } NSLog(@"isRecording: %@", self.isRecording ? @"YES" : @"NO"); [self displayOuptutWritterStatus]; }
