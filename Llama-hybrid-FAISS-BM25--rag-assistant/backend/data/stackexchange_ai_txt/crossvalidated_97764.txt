[site]: crossvalidated
[post_id]: 97764
[parent_id]: 
[tags]: 
Estimating the average amount of time that passes when we we read elements element in a loop/iteration

I was trying to estimate the amount of time that pass by a program when we read one element from its array (say in python). Say that I have the times $t_i$ when I traversed a number of elements in the following program: array = [...] t_i_array = [] while i What is the correct estimate for the average time that passes when we read 1 element? (i.e. time per element, instead of elements per unit time) Is it: $$\mu = \frac{\sum^{n}_{i=1} t_i}{\sum^{n}_{i=1} n_i}$$ or is it: $$\mu = \sum^{n}_{i=1} \frac{t_i}{n_i}$$ Note that $t_i$ is the time to read $n_i$ elements on iteration i. If they do not estimate the quantity I want, then what is the difference between them? Why is the following reasoning flawed? $\frac{t_i}{n_i}$ is the average time to traverse 1 element. Then, why don't we don't this a bunch of times and average those results (what my second equation is doing). Why is that wrong? If you can address that issue at an intuitive level and if possible, at a mathematically rigorous level, that would be optimal.
