[site]: crossvalidated
[post_id]: 100591
[parent_id]: 
[tags]: 
Can I subsample a large dataset at every MCMC iteration?

Problem: I want to perform a Gibbs sampling to infer some posterior over a large dataset. Unfortunatelly, my model is not very simple and thus sampling is too slow. I would consider variational or parallel approaches, but before going that far... Question: I would like to know whether I could randomly sample (with replacement) from my dataset at every Gibbs iteration, so that I have less instances to learn from at every step. My intuition is that even if I change the samples, I would not be changing the probability density and therefore the Gibbs sample should not notice the trick. Am I right? Are there some references of people having done this?
