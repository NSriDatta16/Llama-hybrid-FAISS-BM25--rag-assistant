[site]: stackoverflow
[post_id]: 4046782
[parent_id]: 4044353
[tags]: 
Your example of desired output doesn't look like a 3-way contingency table to me. That would be a mapping from (key1, key2, key3) to a count of occurences. Your example looks like a mapping from (key1, key2) to some number. You don't say what to do when (key1, key2) is duplicated: average, total, something else? Assuming that you want a total, here's one memory-saving approach in Python, using nested defaultdict s: >>> from collections import defaultdict as dd >>> d = dd(lambda: dd(float)) >>> d[3277][4733] += 54.1 >>> d defaultdict( at 0x00D61DF0>, {3277: defaultdict( , {4733: 54.1})}) >>> d[3278][4741] += 51.0 >>> d defaultdict( at 0x00D61DF0>, {3277: defaultdict( , {4733: 54.1}), 3278: defaultdict( , {4741: 51.0})}) >>> and another approach using a single defaultdict with a composite key: >>> d2 = dd(float) >>> d2[3277,4733] += 54.1 >>> d2 defaultdict( , {(3277, 4733): 54.1}) >>> d2[3278,4741] += 51.0 >>> d2 defaultdict( , {(3277, 4733): 54.1, (3278, 4741): 51.0}) >>> It might help if you were to say what you want to do with this data after you've got it grouped together ... If you want (for example) an average, you have two options: (1) two data structures, one for total, one for count, then do "average = total - count" (2) sort your data on the first 2 columns, user itertools.groupby to collect your duplicates together, do your calculation, and add the results into your "average" data structure. Which of these approaches would use less memory is hard to tell; Python being Python you could try both rather quickly.
