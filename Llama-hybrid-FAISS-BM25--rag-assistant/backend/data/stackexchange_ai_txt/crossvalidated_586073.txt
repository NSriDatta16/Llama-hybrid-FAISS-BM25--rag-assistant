[site]: crossvalidated
[post_id]: 586073
[parent_id]: 586070
[tags]: 
Something is weird here. GridSearchCV is used to find optimal parameters. For every pair of parameters in the Cartesian product of param_grid , we fit cv models and average their performance. The eval_set argument in XGboost seems to be evaluating the model on the passed data. IF this was the test set, this doesn't seem to be appropriate because, if I have understood correctly, each model will predict on this test set. The only rationale I can think of is as follows: So long as the model is chosen from the results of the cross validation, and the eval_metric is passed only as a convenience (to have immediately after the cross validation and not to use for model selection purposes), then all is fine. Typically tho, we would just pass the test set once to the selected model, rather than have the test set predictions on all models fit in the cross validation step.
