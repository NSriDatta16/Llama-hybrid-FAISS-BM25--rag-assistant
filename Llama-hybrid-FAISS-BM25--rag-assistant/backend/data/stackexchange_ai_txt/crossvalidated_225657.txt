[site]: crossvalidated
[post_id]: 225657
[parent_id]: 225573
[tags]: 
No, if done properly, $k$-fold cross validation tends to overestimate generalization error, i.e. it has a (usually slight) pessimistic bias. That is, it gives an unbiased estimate of the generalization error for the surrogate model in question. But as the error of the model decreases with increasing training sample size (aka learning curve), the surrogate model on average has (slightly) higher true generalization error than the model trained on the whole data set - which is the model whose error is approximated by the cross validation. Done properly roughly means that the splitting into test and training sets within the cross validation actually leads to test cases that are truly independent of the model. However, there are a number of pitfalls that compromise this independence . Depending on how severely the test data is compromised and how much the model is overfit, this lack of independence means that the cross validation error becomes in fact a training error. I.e., all in all, you may end up with a severe optimistic bias (underestimating the actual generalization error). IMHO it is important to understand that most of these pitfalls are not unique to cross validation but are better characterized as wrong splitting into train and test set : they can (and do) happen just the same with other validation schemes such as hold out or independent test sets that in fact are not as independent as one supposes. Here are examples of the most common mistakes in splitting I see: @geekoverdose's answer gives an example of blatantly using an internal training (!) error estimate as test error. More general, any kind of error estimate used for data-driven model optimization is a training error as there is still training going on using this error estimate. Confounding variables not taken into account for the splitting. One row in the data matrix does not necessarily constitute an independent case, e.g. Treating repeated measurements of the same case/subject/patient as "independent" in general overlooking/ignoring strong clustering in the data not being aware of ongoing drift in the data generating process (future unknown cases vs. just unknown cases), ...
