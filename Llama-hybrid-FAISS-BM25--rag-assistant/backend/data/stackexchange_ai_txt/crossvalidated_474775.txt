[site]: crossvalidated
[post_id]: 474775
[parent_id]: 474693
[tags]: 
If you want to go Bayesian, a mixture of Gaussians can be modelled as follows e.g. 2 components, $z=\{1,2\}$ : for $i=1,\ldots,N$ $$ y_i|\mu(z_i), \sigma(z_i) \sim N(\mu(z_i),\sigma^2(z_i))\\ z_i \sim \text{Cat}(2, \mathbf{\theta}) $$ which states that the mixture component $z_i$ of the sample $y_i$ follows a categorical distribution with probability $\theta=\{\theta_1, \theta_2\}$ for the two possible components. The priors can be: $$ \mu_1 \sim N(\hat{\mu}_1,\hat{\sigma}^2_1)\\ \sigma^2_1 \sim \text{IG}(\hat{\alpha}_1,\hat{\beta}_1)\\ \mu_2 \sim N(\hat{\mu}_2,\hat{\sigma}^2_2)\\ \sigma^2_2 \sim \text{IG}(\hat{\alpha}_2,\hat{\beta}_2)\\ \theta \sim \text{Dir}(2, (\hat{a}_1, \hat{a}_2)) $$ where I've used the conjugate priors and set $\theta$ as iid from a Dirichlet distribution. The hat indicates that you set these parameters. Once fitted (using MCMC, for instance), you just look at the posterior distribution of $z_i$ and you have the probability of $y_i$ to belong to the first or the second Gaussian (you can get a point estimate with MAP or the posterior mean). The model for this (written in JAGS) (just used some random params): model { for (i in 1:N) { y[i] ~ dnorm(mu[z[i]], prec[z[i]]) z[i] ~ dcat(omega) } # Priors mu[1] ~ dnorm(-10, 1) mu[2] ~ dnorm(10, 1) T(mu[1]) # T(mu[1]) if you want to force mu[2]>mu[1] prec[1] ~ dgamma(2,2) prec[2] ~ dgamma(2,2) omega ~ ddirich(1,1) # Return the std instead of the precision sig[1] ~ sqrt(1/prec[1]) sig[2] ~ sqrt(1/prec[2]) }
