[site]: crossvalidated
[post_id]: 635238
[parent_id]: 635184
[tags]: 
I should start by qualifying that I don't have much practical experience with RNA-seq/differential gene expression analysis, though I have studied the common methods in some detail. I'll go over a few trade-offs you might have to consider (some of these methods are, in my humble opinion, questionable or at least unverified), but I hope to convince you at the very least that you shouldn't just 'transform and t-test'. RNA-seq data You haven't given much details about your dataset, commonly in RNA-seq these are short transcriptome reads that are mapped to a reference set of $k$ genes where $k$ is usually on the order of (tens of) thousands. You can see this as a giant multinomial experiment, more commonly the number of reads per gene is individually treated as a count. This count should be normalized for the library size/read depth, because just by sequencing more you'll produce higher counts. I'm assuming you already have these quality-controlled counts & normalization factors ready to go. Differential gene expression analysis Two main models are used for quantifying differential gene expression (DGE) from RNA-seq data: limma-voom ( limma ) transforms counts & normalized library size into log-transformed counts per million (cpm) and assumes these to be normally distributed. A standard L(M)M is applied, though I should note the mixed part is very limited in its implementation. edgeR and DESeq2 apply a negative binomial GLM on the counts directly, with the normalized library size as an offset. I've only actually used edgeR but from glossing the documentation these packages seem almost identical in their approach. Both approaches have their proponents and detractors, my personal feeling is that modelling counts as counts is 'more natural' though limma is certainly more powerful, both in the parametric assumptions that it makes (read: it's less conservative) and in the kinds of tests it enables (if you want mixed effects or gene-set tests there's no other option short of doing it by hand). I'll close this by saying that either approach will allow you to test a single gene, and your 'transform and t-test' suggestion is much closer to the limma pipeline than to the others. The dimensionality problem Where DGE analysis starts getting creative is in its dimensionality: these experiments usually have a pretty low sample size $n$ (your $n=50$ is already higher than many), but $k\ggg n$ features. This is not really a statistically tractable problem because the multiplicity is through the roof, so two steps are taken: Mean-variance stabilization, where the mean and variance (dispersion) of your counts is assumed to follow a trend. limma::voom will fit a LOWESS curve through estimates of these two quantities and calculate weights based on that, edgeR::estimateDisp will apply empirical Bayes shrinkage from the estimated mean-dispersion. I'm not sure if DESeq2 exposes a dedicated function but should follow the edgeR approach. limma::eBayes additionally applies empirical Bayes shrinkage to the genewise t-statistics from its linear model. A multiplicity correction is applied, usually Benjamini-Hochberg. The second point is generally sensible but redundant in this case. I really want to highlight that the first point is probably the most important decision you'll have to make however: do you still want to use data from other genes in this one comparison? My critical take on this is that mean-variance stabilizations are a mathematical trick that plays well with count distributions, but there's no biological reason for genes of the same expression level to have the same dispersion. In fact, doing so pretty much treats such genes as exchangeable, whereas they almost certainly are not! Still, there's some evidence that on average and without specifying which genes you are looking for, applying such shrinkage may lead to better inference. By only looking at a single gene you've avoided the 'curse of dimensionality' which these methods try to address in the first place. Perhaps an important question here is how you ended up with that one gene you want to test: was it a hypothesis going in or did you pull it from the data? In case of the former I might consider skipping all of the information borrowing, in case of the latter, well... you probably already used it to get to where you are. Either way, I hope this provides some reassurance towards whether it 'should be done': a single (pre-specified!) gene is a much more reliable test than all $k$ of them at once. Conclusion That was already a lot of text so I'll try to summarize a recommendation: Don't just apply a t-test, stick to a proper model for your data taking into account normalization. For consistency with your current DESeq2 pipeline that would be a negative binomial GLM, if you switch to the limma approach you could in theory do their cpm transformation followed by a t-test by hand (but I'd still recommend you just call their methods). This will also easily scale beyond a two-groups comparison - you've never given details on this. Think carefully about the information borrowing across genes you want to do. If you want to consider the data of your single gene in se (as I would for a pre-planned comparison) make sure it's not being shrunk towards something else. Throw away your multiplicity correction. Staying with the stabilized mean-variance is easy: just take the comparison you have in your full dataset and look at the non-multiplicity-adjusted P -value. If you do want to remove the shrinkage you might have to re-run your model as if there was only a single gene in your experiment.
