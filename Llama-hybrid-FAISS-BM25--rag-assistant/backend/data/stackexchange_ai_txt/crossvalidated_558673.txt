[site]: crossvalidated
[post_id]: 558673
[parent_id]: 435329
[tags]: 
RF does very poorly when the data is highly sparse, because there's a high probability that the feature it selects to split on will be all 0s. See: When to avoid Random Forest? Something as simple as svd or non-negative-matrix-factorization can improve RF when it recovers a useful dense representation of the sparse data. But this isn't guaranteed. Too rich a tree (too high max depth and related parameters) can be a source of overfit, but the effect is usually small so most people just build the deepest tree and call it a day. Setting the number of features to split on is by far the most important hyper-parameter; I can't find the article that I'm thinking of right now, though. Also, "auto" and " sqrt " do the same thing for RandomForestClassifier according to the documentation. https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html Using F1 score to select the model might not be sensitive enough, and could choose a bogus model. Using a strictly proper scoring rule that takes into account the full probability information is best. Some examples are Brier score and the cross-entropy. Semi-related note: you don't have to tune the number of trees in a random forest Do we have to tune the number of trees in a random forest? Just pick a large enough number that the variance in the predictions is small.
