[site]: crossvalidated
[post_id]: 293989
[parent_id]: 
[tags]: 
How to deal with anti-aliasing in MNIST images?

I'm using MNIST dataset ( http://yann.lecun.com/exdb/mnist/ ) to train a neural network. As pointed out in the site, images in the MNIST dataset were compressed from the original NIST images (which are bilevel: only black & white), resulting in grey-scale anti-aliasing artifacts in the images: The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. Now, after training my neural network, I want to use it to recognize actual handwritten digits by me (in particular, I'm using a TFT touchscreen shield in an Arduino to capture this). My captured digits do not suffer from anti-aliasing because I normalize captured values to 28x28 in Arduino code. In other words, my images are binary/bilevel, like the original NIST images, but not the ones in MNIST. MNIST seems to be the most popular choice for this type of problem, but I don't understand how people using it deal with the fact that new digits will likely not have the same anti-aliasing artifacts than the training dataset (or even not have anti-aliasing at all), and AFAIK this causes huge differences in how for example a neural network will perform. I could capture a bigger image and compress it, thus generating anti-aliasing pixels on purpose, but I wouldn't know how to guarantee that the same anti-aliasing algorithm that was used by MNIST will be applied, resulting in more differences/noise in the capture data. How should I approach this? EDIT: I've found a related question in StackOverflow, I add it here also for more context: https://stackoverflow.com/questions/27925358/pre-processing-before-digit-recognition-for-nn-cnn-trained-with-mnist-dataset
