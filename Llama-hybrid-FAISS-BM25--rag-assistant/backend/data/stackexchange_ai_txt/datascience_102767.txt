[site]: datascience
[post_id]: 102767
[parent_id]: 
[tags]: 
High accuracy in mode.fit but low precision and recall. Overfit? Unbalanced? Error?

Hello ive been training a CNN with keras. A binnary clasificator where it says if a depth image has a manhole or not. Ive labeled manually the datasets with 0 (no manhole) and 1(it has a manhole). I have 2 datasets 1 with 45k images to train the CNN and one with 26k images to test the CNN. Both datasets are unbalanced double of negatives images than positives. This is the code: # dimensions of our images. img_width, img_height = 80, 60 n_positives_img, n_negatives_img = 17874, 26308 n_total_img = 44182 #Labeled arrays for datasets arrayceros = np.zeros(n_negatives_img) arrayunos = np.ones(n_positives_img) #Reshaping of datasets to convert separate them arraynegativos= ds_negatives.reshape(( n_negatives_img, img_height, img_width,1)) arraypositivos= ds_positives.reshape((n_positives_img, img_height, img_width,1)) #Labeling datasets with the arrays ds_negatives_target = tf.data.Dataset.from_tensor_slices((arraynegativos, arrayceros)) ds_positives_target = tf.data.Dataset.from_tensor_slices((arraypositivos, arrayunos)) #Concatenate 2 datasets and shuffle them ds_concatenate = ds_negatives_target.concatenate(ds_positives_target) datasetfinal = ds_concatenate.shuffle(n_total_img) Then i have the same for the second dataset for testing. #Adding batch dimension to datasets 4dim valid_ds = datasetfinal2.batch(12) train_ds = datasetfinal.batch(12) #Defining model model = Sequential() model.add(Conv2D(5, kernel_size=(5, 5),activation='relu',input_shape=(60,80,1),padding='same')) model.add(BatchNormalization()) model.add(MaxPooling2D((5, 5),padding='same')) model.add(Dropout(0.3)) model.add(Conv2D(5, (5, 5), activation='relu',padding='same')) model.add(BatchNormalization()) model.add(MaxPooling2D(pool_size=(2, 2),padding='same')) model.add(Dropout(0.3)) model.add(Conv2D(5, (5, 5), activation='relu',padding='same')) model.add(BatchNormalization()) model.add(MaxPooling2D(pool_size=(2, 2),padding='same')) model.add(Dropout(0.3)) model.add(Conv2D(5, (5, 5), activation='relu',padding='same')) model.add(BatchNormalization()) model.add(Flatten()) model.add(Dropout(0.5)) model.add(Dense(100, activation='relu')) model.add(Dropout(0.2)) model.add(Dense(50, activation='relu')) model.add(Dropout(0.2)) model.add(Dense(1, activation='sigmoid')) #Compiling model model.summary() initial_learning_rate = 0.001 lr_schedule = keras.optimizers.schedules.ExponentialDecay( initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True ) model.compile( loss="binary_crossentropy", optimizer=keras.optimizers.Adam(learning_rate=lr_schedule), metrics=["acc"], ) # Define callbacks. checkpoint_cb = keras.callbacks.ModelCheckpoint( "2d_image_classification.h5", save_best_only=True ) early_stopping_cb = keras.callbacks.EarlyStopping(monitor="val_acc", patience=15) #Fitting the model history= model.fit(train_ds, validation_data=valid_ds, batch_size=100, epochs=5,callbacks=[checkpoint_cb, early_stopping_cb]) This gives me 99% of acc in train dataset and 95% in test dataset. But when i do this it gives me 60% precision for negatives images and 45% for positives: #Get the real labels of valid dataset valid_labels = list(valid_ds.flat_map(lambda x, y: tf.data.Dataset.from_tensor_slices((x, y))).as_numpy_iterator()) valid_labels = [y for x, y in valid_labels] y_pred = model.predict(valid_ds) y_pred = (y_pred > 0.5).astype(float) from sklearn.metrics import classification_report print(classification_report(valid_labels, y_pred)) Why this? I have printed both predicted labels and true labels and it look likes its random. It has no sense. https://colab.research.google.com/drive/1bhrntDItqoeT0KLb-aKp0W8cV6LOQOtP?usp=sharing If u need more information, just ask me. Thanks!!!!
