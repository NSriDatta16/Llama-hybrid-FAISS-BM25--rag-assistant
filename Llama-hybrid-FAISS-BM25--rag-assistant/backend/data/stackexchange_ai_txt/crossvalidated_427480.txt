[site]: crossvalidated
[post_id]: 427480
[parent_id]: 427386
[tags]: 
To me, this looks like normal behaviour. It could happen if, for example: method 1 (black line) is a more complicated model with a large number of trainable parameters method 2 (blue line) is a simpler model with only a few trainable parameters. In that case, for small sample sizes: method 1 has only a few data points for a lot of parameters and overfits on the training data, leading to a high validation loss (I assume that's what you have plotted on the $y$ axis?) method 2 has fewer parameters, so does not overfit. The validation loss is lower than for method 1. However, for large sample sizes: method 1, being a more complicated model, can fit the data better. Since there are more data points, there is less overfitting and the validation loss keeps decreasing. method 2 has only a few parameters, which do not change much upon adding more data. Since the model is simple, it cannot find a good "fit" and the validation loss bottoms out. There's a great paper on this exact topic: Classifier design for computer-aided diagnosis: Effects of finite sample size on the mean performance of classical and neural network classifiers by Chan and Sahiner in Medical Physics, 1999. You can find a copy here . It provides a nice explanation, including diagrams, for some toy problems where this occurs. Here's a picture from that paper that I modified. The $A_z$ is like the accuracy, so the higher the accuracy, the lower the loss. It shows $A_z$ against inverse sample size. The L is a linear model and Q a quadratic one. For small sample sizes, the quadratic model overfits and has a lower validation accuracy. But, for high sample sizes, it can fit the data better than the linear one and has a higher accuracy. I'd recommend reading the paper as it explains what is happening better than I can.
