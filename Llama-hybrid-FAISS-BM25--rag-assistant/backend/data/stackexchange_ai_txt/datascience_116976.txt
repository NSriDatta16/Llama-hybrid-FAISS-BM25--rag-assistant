[site]: datascience
[post_id]: 116976
[parent_id]: 116971
[tags]: 
In general, if there is a lot of difference in the results, it is usually due to a lack of data because it is difficult for the model to generalize patterns. In addition to that, optimized parameters setups (learning rates, dropout, weights initialization, ...) could improve greatly the results consistency. https://medium.com/geekculture/10-hyperparameters-to-keep-an-eye-on-for-your-lstm-model-and-other-tips-f0ff5b63fcd4 This could be done thanks to genetic algorithms that explores many hyper parameters and progressively improve them. https://github.com/sunan93/Optimizing-RNN-parameters-using-Genetic-Algorithms Keep in mind that having a too high accuracy could be an overfitting situation and could result bad in production. The right accuracy is around 85-90% but it depends on the data and the result in production. If you have a model with 95% accuracy in test and production, it should be kept.
