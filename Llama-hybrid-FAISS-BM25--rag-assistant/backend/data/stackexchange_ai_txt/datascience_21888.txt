[site]: datascience
[post_id]: 21888
[parent_id]: 21877
[tags]: 
Decided to go away and find the answers that would satisfy my question, and write them up here for anyone else wondering. The .best_estimator_ attribute is an instance of the specified model type, which has the 'best' combination of given parameters from the param_grid. Whether or not this instance is useful depends on whether the refit parameter is set to True (it is by default). For example: clf = GridSearchCV(estimator=RandomForestClassifier(), param_grid=parameter_candidates, cv=5, refit=True, error_score=0, n_jobs=-1) clf.fit(training_set, training_classifications) optimised_random_forest = clf.best_estimator_ return optimised_random_forest Will return a RandomForestClassifier. This is all pretty clear from the [documentation][1]. What isn't clear from the documentation is why most examples don't specifically use the .best_estimator_ and instead do this: clf = GridSearchCV(estimator=RandomForestClassifier(), param_grid=parameter_candidates, cv=5, refit=True, error_score=0, n_jobs=-1) clf.fit(training_set, training_classifications) return clf This second approach returns a GridSearchCV instance, with all the bells and whistles of the GridSearchCV such as .best_estimator_, .best_params, etc, which itself can be used like a trained classifier because: Optimised Random Forest Accuracy: 0.916970802919708 [[139 47] [ 44 866]] GridSearchCV Accuracy: 0.916970802919708 [[139 47] [ 44 866]] It just uses the same best estimator instance when making predictions. So in practise there's no difference between these two unless you specifically only want the estimator instance itself. As a side note, my differences in metrics were unrelated and down to a buggy class weighting function. [1]: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV.fit
