[site]: datascience
[post_id]: 47681
[parent_id]: 47679
[tags]: 
Why there are so many research papers suggesting the use of Newton's method based optimization algorithms for deep learning instead of Gradient Descent? Newton method has a faster convergence rate than gradient descent, and this is the main reason why it may be suggested as a replacement for gradient descent. Is Newton's method really needed if Gradient Descent can be modified to rectify all the problems faced during machine learning? Existence of vanishing gradient problem depends on the choice of "activation function" and the "depth" of network. Newton method and gradient descent would both face this problem for a function like Sigmoid , since in the flat extremes of Sigmoid both first and second order derivatives are small and exponentially vanishing by depth. In other words, the problem is solved for both methods by the choice of function. As a side note, 1st- and 2nd-order derivatives of Sigmoid go to zero at the same rate. Here is a graph of Sigmoid and its derivatives; zoom into the extremes. Historical note . Newton method predates the vanishing gradient problem (which was faced after the introduction of Backpropagation in 60s ) by centuries.
