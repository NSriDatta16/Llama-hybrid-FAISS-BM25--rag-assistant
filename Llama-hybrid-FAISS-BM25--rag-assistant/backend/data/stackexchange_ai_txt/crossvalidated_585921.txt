[site]: crossvalidated
[post_id]: 585921
[parent_id]: 
[tags]: 
Different ways to do autoregression in R

Can someone explain why different ways of doing autoregression give different estimates? For example, I can use regression with a lag, or I can use arima(), or gls() in package nlme: x In my experiments, the first two give the best estimates for the autocorrelation coefficient while the Phi estimate in the 3rd is too high. And perhaps more importantly, the coefficient for x is about right in both the arima() and gls() outputs, but too small with the lm() approach. So arima() seems most reliable - but what's wrong with my lm() approach, and what is gls() doing? (I realise that a partial answer has been given here: https://stats.stackexchange.com/a/223392/365775 - but that answer fits my understanding of a moving-average model (order(0,0,1) in arima) rather than a AR model... maybe I need to start a separate thread on that. But it doesn't explain the gls() differences.)
