[site]: crossvalidated
[post_id]: 643917
[parent_id]: 643886
[tags]: 
TLDR: You could this with a standard ML method (like xgboost) assuming fixed batch size but it is likely to be extremely data inefficient. One could code up a Tensorflow model relatively easily that actually does this efficiently and allows for varying batch sizes. Stepping back from this problem, we can think of what our data looks like. We could think of batches of rows of data where we don't know the label of each row, or we could think of each batch as a single row of data and we do have a label that is the sum of the labels we care about. Thinking of each batch as a single row, our features now look like $X_i = (x_{i,1}, x_{i, 2}, ..., x_{i, n})$ and we have observed label $Y_i = \sum_{i=n} y_i$ How do we solve this? Standard ML model: This requires fixed batch sizes . With this framework, we can now learn a simple mapping $g(X)$ from $\mathbb{R}^n \rightarrow \mathbb{R}$ . Feed your batches/rows into your model and learn the mapping. Once you learn that mapping, you can extract $f$ by evaluating $g(X)$ along a grid of values where $x_{i,1} = x_{i, 2} = ... = x_{i,n}$ and then divide your predictions along this grid by n, since the true $g(X) = f(x_{i,1}) + ... + f(x_{i,n})$ . Tensorflow custom model approach: This does not require fixed batched sizes . Theoretically, with enough data, properly flexible ML model and fixed batch sizes, the method above should converge to the correct answer. However, it's not leveraging as much model structure as it can, and this likely to make it very inefficient, especially for large batches! In particular, it's not using the facts that All features are treated exactly the same, i.e. we don't need to learn $n$ different functions for each item of the batch We know that we will be summing the independent inputs of each row. These two facts greatly reduce the space of functions we need to consider and thus leveraging this properly is likely to make for a much more data-efficient approach. I'm not aware of any published models that current take advantage of such structure, but it would be pretty easy to code one up in tensorflow if we again use the converting batches to rows method. All you would need to is code up something like this psuedocode (note: this definitely won't work but I think convey's the idea): def call(self, inputs): """Predict Y = sum(y) from one batch""" n_cols = inputs.shape[1] # Note: n_cols may vary for different batches! Y_hat = 0 for i in range(n_cols): # Need to define functional form of learned_f # during model initialization Y_hat += self.learned_f(inputs[:,i]) return(Y_hat) You do need to pick what your learned_f will be: this could be a simple feed forward network or better yet a bs_spline (assuming $x_{i,j}$ is univariate). Now we are fully leveraging the known model structure and this should be much more data efficient than the standard ML model approach. Also, there's no need for the batch size to be fixed: we simply sum over each input regardless. Final bonus: you can directly query learned_f instead of needing to transform $\hat g(X)$ to $\hat f(x)$ . EDIT : I've added an example of doing this with Tensorflow with a cubic polynomial as the learned function.
