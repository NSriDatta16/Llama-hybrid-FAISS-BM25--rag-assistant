[site]: datascience
[post_id]: 128105
[parent_id]: 
[tags]: 
Aside from trial and error, how do I select the number of layers and unit counts for LSTMS, GRUs, and Transformer units for text and time series?

When deciding on the number of units and layers for text processing or time-series prediction I rely heavily on trial and error. First, I look for a reference or paper on the topic such as the white paper on transformers: Attention Is All You Need . Once I read why the standard is so-and-so, I code the standard. After that, I incrementally adjust the unit counts and number of layers one at a time. I am making wild guesses at that point. Maybe the number of meaningful or non-zero tokens could approximate the units required. If my sequence length is capped at 120 tokens, I'll see how long it takes to train a 128-unit LSTM, GRU, or Transformer model. I arbitrarily set the unit count to the lowest exponent value of 2 greater than the maximum number of tokens and then steadily reduce it. After that, I start adding layers. If the model has bad metrics, I keep adding layers. If the model takes more than a minute to train per layer, I reduce the number of units and layers. I tolerate long training times based on how much free time I think I have. Is there any way to search more systematically? My criteria are all arbitrary. I hope there is a way to calculate the layer and unit counts based on the input data or meta-data. I am trying this out with TensorFlow.
