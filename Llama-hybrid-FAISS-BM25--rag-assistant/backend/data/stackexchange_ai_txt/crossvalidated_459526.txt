[site]: crossvalidated
[post_id]: 459526
[parent_id]: 459358
[tags]: 
It transforms your predicted probabilities to log odds ratios (or logit) and then uses that as a dependent variable to fit a logistic regression. If your prediction can clearly separate the labels, you would get an intercept of 0 and slope 1.. If we check the vignette of the function: Given a set of predicted probabilities p or predicted log odds logit, and a vector of binary outcomes y that were not used in developing the predictions p or logit, val.prob computes the following indexes and ... chi- square with 2 d.f. for testing unreliability (H0: intercept=0, slope=1), its P-value, ..., Intercept, and Slope Of course, bear in mind this is one of many test, and we can use the example from the vignette below, and the slope and intercept values are great because it's simulated: library(rms) set.seed(1) n It's the same as doing: glm(y[101:200] ~ log(phat/(1-phat)),family="binomial") Call: glm(formula = y[101:200] ~ log(phat/(1 - phat)), family = "binomial") Coefficients: (Intercept) log(phat/(1 - phat)) 0.05229 0.95652
