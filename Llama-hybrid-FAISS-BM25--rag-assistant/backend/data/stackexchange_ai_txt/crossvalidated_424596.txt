[site]: crossvalidated
[post_id]: 424596
[parent_id]: 424590
[tags]: 
1 - The architecture in the CS224n course lecture notes is correct. The likelihood is given by the product of the probabilities $ \Pi_{w\in \rm{Text}} \Pi_{c \in C(w)} P(c | w)$ (where $C(w)$ is the context of the target word). Note that I have added the product over all the words in the corpus (see details https://arxiv.org/abs/1402.3722 ). Taking the log of the likelihood to define the loss function, you end up with a sum over all the target words. Your confusion with the first source is related to your comment "Based on the first link I posted, each training example has only one context word". What they mean by having "training examples" of the form (target, context) such as (The, quick)... is that the likelihood decomposes into these $P(c | w)$ terms. 2- As commented previously, the loss function is a sum over all the target words (and for each target word a sum over its context) so this should solve your confusion regarding "Unlike a standard neural network, in which we form the cost function by taking the average of loss function over all training examples...". 3- agree with you, can you give more details?
