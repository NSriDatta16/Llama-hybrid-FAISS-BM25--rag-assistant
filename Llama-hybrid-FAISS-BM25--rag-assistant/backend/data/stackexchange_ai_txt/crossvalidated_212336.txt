[site]: crossvalidated
[post_id]: 212336
[parent_id]: 
[tags]: 
What is the purpose of adding features vector to the output of neural network layer before feeding it to the next layer?

Let's assume our neural network is made from two functions f(x) and g(x). They could be something simple like f(x) = ReLu(W1 * x + b1) and g(x) = Softmax(W2 * x + b2). Of course most of the time they are much more complicated, like for example chain of nested LSTMs, but this is irrelevant to my question. Simplest way of stacking them together is just feeding output of layer f to layer g, so our network would looks like Y^ = g(f(X)), where X is our features vector and Y^ is vector of predicted labels. But after checking more advanced examples, I noticed authors are making slightly different stack, where they add original features vector to output of the middle layers, so network looks then this way Y^ = g(X + f(X)). Could somebody explain to me what is the purpose of this? What is the advantage of using X + f(X) instead of just f(x)?
