[site]: crossvalidated
[post_id]: 629208
[parent_id]: 629142
[tags]: 
First, terminology: Maximum likelihood is a general estimation approach and can be used to estimate pretty much anything. GEE is a specific set of estimators and it estimates what it estimates, which is marginal generalised linear models We can use methods analogous to GEE to estimate other things, though. Now, If you use GEE to estimate, you get marginal coefficients. That's just what it does. Trying to use GEE to get conditional coefficients is like trying to use averages to estimate the median; it doesn't work unless the marginal and conditional coefficients happen to be the same. There are maximum likelihood estimators for marginal generalised linear models. One published set is Heagerty & Zeger's marginalised mixed models. Another is logistic regression models with the correlation parametrised by odds ratios and ratios of odds ratios and ratios of ratios of odds ratios, and so on. They are more computationally difficult than GEE, and they make stronger assumptions, and they gain some efficiency that way, though not a huge amount. There are moment-based estimators for conditionally specified models. You can write down the marginal moments implied by a conditional model and estimate the parameters by matching those modelled moments to empirical moments from the data. I know Patrick Heagerty did this in the late 1990s, but I don't know if anyone has published this approach (I have a vague memory of a JASA paper a bit later) There are other estimators for conditionally specified models that aren't maximum likelihood, that actually get used occasionally. For example, composite likelihood estimators based on the likelihoods of pairs of observations have been used to reduce computational complexity in spatial binary models, and have been used to allow for weighting in complex surveys. One advantage to GEE over maximum likelihood for correlated non-Normal data is that it's massively easier and more reliably computationally (this was even more true in the past). Another advantage of GEE is that you get to choose how you weight different contrasts, so you can choose how you want to model $Y_{it}$ in terms of $X_{is}$ for past and future $s$ . Likelihood and some GEE estimators are biased if past or future $X$ are correlated with $Y$ and are not included as predictors. [And I mean biased , not just that they estimate a different parameter]. This was first studied by Pepe & Anderson and then rediscovered by Pan, Louis & Connett There are, of course, well-known advantages to likelihood. Schildcrout and Heagerty looked at some of the tradeoffs for longitudinal binary data Actually, if you want likelihood fits for conditionally specified models you often get better frequentist performance (and obviously better Bayesian performance) from Bayesian estimation. It is even slower. Yes, this is a convenience sample of references; Patrick Heagerty was my PhD supervisor.
