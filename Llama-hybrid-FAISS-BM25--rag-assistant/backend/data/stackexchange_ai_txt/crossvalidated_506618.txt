[site]: crossvalidated
[post_id]: 506618
[parent_id]: 252693
[tags]: 
On a theoretical level, Bishop in $\S$ 14.1 and Murphy in $\S$ 16.6.3 insist on distinguishing between Bayesian Model Averaging and Ensemble Learning/Model Combination. Namely, in Bayesian Model Averaging we account for our uncertainty in the true population model $$p(y | x) = \sum_k p(\text{model} \ k \ \text{is the true population model}) p(y |x, \text{model} \ k \ \text{is the true model}),$$ and in principle if the true population model is one of the $K$ models we are considering, say $k=1$ , with enough data the posterior $$p(\text{model} \ 1 \ \text{is the true population model} \ | \ \text{data}) \to 1$$ will select the true model, and we will be using $$p(y|x) = p(y|x, \text{model} \ 1)$$ for prediction. In Ensemble Learning we on the other hand postulate that the true model happens to be a mixture of other simpler models, e.g. the regression function can be postulated to be $$f(x)=E_y(y|x) = \sum_k w_k f_{\text{model} \ k}(x).$$ Even after more and more data is available for estimating the weights $w_k$ , the estimates don't select one model (like e.g. $w_1 = 1, \quad w_i = 0 \quad i \neq 1$ ) as in Bayesian Model Averaging, reflecting the composite nature of the regression function. Furthermore, the weights themselves can depend on the input point $w_k(x)$ , allowing for even more flexible mixtures. On the more practical level, since not only the population model is unknown, even its functional specification can rarely be guessed right. That is why I think in ESL the authors are not pedantic about the above distinctions and instead discuss various practical ways of combining models like bagging ( $\S$ 8.7), committee methods ( $\S$ 8.8), boosting ( $\S$ 10), etc.
