[site]: crossvalidated
[post_id]: 543935
[parent_id]: 543933
[tags]: 
Let's start from the beginning. The likelihood is defined as the joint probability of observing the data; I guess for your task, the probability of one observation has the symbol $p(y|x)$ : $$ L = \prod_{i=1}^N p(y_i | x_i) $$ whence the negative log-likelihood is $$ -\log(L) = -\sum_{i=1}^N \log\left( p(y_i | x_i)\right) $$ and the choice to work with $$\text{NLL}=-\frac{\log(L)}{N} = -\frac{1}{N}\sum_{i=1}^N \log\left( p(y_i | x_i)\right)$$ is an estimate of the cross-entropy of the model probability and the empirical probability in the data, which is the expected negative log probability according to the model averaged across the data. Re-scaling by $\frac{1}{N}$ does not change the result of the optimization procedure, since multiplication by any positive scalar only changes the value of $L$ but not the location of the optima.
