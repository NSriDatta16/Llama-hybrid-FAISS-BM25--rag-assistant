[site]: crossvalidated
[post_id]: 398344
[parent_id]: 397848
[tags]: 
The layer-specific learning rates help in overcoming the slow learning (thus slow training) problem in deep neural networks. As stated in the paper Layer-Specific Adaptive Learning Rates for Deep Networks : When the gradient descent methods are used to train deep networks , additional problems are introduced. As the number of layers in a network increases, the gradients that are propagated back to the initial layers get very small ( vanishing gradient problem) . This dramatically slows down the rate of learning in the initial layers and slows down the convergence of the whole network. The learning rates specific to each layer in the network allows larger learning rates to compensate for the small size of gradients in shallow layers (layers near the input layer) . This method also helps in transfer learning as stated in the answer by @Andrey based on the blog post . -- check discriminative fine-tuning by Jeremy Howard and post on CaffeNet . The intuition is that in the layers closer to the input layer are more likely to have learned more general features -- such as lines and edges, which we wonâ€™t want to change much. Thus, we set their learning rate low. On the other hand, in case of later layers of the model -- which learn the detailed features, we increase the learning rate -- to let the new layers learn fast.
