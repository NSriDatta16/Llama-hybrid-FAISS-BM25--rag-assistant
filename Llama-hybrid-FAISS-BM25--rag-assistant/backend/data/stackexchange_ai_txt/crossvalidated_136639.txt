[site]: crossvalidated
[post_id]: 136639
[parent_id]: 136629
[tags]: 
What you typically want from a model is that the parameter estimators be consistent and have a known distribution (typically normal or asymptotically normal). Given these properties, you can rely on them without further adjustments and use the estimated model for inference, policy implications etc. Consistency and distributional properties can be achieved if the model satisfies the statistical assumptions underlying the estimation method. These assumptions are testable, so it makes sense to test them after you have estimated the model. If these assumptions are not satisfied, your parameter estimators may be inconsistent or have weird distributions. Then you cannot readily use them and you have to be more careful about inference. Some more background : There are different traditions of modelling and thus different modelling approaches. A review article by Gilbert (1986) throws some light on that. Some would start from a theoretical model which is supposed to be "true" (i.e. correctly specified) in the subject-matter sense , estimate it, find out it does not withstand the tests on statistical assumptions underlying the estimation method, and then start tackling the problem of how to estimate the parameters of the supposedly "true" model in a good way. Others would say that not passing the test means the "true" model is misspecified (here we see an implication from the statistical domain carrying over to the subject-matter domain). They would rather not postulate a "true" model at the start but would build a reasonable specification so that the statistical assumptions are satisfied. This is a crude take-away, at least how I understood it. Almost 30 years have passed since the publication of the paper, and perhaps one of these approaches is the dominating one now, but I do not have the data on that so I cannot claim anything with certainty.
