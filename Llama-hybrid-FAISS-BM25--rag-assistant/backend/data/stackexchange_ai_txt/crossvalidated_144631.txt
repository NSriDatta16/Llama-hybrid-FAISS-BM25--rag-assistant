[site]: crossvalidated
[post_id]: 144631
[parent_id]: 90874
[tags]: 
As it was already mentioned in the previous answers, stochastic gradient descent has a much noisier error surface since you are evaluating each sample iteratively. While you are taking a step towards the global minimum in batch gradient descent at every epoch (pass over the training set), the individual steps of your stochastic gradient descent gradient must not always point towards the global minimum depending on the evaluated sample. To visualize this using a two-dimensional example, here are some figures and drawings from Andrew Ng's machine learning class. First gradient descent: Second, stochastic gradient descent: The red circle in the lower figure shall illustrate that stochastic gradient descent will "keep updating" somewhere in the area around the global minimum if you are using a constant learning rate. So, here are some practical tips if you are using stochastic gradient descent: 1) shuffle the training set before each epoch (or iteration in the "standard" variant) 2) use an adaptive learning rate to "anneal" closer to the global minimum
