[site]: crossvalidated
[post_id]: 156746
[parent_id]: 156740
[tags]: 
you bring up a good point, but in fact both are incorrect. The text corpus that you use should be large enough to include as many words as possible. It should NOT just include your data. Training on a bunch of wikipedia articles and random tweets is a good way to do this. This will give you a dictionary, where each word corresponds to a vector. Now in order to do anything you have to use that dictonary as a reference for your text. Now you can do your machine learning (I'm assuming you are keeping that vague on purpose). If you aren't doing it already, try using a convnet on the vectors... You want to use a large dataset for learning the dictonary (of vectors). You want to have a feature space that is large enough to separate into whatever you're trying to classify. However, if you only vectorize the training data, you will miss important words. Your current procedure sort of defeats the purpose of cross validation, since cross validation is suppose to simulate data that you haven't seen.... thats why you want a very large training corpus that is independent of your training and test data....
