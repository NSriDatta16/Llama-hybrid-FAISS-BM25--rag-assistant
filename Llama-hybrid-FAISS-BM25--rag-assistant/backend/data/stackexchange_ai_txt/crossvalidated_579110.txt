[site]: crossvalidated
[post_id]: 579110
[parent_id]: 
[tags]: 
Am I falling for the ecological fallacy?

I am engaging in a machine learning project to predict some very noisy data - and I have a trained (LASSO) model which produces predictions which look reasonable when plotted on a scatter; of course it is quite inaccurate (R^2 of 0.05) but at least to my eye it appears to have a positive correlation. Training set of about 50k observations and testing over about 10k observations. I am trying to measure how 'good' my model is. To cut through the noise I decided to group into percentiles, so roughly 100 samples per aggregated bin. (Of course this flatters the aggregated R^2, as a lot of the noise is removed). However what I can’t work out is whether this is ‘wrong’ w.r.t. the ecological fallacy, and understanding the trend of the model. From what I could tell, the ecological fallacy is more about learning from aggregated populations, not interpreting aggregated model predictions. Doing this percentile aggregation results in 100 points - which could themselves be scattered randomly, or even have a negative correlation - but crucially instead I see a clear linear relationship between the predicted and actual bins. This holds for the mean and the median within each bin. So - my conclusion is the model learns to explain the target, given the features I have trained it on, and, albeit noisy, the average is near-enough right, and the noise is unbiased. With more/better features in theory we could reduce more of the noise (explain more of the variance) but if being right on average is what’s important, then perhaps this model is helpful. Am I falling for a fallacy or missing something here? Secondly, is it meaningful/common to decompose the trend of the model (i.e Y=X) with the noise (which appears unbiased but increases with X) - and if so how should this be measured/presented?
