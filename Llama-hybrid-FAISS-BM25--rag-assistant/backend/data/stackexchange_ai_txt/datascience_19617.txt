[site]: datascience
[post_id]: 19617
[parent_id]: 14581
[tags]: 
*To complement already great answers above. From my experience, GRUs train faster and perform better than LSTMs on less training data if you are doing language modeling (not sure about other tasks). GRUs are simpler and thus easier to modify, for example adding new gates in case of additional input to the network. It's just less code in general. LSTMs should in theory remember longer sequences than GRUs and outperform them in tasks requiring modeling long-distance relations. *Some additional papers that analyze GRUs and LSTMs. "Neural GPUs Learn Algorithms" (≈Åukasz Kaiser, Ilya Sutskever, 2015) https://arxiv.org/abs/1511.08228 "Comparative Study of CNN and RNN for Natural Language Processing" (Wenpeng Yin et al. 2017) https://arxiv.org/abs/1702.01923
