[site]: crossvalidated
[post_id]: 385834
[parent_id]: 385812
[tags]: 
Perhaps the approach of "Granger causality" might help. This would help you to assess whether X is a good predictor of Y or whether X is a better of Y. In other words, it tells you whether beta or gamma is the thing to take more seriously. Also, considering that you are dealing with time series data, it tells you how much of the history of X counts towards the prediction of Y (or vice versa). Wikipedia gives a simple explanation: A time series X is said to Granger-cause Y if it can be shown, usually through a series of t-tests and F-tests on lagged values of X (and with lagged values of Y also included), that those X values provide statistically significant information about future values of Y. What you do is the following: regress X(t-1) and Y(t-1) on Y(t) regress X(t-1), X(t-2), Y(t-1), Y(t-2) on Y(t) regress X(t-1), X(t-2), X(t-3), Y(t-1), Y(t-2), Y(t-3) on Y(t) Continue for whatever history length might be reasonable. Check the significance of the F-statistics for each regression. Then do the same the reverse (so, now regress the past values of X and Y on X(t)) and see which regressions have significant F-values. A very straightforward example, with R code, is found here . Granger causality has been critiqued for not actually establishing causality (in some cases). But it seems that you application is really about "predictive causality," which is exactly what the Granger causality approach is meant for. The point is that the approach will tell you whether X predicts Y or whether Y predicts X (so you no longer would be tempted to artificially--and incorrectly--compound the two regression coefficients) and it gives you a better prediction (as you will know how much history of X and Y you need to know to predict Y), which is useful for hedging purposes, right?
