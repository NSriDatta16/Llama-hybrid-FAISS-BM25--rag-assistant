[site]: crossvalidated
[post_id]: 615778
[parent_id]: 
[tags]: 
Simple logistic regression - calculate likelihood by hand

I am following Introduction to Linear Regression Analysis by Montgomery, Peck Vining and on p. 430 they present the likelihood ratio test. It says that $$ \text{LR} = 2 \left[\ln(L(\text{FM})) - \ln(L(\text{RM})) \right] $$ The formula then claims that this is equal to: $$ LR = 2 \left[ \left (\sum_{i=1}^n y_i \ln(\hat{\pi_i}) + \sum_{i=1}^n (n_i -y_i)\ln(1-\hat{\pi}_i) \right) - \left( y \ln(y) + (n-y)\ln(n-y) - n\ln(n) \right) \right] $$ When I calculate the log-likelihood using logLik I get different numbers than when I use the formula above. I just don't understand how to calculate the log-likelihood by hand. Here are my workings. # Data exposure = c(5.8, 15.0, 21.5, 27.5, 33.5, 39.5, 46.0, 51.5) cases = c(0,1,3,8,9,8,10,5) miners = c(98,54,43,48,51,38,28,11) successes = cases failures = miners - cases # Full model (FM) m = glm(cbind(successes, failures) ~ exposure, family=binomial(link='logit')) # Reduced model (RM) m0 = glm(cbind(successes, failures) ~ 1, family=binomial(link='logit')) # Likelihoods ln_L_FM = logLik(m)[1] ln_L_RM = logLik(m0)[1] LR = 2*(ln_L_FM - ln_L_RM) print(LR) Now, to calculate the logLikelihood of the full model: $$ \ln[L(FM)] = \sum_{i=1}^n y_i \ln(\hat{\pi}_i) + \sum_{i=1}^n (n_i -y_i)\ln(1-\hat{\pi}_i) $$ y = cases/miners n = miners p = fitted(m) t1 = y*log(p) t1[1] = 0 t1 = sum(t1) t2 = (n-y)* log(1-p) t2 = sum(t2) loglikFM = t1 + t2 However, this gives -52.70837 whereas logLik(m) gives -14.43861. What am I doing wrong? How can I get logLik(m) by hand? How can I get logLik(m0) by hand? I can't seem to get the same numbers for either of them.
