[site]: crossvalidated
[post_id]: 351828
[parent_id]: 
[tags]: 
Contraction or Reduction of a Neural Network

Circuit size reduction is common practice in Theoretical Computer science. It is very common to approximate a circuit with a smaller circuit (Or a polynomial and so on). Are there any such techniques in Neural networks? i.e Given a trained neural network can I reduce the size (Say number of perceptrons or layers) without any significant reduction in the performance. If yes can you please mention the references. Thank you Edit: Contraction of a circuit can be seen as an approximation of a larger circuit by a smaller circuit. Suppose I have a circuits $C$ with $n$ gates. I will remove certain gates(say k number of gates) and tweak some connections(wires) to obtain circuit $C'$(These gates are typically the one that doesn't affect the output significantly). Now the new circuit $C'$ and the old circuit $C$ will give the same output for most of the input except an $\epsilon$ fraction of them.
