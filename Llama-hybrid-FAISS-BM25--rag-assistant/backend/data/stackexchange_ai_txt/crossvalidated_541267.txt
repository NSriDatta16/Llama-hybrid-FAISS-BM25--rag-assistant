[site]: crossvalidated
[post_id]: 541267
[parent_id]: 541216
[tags]: 
As in Tim's answer , obtaining k-fold cross-validated predictions instead of the single split of data gives each estimator in the ensemble more data to learn from. It is important to not use predictions from the base learners on their own training set, lest the meta-estimator just copy the most-overfit base learner, but using k-fold takes care of that (see the next sentence from the paragraph you quoted). When the docs say the base estimators are trained on the entire dataset, it means for the final ensemble ("when calling predict or predict_proba "). Again the point is that more data is better, and since the meta-estimator is already trained, the overfitting question is avoided. Besides, with the k-fold predictions of the base estimators, we otherwise don't have a single trained version of each base model anymore, so something needs to be retrained in any case.
