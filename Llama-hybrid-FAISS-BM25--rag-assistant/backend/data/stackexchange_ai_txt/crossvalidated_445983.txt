[site]: crossvalidated
[post_id]: 445983
[parent_id]: 
[tags]: 
Effect of Reward Decay Factor in Reinforcement Learning

The reinforcement learning penalizes reward at a long horizon by a factor of $\gamma^t$ , where $\gamma$ is reward decay factor and $t$ is the time delay before collecting the reward. I do not understand why we need such a reward factor except for making the potentially infinite sum of rewards convergence: In practice, usually there is no such analogy for such reward decay in environment setup. For example, we do not say winning a game with 10 minutes is "half as good as" winning a game with 5 minutes. Why do we need such a factor? Is the value of the reward decay factor completely arbitrary? OpenAI dota agent "we annealed Î³ from 0.998 (valuing future rewards with a half-life of 46 seconds) to 0.9997 (valuing future rewards with a half-life of five minutes)". How do we pick the value of reward decay? Is there anyway to completely get rid of this hyperparameter?
