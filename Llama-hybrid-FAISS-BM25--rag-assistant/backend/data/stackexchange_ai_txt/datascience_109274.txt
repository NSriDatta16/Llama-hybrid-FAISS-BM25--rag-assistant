[site]: datascience
[post_id]: 109274
[parent_id]: 109261
[tags]: 
There are various text representation techniques, ranging from bag-of-words methods like TFIDF to embeddings. These techniques are used to build a fixed-length representation of any input text. If the text is the only input in an instance, as in text classification for example, then this representation is directly usable as the vector of features values. If there are other features or several distinct texts which must be represented separately, then the vector and the other features can be concatenated. In general one should be careful to prevent the resulting number of features to become too high. For example, it's very common to ignore the least frequent words in the case of a bag-of-words representation.
