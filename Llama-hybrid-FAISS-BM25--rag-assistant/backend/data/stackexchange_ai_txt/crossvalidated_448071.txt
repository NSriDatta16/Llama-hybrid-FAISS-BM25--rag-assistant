[site]: crossvalidated
[post_id]: 448071
[parent_id]: 
[tags]: 
What is the reason for feeding input image as stacked patches to CNN networks?

I am looking at the code of PredRNN architecture which is a modified version of ConvLSTM architecture for future frame prediction. When I was looking at the implementation, I realized that authors first resize the 2D image then feeds it to the network. The resizing was also performed in the ConvLSTM paper. An example, rather than feeding 64x64x1 (height,width,channel) image they resize it to 16x16x16 by stacking the image patches (non overlapping subsets of image). Are there any practical reasons for that? One reason that comes to my mind is that that makes the whole patches of the image connected in terms of convolutions, meaning that pixels of the upper left corner can be processed with the ones from the lower right corner. real_input_flag = np.reshape(real_input_flag, (args.batch_size, args.total_length - args.input_length - 1, args.img_width // args.patch_size, args.img_width // args.patch_size, args.patch_size ** 2 * args.img_channel))
