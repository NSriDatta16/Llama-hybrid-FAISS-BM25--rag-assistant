[site]: crossvalidated
[post_id]: 452453
[parent_id]: 
[tags]: 
Regression hypothesis testing via out-of-sample testing

Let's consider two linear models. $$\text{Full model}\\\mathbb{E}\big{[}Y\big{\vert} X_1,\dots,X_p, X_{p+1},\dots,X_{p+k}\big{]}=\\\beta_0 + \bigg[\beta_1X_1+\dots + \beta_pX_p\bigg] + \bigg[\beta_{p+1}X_{p+1}+\dots + \beta_{p+k}X_{p+k}\bigg]$$ $$\text{Reduced model}\\ \mathbb{E}\big{[}Y\big{\vert}X_1,\dots,X_{p}\big{]}=\beta_0 + \bigg[\beta_1X_1+\dots + \beta_pX_p\bigg]$$ If we want to test the full model against the reduced model and do an F-test, we are essentially saying that, while $SSE$ will be lower for the full model, we want to know if it is enough of an improvement to justify including the extra parameters. (Ditto for deviance testing in GLMs.) This is alluding to overfitting, and machine learning people check for overfitting by testing out of sample. It seems like we could do model inference by testing out-of-sample and seeing how each model performs. If the full model has significantly $^{\dagger}$ better performance, then we say that the $X_{p+1},\dots,X_{p+k}$ parameters contribute significantly, the same conclusion that we would make for an F-test of the full model versus the reduced model. This seems like a reasonable approach to doing model comparisons, and it would encompass the usual regression inferences like ANOVA and ANCOVA. Has any work been done on this? $^{\dagger}$ There would be some kind of hypothesis test (would there?), though I am not sure what.
