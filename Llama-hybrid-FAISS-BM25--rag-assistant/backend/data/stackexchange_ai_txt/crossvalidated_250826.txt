[site]: crossvalidated
[post_id]: 250826
[parent_id]: 
[tags]: 
Quadratic approximation of the regularized cost function for neural network

I've been working on the topic of regularization for neural networks and in the textbook I'm following I found this quote: "We will further simplify the analysis by making a quadratic approximation to the objective function in the neighborhood of the value of the weights that obtains minimal unregularized training cost. The approximation of J is given by $$\hat J(\theta) = J(w^*) + 1/2(w − w^∗)^\intercal H(w − w^∗)$$ where H is the Hessian matrix of J with respect to w evaluated at w*." Now I'm working on quadratic approximations in terms of calculus, but can somebody please explain how do we take quadratic approximation of the function J, if it is defined as usual mean squared error: $$J(\theta) = \frac{1}{2n}\sum_{i=1}^n (y_n - \hat y_n)^2$$ I mean, how do we get this approximation of J defined earlier? Thank you in advance.
