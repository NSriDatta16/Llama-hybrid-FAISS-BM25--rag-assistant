[site]: crossvalidated
[post_id]: 74822
[parent_id]: 70191
[tags]: 
If by technique you mean classification method (logistic regression, classification tree, ...), you can use any of these methods to obtain the result you want. Each method usually has a build in cost-function that you can adjust to obtain your desired results. All of these methods end up as being equivalent to building an ROC curve and choosing which point on that curve you want. Usually this is done automatically for you so you might not be aware that there is a tuning parameter that should be changed if you have an explicit cost function. Thus logistic regression usually uses the classification split at 0.5 probability, but based on your cost function you can change this to obtain the desired sensitivity/specificity. Most standard statistical packages will contain a post-estimation command that you can use after you build your regression model to provide sen/spec/ppv/npv for all the possible cut-point from 0 to 1. Perhaps it is worth noting that the cost function is rarely expressed at "goal of 100%PPV" but often as a ratio: the cost of false negatives/cost of false positives. In your case this ratio is low. The cost of a false positive >> cost false negative. But estimating this ratio you can give a more precise measure of your cost function. Edit: what i have called the cost function above is usually called the "utility function" in texts.
