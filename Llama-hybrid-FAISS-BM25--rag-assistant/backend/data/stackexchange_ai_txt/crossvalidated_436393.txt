[site]: crossvalidated
[post_id]: 436393
[parent_id]: 435934
[tags]: 
If the batch size is small and the size of the dataset is not massive, you are likely to get jagged curves, as the gradient of the loss could be noisy. Note that noisy gradients are what makes mini-batching effective: noise helps escape local minima. That said, the plots you are showing refer to a single epoch, is it correct? If so, they are showing you how the loss behaves in a single epoch, for each data batch. During a single epoch, you process potentially different batches, so the loss might oscillate. Instead, you should see the learning curve where you plot different epochs on the x-axis and the average loss over all batches in the y-axis.
