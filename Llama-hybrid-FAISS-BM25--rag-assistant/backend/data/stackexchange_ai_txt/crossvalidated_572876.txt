[site]: crossvalidated
[post_id]: 572876
[parent_id]: 
[tags]: 
Motivating use of Bayesian splines in excess mortality estimation

I'm reading this paper estimating excess deaths induced by the pandemic. That is, roughly, it constructs a model to estimate how many deaths (from all causes) would have occurred if the pandemic had not occurred, using historical mortality data. Then it extrapolates this forward into 2021, and compares that prediction to the actual number of deaths that occurred, where the pandemic did happen, of course. The model used for expected all-cause mortality is a Bayesian spline. Now, I'm an undergraduate that knows what Bayesian inference is, and what a spline is, but not what a Bayesian spline is. I'll outline the model and just wanted to know why this is approppriate for this setting, as opposed to a simple spline (piecewise polynomial regression) fitting on deaths over time historically. The model in the paper has deaths, $d$ , given by $d \sim \text{Poisson}(\mu)$ , where $\mu \sim \text{Exp}(\log p + \text{spline}(t))$ , where $p$ is population of the country and $t$ is time. This to me, is a heirarchical model, where the number of deaths (say time is indexed weekly) per week is a random event distributed Poisson (unclear why we don't just regress to spline straight up). And then to choose the Poisson rate parameter, we use an exponential model (unclear to me why); including $\log p$ seems sensible as deaths will be proportional to population and we want to correct for that, but I don't understand quite how the spline within the exponential is working to determine the rate parameter. Could someone explain why this is a sensible, motivated model for deaths at a particular point in time? Or just broadly motivate what I guess are "Bayesian splines"? Thank you!
