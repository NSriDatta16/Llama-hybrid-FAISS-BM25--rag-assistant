[site]: datascience
[post_id]: 39881
[parent_id]: 
[tags]: 
Scaling features in artificial neural networks

So it is a well known thing that it is a good idea to scale features/training samples in the training set, so that the values do not differ too much in the absolute sense. For example we want to train a neural network to learn a simple quadratic function y = x*x. The training data, say, looks like x = [1, 2, 3, 4, 5, 6] for input data and y = [1, 4, 9, 16, 25, 36] for target data. Obviously, one would scale both the input and the output data by something like x := x/6 and y := y/36, so all the training values live in the [0:1] range. My question is, what if I then want to predict the output value for the input 7? If I feed 7 / 6 into the network and then rescale the output by doing output := output * 36, I will not get the correct result. How to resolve such an issue?
