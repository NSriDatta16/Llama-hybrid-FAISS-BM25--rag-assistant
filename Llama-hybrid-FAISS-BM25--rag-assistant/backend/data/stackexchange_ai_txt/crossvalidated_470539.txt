[site]: crossvalidated
[post_id]: 470539
[parent_id]: 
[tags]: 
How do the errorbars change when time series is mean subtracted?

I want to subtract the mean from my time series. Each data point has a corresponding errorbar. I calculate the mean by fitting a constant with a MLE estimation and estimate the standard error with the inverse of the fisher matrix. If I subtract the mean, do the errorbars of the residuals change? If I use gaussian error propagation then the errorbar of each data point needs to be quadratically summed with the standard error. Is this the correct way?
