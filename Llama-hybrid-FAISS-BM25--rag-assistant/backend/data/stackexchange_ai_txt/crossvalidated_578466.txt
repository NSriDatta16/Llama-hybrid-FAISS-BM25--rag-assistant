[site]: crossvalidated
[post_id]: 578466
[parent_id]: 578454
[tags]: 
First we should distinguish between errors and residuals . If $X_1,\ldots,X_n$ are sampled from a population whose average is $\mu,$ then $X_1-\mu,\ldots,X_n-\mu$ are the errors and $X_1-\overline X,\ldots,X_n-\overline X,$ where $\overline X = (X_1+\cdots+X_n)/n$ is the mean of the sample rather than of the population, are the residuals. A simple linear regression model says $Y_i=\beta_0+\beta_1 x_i + \varepsilon_i$ for $i=1,\ldots ,n$ and that the errors $\varepsilon_i$ are normally distributed with expected value $0$ are variance $1$ and are mutually indpendent, the parameters $\beta_0,\beta_1$ are not random (and in this context "constant" usually means not random ), the parameters $\beta_0,\beta_2$ are not observable but instead must be estimated based on the observed data $(x_i,Y_i),\, i=1,\ldots,n.$ Notice that I have written capital $Y_i$ and lower-case $x_i;$ that is because the $x_i$ are treated as non-random. The word "random" in this context in effect means this: If we toss out the data $(x_i, Y_i),\,i=1,\ldots,n$ and collect new data, that which changes when that is done is "random"; that which does not change is not "random". However, realistically, in many (most) cases, the $x_i$ do change. But that are treated by the model as described above as if they were not random since what is considered is the conditional distribution of $Y_i$ given $x_i.$ (In designed experiments the $x_i$ are in some instances chosen by the experimenter and would remain the same if the experiment is replicated.) Are the "data" normally distributed? The $Y_i$ are normally distributed because the $\varepsilon_i$ are normally distributed. The errors $\varepsilon_i$ all come from the same distribution; the $Y_i$ come from different distributions because the $x_i$ differ from each other. But that's just a commonplace and often useful model. Is it "safe to assume" the errors are normally distributed? Certainly not. Statisticians look at the residuals to assess that. The Gaussâ€“Markov theorem makes weaker assumptions: the errors $\varepsilon_i$ are assumed uncorrelated rather than independent; they are assumed to have expected value $0$ and equal variances but they are not assumed to be identically distributed; hence not assumed to be normally distributed. The theorem states that among all unbiased estimators of $\beta_0,\beta_1$ that are linear as a function of $(Y_1,\ldots,Y_n),$ the one with the smallest variance is the least-squares estimator.
