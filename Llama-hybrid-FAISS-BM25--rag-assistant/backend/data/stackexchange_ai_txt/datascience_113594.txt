[site]: datascience
[post_id]: 113594
[parent_id]: 
[tags]: 
Finding the optimum sample lenght for sound recoginition

I am working on a sound recognition problem with a self-made data set of very long recordings. My current process looks like this: Time series segmentation to extract sound events cropped to a fixed length Save samples as grayscale spectrograms Run a almost vanilla VGG-16 Net with an input size that exactly fits my spectrogram My question: How can I determine the minimum sample length before the (temporal) information contained in the samples is significantly reduced (which would reduce my models potential performance)? The basic approach of just testing different sample length with the same Modell and comparing the classification performance seems straightforward. But when looking into it a little more detailed the correlations between the different parameters are heavily intertwined. As I reduce the sample length, I can extract more samples from the same data set. Further, if I stick to my approach of adapting the model design to exactly fit the spectrogram size in the input layer, I have a relation between the sample length and the model size = number of parameter = mode capacity, which in term changes not only the set of optimal hyper parameters such as learning rate and batchsize but also shifts the relation of training data to model capacity, thereby shifting the optimum amount of regularization (L1, L2, BatchNorm, Dropout) needed, too. The solution I came up with is to zero-padd the input, by placing the smaller spectrograms in the middle of a black image with the original size and using the same number of randomly selected samples for every class. Is this a valid approach, or will this have any effects on the training process that I am overlooking right now?
