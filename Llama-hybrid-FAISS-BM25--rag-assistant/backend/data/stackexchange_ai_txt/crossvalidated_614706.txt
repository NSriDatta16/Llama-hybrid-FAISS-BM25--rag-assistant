[site]: crossvalidated
[post_id]: 614706
[parent_id]: 530077
[tags]: 
Random forest is better for problems where random forest does better. Multinomial logistic regression is better for problems where multinomial logisitc regression is better. - Sycorax (in the comments) This is really all there is to it, but it might help to unpack this comment. If you have a relationship that (at least approximately) follows the multinomial logistic model, then that is the (approximately) correct model. It might be that a random forest can achieve high in-sample performance, but when it comes to making out-of-sample predictions, it will have achieved that awesome performance by fitting to coincidences in the training data rather than the true relationship between the features and the outcome. In such a scenario, the logistic regression outperforms the random forest, even in terms of pure predictive ability, contradicting your source. To be fair to your source, however, relationships in real life tend to be complicated. Simple models like multinomial logistic regressions are likely to miss important relationships that flexible models like random forests are able to detect. If you control for overfitting concerns, then letting a random forest "go do its thing" might make for a better out-of-sample predictor. Yes, there are theorems like Stone-Weierstrass saying that (generalized) linear models with polynomial features can be as good at approximating nonlinear relationships as is demanded (under decent conditions), but you have to engineer those polynomial features and know which ones to engineer. For a random forest, all you do is program considerable flexibility and let it take care of the rest (for better or for worse).
