[site]: crossvalidated
[post_id]: 19742
[parent_id]: 19735
[tags]: 
I think you're being a little over-ambitious --- using the beta distribution vs. another distribution function is the least of your problems. I'm not really sure what resampling the forecasts is supposed to achieve, and then re-estimating some parameters based on those resampled data isn't getting you anything that couldn't be estimated from the original forecasts. A better approach might be to track the estimate and the actual outcome over time and keep track of some other information about the task. Then you can look at bias (i.e. the average difference between the estimate and the outcome) and how the bias depends on other task characteristics. Unless you're in a setting where you have a lot of historical data (offhand, I'd say estimates and outcomes for ~ 20 past tasks if you don't car about conditioning on characteristics; more if you do, but that number's just off the top of my head) or are going to be accumulating new forecasts quickly (weekly forecasts or so) you're not going to be able to do anything that is very statistically airtight, so that shouldn't be your focus. You should be able to detect obvious problems pretty quickly, though, and may be able to give the forecasters useful feedback. In general, averaging the various forecasts is a pretty good strategy; you want to discard any seriously inferior forecasters first, though, which is where the stuff from the previous paragraph matters.
