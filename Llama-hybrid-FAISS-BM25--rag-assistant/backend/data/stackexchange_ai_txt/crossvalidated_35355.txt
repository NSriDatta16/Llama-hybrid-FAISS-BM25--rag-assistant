[site]: crossvalidated
[post_id]: 35355
[parent_id]: 22414
[tags]: 
I have found the Nelder-Mead simplex method very handy for optimizing the hyper-parameters of kernel machines. It doesn't require gradient information and it is almost as fast as gradient descent. If you are using MATLAB it is the fminsearch function in the optimisation toolbox, or alternatively you can download my implementation here . I would not advise using algorithms such as simulated annealing or genetic algorithms as it is very easy to over-fit the model selection criterion, even if you only have a couple of hyper-parameters to tune, and such methods that optimise the criterion very aggressively are likely to encounter this pitfall. This is actually a benefit of grid-search - it doesn't optimise the model selection criterion too closely and this helps avoid over-fitting. Don't pay any attention to the difference between trainings set and test set performance, it generally doesn't mean much (in fact I would recommend not looking at training set performance at all unless you suspect something is going wrong). Make sure you use a completely separate test set for evaluating the final performance of the model. If you tune the hyper-parameters using the test set, you will get a (possibly highly) optimistic performance estimate. Use a training-validation-test partitioning scheme and optimise the hyper-parameters using the validation set and measure performance on the test set. I use nested cross-validation. As I understand it, cross-validation is O.K. for time-series data, provided the data in each partition are essentially statistically independent (e.g. the time period covered by each partition is much longer than the effects of autocorrelation in the time series etc.). However, I am no time-series expert, so caveat lector !
