[site]: datascience
[post_id]: 118262
[parent_id]: 118260
[tags]: 
Summary ChatGPT is the fine-tuning of GPT-3.5, which is a language model based on a Transformer decoder with some modifications with respect to the original Transformer architecture. Therefore it is a decoder-only model. Complete information with references The origin of ChatGPT was GPT (Generative pre-Trained Transformer). The evolution from GPT to ChatGPT was as follows: GPT (see the OpenAI announcement ) was a normal Transformer decoder. From the GPT paper : In our experiments, we use a multi-layer Transformer decoder [34] for the language model [...] GPT-2 (see the OpenAI announcement and the source code ) is also a Transformer decoder, but with some modifications. It is also bigger and trained on more data. From the GPT-2 paper : We use a Transformer (Vaswani et al., 2017) based architecture for our LMs. The model largely follows the details of the OpenAI GPT model (Radford et al., 2018) with a few modifications. Layer normalization (Ba et al., 2016) was moved to the input of each sub-block, similar to a pre-activation residual network (He et al., 2016) and an additional layer normalization was added after the final self-attention block. A modified initialization which accounts for the accumulation on the residual path with model depth is used. We scale the weights of residual layers at initialization by a factor of $1/\sqrt{N}$ where $N$ is the number of residual layers. The vocabulary is expanded to 50,257. We also increase the context size from 512 to 1024 tokens and a larger batch size of 512 is used. GPT-3 is GPT-2 scaled up and with some modifications. From the GPT-3 paper published at NeurIPS'20: We use the same model and architecture as GPT-2 [RWC+19], including the modified initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer [CGRS19]. To study the dependence of ML performance on model size, we train 8 different sizes of model, from 125 million parameters to 175 billion parameters, with the last being the model we call GPT-3. InstructGPT (see the paper ) is a fine-tuned version of GPT-3. From the paper [...] we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning GPT-3.5 (see OpenAI announcement ) ( text-davinci-003 ) is a fine-tuned version of InstructGPT. From the announcement: code-davinci-002 is a base model, so good for pure code-completion tasks text-davinci-002 is an InstructGPT model based on code-davinci-002 text-davinci-003 is an improvement on text-davinci-002 ChatGPT ( gpt-3.5-turbo* ) is a GPT-3.5 fine-tuned on human instructions by Reinforcement Learning with Human Feedback (RLHF). From the OpenAI website : gpt-3.5-turbo-0301 is an improvement on text-davinci-003, optimized for chat From the ChatGPT presentation page : We trained this model using Reinforcement Learning from Human Feedback (RLHF), using the same methods as InstructGPT, but with slight differences in the data collection setup. We trained an initial model using supervised fine-tuning: human AI trainers provided conversations in which they played both sidesâ€”the user and an AI assistant. We gave the trainers access to model-written suggestions to help them compose their responses. We mixed this new dialogue dataset with the InstructGPT dataset, which we transformed into a dialogue format. To create a reward model for reinforcement learning, we needed to collect comparison data, which consisted of two or more model responses ranked by quality. To collect this data, we took conversations that AI trainers had with the chatbot. We randomly selected a model-written message, sampled several alternative completions, and had AI trainers rank them. Using these reward models, we can fine-tune the model using Proximal Policy Optimization. We performed several iterations of this process. To follow in detail the GPT evolution, I recommend the article How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources .
