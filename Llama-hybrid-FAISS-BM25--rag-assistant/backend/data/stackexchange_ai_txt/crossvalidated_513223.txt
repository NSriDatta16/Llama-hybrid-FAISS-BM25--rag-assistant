[site]: crossvalidated
[post_id]: 513223
[parent_id]: 513204
[tags]: 
Simplified case : Logistic regression uses the form: $$ p(X)=\frac{e^{\beta_{0}+\beta_{1} X}}{1+e^{\beta_{0}+\beta_{1} X}} $$ It is easy to see that no matter what values $\beta_{0}, \beta_{1}$ or $X$ take, $p(X)$ will have values between 0 and 1 . A bit of rearrangement gives $$ \log \left(\frac{p(X)}{1-p(X)}\right)=\beta_{0}+\beta_{1} X $$ This monotone transformation is called the log odds or logit transformation of $p(X)$ . For multivariate case : \begin{array}{c} \log \left(\frac{p(X)}{1-p(X)}\right)=\beta_{0}+\beta_{1} X_{1}+\cdots+\beta_{p} X_{p} \\ p(X)=\frac{e^{\beta_{0}+\beta_{1} X_{1}+\cdots+\beta_{p} X_{p}}}{1+e^{\beta_{0}+\beta_{1} X_{1}+\cdots+\beta_{p} X_{p}}} \end{array} Why we say the logistic regression is a linear model ? Short answer : because of the logit function. Long answer : A classifier is linear if its decision boundary on the feature space is a linear function: positive and negative examples are separated by a hyperplane .
