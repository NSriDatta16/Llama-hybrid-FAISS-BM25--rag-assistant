[site]: crossvalidated
[post_id]: 531522
[parent_id]: 531491
[tags]: 
I'm not sure these analogies to psychology / the human brain are really that useful or accurate. It's not like a neural network is truly affected by psychological effects, it's simply several bits of matrix algebra chained together. However, you certainly can observe the following: Dead "neurons": You can end up with weights in a neural network that make it so that the neuron never "activates" again and then no matter what training data you throw at your model, those particular neurons will never do anything, again. This could happen for many reasons, but fitting too aggressively to extreme data could be one scenario. Even if nothing much goes wrong during training, there will typically be some to which this has happened. However, unless you really get to this "dead" state, I believe you should normally be able to recover other behaviors, again given enough training data. Overfitting: You can certainly end up with extremely overfit (to the training data) neural networks that would exhibit poor behaviour (e.g. bias etc.), where it could be really hard to recover a more generalizable model by training more (in contrast, when you are underfit, it is often more realistic to train more and still get something "good"). I'd imagine you could manage to get there by training on a subset of the data, if that is then still a local minimum of the model. You could also try, very hard, to find a really narrow optimum in the loss landscape (broader ones tend to be the ones you really want for good generalization), which might exhibit poor behaviours. However, an non-overfit neural network should generally with more training data be able to get to a decent performance, again, unless it is somehow caught in a really unfortunate local minimum. Assuming that's not the case, then how long it takes to correct issues induced by early training will depend on a few things such as In how bad a place did you end up? The closer we are too good weights and the clearer the path towards better weights is (ideally a steep gradient in an unchanging direction for all weights), the faster and easier this should be. How high is your learning rate influences how fast weights change. Ideally, you get this just right: not so fast that the optimization goes off on a tangent, not so slow that it takes forever to improve. How clear are the signals in the data? How high dimensional is the problem? Is the model inducing useful inductive biases for the problem (e.g. convolutional neural networks are quite a nice fit for vision problems)?
