[site]: crossvalidated
[post_id]: 602273
[parent_id]: 602261
[tags]: 
$\renewcommand{\epsilon}{\varepsilon}$ To clarify, let me first restate your question as follows: Given a response vector $y \in \mathbb{R}^n$ and a design matrix $X \in \mathbb{R}^{n \times p}$ . After fitting $(y, X)$ to a linear model, the residual vector is $\hat{\epsilon} = (I - H)y$ , where $H = X(X'X)^{-1}X'$ is the hat matrix. Show that the $R^2 = 1 - \frac{SSE}{SST}$ of the linear model fitted by $(\hat{\epsilon}, X)$ is $0$ . To prove it, just note that when the response vector is $\hat{\epsilon}$ and design matrix is $X$ , the average of new responses $\hat{\epsilon}_1, \ldots, \hat{\epsilon}_n$ , denoted by $\bar{\hat{\epsilon}}$ , equals to $n^{-1}1'\hat{\epsilon} = 0$ (as the sum of residuals in a regression is $0$ -- under the condition that the regression model contains an intercept), and the new residual vector is $(I - H)\hat{\epsilon} = (I - H)(I - H)y = (I - H)y$ , which coincides the old residual vector, thanks to the idempotency of $I - H$ . It then follows by definitions of $SST$ and $SSE$ that \begin{align} & SST = (\hat{\epsilon} - \bar{\hat{\epsilon}})'(\hat{\epsilon} - \bar{\hat{\epsilon}}) = \hat{\epsilon}'\hat{\epsilon} = y'(I - H)y, \\ & SSE = ((I - H)\hat{\epsilon})'(I - H)\hat{\epsilon} = ((I - H)y)'(I - H)y = y'(I - H)y. \end{align} Therefore, $SST = SSE$ , whence $R^2 = 0$ . Intuitively, since $\hat{\epsilon}$ is orthogonal to the space spanned by columns of $X$ , $X$ does not provide any information to predict $\hat{\epsilon}$ . Therefore, $R^2 = 0$ .
