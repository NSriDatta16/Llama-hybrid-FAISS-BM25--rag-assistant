[site]: datascience
[post_id]: 49307
[parent_id]: 49299
[tags]: 
Here is an animation of fractionally-strided convolution (from this github project ): where the dashed white cells are zero rows/columns padded between the input cells (blue). These animations are visualizations of the mathematical formulas from the article below: A guide to convolution arithmetic for deep learning Here is a quote from the article: Figure [..] helps understand what fractional strides involve: zeros are inserted between input units, which makes the kernel move around at a slower pace than with unit strides [footnote: doing so is inefficient and real-world implementations avoid useless multiplications by zero, but conceptually it is how the transpose of a strided convolution can be thought of.] Also, here is a post on this site asking "What are deconvolutional layers?" which is the same thing. And here are two quotes from a post by Paul-Louis Pröve on different types of convolutions: Transposed Convolutions (a.k.a. deconvolutions or fractionally strided convolutions) and Some sources use the name deconvolution, which is inappropriate because it’s not a deconvolution [..] An actual deconvolution reverts the process of a convolution.
