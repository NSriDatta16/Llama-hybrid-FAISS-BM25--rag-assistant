[site]: crossvalidated
[post_id]: 283631
[parent_id]: 
[tags]: 
How to correctly use validation and test sets for Neural Network training?

I am in the machine learning business for a long time, but still, this fundamental fact gets me confused, since every paper, article and/or book describe different kind of usages for validation and test sets. I aim to set my mind at ease forever on this issue: Let's say I have a relatively small Convolutional Neural Network and I want to train it on MNIST dataset. Naturally, I want to learn the best hyperparameters for the given CNN, like the weight decay coefficient $\lambda$, the learning rate $\alpha$, etc. Naturally, MNIST has 60K training images and 10K test images. The basic practice is allocating 10K of the training set as the validation set for MNIST. Assume that we are doing a grid search on $\lambda$ and $\alpha$ hyperparameters and assume that we have fixed a $(\lambda^*,\alpha^*)$ pair. Then we start training the CNN using SGD, using the 50K training set and measuring the performance on the 10K validation set in the end of each epoch (a complete pass over the 50K training set). Usually we either stop training if a fixed budget of epochs has been depleted or we start to lose accuracy on the validation set. And then, only for a single time in the training process of the CNN with the $(\lambda^*,\alpha^*)$ pair, we measure the real performance on the test set. We pick the one with best performance. This usage is intuitive and does not cause the network to unfairly overfit on the test set. But the problem is, we did not ever used the 10K validation set in the actual training with SGD. So, now, what is the correct way to use the validation set here? After we finish training with the $(\lambda^*,\alpha^*)$ pair, either by running out of allowed epochs or early exiting due to overfit on validation set, should we merge the validation set and training set, and train a few epochs more? At this point, our learning rate may already be very small due to decaying it gradually. Should we train everything from scratch by using the 60K training + validation set, setting the learning rate to its initial value? Another alternative may be to train the CNN with every $(\lambda,\alpha)$ pair in consideration of our grid search, using only 50K training samples in the SGD and 10K validation samples for measuring the accuracy. No interaction with the test set is allowed. Let's assume that after we train with every $(\lambda,\alpha)$ pair, we pick the pair which yields the highest accuracy on the validation set . Then we train the CNN with the picked hyperparameters, from scratch, this time using the 60K training + validation samples directly in SGD, for an amount of fixed epochs. After the training ends, we use the test set once and for all to declare our final accuracy performance. This method, in my mind, would cause the following issue: The hyperparameters we have picked would be the optimal ones for the 50K training samples! Since we have used a smaller subset of the actual training set, most probably the model would more easily overfit, seeing a lower data variation, so our hyperparameter search may tend to find a higher $\lambda$. And with this $\lambda$ we would probably see a lower accuracy on the actual test set with the full 60K training samples, since the model won't be able to compensate for the variation of the extra 10K samples, due to high $\lambda$. So, I am not certain what the ultimately correct way to use the validation-test sets. What would be the valid - logical approach here?
