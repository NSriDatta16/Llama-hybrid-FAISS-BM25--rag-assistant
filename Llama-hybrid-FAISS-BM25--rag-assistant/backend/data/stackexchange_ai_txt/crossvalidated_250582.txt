[site]: crossvalidated
[post_id]: 250582
[parent_id]: 
[tags]: 
UCB bandit reward bound

The original paper of Auer [1] requires bandit arms to have a reward distribution bounded on [0,1] for the Upper-Confidence-Bound (UCB) theorems to hold. I was wondering how strict this requirement is in practice, i.e. does UCB performs well when this reward bound requirement is not met? If this requirement is strict, I'm aware I could attempt to rescale the rewards. I was however not able to find much pointers on how to rescale a floating point (say an IEEE-754 double precision float) properly to a double precision float in [0,1]. With properly, I mean, to retain as much of the features of the reward distribution as possible and to keep the updating of Q values as numerically stable as possible. [1] Auer, Peter, Nicolo Cesa-Bianchi, and Paul Fischer. "Finite-time analysis of the multiarmed bandit problem." Machine learning 47.2-3 (2002): 235-256.
