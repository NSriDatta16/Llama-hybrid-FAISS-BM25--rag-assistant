[site]: crossvalidated
[post_id]: 77597
[parent_id]: 77591
[tags]: 
Basically, you are on the right track. You will find a discussion about the aspect of normality in Normality of dependent variable = normality of residuals? Some assumptions of the classic linear model are indeed about errors (using residuals as realizations of them): Are they uncorrelated? (Relevant for inference and optimality of the OLS-estimators) Do they have equal variance? (Relevant for inference and optimality of the OLS-estimators) Are they centered around 0? (Key assumption to get unbiased estimators and predictions) If the sample is very small: are they normal or at least symmetrically distributed? (Relevant for inference) Other conditions are about "raw data": Are there no gross outliers in regressors? (High leverage observations can destroy the whole model) No perfect multicollinearity? (Would cause computational problems, at least in some software packages) Now, your undergrad teacher might be correct as well: Maybe you were focusing on univariate tests like the one-sample t-test. There, the assumptions are about the raw data. If the $R^2$ is quite low and the response variable looks everything but normal, then the same will most likely also be true for the residuals. How would you check homoscedasticity etc. based on raw data? Maybe you misunderstood him or her.
