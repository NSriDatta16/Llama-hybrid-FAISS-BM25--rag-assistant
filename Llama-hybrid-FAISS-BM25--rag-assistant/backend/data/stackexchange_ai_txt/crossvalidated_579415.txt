[site]: crossvalidated
[post_id]: 579415
[parent_id]: 
[tags]: 
Explanation of the derivation of the analytical gradient for a SVM?

I'm trying to understand how to derive the analytical gradient for a SVM. I know that in a SVM, the loss function is defined as follows: From this blogpost , I know the full loss for each element in the weights matrix, $W$ , is: I know that the above turns into: However, I don't quite understand the final step, which is that if $(x_iw_1 - x_iw_{y_i} + \Delta) > 0$ then we know: $\frac{dL_i}{dw_{11}} = x_{i1}$ My guess is that the intuition is: The derivative of $x_{i1}w_{11}$ with respect to $w_{11}$ is $x_{i1}$ , but I'm not sure. Could someone help clarify what's going on here for me?
