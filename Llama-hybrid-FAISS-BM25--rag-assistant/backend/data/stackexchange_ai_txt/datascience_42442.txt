[site]: datascience
[post_id]: 42442
[parent_id]: 42394
[tags]: 
Yes, in principle this type of feature engineering can help a model and - if the transformation is well-chosen - almost always does in practice. Some models may simply not be able to figure out the kind of transformation that you give them without your help (e.g. a model that uses all predictors only in a linear fashion will not be able to represent non-linear relationshps). But even those that can figure it out (e.g. xgboost or some deep neural network should eventually with enough data manage to represent arbitrarily complex functions of multiple variables) can be helped a lot (in terms of performing better / getting to the same level of performance with less data) by providing them a good transformation that they otherwise need to learn by seeing lots and lots of examples. If you look at the description of their chosen approaches of winners of kaggle competitions, you will see that some kind of clever feature engineering often played a major role.
