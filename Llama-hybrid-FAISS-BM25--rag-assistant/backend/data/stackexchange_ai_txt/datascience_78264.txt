[site]: datascience
[post_id]: 78264
[parent_id]: 78221
[tags]: 
I see two solutions: either you pass a list of dictionnaries to param_grid avoiding irrelevant combinations or you use a single variable in your pipeline for feature_selector__feature__selector_k and classifier__input_shape First solution : you can generate the right list of combinations using something close to this: param_grid = [ { 'feature_selector__feature_selector__score_func' : [f_classif], 'feature_selector__feature_selector__k' : [k], 'classifier__input_shape' : [k], 'classifier__dropout_rate' : [0.0, 0.5] } for k in [7, 9, 15] ] Second solution , you can use a specific class that create your model when fitting based on the shape of X. Here is a code sample: class MyKerasClf(): def predict(self, X): y_pred_nn = self.clf.predict(X) return np.array(y_pred_nn).flatten() def create_model(self, learn_rate = 0.01, weight_constraint = 0 ): model = Sequential () model.add (Dense (units = 64, activation = 'relu', input_shape = (self.input_shape, ))) model.add (Dropout (self.dropout_rate)) model.add (Dense (32, activation = 'relu')) model.add (Dense (1, activation = 'sigmoid')) model.compile (loss = 'binary_crossentropy', optimizer = Adam (lr = learn_rate), metrics = ['accuracy']) return model def fit(self, X, y, **kwargs): self.input_shape = X.shape[1] self.clf = KerasClassifier(build_fn = self.create_model, verbose = 2) self.clf.fit(X, y, **kwargs) def set_params(self, **params): if 'dropout_rate' in params: self.dropout_rate = params['dropout_rate'] else: self.dropout_rate = 0.0 Then you can use the class in your pipeline X, y = make_classification(n_features=50, n_redundant=0, n_informative=2, random_state=42, n_clusters_per_class=1) my_scaler = StandardScaler () steps = list () steps.append (('scaler', my_scaler)) standard_scaler_transformer = Pipeline (steps) my_feature_selector = SelectKBest () steps = list () steps.append (('feature_selector', my_feature_selector)) feature_selector_transformer = Pipeline (steps) # Create a specific clf my_clf = MyKerasClf( ) pip_clf = Pipeline (steps = [('scaler', my_scaler), ('feature_selector', feature_selector_transformer), ('classifier', my_clf)], verbose = True) param_grid = {'feature_selector__feature_selector__score_func' : [f_classif], 'feature_selector__feature_selector__k' : [7, 15], 'classifier__dropout_rate' : [0.0, 0.5] } cv = RepeatedStratifiedKFold (n_splits = 5, n_repeats = 1, random_state = 42) grid = GridSearchCV (estimator = pip_clf, param_grid = param_grid, scoring = 'f1', verbose = 1, n_jobs = 1, cv = cv) grid_result = grid.fit(X, y) Note : I although added the dropout to be tested in the gridsearch as an example.
