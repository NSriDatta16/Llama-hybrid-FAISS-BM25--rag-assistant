[site]: crossvalidated
[post_id]: 20963
[parent_id]: 20956
[tags]: 
(classical) Mahalanobis distances cannot be used to find outliers in data because the Mahalanobis distance themselves are sensitive to outliers (i.e., they will always by construction sum to $(n-1)\times p$ , the product of the dimensions of your dataset). The recommended solution depends on what you mean by 'high dimensional'. Denoting $p$ the number of variables and $n$ the number of observations, broadly, for $10\leq p \leq 100$ , the current state of the art approach for outlier detection in high dimensional setting is the OGK algorithm of which many implementations exists. The numerical complexity of the OGK in $p$ is $\mathcal{O}(np^2+p^3)$ , very close to the theoretical lower bound for this problem because methods based on convex loss functions cannot reliable detect outliers if there are more than $n/p$ of them. If your number of dimensions is much larger than the 100's, you will have to first do a (robust) dimension reduction of your data-set. Here, the current state of the art methods is the PCA-grid approach , which again, is well implemented , see also here for direct estimation of the co-variance matrix. An alternative procedure is the ROBPCA approach , which is, again, well implemented . Notice that neither of the three procedures requires $n$ to be larger than $p$ . The last two methods have complexity in the order of $\mathcal{O}(nk^3)$ where $k$ is the number of components one wishes to retains (which in high dimensional data is often much smaller than $p$ ). Relative merits: The big advantage of ROBPCA is its computational efficiency in high dimensions. The big advantage of PCA-Grid is that it can perform both robust estimation and hard variable selection (in the sense of giving exactly 0 weight to a subset of the variables) see here for a link and the sPCAgrid() function in the pcaPP R package.
