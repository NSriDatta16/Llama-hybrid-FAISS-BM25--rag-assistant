[site]: crossvalidated
[post_id]: 223589
[parent_id]: 
[tags]: 
Error Backpropagation, Christopher Bishop “Pattern Recognition and Machine Learning”

I'm trying to understand the description of the error backpropagation algorithm as explained in Christopher Bishop's book, in particular, section 5.3.1 "Evaluation of error-function derivative". The thing that really bothers me is Eq. (5.54) for the derivative of the cost function with respect to the weights the activations of the output layer: $\delta_k(:= \frac{\text{d}E_n}{\text{d}a_k}) = y_k - t_k$ Is it implicitly based on the assumption of linearity for the output units? It's not mentioned anywhere explicitly, and I cannot explain the absence of the derivative of the activation function of the output neurons in Eq. (5.54).
