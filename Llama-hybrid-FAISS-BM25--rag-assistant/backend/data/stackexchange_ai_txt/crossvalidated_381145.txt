[site]: crossvalidated
[post_id]: 381145
[parent_id]: 380965
[tags]: 
So I think there are a couple of options, but they may depend on how much data you have. You could try a model in the random forest family, preferably with boot-strap aggregation (i.e., "bagging"). Random forest models and other decision-tree rooted (pun intended) models and their extensions (e.g., gradient boosting machines), can handle 0,1 outcome data, but testing the predictive utility of these models requires that you have a large enough data set that you can split your data into training and testing sets to evaluate the effects of changes you make to the hyperparameters of the model (i.e., number of trees, interaction depth, learning rate if applicable). Given that you are largely interested in effective predictions from your model, you are almost certainly going to have to perform a testing/training split no matter the approach you settle on. With a logistic regression, you have to specify the model up front that you think best accounts for your log odds (i.e., logits - what the model transforms your probability of purchase variable into). You can specify all interactions between your predictors in your model upfront, and really your goal here is just to maximize the model's ability to predict purchase rates this year. I would recommend trying both approaches and seeing which is more accurate... but accuracy is going to be your problem. You do not have a 2018 purchase variable it looks like. At least I do not see one in the data above. Therefore, there is no way to test if your model performs well given observed data in 2018. This is a common problem in forecasting, we know very little about future scores in a data set and therefore have to make some assumptions (i.e., trends continue, trends level off, there is a periodic component, etc.). In your case, I would say the best option would be to fit a model to 2017 data, optimize it for prediction that year by comparing a trained model against a held-out testing sample (i.e., data that the model has not yet "seen"), and then predict 2018 using the same variables shifted one year up (if they were measured yearly). Because your data appears nested within-subject, you also have the option of conducting a generalized linear mixed effects model (in your case an extension of your logistic regression but with random effects). In R, the base stats package provides you a glm function that can be used to fit a straightforward logistic regression. The randomForest and gbm libraries are good places to start for the use of more advanced decision tree models. Finally, the lme4 package has a glmer function that you can use to create mixed effects model for your data. I would say that this last option is going to be the most difficult to apply to your current situation as the data would need to be reshaped (stacked as opposed to wide) and you will have to think more carefully about how to implement training/testing splits.
