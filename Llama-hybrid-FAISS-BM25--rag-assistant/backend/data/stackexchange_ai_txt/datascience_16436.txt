[site]: datascience
[post_id]: 16436
[parent_id]: 16370
[tags]: 
I think that in order to understand how the SVM handles the new synthetic data, you should look at the loss function SVM uses, i.e. hinge loss and the behavior on an imbalanced dataset. Intuitively this function will try to fit the hyperplane that best separates the data. For example imagine you have a dataset that is not linearly separable and you have to classes A (with a 1000 samples) and B (with 50 samples). So the SVM will "move" the hyperplane to classify right all samples of A even if this means failing to classify correctly some samples of class B.Then when you create new samples of the classes B the classifier will try to find a balance between the error of the both classes. A more detailed explanation can be found in the 6.3 section fo this article In general all classifiers tend to classify correctly the most dominating class because of the loss function they use. One way of dealing with it is generating synthetic data, other way is using a cost sensitive classifier, this is the cost of misclassification of the minority class is higher than the majority class. More details can be found in the following links: https://svds.com/learning-imbalanced-classes/ http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/
