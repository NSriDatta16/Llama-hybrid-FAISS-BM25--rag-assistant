[site]: crossvalidated
[post_id]: 258638
[parent_id]: 258572
[tags]: 
Normalizing your data may help for faster convergence. Take a look to this paper: http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf In your case for faster convergence you should probably take these basic steps: normalize your data use hyperbolic tangent as activation function (and set the values of the outputs to [-1,1]) be careful with correlation of your inputs Anyway, these will only make that the training of your neural network is faster, they will not give you better results. From your description, you seem to have a problem with variance (overfitting). When you make your network more complex by adding more layers to it, you are addressing bias, not variance. You could try these basic steps for variance: use regularization try with a smaller subset of your features When you fix your variance problems, then you can try to make your neural network more complex, in particular if you apply regularization. For doing that effectively, you will need to take care of convergence as well. Another thing to take into account, although you don't mention it and maybe you're doing it already: since it is a classification problem, use cross-entropy for the error (not MSE). Hope it is clear and it helps.
