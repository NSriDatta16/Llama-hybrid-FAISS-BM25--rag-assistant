[site]: crossvalidated
[post_id]: 356127
[parent_id]: 
[tags]: 
Clarification of the intuition behind backpropagation

I've been taking some time to try and understand the computations and mechanics of the machine learning algorithms I use in my day to day life. Studying the backpropagation literature on the CS231n course, I want to make sure that I have understood the chain rule correctly before continuing my study. Say I have the sigmoid function: $$\sigma(x) = \frac{1}{1+e^{-x}}$$ in this case, $x=w0x0+w1x1+w2$ We can write this function as a computational graph (Ignoring the coloured values for now): We can group the modularised nodes for computing the gradient of the sigmoid $w.r.t.$ its input into a single derivation: $$\frac{d\sigma(x)}{d x}=(1 - \sigma(x))\sigma(x)$$ First, we perform forward propogation to obtain the outputs at each unit: w = [2,-3,-3] x = [-1, -2] # Compute the forward pass product = [w[0]*x[0]+w[1]*x[1]+w[2]] activation = 1 / 1 + math.exp(-product) To calculate the gradient of the activation, we can use the above formula: grad_product = (1 - activation) * activation Where I feel I may be getting confused, or, at least less intuitive, is calculating the gradient for x and w : grad_x = [w[0] * activation + w[2] * activation] grad_w = [x[0] * activation + x[1] * activation + 1 * activation] More concretely, I'm confused as to why we apply 1 * activation when calculating the gradient $w.r.t.$ w. It may help the reader spot my theoretical difficulty if I try to reason the calculations of both x and w's gradients... The gradient of each $x_i$ is given by the corresponding $w_i$ under the rule of multiplication: if $f(x,y) = f(xy)$ then $\frac{\partial f}{\partial x}=y$. Then, using the chain rule, we multiply these local gradients by the gradient of the successive node (for each path of $x$) to obtain its gradient w.r.t. the function output. This explains the computation for computing $\partial x$. The gradient of $w_i$ is given in the exact same (inverse) way as explained above with the additional 1 * activation . I believe this additional expression is coming from $w_2$? The local gradient of an addition unit is always 1 for all inputs and the multiplication with activation is a result of chaining the gradient to the function's output? I'm partially confident with my current understanding but would appreciate if someone could clarify my current intuition on the computations involved in computing gradients.
