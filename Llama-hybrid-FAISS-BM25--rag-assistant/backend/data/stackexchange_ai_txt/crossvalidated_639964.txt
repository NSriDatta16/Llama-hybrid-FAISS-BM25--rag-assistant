[site]: crossvalidated
[post_id]: 639964
[parent_id]: 
[tags]: 
Why are numbers tokenized and/or embedded before their input into transformers?

For all transformer models that are either trained for text tasks or specialized for time series or simple equation solving, they tokenize and embed numbers rather than using their raw representation. Why is that?
