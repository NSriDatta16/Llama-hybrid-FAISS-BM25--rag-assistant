[site]: datascience
[post_id]: 121069
[parent_id]: 121003
[tags]: 
From my experience in real-world data, I have never seen consistent domains in which resample techniques improve the model's performance. Remember that one of the main assumptions of learning is that training data is the same system generation that test data, i.e. they both have the same distribution, which holds not true when applying resampling. Instead, I would go for cost sensitive learning so that we penalise most of the minority class cases. Im sharing an example of data in which SMOTE showed a slight increase across different metrics. import pandas as pd from sklearn.model_selection import train_test_split from imblearn.over_sampling import SMOTE from lightgbm import LGBMClassifier from sklearn.metrics import classification_report import urllib.request # Load the dataset from a URL url = 'https://storage.googleapis.com/download.tensorflow.org/data/creditcard.csv' filename = 'creditcard.csv' urllib.request.urlretrieve(url, filename) # Load the dataset into a Pandas dataframe df = pd.read_csv(filename) # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(df.drop('Class', axis=1), df['Class'], test_size=0.3, random_state=42) # Perform SMOTE oversampling on the training set smote = SMOTE(random_state=42) X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train) # Train a classifier on the resampled training set rfc = LGBMClassifier(random_state=42).fit(X_train_resampled, y_train_resampled) # Evaluate the classifier on the original testing set y_pred = rfc.predict(X_test) print(classification_report(y_test, y_pred)) # Train a classifier on the original training set rfc = LGBMClassifier(random_state=42).fit(X_train, y_train) # Evaluate the classifier on the original testing set y_pred = rfc.predict(X_test) print(classification_report(y_test, y_pred)) Hope it helps!
