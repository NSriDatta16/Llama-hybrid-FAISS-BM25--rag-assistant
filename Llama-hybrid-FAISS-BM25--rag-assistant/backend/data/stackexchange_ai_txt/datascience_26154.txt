[site]: datascience
[post_id]: 26154
[parent_id]: 26153
[tags]: 
TL;DR: It is a convention to convert categorical features to numeric features before you can use them in regression, or any other machine learning algorithm for that matter. Technically , there is nothing that is stopping ML algorithms from working with categorical features but their software implementation would be prohibitively expensive, hence this convention in ML practice. Turning to your problem, if you are trying to convert categorical label values into numerical values, there are various methods but it appears the best one for your data would be One-Hot encoder. Both Scikit-Learn and PySpark , and most other libraries provide handy functions for it, as it is very common operation. For example: from sklearn.preprocessing import OneHotEncoder one_hot = OneHotEncoder() one_hot.fit(...your_columns...) Once you have all data in numeric form, you can use pretty much any algorithm. As for the last figure, it is scatter plot between x and y, and it is not obvious what x and y are. Weirdly enough, it is not one-on-one relationship because a given value of x seems to produce multiple value of y?!!! And I am not sure what you are plotting in earlier figures that is giving you straight lines!
