[site]: crossvalidated
[post_id]: 258404
[parent_id]: 
[tags]: 
How can the ICC(1,1) reliability of group A possibly be higher than group B?

I have calculated the ICC(1,1) reliability of my groups A and B over 2 trials using SPSS (syntax below), and found (with 95% CI's): ICC(1,1) Group A = .832 [.766, .881] ICC(1,1) Group B = .801 [.725, .858] RELIABILITY /VARIABLES=trial1 trial2 /SCALE('ALL VARIABLES') ALL /MODEL=ALPHA /ICC=MODEL(ONEWAY) CIN=95 TESTVAL=0. I am struggling to understand how the reliability from trial 1 to 2 could possibly be higher for Group A than for Group B of my dataset. It seems counter-intuitive since (1) Group A scores are on average 0.5 higher on trial 2 than trial 1 (a bulk shift that ICC model 1 should pick up since we treat raters, that is, trials, as random - and are checking for absolute agreement not just consistency), while on average Group B scores remain the same, and (2) the degree of change in scores from trial 1 to 2 seems lesser in general (as well as being more central) for Group B than for Group A (see histograms of trial-1-to-2 difference scores below). After inspecting the data, the idea that the ICC(1,1) reliability as I've calculated it is higher for group A than B makes me wonder if I'm doing it wrong... and if not, where is the hidden variance in group B scores?
