[site]: crossvalidated
[post_id]: 593807
[parent_id]: 579088
[tags]: 
You do NOT need to pad inputs and outputs to have the same length. The interaction between input and output occurs when you compute attention in the second sublayer of the decoder: $Q(K^TV)$ , where $\begin{equation} Q \in \mathbb{R}^{d_{output} \times d_{model}} \\ K \in \mathbb{R}^{d_{input} \times d_{model}} \\ V \in \mathbb{R}^{d_{input} \times d_{model}} \\ \end{equation}$ $K$ and $V$ are from the encoder (inputs) and will produce a $d_{model} \times d_{model}$ matrix that you can multiply with $Q$ from the decoder.
