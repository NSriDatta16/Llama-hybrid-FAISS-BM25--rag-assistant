[site]: crossvalidated
[post_id]: 313617
[parent_id]: 
[tags]: 
Is the invariance property of the ML estimator nonsensical from a Bayesian perspective?

Casella and Berger state the invariance property of the ML estimator as follows: However, it seems to me that they define the "likelihood" of $\eta$ in a completely ad hoc and nonsensical way: If I apply basic rules of probability theory to the simple case wheter $\eta=\tau(\theta)=\theta^2$, I instead get the following: $$L(\eta|x)=p(x|\theta^2=\eta)=p(x|\theta = -\sqrt \eta \lor \theta = \sqrt \eta)=:p(x|A \lor B)$$ Now applying Bayes theorem, and then the fact that $A$ and $B$ are mutually exclusive so that we can apply the sum rule: $$p(x|A\lor B)=p(x)\frac {p(A\lor B|x)}{p(A\lor B)}=p(x|A\lor B)=p(x)\frac {p(A|x)+p(B|x)}{p(A)+p(B)}$$ Now applying Bayes' theorem to the terms in the numerator again: $$p(x)\frac {p(A)\frac {p(x|A)}{p(x)}+p(B)\frac {p(x|B)}{p(x)}}{p(A)+p(B)}=\frac {p(A)p(x|A)+p(B)p(x|B)}{p(A)+p(B)}$$ If we want to maximize this w.r.t to $\eta$ in order to get the maximum likelihood estimate of $\eta$, we have to maximize: $$p_\theta(-\sqrt \eta)p(x|\theta = -\sqrt \eta)+p_\theta(\sqrt \eta)p(x|\theta = \sqrt \eta)$$ Does Bayes strike again? Is Casella & Berger wrong? Or am I wrong?
