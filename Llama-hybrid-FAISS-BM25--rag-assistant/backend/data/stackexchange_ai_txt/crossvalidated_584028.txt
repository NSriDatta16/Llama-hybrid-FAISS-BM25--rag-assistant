[site]: crossvalidated
[post_id]: 584028
[parent_id]: 583925
[tags]: 
Dimensionality reduction techniques such PCA occupy a somewhat unusual position, as John Madden points out. It's fair to think of them as algebraic building blocks to be used in statistical and machine learning methods. But setting aside PCA for the moment, it's perfectly reasonable for two different frameworks to depict the same entity. It's also legitimate to create new frameworks that ignore older formulations, even if there's nothing wrong with the older one. Machine learning frames methods such as regression in terms of supervised or unsupervised learning while statistics does not. One can argue about whether this framing is useful or not, but neither field gets to deny the other the right to frame ideas in its own way. Statistics may have come up with some methods before machine learning, but priority is rarely worth getting worked up about. Plenty of ideas are credited to people who did not come up with them first; it's unfair to the originators but science progresses nonetheless. At worst, there's time wasted reinventing the wheel when a newer field is unaware of the older field's literature. This used to be the case with machine learning and statistics but seems to be less of an issue nowadays; both fields have been learning from each other more. TL;DR There's no sharp boundary between the two fields. List PCA wherever it helps you communicate your message to your intended audience.
