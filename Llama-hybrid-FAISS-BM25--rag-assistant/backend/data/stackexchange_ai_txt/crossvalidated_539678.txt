[site]: crossvalidated
[post_id]: 539678
[parent_id]: 539676
[tags]: 
For each variable containing missing values, add an indicator variable for whether this variable is missing. Then, impute the missing variable values in whatever way you desire (good imputation can help but not necessary). This captures all the information in the problem and is usually better than simply naively imputing (which might weaken the signal in the variable). Adding the missingness indicator variables mimics dropping the rows that have missing observations. If the best fit is obtained by dropping these observations, a flexible machine learning algorithm can do so adaptively by leveraging the indicator variable. However, the machine-learning algorithm might decide to utilize all observations including those that are missing. This also allows the ML algorithm to leverage the missing/imputed values for some variables and to not leverage them for the other variables. The addition of good imputation of the missing values allows the machine-learning to leverage an (imputed) signal from the missing values. A flexible ML algorithm would intrinsically be imputing these values anyway but imputing as a preprocessing step makes its job easier (and also allows less flexible ML algorithms to work well). The -999 approach is not terrible if your ML algorithm is very flexible. Because the missing values are all grouped up, the ML algorithm can isolate these values when fitting. However, if you do something like linear regression/GAM, this will likely perform quite poorly. This also does not allow you to leverage imputation, which would allow glm/gam to work well.
