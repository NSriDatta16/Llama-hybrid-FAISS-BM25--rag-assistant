[site]: datascience
[post_id]: 82448
[parent_id]: 82397
[tags]: 
By normalizing the data the code is not likely to get negatively affected. This is because the network doesn't know a priori that images are being used as input data, it only receives a set of numeric values (which can represent any type of data) and finds the appropiate values of weights and biases that decrease our cost function. Thereby even if we get negative values as the new input data, that doesn't mean that this preprocessing is going to cause a bad effect. But, why normalization may be useful? The useful thing about Z-score normalization here is that by doing it, all the "new pixels" have a value with zero-mean and the same variance. This is a good thing as explains Yann LeCun in his paper " Efficient Backprop " (page 8): Why use data with features having a mean close to zero: Convergence is usually faster if the average of each input variable over the training set is close to zero... ....When all of the components of an input vector are positive, all of the updates of weights that feed into a node will be the same sign. As a result, these weights can only all decrease or all increase together for a given input pattern. Thus, if a weight vector must change direction it can only do so by zigzagging which is inefficient and thus very slow The expression that he is refering to is the one that is used to update the value of the weights of the first layer, which is given by: $$ \frac{\partial C}{\partial w_{jk}^{l=1}} = \delta_j^{l=1}x_k$$ Where $x_k$ represents an input pixel value and $w_{jk}^l$ a weight that connects that input $k$ to the neuron $j$ of the first layer ( $l=1$ ). So, given the scalar term $\delta_j^{l=1}$ it's clear that all the weights that connect the input layer to the first layer of neurons will be updated in the same direction if all $x_k$ are positive, as the quote explains. Why use data with features having the same variance?: Scaling speeds learning because it helps to balance out the rate at which the weights connected to the input nodes learn. As we saw earlier, the expression that gives the updates on the weights $w_{jk}^{l=1}$ are proportional to the input $x_k$ . So if e.g. $x_1$ samples have bigger values than $x_2$ samples, then the updates on $w_{j1}$ on $w_{j2}$ may not be balanced and therefore the updates are made with different rythms on both parameters. Just a side note $\rightarrow$ The method of dividing each feature (each pixel intensity) by their maximum value is not a Z-score normalization but is another way of normalizing the data (in order to use the Z-score, the quantity that we would have to use to divide the features is the standard deviation of each pixel throughout all the samples).
