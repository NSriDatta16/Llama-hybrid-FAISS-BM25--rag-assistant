[site]: datascience
[post_id]: 8753
[parent_id]: 
[tags]: 
What is a better input for Word2Vec?

This is more like a general NLP question. What is the appropriate input to train a word embedding namely Word2Vec? Should all sentences belonging to an article be a separate document in a corpus? Or should each article be a document in said corpus? This is just an example using python and gensim. Corpus split by sentence: SentenceCorpus = [["first", "sentence", "of", "the", "first", "article."], ["second", "sentence", "of", "the", "first", "article."], ["first", "sentence", "of", "the", "second", "article."], ["second", "sentence", "of", "the", "second", "article."]] Corpus split by article: ArticleCorpus = [["first", "sentence", "of", "the", "first", "article.", "second", "sentence", "of", "the", "first", "article."], ["first", "sentence", "of", "the", "second", "article.", "second", "sentence", "of", "the", "second", "article."]] Training Word2Vec in Python: from gensim.models import Word2Vec wikiWord2Vec = Word2Vec(ArticleCorpus)
