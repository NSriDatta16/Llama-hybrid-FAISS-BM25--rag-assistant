[site]: crossvalidated
[post_id]: 425086
[parent_id]: 320085
[tags]: 
It depends on your model and the data. You can calculate the median globally if you have all the data at hand anyway. For example, in a Kaggle competition, you have training and test set fixed and usually at hand. So, you can calculate the median of the features for the whole dataset at once. It might improve your competition-score, which in this case may be seen as the purpose of the model. Though, that does not work if new data is coming in, because you cannot calculate the median of data that you don't yet have. So, a model that has to evaluate new samples (to which you did not have access to during the time of the model building process) should be built and validated with that in mind. Here, it is important to validate the model with the same method as it is being used or tested. So, if you do cross-validation you should use the same technique as in production: That means, in this case, you would calculate the median for each training fold and impute those values in training and test fold. As well as you would calculate the median for the whole training set and impute it into the test set. On the other hand, you could have a time series where you calculate a rolling median over a given time span. I am thinking about a data stream where you have access to all data, but dependent on time. Here you could impute in a time-dependent manner e.g. imputing the median of the past 24h. Here the imputed values are calculated from the training set and test set mainly independently - with a little overlap in the timespan where training and test set are close to each other. The leakage problem is more concerning when data leaks from the test or validation set into the training set. Or from the validation fold into the training fold. You may end up with overly optimistic validation/testing scores, but in production, the model does not work. Using medians from the training set in the test set is not problematic at all.
