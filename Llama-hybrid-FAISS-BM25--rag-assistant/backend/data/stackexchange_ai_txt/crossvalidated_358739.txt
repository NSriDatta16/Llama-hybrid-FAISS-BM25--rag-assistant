[site]: crossvalidated
[post_id]: 358739
[parent_id]: 
[tags]: 
Retrain random forest with important variables

So I have a classification problem with around 2000 predictors. First I run a random forest model to get important variables. Then I only use those variables (let say the top 30) to run the model again. I got a substantial improvement in cross-validation accuracy and AUC. I am just wondering whether it is a good practice and there is any theoretical background behind this?
