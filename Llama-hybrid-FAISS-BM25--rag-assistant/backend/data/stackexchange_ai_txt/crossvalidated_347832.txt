[site]: crossvalidated
[post_id]: 347832
[parent_id]: 347826
[tags]: 
It's valid in the sense that you can do it. In fact the most basic sequence to sequence translation / autoencoder models do the same thing -- encoding a variable length sequence in a fixed size latent space. It's not too different from setting a video encoder to produce a fixed size output. You are correct that too much variation might produce weird results. This is usually mitigated by attention mechanisms. Alternatively, with a sequence to (latent) sequence to sequence model, you could produce a variable length latent space.
