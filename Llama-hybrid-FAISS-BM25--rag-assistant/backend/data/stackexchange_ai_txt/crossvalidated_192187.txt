[site]: crossvalidated
[post_id]: 192187
[parent_id]: 
[tags]: 
Neural networks with complex weights

I am currently wishing to give a neural network starting weights with complex values (because of the nature of the specific task I am working with). I was trying to use the standard neural net libraries from scikit learn and the optimization functions it uses do not work with complex numbers. Thus I am confronting the following problem: Optimize a function numerically $f(z)$ where $z \in \mathbb{C} $ . I am doing this via numerical packages (specifically scipy in python) and I have noticed that all the optimization methods in this package are tailored to only functions of domain in $\mathbb{R}$. I Played around for a bit with the complex optimization problem and at first sight it just seems like numerically optimizing a function of two variables since $z = (x,y) \equiv z = x + iy$ . I am aware that the definition of an analytic function is more rigorous than that of a real differentiable function in the sense that it should be differential from all directions $\Delta z = (\Delta x, \Delta y)$ so things become a bit more complicated. Given the situation above how can one approach the task of numerical optimization on a function of complex domain? Are there any standard routines for this ? Are there any works/ standard methods for neural networks and gradient descent with complex weights ?
