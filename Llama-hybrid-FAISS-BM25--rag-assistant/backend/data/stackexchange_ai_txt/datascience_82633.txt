[site]: datascience
[post_id]: 82633
[parent_id]: 82582
[tags]: 
In the case of Random Forest, a new tree is built without any input from the previously built trees. If the number of trees built is high, dropping any one tree when making a decision won't affect the final output of the random forest model unless the dropped tree holds information about an extreme outlier that impacts the ensemble model. In the case of Boosting, the output of the trees are aggregated in the following fashion: $f^1(x) = f^{0}(x)+\theta_1\phi_1(x)$ $f^2(x) = f^{0}(x)+\theta_1\phi_1(x) + \theta_2\phi_2(x) = f^{1}(x)+\theta_2\phi_2(x)$ $f^2(x) = f^{0}(x)+\theta_1\phi_1(x) + \theta_2\phi_2(x) +\theta_3\phi_3(x) = f^{2}(x)+\theta_3\phi_3(x)$ ... $f^n(x) = f^{(n-1)}(x)+\theta_m\phi_m(x)$ where $f^0(x)$ is an initial guess, $f^i(x)$ is the function learnt by the ensemble with $i$ trees, $\phi_i(x)$ is the $i$ -th tree, $\theta_i$ is the $i$ -th weight associated with the $i$ -th tree and tree $\phi_i$ is learnt based on the error made by $f^{i-1}(x)$ . How the tree removal impacts the ensemble model depends on the function $f(x)$ you are trying to learn. Here are 2 simplified scenarios: If $f(x)$ is simple enough that the $f^1(x)$ is able to capture $f(x)$ from the first tree $\phi^1(x)$ , the subsequent trees will add little value to the ensemble model. In that case, you may not see any noticeable drop in performance if you drop the final trees. If $f(x)$ is complex, then dropping $f^1(x)$ from the ensemble model will noticeably impact the ensemble model's performance. In this setting when $n$ is large, the $n$ -th tree might add little value to the ensemble model.
