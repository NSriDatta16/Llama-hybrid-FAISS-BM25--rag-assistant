[site]: datascience
[post_id]: 120374
[parent_id]: 
[tags]: 
How do GPT models go from token probabilities to textual outputs?

Suppose GPT-2 or GPT-3 is trying to generate the next token, and it has a probability distribution (after applying softmax to some output logits) for the different possible next tokens. How does it choose what token to use in its textual output? The GPT-2 paper mentions top-k random sampling (citing " Hierarchical Neural Story Generation ") and never mentions beam search. The GPT-3 paper mentions nucleus sampling (citing " The Curious Case of Neural Text Degeneration ") and mentions beam search (citing " Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer ").
