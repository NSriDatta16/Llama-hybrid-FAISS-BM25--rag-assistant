[site]: crossvalidated
[post_id]: 65886
[parent_id]: 
[tags]: 
Confusion related to HMM

I was reading this book related to Pattern Recognition and Machine learning by Bishop. However, I have a confusion related to a derivation In the book when they calculate the marginal posterior of the hidden variables, I am ok up to. But in the page attached, I am confused what they mean by conditional probability of $z_{nk} = 1$ and they have $\gamma(z_{nk}) = \sum_{z}\gamma(z) z_{nk}$ what does z signify here? Lets say I have an example with z=[z1...zN] does that mean expectation over all possible combination of z lets say I have 3 possible values for each latent variable. So it means N^3 is it? I am confused any clarifications? Also I am confused about the maximization part During maximization they calculate the $\pi_{k}$ as $\pi_{k} = \frac{\gamma(z_{1k})}{\sum_{j=1}^{K} \gamma{z_{(1j)}}}$. Isn't the sum of the lower part equal to 1? So it doesn't change anything. Suggestions?
