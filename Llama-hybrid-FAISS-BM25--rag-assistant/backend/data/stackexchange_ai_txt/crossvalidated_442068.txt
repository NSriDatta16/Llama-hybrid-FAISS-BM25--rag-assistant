[site]: crossvalidated
[post_id]: 442068
[parent_id]: 
[tags]: 
Why am I getting 100% accuracy in XGBoostClassifier?

So I am doing some predictive analytics on banking information and the target variable is whether or not a member has a checking. I printed the correlations between each variable to the HasChecking variable, and then removed collinear variables to reduce the impact on my model (this includes removing any Checking related variables). This is a list of my chosen features, and the code below is how I got to them. 'CD_Balance', 'Vehicle_Balance', 'HasCreditCard', 'HasCD', 'CreditCard_CurrentMonthTransAmt', 'FICO', 'NearestBranchDistance', 'Saving_Balance', 'HasVehicle', 'Service_SavingNSF/CourtesyTransCnt', 'HouseholdIncomeDesc', 'CreditCard_CurrentMonthTransCnt', 'HasSaving', 'Vehicle_CurrentMonthTransCnt', 'CD_Count', 'Saving_CurrentMonthTransCnt', 'TotalLoanDelinquents', 'CreditCard_Balance' Collinearity Pruning # Select target variable targetVariable = "HasChecking" # Service_SavingNSF/CourtesyTransCnt leads to a 100% accuracy rate and that is not possible dropColumns = [targetVariable, "MemberMPI", "Tenure"]#, "Service_SavingNSF/CourtesyTransCnt"] # Grab the correlations and print them for HasChecking dfCorr = df.corr() dfCorr.sort_values([targetVariable], ascending=False, inplace=True) print(dfCorr[targetVariable]) dfCorrelationTable = dfCorr.HasChecking.to_frame() dfCorrelationTable = dfCorrelationTable.reset_index() dfCorrelationTable.columns = ["ColumnName", "Correlation"] collinearThreshold = .90 # Get only features that have a magnitude .15 correlation or higher) dfCorrelationTable = [col for col in ','.join( dfCorrelationTable[ (abs(dfCorrelationTable["Correlation"]) > .10) & (dfCorrelationTable["Correlation"] collinearThreshold): lowerCol = col if abs(dfCorr[targetVariable][col]) Now, my model runs decently without the input feature Service_SavingNSF/CourtesyTransCnt . But when I add in Service_SavingNSF/CourtesyTransCnt , the accuracy is impossibly high. What are some issues I may have missed? I also left my training/testing code down below as well. My first thought is multicollinearity, but I thought ML algorithms aren't too affected by that. Resample and Training from sklearn.utils import resample from sklearn.metrics import roc_auc_score from sklearn.metrics import confusion_matrix from sklearn.metrics import classification_report # Grab which boolean has more samples so we can do resampling majorityBoolean = 0 if len(dfInputData.loc[df[targetVariable] == 0]) >= len(dfInputData.loc[df[targetVariable] == 1]) else 1 df_majority = dfInputData.loc[dfInputData[targetVariable] == majorityBoolean] df_minority = dfInputData.loc[dfInputData[targetVariable] != majorityBoolean] # Resample the majority down to minority df_majority_downsampled = resample(df_majority, replace=False, n_samples=len(df_minority), random_state=45) df_downsampled = pd.concat([df_majority_downsampled, df_minority]) #filteringDf = dfInputData[np.logical_not(dfInputData.MemberMPI.isin(df_downsampled.MemberMPI))] df_downsampled = pd.concat([df_majority.iloc[0:int(len(df_majority)*.25),:], df_minority.iloc[0:int(len(df_majority)*.25),:]]) X_train, X_test, y_train, y_test = train_test_split(df_downsampled.drop(dropColumns, axis=1).as_matrix(), df_downsampled[targetVariable], test_size=0.20) X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.20) xgb = XGBClassifier(learning_rate=0.0732595425042909, max_depth=8, n_estimators=181, subsample=0.1554808063285612) xgb = xgb.fit(X_train, y_train) pscore = xgb.predict_proba(X_val)[:,1] preds = xgb.predict(X_val) print(classification_report(y_val, preds)) print("ROC AUC Score Validation = " + str(roc_auc_score(y_val, pscore))) print("Validation Accuracy = " + str(xgb.score(X_val, y_val))) print() preds = xgb.predict(X_test) pscore = xgb.predict_proba(X_test)[:,1] print(classification_report(y_test, preds)) print("ROC AUC Score Test = " + str(roc_auc_score(y_test, pscore))) print("Test Accuracy = " + str(xgb.score(X_test, y_test)))
