[site]: datascience
[post_id]: 40130
[parent_id]: 
[tags]: 
PySpark: java.io.EOFException

System: 1 name node, 4 cores, 16 GB RAM 1 master node, 4 cores, 16 GB RAM 6 data nodes, 4 cores, 16 GB RAM each 6 worker nodes, 4 cores, 16 GB RAM each around 5 Terabytes of storage space The data nodes and worker nodes exist on the same 6 machines and the name node and master node exist on the same machine. In our docker compose, we have 6 GB set for the master, 8 GB set for name node, 6 GB set for the workers, and 8 GB set for the data nodes. I have 2 rdds which I am calculating the cartesian product of, applying a function I wrote to it, and then storing the data in Hadoop as parquet tables. After around 180k parquet tables written to Hadoop, the python worker unexpectedly crashes due to EOFException in Java. conf = SparkConf().setAppName( "TBG Input Creation App").setMaster("spark://master:7077").setAll( [('spark.executor.memory', '6g'), ('spark.driver.memory', '4g'), ('spark.executor.heartbeatInterval', '3s'), ('spark.driver.extraJavaOptions', '-XX:+UseG1GC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps'), ('spark.executor.extraJavaOptions', '-XX:+UseG1GC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps')]) rdd_cart = rdd.cartesian(rdd2) rdd.unpersist() rdd2.unpersist() rdd_cart.foreach(lambda row: calc_model(row, fields, vfp_fields)) Then inside the calc_model function, I write out the parquet table. After the crash, I can re-start the run with PySpark filtering out the ones I all ready ran but after a few thousand more, it will crash again with the same EOFException. I am using foreach since I don't care about any returned values and simply just want the tables written to Hadoop. How can identify the root cause of this Py4JJavaError and fix it to prevent constant crashing of the workers? stackoverflow relevant question and answer Job aborted due to stage failure: Task 10 in stage 148.0 failed 4 times, most recent failure: Lost task 10.3 in stage 148.0 (TID 4253, 10.0.5.19, executor 0): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed) at org.apache.spark.api.python.BasePythonRunner $ReaderIterator$$anonfun$ 1.applyOrElse(PythonRunner.scala:333) at org.apache.spark.api.python.BasePythonRunner $ReaderIterator$$anonfun$ 1.applyOrElse(PythonRunner.scala:322) at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36) at org.apache.spark.api.python.PythonRunner $$anon$1.read(PythonRunner.scala:443) at org.apache.spark.api.python.PythonRunner$$ anon $1.read(PythonRunner.scala:421) at org.apache.spark.api.python.BasePythonRunner$ ReaderIterator.hasNext(PythonRunner.scala:252) at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) at scala.collection.Iterator $class.foreach(Iterator.scala:893) at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28) at scala.collection.generic.Growable$ class. $plus$ plus $eq(Growable.scala:59) at scala.collection.mutable.ArrayBuffer.$ plus $plus$ eq(ArrayBuffer.scala:104) at scala.collection.mutable.ArrayBuffer. $plus$ plus $eq(ArrayBuffer.scala:48) at scala.collection.TraversableOnce$ class.to(TraversableOnce.scala:310) at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28) at scala.collection.TraversableOnce $class.toBuffer(TraversableOnce.scala:302) at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28) at scala.collection.TraversableOnce$ class.toArray(TraversableOnce.scala:289) at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28) at org.apache.spark.rdd.RDD $$anonfun$collect$1$$ anonfun $12.apply(RDD.scala:939) at org.apache.spark.rdd.RDD$$anonfun$ collect $1$$anonfun$ 12.apply(RDD.scala:939) at org.apache.spark.SparkContext $$anonfun$runJob$5.apply(SparkContext.scala:2074) at org.apache.spark.SparkContext$$ anonfun $runJob$ 5.apply(SparkContext.scala:2074) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:109) at org.apache.spark.executor.Executor $TaskRunner.run(Executor.scala:345) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$ Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Caused by: java.io.EOFException at java.io.DataInputStream.readInt(DataInputStream.java:392) at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:428) Driver stacktrace: at org.apache.spark.scheduler.DAGScheduler.org $apache$ spark $scheduler$ DAGScheduler $$failJobAndIndependentStages(DAGScheduler.scala:1602) at org.apache.spark.scheduler.DAGScheduler$$ anonfun $abortStage$ 1.apply(DAGScheduler.scala:1590) at org.apache.spark.scheduler.DAGScheduler $$anonfun$abortStage$1.apply(DAGScheduler.scala:1589) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589) at org.apache.spark.scheduler.DAGScheduler$$ anonfun $handleTaskSetFailed$ 1.apply(DAGScheduler.scala:831) at org.apache.spark.scheduler.DAGScheduler $$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831) at scala.Option.foreach(Option.scala:257) at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761) at org.apache.spark.util.EventLoop$$ anon $1.run(EventLoop.scala:48) at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099) at org.apache.spark.rdd.RDD$$anonfun$ collect $1.apply(RDD.scala:939) at org.apache.spark.rdd.RDDOperationScope$ .withScope(RDDOperationScope.scala:151) at org.apache.spark.rdd.RDDOperationScope $.withScope(RDDOperationScope.scala:112) at org.apache.spark.rdd.RDD.withScope(RDD.scala:363) at org.apache.spark.rdd.RDD.collect(RDD.scala:938) at org.apache.spark.api.python.PythonRDD$ .collectAndServe(PythonRDD.scala:162) at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala) at sun.reflect.GeneratedMethodAccessor101.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) at py4j.Gateway.invoke(Gateway.java:282) at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) at py4j.commands.CallCommand.execute(CallCommand.java:79) at py4j.GatewayConnection.run(GatewayConnection.java:238) at java.lang.Thread.run(Thread.java:748) Caused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed) at org.apache.spark.api.python.BasePythonRunner $ReaderIterator$$anonfun$ 1.applyOrElse(PythonRunner.scala:333) at org.apache.spark.api.python.BasePythonRunner $ReaderIterator$$anonfun$ 1.applyOrElse(PythonRunner.scala:322) at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36) at org.apache.spark.api.python.PythonRunner $$anon$1.read(PythonRunner.scala:443) at org.apache.spark.api.python.PythonRunner$$ anon $1.read(PythonRunner.scala:421) at org.apache.spark.api.python.BasePythonRunner$ ReaderIterator.hasNext(PythonRunner.scala:252) at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) at scala.collection.Iterator $class.foreach(Iterator.scala:893) at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28) at scala.collection.generic.Growable$ class. $plus$ plus $eq(Growable.scala:59) at scala.collection.mutable.ArrayBuffer.$ plus $plus$ eq(ArrayBuffer.scala:104) at scala.collection.mutable.ArrayBuffer. $plus$ plus $eq(ArrayBuffer.scala:48) at scala.collection.TraversableOnce$ class.to(TraversableOnce.scala:310) at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28) at scala.collection.TraversableOnce $class.toBuffer(TraversableOnce.scala:302) at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28) at scala.collection.TraversableOnce$ class.toArray(TraversableOnce.scala:289) at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28) at org.apache.spark.rdd.RDD $$anonfun$collect$1$$ anonfun $12.apply(RDD.scala:939) at org.apache.spark.rdd.RDD$$anonfun$ collect $1$$anonfun$ 12.apply(RDD.scala:939) at org.apache.spark.SparkContext $$anonfun$runJob$5.apply(SparkContext.scala:2074) at org.apache.spark.SparkContext$$ anonfun $runJob$ 5.apply(SparkContext.scala:2074) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:109) at org.apache.spark.executor.Executor $TaskRunner.run(Executor.scala:345) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$ Worker.run(ThreadPoolExecutor.java:624) ... 1 more Caused by: java.io.EOFException at java.io.DataInputStream.readInt(DataInputStream.java:392) at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:428) ... 24 more
