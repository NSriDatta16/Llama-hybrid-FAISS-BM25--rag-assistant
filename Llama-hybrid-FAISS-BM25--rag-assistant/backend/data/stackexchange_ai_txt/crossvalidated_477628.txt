[site]: crossvalidated
[post_id]: 477628
[parent_id]: 477627
[tags]: 
Autoencoders are for finding a (typically) lower dimensional representation of your data. In other words, it tries to compress / encode your data, which the name comes from. There are mainly two components: encoder and decoder. Encoder's responsibility is to compress your data (maps it onto another vector space), and Decoder's responsibility is get back the original point, given the encoded version. Both encoder and decoder are neural networks. The aim is to minimize the reconstruction error, i.e. an input $\mathbf x$ is given to the encoder which outputs $\mathbf y$ , and then $\mathbf y$ is given to the decoder which outputs $\mathbf x'$ , an estimate of the original data point, $\mathbf x$ . The error to be minimized is the reconstruction error $||\mathbf x-\mathbf x'||^2$ . So, just like any other neural network, there is an input/output relationship, target variable, and a cost function. The rest is back-propagation. Note that this is the common type of an autoencoder, and there are variations and additions in the literature.
