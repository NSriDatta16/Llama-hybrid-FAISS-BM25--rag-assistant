[site]: stackoverflow
[post_id]: 2322612
[parent_id]: 
[tags]: 
Horizontally partitioning data into an "archive" in SQL Server taking months to execute?

There is a project in flight at my organization to move customer data and all the associated records (billing transactions, etc) from one database to another, if the customer has not had account activity within a certain timeframe. The total number of rows in all the tables is in the millions. Perhaps 100 million rows, with all the various tables combined. The schema is more-or-less normalized. The project's designers have decided on SSIS to execute this and initial analysis is showing 5 months of execution time. Basically, the process: Fills an "archive" database that has the same schema as the database of origin Delete the original rows from the source database I can provide more detail if necessary. What I'm wondering is, is SSIS the correct approach? Is there some sort of canonical way to move very large quantities of data around? Are there common performance pitfalls to avoid? I just can't believe that this is going to take months to run and I'd like to know if there's something else that we should be looking into.
