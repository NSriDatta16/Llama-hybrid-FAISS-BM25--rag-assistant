[site]: crossvalidated
[post_id]: 541384
[parent_id]: 451480
[tags]: 
I agree with the others that p values are not useful here and that regularized regression (ridge, elastic net, lasso) are potential way to go (elastic net might be more useful if the variables are correlated - but which one is best is an empirical question). I would also decide whether theoretical or potential interactions in the predictors or nonlinearities in the relationships between the predictor and outcome are important to you. If so you will either need to create them ahead of time - here is a resource with potential considerations looking at interactions in a regularized regression. Also, if interested in interactions or nonlinear relationships you could consider using or combining your model with a random forest model. One popular option that I have also found success with is Boruta , which is a wrapper around a random forest model that examines whether your features are better than randomly permuted versions of the features. As Demetri pointed out above, any predictor with your sample size would likely have some nonzero relationship with the outcome, making p values for that purpose not useful. Yet, comparing whether the features are significantly better than their random permutations as a Boruta does is a way that a significant difference using p values can become useful again. Either way, if the 12 variables you have are considered theoretically useful, you seem to have three options - keep them all (that's not a lot of features - why not include them all), trying to figure out if some can be dropped with two large a loss in prediction accuracy), or trying to figure out what relationships between these predictors and the outcome is the most useful for prediction. The second option seems to be what you are asking and might be fastest, but the third option might help you the most with prediction over time.
