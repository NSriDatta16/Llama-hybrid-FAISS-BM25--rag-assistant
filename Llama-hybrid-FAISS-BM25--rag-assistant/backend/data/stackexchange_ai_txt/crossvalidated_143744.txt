[site]: crossvalidated
[post_id]: 143744
[parent_id]: 142903
[tags]: 
Use the paired t -test As long you have enough ratings (15 is sufficient, and I would be happy even with fewer) and some variation in the rating differences, there is no problem at all using the paired t -test. Then you get estimates that are very easy to interpret – the mean ratings on a 1–5 numeric scale + its difference (between products). R code It’s very easy to do in R: > ratings = c("very bad", "bad", "okay", "good", "very good") > d = data.frame( customer = 1:15, product1 = factor(c(5, 4, 3, 5, 2, 3, 2, 5, 4, 4, 3, 5, 4, 5, 5), levels=1:5, labels=ratings), product2 = factor(c(1, 2, 2, 3, 5, 4, 3, 1, 4, 5, 3, 4, 4, 3, 3), levels=1:5, labels=ratings)) > head(d) customer product1 product2 1 1 very good very bad 2 2 good bad 3 3 okay bad 4 4 very good okay 5 5 bad very good 6 6 okay good First let’s check the average ratings: > mean(as.numeric(d$product1)) [1] 3.9333 > mean(as.numeric(d$product2)) [1] 3.1333 And the t -test gives us: > t.test(as.numeric(d$product1), as.numeric(d$product2), paired=TRUE) Paired t-test data: as.numeric(d$product1) and as.numeric(d$product2) t = 1.6, df = 14, p-value = 0.13 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -0.27137 1.87137 sample estimates: mean of the differences 0.8 The $p$-value is 0.13, which does not strongly suggest that products are rated differently, despite the apparent difference of 0.8 (but do note the quite confidence interval – we really need more data). Fake data? Curiously, and unexpectedly, an unpaired t -test gives a lower p -value. > t.test(as.numeric(d$product1), as.numeric(d$product2), paired=FALSE) Welch Two Sample t-test data: as.numeric(d$product1) and as.numeric(d$product2) t = 1.86, df = 27.6, p-value = 0.073 […] This does suggest that the example data are fake. For real data, one would expect a (quite high) positive correlation between ratings from the same customer. Here the correlation is negative (though not statistically significantly so): > cor.test(as.numeric(d$product1), as.numeric(d$product2)) Pearson's product-moment correlation data: as.numeric(d$product1) and as.numeric(d$product2) t = -1.38, df = 13, p-value = 0.19 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: -0.73537 0.18897 sample estimates: cor -0.35794 Missing data When not all customers have rated both products (i.e., unbalanced data), a better approach is using a mixed-effects model: Let’s first convert the data to numeric form: > d2 = d > d2[,-1] = lapply(d2[,-1], as.numeric) And convert it to ‘long’ form: > library(tidyr) > d3 = gather(d2, product, value, -customer) And finally fit a mixed-effects model with customer as a random effect: > l = lme(value~product, random=~1|customer, data=d3) > summary(l) Linear mixed-effects model fit by REML Data: d3 AIC BIC logLik 101.91 107.24 -46.957 Random effects: Formula: ~1 | customer (Intercept) Residual StdDev: 3.7259e-05 1.1751 Fixed effects: value ~ product Value Std.Error DF t-value p-value (Intercept) 3.9333 0.30342 14 12.9633 0.0000 productproduct2 -0.8000 0.42910 14 -1.8644 0.0834 […] The $p$-value is 0.0834. Usually for balanced data it will be almost identical to the p -value from a paired t -test. Here it is closer to the p -value of an unpaired t -test, because of the negative correlation. Note that the variance for the customer effect (random intercept) is almost zero. This would rarely happen with real data. Summary In summary, use the paired t -test. Then you get estimates that are easy to interpret (simple numerical averages). If not all customers have rated both products, use a mixed effects model instead. (This will give approximately the same results as the paired t -test when they have all rated both products, so you might as well always use it.)
