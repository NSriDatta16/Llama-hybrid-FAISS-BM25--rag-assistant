[site]: crossvalidated
[post_id]: 622978
[parent_id]: 
[tags]: 
How to mitigate large sample number for multimodal posteriors in Approximate Bayesian Computation-Sequential Monte Carlo (ABC-SMC)?

I want to do Bayesian inference for a model function for which the likelihood cannot be explicitly computed, which is why I turned to Approximate Bayesian Computation (ABC). In particular, I am using the sequential Monte Carlo (SMC) algorithm. I am currently evaluating the approach by generating synthetic data from parameters of the model I choose randomly. Inferring the parameters via ABC-SMC nearly always works well, and performance is good as the number of samples stays constant during all generations of the algorithm. However, for some parameters, there is an exponential scaling in the sample number of each generation, as e.g. in this run from generation ~15-25: In this example the exponential is 'overcome', however, there are parameters for which the number of samples grows to a degree that the inference is computationally infeasible. Analysing this issue, it seems like the exponential scaling always occurs when the posterior is multi-modal. The following plots show the posterior (estimated as a kernel density estimate (KDE) from the particle) for single-variable and 2-variable cuts in the left-hand corner heatmaps and diagonal distributions respectively, for an example where 4 parameters are supposed to be inferred ( delta_t , ...). The upper right-hand corner shows the locations of the successful samples. (Orange marks the true parameters of the synthetic data was generated with, i.e. the value the posterior should converge to.) The first figure corresponds to generation 22 and thus to exponential sample number scaling, and the second one to generation 31 and thus a constant number of samples per generation: Note how the first one shows a multi-modal distribution in the delta_onsite_potential - delta_t plot; the second one is a mono-modal - from analysing all the parameter samples with exponential scaling, this seems to be the reason for the issue. I am now wondering what I could do to mitigate this bad sample number scaling and keep the number of samples per generation approximately constant at all times? I already tried: to use different distance functions in the ABC-SMC, which might have an easier-to-optimise loss landscape. This did not change the situation. to use different kinds of KDEs. In particular, I initially used a multivariate normal with a global covariance chosen according to the Silverman rule of thumb, and switching this to a multivariate with a covariance estimated for each particle from its k nearest neighbours did help with, but did not entirely fix, this issue. changing the strategy with which the acceptance threshold is chosen in each generation. This slightly helped avoid the divergences, but not much. Some more details in case they are relevant: I am using the pyABC implementation of ABC-SMC. The acceptance threshold $\epsilon$ is updated by taking a weighted quantile of distances in the previous generation. Additionally, this is multiplied by a constant multiplier. ( QuantileEpsilon ) The KDE is estimated with covariance taken from the k nearest neighbours of each particle. ( LocalTransition ) I'm not entirely sure which other details might be relevant and am happy to provide more information!
