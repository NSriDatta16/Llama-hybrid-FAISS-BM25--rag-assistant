[site]: crossvalidated
[post_id]: 359510
[parent_id]: 312877
[tags]: 
This is a fairly common occurrence with neural network models. My answer here is a conjecture, but I suspect that the reason this occurs is related to the problem of adversarial examples. Neural networks tend to have very steep transitions between classes, so examples can be near the boundary between two classes, and yet be dramatically misclassified. Adversarial examples take advantage of that by finding small modifications to an image from class $a$ that cause it to be classified as class $b$. From this perspective, the problem appears to be one of regularization , but I don't think that anyone has a great sense of the specific sequence of steps to take so that ambiguous or unclear inputs are not strongly classified in any category.
