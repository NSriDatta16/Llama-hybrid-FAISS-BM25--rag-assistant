[site]: datascience
[post_id]: 54022
[parent_id]: 
[tags]: 
Implementing SVM with Gaussian Kernel

This is referencing Prof. Andrew Ng's course on machine learning. In the part that details implementing an SVM with the Gaussian kernel, we are supposed to use all the training examples as our landmarks/support vectors to calculate the Gaussian kernel. Specifically, for each training example, we will get a $m+1$ dimensional vector (including a bias unit) when we calculate the Gaussian kernel for each training example with every other training example (including itself). So, will doing this with every example not result in a $m+1 \times m+1$ matrix instead of a $m+1$ dimensional feature vector? According to my understanding, we need $m+1$ features for us to be able to write the cost function. However, here, won't we end up with $m+1 \times m+1$ features instead?
