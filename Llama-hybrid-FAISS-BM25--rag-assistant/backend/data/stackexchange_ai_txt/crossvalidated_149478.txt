[site]: crossvalidated
[post_id]: 149478
[parent_id]: 
[tags]: 
What is the intuition behind the sparsity parameter in sparse autoencoders?

Sparse autoencoders is a unsupervised learning algorithm which tries to learn an identity function of the input. As mentioned in the notes of Andrew Ng's lecture on deep learning the average activation of neurons in the hidden layer over the training set are restricted lets say to 0.01 ( rho ) which is called the sparsity parameter. I am confused as to why would we be interested to restrict the activation of hidden neurons ?
