[site]: crossvalidated
[post_id]: 503776
[parent_id]: 502988
[tags]: 
What you need is essentially a good model for the "switching time" of a call from good to bad. When designing a model it is most useful to start from an ideal situation and then approximate that using the data at hand. The following are just my own thoughts, read them critically. :) Ideal model The scope is to statistically see if $\mu=40$ is a good value for the average time within which calls don't change their status from bad to good. Let $Z_{it}$ be the $t$ 'th call from number $i$ and $X_{it}$ the time between this and the previous call, i.e. $X_{it}=time(Z_{t})-time(Z_{t-1})$ ( $X_{i0}$ doesn't exist), with $Z_{it}\in \{good,bad\}$ . Ideally, we would know the true classification of each call, i.e. $Z_{it}$ . Then, since we want to test only how fast bad calls turn to good calls, we would take all the times between consecutive calls which switched from good to bad, i.e. the set $\mathcal{X}_{bad \to good}=\mathcal{X}= \{ X_{it}:Z_{it-1}=bad \ \land\ Z_{it}=good \}$ . We can argue that we can assume that $X_{it} \sim iid(\mu,\sigma^2)$ across $t$ , since there's no reason why the time between a bad and a good call should influence the next one. Across the callers $i$ however, each can have different calling reasons and thus require more/less urgent responses leading to different $\sigma_i^2$ . If so, then we could do the test for each class of callers. For simplicity let's assume the same behavior across all callers. Then, assuming by the CLT that the mean $\bar{\mathcal{X}}\overset{a}{\sim}N(\mu,s^2/n)$ , where $n$ is the number of elements in $\mathcal{X}$ , we could build a test statistic: $$ t=\frac{\bar{\mathcal{X}}-\mu}{s/\sqrt{n}}\sim N(0,1) $$ where $s$ is the sample standard deviation. Now a sensible test to the problem at hand would be if the value of 40 is too stringent, i.e. classifies bad when a reclassification would be needed. The null would be $H_0: \mu \ge 40$ . If we reject, then the actual mean time in which bad calls turn good is smaller than the threshold of 40, meaning that no chance is given to bad calls that quite likely would turn good. In that case, the number 40 should be replaced with a certain small percentile of $X_{it}$ from the data, i.e. the percentage of misclassifications that are tolerated. If we don't reject then the true mean is actually greater or equal to 40, meaning that the company gives bad calls if anything too much of a chance, when statistically they would keep being bad for a longer period. Real-life approximation The problem in real life is that we don't know the true $Z_{it}$ for all $t$ , since if $Z_{it-1}$ is bad and the corresponding $X_{it} \le 40$ , it automatically gets classified as bad too. Your boss suggested/requested to calculate the time (with an upper limit of 1 week) between all calls from the same phone number that turned from bad to good within a period of 4 months. This is essentially an approximation for the set $\mathcal X$ , with the assumption that every call that turned from bad to good within 4 months could have turned from bad to good at any time prior to that . For example: caller 1 calls at time 1,2,3 and 4. At time 1 he gets labeled as bad, at time 4 as good. By using the difference between times 1 and 2, times 2 and 3, times 3 and 4, one implicitly makes the assumption that the relabeling could have happened between any of those times - not only as it is shown, namely between times 3 and 4. I don't see the logic of calculating combinations of time differences (e.g. between times 1 and 3), since a relabeling can only happen between two consecutive calls - even more so, then taking the difference between non-consecutive times would wrongly increase the mean. Note that the time has to be calculated between calls of the same caller - i.e. it wouldn't make any sense to calculate the time e.g. between $Z_{11}$ and $Z_{21}$ , since the second could call at 1 second before the first, which obviously doesn't tell anything about the bad to good switch. Regarding the time period of 4 months, you need to specify a time where to get the data from, might as well do. The 1-week cap for the time between calls doesn't make much sense to me, unless there are very few callers that have a week in between their calls - otherwise it would wrongly decrease the mean. The approach of trying to approximate $\mathcal X$ is correct. The restrictions should be as few as possible, so I agree that the filters are very arbitrary - might make sense again, only if very few cases (outliers) fall outside of those. If we leave all the restrictions out, the model seems quite ok given the data at hand. (Another restriction that would maybe make sense is restricting the set $\mathcal X$ only to callers who had at least 2 calls within the 40-hour period. Then the assumption made in the beginning (that the switch from bad to good could have happened at any time prior) becomes very realistic. All calls that have more than 40h between them should be disregarded. This might change the underlying question being asked though and hence maybe even the test would have to be adapted. [Haven't given it much thought yet.]) Regarding your questions: Questions 1 & 2 have implicitly been answered I hope. Regarding 3, you can bootstrap to calculate the standard deviance for the mean, but if we assume (asymptotic) normality and homoskedasticity it can be calculated by formula (see the test statistic). The CLT would be visible by taking many samples of increasing sizes each time and plotting the distribution. In your case you only have 1 sample. 4. Yes, you can check for normality of $X_{it}$ within $\mathcal X$ , using different tests. You could also just argue asymptotically under the iid assumption, so you would need to argue that iid is realistic.
