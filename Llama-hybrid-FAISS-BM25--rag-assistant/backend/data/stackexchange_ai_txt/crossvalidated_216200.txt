[site]: crossvalidated
[post_id]: 216200
[parent_id]: 215970
[tags]: 
If tweaking the software and model architecture doesn't do the trick, there's another interesting approach. Say you have a large ensemble model (like a random forest) that has good prediction performance but slow runtime. It's possible to translate the ensemble model into a more efficient neural net. This paper describes how: Bucila et al. (2006) . Model compression. The idea is to generate synthetic, unlabeled data that mimics the distribution of the training data. Arbitrarily many synthetic data points can be generated (e.g. more than in the original training set). Alternatively, real unlabeled data can be used if a source is readily available. The synthetic data is fed through the ensemble model to generate labels. The synthetic data and labels are then used to train a neural network. Here's a talk by Geoff Hinton describing a similar approach.
