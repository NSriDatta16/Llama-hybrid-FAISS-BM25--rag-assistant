[site]: crossvalidated
[post_id]: 522840
[parent_id]: 522829
[tags]: 
Informally Bias is how far away on average your estimator is from the true value. "On average" hides the point that in practice you are only going to make one estimate, which could be a long way away from the true value even if the bias is low. Your how far your model parameters are from true parameters is not a reasonable description of this as it misses the average point Variance is a measure of (the square of) the dispersion of your estimator from its average. Again this hides the point that you are going to make a single estimate. It also ignores errors from a high bias. So your how well it generalizes to new instances is not a reasonable description of this either, as it misses the point that it measures dispersion from the wrong figure You do not want either or both of these to be high, as that could cast doubt on the value of any estimate. You can combine them: the variance plus the square of the bias gives the expected mean-square error, so taking the square-root of that sum gives a measure of the scale of the potential error of a single use of your estimator from the true value.
