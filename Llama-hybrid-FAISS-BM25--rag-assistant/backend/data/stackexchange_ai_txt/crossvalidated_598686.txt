[site]: crossvalidated
[post_id]: 598686
[parent_id]: 598394
[tags]: 
It seems you're confusing "number of output neurons" in the 1st linked webpage with "number of LSTM cells". The former refers to the number of the hidden units, i.e. the dimension of the LSTM cell output vector. It is also known by other names such as the size of the hidden layer, the hidden size, the hidden dimension, etc. An LSTM only has a single cell that consumes a sequential input step by step. This computation is often unrolled when visualised, which is why it looks as if the LSTM has a fixed number of cells; it doesn't. So in your figure, there shouldn't be cell 51, 52, and so on. In fact, the cells shouldn't be numbered at all as it gives the impression of multiple cells. See the 2nd figure by Christopher Olah depicting an unrolled RNN for reference (LSTMs are a special kind of RNNs). This explanation also answers your question about the dimension of the hidden and cell state: they're both equal to the number of hidden units. As for visualising the dropout layer, there isn't a standard way. You're allowed to just draw a box labelled "Dropout 0.2" similar to that on the 1st linked webpage. If the LSTM has multiple layers, the output $\mathbf{h}^{(\ell)}_t$ at timestep $t$ of the $\ell$ -th layer becomes the input $\mathbf{x}^{(\ell+1)}_t$ of the same timestep in the next layer.
