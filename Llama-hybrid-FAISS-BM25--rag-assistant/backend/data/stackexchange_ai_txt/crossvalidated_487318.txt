[site]: crossvalidated
[post_id]: 487318
[parent_id]: 
[tags]: 
Which optimizer to use and when in neural networks?

I have gone through this quite nice article on optimizers . I know what each does. I've perused the web on when to use which optimizer. I know that generally the deep learning field says "just use Adam and problem solved." But this isn't always the case! In fact, a lot of papers, especially in CVPR and NIPS are using the good old stochastic gradient descent, even in 2020 - well after the arrival and popularization of Adam. Consider this canonical publication on inductive graph learning in 2017 with currently ~2000 citations. It uses SGD. In discussion with AI professors, they've told me that Adam can sometimes be unstable and you don't have as much control over it. So it is better to use SGD with learning rate update methods. In fact, even Francois Chollet, the creator of Keras, in many of his blog posts doesn't use Adam. Take this post for example: building autoencoders . This article was written by him in 2016. Adam came out in 2014. I'm sure an expert such as Francois is fully aware of Adam since it is integrated into this Keras library. However, in that article he uses adadelta over adam. So my question is as follows. If one were to specify a low starting learning rate to Adam, is the instability still such a problem? Why are researchers still turning to SGD and Adadelta? In other words, I somewhat repose the question asked here : when to use which optimizer? Though I know what each does, I don't have an intuitive understanding on when to choose one optimizer over another.
