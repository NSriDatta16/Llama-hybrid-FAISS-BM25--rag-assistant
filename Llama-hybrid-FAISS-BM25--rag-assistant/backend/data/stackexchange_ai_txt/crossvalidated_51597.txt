[site]: crossvalidated
[post_id]: 51597
[parent_id]: 
[tags]: 
Binning answers from a Likert-scale question

I have six questions that each have a five-point Likert scale. We are interested in whether participants, overall, "agree" or "disagree" (or are neutral) toward the questions. To do this, we currently calculate the average of the items, which results in an average from 1 to 5, then we collapse it into 3 bins, where 5 and 4 are "Agree", 3 is Neutral, and 2 and 1 are "Disagree". We then compare this using a $\chi^2$ Goodness of Fit to the expected outcome, which we assume is randomly uniform: we anticipated that as 40% = agree, 20% = neutral, and 40% = disagree, since on a five-point scale 2 options are agree, 1 is neutral, and 2 options are disagree. However I have doubts that we can average on a five point scale, then collapse into a 3 point scale and claim the expected averages are the same. I'm wondering if we can make the expected values claim if we collapse the categories into a 3-point scale before calculating the average. My colleagues don't see the problem. Is there a mathematical argument that shows whether these approaches (collapse before average, or collapse after average) are equivalent?
