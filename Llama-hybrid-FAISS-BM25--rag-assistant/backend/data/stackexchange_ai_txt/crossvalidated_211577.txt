[site]: crossvalidated
[post_id]: 211577
[parent_id]: 211296
[tags]: 
Does it only apply to filtering of features, or does it apply to any feature engineering/model tuning, even those which do not use the entire dataset? Testing performance can become overoptimistic if test cases somehow enter any kind of calculation during the model training. This can include pre-processing steps (such as your example of filtering by correlation). This applies also if only some test cases leak into the training set (e.g. because some cases have repeated measurements which randomly ended up some in training, some in the test set). A good rule of thumb is that any kind of calculation that involves more than one case is training and needs to be done on the training set only, so the parameters calculated from the training set can then be applied to the test cases. Whether you spit your data once (hold out) or repeatedly (cross validation or out-of-bootstrap) is not relevant: Hold out can be subject to bias caused by incorrect splitting and data-driven modelling as well (but it may be easier to avoid programming mistakes with hold out, and steps that need manual control are tedious with repeated splits).
