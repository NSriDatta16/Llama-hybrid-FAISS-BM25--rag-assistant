[site]: crossvalidated
[post_id]: 342399
[parent_id]: 
[tags]: 
Back Propagation in RNNs

I just started learning RNNs. I'm stuck with two basic doubts: 1.)We start with same weights for each timestamp; loss function can be the logistic loss or squared error across timestamps. In truncated BPTT, I assume we only unroll k timestamps back at each timestamp for calculating the partial derivative of loss with respect to weights. So for calculating partial derivate of loss w.r.t, last time stamp activation involves calculating partial derivative w.r.t previous k timestamps. So I assume we first start calculating partial derivatives of loss from the first stamp so they can be used at the later stage. Please correct me if I'm stating some obvious erroneous statements. 2.) Do we reset weights and perform BPPT for each training example?
