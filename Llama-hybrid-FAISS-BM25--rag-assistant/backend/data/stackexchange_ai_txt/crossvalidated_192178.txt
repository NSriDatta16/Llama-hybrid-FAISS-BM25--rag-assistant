[site]: crossvalidated
[post_id]: 192178
[parent_id]: 192161
[tags]: 
No technique is superior in every case (cf., the No Free Lunch Theorem ). If your problem is amenable to logistic regression, why not use a method which tends to provide well-calibrated scores? SVMs don't, and in most of my work, well-calibrated scores are very useful. Here's an interesting quote from Murphy ( Machine Learning: A Probabilistic Perspective ) that doesn't say that you would not want to use an SVM, but it does give you a perspective that alternatives (not necessarily logistic regression) are something worth considering: Note that SVMs are very unnatural from a probabilistic point of view. First, they encode sparsity in the loss function rather than the prior. Second, they encode kernels by using an algorithmic trick, rather than being an explicit part of the model. Finally, SVMs do not result in probabilistic outputs, which causes various difficulties, especially in the multi-class classification setting. (Personally, I was initially taken by SVMs, but when applying them to real-world problems have found them to be fiddly and slow.)
