[site]: crossvalidated
[post_id]: 632450
[parent_id]: 
[tags]: 
Bayesian Hierarchical Clustering prior update

I am working through Heller and Ghahramani's "Bayesian Hierarchical Clustering" paper ( https://www2.stat.duke.edu/~kheller/bhc.pdf ) and things aren't quite working out the way I expect with regards to the Dirichlet Mixture Model Process priors, so I would appreciate any help. Particularly, Figure 3 gives the algorithm for computing the prior on the merging of two subtrees: initialize each leaf $i$ to have $d_i = \alpha, \pi_i = 1$ for each internal node $k$ do $d_k = \alpha \Gamma(n_k) + d_{left_k} d_{right_k}$ $\pi_k = \frac{\alpha \Gamma(n_k)}{d_k}$ end for Suppose I start by merging points 1 and 2. Then using the algorithm from Figure 3, we have: $d_{(12)} = \alpha \Gamma(2) + \alpha \cdot \alpha$ $ = \alpha + \alpha^2$ $\pi_{(12)} = \frac{\alpha \Gamma(2)}{\alpha + \alpha^2}$ $ = \frac{1}{1 + \alpha}$ This works out as expected for the Dirichlet process mixture model prior (given in Appendix A). However, suppose we then merge point 3 into the same cluster with points 1 and 2: $d_{(123)} = \alpha \Gamma(3) + d_{(12)} \cdot d_3 = \alpha \Gamma(3) + (\alpha + \alpha^2) \cdot \alpha = 2 \alpha + \alpha^2 + \alpha^3$ $ \pi_{(123)} = \frac{\alpha \Gamma(3)}{2 \alpha + \alpha^2 + \alpha^3} = \frac{2}{2 + \alpha + \alpha^2}$ This doesn't seem to be quite right with respect to the $\alpha$ term because I would expect the prior to be: $\pi_{(123)} = \frac{1}{\alpha + 1} \cdot \frac{2}{\alpha + 2} = \frac{2}{\alpha^2 + 3\alpha + 2}$ What am I missing?
