[site]: crossvalidated
[post_id]: 568919
[parent_id]: 568902
[tags]: 
MICE is usually used for imputation in iid data. Say you have lots of iid measurements, where each should consist of $k$ attributes $(a_i)_{i=1}^k$ , but in some of those measurements, the number of observed attributes is smaller than $k$ . Then MICE helps you to impute the missing attributes from the present (not missing) attributes. But in your case, each measurement contains either all attributes or none, so you cannot deduce the missing attributes of an incomplete measurement from those still present. The notions of MCAR and MAR are also usually referring to the situation of iid data: MCAR (missing completely at random) means that the fact that a certain attribute $a_m$ is missing is completely independent of the values of any of the attributes $(a_i)_{i=1}^k$ , including the non-observed attribute $a_m$ . And MAR (missing at random) means that the fact that a certain attribute $a_m$ is missing is independent of the values of the observed attributes. So, again, in your case, those notions are not really applicable. But you say that your data is a time series. I.e. you don't have iid data, but some time-dependent series of vectors of attributes $(a_{tk})$ , $k=1,\ldots,n$ , $t\in\mathbb{Z}$ , where there is some historical dependency, i.e. dependencies between attribute values of different times. So I would suggest you exploit this dependency, but not with the packages dealing with iid data, but with imputation software for time series. There are many possibilities to do this, from the easiest which is just replacing the missing attributes with the mean over time of that attribute, or those doing simple interpolation, up to the complex ones which use (vector-valued) time series analysis or deep learning. Which is the best for you depends on your actual goal and your context. As a possible first approach, you might want to look into time series imputation with state-space models (using their smoothing capabilities), since they also deal with vector-valued time series and don't require special hardware like deep learning approaches. Unevenly spaced time series You can also look at the problem in a more general way, by considering unevenly spaced (or irregular ) time series . For many time series analysis methods it is presumed, that they are evenly spaced (or regular ), i.e. the time intervals between measurements are all of equal length. If this is not the case, you have mainly the following options: If the unevenness of your time series (TS) consists only of some missing events and the TS is otherwise regular, then you can try some kind of time series imputation, as mentioned above. Otherwise, if you know (using some expert knowledge) that you can shift the times of your events slightly, so as to obtain an evenly spaced TS, without changing the "characteristics" of your TS too much, this should be given a try. Finally, if the above is not applicable, i.e. you have some fundamentally irregular TS, you can try to apply some of the rarer TS methods designed for unevenly spaced TS. For the last option, software is available in R, Julia, and Python, and references can be found e.g. at the above Wikipedia link.
