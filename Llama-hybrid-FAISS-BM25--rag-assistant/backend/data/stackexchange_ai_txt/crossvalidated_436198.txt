[site]: crossvalidated
[post_id]: 436198
[parent_id]: 
[tags]: 
Research on explaining generalizability of deep learning methods

I've read a few classic papers on different architectures of deep CNNs used to solve varied image-related problems. I'm aware there's some paradox in how deep networks generalize well despite seemingly overfitting training data. A lot of people in the data science field that I've interacted with agree that there's no explanation on why deep neural networks work as well as they do. That's gotten me interested in the theoretical basis for why deep nets work so well. Googling tells me it's kind of an open problem, but I'm not sure of the current state of research in answering this question. Notably, there are these two preprints that seem to tackle this question: Generalization in Deep Learning Quantifying the generalization error in deep learning... If anyone else is interested in and following this research area, could you please explain the current state of research on this open problem? What are the latest works/preprints/publications that attempt to tackle it?
