[site]: crossvalidated
[post_id]: 63581
[parent_id]: 63568
[tags]: 
A problem is knowing what a good underlying model should be when you focus on the most common choice. After all, there will usually be a most common choice. You might find the analysis of Wilkinson (2006) of interest. Wilkinson (2006) briefly reviews Pareto charts which commonly combine two displays in one. Frequencies in various categories are shown by a series of bars arranged in frequency order, from most common downwards. On that is often superimposed a rising curve showing cumulative frequency. Frequency and cumulative frequency may or may not have consistent scales. Examples from quality management studies often show categories of accidents, complaints, defects, failures, rejects, returns, or other such unwelcome phenomena. Wilkinson gives several cogent criticisms of this design and suggests an alternative: show frequencies in order, but by a dot plot, but add as reference a series of acceptance intervals. These acceptance intervals are perhaps pertinent to your problem. They are calculated by simulation. Imagine as benchmark a population in which $k$ categories are equally probable, and imagine taking samples of size $n$. Just by chance the observed frequencies of the $k$ categories will typically differ. For each sample we can label the frequencies $f_{(1)} \ge f_{(2)} \ge \cdots \ge f_{(k-1)} \ge f_{(k)}$: thus $f_{(1)}$ is the frequency of the most abundant category, and so forth. Across several samples we can get order statistics for each $f_{(j)}$ and use those to calculate intervals with desired coverage. The acceptance intervals should not be over-interpreted, for various different reasons. First, the reference distribution is just an agnostic guess assuming that we know just the numbers of values and categories and that we have no reason to suppose that categories differ in probability. More commonly, we would not really expect that the categories are equal in probability; it is just that we would rarely know how to make our expectations precise. Second, although making the sample size bigger will stabilize results, some variability will always be experienced in intervals produced by simulation. Third, there are various slightly different recipes for producing percent points from order statistics. A side-comment is that there appears to be no evidence that Pareto used what is now known as the Pareto chart, which seems to have emerged in quality management after 1951. See Wilkinson (2006) for more on the latter point. Wilkinson, Leland. 2006. Revising the Pareto chart. American Statistician 60(4): 332-334.
