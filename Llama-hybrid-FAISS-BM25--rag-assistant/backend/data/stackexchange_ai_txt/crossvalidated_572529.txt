[site]: crossvalidated
[post_id]: 572529
[parent_id]: 572325
[tags]: 
Machine learning (often) needs a lot of data because it doesn't start with a well defined model and uses (additional) data to define or improve the model. As a consequence there are often a lot of additional parameters to be estimated, parameters or settings that are already defined a-priori in non-machine-learning methods. Statistical inference, if it only requires little data, is often performed with some model that is already known/defined before the observations are made. The learning has already been done. The goal of the inference is to estimate the few missing parameters in the model and verify the accuracy of the model. Machine learning is often starting with only a very minimal model or has not even a model but just a few set of rules from which a model can be created or selected. For instance, one learns which variables are actually suitable to make good predictions or one uses a flexible neural network to come up with a function that fits well and makes good predictions. Machine learning does not just search for a few parameters in an already fixed model. Instead it is the model itself that is being generated in machine learning. For that you need additional data. Sometimes it is also the other way around: a lot of data needs machine learning. That is the situation with lots of variables but without a well defined model.
