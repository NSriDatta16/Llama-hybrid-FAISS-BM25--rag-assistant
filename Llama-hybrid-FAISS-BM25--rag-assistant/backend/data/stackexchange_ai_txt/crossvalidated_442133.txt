[site]: crossvalidated
[post_id]: 442133
[parent_id]: 
[tags]: 
How to deal with over-represented features in anomaly detection

Let me illustrate the problem with an example: Our dataset consists of a collection of letters written to Santa Claus mostly by kids together with the age of the person who wrote them. We want to run anomaly detection on this. We have 1 integer feature for the age and 1000 features for the letter's body (because we have chosen a bag of words approach, e.g. tf-idf). Lets say that I fit an neural auto-encoder or an isolation forest to this dataset. Is it a problem to have defined the feature space like this? My intuition is that yes. Ideally, I would like my anomaly detection model to pick up on the weird case where a "normal" letter is written by a 84 y/o person . My fear is that there will be a tendency of that not happening, because the age (1 feature) is under-represented in comparison to the text (1000 features). If I am right, to what extent will dimensionality reduction techniques like PCA/SVD on the text features help? I mean, maybe I can reduce the space from 1000 to 30 before starting to lose most of the information in the text features, but even then 1 vs 30 still would still look disproportionate to me.
