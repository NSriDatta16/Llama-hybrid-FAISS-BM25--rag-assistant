[site]: crossvalidated
[post_id]: 76489
[parent_id]: 76483
[tags]: 
prcomp has a predict S3 method you can use to apply the same transformations to new data quickly. Pass in the data for the new month and the prcomp object like so: new.pca = predict(p, newdata=x.new) But, the fact that you are asking this suggests that you are missing something fundamental about what PCA is doing, because you can also do this with the "loadings" rotation matrix. So I'm just going to give you a full PCA example so you can better understand what's going. # sample data set.seed(123) n=500 mu=c(1,2) sigma So what's so magical about this loadings matrix? Well, all PCA is doing is rotating our data into a new basis. That's why the components are always orthogonal to each other: they're the basis of the space after the transformation. I chose a two-dimensional example because it's much easier to see what's happening in two dimensions. Here's our data before we applied PCA (but after we scaled and centered): plot(x.scaled, main='original, scaled data',xlim=c(-5,4.5), ylim=c(-3,3)) Here's the data after we applied PCA. As you can see, we just rotated it so that the direction along which we have the most variance (the first principle component) becomes our new x-axis. plot(p$x, col='red', xlim=c(-5,4.5), ylim=c(-3,3)) To make this rotation really explicit, here's how the original location of our data maps to the new location after it's transformed by pca: plot(x.scaled, main='Mapping of points from before and after rotation', xlim=c(-5,4.5), ylim=c(-3,3)) points(p$x, col='red') sapply(1:n,function(i){ lines(c(x.scaled[i,1], p$x[i,1]), c(x.scaled[i,2], p$x[i,2])) }) (It's subtle, but you might have noticed that the transformed data is actually flipped along the x-axis. This is due to a negative PC, which isn't actually a problem ) And since we've gone this far, he's a sample regression on dimension-reduced data. # let's do the regression you described to complete the example. # sample data set.seed(123) n=500 mu=c(1,2) sigma
