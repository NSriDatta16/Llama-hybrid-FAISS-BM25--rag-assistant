[site]: crossvalidated
[post_id]: 557908
[parent_id]: 
[tags]: 
What does it mean "loss function should be decoupled" in backprop for deep learning?

While revising my notes that i took in a deep learning related course (in calculating gradients step), I see "we assume our loss function is a decoupled function". Can someone explain what this sentence means? I do not remember at all. Edit 1 - more explanation: I also noted that in order to be able to apply backpropagation, our loss function should be decoupled in the sense that only one value (average loss) is used to backpropagate the error. We do not use a vector of loss that contains the loss of each data point in a mini-batch for example. Instead, I noted that "we need a decoupled loss function to be able to apply backpropagation". Why?
