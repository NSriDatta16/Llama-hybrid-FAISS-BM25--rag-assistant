[site]: crossvalidated
[post_id]: 364786
[parent_id]: 
[tags]: 
Where's wrong in my reasoning behind upper bound for reconstruction error?

In the paper Mutual Information Neural Estimation , the authors derive the reconstruction error in BiGAN as $$ \mathcal R=E_{x\sim q(x)}E_{z\sim q(z|x)}\left[-\log p(x|z)\right] $$ where $q(z|x)$ is the encoder and $p(x|z)$ is the generator/decoder. Here's the derivation I derive following the paper $$ \begin{align} \mathcal R&=E_{x\sim q(x)}E_{z\sim q(z|x)}\left[-\log p(x|z)\right]\\ &=E_{x,z\sim q(x,z)}\left[\log {p(z)\over p(x, z)}\right]\\ &=E_{x,z\sim q(x, z)}\left[\log {q(x,z)\over p(x,z)}-\log q(x,z)+\log p(z) \right]\\ &=D_{KL}(q(x,z)\Vert p(x,z))+H_q(x,z)+ E_{z\sim q}\left[\log p(z)\right]\\ &=D_{KL}(q(x,z)\Vert p(x,z))+H_q(x,z)-D_{KL}(q(z)\Vert p(z))-H_q(z)\\ &=D_{KL}(q(x,z)\Vert p(x,z))-D_{KL}(q(z)\Vert p(z))+H_q(x|z)\\ &=D_{KL}(q(x,z)\Vert p(x,z))-D_{KL}(q(z)\Vert p(z))-I_q(x,z)+H_q(x)\\ &\le D_{KL}(q(x,z)\Vert p(x,z))-I_q(x,z)+H_q(x) \end{align} $$ There're two things confusing me: I think $H_q(x,z)-H_q(z)=H_q(x|z)$, but the authors wrote $H_q(x,z)-H_q(z):=H_q(z|x)$. I searched $:=$ and found that this symbol means "is defined to be". Why are $H_q(x,z)-H_q(z)$ defined to be $H_q(z|x)$? In the above derivation or in the proof from the paper, $q$ concerns the encoder and $p$ the generator. However, in section 5.2 of the paper, $q$ represents the generator and $p$ the encoder. The mystery is that both use $I_q$ in the upper bound. I can make sense of the reconstruction error if $q$ is the encoder, but not the generator. However, assuming $q$ is associated with the encoder and $p$ the generator, I think maximizing the mutual information $I_p(x,y)$ is more intuitive than maximizing $I_q(x,z)$ since we want to have a better generator rather an encoder. So I now don't know which one is right.
