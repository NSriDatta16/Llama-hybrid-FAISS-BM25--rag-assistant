[site]: crossvalidated
[post_id]: 313131
[parent_id]: 311873
[tags]: 
In a general machine learning problem, in order to avoid overfitting/underfitting, the network trains on training data, validates the training using a separate validation set and then evaluates the performance of the model on another separate test set. However, by partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets. A solution to this problem is a procedure called cross-validation (CV). A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called k-fold CV, the training set is split into k smaller sets. A model is trained using k-1 of the folds as training data and the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy). The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop. (see here) . I have not had the chance to use createFold of R but I guess that it is a bit more different than KerasClassifier of Python. As far as I see, it creates index numbers and you should call those index numbers to divide your data set into k-folds. install.packages('caret') library(caret) data(oil) idx Answering your questions, for createFolds function, you need to call your training data set that you prepare beforehand. Then, you have to call k-1 of the folds (k=1,2,...,10 in the case of 10-fold CV) as training data (which are generated from CreateFolds), every time you run model.fit and validate on the remaining part of the data. Then, find the average of model metrics and evaluate your model by using your test data set.
