[site]: crossvalidated
[post_id]: 275879
[parent_id]: 78570
[tags]: 
Since this question is getting a fair number of hits, and I stumbled across it on Google when I was looking for similar information, I am going to expand on the comments above. The Purpose Of The T Test What we are doing when we do a t test is measuring the number of standard deviations our measured mean is from the baseline mean, while taking into account that the standard deviation of the mean can change as we get more data. For instance if I have a know the standard deviation of a set, and I draw a single point from that set, we expect a bell curve to define the probability distribution of the point that I draw. The bell curve is defined by the 's' in the t test equation. However as you draw more samples from that set, and interesting thing happens. The standard deviation of the mean value of all the samples is actually less than the standard deviation of the set. The decrease is proportional to the square root of n, which is the division by the square root of n that is in the t test equation. A useful example that I like to think of is what happens when you are rolling dice. When you roll a single die you are equally likely to get any value between 1 and 6, with an average value of 3.5. The standard deviation of that result is 1.7078. Now if you roll two dice and take their average, it is still 3.5. But not equally likely to get all the possible averages. You are actually more likely to get an average near 3.5 than one near 1 or 6. (This is a fact the casinos exploit in the game 'craps' since 7 is the most likely sum of two dice) The probability distribution of that average for 1, 2, and 3 dice is shown below. The standard deviation for the average of 1 die is 1.7078, for two dice it is 1.207, for 3 dice it is .986. This has the effect of narrowing the bell curve in the t-test, which results in a higher 't' value. This is shown below As the bell curve gets narrower and narrower, the t value increases towards infinity. What you have seen in your question is a standard deviation of zero. That reflects an infinitely narrow bell curve. As a result t value would be infinity, i.e. you could be completely confident that you had a statistically significant difference. That is a similar effect as what you would see if you had an infinitely large sample size, since that would also make the bell curve infinitely narrow. Now as whuber and Nick Cox pointed out in the comments on the original question, you do have to think if you really have a zero standard deviation. Rounding of the data or other forms of truncation could give zero standard deviation when in fact you have some. And if the difference that you are trying to measure is within your measurement error that is a problem not addressed by the t-test. I have expanded on the analogy of how the normal curve decreasing in width vs rolling dice in this blog post here and I also have a short Kindle book on different examples of using the t-test and z test here
