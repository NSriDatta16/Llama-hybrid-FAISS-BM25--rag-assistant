[site]: crossvalidated
[post_id]: 76577
[parent_id]: 76490
[tags]: 
This is a really deep question. I am going to try to answer it for your specific case and make broader points at the same time. Has anyone an idea on how to proceed here? How to proceed from here is really a question of which method to use. The answer for your particular case seems to be CART ( Classification and Regression Trees ). CARTs will allow you to get nice, rectangular regions for prediction, but they are very noisy. That is why Random Forests and other similary algorithms were created. Random Forests and the like trade clarity for an improvement in prediction. In a general sense, the choice of method depends on two things: what your goals are for the analysis and how well the model fits your data. Nothing is stopping you from trying a couple of methods and choosing which fits bets. Note that trees can output a continuous probability. You can change the threshold for classifying an event (good in your case) to raise or lower sensitivity. Examine the ROC curve to see how the two relate before doing so. Does it make sense to split the dataset into training- and test-set? Yes and no. You should never measure the performance of the classification algorithm on the same data that it was fit on. You will end up drastically overfitting the data and over estimating. In this case, a training and test set does not make sense because of the small sample size (~70). Instead, I would use Leave One Out Cross Validation (LOOCV) . The algorithm goes like this: Hold one observation out. Fit the model on the data except the hold out from 1. Classify the hold out from 1. Repeat 1-3 until all observations have been held out. Estimate fit based on the classifications from 3. For LOOCV, the final model is the model fit on the entire dataset. What proportion of the data should the training set be (I thought around a 60/40-split). In general, 60/40 or 50/50 splits are good. If you have enough data, do 50/25/25 where the second 25% goes to a validation set. When you have a validation set, you fit the model on the test set then check its performance on the test set. If you think the model should be tweeked, do so and the retest on the test set until you are satisfied. Then, once the model is locked down, classify the data in the validation set. The results from the validation set will be those that you report. For your case, I would recommend LOOCV. How to avoid overfitting (i.e. boxes that are too small)? Most algorithms have control parameters ( e.g. cost for SVMs). In the tree package in R, there are several control parameters to help prevent overfitting (see tree.control ). What is a good statistic to assess the predictive performance in this case? AUC? Accurary? Positive predictive value? Matthews correlation coefficient? It depends on the purpose. I would recomend at lease reporting Accuracy, Sensitivity, Specificity, Positive and Negative Predictive Values. Since you said the emphasis should be on sensitivity, that should be a focus. AUC is also commonly used but is noisy.
