[site]: crossvalidated
[post_id]: 224062
[parent_id]: 223912
[tags]: 
k-means is not particularly expensive. It is also not applicable to "big data" because it only works on continuous numerical data. You can try running it on document-term-matrices but the results are usually very disappointing (and it was not designed for this). Probably the only real use case where k-means is fairly costly is the "bag of visual words" approach, where you need to cluster billions of 128 dimensional dense SIFT vectors. However, this approach is dead now, because deep learning works much better for object recognition in images. A billion vectors times 128 dimensions times 8 bytes for double - that may actually exceed your PCs memory. The main reason you find k-means in every big data tool is because at the textbook algorithm is dead simple to implement (even in parallel), and then people can stick the "can do cluster analysis" sticker on their product feature chart. Itjs about selling the product, not solving anything, unfortunately. Before doing any further optimization on k-means, you should first do a big big literature review. For example Hamerly has much improved version of k-means, and the Pelleg-Moore approach scales extremely well. A single PC with these good algorithms will be much faster than all the cluster or GPU based solutions (which usually compare only to the dead slow Lloyd algorithm, and which are also based on this "parallelization friendly" algo - the nice ones like Elkan's, Hamerly's, Pelleg-Moore's are not GPU parallelization friendly because they do involve "if" statements to avoid unnecessary computations, and GPUs are bad at branching).
