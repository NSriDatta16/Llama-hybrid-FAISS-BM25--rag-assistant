[site]: datascience
[post_id]: 117590
[parent_id]: 
[tags]: 
Empirical papers for ML methods in large categorical datasets?

Just wanting to see if anyone has any additional resources regarding appropriate machine learning methods for large categorical datasets (150,000 observations, health survey, ~10-15 features). Need help because it takes so long to run these algorithms, so would be good to have a handle on some general indicators to move towards. It's hard finding the specifics of my concerns (and there are many). Some things I've looked into and considered: We have a massive amount of missing data, sometimes 90%. Had looked into forms of missingness, looks like our result will be biased regardless. Best imputation methods for this, if this is indicated? Or does evidence show KNN more appropriate, perhaps CCA on its own? Have mainly used MICE. Most appropriate ML method for this form of data? All categorical, some multi-level, others binary. From what I've seen, RF is likely to be most appropriate, but happy to be shown alternative papers/evidence on better methods. Balancing vs not-balancing the data? Outcome is 1% of the data. I've seen reasons for both, we've presently used under-sampling, as it was just computationally efficient. Not sure if other methods induce less bias in this instance. Whether or not we perform some form of variable selection (using something like VSURF). Again have heard that this is obviously good for predictive ability and reducing error, but I've also seen this may essentially reflect P-hacking (correct me if this is incorrect). Thanks!
