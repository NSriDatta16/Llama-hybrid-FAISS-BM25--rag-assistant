[site]: crossvalidated
[post_id]: 326585
[parent_id]: 326583
[tags]: 
The benchmarks described in https://github.com/jcjohnson/cnn-benchmarks are very instructive. If you want to estimate how much slower your model will be,a rough guideline is to do one forward and backward pass on the code described above, divide it by the time taken on a GPU, and you will get an idea of how many X your model will be slower. GPUs are not "necessary", they are just helpful. I ran into similar issues a while back, when I was training some of the models on a machine which did not have a gpu. Unfortunately, some of the open source code assumes that you will have GPUs at your disposal, since otherwise the training time is irrationally high. If you run the code you pull on github, you might have to spend a reasonable amount of time downgrading the code to work on "cpu only mode" (Its not merely a flip of a switch, you might need to hack with the actual code, even those you might get from reliable places like FB research..I am speaking from experience) If you have access to only a CPU, it might not be worth training a model, simply download a trained model, and backpropopgate through the last 1-2 layers. As an instructive example, at TEST time, faster-rcnn is 10% slower (source - https://github.com/rbgirshick/py-faster-rcnn ) on CPU alone; at training time, I can only imagine.
