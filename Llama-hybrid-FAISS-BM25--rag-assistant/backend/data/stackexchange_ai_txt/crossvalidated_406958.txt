[site]: crossvalidated
[post_id]: 406958
[parent_id]: 406496
[tags]: 
I'll frame this in the context of linear regression. Say your model is a relatively simple one. Something like $$ \begin{align} \beta_0 &\sim \mathcal{N}(0,1)\\ \beta_1 &\sim \mathcal{N}(0,1)\\ y_i &\sim \mathcal{N}(\beta_0 + \beta_1 x_i, \sigma) \end{align}$$ Here, $\sigma$ is known. You then stumble upon a new input $x_n$ and would like to predict $y_n$ . There are a few ways to, as you say, how do we predict a new value given the fact that we have only points that follow some distribution. I'll outline only one here and assume we are working in a language like R. Some pseudocode for how to compute this is as follows: mean_samples And this totally makes sense in the context of Bayesianism. The mean of the likelihood is a random variable. It has expectation, variance, etc. Why not just approximate the mean of the distribution of the mean of the likelihood and use that as a prediction? You note that we have to store the samples in order to do this sort of computation, and you're right. But we don't usually need millions of samples. A few thousand might do fine, especially with new methods which find the typical set very quickly. There are other ways to do this (e.g. MAP, median, etc), but the mean of the posterior samples is easiest to understand.
