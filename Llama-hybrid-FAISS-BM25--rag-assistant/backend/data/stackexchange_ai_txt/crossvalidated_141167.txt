[site]: crossvalidated
[post_id]: 141167
[parent_id]: 141100
[tags]: 
I haven't worked through the details yet, but here is a paper that might help: http://www.utdallas.edu/~herve/abdi-WiresCS-mfa-2013.pdf You need to somehow 'weight' the contributions from different participants. The basic idea, with steps, is as follows (assuming each questionnaire has the same number of questions and each participant answers the same number of questions - see the paper for a genearlization). I'm following the "MFA as simple PCA" section: Step0: Form 10 datasets (one for each participant) for each participant's set of answers. So each will have 8 rows (one for each questionnaire) and K columns (one for each answer). Step1: For each dataset, normalize the columns such that the column mean is 0 and the sum of squares of the columns = 1. Step2: Run a PCA on each normalized dataset, obtaining the SVD for each. Note down the first (largest) singular value per dataset, s. The inverse of this value will be used as a weight. Step3: Concatenate the 10 datasets into one matrix, but multiplying each dataset by the inverse of it's first singular value (i.e. 1/s for each participant's dataset); call this 8 by 8*K matrix X. Step4: run a SVD on X to get: X = PDQ^T (where Q^T is the transpose of Q). Step5: PD gives the factor scores, and the loadings for the kth participant's dataset are given by (1/s)*Qk where Qk is the kth partition of Q. Step6: Maybe it's easier to read the paper than to read my quick summary of one part of it! I hope that helps anyway.
