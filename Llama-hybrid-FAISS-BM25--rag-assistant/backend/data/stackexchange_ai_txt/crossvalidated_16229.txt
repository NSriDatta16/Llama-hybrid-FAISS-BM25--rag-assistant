[site]: crossvalidated
[post_id]: 16229
[parent_id]: 
[tags]: 
Calculate average frequency of an event based on probability and number of chances per hour

I do not have any idea how to calculate this, but I do have the data points available. I have a computer system that has a setting for something -- garbage collection ("gc") -- to occur with p probability on any given time that a certain process runs (I'll call that process "the process"). I have the frequency of the process itself running. I'd like to know how often, on average, will the gc run based on the probability setting, and how will that change as I increase that setting? The probability is 1% chance any time "the process" runs, "gc" will be run. If I know that "the process" runs 1000 times per hour, what is the average frequency that "gc" will run? Also, I'd love to see a simple equation for this so I can plug in different values for X. So, in summary: frequency, "f", of "the process" = 1000 times/hour probability, "p", that "gc" will run for any given "the process" = 1% how many times per hour, on average, will "gc" run? Or, how many minutes, on average, between runs of "gc"? Is there a simple formula I can use to change values of "f" and "p" so I can tune the value of "p" with real world data?
