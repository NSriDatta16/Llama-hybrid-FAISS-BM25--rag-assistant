[site]: datascience
[post_id]: 124173
[parent_id]: 120358
[tags]: 
Thanks to Bruno Lubascher's answer I asked ChatGPT and double-checked its answer. The fixed formula was: $V \times d_{model} + V \times d_{model} + N \times (2 \times h \times 3 \times d_{model} \times d_k + 2 \times d_{model} \times d_{ff})$ for respectively: the token input embeddings the final layer (equivalent to a dot product with token output embeddings) the attention heads the feedforward networks. Using the base values of the table (plus $V = 37000$ of vocabulary size), I got a total of 60M which was close enough to me to the 65M. Which means that token input/output embeddings make a total of 38M which is 58% of a total of 65M parameters.
