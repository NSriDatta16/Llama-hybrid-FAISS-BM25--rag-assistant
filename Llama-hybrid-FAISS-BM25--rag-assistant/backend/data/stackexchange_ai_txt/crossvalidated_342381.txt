[site]: crossvalidated
[post_id]: 342381
[parent_id]: 342365
[tags]: 
My question is why don't we just set the first layer with static filters that find various angles of lines, and only train the rest? Based on your description, what you are suggesting is called Extreme Learning Machine (ELM). These are specific types of feed-forward neural networks that basically are different from the rest by having their hidden layers fixed (not trained), and instead just train to adjust the output layer . These layers are randomly initialized (or manually set based on heuristics) and don't change during training. The idea is that, just like you say, by doing this one could train considerably faster than having to optimize for all hidden layers and nodes. According to some literature they tend to generalize better (as they are less prone to overfitting) and even outperform some other methods. I would also guess that this principle could be extended or modified, so you leave an arbitrary number of layers fixed, while training the others; if we leave fixed the first layer and train for the others we get the specific situation you illustrated. Furthermore, a way to implement this I can think of is to "freeze layers", a feature some APIs have already, like Tensorflow (check this question for several alternatives). The most straightforward option there mentioned is to set trainable=False on such variables.
