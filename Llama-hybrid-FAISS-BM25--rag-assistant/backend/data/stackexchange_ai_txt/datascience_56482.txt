[site]: datascience
[post_id]: 56482
[parent_id]: 56447
[tags]: 
Neural Networks can in general be interpreted as a regression problem and as such, you could apply well known ways of dealing with this. This paper gives you a good introduction to different approaches. For instance you can upsample the minority class, or you could do loss weight balancing during training . For instance, consider the data point $x_i$ that can belong to one of two classes $a$ and $b$ . For instance, class $a$ is here the minority. Then you would, during training, multiply the calculated loss with the weight. $$L_i=\begin{cases}l_a\cdot x_i,&\text{if datapoint $x_i$ is in the minority class}\\l_b\cdot x_i,&\text{if datapoint $x_i$ is in the majority class}\end{cases}$$ with $l_a$ > $l_b$ which would be natural for this example. You could also look into continous performance measures that could work nicely with imbalanced dataset. For instance the generalized dice (F1) score . Referenced papers: Provost 2000, Machine Learning from Imbalanced Data Sets 101 Sudre et. al 2017, Generalised Dice Overlap as a Deep Learning Loss Function for Highly Unbalanced Segmentations
