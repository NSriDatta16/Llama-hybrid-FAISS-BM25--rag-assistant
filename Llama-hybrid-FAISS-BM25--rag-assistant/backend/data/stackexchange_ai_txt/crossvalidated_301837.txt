[site]: crossvalidated
[post_id]: 301837
[parent_id]: 65877
[tags]: 
Make sure that your gradients are not going beyond limits or it is also possible that the gradients are becoming zero. This is popularly known as exploding gradients and vanishing gradients problems. One possible solution is to use a adaptive optimizer such as AdaGrad or Adam. I had faced a similar problem while training a simple neural network when I was getting started with neural networks. Few references: https://en.wikipedia.org/wiki/Vanishing_gradient_problem https://www.youtube.com/watch?v=VuamhbEWEWA
