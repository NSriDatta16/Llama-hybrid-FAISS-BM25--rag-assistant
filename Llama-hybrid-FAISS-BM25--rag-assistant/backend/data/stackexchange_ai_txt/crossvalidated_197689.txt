[site]: crossvalidated
[post_id]: 197689
[parent_id]: 
[tags]: 
Is backpropagation a frequentist approach?

Frequentist statistics says that there exists some true underlying parameters for a model . While Bayesian statistics says that there are different beliefs about parameters for a model . In backpropgation, we essentially throw data at a neural network, backpropagate the gradients error, and hope that the network learns the true underlying parameters. Is training a neural network model with backpropagation, a frequentist approach?
