[site]: crossvalidated
[post_id]: 174031
[parent_id]: 174026
[tags]: 
Yes, Of course, the accuracy you should report is from training with the test set held out. But now it's time to make predictions and you want them to be as good as they can be. This is certainly legitimate and important for exactly the reasons you mention. Your most recent data is what you want to test against and what you want final predictions to be made based on. There are some things worth being careful of. In some machine learning algorithms, parameters are sensitive to the size of your training data for instance, in k-Nearest-Neighbour Regression, the optimal $k$ grows with $n$. So you may want to cross validate with several training sizes and extrapolate to find the best $k$ for your full training data. Another example is if you are using a regularized GLM, with an objective function of the form $f(\theta) = L(\theta) + \lambda R(\theta)$ where $L$ is your loss and $R$ is your regularization. If you Total Logloss as $L$ this will be sensitive to training set size, whereas Average Logloss will not. So using average logloss makes the $\lambda$ you establish in CV usable with more data.
