[site]: crossvalidated
[post_id]: 637177
[parent_id]: 576429
[tags]: 
I'm not sure from the short tweet whether the same is meant but it may be a thought that I usually express as: When we sample for some application task, we often have several different needs/purposes, that do not necessarily ask for the same experimental design: data that is application-representative may not have the most efficient design for training/fitting a predictive model for that application, and validation of that model may require yet another experimental design. There is a situation that I quite frequently encounter: I'm chemometrician and work a lot with spectroscopic data. When we built regression models for quantitative prediction, we may have an application situation where the property of interest has some (maybe normal, maybe whatever) distribution. Still we may prefer the training data for our predictive model to have a (roughly) uniform distribution over the relevant space. This is more efficient for our application purposes: Consider here just classical least squares as model, and compare data that follows a) a normal distribution in y or b) a uniform distribution symmetric about the mean y of scenario a). Both have the same number of training cases. When we look at the prediction intervals of the corresponding models, the normally distributed training data leads to a model with narrow prediction intervals at the center, and wider prediction intervals at the tails. Part of this is because uncertainty is always higher at the boundary of the training data range, but the normal distribution compared to the uniform distribution has in addition lower sampling density in the tails. We may say, we prefer to oversample the tails in order to get roughly/almost equal predictive performance over pretty much the whole training range. (Predictions outside the training range will at the very least be marked as such and not trusted at all.) Here's an example: we were training a predictive model for caraway seed oil (2 analytes, PLS model, input data NIR spectra) for a caraway breeding program. A large number of samples was measured with NIR, they were all to be predicted. A subset of given size (70 analyses, of which I "spent" a few on replicates in order to measure portion-to-portion uncertainty within the same genotype) was to be sent for reference analysis of the oil to generate y training data. We went with a 2 stage strategy: first a uniform sample in spectra space was chosen. From the results, a preliminary predictive model was build and all available samples predicted. Further samples for reference analysis were chosen in concentration space via the Kennard-Stone algorithm (initialized with the previous training samples), thus generating a roughly uniform distribution in concentration space. One may want to go even further in this application: since the breeding program wants to increase oil content, they region of interest is the high concentration range (for both analytes). One may thus choose to oversample even more in that region: noone is really interested in knowing whether a particular sample has low or very low oil content. But low uncertainty in the higher concentrations would be of practical use. In addition, we may design training experiments to deliberately vary some other influencing factors (e.g. temperature) in order to train more robust/rugged models. There, it's often hard to know what the distribution during the application measurements will be. I'd think normal distribution is an OK assumption, but not even the mean is known (somewhat above the temperature in the room where the instrument is placed). So instead, we specify a temperature range where the model is supposed to be used, and then would typically go for a uniform coverage of that range (and raise a warning/error if data outside this range is encountered). One example where validation asks for a fundamentally different experimental design than training are one-class classifiers. They are trained with positive instances only. Consider food fraud or falsified pharmaceuticals as application examples. While the fact that you can never know whether the next negative case follows any known fraudulent procedure is a very good reason to set this up as a one-class classifier, for validation you'd still want to test all kinds of known (or thinkable) false products (and not lump them into one figure of merit, but report their performance separately).
