[site]: crossvalidated
[post_id]: 231599
[parent_id]: 231585
[tags]: 
No, there are special cases where these two overlap but in general they are different. It is confusing because they are related and can both be used for nonparametric regression. Also the word "kernel" is ambiguous here as there is a distinction between kernel machines and kernel density estimate type nonparametric regression. An example of overlap is that relevance vector machines (RVMs) which can be seen as type of Bayesian kernelised GLM with sparsity inducing priors, can also be formulated as a Gaussian process. This is described in the Rasmussen & Williams book mentioned in the comment. Gaussian processes are, strictly speaking, a type of distribution where every finite sample has a joint Gaussian distribution. Nothing says that this distribution needs to be used for regression. Gaussian processes can be used for unsupervised learning, such as Gaussian process latent variable models. Gaussian processes can also be used for optimisation. Kernelised GLMs don't really make sense in either of these contexts. There are a couple of other differences: GPs require the kernel to be positive semi-definite, kernelised GLMs do not. fitting kernelised GLMs requires parameter estimation, fitting GPs do not.
