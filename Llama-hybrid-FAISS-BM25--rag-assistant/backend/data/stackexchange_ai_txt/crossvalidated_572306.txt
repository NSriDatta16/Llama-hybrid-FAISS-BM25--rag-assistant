[site]: crossvalidated
[post_id]: 572306
[parent_id]: 
[tags]: 
Neural networks: normalizing data which already lies in [0,1]

I am running neural networks with a sigmoid activation function, and have output data which already lies within [0,1]. However, the minimum value of the data is around 0.05, and the maximum is about 0.2. My question is this: Is there any benefit from using min-max normalization to "stretch" the data over the full range of the sigmoid function's possible output range? Or would it be best if I simply left it as it is. The closest thing I've found to an answer to my question in the literature is here: "the dependent variable does not have to be converted when modeling a binary response variable because its values already fall within this range" (Olden & Jackson, 2002)" However, the obvious difference is that the output values of a classification problem clearly extend from 0 to 1, while I am wondering if the very tight range of my output variables will produce sub-optimal results.
