[site]: crossvalidated
[post_id]: 285528
[parent_id]: 
[tags]: 
Regression Trees' greedy algorithm in Hastie et al. (2009)

I am reading The Elements of Statistical Learning (Hastie, Tibshirani and Friedman, 2009), more specifically the section on regression decision trees (p. 307 of the book). There is something I do not understand about their splitting algorithm. The authors are explaining the mechanism to derive the splitting variable and the split point; they write $-$ my emphasis: " If we adopt as our criterion minimization of the sum of squares $\sum(y_i-f(x_i))^2$ , it is easy to see that the best $\hat{c}_m$ is just the average $y_i$ in region $R_m$: $$ \hat{c}_m=\text{ave}(y_i|x_i \in R_m) $$ Now finding the best binary partition in terms of minimum sum of squares is generally computationally infeasible. Hence we proceed with a greedy algorithm. Starting with all of the data, consider a splitting variable $j$ and split point $s$ , and define the pair of half-planes $$ R_1(j,s)=\{X|X_j\leq s\} \quad \text{and} \quad R_2(j,s)=\{X|X_j > s\} $$ Then we seek the splitting variable $j$ and split point $s$ that solve $$ \min_{j,s} \left[\min_{c_1} \sum_{x_i \in R_1(j,s)}(y_i-c_1)^2 + \min_{c_2} \sum_{x_i \in R_2(j,s)}(y_i-c_2)^2 \right] $$ For any choice $j$ and $s$ , the inner minimization is solved by $$ \hat{c}_1=\text{ave}(y_i|x_i \in R_1(j,s)) \quad \text{and} \quad \hat{c}_2=\text{ave}(y_i|x_i \in R_2(j,s)) $$ For each splitting variable, the determination of the split point $s$ can be done very quickly and hence by scanning through all of the inputs, determination of the best pair $(j, s)$ is feasible. " I am struggling to understand how the greedy algorithm they describe is different from finding the best binary partition. Is it simply because we are limiting the set of admissible partitions to partitions of the form " variable $j$ is above/below $s$ ", hence if we were to consider all possible partitions we should also consider much more complex partitions?
