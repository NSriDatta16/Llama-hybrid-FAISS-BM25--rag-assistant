[site]: datascience
[post_id]: 77663
[parent_id]: 
[tags]: 
decision -tree regression to avoid multicollinearity for regression model?

I read in comments a recommendation for decision treeÂ´s instead of linear models like neural network, when the dataset has many correlated features. Because to avoid multicollinearity. A similar question is already placed, but not really answered. https://stats.stackexchange.com/questions/137573/do-classification-trees-need-to-consider-the-correlation-between-attributes or here In supervised learning, why is it bad to have correlated features? https://www.quora.com/Is-multicollinearity-a-problem-in-decision-trees#:~:text=Decision%20trees%20follow%20the%20non%20parametric%20approach.&text=Though%20single%20tree%20leads%20to,robust%20to%20the%20multi%20collinearity%20 . My problem: I have a dataset of about 30 columns. 10 columns have a high correlation with the target/dependend variable. Data are numerical. I would like to do a prediction (regression model) include all variables if possible? One big problem is to avoid multicollinearity. Is there a decision tree regression model good when 10 features are high correlated? (if I follow the answers of the links, but there is no really good explanation for that). Is there a scientific or math explanation or recommendation (to use decision tree regression)?
