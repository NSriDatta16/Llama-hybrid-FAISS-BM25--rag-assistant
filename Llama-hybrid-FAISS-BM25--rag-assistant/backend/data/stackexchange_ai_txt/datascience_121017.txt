[site]: datascience
[post_id]: 121017
[parent_id]: 121014
[tags]: 
It means that, because each attention head has its own projection, it can learn to capture different aspects of the sequences. For instance, one sentence may focus on negation, while another head may focus on coreference resolution. While these examples serve as illustrations of the concept, they are probably not happening in an actual trained Transformer model, because they are basically bound to human interpretation of language and Transformers are probably not bound by them, especially taking into account that most Transformer models use sub-word tokens instead of word-level tokens, which limit the applicability of word-centered interpretation of language.
