[site]: crossvalidated
[post_id]: 13812
[parent_id]: 
[tags]: 
Modelling worst, average and best case capacity of a system

A system has an incoming request rate of N requests per second. The system hosts a pool of x workers where x >= N. Each worker can complete a single request in approximately t seconds. The maximum parallel requests which can be processed by the system is x. But sometimes a worker can get delayed indefinitely and when it exceeds T seconds the current request is aborted and a new request processed (t . This could happen together for 1% - 100% of the workers for several seconds. During those T seconds the worker is blocked and cannot process any other request. These unprocessed requests are dropped and never processed again. To avoid being indefinitely blocked, the system aborts the current request to start process a new request. On an average n requests are aborted every hour. How do you model this information to find out the following ? The number of dropped requests as a function of requests aborted. The effect of tuning timeout(T), pool size of workers(x), average process time(t) on reducing or increasing the number of dropped requests or it's inverse, the number of processed requests ? The worst, average or best case capacity of the system.
