[site]: crossvalidated
[post_id]: 350368
[parent_id]: 
[tags]: 
Considering Test Error when Computing Classification Confidence

Given is binary classifier, which, together with the class-label also returns a corresponding probability/confidence, e.g., logistic regression. Even with only very few training examples, such a classifier can produce large probability values on individual observations, while the test error is still high (and I don't feel comfortable reporting those to an end user when I know that the model accuracy is very low.) Does anybody know of a line of work which factors in the test error into the prediction confidence of a classifier?
