[site]: crossvalidated
[post_id]: 318370
[parent_id]: 
[tags]: 
Does RMSProp/Adam solve vanishing gradient problem?

RMSProp and Adam both scale the effectively learning rate by dividing the moving average of past gradients (root mean squared). So if the first layer has gradient much smaller than the last layer, each update to the weight should be still similar in magnitude, right? Then why do we need other methods like batch normalization to deal with vanishing gradients?
