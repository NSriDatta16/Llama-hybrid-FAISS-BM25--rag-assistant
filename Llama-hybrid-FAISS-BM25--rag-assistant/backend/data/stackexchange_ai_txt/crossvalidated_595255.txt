[site]: crossvalidated
[post_id]: 595255
[parent_id]: 594375
[tags]: 
This problem looks very much like the masked language modeling task that is used for pre-training of BERT and similar models. The training procedure is: You replace a randomly selected fraction of tokens with a special [MASK] token, or - in your example; You embed the tokens (in this case characters) and process them with a stack of layers. BERT uses the Transformer, but here 1D CNN should do the job; Do a character prediction with a linear projection and softmax. Technically, you need to do it for each character, but you only consider the loss at the places where you replaced the original character with the - . In this way, you can generate as many training instances as necessary.
