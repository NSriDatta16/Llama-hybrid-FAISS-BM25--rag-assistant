[site]: datascience
[post_id]: 100490
[parent_id]: 
[tags]: 
When to tune hyperparameters in deep learning

I am currently playing around with different CNN and LSTM model architectures for my multivariate time series classification problem. I can achieve validation accuracy of better than 50%. I would like to lock down an exact architecture at some stage instead of experimenting endlessly. In order to decide this, I want to also tune my hyperparameters. Question: How do I balance the need to experiment with different models, such as standalone CNN and CNN with LSTM against hyperparameter tuning? Is there such a thing as premature optimization? I am running my training on AWS SageMaker and I can work in parallel if needed. Cheers.
