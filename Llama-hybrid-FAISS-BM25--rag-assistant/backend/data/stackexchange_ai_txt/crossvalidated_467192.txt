[site]: crossvalidated
[post_id]: 467192
[parent_id]: 
[tags]: 
Does log-likelihood cost function in a multinomial classification consider only the output at the neuron that should be active for that class?

Consider a neural network with an output layer of softmax neurons and a log likelihood cost function. For easiness consider one wants to train a MNIST classifier. The output layer will have 9 neurons each one outputting the probability of the corresponding digit. When training with such a configuration, does the cost function consider only the output at the neuron that should be active for that specific digit? For example, suppose parameters are updated at every sample (input image) and the next image is a $7$ . Let me call $a_j$ the activation of the $j$ -th neuron. For this input is the cost function just $C=-\log a_7$ or does $C$ depends also on $a_i, \forall i\in[1,9]$ ? Since in softmax layers increasing one probability decreases automatically the others, I expect the former to be correct while the latter to be redundant. Here in eq. 81 and 82 however, it does not seem so. Consider for example the gradient w.r.t. the biases: in the ref it is expressed as: $$\frac{\partial C}{\partial b_j} = a_j - y_j$$ where $y_j$ is $1$ if $1$ for the seventh neuron (the one that should be active when the image is a $7$ ), $0$ otherwise. I know the formula is correct, but does the cost function consider just $a_7$ ?
