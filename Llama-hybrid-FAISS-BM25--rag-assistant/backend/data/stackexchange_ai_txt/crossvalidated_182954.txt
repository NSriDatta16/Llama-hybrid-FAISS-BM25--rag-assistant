[site]: crossvalidated
[post_id]: 182954
[parent_id]: 
[tags]: 
Classification and random forests in Python: predictions are the same regardless of predictors

I'm working with a small data set of 4 categorical predictors, one binary outcome, and ~90k observations. I've tried fitting a random forest classifier mimicking the iris example from http://blog.yhathq.com/posts/random-forests-in-python.html . However, my challenge is that my predicted values are all the same: 0. I'm new to Python, but familiar with R. Not sure if this is a coding mistake, or if this means my data is trash. Choosing random forests for variable selection before doing a logistic regression. Please me know if you need more information. Not sure how to provide data for a MWE. from sklearn.ensemble import RandomForestClassifier data = train_df[cols_to_keep] data = data.join(dummySubTypes.ix[:, 1:]) data = data.join(dummyLicenseTypes.ix[:, 1:]) data['is_train'] = np.random.uniform(0, 1, len(data)) Predictions of only one class, Type1: In[583]: pd.crosstab(Mytest['type'], preds, rownames=['actual'], colnames ['preds']) Out[582]: preds Type1 actual Type1 17818 Type2 7247 Update: First few rows of data: In[670]: Mytrain[Myfeatures].head() Out[669]: subtype_INDUSTRIAL subtype_INSTITUTIONAL subtype_MULTIFAMILY \ 0 0 0 0 1 0 0 0 2 0 0 0 3 0 0 0 4 0 0 0 subtype_SINGLE FAMILY / DUPLEX 0 0 1 0 2 0 3 1 4 1 When I predict on the training inputs, I get predictions of only one class: In[675]: np.bincount(rf.predict(Mytrain[Myfeatures])) Out[674]: array([ 0, 75091])
