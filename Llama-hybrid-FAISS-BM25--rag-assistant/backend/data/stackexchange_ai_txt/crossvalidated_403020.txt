[site]: crossvalidated
[post_id]: 403020
[parent_id]: 403013
[tags]: 
Well, in Bayesian statistics, you don't just "make up" your priors. You should be building a prior that best captures your knowledge before seeing the data. Otherwise, why anyone should care about the output of your Bayesian analysis is very hard to justify. So while it's true that the practitioner has some sense of freedom in creating a prior, it should be tied to something meaningful in order for an analysis to be useful. With that said, the prior isn't the only part of a Bayesian analysis that allows this freedom. A practitioner is offered the same freedom in constructing the likelihood function, which defines the relation between the data and the model. Just as using nonsense priors will lead to a nonsense posterior, using a nonsense likelihood will also lead to a nonsense posterior. So in practice, ideally one should chose a likelihood function such that it is flexible enough to handle one's uncertainty, yet constrained enough to make inference with limited data possible. To demonstrate, consider two somewhat extreme examples. Suppose we are interested in determining the effect of a continuous-valued treatment on patients. In order to learn anything from the data, we must choice a model with that flexibility. If we were to simply leave out "treatment" from our set of regression parameters, no matter what our outcome was, we could report "given the data, our model estimates no effect of treatment". On the other extreme, suppose we have a model so flexible that we don't constrain the treatment effect to have a finite number of discontinuities. Then, (without strong priors, at least), we have almost no hope of having any sort of convergence of our estimated treatment effect no matter our sample size. Thus, our inference can be completely butchered by poor choices of likelihood functions, just as it could be by poor choices of priors. Of course, in reality we wouldn't chose either of these extremes, but we still do make these types of choices. How flexible a treatment effect are we going to allow: linear, splines, interaction with other variables? There's always the tradeoff between "sufficiently flexible" and "estimatable given our sample size". If we're smart, our likelihood functions should include reasonable constraints (i.e., treatment continuous treatment effect probably relatively smooth, probably doesn't include very high order interaction effects). This is essentially the same art as picking a prior: you want to constrain your inference with prior knowledge, and allow flexibility where there is uncertainty. The whole point of using data is to help constrain some of that the flexibility that stems from our uncertainty. In summary, a practitioner has freedom in selection of both the prior and the likelihood function. In order for an analysis to be in anyway meaningful, both choices should be a relatively good approximation of real phenomena. EDIT: In the comments, @nanoman brings up an interesting take on the problem. One way we can think that the likelihood function is a generic, non-subjective function. As such, all possible models can be included in the functional form likelihood before the prior . But typically, the prior only puts positive probability on a finite set of functional forms of the likelihood. Thus, without the prior, inference is impossible as the likelihood would be too flexible to ever make any form of inference. While this isn't the universally accepted definition of prior and likelihood function, this view does have a few advantages. For one, this is very natural in Bayesian model selection. In this case, rather than just putting priors on parameters of a single model, the prior puts probability over a set of competing models. But second, and I believe more to @nanoman's point, is that this view cleanly divides inference into subjective (prior) and non-subjective (likelihood function). This is nice, because it clearly demonstrates one cannot learn anything without some subjective constraints as the likelihood would be too flexible. It also clearly demonstrates that once someone hands you a tractable likelihood function, some subjective information must have snuck in.
