[site]: datascience
[post_id]: 6913
[parent_id]: 6908
[tags]: 
I think different people probably have varying approaches to this depending upon their background. Often the linux toolset gets over looked, things like awk , sed , grep , cut , paste , sort , uniq and so on, can be combined in many sophisticated ways and are very powerful and scalable, but they aren't for everyone. I suspect many people use a programming language that they already have familiarity with or that they use for their data science work, R , Python , Matlab , Mathematica , C++ , Julia , the list goes on. Others may use commercial products like SAS. Certainly Python, and it's relevant libraries, and R have large users bases and are popular choices. Ultimately you may have to look at a few options and decide which one suits you and your skill set best. As you've already mentioned, WEKA provides an accessible user interface to get started quickly and is certainly a good place to start for early experimentation and to produce results quickly. It has a range of filters which can be used to preprocess data which may be of use to you. It exposes much more of its functionality via its API which is available via Java and this gives you the ability to be much more creative as you build your own solutions to your problems, but you will need to learn to program in Java . Another, more eclectic, approach is that you draw from many toolboxes like R , Python , WEKA , C++ , Apache Mahout etc and fuse them using your own framework written in your favourite programming language. Exactly which data cleansing operations you need to apply, missing value replacement, addition of noise, resampling, etc., will depend more heavily on the specific nature of your data and it's application.
