[site]: datascience
[post_id]: 48703
[parent_id]: 33580
[tags]: 
Based on my experience so far, having too many features as inputs to your NN ,tends to degrade performance *full disclaimer i'm no expert, but smarter people than me have coined a term called The curse of dimensionality. Here is a paragraph I took from Medium Curse of dimensionality and feature reduction The curse of dimensionality occurs because the sample density decreases exponentially with the increase of the dimensionality Good now we know that having too many features is bad for our model performance (or feature to sample ratio which increases significantly) what can we do to solve it? Right now I can think of 3 ways Feature Selection Feature Extraction Ensemble learning of different sub series of those features (yummmyyy :) )
