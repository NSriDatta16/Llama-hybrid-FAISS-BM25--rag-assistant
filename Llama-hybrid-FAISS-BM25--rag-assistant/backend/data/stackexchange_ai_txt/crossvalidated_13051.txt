[site]: crossvalidated
[post_id]: 13051
[parent_id]: 
[tags]: 
Problem on parametric learning when datasets are small

I'm currently writing a program to learn a TAN (Tree-Augmented Bayesian network) classifier from data, and I have almost finished it. I use the algorithm described in Friedman's paper 'Bayesian Network Classifiers' , and for continuous variables, I assume a Gaussian distribution. I have tested my program on some dataset from UCI repo and basically it works fine. However my original task requires incremental training (and the data are all continuous). The problem is that initially the size of dataset is small, and there may be a variable whose values in the whole dataset are the same, which results in a variance of 0. The algorithm I use requires calculating the conditional mutual information of every to attribute variables give the class variable C, below is the equation I use for continuous variables, $$I(X_i; X_j|C) = -\frac{1}{2} \sum_{c=1}^{r} P(c) \log(1-\rho_c^2(X_i,X_j)),$$ $$\rho_c(X_i,X_j)=\frac{\sigma_{ij|c}}{\sqrt{\sigma_{i|c}^2\sigma_{j|c}^2}}$$ As you can see, if one variable's variance is 0, I will run into a zero division error. In order not to crash my program, in such cases I set the variable's variance to 1 hundredth of its expectation or 0.01 if the expectation is 0 (which means all values are 0). However, when there are two such variables and their values are all the same (in the case I came across, all 0), the program still crashes because then it will be calculating log of 0. How can I deal with such situations? As in an all discrete-variable case I use a Dirichlet prior to avoid dividing by zero, maybe I could similarly use some kind of prior for continuous variables? What are the common methods to deal with small datasets? I'm poor in statistics, hoping my question isn't too annoying.
