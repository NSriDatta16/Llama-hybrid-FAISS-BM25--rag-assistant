[site]: crossvalidated
[post_id]: 444920
[parent_id]: 444916
[tags]: 
It is unusual, even with studentization, to see less variability for greater predicted values, and yet that's what you have here. There are three causes to consider: 1) misspecified mean-variance relationship, 2) omitted regressors or 3) undetected non-independence. They imply vastly different approaches to the problem of obtaining valid inference. 1) Misspecified mean-variance relationship GLMs are characterized by a link and a mean-variance relationship. The mean-variance relationship of a negative binomial GLM is the same as that of a quasipoisson model where $V(\mu) = \alpha \mu$ where $\alpha$ is a scale or dispersion parameter. The GLM is also called the iteratively-reweighted least squares, because the estimated variance (taken from the mean) is used to recalculate weights and refit a inverse variance weighted least squares model. This is the iterative process used to fit GLM. GLMs also allow the user to input an ancillary set of weights . Note they are not called prior weights because this is not a Bayesian procedure. Weights can be frequency weights, probability weights (in sampling designs and missingness), or inverse variance weights. Inverse variance weights are used in meta-analyses or hierarchical analyses where heteroscedasticity is present within known clusters of data. Estimating weights from the sample, based on fitted values, however, can lead to shrinkage and anti-conservative inference (overly optimistic SEs) .So applying weights successfully to lead to a nice residual plot seems unlikely. GLM families can be custom built to accommodate nearly any problem, but we must be careful with what we call variability. For instance in your problem, the "fishing process" might be such that any boat has 3 hours out to fish, and they throw 4 nets over the boat, they might catch nothing, or if the nets fill, they can haul those in but most only throw additional nets over one at a time to prevent overfishing. So you might have a variance function like: $$V(\mu) = \alpha \exp( -(\mu-\delta)^2)$$ where the shape of the mean-variance relationship is truncated normal shaped. 2) a presumed omitted regressor would have to meet the following criteria to produce such a residual plot: a strong negative correlation with a highly predictive variable, a very high effect size/predictiveness, a high skew, and concordant association/skewness direction. In this fashion, the apparently highly variable values in the low-predicted domains are simply explained by a dominant association with this variable. 3) Unknown dependence structure Correlation is surprisingly similar to misspecified variance/heteroscedasticity in terms of the problem and solution. Not knowing what "stratum" is, I can't tell if it's the specific "trip" or "expedition": essentially a more granular measure of the fundamental "sampling unit". But with 23 units, we must take caution at over adjustment when a mixed model or a GEE might suffice. In all cases, the superiority of the GEE stands out in particular to me. If you apply the Poisson GEE, the GEE gives valid and efficient inference even with omitted regressors and undetected dependence. I think GEE precludes doing any residual analysis provided you have a large sample size, which you clearly do from the scatter plot.
