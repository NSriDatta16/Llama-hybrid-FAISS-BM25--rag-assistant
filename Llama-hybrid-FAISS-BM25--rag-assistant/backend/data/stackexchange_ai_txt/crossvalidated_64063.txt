[site]: crossvalidated
[post_id]: 64063
[parent_id]: 64053
[tags]: 
I didn't see anything wrong with the results you had. As others pointed out already, you are facing a typical $P \gg N$ situation, where the number of predictors, i.e., features, is much greater than the number of instances. In this situation, SVM is generally a good choice, as its maximum margin property gives some guarantees on the generalization performance. But in this situation, I would go for a linear kernel instead of a polynomial or RBF kernel. Because a linear kernel with 14000 features is already very powerful. If you later find out that the representation power of linear kernel is not enough, you can then try some more powerful kernels instead. As you mentioned that the data are well separable with fewer predictors, you might want to get rid of some predictors in the first place. In general, we try to avoid the $P \gg N$ situation. Maybe you want to use some Lasso methods to do some implicit feature selection for you.
