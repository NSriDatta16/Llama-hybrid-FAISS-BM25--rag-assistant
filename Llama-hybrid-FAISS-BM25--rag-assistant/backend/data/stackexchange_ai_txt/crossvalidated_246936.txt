[site]: crossvalidated
[post_id]: 246936
[parent_id]: 246921
[tags]: 
The motivation to use Gaussian process priors comes from richness of the set of functions described by these priors and small number of kernel parameters required to get the final model. In more details, Gaussian process prior with specified covariance function corresponds to a function from RKHS (Reproducing Kernel Hilbert Space), and these priors are dense in a space of continuous functions. We can vary smoothness of prior by varying smoothness of covariance function which is desirable in many application areas. Another nice property is that this family of priors always contains a function that passes exactly through all the training examples i.e. for a Gaussian process prior approximation $\hat{y}(x)$ and a training sample $D = \{(x_i, y(x_i))\}_{i = 1}^N$ it is easy to get $\hat{y}(x_i) = y(x_i)$, $ i = \overline{1, n}$.
