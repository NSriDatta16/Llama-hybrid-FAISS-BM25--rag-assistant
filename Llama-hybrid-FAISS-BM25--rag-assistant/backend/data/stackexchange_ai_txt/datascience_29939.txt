[site]: datascience
[post_id]: 29939
[parent_id]: 29933
[tags]: 
As far as I understand, in each iteration, Q-learning algorithm predicts the future reward of next step (and next step only) using the machine learning technique in use (be it the CNN, DNN etc.). The Q values should eventually converge to the expected sum , future , discounted reward when taking action A in state S and following the optimal policy. Breaking it down: Expected sum is not exactly the same as "predicted", but close enough for our purposes. And it really does mean sum of the rewards, not a single reward. To differentiate, this is often called the "return" or "utility" Future -> from the step being evaluated onwards until end of episode, or the limit as time goes to infinity for continuous tasks with discounting. Discounted -> a discount factor is only necessary for continuous tasks. And we are multiplying the reward of next step (and that specific next step only) with discount rate, to make it less important than the immediate reward (with the ratio we specified). No, there is no multiplication of the reward. Let's take a look at the line: target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0])) The reward is not being multiplied by anything. What is being multiplied by $\gamma$ is the Q value of the next state. That value represents the total sum of all rewards following on from that point - not a single reward value at all. So, my question is, how does the algorithm takes even further steps (say, 5 steps) ahead into account? It is in the Q values. The pseudocode for the code you are looking at is not: target_for_Q(s,a) = next_step_reward * gamma It is: target_for_Q(s,a) = next_reward + gamma * current_value_of_Q(s',a') Or: target_for_Q(s,a) = next_reward + gamma * estimate_all_future_return This is closely related to the Bellman function for policy evaluation . Intuitively what is happening is that you start with (really poor) estimates for expected return (not expected reward), and update them by inserting observed values of next_reward , s' and a' into the update rule above. The values always represent a learned estimate of total expected return .
