[site]: crossvalidated
[post_id]: 450398
[parent_id]: 
[tags]: 
Choosing a ‘noninformative’ hyperprior distribution

I am trying to better understand hierarchical Bayesian models. I started here: https://blog.dominodatalab.com/ab-testing-with-hierarchical-models-in-python/ And ran into the following sentence regarding choice of distribution to determine hyper-parameters We could assign a prior distribution to choose these hyper-parameters, but then our prior distribution has a prior distribution and it’s priors all the way down. A better alternative is to sample a and b from the distribution f(a,b) ~ (a+b)^(-5/2) where a,b > 0. I’m sure this function looks like math magic to many of you, but an in-depth explanation of where this probability distribution comes from can be found in Andrew Gelman’s book or in this tutorial on Youtube . Here a and b are the hyperparameters of a beta distribution. I understand why we cannot use a prior distribution to sample these (as we would then need another prior to sample the parameters of this one etc. etc.). My questions then relate to the choice of the function f(a,b) ~ (a+b)^(-5/2). Why not just use a flat 'uninformative' prior to sample these parameters? I found a quote in Andrew Gelamn's book (pg 110) which reads: Unfortunately, a uniform prior density on these newly transformed parameters yields an improper posterior density, with an infinite integral in the limit (α+β)→ ∞, and so this particular prior density cannot be used here. My understanding (in layman's terms) of this is we cannot use a flat prior as the resulting integrals we perform to get the joint distributions integrate to infinity. So we can't use a flat prior. So why the suggested function: f(a,b) ~ (a+b)^(-5/2) As stated we can't use a flat prior but we still need another prior that is still relatively 'uninformative' but still 'proper'. I then ran into the following quote from user Sycorax says Reinstate Monica A similar problem is discussed in Gelman, Bayesian Data Analysis, (2nd ed, p. 128; 3rd edition p. 110). Gelman suggests a prior p(a,b)∝(a+b)−5/2, which effectively constrains the "prior sample size" a+b, and therefore the beta hyperprior is not likely to be highly informative on its own. (As the quantity a+b grows, the variance of the beta distribution shrinks; in this case, smaller prior variance constrains the "weight" of the observed data in the posterior.) Additionally, this prior does not set whether a>b, or the opposite, so appropriate distributions of pairs of (a,b) are inferred from all the data together, as you would prefer in this problem. So, based on this we use the aforementioned function because it constrains the values of the hyperparameters: In a relatively 'uninformative' way In proportion to the size of the data. Such that it does not influence the relationship between them So, Is my understanding correct? :) (Appreciate that this is quite a long unconventionally formatted question so happy to answer questions.)
