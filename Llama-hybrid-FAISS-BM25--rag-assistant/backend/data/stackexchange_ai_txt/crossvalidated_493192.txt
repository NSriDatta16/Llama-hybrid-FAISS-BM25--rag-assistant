[site]: crossvalidated
[post_id]: 493192
[parent_id]: 
[tags]: 
Questions regarding implementation of cross-validation

I have a paper coming up and I would like to clear some questions regarding cross-validation, because I could not find this information explicitly stated anywhere in literature or it differs. I understand that cross-validation is used for finding the optimal hyperparameters of a model*. Does this mean that for every training/validation split of the cross-validation, a new model (with reinitialized weights) with the current set of hyperparameters to be evaluated, is trained? For example, using a 10-fold cross-validation, we have trained 10 different models with 10 different training/validation splits but with the same hyperparameters. This brings me to the second question. After we have trained 10 models and achieved 10 performance scores in a cross-validation, do we average the scores, or take the model with the best score and evaluate it against a test set that was withheld in the beginning? Since I would like to test my model (lets say I have found the hyperparameters) on a known dataset, and this dataset doesn't have evaluation benchmarks (which samples to take for training and which for test), is a valid evaluation to do a 10-fold cross-validation on this dataset, without withholding a test set in the beginning, and plot a confusion matrix for each training/validation split, summing all matrices in the end element-wise? An alternative evaluation would be to average all 10 accuracies returned. *the model is of a convolutional neural network, the task is classification
