[site]: crossvalidated
[post_id]: 639287
[parent_id]: 
[tags]: 
What is the "right" way to define "extreme" in a hypothesis test?

In frequentist hypothesis testing, the value $P(\text{data as extreme as observed} | H_0)$ is calculated, and if it is less than a certain threshold, $H_0$ is rejected. I understand that the notion of "extreme" needs to be defined, which depends on $H_1$ . It might be defined explicitly using likelihood ratio, but often, as far as I have seen in the literature (in my case, biology), the dependence on $H_1$ is implicit. This raises the question: is what people use the "right" definition of extreme? Now, this might sound like I'm being pedantic. But the reason I am asking this is that, on my dataset I am analyzing, I defined a statistic and an intuitive definition of extreme, and using sampling I calculated the pvalue $P(\text{data as extreme as observed} | H_0)$ . I was all happy when I saw that this value was very small. Since my alternative hypothesis $H_1$ was also well defined, I could calculate $P(\text{data as extreme as observed} | H_1)$ as well, and saw that this value was also small. This made me wonder: How can I interpret this? Intuitively, this makes me think that neither $H_0$ nor $H_1$ are good hypotheses, and the "reality" (loosely defined, of course), is something else. Another interpretation I can think of is that my data (or my summary statistic at least) is not informative about these hypotheses. Another way to see it is that I rejected the composite hypothesis $H_C:H_0 \lor H_1$ (if the sum of those probabilities are also small). If I do not report $P(\text{data as extreme as observed} | H_1)$ , and only report the pvalue, this might actually convince at least some people that I have been able to reject $H_0$ , and since the alternative that intuitively led to the test was $H_1$ , this might seem as an evidence for $H_1$ to those people. This tells me that there might be at least some papers out there that have a similar problem. Has anyone looked into this? Is my concern justified? If this is in fact a point of concern, what is the remedy (besides abandoning the notion of pvalue altogether and going for Bayesian testing)? Should people be asked to always calculate and report $P(\text{data as extreme as observed} | H_1)$ as well as pvalue? Would that solve the issue?
