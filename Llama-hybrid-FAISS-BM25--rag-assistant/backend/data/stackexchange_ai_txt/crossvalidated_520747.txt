[site]: crossvalidated
[post_id]: 520747
[parent_id]: 520284
[tags]: 
Given the MA(1) model $x_t=\mu + z_t + z_{t-1}$ , you can think of your vector of observations $\mathbf{x}=(x_1,x_2,\dots,x_n)$ as coming from an intercept-only regression model $$ \mathbf{x} = \mathbf{1}\mu+\boldsymbol\epsilon $$ where $\mathbf{1}=(1,1,\dots,1)^T$ , $\boldsymbol\epsilon \sim N(\mathbf{0},\mathbf{\Sigma})$ and $\mathbf\Sigma$ is tridiagonal with $2\sigma^2$ on the diagonal and $\sigma^2$ on the two off-diagonals. The general least square solution of $\mu$ is then unbiased and efficient and is given by $$ \hat\mu =(\mathbf{1}^T \mathbf{\Sigma}^{-1} \mathbf{1})^{-1} \mathbf{1}^T \mathbf{\Sigma}^{-1} \mathbf{x}. $$ This is simply a weighted average of the observations with weights proportional to the sum of each column of $\mathbf{\Sigma}^{-1}$ . This MLE is also computed by the R arima function (by numerically maximising the likelihood computed via the Kalman filter) when fitting an MA(1) model including an intercept. A closed form solution can be derived however. The entries of $\mathbf{\Sigma}^{-1}$ are given by $$ (\mathbf{\Sigma}^{-1})_{ij}=\frac{\sigma^{-2}(-1)^{i-j}}{2(n+1)} \begin{cases} (n-i+1)j &\text{for} & j see Huang & McColl (1997), eqs. (22) and (23) . From this, for odd $n$ , one can show that the even columns of $\mathbf\Sigma^{-1}$ sum to zero whereas the sums of the odd columns are all equal so the MLE of $\mu$ is $$ \hat\mu = \frac2{n+1}(x_1 + x_3 + \dots + x_n). $$ This estimator has variance $\operatorname{Var}\hat\mu=\frac{4\sigma^2}{n+1}$ , whereas the sample average has variance $\operatorname{Var}\bar x=\frac{4\sigma^2}n (1-\frac1{2n})$ . For even $n$ the weights of the MLE have a certain regular oscillatory behaviour. For example, for $n=12$ , the MLE is $$ \hat\mu=\frac1{42}(6 x_1 + 1 x_2 + 5 x_3 + 2 x_4 + 4 x_5 + 3x_6 + 3x_7+ 4x_8 +2x_9+ 5x_{10}+ 1x_{11}+ 6x_{12}). $$
