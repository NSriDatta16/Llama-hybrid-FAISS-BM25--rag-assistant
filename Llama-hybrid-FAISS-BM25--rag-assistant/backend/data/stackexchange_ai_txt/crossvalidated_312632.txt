[site]: crossvalidated
[post_id]: 312632
[parent_id]: 194035
[tags]: 
Personally I'm having difficulty thinking of a situation where the frequentist answer would be preferred over a Bayesian one. My thinking is detailed here and in other blog articles on fharrell.com about problems with p-values and null hypothesis testing. Frequentists tend to ignore a few fundamental problems. Here is just a sample: Outside of the Gaussian linear model with constant variance and a few other cases, the p-values that are computed are of unknown accuracy for your dataset and model When the experiment is sequential or adaptive, it is often the case that a p-value can't even be computed and one can only set an overall $\alpha$ level to achieve Frequentists seem happy to not let the type I error go below, say, 0.05 no matter now the sample size grows There is no frequentist prescription for how multiplicity corrections are formed, leading to an ad hoc hodge-podge of methods Regarding the first point, one commonly used model is the binary logistic model. Its log likelihood is very non-quadratic, and the vast majority of confidence limits and p-values computed for such models are not very accurate. Contrast that with the Bayesian logistic model, which provides exact inference. Others have mentioned error control as a reason for using frequentist inference. I do not think this is logical, because the error to which they refer is the long-run error, envisioning a process in which thousands of statistical tests are run. A judge who said "the long run false conviction probability in my courtroom is only 0.03" should be disbarred. She is charged with having the highest probability of making the correct decision for the current defendent . On the other hand one minus the posterior probability of an effect is the probabiity of zero or backwards effect and is the error probability we actually need.
