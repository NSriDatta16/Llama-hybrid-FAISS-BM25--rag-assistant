[site]: datascience
[post_id]: 121558
[parent_id]: 121467
[tags]: 
It depends on the architecture chosen, but generally speaking, they do have some differences that can be measured as follows: How they learn and the training speed Variational Autoencoder learn by modelling explicit densities. On the other hand GANs are a min-max game and for that reason they learn based on competition. Because of the non-cooperative nature of GANs their convergence is harder to be ensured and for that reason while training you can observe more oscillations and variability. Nevertheless, this is not exactly bad, it will depend on the variability that you want to introduce to your synthetic data. GANs are harder to optimize and they tend to suffer from mode collapse. This can usually be mitigated with the right loss function (aka you want to ensure a more cooperative training). Both are not exclusive, and you can see some architectures that combine the pros and cons of both worlds, such as TimeGAN. Sampling space VAE generate new records by reconstructing the data from a low-dimensional representation of the original records. This process introduces some noise, but allows the generation of data with quality. GANs generate data from any random input, which allows the generation of more diverse samples when compare to VAE. This poses a huge benefit in cases of augmentation for instance. In a nutshell, GANs are harder to train, but when well fine tune can generate outputs with bigger variability and also more realistic when compared to VAE. The choice will mainly depend on the objective of the generated data - if you want to stress test a model or even augment fraud cases, GANs are better candidates. If you just want to replicate more of the same data for compression purposes for instance, VAE are a great way to go. Attention models can be also very interesting indeed, but will depend on the data types that you want to focus on (structured data, images, text, etc.)
