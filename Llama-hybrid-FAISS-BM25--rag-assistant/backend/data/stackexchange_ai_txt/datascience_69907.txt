[site]: datascience
[post_id]: 69907
[parent_id]: 
[tags]: 
80-20 or 80-10-10 for training machine learning models?

I have a very basic question. 1) When is it recommended to hold part of the data for validation and when is it unnecessary? For example, when can we say it is better to have 80% training, 10% validating and 10% testing split and when can we say it is enough to have a simple 80% training and 20% testing split? 2) Also, does using K-Cross Validation go with the simple split (training-testing)?
