[site]: crossvalidated
[post_id]: 222997
[parent_id]: 222949
[tags]: 
Akaike weights only provide information about the set of models from which they are calculated, so in your example, you can't really learn anything from comparing weights for the glmm set of models to weights for the glm set of models (i.e. Akaike weights won't tell you whether the random effect is appropriate). There's a good discussion on testing random effects here . It does specifically state "do not compare lmer models with the corresponding lm fits, or glmer/glm; the log-likelihoods are not commensurate". That said, there is a worked glmer example by Ben Bolker here that does explicitly compare log-likelihoods and AICc values between glm and glmer models. If you do go this route, it will be the AICc values that you want to compare between glm and glmer, not the delta or weight values (which are only meaningful within a set). The fact that the variance of your random effect is high (and sd relatively low) suggests to me that you should retain the random effect. Likewise, comparing the conditional and marginal R2 values (0.8 vs. 0.01) suggests that the random effect is explaining a lot of the variation in your response. Update It doesn't make sense to ask whether the glmer weight values are "too low". An Akaike weight is the probability that a model is the 'best', given the data and the set of models under consideration. If the top models in a given set all have low and similar Akaike weights, it just means that no one model in that set stands out as being much better than the other models in that set... it doesn't mean that the top models in your set are bad models in an absolute sense. It doesn't tell you either way. Akaike weights are only useful for relative comparison. To assess goodness of fit in an absolute sense, you could look at R2 values. Your R2c of 80% suggests to me that that particular model is very good, though that's mostly driven by the random effect (i.e. individuals varied a lot in habitat use). Since your aim is to get fixed effect coefficients for use in a RSF, and since no single model stands out as the 'best', model averaging would be a good approach. If mod is your glmer object, you can get model-averaged coefficients to use in your RSF as follows: mod_dredge Since your fixed effects have relatively low explanatory power (R2m = 1%), an RSF based on those coefficients may not have much predictive power. As for what you can do to improve this model, it's tough to say... it could just be a biological reality that your study animals vary a lot in habitat use (thus the important random effect), but overall habitat use by these animals is simply not strongly related to the set of specific fixed effects you have examined.
