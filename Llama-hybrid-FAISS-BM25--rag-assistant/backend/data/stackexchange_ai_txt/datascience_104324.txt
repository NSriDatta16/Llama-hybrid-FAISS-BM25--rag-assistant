[site]: datascience
[post_id]: 104324
[parent_id]: 103983
[tags]: 
There could be many reasons for deep learning to have high variance in evaluation metric performance. Here are a couple of ideas: Initialization: Deep learning models are initialized with random parameter values. Different starting parameters could result in final parameter values, especially if there are few epochs. Traditional machine learning might not have random parameter initialization. Optimization: Deep learning is often optimized with stochastic gradient descent (SGD) which does not have convergence guarantees. Traditional machine learning algorithms can be optimized with other methods that have convergence guarantees. Depth: Deep learning is a stack of non-linearities which is a complex system that could result in different solutions in different runs. Traditional machine learning might not have the same complexity.
