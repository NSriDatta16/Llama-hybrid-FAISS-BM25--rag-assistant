[site]: datascience
[post_id]: 76846
[parent_id]: 76844
[tags]: 
You're right, the idea is that, if $\alpha d$ is not used, then you might have $\hat{\theta_i}$ bigger than 1. That being said, I don't think the main reason of Laplace smoothing is to have probabilities greater than 0. I think the important thing is that it acts as a regularization technique (a smoother, indeed). The smoother works the following way: we are mixing the empirical observations ( $\frac{x_i}{N}$ ) and the theoretical distribution without data ( $\frac{1}{d}$ ), and $\alpha$ is a parameter that controls how much do we care about the theoretical distribution. The idea is: the more we trust the theoretical distribution, the more regularization we are adding. It even has a Bayesian interpretation in terms of priors and posteriors. So I don't think it has to do with probabilities being between 0 and 1, I think it has to do with performing the mixture of distributions (or update of the posterior, if you want) properly.
