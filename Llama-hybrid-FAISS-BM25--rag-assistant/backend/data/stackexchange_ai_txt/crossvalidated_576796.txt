[site]: crossvalidated
[post_id]: 576796
[parent_id]: 357963
[tags]: 
Some answers are already provided, while I would like to point out regarding the question itself measure the distance between two probability distributions that neither of cross-entropy and KL divergence measures the distance between two distributions-- instead they measure the difference of two distributions [1]. It's not distance because of the asymmetry, i.e. $\textrm{CE}(P,Q) \ne \textrm{CE}(Q,P)$ and $\textrm{KL}(P,Q) \ne\textrm{ KL}(Q,P).$ Reference: [1] I. Goodfellow, Y. Bengio, A. Courville, and Y. Bengio, Deep learning, vol. 1 (MIT Press Cambridge, 2016).
