[site]: crossvalidated
[post_id]: 167716
[parent_id]: 167649
[tags]: 
You could try glmer , which allows a variety of non-normal distributions. Your data are not "zero-truncated" -- your response is time which is perforce non-negative. No truncation happened. It's just not normally distributed data. Do the zero time observations correspond to genuinely instantaneous events? Or do you actually have grouped observations .. whereby t=0 actually means 0 short time interval ? Later: the OP has supplied raw data, so in light of that, I have changed my answer. The data set is not very large (65 observations), divided almost equally between 0's and non-zeros (32, 33). The non-zero observations are very long-tailed. I'm not sure how much analysis a data set like this can support. As a first crack, you can dichotomize time to 0 or 1, according as time is 0 or greater than 0. Then build up a model using logistic regression. Note that most of these variables are not random effects. Treatment, day_time and Area should be fixed. Trial is possibly random, but possibly also irrelevant. I ran the following: dat$time2 In model M2, the variance of random effect trials was 0, so there doesn't seem to be much difference between trials as such. Here are the results of model M1. Call: glm(formula = time2 ~ area + day_time + treatment, family = binomial(), data = dat) Deviance Residuals: Min 1Q Median 3Q Max -1.9621 -0.6714 0.3580 0.6508 2.5680 Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) 1.7673 0.7667 2.305 0.0212 * areaB -1.1782 1.0218 -1.153 0.2489 areaC -3.1522 0.7882 -3.999 6.36e-05 *** day_timeB 0.9479 0.7688 1.233 0.2176 day_timeC -0.3229 0.9400 -0.343 0.7312 treatmentOrca2 ON -1.8748 0.8734 -2.147 0.0318 * treatmentSR2 ON -0.9380 0.7753 -1.210 0.2264 --- Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 90.094 on 64 degrees of freedom Residual deviance: 62.176 on 58 degrees of freedom AIC: 76.176 Number of Fisher Scoring iterations: 5 It looks as if Area C matters and treatment Orca2. Note that glm() is part of base R and glmer() is in package lme4 . I'm not sure it's worth doing much more than this on your data. If you want to go further, you need to treat this as a ZIP design. The non-zero stuff seems to be a gamma, or something like it. As a first pass, you could try and model a gamma to the non-zero part of the data and see if what you get makes sense. In Poisson ZIP's, the zeroes belong either to the Poisson, or to the "inflation". When the non-zero part is a continuous distribution, strictly speaking, you can't model a gomp of zeroes, so you would need to jitter the data away from zero. I guess the question you need to ask yourself is what you believe the treatments are going to do to the times? Do they basically change the number of zeroes (in which case my binomial kludge is good enough), or will they change the non-zero times? In any case, I don't think you have enough data to go too deeply into this. If you had more, I would suggest fitting two models: a binomial model that looks at the zero/non-zero dichotomy, and something else (possibly gamma) to look at the non-zero values on their own.
