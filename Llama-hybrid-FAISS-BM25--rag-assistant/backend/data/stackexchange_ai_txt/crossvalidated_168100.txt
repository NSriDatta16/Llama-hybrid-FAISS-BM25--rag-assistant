[site]: crossvalidated
[post_id]: 168100
[parent_id]: 
[tags]: 
How to calculate ideal amount of reps needed for good average in R

Basically, I have some samples I'm processing to get moisture results on and I want to get a good average on the readings based on the ideal amount of repetitions. Duplicates is not enough and 10 reps would take too long. So far, I have run a control sample hundreds of times across many days and months, and I calculated off of those results what I think would be the best amount of reps before I start wasting time. Example moisture results: 10.5, 11.5, 12.2, 10.8, 10.9, 11.8, [hundreds more results from the same sample that all range between 10.5 and 12.5]. The way I did this, which will probably make some statisticians cry, is to take the results and average them with the next 2,3,4,5, up to 10 runs and compare the min, max, and averages of each groups of averages. So for my 2 rep test, I took the first result and averaged it with the second. Then I took the second result and averaged it with the third. At the end, I took the difference between the min and max of the averaged data and it came out to 1.8. This was "better" than individual samples as the min/max difference was 2.0. I then did the same test with 3 reps instead of just 2. So, the first, second, and third results were averaged, then the second, third, and fourth results were averaged, etc. I did this in Excel all the way up to 10 reps. Results: Reps StDev Min Max Range 1 0.48 10.5 12.5 2.0 2 0.40 10.5 12.3 1.8 3 0.35 10.5 12.2 1.7 *4 0.33 10.6 12.0 1.4 *5 0.31 10.7 12.0 1.3 6 0.30 10.8 12.0 1.3 7 0.29 10.8 12.0 1.2 8 0.29 10.8 11.9 1.1 9 0.28 10.8 11.9 1.1 10 0.28 10.9 12.0 1.1 Based on time and resources, I decided that 4 or 5 reps is the "sweet spot". I know this isn't the right way to do it, which is why I'm asking here. The results I used are just sequential, in the order they were created across many days and months. So, I decided the sweet spot would be at 4 reps based on how long each rep takes. This isn't the right way to do it, as it's just taking the results sequentially and averaging them in groups. So some are run on the same days, some different days, etc. Is there a way I can run my control sample data through R and it give me a better way to figure this out? I've never really done any statistics, so I don't even know what this is called to search for a solution. Everything I tried resulted in search results like "Build the most mass in the shortest amount of time using X amount of reps in the gym" and such. I'm wanting to do this in R so I can process it through C# and R.NET. This is a win-win for me. I'm learning how to use R through R.NET and this would help me find the most accurate results before the cost in time and resources for doing multiple reps starts to outweigh the need for a good, solid result. tl;dr: have hundreds of process result reps of same sample, how can I use that data in R to see how many reps to use on other samples before "overdoing" it?
