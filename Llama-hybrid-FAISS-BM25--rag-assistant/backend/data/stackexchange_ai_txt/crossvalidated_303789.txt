[site]: crossvalidated
[post_id]: 303789
[parent_id]: 303784
[tags]: 
The posterior will be $p(\theta|y,x)$, the posterior over the weights. The reason for this is you're not estimating $y$, you're estimating $\theta$. You can then use the posterior over the weights to ${predict}$ a new $y$ given a new $x$, using the posterior predictive distribution $$p(y_{new}|y_{previous},x_{previous},x_{new})=\int p(y_{new}|x_{new},\theta)p(\theta|y_{previous},x_{previous})d\theta.$$ It helps to write out explicitly what your likelihood and priors are. If you're placing a prior on a parameter then you're going to have a posterior over that parameter. You wouldn't place a prior on $y$, so why would you have a posterior over $y$? Try looking at the wikipedia page for bayesian linear regression ( https://en.wikipedia.org/wiki/Bayesian_linear_regression ) or one of the previously asked questions on this website ( Bayes regression: how is it done in comparison to standard regression? ).
