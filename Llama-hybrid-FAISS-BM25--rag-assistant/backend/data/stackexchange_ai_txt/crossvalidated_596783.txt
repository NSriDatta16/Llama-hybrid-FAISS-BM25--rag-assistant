[site]: crossvalidated
[post_id]: 596783
[parent_id]: 
[tags]: 
Logistic Regression With Imbalanced Data?

In class we are learning about the SMOTE (Synthetic Minority Oversampling Technique) algorithm. As I understand, this algorithm can be used to increase the effectiveness of Machine Learning models when the datasets are imbalanced. An example our professor gave was that suppose you have a dataset with information belonging to medical patients (e.g. age, gender, height, etc.). Suppose that the response variable is whether or not the patient has a specific disease or not - thus, the goal would be to predict whether or not a new patient has a disease (i.e. supervised binary classification). Now, imagine that this is a rare disease and only 5% of the patients in your dataset has the disease. If you fit a Machine Learning model to this data (e.g. Random Forest), the model can "get away" with saying that all patients do not have the disease - and still produce a very good accuracy (although the F-score will be bad)! This is because the Machine Learning model will not have observed enough disease cases to effectively "learn" the difference between diseased cases and non-diseased cases - and therefore poorly generalize, and state that all future patients do not have the disease. In practice, the SMOTE algorithm is allegedly able to partly rectify this problem. Suppose there is a dataset of 100000 patients and 5% of the patients (5000 patients) have the disease. Suppose you randomly select 10000 patients (10% of the original dataset) who have the disease and the 5000 patients that have the disease - now, if you create a new dataset with these 15000 rows, you might be able to create enough of a "contrast" in such a way that the Machine Learning model will have a better way of distinguishing between Disease and Non-Disease cases. What is described here is a general "Oversampling/Undersampling" technique - the SMOTE algorithm is a more sophisticated version of this same premise. I have the following question: When reading online, the SMOTE algorithm is often used in more Machine Learning Model contexts compared to "Traditional Statistical Models" contexts. For example, I see SMOTE being used more alongside models such as XGBOOST and Random Forests when compared to SMOTE being used for Regression Models. Suppose in this above example with the medical patients and the imbalanced data, I want to use a Logistic Regression model for interpretability reasons (e.g. Odd's Ratio, Statistical Significance of Coefficients, etc.) : In theory, is there anything which suggests that Oversampling/Undersampling approaches are inherently ill-suited for a Logistic Regression model? Or should such approaches only be used alongside Machine Learning models?
