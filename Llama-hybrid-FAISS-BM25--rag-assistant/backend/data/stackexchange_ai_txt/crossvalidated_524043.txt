[site]: crossvalidated
[post_id]: 524043
[parent_id]: 
[tags]: 
What are exact inputs and their dimension for Decoder part of the transformers?

In the Illustrated transformer article from Jay Alamar, in the decoder side paragraph, he says The encoder starts by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. What I understand, the output of the encoder is a matrix (Number of words x Embedding Length). So where those K and V come from? What is the dimension of this K and V? Can someone please explain the exact input and its dimension for Decoder ?
