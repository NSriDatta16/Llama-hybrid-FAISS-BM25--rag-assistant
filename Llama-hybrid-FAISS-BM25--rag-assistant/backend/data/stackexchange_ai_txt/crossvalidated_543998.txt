[site]: crossvalidated
[post_id]: 543998
[parent_id]: 543994
[tags]: 
Yes, the random forest is probably overfitting. Assuming that your training and test sets are drawn from the same distribution (shuffling the data before splitting is a good way to ensure this), the improved accuracy on the training set compared to the test set is due to characteristics of the training set rather than the data in general - the definition of overfitting. In this case you may in theory be able to get better test set performance by preventing the overfitting, but it's hard to say for sure.
