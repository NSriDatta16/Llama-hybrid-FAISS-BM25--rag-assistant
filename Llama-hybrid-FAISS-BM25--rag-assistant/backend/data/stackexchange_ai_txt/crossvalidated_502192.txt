[site]: crossvalidated
[post_id]: 502192
[parent_id]: 
[tags]: 
Prove the linear transformation of Gaussian distribution from Bayesian perspective

I understand the linear transformation property of Gaussian distribution, i.e., if $x\sim\mathcal{N}(\mu, \sigma^2)$ , then $cx\sim\mathcal{N}(c\mu, c^2\sigma^2)$ given $c$ is a constant. If we write this as a marginalization: $$p(f(c)|c) = \int p(f(c)|c, x)p(x)dx$$ and here $f(c) = cx$ . $p(x)$ can be expressed as $\dfrac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2\sigma^2}(x-\mu)^2}$ . But how do we obtain $p(f(c)|c)$ from the above marginalization form (or the posterior predictive distribution, here I simplified the posterior to just $p(x)$ ), that is to get $$\quad \, p(f(c)|c) = \dfrac{1}{\sqrt{2\pi}c\sigma}e^{-\frac{1}{2c^2\sigma^2}(f(c)-\mu c)^2} \quad\text{?}$$ What troubles me is $p(f(c)|c, x)$ . I think it is $1$ , as it is deterministic when $c$ and $x$ are given, but I'm not sure how to proceed.
