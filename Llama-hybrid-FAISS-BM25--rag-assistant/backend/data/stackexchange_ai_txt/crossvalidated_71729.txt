[site]: crossvalidated
[post_id]: 71729
[parent_id]: 71708
[tags]: 
You can get the hyperplane only in the case of linear kernel (a.k.a dot-product) case. Here, the input for the computation are (based on what I could interpret from the documentation and a helpful thread ) SVMStruct.Bias (call it $b$) SVMStruct.SupportVectors (call it $\{x_j\}$) ( Note : These are data points closest to the hyperplane ) SVMStruct.Alpha (call it $\{\alpha_j\}$) The output is: $w^T = [(\sum_{j}\alpha_jx_j)^T\;\; b]$. The distance of every training point to the hyperplane specified by this vector $w$ is $w^T[x_i]/||w||_2$. For RBF kernel, the representation of the classifier or regressor is of the form $\sum_{i=1}^n \alpha_i K(x_i,x)$ where $n$ is the number of training examples and $K$ is the kernel we choose and $\{x_i\}$ are our training data points. The hyperplane lives in a possibly higher (even infinite) dimension. This hyperplane is of course different from the decision boundary (which is non-linear) which you may visualize when you have only 2-dimensional features. Notation: vectors are in column format.
