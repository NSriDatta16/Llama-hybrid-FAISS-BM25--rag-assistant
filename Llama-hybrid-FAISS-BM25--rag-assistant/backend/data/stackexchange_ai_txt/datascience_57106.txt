[site]: datascience
[post_id]: 57106
[parent_id]: 57105
[tags]: 
I'll go through your questions one by one: What if we have to check the correlation between a continuous and categorical variable? One option is to use point biserial correlation . You can read more here . That's not the only option of course, you can find a good series of examples here . Removing the variable is the only solution? No it's not, you can use dimensionality reduction techniques to "summarize" multicollinear variables. That's what I usually do to control multicollinearity, I strongly prefer this to removing a variable arbitrarily. The most common technique is Principal Component Analysis , but the list is really endless. Autoencoders can be used, if you are into Neural Networks. How multicollinearity is different from correlation? Correlation measures the association between two variables. This association can be either very noisy or not. Two variables can be highly correlated but their scatterplot can be very "spread out". Multicollinearity is a stronger concept instead. It happens when two variables are linearly associated, so that the variation of one can be used to explain the variation of the other in great detail. That represents a problem for regressions, since a small change in a variable can completely mess up the estimation of your parameters. That doesn't happen with all correlated variables. Of course there is some connection between the two. Two variables that are highly multicollinear must by definition be highly correlated as well, but they are not the same. Most importantly, multicollinearity is a problem for the reliability of your model, while correlation is not.
