[site]: stackoverflow
[post_id]: 620568
[parent_id]: 605480
[tags]: 
These "start-step-stop" codes looks like a different way of calling Huffman codes . See the basic technique for an outline of the pseudo-code for calculating them. Essentially this is what the algorithm does: Before you start the Huffman encoding you need to gather the statistics of each symbol you'll be compressing (Their total frequency in the file to compress). After you have that you create a binary tree using that info such that the most frequently used symbols are at the top of the tree (and thus use less bits) and such that no encoding has a prefix code . Since if an encoding has a common prefix there could be ambiguities decompressing. At the end of the Huffman encoding your start value will be depth of the shallowest leaf node, your step will always be 1 (logically this makes sense, why would you force more bits than you need, just add one at a time,) and your stop value will be the depth of the deepest leaf node. If the frequency stats aren't sorted it will take O(nlog n) to do, if they are sorted by frequency it can be done in O(n). Huffman codes are guaranteed to have the best average compression for this type of encoding: Huffman was able to design the most efficient compression method of this type: no other mapping of individual source symbols to unique strings of bits will produce a smaller average output size when the actual symbol frequencies agree with those used to create the code. This should help you implement the ideal solution to your problem. Edit: Though similar, this isn't what the OP was looking for. This academic paper by the creator of these codes describes a generalization of start-step-stop codes, start-stop codes. However, the author briefly describes how to get optimal start-step-stop near the end of section 2. It involves using a statistical random variable, or brute-force funding the best combination. Without any prior knowledge of the file the algorithm is O((log n)^3). Hope this helps.
