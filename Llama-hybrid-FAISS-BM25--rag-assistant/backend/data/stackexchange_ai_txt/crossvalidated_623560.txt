[site]: crossvalidated
[post_id]: 623560
[parent_id]: 
[tags]: 
Maximum Likelihood Estimation is increasing the Variance instead of learning something useful from the data

I have a simple neural network $\tau(x)=[\mu(x), \sigma(x)]$ to model the following distribution: $$ \hat y = \mathcal N(\mu(x),\text{Softplus}(\sigma(x))) $$ However, instead of learning something useful in the data, the network $\tau$ instead increases the expected value of $\sigma(x)$ which does maximize the log likelihood. In other words, the network learns the data as noise. This issue is fixed if I let $\sigma(x)=c$ , a constant that I manually tune. However, this will add additional biases to the model and I want to learn the variance from the data as well. I have tried using a Sigmoid activation multiplied to a large constant: $$ \text{Sigmoid}(x)\cdot k $$ where $k=\mathbb E[y] / 3$ to limit the possible variance of the model but this suffers from numerical instabilities. Please help me, 'What to do when the model learns the data as noise when minimizing the NLL?'
