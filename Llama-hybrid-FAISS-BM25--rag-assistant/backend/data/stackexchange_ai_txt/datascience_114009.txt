[site]: datascience
[post_id]: 114009
[parent_id]: 114008
[tags]: 
Standard BERT models take 768 (1024) dimensional vectors as their input. There is an encoding step that tokenizes and encodes a sentence from a string to a 768-dimensional vector. You can make changes in your BERT model or Tokenizer. Change in BERT model: You can add some extra dimension to your BERT model to take more than a 768-dimensional vector. So, you will be now providing a 768 vector of your sentences and some additional features as input in your BERT model. Making this change will require writing a custom BERT model and can be a bit difficult for beginners. Change in Tokenizer model: On the other hand, you can train a custom tokenizer for your BERT model which will output a vector with less than 768 dimensions and you can use the leftover dimension as your categorical feature. Hence, you will input a 768 vector in your BERT model. This does not require you to change the BERT model and can be a bit easier. Disclaimer This change will not necessarily make the model perform better on your task, it might even make the performance worse. You have to apply trial and error to find the methods that suit best for your task.
