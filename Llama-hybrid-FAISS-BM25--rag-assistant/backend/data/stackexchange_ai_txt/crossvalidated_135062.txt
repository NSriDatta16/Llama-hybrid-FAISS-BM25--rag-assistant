[site]: crossvalidated
[post_id]: 135062
[parent_id]: 134694
[tags]: 
I just wanted to add to the input, but don't at the moment have a concise answer and it's too long for a comment. Hopefully this gives more insight. It seems that the function of interest is in the unpacked glmnet library, and is called cv.lognet.R It's hard to explicitly trace everything, as much is in S3/S4 code, but the above function is listed as an 'internal glmnet function,' used by the authors and seems to match how the cv.glmnet is calculating the binomial deviance. While I didn't see it anywhere in the paper, from tracing the glmnet code to cv.lognet, what I gather is that it is using something called the capped binomial deviance described here . $-[Y\log_{10}(E) + (1-Y)\log_{10}(1-E)]$ predmat is a matrix of the capped probability values (E, 1-E) output for each lambda, that are compared to the y and y's complement values resulting in lp. They are then put in the 2*(ly-lp) deviance form and averaged over cross-validated hold out folds to get cvm - The mean cross-validated error - and cv ranges, that you have plotted in the first image. I think the manual deviance function (2nd plot) is not calculated the same way this internal one (1st plot) is. # from cv.lognet.R cvraw=switch(type.measure, "mse"=(y[,1]-(1-predmat))^2 +(y[,2]-predmat)^2, "mae"=abs(y[,1]-(1-predmat)) +abs(y[,2]-predmat), "deviance"= { predmat=pmin(pmax(predmat,prob_min),prob_max) lp=y[,1]*log(1-predmat)+y[,2]*log(predmat) ly=log(y) ly[y==0]=0 ly=drop((y*ly)%*%c(1,1)) 2*(ly-lp) # cvm output cvm=apply(cvraw,2,weighted.mean,w=weights,na.rm=TRUE)
