[site]: stackoverflow
[post_id]: 4731869
[parent_id]: 4731774
[tags]: 
Basically it wants to be a predictor of the risk of change for a method. It has two factors in it: code complexity of the method ( cyclomatic complexity ) aka how many decisions paths exists in said method: comp(m) . how testable is that method (via automated tests, provided by a code coverage tool). Basically this measures how many decisions in said code are automatically testable. If the method has 100% coverage than the risk of change is considered to be equivalent only with the complexity of the method: C.R.A.P.(m) = comp(m) . If the method has 0% coverage than the risk of change is considered to be a second degree polinomial in the complexity measure (reasoning being that if you can't test a code path changing it increases risk of breakage): C.R.A.P.(m) = comp(m)^2 + comp(m) Hopefully this will help you. I just noticed that I only provide the half answer (the read part). The how to improve it should be pretty clear if you understand the reasoning of the index. But a much more clear explanation is given in @edorian's answer . The short story is: write tests until you have near 100% coverage and after that refactor the methods to decrease the cyclomatic complexity. You can try to refactor before having tests but depending on the actual method complexity you risk introducing breakage if you can't reason (because of the complexity involved) all the consequences of the change you are doing.
