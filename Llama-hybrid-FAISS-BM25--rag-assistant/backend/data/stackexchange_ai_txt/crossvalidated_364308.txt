[site]: crossvalidated
[post_id]: 364308
[parent_id]: 363937
[tags]: 
I think you have two options: Keep a representation of chords that is able to reproduce the complexities that you desire, for example several chords at the same time, such as the *kern representation or midi. These representations have the advantage that some complexities can be included such as multiple instruments, note durations and (for midi) even note intensities. The problem can then again be formulated as a time-series. A *kern file is in fact simply a text file that can be transformed into a sequence of symbols. Transform all your notes to an array of on or off notes, as you suggest. There is no reason why you can't model this as a time-series in the sense that each timestep would simply have an entire embedding as an input that could be fed to your model (eg 1D convolutional layer or LSTM). The problem here is that, depending on the complexity of the music you would like to represent, you would probably need to implement the overhead yourself that adds the relevant information about note duration/instrument/etc and append it to your embedding as well.
