[site]: crossvalidated
[post_id]: 527258
[parent_id]: 
[tags]: 
Embedding data into a larger dimension space

Embeddings or latent spaces are vector spaces that we embed our initial data into that for further processing. The benefit of doing so as far as I am aware, is to reduce the dimension. Often data has many discrete features that doesn't make sense to turn each of them to a new dimension. Embedding makes it possible to embed data into a lower dimensional space that dimensions do not correspond to the features. Depending on the situation embedding can be trained separately or simultaneously with the rest of the model. My question is: are there possible benefits for embedding into a space of larger or same dimension? The reason I am asking the question is that, I am reading a paper which claims one of their innovations is that they are embedding into a latent space and train the embedding together with the rest of the network. Lowering dimensionality is stated as one of the benefits of this action. But in their code implementation they have examples that is embedding a 5 dimensional space into 24 dimensions.
