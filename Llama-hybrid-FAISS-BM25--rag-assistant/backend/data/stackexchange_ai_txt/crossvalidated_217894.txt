[site]: crossvalidated
[post_id]: 217894
[parent_id]: 
[tags]: 
Reasons NOT to use standardised data in multivariate analysis

Question: Can you give any reasons/examples when it is more appropriate NOT to standardise continuous metric independent variables when performing multivariate analysis? Background: I am an undergrad business student with passion for data analytics and have taken a couple of statistics modules. I am also a self-learner in R with the help of Data Science specialisation on Coursera . This question has popped in my mind while revising for one of my statistics exams. From what I understand, it is ALWAYS better to use standardised data in multivariate analysis, for example: in linear regression, it helps to detect outliers, in logistic regression and discriminant analysis, it can help us to identify variables that best discriminate between groups, in factor analysis, we need it anyways to calculate factor scores, in cluster analysis, it is essential as different scales can dominate clustering (based on the distance method). Yet I often see my lecturers / lecturers in the Data Science specialisation using non-standardised data to perform all of the activities I mentioned. Can you please show me the limits of my understanding here and perhaps give examples when it is better not to standardise data? Please bear in mind that I do understand using non-standardised data for interpreting the model and drawing conclusions, but I don't get why people don't standardise data straight away when exploring it / fitting the first model.
