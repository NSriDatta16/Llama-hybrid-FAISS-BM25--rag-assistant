[site]: crossvalidated
[post_id]: 522932
[parent_id]: 296572
[tags]: 
Something that I think could be stressed more is that econometric modelling often assumes that the model chosen is in fact the true model, in the sense that this model is equivalent to the data generating process (DGP). This is needed to derive powerful distributional results in order to do inference and express uncertainty and make statements such as OLS being the best linear unbiased estimator (BLUE) under standard assumptions. The obtained results are incredibly useful for testing model hypotheses which also helps explain how this framework is so useful for testing economic theory. On the other hand, machine learning often makes less restrictive assumptions which does not allow for these kind of results and machine learning also focusses more on the approximation error, which is defined as the error between the best predictor in a chosen model and the best predictor among all predictors (often called Bayes predictor). More general "learning guarantees" can be proven which allows to bound the estimation error of the model which very little assumptions. It is then often more natural in machine learning to take a very flexible approach to modelling which is sensible as machine learning researchers often focus on predictive modelling. I would like to add that advanced methodology in econometrics allows for (among many other things) expressing the model uncertainty e.g. using Bayesian modelling a comparison can be made between the posterior probabilities between candidate models. My point here is that depending on the methodology chosen, econometrics can incorporate more uncertainty then the uncertainty inherent to the parameters and the errors which is often the only uncertainty expressed in many types of analysis.
