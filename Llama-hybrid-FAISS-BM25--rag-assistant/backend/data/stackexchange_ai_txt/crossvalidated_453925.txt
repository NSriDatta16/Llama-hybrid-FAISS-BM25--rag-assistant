[site]: crossvalidated
[post_id]: 453925
[parent_id]: 
[tags]: 
How to interpret a sampling distribution from a Frequentist and Bayesian perspective

I've read multiple of the threads about Bayesian vs Frequentist interpretations of probability, but I'm having trouble trying to reconcile them with the idea of the sampling distribution when performing parameter estimation. Here is what is going on in my head: Say we want to fit a probability distribution to a collection of data. Examining the data we choose a probability distribution (say the Poisson as an example). To fit the distribution to our data we have to obtain an estimate for the population parameter $\lambda$ . So we obtain the estimate using one of the known procedures (e.g. Maximum Liklihood Estimation). We obtain an estimate for the true parameter value, $\hat{\lambda}$ . Since our estimate $\hat{\lambda}$ is a function of our sampled data this implies it has a probability distribution, which is commonly referred to as the sampling distribution. Now this is where I'm having trouble tying things together. My interpretation of what a sampling distribution was that it "represents the possible values that the true parameter value could take on with respective probability attached to them." But also from the frequentist approach the parameter values are constant and as such have no probability attached to them. But we obtained these parameter estimates from a random process of sampling, so how can they not have probability attached to them? As can be seen there a few things I'm confused about. Some clarification would be much appreciated. EDIT: I broke the rules (sorry) but I also asked this question on math stack exchange and got a very good answer. I'll post it here for completeness. https://math.stackexchange.com/questions/3579310/interpretation-of-sampling-distribution-from-frequentist-point-of-view
