[site]: crossvalidated
[post_id]: 166944
[parent_id]: 
[tags]: 
In back propagation for neural networks, what exactly is the "error signal"?

For example: Imagine we end up with a sum of 0.755 on our output node. We then apply the activation function (in this case I'll use a sigmoid) which gives us a final value of 0.68 . Now imagine the actual output we were looking for was 0 . This means our error was 0 - 0.68 or -0.68 . Now, this is the weird part... all the literature then says we need to calculate the "error signal" by multiplying the derivative of the sigmoid at 0.755 by this error. ie. Error Signal = S'(0.755) * (0 - 0.68) My question is, what exactly is this 'Error signal'? I had thought maybe it was something like Euler's Method but we're multiplying by the error in y here rather than a change in x so that can't be right.
