[site]: stackoverflow
[post_id]: 4647920
[parent_id]: 4647793
[tags]: 
The way I would do it is I'd generate a really small (say.. 1/50 of the original image size) image from every image you're comparing against, and store the thumbnail image path along with the original size in the database. I'd keep the thumbnails as uncompressed bmp's for speed and loss-free-ness (I just made that word up!), since they're so small anyway. To compare your new image against the other ones, shrink it down by the same amount and compare it against the others pixel by pixel, with a certain threshold (say.. 10% difference from the original). If it passes this test, you can do a full blown pixel by pixel compare against the original image. edit: I just want to mention that I went down the probabilistic way before too. It worked OK, but building the meta data for the images took forever, and there were a lot of false positives. Instinctively, I think that calculating local averages for each grid rectangle of your image (which is what shrinking your image down does) would give similar, if not better results.
