[site]: crossvalidated
[post_id]: 236126
[parent_id]: 236111
[tags]: 
If you're using XGBoost within R, then you could use the caret package to fine tune the hyper-parameters. Here is an example tuning run using caret: library(caret) library(xgboost) # training set is stored in sparse matrix: devmat myparamGrid You should also refer to the R manual pages for caret and xgboost, they provide very good examples and pointers. For Python, (although I haven't used this particular module) you could try the grid search classes form sklearn.grid_search , refer to this list: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.grid_search . See this link for an example of using this module for finding the best hyper-parameters for a Random Forest model - http://scikit-learn.org/stable/auto_examples/model_selection/randomized_search.html#example-model-selection-randomized-search-py
