[site]: crossvalidated
[post_id]: 505001
[parent_id]: 504980
[tags]: 
Your argument is right. From the perspective of the language model, you have well-defined target labels and use supervise learning methods to teach the model to predict the labels. Calling it unsupervised pre-training is certainly sort of paper-publishing marketing, but it is not entirely wrong. It is unsupervised from the perspective of the downstream tasks. The MLM-pre-trained model learned something useful for a particular downstream task (e.g., sentiment analysis) without using any labeled data for the task, but using unlabeled data only. There is also a strong analogy with clustering. The inputs to the model are very high-dimensional data: the vocabulary has tens of thousand items, there is very little structure too (all one-hot vectors are equidistant). The MLM pre-training learns to embed such inputs into a much lower-dimensional and very structured space using nothing else than unlabeled data: the text itself.
