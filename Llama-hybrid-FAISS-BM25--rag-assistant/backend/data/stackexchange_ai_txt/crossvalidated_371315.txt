[site]: crossvalidated
[post_id]: 371315
[parent_id]: 371303
[tags]: 
To my knowledge, there is not a single paper that would summarize proven mathematical results. For general overview, I recommend going for textbooks instead, which are more likely to give you a broad background overview. Two prominent examples are: Bishop, Christopher M. Neural networks for pattern recognition . Oxford university press, 1995. Goodfellow, Ian, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning . Vol. 1. Cambridge: MIT press, 2016. These are rather introductory books, compared to the level of some papers you cited. If you want to go deeper into PAC learning theory (which you really should, if you plan on doing research on the learnability of NN models), read both of these: Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar, Foundations of Machine Learning , MIT Press, 2012 (but wait for the 2018 edition, it's due on Christmas and it has a few considerable improvements) Shai Shalev-Shwartz , Shai Ben-David, Understanding Machine Learning: From Theory to Algorithms , Cambridge University Press, 2014 Also, if you are interested in historical stream of development of neural networks, read: Schmidhuber, J., 2015. Deep learning in neural networks: An overview. Neural networks, 61, pp.85-117. The tricky thing with mathematical theory and proofs in deep learning is that many important results don't have practical implications. For example, the super famous Universal approximation theorem says that a neural network with a single hidden layer can approximate any function to arbitrary precision. Why would you care for using more layers then if one is enough? Because it was empirically demonstrated that it works. Also, the Universal approximation theorem only tells us that such a network exists, but it doesn't tell anything about what we are really interested in, i.e., the learnability of such a network from training samples (an Information Theory problem, or a PAC learning problem, depending on how you cast it). the existence of an algorithm which can learn the correct network (its weight) in polynomial time (a theory of computation problem). For example, we have a universal approximation theorem for polynomials and a universal approximation theorem for Gaussian processes with the squared exponential kernel . But do we win ImageNet/Kaggle competitions with polynomials or GPs? Of course we don't. Another example is the convergence: Training neural networks using first order methods (gradient descent and the likes) is guaranteed 1 to converge to a local minimum but nothing more. Since it is non-convex optimization problem, we simply cannot prove much more useful about it (although some research is being done about the local minima distance from a global minimum [1,2]). Naturally, much more attention is paid to empirical research studying what we can do even if we cannot prove it 2 . Finally, I am not aware of works proving much important about the network architecture or about their generalization ability (to be honest, I am not sure what kinds of proofs are you looking for here; maybe if you reply in comments or add details to your question, I can expand on it here.) [1]: Choromanska, A., Henaff, M., Mathieu, M., Arous, G.B. and LeCun, Y., 2015, February. The loss surfaces of multilayer networks. In Artificial Intelligence and Statistics (pp. 192-204). [2]: Soudry, D. and Carmon, Y., 2016. No bad local minima: Data independent training error guarantees for multilayer neural networks. arXiv preprint arXiv:1605.08361. 1 Guaranteed almost surely; see discussion around this answer for some pathological counterexamples. 2 This is not necessarily bad and it does not mean that deep learning is alchemy : Proofs and rigorous math theories often follow empirical evidence and engineering results.
