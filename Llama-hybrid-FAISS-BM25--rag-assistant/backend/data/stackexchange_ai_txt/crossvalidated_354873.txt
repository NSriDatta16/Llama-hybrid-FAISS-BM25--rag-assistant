[site]: crossvalidated
[post_id]: 354873
[parent_id]: 
[tags]: 
How do I pick a good value of num_epochs?

I'm running a program that uses an RNN to build a language model of some sample text: https://github.com/crazydonkey200/tensorflow-char-rnn When training it, I currently use the default value of num_epochs (50). Passing --verbose 1 to the program, I notice that the perplexity seems to stop decreasing at around 25 or 30 epochs, and since perplexity is just exp(train_loss) this means that the training loss is stabilizing as well. Is it a good idea to limit the number of epochs to the point where the training loss stops decreasing significantly? I assume that means the model isn't getting any better from extra training, right?
