[site]: datascience
[post_id]: 99608
[parent_id]: 99577
[tags]: 
This is likely behavior when several of your original features are discrete. Each tree, when splitting, considers a split for each unique value of each feature (in the current node). For discrete features this is often significantly smaller than the number of rows, while for continuous features it is often very nearly the same as the number of rows. When some features are discrete then, the count of candidate splits is generally much smaller than when all the features are continuous. Since PCA amounts to essentially a rotation of the feature space, the resulting feature space will generally be purely-continuous, making the resulting tree decisions more expensive even when removing a few of the last principal components. You could try using histogram splitting in xgboost, which discretizes the features before splitting to reduce computation time. I don't think this is currently available in sklearn's implementation of adaboost.
