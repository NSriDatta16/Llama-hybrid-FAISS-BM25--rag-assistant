[site]: crossvalidated
[post_id]: 330329
[parent_id]: 
[tags]: 
Working for Logistic regression partial derivatives

In Andrew Ng's Neural Networks and Deep Learning course on Coursera the logistic regression loss function for a single training example is given as: $$ \mathcal L(a,y) = - \Big(y\log a + (1 - y)\log (1 -a)\Big) $$ Where $a$ is the activation of the neuron. The following slide gives the partial derivatives, including: $$\frac{\delta \mathcal L(a, y)}{\delta a} = - \frac y a + \frac{1-y}{1-a}$$ Why isn't the 2nd term negative, ie: $ - \frac{1-y}{1-a}$, given the negative outside the brackets in the definition of $\mathcal L(a, y)$?
