[site]: datascience
[post_id]: 90592
[parent_id]: 
[tags]: 
Effect of removing duplicates on Random Forest Regression

I have a dataset with several million samples that have 5 features and 1 target, which I am using for a regression model. With very large sample counts some models (like Random Forests) become very large (several GB when pickled). These data often have duplicates or near duplicates - these are real observations - but the measured values are just coincidentally identical (a consequence of the limited input range and precision of the instruments). What is the effect (in theory) or removing duplicates on model accuracy?
