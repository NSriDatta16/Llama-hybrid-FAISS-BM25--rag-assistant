[site]: crossvalidated
[post_id]: 299977
[parent_id]: 299963
[tags]: 
Neural networks could "learn" where the labels are completely random. In this case nets will simply memorize the label for each sample thus giving you low training error but high testing error. There is an interesting paper on this topic where authors trained nets on ImageNet, but corrupted the label to be random . What they found was that is the net has enough parameters, but labels are completely random it can still memorize all of them.
