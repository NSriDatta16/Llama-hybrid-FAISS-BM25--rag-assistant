[site]: datascience
[post_id]: 90290
[parent_id]: 
[tags]: 
Decoder Transformer feedforward

I have a question about the decoder transformer feed forward during training. Let's pick an example: input data "i love the sun" traduction i want to predict (italian traduction) "io amo il sole" . Now i feed the encoder with the input "i love the sun" and i get the hidden states. Now i have to do multiple feed forwards on the decoder with the input "BOS io amo il" where BOS is a token that stands for beginning of sentence. So i have this feedforward i assume [BOS, IO, AMO, IL] -> decoder -> IO [BOS, IO, AMO, IL] -> decoder -> AMO [BOS, IO, AMO, IL] -> decoder -> IL [BOS, IO, AMO, IL] -> decoder -> SOLE I think this is the correct way. And what should be applied to differentiate the training i think is the masked attention mechanism maybe(?) is it right to assume that the masking will be [1 0 0 0, 0 0 0 0 , 0 0 0 0, 0 0 0 0] for the first feed forward [1 0 0 0, 1 1 0 0 , 0 0 0 0, 0 0 0 0] for the second feed forward [1 0 0 0, 1 1 0 0 , 1 1 1 0, 0 0 0 0] for the third feed forward [1 0 0 0, 1 1 0 0 , 1 1 1 0, 1 1 1 1] for the fourth feed forward is it the correct way? or what should be different? If you can provide me also a python implementation could be useful, thanks in advance.
