[site]: crossvalidated
[post_id]: 89175
[parent_id]: 
[tags]: 
Orthogonality in bias variance tradeoff

I have a function class $\mathcal{F}$. I get $n$ samples according to a model $$y = f^*(x)+\epsilon$$ I find the best $\hat{f}$ from these training samples i.e. $$\hat{f} = \arg\min\limits_{f\in \mathcal{F}} \frac{1}{n}\sum\limits_{i=1}^n (y_i-f(x_i))^2$$ Let $\bar{f} = E[\hat{f}]$, where the expectation is over all training samples. Now, the average testing error is given by \begin{align*} E[(y_0-\hat{f}(x_0))^2] &= E\left[\left((y_0-f^*(x_0)) + (f^*(x_0)-\bar{f}(x_0)) + (\bar{f}(x_0)-\hat{f}(x_0))\right)^2\right]\\ &= E[(y_0-f^*(x_0))^2] + \color{red}{E\left[\left((f^*(x_0)-\bar{f}(x_0)) + (\bar{f}(x_0)-\hat{f}(x_0))\right)^2\right]}\\ &\stackrel{\color{red}{?}} = E[(y_0-f^*(x_0))^2] + \color{red}{E\left[(f^*(x_0)-\bar{f}(x_0))^2\right] + E\left[(\bar{f}(x_0)-\hat{f}(x_0))^2\right]} \end{align*} The second step is because $y_0-f^*(x_0)=\epsilon_0$, which is independent of $x_0$. However, I don't see how $f^*(x_0)-\bar{f}(x_0)$ is independent of $\bar{f}(x_0)-\hat{f}(x_0)$ for the third step. $f^*(x_0)-\bar{f}(x_0)$ relates to the average over all samples while $\bar{f}(x_0)-\hat{f}(x_0)$ depends on the current sample, and we know the samples are independent, but how does that imply their independence?
