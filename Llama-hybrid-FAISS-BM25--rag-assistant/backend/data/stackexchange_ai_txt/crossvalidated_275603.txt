[site]: crossvalidated
[post_id]: 275603
[parent_id]: 275527
[tags]: 
There are two attempts to do exactly what you have said in statistical history, the Bayesian and the Fiducial. R. A. Fisher founded two schools of statistical thinking, the Likelihoodist school built around the method of maximum likelihood and the Fiducial, which ended in failure but which attempts to do exactly what you want. The short answer as to why it failed is that its probability distributions did not end up integrating to unity. The lesson, in the end, was that the prior probability is a necessary thing to have to create what you are trying to create. Indeed, you are going right down the path of one of history’s greatest statisticians and more than a few of the other greats died hoping for a resolution to this problem. If it were found it would place null hypothesis methods on par with Bayesian methods in terms of the types of problems that they could solve. Indeed, it would push past Bayes except where real prior information existed. You also want to be careful with your statement that a p-value indicates a higher likelihood for the alternative. That is only true in the Fisherian Likelihoodist school. It is not at all true in the Pearson-Neyman Frequentist school. Your bet at the bottom appears to be a Pearson-Neyman bet while your p-value is incompatible as it is coming from the Fisherian school. To be charitable I am going to assume, that for your example, that there is no publication bias and so only significant results appear in journals creating a high false discovery rate. I am treating this as a random sample of all studies performed, regardless of the results. I would argue that your betting odds would not be coherent in the classical de Finetti sense of the word. In de Finetti’s world, a bet is coherent if the bookie cannot be gamed by players so that they face a sure loss. In the simplest construction, it is like the solution to the problem of cutting the cake. One person cuts the piece in half, but the other person chooses which piece they want. In this construction one person would state the prices for the bets on each hypothesis, but the other person would choose to either buy or sell the bet. In essence, you could short sell the null. To be optimal, the odds would have to be strictly fair. P-values do to not lead to fair odds. To illustrate this, consider the study by Wetzels, et al at http://ejwagenmakers.com/2011/WetzelsEtAl2011_855.pdf The citation for which is: Ruud Wetzels, Dora Matzke, Michael D. Lee, Jeffrey N. Rounder, Geoffrey J. Iverson and Eric-Jan Wagenmakers. Statistical Evidence in Experimental Psychology: An Empirical Comparison Using 855 t Tests. Perspectives on Psychological Science. 6(3) 291-298. 2011 This is a direct comparison of 855 published t-tests using Bayes factors to bypass the problem of the prior distribution. In 70% of the p-values between .05 and .01, the Bayes factors were at best, anecdotal. This is due to the mathematical form used by Frequentists to solve the problem. Null hypothesis methods presume that the model is true and by their construction use a minimax statistical distribution rather than a probability distribution. Both of these factors impact differences between Bayesian and non-Bayesian solutions. Consider a study where the Bayesian method evaluates the posterior probability of a hypothesis as three percent. Imagine that the p-value is less than five percent. Both are true since three percent is less than five percent. Nonetheless, the p-value isn’t a probability. It only states the maximum value that could be the probability of seeing the data, not the actual probability a hypothesis is true or false. Indeed, under the p-value construction, you cannot distinguish between effects due to chance with a true null and a false null with good data. If you look at the Wetzel study, you will note that it is very obvious that the odds implied by the p-values do not match the odds implied by the Bayesian measure. Since the Bayesian measure is both admissible and coherent, and the non-Bayesian is not coherent, it is not safe to assume the p-values map to the true probabilities. The forced assumption that the null is valid provides nice coverage probabilities, but it does not produce nice gambling probabilities. To get a better feel as to why, consider Cox’s first axiom that the plausibility of a hypothesis can be described by a real number. Implicitly, this means that all hypothesis have a real number tied to their plausibility. In null hypothesis methods, only the null has a real number tied to its plausibility. The alternative hypothesis has no measurement made and it is certainly not the complement to the probability of observing the data given that the null is true. Indeed, if the null is true, then the complement is false by assumption without regard to the data. If you constructed the probabilities using p-values as the basis of your measurement, then the Bayesian using Bayesian measurements would always be capable of getting an advantage over you. If the Bayesian set the odds then Pearson and Neyman decision theory would provide a statement of bet or do not bet, but they would not be able to define the amount to bet. As the Bayesian odds were fair, the expected gain from using Pearson and Neyman’s method would be zero. Indeed, the Wetzel study is really what you are talking about doing, but with 145 fewer bets. If you look at table three you will see some studies where the Frequentist rejects the null, but the Bayesian finds that the probability favors the null.
