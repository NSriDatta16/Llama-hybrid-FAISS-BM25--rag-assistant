[site]: crossvalidated
[post_id]: 197913
[parent_id]: 197875
[tags]: 
The obvious answer is when the loss function is quadratic loss. In this scenario, the posterior mean is the Bayes estimator since it minimizes this loss. Often the experts in a field don't have a well-defined loss, e.g. "give us the most probable parameter value", and therefore quadratic loss, which penalizes more strongly the farther you are from the truth, may be a good approximation to their true loss. A second answer is when the posterior mean is easier to estimate. Many Bayesian analyses utilize a Monte Carlo procedure to provide samples from the posterior. By taking an average of these samples we have an estimate of the posterior mean which converges to the true posterior mean by the Law of Large Numbers as we increase the Monte Carlo sample size. Providing an estimate of the MAP is more involved, e.g. constructing a kernel density estimate based on samples and then finding the mode of this kernel density estimate, and there are non-trivial choices here, e.g. bandwidth, that may affect the quality of the estimate. A third answer is when the MAP is not a reasonable summary of the posterior. For example, suppose the posterior is multi-modal and the highest mode just happens to be at one extreme or suppose the posterior is almost uniform over some interval and that the MAP happens to be toward one endpoint of that interval. In these cases, I doubt the experts really want "the most probable parameter value", but instead would prefer an estimate in the middle of the distribution.
