[site]: datascience
[post_id]: 22708
[parent_id]: 22699
[tags]: 
What is Word2Vec? Word2Vec is an implementation of word embedding techniques. Word embedding tries to represent relationships that may exist between the individual words (those contained in your processing texts) by giving them each a vector with same predefined dimension. In this vector space words that share common contexts may be located closer. How to assign the words into the vector space? This work has often been done through neural network training. What problems can it solve? The main problem solved by Word2Vec (or word embedding) is that it creates a way to represent relationships between the words in your processing texts, as opposed to just treat the words as individual symbols. This makes any following data mining or machine learning more effective. Besides Word2Vec, you can also use many other pre-trained embeddings like GloVe . If the dataset is large, it is better to train your own word embedding which will get a better performance than using pre-trained embeddings.
