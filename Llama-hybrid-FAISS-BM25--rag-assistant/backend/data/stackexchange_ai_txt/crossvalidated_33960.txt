[site]: crossvalidated
[post_id]: 33960
[parent_id]: 33683
[tags]: 
Not quite the answer you are looking for but... Do you really need to use that much data? How many predictors do you have? If the number of predictors is small, how much of the data is redundant and just filling in the predictor space? Is it all relevant? Are there data points that are not really in the application domain of the model (i.e. 20 year old cases)? Just because you have it doesn't mean you must use it. I would sample the data based on similarity - come up with a subset of points that are most dissimilar from the other points n the training set. Finally, why use a tree ensemble method? I love RF but it is perhaps the model that will give you the largest possible footprint (i.e. thousands of very large unpruned trees). Try 100 bagged trees if you want to use a tree ensemble. Heck, boosted C5 trees probably need a much smaller number of iterations that CART-like boosting methods (from what I've seen so far) You may have no idea going into this process which model will be best or even good enough. Again, this depends on $p$, but start with some high bias models (LDA, logistic regression, naive Bayes) and see what you can get out of them before bringing out the big, complex, computational expensive (or infeasible) tools. Try logistic regression with cubic smoothing splines to approximate any non-linearities.
