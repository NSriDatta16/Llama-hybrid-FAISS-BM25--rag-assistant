[site]: stackoverflow
[post_id]: 3982372
[parent_id]: 3980348
[tags]: 
This is definitely a question for http://metaoptimize.com/qa - a stackoverflow clone for the machine learning community. That aside, the procedure you describe is basically a way to do regularization. For example consider how your procedure compares against using millions of RBF's with a L1 or L2 regression term. Obviously it is different only in the way the the regression term works. What you are basically doing is regularization on only the number of non-zeros weights instead of the sum or the squared sum. This method of regularization is not new, people have been trying to do this effectively for a long time but the problem is that it is not convex or even continuous. That means the optimization is NP-Hard in general. Your technique is basically an approximation alogorithm for this problem and, while it might work on some example, you will have no guarantees of optimality (finding the "best" N rbfs to fit) and it will be highly sensitive to the starting point you choose. It is for these reasons that you usually do not see research papers in top conferences and journals doing this, but rather trying to L1-regularization more effectively, since that has been proven to be as close as we can possibly get while maintaining convexity.
