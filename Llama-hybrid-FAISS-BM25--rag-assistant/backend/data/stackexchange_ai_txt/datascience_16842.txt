[site]: datascience
[post_id]: 16842
[parent_id]: 15670
[tags]: 
Mapping your inputs into a sparse matrix and using logistic regression with an $\ell_1$ penalty should work well. In R this is very straight forward with the glmnet and matrix libraries. Additionally, if you have boolean features with a low frequency, you can filter some of them by taking a subset with at least some number of observations. Here's a quick simulation in R of a lasso logistic regression with sparse features. library(glmnet) library(Matrix) n 0.5,1,0) trainfilter The classifier doesn't matter as much in this case as the underlying algorithm's ability to handle sparse matrices. It's also worth noting that you can handle sparse boolean features by mapping them to arbitrary numeric indices and running them through a tree based algorithm (e.g., Random Forest or Gradient Boosting). In practice, this seems to work well.
