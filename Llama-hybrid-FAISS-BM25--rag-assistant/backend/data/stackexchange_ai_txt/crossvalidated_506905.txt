[site]: crossvalidated
[post_id]: 506905
[parent_id]: 506901
[tags]: 
There are many measures of forecast accuracy. (That online forecasting textbook is concerned with time series forecasting, but the KPIs can be applied in any prediction.) We also have tags for the mape , mae , rmse or mase . You may be interested in a short paper of mine (Kolassa, 2020, IJF ) in making sense of how these various measures hang together. You can in principle use a t-test to assess whether your predictions have the same mean as your actuals. One question is whether this is really interesting. If you have $n$ predictions, and they are on average too high by $x$ , then you can subtract $nx$ from any one prediction, and the resulting vector of predictions will have the same mean as your actuals. Does this mean that this ad hoc modified prediction is better than the original one? I wouldn't think so. In the time series forecasting community, this kind of testing whether predictions have the same mean as actuals is usually not done, likely because of problems like this. Rather, the focus is on whether predictions are "good enough", or whether a particular prediction/forecasting algorithm improves on a very simple benchmark method, like the overall mean (which can be surprisingly hard to beat). (Also, if you do decide to use a t-test, don't use equal_var=True . Your predictions will almost certainly have a lower variance than the actuals, simply because in predicting, you filter out unpredictable noise.)
