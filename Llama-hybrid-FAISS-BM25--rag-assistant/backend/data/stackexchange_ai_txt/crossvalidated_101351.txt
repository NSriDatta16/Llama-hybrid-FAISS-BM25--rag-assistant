[site]: crossvalidated
[post_id]: 101351
[parent_id]: 
[tags]: 
Entropy and information content

I am curious to know about the relation between Entropy and information content of a signal or trajectory of time series. When a system is at equilibrium, then it has maximum entropy. Does entropy mean a measure of information loss ? Higher entropy == loss in information? In my understanding, Shannon's information theory entropy measures how uncertain we are about the next incoming symbol. Hence, Entropy = measure of uncertainty. Entropy is maximum when we are absolutely unsure of the outcome of the events. So, would this imply that Max Entropy = minimum information? if we consider a trajectory in phase space and each element of the trajectory to be a symbol, then would the evolution of the trajectory mean that information is being created? How do we infer entropy for a trajectory?
