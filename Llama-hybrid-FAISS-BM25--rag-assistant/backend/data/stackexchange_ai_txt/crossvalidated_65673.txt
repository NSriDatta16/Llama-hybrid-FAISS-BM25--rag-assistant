[site]: crossvalidated
[post_id]: 65673
[parent_id]: 64162
[tags]: 
One solution to your problem, which you may or may not deem as "cheating", is to use dimension reduction before running your sampler (see this review ). The most straight-forward approach that comes to mind (and is also discussed in the aforementioned review) is partial least squares regression (PLS). The first use of PLS in ABC (at least that I know of) was by Wegmann et al . In this approach, one introduces a "calibration" step before running whatever ABC algorithm (rejection sampling, MCMC, SMC, etc.) wherein a large number of summary statistics are simulated based on draws from the prior and PLS is used to calculate a transform of the summary statistics explaining the most variance in the parameter space. The observed summary statistics are then transformed and in the ensuing ABC rejection steps the distance between transformed simulated and observed summary statistics will be used instead of the original summary statistics. In this way, you will avoid having to calculate (or invert) $\Sigma_\theta$ . The calibration step can be outlined as follows- Sample $B$ draws from $\theta' \sim P(\theta)$ Conduct $B$ simulations from $S \sim P(S|\theta')$ Estimate PLS transform of of $S$ to $S'$ PLS is available for R and python and C++ .
