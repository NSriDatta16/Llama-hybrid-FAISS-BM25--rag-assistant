[site]: crossvalidated
[post_id]: 621511
[parent_id]: 
[tags]: 
Is there an eigenvector based decomposition scheme that will find the longest axis of variation collinear with a named variable?

So far as I understand, algorithms like principal component analysis, spectral decomposition, etc. are similar algorithms that identify orthogonal vectors in a dataset using a different set of heuristics. For instance, PCA proceeds by identifying the longest axis of variation in the dataset, assigning that to PC1, and then successively assigning the next longest axes of variation that are orthogonal to PC1 to PC2, PC3 ... PCn. After this is done, one can correlate the PC loadings against covariates that are named in one's analysis, and thereby look for collinearity between a given PC and a covariate. Suppose instead one wants to identify an eigenvector satisfying the following criteria: 1) has collinearity with the named variable that exceeds a specified threshold 2) accounts for more variance than the named variable. This should enable unique specification of a closely related eigenvector to the named variable; from here, orthogonal vectors could be identified (but for my purposes need not be). Has such an algorithm been composed?
