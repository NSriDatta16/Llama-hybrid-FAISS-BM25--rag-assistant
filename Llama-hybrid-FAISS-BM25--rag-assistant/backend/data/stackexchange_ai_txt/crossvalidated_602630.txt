[site]: crossvalidated
[post_id]: 602630
[parent_id]: 602621
[tags]: 
At a high level of abstraction, there are three steps to VAEs. (The most common use-case of a VAE assumes that the $z$ are distributed as a multivariate normal distribution with a diagonal covariance matrix, so I focus on that.) Encode . Use a neural network $f$ to map the input $x$ to a vector means and a vector of standard deviations: $f(x) = (\mu, \sigma)$ Sample . Draw a vector from the multivariate normal distribution defined by $(\mu, \sigma)$ . This is $z \sim \mathcal{N}(\mu, \sigma^2 I)$ . Decode . Use a neural network to map the vector $z$ to something that "looks like" the input $x$ . We could write this as $\hat x = g(z)$ . During, the loss is computed as the sum of the KLD and the reconstruction error: $\| \hat x - x \|_2^2 + KLD$ , for example. If you're trying to generate new data that "looks like" the input data, then you want $\hat x$ . So neither of your proposals is correct -- you want to decode $z$ .
