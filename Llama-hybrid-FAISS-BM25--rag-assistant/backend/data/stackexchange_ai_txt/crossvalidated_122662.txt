[site]: crossvalidated
[post_id]: 122662
[parent_id]: 117660
[tags]: 
The direct answer to your question is that the last model you wrote, anova(lmer(y ~ a*b*c +(1|subject) + (1|a:subject) + (1|b:subject) + (1|c:subject) + (1|a:b:subject) + (1|a:c:subject) + (1|b:c:subject), d)) I believe is "in principle" correct, although it is a strange parameterization that doesn't always seem to work well in actual practice. As for why the output that you get from this model is discrepant with the aov() output, I think there are two reasons. Your simple simulated dataset is pathological in that the best-fitting model is one that implies negative variance components, which mixed models fit by lmer() (and most other mixed model programs) won't allow. Even with a non-pathological dataset, the way you have the model set up, as mentioned above, does not always seem to work well in practice, although I must admit I don't really understand why. It is also just generally strange in my opinion, but that's another story. Let me first demonstrate the parameterization that I prefer on your initial two-way ANOVA example. Assume that your dataset d is loaded. Your model (note that I changed from dummy to contrast codes) was: options(contrasts=c("contr.sum","contr.poly")) mod1 which worked fine here in that it matched the aov() output. The model that I prefer involves two changes: manually contrast-coding the factors so that we are not working with R factor objects (which I recommend doing in 100% of cases), and specifying the random effects differently: d The two approaches are totally equivalent in the simple 2-way problem. Now we'll move to a 3-way problem. I mentioned earlier that the example dataset you gave was pathological. So what I want to do before addressing your example dataset is to first generate a dataset from an actual variance components model (i.e., where non-zero variance components are built into the true model). First I will show how my preferred parameterization seems to work better than the one you proposed. Then I will demonstrate another way of estimating the variance components which does not impose that they must be non-negative. Then we will be in a position to see the problem with the original example dataset. The new dataset will be identical in structure except we will have 50 subjects: set.seed(9852903) d2 The F-ratios we want to match are: aovMod1 Here are our two models: mod3 As we can see, only the second method matches the output from aov() , although the first method is at least in the ballpark. The second method also achieves a higher log-likelihood. I am not sure why these two methods give different results, as again I think they are "in principle" equivalent, but maybe it is for some numerical/computational reasons. Or maybe I am mistaken and they are not equivalent even in principle. Now I will show another way of estimating the variance components based on traditional ANOVA ideas. Basically we will take the expected mean square equations for your design, substitute in the observed values of the mean squares, and solve for the variance components. To get the expected mean squares we will use an R function that I wrote a few years ago, called EMS() , which is documented HERE . Below I assume the function is loaded already. # prepare coefficient matrix r Okay, now we will return to the original example. The F-ratios we are trying to match are: aovMod2 Here are our two models: mod5 In this case the two models yield basically the same results, although the second method has a very slightly higher log-likelihood. Neither method matches aov() . But let's look at what we get when we solve for the variance components as we did above, using the ANOVA procedure that does not constrain variance components to be non-negative (but which can only be used in balanced designs with no continuous predictors and no missing data; the classical ANOVA assumptions). # prepare coefficient matrix r Now we can see what is pathological about the original example. The best-fitting model is one that implies that several of the random variance components are negative. But lmer() (and most other mixed model programs) constrains the estimates of variance components to be non-negative. This is generally considered a sensible constraint, since variances can of course never truly be negative. However, a consequence of this constraint is that mixed models are unable to accurately represent datasets that feature negative intraclass correlations, that is, datasets where observations from the same cluster are less (rather than more) similar on average than observations drawn randomly from the dataset, and consequently where within-cluster variance substantially exceeds between-cluster variance. Such datasets are perfectly reasonable datasets that one will occasionally come across in the real world (or accidentally simulate!), but they cannot be sensibly described by a variance-components model, because they imply negative variance components. They can however be "non-sensibly" described by such models, if the software will allow it. aov() allows it. lmer() does not.
