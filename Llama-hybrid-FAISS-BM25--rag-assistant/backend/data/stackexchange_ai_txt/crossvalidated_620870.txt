[site]: crossvalidated
[post_id]: 620870
[parent_id]: 
[tags]: 
Likelihood in a Bayesian interference problem

I'm currently reading some lecture notes in the field of statistical physics for optimization problems. In there we are given a $N \times N$ symmetric matrix $Y$ as follows $$Y = \sqrt{\frac{\lambda}{N}} \vec{x}^*\vec{x}^{*\top} + \xi$$ where $\vec{x}^* \in \mathbb{R}^N$ , $\xi_{ij} = \xi_{ji} \overset{\mathrm{i.i.d}}{\sim} \mathcal{N}(0, 1)$ and $\lambda$ is the signal-to-noise ratio. So $Y$ is basically our measured data, $\vec{x}^*$ is the true signal and $\xi$ is Gaussian noise. The task is now to recover $\vec{x}^*$ from $Y$ and we do that by Bayes rules. We then define the likelihood $P(Y\mid \vec{x})$ : $$P(Y\mid \vec{x}) = \prod_{i\leq j} \left(\frac{1}{\sqrt{2\pi}} \exp\left[ -\frac{1}{2} \left( y_{ij} - \sqrt{\frac{\lambda}{N}}x_i x_j\right)^2\right]\right)$$ The biggest confusion for me here are the limits on the product. I guess the reason why we are only considering terms on the diagonal and the upper triangle part of the matrix $Y$ is that it's symmetric. But intuitively it somehow does not make sense to me to just ignore the lower triangle part of the matrix. Would we just do double counting and need to include an additional factor of $1/2$ if we were to also consider the remaining lower part since it's just the same as the upper part? Unfortunately I did not dive too deep into statistics in my undergrad so far but I would really like to understand how one needs to think about this equation. If someone could explain the underlying concept to me or maybe just guide me to some notes which go over these kind of things, I would be really glad.
