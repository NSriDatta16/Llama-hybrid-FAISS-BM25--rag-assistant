[site]: crossvalidated
[post_id]: 623303
[parent_id]: 623297
[tags]: 
The eigenvectors aren't the same. The eigenvalues are, to some extent. Consider the singular value decomposition $X=U^TDV$ . $U$ are the left singular vectors, $V$ are the right singular vectors, and $D$ is a diagonal matrix of $\min(N,d)$ positive real singular values and a bunch of zeros pasted either on the bottom or the right to make the dimensions match. This gives $X^TX=V^TD^TDV$ and $XX^T=U^TDD^TU$ . The eigenvalues of $X^TX$ are the diagonal elements of $D^TD$ and the eigenvalues of $XX^T$ are the diagonal elements of $DD^T$ . The $\min(N,d)$ non-zero eigenvalues are the same, and they are padded out by as many zero eigenvalues as needed. The eigenvectors aren't the same. They can't be. They aren't even the same length. The eigenvectors of $X^TX$ are the left singular vectors of $X$ and the eigenvectors of $XX^T$ are the right singular vectors of $X$ . However, the first $k$ eigenvectors of $X^TX$ explain the same proportion of the variance as the first $k$ eigenvectors of $XX^T$ (because that's an eigenvalue thing), so they are just as good for PCA. The difference matters more if you want to interpret the individual vectors -- eg if $X$ is a genotype matrix then $V$ represent ways that genetic loci are interestingly different between people and the $U$ represent ways that individuals are interestingly different. If, as is often the case, you only want the first few principal components and you aren't already given $X^TX$ or $XX^T$ , it's even faster to work with $X$ and extract the largest few singular values and corresponding singular vectors, using something like a Lanczos-type algorithm or stochastic SVD.
