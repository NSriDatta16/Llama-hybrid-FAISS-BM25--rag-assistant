[site]: crossvalidated
[post_id]: 163390
[parent_id]: 163388
[tags]: 
For a linear model with multivariate normal prior and multivariate normal likelihood, you end up with a multivariate normal posterior distribution in which the mean of the posterior (and maximum a posteriori model) is exactly what you would obtain using Tikhonov regularized ($L_{2}$ regularized) least squares with an appropriate regularization parameter. Note that there is a more fundamental difference in that the Bayesian posterior is a probability distribution, while the Tikhonov regularized least squares solution is a specific point estimate. This is discussed in many textbooks on Bayesian methods for inverse problems, See for example: http://www.amazon.com/Inverse-Problem-Methods-Parameter-Estimation/dp/0898715725/ http://www.amazon.com/Parameter-Estimation-Inverse-Problems-Second/dp/0123850487/ Similarly, if you have a Laplacian prior and a multivariate normal likelihood, then the maximum of the posterior distribution occurs at a point that you could get by solving an $L_{1}$ regularized least squares problem.
