[site]: crossvalidated
[post_id]: 399418
[parent_id]: 336520
[tags]: 
The removal of $a′_{output}$ is what should give cross-entropy it's superiority from my understanding, since the partial derivative will be bigger? The partial derivative in MSE will not be bigger here, rather it will be smaller, and that will create a lot of difficulty in learning when the output layer neuron is badly wrong at the beginning of learning - far more difficult than when it's just a little wrong. For example, if the output layer target was to predict 0, but suppose it predicted 0.9 at the beginning and started learning from there. Now think about the shape of the sigmoid function: it's flatter when it is close to 1, and if you take $a′_{output}$ at the position of 0.9, it will be very small. Eventually, in the derivatives of the cost function, the large error spotted by the $(o_{out}−o_{target})$ term will be suppressed by the small values of $a′_{output}$ . So, the gradient $∂C/∂w$ and $∂C/∂b$ will be very small for a long time during the backpropagation and thus the network will learn very slowly to come out from this saturation at the wrong end. However, in the case of cross-entropy since there is no $a′_{output}$ term, the $(o_{out}−o_{target})$ term is able to make the $∂C/∂w$ and $∂C/∂b$ very large when the output neuron is badly wrong, as a result, no neuron saturation will occur here and the learning in the network is very faster. But my question is about earlier layers, and if this new cost function affect the training of these layers? The new cost function affects the training of the earlier layers through backpropagation. During the backpropagation, the error in the output layer is propagated in the earlier layer. Then the derivatives in the cost function in the earlier layers will be updated based on the errors in the later layers and the weights and biases will be updated based on these derivatives. [Please go through chapter-2 from the book of Michael Nielsen for detailed understanding of the backpropagation] And a follow up question would be about a deep neural network with 100 hidden layers. If this new cost function does not affect the training of earlier layers, does it then really do much for a very deep neural network, and its training? The new cost function will not affect the training of the earlier layers if and only if there is a problem of vanishing gradient or neuron saturation at the wrong end, and if that occurs, that would badly affect the training process.
