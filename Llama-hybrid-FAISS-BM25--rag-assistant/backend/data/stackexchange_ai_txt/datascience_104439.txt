[site]: datascience
[post_id]: 104439
[parent_id]: 
[tags]: 
Is reinforcement learning analogous to stochastic gradient descent?

Not in a strict mathematical formulation sense but, would there be there any key overlapping principals for the two optimisation approaches? For example, how does $$\{x_i, y_i, \mathrm{grad}_i \}$$ (for feature, label and respective gradient from training example of SGD) defer from $$\{s_i, a_i, r_i\}$$ for state, action and reward example for RL? Given that $x_i$ can be viewed as a state, label $y_i$ as a reward (e.g. good/bad label) and $\mathrm{grad}_i$ as action. I appreciate that reinforcement learning is (a) learning what to do and how to map situations to actions as well as (b) learning from interaction and how in such a setting it is impractical to acquire "supervised training" training examples from all possible set of actions/rewards. But in essence, I would like to see whether there is a clear differentiation between the two abstractions above.
