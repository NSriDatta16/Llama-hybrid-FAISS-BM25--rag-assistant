[site]: crossvalidated
[post_id]: 616623
[parent_id]: 
[tags]: 
What is a reasonable range of penalty values to try in PELT changepoint analysis?

I have a simulated a time series data set several predictor variables and response class that I created to practice different analytical techniques before I apply methods to my real (very large) dataset and I'd like to use change point analysis to identify the segments of the data that are consistent and therefore likely consist of the individual behavioral bouts. While my question is ultimately theoretical in nature, it may be helpful to know that I'm using the changepoint library in program R and that I am using PELT with a range of penalty values (CROPS) to help me determine the optimal penalty values and thus number of change points in my dataset. However, I am confused as to what exactly these penalty values are and therefore how to choose an optimal "range" of penalty values to use. I found these two stackexchange posts that also ask similar questions but I don't see a firm answer onto what exactly the penalty value is post1 and post2 . Confusing the matter a little bit, is that I think the values of the penalty values (pen.values in the cpt fcn) seem to change based on the method being considered. Like for binary segmentation the pen.value can take on a range from 0 to 1, and similarly an asymptotic penalty (which maybe is only available for the AMOC method?), the pen.value attributes is the theoretical type I error. In other places I've seen it specified that the pen.value takes on a value of 0, or "2*log(n)" or the length of the data. This makes me think that the penalty value should be, at least in some way, related to the length of the dataset. Or maybe the expected number of breakpoints in the dataset? My dataset is 15,900 observations long composed of 1590 seconds of behavior (sampled at a 10 Hz frequency). I've tried running a large combination of penalty values just to see how it affects the data set as well as varying the penalty type used to estimate the segments but all vastly overestimate the number of change points in my data set which makes me wonder if the penalties (like 0, 2*log(n), (# of seconds), or even n) is somehow unrealistic for my dataset? Can someone please explain what the penalty values are? And if these is any "rule of thumb" type metric that I can use to help me figure out a decent range to consider? Thank you for your help.
