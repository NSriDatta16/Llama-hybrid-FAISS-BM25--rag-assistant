[site]: crossvalidated
[post_id]: 8139
[parent_id]: 7982
[tags]: 
Assuming data are considered missing completely at random (cf. @whuber's comment), using an ensemble learning technique as described in the following paper might be interesting to try: Polikar, R. et al. (2010). Learn++.MF: A random subspace approach for the missing feature problem . Pattern Recognition , 43(11) , 3817-3832. The general idea is to train multiple classifiers on a subset of the variables that compose your dataset (like in Random Forests), but to use only the classifiers trained with the non-missing features for building the classification rule. Be sure to check what the authors call the "distributed redundancy" assumption (p. 3 in the preprint linked above), that is there must be some equally balanced redundancy in your features set.
