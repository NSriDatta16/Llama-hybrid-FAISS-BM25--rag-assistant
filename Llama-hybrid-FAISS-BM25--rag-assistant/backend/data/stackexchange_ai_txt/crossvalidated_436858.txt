[site]: crossvalidated
[post_id]: 436858
[parent_id]: 436306
[tags]: 
Do NOT take my word as gospel. That said, I think it's impossible. Or possible in a very specific way. First problem is that D() and G() are not functions proper, they are variables . The second problem is that they are not independent. Worse, they are not only interdependent, they are recursively dependent on D-1(x) and G-1(x). Note that the minmax on the left hand side you mentioned has subscripts, D and G. They are quite significant; what that equation is saying is that, from the system's POV, you're trying to find a pair of functions, where G(X) is the simplest possible function that causes the current D(X) to be wrong . Let's say you pick D(x) = x1 , then the optimal G+1 will be something like, say G(x1) := (1 - x1) := x2 , which then forces D+1(x2) to be, say, D(x2) := x2+1 = x1+2 ... and that's just with univariate linear functions. The only reason an idealized GAN stabilizes is that you've exhausted the limits of your architecture; otherwise, it's an infinitely iterated game of cops-and-robbers between two functions evolving to counter each other.
