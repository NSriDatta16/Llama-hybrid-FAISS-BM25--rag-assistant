[site]: datascience
[post_id]: 75058
[parent_id]: 72586
[tags]: 
A decision tree has implicit feature selection during the model building process. That is, when it is building the tree, it only does so by splitting on features that cause the greatest increase in node purity, so features that a feature selection method would have eliminated aren’t used in the model anyway. This is different than say a random forest where each split is chosen on only a subset of features, so it is possible that a given split is chosen from only “bad” features. By performing feature selection you remove this possibility and improve performance in the forest
