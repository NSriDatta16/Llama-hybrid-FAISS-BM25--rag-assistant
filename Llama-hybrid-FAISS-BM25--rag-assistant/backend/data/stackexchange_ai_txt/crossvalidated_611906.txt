[site]: crossvalidated
[post_id]: 611906
[parent_id]: 264751
[tags]: 
In classification or in supervised learning in general, we can check how well our model performs because we have the true outcomes. We can compare the predicted and observed values using some evaluation metric, such as mean squared error, mean absolute error, log loss, or any number of other measures of performance. So do the same and calculate some evaluation metric of interest for the clustering algorithm, right? But you lack the labels, which is why this is an unsupervised problem, and you cannot evaluate how close you got to the right answer if you do not know the right answer. I see PCA and its explained variance as a separate beast. There, you consider how much of the variance within the variables is explained by the PCs, but that is independent of a label (which is why PCA is unsupervised). For evaluating a clustering algorithm and if it allocates points into the right clusters, you need some sense of what constitutes the right cluster to which the point should be allocated (supervised).
