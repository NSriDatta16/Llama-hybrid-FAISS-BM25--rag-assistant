[site]: crossvalidated
[post_id]: 421935
[parent_id]: 
[tags]: 
What exactly are keys, queries, and values in attention mechanisms?

How should one understand the keys, queries, and values that are often mentioned in attention mechanisms? I've tried searching online, but all the resources I find only speak of them as if the reader already knows what they are. Judging by the paper written by Bahdanau ( Neural Machine Translation by Jointly Learning to Align and Translate ), it seems as though values are the annotation vector $h$ but it's not clear as to what is meant by "query" and "key." The paper that I mentioned states that attention is calculated by $$c_i = \sum^{T_x}_{j = 1} \alpha_{ij} h_j$$ with $$ \begin{align} \alpha_{ij} & = \frac{e^{e_{ij}}}{\sum^{T_x}_{k = 1} e^{ik}} \\\\ e_{ij} & = a(s_{i - 1}, h_j) \end{align} $$ Where are people getting the key, query, and value from these equations? Thank you.
