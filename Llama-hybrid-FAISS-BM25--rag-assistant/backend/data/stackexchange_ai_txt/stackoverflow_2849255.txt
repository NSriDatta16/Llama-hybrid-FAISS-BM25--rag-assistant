[site]: stackoverflow
[post_id]: 2849255
[parent_id]: 2849101
[tags]: 
It really depends on what you want to do with the values in your inner lists/dictionaries. If, when you add a new entry, you want the inner list to have only unique values, then the list implementation will be much slower for large lists. It scales roughly at O(n), rather than O(1) [average case] for dictionaries. If you don't care about multiples in those inner lists, then it is a closer thing. I would use dictionaries, as you have. Python's dictionaries are very efficient (speaking as someone who's tried to implement dictionary data structures in C for real time applications). As for not using sets. It would be better (since memory isn't an issue, you say), to tweak the serialization, and have the speed critical part of your code be as simple as possible. After deserialization, just go through and convert the lists to sets: big_dict = {"the" : [1, 2, 3, 4, 5]} # imagine we got this from JSON for key, value in big_dict: big_dict[key] = set(value) Should do it. Unless you are serializing / deserializing the whole index all the time, this added pre-processing costs should be amortized over enough requests not to matter. Alternatively you can register encoders and decoders with JSON so you can do this conversion automatically. I usually don't bother when the issue is something so small and contained, however. So in your dictionary based approach you could do: for key, value in smaller_dict.items(): if key in big_dict: big_dict[key].update(value) else: big_dict[key] = value If you want the big_dict to only copy the dictionary, use dict(value) instead of value on the last line. You could also use try: and except KeyError in the last loop, but if ... else is a fraction faster (on my machine, YMMV).
