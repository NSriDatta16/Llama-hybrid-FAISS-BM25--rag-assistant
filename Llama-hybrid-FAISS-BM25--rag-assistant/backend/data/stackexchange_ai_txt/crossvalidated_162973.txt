[site]: crossvalidated
[post_id]: 162973
[parent_id]: 
[tags]: 
Computing fit of model to horizontally-misalligned time-series data

I have a model that predicts the level of harmonic tension in a piece of music, at every note/chord in the piece. I also have participant data (time series) that contains subjective ratings of tension for each musical piece, continuously recorded (at a 10 Hz sampling rate). I am wondering what is the most suitable description of how well each piece's model fits the participant data, i.e. the similarity between the two time-series. A colleague suggested I use a simple correlation (Pearson's r), but it does not seem an appropriate measure of time-series similarity to me, as the model values and the empirical values do not make sense as the x and y values of a plot. Another reason is that, given the high number of data points in each time series, the correlation would probably be very weak, given that this would increase R's second degree of freedom. What seems appropriate to me (but please correct me if I am wrong) is a statistic based on the sum (across all samples in the piece) of the squares of the differences between the model-predicted value and the actual value. However, I have the problem that, while both the model and the data are discrete in time, the model produces "event-related" values (at each note or chord, however spaced-apart these happen to be), whereas the subject data is acquired at fixed intervals (100 ms). So there's also the problem of horizontal allignment, before comparing the model with the data on the vertical axis. Perhaps some sort of time-sliding window can be defined, that permits the comparison of values that are to a certain degree shifted horizontally (offset)?
