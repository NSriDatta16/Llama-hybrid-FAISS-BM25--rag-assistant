[site]: datascience
[post_id]: 88634
[parent_id]: 
[tags]: 
Sampling n data points from high dimensional data

I have some face images(of a single person), which I ran through an embedding generator to get 128-dimensional embedding. I have 1000 such embedding (shape of the dataset (1000, 128)). I have a restriction on the number of embeddings that can be used to train the model (100 embeddings). I want to pick 100 embeddings from all 1000 which will represent all 1000 embeddings. My question is, how can I choose the best 100 embeddings which will represent the entire 1000 embeddings. Some things I have tried out. Picking the 100 farthest points in the cluster. (all the images are edge cases like blurred image, improper pose e.t.c) Random sampling ( works ok but sometimes pick embeddings which were calculated from a face which was having some problem, like blurred e.t.c.) There is one more thing, I have thought of but not tested. Sampling with probability. Probability = 1/(euclidean_distance of point from cluster centroid). I wanted to know if there are any alternatives, that I can look into which can provide better results.
