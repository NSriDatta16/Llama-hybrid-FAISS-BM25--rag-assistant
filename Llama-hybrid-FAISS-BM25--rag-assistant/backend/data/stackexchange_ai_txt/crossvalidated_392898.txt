[site]: crossvalidated
[post_id]: 392898
[parent_id]: 
[tags]: 
Why are rewards scaled when using Reinforcement Learning (RL) algorithms in practice?

I was going through this tutorial in pytorch and saw the following code: def loss(policy,gamma): R = 0 rewards = [] rs = [] # Discount future rewards back to the present using gamma reverse_list = policy.episode_rewards[::-1] for r in reverse_list: R = r + gamma * R rs.insert(0,r) rewards.insert(0, R) # Scale rewards rewards = torch.FloatTensor(rewards) rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps) I've never done RL in practice I'm trying to wrap my head around whats going on and why. I think I can summarize my confusion in 4 bullet points: I guess I never thought about the fact that we can't actually implement discounted rewards to infinity in practice. But discounting a finite horizon and then adding it seemed weird to me. Is that really the way its done? Why are we scaling the rewards at all? Seems really weird to me. Never seen this before (don't think I've ever seen it in supervised either). We are taking an average of the recursive equation $R_n = r_n + \gamma R_{n-1}$ rather than just the single reward $r_n$ , which also seems odd and confusing. Can anyone clarify why we prefer $J_R(\theta) = \frac{1}{n} \sum^n_{i=1} R_i$ rather than $J_r(\theta) = \frac{1}{n} \sum^n_{i=1} r_i$ . For some reason I;d expect the second one. my complain about the scaling (point #1) is that if we were to run this forever (which is the usual formulation I'm familiar with), we'd get everything being "on the same scale" but by dividing it into random episodes/chunks, I feel things might be shifted weirdly and I wouldn't know if we are optimizing the right thing anymore. Can someone give an intuition why this scaling thing is ok?
