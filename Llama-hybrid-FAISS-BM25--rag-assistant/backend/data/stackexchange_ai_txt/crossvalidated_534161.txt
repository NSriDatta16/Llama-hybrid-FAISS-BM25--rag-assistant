[site]: crossvalidated
[post_id]: 534161
[parent_id]: 
[tags]: 
What are the impacts of different learning rates on this model and why does it keep overfitting?

I am training a custom CNN model with 10 Layers based on this paper: https://journals.sagepub.com/doi/full/10.1177/1558925019897396 I have two classes (defect integrated circuit & non defect integrated circuit) with around 8.000 images after Data Augmentation for each class. I wanted to train this model from Scratch and used the Hyperparameters from the paper: parameter value Learning Rate 10^-4 Epochs 2000 Val Split 0,25 Batch Size 64 Optimizer Adam My architecture is as follows: input image is 384x384x3 conv 1-1 with 64 filters max pool conv 2-1 with 64 filters max pool conv 3-1 with 128 filters conv 3-2 with 128 filters max pool conv 4-1 with 256 filters conv 4-2 with 256 filters max pool conv 5-1 with 64 filters max pool dense with 512 units dropout with 0.5 ratio dense with 2 units softmax After training this model I see that the model is likely overfitting due to the validation loss stagnating: Until Epoch 100 the validation loss decreases and it seems to be a good model but after that as you see the validation loss gets out of hand. After reducing the Learning rate to 10^-5 because it seemed that the weights are jumping too much so the model can't find a local minimum to stay in. But the result was this: Now the validation loss doesn't only seem to be instable but it also increases over time. I also tried a learning rate of 10^-6, but it also seems not to be good: So I tried with a learning rate of 10^-3 but that was clearly the worst model: Now I know I could schedule the learning rate and start to decrease it over time, but my professor wants me to train the model with a "static" learning rate. How would you explain these training results? I thought decreasing the learnig rate would help the model but it clearly doesn't. Does that mean that the overfitting doesn't have to do with the learning rate (I know there are many factors to it) but maybe with the complexity of the architecture? Are there too many layers or why does it keep overfitting? Or would you just stick to a learning rate of 10^-4 and stop training around 100 epochs. Because this is what I get when I stop then: The thing is 80 epochs are very low for a model that is trained from scratch or not?
