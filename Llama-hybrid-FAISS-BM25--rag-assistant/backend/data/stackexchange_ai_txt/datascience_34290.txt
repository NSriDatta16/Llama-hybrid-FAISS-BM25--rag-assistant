[site]: datascience
[post_id]: 34290
[parent_id]: 25739
[tags]: 
Word embeddings are just a way to represent tokens (often words, but could be characters) in a way that it inherently carries semantic meaning (i.e. as opposed to simply one-hot encoding), but it is not a keyphrase extraction technique . Word embeddings can help you extract key phrases better because they will make your input more meaningful, but unless you have a technique to extract such phrases, they won't help you.
