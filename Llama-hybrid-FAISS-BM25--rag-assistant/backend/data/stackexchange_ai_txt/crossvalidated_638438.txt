[site]: crossvalidated
[post_id]: 638438
[parent_id]: 638416
[tags]: 
The intent of each classifier's use is to provide a predicted class label. Thus, the softmax within one neural network is used to generate the vote. Since it sounds like you have more than one softmax function, you could add together class-specific softmax results and then apply softmax on the sum, but you don't have to. Enter ensemble classifier fusion . Under this approach, classifiers are trained with the same feature sets and training/testing objects, and then classifier votes are combined using the ensemble majority voting (EMV) and ensemble weighted majority voting (EWMV) ensemble combination techniques [1]. Let $d_{l,\omega}({\bf x}) \in \{0,1\}$ be the decision rule for an object $\mathbf{x}$ by the $l$ th classifier $(l=1,2,\ldots,L)$ for class $\omega$ $(\omega=1,2,\ldots,\Omega)$ . The support for EMV and EWMV, respectively, is functionally composed as \begin{equation} \boldsymbol{\mu}_\omega({\bf x}) = \sum _{l=1}^L d_{l,\omega}({\bf x}), \quad \quad d_{l,\omega}({\bf x}) \in \{0,1\}, \end{equation} \begin{equation} \boldsymbol{\mu}_\omega({\bf x}) = \sum _{l=1}^L w_l d_{l,\omega}({\bf x}), \quad \quad d_{l,\omega}({\bf x}) \in \{0,1\}, \end{equation} where $w_l$ is the normalized weight reflecting the accuracy of the $l$ th classifier. Here, accuracy is based on the proportion of classified test objects assigned to the diagonal of the confusion matrix divided by the number of test objects. Let the set of class labels be $\omega=1,2,\ldots,\Omega $ and the ensemble decision for object $\mathbf{x}$ be $\cal{E}({\bf x}\rightsquigarrow \omega)$ . The decision rule for test object $\mathbf{x}$ is \begin{equation} {\cal{E}}({\bf x}\rightsquigarrow \omega) \equiv \arg \underset{c} \max \{ \boldsymbol{\mu}_c({\bf x})\}. \end{equation} M. van Erp, L. Vuupijl, L. Shomaker. An overview and comparison of voting methods for pattern recognition. Proc. 8th Int. Workshop Frontiers in Handwriting Recognition (WFHR02) . Hoboken(NJ), IEEE Press, 2002.
