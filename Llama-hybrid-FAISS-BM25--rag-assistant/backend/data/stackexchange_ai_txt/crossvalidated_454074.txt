[site]: crossvalidated
[post_id]: 454074
[parent_id]: 
[tags]: 
How to retrieve elasticity from neural network when data was normalised before estimation?

I have a binary choice neural network where I want to calculate the elasticity of probability of positive for observation $n$ , $P_n$ with respect to input variable, $X_n$ . By definition, this elasticity is $$E_x=\frac{\partial P_n}{P_n}\frac{X_n}{\partial X_n}.$$ Before estimation, I have normalised my data set. In this case, let $Z_n=(X_n-\mu)/{\sigma}$ . However, this means my elasticity calculations would be less straightforward. For example, in a binary logit model, denote $$E_x=\frac{\partial P_n}{P_n}\frac{X_n}{\partial X_n}=\beta_xX_n(1-P_n)$$ $$E_z=\beta_zZ_n(1-P_n)$$ Since what I want is $E_x$ , after getting $E_z$ from my logit model, I can calculate $E_x=E_z+\frac{\mu}{\sigma}\beta_z(1-P_n)$ . I observe similar issues exist with my neural network. How should I retrieve the 'real' elasticity after calculating elasticity directly from the network? Edit: I think I might have worked out the solution, please feel free to critique. For simplicity of notation, we omit the $n$ in subscripts. Denote the output choice probability from the classifier as $P_Z(Z=z)$ . Suppose in an ideal classifier that takes $X$ as input, the same metric is denoted $P_X(X=x)$ , where, again, $z=(x-\mu)/{\sigma}$ . We should have $$P_Z(Z=\frac{x-\mu}{\sigma})\equiv P_X(X=x),$$ although computationally this might not always be true. We want to get $$E_X(x)=\lim_{\delta \to 0}\frac{x}{\delta}\frac{P_X(x+\delta)-P_X(x)}{P_X(x)}=\lim_{\delta \to 0}\frac{\sigma z+\mu}{\delta}\frac{P_Z(\frac{x+\delta-\mu}{\sigma})-P_Z(\frac{x-\mu}{\sigma})}{P_Z(\frac{x-\mu}{\sigma})}=\lim_{\delta \to 0}\frac{\sigma z+\mu}{\delta}\frac{P_Z(z+\frac{\delta}{\sigma})-P_Z(z)}{P_Z(z)}=E_Z(z)+\frac{\mu}{\sigma}\frac{1}{P_Z(z)}\frac{\partial P_Z(z)}{\partial z}$$ Therefore with any affine transformation of input variables, we can get the elasticity of choice with respect to $X$ . The logit model can thus be viewed as a special case of this generalisation.
