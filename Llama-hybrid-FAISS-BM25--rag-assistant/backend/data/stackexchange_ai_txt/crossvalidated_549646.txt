[site]: crossvalidated
[post_id]: 549646
[parent_id]: 347443
[tags]: 
You are doing it wrong. If you need to find out an appropriate weights like $x^\alpha$ or $\gamma^x$ , than those should be parameters of the model , rather than something you tune during hyperparameter tuning. First of all, if you just tune them using a fixed grid (0.50, 0.51, 0.52, ...) you are missing the intermediate values, so you won't be able to find the values that might work better than the ones you checked. Second, how would those values differ from parameters of a model? It shouldn't really matter if you transform the data with $x^{0.51}$ vs $x^{0.52}$ , it would matter if you ask about square root vs squared values, but the model has parameters that would enable it to accommodate to the data and you should not need to pick the transformations by hand. If you are seeing performance difference, than it is likely that it's either numerical issues (so you should rather do something like normalization of the data) or the differences are by chance (random initialization). In both cases, using something like $k$ -fold cross-validation would be strongly encouraged to verify if the results are stable across different subsamples and runs.
