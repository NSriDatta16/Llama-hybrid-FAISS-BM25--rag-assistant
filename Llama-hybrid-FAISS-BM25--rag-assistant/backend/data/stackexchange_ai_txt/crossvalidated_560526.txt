[site]: crossvalidated
[post_id]: 560526
[parent_id]: 
[tags]: 
Non-reproducible RMSE for linear regression when including categorical features

ML newbie here. I searched on the web but apparently I'm missing some ML lingo to make the search efficient, because the problem is probably quite ordinary. What I'm on : A linear model to predict house sale prices using a dataset with about 2600 examples. I selected 15 numerical and 11 categorical features. When I fit the model using only all numerical features, it goes reasonably well: average RMSE is around 27000 USD (calculated using 10-fold cross validation, but reproducible changing k and random_state in Python within a few hundreds USD). Things go well even when I start adding categorical features to the model one at the time: each model has its own average RMSE, between 24 and 26000 USD. What's the problem : When I use two or more categorical features (combined with all numerical feats), the avg RMSE starts to be unpredictable (see image). The model sometimes performs consistently better than all the other models (see right column in image), but some other time it can explode to RMSE of the order of 10^16. The problem seems to come especially (but not uniquely) from the feature "Neighborhood", which has 28 categories, much more than the other cat. features. (I add that the situation was even worse than this before, out of despair, I dropped one of the classes from the one-hot encoded cat. features [i.e. the drop_first=True in the pandas.get_dummies function]. RMSEs before were even more erratic.) My best guess : Sometimes the train set doesn't have all the classes of the cat. features, so the prediction on the test set badly fail (even one giganticly wrong prediction can make the RMSE explode). IMG explanation : the left column represents the avg RMSE without the Neighborhood feature for a few iterations changing the random state (k+1 is the number of folds). The right column is the same including Neighborhood in the model. Any possible clue? Thanks
