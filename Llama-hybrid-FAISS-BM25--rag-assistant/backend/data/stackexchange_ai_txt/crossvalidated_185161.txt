[site]: crossvalidated
[post_id]: 185161
[parent_id]: 185160
[tags]: 
Looking at extensions is cheating so I won't go into that. The point of compression is to increase the information density in the resulting data. Random data has the highest information density (see e.g. wikipedia ). Without going into details, this implies for binary compressed files that, ideally, the probability of the next bit to be 1 given the seen compressed data should be close to 50%. Hence, if data is properly compressed, its bit representation should not contain residual patterns. You can test for randomness of the bit representation using standard statistics. Fancy machine learning is probably not necessary, though it may be interesting to see if you can identify which algorithm (if any) was used to compress a given file.
