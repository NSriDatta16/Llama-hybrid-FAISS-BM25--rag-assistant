[site]: crossvalidated
[post_id]: 483741
[parent_id]: 483739
[tags]: 
No, there is no such consensus but common sense and relative success (compared to others in the literature). An $R^2$ of 0.8-0.9 is typically a good thing, but in some applications 0.6 can also be regarded as very good . It all depends on how hard the problem is. I know that this is not your original question, but I wanted to give you and anyone reading this post a heads up: Sometimes plotting true vs real in time series problems can be misleading about how good the predictions are because if the time range is long enough, even using a very simple estimator, e.g. $\hat x_t=x_{t-1}$ , might give the impression that the true and predicted values overlap very well, however it is just a visual illusion. In your plot, I see a similar time lag between true vs predicted.
