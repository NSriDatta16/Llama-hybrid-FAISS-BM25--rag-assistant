[site]: crossvalidated
[post_id]: 535262
[parent_id]: 350915
[tags]: 
TLDR: try a default threshold like $10^{-3}$ first. If that is slow, switch to a faster-than-linear algorithm that uses/approximates the Hessian. It greatly depends. If your log likelihood has a low curvature, then you could be stuck waiting a long time for the objective function increments to fall below a fixed threshold like $10^{-3}$ . This is because EM is only a first-order optimization method. So, my advice would be to first run your EM algorithm using one of the default recommendations like $10^{-3}$ . By the way, this default number should be with respect to the average objective (e.g., divide the ELBO sum by the sample size $N$ ). Ideally, it would terminate relatively quickly. However, if you get stuck in a long loop where each step is always increasing the objective function by only a small amount (but larger than your threshold), then you should look into an algorithm that has better convergence rate. For example, if you can compute the EM steps, you can probably calculate the gradient instead (eq. 8). With the gradient you can run a faster-than-linear algorithm like BFGS that is widely implemented . Personally, this trick has saved me countless hours.
