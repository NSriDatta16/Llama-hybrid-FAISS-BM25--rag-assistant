[site]: crossvalidated
[post_id]: 158634
[parent_id]: 
[tags]: 
What functions do decision trees and random forests learn?

We know that training a function $y = f_\theta(x)$ (parameterised by $\theta$ in some fashion, for e.g., the class of linear functions) using data $\{(x_i, y_i)\}$ drawn i.i.d from some unknown distribution $p(x,y)$, by optimising the empirical mean squared loss objective function, essentially learns to represent the function $y = E[y|x]$. I am curious to know if there's a similar formulation for decision trees and random forests. What do they learn? That is, suppose I am given samples $\{(x_i,\tilde{y_i})\}$ (drawn i.i.d from some unknown distribution), where the $\tilde{y}$ is (say) the 0-1 class label, what function the decision tree learn to represent? I have read [1] that training a classifier under the cross-entropy loss function results in a minimiser that approximates $p(\tilde{y}|x)$, where $\tilde{y}$ is the 0-1 class label. [1] https://github.com/JohnLangford/vowpal_wabbit/wiki/Loss-functions
