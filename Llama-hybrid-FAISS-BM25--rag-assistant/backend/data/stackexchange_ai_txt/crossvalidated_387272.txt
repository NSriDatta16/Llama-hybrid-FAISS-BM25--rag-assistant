[site]: crossvalidated
[post_id]: 387272
[parent_id]: 
[tags]: 
Hard attention loss function

I am referring to paper: Show, Attend and Tell: Neural Image Caption Generation with Visual Attention (page 4). I wished to know, why we look to maximize the lower bound of the log likelihood probability $log p(y|a)$ and not itself. I agree that we sample images for the $t_{th}$ word stochastically, but I don't get the complete reasoning as to why we do this...
