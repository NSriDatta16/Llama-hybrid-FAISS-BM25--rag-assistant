[site]: crossvalidated
[post_id]: 327584
[parent_id]: 
[tags]: 
Using the latent representation from an autoencoder as input to layers of LSTMs

When people talk about using an autoencoder for feature extraction before some network, in this case a deep network of LSTMs, are they referring to first training the autoencoder, and then passing the latent representation to the next network, or are they talking about passing the reconstruction? What are the pros and cons? Would passing the latent representation of the autoencoder to a deep LSTM aid in time series analysis?
