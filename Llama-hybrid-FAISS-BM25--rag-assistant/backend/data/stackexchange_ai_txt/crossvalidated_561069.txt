[site]: crossvalidated
[post_id]: 561069
[parent_id]: 179383
[tags]: 
$R^2$ compares your model to a naïve model that always predicts the same value. When your model includes an intercept, it makes sense that the naïve guess, ignoring the features, of the conditional mean would be the pooled/marginal mean of all $y$ observations: $\bar y$ . If we ignore the features, the linear regression equation is: $$ y = \beta_0 + 0x_1 +\cdots + 0x_p =\beta_0 $$ OLS regression gives us $\hat\beta_0=\bar y$ . This is how we wind up with the denominator of the usual $R^2$ equation. Every prediction is the same $\bar y$ value. $$ R^2_{usual} = 1-\dfrac{ \sum\bigg( y_i - \hat y_i \bigg)^2 }{ \sum\bigg( y_i - \bar y \bigg)^2 } $$ If we exclude an intercept from the model, then the model with no features is: $$ y = 0x_1 +\cdots + 0x_p = 0 $$ That is, the naïve model always predicts zero, and the comparison to the naïve model would reflect that fact. $$ R^2_{no\_intercept} = 1-\dfrac{ \sum\bigg( y_i - \hat y_i \bigg)^2 }{ \sum\bigg( y_i - 0 \bigg)^2 } $$ If you want a general way of thinking about $R^2$ , I would go with the idea of comparing to a naïve model that has no features. It doesn't stop there. While this will wreck the typical interpretation of $R^2$ as the "proportion of variance explained" (which does not apply to the no-intercept version, anyway, and this interpretation vanishes in many settings with intercepts, too ), you can use any baseline model in the denominator. Want to compare your fancy neural network to a linear model? $$ R^2_{sorta} = 1-\dfrac{ \sum\bigg( y_i - \hat y_{i, neural} \bigg)^2 }{ \sum\bigg( y_i - \hat y_{i, linear} \bigg)^2 } $$ This is equivalent to comparing the models on mean squared error, $MSE = \frac{1}{n}\sum\big( y_i - \hat y_i \big)^2$ , however, and I do not see an advantage to $R^2_{sorta}$ except that it forces you to make this comparison. Viewing the problem in terms of $R^2_{sorta}$ , the $R^2_{usual}$ and $R^2_{no\_intercept}$ equations are comparing to baseline models that are totally naïve. Cardinal wrote a rather excellent answer related to this topic.
