[site]: stackoverflow
[post_id]: 4332799
[parent_id]: 4332458
[tags]: 
There are some issues that will make your code practically unusable: Firing up as many processes as you have files - big no-no. You will congest your CPU and won'2 get any benefits of your super multicore machine afterall. Rule of thumb: up to 2 processes per core. That will warm um the processor just fine. Disk fragmentation. Writing to 100 files at once will leave your hard drive so fragmented, you'l have it choke in no time. Reusing Process object: again, bad thing. If you want it like that: create one Process instance in a loop, and store it in some kind of List. If you really stick with idea of 'run all at once' - run them, store them in a list, then iterate the list and wait each one to complete! Creating processes then asking process list from the system and searching them by name - why when you created them in the first place? EDIT: How you could do it: investigate how many CPU cores you have create Array twice as big in foreach loop, do this: determine if you have place in your array (any of the processes is null) if so - create new process, put it into the array if no, loop: check (nonblocking) if any of your processes is completed; first one that is, set it to null (inside the array) - if none are done, Sleep a little (my magic number is 350 - you choose your own)
