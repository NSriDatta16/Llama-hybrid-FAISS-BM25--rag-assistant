[site]: crossvalidated
[post_id]: 363978
[parent_id]: 
[tags]: 
"We have to apply feature scaling on test set using scaling parameters from train set." Is this statement true? If yes, why?

I understand that in standard data cleaning and pre-processing pipelines, we have to make sure that the information from the test set (or what would be the test set after splitting) does not leak into the training process, so that it simulates real-world scenarios where real test data, for which we are building the model, is unknown at the time of training the model. However, in processes like scaling the features, such as min-max-scaling, $x'={\frac {x-{\text{min}}(x)}{{\text{max}}(x)-{\text{min}}(x)}}$ and standardisation, $x' = \frac{x - \bar{x}}{\sigma}$, why should we use the scaling parameters (min, max, mean, variance) from the training set when scaling the test set? Why can't we compute these parameters from the test set itself and use them to scale test set? As for as I can tell, no information is being leaked or assumed. (I know ideally both sets should have similar values for these parameters.) The second (worrying) part of the question is, by scaling the test set with its own parameters, have I only been compromising the performance of the model or have I been breaking some more fundamental logic behind machine learning process?
