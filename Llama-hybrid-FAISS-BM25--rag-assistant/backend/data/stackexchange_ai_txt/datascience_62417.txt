[site]: datascience
[post_id]: 62417
[parent_id]: 62409
[tags]: 
In the general case, this is by no means true. Let's break down the case for different data scenarios: For discriminative image models (e.g. image classification/labeling) this is true for some scenarios. You just throw some convnets (even pretrained models) at your data, and that's it. Nevertheless, convnets themselves profit from the "expert knowledge" that information locality is important and so is hierarchical information processing. For some other scenarios, applying domain knowledge (e.g. specific data transformations) may give the edge to reach the needed level of quality in the results. For many image processing problems, neural networks work best when infused with some kind of inductive bias, e.g. attention. For Natural Language Processing (NLP) problems, a good amount of craftsmanship is needed nowadays, especially in the data preprocessing stage. For "typical data science" problems, it is also crucial to do feature extraction. You can have a look at Kaggle competitions to verify this. For time series problems, it is also normal to rely on expert knowledge to understand which models fit best based on the nature of the data. However, I think that the trend of the areas where deep learning is applicable (i.e. tons of available data) is to try to devise systems that are trained end-to-end, with the least possible ad hoc processing. Nevertheless, many times this is achieved by infusing the expert knowledge into the network in the form of inductive biases.
