[site]: crossvalidated
[post_id]: 392307
[parent_id]: 392283
[tags]: 
As explained in the answer you cited above, the covariance matrices are referring to two different models, one in the marginal model (integrating out the random effects), and the other on the conditional model on the random effects. It is not that one is better than the other because they are not referring to the same model. Which one you select depends on your question of interest. Regarding your second question, you have to be a bit more specific on what you exactly you mean by "BLUPs" under the two models. For example, the empirical Bayes estimates of the random effects are derived using the same idea under the two approaches, i.e., the mode of the conditional distribution of the random effects given the observed outcomes. You can add these to the fixed-effects part to obtain subject-specific predictions, taking into account though that in the case of glmer() you also have a link function. EDIT: Regarding the two models mentioned above; say, $y$ is your outcome variable and $b$ are the random effects. A general definition of a mixed model is: $$\left\{ \begin{array}{l} y \mid b \sim \mathcal F_y(\theta_y),\\\\ b \sim \mathcal N(0, D), \end{array} \right.$$ where $\mathcal F_y(\theta_y)$ is an appropriate distribution for the outcome $y$ , e.g., it could be Gaussian (in which case you obtain a linear mixed model), Binomial (and you obtain a mixed effects logistic regression), Poisson (mixed effects Poisson regression), etc. The random effects are estimated as the modes of the posterior distribution $$ p(b \mid y) = \frac{p(y \mid b) \; p(b)}{p(y)}, $$ where $p(y \mid b)$ is the probability density or probability mass function behind $\mathcal F_y$ , and $p(b)$ the probability density function of the multivariate normal distribution for the random effects. With regard to your question, the covariance matrix of the empirical Bayes estimates obtained from ranef() is related to the covariance of this posterior distribution, whereas VarCorr() is giving the $D$ matrix, which is the covariance matrix of the prior distribution of the random effects. These are not the same. EDIT 2: A relevant feature of the estimation of the random effects is shrinkage. That is, the estimates of the random effects are shrunk towards the overall mean of the model. The degree of shrinkage depends on The relative magnitude of the variance of the random effects versus the variance of the error terms. I.e., the larger the variance of the random effects with respect to the error variance, the smaller the degree of shrinkage. The number of repeated measurements. The more repeated measurements, the smaller the degree of shrinkage. The following code illustrates this in the simple random-intercepts model: prior_vs_post $y id], sqrt(error_variance)) ############### # Fit the model fm $id[1L], BLUPs_variance = var(ranef(fm)$ id[[1L]])) } # high variance of REs, low variance error terms # 2 repeated measurements => low shrinkage prior_vs_post(prior_variance = 10, error_variance = 1, repeated_meas_per_id = 2) #> estimated_prior_variance BLUPs_variance #> 11.05215 10.58501 # high variance of REs, low variance error terms # 20 repeated measurements => almost no shrinkage prior_vs_post(prior_variance = 10, error_variance = 1, repeated_meas_per_id = 20) #> estimated_prior_variance BLUPs_variance #> 10.07539 10.02580 # low variance REs, high variance error terms, # 20 repeated measurements => considerable shrinkage prior_vs_post(prior_variance = 1, error_variance = 10, repeated_meas_per_id = 20) #> estimated_prior_variance BLUPs_variance #> 1.0002202 0.6666536 # low variance REs, high variance error terms, # 2 repeated measurements => extreme shrinkage prior_vs_post(prior_variance = 1, error_variance = 10, repeated_meas_per_id = 2) #> estimated_prior_variance BLUPs_variance #> 0.9479291 0.1574824
