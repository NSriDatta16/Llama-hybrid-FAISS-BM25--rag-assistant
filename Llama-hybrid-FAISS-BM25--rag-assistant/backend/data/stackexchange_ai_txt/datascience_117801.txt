[site]: datascience
[post_id]: 117801
[parent_id]: 117792
[tags]: 
So I think there are a few concepts being mixed up in your question, I will do my best to address them one by one. The "weight matrix" you refer to (if it's the same one as Aneesh Joshi's post) is related to "attention", a concept in Deep Learning that, in simple terms, changes the priority given to different parts of the input (quite literally like human attention) depending on how it is configured. This is not necessarily a the weight of a single neuron in a neural network, so I will come back to this once I have cleared up the other concepts. To understand Transformers and NLP models, we should go back to just basic neural networks, and to understand basic neural networks we should understand what a neuron is, how it works, and how it is trained. History break: basically for a long time (in particular popularity since the 1800s) a lot of modelling was done using linear models and linear regression (you see the effects of this in classical statistics/econometrics). However, as the name implies, this only captures linear relationships. There were two questions that led to resolving this: (1) how do we capture non-linear relationships? (2) how do we do so automatically in an algorithmic manner? For the first one, many approaches existed, such as Logistic regression. You might notice that in logistic regression, there is still a linear equation that is being estimated, just not as the final output. This disconnect between the input linear equation and output non-linear equation is now known as having an input Linear cell with an Activation function. In logistic regression, the Activation function is the logistic function, whereas in linear regression it is just the Identity function. In case it is still not clear, imagine this: Input -> Linear Function -> Activation Function -> Output. Putting that entire sequence together gives you the Perceptron (first introduced in the 1940s). Methods of optimizing this were done via gradient descent and various algorithms. Gradient descent is probably the most important to keep in mind and helps us answer the second question. Essentially what you are trying to do in any form of automated model development, is you have some linear equation (i.e. $y = \beta\times w + \beta_{0}$ ), which you feed an input, pass through an activation function (i.e. Identity, sigmoid, ReLu, tanh, etc), then get an output. And you want that output to match a value that you already know (in statistics imagine $y$ and $y_{hat}$ . In order to do that (at least in the case of a continuous target class) you need to know how far off is your prediction from the true value. You do this by taking the difference. However the difference can be positive/negative, so usually we have some way of making this semi-definite positive by either squaring (Mean Squared Error) or taking the absolute value (Mean Absolute Error). This is known as our loss function. Essentially we would have developed a good estimator/predictor/model if our loss is 0 (which means that our prediction matches our target value). Linear regression solves this using Maximum Likelihood Estimation. Gradient descent effectively is an iterative algorithm that does the following. First we take the derivative of our loss function with respect to one of our model weights (i.e. w) and we apply something similar to the update that happens in the Newton-Raphson method, namely $w_{new} = w_{old} - step\times\frac{dL}{dw}$ where step is a step-size (either fixed or changing). A stopping condition is typically if your change from the new weight to the old weight is sufficiently small you stop updating as you have reached a minimum. In a convex scenario this would also be your global minimum, however classically you have no way of knowing this, which is why multiple algorithms use some approaches like random starts (multiple starting points randomly generated) to try and avoid getting stuck in local minima (aka a loss value that is low but not the lowest it could be). Ok so if you have read that take a quick stretch and process it since I am not entirely sure of the reader's background so it may have been a lot to process or fairly straightforward. So far we covered how to capture non-linear relationships and how to do it in an automated way. So does a single neuron train like that? Yes. Do a collection of neurons train like that? No. Recall that a neuron is just a linear function with an activation function on top that performs some form of gradient descent. However a neural network is a chain of these, sometimes interconnecting, etc. So then how do we get a derivative of the loss which is at the end of a network to the very beginning. Through a technique that took off in the mid-1980s the technique called backpropagation (as a rediscovery of techniques dating back to the 1960s). Essentially we take partial derivatives, and then through an application of the chain rule are easily able to propagate various portions of the loss backwards, updating our weights along the way. This is all done in a single pass of training, and thankfully, automatically. The classical approach is to feed in your entire dataset THEN take the loss gradient, known as Gradient Descent. Or you can feed only a single data point before updating, known as Stochastic Gradient Descent. There is, of course a middle ground, thanks to GPUs, taking a batch of points known as Batch learning. A question that might have come up is, is the derivative of our loss function just linear? No, because the activation can be non-linear, so essentially you are taking the derivatives of these non-linear functions, and that can be computationally expensive. The popular choice today is ReLU (Rectified Linear Unit) which, for some linear output $y$ is basically a max function saying $output = max(y,0)$ , that's it. Things like weight initialization make more of an impact as performance across different non-linear activation functions are pretty comparable. Ok so we covered how a neuron works and how a neural network optimizes its weights in a single pass. A side note is you will often hear "train/validation/test" set, which are basically splits of your dataset. You split your dataset prior into a training subset, for, well, training, which is where you modify the weights through the process I described above for the entire training set or each data point (or batches of data points if you are using GPUs/batches in Deep Learning). Usually your data might need to be transformed depending on the functions you are using, and the mistake most practitioners make is pre-processing their data on the entire dataset, whereas the statistically correct way of doing so is on the training set only, and extrapolating to the validation/test set using that (since in the real world you may not have all the information). The validation set is there for you to test out different hyper-parameters (like step-size above, otherwise known as learning rate) or just compare different models. The test set is the final set that you use to truly see the quality of your model, after you have finished optimizing/training above. Now we can finally get to your question on attention. As I described a basic neural network, you may have noticed that it receives an input once, does a run through, and then optimizes. Well what if we want to get inputs that require some processing? Like images? Well this is where different architectures come up (and you can read this all over the web, I recommend D2L.ai or FastAI's free course), and for images the common one are Convolutional Neural Networks, which were useful in capturing reoccurring patterns and spatial locality. Sometimes we might want more than one input, aka a previous input influencing how we process this next input, this is where temporal based architectures come in like Recurrent Neural Networks, and this was what was initially used for languages, but chaining a bunch of neurons takes a while to process since we can't parallelize the operations (which is where the speed of neural network training comes from). Plus you would have to have some way of pre-processing your language input, such as converting everything to lowercase, removing non-useful words, tokenizing different parts of words, etc, depending on the task. There was quite a lot to deal with. Up until a few years ago when Attention based models came out called Transformers, and they have pretty much influenced all aspects of Deep Learning by allowing parallelization but also capturing interactions between inputs. A foundational approach is to pre-process inputs using an attention matrix (as you have mentioned). The attention matrix has a bit of complexity in terms of the math (Neuromatch's course covers this well), but to simplify it, it is effectively the shared context between two inputs (one denoted by the columns, the other the rows), like two sentences. The way this is trained (and it depends on the model) is generally by taking an input and converting it into a numerical representation (otherwise known as an embedding) and outer-multiplying these two vectors to produce a symmetric matrix, which obviously has the strongest parts on the main diagonal. The idea here is to then zero out the main diagonal, and then train the network to try and use the remaining cells to fill this main diagonal in (you can once again do this via a neural network and the process described above). Then you can apply this mechanism with two different inputs. In a translation task for example, you would have a sentence in language A and another in B, and their matrix would highlight the shared context, which may not be symmetric depending on the word order/structure, etc. Transformers are not like Recurrent Neural Networks in that they take an input at once and then give an output, but compromises exist and pre-training models is also a thing. There is a lot I haven't touched upon, but the resources I mentioned should be a useful starting point, and I wish you luck in your Data Science journey!
