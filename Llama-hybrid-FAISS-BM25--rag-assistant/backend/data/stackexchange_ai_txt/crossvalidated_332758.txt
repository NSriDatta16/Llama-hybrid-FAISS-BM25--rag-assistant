[site]: crossvalidated
[post_id]: 332758
[parent_id]: 
[tags]: 
How does a marginal likelihood come to include both parameter and model values?

I am new to this issue and am trying to get my head around how Bayes Factors work. I think that when using a Bayesian approach to estimate parameters in a model, we use the formula: P(theta|data) = P(data|theta) x P(theta) / P(data) Now, suppose I want to use a Bayesian approach to test the hypothesis that theta = 0 (i.e., the null hypothesis in this regard). In theory, I think the formula is: P(hypothesis|data) = P(data|hypothesis) x P(hypothesis) / P(data) In practice, one way that this is done is to use Bayes Factors. From my discussion here , I've gathered that this can be done in two ways: comparing the density of the posterior at zero to the density of the prior at 0. Another way to do this would be to compare a model with and without the parameter of interest. This is where I start to get lost in terms of the corresponding formula. From reading this , for instance, it sounds like the way to do this is to compare the marginal likelihoods of the two models. However, up until now, the marginal likelihood has been ignored. The paper I just linked gives a model's marginal likelihood as this . This formula doesn't make sense to me as I have up until now understood a formula to either be involving a parameter value or a hypothesis value (i.e., true or false). Can you help me understand just what this formula means, in the context of the previous two I have given?
