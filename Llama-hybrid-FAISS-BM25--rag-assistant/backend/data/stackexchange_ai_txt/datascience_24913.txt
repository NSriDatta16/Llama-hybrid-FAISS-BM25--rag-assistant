[site]: datascience
[post_id]: 24913
[parent_id]: 24867
[tags]: 
Accuracy is not a function of number of features. You might have heard about feature selection processes, and that is the reason we use feature selection or dimensionality reduction. There is huge theory behind this, but in very generic terms, we need to find out the best features, to make our model least complex, still keeping all relevant features in the model. One example, can be adding a new feature reduces accuracy, which is highly biased towards one out come and hence it creates inaccuracy in your results. This may also happen, if all your features are not normalised, and hence the newly added features is biasing your results and hence introducing inaccuracy.
