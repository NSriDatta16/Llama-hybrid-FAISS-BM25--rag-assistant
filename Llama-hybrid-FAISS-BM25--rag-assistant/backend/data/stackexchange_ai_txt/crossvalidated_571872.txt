[site]: crossvalidated
[post_id]: 571872
[parent_id]: 
[tags]: 
VAE loss doesn't converge to zero. Does it make sense to sample new instances from trained latent space?

I aim to use a variational autoencoder (VAE) as a generative model. Does this make sense only if the reconstruction loss converges towards zero? On a project I'm working on, the loss is getting reduced but at some point it won't converge to 0. Epoch 1 of 1000 100%|██████████| 506/506 [00:07 And it won't improve that much. So the network is learning, maybe just not that much as it should. Despite this, does it still make sense to sample new instances from the trained latent space?
