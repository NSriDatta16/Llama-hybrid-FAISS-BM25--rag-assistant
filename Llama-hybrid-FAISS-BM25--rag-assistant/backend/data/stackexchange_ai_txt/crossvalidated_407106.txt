[site]: crossvalidated
[post_id]: 407106
[parent_id]: 407085
[tags]: 
But my question is : if we maximize only the likelihood, we are back to a frequentist approach no ? i thought that the whole thing with Bayesian Methods was to consider both the likelihood and the prior You are correct, if we only maximize likelihood , then it's maximum likelihood , not Bayesian approach. In Bayesian setting we maximize posterior (i.e. prior times the likelihood), or even estimate the distribution rather then finding point estimate. On another hand, if you choose uniform prior $p(\theta) \propto 1$ , then $p(X|\theta) \times 1$ is the same as maximizing likelihood alone. Notice that in section 5.4.1 they say In this section we first apply the general Bayesian inference principles from section 5.2 to the specific Gaussian process model, in the simplified form where hy- perparameters are optimized over. We derive the expressions for the marginal likelihood and interpret these. So they describe maximizing the likelihood alone as a simplified example. As about your second question, I must say that I don't fully understand what you mean. We have many MCMC algorithms that let us sample form posterior distribution given that we specify the model in terms of likelihood and priors, only some of them need you to take the derivatives. But in the case of a Monte Carlo, how to we modify the weights parameters ? We don't modify anything. Likelihood and prior are enough to specify the posterior distribution (recall Bayes theorem). Given them, we use MCMC to take samples from the posterior distribution. Those samples enable us to characterize the posterior distribution even if it is not analytically tracable (otherwise we wouldn't need the samples, we would just derive the posterior analytically). Even if you are interested only in the mode of the posterior distribution, i.e. maximum a posteriori estimate, and if you want to optimize such functions that are not differentiable, there are many optimization algorithms that don't need derivatives .
