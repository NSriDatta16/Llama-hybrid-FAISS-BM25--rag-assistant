[site]: crossvalidated
[post_id]: 202755
[parent_id]: 
[tags]: 
Proper way of doing predictions in Bayesian linear Model

I'm working on a Bayesian linear model with student scores as response variable and a set of predictors. My model is: $Y_{i} = \beta*X + \epsilon_i$ $i = 1,...,n $ $\beta$ ~ $N(0,\Sigma)$ $\epsilon_i$ ~ $N(0,\sigma^2)$ $\sigma^2$ ~ $inv-gamma(a,b)$ Having a training set and a test set I was wondering which was the best way to make predictions and assess my results. I considered the following: Consider the posterior mean of $\beta$, say $\beta_m$ and calculate $E[Y]$ = $X_{test}*\beta_m$ , completely ignoring the variance (this is done in R through blinregexpected in LearnBayes package). Sampling many times from the predictive distribution $L(Y_{new}|Y_{train})$ and see if the real value $Y_{test_i}$ falls in a 90% Credible interval, if so I guessed it right. (Done in R through blinregpred) Your suggestions. I actually did the above obtaining very similar results. Thank you for your help!
