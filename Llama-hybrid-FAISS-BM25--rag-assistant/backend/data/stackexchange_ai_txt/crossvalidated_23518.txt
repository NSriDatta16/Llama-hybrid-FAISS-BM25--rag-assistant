[site]: crossvalidated
[post_id]: 23518
[parent_id]: 23490
[tags]: 
Having used Naive Bayesian Classifiers extensively in segmentation classification tools, my experience is consistent with published papers showing NBC to be comparable in accuracy to linear discriminant and CART/CHAID when all of the predictor variables are available. (By accuracy both "hit rate" in predicting the correct solution as the most likely one, as well as calibration, meaning a, say, 75% membership estimate is right in 70%-80% of cases.) My two cents is that NBC works so well because: Inter-correlation among predictor variables is not as strong as one might think (mutual information scores of 0.05 to 0.15 are typical) NBC can handle discrete polytomous variables well, not requiring us to crudely dichotomize them or treat ordinal variables as cardinal. NBC uses all the variables simultaneously whereas CART/CHAID use just a few And that's when all the variables are observed. What makes NBC really pull away from the pack is that it gracefully degrades when one or more predictor variables are missing or not observed. CART/CHAID and linear discriminant analysis stop flat in that case.
