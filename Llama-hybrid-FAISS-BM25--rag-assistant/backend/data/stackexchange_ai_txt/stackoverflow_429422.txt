[site]: stackoverflow
[post_id]: 429422
[parent_id]: 429385
[tags]: 
FWIW, this has nothing to do with database normalization . This is a data cleanup task. Data cleanup cannot be fully automated in the general case. Many people try, but it's impossible to detect all the ways that the input data might be malformed. You can automate some percentage of the cases with techniques such as: Force users to select company names from a list instead of typing them. Of course this is best for single entries, not for bulk uploads. Compare the SOUNDEX of the input company names to the SOUNDEX of company names already in the database. This is useful for identifying possible matches, but it can also give false positives. So you need a human to review them. Ultimately, you need to design your software to make it easy for an administrator to "merge" entries (and update any references from other database tables) as they are discovered to be duplicates of one another. There's no elegant way to do this with cascading foreign keys, you just have to write a bunch of UPDATE statements.
