[site]: datascience
[post_id]: 118793
[parent_id]: 
[tags]: 
Which of 2 options is better practice for model optimization: 1) Nested CV wrongly averaging inner CV scores. 2) Two successive CVs on X_all. Altrntv?

Goal: Compare preprocessing methods, models, and hyperparameters without leaking into the final generalization estimate, applying cross-validation (cv), i.e. NOT applying any fixed train/test splits. Solutions found so far (and problems): "Nested cv": scores of different hyperparameters (from inner SearchCVs) are averaged in an outer cv and the best score is selected. If the meaning of cv is to average scores from ONE method on different splits to get an estimate of generalization performance, this option appears as simply wronlg applying cv or am I misunderstanding this? In addition, how can I include different preprocessing methods in the search, since preprocessing defines X (has to be done before splitting X)? Successive cvs on the same entire dataset X_all. First trying different preprocessing methods, comparable models and hyperparametersets saving always the average test score. Second, using only the methods of the best score, perform a cv again on X_all only for generalization performance. With this procedure, is it not easier for validation "test" information to leak into the generalization performance test? Or can this be mitigated/minimized somehow? How about a nested cv for each prepro-method loop, saving a "validation score" (result of of inner grid_search, i.e. .best_score_ with refit=True) in each fold of outer cv. In addition, saving each average test score of outer cv. Later taking the best of validation scores together with corresponding test score as estimate of generalization performance? This still contains the averaging of different hyperparam combis but perhaps its better than fully leaking into the test set. The only other option appears to be fixed splits instead of cv.
