[site]: datascience
[post_id]: 92540
[parent_id]: 92531
[tags]: 
In very simple words, Those models which use all the features of the instances will suffer from irrelevant features e.g. Neural Nets, KNN etc . While the models which have an internal strategy to compares the features to decide the best while training will not suffer( at least for this reason) e.g. Tree Here is a snap from " The Elements of Statistical Learning, by Trevor Hastie, Robert Tibshirani, Jerome Friedman You can also read this section 10.3 Effect of Irrelevant Features from " Feature Engineering and Selection: A Practical Approach for Predictive Models by Max Kuhn and Kjell Johnson " You can try these approaches to filters the features - Using correlation with the target Using Feature Importance Using L1 penalty Also, keep in mind that you might face other issues because of this and the models may suffer indirectly e.g. High dimensionality Issue of Trees " default feature importance approach" with high Cardinal features etc.
