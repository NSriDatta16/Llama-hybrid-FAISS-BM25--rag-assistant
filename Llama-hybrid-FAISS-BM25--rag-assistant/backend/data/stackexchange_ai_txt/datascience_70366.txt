[site]: datascience
[post_id]: 70366
[parent_id]: 
[tags]: 
Variable with extra small Pearson coefficient has bigger positive impact on ML model performance than variable with bigger Pearson

I made some machine learning models using Python sci-kit learn library and I found some strange situation for me regarding the real importance of some variables (features) to the ML model. I found that a variable that has a smaller Pearson coefficient has higher importance on the ML model (when excluding variable from the model using backward elimination principle) than variables which has higher Pearson. Below I send the real results of three models where the first model includes all three variables and another two models exclude some variables (- means variable is excluded). I use Random Forest method. Model Name MAE (Mean Absolute Error) $ Model_{V1V2V3}=0.92$ $ Model_{V1V2} = 3.86$ $ Model_{V1-V3}= 2.96 $ $ Pearson_{V1}=0.99 $ , $ Pearson_{V1}=0.82 $ , $ Pearson_{V3}=0.02$ When I exclude variable which has no importance based on Pearson (0.02) and I got to model with better performance compared to a model which includes another variable ( $V2$ ) which has far higher Pearson (0.82). Why? Please, help me to explain this situation. Is there any explanation in literature or similar case reported?
