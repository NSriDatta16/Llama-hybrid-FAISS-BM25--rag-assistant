[site]: crossvalidated
[post_id]: 337690
[parent_id]: 
[tags]: 
Variable selection in Hierarchical Linear Modelling HLM through nlme lme()

Background of my question:- In Linear Regression through R we can mention the direction="both"/"forward"/"backward" in step(lm()) function to tell R for choosing the best set of variables based on AIC. The output which we get is the final selection of a reduced number of attributes that best explains my dependent variable. My Question:- If I want to use HLM method through nlme package for a dataset with many attributes then is there any optimized method to choose best selection of variables? E.g code:- model_mydf This data comes from data("Gasoline",package = "plm"). In this case I have only 3 explanatory variables and it is less time consuming to iterate and see which is my best set of variables. Suppose I have larger attributes (or explanatory variables) then we need to keep on iterating lme() function to check the coefficients, p-value, Rsquare and MAPE. Because there is no option to mention direction="" argument similar to step(lm()). How can I reduce my iteration time if I want to adopt HLM method through lme()? Is there anything similar to step-wise/backward/forward in lme() function? With response to @Gregg H dated 31st March 2018: ctrl D_ML & D_REML are respective 2*LL for fixed effect and random effect models using ML and REML method. df_ML & df_REML are respective degrees of freedom (in this case both are 324). How do I do a chisquare test now using chisq.test()? How do I keep iterating for lincomep+lrpmg+lcarpcap one by one to get the best combination? In above case I have tested only intercept. How do I combine both ML and REML combination of variables in one final lme() function to get the best combination result? With response to @BenBolker dated 31st March 2018: ctrl
