[site]: datascience
[post_id]: 78116
[parent_id]: 77665
[tags]: 
First of all, I would like you to discourage you from using structured input in NMT. In most cases, the best thing you can do is just do some subword input and output segmentation and learn simple sequence-to-sequence conversion. You can certainly pass the parsed input in the format that you showed above and the worse thing you can expect the model will to ignore the mark-up. Some modest improvements from using syntax in this way are shown in this WTM19 paper from Edinburgh . People were more successful when using graph convolutional networks for processing the structured input, AFAI used for the first time in 2017 . In this case, you would need to add a custom encoder into FairSeq and specify a model with this encoder. There are several PyTorch implementations of graph convolutional networks that you can reuse in FairSeq, e.g., gcn-over-pruned-trees pygcn AllenNLP also has some tools for structure input. Note, however, that once you change model architecture, you need to be careful about the learning rate schedule that is otherwise tuned for the standard Transformers.
