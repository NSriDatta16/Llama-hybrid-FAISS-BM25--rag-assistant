[site]: crossvalidated
[post_id]: 415413
[parent_id]: 
[tags]: 
Word2vec Skip-Gram - Overfitting

i am currently training a skip-gram model on my own dataset. After each run i compare the cosine-similarity between all the vectors and get the following diagramm: So my model creates each run nearly the same similarities between the vectors and does not vary much. My loss function looks like this and around 10.000 steps it approches the value 2 asymptotically; i have no analogy dataset to get the accuracy of my current word embeddings and so far i don't know how to evaluate this model further. My question is, can you overfit a word2vec model and are there more options to get the accuracy of the model?
