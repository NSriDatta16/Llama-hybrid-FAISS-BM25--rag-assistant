[site]: crossvalidated
[post_id]: 631257
[parent_id]: 
[tags]: 
Hyperparameter selection after nested cross-validation and making comparisons with DeLong's test

I have already read all the associated questions on the topic but couldn't find a clear answer. I initially split my data into training (80%) and hold-out testing (20%). Then, I am performing nested cross-validation (outer loops: Leave one out approach) using all the data except the 20% that is hold-out. I repeat the nested CV process for four different learning algorithms (like XGBoost, Gradient Boosting etc.). Then, I pick the learning algorithm that performed the best by looking at which learning algorithm's nested CV gave the highest AUC. After I select the nested cross-validation that is the highest performing, I want to now train one model with this 80% of data, and then test this on the 20% hold out. Is it fine if I do something like pick the parameters that were found with the highest frequency across the folds? Is that okay? I also have multiple after steps with comparison using DeLong's test, and I do not want to complicate things further. Or is it unacceptable to do something like take the most frequently suggested best hyperparameters by looking at the folds, and then use those for training the final algorithm? My second question is: I need single models to compare between them statistically using DeLong's test, am I right? From what I read, I cannot make statistical comparisons using two nested cross-validations so a single model (per approach) would need to be trained. This question is kind of independent from the first question (I am not referring to comparing between learning algorithms). I would really appreciate your input, thank you
