[site]: crossvalidated
[post_id]: 484600
[parent_id]: 484592
[tags]: 
It is not true that big models always outperform smaller, simpler ones. There are examples where logistic regression outperforms deep neural networks, same there are examples where LSTM outperforms more complicated language models , or even where much simpler models work well for NLP tasks. Your data may differ from the data that was using for training BERT, or other pretrained models. Say that the pretrained model was trained on Reddit corpus, while you want to classify legal, or technical documents, or theoretical physics journal papers, they would greatly differ in language on all levels, starting from words, their length, grammar etc. Tuning pretrained models in such cases may, but does not have to help. Finally, in some cases you don't need complicated models. Many problems can and are solved with very simple models, like random forest, logistic regression, or even rule based systems. It can be the case that it is enough to classify text just by checking if they contain particular keywords (say "covid" for documents related to COVID-19 illness).
