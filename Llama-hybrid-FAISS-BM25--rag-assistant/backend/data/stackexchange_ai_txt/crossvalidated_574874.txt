[site]: crossvalidated
[post_id]: 574874
[parent_id]: 574871
[tags]: 
Both questions are actually answered by the same line in the paper, from §3.2.2 “Multi-Head Attention”. …we found it beneficial to linearly project the queries, keys and values $h$ times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv -dimensional output values. That’s your “three separate matrix multiplications”. There’s no “subsequent” mapping going on, because it wouldn’t help. The composition of two linear mappings (the product of two matrices) is another linear mapping, so it wouldn’t increase the expressive power of the model. You could instead just replace those two parameter matrices with their product. Bringing this back to your main question: the matrix multiplications directly give you the transformation into what you call the “multi-head-representations” (the inputs to $h$ softmax functions).
