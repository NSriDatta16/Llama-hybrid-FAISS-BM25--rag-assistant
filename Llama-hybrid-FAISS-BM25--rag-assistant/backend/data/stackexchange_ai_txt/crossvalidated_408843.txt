[site]: crossvalidated
[post_id]: 408843
[parent_id]: 
[tags]: 
Partial Collinearity in Regression

I had a doubt about the effect of multi-colinearity in regression analysis. I understand if two variables are co-related we cannot disentangle the effects of one from the other on the target variable thus the coefficient we give them is not optimal. However, what happens in the cause of 0.8 co-linearity or 0.6 or such? There are situations within the dataset where the model has the information to disentangle each's effects. A human would look at those causes to first to do so. The places where they are uncorrelated (i.e opposite directions from their own means) should convey the true relationship, right? For high co-linearity like 0.9+ I was thinking the cases where they are uncorrelated would seem like noise to the model for eg) If I always saw two people together and didn't see them once, I thought this is an outlier or not representative. I have done linear regression in machine learning where we use gradient descent but just not learning the more theoretical aspects of it. If we were fitting a flexible non-linear function to the case of 0.8 co-linearity, I guess it would be able to model it correctly....
