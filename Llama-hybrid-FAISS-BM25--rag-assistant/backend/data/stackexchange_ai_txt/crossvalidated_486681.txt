[site]: crossvalidated
[post_id]: 486681
[parent_id]: 486672
[tags]: 
A linear regression is a statistical procedure that can be interpreted from both perspectives. Instead I will tackle the question of comparing linear regression (and its assumptions) to other methods. A linear regression takes the form $$ Y_i = X_i'\beta + \varepsilon_i$$ Texbooks usually ask you to check (i) Exogeneity $\mathbb{E}[\varepsilon_i \mid X_i] = 0$ , (ii) Non-colinearity: $\mathbb{E}[X_iX_i']$ is invertible and (iii) homoskedasticity, $\mathbb{E}[\varepsilon_i \mid X_i] = \sigma^2$ . Only (i) and (ii) are considered identifying assumptions, and (iii) can be replaced by much weaker assumptions. Normality of residuals sometimes appears in introductory texts, but has been shown to be unnecessary to understand the large sample behavior. Why do we need it? $$ \widehat{\beta} = \beta + {\underbrace{\left(\frac{X'X}{n}\right)}_{\to^p \mathbb{E}[X_iX_i']}}^{-1} \ \underbrace{\left(\frac{X'\varepsilon_i}{n}\right)}_{\to^p \mathbb{E}[X_i\varepsilon_i']}$$ Condition (i) makes the second term zero, (ii) makes sure that the matrix is invertible, (iii) or some version of it guarantees the validity of the weak law of large numbers. Similar ideas are used to compute standard errors. The estimated prediction is $X_i'\widehat{\beta}$ which converges to $X_i'\beta$ . A typical machine learning (ML) algorithm attempts a more complicated functional form $$ Y_i = g(X_i) + \varepsilon_i $$ The ``regression'' function is defined as $g(x) = \mathbb{E}[Y_i \mid X_i = x]$ . By construction $$\mathbb{E}[\varepsilon_i \mid X_i] = \mathbb{E}[Y_i - g(X_i) \mid X_i] = 0$$ Assumption (i) is automatically satisfied if the ML method is sufficiently flexible to describe the data. Assumption (ii) is still needed, with some caveats. Non-collinearity is a special case of a regularization condition. It says that your model can't be too complex relative to the sample size or include redundant information. ML methods also have that issue, but typically adjust it via a "tuning parameter". The problem is there, just that some state-of-the-art ML method push the complexity to squeeze more information from the data. Versions of (iii) are still technically there for convergence, but are usually easy to satisfy in both linear regressions and ML models. It is also worth noting that some problems in experimental analyses involve latent variables (partially unobserved $X_i$ ). This sometimes changes the interpretation of the exogeneity condition in both linear regression and ML models. Off-the-shelf ML just makes the most out of observed data, but state-of-the-art research adapts ML for causal models with latent variables as well. *PS: In the linear regression $\mathbb{E}[X_i\varepsilon_i] = 0$ can replace (i).
