[site]: crossvalidated
[post_id]: 315057
[parent_id]: 315053
[tags]: 
It looks like you are making the mistake of confusing data with the random variables. Let a random variable $X$ have mean $\mu$. Then if $(x_1, \ldots x_n)$ are realizations from $X$, $\frac{1}{n} \sum x_i = \hat{\mu}$ is an unbiased estimator of $\mu$. This is because $\mathbb{E}[\mu - \hat{\mu}] = 0$, a fact that we will use later. In fact if the data is drawn IID from $X$ then each individual sample is an unbiased estimator of the mean; the reason we take many samples is to lower the variance of our estimate. Now if, as you mentioned, we have multiple random variables, $X_1, \ldots, X_p$, each of which has mean $\mu$, then $\mathbb{E}\left[\frac{1}{p}\sum_{i=1}^p X_i \right] = \mu$ as you suggest. Furthermore, $\mathbb{E}X_1 = \mu$. That is, $\mathbb{E}[\mu - \mathbb{E}X_1]=0$. This implies that the observed realization of $X_1$ is an unbiased estimator of $\mu$. There is nothing startling here but it is distinct, I think, from the question you meant to be asking which was answered in the previous paragraph. This distinction is important and worth understanding well.
