[site]: crossvalidated
[post_id]: 90755
[parent_id]: 90750
[tags]: 
You may not know "the" definition because there are two definitions. I see you've tagged "confidence interval", but oft neglected Bayesians also have "credible intervals" which also have associated coverage probability. For frequentists, the coverage probability is defined using frequentist probability definitions. The coverage probability for an interval estimate is the proportion of instances in which the sample statistic obtained from infinite independent and identical replications of the experiment is contained. If one interval estimate has a higher coverage probability than another interval estimate, that's not telling you much. $(-\infty, \infty)$ is by that reasoning the best interval estimate allowable. You want coverage probability equal to the nominal confidence of the interval estimate. Most interval estimates are calculated using the sample standard error of the parameter estimate (either exact or asymptotic, as derived using limit theorems and normal approximations). In that case, unbiased and efficient standard error estimates provide correct levels for confidence intervals (i.e. non-coverage probabilities equivalent to nominal alpha level). Alternately, intervals derived from inverted tests of hypotheses have 1-coverage probability equal to the size of the test (not the level ) e.g. as with Fisher's Exact Test. Coverage probabilities can often be compared to the nominal confidence of the interval. Low coverage probabilities mean: biased estimates or anticonservative standard error estimates or both. High coverage probabilities indicate overly conservative standard error estimates (again, like the Fisher's Exact Test). I like comparing the performance of estimators using coverage probabilities. I'm reminded of the David Cox quote, paraphrased, "The provision of unbiased estimates is rarely useful in its own right." Some very interesting biased interval estimators (such as those for Bayes' estimators) achieve better coverage probability than their unbiased counterparts (usually UMVUE type estimators). Credible intervals (or regions) on the other hand are estimates of quantiles (or other definitions) of the posterior density of the parameter. A Bayesian credible interval is a degree of belief that the parameter lies in that portion of its posterior density. (based on our estimate of the posterior density using data) One interval estimate for a Bayesian statistic having higher coverage probability could mean one of a few things: prior specification or likelihood model, only the latter being analogous to the frequentist case. Prior specification is a whole other can of worms. As Lindley said, "With probability 0 that the moon is made of green cheese, even astronauts carrying arm loads of the stuff couldn't convince." So, making assumptions about the support can compromise the interpretation and validity of the posterior (making credible intervals more consistent with the posterior, but less consistent with the data).
