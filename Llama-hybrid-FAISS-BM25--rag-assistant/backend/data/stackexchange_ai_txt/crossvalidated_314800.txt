[site]: crossvalidated
[post_id]: 314800
[parent_id]: 314766
[tags]: 
Actually, a term is missing in your (two) models: the error term. Let's play with the first one: $y_i = ax_i^b + u_i$ for $i=1,...,n$ The (distribution-specific) likelihood function used in the Bayesian Information Criterion (BIC) is that of $\boldsymbol{u}$ (of the residual vector $\boldsymbol{y} - \widehat{a}\boldsymbol{x}^\widehat{b}=\widehat{\boldsymbol{u}}$ in practice). This means that you first need to assume a probability density function for $\boldsymbol{u}$, say, $f(\boldsymbol{u},(a,b))$. Then you can compute -- not to say maximize -- the corresponding log-likelihood function, $\ln(\widehat{L})=\sum_{i=1}^n\ln(f(\widehat{u_i},(\widehat{a},\widehat{b})))$. Finally, $\text{BIC} = k \ln(n) -2 \ln(\widehat{L})$. In the (first model) case you describe, $k=2$ and the sensible part is about specifying the probability density function. With your normally distributed residuals, one has $f(\widehat{u_i},(\widehat{a},\widehat{b})) = (\widehat{\sigma}^22\pi)^{-.5} e^{-.5\widehat{u_i}/\widehat{\sigma}}$
