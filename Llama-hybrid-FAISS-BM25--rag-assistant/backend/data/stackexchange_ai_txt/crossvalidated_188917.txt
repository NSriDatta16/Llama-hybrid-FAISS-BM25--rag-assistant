[site]: crossvalidated
[post_id]: 188917
[parent_id]: 188677
[tags]: 
The goal of the question is to show that in the (Binary) Bayesian Hypothesis Testing framework, one can write the expected risk $\varphi(f) = \mathbb{E}_{H,y}[C(f(y), H)]$ as follow: $$ \varphi(f) = \alpha P_F - \beta P_D + \gamma$$ were $\alpha, \beta, \gamma $ are constants that depend on the cost function ($C_{i,j} = C(f(y) = H_i, H = H_j)$) chosen and the prior probabilities on the hypothesis ($P_m$'s). So lets write down what $ \varphi(f)$ means and do some algebra: $$ \varphi(f) = \mathbb{E}_{H,y}[C(f(y), H)] = \sum_{H_j,y} C(f(y), H_j) P(y,H_j)$$ $$ \varphi(f) = \sum_{H_j} \sum_{H_i} \sum_{y : f(y) = H_i} C(f(y) = H_i, H_j) P(y \mid H_j) P(H_j)$$ $$ \varphi(f) =\sum_{H_j,H_i} C(H_i, H_j) P(f(y) = H_i \mid H_j) P(H_j)$$ for the binary case we have (were $P_i = P(H_i)$, $C_{ij} = C(f(y) = H_i, H_j)$, $P(H_i \mid H_j) = P(f(y) = H_i \mid H = H_j)$): $$ \varphi(f) = C_{00} P(H_0 \mid H_0)P_0 + C_{11} P(H_1 \mid H_1)P_1 + C_{10} P(H_1 \mid H_0)P_0 + C_{01} P(H_0 \mid H_1)P_1$$ $$ \varphi(f) = C_{00} P(H_0 \mid H_0)P_0 + C_{11} P_DP_1 + C_{10} P_F P_0 + C_{01} P(H_0 \mid H_1)P_1$$ Now notice that we want to express the expected risk in terms of the detection probability and false alarm probability only . For this we need to find expression for $P(H_0 \mid H_0)$ and $P(H_0 \mid H_1)$ in terms of $P_F$ and $P_D$. Its easy to see that the equations that we need are: $$P(H_0 \mid H_0) = 1 - P_F$$ and $$ P(H_0 \mid H_1)$ = 1 - P_D$$ So the expected risk $ \varphi(f)$ becomes: $$ \varphi(f) = C_{00} (1 - P_F) P_0 + C_{11} P_D P_1 + C_{10} P_F P_0 + C_{01} (1 - P_D) P_1$$ $$ \varphi(f) = (C_{10} - C_{00}) P_0 P_F - (C_{01} - C_{11})P_1 P_D + C_{00}P_0 + C_{01} P_1$$ Thus: $$\alpha = (C_{10} - C_{00}) P_0 $$ $$\beta = (C_{01} - C_{11})P_1 $$ $$\gamma = C_{00}P_0 + C_{01} P_1 $$ as required $ \varphi(f) = \alpha P_F - \beta P_D + \gamma$. To finish with some interesting remarks, why would we want to express the expected risk like that? Well, the interesting thing is that if one plots an operating characteristic (OC) for some hypothesis test, one can see how a choice of priors determines which point $(P_D, P_F)$ on the OC we will have. So a choice of priors and costs intrinsically chooses a point $(P_D, P_F)$. Also, notice that there is a constant $\gamma$ that is irreducible (i.e. independent of which decision rule $f$ we choose). Thus, no matter what we do (unless we choose costs and priors that are exactly zero as to make $\gamma$ is zero), there might be some irreducible expected error that we will have. Lastly, if we are able to show there is an intrinsic trade-off between large $P_D$ and small $P_F$, then this equations express this trade off for the bayesian hypothesis testing case. To intuitively see this notice that if one can show that $P_F$ and $P_D$ are positively related (which one can show $\frac{d P_D}{d P_F} \geq 0$) then if $P_D$ is increases (so to decrease expected/bayes risk) then $P_F$ increases too. Thus, its unclear how increasing $P_D$ would necessarily lead to a decrease in the average risk.
