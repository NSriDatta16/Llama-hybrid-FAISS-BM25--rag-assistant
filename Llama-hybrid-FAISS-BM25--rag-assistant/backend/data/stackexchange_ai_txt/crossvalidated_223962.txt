[site]: crossvalidated
[post_id]: 223962
[parent_id]: 
[tags]: 
Combining ordinal and categorical (one-hot encoded) variables in one model

I have a dataset with combination of ordinal and categorical (strictly discrete) variables. I want to predict another discrete variable (can be probably restricted to binary, but better, ordinal in general). Lets not be bounded to some specific model, lets say some canonical from scikit-learn , like tree-based methods or logistic regression. I encoded ordinal variables to a natural scale, so they are just integers (with relatively low max size 50, but usually much less, like 0-9). Categorical variables are one-hot encoded (in pandas something like pd.get_dummies ). I am wondering if I can use this mixed-type dataset as an input in a model. I would say that it matters on algorithm used. E.g. logistic regression could be able to handle this mixed dataset well - there will be either 0 or 1 before $\alpha_i$ -ith coefficient in case of categorical, and natural number in front of $\alpha_j$ -jth in case of ordinal feature. In case of decision trees it basically doesn't matter (they just could have less splits on ordinals). The reason why I don't want to encode ordinal variables is losing some information (and increasing dimension). I am not asking if the model performs better - that is of course better to try and evaluate. I am just asking if there is some standard approach to this and if my reasoning is correct or somehow fundamentally incorrect.
