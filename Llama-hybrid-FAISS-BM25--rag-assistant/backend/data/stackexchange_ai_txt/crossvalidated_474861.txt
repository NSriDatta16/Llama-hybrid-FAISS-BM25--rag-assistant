[site]: crossvalidated
[post_id]: 474861
[parent_id]: 465476
[tags]: 
There are many ways to think extensions of MAB that may model what you describe. A first level of generalisation would be to consider a Contextual MAB problem, where at the beginning of each round you observe a context $x_t$ , you choose an action $a_t$ and observes a reward $r_t = r(x_t,a_t)$ that depends on both your context and action. This models usually assume that the context are drawn iid from a fixed distribution (Stochastic MAB) or are chosen in advance by an adversary (adversarial CMAB) A more general setting is the RL setting, modelled by a Markov Decision Process (MDP). Here you also have a (stochastic) state evolution, i.e. you have a state transition probability, that detemines the next state based on your current state and the action that you take. This last setting (MDP) is able to perfectly describe the scenario that you mention: "5000 \$ after ten trials and the scenario when I have lost 5000 \$ after ten trials".
