[site]: crossvalidated
[post_id]: 479122
[parent_id]: 
[tags]: 
Understanding The Visual Representation Of A Neural Network?

Could someone provide just a quick overview on what I am seeing when I see a typical example of a neural network and how it relates to the matrix math behind it? We normally see a typical graph like the below: Now, according to the author, in matrix math, this is represented in the below, where Ws are the coefficients to whatever activation function is in a node of hidden layer 1 and X are the features or predictor values: When it comes to estimating the parameters of each hidden layer, this representation makes sense. We have a bunch of data, and we feed them all into the model and produce predictions. If we have 5 observations, we have 5 predictions in the picture. Thus, the number of inputs equals the number of outputs. What I don't really understand is once the model is built, how does the graphical representation work? Let's say you have a single new predictor value and you feed it into the NN below. How does NN know which coefficient is assigned to this single predictor value? What happens to the other coefficients? Would the vector of Xs values be equal to [X1, 0, 0, 0, 0]? Thanks.
