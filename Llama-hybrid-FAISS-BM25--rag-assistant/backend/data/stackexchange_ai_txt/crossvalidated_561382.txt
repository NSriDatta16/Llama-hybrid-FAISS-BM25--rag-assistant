[site]: crossvalidated
[post_id]: 561382
[parent_id]: 561379
[tags]: 
Five reasons come to mind quickly. Square loss brutally punishes bad misses. If you miss by $1$ , your square loss is $1$ , but if you miss by $2$ , your square loss is $4$ . This helps keep a model from making gigantic errors. Square loss is related to the variance of an error term, if you’re willing to assume that variance to be constant. Minimizing square loss seeks out the conditional expected value. Minimizing square loss corresponds to maximum likelihood estimation if the conditional distribution is Gaussian. Maximum likelihood estimation is a technique with which statisticians are quite comfortable. Tradition! The first type of machine learning one learns is “trendline” in Excel. Then one learns how to extend that by learning simple and then multiple linear regression via ordinary least squares, which minimizes square loss. That being said, many other loss functions are popular. “Crossentropy” is popular for classification problems, for instance. The neural network libraries like Keras list many popular loss functions that are built-in.
