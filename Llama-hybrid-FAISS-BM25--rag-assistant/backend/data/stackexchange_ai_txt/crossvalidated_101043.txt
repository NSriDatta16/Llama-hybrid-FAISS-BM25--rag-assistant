[site]: crossvalidated
[post_id]: 101043
[parent_id]: 101038
[tags]: 
As Nick Stauner underlines, the model here is futile. 12 cases, 10 predictors and a constant imply what they imply. You have an analogue of two distinct points in the plane which allow a perfect straight line summary. Here is a reproducible example for different data in Stata which makes the point again. I use Gaussian noise predictors to make it clear that a model need not have substantive virtues for this to happen. . sysuse auto, clear (1978 Automobile Data) . set seed 2803 . forval j = 1/10 { 2. gen x`j' = rnormal() 3. } . . logit foreign x* in 46/57 Iteration 0: log likelihood = -8.1503192 Iteration 1: log likelihood = 0 Iteration 2: log likelihood = 0 Logistic regression Number of obs = 12 LR chi2(-1) = 16.30 Prob > chi2 = . Log likelihood = 0 Pseudo R2 = 1.0000 ------------------------------------------------------------------------------ foreign | Coef. Std. Err. z P>|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- x1 | 64.69274 . . . . . x2 | 36.59078 . . . . . x3 | 4.771344 . . . . . x4 | -73.23572 . . . . . x5 | -7.960005 . . . . . x6 | 28.43037 . . . . . x7 | 27.70408 . . . . . x8 | -36.96143 . . . . . x9 | 10.73363 . . . . . x10 | 8.389412 . . . . . _cons | -20.56464 . . . . . ------------------------------------------------------------------------------ Note: 7 failures and 5 successes completely determined.
