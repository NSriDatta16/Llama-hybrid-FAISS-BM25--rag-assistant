[site]: datascience
[post_id]: 53285
[parent_id]: 
[tags]: 
Using neural networks derivatives in a loss function

How can I get the gradient of a node in the NN with respect to another one? I need to train a NN, which for the sake of simplicity has 2 neurons as input (x, y), a neuron as a bottleneck (z), and 2 more as output (x_hat, y_hat). I need a loss which looks something like this: loss = (x-x_hat)**2+(y-y_hat)**2+abs(sqrt((d(x_hat)/dz)**2+(d(y_hat)/dz)**2)-1) or a variation of it. The thing is that I need the value of $d(x_{hat})\over dz$ and $d(y_{hat})\over dz$ (so the gradient of the output layer with respect to the inner node). I tried several things using hooks but what I found so far didn’t work. I was thinking to do something like this: loss_1 = (x-x_hat)**2+(y-y_hat)**2 loss_1.backward(retain_graph=True) loss_2 = abs(sqrt((d(x_hat)/dz)**2+(d(y_hat)/dz)**2)-1) loss_2.backward() After I call loss_1.backward(retain_graph=True) , the gradients that I need should be already calculated, I just don’t know how to access them in order to pass them to loss_2 i.e. what is the value of $d(x_{hat})\over dz$ and $d(y_{hat})\over dz$ Can anyone help me with this? Thank you!
