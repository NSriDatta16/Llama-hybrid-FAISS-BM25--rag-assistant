[site]: crossvalidated
[post_id]: 508895
[parent_id]: 508887
[tags]: 
No, Bayesian analysis works perfectly fine with more categories than two. As long as you can write down a likelihood for the model you have in mind (e.g. softmax regression for mutually exclusive unordered categories, ordinal logistic regression for ordered categories with constant odds between categories etc.), a lot of standard Bayesian approaches will work in a relatively straightforward fashion. In fact, you will usually not have to write down the likelihood yourself, but would rather rely on some existing implementation that matches what you want to do (for some options, see below). Of course, if your numerical scores are fine-grained enough, it could also be a reasonable approximation to treat this as a continuous data problem, when Bayesian approaches are also perfectly applicable (e.g. a Bayesian linear regression). In some rare situations, you get nice convenient conjugate prior distribution that you can easily update. One well known case is the Binary/Binomal case, for which a $$\text{Beta}(a,b)$$ prior gets updated to $$\text{Beta}(a+y, b+(n-y))$$ after observing $y$ successes out of $n$ tries). Or for simple unordered categories, where with e.g. 4 categories $$\text{Dirichlet}(a,b,c,d)$$ gets updated to $$\text{Dirichlet}(a+y_1,b+y_2,c+y_3,d+y_4)$$ after seeing $n$ tries coming out as $y_i$ times category $i$ ). Once you assume something more complicated than that (e.g. there are covariates or the categories are ordered), you'll probably end up using some MCMC sampler such as Stan (can be used e.g. via the R rstan library, the Python pystan package and various other tools/programming languages have their own interfaces) or one of the nice simplified interfaces for it such as - if you are using R - rstanarm , brms or stan_polr . E.g. for ordinal logistic regression stan_polr has the stan_polr function .
