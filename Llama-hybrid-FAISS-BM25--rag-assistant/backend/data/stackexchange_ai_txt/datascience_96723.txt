[site]: datascience
[post_id]: 96723
[parent_id]: 96722
[tags]: 
The Keras attention layer is a Luong attention block of type dot-product. Optionally, you can specify the layer to have a learnable scaling factor, with use_scale=True . A single-head attention block from the Transformer model is also a dot-product, but scaled to the fixed dimension of the embedding ( $\frac{1}{\sqrt{d_k}}$ ). Therefore, their only difference is the scaling factor, which is learned in the case of the Keras attention layer (if enabled), while it is fixed in the case of the Transformer attention.
