[site]: datascience
[post_id]: 16963
[parent_id]: 16961
[tags]: 
In reinforcement learning, you should avoid scoring interim results based on heuristics. Unlike supervised learning, or a search algorithm, you are not trying to guide the behaviour, just reward good results. For an inverted pendulum a good result might simply be "has not fallen over so far", although there is nothing inherently wrong with a cost function which expresses cost in terms of minimising differences from an ideal, you do have to take more care with the values used. Assuming you are using discounting, and continuous (not episodic) approach, then reward can be 0 for "not falling over" and -1 for "it fell over", followed by a re-set/continue. You can check for falling by measuring whether the pendulum has reached some large angle to the vertical (e.g. 45 degrees or more). For an episodic approach, it is more natural to have +1 "ok" and 0 for the end state "fell over", although the 0/-1 scheme would also work. However, you want to avoid having negative values for any state which is "ok", because that is basically telling the agent to hurry up and end the episode. In your case, ending the episode is bad, so you don't want that. If you do want to reward "perfection" in your episodic approach, then your formula might work better if you added a positive offset, so that the agent has an incentive to continue the episode if possible. You should choose a value such that recoverable states are positive. Note that the above analysis applies only to certain episode-based approaches. It depends critically on what you count as an episode, and whether the agent is able to take an action which ends the episode.
