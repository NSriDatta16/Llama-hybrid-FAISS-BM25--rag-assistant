[site]: crossvalidated
[post_id]: 628497
[parent_id]: 
[tags]: 
How best to estimate regression parameters subject to constraints?

The setting Consider a least squares model for $y$ as a function of $x,$ possibly nonlinear in the parameters. Abstractly this can be expressed as $$y = f(x;\theta) + \varepsilon$$ with the usual assumptions: the $\varepsilon$ are iid zero-mean errors of finite variance $\sigma^2.$ Further suppose this model represents a well-established mathematical or physical law that constrains the "reasonable" values of the parameter $\theta.$ As a concrete example, consider a standard model in radiological measurements such as a (simplified) Lorentzian peak at $x=0$ added to a constant background: $$f(x,(\alpha,\beta,\gamma)) = \frac{\alpha}{\beta + x^2} + \gamma.$$ All three parameters $\theta = (\alpha,\beta,\gamma)$ are of interest and should be estimated as accurately as possible. However, the only physically realistic values of the parameters are non-negative (and $\beta$ is nonzero). Here is an illustration. Generally, suppose there is a known nontrivial set $\Theta$ of realistic values of $\theta.$ I will refer to $\Theta$ as the constraints. A fundamental question is whether the peak is present at all. That can be tested with the null hypothesis $H_0:\alpha=0,$ a situation that often holds in reality (maybe the peak is produced by a contaminant that usually is not present, for instance). When fitting such a model, it is possible for any of the parameter estimates $\hat\theta$ to lie outside the constraints. (That is, $\hat\theta\notin \Theta.$ ) For instance, $\hat\alpha$ might be negative when (a) the true value $\alpha$ is much smaller than the error standard deviation $\sigma$ and (b) due to random (bad) luck, it looks like the peak is inverted. This happens frequently (about half the time) when $\alpha$ truly is zero. The question One has two options in these circumstances: Use whatever estimates $\hat\theta$ the least squares procedure provides, even if they lie outside the constraints. Force the least squares solution to conform to the constraints. There are cogent arguments for and against both options: (1) can yield unrealistic estimates, but when applied to a large number of datasets might provide better average estimates on the whole; whereas (2) is forced to yield realistic estimates, but that can introduce a bias. For instance, forcing $\alpha$ to be nonnegative will produce positive nonzero estimates from time to time even when $\alpha$ is truly zero. In general, we're considering the possibility $H_0: \theta\in\partial\Theta$ that the parameter lies on the boundary of the constraints and wish to test the alternative $H_A:\theta\in \Theta \setminus \partial\Theta$ (the interior of the constraint space). Is one of these options preferable? If so, which one and why? Is one of these options so problematic that it should not be used? If so, which one and why? I also invite suggestions for additional options (such as regularization, penalized least squares, and so on), provided they can be shown superior to both (1) and (2). I welcome answers from varying perspectives (classical, Bayes, etc. ) and have enough bounty points to reward multiple good answers ;-).
