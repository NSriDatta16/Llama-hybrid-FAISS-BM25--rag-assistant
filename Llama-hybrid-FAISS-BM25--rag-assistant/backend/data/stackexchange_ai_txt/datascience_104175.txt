[site]: datascience
[post_id]: 104175
[parent_id]: 
[tags]: 
Neural network predicting on similar inputs with many features

I am training a deep RL agent (DQN) on states with 333 components that usually are very similar between themselves. The actions predicted by the agent, which is nothing but the max operator applied to the output of a neural network are constant. I think that there could be some sort of saturation on the neural network. I am using 4 layers with 1028,512,512 and 128 neurons respectively and relu activation. Is there any way to modify the states coherently to make them more dissimilar?
