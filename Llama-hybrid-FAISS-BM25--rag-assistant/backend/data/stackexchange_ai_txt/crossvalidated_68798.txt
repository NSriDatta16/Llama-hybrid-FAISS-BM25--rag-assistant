[site]: crossvalidated
[post_id]: 68798
[parent_id]: 68793
[tags]: 
The suggestions that @gung made are certainly worth following up. Having done the coursera course, I think your list is a good start. Some comments: linear algebra and matrix algebra are the same thing, so drop the latter. in calculus be sure to include partial differentiation. This is calculus applied to functions of more than one variable (symbolically, if, say, $z$ is a function of $x$ and $y$ then you want $\frac{\partial z}{\partial x}$ rather than $\frac{\rm{d}z}{\rm{d}x}$ ). Fortunately this isn't difficult. in calculus you don't need anything beyond basic integration (and maybe not even that). This is fortunate because integration is hard. add basic optimization, i.e. finding the maximum or minimum of a function, typically a function of more than one variable. An appreciation of gradient descent at the very least is essential. in terms of difficulty you probably want to be somewhere between the beginning and end of 1st year undergraduate. try to read some basic probability and statistics texts, online or otherwise, but don't worry too much (basic maths is a prerequisite anyway to understanding probability and statistics). If you do some courses such as the one you suggest you'll figure out what you need to learn and where your interests lie. One thing you don't want to do, at least at first, is spend a lot of time learning about hypothesis testing. You would rather want to steer towards understanding basic statistics — random variables, probability distributions (PFDs, CDFs), descriptive statistics — and then try to understand regression. I'd add the book Mathematics for Machine Learning by Marc Peter Deisenroth , published 2020, looks like an excellent foundation, including the above and more.
