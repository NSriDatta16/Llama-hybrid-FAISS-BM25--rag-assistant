[site]: crossvalidated
[post_id]: 345438
[parent_id]: 345053
[tags]: 
In my research going down the rabbit hole of ways to structure/train RNNs I found that it gets difficult to say if one way works better than another because results could be problem specific. I tried these two methods and they seem to converge to about the same minimum, but the first one gets there faster since it simply takes less operations on back prop. Here is a question I had where I came to similar conclusion - that multiple approaches can get the same result, but one might be faster than another. Truncated Back Propagation of LSTM with K2 > K1 On a side note, if you use the first approach it will be important that if your first training pair is x, y = ['a', 'b', 'c'], ['d'] then your next pair needs to be x, y = ['b', 'c', 'd'], ['e'] rather than x, y = ['d', 'e', 'f'], ['g'] to make sure every target value is used. The 2nd method might not be bothered too much by this, but for the first one it is important.
