[site]: crossvalidated
[post_id]: 74962
[parent_id]: 74939
[tags]: 
Instead of using p-values to assess claims we should follow Robert Abelson's advice and use the MAGIC criteria: Magnitude Articulation Generality Interestingness Credibility For more on Abelson see my review of his book And we should be concentrating on effect sizes, not p-values in statistical output (with the possible exception of some sorts of data mining, on which I am not expert at all). And effect sizes are to be judged in context: 1 in 1000 pairs of pants gets the wrong size label - not a big deal 1 in 1000 airplanes are defective in a way that leads to crashes - a big deal 1 in 1000 nuclear reactors is defective in a way that leads to meltdown - uh oh A statistician/data analyst should not be some odd person, used like a black box into which data is put and out from which p values are gotten; he/she should be a collaborator in research designed to make a reasonable argument about the meaning of some set of data in the context of some field, given the current theories (or their lack) and current evidence (or lack of same). Unfortunately, this approach requires thought on the part of the substantive researchers, the data analyst and whoever reviews the results (be it a pointy haired boss, a dissertation committee, a journal editor or whoever). Oddly, even academics seem averse to this sort of thought. For more on my views, here is an article I wrote that got published in Sciences360.
