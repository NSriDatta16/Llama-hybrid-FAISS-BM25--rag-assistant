[site]: datascience
[post_id]: 128128
[parent_id]: 
[tags]: 
Can an OCR model consistently recognize every digit of a long number correctly?

I'm working on OCR on scanned documents and we need to recognize the exact sequence of some printed numbers on it. Imagine you're reading a bank cheque serial number (16 digits) so the system needs to recognize every single digit correctly. After working on this for a few weeks it's starting to dawn on me that even theoretically this task might be hopeless, let alone in practice. I mean even if you have a system that has %95 accuracy for every digit and assuming the prediction for every digit is independent from the others (which I think is reasonable for printed texts and causal models) then the probability of recognizing every digit correctly is 0.95 raised to the power of 16, which is almost %44. so you'd be lucky if the model predicts every digit of some random number correctly. even if the probability of recognizing a digit is 0.99 then the probability of recognizing the whole number becomes 0.85 which is not bad I guess but are we really expected to achieve 0.99 digit accuracy? Is my analysis correct or am I making a mistake somewhere? also, what kind of probability analysis can we do for a non-causal model, like a CRNN model that uses bi-directional LSTMs? T
