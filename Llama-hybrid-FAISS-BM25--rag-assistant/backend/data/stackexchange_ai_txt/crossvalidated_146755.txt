[site]: crossvalidated
[post_id]: 146755
[parent_id]: 
[tags]: 
Is an SVM's (maximum) likelihood uniquely defined as a function of hyperparameters?

I think that I must be reading this paragraph (below) incorrectly. Note that both types of evidence that we have defined in general depend on the inverse noise level $C$ and the kernel $K(x, x^\prime )$ separately. This is in contrast to the conventional SVM solution: The latter is found by maximizing the log-posterior (13), and the position of this maximum clearly only depends on the product $CK(x, x^\prime )$. This is an important point: It implies that properties of the conventional SVM alone—generalization error bounds, test error or cross-validation error, for example—can never be used to assign an unambiguous value to $C$. Since $C$ determines the class probabilities (10), this also means that well-determined class probabilities for SVM predictions cannot be obtained in this way. The intuition behind this observation is simple: If $C$ is varied while $CK(x, x^\prime)$ is kept fixed (which means changing the amplitude of the kernel in inverse proportion to $C$), then the position of the maximum of the posterior $P(\theta | D)$, i.e., the conventional SVM solution, remains unchanged. The shape of the posterior, on the other hand, does vary in a nontrivial way, being more peaked around the maximum for larger $C$; the evidence is sensitive to these changes in shape and so depends on $C$. (While the Dr. Sollich's discussion is in the context of Bayesian SVM methods, I'd like to set aside the Bayesian perspective for the moment and just focus on what the author is saying about conventional SVM methods.) The paper is Sollich, Peter. 2002. Machine Learning. V 46, 1-3. " Bayesian Methods for Support Vector Machines: Evidence and Predictive Class Probabilities " pp 21-52. My interpretation of the paragraph is that the conventional SVM performance surface of the hyper-parameters for a given data set is the same along a hyperbola defined by $\text{constant}=CK(x,x^\prime),$ but because the data are fixed, $K(x,x^\prime)$ only varies through $\gamma$, we have $$\text{constant}=C\gamma$$ In the example of a radial basis function kernel with width parameter $\gamma$, this in turn implies that we don't need to search over a grid of $C\times\gamma$, but can instead fix one parameter and search over the remaining parameter. Clearly this would dramatically speed up any grid search over hyperparameters. (But yes, I am aware that grid search is sub-optimal ). Illustrated, under my interpretation, each of the hyperbolae below has the same likelihood value along the line (but some hyperbolae might have the same value because the hyperparameter surface is nonconvex ). Fixing $\frac{1}{C}=\lambda$ at, say, 4, means that we can still find a maximum likelihood on one of the many hyperbolae corresponding to alternative values of $\gamma$, and that this likelihood will be no worse than any of the other likelihoods on the same hyperbola. Is my interpretation correct?
