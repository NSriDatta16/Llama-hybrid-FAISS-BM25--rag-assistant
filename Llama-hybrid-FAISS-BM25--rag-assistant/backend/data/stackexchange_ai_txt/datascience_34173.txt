[site]: datascience
[post_id]: 34173
[parent_id]: 34171
[tags]: 
Do we calculate the average of all dw (derivative of the loss function wrt the weights) or do we sum all of'em then multiply by the learning rate and substract them from the initial weights or what ? Does not matter if you do either. The formulae for weight update is: new_weights = initial_weights - learning_rate * average(gradient_over_all_samples) . Although it is the thumb rule to calculate average but it will not matter since calculating average is just multiplying it by a scalar 1/n which can be factored out and multiplied with the learning rate. Thus, instead of explicitly calculating average just include it in the learning_rate will save you time, computation and code. Only difference between SGD and batch learning that you are doing is that the update is made after the entire dataset is traversed, whereas in SGD update is made after single example is traversed.
