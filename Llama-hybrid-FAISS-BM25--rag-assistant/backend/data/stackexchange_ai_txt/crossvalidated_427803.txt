[site]: crossvalidated
[post_id]: 427803
[parent_id]: 
[tags]: 
Why does Machine-Learning-Augmented Logistic Regression perform better?

I have a need to build binary classifiers. For institutional reasons I am constrained to using only logistic models. The continuous and integer independent variables are typically binned in a manner that produces monotonic Weights of Evidence vs the binary target. A typical dataset will be segmented into 5-15 groups, with each group being modeled by its own LR equation. It turns out that when the binary target is replaced with a fractional probability prediction from an XGBoost model, that the resulting LR equations have meaningfully better out-of-sample performance, than if they'd been estimated on the true binary outcome, holding equation specification the same. Can anyone explain why this result might be the case? For reference, the logistic models are typically estimated on 15-100k observations, with 6-10 independent variables. The improvement over baseline LR performance is loosely associated with sample size, smaller datasets get the biggest boost, but all models are improved.
