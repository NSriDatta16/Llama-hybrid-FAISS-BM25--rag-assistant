[site]: crossvalidated
[post_id]: 256744
[parent_id]: 256609
[tags]: 
In general, no. In the specific applications that I have in mind, the number of data points to forecast is often not more than a handful. Trying to discriminate between different models or tuning parameters on the basis of a few data points only will likely work very badly, so the validation period that I use is typically much larger than the one for "actual" forecasting. In your specific application, things may of course look a bit different, but there is at least no strong a priori argument why the validation and forecast period lengths should be the same. An interesting related question is how to choose the split between training and validation sample. The soundest way to perform that split in a time series context is to compute the pseudo-out-of-sample performance of your methods for a range of split points and choose the one that performs best overall, see this paper and that paper .
