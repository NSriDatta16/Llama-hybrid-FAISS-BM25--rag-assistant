[site]: crossvalidated
[post_id]: 400123
[parent_id]: 218542
[tags]: 
Softmax outputs produce a vector that is non-negative and sums to 1. It's useful when you have mutually exclusive categories ("these images only contain cats or dogs, not both"). You can use softmax if you have $2,3,4,5,...$ mutually exclusive labels. Using $2,3,4,...$ sigmoid outputs produce a vector where each element is a probability. It's useful when you have categories that are not mutually exclusive ("these images can contain cats, dogs, or both cats and dogs together, or neither cats nor dogs"). You use as many sigmoid neurons as you have categories, and your labels should not be mutually exclusive. A cute trick is that you can also use a single sigmoid unit if you have a mutually-exclusive binary problem; because a single sigmoid unit can be used to estimate $p(y=1)$ , the Kolmogorov axioms imply that when $y$ is binary, we have $1-p(y=1)=p(y=0)$ . Using the identity function as an output can be helpful when your outputs are unbounded. For example, some company's profit or loss for a quarter could be unbounded on either side. ReLU units or similar variants can be helpful when the output is bounded above (or below, if you reverse the sign). If the output is only restricted to be non-negative, it would make sense to use a ReLU activation as the output function. Likewise, if the outputs are somehow constrained to lie in $[-1,1]$ , tanh could make sense. The nice thing about neural networks is that they're incredibly flexible tools, and flexibility in output activation is one aspect of that flexibility.
