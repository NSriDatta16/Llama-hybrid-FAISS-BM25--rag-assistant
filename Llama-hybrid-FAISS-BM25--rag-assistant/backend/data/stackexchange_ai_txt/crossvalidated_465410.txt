[site]: crossvalidated
[post_id]: 465410
[parent_id]: 
[tags]: 
Why is using a mixture of logstic distributions makes sense in pixelcnn++?

I went trough the paper and code of the pixelcnn++ model. From what I understand, they train the network in the following way for predicting the value of a single pixel: the inputs are the pixel values of the pixels before the pixel we are trying to predict the distribution for. (this happens in the same time for all pixels, from what I understand, but lets stick to one of them). The network does what is does, and the outputs are the parameters of a mixture of n logistic distribution: pies for the probability of each distribution to be picked, mu for the mean of the distribution and s for the scale of it. During training, the network is maximizing the likelihood of getting the correct pixel value under the distribution with the parameters its predicting. Now, here is what looks weird to me. Wouldn't the distribution with the highest probability to produce the correct value is the one where all pies except one are 0 (so only one distribution can be picked), and for that distribution the mean is the pixel value and scale is zero, so only the right value can be picked? and if so, whats the point of predicting a distribution? In VAE, we use KL loss to get the scale of the normal distribution to be close to 1, so it cant collapse to 0 - but here I don't see anything like this. Obviously, I don't understand at least one thing about the model - can someone clear this up for me?
