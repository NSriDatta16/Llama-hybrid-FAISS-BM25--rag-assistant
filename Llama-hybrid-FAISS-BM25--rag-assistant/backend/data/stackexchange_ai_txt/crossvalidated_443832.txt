[site]: crossvalidated
[post_id]: 443832
[parent_id]: 443027
[tags]: 
If you're comparing your models to some observed reference, then it's a problem of measuring similarities of the two matrices. A matrix that assigns a scalar value to every single point in some area is, in other words, a really, really simple vector fields. What do you know, calculating differences between vector fields has been something centuries of physicists and engineers had dealt with on a regular basis. The simplest similarity metric here would be just the sum - not the mean - of errors, absolute or squared (because otherwise you're just measuring systematic bias of each method without accounting for random errors) across every single entry of the array. Fairly obviously, the model with the smallest total sum of mispredictions predicts the best. Since every item of your array is spatially connected to the adjacent ones, you may - and seem to - want to go easy on models which simply fat-fingered which coordinate to put some temperature in, in which case you can zoom out a bit and smudge the adjacent entries together by averaging the values in a 3x3 kernel centered on your current entries in both matrices and compare these. Same principle. You could use a weighted combination of the two, and/or you could go to higher levels, but beyond a certain point you'd be basically building a convolutional NN just to compare models, which would be quite silly.
