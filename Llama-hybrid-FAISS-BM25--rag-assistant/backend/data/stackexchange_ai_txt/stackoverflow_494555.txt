[site]: stackoverflow
[post_id]: 494555
[parent_id]: 494502
[tags]: 
My guess is as follows. Assume that each set has a name or ID or address (a 4-byte number will do if there are only 2 billion of them). Now walk through all the sets once, and create the following output files: A file which contains the IDs of all the sets which contain '1' A file which contains the IDs of all the sets which contain '2' A file which contains the IDs of all the sets which contain '3' ... etc ... If there are 16 entries per set, then on average each of these 2^16 files will contain the IDs of 2^20 sets; with each ID being 4 bytes, this would require 2^38 bytes (256 GB) of storage. You'll do the above once, before you process requests. When you receive requests, use these files as follows: Look at a couple of numbers in the request Open up a couple of the corresponding index files Get the list of all sets which exist in both these files (there's only a million IDs in each file, so this should't be difficult) See which of these few sets satisfy the remainder of the request My guess is that if you do the above, creating the indexes will be (very) slow and handling requests will be (very) quick.
