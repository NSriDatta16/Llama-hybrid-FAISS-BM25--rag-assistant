[site]: crossvalidated
[post_id]: 505631
[parent_id]: 503154
[tags]: 
To give an answer to my own question, I think there are probably many approaches in practice, and this is part of the "model" in question. For example, I recently trained a Faster R-CNN Feature Pyramid Network on a dataset. During evaluation I set the min confidence value to be $0.6$ , and the NMS for detected boxes to be $0.7$ with a max detection of $100$ boxes, and this yielded a decent mean average precision on the test set (better than a looser 0.5 confidence threshold). Really there are two scenarios: You have access to all the test data set and can tweak the parameters to get the best outcome (which some people might argue is "cheating" since you are using test performance to guide model parameters, but I'm sure a lot of people do this anyway...) You don't have access to the some/all test data a priori, maybe a model used in production. In this case you have to come up with a strategy and hope it works, where the default from what I can tell seems to be just setting a confidence threshold and a max number of detections.
