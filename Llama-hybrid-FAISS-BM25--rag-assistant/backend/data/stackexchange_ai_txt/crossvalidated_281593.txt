[site]: crossvalidated
[post_id]: 281593
[parent_id]: 16381
[tags]: 
Different assumptions can be used to justify OLS In some situations, an author tests the residuals for normality. But in other situations, the residuals aren't normal and the author uses OLS anyway! You'll see texts saying that homoscedasticity is an assumption. But you see researchers using OLS when homoscedasticity is violated. What gives?! An answer is that somewhat different sets of assumptions can be used to justify the use of ordinary least squares (OLS) estimation. OLS is a tool like a hammer: you can use a hammer on nails but you can also use it on pegs, to break apart ice, etc... Two broad categories of assumptions are those that apply to small samples and those that rely on large samples so that the central limit theorem can be applied. 1. Small sample assumptions Small sample assumptions as discussed in Hayashi (2000) are: Linearity Strict exogeneity No multicollinearity Spherical errors (homoscedasticity) Under (1)-(4), the Gauss-Markov theorem applies, and the ordinary least squares estimator is the best linear unbiased estimator. Normality of error terms Further assuming normal error terms allows hypothesis testing . If the error terms are conditionally normal, the distribution of the OLS estimator is also conditionally normal. Another noteworthy point is that with normality, the OLS estimator is also the maximum likelihood estimator . 2. Large sample assumptions These assumptions can be modified/relaxed if we have a large enough sample so that we can lean on the law of large numbers (for consistency of the OLS estimator) and the central limit theorem (so that the sampling distribution of the OLS estimator converges to the normal distribution and we can do hypothesis testing, talk about p-values etc...). Hayashi is a macroeconomics guy and his large sample assumptions are formulated with the time series context in mind: linearity ergodic stationarity predetermined regressors: error-terms are orthogonal to their contemporaneous error terms. $\operatorname{E}[\mathbf{x}\mathbf{x}']$ is full rank $\mathbf{x}_i \epsilon_i$ is a martingale difference sequence with finite second moments. Finite 4th moments of regressors You may encounter stronger versions of these assumptions, for example, that error terms are independent. Proper large sample assumptions get you to a sampling distribution of the OLS estimator that is asymptotically normal. References Hayashi, Fumio, 2000, Econometrics
