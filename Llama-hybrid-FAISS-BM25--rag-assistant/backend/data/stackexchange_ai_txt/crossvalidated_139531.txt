[site]: crossvalidated
[post_id]: 139531
[parent_id]: 139290
[tags]: 
I see this approach as an attempt to address the inability of social psychology to replicate many previously published 'significant findings.' Its disadvantages are: that it doesn't address many of the factors leading to spurious effects. E.g., A) People can still peek at their data and stop running their studies when an effect size strikes them as being sufficiently large to be of interest. B) Large effects sizes will still appear to have large power in retrospective assessments of power. C) People will still fish for interesting and big effects (testing a bunch of hypotheses in an experiment and then reporting the one that popped up) or D) pretend that an unexpected weird effect was expected all along. Shouldn't efforts be made to address these issues first? As a field going forwards it will make a review of past findings pretty awful. There is no way to quantitatively assess the believability of different studies. If every journal implemented this approach, you'll have a bunch of social scientists saying there is evidence for X when it is totally unclear how believable X is and scientists arguing about how to interpret a published effect or arguing about whether it is important or worth talking about. Isn't this the point of having stats? To provide a consistent way to assess numbers. In my opinion, this new approach would cause a mess if it was widely implemented. This change does not encourage researchers to submit the results of studies with small effect sizes so it doesn't really address the file-drawer effect (or are they going to publish findings with large n's regardless of effect size?). If we published all results of carefully designed studies, then even though the believability of results of individual studies may be uncertain, meta-analyses and reviews of studies that supplied statistical analysis would do a much better job at identifying the truth.
