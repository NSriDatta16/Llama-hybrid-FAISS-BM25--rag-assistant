[site]: crossvalidated
[post_id]: 103790
[parent_id]: 103631
[tags]: 
There is a rich history of literature about dealing with large kernels. "Random features for large-scale kernel machines" by Rahimi and Recht is an important milestone. They embed the input in a lower dimension in a randomized fashion to achieve scalability. This work has spawned further interesting works in dealing with different types of kernels etc. The Nystrom method based approach is another way to deal with large kernels. Again, this approach has been addressed by several works. Recently, Divide-and-conquer kernel SVM has addressed this problem in distributed setting. Check out: www.cs.utexas.edu/~cjhsieh/dcsvm/
