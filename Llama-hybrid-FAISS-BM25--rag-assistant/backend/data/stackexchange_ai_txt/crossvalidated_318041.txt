[site]: crossvalidated
[post_id]: 318041
[parent_id]: 177391
[tags]: 
They are not to be taken in the same context; points 1 and 2 have different contexts. For both AIC and BIC one first explores which combination of parameters in which number yield the best indices (Some authors have epileptic fits when I use the word index in this context. Ignore them, or look up index in the dictionary.) In point 2, AIC is the richer model, where richer means selecting models with more parameters, only sometimes, because frequently the optimum AIC model is the same number of parameters model as BIC the selection. That is, if AIC and BIC select models having the SAME number of parameters then the claim is that AIC will be better for prediction than BIC. However, the opposite could occur if BIC maxes out with a fewer parameters model selected (but no guarantees). Sober (2002) concluded that AIC measures predictive accuracy while BIC measures goodness of fit, where predictive accuracy can mean predicting y outside of the extreme value range of x. When outside, frequently a less optimal AIC having weakly predictive parameters dropped will better predict extrapolated values than an optimal AIC index from more parameters in its selected model. I note in passing that AIC and ML do not obviate the need for extrapolation error testing, which is a separate test for models. This can be done by withholding extreme values from the "training" set and computing the error between the extrapolated "post-training" model and the withheld data. Now BIC is supposedly a lesser error predictor of y-values within the extreme values of range of x . Improved goodness of fit often comes at the price of bias of the regression (for extrapolation), wherein the error is reduced by introducing that bias. This will, for example, often flatten the slope to split the sign of the average left verses right $f(x)-y$ residuals (think of more negative residuals on one side and more positive residuals on the other) thereby reducing total error. So in this case we are asking for the best y value given an x value, and for AIC we are more closely asking for a best functional relationship between x and y. One difference between these is, for example, that BIC, other parameter choices being equal, will have a better correlation coefficient between model and data, and AIC will have better extrapolation error measured as y-value error for a given extrapolated x-value. Point 3 is a sometimes statement under some conditions when the data are very noisy (large $σ$); when the true absolute values of the left-out parameters (in our example $β_2$) are small; when the predictors are highly correlated; and when the sample size is small or the range of left-out variables is small. In practice, a correct form of an equation does not mean that fitting with it will yield the correct parameter values because of noise, and the more noise the merrier. The same thing happens with R$^2$ versus adjusted R$^2$ and high collinearity. That is, sometimes when a parameter is added adjusted R$^2$ degrades while R$^2$ improves. I would hasten to point out that these statements are optimistic. Typically, models are wrong, and often a better model will enforce a norm that cannot be used with AIC or BIC, or the wrong residual structure is assumed for their application, and alternative measures are needed. In my work, this is always the case.
