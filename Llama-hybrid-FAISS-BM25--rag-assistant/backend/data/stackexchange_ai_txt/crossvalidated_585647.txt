[site]: crossvalidated
[post_id]: 585647
[parent_id]: 
[tags]: 
F2 score or the Area under the Precision-Recall-Curve as a scoring metric

I have a dataset with which I want to perform binary classification. The distribution of the target class is imbalanced: 20% positive labels, 80% negative labels. The positive class is more important than the negative class, false negatives (FN) are more costly than false positives (FP), and I need to give out probabilities for all predictions. The image is taken from MachineLearningMastery.com . Should I use the F2 score or the Area under the Precision-Recall-Curve (PR AUC) as a scoring metric? There are multiple ways to calculate the PR AUC with scikit-learn. average_precision = average_precision_score(y_test, y_score) precision, recall, thresholds = precision_recall_curve(y_test, y_score) auc_precision_recall = auc(recall, precision) Which way is more exact or common for calculating the PR AUC?
