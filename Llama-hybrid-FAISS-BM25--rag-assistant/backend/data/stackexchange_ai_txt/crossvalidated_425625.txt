[site]: crossvalidated
[post_id]: 425625
[parent_id]: 
[tags]: 
What is the reason behind the weight updates in Evolution Strategies?

OpenAI introduced Evolution Strategies as an alternative to reinforcement learning technique without backpropagation. A sample code from their website, # simple example: minimize a quadratic around some solution point import numpy as np solution = np.array([0.5, 0.1, -0.3]) def f(w): return -np.sum((w - solution)**2) npop = 50 # population size sigma = 0.1 # noise standard deviation alpha = 0.001 # learning rate w = np.random.randn(3) # initial guess for i in range(300): N = np.random.randn(npop, 3) R = np.zeros(npop) for j in range(npop): w_try = w + sigma*N[j] R[j] = f(w_try) A = (R - np.mean(R)) / np.std(R) w = w + alpha/(npop*sigma) * np.dot(N.T, A) I know how a genetic algorithm works but in Evolution Strategies they use some way to nudge the weights to a solution. In backpropagation we use gradients to update weights but in ES I really dont understand the reason behind weight updates. The problem is in these lines w_try = w + sigma*N[j] R[j] = f(w_try) A = (R - np.mean(R)) / np.std(R) w = w + alpha/(npop*sigma) * np.dot(N.T, A) I assume f is a fitness function and R the list of fitness values corresponding to the probable solutions. But I dont understand the reason behind w + sigma*N[j] , (R - np.mean(R)) / np.std(R) and most importantly w + alpha/(npop*sigma) * np.dot(N.T, A) . What is the reason behind nudging weights using w + alpha/(npop*sigma) * np.dot(N.T, A) ?
