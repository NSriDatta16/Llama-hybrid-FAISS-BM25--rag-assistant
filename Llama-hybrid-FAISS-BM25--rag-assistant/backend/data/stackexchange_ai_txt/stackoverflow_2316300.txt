[site]: stackoverflow
[post_id]: 2316300
[parent_id]: 2256878
[tags]: 
Decompressing 1.6MB to 7MB in over 4 minutes is dreadfully slow. Some items that come to mind: Have you tried changing the size of your working buffer? 1KB is very conservative IMHO. You may be running into a performance issue with the GZip decompression algorithm (but that's merely a WAG). Are there temporary files being stored, and if so, where? Are they in memory or on a storage device? And if on a storage device, which one as they may have different performance characteristics. You could improve your performance by double-buffering. This works by creating two buffers and while one is being filled from a read the other is being flushed by the write (ping-pong effect, requires asynchronous code). It will only be as fast as the slowest operation (the decompressed stream read) but you may be able to all but eliminate the accumulated write delay. We originally started with SharpLibZip but switched to the commercial Xceed ZIP.NET library to take advantage of the batching capabilities (and the support for both .NET and .NET Compact Frameworks). The Xceed library also supports GZip. Even on our archaic 400 MHz Windows CE 4.2 devices it takes us only about 20-30 seconds to decompress a 6 MB file (albeit in a ZIP using XCeed ZIP.NET). Also, if I'm not mistaken, I believe .NET Compact Framework has support for GZip. On a side note, place your FileStreams in using statements to ensure they are closed in the event an exception occurs in the middle of your method.
