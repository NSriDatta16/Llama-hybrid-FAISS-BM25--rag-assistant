[site]: datascience
[post_id]: 112837
[parent_id]: 112833
[tags]: 
The figure and the blog post are simply incorrect. Doing a reverse image search, I see that the image you posted comes from a blog post on Towards Data Science. That image is so wrong. Just think that in a causal language model, the prediction for the word next would be in the time step that receives as input the word very ! Also, causal language models do not use any tokens. You are correct in your understanding : at training time, the loss of all time steps is computed at once. This can be done because: The models attend only to the previous tokens. RNNs do this by construction. Transformers do this because of the mask imposed on the decoder. We are using the true previous tokens as input to the prediction instead of using the model's own predictions as input. This is referred to as "teacher forcing". ALL causal language modeling implementations compute the training loss in a single pass.
