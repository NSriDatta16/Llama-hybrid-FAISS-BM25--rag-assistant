[site]: datascience
[post_id]: 60850
[parent_id]: 
[tags]: 
sklearn.metrics.average_precision_score getting different answers for same data but different formats

I was trying to learn how average precision (AP) is calculated and implemented in scikit-learn. I have read the documentation , but I don't think I fully understand it yet. Consider the following two snippet: import numpy as np from sklearn.metrics import average_precision_score y_true = np.array([0, 1, 0]) y_scores = np.array([0.4, 0.4, 0.8]) average_precision_score(y_true, y_scores) # 0.3333333333333333 and y_true = np.array([[1,0], [0,1],[1,0]]) y_scores = np.array([[0.6,0.4], [0.6,0.4], [0.2,0.8]]) average_precision_score(y_true, y_scores) # 0.45833333333333326 From what I understand, these are the same data but formatted in different ways. The first is only showing the true labels and predicted scores for the positive class, while the second is giving information on both classes. But why are they giving different answers? In particular, how are these two results calculated? And which one is correct? I was reading this post , but I didn't understand how that precision-recall table in the answer is constructed. Can anyone can go through a similar calculation for my example?
