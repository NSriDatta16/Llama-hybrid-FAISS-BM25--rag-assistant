[site]: datascience
[post_id]: 57772
[parent_id]: 57766
[tags]: 
As you want to handle very complex relationship between the inputs, model should be strong enough. It seems that neural network would perform better than svm for example. The problem is the number of points in dataset. However, it just means, that you should try to develop appropriate architecture for your task. There are many challenges in wich nn shows good results on small datasets (with a few hundreds of instances). So, I suggest you experiment with architecture. There are two links with examples how to build baseline model https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/ (dataset for regression problem) https://janakiev.com/notebooks/keras-iris/ (iris is a small but easy dataset for classification) This step is rather easy. Then you can try to change number of layers and neurons, add dropout to prevent overfitting on small dataset. Although it would demand efforts, results could be high. Other good model, as for me, is gradient boosting. It includes ensemble of trees (GB usually based on trees) and performs good results on small datasets https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html But this model should also be tuned for your problem, regularization techniques will help to reduce overfitting on a small number of examples. The main message is: there's not universal algorithm, which works great from scratch with small data and high complexity, but there is a range of tools to make powerful algorithms perform good on small observations (regularization, dropout, depth and number of trees in GB, learning rate).
