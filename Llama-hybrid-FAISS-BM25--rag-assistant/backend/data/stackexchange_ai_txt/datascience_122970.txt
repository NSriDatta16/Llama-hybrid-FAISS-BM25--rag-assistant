[site]: datascience
[post_id]: 122970
[parent_id]: 
[tags]: 
ML model to predict CPU utilization of a server given x amount of tasks

I have comprehensive data points of what the CPU utilization of a server is when x amount of jobs are running, let's say the server is using 40% CPU util time=x and there are 4 jobs running. The tricky part I am trying to figure out is that, the few time intervals before and after time=x can have an impact on the CPU utilization. In the end all I want to figure out is how much CPU will be used when x number of jobs are running. I have tried multiple regression and rnn models with no success, I think the tricky part is that the time before and after can have a big effect, for example util can be 90% when there are 0 jobs, because right before it there was 20 jobs, etc.
