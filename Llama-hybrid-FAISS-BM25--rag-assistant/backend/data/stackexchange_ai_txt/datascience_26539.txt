[site]: datascience
[post_id]: 26539
[parent_id]: 
[tags]: 
Backpropagation with multiple different activation functions

How does back-propagation handle multiple different activation functions? For example in a neural network of 3 hidden layers, each with a separate activation function such as tanh, sigmoid and ReLU, the derivatives of each of these functions would be different, so do we just compute the output error of each of these layers with the derivative of the activation function of that layer? Also will the gradient of the cost function used to compute the output error of the last layer change at all, or is the activation function of that layer used to compute the gradient of the cost function?
