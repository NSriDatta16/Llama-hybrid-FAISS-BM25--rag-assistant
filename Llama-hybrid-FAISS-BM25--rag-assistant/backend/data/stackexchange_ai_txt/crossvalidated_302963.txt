[site]: crossvalidated
[post_id]: 302963
[parent_id]: 302909
[tags]: 
It is of course always a possibility that your classes are unseparable based on your data and it might be possible that they are separable but only with much more data. In the meantime, there are still things you can try. Naive Bayes makes strong assumptions about your variables and is incapable of detecting if some variables are more useful for prediction than others. For example, current state could be a much better indicator of next state than your other variables, but naive bayes would not detect or use that. If you tried a simple decision tree, it would tell you if some class is much more useful than others. With ten classes and few records, you probably need to tune the pruning severity to see where it gets you the best results. Also, decide your performance metric upfront for principled reasons and don't look at 10 different ones and go shopping for the one that supports your narrative best, that's p-hacking. With ten classes, if you don't have misclassification cost imbalance, just take accuracy. If you need to look at precision and recall and all classes have the same importance on the class level, not record level, take macro averaged F-measure for multiple classes. That means that less prevalent classes are as important as more prevalent classes, making records in less prevalent classes more important. If you have specific weighing for the classes in mind, you can also make it a weighed macro averaged F-measure.
