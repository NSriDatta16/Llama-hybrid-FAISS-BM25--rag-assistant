[site]: crossvalidated
[post_id]: 305558
[parent_id]: 
[tags]: 
Intepreting, understanding, and visualizing the output of Random Forests

I am solving a binary classification problem with a random forest that has a rather large number of trees, e.g., 100. I use a standard voting mechanism for classification, i.e., for any input item x the score f(x) in [0,1] computed by the algorithm is the number of trees that classify the item as class 1, divided by the total number of trees. For example, if an item gets the score 0.83, 83 trees in the random forest classify the item as 1 (and the remaining 17 classify it as 0). The input feature space is not extremely large, but non-trivial also (you can assume about 100 features). The output of this classification has to be consumed to make business decisions by people that have no knowledge of machine learning, but can understand basic statistics. I am interested in explaining the output of the random forest algorithm to give users an insight on why it is outputting a certain score for an item. For example, what could I use to show the reason why random forests think an item scored 83? I'd like to keep this (in general) technology-independent. Here's a number of things I thought: Of course, showing all the trees for a given classified item is unpractical. They are way too many, and also generated at random, which means it is hard to understand and combine all they insight they give singularly. I could show a bar chart with the tree's feature importance (using TreeIntepreter ), which highlights what features in the whole model are more strongly correlated to a clear-cut classification into class 0 or 1. This gives little insight on the single prediction of an item, which is ultimately what is important for my users. The best shot I have so far is to use the feature contribution for each prediction (always from TreeInterpreter), and highlight say the 5 features with the largest positive contribution (pushing the score to high values, i.e., into classifying the item as class 1), and the 5 features with the largest negative contribution (pushing the score to low values, i.e., into classifying the item as class 0). In this way, one could say that an item x as been scored high by the random forest model because the 5 features with the highest contribution tend to have values in items that are of class 1 that are similar to the values of item x. What else could I be doing?
