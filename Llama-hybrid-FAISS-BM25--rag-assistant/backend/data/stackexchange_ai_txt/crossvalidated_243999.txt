[site]: crossvalidated
[post_id]: 243999
[parent_id]: 243973
[tags]: 
Using MATLAB, I fit a few distributions to the data using MLE. Basing fit on the Bayesian Information Criterion (BIC) , the better fits I could find were: $\text{Gamma}(\alpha=42.0827,\beta=0.4229)$, $\text{BIC}=96,829$ $\text{GEV}(k=-0.0614,\sigma=2.3768,\mu=16.5714)$, $\text{BIC}=96,137$ Obviously, you could fit other distributions to compare fits. Essentially, MLE is one way to guage the fit of a particular distribution to the data you have. You can compare the fits of the different distributions using Aikaike Information Criterion (AIC) or BIC (above). AIC penalizes fits by the number of fitted parameters whereas BIC's penalty is also related to the size of the sample. The smaller the BIC (or AIC), the better. For example, I'll provide the output from a normal distribution fit, which from the above histogram, is very unlikely to be the underlying distribution. This will highlight a poor fit, and the BIC that goes along with it. $N(\mu=17.798,\sigma=2.8195)$, $\text{BIC}=98,238$ Other common parametric fits: $\text{Weibull}(A=19.022,B=5.8626)$, $\text{BIC}=102,356$ $LN(\mu=2.8672,\sigma=0.1531)$, $\text{BIC}=96,399$
