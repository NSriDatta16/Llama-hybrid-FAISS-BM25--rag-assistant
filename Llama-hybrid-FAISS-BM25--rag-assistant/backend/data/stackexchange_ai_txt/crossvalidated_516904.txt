[site]: crossvalidated
[post_id]: 516904
[parent_id]: 411167
[tags]: 
Some thoughts on your interesting question: Good features are problem dependent. So seems difficult (if possible) to incorporate feature engineering into any rigorous mathematical framework. To me, pushing the problem of finding good features to finding good kernels is a way to separate the problem in two parts. First, the mathematics (learning from good/noisy features), and second, the practical nature of your data (finding features from your observations - sometimes using an arbitrary off-the-shelf mapping, but sometimes using your knowledge and understanding of how the data was generated). It is indeed very nice that neural networks can find their own feature mappings. However this is an empirical observation, it is not the case that we mathematically understand how or why these features should be good. You may call SLT "severely incomplete as a theory of inference", but our understanding of why neural networks should work is even more incomplete. Finally, as you stated, the state of the art results are achieved by using various different architectures for neural networks. While some neural networks achieve stellar performance, others perform poorly on the same datasets. So, to me, the situation for SVM ad neural networks is similar: your performance will be strongly influenced by the choice of kernel or the network. However if you go for SVM, then at least we can mathematically understand something - the "learning" step ;-)
