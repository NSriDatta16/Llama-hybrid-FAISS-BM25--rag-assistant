[site]: crossvalidated
[post_id]: 403060
[parent_id]: 
[tags]: 
Cox-Stuart vs Augmented Dickey Fuller

Given a time series, suppose I wish to determine if trend is present. I understand I can use the Cox-Stuart test ( https://www.r-bloggers.com/trend-analysis-with-the-cox-stuart-test-in-r/ ), but I have also read that the Augmented Dickey Fuller test can be done for this ( https://machinelearningmastery.com/time-series-data-stationary-python/ ). More specifically, the last link says: "The Augmented Dickey-Fuller test is a type of statistical test called a unit root test.The intuition behind a unit root test is that it determines how strongly a time series is defined by a trend." But I have also read from ( https://kourentzes.com/forecasting/2017/03/30/can-you-spot-a-trend-in-a-time-series/ ): "I do not think it is an exaggeration to suggest that even experts often can be confused on the exact definition (and effect) of a unit root and a trend." So my questions are thus: What does it mean for a time series to have a unit root? Does that automatically mean there is a trend in the time series? What is the difference between the Cox-Stuart and Augmented Dickey Fuller Tests? Which should I definitely use for trend detection?
