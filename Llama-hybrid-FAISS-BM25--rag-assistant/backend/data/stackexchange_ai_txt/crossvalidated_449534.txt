[site]: crossvalidated
[post_id]: 449534
[parent_id]: 449410
[tags]: 
I don't know about a score definition that "rules them all," but consider the notion shown in here https://www.ncbi.nlm.nih.gov/pubmed/28715259 . There, the first PC score is defined as a linear combination (score) that maximizes the sum of $R^2$ values when regressing each of the original variables on the score. As such, the awkward and not-easily understood "variance maximization" and "unit length constraint" concepts can be discarded completely. The first two scores are any two linear combinations that similarly maximize the sum of $R^2$ s in the multiple regression model using the two scores as predictors; thus, not only are unit length and variance maximization concepts unnecessary, but so is orthogonality. This approach could be modified to the nonlinear case, see https://users.soe.ucsc.edu/~draper/eBay-Google-2013-breiman-friedman-1985.pdf for nonlinear transformations (scores) that maximize the $R^2$ in a regression with a single dependent variable. I would imagine that such an approach could also be used to maximize the sum of $R^2$ values as in linear principal components analysis, and that this would indeed be very useful dimension reduction technique, with fewer scores explaining a greater (average) percentage of variation in the original variables, but I am not aware that anyone has done it.
