[site]: crossvalidated
[post_id]: 399964
[parent_id]: 
[tags]: 
Upper limit of number of independent features in a random forest?

I have just finished running a random forest algorithm and I am having trouble interpreting whether my results are correct. I ran this algorithm so I could get a measure of feature importance for features in my data. The dependent variable can take 3 values while the 6 features have been encoded (one hot) such that they have become 3400 new columns (one feature is responsible for 3300 out of these 3400). The importance measure I have used (feature_importances_ from sklearn) is normalized (sum == 1) and as a result I have summed these 3400 columns to represent the importance of their original features. The result is that the feature with the most encoded columns (the 3300 one) has the highest importance by far - 68%. My question is does this make sense to do? Or is there some upper limit in terms of: 1. the number of encoded columns you can use in a random forest 2. the % that one features encoded columns can make up (e.g. here one of my columns contibutes 97% to all encoded columns) As an aside I also conducted a Kramers V test which produced a similar but not identical result the one above (in terms of ranking of feature importance) Happy to clarify anything if it is unclear Thanks, J
