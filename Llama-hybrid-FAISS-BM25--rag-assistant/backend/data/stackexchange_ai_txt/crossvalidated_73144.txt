[site]: crossvalidated
[post_id]: 73144
[parent_id]: 
[tags]: 
Using anomalies to calculate trends of seasonal data

I commonly see people doing trend analysis of (monthly) timeseries data which show a strong inter-annual cycle following this scheme: compute climatological means ("mean January", "mean February", ..., "mean December") subtract climatological means from actual data, to yield an "anomaly timeseries" perform linear regression on this "anomaly timeseries" Climatological in this case means multi-year average of individual months , e.g., an average of the 10 Januaries from 2000 to 2009. As Nick Cox points out in his comment, anomaly just means deviation from a reference level; there is no implication of anything pathological or very unusual. While user31264's answer makes sense for processes where the seasonal component is truly purely additive. However, in atmospheric science we often have processes where the amplitude of the seasonal variation depends on the base level, i.e., is somewhat multiplicative. Even in these scenarios, people often use the approach I outlined above. However, I could nowhere find a rigorous statistical explanation of why this approach is actually valid. Why is the linear regression of these anomalies a reasonable solution to the regression of the original timeseries data? Can you give me any justification why this could be reasonable to do? Those people I asked mostly say "everyone's doing it" ...
