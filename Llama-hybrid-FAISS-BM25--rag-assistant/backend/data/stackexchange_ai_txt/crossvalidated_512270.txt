[site]: crossvalidated
[post_id]: 512270
[parent_id]: 
[tags]: 
ABC approximation Bias

In Approximate Bayesian Computation, we approximate the (true) likelihood of our model, $f(x_{obs}|\theta)$ , with the following integral $$f_{ABC}(y_{obs}|\theta)=\int K_{h}(x-x_{obs})f(x|\theta)dx $$ with the use of a Taylor expansion, we can express the ABC likelihood approximately as $$f_{ABC}(y_{obs}|\theta)\approx f(x_{obs}|\theta)+\frac{1}{2}h^{2}Var[K_{h}(x-x_{obs})]f^{''}(x_{obs}|\theta),$$ where $h$ is the scale of the kernel $K_{h}(\cdot)$ and $f^{''}(x_{obs}|\theta)$ the second derivative of the likelihood. So, the bias of the true likelihood approximation can be expressed as $$f_{ABC}(x_{obs}|\theta)-f(x_{obs}|\theta)=\frac{1}{2}h^{2}Var[K_{h}(x-x_{obs})]f^{''}(x_{obs}|\theta)$$ In the case where the $f(x_{obs}|\theta)$ is a continuous function (it is the result of a density), it is meaningful to calculate the derivates. However, in the case where the likelihood is discrete, i.e $\frac{df(x|\theta)}{dx}$ has no meaning. Thus, would it be equivalent to express bias as $h^{2}Var[K_{h}(x-x_{obs})]$ , because $\frac{1}{2}f^{''}(x_{obs}|\theta)$ is a constant, so it will not affect the shape of bias over a grid of $h$ values?
