[site]: crossvalidated
[post_id]: 159684
[parent_id]: 159678
[tags]: 
If the data have a normal distribution, and you're estimating the mean, putting a mostly uninformative normal prior on the mean and inverse gaussian prior on the variance gives you a conjugate prior. Many commonly estimated types of parameters have conjugate priors to simplify estimation... like a Bernoulli probability with a beta prior. We are not bound by law to use conjugate priors. They're not always available, and sometimes it's much easier to throw an "easy to digest" prior at everything rather than going through complex derivations. Easy to digest priors might include a Jeffrey's prior. That introduces the other error. In Bayesian statistics, we know quite a bit about the parameter after data have been observed, we are not just using approximate confidence intervals to summarize our uncertainty, but we have a whole probability distribution for the posterior which needs to be estimated and described ... and done so reliably! MCMC refers to any numerical routine, including direct integration, for obtaining the posterior density. The easiest routines to understand are rejection sampling or the Metropolis-Hasting algorithm. The most commonly used routine is Gibbs sampling. The routine is easy to understand, but somewhat techincal. You can read wiki and try it out in R with the mcmc package.
