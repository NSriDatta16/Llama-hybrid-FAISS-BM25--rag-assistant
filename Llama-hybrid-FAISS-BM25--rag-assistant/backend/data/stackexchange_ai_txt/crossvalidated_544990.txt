[site]: crossvalidated
[post_id]: 544990
[parent_id]: 544980
[tags]: 
I suspect overfitting is likely to be more of a problem in the classification setting, because you have quantised the response variable and hence thrown away some information content of the dataset. Having more information in the data tends to make ML algorithms less susceptible to over-fitting, which is why adding more data usually helps. This definitely seems to be true for finding the hyper-parameters of Gaussian processes, where over-fitting in model selection is a clear problem in classification 1 , but seems less of a problem in a regression setting 2 . 1 GC Cawley, NLC Talbot, On over-fitting in model selection and subsequent selection bias in performance evaluation, The Journal of Machine Learning Research 11, 2079-2107 ( pdf ) 2 Rekar O Mohammed, Gavin C Cawley, Over-fitting in model selection with Gaussian process regression, International Conference on Machine Learning and Data Mining in Pattern Recognition, Pages 192-205, 2017 ( pdf ) BTW if the range of the response variable is vast, you might want to try some transformation (e.g. predict the logarithm of the response variable). Sometimes that is also useful if your application requires relative errors, rather than absolute.
