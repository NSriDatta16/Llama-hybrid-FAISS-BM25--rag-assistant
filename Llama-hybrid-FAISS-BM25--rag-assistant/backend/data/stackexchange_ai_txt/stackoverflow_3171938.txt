[site]: stackoverflow
[post_id]: 3171938
[parent_id]: 3171803
[tags]: 
Thanks for helping me think this out everyone! I'm going to try a hybrid approach here: 1) Pull down pages to a tree structure on the filesystem. 2) Put content into a generic content table that does not contain any full webpage (this means that our average 63k column is now maybe a 1/10th of a k. THE DETAILS 1) My tree structure for housing the webpages will look like this: -- usr_id1k | |-- user1 | | |-- job1 | | | |-- pg_id1k | | | | |-- p1 | | | | |-- p2 | | | | `-- p3 | | | |-- pg_id2k | | | `-- pg_id3k | | |-- job2 | | `-- job3 | |-- user2 | `-- user3 |-- usr_id2k `-- usr_id3k 2) Instead of creating a table for each 'job' and then exporting it we'll have a couple different tables -- the primary one being a 'content' table. content_type, Integer # fkey to content_types table user_id, Integer # fkey to users table content, Text # actual content, no full webpages .... other stuff like created_at, updated_at, perms, etc...
