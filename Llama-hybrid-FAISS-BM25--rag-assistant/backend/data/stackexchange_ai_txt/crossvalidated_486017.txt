[site]: crossvalidated
[post_id]: 486017
[parent_id]: 484480
[tags]: 
A lot of the same techniques to training CNNs and other architectures also apply to training fully connected networks. A list of things that I tried and gave me much better performance: Used SGD with a carefully selected learning rate and learning rate schedule. Using nesterov momentum with a momentum of 0.9 will also likely speed convergence. Adaptive methods are easy to configure, but SGD will do better when properly tuned [1]. Choose your learning rate based on a subset of the data with the batch size you want to use. SGD convergence is robust to the size of the dataset [5] section 1. Use a smaller batch size, but also increase the initial learning rate. Some works have suggested a linear scaling rule, while others have suggested a square root scaling [2,3]. Warm up the learning rate from a smaller value [2] If using LayerNorm, instead use a simple variant that does not learn bias and variance parameters (these tend to overfit and make the network more difficult to train)[4]. If using BatchNorm, be aware of how it is affected by distributed training [2] Monitor network statistics every several iterations. Amount of activation saturation, gradient norms, ratio of the gradient norm to the weight norm and the weight norm can all be informative to how to further tune parameters. Choose layer-wise initial learning rates. This can be done either using second order information (the inverse of the maximum eigenvalue of the hessian) or just first order information (ensure the ratio of the gradient norm to the weight norm is some common value such as 0.1) [5] chapter 1 and 18. Local minimum in overparametrized networks are usually good enough [6]. Some personal anecdotal points: Somehow, deeper networks are easier to optimize than wider networks Bengio [5] section 19 claims that networks with fixed layer size work better than varying layer size. He also claims that using a wider layer size than the input dimension works better, although I have found that using a smaller size is easier to optimize. References [1] Wilson, Ashia C., et al. "The marginal value of adaptive gradient methods in machine learning." Advances in neural information processing systems. 2017. [2] Goyal, Priya, et al. "Accurate, large minibatch sgd: Training imagenet in 1 hour." arXiv preprint arXiv:1706.02677 (2017). [3] Krizhevsky, Alex. "One weird trick for parallelizing convolutional neural networks." arXiv preprint arXiv:1404.5997 (2014). [4] Xu, Jingjing, et al. "Understanding and Improving Layer Normalization." Advances in Neural Information Processing Systems. 2019. [5] Orr, Genevieve B., and Klaus-Robert MÃ¼ller, eds. Neural networks: tricks of the trade. Springer, 2003. [6] Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.
