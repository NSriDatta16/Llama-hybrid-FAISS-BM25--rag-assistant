[site]: crossvalidated
[post_id]: 622660
[parent_id]: 618278
[tags]: 
The average log probability score is equal to the KL divergence up to a constant factor (an entropy term) assuming the expectation is taken with respect to $p$ in your example. We can write the KL divergence as \begin{align} D(p || q) = H(p, q) - H(p) \end{align} where $H(p, q)$ is the cross entropy between $p$ and $q$ and $H(p)$ is the entropy of $p$ . The author is referring to the fact that $H(p)$ is a constant and not needed if we want to compare $q$ and $r$ . Assuming a discrete random variable, \begin{align} H(p, q) &= -\sum_x p(x) log q(x) \\ &=-\mathbb{E}_{p(x)} log q(x) \end{align}
