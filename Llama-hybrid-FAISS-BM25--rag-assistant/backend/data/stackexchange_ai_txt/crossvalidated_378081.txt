[site]: crossvalidated
[post_id]: 378081
[parent_id]: 
[tags]: 
KL-Divergence of $Q(z|X)$ and $P(z)$ in Variational Autoencoder (VAE)

I aim to understand how $D_{KL}[Q(z | X) || P(z)]$ can be converted to $\frac{1}{2} \sum_{k} (\Sigma(X) + \mu^{2}(X) - 1 - \log \Sigma(X))$ , where $k$ is the dimension of the Gaussian distribution. We have known that $Q(z | X)$ distributes as $N(\mu(X), \Sigma(X))$ and $P(z)$ satisfies $N(0, 1)$ . Therefore, I can understand $$D_{KL}[Q(z | X) || P(z)] = D_{KL}[N(\mu(X), \Sigma(X)) || N(0, 1) = \frac{1}{2} (\operatorname{tr}(\Sigma(X)) + \mu(X)^{T}\mu(X) - k - \log \det(\Sigma(X))).$$ We implement $\Sigma(X)$ as a vector as it's a diagonal matrix. However, I don't understand the following, $$D_{KL}[N(\mu(X), \Sigma(X)) || N(0, 1) = \frac{1}{2} \big(\Sigma_{k} \Sigma(X) + \Sigma_{k} \mu^{2}(X) - \Sigma_{k}1 - \log \prod_{k} \Sigma(X) \big).$$ Can anyone please enlighten me how is the last step deduced?
