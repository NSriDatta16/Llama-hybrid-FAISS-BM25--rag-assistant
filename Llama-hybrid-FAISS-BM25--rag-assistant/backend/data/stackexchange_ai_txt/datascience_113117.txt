[site]: datascience
[post_id]: 113117
[parent_id]: 113020
[tags]: 
I think you are trying to do a multilabel + multiclass classification. You would need to do: Have multiple output layers for each multiclass classification(Each for your Ds1, Ds2, Ds3, Ds4) Calculate cross entropy loss for each one them Sum them to have your final loss to be used for backpropagation. Below is an example (Not an optimized one) from torch import nn from transformers import AutoModel class CustomModel(nn.Module): def __init__(self,num_labels1, num_labels2, num_labels3, num_labels4): super(CustomModel,self).__init__() self.num_labels1 = num_labels1 # number of unique values in ds1 self.num_labels2 = num_labels2 # number of unique values in ds2 self.num_labels3 = num_labels3 # number of unique values in ds3 self.num_labels4 = num_labels4 # number of unique values in ds4 #Load Model with given checkpoint and extract its body self.model = AutoModel.from_pretrained("bert-base-uncased") self.classifier1 = nn.Linear(768,num_labels1) # output layer1 self.classifier2 = nn.Linear(768,num_labels2) # output layer2 self.classifier3 = nn.Linear(768,num_labels3) # output layer3 self.classifier4 = nn.Linear(768,num_labels4) # output layer4 def forward(self, input_ids=None, attention_mask=None, labels1=None, labels2=None, labels3=None, labels4=None ): #Extract outputs from the body outputs = self.model(input_ids=input_ids, attention_mask=attention_mask) #Add custom layers sequence_output = self.dropout(outputs[0]) #outputs[0]=last hidden state logits1 = self.classifier1(sequence_output[:,0,:].view(-1,768)) logits2 = self.classifier2(sequence_output[:,0,:].view(-1,768)) logits3 = self.classifier3(sequence_output[:,0,:].view(-1,768)) logits4 = self.classifier4(sequence_output[:,0,:].view(-1,768)) loss = None if self.training: loss_fct = nn.CrossEntropyLoss() #compute individual losses loss1 = loss_fct(logits1.view(-1, self.num_labels1), labels1.view(-1)) loss2 = loss_fct(logits2.view(-1, self.num_labels2), labels2.view(-1)) loss3 = loss_fct(logits3.view(-1, self.num_labels3), labels3.view(-1)) loss4 = loss_fct(logits4.view(-1, self.num_labels4), labels4.view(-1)) #sum them up as final loss and return loss = loss1 + loss2 + loss3 + loss4 # return all the logits inorder to get that vector of outputs--- return (loss, [logits1, logits2, logits3, logits4])
