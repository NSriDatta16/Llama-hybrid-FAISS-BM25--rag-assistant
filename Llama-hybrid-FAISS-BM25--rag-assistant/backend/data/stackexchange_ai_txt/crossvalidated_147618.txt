[site]: crossvalidated
[post_id]: 147618
[parent_id]: 147598
[tags]: 
Suppose that we have our data $X \sim f(x | \theta)$ where $X$ and $\theta$ can be vectors but for ease of typing and notation I won't otherwise indicate this. I'm going to go through the basic procedure for Bayesian inference and hopefully you will see what you should be doing or are doing wrong. Bayes' rule tells us that $$ p(\theta | x) = \frac{f(x|\theta)p(\theta)}{m(x)} $$ where $p(\theta)$ is our prior (note that this is a pdf or pmf), $f(x|\theta)$ is the likelihood of the data, and $m(x)$ is the marginal probability density of the data. Note that $$ m(x) = \int_\Theta f(x, \theta) d\theta = \int_\Theta f(x|\theta) p(\theta)d\theta. $$ This means that we only need the likelihood of the data and the prior and in principle we can calculate the density of the posterior. It is very important to note that these are all densities here. Once we have obtained our posterior distribution of $\theta$ given the data $x$ we can then perform inference. One common estimate of the $\theta$ that generated our data is the expected value of the posterior, i.e. $$ \hat \theta = \int_\Theta \theta \times p(\theta|x) d\theta. $$ We could also look at the median or mode of the posterior (in the slides you linked to, they look at the argmax of the posterior which means the mode). Here's an example to hopefully clarify things. Suppose $X_1, \dots, X_n \sim \ iid \ Bernoulli(\theta)$. This means that $$ f(\vec x | \theta) = \theta ^ {\sum x_i} (1-\theta)^{n - \sum x_i}. $$ For our prior we'll assume that $\theta \sim Beta(a, b)$, i.e. $p(\theta) \propto (\theta)^{a-1}(1-\theta)^{b-1}$. We want to find the posterior distribution of $\theta | \vec x$ so we use Bayes rule: $$ p(\theta | \vec x) \propto f(\vec x|\theta) p(\theta) \propto \theta ^ {\sum x_i} (1-\theta)^{n - \sum x_i} \times (\theta)^{a-1}(1-\theta)^{b-1} $$ $$ = \left( \theta \right)^{(\sum x_i) + a - 1} \left( 1 - \theta \right)^{n - (\sum x_i) + b - 1} $$ which we recognize as the kernel of a $Beta(a + \sum x_i, n + b - \sum x_i)$ distribution. We were able to work purely with proportionalities rather than equalities because Bayes rule guarantees that we end up with a proper density and so if we recognize our posterior kernel we can just throw in the necessary normalizing constant at the end. Now suppose we want a good estimate of $\theta$ for our realized data $\vec x$. Because the posterior is a Beta distribution we can simply look up the equation for its expected value and can take $$ \hat \theta = \frac{a + \sum x_i}{a + \sum x_i + n + b - \sum x_i} = \frac{a + \sum x_i}{a + b + n}. $$
