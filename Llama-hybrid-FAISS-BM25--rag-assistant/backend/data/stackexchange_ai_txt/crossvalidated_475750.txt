[site]: crossvalidated
[post_id]: 475750
[parent_id]: 469799
[tags]: 
I would split logistic regression into three cases: modelling "binomial proportions" with no cell proportions being 0% or 100% modelling "Bernoulli data" something in between What's the difference? case 1 In case 1, your data cannot be separated using your predictors, because each feature $x_i$ has multiple records, with at least 1 "success" and at least 1 "failure". The loss function then becomes $$LogLoss=\sum_i n_i \left[f_i\log(p_i)+(1-f_i)\log(1-p_i)\right]$$ Where $f_i$ is the proportion of times $y=1$ in "cell" $i$ , and $p_i=(1+\exp^{-x_i^Tw})$ is the modelled probability that $y=1$ in "cell" $i$ . The number $n_i$ is the number of training samples you have for "cell" $i$ . What defines a "cell"? The samples with the same set of features $x_i$ are all in the same cell. In case 1, regularisation may not be needed and can actually be harmful. It depends on how big the cell sizes ( $n_i$ ) are. But the loss function looks totally different to the plot you show for this case - it is more like a squared error loss function, and is can be approximated by $\sum_i n_i\frac{(f_i-p_i)^2}{p_i(1-p_i)}$ . This is also known as normal approximation to binomial proportion (and also underlies many gradient based algorithms for estimating the coefficients). Perfect prediction for each sample is impossible in this scenario, and you can think of the cells themselves as a form of regularisation. The predictions are constrained to be equal for samples in the same cell. Provided no cells are homogeneous (at least 1 of both outcomes) you cannot have a coefficient wander off to infinity. You can also think of this as being very similar to linear regression at the cell level on the observed "logits" $\log\left(\frac{f_i}{1-f_i}\right)=x_i^Tw+error$ with each record weighted towards the "high information" samples $n_ip_i(1-p_i)$ (Ie big cell size + prediction close to decision boundary), rather than unweighted. As a side note, you can save a lot of computing time by fitting your models as "case 1" - particularly if $n_i$ are large -compared to binary modelling the data in case 2. This is because you aggregate sums over "cells" rather than "samples". Also your degrees of freedom are defined by the number of "cells" rather than the number of "samples" (eg if you have 1 million samples but only 100 cells, then you can only fit 100 independent parameters). case 2 In this case, the predictors uniquely characterise each sample. This means we can fit the data with zero log loss by setting fitted values to $0$ or $1$ . You can use the notation before as $n_i=1$ or $n_i>1,f_i\in\{0,1\}$ . In this case we need some kind of regularisation, particularly if all the $n_i$ are small. Either "size of coefficients" (eg L1, L2) where large values for $w$ are penalised. You could also penalise "difference in coefficients" - such as needing unit which are "close" in feature space to have similar predictions - similar to forming cells like in case 1 (this is like pruning a regression tree). Interestingly, some regularisation approaches can be characterised as adding "pseudo data" to each cell such that you have a situation more like case 1. That is, for the records with $f_i=0$ we add pseudo data for a $y=1$ case in that cell, and if $f_i=1$ we add pseudo data for a $y=0$ case in that cell. The different levels of regularisation will determine how much "weight" to give the pseudo data vs the observed data. case 3 In this case you may have small segments of the sample that can be perfectly predicted. This is also likely to be where most real data lives. Can see that some kind of adaptive regularisation will likely help - where you focus more on regularising based on $n_i$ . The difficult part is that many choices on what's best really depend on the data you're working with, and not the algorithm. This is one reason we have lots of different algorithms. In fact, the logistic regression MLE, if not penalised, will basically split the training sample into "case 1" and "case 2" datasets. Analytically this approach will minimise the log loss. The problem is computational issues tend to result in the algorithm stopping before this happens. Typically you see large coefficients with even larger standard errors when this happens. Easy enough to find these by simply looking at or filtering the coefficients (probably need to be a bit clever with visualising these if you have a huge number of coefficients).
