[site]: datascience
[post_id]: 22571
[parent_id]: 
[tags]: 
Neural networks: generating g prime(z) in back propagation

I am doing Andrew Ng's excellent new Deeplearning course. I have an issue with implementing back propagation in a one hidden layer network that looks like this: I am trying to derive the derivative of Z1 using the formula: My understanding is that g prime Z1 in this formula is the derivative of g(z) with respect to Z which I believe is sigmoid(Z) * (1-sigmoid(Z)) where the function sigmoid generates the sigmoid function with respect to Z. My final equation looks like this: dZ1 = np.dot(W2.T, dZ2) * (sigmoid(Z1)*(1-sigmoid(Z1))) Where Z1 is the output of the first (and only) hidden layer. Where am I going wrong?
