[site]: datascience
[post_id]: 93068
[parent_id]: 93034
[tags]: 
With regard to a dictionary of words , there can be no single dictionary for BERT because the BERT embeddings incorporate contextual information (i.e. the surrounding words in the sentence change the embedding for your target word). In theory, you could construct a dictionary for your words by passing single word sentences (though a single word may be broken down into multiple tokens). If you're looking for an easy practical way to get the pretrained Bert embeddings, HuggingFace makes it easy. I have given a simple code snippet below using python and specifically pytorch: from transformers import BertTokenizer, BertModel import torch my_sentence = "Whatever your sentence is" tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') model = BertModel.from_pretrained('bert-base-uncased') input_ids = tokenizer(my_sentence, return_tensors="pt") output = model(**input_ids) final_layer = output.last_hidden_state The final_layer tensor will now hold the embeddings (768 dimensional) for each token in your input sentence. Note that the zeroth token is a start token (CLS) and the last token is an end token. If you have a list of sentences (of single words in your case perhaps if you are making a dictionary), you can use the above code in a batched manner. However, you will need to extract a mask from the tokenizer and pass this to your model (to account for different length sentences). Example: tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') encoded_inputs = tokenizer(list_of_sentences, padding=True, truncation=True, return_tensors="pt") ids = encoded_inputs['input_ids'] mask = encoded_inputs['attention_mask'] output = model(ids, mask) final_layer = output.last_hidden_state
