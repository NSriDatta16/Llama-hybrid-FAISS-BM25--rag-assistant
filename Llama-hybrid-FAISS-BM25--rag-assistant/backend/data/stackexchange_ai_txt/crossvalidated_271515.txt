[site]: crossvalidated
[post_id]: 271515
[parent_id]: 270969
[tags]: 
You could see the Convolutional layers as a dimension reduction technique. Indeed, nearby pixels share a lot of covariance and ideally the features for a machine learning approach are independent. If the convolutional operator is replaced by a specific convolutional operator were all the weights are $1/d^2$ (i.e. the average) it effectively reduces the dimensions of the picture. This operation however is rather simple and not tuned to our specific learning goal. By learning the weights of a conv operator we can 'tune' our 'average' to our learning goal. The conv operation works that well for 2d and 3d inputs because it gives structure to this dimension reduction. It focuses on learning the variance of nearby pixels. By removing the variances of nearby pixels the latter fully-connected layers can be more effective.
