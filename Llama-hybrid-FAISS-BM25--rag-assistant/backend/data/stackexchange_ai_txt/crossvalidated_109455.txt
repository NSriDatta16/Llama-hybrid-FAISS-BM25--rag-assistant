[site]: crossvalidated
[post_id]: 109455
[parent_id]: 108466
[tags]: 
"Least squares" means that the overall solution minimizes the sum of the squares of the errors made in the results of every single equation.The most important application is in data fitting. The best fit in the least-squares sense minimizes the sum of squared residuals, a residual being the difference between an observed value and the fitted value provided by a model.Least squares problems fall into two categories: linear or ordinary least squares and non-linear least squares, depending on whether or not the residuals are linear in all unknowns. Bayesian linear regression is an approach to linear regression in which the statistical analysis is undertaken within the context of Bayesian inference. When the regression model has errors that have a normal distribution, and if a particular form of prior distribution is assumed, explicit results are available for the posterior probability distributions of the model's parameters. In some contexts a regularized version of the least squares solution may be preferable. Tikhonov regularization (or ridge regression) adds a constraint that $\|\beta\|^2$, the L2-norm of the parameter vector, is not greater than a given value. In a Bayesian context, this is equivalent to placing a zero-mean normally distributed prior on the parameter vector. An alternative regularized version of least squares is Lasso (least absolute shrinkage and selection operator), which uses the constraint that $\|\beta\|_1$, the L1-norm of the parameter vector, is no greater than a given value . In a Bayesian context, this is equivalent to placing a zero-mean Laplace prior distribution on the parameter vector. One of the prime differences between Lasso and ridge regression is that in ridge regression, as the penalty is increased, all parameters are reduced while still remaining non-zero, while in Lasso, increasing the penalty will cause more and more of the parameters to be driven to zero. This paper compares regular lasso with Bayesian lasso and ridge regression (see figure 1 ).
