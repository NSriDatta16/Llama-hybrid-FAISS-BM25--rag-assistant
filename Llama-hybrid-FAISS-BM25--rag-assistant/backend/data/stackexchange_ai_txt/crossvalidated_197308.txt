[site]: crossvalidated
[post_id]: 197308
[parent_id]: 197301
[tags]: 
For question 1, what I would say if were at your place (while this is obviously an over-simplification, but this is the purpose of the question) is: Bayesians consider probability as a the representation of the belief of a proposition or event (contrary to frequentist that thinks it as a frequencies of occurence). Consequently, they have no problem in attributing probability to a parameter e.g. the mean of a population $\mu$ or the difference between two means $\mu_1-\mu_2$. Then given a model and data $x$ they can compute $p(\theta|x)$ that is the belief we have on $\theta$ after having observed $x$ e.g. $p(\mu_1-\mu_2|x,y)$. (Maybe you can add that in some situation designing a well-received model can go with some problems, the prior, but this is the job of the statistician to do it correctly). When $p(\theta|x)$ is computed, testing weither $\theta$ is lower than $\theta_0$ with a given confidence level can be performed by checking if the region of highest probability of $p(\theta|x)$ covering $1-\alpha$ of the overall density includes or not values $\theta_0>0$ (your graph illustrates this well). For question 2, I think (but I am not so sure) this is not correct : if X% of the overall density (and not credible intervals) falls at the right of zero (we do not need any ROPE here), then it is correct to say that there is an X% probability (given the data and model ) that hypothesis μ1 is less than μ2 is correct. So if, as you stated, X% of the credible interval falls to one side of that ROPE, then there is at least X% probability (given the data) that that hypothesis (eg μ1 is less/greater than μ2) is correct. Nevertheless, IMHO hypothesis testing is also a matter of defintion and I see no strong reason to avoid to define that acceptation of hypothesis $\mu_1
