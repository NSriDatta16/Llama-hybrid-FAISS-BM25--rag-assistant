[site]: crossvalidated
[post_id]: 319683
[parent_id]: 
[tags]: 
Normalizing Flows, Real NVPs and Inverse Autoregressive Flows - Used for Probabilty Density Approximation or for Sampling?

Suppose we have a parametric family $g(x;\theta)$, where $\theta$ are the parameters. As far as I can tell, there are two ways we can use this family to model a probability distribution: Probability density approximation : We can train $g(x;\theta)$ to be very similar to some desired probability distribution $p(x)$, such that for every $x$, $g(x;\theta)\approx p(x)$ Sampling : We can train $g(x;\theta)$ so that it approximately generates samples of a desired random vector $X$: If we feed $g$ with some random input, say $Z\sim\mathcal{N}(0,I)$, then the distribution of the random vector $g(Z;\theta)$ is similar to the distribution of $X$. I know that in the field of Bayesian inference, there have been a lot of work lately about creating flexible models $g(x;\theta)$ for probabilistic modeling. Specifically, I'm referring to normalizing flows , real NVPs and inverse autoregressive flows . I'm confused as to whether these methods are addressing probability density approximation or sampling, or both.
