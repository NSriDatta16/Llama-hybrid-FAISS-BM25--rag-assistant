[site]: crossvalidated
[post_id]: 225401
[parent_id]: 223343
[tags]: 
Convolutional Neural Networks can also be applied to text, e.g. A Convolutional Neural Network for Modelling Sentences , see also Understanding Convolutional Neural Networks for NLP . RBM pretraining is essentially obsolete ( though, maybe not completely ). In particular, layer-wise pretraining was superseded by new techniques . long-short term memory but they look very artificial and there's no reason to expect they can do well LSTM was designed with concrete motivation of handling vanishing gradients problem, and it's quite successful in doing so (compared to vanilla RNN). All the operations performed inside of LSTM can also be explained just as in CNN. There're many novel architectures researched, but the widely used ones are classical feedforward networks, convolutional networks and recurrent networks (LSTM / GRU). For example when the data is NOT continuous Text is obviously not continuous. The way you use NNs for text is you represent each word using one-hot-encoding and then apply fully connected layer to the input. Essentially, it'll learn word embeddings (representations) that're useful for your problem.
