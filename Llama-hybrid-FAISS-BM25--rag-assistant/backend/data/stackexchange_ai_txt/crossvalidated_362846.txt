[site]: crossvalidated
[post_id]: 362846
[parent_id]: 309231
[tags]: 
Neural networks are optimized by starting with an initial, random guess of the parameter values. This guess is iteratively updated, mostly commonly using backpropagation . Researchers have found that the optimization task can be very challenging, but that careful attention to how the parameters are initialized can make the optimization easier. In the case of Xavier initialization (also called "Glorot normal" in some software), the parameters are initialized as random draws from a truncated normal distribution with mean 0 and standard deviation $$\sigma = \sqrt{\frac{2}{a+b}}$$ where $a$ is the number of input units in the weight tensor, and $b$ is the number of output units in the weight tensor.
