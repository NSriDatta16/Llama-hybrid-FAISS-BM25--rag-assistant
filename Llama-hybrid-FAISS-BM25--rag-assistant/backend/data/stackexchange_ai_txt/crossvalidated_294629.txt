[site]: crossvalidated
[post_id]: 294629
[parent_id]: 291733
[tags]: 
Yes, your high-level procedure works great: Map text to a vector of features. Concatenate with the rest of your numeric features and pass the whole thing to your training algorithm. However, there are many, many ways to go about step 1 , and the best way will depend largely on the context of your analysis and the task at hand. For example, the bag-of-words approach ignores any relationships between words. That may be totally fine if your prediction task depends on the presence or absence of certain key words, but it may be insufficient if the prediction depends on key phrases or complex interactions of words, emoji, and punctuation over the entire tweet. Furthermore, you may find that infrequent words are actually important while frequent words are just filler. To address this, you might use TF-IDF , which balances the frequency of a word (or 2- or 3-word sequence, called $n$-grams) in a particular tweet with the relative frequency of the word across all tweets, thereby reducing the impact of common filler words. But you would still have 1 column per word or $n$-gram, which will result in a very high-dimensional feature vector unless you apply some form of dimensionality reduction in addition. At the other end of the complexity scale, a recurrent neural network, such as an LSTM , can capture complex structure and long-term dependency in your tweets in theory, but it requires large amounts of data and careful tuning of its hyperparameters in order to avoid over-fitting.
