[site]: crossvalidated
[post_id]: 200316
[parent_id]: 
[tags]: 
Why is this Bayesian estimate of a truncation-point so poor?

I have several datasets. Each dataset holds the masses of objects that have been subject to physical wear, expressed as a proportion of their original mass ($w$), and the amount of time that the object was exposed to physical wear ($t$). I assume that objects wear at a constant rate while they are exposed to wear, and that within a dataset, wear-rates are approximately Normally-distributed. At some amount of wear, objects fail due to wear. Failed objects cannot be recovered, and so are not recorded in any dataset. For each dataset, I wish to estimate: The mean wear-rate of objects within that dataset ($\mu$) The proportion of wear at which objects fail and become unobservable ($b$). I have attempted a Bayesian approach to to estimating these parameters, as suggested in this question . When the dataset is affected by truncation, my current approach generates upward-biased estimates of $\mu$, and very imprecise estimates of $b$. Code (R and JAGS): points losspoint allPoints Example: Here is a dataset affected by data-truncation due to wear (generated using the code above). Truncation is obvious from the characteristic 'elbow' in the Lowess smoother line. The value of b will therefore be near the lowest value of $w$ in this dataset (the value of $b$ is set to 0.55 in this example). However, the posterior densities for b do not peak near the lowest value of $w$ as I expected them to. Also, the values of annualWear (which is defined as $\mu$*365) are estimated to be higher than they should be - annualWear is set to 0.022, not around 0.0265. annualWear appears to be more-accurately estimated in datasets that are unaffected by truncnation. What am I doing wrong here?
