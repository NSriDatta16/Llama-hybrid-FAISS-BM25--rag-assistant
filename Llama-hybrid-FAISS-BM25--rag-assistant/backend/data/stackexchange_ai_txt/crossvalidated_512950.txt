[site]: crossvalidated
[post_id]: 512950
[parent_id]: 470804
[tags]: 
The original "Attention is all you need" paper use sine positional encoding. You can find a great in-depth explanation on this topic by Jonathan Kernes here: https://towardsdatascience.com/master-positional-encoding-part-i-63c05d90a0c3 . While positional embedding is basically a learned positional encoding. Hope that it helps!
