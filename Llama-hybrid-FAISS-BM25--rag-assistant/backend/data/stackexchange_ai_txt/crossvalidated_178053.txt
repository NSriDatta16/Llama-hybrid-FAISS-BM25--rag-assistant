[site]: crossvalidated
[post_id]: 178053
[parent_id]: 178052
[tags]: 
First, a note about terminology: the "covariance function" in GP parlance means the same thing as the kernel. Thus the area of a covariance kernel is just a constant depending on your hyperparameters; for a Gaussian kernel it's $(2 \pi)^{d/2} \sqrt{\mathrm{det}(\Sigma)}$. What you actually meant to ask seems to be about the area contained within, say, a 95% confidence band. This is a measure of your posterior uncertainty about the underlying function. Calculating it Let's assume the GP has domain $\mathcal X \subseteq \mathbb R^n$ with mean function $m(x)$ and variance function $v(x)$, and say we want the area within $\gamma$ standard deviations (corresponding to a $2 \Phi(\gamma) - 1$ confidence band, i.e. $\gamma = 1.96$ is approximately 95%). Then the quantity in question is $$a = \int_{x \in \mathcal X} \left( \int_{y = m(x) - \gamma \sqrt{v(x)}}^{y = m(x) + \gamma \sqrt{v(x)}} \mathrm d y \right) \mathrm d x = \int_{x \in \mathcal X} 2 \gamma \sqrt{v(x)} \mathrm d x .$$ Note that this doesn't depend on the mean function at all. Now, in most settings (including the usual Bayesian optimization), your GP is the posterior corresponding to observations $\{(X_i, y_i)\}_{i=1}^n$, and the prior has zero mean and covariance $k(x, y) + \sigma^2 I(x = y)$. (This corresponds to observations made with iid $\mathcal{N}(0, \sigma^2)$ noise; if you want a prior mean function, just subtract it from the observations.) Defining $$\mathbf K := \begin{bmatrix} k(x_1, x_1) & \cdots & k(x_1, x_n) \\ \vdots & \ddots & \vdots \\ k(x_n, x_1) & \cdots & k(x_n, x_n) \end{bmatrix} \quad\text{and}\quad \mathbf k(x) = \begin{bmatrix}k(x, x_1) \\ \vdots \\ k(x, x_n)\end{bmatrix} ,$$ the posterior variance function has the form $$ v(x) = k(x, x) - \mathbf k(x)^T \left( \mathbf K + \sigma^2 I \right)^{-1} \mathbf k(x) .$$ Some notes about this: Though this depends on which $x$ you've picked, it doesn't depend on the observations $y$ at all. If $\mathcal X$ is unbounded, as $x$ gets far away from the computed samples the variance goes to $k(x, x)$ (typically a constant) and so $a$ is infinite. This is unlikely to be an issue for you, since Bayesian optimization on unbounded domains without localizing priors rarely makes sense. I doubt it's going to be analytically computable, though you might be able to get an analytic upper bound for particular kernels via Jensen's inequality. For low-dimensional $\mathcal X$ it shouldn't be too terrible to compute numerically. What the function means The doesn't make any claims about the maximum of that function, which is what we usually care about for Bayesian optimization. Instead, this is the kind of criterion that's usually used for active learning of a function. That is, your goal is to query points to maximally reduce your uncertainty about the function as a whole (as measured by some uncertainty function). In Bayesian optimization, we want to reduce our uncertainty about the location of the maximum of the function, which is in some sense an easier problem. You should be able to imagine a GP posterior with high area but very certain maximal position, and likewise a GP posterior with relatively small area but pretty uncertain maximal position. So, this isn't necessarily great to use as an acquisition function for optimization, though it might make sense for active learning of functions. Of course, you're not proposing to use it as an acquisition function, but instead as a measure of uncertainty to decide when to stop. This isn't crazy – when you understand the function well, you're more willing to stop – but since it doesn't distinguish between uncertainty about the maximum and uncertainty about areas you're pretty sure are not the maximum, it's not necessarily the most efficient way to do so.
