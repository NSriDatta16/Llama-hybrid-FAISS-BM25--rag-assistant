[site]: crossvalidated
[post_id]: 231520
[parent_id]: 231410
[tags]: 
When I was learning SGD, this[1] left me with a strong impression and I quote Leon Bottou as possible/alternative answer/solution to your question, "During the last twenty years, I have often been approached for advice in setting the learning rates $\gamma_t$ of some rebellious stochastic gradient descent program. My advice is to forget about the learning rates and check that the gradients are computed correctly. This reply is biased because people who compute the gradients correctly quickly find that setting small enough learning rates is easy. Those who ask usually have incorrect gradients. Carefully checking each line of the gradient computation code is the wrong way to check the gradients. Use finite differences...." (page 8, [1]) [1] Bottou, L. (2012). Stochastic gradient descent tricks. In Neural Networks: Tricks of the Trade (pp. 421-436). Springer Berlin Heidelberg.
