[site]: crossvalidated
[post_id]: 415618
[parent_id]: 415612
[tags]: 
My first thought was that a hierarchical cluster analysis using a method that minimizes variability would be the best approach (e.g., Ward's method). However, this doesn't constrain to a solution with a fixed number of clusters (or more appropriately, a fixed number of items within a fixed number of clusters, such as a half-half split). So, I would propose a less elegant sampling approach to find the "best" split sample with the desired criteria. Randomly sample different splits of the 12 values (into equal groups of 6 each); calculate the difference of the standard deviations for each group; retain the split that gives the lowest value. If you sample over a larger enough number of iterations, you should be able to find a reasonably good solution. Here's the code I applied to your data: vals 50) { print(ctr) print(grp) best.split This resulted in the following: grp vals 1 0 283.7898 2 1 283.9674 And the groups were: 700 850 930 800 1130 1480 490 1220 505 650 810 960 I hope this helps with your project. And, I hope that if there is a more elegant way to cluster the data to achieve minimum differences in variability for fixed sample sizes...I'd very much like to see that.
