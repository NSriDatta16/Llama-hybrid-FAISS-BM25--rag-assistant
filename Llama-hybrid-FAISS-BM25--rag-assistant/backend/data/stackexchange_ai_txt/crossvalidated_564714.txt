[site]: crossvalidated
[post_id]: 564714
[parent_id]: 
[tags]: 
How to handle inconsistency in ML explanations?

I found out that we have different solutions like below to explain the ML predictions a) LIME b) SHAP Despite using all these approaches, I see that all of them work for certain data points and not work for certain data points. for example, let's assume we have a dataset with 6 re ords. LIME works/explanation makes sense for records 1,2,3 and 4 whereas LIME explanation doesn't make sense for records 5 an 6. On the other hand, SHAP works well for 3,4,5 and 6 but doesn't explain well for records 1 and 2. So, my question is below machine learning model are not 100% explainable? How do you deal such scenarios when you wish to explain your predictions to the business users but you find out that local explanations make sense for few records and not for other records? Do you compromise on explanations/ go live even with some inconsistency in explanations? Is it an expected behavior? Simply, what do you amswer when ypur business asks why certain explanations are inaccurate (and some are correct). How do we rely on such a solution?
