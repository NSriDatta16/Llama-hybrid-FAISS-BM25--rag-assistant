[site]: crossvalidated
[post_id]: 533484
[parent_id]: 
[tags]: 
MCMC not constraining parameters correctly

I have to fit a model to some data and I was wondering how to interpret the results I get from the Bayesian parameter inference performed using emcee. Model #1 has 3 parameters: $h_0,\Omega_m,\Omega_{\Lambda}$ .The mathematical expression is (note that $d_H=\dfrac{c}{100h_0}$ and $\Omega_{\Lambda} = \Omega_{de}$ ): Model # 2 has 4 parameters: $h_0,\Omega_m,\Omega_{\Lambda}, w_0$ , with a similar expression: These are the contour plots for the two models. You can see that in the 4-parameter case things get messy, and the posterior shape is far from being Gaussian for the last 2 parameters. Things don't improve tweaking priors, sample size, number of walkers. However, if I impose the constraint $\Omega_m = 1-\Omega_{\Lambda}$ , thereby reducing the number of parameters by 1 for both models, thigs work nicely also for model # 2: Now, the question is: is it correct to conclude that the data are not "strong" enough to constraint both $\Omega_{\Lambda}$ and $w_0$ ? or is there simply something wrong with the code? I'm using flat broad priors and a Gaussian Likelihood. Thank you!
