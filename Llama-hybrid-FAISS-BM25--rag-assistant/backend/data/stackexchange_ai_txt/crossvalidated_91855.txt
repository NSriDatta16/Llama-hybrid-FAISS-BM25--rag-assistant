[site]: crossvalidated
[post_id]: 91855
[parent_id]: 91831
[tags]: 
The point about DAGs is that they reflect the factorization of the density of probability. It does not necessarily have something to do with transitions. Maybe more interestingly (and used in many applications in that sense) has to do with causality . That is, observing some events can have several causes. How can I infer which cause is the most likely? The fact that DAG reflects how the density of probability factorizes, means that it captures the independence relationships, that is, it allows to reason about the data. Based on the graph structure, is possible to derive efficient algorithms for inference. But going back to your original question, undirected models are also Bayesian. A typical case are Markov Random Fields (MRF) in image processing. See for example "Bayesian Methods and Markov Random Fields" by Mario A. T. Figueiredo. The idea is that the pixel values (the observations) are the consequences of a cause (which is defined by the application: segmentation, object detection, and so on). In this indirected models you do not reason in terms of causality, but make inference from relationships among observations and local observations. In undirected models one defines the way the observations interact with each other, and that allows you to solve big combinatorial problems in an efficient manner. You may take a look at the paper: "Markov Random Field modeling, inference & learning in computer vision & image understanding: A survey", by Wang et al. or the wikipedia site on MRFs .
