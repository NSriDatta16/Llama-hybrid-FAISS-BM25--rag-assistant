[site]: crossvalidated
[post_id]: 442109
[parent_id]: 
[tags]: 
Why does differencing a time series remove its memory?

SO Post 345798 asks to enumerate the problems with overdifferencing in time series, and so far that post has focused on one specific problem: the removal of process memory in a manner that could hurt forecasts. Fractional differencing is referenced as a solution. There are some comments about why (or when) a first difference would remove memory. Certainly if the process was a true random walk, no memory would be lost by differencing. What is the mechanism behind "memory loss" from taking a difference? When does it occur, and why does it occur?
