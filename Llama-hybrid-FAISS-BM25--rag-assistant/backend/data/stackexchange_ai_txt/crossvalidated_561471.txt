[site]: crossvalidated
[post_id]: 561471
[parent_id]: 
[tags]: 
Bias-variance trade-off in linear regression

As it’s understood, in the bias-variance trade-off, variance refers to overfitting of the model and it examines the variability of output predictions. Suppose we have a simple dataset with one predictor which relates perfectly linear with the target. For example, y = 3x. In that case, obviously the function that fits better is f(x) = 3x. Defining the loss function as the squared error loss, we must have Err(x) = E[(Y-f(x))^2] which implies a zero error. But, on the other hand, after decomposition, we have Err(x) = bias^2 + variance + irreducible error. Although the selected function fits perfectly the data, the variance of predictions is not null, so the error isn’t neither following this formula. And, even more, if the relationship between the input and the target is perfectly linear (and later we check it’s also in test data), there’s no overfitting and, otherwise, variance is not null. What’s wrong in this reasoning? Thanks in advance. Edit: As Raymond says, I was confused because of the article “Understanding the bias-variance trade-off” in Towards Data Science. They decompose the MSE into variance and bias, through the next expression: But, according what Raymond says, that wouldn’t be right. In fact, it’s not about the variance of output model itself (as it’s understood according the second term of the sum), but the variance among different predictions got in several samples of the original dataset.
