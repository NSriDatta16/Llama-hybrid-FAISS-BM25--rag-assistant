[site]: crossvalidated
[post_id]: 489676
[parent_id]: 489666
[tags]: 
Good questions, but they do not have simple answers. When we have more than one predictor, things become much more complicated. Even more complicated when there is some correlation/relationship between the predictors. Note that if $x_1$ and $x_2$ are moderately to strongly correlated, then there will a moderate to strong relationship between $x_1^2$ and $x_1 \times x_2$ . Which means that your model may not be able to tell the difference between a quadratic relationship with $x_1$ and an interaction effect between $x_1$ and $x_2$ . Depending on the goal of your analysis, it may not matter which one you use (predictive modeling where you just want to predict a new case and your training data is representative of the population of interest). But other cases (causal inference, really understanding what leads to these relationships) will be very different between the models that the computer cannot distinguish between. In those cases you may need to depend on the science behind the data to decide what makes the most sense, or resort to more formal experimentation where you control the predictor values and remove the natural relationships. As you have more predictors and more relationships between them it becomes likely that the simple 2 variable relationships will be different from the relationship when including multiple variables. You will need to be guided by the science behind the data and the goals of the analysis. There is a famous quote by Box: "All models are wrong, some models are useful". Whether you use polynomials, splines, etc. these are all approximations to some underlying truth. You need to use your knowledge and experience to determine what the models are telling you. Sometimes we fit things like splines, then look at the relationship and see that it looks like a particular transformation, then if that transformation makes sense with the science, refit the model using the transform. For visualizing if the relationship is linear or not while correcting for other variables, do a search on the phrases "partial residual plot" and "added variable plot". These are primarily for linear regression, but with some practice (and enough data) they can also be suggestive for logistic regression models. Your approach of binning a predictor is one approach to look for non-linearity, another approach is to fit a model that is linear in the predictor of interest, then refit with some type of curvature (splines are good, but not the only option) and compare the fits of the 2 models. You can use a formal full-reduced model test to compare them, but I prefer using tools like AIC, comparing the predictions, or other measures of goodness of fit to decide rather than the p-value from the full-reduced model test. Your questions illustrate why statistics requires people with knowledge beyond a memorized formula who can do background research and reason out what makes sense.
