[site]: crossvalidated
[post_id]: 616124
[parent_id]: 616123
[tags]: 
Classify everything as positive. You now are 100% sure you do not misclassify any positives. Problem solved. I would thus recommend you be a little more detailed on the costs of misclassification, where you will likely need to keep subsequent decisions made on the basis of the classification in mind. The beginning of this answer may be helpful. "Does not misclassify any positives" is an extremely hard standard to reach. Real life always contains edge cases and Black Swans. "Zero defects" sounds good on paper, but in practice it will mean that your sample will need to encompass the entire population, because you can never be sure that one of the data points you did not sample will throw off your model. Once you have a good notion of the costs of misclassifications, you can start simulating with different sample sizes and recording the resulting costs your model causes. Beyond some sample size, these costs will hopefully stay more or less flat. Weigh this sample size against the costs of collecting and processing data. There is likely no closed formula that tells you the required sample size, because it depends hugely on your data - if you have a predictor that reliably tells you which class an instance belongs to, you need no sample size at all. Conversely, your problem may be so noisy that the cost you want to reach is simply not achievable: How to know that your machine learning problem is hopeless?
