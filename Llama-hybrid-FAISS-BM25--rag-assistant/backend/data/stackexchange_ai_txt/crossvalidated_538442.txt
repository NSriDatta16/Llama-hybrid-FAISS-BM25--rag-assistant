[site]: crossvalidated
[post_id]: 538442
[parent_id]: 464636
[tags]: 
I think it is worth making the distinction between performance evaluation and model selection criteria. In performance evaluation you want to know how well your system is likely to perform in operation, based on the data you have available now. When evaluating performance, you need to use the metric that is appropriate for your task. So if you have a classification problem (i.e. you do need to make a hard "yes"/"no" decision) and the false-positive and false-negative costs are the same, then classification accuracy is an appropriate performance evaluation criterion. Obviously you would probably want to evaluate the uncertainty of the performance estimate, perhaps using bootstrap replication. For a model selection criterion, on the other hand, where we want to choose between competing methods, accuracy might not be a good criterion, even if it is the primary quantity of interest. The problem is that accuracy is brittle, if you ran the experiment again, but with a different sample of training data, you might get a slightly different decision boundary that gave a very different accuracy on the same test data. In this case, because the margins on the first classifier are quite small, it is more likely that the decision boundary will shift if it is trained on a different sample of data than the second one. Likewise, if the classifier is evaluated on a different set of test data, the first classifier is more likely to make mistakes than the second. As the margins are small, the new test data has to be only slightly different to the old data near the decision boundary for there to be a misclassification. Thus the Brier score is rewarding the second classifier for its greater stability. This is important as we want a classifier that will perform well in operation, not just on this particular test set. I use least-squares support vector machine (and kernel logistic regression) a fair bit, and you need a model selection criterion for tuning the hyper-parameters. The obvious thing to do is to use cross-validation accuracy as the selection criterion for problems where accuracy is the quantity of primary interest. However it is generally better to use the cross-validated Brier score (or equivalently PRESS criterion in my case), which is less brittle. You can get better accuracy from the final model by using a proper scoring rule as the model selection criterion. I learned this from taking part in a valuable "Performance Prediction Challenge" hosted at a conference, where you had to provide not just the predictions, but also an estimate of how accurate (balanced accuracy in this case) your model would be on the test data. So this is based on experimentation and experience - challenges like these are a good place to find out what actually works and what doesn't as there are no straw men or "operator bias" issues in the evaluation. The paper is here FWIW (preprint here ).
