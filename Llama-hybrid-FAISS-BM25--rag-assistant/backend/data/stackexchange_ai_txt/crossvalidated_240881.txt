[site]: crossvalidated
[post_id]: 240881
[parent_id]: 240871
[tags]: 
They are very similar, and different people in different community may have different definition. The following answer is based on my understanding. In numerical analysis framework, "curve fitting" is often used to describe interpolation , where the ultimate goal would be trying to minimize the "training loss", i.e., the loss for all seen data points. And there is no notion about "over-fitting", which means if the model can perfectly pass though all the data points, the model is perfect. On the other hand, "function approximation" may be more used in "machine learning" community, where just like any other learning problems, there are samples from the function (training data), and there are "ground truth function" or "hold out data for validation", therefore we may need to consider the "over-fitting". Where, perfectly predict seen data may not be good enough.
