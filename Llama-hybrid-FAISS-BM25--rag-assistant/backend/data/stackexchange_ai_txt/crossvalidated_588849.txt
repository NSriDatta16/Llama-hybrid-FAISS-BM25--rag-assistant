[site]: crossvalidated
[post_id]: 588849
[parent_id]: 
[tags]: 
Skip gram model and negative log loss likelihood

I recently just started learning about NLP and word representation. I have been trying to implement the negative log loss likelihood function but am having some trouble with it and would like to ask some questions about negative log loss likelihood. Here is what I have look into: Given 2 matrices of $V\times N,$ one for the center word $v$ and one for the context word $u$ , and k pairs of training dataset (each pair has one center word $w_c$ and one context word $w_o$ ), I have to find the likelihood using this formula: $$\prod_{(w_c,w_o)}\frac{\exp\left(u_o^Tv_c\right)}{\exp\left(\sum_{(w_c,w_k)}u_k^Tv_c\right)}$$ In my own words, I think what this formula is saying is that for a pair of training data, get the exp of the dot product between the center word and the context word, then divide it by the sum of the exp of the dot product of the current center words and all other words in the vocab (in this case there will be V vocabs). The exp here is so that the numbers are converted to probability. This will give me the probability of $w_o$ appearing given $w_c$ Then do this to all k pairs of training data and multiply them together, and it will give me an estimate of how good my current matrices of center and context words are doing. We can then apply log to it, transforming it from product to summation, then apply negation to it. After simplifying it, we will get: $$-\sum_{(w_c,w_o)}\left(u_o^Tv_c-\ln\sum_{(w_c,w_k)}\exp\left(u_k^Tv_c\right)\right)$$ where $u_o^Tv_c$ is the dot product between the center word and the context word, and $\sum_{(w_c,w_k)}\exp\left(u_k^Tv_c\right)$ is still the sum of the exp of the dot product of the current center words and all other $V$ words in the vocab. 1.Is what is described on the top correct? Is the simplification correct? 2.Why dot product? I read somewhere that states if two vectors are similar to each other, the dot product will be bigger. Is this true and is that the reason why we use dot product? 3.In some other formulas for negative log loss likelihood, there are another formula, $-\sum^{2m}_{j=0,j\neq m} u^\top_{c-m+j} v_c + 2m \log \left( \sum^{|V|}_{k=1} \exp(u^{\top}_k v_c) \right)$ , I think the latter part is the sum of the exp of the dot product of the current center words and all other words in the vocab getting dragged out of the bigger summation. However, the first part skipped one word (which I think is the center word?). What is the reason for that, and did the simplified function on top take account of that? 4.There is also something similar to this called negative sampling, in which calculating the dot product to the center word to ALL the other V words, we find x amount of words that are not in the context words and use those words instead. Why does this work (or to what extent will this work) and how should one be deciding the optimal x in this case?
