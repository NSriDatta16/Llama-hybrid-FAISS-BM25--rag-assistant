[site]: crossvalidated
[post_id]: 559031
[parent_id]: 559028
[tags]: 
Typically we assume we see $n$ i.i.d samples of the form $(X_1, \dots, X_p, Y)$ from some joint distribution. Suppose we think we're observing enough meaningful covariates $X_1 ,\dots, X_p$ that $Y$ is truly a deterministic function of them, so $Y = f^{*}(X_1, \dots, X_p)$ for some function $f^{*}$ . This is distinct from the more typical additive model assumption that $Y = f^{*}(X_1, \dots, X_p) + \epsilon$ where $\epsilon$ is some independent noise. Observing the data we have, we want to pick a function $f$ in some class of functions $\mathcal{F}$ such that $f(X_1 , \dots, X_p) \approx Y$ for new (not just training) samples. If again $\mathcal{F}$ is a parametric class, this is exactly a parametric prediction problem. The key difference is the randomness now solely comes from whatever training sample you drew, as the $f$ you choose will depend on this sample. Overfitting will not be a result of overfitting to noise, but fitting a function $f$ that matches $f^{*}$ on the training data but does not match $f^{*}$ for new samples. If you want to completely remove randomness from the problem, you could imagine there's a fixed set of potential inputs and outputs $\{(x^{(i)}_1, \dots, x^{(i)}_p, y^{(i)}) \}_{i=1}^N$ (we could generalize to an infinite collection) that you're interested in and you want to find a function $f$ such that $f(x_1^{(i)}, \dots, x_{p}^{(i)}) = y^{(i)}$ for all $i$ . The caveat is you only get to see $y^{(i)}$ for a subset of the fixed points. Your goal would likely to be to find a function $f$ that satisfies $f(x_1^{(i)}, \dots, x_{p}^{(i)}) = y^{(i)}$ for all the $i$ that you observe $y^{(i)}$ for, and is additionally constrained (e.g. it is smooth or $L$ -lipschitz) in a way that leads you to think it'll have the desired property for the data points you didn't get to observe $y^{(i)}$ for as well. To answer your final question, there are probably some situations, particularly areas where deep learning has been successful (like natural language process and computer vision), where we have "gathered" all the covariates needed to predict $Y$ deterministically. These are often deemed "high signal" tasks. But in many cases, it's often unknown to us what these covariates are/should be included in the model, and extensively searching for and including such covariates will likely result in our model learning a bunch of spurious, not useful correlations that won't generalize outside the training data (the reasoning for why is identical to $p$ -hacking). If the question is further what is the advantage of one approach (including randomness in your quantitative modeling) vs. the other (not including randomness in your quantitative modeling), making stochastic assumptions puts you in a statistical framework and allows you to do inference. You can make predictive intervals for new samples, confidence intervals for parameters, get guarantees about convergence from empirical risk minimization etc. Of course, these results only hold if the stochastic assumptions are correct, something which is often not true or hard to affirm (depending on the strength of the assumptions). Without the stochastic assumptions there's no notion of such things, but we don't have to worry about our stochastic assumptions holding.
