[site]: datascience
[post_id]: 113757
[parent_id]: 113698
[tags]: 
The problem with cosine similarity is that is a one-to-one alignment, that is, each value in one vector is aligned with one value in the other vector; the same holds for the Euclidean distance. Dealing with unstructured data , such as time series, videos, audio, etc. has its shortcomings, which in general is more challenging to handle; I encourage you to read also about structured data to have a clear idea between the two philosophies. For your case, the time series case, there are other distance/similarity-based metrics such as Dynamic Time Warping (or, for brevity, DTW) which is a one-to-many alignment, and I would suggest you give it a try. DTW is very powerful and has its roots in dynamic programming; in particular, in the theory of algorithms on strings where given two strings what is the edit distance to convert one string to another, and vice versa. Then, there is the unsupervised ranking problem that you mentioned. Ranking could be univariate (i.e., without considering any interaction among variables) or multivariate . Univariate feature selection ranking methods are easier to implement, while multivariate ones are more challenging and, in general, boil down to computing a correlation matrix between variables, which in your case are time series, that is, there is no standard methodology for doing it. My argument here is general enough so that you could grasp the idea; the discussion could go on if you need other insights. Edit Moreover, another issue with one-to-one alignments is that both vectors must have the same length, whereas one-to-many methods do not present such an issue. Furthermore, a simple way to reduce the unstructured case to a structured case using the DTW approach so that you can use standard analytics is the following. Let the original dataset by a $m \times n$ matrix (i.e., $m$ rows and $n$ columns) where each entry is a time series of length, say, $N$ ; if the time series do not have the same length ( $N$ ) this is a not an issue as I have already observed previously. The idea now is to construct a structured dataset which is a new $m' \times n$ matrix, where, in general, $m' \neq m$ . In the degenerate case where you may have only one column, that is, the case of univariate time series, your original matrix is $m \times 1$ ; therefore, let $A_i$ , with $1 \le i \le m$ , be the $i$ -th time series in such degenerate case. The goal is to construct a $\frac{m\cdot(m-1)}{2} \times 1$ matrix whose entries are computed as follows: $$ DTW(A_i, A_j) \text{ for all } 1 \le i,j \le m \text{ with } i Observe that, the constraint $i enforces that the resulting number of values/rows are $\frac{m \cdot (m-1)}{2}$ . Now, moving from the degenerate case to the more general case where the original matrix was $m \times n$ , the new matrix computed as described will be $\frac{m\cdot(m-1)}{2} \times n$ which is a matrix of real numbers, and this is sufficient to have a structured dataset. On this new matrix/dataset you can apply any unsupervised feature selection method, either univariate or multivariate, to select $n'$ features, with $n' . Finally, if your goal is to train a machine learning model, you must know that deep neural networks are the to-go strategy on unstructured data, whereas decision trees, and their siblings (e.g., random forests, grandient boosted trees) are still dominant on structured data, as this paper points out. I hope that my edit gives you more perspective on the complexity when dealing with unstructured data.
