[site]: crossvalidated
[post_id]: 490646
[parent_id]: 490643
[tags]: 
Being Bayesian is not only about information fed through the prior. But even then: Where the prior is zero, no amount of data will turn that over. Having a full Bayesian posterior distribution to draw from opens loads and loads of ways to make inference from. It is easy to explain a credible interval to any audience whilst you know that most audiences have a very vague understanding of what a confidence interval is. Andrew Gelman said in one of his youtube videos, that $p$ is always slightly lower then $0.05$ because if it wasn't smaller then we would not read about it and if it was much smaller they'd examine subgroups. While that is not an absolute truth, indeed when you have large data you will be tempted to investigate defined subgroups ("is it still true when we only investigate caucasian single women under 30?") and that tends to shrink even large data quite a lot. $p$ -values tend to get worthless with large data as in real life no null hypthesis holds true in large data sets. It is part of the tradition about $p$ values that we keep the acceptable alpha error at $.05$ even in huge datasets where there is absolutely no need for such a large margin of error. Baysian analysis is not limited to point hyptheses and can find that the data is in a region of practical equivalence to a null hypotheses, a Baysian factor can grow your believe in some sort of null hypothesis equivalent where a $p$ value can only accumulate evidence against it. Could you find ways to emulate that via confidence intervals and other Frequentist methods? Probably yes, but Bayes comes with that approach as the standard. "But for large enough data, wouldn't the posterior just collapse to the MLE" - what if a posterior was bimodal or if two predictors are correlated so you could have different combinations of e.g. $\beta_8$ and $\beta_9$ - a posterior can represent these different combinations, an MLE point estimator does not.
