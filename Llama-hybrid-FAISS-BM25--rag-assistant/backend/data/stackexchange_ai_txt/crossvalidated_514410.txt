[site]: crossvalidated
[post_id]: 514410
[parent_id]: 
[tags]: 
Are SVM generalization bounds valid if the kernel is learned on a different dataset?

Suppose I have a training dataset with a binary label, and I do an 80/20 split on it. On the first 80% of the data, I train a deep learning embedding model that maps my data unto some higher dimensional space such the normalized distance between the centroids of each label group is maximized. I then apply this mapping to the remaining 20% and train an SVM on the mapped data (the kernel being the dot product of the outputs of the embedding NN). Question: Can I trust the generalization bounds of the SVM or have I violated some assumption of Statistical Learning Theory?
