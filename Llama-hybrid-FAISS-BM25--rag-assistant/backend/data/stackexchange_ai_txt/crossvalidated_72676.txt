[site]: crossvalidated
[post_id]: 72676
[parent_id]: 
[tags]: 
Causality in Time Series

I am reading an article which is trying to justify the need for causal inference in their inferential framework. The thought experiment is as follows: Suppose a statistician is asked to design a model for a simple time series $X_1,X_2,X_3,...$ and she decides to use a Bayesian method. Assume she collects a first observation $X_1 = x_1$. She computes the posterior probability density function (pdf) over the parameters $\theta$ of the model given the data using Bayes’ rule: $$p(\theta|X_1 = x_1) = \int\frac{p(X_1 = x_1|\theta)p(\theta)}{p(X_1 = x_1|\theta')p(\theta')}, $$ where $p(X_1 = x_1|θ)$ is the likelihood of $x_1$ given $\theta$ and p($\theta$) is the prior pdf of $\theta$. She can use the model to predict the next observation by drawing a sample $x_2$ from the predictive ￼pdf: $$p(X_2 = x_2|X_1 = x_1) = \int p(X_2 = x_2|X_1 = x_1,\theta)p(\theta|X_1 = x_1)d\theta,$$ where $p(X_2 = x_2|X_1 = x_1,\theta)$ is the likelihood of $x_2$ given $x_1$ and $\theta$. Note that $x_2$ is not drawn from $p(X_2 = x_2|X_1 > = x_1, \theta)$. She understands that the nature of $x_2$ is very different from $x_1$: while $x_1$ is informative and does change the belief state of the Bayesian model, $x_2$ is non-informative and thus is a reflection of the model’s belief state. Hence, she would never use $x_2$ to further condition the Bayesian model. Mathematically, she seems to imply that: $$p(\theta|X_1 =x_1,X_2 =x_2)=p(\theta|X_1 =x_1)$$ However I hardly believe that what this poor statistician should imply is: $$p(\theta|X_1 =x_1,\text{do}(X_2 =x_2))=p(\theta|X_1 =x_1)$$ Where "do(or set)" here comes from Pearl 's framework of causality which can be found here and here . Now am I right about this?
