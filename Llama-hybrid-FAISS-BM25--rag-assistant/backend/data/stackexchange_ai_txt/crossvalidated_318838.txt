[site]: crossvalidated
[post_id]: 318838
[parent_id]: 317969
[tags]: 
If computational resources are not a problem, then LOOCV (leave one out cross validation) is the optimal approach. That is, training 200 models, each predicting the one left out sample, like you are already doing. The reason for that is that cross-validation techniques generally overestimate the out-of-sample error, since they are trained on a smaller amount of samples, since you leave out a few observations. Thus, the model is undertrained with respect to all your available data, and will make less accurate predictions. The less data you use to train the model (the larger the average hold-out-sample is), the worse your estimator.
