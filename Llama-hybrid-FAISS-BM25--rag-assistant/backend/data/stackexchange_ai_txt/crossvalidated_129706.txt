[site]: crossvalidated
[post_id]: 129706
[parent_id]: 129650
[tags]: 
There (at least) two schools of thought on this. The individual pixel values do not carry much---if any---information about the image content. Instead, much of the information seems to be carried by interactions among the pixels. If you can do some feature engineering to generate features that capture these interactions, these features can then be fed to a support vector machine, decision tree, or other standard classification algorithm. The "best" features probably vary with the application, but you might want to consider: SIFT: Scale-Invariant Image Features (e.g., Lowe, 1999 ) HOG: Histograms of Oriented Gradients (e.g., Dalal and Triggs, 2005 ) The goal here is to generate features that are not sensitive to moderate amounts of translation, rotation, or scaling. For some problems, you may also know that some features (e.g., straight lines for OCR) carry valuable information. An alternative approach is to learn these features directly from the training data, using something like an autoencoder . This is particularly common in deep learning models and you may want to look at the work of Yann LeCun, Geoff Hinton, and Yoshua Bengio, among others. These features are typically fed directly into a neural network. These networks' structures usually help capture the relationship between nearby pixels, as in a convolutional neural network but you could presumably use the autoencoders' outputs for other types of classifiers (e.g., SVMs) too.
