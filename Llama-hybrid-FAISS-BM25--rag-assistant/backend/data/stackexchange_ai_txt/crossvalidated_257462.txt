[site]: crossvalidated
[post_id]: 257462
[parent_id]: 241645
[tags]: 
This answer is a follow-up to Sycorax' great answer , for readers who would like to see how dropout is implemented. When applying dropout in artificial neural networks, one needs to compensate for the fact that at training time a portion of the neurons were deactivated. To do so, there exist two common strategies: Inverting the dropout during the training phase: Scaling the activation at test time: The /p is moved from the training to the predicting code, where it becomes *p : These three slides came from lecture 6 from Standford CS231n: Convolutional Neural Networks for Visual Recognition .
