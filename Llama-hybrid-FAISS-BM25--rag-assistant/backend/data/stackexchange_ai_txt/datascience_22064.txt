[site]: datascience
[post_id]: 22064
[parent_id]: 22023
[tags]: 
There's this misunderstanding that deep learning is generally suitable if you have loads of data, which, judging from your comment, is your case. This is generally an inaccurate belief. Deep learning (including CNN and RNN), are complex models with thousands of parameters that are able to model complex relationships. Such relationships are generally "hidden" in vast amounts of data, but this is not always the case. You may have at hand data that are generated from a simple distribution, and as such it will not need a complex model to approximate, even if your sample size is huge. Here's a fabricated example: Let's say that you have the function y=a*x1 + b*x2 + c*x3 + d*x4 . This function entails linear relationships between all independent variables and the dependent variable, y. You could sample this function a million times and still you would have at hand data that could be approximated through linear regression. Coming to your case: to identify what kind of algorithm you would need, you first need to look at your data, perhaps through performing a statistical analysis. Then I would suggest to start simple. Try logistic regression, for starters. Is the model satisfactory in validation? If not, try perhaps decision trees. Or a shallow neural net. Always validate (you have loads of data so validation should be easy!). My (admittedly wild) guess is that your classification problem could be addressed with much simpler algorithms than DNN. But YMMV, of course. FWIW, here's another answer similar to your case . And another one .
