[site]: crossvalidated
[post_id]: 341971
[parent_id]: 
[tags]: 
How big should the sample be in my translation quality assessment study?

I'm doing my master's thesis on translation quality assessment of academic textbooks of a language pair. With respect to sampling, I'm encountering some questions. I'm assessing the academic textbooks of a given field. Out of about 70 translations, I have decided to sample about 35 books which are read the most and from different subsections of the field. According to my literature review the best unit of translation and therefore assessment is sentence. So, I have decided to to do stratified random sampling from the sentences of the books. I am an independent researcher, my time and resources are limited, and analyzing each sentence by applying the chosen model takes a lot of time. Here, the problem of generalizability comes to the surface. I overestimated the number of pages of the books. I assume that each book has 500 pages while in reality each book averagely has far fewer pages, maybe around 250. And again I estimate the number of sentences in each page the most possible: usually each page has 10 to 20, so I assume that each page has 20. Then, I multiply the number of pages and the number of sentences and the number of books, and reach the number of 300000 sentences. To limit the sample size, I've decided to set the confidence level at 90% And the margin of error at 10%. By doing so, out of 300000 sentences 68 sentences (that is, 2 sentences from each book selected absolutely randomly) would suffice. Is that right? Do you think this sample size is defensible? By the way, my advisor believes instead of this way, it is best to pick 3-6 pages from each book and assess them with a less detailed, less time-consuming model. In some respects he is right, but then the workload will be huge (at least, with the present model). I believe in finding the most from sentences, he believes n finding even less from a bigger unit of text (at least 1 single page).
