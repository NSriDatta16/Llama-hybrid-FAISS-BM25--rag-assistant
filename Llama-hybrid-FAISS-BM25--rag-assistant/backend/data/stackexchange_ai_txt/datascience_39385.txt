[site]: datascience
[post_id]: 39385
[parent_id]: 39364
[tags]: 
Here are a few ways, using some dummy data: In [1]: import numpy as np In [2]: a = np.random.randint(0, 10, (10,)) In [3]: b = np.random.randint(0, 10, (5, 10)) In [4]: a Out[4]: array([4, 1, 0, 6, 3, 3, 6, 6, 1, 8]) In [5]: b Out[5]: array([[9, 0, 6, 1, 1, 1, 4, 7, 4, 7], [5, 8, 8, 3, 4, 8, 7, 3, 0, 4], [2, 2, 5, 3, 9, 6, 1, 5, 8, 3], [2, 0, 4, 3, 5, 3, 3, 4, 3, 3], [3, 3, 6, 4, 7, 5, 8, 6, 7, 3]]) Because of the dimensions you asked for, in order to compute inner products (a.k.a. scalar products and dot products), we need to transpose the matrix b so that the dimensions work out. With a vector of length 10, numpy gives it shape (10,) . So it seems 10 rows and no columns, however it is kind of ambiguous. Numpy will essentially do what it has to in order to make dimensions work. We could force it into a (10, 1) vector by using a.reshape((10, 1)) , but it isn't necessary. The matrix has a defined second dimensions, so we have a shape (5, 10) . In order to multiply these two shapes together, we need to make the same dimensions match in the middle. This means making (10,) * (10, 5) . Performing the transpose on matrix reverses the dimensions to give us that (10, 5) . Those inner 10 s will then disappear and leave us with a (1, 5) vector. That all being said, we can use any of the following to get equivalent answers: The standard standard dot-product: In [7]: a.dot(b.T) Out[7]: array([174, 174, 141, 119, 190]) The convenient numpy notation: In [6]: a @ b.T Out[6]: array([174, 174, 141, 119, 190]) The efficient " Einstein notation ", a subset of Ricci calculus (I leave the interested reader to search online for more information): In [8]: np.einsum('i,ij->j', a, b.T) Out[8]: array([174, 174, 141, 119, 190]) Here as in the comments from shadowstalker : In [9]: np.array([np.dot(a, r) for r in b]) Out[9]: array([174, 174, 141, 119, 190]) If your matrices are of dimensions (100, 100) or smaller, then the @ method is probably the fastest and most elegant. However, once you start getting into matrices that make you wonder if you laptop will handle it (e.g. with shape (10000, 10000) ) - then it is time to read the documentation and this blog about Einstein notation and the amazing einsum module within numpy!
