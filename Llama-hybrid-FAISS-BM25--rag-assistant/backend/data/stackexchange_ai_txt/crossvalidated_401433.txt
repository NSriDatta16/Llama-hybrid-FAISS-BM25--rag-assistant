[site]: crossvalidated
[post_id]: 401433
[parent_id]: 
[tags]: 
Reinforcement Learning: Bellman Equations and relations for optimal (state)/value function?

This question is about (stationary) Markov Decision Processes in the case of discounted reward. My questions are: Why is $v^*(s) = \sup_{a \in A} Q^*(s,a)$ ? Why is $Q^*(s,a) = E[R_t|S_t=s, A_t=a] + \gamma \int_{S} p(s'|s,a) v^*(s) ds'$ ? Given that we have an optimal policy $\pi^*$ , why is $Q^* = Q^{\pi^*}$ ? NOTE: I am seeking mathematical rigorous proofs, not 'intuitive explanations' :-) NOTE: In case of equations nr. 1 and 2 I would prefer a proof that does not explicitly refer to an optimal policy. Thanks in advance!!! More info on the setup: Given a set of states $S$ , a set of actions $A$ and 'input' distributions $p(r|s',a,s)$ (the density value for a reward $r$ given that the agent just moved from state $s$ to state $s'$ using action $a$ ) $p(s'|a,s)$ (the density value for the underlying automata having chosen to actually move into step $s'$ given that the agent was in state $s$ and did choose action $a$ ) and leaving the term $p(a|s)$ variable (the policy, i.e. the rules according to which the agent moves across the state space) we define the vector space $V$ of bounded functions $v : S \to \mathbb{R}$ and $W$ of bounded functions $w : S \times A \to \mathbb{R}$ . Let us assume that the rewards are bounded so that every policy $\pi$ induces elements in $V$ and $W$ by putting $$G_t := \sum_{k=0}^\infty \gamma^k R_{t+k}$$ (a random variable) and $$v^\pi(s) := E^\pi[G_t | S_t=s]$$ and $$Q^\pi(s,a) := E^\pi[G_t | S_t=s, A_t=a]$$ where we use the quirky notation $E^\pi$ to say that this (factorization of the conditional expectation) was computed in an MDP that was setup with policy $\pi$ , i.e. $p(a|s) = \pi(a|s)$ . Here, the expressions are actually independent of $t$ , hence, we define these for $t$ arbitrary. We define $$v^*(s) := \sup_\pi v^\pi(s)$$ and $$Q^*(s,a) := \sup_\pi Q^\pi(s,a)$$ Now in many books and references, people give different relations between $v^*$ and $Q^*$ usually without a proof. I have managed to reduce all of them to the ones mentioned above. Things I already know: Assuming that we have a discrete optimal policy $\pi$ and the third equation I am able to show the second equation: Let this policy be $\pi$ then $v^* = v^\pi$ and $v^\pi(s) = Q^\pi(s,\pi(s))$ . Using the Bellman equation for $Q^\pi$ we obtain \begin{align*} Q^*(s,a) &= Q^\pi(s,a) \\ &= E[R_t|S_t=s,A_t=a] + \gamma \int_{s',a'} p(s',a'|s,a) Q^\pi(s',a') ds'da' \\ &= E[R_t|S_t=s,A_t=a] + \gamma \int_{s'} p(s'|s,a) Q^\pi(s',\pi(s')) ds' \\ &= E[R_t|S_t=s,A_t=a] + \gamma \int_{s'} p(s'|s,a) v^\pi(s') ds' \\ &= E[R_t|S_t=s,A_t=a] + \gamma \int_{s'} p(s'|s,a) v^*(s') ds' \end{align*} (because $p(s',a'|s,a)$ should be zero except for the case when $a'=\pi(s')$ ... (?)) I am also able to prove ' $\leq$ ' in eq. no. 1 (without any reference to an optimal policy!): For every policy $\pi$ , \begin{align*} v^\pi(s) &= \int_{a} p(a|s) E[G_t|S_t=s,A_t=a] da \\ &= \int_{a} p(a|s) Q^\pi(s,a) da \\ &\leq \int_{a} p(a|s) \left[\sup_{\tilde{a}} Q^\pi(s,\tilde{a})\right] da \\ &= \sup_{\tilde{a}} Q^\pi(s,\tilde{a}) \cdot 1 \end{align*} I don't know whether that helps but I also know it ... As in the case of value functions in $V$ , every policy $\pi$ induces an affine linear operator on $W$ by $L_\pi = r_\pi + \gamma P_\pi$ i.e. $$(r_\pi + \gamma P_\pi)(w)(s,a) = E[R_t|S_t=s,A_t=a] + \gamma \int_{s',a'} p(s',a'|s,a) w(s',a') ds'da'$$ $Q^\pi$ is the unique fixed point of $L_\pi$ and $Q^*$ is the unique fixed point of the operator $$\mathcal{L}(w)(s,a) := \sup_{\pi} (L_\pi w)(s,a)$$ (one proves these assertions just as in the case for $V$ , see for example the book by Puterman)
