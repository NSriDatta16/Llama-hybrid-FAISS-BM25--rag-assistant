[site]: crossvalidated
[post_id]: 212758
[parent_id]: 
[tags]: 
Distribution of posterior mean from different datasets

This question has originated from this question . Suppose we have the following simple setup, for $i = 1, \dots, n$ $$y_i \mid \mu \sim N( \mu, 1) \text{ and } \mu \sim N(0,1). $$ Then due to the nice conjugate setup the posterior distribution for $\mu$ is $$\mu \mid \mathbf{y} \sim N\left(\dfrac{\sum_{i=1}^{n} y_i}{n+1}, \dfrac{1}{n+1} \right). $$ Now suppose I do the following $B = 100$ times. Simulate $n = 1000$ from $y \mid \mu$. Draw $r = 2000$ samples from the posterior $X_1, \dots, X_r$ (no need for MCMC). Calculate the posterior mean and posterior standard deviation est_mu and sd_mu . So the code for this would look something like. r The value for mean(sd_mu) makes sense as it is roughly $\sqrt{1/1001}$, but why is sd(est_mu) also roughly the same? Question: How do you show that sd(est_mu) $\approx$ mean(sd_mu) ? Originally I thought that this was impossible, because the posterior mean for each of $j = 1, \dots B$ draw of the likelihood is $$\hat{\mu}_j = \dfrac{1}{r} \sum_{t=1}^{r} X_{r} \, \, \text{ where } X_r \sim \mu \mid Y_j$$ and thus the variance should be $\sigma^2/r$ where $\sigma^2 = Var(\mu|Y) = 1/1001$. But this is clearly not the case as witnessed in the above code. If however, I use the same data everytime, I get the expected result of the variance being $\sigma^2/r$. So clearly, the randomness in generating the data is getting transferred to the posterior mean estimates and this makes sense intuitively. I am just not able to show it by math. In addition, the OP on the original comment said that once you have the posterior means from $B = 100$ independent samples, this a just a sample from the posterior distribution, that is, $\hat{\mu}_i \sim \mu|Y$, and I am not able to show this result. So maybe a more general question is Question: Why is $\hat{\mu}_j \sim \mu|Y$?
