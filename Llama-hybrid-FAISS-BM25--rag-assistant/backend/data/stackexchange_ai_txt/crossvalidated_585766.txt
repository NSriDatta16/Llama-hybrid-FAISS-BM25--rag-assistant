[site]: crossvalidated
[post_id]: 585766
[parent_id]: 458075
[tags]: 
Your question is about the relationship between different metrics for the policy gradient methods. Let $v_\pi(s_0)$ be the state value of a starting state. Let $\bar{r}_\pi$ be the average reward (sorry that I am used to my own notations). Consider the discounted case. The gradient of $v_\pi(s_0)$ is $$\nabla_{\theta} v_\pi(s_0)=\mathbb{E}\big[\nabla_{\theta}\ln\pi(A|S,\theta)q_{\pi}(S,A)\big]$$ where $S\sim \rho_\pi$ and $A\sim \pi(s,\theta)$ . Here, $\rho_\pi$ is a special long-run distribution of the states (not stationary distribution). The gradient of $\bar{r}_\pi$ is $$ \nabla_{\theta} \bar{r}_{\pi}\approx \mathbb{E}\big[\nabla_{\theta}\ln\pi(A|S,\theta)q_{\pi}(S,A)\big] $$ where $S\sim d_\pi$ and $A\sim \pi(s,\theta)$ . Here, $d_\pi$ is the stationary distribution. It is clear that they are not the same because $S$ obeys different distributions. However, they are very similar. Moreover, if we consider the stochastic gradient (that is the value after removing the expectation), then their stochastic gradient are the same. In summary, maximizing the two metrics is not equivalent. However, people usually do not distinguish them. Details of the above two equations can be found in Theorem~9.2 and Theorem~9.3 in the book Mathematical foundation of reinforcement learning .
