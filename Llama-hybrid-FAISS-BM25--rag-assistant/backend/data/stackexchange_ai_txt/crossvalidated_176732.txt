[site]: crossvalidated
[post_id]: 176732
[parent_id]: 176715
[tags]: 
This has been written about on the site in detail. The lasso is meant to be a complete solution and it is completely inappropriate to use it to select features that are fed into a naive method that does not penalize for the context of having tortured data to find the features. You are also making an implicit assumption that the lasso finds the "right" predictors. Should you bootstrap the entire process you may be sorely disappointed to learn that in your case the features selected have a great deal of randomness in them. This is more true in the case of co-linearities. If you want to emphasize parsimony over predictive accuracy, then model approximation (also called pre-conditioning ) may be useful. Here one uses the best available prediction method, e.g. penalized maximum likelihood estimation with a quadratic penalty, i.e., ridge logistic regression. Then $X\hat{\beta}$ from that model is approximated using stepwise regression or recursive partitioning, etc. The approximate model inherits the proper amount of shrinkage from the full model. The approximate model may be chosen to yield $R^{2} = 0.95$ against the gold standard linear predictor.
