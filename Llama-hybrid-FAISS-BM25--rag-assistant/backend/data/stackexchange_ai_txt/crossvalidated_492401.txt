[site]: crossvalidated
[post_id]: 492401
[parent_id]: 492148
[tags]: 
In this situation we have repeated measures within countries. The research question concerns whether variant B performs better than A globally -- from comments in the OP This is a question of inference. When the response differs accross countries this gives rise to non-independence. This must be accounted for otherwise inferences will be wrong. Maybe one performs better in some countries and the other in some other countries, but one of them will be better when you average everything out. -- from comments in the OP ...this is incorrect. The point estimate may be unbiased but the standard errors will be wrong. This means, if there is truly no difference between A and B, then you will not know the correct probability of having observed these data, or data more extreme, and this is precisely the probability that you will be interested in. We can see this from a simple simulation. We simulate 10,000 observations in total, from 25 countries ( C ), with random responses to the treatment factor ( trt ), where we use an effect size of 0.05 for trt library(tidyverse) library(lmerTest) set.seed(15) N $reTrms$ Zt)) betas % summary() ## Fixed effects: ## Estimate Std. Error df t value Pr(>|t|) ## (Intercept) -0.04930 0.62200 26.04703 -0.079 0.937437 ## trt 0.07032 0.01980 9972.83472 3.552 0.000384 *** So we find that if there is actually no difference between A and B, the probability of finding the effect size of 0.07 or larger is 0.00038. However, if we ignore the non-independence, we obtain: lm(Y ~ trt, dt) %>% summary() ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 0.16261 0.04705 3.456 0.00055 *** ## trt 0.01087 0.06640 0.164 0.86997 which is a vastly different, and incorrect, conclusion ! In order to satisfy ourselves that this is not just a result of random variation, we can run the above code 200 times, with a different seed each time, extract the point estimates and p-values: # Monte Carlo simulation of the above n.sim $Y reTrms$Zt)) b $coefficients[2,1] # point estimate for mixed model pv.lmm[i] coefficients[2,5] # p value for mixed model pe.lm[i] $coefficients[2,1] # point estimate for linear model pv.lm[i] coefficients[2,4] # p value for linear model } hist(pe.lmm ) mean(pe.lmm) ## [1] 0.0522676 hist(pe.lm) mean(pe.lm) ## [1] 0.05386004 So we can see that the point estimates are unbiased in both cases. However, when it comes to the p-values: hist(pv.lmm ) mean(pv.lmm) ; median(pv.lmm) ## [1] 0.06440553 ## [1] 0.007470051 hist(pv.lmm) mean(pv.lm) ; median(pv.lm) ## [1] 0.406798 ## [1] 0.3322566 Thus we see that the mixed model successfully returns low probabilities of obtaining these or larger point estimates if there was actually no effect of trt , the bast majority of the time. On the othe hand the linear model fails to do so. In particular, with the often-used 0.05 threshold we find: mean(pv.lmm That is, the mixed model finds a "significant" result at the 0.05 level 76% of the time, whereas the linear model, only does so 17.5% of the time.
