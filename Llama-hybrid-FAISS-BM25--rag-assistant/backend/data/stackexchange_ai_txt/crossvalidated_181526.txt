[site]: crossvalidated
[post_id]: 181526
[parent_id]: 7836
[tags]: 
Whuber's answer is excellent, but it is worth adding emphasis to the fact that a statistical model need not resemble the data generating model in every respect to be an appropriate model for inferential exploration of data. Liu and Meng explain that point with great clarity in their recent arXived paper ( http://arxiv.org/abs/1510.08539 ): Misconception 1. A probability model must describe the generation of the data. A more apt description of the model’s job (in inference) is “Such and such probabilistic pattern produces data which resemble ours in important ways.” To create replicas (i.e., controls) of the Mona Lisa, one does not need to bring da Vinci back to life — a camera and printer will suffice for most purposes. Of course, knowledge of da Vinci’s painting style will improve the quality of our replicas, just as scientific knowledge of the true data generating process helps us design more meaningful controls. But for purposes of uncertainty quantification, our model’s job is to specify a set of controls that resemble (D,$\theta$). Nowhere is this point clearer than in applications involving computer experiments where a probabilistic pattern is used to describe data following a known (but highly complicated) deterministic pattern (Kennedy and O’Hagan, 2001; Conti et al., 2009). We need a descriptive model, not necessarily a generative model. See Lehmann (1990), Breiman (2001) and Hansen and Yu (2001) for more on this point.
