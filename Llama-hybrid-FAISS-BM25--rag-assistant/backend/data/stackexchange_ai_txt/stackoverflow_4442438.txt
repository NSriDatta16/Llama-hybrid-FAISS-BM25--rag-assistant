[site]: stackoverflow
[post_id]: 4442438
[parent_id]: 2017732
[tags]: 
An idea of LSA is based on one assumption: the more two words occur in same documents, the more similar they are . Indeed, we can expect that words "programming" and "algorithm" will occur in same documents much more often then, say, "programming" and "dog-breeding". Same for documents: the more common/similar words two documents have, the more similar themselves they are . So, you can express similarity of documents by frequencies of words and vice versa. Knowing this, we can construct a co-occurrence matrix , where column names represent documents, row names - words and each cells[i][j] represents frequency of word words[i] in document documents[j] . Frequency may be computed in many ways, IIRC, original LSA uses tf-idf index. Having such matrix, you can find similarity of two documents by comparing corresponding columns. How to compare them? Again, there are several ways. The most popular is a cosine distance . You must remember from school maths, that matrix may be treated as a bunch of vectors, so each column is just a vector in some multidimensional space. That's why this model is called "Vector Space Model" . More on VSM and cosine distance here . But we have one problem with such matrix: it is big. Very very big. Working with it is too computationally expensive, so we have to reduce it somehow. LSA uses SVD technique to keep the most "important" vectors. After reduction matrix is ready to use. So, algorithm for LSA will look something like this: Collect all documents and all unique words from them. Extract frequency information and build co-occurrence matrix . Reduce matrix with SVD. If you're going to write LSA library by yourself, the good point to start is Lucene search engine, which will make much easier steps 1 and 2, and some implementation of high-dimensional matrices with SVD capability like Parallel Colt or UJMP . Also pay attention to other techinques, which grown up from LSA, like Random Indexing . RI uses same idea and shows approximately same results, but doesn't use full matrix stage and is completely incremental, which makes it much more computationally efficient.
