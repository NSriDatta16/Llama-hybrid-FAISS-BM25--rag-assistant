[site]: datascience
[post_id]: 31154
[parent_id]: 31153
[tags]: 
You can indeed use the ability of recurrent network like LSTM to handle the varying length problem. But unfortunately if you use keras or Tensorflow, all the Tensor must have the same length in a batch. What you can do : Pad all the sequences with an unused value (typically 0) so that all the sequences have the same length. Use a mask layer just after the Input layer : the 0s will not be taken into account for training the model. Bellow an example using the Keras framework (not complete, not tested) : from keras.models import Model from keras.layers import Input, LSTM, Dense input = Input(shape=[None, nb_of_features_per_timestep]) layer = Masking(mask_value=0)(input) layer = LSTM(50, return_sequences=False)(layer) output = Dense(nb_classe)(layer) model = Model(input,output) You can also use the bucketing trick : you group your dataset into "buckets" of same size (approximately). You can then create batches of same size. You don't need the Masking layer anymore. You can also use fully convolutional network. Those networks can also take Input with different size. I let you search if you want to use that. Concerning encoder/decoder : those model can maybe help you but I think that you will have to use one of the tricks that I mentioned. If you don't understand how RNNs work, I recommend you to dig a little into the subject before.
