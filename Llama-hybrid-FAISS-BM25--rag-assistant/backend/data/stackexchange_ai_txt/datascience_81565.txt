[site]: datascience
[post_id]: 81565
[parent_id]: 
[tags]: 
Is it possible for a model with a large amount of data to perform very well and reach an extremely low cost within a single epoch?

I am working on a project to detect human awareness levels using this dataset . I preprocessed the video data as the following: Convert video to frames(taking a frame every 5 seconds. Rotate the frames to be vertical. Apply OpenCV DNN to extract the faces from the images. Split the data into 90% train, 5% validation and 5% test. All in the dataset has a size of about 570,000 images. I am using the model on a mobile device so I used transfer learning with MobileNetV2. The model classification is extremely good but it feels odd seeing it do so well and reach a very low loss so fast. Is this even possible on a dataset this big? I am feeling that I did something wrong cause when I try to use the model on the mobile device with Tensorflow.js it does not perform well at all. After doing some research I realized that I should be using a model that combines a CNN and a LSTM as this is video data. But I am bit strapped for time to redo the whole preprocessing of the data to convert the images into a sequence of frames and then do the training once more. What I was planning to do was make an average of the predictions on the mobile device to improve the accuracy there but I am wondering if I messed up the process anywhere.
