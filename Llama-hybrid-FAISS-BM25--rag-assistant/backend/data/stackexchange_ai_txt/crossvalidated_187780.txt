[site]: crossvalidated
[post_id]: 187780
[parent_id]: 
[tags]: 
Problem: cannot calculate standard deviation!

I have a following question and struggling a bit. An insurance company holds a portfolio of 100 insurance cover plans. The average cost of each claim against the plan is 20 dollars with a standard deviation of 5 dollars. The company wants to set aside amount of money X, such that 95% of all claims are covered, how much would X to be? Is it correct that first I do the following steps: 20*100 = 2000; to find a mean for distribution 5*100 = 500 to find the standard deviation of this distribution? or should I calculate variance first: 5*5 = 25, then 25*100 = 2500, and then sqrt(2500) = 50 to find the stand.deviation?
