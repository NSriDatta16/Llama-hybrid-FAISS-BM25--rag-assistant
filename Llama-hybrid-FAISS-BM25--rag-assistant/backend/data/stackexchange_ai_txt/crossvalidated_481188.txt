[site]: crossvalidated
[post_id]: 481188
[parent_id]: 481165
[tags]: 
Model comparison is essential to the development of a useful predictor. There a various aspects that come into play. What you are trying to predict, is customer response just like sales predictor . Logistic regression and random forests both estimate posterior probabilities by Bayes rule $ \hat{P}(R \mid M) $ with $R$ being the response and $M$ the marketing communication action. See other question for an explanation of Bayes rule. When comparing the predicted outcomes the first approach is generally to threshold your posterior probabilities $\hat{P}(R \mid M) > \hat{P}(\neg R \mid M) \Rightarrow R=Response$ whereas $\hat{P}(R \mid M) \leq \hat{P}(\neg R \mid M) \Rightarrow \neg R=No Response$ . You first compare the number of correct predictions on your training set from logistic regression with that of random forests. What are the fractions of correct predictions? The fraction of correct predictions is called the accuracy and it is binomially distributed. You can compare the performance of the two accuracies using a $\chi 2$ test for two independent samples. The probabilities $\hat{P}(R \mid M)$ are estimates and they have an underlying distribution too. The computation of $\sigma^2 [\hat{P}(R \mid M)]$ is complex but they are known for some types of predictive classifiers (specifically for linear and quadratic discriminant analysis). A final thing is that you are performing model comparison. A general strategy is to use n-fold cross validation for that. You don't want your models to overgeneralize so eventually the accuracy needs to be computed on an independent test set as well.
