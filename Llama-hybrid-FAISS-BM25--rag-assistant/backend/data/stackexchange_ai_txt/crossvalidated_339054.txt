[site]: crossvalidated
[post_id]: 339054
[parent_id]: 
[tags]: 
What values should initial weights for a ReLU network be?

For a standard feed-forward Neural Network, what range should my initial weights fall under if I'm planning to use Rectified Linear Unit as an activation function? A mathematical justification for the recommendation given would also be helpful. I've read this post regarding initialisation of weights, however it assumes Sigmoid as the activation function. In another post's comments, someone recommends choosing between (0,0.01) or (0, n**(-0.5)) where " 'n' is is the number and length of paths from the current layer". Can anyone confirm or suggest methods?
