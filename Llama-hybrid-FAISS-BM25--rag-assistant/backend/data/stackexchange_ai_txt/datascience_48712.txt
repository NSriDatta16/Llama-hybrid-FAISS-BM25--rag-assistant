[site]: datascience
[post_id]: 48712
[parent_id]: 48709
[tags]: 
This is expected and is not related to SMOTE sampling. The computational complexity of non-linear SVM is on the order of $O(n^2)$ to $O(n^3)$ where $n$ is the number of samples. This means that if it takes 0.8 seconds for 7.5K data points, it should take [3, 48] minutes for 115K, $$[(115/7.5)^{2} \times 0.8, (115/7.5)^{3} \times 0.8]s=[3,48]m,$$ and from 16 hours to 175 days, 11 days for $O(n^{2.5})$ , for 2M data points. You should continue using sample sizes on the order of 100K or less. Also, it is fruitful to track the accuracy (or any other score) as a function of samples for 1K, 10K, 50K, and 100K samples. It is possible that SVM accuracy stops improving well before 100K, therefore, there will be not much to lose by limiting the samples to 100K or less.
