[site]: crossvalidated
[post_id]: 252779
[parent_id]: 
[tags]: 
When implementing TD(Î») with eligibility traces, why do we update towards the same ?

When using $TD(\lambda)$ in a finite state-action space, the update looks like $$ \forall s,a \in S, A(Q(s, a) = Q(s, a) + \alpha \delta e_t(s, a))$$ where $$\delta = R + \gamma Q(s_{t + 1}, a_{t + 1}) - Q(s_t, a_t)$$ This process updates every state-action towards the delta between the observed TD(0) return and the current state. But looking at $TD(\lambda)$ from the forward view, $$\delta = R + (1 - \lambda)\sum_{n = 1}^{\infty} \lambda^{n - 1} \gamma^n Q(s_{t + n}, a_{t + n}) - Q(s_t, a_t)$$ where the delta is between the $\lambda$-return and the current state. In the backwards view it seems like this "current" state is equivalent to the state-action being updated, not the current state of the agent. Shouldn't the update be $$ \forall s,a \in S, A(Q(s, a) = Q(s, a) + \alpha \delta(s, a) e_t(s, a))$$ where $$\delta(s, a) = R + \gamma Q(s_{t + 1}, a_{t + 1}) - Q(s, a)$$ This would mean computing $\delta(s, a)$ for every state-action along with the eligibility trace.
