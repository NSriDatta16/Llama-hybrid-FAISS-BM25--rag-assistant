[site]: datascience
[post_id]: 123765
[parent_id]: 
[tags]: 
Can LLM fine-tuning be used to improve a language?

I'm Danish, and with all the excitement around open LLM models, I'm feeling a little left out. Take Llama 2, for example - it was trained on a very small Danish dataset. Just enough to learn the words and be terrible at Danish. My question is, can fine-tuning (with LORA or QLORA) be used to improve the Danish skills of an existing pretrained language model? Llama 2, for example, knows Danish words, and so (one would hope) perhaps that's why they exposed it to Danish at all? Full-on pretraining isn't financially realistic for an individual, and (to my knowledge) we don't have a good open model in Danish yet. We do however have a pretty large open source training data set. But whoever is using that (if anyone) they're not sharing.
