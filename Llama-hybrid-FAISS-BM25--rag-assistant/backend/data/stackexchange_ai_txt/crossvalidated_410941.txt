[site]: crossvalidated
[post_id]: 410941
[parent_id]: 
[tags]: 
Why disentangled factors learnt by unsupervised models could meet human criterion of high-level or semantic meaning?

Multiple generative models (beta-VAE, InfoGAN, Glow, etc.) claim the capabilities to disentangle and control high-level factors in their generated samples. For instance, for a VAE decoder network F(z) mapping z to a sample of image/audio/whatever, in which z is a, say, n-dimensional latent code, then the models often demonstrate that manipulating some dimension of z would control the high-level features of generated sample, like emotion of face, speed of speech, etc. (See the picture in the paper as a sample) Now the question comes that: why the disentangled factors fall to match those human-interpretable features? Indeed, VAE-like models encouraged disentanglement by enforcing Gaussians with diagonal covariance in latents, however it seems that those disentanged dimensions are not necessarily ensured to impact human-interpretable characteristic separately in the generated samples. It is somehow more "natural" if the dimensions could only control some characteristics in generated samples that are difficult to be concisely interpreted (e.g. whether the photo is taken under sunlight), or could control a set of multiple characteristcs (for each dimension separately). More, though some disentangled and interpretable factors (as demonstrated in the papers, like lighting of images and F0 of audios) are quite "basic", many other factors appear to be more high-level and require human interpretation, unlike other automatically emerged structures in NNs (e.g. edge detectors in trained CNNs). For example, human emotion (whether smile or not), azimuth, require human knowledge to extract from an image. It is even more weird when the models learn such factor unsupervised. Some of the issues could be interpreted as cognitive bias: we could always pick up a handful of meaningful&interpretable dimensions in the latents, leaving hundreds of meaningless dimensions away. But based on my coleagues and my own experience of training and using such models, I don't think that this bias could cover the entire case. So why the models learn to view the world (i.e. their training data) in the ways same with ours?
