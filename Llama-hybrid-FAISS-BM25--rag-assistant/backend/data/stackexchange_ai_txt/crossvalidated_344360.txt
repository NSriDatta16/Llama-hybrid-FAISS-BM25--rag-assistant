[site]: crossvalidated
[post_id]: 344360
[parent_id]: 344189
[tags]: 
How can you "draw samples from the posterior distribution" without first knowing the properties of said distribution? In Bayesian analysis we usually know that the posterior distribution is proportional to some known function (the likelihood multiplied by the prior) but we don't know the constant of integration that would give us the actual posterior density: $$\pi( \theta | \mathbb{x} ) = \frac{\overbrace{L_\mathbb{x}(\theta) \pi(\theta)}^{\text{Known}}}{\underbrace{\int L_\mathbb{x}(\theta) \pi(\theta) d\theta}_{\text{Unknown}}} \overset{\theta}{\propto} \overbrace{L_\mathbb{x}(\theta) \pi(\theta)}^{\text{Known}}.$$ So we actually do know one major property of the distribution; that it is proportional to a particular known function. Now, in the context of MCMC analysis, a Markov chain takes in a starting value $\theta_{(0)}$ and produces a series of values $\theta_{(1)}, \theta_{(2)}, \theta_{(3)}, ...$ for this parameter. The Markov chain has a stationary distribution which is the distribution that preserves itself if you run it through the chain. Under certain broad assumptions (e.g., the chain is irreducible, aperiodic), the stationary distribution will also be the limiting distribution of the Markov chain, so that regardless of how you choose the starting value, this will be the distribution that the outputs converge towards as you run the chain longer and longer. It turns out that it is possible to design a Markov chain with a stationary distribution equal to the posterior distribution, even though we don't know exactly what that distribution is . That is, it is possible to design a Markov chain that has $\pi( \theta | \mathbb{x} )$ as its stationary limiting distribution, even if all we know is that $\pi( \theta | \mathbb{x} ) \propto L_\mathbb{x}(\theta) \pi(\theta)$. There are various ways to design this kind of Markov chain, and these various designs constitute available MCMC algorithms for generating values from the posterior distribution. Once we have designed an MCMC method like this, we know that we can feed in any arbitrary starting value $\theta_{(0)}$ and the distribution of the outputs will converge to the posterior distribution (since this is the stationary limiting distribution of the chain). So we can draw (non-independent) samples from the posterior distribution by starting with an arbitrary starting value, feeding it into the MCMC algorithm, waiting for the chain to converge close to its stationary distribution, and then taking the subsequent outputs as our draws. This usually involves generating $\theta_{(1)}, \theta_{(2)}, \theta_{(3)}, ..., \theta_{(M)}$ for some large value of $M$, and discarding $B If you already know the properties of your posterior distribution ... then what's the point of using this method in the first place? Use of the MCMC simulation allows us to go from a state where we know that the posterior distribution is proportional to some given function (the likelihood multiplied by the prior) to actually simulating from this distribution. From these simulations we can estimate the constant of integration for the posterior distribution, and then we have a good estimate of the actual distribution. We can also use these simulations to estimate other aspects of the posterior distribution, such as its moments. Now, bear in mind that MCMC is not the only way we can do this. Another method would be to use some other method of numerical integration to try to find the constant-of-integration for the posterior distribution. MCMC goes directly to simulation of the values, rather than attempting to estimate the constant-of-integration, so it is a popular method.
