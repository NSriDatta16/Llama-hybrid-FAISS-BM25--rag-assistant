[site]: crossvalidated
[post_id]: 390987
[parent_id]: 
[tags]: 
Determine input array that approximates a target output array from complex numerical simulation?

I believe the following problem is ideally suited to a machine intelligence approach, but am unsure where to start. I've used scikit-learn previously with downloaded datasets, but the following seems a more complex (and interesting) problem. I have a computational simulation that takes an input array and after computation (basically solving coupled ODEs), returns an equally sized output array. The simulation is complex (so can't be simply reversed analytically) but only takes 10s ms to execute. I'd like to find the input that gives an output as close as possible to a target list of values. i.e. I want to find the system input that gives a user-specified output. Solution idea: I could create a merit function to evaluate how far each output point is from the target (using some sort of least squares metric), for a test input. The input physically represents an audio signal (ie time-varying amplitude) so I expect it to be slowly varying, but this could be included in a merit function too - i.e. to penalise completely random waveforms that happen to give simulation outputs close to the target. Question: is there a way I can pass all this information to a machine learning type of algorithm for it to automatically generate the inputs and create test data, then learn from it? I imagine carefully crafted inputs would be far more efficient than me randomly generating a test set (which may not capture all possible inputs anyway). I appreciate there may not be a single unique solution, but I imagine machine learning could help to at least approximate something useful here? Does this sound a reasonable approach? Any advice / suggestions, both general or more targeted would be great thanks!
