[site]: datascience
[post_id]: 37263
[parent_id]: 37262
[tags]: 
First of all welcome to the community! About the question I would say there are misunderstandings. You say you calculate 4 features "per objects". It means that every data point (object) is described with 4 features. So the length of features are not different. Please note that I say this because "you calculate features for your samples/data points/objects" so every object here is a point in 4-dimensional space. If you mean something else please correct me. In other words, you have a data matrix with some rows (number of objects) and 4 columns. More general about the whole concept. No, this is not a valid approach to create standard data. In standard data the number of columns are always the same because you can not study objects if they are defined in different spaces. Many ML algorithms also work with standard data which means you can not use them if the size of feature vectors vary. More conceptual so to speak, the size of features you calculate for objects can not vary. How? I want to extract features from faces. I say eye color , distance between eyes and distance between ears . I have 3 features and for any person who comes to my study I calculate the same set of features. How can the size be different if the feature extraction process is standard? But in non-standard data you may end up with such a case. In this case you may standardize the features. E.g. as a baby example, imagine different graphs with different number of nodes and edges. You can describe each graph with its average degree , average path length and number of nodes . Then each graph is described with 3 features and you can feed it into an algorithm. But if you don't want to standardize the data (because it usually has loss of information) you need to find tailored analytic methods for which you need to explain more about your data. I hope it helped! Good Luck!
