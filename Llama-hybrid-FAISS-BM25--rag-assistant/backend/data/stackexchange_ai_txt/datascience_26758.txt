[site]: datascience
[post_id]: 26758
[parent_id]: 26751
[tags]: 
A couple of points I have since found myself: I was right in suspecting that self-training could be used for PU learning. In fact, I found the original paper on PU Learning , and indeed the paper is a variation on self-training. (Oddly enough, the original authors had Positive, Unlabeled and Negative examples!) The authors of this survey identify three families of methods: (i) two-step strategy (identify reliable negative examples in the unlabeled data and then use supervised learning), (ii) weight the positive and unlabeled examples, and estimate the conditional probability of positive label given an example (I believe this is akin to semi-supervised self-training ), and (iii) just treat the unlabeled data as highly noisy negative data. There are also some interesting loss functions to be used with neural networks (and I imagine could be adapted for gradient boosting) described in Table 1 of this paper .
