[site]: crossvalidated
[post_id]: 169537
[parent_id]: 
[tags]: 
Generative-Discriminative pairs: Naive Bayes and Logistic Regression

I'm trying to understand the something written in this paper . At the bottom of page 7: This means that if the naive Bayes model $$ p(y,\mathbf{x}) = p(y) \prod_k p(x_k|y) $$ is trained to maximize the conditional likelihood, we recover the same classifier as from logistic regression. Conversely, if the logistic regression model $$p(y|\mathbf{x}) = \frac{1}{Z(\mathbf{x})} exp\left\{\sum_k \lambda_k f_k(y,\mathbf{x}) \right\} $$ is interpreted generatively, as in $$p(y,\mathbf{x}) = \frac{exp\left\{\sum_k \lambda_k f_k(y,\mathbf{x}) \right\}}{\sum_{y', \mathbf{x}'}exp\left\{\sum_k \lambda_k f_k(y',\mathbf{x}') \right\}}$$ and is trained to maximize the joint likelihood p(y, x), then we recover the same classifier as from naive Bayes. I'm having trouble seeing why this is.The next page goes on to say ...consider the family of naive Bayes distributions. This is a family of joint distributions whose conditionals all take the "logistic regression form". But there are many other joint models, some with complex dependencies among $\mathbf{x}$, whose conditional distributions also have the logistic regression form. Aren't these statements contradictory? The conditioned-Bayes distributions are a subset of the logistic-regression-form distributions, so isn't it possible that maximizing over the latter yields a distribution not contained in the former?
