[site]: stackoverflow
[post_id]: 4472763
[parent_id]: 4472171
[tags]: 
The 2 salient facts I see here are: (1) the bottleneck is reads from your OLTP database due to a complex query (2) the data you need are somewhat old (yesterday) and not subject to change for the duration of the day. I automatically think of using a datamart or data warehouse for a situation like this. Overnight, you run SSIS jobs to refresh the datamart data, and then they are available for query when the execs arrive in the morning. The datamart may be denormalized in order to improve query performance--in fact, it is typically encouraged. I would be surprised if your query performance did not improve by at least an order of magnitude. An additional benefit of this approach is that you will have all the reporting and analysis capabilities of SSAS at your fingertips. There are execs who love being able to slice and dice the data in an analysis cube, so they might regard you as an IT rock star if you make this capability available to them. Yet another advantage is that the data mart can be provisioned and managed separately from your OLTP database, which means that other users of your web app will not notice slow performance while these queries are going on. A final advantage is that SSAS is really good at partitioning your data along known dimensions (for example, by year). If you do not have sufficient bandwidth to set up a separate datamart/data warehouse, you could certainly do something similar within the confines of your OLTP database. You would miss out on the SSAS capabilities and segregation of complex queries, but you could try to roll out the data mart in a future release. One approach within your existing DB would be to set up indexed views, but those may impose a serious runtime cost due to the necessity of updating the view with every related data update. Perhaps a better approach would be to simply create denormalized tables within your OLTP database with the specific mission of servicing these specialized queries. Just schedule a job to run every night to refresh the tables. You could even use SSIS for this, although it might be overkill if everything is happening within the confines of one DB. A SQL statement or statements will probably suffice. With either approach, I would suggest performing the calculations at the database level. I see no advantage to calculating these on the fly, as they will not change through the day. Populating the calculated columns should be done as part of the overnight processing.
