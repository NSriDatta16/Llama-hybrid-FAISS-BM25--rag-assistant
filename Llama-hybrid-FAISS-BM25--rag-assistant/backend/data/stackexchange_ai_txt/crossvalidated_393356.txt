[site]: crossvalidated
[post_id]: 393356
[parent_id]: 393344
[tags]: 
For a partial answer, mostly about why using "pre-trained" embeddings. In Word2Vec for instance, each word gets its embedding by looking at the context in which the words occur (i.e, by using the neighboring words). However, it has two major problems: 1) you can not achieve word disambiguation -the word jaguar will have the same embedding whether we are talking about the animal or the car (clearly that's wrong) and 2) which is linked to 1), your embeddings will not change depending on where they appear in the sentence at task time. Thus by using something like BERT, each word embedding is unique depending on the particular context at hand during the task to perform, rather than being a fixed representation such as Word2vec.
