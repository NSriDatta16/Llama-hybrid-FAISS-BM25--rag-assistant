[site]: crossvalidated
[post_id]: 553817
[parent_id]: 
[tags]: 
Why use a hash bucket to handle Out-of-vocabulary tokens in embedding layers?

For example, in the nnlm-en-dim128 model in thug ( https://tfhub.dev/google/nnlm-en-dim128/1 ). It says Small fraction of the least frequent tokens and embeddings (~2.5%) are replaced by hash buckets. Each hash bucket is initialized using the remaining embedding vectors that hash to the same bucket. My questions is why does it work? What is the benefit of this approach? What is the intuition behind it?
