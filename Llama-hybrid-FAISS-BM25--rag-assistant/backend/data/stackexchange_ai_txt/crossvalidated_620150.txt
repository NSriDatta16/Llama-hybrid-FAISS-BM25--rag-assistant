[site]: crossvalidated
[post_id]: 620150
[parent_id]: 619602
[tags]: 
As you can see in the paper to which the repository belongs to, there is a distinction made between unsupervised and supervised anomaly detection. In this repository this is done, as it part of a outlier detection benchmark study. So here another reason for using train and test splits is to compare different outlier detection methods on robustness and other things. So it makes a sense use a train and test set to evaluate, whether a outlier detection strategy might misclassify samples, due to biases in the training data. In the paper they also give additional explanations for specific scenarios why the split data. Usually, when we think about outlier detection we think about the univariate case, where we only look at a single variable to identify an outlier. But it can be more complicated in multivariate case, where each individual variable is not an outlier but the combination of these values is unlikely. It makes sense for example to use a train and test set when, you want to identify outliers based on your x and y (target variable) values. So an outlier could be a case, where both the input variables are not usual as well as the target variable. But this combination of both might be unusual. A second argument for splitting train and test set for outlier removal it to ensure no data leakage. Like we split the data into train and test set before basing out normalisation on the train set, one can also argue also to build your outlier rules only based on the train set. So in my opinion if you are building machine learning models, you should base your outlier detections on the train set, and apply the same rules to the test set. So for example determine the Interquartile range based on the Train set and use the exact values for the testset. If you are interested more in inferential statistics, then I would not use a train and test set split. I mean you usually dont do it anyways. Because here you are not interested in the best performance, rather in informative parameters. So data leakage and a resulting overestimation of model performance is not of great concern. So more generally, when you split data anyways then make use of the split for outlier detection as well.
