[site]: datascience
[post_id]: 60307
[parent_id]: 60290
[tags]: 
Dimensionality around word vectors is already answered here : https://stackoverflow.com/questions/45394949/what-is-dimensionality-in-word-embeddings The reasons 200-300 dimensions are chosen normally, is that they have seen it produce the results that are very close to or equivalent to when they have chosen higher dimensions. When it comes to training time, the time taken to generate word embeddings for 200-300 dimensions are not significantly different for most of the experiments I have personally come across.
