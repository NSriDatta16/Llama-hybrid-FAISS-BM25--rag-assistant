[site]: crossvalidated
[post_id]: 210761
[parent_id]: 188678
[tags]: 
The error here is simply that you should take the derivative of your loss function, $$ E(\theta) = \frac{1}{2}\, (\, target - output\,(\,input, \, \theta)\,)^2\,, $$ and not the one of your network output $J$. So you should calculate $$g(\theta) \approx \frac{E(\theta + \epsilon) - E(\theta - \epsilon)}{2\epsilon}$$ instead. Rationale: you want to minimize $E$ with respect to the parameter $\theta$ (the neural network weights). So you always go a step into the direction in which the decrease of $E$ is maximal, which is given by $-\frac{\partial E}{\partial \theta_{ij}}$ (where $\theta_{ij}$ is a network weight). Bishop gives a short section on numerical evaluation of the derivative and the hessian in neural networks in his book "pattern recognition and machine learning".
