[site]: datascience
[post_id]: 69940
[parent_id]: 69930
[tags]: 
The number of trees in a random forest doesn't really need to be tuned, at least not in the same way as other hyperparameters. Adding more trees just stabilizes the results (you're averaging more samples from a distribution of trees); you want enough trees to get stable results, and adding more won't hurt except for computational resources. More directly (question 1), you could instead train the RF with, say, 1000 n_estimators, then just grab the predictions from each tree and average only the first 100 of them, the first 200, etc. Since the trees are built independently, this is essentially the same as training repeatedly on 100, then 200, etc. trees from scratch (but of course faster). More generally (question 2), yes, this is one-dimensional grid search. Hyperparameters may be interdependent though, so doing independent 1D searches will probably be suboptimal. Look into full grid search, random search, and maybe more advanced hyperparameter optimization methods.
