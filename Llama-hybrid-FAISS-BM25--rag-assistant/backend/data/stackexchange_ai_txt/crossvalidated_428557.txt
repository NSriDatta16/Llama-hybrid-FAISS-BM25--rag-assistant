[site]: crossvalidated
[post_id]: 428557
[parent_id]: 
[tags]: 
Dyna-Q Algorithm Reinforcement Learning

In step(f) of the Dyna-Q algorithm we plan by taking random samples from the experience/model for some steps. Wouldn't it be more efficient if we construct an MDP from experience by computing the state transition probabilities and reward distribution from experience and solve it by dynamic programming? DYNA-Q:
