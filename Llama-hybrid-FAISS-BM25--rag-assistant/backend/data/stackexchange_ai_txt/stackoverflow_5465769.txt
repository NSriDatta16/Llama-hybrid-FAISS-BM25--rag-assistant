[site]: stackoverflow
[post_id]: 5465769
[parent_id]: 5462699
[tags]: 
I think it should be either int pixelPosition = j*a.width + i; or int pixelPosition = i*a.height + j; Suppose your image is 100x10 that means there are 1000 elements. If you are looking at element (50,5) then i=50 and j=5, the original code says pixelPosition = i*a.width + j; that means pixelPosition = 50*100+5 = 5005 which is way out of range. If you look at your code you always use (i +/- 1) * width + (j +/- 1) so everything will be OK for the lines where i is less than the height but everything else is going to be out of range. The next problem is averaging red, green and blue. You should either convert to grey or another colour space variable not merge colours. The next problem is rounding error. Each sum is going to be truncated when the RGB values are small. Use a floating point type and then cast it in the last step. The next problem is in the final line: aBlur.pixels[pixelPosition] = a.pixels[pixelPosition] + color(total); aBlur seems to be like it's supposed to be a greyscale blur of the input image, but I don't know how the colour space is set, let's assume it's RGB. You copy the original pixel and add in a colourised version of the blur ? Suppose the original pixel is (150,150,150) and the averaged value you get is 170. If the function color(170) makes a pixel (170,170,170) then the sum of the original and average is going to be (320,320,320). So now the pixel value is far from a blurred version of the original. I suspect this line is supposed to be aBlur.pixels[pixelPosition] = color(total);
