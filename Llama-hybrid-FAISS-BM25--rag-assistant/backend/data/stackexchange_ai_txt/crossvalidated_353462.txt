[site]: crossvalidated
[post_id]: 353462
[parent_id]: 
[tags]: 
What are the implications of scaling the features to xgboost?

Doing research about the xgboost algorithm I went through the documentation . I have heard that xgboost does not care much about the scale of the input features In this approach trees are regularized using the complexity definition $$ \Omega(f) = \gamma T + \frac12 \lambda \sum_{j=1}^T w_j^2 $$ where $\gamma$ and $\lambda$ are parameters, $T$ is the number of terminal leaves and $w_j$ is the score in each leaf. So does this not make it important to scale the features before feeding into xgboost? $\sum_{j=1}^T w_j^2$ term in the regularization part of the cost function is directly influenced by the scale of the features
