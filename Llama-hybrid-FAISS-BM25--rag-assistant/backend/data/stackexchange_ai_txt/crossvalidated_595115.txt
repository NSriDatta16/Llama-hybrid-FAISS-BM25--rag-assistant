[site]: crossvalidated
[post_id]: 595115
[parent_id]: 
[tags]: 
How do I measure goodness of fit of data transformations that standardize for variables?

I am modeling a dependent variable which has significantly different distributions when grouped by various independent variables. Consequently, it is difficult to compare previous values of the dependent variable, because they may have been in different groups. I am trying to standardize the dependent variable by these independent variables. I am currently using a mixture of regular arithmetic transforms, as well as quantile mapping. Currently, my metric for whether or not a transformation is better than another is the accuracy/calibration of the model after fitting with the transformed data. But if I have dependent variable y, and transform it to remove the effect of independent variables x and z, is there a metric to determine how well y has been standardized by x and z? I have tried looking at the correlation between x,z and y, since reduced correlation could indicate their effect has been removed, but there are also non-linear relationships in my data. EDIT: Some commenters mentioned I have left out important information. I will try to provide that as best I can. To remove the effect of x and z via standardization I am grouping y by all the combinations of x and z. Then, I use the R fitdistr package to select the best distribution for this combination. I then fit a GAMLSS model using the GAMLSS package on each combination using that combination's best fit distribution. This results in every combination having different scale and location, so they are still not comparable to one another. So next I fit a GAMLSS model on the most common/average/typical combination. I call this the standard combination. I then quantile map the transformed y values for each combination to the standard combination. So if combination 1 was fit with a BCT GAMLSS family, I use the pBCT() function to get the quantiles of that combination and evaluate those quantiles using the quantile function of the standard combination, which let's say is also BCT: quantiles The goal of these transformations is to produce new y values that are independent of x and z, which can now be compared to each other more directly. The primary purpose to compare them directly is to be able to take lagged values of y and use them as input features to the model by grouping by every participant ID in the study, j, and using first lag of y. So a very simple model might be y_j ~ intercept + y_t-1,j I partly answered this question in the last paragraph, but what I mean by "previous" is that my data has participants, and I group by participant and use the lagged dependent variable values as features. So if I am predicting y_t,j where t is the timestep and j is the participant, I may use y_t-1,j y_t-2,j and y_t-3,j as independent variables. But like I mentioned earlier, these values are heavily affected by independent variables x and z, so much so that comparing y values between different combinations of x and z is almost useless.
