[site]: crossvalidated
[post_id]: 7758
[parent_id]: 
[tags]: 
On connection weights in an Artificial Neural Network

I play a lot with PyBrain -- Artificial Neural Network implementation in Python. I have noticed that in all the models that I receive the weights of the connections are roughly normally distributed around zero with a pretty low standard deviation (~0.3), which means that they are effectively limited within the [-1, 1] range. What does this mean? Is it a requirement of ANN? An outcome of backpropagaion learning? A sign for network health? Or just a random observation?
