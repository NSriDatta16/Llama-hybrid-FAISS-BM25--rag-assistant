[site]: crossvalidated
[post_id]: 591041
[parent_id]: 
[tags]: 
Why model order selection is a big problem in statistics?

I’m learning statistical signal processing for my studies. I was doing a bit of literature review on model order selection and I didn’t want to miss out on techniques that I might not have seen. I have come across some information criterion like AIC and BIC. Here, although I understood what they are, I feel without the knowledge of parameters themselves (which again depend on model order and are unknown ), these criteria are not best suited to find the model order directly. Correct me if I’m wrong. The second approach that I found was popular was the reversible jump MCMC. They try to jointly estimate model order and the parameters. In my opinion, it’s not very efficient because there are no “intuitive” jump strategies. However, I understand why it’s needed to have some idea about the parameters simultaneously when estimating the model order. The third type of estimators I saw was some sort of matrix sub space based methods where they don’t use any information about the parameters at all; like Subspace based automatic model order selection (SAMOS). I do not completely understand how they work, because they don’t use the information of parameters at all in the estimation of model order? Do they already use a lot of samples of the data so that they have enough resolution already? Being a newbie in statistics, all I can think about model order selection is that it’s like finding the number of poles of the system. Are there any closed form calculus based methods where they estimate the number of poles (without knowing the location of the poles)? In my mind,it doesn’t intuitively make sense. Is there a comparative study or basic understanding of model order selection based on finding number of poles?
