[site]: datascience
[post_id]: 32246
[parent_id]: 
[tags]: 
Q-Learning: Target Network vs Double DQN

I am having a hard time understanding difference between Target Network and Double DQN From this blog: Target Network generates the target-Q values that will be used to compute the loss for every action during training. The target network’s weights are fixed, and are frequently but by small amounts updated towards the primary Q-networks values. Double DQN: instead of taking the max over Q-values when computing the target-Q value for our training step, we use our primary network to chose an action, and our target network to generate the target Q-value for that action That sounds very similar to me, and the equation makes me lost too :( qTarget = r + γQ( s’, argmax(Q(s’,a,ϴ),ϴ’) ) From whaat I know, Target Network Approach allows us to do the following: using Online Network, get scores for each action as the response for the current state. Pick the most appropriate action $a$ and take a note of its score $q$ using that action, "travel" to the next state and get scores from Target Net for even further actions. Select the most appropriate action from step 2) and call it $A$ . also, take a note at its score $Q$ Punish OnlineNetwork for any difference between $q$ and $Q$ . Note: the gradient flows only through $a$ component of the "output vector" $\{a, b, c, d ...\}$ Please correct me if these steps are wrong
