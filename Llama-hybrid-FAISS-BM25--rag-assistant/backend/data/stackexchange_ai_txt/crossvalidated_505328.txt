[site]: crossvalidated
[post_id]: 505328
[parent_id]: 
[tags]: 
Is there a seq2seq model that can encode sentences that include numerical values?

I am trying to build a seq2seq model that encodes sentences which include numerical values. For example, Patient's systolic blood pressure was 128. Conventional seq2seq models (e.g., RNN encoder/decoder) convert each word into an embedding and then model the sequence of embeddings into a vector (hidden state) that is fed to the decoder. However, this sentence includes a number that needs to be regenerated by the decoder and it does not make a lot of sense to treat '128' as a word and include it in the embedding because 1) you can have any number during test time while your corpora likely includes a subset of possible numbers, and 2) it is very inefficient to model 128 as a string as opposed to a number, and 3) if the decoder generates '127' instead of '128', it is still acceptable and this should lead to a small increase in the loss function, while the conventional seq2seq model would penalize an output of '20', '50' or '10000000' (or even a string like 'hello') the same as '127'. Any ideas how to model numerical values in a more effective way?
