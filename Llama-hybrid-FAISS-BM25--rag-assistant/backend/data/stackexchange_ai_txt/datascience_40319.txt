[site]: datascience
[post_id]: 40319
[parent_id]: 40318
[tags]: 
A model is a simplified representation of a complex real world object/phenomena. As a simple example, I see a mountain and because I cannot study the exact shape of the mountain I represent it by a model that it is a triangle. Representing the mountain by a triangle is a model. Someone else comes and makes a better model by saying it is a cone. Every model captures some features of the real world phenomena. A model is better if it is able to capture the true features of the object/phenomena. From a machine learning perspective, this corresponds to exploring the dataset and then thinking about the dataset and creating a model of the dataset. For instance, if I have the famous Titanic dataset, I would be thinking how different features affect the probability of survival. One "model" will be to think that different attributes independently affect the chances of a person's survival. I would be wanting to use a naive bayes classifier to learn the parameters of this model and see how it performs. A different model would be to assume that different attributes probably follow a hierarchy. Maybe, first comes gender. Then among those of the same gender, the deciding factor is probably class and so on. To learn the parameters of this model I would want to train the data with a decision tree classifier or a random forest classifier. This is precisely the process of model generation. It involves thinking about the data by combining your insights from the exploratory data analysis and the domain knowledge you have. You then make certain assumptions. And then test your model.
