[site]: crossvalidated
[post_id]: 324428
[parent_id]: 323549
[tags]: 
Excellent question(s). Benny_dref's suggestions are useful. The first sentence of the OPs query is concerned with "selecting an appropriate forecasting model for panel data." With several DVs, more than a few predictors and a wide variety of possible methods and models to pursue, one has two broad avenues wrt model selection. The first is theoretical -- Wooldridge (referenced below) and the link to the DPM method(s) in the body of the OPs question provide guidance along those lines. The second avenue is empirical, e.g., which model has the most predictive power? explains the most variance? minimizes error? and/or minimizes AIC/BIC metrics? In other words, there are multiple possible criteria for model selection. You can pick one or triangulate across several metrics in distilling the information down to make that decision. The (linked) DPM deck in the body of the OPs query is exhaustive in covering theoretical issues wrt small T panel data, GMMs and instrumental variables (IVs). Note, however, that their approach is confirmatory and presumes that many, if not all, of the OPs more applied and practical concerns have been addressed and answered. Also, its high level of technical rigor can take an inexperienced modeller down a bewildered rabbit hole. My recommendation would be to step back from immediate pursuit of their prescriptions by building up to them using less demanding, more easily digested and exploratory approaches, methods and models. For instance, instead of GMMs, why not build up to their use with simpler, more tractable, easily implemented and understood OLS models? Or consider IVs: econometricians prefer IV solutions to issues wrt endogeneity but noneconometricians question whether or not they create as many problems as they solve. Of course, these problems begin with simply identifying an appropriate IV(s). Why not put these theoretical, last step issues aside until you know more about the behavior of the data? This also raises the possibility of stepping outside a purely econometric framework to consider methods and models proposed in other disciplines. As the OP notes in question 1), there are some exploratory issues to resolve wrt variable selection. PCA is widely employed but has the drawback of blurring the specificity inherent in the individual predictors. Why not use the Lasso for variable selection? Another approach to variable selection could be relative variable importance. Ulrike Groemping's RELAIMPO is one such ( https://cran.r-project.org/web/packages/relaimpo/relaimpo.pdf ). RELAIMPO is an R module that can be easily implemented within the framework of OLS regression and helpful in terms of generating qualitative insights wrt panel data. In addition, Groemping exhaustively reviews the many approaches to variable importance that have been proposed over the years. Wrt 2), the three (A,B,C) correlated dependent variables (DVs) could be simultaneously modeled with canonical correlation or MANOVA-type approaches. This would be informative as it would tell you how the importance of a predictor varies as a function of the DV. Assuming the DVs are non-negative, a useful normalizing transformation different from first-differencing would be the natural log. There are many advantages to natural logs including the fact that the resulting parameters are expressed as elasticities and they minimize retransformation bias when converting predictions back into their original units. If the DVs contain negative values Lee Cooper suggests log-centering ( Market Share Analysis , http://www.anderson.ucla.edu/faculty/lee.cooper/MCI_Book/BOOKI2010.pdf ). Cooper's book is a goldmine of applied ideas and suggestions wrt exploring panel data structures (his term is pooled time series models ). Wrt 3), the short answer is yes, a multi-step, exploratory process makes sense, until you know more...much more. Within the context of econometric literature, Wooldridge's Econometric Analysis of Cross Section and Panel Data is the "go-to" resource. His book has the disadvantage of being almost purely theoretical with little in the way of practical, applied guidance. A good complement to Wooldridge is Badi Baltagi's Econometric Analysis of Panel Data which considers the inevitably messy nature of real world data. M. Hashem Pesaran's papers cover many panel data model issues not otherwise discussed in the econometric literature such as weak cross-sectional dependence and nonstationarity. I would recommend a google search using 'filetype:pdf hashem pesaran' to uncover his published papers. Linear models are not the only game in town. Weiss' reviews nonlinear mixed effects models in his lecture notes ( http://www.unc.edu/courses/2008fall/ecol/563/001/docs/lectures/lecture27.htm ). Similarly, Rob Hyndman's suggestion of 'boosted additive quantile regression' is another excellent example of a rigorous methodology for exploring complex, nonlinear panel data structures ( http://ieeexplore.ieee.org/document/7423794/ ). An additional advantage to BAQR is that, being nonparametric and distribution free, it controls for extreme values (aka outliers) much more effectively than a natural log transformation which, in the presence of truly extreme valued data, does not capture heavy-tails. Econometric panel data models are only one type in the broader class of event data modelling. There is a large literature on the closely related topics of growth and hierarchical models, e.g., Gelman and Hill's Data Analysis Using Regression and Multilevel/Hierarchical Models , Singer and Willett's Applied Longitudinal Data Analysis or Raudenbush and Bryk's Hierarchical Linear Models are all excellent references covering a broad class of noneconometric models and methods including marketing science, education, health, the environment, and more.
