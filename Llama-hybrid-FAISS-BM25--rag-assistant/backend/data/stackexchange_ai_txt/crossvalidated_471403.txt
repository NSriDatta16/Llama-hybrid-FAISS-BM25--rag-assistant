[site]: crossvalidated
[post_id]: 471403
[parent_id]: 
[tags]: 
Conditional probability in Bayesian analysis

Suppose $X_i's$ are independent Poisson( $\lambda_i$ ). The $\lambda_i's$ have a prior distribution which is a mixture of $Gamma(\alpha_1, \beta_1)$ and $Gamma(\alpha_2, \beta_2), i.e., pGamma(\alpha_1, \beta_1) + (1-p)Gamma(\alpha_2, \beta_2)$ . Let $Y_i$ iid $Bernoulli(p)$ . If $Y_i = 1$ , draw $\lambda_i$ from $Gamma(\alpha_1, \beta_1)$ ; if $Y_i = 0$ , draw $\lambda_i$ from $Gamma(\alpha_2, \beta_2)$ . What is the conditional distribution of $Y_i=1$ given $X_i$ ? Thanks! Edit: My initial thought is find the marginal distribution of $X_i, P(X_i)$ by finding the joint distribution of $X_i$ and $\lambda_i$ (using the mixture distribution of $\lambda_i$ ) and integrate over $\lambda_i$ find the cond. dist. of $X_i$ given $Y_i = 1, P(X_i|Y_i=1)$ by finding the joint distribution of $X_i$ and $\lambda_i$ (here, the distribution of $\lambda_i$ is Gamma( $\alpha_1, \beta_1$ ) only) and integrate over $\lambda_i$ then the cond. dist. of $Y_i=1$ given $X_i$ is $p*P(X_i|Y_i=1)/P(X_i)$ Is this a valid way of approaching this question?
