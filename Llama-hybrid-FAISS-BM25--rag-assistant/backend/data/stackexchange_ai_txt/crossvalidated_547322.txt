[site]: crossvalidated
[post_id]: 547322
[parent_id]: 544835
[tags]: 
The expected calibration error is (often) defined as: $$\text{ECE}(f)=\mathbb{E}_{f(x)}\left[\left|f(X)-\mathbb{E}[Y \mid f(X)]\right|\right]$$ where $f(x) \in [0,1]$ is the NN output and $\mathbb{E}[Y \mid f(X)]=\mathbb{P}[Y=1 \mid f(X)]$ in the binary case. This is extended to multi-class problems by simply regarding every class as a binary classification objective and averaging over them (see https://arxiv.org/pdf/1909.10155.pdf ). Your formula is just an estimator of the expectation above: $$\text{ECE}(f)=\mathbb{E}_{f(x)}\left[\left|f(X)-\mathbb{E}[Y \mid f(X)]\right|\right] \approx \sum_{m=1}^M \frac{|B_m|}{n}|conf(B_m)-acc(B_m)|$$ Because the $f(x)$ is continuous, we need to discretise it: Let us partition $[0,1]$ into $M$ intervals. Then we define $B_m$ to be the set data points that have predictions falling into the $m$ th interval (see https://arxiv.org/abs/1706.04599 for more formal definition of $B_m$ ). So $m$ indicites the groupping based on confidence, not the predicted class itself. Note: The $ECE$ is not a good metric as it has a lot of shortcomings. For instance, if $f(X) > 0.9$ for all data points, then the equal width binning of the $ECE$ is useless. Another issue is that the estimated $ECE$ tends to underestimate the true calibration error, which has been shown by multiple papers in different ways.
