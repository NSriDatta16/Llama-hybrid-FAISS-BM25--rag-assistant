[site]: datascience
[post_id]: 18744
[parent_id]: 18742
[tags]: 
In my opinion the learning of MLPs is pretty damn "natural". To answer your questions: You would usually train the network's weights in random minibatches of data using a process called stochastic gradient descent (or some variant such as Adam, RMSprop, Adagrad etc...), rather than general gradient descent over the entire dataset at once. This is because each epoch is too expensive to calculate the gradients of weights over all of the data, vs a smaller random sample. Neural nets are trained iteratively and therefore kind of do improve through time (to a point, then they start overfitting). It is also common not to just run over the data once to get a decent fit, often you must run through a few times (each run through of data is called an "epoch"). Unlike Bayesian you don't really have a prior as the weights are usually randomly initialized, but the model (usually) keeps improving with more data / observations. There are Bayesian approaches to neural networks where you assume that the weights come from a distribution (say Gaussian prior over the weight space), then you can use MCMC - I don't think this is very popular anymore though. You can keep training by iterating over batches of different training data. In fact when data sets are large you basically have to break them up into different batches. So yes you can keep training them with different bits of data. Once some new data comes in you can then continue training, presumably you have saved the weights / model parameters. This won't start you from scratch again obviously. I think if you code a network up or even just start with some Keras or Tensorflow implementation (on MNIST) you'll get a feel for what's going on and how they learn / behave in practice.
