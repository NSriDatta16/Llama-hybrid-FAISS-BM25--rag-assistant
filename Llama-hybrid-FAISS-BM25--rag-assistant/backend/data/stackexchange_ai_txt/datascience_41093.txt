[site]: datascience
[post_id]: 41093
[parent_id]: 
[tags]: 
Using SMAPE as a loss function for an LSTM

I am currently working on a time series forecasting problem and am looking into using an LSTM. My final accuracy metric that I use to determine whether or not the forecast is good or not is defined as follows: $$\text{SMAPE-3} = \frac{\sum^n_{t=1}|y_t-\hat{y}_t|}{\sum^n_{t=1}\left(y_t+\hat{y}_t\right)},$$ where $y_t$ is the actual value and $\hat{y}_t$ is the forecast value. So this formula will produce a score between $[0, 1]$ . This score can then be subtracted from 1 to produce a percentage accuracy of the forecast over the specified forecast horizon $$\text{Accuracy} = 1-\frac{\sum^n_{t=1}|y_t-\hat{y}_t|}{\sum^n_{t=1}\left(y_t+\hat{y}_t\right)}.$$ I am using keras and have seen that I can specify a custom loss function and metric. As my forecast accuracy will be measured using sMAPE 3 (as defined above) it made sense to me to use this as my loss function. This is because my networks success will be measured by how well it scores using this. A score of 0 indicates 100% accuracy and a score of 1 indicates 0% accuracy. So this raises two questions, can a metric such as smape be used as loss function at all, if so why / why not? Are the standard loss functions within keras considered better? Perhaps because they allow quicker convergence? Does my choice of using SMAPE3 as a loss function have an impact on what kind of optimiser I should use? I ask because I haven't seen many people using loss functions other than RMSE, MSE and MAE and there must be a reason for this.
