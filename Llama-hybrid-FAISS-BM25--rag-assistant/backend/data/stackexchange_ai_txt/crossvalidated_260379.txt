[site]: crossvalidated
[post_id]: 260379
[parent_id]: 255230
[tags]: 
Yes, it might be the case but the idea for ensembling is to train simpler models to avoid over fitting while capturing different characteristics of data from different ensembles. Of course there is no guarantee of an ensemble model to outperform a single model while trained with same training data. The outperformance can be gained by combining ensemble models and boosting(e.g. AdaBoost). By boosting you train each next ensemle model by assigning weights on each data point and updating them according to error. So think of it as a coordinate descent algorithm, it allows the training error to go down with each iteration while maintaining a constant average model complexity. In overall this makes an impact on the performance. There are many
