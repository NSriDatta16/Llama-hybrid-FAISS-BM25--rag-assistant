[site]: datascience
[post_id]: 100180
[parent_id]: 100160
[tags]: 
The crucial difference here is that attention allows working with arbitrarily long sequences (or rather sets) of vectors. A linear layer has a constant-sized input. Each output activation in a linear layer is a linear combination of the activations in the previous layer. However, the input is always exactly one vector, so linear layers cannot in principle consider any context. Processing a sequence with linear layers only is equivalent to processing each vector in the sequence independently. (A straightforward update would be doing a sliding window over the vector sequence, this is called 1D convolution.) Attention can work with arbitrarily long input. It computes the similarity between a query vector with all key vectors and retrieves corresponding values. Unlike linear layers, attention brings information about the context of the other vectors. In the self-attention case, all vectors interact with each other.
