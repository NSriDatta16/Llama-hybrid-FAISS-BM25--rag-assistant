[site]: datascience
[post_id]: 123956
[parent_id]: 123914
[tags]: 
Unlike many of the other LLMs, T5 is an encoder-decoder model. (Most are encoder-only like BERT, or decoder-only like the GPT models.) So it is well suited to sequence to sequence applications. The 11B model is also far from being a small model. Running anything bigger is going to require some serious hardware resources.
