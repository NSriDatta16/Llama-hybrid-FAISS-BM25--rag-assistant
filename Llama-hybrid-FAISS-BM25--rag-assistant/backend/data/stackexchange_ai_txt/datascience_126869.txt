[site]: datascience
[post_id]: 126869
[parent_id]: 
[tags]: 
The end-to-end Training Process for Knowledge Distillation

I'm a bit confused on the complete training process for Knowledge Distillation. I was reading the Geoffrey Hinton "Distilling the Knowledge in a Neural Network" 2015 paper and some random blog plots to figure out how to actually train it. However, I don't see any end-to-end training example for that paper. So, here is what I understand and want to confirm the training process. Let's say we will train one big binary classification teacher network and distill the knowledge to student network. Train the Large Teacher Network Distill the Knowledge Use the temperature to soften the softmax (where T is temperature and it's used to reduce confidence such as [2.0, 0.5] with T=1 is [0.82, 0.18] and T=2 is [0.68, 0.32].) $$\text{softmax}(x_i) = \frac{e^{\frac{x_i}{T}}}{\sum_{j=1}^{N} e^{\frac{x_j}{T}}}$$ Then, prepare the subset of training dataset with new labels which are soft targets instead of hard targets [0, 1, 1, ...]. Soft targets = [[0.68, 0.32], [0.44, 0.56], [0.39, 0.61], ...] That new subset of dataset with soft targets is called transfer set. Loss Function To make it simple, let's say I'll use the CrossEntropyLoss with class probabilities between my outputs (with same temperate as transfer set) and soft targets. That's it. Then, I can start the distillation training with transfer set. Is this process/flow correct?
