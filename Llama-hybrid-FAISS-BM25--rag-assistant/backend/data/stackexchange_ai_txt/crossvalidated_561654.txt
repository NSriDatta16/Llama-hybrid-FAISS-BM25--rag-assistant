[site]: crossvalidated
[post_id]: 561654
[parent_id]: 
[tags]: 
Problems with convergence of the EM algorithm for a gaussian mixture regression

I have been implementing a EM-algorithm for a latent-class regression model, where every individual has a vector of observations. Currenly, I have the problem that the model does not converge. The log likelihood seems to be increasing up until a certain point, after which the segments 'swap and the likelihood is decreases sharply again. Plot of $p_s$ Plot of Log-likelihood. Note that the scale is x1e10. My derivation of the algorithm is as follows: \begin{equation} \label{eq: mixture spec} y_{ijt} = x_{{s_i}jt}' \beta_{s_i} + \alpha_{s_i} + \gamma_{ij} + \epsilon_{ijt} \end{equation} \noindent where $s_i$ is a latent, unobserved variable that we treat as a stochastic variable with $P[S_i = s] = p_s$ for $s = 1, ..., K$ and $\sum_{s=1}^K p_s = 1$ . Mixture models are often estimated with the EM algorithm. This is an iterative algorithm that in the E-step calculates the posterior probabilities $\tilde{p}_s = P[s_i = s | y, \hat{\theta}]$ given the current set of parameter estimates $\theta$ and in the M-step maximises the expected log-likelihood function with respect to the set of parameters $\theta$ . Given this specification we have the following likelihood function: \begin{multline} L(\theta) = \prod_{i=1}^N \sum_{s=1}^K p_s \left(\prod_{j=1}^M \prod_{t=1}^T \phi(y_{ijt}, x_{ijt}; \beta_{s_i}, \alpha_{s_i}, \gamma_{ij}) \right) \\ = \prod_{i=1}^N \sum_{s=1}^K p_s \left(\prod_{j=1}^M \prod_{t=1}^T \frac{1}{\sigma_{\varepsilon} \sqrt{2 \pi}} \exp \left(-\frac{1}{2 \sigma_{\varepsilon}^{2}}\left( y_{ijt} - x_{{s_i}jt}' \beta_{s_i} - \alpha_{s_i} - \gamma_{ij}\right)^{2}\right) \right) \end{multline} For the EM algorithm we consider the complete data likelihood function: \begin{equation} L_j(\theta) = \prod_{i=1}^N \prod_{s} \left(p_{s} \prod_{j=1}^M \prod_{t=1}^T \phi(y_{ijt}, x_{ijt}; \beta_{s}, \alpha_{s}, \gamma_{ij}, \sigma_{\epsilon}) \right) ^{I(s_i = s)} \end{equation} and the subsequent log-complete likelihood: \begin{equation} \label{eq: log complete} \ell_{j}(\theta)=\log L_{j}(\theta)= \sum_{i=1}^N \sum_{s=1}^K I(s_i = s) (\log \: p_s + \sum_{j=1}^M \sum_{t=1}^T \left(\log \: \phi(y_{ijt}, x_{ijt}; \beta_{s}, \alpha_{s}, \gamma_{ij}, \sigma_{\epsilon}) \right) ) \end{equation} In the E-step we calculate: \begin{equation} \pi_{i s} \equiv \mathrm{E}\left[I\left(s_{i}=s\right) \mid y_{ijt}, x_{ijt} \right]= \frac{ \phi(y_{ijt}, x_{ijt}; \beta_{s}, \alpha_{s}, \gamma_{ij}, \sigma_{\epsilon}) p_{s}} {\sum_{k=1}^K \phi(y_{ijt}, x_{ijt}; \beta_{k}, \alpha_{k}, \gamma_{{k}j}) p_k} \end{equation} Substituting this in the log-complete likelihood gives: \begin{equation} \ell_{j}(\theta)=\log L_{j}(\theta)= \sum_{i=1}^N \sum_{s=1}^K \pi_{i s} (\log \: p_s + \sum_{j=1}^M \sum_{t=1}^T \left(\log \: \phi(y_{ijt}, x_{ijt}; \beta_{s}, \alpha_{s}, \gamma_{ij}, \sigma_{\epsilon}) \right) ) \end{equation} And the M-step results in maximising: \begin{equation} \max _{p, \alpha, \beta, \gamma, \sigma} \left( \left(\sum_{s=1}^{K} \sum_{i=1}^{N} \pi_{i s} \log p_{s}\right)+\left(\sum_{s=1}^{K} \sum_{i=1}^{N} \pi_{is} \sum_{j=1}^M \sum_{t=1}^T \log \phi(y_{ijt}, x_{ijt}; \beta_{s}, \alpha_{s}, \gamma_{ij}, \sigma_{\epsilon})\right) \right) \end{equation} Solving \begin{equation} max_{p}\left(\sum_{s=1}^{K} \sum_{i=1}^{N} \pi_{i s} \log p_{s}\right) \text { subject to } \sum_{s=1}^{K} p_{s}=1 \end{equation} yields \begin{equation} p_{s}=\frac{1}{i} \sum_{i=1}^{i}\pi_{is} \end{equation} For the second part: \begin{multline} \max _{p, \alpha, \beta, \gamma, \sigma} \left(\sum_{s=1}^{K} \sum_{i=1}^{N} \pi_{is} \sum_{j=1}^M \sum_{t=1}^T \log \phi(y_{ijt}, x_{ijt}; \beta_{s}, \alpha_{s}, \gamma_{ij}, \sigma_{\varepsilon})\right) \\ = max _{p, \alpha, \beta, \gamma, \sigma} \left( \sum_{s=1}^{K} \sum_{i=1}^{N} \pi_{is} \sum_{j=1}^M \sum_{t=1}^T -\frac{1}{2} \log \left( 2 \pi \sigma_{\varepsilon}^{2}\right) - \frac{1}{2 \sigma_{\varepsilon}^{2}} \left( y_{ijt} - x_{sjt}' \beta_{s} - \alpha_{s} - \gamma_{ij}\right)^{2} \right) \end{multline} And my code is: import pandas as pd import numpy as np from tqdm import tqdm import statsmodels.api as sm from config import Config import itertools import pickle from panel_model import transform_yX def convert_log_row(row): new_row = pd.Series(index=row.index, dtype='float64') base = 0 for s in row.index: value = row[s] basevalue = row[base] part1 = value - basevalue other_values = row.drop(base) diff = other_values - basevalue e_diff = np.e ** diff part2 = np.log(1 + sum(e_diff)) pi = np.e ** (part1 - part2) new_row[s] = pi return new_row def eval_log_L(y, X, betas, variances, p_array): logL = 0 artists = list(X.reset_index()['ARTIST_KEY'].unique()) segments = list(betas.columns) es = pd.DataFrame(index=y.index, columns=betas.columns,data= X.dot(betas).sub(y, axis=0)) es = es.reset_index() all_ediff = {} for a in artists: e_a = es[es['ARTIST_KEY'] == a] e_a = e_a.loc[:, betas.columns] logpiphi = pd.Series(index=segments, dtype='float64') for s in segments: logpiphi[s] = eval_phi_ct(e_a[s], variances[s]) + np.log(p_array[s]) logL += logpiphi[0] sume = 0 for s in range(1, len(segments)): diff = (logpiphi[s] - logpiphi[0]) try: e_pow = np.e ** diff all_ediff[(a, s)] = e_pow except: print('ERROR RAISED FOR ARTIST ' + str(a) + ' FOR SEGMENT ' + str(s)) if e_pow != float('+inf') and e_pow != float('-inf'): # print('NOT inf for ' + str(a) + 'segment' + str(s)) sume += e_pow # print('sume=' + str(sume)) else: pass # print('inf for ' + str(a) + 'segment' + str(s)) plus = np.log(1 + sume) logL += plus # print('Artist ' + str(a) + '. Log L = ' + str(logL)) return logL def eval_phi_ct(e, var): return -1 * len(e) * np.log(np.sqrt(2 * np.pi * var)) - (1 / (2 * var)) * sum(e ** 2) if __name__ == '__main__': data = read_joined('joined_onlyartistwithmarketing.csv') #construct data genres = [x for x in list(Config.GENRE_MAPPING.values()) if x != 'Other'] data = data.drop(genres, axis=1) data = clean_artist(data) data = data.drop(['twitter', 'snapchat'], axis=1) #dict to store the data in results = {} results['betas'] = {} results['variances'] = {} results['pi'] = {} results['p'] = {} results['LogL'] = {} results['i'] = {} results['resids'] = {} results['residsum'] = {} results['se'] = {} #perform ac transformation y,X = transform_yX(kind= 'ac', data=data) y = y.iloc[:, 0] #get variables, artists and segments NO_SEGMENTS = 4 segments = list(range(NO_SEGMENTS)) variables = [x for x in data.columns if x not in ['DATE_KEY', 'ARTIST_KEY', 'COUNTRY_CODE', 'ARTIST_NAME','TOTAL_DAILY_STREAMS']] artists = list(data['ARTIST_KEY'].unique()) data = data.set_index(['DATE_KEY', 'ARTIST_KEY', 'COUNTRY_CODE']) # Initialise start values for beta and sigma betas = pd.DataFrame(index=variables, columns=segments) ses = pd.DataFrame(index=variables, columns=segments) y2 = data['TOTAL_DAILY_STREAMS'] X2 = data[variables] data = data.reset_index() # mod = sm.RLM(y, X, M=sm.robust.norms.HuberT()) mod = sm.OLS(y, X) res = mod.fit() beta_start=res.params e = y - X.dot(beta_start) standard_errors_start = res.bse for s in segments: for variable in variables: if s == 0: betas.loc[variable, s] = beta_start[variable] else: betas.loc[variable, s] = beta_start[variable] + norm(0, standard_errors_start[variable]).rvs() #create empty dataframes variances = pd.Series(index=segments, data=res.scale) pi_matrix = pd.DataFrame(index=artists, columns=segments) p_array = pd.Series(index=segments, data=1/ NO_SEGMENTS) segments_artists = list(itertools.product(segments, artists)) for i in range(0, 100): print(betas) results['betas'][i] = betas.copy() results['variances'][i] = variances.copy() results['se'][i] = ses.copy() #determine the new p_is for s, a in tqdm(segments_artists, position=0, leave=True): # for s, a in segments_artists: rel_beta = betas[s] var = variances[s] p = p_array[s] rel_data = data[data['ARTIST_KEY'] == a] y_a = rel_data['TOTAL_DAILY_STREAMS'] X_a = rel_data[variables] e = y_a - X_a.dot(rel_beta) log_p_phi = -1 * len(e) * np.log(np.sqrt(2 * np.pi * var)) - (1 / (2 * var)) * sum(e ** 2) + np.log(p) phi_eval = log_p_phi pi_matrix.loc[a, s] = phi_eval for j in range(len(pi_matrix)): row = pi_matrix.iloc[j, :] newrow = convert_log_row(row) pi_matrix.iloc[j,:] = newrow results['pi'][i] = pi_matrix.copy() #calculate the new S number of p's p_array = pi_matrix.mean() print(p_array) results['p'][i] = p_array.copy() #calculate new betas and sigmas #together method Xconcat = np.kron(np.identity(NO_SEGMENTS), X) yconcat = pd.concat([y.to_frame()] * NO_SEGMENTS, ignore_index=True)[0] empty_weights = pd.DataFrame(index=data.set_index(['DATE_KEY', 'ARTIST_KEY', 'COUNTRY_CODE']).index, dtype='float64').reset_index() pi_matrix_mod = pi_matrix.reset_index().rename(columns={'index' : 'ARTIST_KEY'}) weights = pd.merge(empty_weights, pi_matrix_mod, how='left', on=['ARTIST_KEY']) weights = list(weights.set_index(['DATE_KEY', 'ARTIST_KEY', 'COUNTRY_CODE']).stack()) mod = sm.WLS(yconcat, Xconcat, weights=weights) res = mod.fit() b = list(res.params) v = res.scale se = res.bse results['resids'][i] = res.resid results['residsum'][i] = res.resid.apply(lambda x : x ** 2).sum() for s in segments: # print(s) betas[s] = b[12* s: 12 * s + 12] ses[s] = se[12* s: 12 * s + 12] variances[s] = v resids = pd.Series(data = list(results['residsum'].values())) resids.plot() Log_likelihood = pd.Series(data = list(results['LogL'].values())) Log_likelihood.plot() p_matrices = list(results['p'].values()) P = pd.DataFrame(columns = p_matrices[0].index) for k in range(len(p_matrices)): P = P.append(p_matrices[k], ignore_index=True) P.plot() I perform all the log transformations for calculation LogL and the $pi_as$ in order to prevent numerical errors.
