[site]: crossvalidated
[post_id]: 79454
[parent_id]: 
[tags]: 
Softmax layer in a neural network

I'm trying to add a softmax layer to a neural network trained with backpropagation, so I'm trying to compute its gradient. The softmax output is $h_j = \frac{e^{z_j}}{\sum{e^{z_i}}}$ where $j$ is the output neuron number. If I derive it then I get $\frac{\partial{h_j}}{\partial{z_j}}=h_j(1-h_j)$ Similar to logistic regression. However this is wrong since my numerical gradient check fails. What am I doing wrong? I had a thought that I need to compute the cross derivatives as well (i.e. $\frac{\partial{h_j}}{\partial{z_k}}$) but I'm not sure how to do this and keep the dimension of the gradient the same so it will fit for the back propagation process.
