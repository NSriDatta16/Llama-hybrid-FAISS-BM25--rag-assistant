[site]: crossvalidated
[post_id]: 368505
[parent_id]: 368494
[tags]: 
A simple approach to regression with a multivariate outcome is multilevel regression or multilevel logistic regression in your situation (GLMM). You would have to reshape your data. Each row in $\mathbf{X}$ repeats $m$ times, each row in $\mathbf{Y}$ gets transposed. So $\mathbf{X_{n\times p}}$ becomes $\mathbf{X_{k\times p}}$ and $\mathbf{Y_{n\times m}}$ becomes $\mathbf{y}$ , a $k\times 1$ column vector, where $k=n\times m$ . You would then need to create unique case identifiers for each row in your original data and include it as an additional column in your $\mathbf{X_{k\times p}}$ . You can also create a variable that uniquely identifies each original outcome, $y_{*m}$ , and also include it in $\mathbf{X_{k\times p}}$ . For a reasonable baseline model, you would regress $\mathbf{y}$ on dummy variables for each original outcome and a random intercept by the case identifier. The random intercept would be controlling for the lack of independence between the entries in $\mathbf{y}$ , since some of them come from within the same case. The coefficients of the outcome dummies are the means of the different outcomes. You can then enter columns in $\mathbf{X_{k\times (p+2)}}$ as additional predictors of $\mathbf{y}$ . If a column vector in $\mathbf{X_{k\times (p+2)}}$ goes in as a predictor by itself, you are assuming it has the same relationship with all the outcomes. Alternatively, you can enter the variable as an interaction with the outcome indicators, so it has a different relationship with each of the original outcomes. I'm not sure this is clear. But a similar example in R is here . The difference is that was a linear regression. This would be a multilevel logistic regression.
