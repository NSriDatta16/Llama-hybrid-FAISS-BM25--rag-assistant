[site]: crossvalidated
[post_id]: 62080
[parent_id]: 61783
[tags]: 
In $k$-fold cross-validation we partition a dataset into $k$ equally sized non-overlapping subsets $S$. For each fold $S_i$, a model is trained on $S \setminus S_i$, which is then evaluated on $S_i$. The cross-validation estimator of, for example the prediction error, is defined as the average of the prediction errors obtained on each fold. While there is no overlap between the test sets on which the models are evaluated, there is overlap between the training sets for all $k>2$. The overlap is largest for leave-one-out cross-validation. This means that the learned models are correlated, i.e. dependent, and the variance of the sum of correlated variables increases with the amount of covariance ( see wikipedia ): \begin{equation} \operatorname{Var}\left(\sum_{i=1}^NX_i\right)=\sum_{i=1}^N \sum_{j=1}^N \operatorname{Cov}\left(X_i,X_j\right) \end{equation} Therefore, leave-one-out cross-validation has large variance in comparison to CV with smaller $k$. However, note that while two-fold cross validation doesn't have the problem of overlapping training sets, it often also has large variance because the training sets are only half the size of the original sample. A good compromise is ten-fold cross-validation. Some interesting papers that touch upon this subject (out of many more): A study of cross-validation and bootstrap for accuracy estimation and model selection by Ron Kohavi No unbiased estimator of the variance of k-fold cross-validation by Yoshua Bengio and Yves Grandvalet
