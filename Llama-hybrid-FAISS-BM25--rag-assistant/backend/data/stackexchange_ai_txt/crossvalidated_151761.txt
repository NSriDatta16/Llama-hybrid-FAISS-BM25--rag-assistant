[site]: crossvalidated
[post_id]: 151761
[parent_id]: 
[tags]: 
Stacked Autoencoders

How are the subsequent layers trained exactly? From what I understand, when the first layer is trained, image is split into small patches, those patches are then fed into an auto-encoder which then learns a sparse representation of the data - great! However, how are the larger representations learned (i.e. combinations of edges), since the input to the network is still the same - patches which capture just a single edge. Do you have to "clone" your auto-encoders and feed, say four neighbouring patches and make next layer to learn on their representations (I am pretty sure this is not how it is done, but it might help show where my confusion is).
