[site]: crossvalidated
[post_id]: 32434
[parent_id]: 32430
[tags]: 
If you have two unbiased estimators $L_1$ and $L_2$ then accuracy is measured by variance (for a mean square error loss function). Take $L=aL_1+(1-a)L_2$ , a weighted average of the two with $0 . $$\newcommand{\Var}{\mathrm{Var}}\newcommand{\Cov}{\mathrm{Cov}} \Var(L) =a^2\Var(L_1) +(1-a)^2\Var(L_2) +2a(1-a) \Cov(L_1, L_2) \>. $$ When $\Cov(L_1,L_2) \leq 0$ , $$ \Var(L) \leq a^2 \Var(L_1) +(1-a)^2 \Var(L_2) . $$ If $\Cov(L_1, L_2)$ is sufficiently negative then $\Var(L)$ will also be less than $\min(\Var(L_1), \Var(L_2))$ . If $a$ is not restricted to $[0,1]$ and $1-a$ is replaced by $b$ it is possible to find constants $a$ and $b$ such that $\Var(L) = \Var(a L_1) + \Var(b L_2)$ is less than $\min (\Var(L_1), \Var(L_2))$ . For $\Cov(L_1, L_2)>0$ a particular choice of $a>0$ and $b will work. This shows that there is always a way to improve the accuracy of unbiased estimators if you can determine a linear combination of the two that lowers the variance. One will always exist. Clearly this is a good way to take advantage of the knowledge of both estimators.
