[site]: datascience
[post_id]: 126658
[parent_id]: 
[tags]: 
What is the difference between hidden states in RNN and Transformers model?

I'm very terrible at NLP and I have searched for these questions but didn't find any answer, my question is, in RNNs, there are hidden states to remember information for processing the next state, and in Transformers, there are also hidden states for each attention layer. Are these different or the same ? Additionally, I often read that RNNs or LSTMs have a dimension of $256$ , while Transformers have a dimension of $768$ . What are the meaning of these numbers, what are they used for ? Thanks
