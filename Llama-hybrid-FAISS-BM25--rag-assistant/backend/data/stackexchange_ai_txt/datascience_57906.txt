[site]: datascience
[post_id]: 57906
[parent_id]: 
[tags]: 
LSTM number of units for first layer

I'm trying to use LSTM (with Keras) for a time series problem. I would like predict the next value of the time series given its previous value. I'm using TimeseriesGenerator to create the training data as follows (setting length equal to 1 indicating the prediction is based on the previous value): generator = TimeseriesGenerator(series, series, length=1, batch_size=10) For the modeling I'm using the following: model = Sequential() model.add(LSTM(num_units, activation='relu', input_shape=(n_input, n_features))) model.add(Dense(1)) model.compile(optimizer='adam', loss='mse') model.fit_generator(generator, steps_per_epoch=len(generator)/n_batch, epochs=50) for prediction: yhat = model.predict(x_input, verbose=0) when I set num_units = 1 (in the the first layer), the predicted values are much lower than the typical values in the time series (the typical values in the time series are 30-50 and the prediction is around 0.4). However, when I set num_units = 700 , the predicted values become very close to the test values and the predictions seem to be overfitting. Why is that? What does the num_units in the first layer intuitively represent? If our input data is just one number, what does it mean to have a layer with 700 units? What is the intuition behind mapping a number to 700 neurons and what does one gain from it? In general, if in our time series, we're trying to make a prediction from the past n observations, how many units should be in the first LSTM layer?
