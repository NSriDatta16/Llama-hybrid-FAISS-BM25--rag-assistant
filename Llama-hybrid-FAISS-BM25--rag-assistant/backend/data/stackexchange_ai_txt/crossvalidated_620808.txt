[site]: crossvalidated
[post_id]: 620808
[parent_id]: 620781
[tags]: 
Word2vec is not any more exciting or complicated than a simple, single-layer neural network. If you’re familiar with those, a lot of the same information applies here. The vectors for each word are randomly initialized. As the model is fitted, the optimizer improves the utility of the vectors; it makes them more capable of predicting the word from context (CBOW) or context from word (skip-gram). It considers each word-in-context one by one. (If you’re familiar with batching, this process can also be batched.) This is done by computing a loss function in the forward pass (“how far off is my ability to predict the right word?”), computing gradients of this loss with respect to the vectors in the backward pass, and updating the weights accordingly.
