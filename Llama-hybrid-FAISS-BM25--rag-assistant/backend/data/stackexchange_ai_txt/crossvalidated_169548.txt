[site]: crossvalidated
[post_id]: 169548
[parent_id]: 169539
[tags]: 
Suppose your input is $X_{m \times d}$, where $m$ is the number of data points and $d$ is the dimension of each point. The total variance is a property of your input $X$; call it $v(X)$. Now, suppose the reconstructed input after applying dimensionality reduction is $\tilde{X}$ (of the same dimensions), it has a variance $v(\tilde{X})$. Then the fraction of variance explained is simply: $v(\tilde{X})/v(X)$. This doesn't depend on how you implement the dimensionality reduction. For PCA, the explained variance can be computed directly from the eigenvalues of the covariance matrix. For other non-linear techniques, you can compute the reconstruction and then compute the explained variance from its intuitive definition.
