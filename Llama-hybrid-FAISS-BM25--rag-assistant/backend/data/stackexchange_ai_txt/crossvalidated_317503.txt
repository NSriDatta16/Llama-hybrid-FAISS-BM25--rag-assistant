[site]: crossvalidated
[post_id]: 317503
[parent_id]: 205932
[tags]: 
Another advantage of doing the inverted dropout (besides not having to change the code at test time) is that during training one can get fancy and change the dropout rate dynamically. This has been termed as "annealed" dropout. Essentially the logic is that adding dropout "noise" towards the beginning of training helps to keep the optimization from getting stuck in a local minimum, while letting it decay to zero in the end results in a finer tuned network with better performance. ANNEALED DROPOUT TRAINING OF DEEP NETWORKS Modified Dropout for Training Neural Network
