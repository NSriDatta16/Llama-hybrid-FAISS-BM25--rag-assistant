[site]: datascience
[post_id]: 122380
[parent_id]: 56444
[tags]: 
It depends on what sort of model you're training and how you're going to be consuming these one hot encoded vectors. For example, if you're going to use a deep learning based approach, you probably don't need the one-hot encoded vectors, and you can use an embedding table to project to a higher dimensional space. If you're going to send this into a linear classifier, I'd like to think that it should not matter too much since the coefficients will be learned accordingly. Performing normalization does make the inference time a bit harder since you need to remember to apply the same normalization to ensure that the data statistics remain unchanged, so there's that. This has also been discussed here before Should one hot vectors be scaled with numerical attributes tl;dr try it and see the impact it has on your dataset and problem.
