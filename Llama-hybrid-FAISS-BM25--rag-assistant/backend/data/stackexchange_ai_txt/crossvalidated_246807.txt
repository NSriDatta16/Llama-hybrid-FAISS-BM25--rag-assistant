[site]: crossvalidated
[post_id]: 246807
[parent_id]: 246799
[tags]: 
Decision tree classifiers (DTC) can be tricky when you apply CV to them. In point of fact, I commonly don't employ CV for DTC and Random Forests (RF). Here's why: A DTC algorithm is a distant relative of RF, and since RFs bootstrap the entire data set without CV and randomly draw the features used for finding optimal node-splitting thresholds, they nevertheless use the entire dataset during a run. (When sampling with replacement to construct a bootstrap dataset, on average, 37% of the objects won't be selected and these are assigned to the out-of-bag "OOB" test sample, while the 63% of objects in the bootstrapped "in-bag" sample are used for training). Thus, RF does its own CV via use of what was (was not) selected without replacement. Further, a DTC is an algorithm for which you want to show the reader, audience, (manager), that when using all of the samples and informative features, these are the threshold feature cut-values that separate the majority of classes into final child nodes that have class purity. Now, if you perform CV with a DTC, a reader can ask: "So, based on the CV you performed, what is the final cutpoint of each feature that will allow me to make decision rules?" The problem is when creating your response, you realize the cutpoints changed every time a different set of training folds was used, because the feature values change with the training folds used. Now you think, well, maybe show a histogram of the cutpoints for each feature that was identified over all the CVs. But if you wanted to do this, you would just employ RF, which can provide importance scores for features, outliers, and can generalize better. A DTC can be more meaningful when all objects are used for a single run, using all informative features. If you want to start modifying DTCs with CV, it would be better to use RFs, for which everything already exists.
