[site]: crossvalidated
[post_id]: 605402
[parent_id]: 
[tags]: 
Discussion regarding a non-standard method to compare multiple groups of algorithms on different tests

TLDR: I am trying to compare groups of machine learning algorithms across a set of datasets. I understand the usual methodological procedure to compare the algorithms but I believe there is a different (and better?) procedure. This post discusses the standard and my new approach. I would like comments, criticisms, ideas, half-ideas, recommendations, opinions and so on regarding the new procedure. There are many mitigating algorithms for dealing with imbalanced datasets. These algorithms can be roughly thought as belonging to groups - oversampling strategies, undersampling strategies, special algorithms (such as RUSBoost), ensemble strategies (such as Easy Ensemble), class weighting strategies, and so on. More formally, there are k datasets DS1, DS2, DSk, and there are algorithms A-1, A-2, A-3, B-1, C-1, C-2, ... C-7, D-1, ... where A-i are the algorithms that belong to group A, B-1 is the sole algorithm that belongs to group B, and so on. The goal is to find which group of algorithms is better (have better performance across the set of datasets DSj). I am writing a 2nd version of this paper of mine on ArXiv, and one of the added content is this new methodology to analyze the results. The basic goal of the paper is to help a non-specialist user that has an imbalanced dataset problem to select a subset of the existing algorithms to test in his/her specific problem. I assume that the user has time and computer availability to test some alternative configurations regarding the imbalanced mitigation algorithms, base classifiers (discussed below), hyperparameters for both the base classifiers and the mitigation algorithms. But I would like to limit the number of mitigation algorithms the user have to test and I believe one can limit the alternatives by saying "search for algorithms within this group, say oversampling". But given the goals of the paper, I think there is a better (different?) methodological way of analysing the results. The methodological alternatives The standard method The usual method (which I will call "best across all datasets") is to follow a 2 level comparison. a) all algorithms within a group are compared to each other across all datasets. More commonly, algorithms are ranked within each dataset (1 for the best, 2 for the second best, and so on) and the algorithm with lower mean rank is considered the best. A-1, A-2, and A-3 are compared across all datasets, and the one with lower mean rank is considered the representative of the group A. b) the best of each group is considered the representative of that group, and the representatives are compared across all datasets (as before). Significance is computed using a number of possible different tests (Nemenyi or pairwise Wilcoxon signed-rank plus some p-value adjustment procedure). I used this methodology in the paper above, as did other papers in the same line: Mikel Galar, Alberto Fernandez, Edurne Barrenechea, Humberto Bustince, and Francisco Herrera. A review on ensembles for the class imbalance problem: bagging-, boosting-, and hybrid-based approaches. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 42(4):463–484, 2012. Victoria Lopez, Alberto Fernandez, Salvador Garcıa, Vasile Palade, and Francisco Herrera. An insight into classification with imbalanced data: Empirical results and current trends on using data intrinsic characteristics. Information Sciences, 250:113–141, 2013. Ronaldo Prati, Gustavo Batista, and Diego Silva. Class imbalance revisited: a new experimental setup to assess the performance of treatment methods. Knowledge and Information Systems, 45(1):247–270, 2015. The best for each dataset method The procedure I think it is an alternative is to select for each dataset, the best algorithm within the A group, and that combination of results (for the A group algorithm) is compared with a combination of results for the B group, a combination for the C group and so on. This final comparison follows the usual procedure, but the entries for the A group are not the entries for a single algorithm (say A-2 which was found to be the best across all datasets among the A group), but a combination of A-2 for DS1, A-3 for DS2, A-2 for DS3 and so on (assuming that A-2, A-3, A-2 and so on were the best ranked algorithms for DS1, DS2, DS3 respectively). I will call this strategy the "best for each dataset". The base classifier (or a nuisance factor) A second aspect of imbalanced mitigation strategies is that most of them are a kind of a pre-processing step (for example under and over sampling obviously), and once the pre-processing is done, a standard classifier is used - this is the base classifier. There are many, many different classification algorithms, and the usual is to run a set of them as the base classifier. In the paper above I decided to run only "strong" base classifiers like Random Forests, GBM, and Radial SVM. The other papers also have used a multitude of base classifiers. The standard way of using the base classifier information is (I believe) to average the metric across all base classifiers. Thus, the data relative to algorithm A-1 for DS4 is actually the mean of A-1-svm, A-1-rf, and A-1-gbm on DS4. (CAVEAT: I dont really know that that is the standard approach - it was the first alternative that came to me in the analysis, and now I am not 100% sure if any of the three papers listed above do make it explicit that they use this step.) The alternative approach (following the "best-for-each-dataset") is to select the best of A-1-svm, A-1-rf, and A-1-gbm on each of the datasets. Thus the entry for DS1 for may be that of A2-rf, while the entry for DS2 may be that of A3-gbm. Positive aspects of the best-for-each-dataset procedure: I think that the goal of this paper is to inform the reader that say, undersampling is the best "general strategy" to mitigate imbalance and that he/she should test a set of undersampling algorithms to solve his/her problem. I expect that the user will test some algorithms within undersampling with different base classifiers and select the best of them for that particular dataset, and I think the best-for-each-dataset method correctly reproduces what the user is expected to do. There is a similarity between this problem and the search for hyperparameters for classification algorithms. When one claims that gradient boosting or random forest are probably good algorithms for classification, one does expect that the user will search for the best hyperparameters for these algorithms. This case mimics the my assumptions for the reader of the paper - he/she will test different algorithms, but my goal is to help him/her to limit the algorithms to a few groups. Negative aspects of the best-for-each-dataset procedure: There is a problem with the new methodology: if a group has a lot of member algorithms, it is likely that it will rank higher that a group with few members. If there is a no-free-lunch like situation regarding mitigation algorithms for imbalanced datasets, that is, if the algorithms are "randomly" better or worse than the others, then a large group will find that some of its members is better for most of the datasets, and probably the group will rank better than a group with few or only one member. Yet another methodological alternative The best-for-each-dataset selects the best combination of imbalance algorithm and base classifier for each dataset (within each group) and compare the groups. The standard procedure averages the results for all base classifiers, and compare all algorithms within each group, and then compare the winners of each group to each other. There is a third alternative: select the best base classifier for each algorithm and dataset, but compare the algorithms within each group and then compare the winners of each group to each other. The main advantage of this third alternative is that it does not suffer from the dependence on the group size that the best-for-each-dataset procedure suffers. It does deal with the base classifiers as the user/reader will do: select the best base classifier. But the comparison among the algorithms follows the standard approach - and it does not depend on the group size. Does it matter which comparison procedure is used? I turns out it does. The figure below is a comparison of the three methods ("this.method" is the best-for-each-dataset, "alternative" is the alternative discussed above). The colors are the groups of algorithms. The vertical dark line indicates that there is no statistically significant differences between the groups. The groups are: BA: just the base classifies (1 member) CW: the base classifier with class weights (1 member) EN: ensemble of base classifiers (1 member) OS: oversampling (5 members) SA: specialized algorithms (3 members) US: undersampling (10 members) The results are very different depending on the comparison methodology used. But some of the results can be partially explained by the fact that larger groups have a competitive advantage in relation to groups with few members. US and OS are better using the new methodology maybe because that are large (10 and 5 members respectively). on the other hand, BA as CW are very small (1 member each) and perform very well on the new methodology SA may suffer from the problem that it does not have alternatives base classifiers (they are specializes algorithms with no base classifier). So, the selection of the best base classifier (both in the best-for-each-dataset and the alternative methodology seems an important competitive advantage... What do I want? I would like the input from the CV community regarding the best-in-each-dataset methodology (and/or the alternative methodology). It is a correct methodology given the goals of the analysis? Is the dependency on the number of members of each group a problem?
