[site]: crossvalidated
[post_id]: 380722
[parent_id]: 380708
[tags]: 
You have 30 total variables, making the possibility of generating all possible linear models nearly intractable: $$\sum^n_{k=1} \binom{30}{k} = 1.1 * 10^9$$ . Not to mention the addition of interactions, $$y \sim \beta_0 + \beta_1 a + \beta_2 b + \beta_3 (a*b)$$ . I agree with your idea to try VIF, because multiple co-linear features would increase your likelihood of over fitting. One possible solution to your feature selection issue would be to use Feature Importance generated from a random forest model. Python's library scikitlearn identifies this technique as tree-based feature selection . I agree that you have too few observations to ascertain the level of overfitting with 120 observations and 30 features in the model. However, if you reduce the feature space with feature importance or variance you will be much better off in relying on the CV results. Lastly, with your stated goal of using a linear model wherein you have 30 variables and some number of outcome variables (not sure if this is a classification or regression problem). A careful inspection of each of the 30 variables might lead to some better insight as to which offer more predictive power in specific scenarios as opposed to others.
