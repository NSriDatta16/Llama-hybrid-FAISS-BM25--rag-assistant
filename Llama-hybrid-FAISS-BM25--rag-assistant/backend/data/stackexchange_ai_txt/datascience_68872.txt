[site]: datascience
[post_id]: 68872
[parent_id]: 53110
[tags]: 
The output of the softmax is a probability distribution over all words in the vocabulary, so I assume that when you say "or the softmax output of the function", you actually mean the projection matrix from embedding space to logit space before the softmax. As you can see in equation (2) of the followup paper of the original word2vec article , the embedding matrix and the projection matrix are actually the same: $ \DeclareMathOperator{\exp}{exp} p(w_O | w_I) = \frac{\exp({v^{\prime}_{w_O}}^T v_{w_I})}{\sum _{w=1}^W \exp({v^{\prime}_{w}}^T v_{w_I})} $ Embedded vectors are a reflection of word co-occurrence statistics of the data they are trained on and therefore they depend totally on the training corpus. For instance, embeddings trained on a news corpus may give totally different vectors than if we used a corpus of children's books, even if both embeddings where defined over the same vocabulary.
