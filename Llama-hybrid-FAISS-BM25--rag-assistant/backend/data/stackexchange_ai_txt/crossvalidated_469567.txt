[site]: crossvalidated
[post_id]: 469567
[parent_id]: 141619
[tags]: 
One thing to add to above explanations: based on the experiments in Genuer et al, 2010: Robin Genuer, Jean-Michel Poggi, Christine Tuleau-Malot. Variable selection using Random Forests. Pattern Recognition Letters, Elsevier, 2010, 31 (14), pp.2225-2236. When the number of variables were more than the number of observations p>>n, they added highly-correlated variables with the already-known important variables, one by one in each RF model, and noticed that the magnitude of the importance values of the variables changes (less relative value from the y axis for the already-known important variables) BUT the order of importance of variables remained the same and even the order of the relative values remains pretty similar, and they are still significantly recognisable from noisy variables (less-relevant variables). Also check the table in page 2231 when the number of replications (adding highly-correlated variables with two of the previously-known most important variables) increases, the prediction set for each RF model still shows the most important variable is the already-known most important variable. for variable selection for interpretation purposes, they construct many (e.g., 50) RF models, they introduce important variables one by one, and the model with lowest OOB error rate is selected for interpretation and variable selection. for variable selection procedure for prediction purposes, "in each model We perform a sequential variable introduction with testing: a variable is added only if the error gain exceeds a threshold. The idea is that the error decrease must be significantly greater than the average variation obtained by adding noisy variables. "
