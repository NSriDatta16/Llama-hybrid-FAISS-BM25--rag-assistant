[site]: datascience
[post_id]: 66399
[parent_id]: 66394
[tags]: 
BERT is not trained with this kind of special tokens, so the tokenizer is not expecting them and therefore it splits them as any other piece of normal text, and they will probably harm the obtained representations if you keep them. You should remove these special tokens from the input text. In the case of GPT-2, OpenAI trained it only with , but it has to be added after the tokenization . Some people mistakenly add it before tokenization, leading to problems . is specific to the library gpt-2-simple .
