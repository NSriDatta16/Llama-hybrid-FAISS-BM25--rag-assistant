[site]: crossvalidated
[post_id]: 278806
[parent_id]: 
[tags]: 
Truncated Back Propagation of LSTM with K2 > K1

Every K1 time steps we will run truncated back prop through time (BPTT) for k2 time steps. For k2 k1, and particularly where k1 = 1 (you do BPTT after every forward time step) In this case, the weights used in each previous time step will be different from what they are at the current time step, because they change at every time step. Since the weights are used in calculating the gradients, it's not as simple as the case for k2 I see two ways to handle this - You can keep track of the weights as they were in the past, and as you back prop you use the weights as they were in whatever time step you are on. Or before you back prop, you can run another forward pass starting k2 time steps ago and get all new activation values using just your current set of weights, and then do your back prop using just one set of weights. Either of these will certainly learn. But which one would be the standard way of doing it? Edit: I experimented with storing the past weights. I also read Williams and Peng's 1990 paper on training RNNs which mentions this, saying "When the weights are adjusted on every time step, it may be appropriate to store all the past weights as well, using these on the backward pass rather than the current weights. This variant obviously requires O(n2h) storage. In practice, we have not found much difference in performance between this version and the simpler one described here" My experiments agree with this surprisingly, and almost no difference is seen between the two. But this led me to another interesting observation. Even when using the past weights during back prop, you still don't get the exact "true" gradient. The reason behind this is that you are treating the weight as if it were one variable used multiple times, finding the gradient for each time it is used across multiple time steps and summing to get a final gradient. But really the weight as it was for each time step in the past is its own variable. You can't sum these together to get an exact gradient for the weight no matter how you do it. The only way I can see around this (if you wanted an exact gradient for your past k2 time steps) would be to run another forward pass starting k2 time steps ago to get all new values before you back prop. But this would be computationally expensive. Will edit again soon to compare and see if this is worth doing. Edit2: I made a "sliding window" version of BPTT where at each time step t you forward propagate through k2 time steps, then back prop through those k2 time steps, update the weights, then increment t by 1. I compared this to the version where at each time step t you do a single forward pass, back propagate through k2 times steps, update the weights, then increment t by 1. In the sliding window approach, you can get an "exact" gradient because all of your forward passing uses a fixed set of weights. In the other version, the weights are different at each time step you back prop through, so the gradient you come up with is more of an approximation, even if you store the past weights and use them. The advantage here is you only have to do a single forward pass before you begin updating weights so it gets updated more often. And to clarify what I mean by "true" or "exact" gradient, I mean that you can numerically check the gradient by wiggling the weight up and down by a tiny amount and seeing how it affects the cost, and compare this to your gradient. Here I compare the cost for sliding window (blue) and the single forward pass version (red). The x axis is training time rather than epochs to keep things fair. In all my experiments, both versions were able to reach about the same minima, but the single forward pass always gets there faster. And here the x axis is the number of iterations, and you can see that these two perform just about exactly the same per iteration, the sliding window is just slower. It also shows that although it is an approximation to the gradient, it is a very good approximation. I also tested both of them with k2 = 1 to check that they produce identical results, and they did as expected. In conclusion, the best way to do it seems to be with a single time step forward pass, then back propagate through k2 time steps. Whether you save the past weights or not doesn't really matter. These experiments were done on text, trying to predict the next letter in a sentence. I am curious if anyone else has reached any conclusions on this or can link to other information about it.
