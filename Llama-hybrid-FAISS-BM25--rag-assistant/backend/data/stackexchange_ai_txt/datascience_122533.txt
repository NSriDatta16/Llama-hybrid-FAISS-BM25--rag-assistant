[site]: datascience
[post_id]: 122533
[parent_id]: 
[tags]: 
Prevent Overfitting in Transfer Learning with small data

I have built a feed forward neural network to predict heat pumps energy consumption. Now, i want to use this model as a domain for other heat pumps via transfer learning. I want to simulate the case where a new heatpump is integrated in an energy management system. The simulation is the following: A new heatpump arrives with not many data available (2 weeks of data in a daily solution - 14 data points). I want to make 7 days of predictions and then add the actual values of these 7 days to training set and retrain the neural network and so on. For this simulation i use TimeSeriesSplit to create the training and validation splits. I am encountering the problem that there are some folds where, in the beginning, when not much data is available, the model is clearly overfitting which is indicated by huge validation errors (see picture for the third run): This is the model summary: I tried to use different models for this purpose. First i freezed only the bottom layer, then also the top layer. I removed the bottom dropout layer and added a new one with higher dropout rates. I also tried to reduce the epochs and learning rate for early folds. Still, there are these huge error folds. I know, adding more training data will certainly overcome this. And in fact, after certain folds are added to the training data, this wont happen anymore. But for this case, i want to use as minimal data as possible to beginn the predictions in the energy management system. Thus, is there a way to overcome these errors?
