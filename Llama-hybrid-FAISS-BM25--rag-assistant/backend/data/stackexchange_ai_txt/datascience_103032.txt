[site]: datascience
[post_id]: 103032
[parent_id]: 102599
[tags]: 
Another way to approach the problem is to take all of the trained models and compare each of their performances on the same hold-out dataset. This is the most common way to evaluate machine learning models. Choosing the evaluation metric to use depends on the goal of the project. Most machine learning projects care about predictive ability. RÂ² is not a useful metric for the predictive ability of a model. RMSE can be a useful metric of predictive ability. However, since the errors are squared is sensitive to the properties of the data. You mention that you are using different data. Those differences in data could impact comparing RMSE across different sources. Comparing different models on the same dataset would be better when using RMSE.
