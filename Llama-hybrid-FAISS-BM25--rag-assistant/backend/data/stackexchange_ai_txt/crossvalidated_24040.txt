[site]: crossvalidated
[post_id]: 24040
[parent_id]: 24034
[tags]: 
I know of a few common ways to select PCs. With regards to feeding it to downstream analysis it is commonly done with a cut-off value $\epsilon$ like so. So you have $n$ eigenvalues from the decomposition sorted decreasingly. One can construct a pareto curve where each successive entry i is given by the normalized cumulative sum, e.g. $P(i) = (\sum \lambda_1,..,\lambda_i)/(\sum \lambda_1,...,\lambda_n$). Each entry tells you the fraction of the variance explained by considering up to the $ith$ eigenvector. You pick some value $\epsilon \in (0,1)$ to cut that curve and use all the eigenvectors that are required to explain say, 99% of the variance. Common values are 90%, 95%, 99% and 99.9%. Intuitively this is just a orthogonal component denoising step. This is a rather small loop to try out values on. It requires only one eigendecomposition and 4 runs of your downstream analysis. Furthermore this method naturally extends to kernel PCA methods. I know of another way to select PCs, that to my knowledge isn't popular for feature extraction, but is popular for other analysis. Your pareto curve is monotonically increasing and contains a elbow defined by $max(P(i) - P(i-1))$. The idea being that everything before the elbow is the signal and everything after is the noise. You can try it, but it always gave me bad results for downstream analysis. If you want to consider your labels you can get pretty fancy with factor analysis. There's also the option of testing the dependency of your PC's against your labels, where you keep the ones that are above a threshold for shuffled data. I'm not a big fan of feature selection in this context though because it ignores interaction between features, as in the case of say a XOR gate . Also it sounds like the decomposition was a fruitful effort, you might pursue this further actually using the (weighted?) labels to find the decomposition, i.e. studying cross-covariance matrix decompositions upstream to your classifier.
