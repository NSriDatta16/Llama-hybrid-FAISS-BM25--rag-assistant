[site]: crossvalidated
[post_id]: 312457
[parent_id]: 312453
[tags]: 
For starters you need to realize their example applies to a situation where you'd want to predict the probability of a binary outcome. I.e. does the outcome occur (class '1'), or not (class '0')? Additionally, they use multiple prediction models to estimate the probabilities of these classes. Finally, using some weighting scheme to weigh these models' estimates, the weighted average probability of these estimates is used as the final estimated probability of class (0 or 1) membership. So when referring to their formula: $y$ is the weighted average predicted probality of class membership $p_{ij}$ is the probability for individual $i$ for class membership of class '1', according to classifier/prediction model $j$. $w_j$ is the weight for prediction model $j$'s prediction for ${\operatorname{argmax}}$ see here EDIT after clarification was asked how to estimate $p_{ij}$: any/most model(s) will be able to provide you with the probability of an 'outcome variable'. This means that without further context, any model which provides such estimates of probability will do (for example a logistic regression). Finally, the 'soft voting' from the article you refer to is specifically designed to combine the estimates of multiple (different) models.
