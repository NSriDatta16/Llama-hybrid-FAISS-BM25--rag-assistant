[site]: datascience
[post_id]: 8569
[parent_id]: 8568
[tags]: 
In theory, if you have a large enough random sample of your data set, it should be representative of the characteristics in the larger population of data that will affect the relationship between parameter values and performance. It seems to me that a $5\%$ sample of your data might be too small for what you're trying to do. When I'm performing cross-validation on a test/dev data set, I usually set aside somewhere in the neighborhood of $10\%$ for the hold-out test/evaluation data, and use $90\%$ for my development work, but, as you pointed out, this may be too big given the computational constraints you're up against. Depending on the type of application you're working on, you could look at feature distribution in your sampled data and compare it to that in the larger population. If the two sets are comparable, then you might be ok to do your dev. work on the smaller data set. I say it depends on the type of application you're developing, because many statisticians would consider this cheating, in that it's generally not good model evaluation practice to look at hold-out data at all. Here's the procedure I'd recommend: Separate out $10\%$ of your data for hold-out evaluation. Divide the remaining $90\%$ into $5\%-10\%$ parameter optimization, and the remainder for final parameter eval. Compare the feature distributions of the parameter optimization and final parameter eval data sets, if they're not comparable, redraw the samples (within the context of the $90\%$ development sample) When you have a good subsample, run your parameter optimization experiments on the small parameter optimization data set. With your final parameter settings, evaluate with cross-validation on the the $90\%$ combined parameter optimization and final parameter eval data sets. Perform final model analysis by training on the $90\%$ data set and classifying on the $10\%$ hold-out evaluation set. It might also be worth looking into optimizing your analytics pipeline. For example, is file I/O, feature generation and feature extraction part of the the workflow you're using, or have you done all of that off-line and are just concerned with the SVM evaluation part?
