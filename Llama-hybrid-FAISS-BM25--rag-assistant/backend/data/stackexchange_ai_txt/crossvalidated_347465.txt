[site]: crossvalidated
[post_id]: 347465
[parent_id]: 
[tags]: 
SVM decision non linear

As I understand, to perform a decision in a non linear case (using a kernel) I use the following: $f(x) = sgn(\sum_{i=1}^{n} y_{i} \alpha_{i} \boldsymbol{k}(x,x_{i})+b)$ Where $i=1,\dotsc,n$ are the encountered support vectors. So if I want to classify a new incoming test data vector $x$ , I must to make a sum among all the encountered support vectors. An example of the output of Weka when training with the iris-setosa database using the PUK kernel is: Classifier for classes: Iris-setosa, Iris-versicolor BinarySMO - 1 * * X] - 0.8281 * * X] + 1 * * X] - 1 * * X] - 0.1813 * * X] + 0.7577 * * X] + 0.2662 * * X] - 0.1038 * * X] + 0.6434 * * X] - 0.1494 * * X] + 0.0376 * * X] + 0.5577 * * X] - 0.0052 Number of support vectors: 12 So for every test point, I must perform all the formula above, which already describes $y_{i}$ , $\alpha_{i}$ and the kernel operation required among the support vectors and $x$ . If I want to classify out of Weka, lets say in an app in android. Should I have a copy of all the encountered support vectors, $y_{i}$ , $\alpha_{i}$ and then do that opperation? or there is a way of evaluating only an hyperplane without requiring all the support vectors such as in the linear case? For example, the next weka results are using the polykernel with exp=1. classifier for classes: Iris-setosa, Iris-versicolor BinarySMO Machine linear: showing attribute weights, not support vectors. 0.6829 * (normalized) sepallength + -1.523 * (normalized) sepalwidth + 2.2034 * (normalized) petallength + 1.9272 * (normalized) petalwidth - 0.7091 In this former case, I only evaluate the hyperplane but not across the support vectors.
