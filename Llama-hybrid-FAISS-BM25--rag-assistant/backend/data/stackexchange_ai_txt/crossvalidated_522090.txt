[site]: crossvalidated
[post_id]: 522090
[parent_id]: 
[tags]: 
Question related to using Pooled Output from BERT for similarity between sentences

I was hoping someone could give me advice and feedback on my current approach and possibly suggest to me a possible alternative. I'm trying to find the sentences that are most similar using the pooled output from the CLS token of BERT after the BERT has been trained on my data set. The pooled output returns a vector of 768 numbers for every entity in the data set. Once I get this output, I'm separating the vector into 768 separate columns and then calculating the cosine similarity for the entire data frame. Since my goal is to find the sentences that are most similar among all the vectors in my data frame, I'm then taking the average of each row from the output that I receive from the cosine similarity using the cosine similarity function in sklearn package. One possible thing to note here is that I do get negative cosine similarities at times. I believe this is because the values for the BERT pooled output is not normalized, but I don't think I should normalize the pooled output prior to calculating the cosine similarity because then the cosine similarity might not truly reflect the BERT pooled output. Does anyone have any problems with this approach? Is there a better approach I can take when using the pooled output from a BERT to look at similarity among text?
