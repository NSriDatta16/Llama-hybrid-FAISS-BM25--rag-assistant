[site]: crossvalidated
[post_id]: 240260
[parent_id]: 
[tags]: 
Why does the direction with highest eigenvalue have the largest semi-axis?

So, in PCA, we decompose the covariance matrix by its eigenvalues and eigenvectors. I understand that an ellipsoid is fully characterized by the eigenvalues and eigenvectors of a positive definite matrix $A$, and it has an equation $(x-x_c)^T A (x-x_c) =1$, where $x_c \in \mathbb{R}^n$ denotes the center of the ellipsoid. In PCA we are basically trying to fit an ellipsoid to the data from what I understand. I also understand that $i$-th principal direction of the ellipsoid is given by the direction of the $i-$th eigenvector $v_i$ associated with the eigenvalue $\lambda_i$, sorted out so that $\lambda_1 > \ldots > \lambda_n > 0$. And finally, $\lambda_i$ defines the inverse square of the associated semi-axis, so $r_i = \frac{1}{\sqrt{\lambda_i}}$ is the semi-axis of the $i$-th principal direction, right? Now, wouldn't this mean that $r_1$ is the smallest of all other $r_j$? But that can't make sense I think, because then why would we project our data in that direction over any other if we want to reduce the dimension of our data? I know there has to be something wrong in my reasoning, I'm in no way suggesting PCA is wrong, but I would appreciate some clarification.
