[site]: datascience
[post_id]: 45637
[parent_id]: 45633
[tags]: 
Another approach could be to train several models and then simply take the average of their predictions - essentially using an ensemble . It will be necessary to initialise the models differently and ensure there is some randomness/stochasticity during training such that the models are indeed different. This approach should allow your grupo of models to tolerate one or two models giving odd predictions in each new prediction, while still giving the most sensible result. Studies show that this can dramatically smooth predictions. Have a look at this comparison of use cases and various ensemble technique from Armstrong . There are some guiding principles suggested, depending on saveral factors such as domain knowledge and variance of predictions.
