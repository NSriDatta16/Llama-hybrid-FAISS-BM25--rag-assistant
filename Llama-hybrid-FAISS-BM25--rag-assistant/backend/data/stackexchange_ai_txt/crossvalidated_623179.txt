[site]: crossvalidated
[post_id]: 623179
[parent_id]: 623039
[tags]: 
It's quite simple. Let me change the notation a bit, let's say that $$\mathcal{D}_{new} = \Big\{(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)\Big\}$$ so it has size $|\mathcal{D}_{new}| = n$ . Now, we can write $$ \beta = \frac{\tfrac{1}{n} \sum_{i=1}^n y_i}{\tfrac{1}{n} \sum_{i=1}^n f(x_i)} $$ The average $y_i$ is $\beta$ times higher (or lower) than the average $f(x_i)$ . To correct the predictions (on average) you just multiply it by $\beta$ . This works by the linearity of expectation $E[cX] = c E[X]$ . Since $\tfrac{1}{n}$ s cancel out, we don't need them in the formula and can work with the sums rather than the averages. If you care about the totals, the interpretation makes sense also in terms of the totals. This assumes that there is a multiplicative difference between the observed values and the predictions $E[y] = \beta E[f(x)]$ . If there would be an additive difference (the predictions on average would be off by a constant $\alpha$ ), you could calculate $\alpha = E[y] - E[f(x)]$ and correct it $E[y] = E[f(x)] + \alpha$ . Notice that it does not aim to correct the individual predictions, just their average. It assumes that the function $f$ is correct, but off by a multiplicative constant. $\bar y \big/ \bar x$ wouldn't work for estimating the regression coefficient as in a linear regression model. In this sense, this is not really a regression model. $E[y|x] = \beta x + \varepsilon$ would be a regression model, while you have $E[y] = \beta E[x]$ .
