[site]: datascience
[post_id]: 71471
[parent_id]: 
[tags]: 
Performance Metric Over Expected Value Functions that are Changing

I am trying to find a way to calculate a profitability metric for a model of mine. I currently have data labels corresponding to the value earned during each certain move made in my training set. My goal is to analyze how close my model is to these values by comparing these to the models expected value predictions for each possible move to be made. The problem here is that the value will almost never equal the exact Expected Value, but I want to get some sort of evaluation metric of how close these predicted EV's are to the labels on average. I know that with Expected Values, given the same formula v * p where v (values) and p (probabilities) are vectors of size n, over a large number of simulations the average of these values will converge to the expected value. But for my problem each EV formula is different for every training point which poses problems. The result I came up with includes doing the same exact thing over a large number of iterations and take the average of the differences between Actual Value - Expected Value. My goal was to evaluate a perfect model which has the correct expected value equation and use this as the max limit for what my model can achieve. I just need some help interpreting my result, or hopefully finding a better way where I can get a value that can be directly represented as an expected value of profitability. The code below is my current solution. When num_options is very large the np.mean(differences) converges to 0. But for small num_options like 3 (which represents my problem) it always converges to a certain number. I need help figuring out what this number is so I can interpret the result. Does anyone have any idea? I'm assuming it has to due with the spread of data within the values vector, because if you have more num_options you are more likely to pick an EV thats closer the Value. Code: import numpy as np from bisect import bisect_right import random enumerations = 100000 num_options = 2 #Initialize Random probability vectors and random value vectors of size num_options probability_vectors = [np.random.random(num_options) for _ in range(enumerations)] value_vectors = [np.random.random(num_options) for _ in range(enumerations)] differences = [] for i in range(enumerations): #Normalize for values and probabilities probabilities = [j / sum(probability_vectors[i]) for j in probability_vectors[i]] values = [j / sum(value_vectors[i]) for j in value_vectors[i]] #Calculate EV EV = np.dot(values,probabilities) #Choose random value running_sum = np.cumsum(probabilities) random_number = np.random.random(1)[0] chosen_outcome = bisect_right(running_sum,random_number) value = values[chosen_outcome] differences.append(abs(value - EV)) print(np.mean(differences)) print(np.std(differences)) With large enumerations: num_options = 2 : 0.1490140542519784 num_options = 3 : 0.1275503821941484 num_options = 4 : 0.1045104868184497 num_options = 5 : 0.08740796919164531 num_options = 6 : 0.07480764714073941 num_options = 7 : 0.06507271751946012 num_options = 8 : 0.05784730185234172 num_options = 9 : 0.051894809981252676 num_options = 10 : 0.04697499914796892 num_options = 11 : 0.04300556153302974 num_options = 12 : 0.039613242732981065 num_options = 13 : 0.036683873410078144 num_options = 14 : 0.034154073132997134 num_options = 15 : 0.0320761992188373 num_options = 16 : 0.030082494700577653 num_options = 17 : 0.028371757269382177 num_options = 18 : 0.02695502418214529 num_options = 19 : 0.025488676908302983 .... num_options = +infinity = 0 So Questions: What is this offset from 0? Is there a better approach to calculate a long term profitability metric over Expected Value equations that keep changing?
