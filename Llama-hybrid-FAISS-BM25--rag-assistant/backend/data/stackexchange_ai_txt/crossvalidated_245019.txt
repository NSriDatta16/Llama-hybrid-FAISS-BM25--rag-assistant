[site]: crossvalidated
[post_id]: 245019
[parent_id]: 153848
[tags]: 
I suppose you're refering to Bishop's book "Pattern Recognition And Machine Learning". If yes one should first mention those notations are a bit confusing as $p$ is used for both the measure on the implicit probability space and densities for all continuous random variables. Let's make this more explicit. We have a probability space $(\Omega, \sigma_\Omega, P)$ with probability measure $P$. Let's remind that a random variable $X$ defined on $\Omega$ has a density if function $x \in \mathbb{R} \mapsto P(X \leq x)$ is differentiable. In that case we can define its density as: $$f_X : x \in \mathbb{R} \mapsto \frac{d}{dx}P(X \leq x)$$ For two continuous random variables $X$ and $Y$ we can defined the joint density $$f_{X,Y} : (x,y) \in \mathbb{R}^2 \mapsto \frac{\partial}{\partial x}\frac{\partial}{\partial y}P(X \leq x \wedge Y \leq y)$$ What Bishop calls the sum rule is then the fact that: $$f_X(x)=\int_{y \in \mathbb R} f_{X,Y}(x,y) dy$$ which is pretty straightforward: $$\int_{y \in \mathbb R} f_{X,Y}(x,y) dy = \int_{y \in \mathbb R} \frac{\partial}{\partial x}\frac{\partial}{\partial y}P(X \leq x \wedge Y \leq y) dy$$ $$= \frac{d}{dx}P(X \leq x \wedge Y \in \mathbb R) = \frac{d}{dx}P(X \leq x) = f_X(x)$$ To my knowledge the product rule just follows from the definition of the conditional density: $$f_{Y|X}(y|x) := \frac{f_{X,Y}(x,y)}{f_X(x)}$$
