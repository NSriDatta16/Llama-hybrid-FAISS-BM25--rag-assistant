[site]: datascience
[post_id]: 122914
[parent_id]: 
[tags]: 
Are there other "interactive" non-linear neural network layers besides self-attention layer?

In the self-attention layer $$ \operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V $$ $Q$ , $K$ and $V$ are all linear with respect to embedding vectors $x$ , thus making the result a complicated non-linear function with respect to components of $x$ . Because of the scalar product, the argument to softmax is a second degree polynomial $\sum a_{ij}x_ix_j$ mixing different $x$ components. As it is normally taught, neural networks contain strictly linear layers or non-linearities which act on a single output (ReLU, tanh etc). Are there other popular NN constructs which implement some form of highly non-linear functions which cross-mix the input components and what are they? If not, why?
