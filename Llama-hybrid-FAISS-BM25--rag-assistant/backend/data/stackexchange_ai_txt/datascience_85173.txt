[site]: datascience
[post_id]: 85173
[parent_id]: 85150
[tags]: 
What is the value of Key, Value in the self attention calculation of Transformer model? You have the queries matrix Q. Imagined it to a coordinate system. the x-axis is the key and y-axis is the resulting value. Query vector is embedding vector for the word that is queried, is that right? Yes, for the word the word or token Is attention calculated in RNN is different from self attention in Transformer? RNNs is a family of neural networks. They don't include attention per se. A resource, that helped me greatly is: https://jalammar.github.io/illustrated-transformer/ maybe it will you also to better understand transformers.
