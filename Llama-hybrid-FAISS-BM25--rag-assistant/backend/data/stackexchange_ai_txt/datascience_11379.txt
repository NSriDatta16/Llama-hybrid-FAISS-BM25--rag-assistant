[site]: datascience
[post_id]: 11379
[parent_id]: 
[tags]: 
How can decision trees be tuned for non-symmetrical loss?

Suppose we use a decision tree to predict if a bank customer can pay back a credit. So it is a two class classification problem. Now we can make two mistakes: $\alpha$ error: The customer can back the credit, but we predict he can't. $\beta$ error: The customer can't pay back the credit, but we predict he can. Now we know that $\beta$ errors are 123.4 times as expensive as $\alpha$ errors. But we only have a given set of data. In this set we have $n_1=10000$ customers who paid back the credit and $n_2 = 100$ customers who didn't. How can the training of the decision tree be adjusted to account for the fact that $\beta$ errors are more expensive? (Note: This is a theoretical question to learn about decision trees. I know about other classifiers like neural networks and I know of ensembles. However, I only want to know about decision trees here.)
