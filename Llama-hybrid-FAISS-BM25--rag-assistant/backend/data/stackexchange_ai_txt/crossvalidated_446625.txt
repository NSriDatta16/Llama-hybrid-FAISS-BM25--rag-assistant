[site]: crossvalidated
[post_id]: 446625
[parent_id]: 446560
[tags]: 
Let's start by reviewing what linear regression does. The typical setup is that we posit that there is some phenomenon occurring that follows distribution with a mean that changes, depending on the input variables (predictors). We use regression to estimate this conditional mean (expected value), conditioned on the predictors. In your case, your predictors are $a$ , $b$ , $c$ , and $d$ . For each subject $i$ , you get some $a=a_i$ , $b=b_i$ , $c=c_i$ , and $d=d_i$ . This is how we wind up with $\widehat{y}_i = \mathbb{E}[y\vert a=a_i, b=b_i,c=c_i, d=d_i]$ (maybe throw a hat over the expected value like $\widehat{\mathbb{E}}$ , since it's still just an estimate). We want to know the average value of $y$ that we would get when $a=a_i$ , $b=b_i$ , $c=c_i$ , and $d=d_i$ . The key point here is that there is a full distribution at each combination of $a$ , $b$ , $c$ , and $d$ values. We might not be interested in the mean. The median or first quartile or 90th percentile might be more interesting, and this is where quantile regression comes in. Instead of estimating the conditional mean, quantile regression estimates the conditional quantile. Therefore, we do not write the linear combination of predictors as the expected value of the conditional distribution but as a particular quantile of the conditional distribution. But that's all you change. The side of the equation with the parameter estimates is the same, except that they are not estimated via least squares. In matrix notation, we go from $\mathbb{E}[y\vert X] = X\hat{\beta}_{ols}$ to $ Q_{q_0}(y\vert X)= X\hat{\beta}_{q_0}$ , where $q_0$ is the quantile of interest. The notation I'm using is what I like at this particular moment but is not universal. However, I think it captures all pertinent information: that it is a quantile regression, the quantile under consideration, and that there is a conditional distribution at play.
