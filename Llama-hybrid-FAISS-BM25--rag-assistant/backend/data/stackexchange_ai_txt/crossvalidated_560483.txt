[site]: crossvalidated
[post_id]: 560483
[parent_id]: 
[tags]: 
Many-to-many LSTM with large number of (correlated timestamps)

We are considering building a set of CRM tools (i.e. churn) using LSTM network for our online store. LSTM is chosen since it can handle naturally sequential nature of our (i.e. transactional) data, and since we can interpret the hidden states as customer embedding. Also, we found many papers (i.e. https://ieee-cog.org/2019/papers/paper_16.pdf ) that we can take as benchmarks. One of the requirements we have is that we have daily predictions, so my initial idea is to simply use daily aggregated data and take 365 timestamps as the length of the LSTM layer (a long history is intuitively needed since we want to learn about rare situations and customer behaviour in it). In that case, the information going to the neighbouring cells is going to be heavily correlated (little new info), since it can happen that there was no activity between two days. Is this a problem? Intuitively, I don't see an issue, but I did not see application as this one, so am wondering if anybody has something to add. Also, since we are looking to serve predictions daily, many-to-many network, where hidden state of each cell is going to be transferred to its own softmax, seems like a way to go. All in all, using this approach, the network would be fed (no_users, no_timestamps=365, no_features) data, and for each user would need to spit out 365 outputs (did he churn on that day). This daily data is going to be highly correlated. Does this look like a good setup, or we should think about aggregating, for example, daily into weekly data and reduce the number of timesteps in the network. Thanks!
