[site]: crossvalidated
[post_id]: 262480
[parent_id]: 233248
[tags]: 
eta introduces a 'relative' regularization (multiplying the weight by a constant factor) but in extreme cases where hessian is nearly zero (like when we have very unbalanced classes) this isn't enough because the weights (in which computation the hessian is in denominator) becomes to nearly infinite. So what max_delta_steps do is to introduce an 'absolute' regularization capping the weight before apply eta correction. If you see the code of xgboost (file parameter.h, procedure CalcWeight), you can see this, and you see the effect of other regularization parameters, lambda and alpha (that are equivalents to L1 and L2 regularization). In special lambda effect complement (or may substitute) max_delta_step, as a lambda greater than zero do the weight smaller.
