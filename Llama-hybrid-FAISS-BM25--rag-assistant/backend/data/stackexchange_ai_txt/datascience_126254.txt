[site]: datascience
[post_id]: 126254
[parent_id]: 126252
[tags]: 
There are two generic solutions/ post-processing steps to work around hitting the max_token limit irrespective of the model/ LLM you are consuming in langchain . Retain the top_n matches or chunks based on similarity scores where top_n would depend on the average chunk size and max_token restriction of the LLM in consumption. Extract only relevant part of the document returned from vector search. You can look into this langchain blog for Contextual Compression.
