[site]: crossvalidated
[post_id]: 204282
[parent_id]: 76830
[tags]: 
Most classifiers are notoriously bad at unbalanced problems because they optimize error, accuracy or f-measure which is basically saying all INSTANCES are equal. On the other hand measures like ROC AUC, Gini, Informedness, Correlation and Kappa are designed on the assumption that all CLASSES are equal. Another possibility is that you COST classes or instances explicitly and use that as a basis for optimization. In your case, saying everything is C will net you an accuracy of 80.7 and most neural network and tree classifiers have a bias to this for such an unbalanced set. However, the chance-correct evaluation of this is 0 (Gini=Youden=Informedness=Correlation=Kappa=0 means chance level performance is obtained). Any model which allocates A% to A, B% to B, C% to C randomly should average out (over multiple runs) to 0 for these measures. The "say everything is C" model arbitrarily allocates 0% to A, 0% to B, 100% to C (randomly with the indicated probability). You need to look for algorithms that optimize the measure you are interested in, viz. ROC AUC or Gini or Informedness (chance-correct learning), or explicitly balance (balanced learning), or allow you to specify class or instance costs explicitly(cost-sensitive learning). On the other hand, some algorithms explicitly debalance (nodes in a decision tree or stages of boosting for example). I've written several papers about these issues, including deriving chance-correct versions of both NN and Boosting algorithms. There are a lot of people using Kappa and AUC - but you need to look particularly at those addressing the multiclass case. And there is a growing literature on balanced/unbalanced/rebalanced learning.
