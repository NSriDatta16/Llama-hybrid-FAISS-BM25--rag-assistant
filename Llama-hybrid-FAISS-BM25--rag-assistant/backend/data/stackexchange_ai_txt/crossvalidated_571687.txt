[site]: crossvalidated
[post_id]: 571687
[parent_id]: 564878
[tags]: 
In nested CV, the average of the errors of the outer loop can be seen as the expected error of the model building process (including the hyperparameter tuning). Let me try to clarify this. First, let us consider a non-nested CV case where the number of folds is 5. Suppose that in each fold a tree-based model is built with the default hyperparameters. In this situation, even though we are using the same learning algorithm with the same parameters, in each fold a different model will be built since the training data used by the algorithm is different in each fold. So, we have 5 different models and a different error value for each model. Now, you can ask a similar question, what does the average error (over 5 folds) represent? In this case the average error represents the expected error of this particular model building process. Similarly, in the nested CV case, the average error represents the expected error of the model building process. Different from the non-nested CV case, in nested CV the model building process includes a grid search over the hyperparameters. Hope that makes sense.
