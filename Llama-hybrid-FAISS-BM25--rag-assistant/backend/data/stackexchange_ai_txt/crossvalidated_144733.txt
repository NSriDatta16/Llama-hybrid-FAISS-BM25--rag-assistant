[site]: crossvalidated
[post_id]: 144733
[parent_id]: 
[tags]: 
Can I use ReLU in autoencoder as activation function?

When implementing an autoencoder with neural network, most people will use sigmoid as the activation function. Can we use ReLU instead? (Since ReLU has no limit on the upper bound, basically meaning the input image can have pixel bigger than 1, unlike the restricted criteria for autoencoder when sigmoid is used).
