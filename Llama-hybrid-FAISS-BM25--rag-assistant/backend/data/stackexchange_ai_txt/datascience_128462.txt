[site]: datascience
[post_id]: 128462
[parent_id]: 128428
[tags]: 
Can I ask what the logic is for dividing the log-differenced number of sales by the log-differenced moving average of price? Not saying the logic is wrong, I just haven't really seen this before. Wouldn't the scale of the elasticity be on the differenced series rather than the original series? IMO, an easier way would be to use basic linear regression, with a logged response. Maybe fit a model (for the ith observation in the jth category) such as: $$\text{log(sales}_i) = \beta_0 + \beta_1 \text{time}_i + \beta_2\text{log(price}_i) + \beta_3\text{category}_{ij} + \beta_{4,j} \times\text{log(price}_i)\times\text{category}_{ij} + \epsilon_i. $$ The model above would fit separate elasticities for each product category, where you would be interested in the quantity $\beta_2+ \beta_{4,j}$ from your model. The interpretation of this would be "a one percent change in price yields a $(\beta_2+ \beta_{4,j})$ % change in sales for category j, on average (holding all else constant)". Check for violations in homescedasticity and/or auto correlated residuals by time + product and maybe consider the use of sandwich estimators for standard errors to account for any violations. Another alternative that is more in line with what you were doing: consider an ARIMAX model, using the exact model above as exogenous regressors (I'd probably drop time as an independent variable in this case though) but model $\epsilon_i$ as an ARIMA process. This would also correct for any kind of auto correlation in your errors. You would conduct inference in the exact same way as described above. Yet another option is to consider mixed effect models (include a random intercept by product category, but don't include a random slope by product category since you are interested in estimating elasiticities by category). These models will likely be more efficient than the models above, albeit more complicated.
