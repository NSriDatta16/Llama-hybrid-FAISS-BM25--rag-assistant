[site]: crossvalidated
[post_id]: 15152
[parent_id]: 14856
[tags]: 
First, based on your comments, I would treat this as 300 binary (yes/no) classification problems. There are many easy-to-use open source binary classifier learners, and this lets you trade time for memory. SVMs and logistic regression are probably the most popular approaches for text classification. Both can easily handle 1000000 dimensions, since modern implementations use sparse data structures, and include regularization settings that avoid overfitting. Several open source machine learning suites, including WEKA and KNIME , include both SVMs and logistic regression. Standalone implementations of SVMs include libSVM and SVMlight . For logistic regression, I'll plug BXRtrain and BXRclassify , which I developed with Madigan, Genkin, and others. BXRclassify can build an in-memory index of thousands of logistic regression models and apply them simultaneously. As for converting text to attribute vector form, I somehow always end up writing a little Perl to do that from scratch. :-) But I think the machine learning suites I mentioned include tokenization and vectorization code. Another route would be to go with more of a natural language toolkit like LingPipe , though that may be overkill for you.
