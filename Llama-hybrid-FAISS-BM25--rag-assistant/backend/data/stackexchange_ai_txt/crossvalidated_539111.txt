[site]: crossvalidated
[post_id]: 539111
[parent_id]: 538395
[tags]: 
The number of possible action vectors is huge, so I'm worried the task becomes very hard. If each component of the action vector is "independent", then you could just parameterize a bunch of univariate distributions for each component, and sample them independently. Of course, this only works when the components are really independent -- for example, if your game is actually the "product" of two MDPs, where you're trying to train your agent to multi-task by playing two games simultaneously, then game 1 action components and game 2 action components are independent. On the other hand, if the game is a flight simulator, and one action component is yaw and another is roll, and another is pitch, these are quite dependent on each other.* In the case where components of an action vector are dependent, you can model the distribution auto-regressively: $\pi(x,y,z;s) = \pi(z|x,y;s)\pi(y|x;s)\pi(x;s)$ , using a neural network to parameterize each factor. This is fairly easy to sample from, and lets you model complicated joint distributions. What if the network only produces illegal vectors? This shouldn't be possible, since at least one action must be legal in every non-terminal state of an MDP. In particular, for a factored distribution like $\pi(z|x,y;s)\pi(y|x;s)\pi(x;s)$ , you would first sample from $\pi(x;s)$ , masking out all $x$ which are illegal. This guarantees that $\pi(y|x;z)$ contains some $y$ for which the combination $(x,y)$ is legal. Mask out the other y's which are illegal and sample. Repeat in the same manner for $z$ . *Most situations probably fall somewhere in between these two examples in terms of action independence, and you just have to use judgement to trade-off between more expressive policies, and easier to work with policies.
