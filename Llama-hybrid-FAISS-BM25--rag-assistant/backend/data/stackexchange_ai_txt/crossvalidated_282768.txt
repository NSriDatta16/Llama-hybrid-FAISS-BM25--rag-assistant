[site]: crossvalidated
[post_id]: 282768
[parent_id]: 282433
[tags]: 
I find that the broad use of the term "model" to mean "the model type" in some contexts, and "the specific instantiation of a particular model type" in other contexts to be partly responsible for a lot of confusion when one discusses topics like this. A model type is a particular functional form which we believe fits our data best. E.g. it may be a linear model, quadratic, gaussian, etc. These models have a general form which relies on parameters; when we talk about a specific instance of a particular model type (or a "model instance"), we mean a model of that type for a specific set of parameters. However , some model types can be expressed as a family of model types, and the choice of a particular model type from that family is often selected by using what's called hyperparameters . A good example of this are polynomial models. E.g. if you're trying to model the probability of a variable $x$ using a polynomial model, then your model has functional form $y(x ; \mathbf{w}) = w_0 + w_1x + w_2x^2 + \dots + w_Mx^M$. There are two things to note here. This is not a single model. It represents a family of polynomial models. So, for $M=1$ the model type is specifically a polynomial of degree 1, for $M=2$ it is a specific model type of polynomial of degree 2, etc. Then, once you choose your actual explicit model type, you can try to perform optimisation for that model type, by optimising its parameters $\mathbf{w}$. If you repeat your experiment for different values of $M$, you are effectively trying to choose the model type (albeit still restricting yourself to the family of polynomial models) which has the potential to fit your data best. For each experiment (i.e. a given model type, i.e. a specific value for $M$), your objective is to find the most optimal set of parameters $\mathbf{w}$ for that model type. If you had partitioned your dataset into a training set and a validation set, and you were to then assess the accuracy of your optimal model instance (i.e. the optimal solution for $\mathbf{w}$) on those two sets at each M, you would probably get something like this: which indicates that for models that are too simple (i.e. choice for $M$ is too low, e.g. a straight line can't approximate a curve too well) the model undeperforms (has a high error), but for models that are too complicated (i.e. choice for $M$ is too high) the model overfits the data because it is more susceptible to noisy data. So what's the role of cross -validation then? Well, if you partition your dataset (i.e. split it into training and validation sets) one way, you get one value for training set accuracy, and one validation set accuracy at each M. But if you try a number of different partitions, then you can get many values for training set and validation set accuracy at each M. In other words, you can now also talk about the distribution of the accuracy in the training and validation sets at each M. You can imagine however, that this comes at the cost of having to run more experiments, to obtain that kind of information. Specifically, if you try N random partitions for each M, you will have to perform $N\times M$ experiments. A structured way of performing cross validation is to divide a dataset into N partitions, and to use one of those for validation, and the remaining $N-1$ partitions as the training set. If you choose a different partition for the validation set each time, you effectively have N experiments you can run. But in reality, this is just a conventional way of doing it, and in reality your partitioning scheme could be arbitrary. So in summary, cross-validation is a method to get a better idea how optimal solution accuracy is distributed for a particular model type , and e.g. talk about the model type's accuracy on average (rather than, say, rely on a single accuracy measurement, which may represent an outlier). And more generally, it is considered to be a method of making a more informed choice for the hyperparameters of a model type, or in terms of selecting between model types in general. Compare and contrast this to bootstrapping where you say, I will obtain a number of samples (nothing to do with partitions) from my dataset by sampling with or without repetition, and find the optimal $\mathbf{w}$ for each of those samples. I.e. this attempts to build a distribution over the optimal $\mathbf{w}$ solutions. But this still represents a single experiment, and the outcome of that experiment may be, for instance, that you pick the mode of that $\mathbf{w}$ distribution as the most representative optimal solution for that experiment. Which you would then validate to assess its accuracy. If you were to perform cross validation in that context, then you'd essentially partition your dataset first, and then bootstrap over your training partition each time to obtain bootstrapped samples. Read Bishop, Pattern Matching and Machine Learning Chapter 1 for more details
