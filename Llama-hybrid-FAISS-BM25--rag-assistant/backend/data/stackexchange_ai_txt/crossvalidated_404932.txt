[site]: crossvalidated
[post_id]: 404932
[parent_id]: 
[tags]: 
How do Bayesian hierarchical models adaptively learn the prior?

It seems the main difference between a hierarchical and a non hierarchical model is that the hierarchical model learns the prior. That is it adaptively comes up with a regularizing prior to be applied to all clusters/groups in the hierarchy. This is similar to simply finding an empirical mean and variance (or other parameters) and using these to define your regularizing prior. My understanding is that this method is called empirical Bayes and ignores the uncertainty around the mean and the uncertainty around the variance (or any of the prior parameter empirical estimates). And it is reasonable clear how one comes up with these empirical estimates. But how does allowing a hyperparameter to be distributed by some hyperprior allow the model to learn from all the other clusters/groups and find the ideal parameters to regularize? I am reading McElreath's Statistical Rethinking and have a sense how to mechanical set up a sampling chain, however I seem to be missing the intuition on how the model can transfer information from cluster to cluster.
