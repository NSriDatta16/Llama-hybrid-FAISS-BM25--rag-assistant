[site]: crossvalidated
[post_id]: 453318
[parent_id]: 453308
[tags]: 
Yes, it makes full sense to use cross-validation to find optimal hyper-parameters values in the case of SVM with a linear kernel. If anything, the choosing the regularisation parameter $C$ in the Lagrange formulation is analogous to choosing the ridge regularisation parameter in ridge regression. Therefore it is necessary for our training data to be scaled appropriately (usually to mean $0$ and st.dev. $1$ ). Regarding the actual choice of the $C$ , it is generally common to use exponentially growing sequences of $C$ ; e.g. $C = 2^{-6},2^{-5},\ldots,2^{5},2^{6}$ , etc. CV.SE has a nice thread on this matter if one wants to explore this further: " Which search range for determining SVM optimal C and gamma parameters? ".
