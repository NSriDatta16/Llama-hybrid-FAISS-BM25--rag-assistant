[site]: crossvalidated
[post_id]: 334044
[parent_id]: 334042
[tags]: 
In a unit of a typical neural network, you always: Compute weighted sum of all its inputs, including the bias term Apply the activation function So in your first example, $y=\sigma(ax+b)$ and $z=\sigma(cy+d)$. If this does not match, make sure the unit $y$ really computes a sigmoid and it is not only a linear unit. In your second example, $z = \sigma(ey_1+fy_2+g)$. Note that it is not really infeasible to write a formula for a large network, you just use matrix operations and the following recursive formula: $$\mathbf{y}_\ell=\sigma(W_\ell\mathbf{y}_{\ell-1}+\mathbf{b}_\ell),$$ where $\mathbf{y}_\ell$ is the vector of activations in $\ell$-th layer ($\mathbf{y}_0$ is the input), $W_\ell$ is the weight matrix in layer $\ell$, whose element at position $i,j$ corresponds to the weight between $i$-th unit in the layer $\ell-1$ and $j$-th unit in layer $\ell$; finally, $\mathbf{b}_\ell$ is the bias vector.
