[site]: crossvalidated
[post_id]: 497362
[parent_id]: 497331
[tags]: 
Well bagging a linear model will just converge in prediction to NOT bagging depending on your sample rate and number of estimators. This is not true if you use some piecewise functions in your model though. But this is just because bagging won't add any complexity to a linear model so you just bias the model by some amount then another model and so on, but the average just becomes essentially un-biased. Now you may want to use bootstrapping in general to do statistics on your coefficients/error rather than relying on the standard equations to derive those variances but once again I think those numbers should all be close with enough samples and with 10 million samples I wouldn't be worried about whether to bag or not, just do the normal stuff. Now when do we typically want an ensemble approach? Typically for prediction when adding two models also adds complexity such as a decision tree or some piecewise function in our regression. OR if you use a different ensembling technique, gradient boosting can actually add regularization to your coefficients similar in effect to a ridge regression, once again though this makes interpreting our coefficients fuzzy so typically done for predictive power. I think it's important to think of the bias-variance tradeoff. Ensembling is done to increase bias in low bias models in order to decrease their variance to new data. But typically a linear model is not 'low' enough bias for ensembling to do much so we have to decrease the bias (and increase the variance) with polynomial expansions or something like that first, then we can look to ensembling.
