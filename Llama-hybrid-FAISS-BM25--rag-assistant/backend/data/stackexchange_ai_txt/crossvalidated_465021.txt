[site]: crossvalidated
[post_id]: 465021
[parent_id]: 464969
[tags]: 
We have a closely related question here What are the impacts of choosing different loss functions in classification to approximate 0-1 loss Note that, doubllle had a nice answer from MLE perspective. On the other hand, in some machine learning literature, people just think about logistic loss or hinge loss is a convex approximation of 0-1 loss, without any probability interpretation. In short: In classification problem, we want to minimize 0-1 loss*. But 0-1 loss is hard to minimize, we use logistic loss to approximate. 0-1 loss is the "mis-classification cost": when make wrong prediction, loss is 1, and making right prediction the loss is 0 for a given point.
