[site]: datascience
[post_id]: 91006
[parent_id]: 90994
[tags]: 
The assumption in supervised ML is that the test set follows the same distribution as the training set. It's clear from your description that this assumption is not satisfied in your data, so it's not very surprising that things don't work well. Beyond the difference in the range of the dependent variable, if the training and test set have been collected independently it's likely that the distributions of the features also differ. So how to fix this depends on the goal of the task and in particular why the test set and training set don't follow the same distribution: If the difference is not meaningful, i.e. there is no reason to assume that real production data would be different than the training set, then the experiment should be redone after properly randomizing the split between training and test set. If the difference is meaningful, i.e. the test set is purposefully different from the training set because the production data will also be different, then the setup should be redesigned to account for that. For example there could be a step of semi-supervised learning on the test set, which would adapt the model to the new data distribution while leveraging the information from the training data.
