[site]: crossvalidated
[post_id]: 383947
[parent_id]: 
[tags]: 
Deep Learning Variable Length Sequence Handling

I am trying to understand the best practice for handling different lengths of sequences in NLP tasks. Lets consider an example of convolution on sequences followed by max pool layer. We can handle this in two ways. Make sure each minibatch contains sequences of same length - in this case we don't need any masking after convolution before applying max pooling. Each minibatch can contain sequences of different length - in this case we need to apply masking (to padding added to the each sequence to make it constant length) after convolution before applying max pooling. Option 1 is simple to implement because it is handled only at data side, option 2 is more complicated because masking should be done correctly. But I feel option 1 does not feed randomized data to the network. Especially in cases where modelling long sequences is more harder than short sequences, in these scenario, loss of long sequence minibatch will be higher than that of short sequence minibatch. Will this lead to any convergence issues ? Can anyone share the best practices in deciding ?
