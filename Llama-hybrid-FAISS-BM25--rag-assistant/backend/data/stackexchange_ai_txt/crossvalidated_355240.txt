[site]: crossvalidated
[post_id]: 355240
[parent_id]: 
[tags]: 
Output a value normalised to 1 over a number of groups from a neural network

Suppose I want to use a (deep) neural network to output a probability that is normalised to sum to 1 within each of a potentially arbitrary number of groups. For example, this might be something like attempting to project the percentage of a team's goals that a given player will score for his team during a given game. The data might look something like this: FixtureId TeamId PlayerId Actual % of Team Goals Explanatory Variables 1 1 1 0.8 ... 1 1 2 0.2 ... 1 1 3 0 ... 1 1 4 0 ... 1 2 5 1 ... 1 2 6 0 ... 1 2 7 0 ... 1 2 8 0 ... 2 1 1 0.5 ... 2 1 2 0.5 ... 2 1 3 0 ... 2 1 4 0 ... 2 2 5 0 ... 2 2 6 1 ... 2 2 7 0 ... 2 2 8 0 ... ... where we're attempting to model Actual % of Team Goals ~ Explanatory Variables. The way that I look at this is we should probably have an arbitrary number being produced by some layer, with the output from each individual fixture and team combination being fed through a softmax layer, and then output. (I think that it makes sense to use one row for each player here rather than one row per fixture/team given the potentially large number of explanatory variables/features.) I guess I have a number of questions. Does any of this formulation make sense? How would a potential NN solution be achieved (especially as there's the potential for problems during training with batching, etc)? Are there any much more sensible ways to go about this? I'm familiar with Keras, and know I can create my own loss function/activation function. But given there could be an arbitary number of fixtures/teams/players within each team, I'm not sure if that level of complexity is possible. (I couldn't even find a name for this kind of problem - any help at this stage would be much appreciated.)
