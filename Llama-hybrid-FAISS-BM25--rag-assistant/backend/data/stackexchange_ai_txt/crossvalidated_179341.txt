[site]: crossvalidated
[post_id]: 179341
[parent_id]: 
[tags]: 
cv.glmnet elimintating all variables in logistic regression (response in the form of total successes and failures)

Relatively new to GLMNET and having an odd problem. I know the binomial family of regression requires a binary response variable (IE a 1 or 0). I have data in which each entry has a large number of trials and a proportion of successes. IE, the first line has a bunch of potential explanatory variables and then the response is like 27 successes in 427 attempts. My understanding on how to do this is to duplicate each line with the response being a '1' for one line and a '0' for the other. And then weight each of those by the number of successes and failures. So 27 and 400 respectively. I have about 6000 of those. But what happens is that cv.glmnet pretty much always returns nothing but an intercept and eliminates all the explanatory variables (regardless of lasso, elnet or ridge). This is wrong as there are several variables with strong relationships to the response. Switching over to gaussian regression causes them to show up immediately, but I specifically want logistic regression. It seems like the weighting is working, and since for every '1' there's an identical set of explanatory variables for every '0', without the weights obviously everything cancels out. Now if I use penalty factors and demand it accepts certain variables, it will work after a fashion, but then all it does is use those variables and eliminates the rest. It unfortunately is a roadblock right now. I could do a gaussian regression to find the variables, and then switch to binomial with the penalty factors set appropriately, but I understand that there are fundamental differences between the two that makes that invalid.
