[site]: crossvalidated
[post_id]: 245227
[parent_id]: 245063
[tags]: 
There are a number of sources of polling error: You find some people hard to reach This is corrected by doing demographic analysis, then correcting for your sampling bias. If your demographic analysis doesn't reflect the things that make people hard to reach, this correction does not repair the damage. People lie You can use historical rates at which people lie to pollsters to influence your model. As an example, historically people state they are going to vote 3rd party far more than they actually do on election day. Your corrections can be wrong here. These lies can also mess up your other corrections; if they lie about voting in the last election, they may be counted as a likely voter even if they are not, for example. Only the people who vote end up counting Someone can have lots of support, but if their supporters don't show up on election day, it doesn't count. This is why we have registered voter, likely voter, etc models. If these models are wrong, things don't work. Polling costs money Doing polls is expensive, and if you don't expect (say) Michigan to flip you might not poll it very often. This can lead to surprised where a state you polled 3 weeks before the election looks nothing like that on election day. People change their minds Over minutes, hours, days, weeks or months, people change their minds. Polling about "what you would do now" doesn't help much if they change their minds before it counts. There are models that guess roughly the rate at which people change their minds based off historical polls. Herding If everyone else states that Hillary is +3 and you get a poll showing Hillary +11 or Donald +1, you might question it. You might do another pass and see if there is an analysis failure. You might even throw it out and do another poll. When you get a Hillary +2 or +4 poll, you might not do it. Massive outliers, even if the statistical model says it happens sometimes, can make you "look bad". A particularly crappy form of this happened on election day, where everyone who released a poll magically converged to the same value; they probably where outlier polls, but nobody wants to be the one who said (say) Hillary +11 the day before this election. Being wrong in a herd hurts you less. Expected sampling error If you have 1 million people and you ask 100 perfectly random people and half say "Apple" and half say "Orange", the expected error you'd get from sampling is +/- 10 or so, even if none of the above problems occur. This last bit is what polls describe as their margin of error. Polls rarely describe what the above correction factors could introduce as error. Nate Silver at 538 was one of the few polling aggregators that used conservative (cautious) means to handle the possibility of the above kinds of errors. He factored in the possibility of systemic correlated errors in the polling models. While other aggregators were predicting a 90%+ chance HC was elected, Nate Silver was stating 70%, because the polls were within "normal polling error" of a Donald victory. This was a historical measure of model error , as opposed to raw statistical sampling error; what if the model and the corrections to the model were wrong? People are still crunching the numbers. But, preliminary results indicate a big part of it was turnout models. Donald supporters showed up to the polls in larger numbers, and Hillary supporters in lesser numbers, than the polling models (and exit polls!) indicated. Latino's voted more for Donald than expected. Blacks voted more for Donald than expected. (Most of both voted for Hillary). White women voted more for Donald than expected (more of them voted for Donald than Hillary, which was not expected). Voter turnout was low in general. Democrats tend to win when there is high voter turnout, and Republicans when there is low.
