[site]: crossvalidated
[post_id]: 189449
[parent_id]: 
[tags]: 
Training set error as an estimate of bias

In his machine learning lectures (1 min 30 sec), Andrew Ng seems to estimate the bias using the training set error. Why is it ok to do it? The definition of "bias" in machine learning (see wiki or any references from it), at least for the mean squared error cost function, is the expected error when using as a prediction the expected prediction made by various training sets. So why would the error (for a given point) for a single training set be anywhere close to the error (for that point) when using the average of all possible training sets?
