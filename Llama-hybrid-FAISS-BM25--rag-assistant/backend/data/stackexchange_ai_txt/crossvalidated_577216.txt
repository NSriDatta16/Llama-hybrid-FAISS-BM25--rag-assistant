[site]: crossvalidated
[post_id]: 577216
[parent_id]: 
[tags]: 
If a Random Forest scores high accuracy on test set, but high false negative on real-world application, is this an overfit?

I'm very new to Machine Learning and I'm making a Random Forest Classifier to classify if a file is a malware or benign. This is done through analyzing the PE file's headers. The dataset used contains approximately 40k clean file data and 90k malware file data . I put the n_estimators to 1000, but this is the only hyperparameter that I added. I'm using sklearn's train_test_split to split the data into train and test data. After fitting the model and outputting the accuracy using accuracy_score(), I get 0.99% on both train and test. But later found out after using the model to predict some malicious and legit .exe that the model will almost always predict the legit .exe files (mostly installers) as malware. After some more digging, I'm still confused whether it's caused by: Overfitting? (I'm thinking this is likely since the accuracy is too good to be true) Imbalanced dataset? Need more hyperparameter tweaking for the Random Forest Classifier (maybe the tree depth is too deep)? Bad dataset?
