[site]: datascience
[post_id]: 47362
[parent_id]: 
[tags]: 
Why increasing the number of units or layers does not increase the accuracy and decrease the loss?

I have an LSTM neural network; when I increase the number of units, layers, epochs or add dropout, it seems it has no effect and still I have persistent errors and accuracies like the following: loss: 3.5071 - acc: 0.0981 - val_loss: 6.7042 - val_acc: 0.0122 Why this happens and how can I fix it?
