[site]: crossvalidated
[post_id]: 530948
[parent_id]: 
[tags]: 
Is there a reason to use variational inference for point estimates?

I have seen Bayesian hierarchical models, particularly in computational biology, that use variational inference, but do not use the uncertainty provided by a variational solution. For example, MOFA is a factor analysis model that uses stochastic gradient descent to perform variational inference. However, the authors use only the mean of the variational distribution as a point estimate, and do not consider the uncertainty quantified by posterior variance. Is there some computational advantage to doing gradient descent on the parameters of the variational distributions for each parameter, rather than doing gradient descent directly on each parameter? I have assumed that gradient descent to obtain a MAP solution would be faster and simpler than trying to optimise the KL divergence between a variational approximation and the true posterior. Could it be that variational inference is an easier solution to derive mathematically?
