[site]: crossvalidated
[post_id]: 482114
[parent_id]: 482033
[tags]: 
I imagine that people could come up with a justification for the two step approach, but to me, it seems like a bit of a waste. Most critical from my perspective, if you run everything in a single model, $logit(p(y_{it}=1))=\beta_0 + u_{0i} + (\beta_1 + u_{1i})x$ then the $u_{0i}$ and $u_{1i}$ remain latent variables, thus reducing measurement error that is induced when you predict them using an Empirical Bayes (EB) approach. Said differently, there is a fair degree of uncertainty about each individual's value of $u_{0i}$ and $u_{1i}$ , and this uncertainty is preserved in the LMM above. In contrast, Empirical Bayes prediction assigns a single value to each person's $u_{0i}$ and $u_{1i}$ . There is an associated standard error for the EB prediction, but you would need to go fully Bayesian to incorporate such uncertainty back into a model. Mark Lai has example code showing how to do this with lmer() and brms() . The only thing this single model does not presently have is a mean value for $x_{ij}$ in the prediction of $y_{ij}$ . However, you could easily compute each person's mean x value ( $\bar x_i$ ) and add it as a predictor to the model.
