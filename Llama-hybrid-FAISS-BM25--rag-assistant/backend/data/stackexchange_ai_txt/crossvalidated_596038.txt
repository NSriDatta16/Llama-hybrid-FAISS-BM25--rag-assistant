[site]: crossvalidated
[post_id]: 596038
[parent_id]: 596033
[tags]: 
There are two approaches to building models: In some cases you know what the functional form should be, so you use this knowledge when building the model. This is often the case in statistics. This may be the case, for example, when there is a known physical relationship between the variables. Another example might be time-series forecasting, where there are some "usual suspects" for feature engineering like Fourier-transformations . Another possibility is when you don't know the functional relationship. This is usually the case of using machine learning. In such a scenario, you need a model that is flexible enough to find the appropriate functional form. It can be achieved by using polynomials because they can approximate any function . There are also many models that are universal approximators like kernel regression, tree-based models, $k$ NN, neural networks, etc. We use those models because we can't "try all the possible functions" (the infinite search space) while the models by themselves are able to approximate any function. Finally, if you want to "try all the possible combinations and pick the best one" you are likely to end up with an overfitting model, so you need additional safeguards to prevent this.
