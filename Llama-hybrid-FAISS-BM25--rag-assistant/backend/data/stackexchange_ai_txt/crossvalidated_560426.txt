[site]: crossvalidated
[post_id]: 560426
[parent_id]: 560390
[tags]: 
Let's look at at some specific fictitious data to illustrate the two estimates of $\sigma^2 = 16,$ the common variance of the $k=3$ levels of a factor in a balanced design with $r = 30$ replications in each of the three groups. Data and summaries. set.seed(2022) x1 = rnorm(30, 50, 4) a1 = mean(x1); v1 = var(x1) x2 = rnorm(30, 60, 4) a2 = mean(x2); v2 = var(x2) x3 = rnorm(30, 80, 4) a3 = mean(x3); v3 = var(x3) The three group means are estimated as: a1; a2; a3 [1] 49.53628 [1] 55.43423 [1] 61.49257 And the three group variances as: v1; v2; v3 [1] 13.71546 [1] 11.1894 [1] 22.56334 Here are separate boxplots of the data in the the three groups: x = c(x1, x2, x3) g = as.factor(rep(1:3, each=30)) # see Note (2) boxplot(x~g, col="skyblue2") Two variance estimates for ANOVA. Ross's second estimator of $\sigma^2$ is sometimes called $S_w^2,$ for 'variance within groups'. [It is analogous to the pooled variance $S_p^2$ in a two-sample t test.] It can be found by averaging the three group variances. With only 45 observations altogether, we can't expect a wonderfully accurate estimate of $\sigma^2,$ but is the best estimate available from our data. It is the best estimate whether or not the three group means are equal. v.w = mean(c(v1,v2,v3)); v.w [1] 15.82273 Ross's first variance estimate is based on the three group means. If $H_0$ were true so that $\mu_1 = \mu_2 = \mu_3 = \mu,$ then the three group population means would be a sample from $\mathsf{Norm}(\mu, \sigma/\sqrt{30}),$ so that the variance of the three means should estimate $\sigma^2/30.$ Thus, $r=30$ times the variance of the three means would also estimate $\sigma^2.$ This possibly inflated estimate is sometimes called $S_a^2$ for the variance among groups. For our data, it is computed in R as: 30 * var(c(a1, a2, a3)) [1] 7917.119 One can show that $$F = S_a^2/S_w^2 \sim\mathsf{F}(k-1 = 2,\; k(n-1) = 3(29)=87),$$ Snedecor's F-distribution with 2 and 87 degrees of freedom. ANOVA table. It is customary to display the computations for an analysis of variance in a table such as the one for our fictitious data below (slightly abridged): anova(lm(x~g)) Analysis of Variance Table Response: x Df Sum Sq Mean Sq F value Pr(>F) g 2 15834.2 7917.1 500.36 In the column headed Mean Sq you can recoginze $S_a^2 = 7917.119$ and $S_2^2 = 15.82273$ from above (slightly rounded to save space). Because $S_a^2$ is inflated by differing means to be so much larger than $S_w^2,$ the F-statistic is very large and the P-value is near $0.$ Notes: (1) In condensed format here are computations for fictitious data where the population means are equal, so that $S_a^2$ is not inflated, and the null hypothesis is not rejected. set.seed(114) y1 = rnorm(30, 60, 4) a1 = mean(y1); v1 = var(y1) y2 = rnorm(30, 60, 4) a2 = mean(y2); v2 = var(y2) y3 = rnorm(30, 60, 4) a3 = mean(y3); v3 = var(y3) v.w = mean(c(v1,v2,v3)); v.w [1] 16.73631 v.a = 30 * var(c(a1, a2, a3)); v.a [1] 8.239104 y = c(y1, y2, y3) g = as.factor(rep(1:3, each=30)) anova(lm(y~g)) Analysis of Variance Table Response: y Df Sum Sq Mean Sq F value Pr(>F) g 2 16.48 8.2391 0.4923 0.6129 Residuals 87 1456.06 16.7363 (2) It is crucial to declare g as a 'factor' variable, otherwise R will do a regression instead of an ANOVA. (3) This Answer is for a balanced design with equal numbers of replications in each group. Formulas for an unbalanced design are a bit messier.
