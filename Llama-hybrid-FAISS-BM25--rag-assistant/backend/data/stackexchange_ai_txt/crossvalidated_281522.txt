[site]: crossvalidated
[post_id]: 281522
[parent_id]: 
[tags]: 
Using TensorFlow Neural Network with Sklearn's Adaboost

I'm kind of new to ensemble methods, but since bagging, pasting and voting gave me some improvement in accuracy of my feed forward neural network (implemented in Tensorflow with multiple hidden layers) I thought I'd give boosting a try. Since I already used Sklearn's BaggingClassifier and VotingClassifier, I thought for boosting the easiest way would be to use the AdaboostClassifier of Sklearn. My Tensorflow model already inherits from Sklearn's BaseEstimator and implements predict_proba(X) function. The fit function of my NN takes the argument sample_weights which is passed by AdaboostClassifier of Sklearn while training. But here is my problem: I have no clue how to actually use the sample_weights to give more weight on samples with higher weights. I found this question , but the guy in the answer didn't explain the 2nd step, which is my problem. The only thing I can think of is initializing the weights in some other way or multiplying the weight of each sample with the calculated cost for this sample maybe. But I don't know if that makes any sense at all. I'm grateful for every hint, that puts me in the right directions. Thanks! EDIT Since I've been asked for, I'll provide some snippets of the code, which might help to understand my problem better. snippet of pipeline class that calls the boosting classifier: mlp = MLP(n_classes=4, batch_size=200, hm_epochs=1, keep_prob_const=1.0, optimizer='adam', learning_rate=0.001, step_decay_LR=True, weight_init='sqrt_n', bias_init=0.01, hidden_layers=(10, 10, 10), activation_function='relu6') clf = AdaBoostClassifier(mlp, n_estimators=3, algorithm="SAMME.R", learning_rate=0.5) snippets of the multilayer perceptron, which I simplified quite a bit, so there might be some mistakes in it (partly taken from a tutorial of the page https://pythonprogramming.net ): from sklearn.base import BaseEstimator class MLP(BaseEstimator): def __init__(self, n_classes=4, batch_size=200, hm_epochs=15, keep_prob_const=1.0, optimizer='adam', learning_rate=0.001, step_decay_LR=False, bias_init=0.0, weight_init='xavier', hidden_layers=(600, 600, 600), activation_function='relu'): # init arguments... def fit(self, X_train, y_train, sample_weight=None): self.sample_weight = sample_weight #
