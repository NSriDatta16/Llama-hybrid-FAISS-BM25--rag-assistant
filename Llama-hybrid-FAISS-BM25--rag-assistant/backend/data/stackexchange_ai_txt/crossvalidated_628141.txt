[site]: crossvalidated
[post_id]: 628141
[parent_id]: 628135
[tags]: 
(It seems philosophically wrong to me to predict something that is under your control. You don't have to guess what will happen. You dictate what you do.) Given particular circumstances (feature values), you can choose to take any possible action. Some of those actions, however, might lead to better outcomes than others, so it is that outcome (whatever it is) that you want to predict, using both the circumstances and the possible actions. After all, we slow down when we see the car ahead of us slowing down because we predict that continuing our speed will lead to a collision, and we choose to brake harder if our speed needs to decrease faster, and we perform these actions because we predict that doing otherwise makes a collision likely and doing so makes a collision less likely (so collision/not is the outcome of interest, the $y$ variable in your brain). Thus, I propose you set up your modeling as follows. Decide on what your outcome of interest is, such as profit for a business setting, patient survival in a healthcare setting, or collisions in an autonomous vehicle setting. Use the usual set of features, whatever you would have used to predict the action. To that existing set of features, add indicator variables for each action being taken. Some variable is one if action $A$ is taken and zero otherwise; another variable is one if action $B$ is taken and zero otherwise; etc. (The action could be a number, too, such as drinking $100$ mL of water vs drinking $101$ mL or $99$ mL or $200$ mL. You can bring such an action into the feature space, too.) Do the usual machine learning on the combination of the original features and the features that ndicate actions. Under certain circumstances, some actions or combinations of actions will lead to positive outcomes, while others will lead to worse outcomes. Once you have a model that reliably fits well (the usual validation and assessment of performance), you can try to find the optimal set of actions, given some set of circumstances. As an example, if your outcome of interest is how soggy your feet get in order to avoid soggy feet, if your circumstance is that it is raining hard outside, an action of leaving the umbrella in the house will lead to a worse outcome than bringing the umbrella. For whatever task you have, you would train the model on the observed circustances and actions taken that lead to observed outcomes that you want to optimize. Then you would set some circumstances of interest and investigate what happens for various actions and combinations of actions. You might not get to choose the circumstances under which you are operating, but you can pick your actions to lead to the best outcome. If the optimal set of actions is not possible, go to the second-best. If that is not possible, go to the third-best... If multiple combinations of actions lead to acceptable outcomes, perhaps all of those can be regarded as "correct" actions to take. As always, assess your uncertainty. If a predicted outcome is pretty good but uncertainty quantification gives that it might be rather poor (wide prediction interval), you might be less inclined to go with the actions that lead to such an outcome when another set of actions might lead to a near-certain good outcome, even if that outcome is not as good. (In some sense, this is why older people move their retirement savings out of high-yield-but-high-volatility investments like stocks and into low-yield-but-low-volatility investments.) At least in a crude sense, this is what reinforcement learning does: try actions under particular circumstances, assess the outcome (e.g., victory/defeat in a game of Go), and try to improve the outcome by changing the actions taken. You might not be able to explicitly model your problem as reinforcement learning, but your task is related. In the language of reinforcement learning, the actions taken form the policy . HOWEVER... ...from the comments to the original question, such data on the ultimate outcome seem to be unavailable. As much as I believe the above approach to be correct, you might be able to hack around it in either of two possibly viable ways. Model as a multi-label problem where the possible actions (do it or not do it) are binary categories. Evaluate your model using the usual methods of multi-label problems. Group the action combinations so you predict if action A should be performed alone; if actions A and B should be performed but C should not; if actions B and C should be performed but A should not; etc. Model these as distinct categories and predict these categories. Of the two, the second has the major advantage that you model the correct combination of outcomes and (I think appropriately) harshly punish the model for missing an essential action. That is, if you must perform actions A, B, and C together to get a desirable outcome, you very harshly punish the model for only predicting actions A and B while missing C. A drawback is that you can end up with a lot of groups. With three possible binary actions that can be taken in any combination, there are eight categories: $$ \text{None}\\ \text{A Only}\\ \text{B Only}\\ \text{C Only}\\ \text{A & B Only}\\ \text{A & C Only}\\ \text{B & C Only}\\ \text{A, B, and C} $$ With four binary actions, there are $16$ possibilities. With ten binary actions, there are $1024$ . This follows from the binomial coefficient and the sum of row $n$ in Pascal's triangle for $n$ binary actions, so this number grows quite quickly, possibly to the point where you have far more action combinations than observations. In both of these, you have to make the assumption that the actions taken are ones that tend to lead to good outcomes. That is, the actions taken must be a reasonable proxy for good outcomes. Without such an assumption, you're modeling how people do behave instead of how they should behave. These notions need not coincide.
