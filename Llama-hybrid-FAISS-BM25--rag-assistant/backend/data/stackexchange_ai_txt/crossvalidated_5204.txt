[site]: crossvalidated
[post_id]: 5204
[parent_id]: 5196
[tags]: 
Another method of avoiding over-fitting that I have found useful is to fit the univariate logistic regression model to the leave-out-out cross-validation output of the SVM, which can be approximated efficiently using the Span bound . However, if you want a classifier that produces estimates of the probability of class membership, then you would be better off using kernel logistic regression, which aims to do that directly. The ouput of the SVM is designed for discrete classification and doesn't necessarily contain the information required for accurate estimation of probabilities away from the p=0.5 contour. Gaussian process classifiers are another good option if you want a kernel based probabilistic classifier.
