[site]: crossvalidated
[post_id]: 90246
[parent_id]: 90242
[tags]: 
The spread of those residuals look okay to me. With that arrangement of x's what did you expect the spread should look like? (I'd worry more about transformations making your mean function nonlinear in x.) Specifically, if I generate random data with a similar pattern of x's for which the y's have constant variance, the residual plots do often look like that. Note that your impression of spread is heavily influenced by the range of residuals at each x, which does shrink on average as the x-density gets thinner, even when the sd is constant. Here's four example residual plots, for which the data was randomly generated -- where the population spread at each x is constant (I know because I generated the data that way). As you see, the plots have a broadly similar appearance to yours, tending to look like vaguely elliptical point clouds (somewhat reminiscent of the shape of Stewie Griffin's head ). When the x's are vaguely normal, that's how residual plots should look. So if that's your objection, it's not only not a problem, its what you should hope to see . Now let's imagine we had instead clear evidence of actual heteroskedasticity. Can we ignore that? Well, that depends. Your estimates of standard deviations (e.g. of regression coefficients) will be biased. Your p-values in tests will be wrong. Confidence intervals will be biased and prediction intervals too narrow in some places and too wide in other places. If you're not doing any of those things, it may not matter so much; some inefficiency in estimation of the coefficients may be about the worst of it.
