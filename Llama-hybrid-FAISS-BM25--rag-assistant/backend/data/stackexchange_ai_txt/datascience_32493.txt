[site]: datascience
[post_id]: 32493
[parent_id]: 32491
[tags]: 
No. An example: feature engineering for Gradient Boosting algorithms. XGBoost can't handle categorical variables - you need to use one-hot encoding, target encoding, or something like that if you want to use it. On the other hand more recent GBM libraries like CatBoost or LightGBM can handle categorical data and have reasonable defaults for doing that. I wouldn't say encoding categorical variables would be 'few modifications' because depending on what you do (one-hot encode vs target encoding) your model's behavior can change significantly.
