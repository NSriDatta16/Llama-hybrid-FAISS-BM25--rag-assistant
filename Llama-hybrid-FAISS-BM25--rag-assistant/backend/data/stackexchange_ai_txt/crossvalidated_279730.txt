[site]: crossvalidated
[post_id]: 279730
[parent_id]: 
[tags]: 
Why gradient boosting/random forest generate "unstable" feature importance?

I'm using gradient boosting(implementation is XGBRegressor) for regression prediction, and show special interest in feature importance. I only tweak parameters for learning rate, n_estimator and max_depth. clf = XGBRegressor( learning_rate = 0.02, n_estimators = 300, max_depth = 3, silent = False ) Then I applied importance = clf.feature_importances_ to extract important features. Question is: Everytime I ran this using same parameter set, top important features are quite different. 1st run: Importance Survival 0.187797 Onset Delta 0.144407 bps_k 0.123390 Creatine Kinase_k 0.051525 Creatinine_k 0.043390 Creatine Kinase_Vmin 0.037966 Albumin_k 0.032542 2nd run: Importance Survival 0.115211 Onset Delta 0.067965 bps_k 0.027064 bpd_Dmax 0.026549 pulse_Dmax 0.026316 Age 0.023665 bps_Dmax 0.022801 bps_b 0.020935 In my case, except Survival and Onset Delta , two strongest features, other relatively "weak features" are quite unstable. I got similar results if applying random forest. So this is normal? Because those features are weak, so unstable? Also, my project here is very noisy, so pearson correlation is only 60% ,indicating that model is not that perfect.
