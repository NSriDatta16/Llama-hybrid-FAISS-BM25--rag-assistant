[site]: crossvalidated
[post_id]: 511282
[parent_id]: 506317
[tags]: 
Everything in your understanding is correct, except for a nomenclature thing. I would caution you that H isn’t actually a weight matrix. The words “weights” and “parameters” are pretty interchangeable. In the case of H, it’s not a parameter. It depends on the particular sequence x that you observe. It’s not entirely true that you can’t parallelize the RNN computation over the sequence lengths see Martin and Cundy (2018) for how the prefix sum algorithm can be used to build up the computation for several segments of the input sequence, then merge these. (It’s similar to how carry-lookahead adders improve on ripple-carry adders.) This only works for certain classes of RNNs. In general, though, it’s fair to say what you said. The sequential dependencies in the RNN make parallelization impossible.
