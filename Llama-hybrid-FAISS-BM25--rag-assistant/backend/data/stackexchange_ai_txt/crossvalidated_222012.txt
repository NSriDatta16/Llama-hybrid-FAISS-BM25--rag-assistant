[site]: crossvalidated
[post_id]: 222012
[parent_id]: 221912
[tags]: 
Classification : The introduction of the paper you quoted contains your answer I believe : This problem is split in many two-class classification problems To classify something into L classes {1,2,..,L}, you can cut your problem in L classification sub-problems : {1}/{2,3,..,L} {2}/{1,3,..,L} ... {L}/{1,2,...,L-1} Each of these sub problems can now be easily solved by a neural networks giving you L binary coordinates. If your neural-networks are linear (perceptron) then you can also visualise the results with a PWL (as in figure 3 of the paper you quoted). For such classification task, the current approach is to write a single network using a sofmax output layer . Dimensionality reduction : using a function h while preserving the neighbors of x in the binary space. If you want to preserve the neighbours then that looks like dimensionality reduction rather than classification. An auto-encoder with an output layer of size L made of strong activation functions would take your data and express them in {0,1}^L (each bit being the binarised output of it's corresponding activation function). (If you don't especially need a binary output then other methods such as PCA and t-SNE might bring better results (preserve the neighbours) depending on the exact nature of your problem.)
