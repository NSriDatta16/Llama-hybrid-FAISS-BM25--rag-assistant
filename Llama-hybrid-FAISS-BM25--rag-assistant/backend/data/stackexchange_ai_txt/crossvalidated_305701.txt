[site]: crossvalidated
[post_id]: 305701
[parent_id]: 305691
[tags]: 
LDA works with the covariance(correlation) matrix based on the input features. It's best to use cross validation (CV), something like 10-fold, during which you randomly split the objects in the entire dataset into 10 partitions or "folds." Then apply LDA to objects (i.e., train) in the first 9 folds and predict class membership for the objects left out of training in the 10th fold, and then increment elements of the confusion matrix accordingly. Next, apply LDA to objects in the first 8 folds, and 10th fold, and predict class for objects in the 9th fold. Repeat the above until you have predicted class for objects in all 10 of the folds which are left out of LDA each time. So, LDA will be applied 10 times to your data in this case. It's inefficient to split a dataset into training and testing one time and then apply a classification algorithm. Some of the best work on CV was done by Ron Kohavi at Stanford-U, and his classic paper on CV is here . There's another CV method called leave one out CV (LOOCV), which performs the same operations outlined above, but which predicts class membership for each object when it is left out of training singly. (thus, here, a fold has one object). LOOCV gets computational expensive and is inefficient for large datasets with $n$ samples, since you'd have to run LDA $n$ times. I also don't recommend LDA as a first choice, since it looks at correlation between features -- if you have highly correlated features, their corr matrix could be positive semi-definite or singular, rather than positive definite -- called "pathologies" among corr matrices. It's better to first employ multivariate linear regression (LREG), for which you set $\mathbf{y}_i=(-1,+1,-1,-1)$ for e.g. an object whose true class is 2 for a 4-class problem, and then assign this example object to class 1 if the predicted y-hat is e.g. $\mathbf{\hat{y}}_i=(+0.35,-0.7,-1.2,-2.6)$. If the prediction accuracy for LREG 10-fold CV is very poor, then maybe your dataset is not linearly separable . Then you know that LDA may not be the best choice, but rather, maybe kNN will work better. Certainly, when data are not very linearly separable, SVM with an RBF kernel function will probably work the best -- but never use SVM first, like most students do who know nothing about first determining whether the data are linearly separable. I like to use LREG and kNN first before any other classifier -- since it lets me know if linear prediction works. For very large datasets, kNN is very fast. Also, ID3 is a nice classifier. But if you start with LDA, you're committed to a lot of excess baggage involving quality of the correlation matrix from the standpoint of eigendecomposition -- which is not an optimal method if linear separability is low, and there are non-linearities present. LDA is like a school bus, while kNN is like a go-kart.
