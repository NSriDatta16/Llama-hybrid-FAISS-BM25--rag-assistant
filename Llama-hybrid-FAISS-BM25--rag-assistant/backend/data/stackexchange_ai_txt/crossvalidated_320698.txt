[site]: crossvalidated
[post_id]: 320698
[parent_id]: 
[tags]: 
deriving posterior conditionals for gibbs sampling

I'm new to Bayesian inference and Gibbs sampling in general, and I'm struggling trying to derive the conditional posteriors for a particular data generating process I'm trying to model. The model I am considering is as follows: $ k \sim Beta(a,b) \\ \lambda \sim Gamma(c,f) $ for i = 1 to N $ d_i \sim Bernoulli(k) \\ s_i \sim Poisson(\lambda) \; if d_i = 1 \\ s_i = 0 \; if d_i = 0 $ The idea is that for each $s_i$, we flip a coin with some probability, and if the coin is heads we produce a value from the Poisson, and if it is tails, we produce a 0. I hope my notation above isn't too incorrect. Given observed $s_1 \ldots s_n$, I want to infer the parameters which most likely generated the observations. I think Gibbs sampling can help with this. The way I've written out the joint posterior is as follows: $ P(k, \lambda, d_{1:n}, s_{1:n}) = \left[k^{a-1}(1-k)^{b-1}\right] \cdot \left[\lambda^{c-1} e^{-\lambda/f}\right] \cdot \prod\limits_{i=1}^{n} \left[ k^{d_i}(1-k)^{1-d_i} \right] \cdot \left[(\lambda^{s_i}e^{-\lambda})^{d_i} (0^{s_i})^{1 - d_i} \right] $ The first square-bracket enclosed part corresponds to the beta prior, the second to the Gamma prior, the third to the Bernoulli toss, and the fourth to the "Poisson or 0" component. I think I've defined this correctly, but I had some trouble figuring out how to write out the "conditional if" part of the process notationally. My question now is, how can I go about deriving the conditional posteriors from here? I think ultimately I need to derive conditional posteriors for $k$ and $\lambda$, but the $d_i$ terms are confusing me -- should I have a conditional posterior for each $d_i$ as well? It seems that to derive and draw from the posterior conditionals of any variables, I need to know what the values of $d_i$ are for each $i$. Sorry if this vaguely worded, but I would really appreciate any help in figuring out the direction here.
