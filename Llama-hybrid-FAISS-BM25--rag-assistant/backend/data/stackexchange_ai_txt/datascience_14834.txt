[site]: datascience
[post_id]: 14834
[parent_id]: 
[tags]: 
Which optimization technique to use with noisy and slow black-box system?

I have a complex physical system that depends on many continuous input parameters (let's say 10 important ones) and produces an output that I can boil down to one continuous figure of merit. I want to optimize this system and find the set of parameters that produces the best output. The challenges are the following: The system is sufficiently complicated that I cannot predict the behavior well enough to build a useful model. I have to treat it as a black box and do all tests with the actual system. The input parameters are generally correlated. So, optimizing the parameters individually is not enough. The system is slow to react. When changing the parameters it takes some time until an equilibrium state is reached again, depending also on the magnitude of the change. The system is noisy. Even when not touching the system the output figure of merit will fluctuate strongly. I want to optimize the long-term average of the figure of merit, but in order to get a good estimate of that I have to measure for some time (many minutes if I wanted to bring the noise down to a negligible level). On the other hand, the system is relatively well behaved when it comes to the shape of the function to optimize: I know that I'm already close to the global optimum and there are probably no local optima to get stuck in. I expect no sudden changes of the figure of merit. The gradients vary by not more than maybe two orders of magnitude over the permissible parameter space. Question: What is a good technique to apply to this problem? Na√Øve approaches like a simple grid search are prohibitively time-intensive. It seems like there will be a trade-off between long averaging and dealing with noisy data. So I would either need an algorithm that makes as few queries as possible, or one that's very robust to noise.
