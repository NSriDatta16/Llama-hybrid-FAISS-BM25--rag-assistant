[site]: crossvalidated
[post_id]: 461864
[parent_id]: 460808
[tags]: 
I assume when saying "test wether the independent variable has a significantly larger effect on the dependent variable in the adjusted panel than in the unadjusted panel", you are actually trying to find out which model can better describe the uncertainties and relations among the observations . So instead of comparing the difference of the coefficients, a better approach is to perform model selection on your models. Since model selection has to be done on the same set of samples, you need to some how tweak your models to make them applying to the same sample set: Model 1: $$ \begin{align}y & = \beta_0 + \beta_1x_1 + ... + \beta_n x_n + \epsilon \\\Rightarrow y &\sim F(y|x_{1:n},\theta_1) \\\end{align} $$ Where $\theta_1 = \{\beta_{0:n} \text{ and all the other parameters}\}$ , you can understand $F(y|x_{1:n},\theta_1)$ as a distribution of $y$ conditioned on $(x_{1:n},\theta_1)$ . For example when the model is a simple linear regression $y = \beta_0 + \beta_1x_1 + ... + \beta_n x_n + \epsilon,\epsilon \sim N(0,\sigma^2)$ , then $F(y|x_{1:n},\theta_1)$ will be a normal distribution with mean $\beta_0+\beta_1x_1+...+\beta_nx_n$ and variance $\sigma^2$ , i.e. $F(y|x_{1:n},\theta_1) = N(y|\beta_0+\beta_1x_1+...+\beta_nx_n, \sigma^2)$ Model 2: No matter how you "adjust" your samples, there must be a way to represent the adjustment with a function, say $p = h(y)$ , $k=g(x)$ . For example if the adjustment is discounting future payment $y$ into current value $p$ , then the function $h(y)$ will be something like $h(y) = \frac{y}{(1+r)^m}$ . With this idea in mind, your second model $p = \gamma_0 + \gamma_1k_1+,...,+\gamma_nk_n+\epsilon_2$ can be rewritten as: $$ \begin{align}y & = h^{-1}(\gamma_0 + \gamma_1g(x_1)+,...,+\gamma_ng(x_n)+\epsilon_2) \\\Rightarrow y &\sim G(y|x_{1:n},\theta_2) \\\end{align} $$ Where $\theta_2 = \{\gamma_{0:n} \text{ and all the other parameters involved in h() and g()}\}$ . Now that both the models are put to the same set of samples, you can start comparing them. There are two common ways to perform the comparison: method1: If $F()$ and $G()$ are Bayesian models, you can compare their marginal likelihood (the higher the better) or BIC (the lower the better). method2: Use cross validation and compare their cross validated expected-prediction-error.
