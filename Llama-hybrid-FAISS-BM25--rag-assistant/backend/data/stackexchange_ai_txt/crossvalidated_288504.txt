[site]: crossvalidated
[post_id]: 288504
[parent_id]: 
[tags]: 
Does it make sense to implement a PCA after an Autoencoder processing?

I would like to use Autoencoder to pre-process images, then extract vector from the "bottleneck" of the Autoencoder. Then, do a PCA based on the extracted vectors. I feel doing this won't make sense, because it's like repeating the same task---both Autoencoder and PCA do dimensionality reduction. However, the following is why I think I need a PCA after Autoencoder. Could someone point out where I messed up? I want to reduce images with size of 224*224*3 (3 is RGB channels) to a vector with dimension 10~50. I tried through Convolution Autoencoder---if the vector extracted from the bottleneck is able to reconstruct the images, then I'm confident that the vector can represent the images. I experimented a lot. However, seems vector with dimension 10~50 is not able to reconstruct the images at a good level. Then I thought may be this was because reducing dimension from 224*224*3 to 50 had lost too much information. The next I did was to make the bottleneck of the Convolution Autoencoder have a size of 64*14*14 (where 64 means 64 convolution filters, 14*14 is the size of feature map). Now I'm able to well reconstruct the images. But 64*14*14 has way more dimensions than expected 10~50. So I implemented a PCA based on the extracted vectors (64*14*14 dimensions). I found that the top 20 principle axes explained over 80% of the variance in the sample. Based on the large variance explained by the 20 principle axes, can I say that the vector along the 20 principle axes is able to represent the input images? What I'm troubled with, is that, if the top 20 principle axes explained 80+% variance in the vectors, which can be used to reconstruct images. Then why did I fail to reconstruct images with antoencoder that reduced the vector size to 20~50?
