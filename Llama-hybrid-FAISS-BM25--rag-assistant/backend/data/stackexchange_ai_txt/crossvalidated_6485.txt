[site]: crossvalidated
[post_id]: 6485
[parent_id]: 6478
[tags]: 
Variable importance might generally be computed based on the corresponding reduction of predictive accuracy when the predictor of interest is removed (with a permutation technique, like in Random Forest) or some measure of decrease of node impurity, but see (1) for an overview of available methods. An obvious alternative to CART is RF of course ( randomForest , but see also party ). With RF, the Gini importance index is defined as the averaged Gini decrease in node impurities over all trees in the forest (it follows from the fact that the Gini impurity index for a given parent node is larger than the value of that measure for its two daughter nodes, see e.g. (2)). I know that Carolin Strobl and coll. have contributed a lot of simulation and experimental studies on (conditional) variable importance in RFs and CARTs (e.g., (3-4), but there are many other ones, or her thesis, Statistical Issues in Machine Learning â€“ Towards Reliable Split Selection and Variable Importance Measures ). To my knowledge, the caret package (5) only considers a loss function for the regression case (i.e., mean squared error). Maybe it will be added in the near future (anyway, an example with a classification case by k-NN is available in the on-line help for dotPlot ). However, Noel M O'Boyle seems to have some R code for Variable importance in CART . References Sandri and Zuccolotto. A bias correction algorithm for the Gini variable importance measure in classification trees . 2008 Izenman. Modern Multivariate Statistical Techniques . Springer 2008 Strobl, Hothorn, and Zeilis. Party on! . R Journal 2009 1/2 Strobl, Boulesteix, Kneib, Augustin, and Zeilis. Conditional variable importance for random forests . BMC Bioinformatics 2008, 9:307 Kuhn. Building Predictive Models in R Using the caret Package . JSS 2008 28(5)
