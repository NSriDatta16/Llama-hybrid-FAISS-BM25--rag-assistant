[site]: crossvalidated
[post_id]: 589869
[parent_id]: 589863
[tags]: 
I'm not very familiar with the paper in question, but a skimming suggests that it isn't particularly different for gradient boosted trees or random forests. What matters is that the tree ensemble is constant on what the paper calls cells , the partition of space by the tree splitting hyperplanes (fully extended, so a refinement of the partition of space into the constant-prediction regions). Those are the same for random forests and gradient boosted trees, the only difference is that the predicted value is aggregated differently in each of those cells. I have no idea if this is implemented anywhere; I would guess the repository you've linked could be modified very slightly to make it work. You'd need to be able to export the xgboost model into the specific format they use , and then the actual scripts. E.g., in their RandomForest.cpp , the hyperplane getting and leaf regions will be the same, but instead of majorityClass you'll need the predicted values in a leaf.
