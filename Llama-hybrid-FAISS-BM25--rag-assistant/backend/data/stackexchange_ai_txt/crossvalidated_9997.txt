[site]: crossvalidated
[post_id]: 9997
[parent_id]: 
[tags]: 
Information on how value of k in k-fold cross-validation affects resulting accuracies

I've been doing some Machine Learning, and have been using k-fold cross-validation to assess the generalisation performance of the algorithm. I've tried k-fold cross-validation with k = 5 and k = 200 and get very different results for Support Vector Machine classification. k SVM accuracy ----------------- 5 75% 200 94% This seems like a huge difference in accuracy caused by changing the number of splits we're doing for the k-fold cross-validation. Is there any reason for this? I can't seem to find any references on studies that have been done investigating the effects of using different k values. Obviously, which k value I decide to use in my report gives completely different impressions of the quality of my classifier!
