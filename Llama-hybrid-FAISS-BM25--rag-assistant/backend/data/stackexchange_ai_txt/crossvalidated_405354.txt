[site]: crossvalidated
[post_id]: 405354
[parent_id]: 405352
[tags]: 
First of all, $p(\mu|\mathbf{x})$ is not only a function of $\mathbf{x}$ . It is again a function of both $\mu$ and $\mathbf{x}$ , as joint PDF is. Your question would make sense if we were using $p(\mathbf{x})$ , i.e. a function of only $\mathbf{x}$ , instead of the joint, which we don't do of course. Another thing is, we don't approximate the conditional PDF with joint PDF. The $\mu$ that maximizes the joint , also maximizes the conditional . This is just MAP estimation, where we choose $\mu$ such that the posterior, i.e. $p(\mu|\mathbf{x})$ , is maximized (this also means that $p(\mu|\mathbf{x})$ is not only a function of $\mathbf{x}$ , but also $\mu$ ). There, you can ignore the denominator since it doesn't depend on $\mu$ , and acts as a scalar for the specific optimization problem. There might be cases where you don't ignore the denominator. For example, conditional mean , i.e. $E[\mu|\mathbf{x}]$ , sometimes called Bayesian Parameter Estimation in which you generally need to explicitly find $p(\mu|\mathbf{x})$ , (especially if it's not in a common format) and calculate the conditional mean.
