[site]: datascience
[post_id]: 66410
[parent_id]: 
[tags]: 
Non differentiable loss function

I have a loss function that minimizes the error according to what I want the neural network to do. The problem is, that it is a nondifferentiable function. How can I handle this? the loss function: $(1-y) \cdot log(1-p) + min((1-y)-(y \cdot log(p)))$ $y$ : target $p$ : prediction len((1-y)-(y*log(p))) = len(y) = len(p) I have tried to smooth the minimum, but I am not sure this is good enough. As you can see, the min operator is nondifferentiable How to handle a nondifferentiable loss function with Neural Networks?
