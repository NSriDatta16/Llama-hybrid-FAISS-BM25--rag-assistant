[site]: crossvalidated
[post_id]: 79050
[parent_id]: 78440
[tags]: 
This sounds like the perfect job for Bayesian parameter estimation. So the model you have is: $$ k \sim \text{Binom}(p,n) $$ And what you want to know is the probability of $p > 0.1$. Using Bayesian parameter estimation there is nothing stopping you from estimating $\text{prob}(p > 0.1)$ at $n=1, n=2, \dots$ and stopping when the probability $\text{prob}(p > 0.1)$ is too small ("these parameters are probably not going to work") or large enough ("These parameters are probably going to work"). The two things you have to decide is: (1) What are the probability cut-offs when you are going to stop and either keep the parameters because they seem good or toss the parameters and try some new ones. (2) A prior probability for what values of $p$ are likely before you start your experiment. For (2) a reasonable starting point could be to assume a "flat prior", that is, assume that all values of $p$ are equally likely before having seen any data. If, as you state, the typical value of $p$ seems small there might be priors that better reflect the information you have. A great introduction to how to do Bayesian inference on binomial proportions can be found here . I don't know if you use R but a quick function that calculates $p > 0.1$ given $k$ and $n$ would be: bin_prob 0.1) } This however assumes that before seeing any data all possible values of $p$ are equally probable. That is, before seeing any data the probability of $p > 0.1$ is 0.9. A function that perhaps would be better calibrated to your prior information would be: bin_prob 0.1) } This function assumes that prior to any data the probability of $p > 0.1$ is 50%. Here follows some sample output: > bin_prob(k=0, n=0) [1] 0.5 > bin_prob(k=0, n=1) [1] 0.448 > bin_prob(k=1, n=1) [1] 0.829 > bin_prob(k=0, n=4) [1] 0.328 > bin_prob(k=4, n=4) [1] 0.998 > bin_prob(k=8, n=25) [1] 0.997 > bin_prob(k=0, n=25) [1] 0.037
