[site]: stackoverflow
[post_id]: 5379975
[parent_id]: 5379871
[tags]: 
The basic principle is: Set up empty array. Obtain hash-code. Re-hash hash to fit size of array (e.g. if the array is 31 items in size, we can do hash % 31 ) and use this as an index. Retrieval is then a matter of finding the index in the same way, obtaining the key if it's there, and calling Equals on that item. The obvious issue here is what to do if there are two items that belong at the same index. One approach is that you store a list or similar in the array rather than the key-value pair itself, another is "reprobing" into a different index. Both approaches have advantages and disadvantages, and Microsoft use reprobing a list. Above a certain size, the amount of reprobing (or the size of the stored lists if you took that approach) gets too large and the near-O(1) behaviour is lost, at which point the table is resized so as to improve this. Clearly though, a really poor hash algorithm can destroy this, you can demonstrate this to yourself by building a dictionary of objects where the hashcode method is the following: public override int GetHashCode() { return 0; } This is valid, but horrible, and turns your near-O(1) behaviour into O(n) (and bad even as O(n) goes. There are plenty of other details and optimisations, but the above is the basic principle. Edit: Incidentally, if you have a perfect hash (you know all possible values, and have a hash method that gives each such value a unique hash in a small range) it's possible to avoid the issues of reprobing that occur with more general-purpose hash-tables, and just treat the hash as an index into an array. This gives both the O(1) behaviour, and minimal memory use, but only applies when all possible values are in a small range.
