[site]: crossvalidated
[post_id]: 151831
[parent_id]: 
[tags]: 
Does the opposite of nested cross-validation make sense?

I'm asking the question from a machine learning point of view. I have a dataset with relatively high sparsity, so if I use nested cross-validation for my feature tuning and evaluation, that is tune using CV and test on a separate test set after, I get quite different results if I re-sample the datasets (keeping the proportions) and repeat the tuning process. I wouldn't be bothered by some difference, but in my case the differences cannot be ignored. I want to try the opposite configuration in which I train on X, and test on Y during development, and then evaluate final results using CV on X+Z where Z is of the same size as Y. Or even simply have X to train during development and Y to test, and then CV on X for final evaluation. Is this cheating? Has someone done this before (references would be useful)? Does it make sense?
