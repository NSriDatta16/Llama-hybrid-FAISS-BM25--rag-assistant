[site]: crossvalidated
[post_id]: 355238
[parent_id]: 
[tags]: 
Why Kullback-Leibler in Stochastic Neighbor Embedding

Stochastic Neighbor Embedding (and t-SNE) relies on Kullback-Leibler divergence between the point distributions in the original and the low-dimensional space. Why? Why not any other dissimilarity measure (Wasserstein, Jensen-Shannon, Kolmogorov-Smirnov...) The authors, Hinton and Roweis, simply state: The aim of the embedding is to match these two distributions as well as possible. This is achieved by minimizing a cost function which is a sum of Kullback-Leibler divergences between the original and induced distributions over neighbors for each object without giving a justification.
