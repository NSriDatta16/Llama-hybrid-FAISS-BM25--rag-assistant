[site]: crossvalidated
[post_id]: 151116
[parent_id]: 35071
[tags]: 
user974's answer is fantastic from a modelling perspective and gui11aume's answer is fantastic from a mathematical perspective. I want to refine the former answer strictly from a mixed modelling perspective: specifically a generalized mixed modelling (GLMM) perspective. As you can see, you have referenced the R function mer_finalize which is in the fantastic lme4 package. You also say that you are fitting a logistic regression model. There are many issues that crop up with such types of numerical algorithms. The issue of the matrix structure of the model matrix of fixed effects is certainly worth considering, as user974 alluded to. But this is very easy to assess, simply calculated the model.matrix of your formula= and data= arguments in a model and take its determinant using the det function. Random effects, however, greatly complicate the interpretation, numerical estimation routine, and inference on the fixed effects (what you typically think of as regression coefficients in a "regular" regression model). Suppose in the simplest case you are only fitted a random intercepts model. Then you are basically considering there to be thousands of unmeasured sources of heterogeneity that are held constant in repeated measures within clusters. You estimate a "grand" intercept, but account for the heterogeneity by assuming that the cluster-specific intercepts have some mean0 normal distribution. The intercepts are iteratively estimated and used to update model effects until convergence is achieved (the log likelihood--or an approximation of it--is maximized). The mixed model is very easy to envision, but mathematically the likelihood is very complex and prone to issues with singularities, local minimae, and boundary points (odds ratios = 0 or infinity). Mixed models do not have quadratic likelihoods like canonical GLMs. Unfortunately, Venerables and Ripley did not invest much into diagnostics for converge-fails like yours. It is practically impossible to even speculate at the myriad of possible errors leading to such a message. Consider, then, the types of diagnostics I use below: How many observations are there per cluster? What are the results from a marginal model fit using GEE? What is the ICC of the clusters? Is the within cluster heterogeneity close to the between cluster heterogeneity? Fit a 1-step estimator and look at the estimated random effects. Are they approximately normal? Fit the Bayesian mixed model and look at the posterior distribution for the fixed effects. Do they appear to have an approximately normal distribution? Look at a panel plot of the clusters showing exposure or regressor of interest against the outcome using a smoother. Are the trends consistent and clear or are there many possible ways that such trends may be explained? (e.g. what is the "risk" among unexposed subjects, does the exposure appear protective or harmful?) Is it possible to restrict the sample to subjects only having a sufficient number of observations-per-person (say, n=5 or n=10) to estimate the "ideal" treatment effect? Alternately, you can consider some different modelling approaches: Are there few enough clusters or time points that you can use a fixed effect (such as group indicators or a polynomial time effect) to model the cluster level/autoregressive heterogeneity? Is a marginal model appropriate (using a GEE to improve efficiency of standard error estimation, but still using only fixed effects) Could a Bayesian model with an informative prior on the random effects improve estimation?
