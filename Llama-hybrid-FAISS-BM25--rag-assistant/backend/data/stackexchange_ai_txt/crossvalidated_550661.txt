[site]: crossvalidated
[post_id]: 550661
[parent_id]: 273230
[tags]: 
The high variability in performance depending on seed is an indication that you have a noisy validation strategy. A single train-test split is a terrible idea for small to moderate sized datasets. The variability could be reduced e.g. with repeated cross-validation. Note: you should never evaluate repeatedly on a true holdout test set or try different ways of splitting, so I'll call it a validation set. I guess, for a true holdout set, one thing you'd probably do for a small dataset is to think about whether it makes sense to stratify the split by outcome. I'd also be highly suspicious of whether this means anything meaningful about some "training data being better" for a XGBoost model, as I also sort of expressed for random forest for a similar question . You may just be picking the validation set that is the easiest to fit.
