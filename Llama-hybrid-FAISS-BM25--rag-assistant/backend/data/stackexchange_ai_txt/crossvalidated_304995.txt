[site]: crossvalidated
[post_id]: 304995
[parent_id]: 304977
[tags]: 
Nice discussion of this problem is provided by Andrew Ng on his Deep Learning course on Coursera.org . As he notes, the standard splits like 8:2, or 9:1 are valid if your data is small to moderately big, but many present day machine learning problems use huge amounts of data (e.g. millions of observations as in your case), and in such scenario you could leave 2%, 1%, or even less of the data as a test set, taking all the remaining data for your training set (he actually argues for using also a dev set). As he argues, the more data you feed your algorithm, the better for its performance and this is especially true for deep learning * (he also notes that this must not be the cases for non-deep learning machine learning algorithms). As already noticed in comment by Alex Burn , it is not really about size of your test set, but about its representativeness for your problem. Usually with larger size of the data we hope for it to be more representative, but this does not have to be the case. This is always a trade-off and you need to make problem-specific considerations. There is no rules telling that test set should not be less then X cases, or less then Y% of your data. * - Disclaimer: I am repeating Andrew Ng's arguments in here, I wouldn't consider myself as a specialist in deep learning.
