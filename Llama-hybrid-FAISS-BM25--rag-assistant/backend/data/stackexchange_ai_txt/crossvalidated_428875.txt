[site]: crossvalidated
[post_id]: 428875
[parent_id]: 428843
[tags]: 
The point of the RNN is that, in your terminology, w1...w10 are actually all the same sets of weights. The weights are not different for different time steps. So really you have w1=w2=w3=...=w10=w. There are only one set of weights for an RNN regardless of how long the input sequence is. Conceptually you are feeding your outputs back into the inputs of the same neuron - hence recurrence. You therefore only need to store one W which is reused at every time step.
