[site]: datascience
[post_id]: 55439
[parent_id]: 55419
[tags]: 
Logically speaking, I think "X is Green Circle" is a reasonable conclusion. I find the idea in the paper in your question is quite similar to this paper: KRNN: k Rare-class Nearest Neighbour Classification . Intuitively, for example, if a new data point is close to one rare class' point and one common class' point, it is more likely to belong to the rare class. There's no conflict here because a data point will be much more likely to be close to a common class' point. However, once it's already close to a rare class's data point, it's more likely to belong to this class. That said, I didn't check your calculation, I just don't think the "X is Green Circle" conclusion is illogical for this algorithm. [Update] After reconsidering this problem, I think Weighted kNN want to put an emphasis on the rare class data point because that is the class of interest (e.g. Anomaly detection). It's possible that Accuary is not the metrics here but a Weighted Accuracy metrics that penalize a misclassified rare class data point harder so that we can detect more rare class data point.
