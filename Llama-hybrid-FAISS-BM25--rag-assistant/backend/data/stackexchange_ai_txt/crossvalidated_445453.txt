[site]: crossvalidated
[post_id]: 445453
[parent_id]: 
[tags]: 
Realistically, does the i.i.d. assumption hold for the vast majority of supervised learning tasks?

The i.i.d. assumption states: We are given a data set, $\{(x_i,y_i)\}_{i = 1, \ldots, n}$ , each data $(x_i,y_i)$ is generated in an independent and identically distributed fashion . To me, physically this means that we can imagine that the generation of $(x_i,y_i)$ has no affect on $(x_j,y_j)$ , $j \neq i$ and vice versa. But does this hold true in practice? For example, the most basic machine learning task is prediction on MNIST dataset. Is there a way to know whether MNIST was generated in an i.i.d. fashion? Similarly for thousands of other data sets. How do we "any practitioner" know how the data set is generated? Sometimes I also see people mentioning shuffling your data to make the distribution more independent or random. Does shuffling tangibly create benefit as compared to a non-shuffled data set? For example, suppose we create a "sequential" MNIST dataset contained digits arranged in an increasing sequence 1,2,3,4,5,6,..obviously, the data set was not generated in an independent fashion. If you generate 1, the next one must be 2. But does training a classifier on this data set has any difference as compared to a shuffled dataset? Just some basic questions.
