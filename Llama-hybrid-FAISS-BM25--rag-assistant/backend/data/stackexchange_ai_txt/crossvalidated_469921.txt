[site]: crossvalidated
[post_id]: 469921
[parent_id]: 468492
[tags]: 
There is an association between the error of the measurement and the width of the distribution of measurements. I don't know of an official source where it is written down to reference it. In every natural science and engineering this implicitly used like this. It is assumed that the measurements have a fixed and homoskedastic distribution, so a single value is sufficient for the variation. In the case of a Gaussian distribution, it is obvious to use the standard deviation. But for every other (well behaved) case, you can use principally use a transformation; but the shorthand is also just the standard deviation $\sigma$ . So it's the usual formulation for the second central moment of the distribution: $$\sigma_i^2 = \int (y_i- \hat{y_i})^2y_i() dy_i$$ (for a distribution $y_i()$ and its central value $\hat{y_i}$ ) For the weighting matrix, the general case is the inverse covariance matrix. For "visualization", you can start with the most simple case: if you chose a diagonal matrix, the $\chi^2$ is simply the squared sum of deviations divided by the variance -- the inverse elements of the covariance matrix. You can think of these inverse variances $\frac{1}{\sigma^2}$ as weights. A non diagonal covariance matrix simply means that the measurements are not independent from each other, but have a mixed expression. You might know the expression and could diagonalize it, but you don't need to. The matrix multiplication expression of the $\chi^2$ formula entirely takes it into account. You can refer to a standard book of applied statistics, depending on your field. The elements of the covariance matrix are computed pairwise: $$cov(y_i, y_j) = \sigma_{i,j} = \int (y_i-\hat{y_i})(y_j-\hat{y_j}) dy_idy_j $$ First of all you can see that the diagonal elements are the variances, $cov{i,i} = \sigma_i^2$ . And the off-diagonal elements are values that describe the kind of average deviation from the mean value: a positive number means that if you go above the mean value in one variable, the other variable will be distributed around a higher value than its central value. If the covariance value is smaller than zero, you expect values smaller than the central value in the second variable, if you still chose higher values than the central value in the first. For a plot of this, simply look at the first image in the wikipedia entry on covariance .
