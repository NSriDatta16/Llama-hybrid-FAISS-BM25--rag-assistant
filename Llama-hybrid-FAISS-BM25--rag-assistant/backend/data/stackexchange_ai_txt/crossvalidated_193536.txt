[site]: crossvalidated
[post_id]: 193536
[parent_id]: 193505
[tags]: 
If your $X$ is a de-meaned standardized design matrix $X_{ij}$, where $i={1,2,\dots,T}$ - observations, and $j={1,2,\dots,N}$ - variables, then the covariance matrix' MLE estimator would be $$\frac{1}{T}X'X$$, where $X'_{ij}=X_{ji}$ If it's not de-meaned and standardized, then the equations don't look as nice: $$\Omega_{ij}=\frac{1}{T}\sum_{t=1}^T \left[ \left(X_{ti}-\mu_i\right)\left(X_{tj}-\mu_j\right) \right]\frac{1}{\sqrt{s^2_is^2_j}} $$ Where $$\mu_j =\frac{1}{T}\sum_{t=1}^T X_{tj}$$ and $$s^2_k=\frac{1}{T}\sum_{t=1}^T\left( X_{tk}-\mu_k\right)^2$$ Now, this is all dull standard stuff from text books, where implicitly assumed $N/T\to 0$. The reality is more interesting when this does not hold. For instance, in finance and machine learning often $N\sim T$, then things get hairy. Take a look at one of the standard approaches to deal with this situation in this paper by Ledoit called shrinkage .
