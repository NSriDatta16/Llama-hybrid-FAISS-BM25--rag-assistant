[site]: datascience
[post_id]: 68085
[parent_id]: 68082
[tags]: 
BERT is trained on a combination of the losses for masked language modeling and next sentence prediction. For this, BERT receives as input the concatenation of the special token [CLS] , the first sentence tokens, the special token [SEP] , the second sentence tokens and a final [SEP] . [CLS] | First sentence tokens | [SEP] | Second sentence tokens | [SEP] Some of the tokens in the sentences are "masked out" (i.e. replaced with the special token [MASK] ). BERT generates as output a sequence of the same length as the input. The masked language loss ensures that the masked tokens are guessed correctly. The next sentence prediction loss takes the output at the first position (the one associated with the [CLS] input and uses it as input to a small classification model to predict if the second sentence was the one actually following the first one in the original text where they come from. Your task is neither masked language modeling nor next sentence prediction, so you need to train in your own training data. Given that your task consists of classification, you should use BERT's first token output ( [CLS] output) and train a classifier to tell if your first and second sentences are semantically equivalent or not. For this, you can either: train the small classification model that takes as input BERT's first token output ( reuse BERT-generated features ). train not only the small classification model, but also the whole BERT, but using a smaller learning rate for it ( fine-tuning ). In order to decide what's best in your case, you can have a look at this article . In order to actually implement it, you could use the popular transformers python package , which is already prepared for fine-tuning BERT on custom tasks (e.g. see this tutorial ).
