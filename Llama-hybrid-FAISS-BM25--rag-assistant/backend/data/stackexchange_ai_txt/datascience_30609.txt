[site]: datascience
[post_id]: 30609
[parent_id]: 30214
[tags]: 
I don't think we are creating a number of negative examples for every positive example. Negative sampling is done to efficiently compute the softmax. Word2vec tries to maximize the similarity for words in similar contexts. For e.g. given drink the NN should predict water with maximum probability. Suppose you have V (typically > 1 million) words in your training data, so the last layer of your model has V neurons. For every word in your training data, you would have to compute output from all the V neurons (to compute softmax). It is computationally very expensive. Negative sampling is one way to address this problem. Instead of computing the all the V outputs, we just sample few words and approximate the softmax. Negative sampling can be used to speed up neural networks where the number of output neurons is very high. Hierarchical softmax is another technique that's used for training word2vec.
