[site]: crossvalidated
[post_id]: 620563
[parent_id]: 
[tags]: 
How do we calculate the loss (and adjust the weights) in reinforcement learning if we don't know what the full correct output should look like?

I think my question would be best explained using an example. Let's say I want to train a neural network to play Tic-Tac-Toe. Since I'm using reinforcement learning I initialise the network with random weights and let it play a single game. In the fifth move, it returns an output of $[a_1=0.1,a_2=0.09,a_3=0.2,...a_n=0.49,...,a_m=0.05]$ , so since the highest value is $a_n=0.49$ which (for example) corresponds to putting x in the right bottom corner, that's what the network does. After that, it wins the game in 2 moves. The reward function is -1 for a loss, 0 for a draw and 1 for a win. This gives us the reward of 0.81 for this move (assuming $\gamma=0.9$ ). How do I use this information to adjust the weights of my neural network? In a general neural network, we would need to use the loss function to compare the desired output in the fifth move to the expected one. For example, we would know that the optimal strategy would correspond to the vector $[a_1=0,a_2=1,a_3=0,...,a_m=0]$ . We could then calculate the weights that minimise the loss function with respect to this vector. However, in the case of reinforcement learning, we cannot do that since we do not know what the optimal vector for the fifth move looks like. My understanding was that reinforcement learning still uses gradient descent on the loss function, but I do not see how that's possible without knowing what the full expected output looks like. Do we just assume it's $[a_1=0,a_2=0,a_3=0,...a_n=1,...,a_m=0]$ since this move leads to a win, but lower the learning rate based on the reward function or is this done differently? Do we need to use a different loss function?
