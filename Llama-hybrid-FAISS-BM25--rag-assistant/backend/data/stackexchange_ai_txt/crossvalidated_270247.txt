[site]: crossvalidated
[post_id]: 270247
[parent_id]: 155957
[tags]: 
Even in frequentist model building using a criterion of $p\leq 0.05$ for a whether a parameter should be in a model is not in general a useful approach. It may to some extent be reasonable for judging whether a variable is associated with the outcome (when there are multiple parameters there is clearly a multiplicity problem when doing so). However, it is not appropriate to remove such a variable from the model (either based on whether some value is in the posterior credible interval, some model probability or the like), simply re-fit it and then make statements based on the remaining model (i.e. do model selection and then do inference as if no model selection had occurred) - whether these statements are about the association of the remaining variables with the outcome, about prediction or any other thing. When you are in a Bayesian framework this does not become any more appropriate. Many of the same strategies as in frequentist inference can be considered in a Bayesian setting when there is model uncertainty. E.g. Bayesian model averaging with priors given to each model, various approximations to a full Bayesian model averaging (e.g. model weights based on $\exp(-\text{BIC}/2)$, using shrinkage priors like the horseshoe etc.), methods that can potentially set coefficients exactly to zero (mostly maximum-a-posteriori methods), cross-validation and so on.
