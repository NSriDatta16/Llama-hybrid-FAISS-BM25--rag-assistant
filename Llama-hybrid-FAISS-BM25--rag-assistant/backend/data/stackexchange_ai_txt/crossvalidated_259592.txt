[site]: crossvalidated
[post_id]: 259592
[parent_id]: 259531
[tags]: 
I agree with what was said in the answer by @Tim, especially about how you need to think about the extent to which the experiments are similar or different. In effect this is (or ought to be) part of the prior information. At the two extremes you can treat the experiments as independent or as a single experiment. The former case can be called "no pooling" and the latter case can be called "complete pooling". The purpose of my answer is to point out the possibility of "partial pooling", in which the information from each experiment (each dataset) contributes to the prior for every other experiment/dataset. This perspective is discussed in a number of texts, including Bayesian Data Analysis by Gelman and others. The framework for this approach typically involves a hierarchical model in which the prior for the parameters (the regression coefficients in your case) involves hyperparameters along with their hyperpriors. Each dataset informs the hyperparameters, thereby providing the channel through which information flows from one dataset to another. In effect this approach produces "shrinkage" estimates of the coefficients where the individual posterior distributions are shrunk toward a each other. The amount of shrinkage depends on the amount of similarity found during the estimation, which itself can be thought of as an exercise in "signal extraction". Let me provide an example for explicitness. Let $Y_{1:n} = (Y_1, \ldots, Y_n)$ denote $n$ datasets of the form $Y_i = (y_{i1}, \ldots, y_{iT_i})$. In other words, there are $T_i$ observations for dataset $i$. For simplicity, let $$ p(Y_i|\theta_i) = \prod_{t=1}^{T_i} p(y_{it}|\theta_i) $$ and let $$ p(Y_{1:n}|\theta_{1:n}) = \prod_{i=1}^n p(Y_i|\theta_i) . $$ Thus far we can assumed independence in the likelihoods. Now we come to the prior for the parameters $\theta_{1:n}$. Assume the parameters $\theta_i$ are distributed independently and identically given the the hyperparameter(s) $\psi$: $$ p(\theta_{1:n}|\psi) = \prod_{i=1}^n p(\theta_i|\psi). $$ If the hyperparameter $\psi$ is fixed at some value (say $\psi_0$), then the priors for each of the parameters $\theta_i$ are independent and no information is conveyed across datasets. On the other hand, by providing a (non-degenerate) prior for $\psi$, we induce dependence in the prior among the $\theta_i$: $$ p(\theta_{1:n}) = \int p(\theta_{1:n}|\psi)\, p(\psi)\,d\psi . $$ It is this dependence that allows for learning across the parameters of the various datasets. You will learn (to one extent or another) as long as you are "willing to learn." This willingness is expressed in the prior. (It is not a feature of the likelihood.) From this perspective, the prior is not a burden ("I have to have a prior"), rather it is more like a blessing (I get to use a prior). The posterior distribution for $\theta_i$ can be expressed as $$ p(\theta_i|Y_{1:n}) = \frac{p(Y_i|\theta_i)\,p(\theta_i|Y_{1:n}^{-i})}{p(Y_i|Y_{1:n}^{i})} , $$ where $Y_{1:n}^{-i} = Y_{1:n} \setminus \{Y_i\}$. In this expression, we see that the prior for $\theta_i$ is based on all the other datasets. Of course this leaves open the question as to how exactly to structure $p(\theta_i|\psi)$ and $p(\psi)$. A approach that is both simple and natural is to take whatever prior you would have used for a single experiment/dataset and treat the parameters in that prior as your hyperparameters and put a prior on them. This approach is described in detail in Gelman's text (and other texts as well). I will refer to this a the parametric approach. I wish to point out another more general approach that may be of use at some point, if not currently. It is possible to interpret the problem at hand as one of latent-variable density estimation. From a Bayesian perspective, density estimation involves computing a posterior predictive distribution for an observable variable. In this case, however, we wish to compute $$ p(\theta_{n+1}|Y_{1:n}) , $$ which summarizes what is known about the parameter for the next experiment based on all of the observations for the data we have. From the perspective of density estimation, we seek a prior with sufficient flexibility to capture a wide variety of shapes including multimodality. The parametric approach mentioned above is typically too restrictive for this task. Here is the outline of a more flexible approach. Consider the mixture model $$ p(\theta_i|\psi) = \sum_{c=1}^m w_c\,f(\theta_i|\phi_c), $$ where $\psi = (w,\phi)$, $w = (w_1, \ldots, w_m)$ are the mixture weight, $\phi = (\phi_1, \ldots,\phi_m)$ are the corresponding mixture component parameters, and $f(\theta_i|\phi_c)$ is called the kernel . The prior for the hyperparameters is given by letting $\phi_c$ be iid from some distribution $H$ and letting $w$ be drawn from the Dirichlet distribution or some "stick breaking" distribution. In fact, this class of priors includes the Dirichlet Process Mixture model. The purpose of this digression into latent variable density estimation is not to show how to do it, but rather to indicate that such a thing is possible and show the skeletal features of the structure.
