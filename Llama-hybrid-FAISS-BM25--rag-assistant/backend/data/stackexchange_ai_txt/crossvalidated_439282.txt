[site]: crossvalidated
[post_id]: 439282
[parent_id]: 
[tags]: 
Bayesian statistics with a probability observation is wrong

I have data coming from a real system taking measurements, and it occasionally gives a number completely wrong (ranging system with multiple returns). I'm using bayesian inference to accumulate measurements and I'd like to incorporate this chance into my model. Now Bayes theorem gives us $$ P(H|E) = \frac{P(E|H)P(H)}{P(E)}. $$ If $T$ is the event that the observation $E$ is "true" then $$ P(E|H) = P(E|H,T)P(T) + P(E|H,!T)P(!T). $$ So the posterior becomes $$ P(H|E) = \frac{P(H)}{P(E)}\big(P(E|H,T)P(T) + P(E|H,!T)P(!T)\big). $$ In the case that $P(!T)=1$ we would like $$ P(H|E) = \frac{P(H)}{P(E)}\big(0 + P(E|H,!T)\big) =P(H), $$ that is, the posterior is equal to the prior, as we have gained no information. This gets us to $$ \frac{P(E|H)}{P(E)} = 1 $$ which says the probability of the observation is independent of the hypothesis, which is true in the model. But I don't know how to generalise it to the case where $0 and I think I'm missing something obvious. Do I just set $$ P(E|H,!T) = P(E)? $$
