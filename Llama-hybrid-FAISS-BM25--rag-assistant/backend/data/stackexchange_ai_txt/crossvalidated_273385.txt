[site]: crossvalidated
[post_id]: 273385
[parent_id]: 273373
[tags]: 
Following are some things that may work. I am assuming that you are using scikit-learn. use cross validation to avoid overfitting. It could be that the 10% test data that you use for validation is randomely the worst/noisy bit. k-fold cross validation would rotate the tezt subset k times to give you average scores with substantialy reduced overfitting. More information here: http://scikit-learn.org/stable/modules/cross_validation.html Other parameters can be optimized by testing range of values at once using Grid-search. More information here: http://scikit-learn.org/stable/modules/grid_search.html Following are two parameters that often improve the performance of random forest. Note that they need more computational resources. 2.1 Number of trees (n_estimators) may be increased (500-1000) to increase the search space of the algorithm. 2.2 Nodes can be expanded till all leaves are pure (max_depth=None).
