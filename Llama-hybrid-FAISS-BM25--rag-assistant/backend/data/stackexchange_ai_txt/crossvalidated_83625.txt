[site]: crossvalidated
[post_id]: 83625
[parent_id]: 83604
[tags]: 
There is an important difference between the two importance measures: MeanDecreaseAccuracy is calculated using out of bag (OOB) data, MeanDecreaseGini is not. For each tree MeanDecreaseAccuracy is calculated on observations not used to form that particular tree. In contrast, MeanDecreaseGini is a summary of how impure the leaf nodes of a tree are. It is calculated using the same data used to fit trees. When you bootstrap data, you are creating multiple copies of the same observations. Therefore the same observation can be split into two copies, one to form a tree, and one treated as OOB and used to calculate accuracy measures. Therefore, data that randomForest thinks is OOB for MeanDecreaseAccuracy is not necessarily truly OOB in your bootstrap sample, making the estimate of MeanDecreaseAccuracy overly optimistic in the bootstrap iterations. Gini index is immune to this, because it is not relying on evaluating importance on observations different from those used to fit the data. I suspect what you are trying to do is use the bootstrap to generate inference (p-values/confidence intervals) indicating which variables are "important" in the sense that they are actually predictive of your outcome. The bootstrap is not appropriate in this context, because Random Forests expects that OOB data is truly OOB and this is important for building the forest in the first place. In general, bootstrap is not universally applicable, and is only useful in cases where it can be shown that the parameter you're estimating has nice asymptotic properties and is not sensitive to "ties" in the data. A procedure like Random Forest which relies on the availability of OOB data is necessarily sensitive to ties. You may want to look at the caret package in R, which uses random forest (or one of a set of many other algorithms) inside a cross-validation loop to determine which variables are consistently important. See this vignette
