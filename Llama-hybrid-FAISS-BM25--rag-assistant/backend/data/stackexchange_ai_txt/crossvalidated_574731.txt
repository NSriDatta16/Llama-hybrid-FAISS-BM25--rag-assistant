[site]: crossvalidated
[post_id]: 574731
[parent_id]: 
[tags]: 
Backprop in Residual neural network?

I'm trying to build a Residual neural network with 2 layers , and I'm having difficultiy understanding what are the equations for the backprop for the following : $$ W_{2}x+tanh(W_{1}x+b_1)+b_2 $$ for example , I tried to arrive at the jacobian for x , assuming some vector V from previous layers which propogate the error exists , $$ \frac{f}{\partial x}V=W_{2}^{T}V+(1-tanh^{2}(W_{1}x+b_{1}))W)^{T}V $$ But the dimension doesn't match when I try to add them up? a similiar thing happens for $W_1$ $$ \frac{f}{\partial W_{1}}V=(1-tanh^{2}(W_{1}x+b_{1}))x)^{T}V $$ But since V and $W_1x$ are of the same dimension , the deriviate doesn't match up with the dimensions of $W_1$ The main thing which confuses is how to take into account the derv for x if he's appearing twice on the same layer with addition. in the example of $W_1x+W_2x$ , it would remain the same solution , $W_1V+W_2V$ . but I can't seem to understand how to solve it with the added activation function.
