[site]: crossvalidated
[post_id]: 157434
[parent_id]: 157129
[tags]: 
The first three steps are just the substitutions given in the explanation. The fourth step deserves a little explanation. (5.55) expands the chain rule using "all units $k$ to which unit $j$ sends connections." Then, (5.48) expands $a_k$ in terms of its feed forward inputs, the same layer that $a_j$ is from. For example, in a 3 layer neural network, let $a_j$ be one of the hidden layer units. Then $a_k$ would be the output layer units that $a_j$ sends connections to. $a_k$ is computed from the hidden units and that is $a_i$. So $a_j$ and $a_i$ are both the hidden layer. So the partial derivative is zero except when $i=j$ so we are only left with one term remaining. The last step is because $h'(a_j)$ doesn't depend on $k$. $$ \begin{align} \delta_j \equiv \frac{\partial E_n}{ \partial a_j} &= \sum_k \frac{\partial E_n}{ \partial a_k} \frac{\partial a_k}{ \partial a_j} \quad (5.55)\\ &= \sum_k \delta_k \frac{\partial a_k}{ \partial a_j} \quad (5.51, \text{definition of } \delta_j)\\ &= \sum_k \delta_k \frac{\partial}{ \partial a_j}\big(\sum_i w_{ki}z_i) \quad (5.48)\\ &= \sum_k \delta_k \frac{\partial}{ \partial a_j}\big(\sum_i w_{ki}h(a_i)) \quad (5.49)\\ &= \sum_k \delta_k w_{kj}h'(a_j)\\ &= h'(a_j)\sum_k \delta_k w_{kj} \end{align} $$
