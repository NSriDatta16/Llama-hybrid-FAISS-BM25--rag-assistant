[site]: datascience
[post_id]: 102312
[parent_id]: 76872
[tags]: 
Here are the four modifications RoBERTa made for BERT: training the model longer, with bigger batches, over more data removing the next sentence prediction objective training on longer sequences dynamically changing the masking pattern applied to the training data. The authors also collect a large new dataset () of comparable size to other privately used datasets, to better control for training set size effects Because the NSP, in a single task, mixes two tasks: 1) topic prediction and 2) coherence prediction. RoBERTa trains the model on longer sequences and by dynamically changing the masking pattern, making the masked language modeling more difficult, and hence the first one becomes redundant. The masked language modeling task overlaps the topic prediction task. The difference between the FULL-SENTENCES and DOC-SENTENCES is that the former contains topic prediction tasks in the masked language modeling while the other doesn't, and the experiments show that the topic prediction(in the masked language modeling) is also unnecessary.
