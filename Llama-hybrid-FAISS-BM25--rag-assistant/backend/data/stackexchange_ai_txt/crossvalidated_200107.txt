[site]: crossvalidated
[post_id]: 200107
[parent_id]: 
[tags]: 
Learning the Confidence of a Neural Network

Suppose I want to train a deep neural network for classification. The network takes an input vector $x$, and maps this to an output vector $y$. Now, $x$ is of length $n$ and is in fact composed of a vector $a$ of length $n-1$, together with a single element $b$ of length 1, and $b$ is in the range 1 - 10. Now, in my research, I work with robotics. Specifically, $y$ is a representation of the state of the robot, and $x$ is a representation of an observation made by the robot. You can think of $a$ as being some sensor measurements, and $b$ as representing which sensor is being used, for example a camera, gyroscope, GPS etc. During training, I have multiple observation vectors $x$, each with their class labels, and I train the network for classification. During testing, whilst the robot is stationary, I take 10 observation vectors $x$, each with a different value of $b$. To find an overall class distribution, I take all 10 outputs $y$ from these 10 inputs $x$, and average them. But what I would like to do now is to see if I can weight each $y$ during the averaging. The idea would be that some values of $b$ are more confident at predicting $y$ than others, and therefore should have a higher weight in the averaging. However, I am not sure whether this makes sense, given that the network already gives a confidence in the form of the entropy of its output. So, please could somebody explain which of these is correct: 1) To find the confidence of the network for a particular $b$, I take all my training vectors which had that value of $b$, and then find the cross-entropy of the trained network when evaluated on these training vectors. I then use this cross-entropy during testing to weight the observation vector which has that value of $b$ 2) The confidence for a particular $b$ is already built into the network, and so weighting each $b$ differently does not make sense. If a particular $b$ has high confidence, then the output of the network will just have lower entropy. Therefore, providing a weight during averaging is just doing the same thing twice.
