[site]: crossvalidated
[post_id]: 547133
[parent_id]: 
[tags]: 
Why difference a time series for forecasting?

From various econometrics/time series analysis/forecasting texts I take that it is common practice to difference time series that have a stochastic trend before modeling them with forecasting models. I assumed that this somehow improves forecasts. To check this, I coded up an example in which I simulate $N=200$ time points from an unstable AR-process ( $\rho = 1.01$ ), and hence the time series has a stochastic trend. Then I use the first 180 time points to forecast the last 20 time points either by fitting an AR model directly to the raw data, or by fitting it to the differences. Across many repetitions, the forecasting error based on the second approach is on average a bit lower. Why is this the case? I know that differencing is supposed to remove stochastic trends. But why not simply model the stochastic trend? I am doing this here in the first approach, because the AR parameter $\rho$ is not restricted to $|\rho| . Yet, the differencing approach is still better. I am looking for an answer that goes beyond "because model XYZ assumes stationary time series", which I find everywhere and which I don't find very insightful.
