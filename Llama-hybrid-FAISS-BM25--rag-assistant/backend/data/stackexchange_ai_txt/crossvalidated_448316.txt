[site]: crossvalidated
[post_id]: 448316
[parent_id]: 
[tags]: 
Confusing results from lsmeans and pairwise tests

Experiment: I collected data from N participants, each was shown 50 photos and asked to provide sharing likelihood (dependent variable). I also measured personal traits (e.g., affiliative score which where then grouped), and each photo had an associated valence score (also grouped). There were three experimental conditions. I want to measure the impact of condition, valence, and personal traits on the sharing likelihood. I created a lmer model: lmer1.model=lmer(share ~ gender+ age_group + overall_photo_share_freq_group+ affiliative_score_group*valence_group*condition+ self_enhancing_score_group*valence_group*condition+ self_defeating_score_group*valence_group*condition+ aggressive_score_group*valence_group*condition+ (1|photo), data = picshare_df, REML = TRUE) Question Should the random part be participant_id instead of photo ? I want to use it for post-hoc tests. But the mean values I am getting from the model are very different than the mean values I directly calculate. In the model, the dependent variable is share and I want to test hypotheses for different levels of affiliative_score_group , which has three levels. From aggregate(picshare_df $share, by=list(picshare_df$ affiliative_score_group),FUN=mean) I get Low -0.8699349 Medium -0.9134223 High -0.8120141 But from the model: lsmeans(lmer.model,list(trt.vs.ctrl1~affiliative_score_group)) affiliative_score_group lsmean SE df asymp.LCL asymp.UCL Low -0.707 0.0568 Inf -0.819 -0.596 Medium -1.038 0.0565 Inf -1.149 -0.927 High -1.088 0.0563 Inf -1.198 -0.977 These means are different than before, even the order is also different, previously Medium had the lowest value and High had the highest. The test results using lsmeans $`differences from control of affiliative_score_group` contrast estimate SE df z.ratio p.value Medium - Low -0.331 0.0262 Inf -12.639 If I do the tests directly: pairwise.t.test( g=picshare_df $affiliative_score_group , x=picshare_df$ share ,p.adjust.method="bonferroni" ,pool.sd=TRUE ) Low Medium Medium 0.28793 - High 0.05501 0.00015 Now only High-Medium contrast is significant. Both ways of testing shows results consistent with the respective means (from lsmeans and arithmetic average, respectively), but they give contradictory results for the same output. Also I am very confused about the mean values. Question What I am doing wrong or not understanding? Update With a simpler model, I get this: lmer1.model=lmer(share ~ gender+ age_group + overall_photo_share_freq_group+ affiliative_score_group*valence_group*condition+ (1|photo), data = picshare_df, REML = FALSE) lsmeans(lmer1.model, list(trt.vs.ctrl1~affiliative_score_group)) $`lsmeans of affiliative_score_group` affiliative_score_group lsmean SE df asymp.LCL asymp.UCL Low -0.892 0.0550 Inf -1.000 -0.785 Medium -0.995 0.0554 Inf -1.103 -0.886 High -0.889 0.0547 Inf -0.997 -0.782 Results are averaged over the levels of: gender, age_group, overall_photo_share_freq_group, valence_group, condition Degrees-of-freedom method: asymptotic Confidence level used: 0.95 $`differences from control of affiliative_score_group` contrast estimate SE df z.ratio p.value Medium - Low -0.102 0.0250 Inf -4.093 0.0001 High - Low 0.003 0.0235 Inf 0.128 0.9817 These are consistent with the observed means and results from pairwise.t.test . Is it valid to run 4 such simpler models than the complex one? I do not think so. Also, to me, intuitively the means from lsmean from the complex model make more sense (it has the order Lowaffiliative_score_group predictor) than the observed mean. More questions If I use the complex model, should I report the estimates from lsmeans when I talk about the average effect, or plot them (e.g., bar plot or interaction plot)?
