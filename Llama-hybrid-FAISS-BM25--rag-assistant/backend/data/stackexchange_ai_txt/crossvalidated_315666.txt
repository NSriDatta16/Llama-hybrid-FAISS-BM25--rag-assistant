[site]: crossvalidated
[post_id]: 315666
[parent_id]: 
[tags]: 
Relation between Variance of Bernoulli and Logistic Regression?

In reading this article, I came across the likelihood function for logistic regression which is defined as below (for discussion reasons, please assume discrete case): $$L(X|P)=\prod_{i=1,y_i=1}^{N} P(\mathbf{x}_i)\prod_{i=1,y_i=0}^{N} (1-P(\mathbf{x}_i))$$ I'm trying to make some sense out of how this equation was derived(it might be in the article, but I may not have understood it). Through searching around, I found out that the right hand side very similarly resembles the variance of a bernoulli trial $p(1-p) $, and since a discrete logistic regression is used for a multiple bernoulli trial case, thought that there might be something related between the two. In linear regression, one of the metrics used to calculate a good model is by measuring how much of the variance of the dataset is explained by the model. I thought maximizing the likelihood may be something similar, to maximize the explained variance of the bernoulli trials. Is my intuition on the right path, or do I have a very fundamental misunderstanding?
