[site]: crossvalidated
[post_id]: 355435
[parent_id]: 354997
[tags]: 
You are thinking along the correct lines, but it looks like you have more to learn as well. This is a topic that you can spend a lot of time on (much more that can really be answered in this type of forum). I would suggest the Wikipedia page https://en.wikipedia.org/wiki/Multiple_comparisons_problem as a place to start, then follow the links and read the references from there. The Bonferroni is known to be conservative in most cases (it over adjusts so that the real probability of getting any type I errors is less than the target rate as you computed with the 7.25% compared to 10%). The False Discovery Rate and others are becoming more popular, but require thinking about it a little differently. Most of the standard corrections just increase the p-value or make the confidence intervals wider to take into account the extra variability from the multiple tests. But these do not do anything to take into account that the most "significant" estimates are likely to be biased away from the null by chance. There are penalized regression methods (ridge, lasso, etc.) that bias the extreme estimates towards 0 (the null) in an attempt to counter the multiple comparison bias, but these do not provide nice simple p-values or confidence intervals (but can be very good for prediction modelling and general exploration). Bayesian hierarchical models provide another option for inference where you fit all the models together and let them learn from each other. This also pulls the extreme estimates towards the center for a more meaningful model. Edit for expanded question. The way that your expanded question is worded indicates that you are still mis-understanding some fundamentals of frequentist hypothesis testing. All your calculations are conditional on the number of hypotheses that are actually true or false (just unobserved). You talk about the probability of rejecting at least one hypothesis out of 100 by chance as being 99%, but that is only if all the null hypotheses are true. If they are all false then the probability of rejecting any by chance alone is 0%. If half are true and half are false then the probability of rejecting by chance is between 0% and 99%. You are partially correct in that if you reject one null out of 100 that that can easily be explained by that (and others) being true and just chance gave you significance. But it is also possible that that null is false and you correctly rejected it. It is even possible that many of the other nulls are also false and by not rejecting them you made type II errors. So basically in this case there are many possibilities and you have not ruled any of them out. You talk about the probability of having made a type I error conditional on rejecting hypotheses, but you can't do that under frequentist theory. Each null hypothesis is either true or false (just unknown which) so the probability of having made a type I error is either 0% or 100% (just unknown which). If 15 of the 100 were significant this could be because all 15 nulls were false and you made correct decisions, all 15 nulls were true and you made a type I error (this is unlikely due to chance, but still possible), or some subset of the 15 are true and the rest false. Part of the idea of the False Discovery Rate is to say I know that I will falsely reject some nulls (and possibly correctly reject some in the process as well) I just want to limit how many false positives I will find (on average). If you have the false discovery rate set at 10% and reject 15 out of 100 nulls, this would imply that 10 of those rejections are false positives and 5 are true positives (but this is on average, so it could be 9 and 6, or 11 and 4, or any other combination). The usual follow-up question is then "which 5 are real?" and the answer is "You don't know". Now if your goal is just to say "at least 1 slope is not equal to 0" and you don't care specifically which one, then your binomial calculations can work and you can say 15 out of 100 is unlikely if all the nulls were true, so you reject the composite null of all 100 being true. This is a form of multiple comparison adjustment (based on different assumptions than the Bonferroni, but still legitimate if you are happy with the assumptions, this is one form of meta-analysis). Your final question is that you think you don't need a correction, but you made a correction, just different from the Bonferroni (you don't need the Bonferroni in addition to what you did). But note that your slope estimates (conditional on rejecting the null) are likely biased away from 0. You can see this for yourself with some simulations. Here is some simple R code: set.seed(1) x In this case all the slopes are 1, 100 regressions are fit and 16 have p-values less than 0.1. If you look at the summary of all the slopes they are centered close to 1, but if you only look at the "significant" ones, then they are all greater than 2, quite a bit of bias. You can change the seed and other pieces of this simulation and try it multiple times for yourself. The bias is not a problem if you only want to say at least one slope is not 0, but as soon as you start to try to predict new values or interpret any of the "significant" slopes, this will be a problem. You talk about the probability of the null hypothesis being false (or true), then you are starting to think more along Bayesian than Frequentist lines. In that case it is better to just do a full Bayesian approach (done properly this can also help with the bias in the slope estimates). But you should do the Bayesian approach properly with a meaningful prior (this takes some thought). Edit 2 Maybe these 2 references will help: https://www.sciencedirect.com/science/article/pii/S0140673605664616 https://www.sciencedirect.com/science/article/pii/S0140673605665166
