[site]: datascience
[post_id]: 40276
[parent_id]: 40275
[tags]: 
It is essential for all input patterns to have the same number of features. The reason is that each input feature is connected to specified neurons and they have specified trained weights. A typical solution is to resize your input by reserving its aspect ratio. You should consider that if you want to have good accuracy, you have to have such an operation while training the network too. Even in convolutional networks, it is essential to have the same input size. Although the convolutional layers won't bother you, the connection of fully connected layers and flattened layers used after convolutional layers will have a problem if you don't use correct input shape; the dimensions won't match. About recent studies, I have not seen yet, but a typical solution can be employing PCA and using let's say its top-10 features as the input of a fully connected network, although for input patterns with a huge difference in the number of input dimensions I guess it is not logical.
