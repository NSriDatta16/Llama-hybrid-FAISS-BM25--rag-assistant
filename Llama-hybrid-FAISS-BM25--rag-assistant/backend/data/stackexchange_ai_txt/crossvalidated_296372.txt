[site]: crossvalidated
[post_id]: 296372
[parent_id]: 262429
[tags]: 
According to Watkins (1989) (see page 220 and further): "The agent learns using an initial estimate of Q and data from experience, which consists of observations of the form $[x, a, r, y]$, where $x$ is starting state, $a$ is action in state $x$, $r$ is immediate reward and $y$ is state after the transition." The important thing is, that sample tuples $[x_t, a_t, r_t, y_t]$, where $t$ is $t$-th observation may not come from a connected sequence of actions. In fact, you can sample them pretty randomly from the environment. However, you need sufficient amount of samples from all states and all actions to get good Q estimates. It means, that you can choose arbitrary policy for discovering the environment, that runs through all possible transitions sufficient number of times or you may simply be given a batch of sample transitions and you do not need to have any policy at all. That is why Q-learning is an off-policy method. It only needs sample transitions from the environment sampled by some policy and outputs Q values. If you then want to find optimal policy for the environment, in each state it is enough to choose the action with highest Q value. To see why this is optimal policy, again see Watkins. So to finally answer your question, since Q-learning is an off-policy method and it only needs samples of transitions, I do not think, it is a probabilistic method. A probabilistic method would be a method, that updates its value function online, which Q learning does not necessarily do. (Even though it can, combined with e.g. $\epsilon$-softmax policy - see Sutton & Barto 2012 , softmax policies)
