[site]: crossvalidated
[post_id]: 292378
[parent_id]: 
[tags]: 
Are larger neural networks more suceptible to get getting stuck in local minimum than smaller ones?

I am training a "mini 3D Alex net" (2 convolutions + 1 max pooling + 1 dense layers) and a standard 3D Alex net on some simple synthetic data I've generated that just consists of cubes and squares (I have more complex data but I want to make sure it works on this simple test first). The mini Alex net converges relatively quickly (around 500 iterations for batch size of 30). However, the full Alex next has not converged yet and the loss functions seems to be bouncing around a small range. This may just be because the full Alex net is so much larger (especially in the 3D) that it's taking longer for all the weights to learn. But, in the case that the full alexnet is getting stuck in the local minimum, is there a reason why large nets are more likely to get stuck in local minimum?
