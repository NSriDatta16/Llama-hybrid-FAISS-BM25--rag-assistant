[site]: crossvalidated
[post_id]: 253285
[parent_id]: 252992
[tags]: 
This question gets to an interesting issue with using the Shannon entropy in practice: due to the logarithm in the entropy formula, the entropy value you get by plugging in a set of probabilities is necessarily biased below the true population entropy value, depending on the number of classes and the number of observations. This Cross Validated page discusses this issue further and contains some links to further reading. So if the samples you are averaging over are of different size or have different numbers of classes/probabilities, you will be averaging over samples whose entropy values have different biases, which could tend to lead to trouble. Even if all your samples have the same size and numbers of classes, you will still get an average entropy that is biased low. Depending on your application that might be acceptable, but it would be wise to examine the literature on this as you proceed, as another answer suggests.
