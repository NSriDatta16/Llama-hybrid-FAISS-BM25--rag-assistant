[site]: crossvalidated
[post_id]: 615935
[parent_id]: 615910
[tags]: 
If I understand this correctly, there is no data leakage. You are just using information that at the point at which you want to predict the outcome is available, and therefore can be used. The concept of information leakage comes into play when using resampling, data set splitting, cross-validation and the like. These technques use some part of the data in order to predict some other part of the data that in fact you already have, but in order to assess the prediction quality on unseen data properly, you pretend that you don't have it (as you are emulating a real situation in which you wouldn't have that information at the point where you use the other part for prediction). Information leakage means that for some reason you set up things in such a way that at some stage you use information that you shouldn't use, because in a real situation you wouldn't have it. As far as I understand your situation, this is not the case here. But what if ML model easily captures the relationship (if target achieved >= target set - high likelihood to meet the target else low likelihood to meet the target). So, in this case do we need ML at all in the first place? Why would the fact that ML does a job successfully (and rather easily) be a reason that we don't need it at all in the first place? If your prediction problem indeed is to predict achievement of the target in a situation in which you already have strong information that indicates with large probability what will happen, so be it! (The only thing one could worry about is whether a simpler approach will do the trick already, but as long as it's not a big problem to set up your random forest, you may well use it.)
