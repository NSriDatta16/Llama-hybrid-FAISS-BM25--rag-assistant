[site]: crossvalidated
[post_id]: 262938
[parent_id]: 262914
[tags]: 
If these features are not ordinal i.e. 2 is not greater or lesser than 1 then it makes sense to do one hot encoding . As long as the values are nominal it does not matter if they are {0,1} or {1,2} or {3,4} or {male,female}. PCA is good for dimentionality reduction but if your end aim is classification I would rather use methods like LDA's which also take your labels into account. Also, in my research ive observed that if you have enough data and your aim is to do classification then you should go for autoencoders (i was working with audio but I suspect it is equally well for other data sets also as per the original authors, it learns PCA like features and is a better basis as seen by many others). However what constitutes as "Enough data" is dependent on the problem . They have an added benefit as most times a non linear transformation of data may work for you, but with PCA you are limiting yourself to linear transformations and that too without taking advantage of the labels also if the data is too big it becomes difficult to really do the matrix operations involved ,on the other hand auto encoders can be batch processed. Autoencoders can be fine tuned and thus overall you end up achieving a better basis for classification. That being said ,its good to experiment with PCA....It may work better or not, all depends on your problem.You might want to check out this thread .
