[site]: crossvalidated
[post_id]: 449735
[parent_id]: 
[tags]: 
Why are frequentists uncomfortable with bayesian statistics when "optimization" algorithms used in frequentist statistics is bayesian?

In Step 1, we have a prior. Using bayes rule we construct the posterior. In step 2 of some iterated bayesian procedure, the prior becomes the posterior from step one and use bayes rule to calculate the new posterior. By induction we can do this forever and construct new posteriors based on priors. I heard this is how Bayesian anything works. This sounds like any optimization algorithim including Newton-raphson which uses previous information to predict the next step and updates some value. I'm not sure why Frequentists are okay with optimization which is converging to a solution using prior information but are uncomfortable with priors and posteriors. Why? Does frequentist optimization not update values to converge at a solution?
