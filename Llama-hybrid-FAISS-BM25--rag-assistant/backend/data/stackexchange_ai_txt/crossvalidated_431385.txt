[site]: crossvalidated
[post_id]: 431385
[parent_id]: 
[tags]: 
Basic question about cross-validation

I try to understand how cross-validation (CV) works. What I wonder is the following: For example, let's take a view at split2 and fold2. The model saw this data already in split1, so what is the benefit here now? Is it to create a kind of disruption as there is now no direct connection between fold1 and fold3 (when treating the data as a time series, for example)? And furthermore, however, when the model is evaluated on these certain folds, what is the purpose of the test set when there is already a validation set during the training in each split? Thus, in split1, fold2-5 are training sets and fold1 is the validation set, right? edit: A further question due to Paul Hewson's comment: When CV is meant to provide more data to train on, why not just repeating the whole procedure arbitrary times? After performing all splits, you just repeat the entire process again. How does the model train on the data several times due to CV? After performing the first split, what does the model keep of that information? Let's consider a neural network: The weights have been adjusted accordingly and when the second split starts, will it enter this 2nd split with the weights from the 1st split? So is it a kind of weight initialization? Some aspects are also addressed here: Cross-validation including training, validation, and testing. Why do we need three subsets? but though I get the principle, I don't get how this idea enters reality.
