[site]: crossvalidated
[post_id]: 467519
[parent_id]: 
[tags]: 
Conceptual question on Image semantic/ instance segmentation networks

I am trying to understand better how the image semantic/ instance segmentation work. I understand going from the concept of the perceptron that Deep Neural Networks have one or both of the following: a CNN for features leaning and outputs feature maps and/or a FCN that flattens its inputed feature maps and may or may not be dense with final output layer having n nodes where n is the number of class that we wish to classify. While training the network, we usually design a loss function that minimizes the error between the true outputs y_true (Ground Truth or Label), and the predicted output y_pred that the network has generated. We can then update the weight with our obtained minima often via backpropagation. 1) How does this work when it comes to image segmentation networks? 2) How are the masks that come with the dataset used in getting the correct weights to output the correct predicted masks for each input image? 3) Is the loss function here minimizing the error in each pixel? that seems to be a lot of calculations. 4) if that is the case, how do we ensure that after the weights have been trained to properly predict the mask of a given image, those same weights will also perform well for a different image? Thank you very much.
