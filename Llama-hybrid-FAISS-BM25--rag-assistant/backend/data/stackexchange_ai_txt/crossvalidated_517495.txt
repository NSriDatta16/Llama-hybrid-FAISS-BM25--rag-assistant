[site]: crossvalidated
[post_id]: 517495
[parent_id]: 516914
[tags]: 
Short answer: for some applications, models are treated with uncertainty, but usually the interest is in $p(M | X)$ , not $p(M | \theta)$ . I think the example - $p(M | \theta)$ - is obscuring the question - "Are models random variables?".[1] $p(M | \theta)$ is kinda an odd thing to ask about. Does specifying an intercept of a linear model tell you much about whether a linear or quadratic model is more plausible, independently of any data ? On the other hand, $p(M | X)$ is of intense interest in model comparison / selection tasks. Suppose I have some data and want estimate whether a linear model $m_0$ : $y = \alpha + \beta x$ is more plausible than simple quadratic model $m_1$ : $y = \alpha + \beta x^2$ . The set of events $M$ for the distribution $p(M)$ will then have two elements: $M = \{m_0, m_1\}$ . $p(M)$ is then an assignment of values in $(0, 1)$ to each element of $M$ s.t. they sum to 1, representing their relative a priori plausibility. If I want to determine which model is more plausible for my data, I can compute $P(M | X)$ using Bayes' theorem: $$p(M | X) = \frac {p(X | M) p(M)}{p(X)}$$ Where $$p(X) = \sum_{m \in M}{p(X | m)p(m)}$$ And as you have above (using $\theta_m$ to clarify that different models can have different parameters). $$p(X | m) = \int_{\theta_m} { p(X | m, \theta_m) p(\theta_m | m)d \theta_m}$$ The posterior $p(M|X)$ allocates mass to models in $M$ that were more plausible given the data. You might find Bayes' factors relevant here, as an example usage of posteriors over models. [1] You'll probably see Bayesians often talking about quantifying uncertainty over sets of events/propositions/beliefs rather than "random variables" per se. Models themselves aren't random in some metaphysical sense, but we are often uncertain which model is "the best" given our observations. So it's often more natural in this context to talk about uncertainty over beliefs. Edit: copying some notes from the comments. To explicitly answer the question, I would say $$ is an RV as long as you interpret it carefully. If $$ is the set of all models, and $p($ ) is the function that maps models to plausibilities, then $$ is an RV that just indexes our set of models $$ . Basically a categorical- or function-valued RV. This is not a very common way to talk about it, though. However, to me it's not intuitive to talk about $M$ as an RV. If somebody is about to roll a die, then it's natural to use an RV that maps the set of die faces (outcomes) to the numbers 1-6. But if somebody rolls a die and hides it, and asks what side I think is up, it's no longer "random" in the sense that there's already an outcome; I just don't know it yet. This is closer to the situation in model comparison, where it'd be more typical to talk about plausibilities than random "outcomes". As for how a model relates to parameters. You can think of a function as a mapping from values in one set to another. Eg $=$ maps values in $\mathcal{R}^2→\mathcal{R}$ . You could also say $=$ describes a set of functions, one for each possible value of $$ , that map $\mathcal{R} \rightarrow \mathcal{R}$ . We call $$ a parameter when we have observations for $$ and $$ , but not $$ . Another way to think about it is that $$ "selects" a function, such as $=2$ for $=2$ . $(|,)$ is then the likelihood of param $$ for the model $=$ , given our observations. (This relates to something the other answer was trying to get at: models don't depend on parameters. For example, if we have two models: $=_0+_0$ and $=^2+_1+_1$ , we can see immediately that they don't share the same parameters. If we knew the parameters for one, it wouldn't tell us anything about the plausibility of either model independent of any data.) [...] And since models in general don't/cannot share parameters we cannot really express $(∣)$ . Similar for $(∣)$ unless $$ denotes models corresponding to the parameters of those models. I believe technically you can express both of these as long as $m \in M$ and $\theta_m$ are consistent. What's challenging is the interpretation of $p(M | \theta)$ ; I can't think of a use for this, and my intuition is that it's actually just $p(M)$ . ( $p(\theta_m | M=m)$ is fine, as it just describes the prior we have on parameters for some selected model.)
