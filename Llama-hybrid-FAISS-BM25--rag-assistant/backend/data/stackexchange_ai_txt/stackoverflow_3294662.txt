[site]: stackoverflow
[post_id]: 3294662
[parent_id]: 3289589
[tags]: 
Sure it can. Information gain is just the change in information entropy from one state to another: IG(Ex, a) = H(Ex) - H(Ex | a) That state change can go in either direction--it can be positive or negative. This is easy to see by example: Decision Tree algorithms works like this: at a given node, you calculate its information entropy (for the independent variable). You can think of it like this: information entropy is to categorical/discrete variables as variance is to continuous variables). Variance, of course, is just the square of the standard deviation. So for instance, if we are looking predicting price based on various criteria, and we have arbitrarily group our data set into two groups, in which the prices for group A are (50, 60, and 70), and the prices for group B are (50, 55, 60), group B has the lowest variance--i.e., they are close together. Of course variance cannot be negative (because after you sum the distances of each point from the mean, you square it) but the difference in variance certainly can . To see how this relates to Information Entropy/Information Gain, suppose we aren't predicting price but something else, like whether the visitor to our Site will become a registered user or a premium subscriber, or neither. The indepdendent variable here is discrete, not continuous like price, so you can't calculate variance in a meaningful way. Information entropy is what is used instead. (If you doubt the close analogy here between variance and IE, you should know that most decision tree algorithms capable of handling both discrete and continuous variables, in the latter case, the algorithm will use variance as the splitting criterion, rather than using IG.) In any event, after you calculate the information entropy for a given node, you then split the data at that node (which is the entire data set if you are at the root node) on every value for every variable, then for each split, calculate the IE for both groups, and take the weighted average IE. Next take the split that results in the lowest weighted average IE and compare it with the node IE (which is obviously just a single group). If that weighted average IE for the split is lower than the node IE, then you split the data at that node (form a branch), if not, then you stop, i.e., that node can't be further split--you are at the bottom. In sum, at the heart of the decision tree algorithm is the criterion to determine whether to split a node--that's how they are constructed. That criterion is whether IG is positive or negative.
