[site]: crossvalidated
[post_id]: 290819
[parent_id]: 
[tags]: 
Convergence of sum of residuals to zero vs Minimizing sum of squared residuals

Disclaimer: This question is not about the comparison between OLS, sum of squared residuals (mean based approach) and the sum of absolute residuals (median based approach), which has been already fully debated on this site. The question is about the feasibility of using a simple sum of residuals to find the best fitting line of a dataset. Intuition: Given a set of values, its mean not only minimizes the squared residuals from the mean, but also ensures that the total sum of residuals is zero. Therefore the mean can be found leveraging either of those properties. My Question: When performing a linear regression, wouldn't we obtain the same result of OLS if we required the total sum of residuals to simply converge to zero? In that way instead of looking for the minimum of a quadratic equation, we would directly solve a simpler linear problem. I understand the benefit of being able to compare the goodness of fit across datasets by having the total sum of squares, but I have the feeling (and here I would like you to prove me wrong) that the problem of finding the best fitting line doesn't require a quadratic equation.
