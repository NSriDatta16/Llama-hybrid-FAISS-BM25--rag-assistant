[site]: datascience
[post_id]: 106401
[parent_id]: 
[tags]: 
How to understand expected value function in a stochastic policy setting?

I am currently reading "Deep Reinforcement Learning with Python" by Sudharsan Ravichandiran. I am still on the first chapter of Introduction and understanding the most basic concepts. He has introduced this grid reinforcement learning problem: Where the goal is to go from A to I without touching the shaded parts. Using a stochastic policy, the author specifies two trajectories the agent takes starting from state A on two episodes: A - D - E - F - I A - B - E - F - I Looking at the value function: The author says that the expectation is equal to: However, I am confused. Why are we only looking at the probabilities of the starting state A of going down and right, and not what happens after? My confusion lies in the following: If we assume we have a third episode with a third trajectory: A - B - C - F - I (in this case lets assume C is not shaded so that we don't multiply by 0) How would the formula change? I can understand that we would add another summation part (R(\tau_3)\pi(right|A), but it seems counterintuitive to me to use the same probability of going right (0.2) even though the future actions have different probabilities. Furthermore, this is just the addition of a single trajectory. If we were to add many more, it seems like it would be possible to get an end expected value state function of infinity, which does not make any sense. Should we take the mean of expected values of going right? Meaning, since there are two different possible scenarios of what happens after we go right from A, should we divide by two to get an average return from going right?
