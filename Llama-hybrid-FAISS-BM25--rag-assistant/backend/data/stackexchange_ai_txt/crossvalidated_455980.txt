[site]: crossvalidated
[post_id]: 455980
[parent_id]: 
[tags]: 
Is it actually "wrong" to drop hypothesis tests after looking at the data?

After looking at the dataset and some summary statistics, one can often already "eyeball" that statistical testing is not needed to tell certain experimental treatments are not different from each other. From a Bayesian point of view, one can then update your hypotheses to be tested based on this prior information (and narrow the scientific question you try to address, if you start off with fairly general and wide coverage mindset). So, whether a full multiple comparison (with heavy penalty) as according to what is implicitly pre-planned in the experimental design is actually needed is very debatable in my opinion. Is this way of thinking legit? After reading this , I think the take-home is that you can "group" statistical hypotheses tgt which contribute to a decision made regarding one of your original scientific questions/hypotheses (not the same as statistical hypotheses). So, depending on the question needed to address (with answer/conclusion deriving from interpreting statistical test), one could actually run different sets of comparisons on the same dataset to answer different, unrelated questions. But I could be very wrong because I am not a statistician. Can someone shed some conclusive insight?
