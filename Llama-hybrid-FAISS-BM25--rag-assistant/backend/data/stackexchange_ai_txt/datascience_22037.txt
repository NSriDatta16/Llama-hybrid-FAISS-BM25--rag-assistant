[site]: datascience
[post_id]: 22037
[parent_id]: 
[tags]: 
Why does it speed up gradient descent if the function is smooth?

I now read a book titled "Hands-on Machine Learning with Scikit-Learn and TensorFlow" and on the chapter 11, it has the following description on the explanation of ELU (Exponential ReLU). Third, the function is smooth everywhere, including around z = 0, which helps speed up Gradient Descent, since it does not bounce as much left and right of z = 0. The z means the x-axis on the graph above. I understand the derivative is smooth since the z line has a curve and in that realm the derivative is no longer equal to 0 . However, why is it the case that if the function is "smooth everywhere, including around z=0", it speeds up Gradient Descent?
