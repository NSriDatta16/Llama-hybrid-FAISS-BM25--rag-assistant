[site]: crossvalidated
[post_id]: 487894
[parent_id]: 487886
[tags]: 
It means that the test data look like the training data. For example, if your facial recognition system was developed in China, it might work well in China, but not if you try to use it in a country where people look different. The "drawn" part means that the data is sampled at random from some population of interest. For example, the population could be everybody in China, and you assume that everyone has an equal chance of being in your training data. This might not be true, but it could be close enough to have a reasonable useful model. The "distribution" part is what I called the "population of interest" in the previous paragraph. This assumption is crucial when you try to evaluate the model. Usually this is done by splitting off a part of the data, chosen randomly, and using it to test the model. But if the model is evaluated in this way, then in the future it needs to be applied to data that looks like the data which was originally put into it, or else any claims about the model's performance will be misleading. For example, it could be that your facial recognition system is 90% accurate when applied to Chinese faces, but only 50% accurate when applied to Russian faces. But you wouldn't have known that in advance, because your test data consisted only of Chinese faces. In the case of facial recognition, it's fairly obvious what will go wrong if you apply it to unseen data. But this problem also happens for other models, often because future data doesn't look like past data. For example, banks have to re-do their credit risk models every year. This is a huge project which provides employment for a lot of people. But (as an auditor once said to me) if the models work, why do they have to be re-done every year? And if they don't work, why make the effort to build them at all?
