[site]: crossvalidated
[post_id]: 384610
[parent_id]: 
[tags]: 
Error propagation in Gaussian processes

I am trying to predict a time-series measurements, $x_{t+1},x_{t+2},...$ using a Gaussian process model, where the kernel is a Gaussian kernel with small noise-level $\sigma_w^2$ , i.e. $k_{ij}=e^{-\frac{\Vert x_i-x_j\Vert_2^2}{\epsilon}}+\sigma_w^2$ Each feature vector $x_t$ consists of an interval in time of $k$ historical measurements, i.e. $x_t=[r_t, r_{t-1},...r_{t-k+1}]$ . Suppose I want to predict the value of $r_{t+1}$ , then given the Gaussian process formula, I get that: $\hat{r}_{t+1}=k_*^TK^{-1}f$ , where $f=[r_t,r_{t-1},...]$ and the variance is given by $\text{var}(r_{t+1})=1+\sigma_r^2-k_*^TK^{-1}k_*$ Where $k_*=e^{-\frac{\Vert x_i-x_{t+1}\Vert_2^2}{\epsilon}}, i=1,...,t$ Now, suppose that at time $t$ , I want to predict $r_{t+L}$ , so what I was doing is predicting recursively $r_{t+1}$ , then using it to predict $r_{t+2}$ and so on. The question is, how to predict the variance for $\hat{r}_{t+L}$ ? Since now I don't use $r_{t-k}$ for some $k$ , which are determined, but rather $\hat{r}_{t-k}$ which are estimated with some variance. Trying to block the error in a straight forward way, using error propagation approaches does not seem to be very elegant. I hope the question is clear, I'd appreciate any help!
