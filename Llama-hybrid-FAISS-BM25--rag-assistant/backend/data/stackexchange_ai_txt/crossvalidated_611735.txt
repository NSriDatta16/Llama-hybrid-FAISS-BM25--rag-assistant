[site]: crossvalidated
[post_id]: 611735
[parent_id]: 
[tags]: 
Multivariate analysis for subjective decision making

I am trying to find the best US state using 25 columns of normalized data (best = 1, worst = 0) such as crime rate, GDP, house prices, and others. This results in a 50x25 Excel table. Afterward, each state is given a score by adding the products of each variable with its respective relative weight. Here's a simplified example with trivial data: State Crime rate (w = 0.2) GDP (w = 0.3) House prices (w = 0.5) Mark Iowa 0.20 1.00 0.15 42 % Arkansas 0.30 0.60 0.65 57 % New Mexico 0.10 0.40 0.30 29 % ... ... ... ... ... This approach is functional, but working with such large amounts of data in Excel can be tedious, and maintaining the relative weight sum of 1 while staying true to my opinion becomes more challenging as more variables are added to the calculation. My goal is to gather as much data as possible to make the most objective decision subjectivity can provide. Perhaps Python could be a helpful tool? Machine learning, pandas...? I am curious if anyone has suggestions for a better approach. Thank you in advance!
