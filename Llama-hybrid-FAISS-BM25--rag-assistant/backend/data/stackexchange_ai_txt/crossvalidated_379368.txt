[site]: crossvalidated
[post_id]: 379368
[parent_id]: 
[tags]: 
Why does the Conv Neural Net using Tensorflow returns same predictions for all the data points

I am predicting usage quantity for different customers across different categories. Following is the network architecture. #Create input placeholders x = tf.placeholder("float", shape=[None, 6,15,1]) y = tf.placeholder("float", shape=[None, n_classes]) #Define Weights weights = { 'wc1': tf.get_variable('W0', shape=(2,2,1,8), initializer=tf.contrib.layers.xavier_initializer()), 'wc2': tf.get_variable('W1', shape=(2,2,8,12), initializer=tf.contrib.layers.xavier_initializer()), 'wc3': tf.get_variable('W2', shape=(2,2,12,16), initializer=tf.contrib.layers.xavier_initializer()), 'wc4': tf.get_variable('W3', shape=(2,2,16,20), initializer=tf.contrib.layers.xavier_initializer()), 'wd1': tf.get_variable('W4', shape=(1*1*20,20), initializer=tf.contrib.layers.xavier_initializer()), 'out': tf.get_variable('W5', shape=(20,n_classes), initializer=tf.contrib.layers.xavier_initializer()), } #Define Biases biases = { 'bc1': tf.get_variable('B0', shape=(8), initializer=tf.contrib.layers.xavier_initializer()), 'bc2': tf.get_variable('B1', shape=(12), initializer=tf.contrib.layers.xavier_initializer()), 'bc3': tf.get_variable('B2', shape=(16), initializer=tf.contrib.layers.xavier_initializer()), 'bc4': tf.get_variable('B3', shape=(20), initializer=tf.contrib.layers.xavier_initializer()), 'bd1': tf.get_variable('B4', shape=(20), initializer=tf.contrib.layers.xavier_initializer()), 'out': tf.get_variable('B5', shape=(n_classes), initializer=tf.contrib.layers.xavier_initializer()), } #Define convolutional layer def conv2d(x, W, b, strides=1, reuse=True): # Conv2D wrapper, with bias and relu activation x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME') x = tf.nn.bias_add(x, b) return tf.nn.relu(x) #Define Maxpool layer def maxpool2d(x, k=2): return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],padding='SAME') #Define a convolutional neural network function def conv_net(x, weights, biases): conv1 = conv2d(x, weights['wc1'], biases['bc1']) conv1 = maxpool2d(conv1, k=2) conv2 = conv2d(conv1, weights['wc2'], biases['bc2']) conv2 = maxpool2d(conv2, k=2) conv3 = conv2d(conv2, weights['wc3'], biases['bc3']) conv3 = maxpool2d(conv3, k=2) conv4 = conv2d(conv3, weights['wc4'], biases['bc4']) conv4 = maxpool2d(conv4, k=2) # Fully connected layer # Reshape conv2 output to fit fully connected layer input fc1 = tf.reshape(conv4, [-1, weights['wd1'].get_shape().as_list()[0]]) fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1']) fc1 = tf.nn.relu(fc1) # Output, class prediction out = tf.add(tf.matmul(fc1, weights['out']), biases['out']) return out #Define Loss and Activation functions pred = conv_net(x, weights, biases) print(pred.shape) cost = tf.reduce_mean(tf.square(y - pred)) #cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y)) optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) #Train and Test the Model with tf.Session() as sess: sess.run(init) train_loss = [] test_loss = [] train_accuracy = [] test_accuracy = [] summary_writer = tf.summary.FileWriter('./Output', sess.graph) for i in range(training_iters): for batch in range(len(train_np)//batch_size): batch_x = train_np[batch*batch_size:min((batch+1)*batch_size,len(train_np))] batch_y = train_np_y[batch*batch_size:min((batch+1)*batch_size,len(train_np_y))] # Run optimization op and Calculate batch loss and accuracy opt = sess.run(optimizer, feed_dict={x: batch_x, y: batch_y}) loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x, y: batch_y}) print("Iter " + str(i) + ", Loss= " + \ "{:.6f}".format(loss) + ", Training Accuracy= " + \ "{:.5f}".format(acc)) print("Optimization Completed") # Calculate accuracy for all test data test_acc,valid_loss = sess.run([accuracy,cost], feed_dict={x: valid_np,y : valid_np_y}) train_loss.append(loss) test_loss.append(valid_loss) train_accuracy.append(acc) test_accuracy.append(test_acc) print("Testing Accuracy:","{:.5f}".format(test_acc)) summary_writer.close() predictions = pred.eval(feed_dict = {x:valid_np}) What I have observed is if I change the cost function to sigmoid or softmax cross entropy, the predictions are negative and huge numbers (to the order of 10^12) the response value range is between 0.05 - 7900 so I'm pretty sure this is wrong prediction. When I change the cost function to MSE the predictions are comparable to response variable. However sometimes all the input data points have exactly identical predictions. If somebody understands what might be happening please explain. I really appreciate any input. Note : This question is meant to understand the outcome of neural network conceptually and not looking for suggestions on improving the model.
