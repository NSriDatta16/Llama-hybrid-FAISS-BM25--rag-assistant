[site]: crossvalidated
[post_id]: 406695
[parent_id]: 
[tags]: 
Question about the gradient of weight normalization

In Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks , they define the weight vector as $$ \mathbf w={g\over\Vert\mathbf v\Vert}\mathbf v $$ Then they differentiate through it to obtain the gradient of a loss function $L$ w.r.t. $\mathbf v$ , obtaining $$ \nabla_{\mathbf v}L={g\over \Vert\mathbf v\Vert}\nabla_\mathbf wL-{g\nabla_gL\over \Vert\mathbf v\Vert^2}\mathbf v $$ I don't understand how they obtain the second term, ${g\nabla_gL\over \Vert\mathbf v\Vert^2}\mathbf v$ . How can I compute this term?
