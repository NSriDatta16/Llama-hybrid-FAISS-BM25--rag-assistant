[site]: datascience
[post_id]: 118376
[parent_id]: 118371
[tags]: 
There is a major misunderstanding in the general public and especially in the news about what ChatGPT really does. This misunderstanding is not really new, there has been confusion about ML techniques for a long time. Imho a large part of the issue comes from the term "Artificial Intelligence", which covers everything in this area supposed to represent some form of intelligence. Clearly most people would like to believe that there is some real intelligence at play (sci-fi, etc.). The fact that many companies or institutions have an interest in keeping the ambiguity alive (e.g. "smart cars", "smart home", etc.) and that some experts support it greatly contributes to the confusion. But as far as I know, from the point of view of ML experts, a system like ChatGPT is more like a highly complex calculator than an intelligence. Its calculations cannot be simulated manually (well technically they can, but it would take a lot of people and a very long time) so it gives the impression of something smart. It's also worth noting that the evolution of ML has been to give less and less direct human supervision to the models, i.e. they have more autonomy in which way and what they learn. What this kind of system is trained to do is to simulate giving an answer. The more the answer looks real, the better. It is not trained to give a correct answer or give any importance to the truth, because this is not easily measurable. The Chinese Room argument explains why such a system does not comprehend anything of what it says, it just applies a lot of complex rules/calculations based on its training. One could say that the goal of a system like ChatGPT is to create the illusion that there is some intelligence behind it, like the goal of a magician is to make you believe whatever they are showing. As a consequence, it is well known that ChatGPT answers are meaningless in general: see for instance this article or the ban on StackExchange .
