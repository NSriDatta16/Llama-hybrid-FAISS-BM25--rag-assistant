[site]: crossvalidated
[post_id]: 603464
[parent_id]: 
[tags]: 
Min_gain_split: Random Forest vs Gradient Boosting

In Chapter 8 of Intro to Statistical Learning , the authors state with regard to pruning a random forest that: "One possible alternative to the process described above is to build the tree only so long as the decrease in the RSS due to each split exceeds some (high) threshold. This strategy will result in smaller trees, but is too short-sighted since a seemingly worthless split early on in the tree might be followed by a very good splitâ€”that is, a split that leads to a large reduction in RSS later on." This definitely makes sense in the context of a random forest where we are unconcerned with the variance of any given tree, but I am wondering if the same logic should be applied to gradient boosted trees. In my case, I'm using LightGBM for a learning task and I am deciding whether to tune num_leaves or min_gain_split. I don't want to tune both, as this will increase the size of the hyperparameter space to search over (with limited benefit, I presume).
