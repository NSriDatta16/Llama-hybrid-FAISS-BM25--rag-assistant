[site]: datascience
[post_id]: 56529
[parent_id]: 56514
[tags]: 
Simple definitional question: In the context of machine learning, is the error of a model always the difference of predictions ()=Ì‚ and targets ? Or are there also other definitions of error? No, I don't think the word "error" can be considered as a technical term which always follows this specific definition. The word "error" is frequently used in the context of evaluation: in a broad sense, an error happens every time a ML system predicts something different than the true value. It's obviously the same concept as in the definition, but it can be applied to non numerical values for which the mathematical difference doesn't make sense. For example in the context of statistical testing and in classification it's common to talk about "type I errors" and "type II errors" for instances misclassified as false positive and false positive respectively.
