[site]: crossvalidated
[post_id]: 229861
[parent_id]: 
[tags]: 
LSTM Output Clarification

I'm attempting to work through an LSTM network and have a question regarding the final output. A LSTM appears to function similarly to an RNN with a few additional processes. The question that I have is pertains to the bottom two images. The first image is of a standard RNN and the second is of an LSTM. In the first image each state $h^{(t)}$ get outputted up the layer $o^{(t)}$ before going through to the loss function $L^{(t)}$. In addition $h^{(t)}$ gets sent to the the next time step $t+1$. If I look at the image for a LSTM network I do not see $h^{(t)}$ being outputted to some other layer before being going through to the loss function and was wondering if that is the case or if that was simply omitted from the image?
