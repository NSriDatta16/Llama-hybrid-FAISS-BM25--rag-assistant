[site]: crossvalidated
[post_id]: 638137
[parent_id]: 
[tags]: 
Given N observations - Bayesian Posterior for Unknown Variance of a Normal Distribution with a Known Mean?

So, starting from no information besides N trials from a Gaussian with $\mu = 0$ , I'd like to know the best Bayesian posterior for the unknown variance, $\sigma^2$ . My approach so far as been to assume a uniform prior for $\tau = \frac{1}{\sigma}$ , i.e. the scaling factor for the standard Gaussian. In addition to being more intuitive (to me) , this is also necessary, since attempting to use a uniform prior for $\sigma$ or $\sigma^2$ would result in a divergent integral. Then for N observations, this results in a posterior distribution of: $$\Pr(\tau|x_1,x_2,...,x_n) = \frac{S^{n+1}\tau^n}{\Gamma(\frac{n+1}{2})2^{\frac{n-1}{2}}}\exp(-\frac{\tau^2}{2}S^2)$$ where $S^2 = x_1^2+x_2^2+\cdots+x_n^2$ , i.e. the sum of squares of observations. If we transform variables to the variance, then we have: $$\Pr(\sigma^2|x_1,x_2,...,x_n) = \frac{(\frac{S^2}{2})^{\frac{n+1}{2}}}{\Gamma(\frac{n+1}{2})(\sigma^2)^{\frac{n+3}{2}}}\exp(-\frac{S^2}{2}\frac{1}{\sigma^2}) = InvGamma({\sigma^2;\alpha = \frac{n + 1}{2},\beta = \frac{S^2}{2})}.$$ Thankfully, this is none other than the Inverse Gamma Distribution with parameters $\alpha = \frac{n + 1}{2}, \beta = \frac{S^2}{2}$ , which comports with this rather cryptic statement on this wikipedia page https://en.wikipedia.org/wiki/Inverse-gamma_distribution : "Perhaps the chief use of the inverse gamma distribution is in Bayesian statistics, where the distribution arises as the marginal posterior distribution for the unknown variance of a normal distribution, if an uninformative prior is used" So my question: Well, this is all well and good, but I haven't found anything resembling this analysis anywhere else online, which is worrying. Moreover, the mean of the posterior distribution I derived is $\frac{S^2}{n-1}$ , and the mode is $\frac{S^2}{n+3}$ . I'd sort of suspect at least one of those would be $\frac{S^2}{n}$ since we know the mean, and we therefore don't lose a degree of freedom when estimating the variance.... Help point me in the right direction? I'm confused.
