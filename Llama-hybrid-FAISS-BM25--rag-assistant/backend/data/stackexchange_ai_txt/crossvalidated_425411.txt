[site]: crossvalidated
[post_id]: 425411
[parent_id]: 
[tags]: 
Is two group comparison test combinig rarefaction and permutation usefull and correct?

I wish to know if the analytical procedure using R environment described below is statistically coherent to enable interpretations and conclusions. I looking for evaluations and opinions about the bootstrap method presented (answers like: "is correct because...", or " is wrong because of"). For sake of sharing correctly the concept, the analytic process is divided on two main steps. At first step, the goal is calculate differences between two groups using all available information but controlling for differences in number of observations inside each group. Note that the example described below has one group bigger then the other in terms of observations: data.1 The difference is calculated subtracting the averages of many simulated averages from each compared group. Each group values considered to calculate these averages are free to be re sampled, which is called sampling with reposition. The dataset is then analyzed using rarefaction* during the averages calculations. This approach reduce possible undesirable effects of different groups sizes. To achieve this, the bigger group (data.2-last eleven values from dataset) is sampled using the same number of observations as in the smaller group (data.1-first six values from dataset). After these sampling routines, averages are obtained. Below, a function to do what is described above, which also produces the difference statistic required latter: dif.sim.obs.means The above function is applied internal to boot function from boot package: library(boot) DIFF.OBS.RAR mean(DIFF.OBS.RAR$t)# average of all 4999 simulated values The result is a number around ~16.91655, which could be a little distinct if calculated again since each sampling return different values combinations from original dataset. This value is a kind of expected difference between both groups, obtained reducing sample size effect due to rarefaction. Also, this is made unconditionally**, since the reposition turns these estimates not limited to observed data. This is interesting because mimics the real probability that values observed could occur more times than registered on this dataset, which is very probably if more real sampling effort could be applied. Step two investigates if the difference of ~16.91655 is statistically significant. The approach also include rarefaction on bigger group and reposition, but allows that values from one group occur in the other before the mean difference be calculated, analogue a Monte Carlo t test***. If only in very few times the simulations return 16.91655 or even more extreme values, this implies that the difference hardly could be obtained by chance, hence, this a significant difference between groups. The description above is coded below: permdif.simeans PERMDIFF.RAR Now, a p value is calculated, including a correction**** relevant on this analysis. This correction***** relates to possible results that do not occurred during the analysis, since not all probable values combinations were represented on simulations. (length(which(PERMDIFF.RAR$t >= mean(DIFF.OBS.RAR$t)))+1)/5000 The p value around 0.0194 shows a significative (assuming a 0,05 alfa) difference between groups. Even though I aware of many drawbacks****** of this kind of approach, I really appreciate if anyone show me any overlooked problems/alternatives with the analytical method presented here or in the code. REFERENCES *Look for "candy-jar sampling" in Gotelli & Ellison, 2013-pg 453. This method is very used in biodiversity analysis - GOTELLI, Nicholas J. et al. A primer of ecology. Sunderland, MA: Sinauer Associates, 2013. **Another concept from diversity analyses: rarefaction curves that confidence intervals vanishes at end are made conditionaly to the dataset: more observations values are not considered. In the other hand, if confidence intervals are wider at end, these are made inconditionaly to the data set: more observations values are included by reposition. Although is a concept related to confidence intervals, maybe the principle is also valid and usefull here. https://stevencarlislewalker.wordpress.com/2013/02/25/a-common-common-common-mistake-in-rarefaction-analysis/ *** pg 109 - Gotelli & Ellison - GOTELLI, Nicholas J. et al. A primer of ecology. Sunderland, MA: Sinauer Associates, 2013. ****see page 131 from: ZIEFFLER, Andrew S.; HARRING, Jeffrey R.; LONG, Jeffrey D. Comparing groups: Randomization and bootstrap methods using R. John Wiley & Sons, 2011. *****[edited later] I found that using this fix is debatable (see: https://www.cell.com/action/showPdf?pii=S0002-9297%2807%2960560-6 ). In practical terms, the final result does not change too much. Because of this, I kept the the correction. Therefore, I suggest that any evaluations of the procedure focus more on the early parts of the analysis rather than on correction ****** Problematic issues such as: if sampling do not reflect the real population, the simulations will not; the problem with small number of observations; dependence between values...etc
