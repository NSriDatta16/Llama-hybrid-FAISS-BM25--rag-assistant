[site]: crossvalidated
[post_id]: 603793
[parent_id]: 603791
[tags]: 
I would recommend against this. First off, if your model were to be used in the future, you would really need two models since the decision tree is used as an input to the logistic regression. Second, decision trees are highly variable, so there is no guarantee that a patient would fall into the same leaf were you to add more data, or were they to be part of a different dataset. Decision trees are highly unstable, which is why we typically use techniques like bagging to reduce their variance. Third, if one of the leafs has all 1s or 0s, then the standard error for the coefficient for that leaf is going to be absurdly high. See the example below library(tidyverse) N % group_by(leafs) %>% summarise( y = sum(y), n = n() ) fit If you're not doing inference, then this isn't a problem. But if you are doing prediction, then there is some literature you should be abiding by, like TRIPOD which instead recommends you use splines to add non-linearity to the logistic regression. All in all I would recommend against this approach. I don't think any potential benefits are worth the very real thread of these points I've made.
