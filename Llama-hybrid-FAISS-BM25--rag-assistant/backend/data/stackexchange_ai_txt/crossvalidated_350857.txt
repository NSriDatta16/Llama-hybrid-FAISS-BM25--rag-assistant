[site]: crossvalidated
[post_id]: 350857
[parent_id]: 350773
[tags]: 
No, double Q-learning is not redundant, since that is not the main motivation for double Q-learning. The abstract of the paper says In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. And then We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games. So a side-effect of DDQN is to mitigate the "moving target" problem, which the target network also solves. However, that is not the main point. The main point is to reduce over-optimism
