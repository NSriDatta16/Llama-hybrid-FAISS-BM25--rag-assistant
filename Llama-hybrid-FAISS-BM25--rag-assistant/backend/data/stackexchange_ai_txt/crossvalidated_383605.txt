[site]: crossvalidated
[post_id]: 383605
[parent_id]: 
[tags]: 
Did Statistics.com publish the wrong answer?

Statistics.com published a problem of the week: The rate of residential insurance fraud is 10% (one out of ten claims is fraudulent). A consultant has proposed a machine learning system to review claims and classify them as fraud or no-fraud. The system is 90% effective in detecting the fraudulent claims, but only 80% effective in correctly classifying the non-fraud claims (it mistakenly labels one in five as “fraud”). If the system classifies a claim as fraudulent, what is the probability that it really is fraudulent? https://www.statistics.com/news/231/192/Conditional-Probability/?showtemplate=true My peer and I both came up with the same answer independently and it doesn't match the published solution. Our solution: (.9*.1)/((.9*.1)+(.2*.9))=1/3 Their solution: This is a problem in conditional probability. (It’s also a Bayesian problem, but applying the formula in Bayes Rule only helps to obscure what’s going on.) Consider 100 claims. 10 will be fraudulent, and the system will correctly label 9 of them as “fraud.” 90 claims will be OK, but the system will incorrectly classify 72 (80%) as “fraud.” So a total of 81 claims have been labeled as fraudulent, but only 9 of them, 11%, are actually fraudulent. Who was right
