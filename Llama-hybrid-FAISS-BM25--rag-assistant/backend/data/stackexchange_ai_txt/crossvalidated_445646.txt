[site]: crossvalidated
[post_id]: 445646
[parent_id]: 386535
[tags]: 
Hard-attention is a good biological inductive bias to try to build an attentive mechanism into neural networks. Unfortunately, it would necessarily map into the "arg max" function, to return the position of the input with the biggest attention score. The "argmax" function is not differentiable, as you point out in your question. However, the "max" function used in ReLU and MaxPool is, in fact, piecewise-differentiable. $$\max(x,y) = {x + y + |x - y|\over 2} = \begin{cases}{x, x \gt y\\ y, x \lt y}\end{cases}$$
