[site]: crossvalidated
[post_id]: 507910
[parent_id]: 
[tags]: 
How to quantify a bias with a score value (e.g. RMSE)

In machine learning, when are using some validation methods (e.g. CV or Bootstrap methods) to evaluate the performance of the machine learning algorithms. Apart from the prediction accuracy, other things to evaluate the bias/variance of the machine learning algorithms. For ex, I read somewhere that with cross validation methods, the bias of the machine learning algorithm could be reduced but the variance goes high. My question is we measure the variance by calculating how the performance estimates varies but how can we measure the bias of a machine learning algorithm? I read that the bias is the difference between the predicted value and the ground truth, but how can we measure it from a score (e.g. RMSE value)? Plz if someone could explain in simple way as I have read dozen of articles about that and could not grasp the concept. One confusion in my mind: If bias is the difference between the predicted and ground truth (i.e. actual value), then what is the difference between it and RMSE value because RMSE (or other metrics like AUC) is also the difference of the predicted and actual values? I am sorry if the question seems very novice. Thank you
