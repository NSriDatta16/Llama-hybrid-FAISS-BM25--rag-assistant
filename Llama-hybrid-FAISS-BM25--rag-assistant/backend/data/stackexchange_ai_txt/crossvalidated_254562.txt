[site]: crossvalidated
[post_id]: 254562
[parent_id]: 
[tags]: 
Using fine-grained classes in training data to help predict coarse-grained classes

Let's say I want to build a boolean classifier to classify an animal as 'dog' vs 'cat' based on some of its attributes. I can do the standard machine learning approach of building a training set of instances with a known label, training a classifier, and using that for prediction. All well and good. But now suppose that I have additional information about each training instance: not only do I know whether it is a cat or dog, I also know the breed of each instance (e.g., dog - poodle; cat - Persian). In other words, I have finer-grained labels than needed. Is there a way to make good use of this extra information about the training instances, to build a more accurate classifier? Obviously, I could ignore the information about the breeds of the training instances and just train a boolean classifier as usual. But this is throwing away information, which seems like it might be wasteful. Alternatively, I could train a classifier that predicts both species (dog vs cat) and breed, based on this training set. Then, when I see any new animal, I could apply the classifier, ignore the breed part of its output, and use just the species part. This seems like a natural strategy, though it's a bit trivial. Are there any other, more sophisticated strategies I'm overlooking for how to make use of the additional labels for the training instances?
