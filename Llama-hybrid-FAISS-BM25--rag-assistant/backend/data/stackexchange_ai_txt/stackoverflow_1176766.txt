[site]: stackoverflow
[post_id]: 1176766
[parent_id]: 1176589
[tags]: 
A couple of important design recommendations for such a task: Don't read the entire file into memory at once. Use a file pointer and read in reasonable chunks (say, a few kilobytes .. depends on the average record-size). Then process each record and ditch out the buffer. I'm not sure from your description whether you're already doing this or not. If your mysql storage type supports transactions (the table must be InnoDB), you can use them for optimisations. Start a transaction and process f.ex. 100k rows, then flush by committing the transaction and opening a new one. This works because MySql will only update the index once, instead of for each row. Another option is to use bulk insert. If your database isn't local (eg. you connect over network), this can give a boost. I think (not sure though) it also gives the same benefits as transactions - possibly even for MyIsam tables. Finally, if nothing else works, you can remove php from the equation and use LOAD DATA INFILE . You may have to pre-process the file first, using php or some other text-processing language (awk or sed have very good performance profiles)
