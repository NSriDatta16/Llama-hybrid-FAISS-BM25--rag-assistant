[site]: crossvalidated
[post_id]: 107533
[parent_id]: 
[tags]: 
How to use SVD for dimensionality reduction to reduce the number of columns (features) of the data matrix?

My original data has many more columns (features) than rows (users). I am trying to reduce the features of my SVD (I need all of the rows). I found one method of doing so in a book called "Machine Learning in Action" but I don't think it will work for the data I am using. The method is as follows. Define SVD as $$A = USV^\top.$$ Set an optimization threshold (i.e., 90%). Calculate the total sum of the squares of the diagonal $S$ matrix. Calculate how many $S$ values it takes to reach 90% of the total sum of squares. So if that turns out to be 100 $S$ values, then I would take the first 100 columns of the $U$ matrix, first 100 rows of the $V^\top$ matrix, and a $100\times 100$ square matrix out of the $S$ matrix. I would then calculate $A = USV^\top$ using the reduced matrices. However, this method does not target the columns of my original data, since the dimensions of the resulting $A$ matrix are the same as before. How would I target the columns of my original matrix?
