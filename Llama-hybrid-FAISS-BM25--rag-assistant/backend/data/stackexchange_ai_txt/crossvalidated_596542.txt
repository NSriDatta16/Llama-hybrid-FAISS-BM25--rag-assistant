[site]: crossvalidated
[post_id]: 596542
[parent_id]: 596264
[tags]: 
The best solution would probably be to re-OCR the documents using a better OCR, ideally with a language model that is specifically tuned for the domain of the documents. Otherwise, you can use a spell-checker with an additional custom vocabulary, there are several Python packages for that, e.g., pyspelling has an interface for both Hunspell and Aspell. The advantage of existing spell-checkers is that they have optimized algorithms for retrieving correct similar words. The disadvantage is that they treat all edits equally. To get around this, you can get more spell-checker suggestions than you would normally display to a user and re-score them. The simplest way is designing a weighted edit distance where errors that are frequent in your data would get a low cost (e.g., substituting s and 5 should be a cheap operation). There are also several Python packages for that, weighted-levenshtein should be pretty fast. If you want a really fancy solution, you can re-score the suggestions using a language model.
