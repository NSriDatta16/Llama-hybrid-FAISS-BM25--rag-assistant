[site]: stackoverflow
[post_id]: 3481036
[parent_id]: 3480540
[tags]: 
if you want to block crawlers from accessing certain links, create a Robots.txt file in your root directory, with something like: User-agent: * Disallow: / // blocks the default route / page Disallow: /MyPage.aspx check http://en.wikipedia.org/wiki/Robots_exclusion_standard & http://www.google.com/#hl=en&q=robots.txt
