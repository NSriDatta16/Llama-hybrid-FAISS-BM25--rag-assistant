[site]: crossvalidated
[post_id]: 148229
[parent_id]: 148227
[tags]: 
This is a big issue in some areas of machine learning. I'm not as familiar with it as I'd like, but I think these should get you started. Dimensionality Reduction by Learning an Invariant Mapping (DrLIM) seems to work very well on some data sets. Neighborhood components analysis is a very nice linear algorithm, and nonlinear versions have been developed as well. There's a whole literature that deals with this issue from the perspective of "learning a kernel". I don't know much about it, but this paper is highly cited. Given that your data is so high-dimensional (and probably sparse?), you might not need anything too nonlinear. Maybe neighborhood components analysis is the best place to start? It's closest to the idea of a weighted $L_2$ norm, like you suggested in your question.
