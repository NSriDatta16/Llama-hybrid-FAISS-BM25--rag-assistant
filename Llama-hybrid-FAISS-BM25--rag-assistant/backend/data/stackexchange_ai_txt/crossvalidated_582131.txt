[site]: crossvalidated
[post_id]: 582131
[parent_id]: 580148
[tags]: 
This is an extended comment, not an answer. You want to compute the AIC for a linear and a nonlinear model in order two compare the two models in terms of their predictive performance. The Aikake information criterion (AIC) estimates how well a model generalizes to new data when the model is well-specified. This assumption is required to derive the number of parameters penalty from the Fisher information matrix [1]. In other words, the AIC is not model-robust [2]. See Figure 7.4 in [3] for a (logistic regression) example that shows how the AIC agrees well with test-sample error when the model is well-specified but not when it is mis-specified (in that case over-parameterized). So not surprisingly, both the linear and non-linear model should be reasonable fit to the data in order to make a meaningful comparison. Let's take a look at the models you want to compare. Your linear model is a line without an intercept; it's equivalent to a no-intercept linear regression. Your nonlinear model is the logistic function; it's not equivalent to a logistic regression because, to compute the AIC, you implicitly assume that the errors are iid Normal with mean 0 and variance $\sigma^2$ . Aside about leaving out the intercept. This is usually not a good idea but if you stare at your two models carefully (or even better, plot them), you'll notice that leaving out the intercept might be a particularly unfortunate choice in this case. The linear model is a line with slope $a_1$ which passes through the origin (0,0). The logistic curve doesn't pass through the origin but through (0, $a_2$ /2). In fact it approaches 0 as x → -∞ and $a_2$ as x → +∞. The normality assumption may be tricky to justify if the range of y is bounded. Even then you have to show that both a linear and a logistic model are well-specified. The logistic curve is approximately linear only around x = 0. Over a wide range for x it may be hard to argue that both a straight line and a bounded curve are well-specified models for the same observations. I illustrate this by generating data from a logistic model, adding noise and then plotting the best fitting line (in red) and logistic curve (in blue). For fun, I also add the no-intercept line (in gray) and a restricted cubic spline with 4 knots (in green). You could consider making a similar plot for your real data. [1] C. R. Shalizi. Advanced Data Analysis from an Elementary Point of View (2021). Available online . [2] G. Claeskens and N. L. Hjort. Model Selection and Model Averaging . Cambridge University Press (2008) [3] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning: Data Mining, Inference, and Prediction (2008) model parameters AIC (left) AIC (right) straight line 3 -146.6279 -182.7456 logistic curve 3 -181.9461 -182.2991 cubic spline 5 -181.0472 -182.3560 The restricted cubic spline, which allows for nonlinearity in a flexible way, has the same predictive performance (in term of the AIC) as the logistic curve even though the data is simulated from the logistic model and the spline has two extra parameters. The line without intercept doesn't fit at all in either case while the line with an intercept is the best fit for the data in the right panel.
