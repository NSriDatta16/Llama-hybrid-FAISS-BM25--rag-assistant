[site]: crossvalidated
[post_id]: 504482
[parent_id]: 504400
[tags]: 
What is going wrong with ARIMA? The reason is, as @chris-haug suggested, that the large distance between the initial value and the mean of the process at the beginning of the series make it appear unlikely (in the sense of the likelihood function) that the series is stationary (in the sense of all the values being drawn from the same conditional distribution). It does not have to do with whether the model is parameterized with the constant in mean form or in intercept form (see the end of this answer for more details on that). Technically, the reason is that ARIMA model assumes that, in stationary models, the first observation is drawn from the unconditional distribution implied by the parameters. The unconditional distribution implied by your generating parameters is $N(c / (1 - \phi), \sigma^2 / (1 - \phi^2))$ . In your model, $c = 1, \phi = 0.5,$ and $\sigma^2 = 0.0001$ , so the unconditional distribution is $N(2, 0.00013)$ . Recall that the rule of thumb for Gaussian distributions is that 99.7% of the mass of the distribution falls within 3 standard deviations of the mean. Here, the standard deviation is $\sqrt{0.00013} = 0.0115$ so here that means that 99.7% of the mass of this distribution lies in the range $[2 - 0.0345, 2 + 0.0345] = [1.9655, 2.0345]$ . In summary, it is incredibly unlikely that the value 0 would arise from this unconditional distribution. So, when the model tries out your parameters, it finds that they are extremely unlikely to fit the data. We can check this using the SARIMAX model: model = sm.tsa.SARIMAX(S, order=(1,0,0), trend='c') print(model.loglikeobs([1., 0.5, 0.01**2])) yields: [-14996.45760938 3.5751061 2.9770201 2.65990702 ... The first observation, S[0] = 0 is just incredibly unlikely. Now, you might think that given our discussion above, the second observation, S[1] = 1.004714 is still extremely unlikely, so why is its likelihood so much higher? The answer is that conditional on the first value being zero , then observing the next value to be 1.004714 is actually right in line with the parameters of the model. This is because, given these parameters, the model would have predicted $y_2 = 1 + 0.5 y_1 = 1$ . Why is OLS doing better? This leads directly into the answer of why running OLS does a better job. That's because OLS works exactly by conditioning on the first observation . So OLS is not evaluating how good or bad the first observation is at all, it's simply saying: conditional on the first observation's value, what are the parameters that minimize the sum of squared errors for the remaining observations . Can we understand what ARIMA is doing here? Basically, it appears to the model that the first few observations were not drawn from the same conditional distribution as the remaining observations. Time series that have this property are called non-stationary, and one characteristic of them is that they have a unit root - which is to say, they have $\phi = 1$ . Fitting the model we constructed earlier, we can see that the model is trying to fit the data by estimating non-stationary parameters (actually, these models are constrained to be stationary, so instead it is estimating parameters that are almost non-stationary): results = model.fit() print(results.summary()) yields: SARIMAX Results ============================================================================== Dep. Variable: 0 No. Observations: 1000 Model: SARIMAX(1, 0, 0) Log Likelihood 1827.942 Date: Mon, 11 Jan 2021 AIC -3649.884 Time: 13:44:30 BIC -3635.161 Sample: 0 HQIC -3644.288 - 1000 Covariance Type: opg ============================================================================== coef std err z P>|z| [0.025 0.975] ------------------------------------------------------------------------------ intercept 0.0354 0.003 12.908 0.000 0.030 0.041 ar.L1 0.9810 0.003 338.304 0.000 0.975 0.987 sigma2 0.0015 9.04e-06 166.245 0.000 0.001 0.002 =================================================================================== Ljung-Box (L1) (Q): 25.71 Jarque-Bera (JB): 7100021.57 Prob(Q): 0.00 Prob(JB): 0.00 Heteroskedasticity (H): 0.03 Skew: 16.69 Prob(H) (two-sided): 0.00 Kurtosis: 414.44 =================================================================================== So it has estimated $\phi$ to be close to 1, as expected. Something that's interesting to notice is that even though the estimated intercept isn't what you were expecting, the model has still done a pretty good job estimating the mean of the series, since $\mu = c / (1 - \phi) = 0.0354 / (1 - 0.9810) = 1.86$ . The model is trying to do the best it can, given that the data appears to be approximately non-stationary. How to move forward with ARIMA In your case, the answer is just to initialize the simulated data closer to mean, just as @Chris Haug suggested. More generally, if your dataset was not simulated, you would likely think of the first few observations as outliers. Then, depending on the context, you would either want to model them (for example by using a fat-tailed distribution rather than a Gaussian distribution) or, if they weren't central to the analysis, you might drop them. Here, if we use your code but drop the first 6 observations, we get results that are in line with what you were expecting: model_subset = sm.tsa.SARIMAX(S[6:], order=(1,0,0), trend='c') results_subset = model_subset.fit() print(results_subset.summary()) yields: SARIMAX Results ============================================================================== Dep. Variable: 0 No. Observations: 994 Model: SARIMAX(1, 0, 0) Log Likelihood 3193.052 Date: Mon, 11 Jan 2021 AIC -6380.103 Time: 14:31:47 BIC -6365.398 Sample: 0 HQIC -6374.512 - 994 Covariance Type: opg ============================================================================== coef std err z P>|z| [0.025 0.975] ------------------------------------------------------------------------------ intercept 1.1044 0.055 19.952 0.000 0.996 1.213 ar.L1 0.4479 0.028 16.180 0.000 0.394 0.502 sigma2 9.48e-05 4.23e-06 22.417 0.000 8.65e-05 0.000 =================================================================================== Ljung-Box (L1) (Q): 0.02 Jarque-Bera (JB): 0.25 Prob(Q): 0.89 Prob(JB): 0.88 Heteroskedasticity (H): 1.04 Skew: -0.04 Prob(H) (two-sided): 0.72 Kurtosis: 3.02 =================================================================================== What about regression with ARIMA errors? There is very little practical difference between using regression with ARIMA errors form (i.e. the constant term as a mean value) versus an ARIMA model with the constant as an intercept. This post by Rob Hyndman notes that "there is not much to choose between the models in terms of forecasting ability, but the additional ease of interpretation in [regression with ARMA errors] makes it attractive.". For example, we can run regression with AR errors on the stationary subset of the data as follows: model_subset_reg = sm.tsa.SARIMAX(S[6:], order=(1,0,0), exog=np.ones_like(S[6:])) results_subset_reg = model_subset_reg.fit() print(results_subset_reg.summary()) which yields: SARIMAX Results ============================================================================== Dep. Variable: 0 No. Observations: 994 Model: SARIMAX(1, 0, 0) Log Likelihood 3193.054 Date: Mon, 11 Jan 2021 AIC -6380.109 Time: 14:35:47 BIC -6365.403 Sample: 0 HQIC -6374.518 - 994 Covariance Type: opg ============================================================================== coef std err z P>|z| [0.025 0.975] ------------------------------------------------------------------------------ const 2.0002 0.001 3576.525 0.000 1.999 2.001 ar.L1 0.4500 0.028 16.258 0.000 0.396 0.504 sigma2 9.48e-05 4.23e-06 22.417 0.000 8.65e-05 0.000 =================================================================================== Ljung-Box (L1) (Q): 0.04 Jarque-Bera (JB): 0.25 Prob(Q): 0.83 Prob(JB): 0.88 Heteroskedasticity (H): 1.04 Skew: -0.04 Prob(H) (two-sided): 0.72 Kurtosis: 3.02 =================================================================================== This is giving basically the same results as before, and notice that we can recover the intercept directly from the formula $c = (1 - \phi) \mu = (1 - 0.4500) * 2.0002 = 1.10011$ , which is essentially identical to the result we got when estimating the intercept directly.
