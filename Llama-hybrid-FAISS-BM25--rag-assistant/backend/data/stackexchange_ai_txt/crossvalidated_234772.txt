[site]: crossvalidated
[post_id]: 234772
[parent_id]: 234694
[tags]: 
Can all neural network having directed acyclic graph (DAG) topology be trained by back propagation methods? I mean by the back propagation methods like Stochastic gradient decent, AdaGrad, Adam, etc. The methods you mention are gradient-based, and subsequently won't work if one activation function used by the artificial neurons isn't differentiable. However, they are some ways around, e.g. using reinforcement learning .
