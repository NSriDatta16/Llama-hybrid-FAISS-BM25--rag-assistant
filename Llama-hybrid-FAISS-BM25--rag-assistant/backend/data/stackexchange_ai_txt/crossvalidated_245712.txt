[site]: crossvalidated
[post_id]: 245712
[parent_id]: 
[tags]: 
Intuition as to why estimates of a covariance matrix are numerically unstable

It is well known that estimating the covariance matrix by the ML estimator (sample covariance) can be very numerically unstable (in high dimensions), which is why it is preferable to do PCA with the SVD, for example. But I haven't been able to find an intuitive explanation of why this is the case. For matrix inversions it is clearer where the numerical instability arises, but a covariance matrix for (centered and standardized data) is just the seemingly innocuous product: XX' EDIT: here is a reference: Regularized estimation of large covariance matrices
