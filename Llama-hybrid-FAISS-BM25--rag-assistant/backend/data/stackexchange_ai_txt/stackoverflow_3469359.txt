[site]: stackoverflow
[post_id]: 3469359
[parent_id]: 3466714
[tags]: 
Ignoring the debate in the question's comments, a bunch of arbitrary productivity-ish metrics you could measure... lines of code written user stories/tasks completed bugs fixed tests written tests passing first time bugs found code churn vs new code (i.e. "right first time" vs "rewritten repeatedly") %age of time in IDE vs debugging %age of time in IDE vs non-work applications code quality (using another similarly arbitrary measure like FxCop compliance or cyclic complexity) code performance (against some arbitrary or customer-specified benchmark) The best metrics tend to be combinations - say, "average of bugs found per line of code written" - rather than a single measure. Still, these are all subjective and innacurate. I'd suggest the best thing to do is decide what your goal is when you're programming. Is it to produce high-quality code, or super-performant realtime code, or mission-critical-must-be-bug-free code, or do you just need to ship something that works in the shortest time? Until you've defined "productive", it's hard to suggest what would be a meaningful measurement.
