[site]: crossvalidated
[post_id]: 239048
[parent_id]: 
[tags]: 
Bayesian model for estimation of a total by imperfect counts

Let's say we have the following setup: There are $n$ participants doing a simple task: They watch a video. At random times, a symbol is shown somewhere on the screen. The total number of times the symbol is displayed, say $N$, is fixed and the same for all participants. At the end, the participants have to provide their guesses of the total number of times they saw the symbol during the video. Let's call this variable $N_{obs,i}$ with $i=1,2,\ldots, n$. We assume that each instance of the symbol has the same detection probability. Also, the information whether or not a single symbol was detected is not available, just the total from each participant. The detection probabilities differ between participants. So some subjects might count the symbols corrects while other might under- or overestimate the total number of symbols. Example Let's say $N=29$. 20 subjects participate in the study. The following $N_{obs}$ are recorded: $29, 28, 29, 29, 27, 30, 28, 26, 28, 28, 27, 29, 28, 28, 29, 26, 29, 29, 28, 25.$ Question How could a Bayesian model for estimation of the true total $N$ from the estimated numbers $N_{obs,i}$ look like? This question assumes that the researcher that analyzes the data doesn't know the true $N$, of course. Were I'm stuck Applying Bayes' Theorem, I know that: $$ P(N|N_{obs}) = P(N_{obs}|N)\times \frac{P(N)}{P(N_{obs})} $$ I have trouble fleshing out the details. $P(N)$ is the prior for the total number of symbols (a uniform distribution, for example). The likelihood $P(N_{obs}|N)$ is binomial, but I have trouble putting all the pieces together. Edit Thanks to @Kodiologist's great answer, I was able to implement his model in JAGS. I first randomly generated 20 observations (see above under "Example") according to the model. Then, I fitted the model. Here is the model in JAGS: model{ for(i in 1:N) { obs[i] ~ dsum(a[i], b[i]) a[i] ~ dbin(theta, Nt) b[i] ~ dpois(lambda[i]) lambda[i] ~ dt(0,1,1)T(0,) # Half-Cauchy prior } theta ~ dbeta(5, 1) # Informative prior leaning towards 1 Nt ~ dcat(p[]) } Here are the simulated data: # Simulated data set.seed(123) win.data Some initial values and the fit using JAGS: inits Here is the relevant output (cropped): mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff Nt 28.807 0.939 28.000 28.000 29.000 29.000 31.000 1.017 260 theta 0.943 0.030 0.870 0.930 0.948 0.965 0.981 1.016 310 So it seems it worked really well.
