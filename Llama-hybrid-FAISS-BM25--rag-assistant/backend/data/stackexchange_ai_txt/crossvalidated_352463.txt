[site]: crossvalidated
[post_id]: 352463
[parent_id]: 352452
[tags]: 
The Bayesian approach to probabilistic problems often uses graphical models like: $X \sim F(Y)$ $Y \sim G(Z)$ $Z \sim H$ Where one set of observations has a distribution conditional on another. This is most$\dagger$ of what you need for neural networks ( survey ). For example the model above could describe a single layer network where $Y$ was the activation in the hidden layer, $X$ the observations, and $Z$ a latent class. We'd just be 'hiding' some non-linear transformations in $F$, $G$ etc. It's worth bearing in mind that neural networks $\neq$ gradient descent. Gradient descent is just a particularly effective way of fitting the models. A Bayesian inference technique which leverages the same tools is Variational Bayes . Although there are sampling approaches (HMC, SGLD etc etc.) that can also make use of gradient information. So the difference is that in a Bayesian approach you also get some information about the distribution of your model parameters in addition to a point estimate of them. This could help, e.g. in problems where a peturbation to the parameters would cause a reclassification of an observation which would affect your decision making. $\dagger$ the question arises of how to do things like max pooling, drop out and so forth. Many of these operations have Bayesian analogues in the form of particular distributions or transformations.
