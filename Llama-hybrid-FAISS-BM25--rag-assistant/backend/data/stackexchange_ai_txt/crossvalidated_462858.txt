[site]: crossvalidated
[post_id]: 462858
[parent_id]: 
[tags]: 
Use CNN to forecast time series value accuracy problem

I would like to use a CNN to predict a value based on some historical data. The concept is easy: I have a numerical value (label) the depends on some other numerical values (features). Each set of features is linked to just one label. From a set of features I want to predict the label. This is the model schema I've imagined for the situation: model = tf.keras.Sequential() model.add(tf.keras.layers.InputLayer(input_shape=(Params.time_frame, Params.features))) #model.add(tf.keras.layers.Conv1D(kernel_size=2, filters=256, strides=1, use_bias=True, activation='relu', kernel_initializer='VarianceScaling')) #model.add(tf.keras.layers.AveragePooling1D(pool_size=2, strides=1)) model.add(tf.keras.layers.Conv1D(kernel_size=2, filters=128, strides=1, use_bias=True, activation='relu', kernel_initializer='VarianceScaling')) model.add(tf.keras.layers.AveragePooling1D(pool_size=2, strides=1)) model.add(tf.keras.layers.Conv1D(kernel_size=2, filters=64, strides=1, use_bias=True, activation='relu', kernel_initializer='VarianceScaling', padding='SAME')) model.add(tf.keras.layers.AveragePooling1D(pool_size=2, strides=1)) model.add(tf.keras.layers.Conv1D(kernel_size=2, filters=32, strides=1, use_bias=True, activation='relu', kernel_initializer='VarianceScaling', padding='SAME')) model.add(tf.keras.layers.AveragePooling1D(pool_size=2, strides=1)) model.add(tf.keras.layers.Conv1D(kernel_size=2, filters=16, strides=1, use_bias=True, activation='relu', kernel_initializer='VarianceScaling', padding='SAME')) model.add(tf.keras.layers.AveragePooling1D(pool_size=2, strides=1)) model.add(tf.keras.layers.Conv1D(kernel_size=2, filters=8, strides=1, use_bias=True, activation='relu', kernel_initializer='VarianceScaling', padding='SAME')) model.add(tf.keras.layers.AveragePooling1D(pool_size=2, strides=1)) model.add(tf.keras.layers.Conv1D(kernel_size=2, filters=4, strides=1, use_bias=True, activation='relu', kernel_initializer='VarianceScaling', padding='SAME')) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(units=128, kernel_initializer='VarianceScaling',activation='relu')) model.add(tf.keras.layers.Dense(units=128, kernel_initializer='VarianceScaling',activation='relu')) model.add(tf.keras.layers.Dense(units=1, kernel_initializer='VarianceScaling',activation=tf.keras.activations.sigmoid)) model.compile(optimizer=tf.keras.optimizers.Adam(0.0001), loss=tf.keras.losses.mean_squared_error, metrics=['accuracy']) I'm using TensorFlow with Keras . Since I'm working with a time series I've used some 1-D Conv Layer. At the end I've tried to preserve the historical data with two fully connected Dense layers. Now when I try to train the model I notice that accuracy metric never update after first two epochs: Train on 4889 samples Epoch 1/50 4889/4889 [==============================] - 1s 151us/sample - loss: 0.0416 - accuracy: 2.0454e-04 Epoch 2/50 4889/4889 [==============================] - 0s 80us/sample - loss: 0.0011 - accuracy: 4.0908e-04 Epoch 3/50 4889/4889 [==============================] - 0s 79us/sample - loss: 8.2775e-04 - accuracy: 4.0908e-04 ... Epoch 50/50 4889/4889 [==============================] - 0s 86us/sample - loss: 4.3382e-04 - accuracy: 4.0908e-04 Accuracy is also very low. So I think that there's must be a problem with the model structure. My knowledge in ML in currently basic, so I need an advice to keep the right direction.
