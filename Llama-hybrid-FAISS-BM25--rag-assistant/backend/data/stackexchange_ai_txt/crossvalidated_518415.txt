[site]: crossvalidated
[post_id]: 518415
[parent_id]: 518283
[tags]: 
I try to add something to the already existing answers that are worthwhile to read. I do think that the foundations discussion touch basic questions that are important to think about as statisticians, particularly "what do we mean by probability?" Also understanding "inferential logic" when running, say, tests, confidence intervals, or computing posteriors, is crucial. I also think it is important to know that issues go beyond the Bayesian/frequentist distinction. Particularly, there are different varieties of Bayesians, which have at least to some extent a different understanding of probabilities, mainly the radical subjectivists, so-called objective Bayesians, and people who prefer Bayesian reasoning about models and parameters, but however give models and parameters a frequentist meaning, called "falsificationist Bayes" in Sec. 5 of Gelman and Hennig (2017) , where we try to give a reasonably "neutral" overview. Furthermore, there are concepts of "aleatory probabilities" (as opposed to epistemic, i.e., formalising subjective uncertainty) that are not directly connected to long run frequencies (often referred to as "propensities"). From my point of view, a key for understanding concepts in the foundations of statistics and probabilities is that we are generally dealing with mathematical models, and reality is different, i.e., there are no "true" frequentist probabilities to be found, and neither is there any "truly rational and correct reasoning" that is identical to the Bayesian model of it. Regardless of whether we work in a Bayesian or frequentist way (and which specific variety of these), we use models in order to make mathematical reasoning available for understanding phenomena in reality, which involves abstraction, simplification, and also, in one way or another, manipulation. We are using them as tools for thinking; they are adapted to our thinking, not in the first place to any reality outside our thoughts. For this reason, all kinds of practical issues (like issues with sampling schemes, measurements, missing values, unobserved confounders etc.) are important, to some extent for improving our models, and to some extent in order to understand the limits of what we can do with whatever approach. This means that I personally believe that the different foundational approaches should not be seen as a "right" or "wrong" philosophy of statistics, and this also means that nobody needs to commit themselves to one of them only. Particularly, "epistemic" probabilities (as often but not always employed in subjectivist/"objective" Bayesian reasoning) model the uncertainty of either an individual or of science/humankind as a whole, whereas "aleatory" probabilities (as usually employed in frequentist reasoning) model the behaviour of data generating processes out there in the world. These are different, and one can well be interested in one thing regarding one research question and the other thing regarding another. I do think though that "mixing them up" in the same study is problematic. When doing probability modelling, results come as probabilities (be it p-values, confidence levels, or posterior probabilities), so a consistent meaning should be used for all probabilities that occur in the same model. I think that the statisticians should be clear about what they mean when employing probabilities in given circumstances, and mixing often is done in such a way that this is unclear (particularly I see a lot of Bayesian work in which the likelihood is apparently interpreted in a frequentist manner, referring to really existing data generating processes, where no explanation is given what the prior probabilities are meant to express, even if sometimes related in a very rough fashion to some available knowledge). However, I also think that there can be "legitimate mixing", for example in situations in which the prior distribution can be interpreted as itself being generated from a "real process" (e.g., of studies/problems of a similar kind), or when Bayesians, when applying consistently epistemic probabilities, are still interested in the frequentist properties of what they are doing, because they may find the logic of frequentist modelling ("what would happen if reality behaved according to frequentist model X") useful for learning about the implications of Bayesian methods. Sometimes priors can be introduced arguing that their introduction improves the frequentist characteristics of a method, rather than arguing that they appropriately express subjective or objective epistemic probabilities. So I think "mixing" requires a careful distinction between what the different probabilities mean in the different circumstances, clear understanding why one is used in one place and another in another place, and why they were brought together.
