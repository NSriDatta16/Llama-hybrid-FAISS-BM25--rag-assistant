[site]: datascience
[post_id]: 122674
[parent_id]: 122667
[tags]: 
Random Forest: Random Forests are often used for feature selection in a data science workflow. This is because the tree-based strategies that random forests use, rank the features based on how well they improve the purity of the node. Recursive Feature Elimination (RFE): RFE is a popular feature selection algorithm that recursively removes attributes and builds a model on those attributes that remain. It uses model accuracy to identify which attributes (and combinations of attributes) contribute the most to predict the target attribute. Lasso Regression: Lasso Regression is a popular method to reduce overfitting in a model. It does this by making the coefficients of the features that do not contribute to the model output zero, effectively eliminating them. Ridge Regression: Ridge Regression is a method that adds a degree of bias to the regression estimates, which reduces the standard errors. It might not eliminate the features like Lasso, but it will assign them lower weights. Elastic Net: Elastic Net is a middle ground between Ridge Regression and Lasso. It penalizes the absolute and square size of the regression coefficients. Boruta: Boruta is an all-relevant feature selection method. It tries to capture all the important, interesting features you might have in your dataset with respect to an outcome variable. XGBoost: XGBoost provides a built-in feature importance plot which ranks the features based on their importance in the model. LightGBM: LightGBM also provides a built-in feature importance plot which ranks the features based on their importance in the model. Variance Threshold: This method removes features whose variance doesnâ€™t meet a certain threshold. It assumes that features with a higher variance may contain more useful information. SelectKBest: This method selects features according to the k highest scores of a given scoring function. Remember, these algorithms do not remove or alter missing data points, but they may be affected by them, so it's always a good idea to handle missing data appropriately before feature selection. Caution , if you want to make a model without touching NaN values and you want a model that can handle it directly then you can go with XGBoost, CatBoost
