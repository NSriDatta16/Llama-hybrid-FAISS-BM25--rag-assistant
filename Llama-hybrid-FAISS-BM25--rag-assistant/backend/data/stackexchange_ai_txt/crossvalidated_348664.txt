[site]: crossvalidated
[post_id]: 348664
[parent_id]: 348659
[tags]: 
Unfortunately, the dichotomy you describe is invalid. ML models (almost always) define a response distribution. For example, the extremely popular gradient boosting machine library XGBoost defines particular learning objectives (e.g. linear, logistic, Poisson, Cox, etc.). The implementation of linear regression and GLMs in Spark's MLlib is definitely based on standard Statistical theory for linear models. For example, quoting directly from pyspark/mllib/regression.py 's LinearRegressionWithSGD comments: Train a linear regression model using Stochastic Gradient Descent (SGD). This solves the least squares regression formulation f(weights) = 1/(2n) ||A weights - y||^2 which is the mean squared error. i.e. this is a standard linear regression algorithm for Gaussian response. The implementation of the particular algorithm might be optimised such that it works for very large datasests (see for example this excellent thread on " Why use gradient descent for linear regression, when a closed-form math solution is available? ") but the theory behind an algorithm is exactly the same.
