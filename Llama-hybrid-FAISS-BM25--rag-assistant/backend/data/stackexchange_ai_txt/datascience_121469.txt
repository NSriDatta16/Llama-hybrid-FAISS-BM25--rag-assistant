[site]: datascience
[post_id]: 121469
[parent_id]: 121467
[tags]: 
VAEs were a hot topic some years ago. They were known to generate somewhat blurry images and sometimes suffered from posterior collapse (the decoder part ignores the bottleneck). These problems improved with refinements. Basically, they are normal autoencoders (minimize the difference between the input image and output image) with an extra loss term to force the bottleneck into a normal distribution. GANs became popular also a few years ago. They are known for being difficult to train due to their non-stationary training regime. Also, the quality of the output varies, including suffering the problem of mode collapse (always generating the same image). They consist of two networks: generator and discriminator, where the generator generates images and the discriminator tells if some image is fake (i.e. generated by the generator) or real. The generator learns to generate by training to deceive the discriminator. Nowadays the hot topic is diffusion models . They are the type of models behind the renowned image-generation products Midjourney and DALL-E. They work by adding random noise to an image up to the point they are become only noise, and then learning how to remove that noise back into the image; then, you can generate images directly from noise.
