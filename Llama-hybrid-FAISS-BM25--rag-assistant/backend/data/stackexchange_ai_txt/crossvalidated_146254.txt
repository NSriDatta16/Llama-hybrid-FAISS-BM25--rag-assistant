[site]: crossvalidated
[post_id]: 146254
[parent_id]: 
[tags]: 
Trend Analysis: How to tell random fluctuations from actual changes in trends?

I hope somebody in here can help me: I'm looking for some pointers as to how to distinguish random fluctuation from actual changes in trends, e.g.: In a time series with measures taken at monthly (daily) intervals, for how many months (days) does the change have to be consistent (e.g. going from decreasing to increasing) before one can say that it is an actual change in trend and not just a random fluctuation? How large (percentage-wise or in comparison with the variation) does the change have to be in order to be a real change and not just a random fluctuation? Is there any any rules of thumb or hard rules on this? Thanks! EDIT: Example: If I have a time series with 60 observations, and the trend change (from decreasing to increasing, say) at observation 30, when will I be able to detect that it is a real change in the trend and not a random fluctuation? At observation 31? 32? 33? Something else? r1
