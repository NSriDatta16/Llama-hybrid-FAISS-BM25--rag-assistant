[site]: crossvalidated
[post_id]: 515832
[parent_id]: 515387
[tags]: 
Some classifiers, including LDA default (hyper)parameters in some implementations, use the relative frequency of the classes. When doing leave-one-out with such a classifier, you'll systematically test with a class that is underrepresented in the training set (compared to the whole data set). Particularly with small sample sizes, this can lead to surprisingly large pessimistic bias. The huge difference you observe with stratified testing points into this direction. This has been described in the literature, e.g. in R. Kohavi: A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection. In: Artificial Intelligence Proceedings 14 t h International Joint Conference, 20 – 25. August 1995, Montréal, Québec, Canada. Hrsg. von C. S. Mellish. Morgan Kaufmann, USA, 1995, S. 1137–1145. I've seen this happen with vibrational spectra and PLS-DA, see e.g. Beleites, C.; Baumgartner, R.; Bowman, C.; Somorjai, R.; Steiner, G.; Salzer, R. & Sowa, M. G. Variance reduction in estimating classification error using sparse datasets, Chemom Intell Lab Syst, 79, 91 - 100 (2005).
