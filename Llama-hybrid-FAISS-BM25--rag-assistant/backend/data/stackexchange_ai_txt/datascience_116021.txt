[site]: datascience
[post_id]: 116021
[parent_id]: 
[tags]: 
In neural networks, is applying dropout the same as zeroing random neurons?

Is applying dropout equivalent to zeroing output of random neurons in each mini-batch iteration and leaving rest of forward and backward steps in back-propagation unchanged? I'm implementing network from scratch in numpy .
