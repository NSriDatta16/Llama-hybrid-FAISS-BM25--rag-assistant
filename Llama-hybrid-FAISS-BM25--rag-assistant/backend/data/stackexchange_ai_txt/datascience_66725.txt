[site]: datascience
[post_id]: 66725
[parent_id]: 66723
[tags]: 
Don't get hung up on the word "govern" here. $W_{ax}$ , $W_{ay}$ and $W_{aa}$ are simply the weights and they play in principle the same role weights play in feed forward network (except that feedforward networks do not have $W_{aa}$ ): $W_{ax}$ are the weights from your input layer to the first hidden layer (just as they are in feedforward networks) $W_{ay}$ are the weights from your last hidden layer to the output layer (just as they are in feedforward networks) $W_{aa}$ are the weights applied to the hidden state when it is fed from $t$ to $t+1$ (this is what you do not have in feedforward networks since they do not propagate through time) You can also read more about this in the respective chapter of the deep learning book which, I think, provides a good explanation of RNNs. EDIT (based on your comment): In a feedforward network you would have $\hat{y} = g(W_{ay}\text{ }a+b_y)$ while your RNN has $\hat{y}^{ } = g(W_{ay}\text{ }a^{ }+b_y)$ (with $i$ being an index for the time step). There is really nothing new here with regards to the output layer in an RNN. In the below image you see to which connections the weights are applied in a feedforward net and in an RNN (I omitted the bias for simplicity):
