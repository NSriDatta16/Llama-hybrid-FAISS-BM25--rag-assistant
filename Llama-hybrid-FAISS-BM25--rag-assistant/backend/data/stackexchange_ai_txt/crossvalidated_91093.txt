[site]: crossvalidated
[post_id]: 91093
[parent_id]: 91091
[tags]: 
The difference is the number of classifiers you have to learn, which strongly correlates with the decision boundary they create. Assume you have $N$ different classes. One vs all will train one classifier per class in total $N$ classifiers. For class $i$ it will assume $i$-labels as positive and the rest as negative. This often leads to imbalanced datasets meaning generic SVM might not work, but still there are some workarounds. In one vs one you have to train a separate classifier for each different pair of labels. This leads to $\frac{N(N-1)}{2}$ classifiers. This is much less sensitive to the problems of imbalanced datasets but is much more computationally expensive.
