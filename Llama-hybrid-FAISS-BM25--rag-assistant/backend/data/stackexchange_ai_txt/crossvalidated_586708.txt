[site]: crossvalidated
[post_id]: 586708
[parent_id]: 586519
[tags]: 
As I understood, in general these values are taken as N_train >> N_val ~= N_test. As I understood, the loss and metrics are evaluated (in an average sense) on the whole training set. In this context, how can we compare the performance on sets of different sizes? usually, the training data loss is used as part of the training algorithm, but not for establishing generalization performance. performance estimates are usually calculated in a manner that makes them independent of the test sample size, e.g. root mean squared error, fractions of tested cases such as fraction of correct predictions. More precisely, their point estimates are independent of the number of tested cases, confidence intervals will (and should) depend on the number of tested cases. Why isn't it like model performance is evaluated on a subset of the training set whose size is comparable to that of the validation or test set? Sample size requirements for model training and testing are largely independent of each other. For model training, often the number of training cases relative to model complexity (e.g. number of variates) is the crucial point. In addition, we can adapt e.g. via regularization or ensemble predictions to small sample size/high dimensionality situations. In contrast, the precision of test results depends on the absolute number of tested cases. From this it makes sense that typically tests used for hyperparameter optimization and final tests (to estimate generalization performance of the obtained model) are designed to have (approximately) equal sample size. Training error, i.e. the loss function used for parameter fitting, is a totally different beast. But since the actual testing of an available caes is hardly any computational effort (compared to training), and since the test results are more precise with larger sample size, one may as well use all the cases assigned for training to evaluate the loss function during training. Outside that scope, training error is often almost useless anyways since it does not detect overfitting. One can argue that it might increase the computational cost, but we can at least draw (randomly) from the losses evaluated during the training step. The point here is that once the parameters have been fitted with the help of the loss calculations, they are hardly ever useful. Often the performance metrics for training data are available, but one ignores them because they don't provide useful information.
