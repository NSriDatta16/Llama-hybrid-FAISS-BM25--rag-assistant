[site]: crossvalidated
[post_id]: 309261
[parent_id]: 
[tags]: 
Clarification of k-sparse autoencoder prediction step

Been reading Makhzani's paper detailing the k-sparse autoencoder, which seems like a pretty intuitive and computationally cheap algorithm for imposing a sparsity constraint on an autoencoder. Paper here: https://arxiv.org/pdf/1312.5663.pdf I notice that having trained the model, the sparse encoding step on new data isn't just a matrix multiplication - rather, we find the top $k$ activations and zero the rest (see 'algorithm description'). My question - do we find the $k$ highest activations across an entire matrix of new samples, or do we have to recompute the $k$ highest activations for each input vector? And if so, does this pose a problem for computational speed when encoding many many input vectors, because we cannot simply do one matrix multiplication?
