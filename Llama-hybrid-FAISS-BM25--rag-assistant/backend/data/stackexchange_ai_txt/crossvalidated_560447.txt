[site]: crossvalidated
[post_id]: 560447
[parent_id]: 560437
[tags]: 
Your question is more theoretically founded, but goes in the same direction as this one . You might want to check the answers there (disclaimer: one of them is mine). In order to answer your question from the last paragraph, I believe it is useful to identify conceptual basic blocks in machine learning: feature selection feature transformation parameter optimisation Regarding feature selection : Objects from the real world can be described by a very high number of features (or variables), potentially in the millions or even more (think of images with megapixel resolution). The number of real-world features is, in all practically relevant applications, much higher than the amount of data (observations, points) you have, so the curse of dimensionality kicks in. You cannot learn reliably using all the available features. You need to select the relevant ones. The feature transformation is used for the purpose you precisely identified in your question (Cover's theorem etc.). Parameter optimisation concerns finding (or at least approximating) the optimal parameters of the model, leading to the best performance on new, unseen data. Now, regarding neural networks and kernel-based methods, specifically support vector machines, they approach these three problems from a different angle, and I personally wouldn't say that one approach is uniformly better over all of the three. In particular, to my knowledge, support vector machines still beat neural networks (and everything else, as long as you don't know which distribution lies behind the data) regarding the third point. They solve a quadratic programming problem and are guaranteed to find (or at least approach in the limit) the best separating hyperplane. On the other hand, neural networks perform gradient descent over a non-convex function, with many local optima, and offer only a "best effort" solution. But, finding the optimal solution in SVMs comes at a computational price: In order to find it, SVMs require pair-wise dot products in the feature space (the kernel matrix), so the complexity rises at least quadratically with the number of points. This makes them impractical for large data sets, with millions of observations. Regarding the second point, feature transformation , SVMs do it using the kernel, while neural networks typically use multiple layers with many non-linear neurons. Computationally, computing the kernel is usually more efficient than propagating the signal through the layers, but this advantage of the SVMs is usually not enough to cancel out the above mentioned complexity in the optimisation part. Also, in kernel-based methods, the feature selection part is left out. You need to do it manually, or use a different algorithm for preprocessing your data in a way that produces meaningful features. Neural networks, especially convolutional ones, elegantly combine the feature selection and the feature transformation part in a same architecture, making them easy to deploy and require less human expert input. To summarise: It's a combination of computational complexity issues and ease of use that give the neural networks an advantage, at least for large and high-dimensional data sets. In the answer here I give a short historical overview regarding the development of kernel-based methods in relation to neural networks, which you might also find relevant. Update: To give you a feeling how the methods differ regarding computational time, let us consider a classification task on a toy "two moons" dataset: For small datasets, an SVM is faster and even more accurate than a neural network: But, as the number of samples surpasses ~10,000, SVMs become computationally too expensive: Notice that the scale is logarithmic, so the slope of a curve actually tells you about the power by which the computational time rises with the sample size. For neural networks it is almost linear, but for SVMs it is about quadratic. (I also included a linear SVM and a logistic regression for comparison, but bear in mind that these cannot properly classify this data set).
