[site]: crossvalidated
[post_id]: 351324
[parent_id]: 351280
[tags]: 
Type 2 error in the context of science refers to incorrectly deciding that some data provides good evidence for a hypothesis that is not actually true. This really only makes sense if you have some hypothesis. Think about this as a "False Negative" in the context of science. A "Positive" is when we flag a hypothesis as untrue, and a "Negative" is when we decide there is nothing interesting to see here (broadly speaking). So a "False Negative" is when we incorrectly decide there is nothing interesting going on here and retain our null hypothesis. (The null hypothesis typically states "There is no difference between these groups" or "This drug doesn't work".) However, in the context you're talking about, you don't have an explicit hypothesis. Instead, you will want to use the same general concept to evaluate the performance of your model. This can be done by producing something called a confusion matrix on the results of the second dataset (you would indeed have to get some new values, or wait for these same groups to receive funding to check your predictions). This Wikipedia page explains it quite well, and you can see from the Confusion Matrix table that it has labelled 'Type 2 error' which is what you are looking for. I am fairly new to machine learning, but I believe that it's a perfectly reasonable metric for estimating how well your model performs.
