[site]: crossvalidated
[post_id]: 265094
[parent_id]: 
[tags]: 
Is it true that Bayesian methods don't overfit?

Is it true that Bayesian methods don't overfit? (I saw some papers and tutorials making this claim) For example, if we apply a Gaussian Process to MNIST (handwritten digit classification), but only show it a single sample, will it revert to the prior distribution for any inputs different from that single sample, however small the difference?
