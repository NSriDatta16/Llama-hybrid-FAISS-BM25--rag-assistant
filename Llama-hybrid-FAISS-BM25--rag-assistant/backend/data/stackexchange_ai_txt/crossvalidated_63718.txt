[site]: crossvalidated
[post_id]: 63718
[parent_id]: 63026
[tags]: 
The notion of "topics" in so-called "topic models" is misleading. The model does not know or is not designed to know semantically coherent "topics" at all. The "topics" are just distributions over tokens (words). In other words, the model just capture the high-order co-occurrence of terms. Whether these structures mean something or not is not the purpose of the model. The "LDA" model has two parts (essentially all graphical models): a) model definition and b) an implementation of an inference algorithm to infer / estate model parameters. The thing you mentioned may or may not be the problem of "LDA" model but can be some bug / error / misconfig of the specific implementation you used (R package). Almost all implementations of "LDA" requires some randomization. And by the nature of inference algorithms (e.g., MCMC or variational inference), you'll get local minimum solutions or a distribution of many solutions. So, in short, what you observed is somehow expected. Practical Suggestions: Try different R packages: For example, this package is done by David Blei's former graduate student. Or, even try another environment, such as this one . If you get similar results from all these stable packages, at least, you get reduce the problem a bit. Try playing a bit with not removing stop-words. The rationale is that, these stop-words play important role in connecting semantic meanings in such a small corpus (e.g., 100 or so articles). Also, try not filtering things. Try playing a bit with hyper-parameters, like different numbers of topics. Papers about topic coherences: http://www.aclweb.org/anthology-new/D/D12/D12-1087.pdf http://people.cs.umass.edu/~wallach/publications/mimno11optimizing.pdf
