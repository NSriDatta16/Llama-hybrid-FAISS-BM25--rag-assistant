[site]: crossvalidated
[post_id]: 460810
[parent_id]: 460794
[tags]: 
Time series don't have i.i.d. components, otherwise there's no need to model them sequentially taking time into account but we could treat each sample individually (shuffle all of them and learn distribution). In general we assume that the distribution of a given sample depends on the past samples as you said. But this doesn't contradicts the assumption that each of the sequences (not samples of each sequence) in the dataset are i.i.d. Say you have a speech dataset. You can consider each chunk or each audio as a vector and each of such vectors to be i.i.d (not amplitude values but a representation of the speech sequence). To illustrate this better, you can think of the problem of modelling images. You can vectorize the image or treat it as a matrix. You usually have a lot of statistical dependence between neighboring pixels (that's what image coders exploit) but still you can consider each image to be i.i.d (each vector or each matrix come from the same distribution). Hope this helps you clarify. PS: For the formulation of the joint probability of a sequence and how to predict sequences with CNNs you can check the famous WaveNet paper .
