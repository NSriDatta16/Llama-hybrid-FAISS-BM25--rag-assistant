[site]: crossvalidated
[post_id]: 371619
[parent_id]: 
[tags]: 
Does optimizing the parameters in an exponential smoothing model constitute "learning"?

I'm having an argument at work with a colleague who's saying that we need to use machine learning models instead of the current exponential smoothing models (Holt, Holt-Winters) for demand forecasting, because according to him, our current software doesn't do any "learning" from data the way an ML method does. The software we use has optimization routines that find the optimal $\alpha$ , $\beta$ and $\gamma$ for a Holt-Winters model that we use for demand forecasting, by minimizing the BIC. As far as I know this constitutes "learning" in the Machine Learning sense, albeit a simplified version of learning compared to what a deep learning model or xgboost model would learn. But it is nonetheless learning. He is insisting that it doesn't constitute learning, because it is not incrementally improvable the way learning in a more complex model is (for example a NNet can learn more after 2000 epochs than it can after 100 epochs, etc...). So does optimizing the parameters of an exponential smoothing model (or ARIMA model) using the BIC (or the AIC) constitute proper "learning" in the ML sense of the word?
