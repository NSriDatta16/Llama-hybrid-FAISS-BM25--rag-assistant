[site]: crossvalidated
[post_id]: 589994
[parent_id]: 
[tags]: 
GridSearchCV returns unrealistic AUC score with Logistic Regression

Long time lurker but I've just created an account because its the first time one of my questions has not actually been answered. I'm currently struggling with optimizing the hyperparameters of a Logistic Regression using GridSearchCV. My current model is severely overfitting (ROC AUC train = 0.93, ROC AUC test = 0.54). When performing GridSearchCV (on the train set), all the ROC values are > 0.9 and the final ROC is 0.96, as if the score was calculated from the train split. Here is the code : #Define a list of hyperparameters we want to tune parameters = { 'log_r__penalty' : ['l1','l2'], 'log_r__C' : np.logspace(-3,3,7), 'log_r__solver' : ['saga','lbfgs'], } #Defining our LogR pipeline pipe_lr = Pipeline([('scaler', StandardScaler()), ('log_r', LogisticRegression(max_iter = 1000))]) #Defining our Grid Search operator clf = GridSearchCV(pipe_lr, param_grid = parameters, scoring = 'roc_auc', cv = 5, n_jobs=-1, verbose=10) #Apply our Grid Search operator to our train set clf.fit(X_train,y_train) print("tuned hpyerparameters :(best parameters) ",clf.best_params_) print("ROC AUC :",clf.best_score_) The ROC values are also very similar for all splits even when the hyperparameters are widely changing. Here is an extract of my cv_results_ : When I apply this model to my validation set the ROC falls back to 0.56. I don't know if it makes any difference, but the initial dataset is widely unbalanced (92% / 8%) so I performed ADASYN oversampling to bring back our ratio to 50/50. So in that case X_train has been oversampled while (obviously) the validation and test sets haven't. Am I missing something here? I feel like GridSearchCV completely fails to optimize my model. Should I apply GridSearchCV before oversampling?
