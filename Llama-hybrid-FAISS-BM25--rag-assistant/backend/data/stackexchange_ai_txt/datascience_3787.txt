[site]: datascience
[post_id]: 3787
[parent_id]: 3784
[tags]: 
Short Version The difference is in the trade-off between complexity of model and how hard it will be to train. The more states your variable can take, the much more data you'll need to train it. It sounds like you're reducing the feature space through some front-end processing. Example Context: Say you're using NBC to model if you go outside given the high temperature that day. Let's say temperature is given to you as an int in Celsius which just happened to range between 0 and 40 degrees. Over one year of running the experiment - recording the temperature and if you went outside - you'll have 365 data points. Internal Representation: The internal structure the NBC uses will be a matrix with 82 values (41 values for your one input variable * 2 because you have two states in your output class). That means each bin will have an average of 365/82 = 8.9ish samples. In practice, you'll probably see a few bins with lots more samples than this and a many bins with 0 samples. A Pitfall Say you saw 8 cases at a temperature of 5 C (all of which you stayed inside) and nothing at a temp of 3 or 4 C. If, after the model is built, you 'ask it' what class a temp of 4 C should be in. It won't know. Intuitively, we'd say "stay inside", but the model won't. One way to fix this is to bin the classes 0,1,2,... into larger groups of temperatures (i.e., class 0 for temp 0-3, class 1 for temp 4-7, etc). The extreme case of this would be two temperature states "high" and "low". The actual cut-off should depend more on the data observed, but one such scheme is converting temperatures within 0-20 to "low" and 21-40 is "high". Discussion This front end processing seems to be what you're talking about as the "wrapper" around the NBC. The result of our "high" and "low" binning scheme will result in a much smaller model (2 input states and 2 output classes gives you a 2*2 = 4 number of classes. This will be way easier to process and will take way less data to confidently train at the expense of complexity. One example of a drawback of such a scheme is: Say the actual pattern is that you love spring and fall weather and only go outside when it's not too hot or not too cold. So your outside adventures are evenly split across the upper part of the "low" class and the lower part of the "high" class giving you a really useless model. If this were the case, a 3-class bin scheme of "high", "medium", and "low" would be best.
