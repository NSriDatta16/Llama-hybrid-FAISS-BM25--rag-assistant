[site]: crossvalidated
[post_id]: 242924
[parent_id]: 
[tags]: 
Neural networks and signal-to-noise ratio

My guess is that neural networks do not work very well in noisy environments, i.e. the lower the signal-to-noise ratio, the worse the result of a neural network, if compared to other statistical modeling tools. Thus, for example, neural networks are good at predicting credit cards frauds, but they get much less exciting results when trying to predict financial markets (very noisy, at least in the short term). Any theoretical result and/or empirical evidence on that?
