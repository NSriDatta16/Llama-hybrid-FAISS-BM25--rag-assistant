[site]: crossvalidated
[post_id]: 265037
[parent_id]: 
[tags]: 
Why use the cosine distance for machine translation (Mikolov paper)?

I am currently reading the paper "Exploiting Similarities among Languages for Machine Translation" by Mikolov et al. (available here : https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/44931.pdf ) and I was wondering why they used the cosine similarity to find the closest word to z (page 4, after equation (3)) instead of a more classic distance (like the squared sum of differences of each component). So my question is large : why this distance since when computing the matrix W, it should act as a rotation and a scaling ? And is there any record of using word embeddings with different distance metrics and their results ?
