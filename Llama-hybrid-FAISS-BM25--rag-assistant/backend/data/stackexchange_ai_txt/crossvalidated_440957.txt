[site]: crossvalidated
[post_id]: 440957
[parent_id]: 
[tags]: 
Monte Carlo Sampling is performing better then Bayesian Optimization

So I am testing the Bayesian optimization library for determining where to sample next by quering a test function such as 2d Rosenbrock to better reconstruct that function using Gaussian Process regression. In other words I want to perform selective sampling of the function to reconstruct that function using Gaussian Process regression (GP) in least amount of selective sampling. So in the literature Bayesian optimization is considered a best way to learn a function in fewer steps efficiently. So lets say I want to sample at 100 locations and then construct the estimation of Rosenbrock function using GP, performing pure random or monte carlo sampling gives better results then performing Bayesian optimization! So I want to ask why is Bayesian optimization considered useful then? Additional notes: The reproducible code would be very huge beyond the scope of SE guidlines. Based on the answer the additional information is following: the a priori information is initially constructed by performing random sampling or MC sampling of the function and fitting the GP - which automatically tunes the hyperparameters in the Sklearn library. The kernel that is fitted is Matern which is widely used for such cases. In general if I am doing sampling such as 100 samples, then in case of pure MC sampling i am taking all 100 samples randomly and then fitting the GP and comparing with the Rosenbrok function to see the error and for Bayesian i am taking 50 samples randomly initially - fitting the GP to form the prior - 50 using Bayesian optimization and then re-fitting the GP and comparing with the Rosenbrok.
