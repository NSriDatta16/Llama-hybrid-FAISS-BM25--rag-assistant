[site]: crossvalidated
[post_id]: 380890
[parent_id]: 267944
[tags]: 
To clarify terminology, a GLM with Gaussian distribution and identity link is identical to the general linear model or linear regression. So I will simply refer to it as linear regression going forward. Additionally, the normality assumption in regression is on the error term. Since the residuals are estimated errors, we verify this assumption using the residuals or variants of the residuals, like studentized deleted residuals. Claiming the distributional assumption applies to the error term works in linear regression. However, for other GLMs like logistic regression, we do not assume the error term is binomial. A more general assumption for linear regression is that the data are normally distributed with a mean that depends on the predictors and a variance that is constant. Since the mean of the distribution depends on the predictors, to verify normality, we extract these means, hence giving all of the data the same center/location. This is what residuals are and then we can plot these residuals to check normality. Beyond linear regression, such empirical verification becomes more complicated. It happens that the normality assumption in linear regression is one of the assumptions we do not need to care much about. The least squares coefficients do not depend on normality. However, classical inference using t -tests relies on the normality assumption. But from central limit theorem, we know this assumption does not matter too much once the sample sizes are large enough. Rather than moving away from linear regression because of non-normality, I might move to another GLM if I could hypothesize a theoretical reason for why another distribution would be more appropriate. For example, if my data were count so non-negative and they were largely low counts with a few high values, I might start to consider a Poisson distribution. If the data were binary, then Binomial distribution is reasonable. The normal distribution is a reasonable choice if all we are willing to assume is that the data have finite variance and their range is $(-\infty, \infty)$ . Usually, we can make more assumptions about the data than these two assumptions. Additionally, the linear regression can handle multiple predictors predicting a single outcome. Wilcoxon/Spearman are for single predictor situations, so they are not comparable to linear regression in this way. They are directly comparable to specialized linear regressions like the Pearson correlation and the t test. However, they ask different questions of the data. Wilcoxon tests are stochastic dominance tests comparing two groups, while Spearman test is a measure of monotonic relation (like an ordinal linear correlation). So even when they are similar to linear regression, they ask different questions of the data. Finally, with regard to your final question, modeling is hard. Model selection is a difficult problem. A reasonable approach is to attempt different models then observe their varying implications. If their implications are similar, that makes life easy. If they differ, that can be interesting for research purposes. With Bayesian modeling, we have a few more options in that we do not simply have to select a model, we can combine different models to account for our uncertainty in our final model. Edit An error on my part from the first paragraph: general linear model also includes MANOVA; this differentiates general linear model from the standard linear regression. All other members of the general linear model family are linear regression models or generalized linear models with gaussian distribution and identity link function.
