[site]: datascience
[post_id]: 84360
[parent_id]: 
[tags]: 
Understanding image size changes in DCGAN

I have been studying and trying to implement Generative Adversarial Networks using PyTorch. More precisely I tried to replicate the DCGAN PyTorch Tutorial tutorial using some custom dataset. My code worked well and I was able to get some nice results, but when I look at the architecture of both the generator and the discriminator I struggle to understand how the image sizes change as it goes through the different convolutional layers. To make my question a bit more clear, I do not understand the process to go from the original noise vector of size 100 to the 64X64X3 output image in the below architecture: class Generator(nn.Module): def __init__(self, ngpu): super(Generator, self).__init__() self.ngpu = ngpu self.main = nn.Sequential( # input is Z, going into a convolution nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False), nn.BatchNorm2d(ngf * 8), nn.ReLU(True), # state size. (ngf*8) x 4 x 4 nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False), nn.BatchNorm2d(ngf * 4), nn.ReLU(True), # state size. (ngf*4) x 8 x 8 nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False), nn.BatchNorm2d(ngf * 2), nn.ReLU(True), # state size. (ngf*2) x 16 x 16 nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False), nn.BatchNorm2d(ngf), nn.ReLU(True), # state size. (ngf) x 32 x 32 nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False), nn.Tanh() # state size. (nc) x 64 x 64 ) def forward(self, input): return self.main(input) I checked some online resources as well as the PyTorch documentation, and I found some different formulas to calculate the output size of convolutional layers. However none of them were enough to explain the big transformations that occur in this particular architecture. I hope someone here can help me understand this!
