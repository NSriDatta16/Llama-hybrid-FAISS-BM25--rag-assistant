[site]: datascience
[post_id]: 48025
[parent_id]: 48024
[tags]: 
There are two options : CBOW . Modify Word2Vec CBOW code to save the whole trained model (current implementations only persist embedding layer) CBOW Model: This method takes the context of each word as the input and tries to predict the word corresponding to the context. Intro : https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa Example : https://www.tensorflow.org/tutorials/representation/word2vec Train an LSTM / GRU to predict next word (given previous N words) Karpathy's article is probably the best introduction to text generation with RNN (this works at character level, you will have to modify it to work at word level [Word-Vector level]) http://karpathy.github.io/2015/05/21/rnn-effectiveness/ Example : https://medium.com/phrasee/neural-text-generation-generating-text-using-conditional-language-models-a37b69c7cd4b
