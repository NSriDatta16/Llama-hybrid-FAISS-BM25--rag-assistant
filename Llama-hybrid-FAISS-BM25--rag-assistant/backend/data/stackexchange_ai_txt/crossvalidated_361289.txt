[site]: crossvalidated
[post_id]: 361289
[parent_id]: 
[tags]: 
Is Tom Mitchell's Machine Learning book incorrect in its definition of Confidence Interval?

In Tom Mitchell's 1997 Machine Learning, a confidence interval is defined on page 138 as An N% confidence interval for some parameter p is an interval that is expected with probability N% to contain p. However, from questions as Why does a 95% Confidence Interval (CI) not imply a 95% chance of containing the mean? , it seems as if this is precisely not the case. Is Tom Mitchell mistaken?
