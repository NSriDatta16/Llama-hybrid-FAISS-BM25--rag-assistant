[site]: crossvalidated
[post_id]: 270045
[parent_id]: 269996
[tags]: 
For fully-connected layers, the number of 'nodes' is the output dimension of the weight matrix. In other words, if we have a hidden layer, where: input layer, dimension $d_i$ hidden layer 1, dimension $d_h$ ... then the weight matrix for hidden layer 1 will be $d_i \times d_h$. $d_h$ in this case is the number of 'nodes' of hidden layer 1, it is the output dimension. In RNNs and LSTMs, these concepts are unchanged. However, nuance: there is an embedding layer in the input and the output, for RNNs, in general. So, the layers are like this: input layer, dimension $d_i$ (corresponds to the one-hot dimension) embedding layer, dimension $d_h$ (embeds the one-hot vector into embedding dimension $d_h$; generally this is just a matrix multiplication) LSTM, dimension $d_h$ (contains various $d_h \times d_h$ matrices) output embedding, dimension $d_o$ (in char-rnn, $d_o = d_i$, it decodes from the output embedding vectors, back into a probabilitiy distribution over possible characters)
