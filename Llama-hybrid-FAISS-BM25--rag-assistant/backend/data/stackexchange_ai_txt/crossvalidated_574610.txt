[site]: crossvalidated
[post_id]: 574610
[parent_id]: 530205
[tags]: 
The issue you're encountering is that, in a regression problem, most people optimize the loss function of interest, often MSE (or something equivalent like SSE, RMSE, or $âˆ’R^2$ , all of which are equivalent as loss functions). In a classification problem, people often have an interest in the classification accuracy: how many times I call a dog picture a dog and a cat picture a cat. That requires a decision based on the probability output, often just rounding the probability to 0 or 1. Such an accuracy metric is an improper scoring rule and is less useful than many realize. Nonetheless, people still focus on the rounded probabilities, and this is true to such an extent that in the popular Python machine learning sklearn library, the predict function gives a discrete outcome (it takes the category with the highest probability), while predict_proba is needed for the probability outputs.
