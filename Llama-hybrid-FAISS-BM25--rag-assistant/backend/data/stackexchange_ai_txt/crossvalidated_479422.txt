[site]: crossvalidated
[post_id]: 479422
[parent_id]: 479382
[tags]: 
Model selection for neural networks is complex. You can vary their 'width', the number of hidden nodes in one layer, and their 'depth', the number of hidden layers. Specifically for image processing, deep convolutional neural networks have proven really successful. They explore the deep structure in images in a bottom-up manner. You do not mention that your NNs are trained with image data. Occam's razor : use the simplest possible model that can predict well ("entities should not be multiplied without necessity"). This means that you should choose the MLP with the smallest number of parameters that is well-performing. It is the total number of parameters (weights + bias terms) that needs to be kept minimal. Note that more restricted MLPs (with fewer parameters) are often more difficult to train. I always split the training set into a training adjustment set and a training generalization set (The separate test set is kept out of the loop, until all training and model selection has taken place). The split into two training subsets is performed by a random generator. Your training algorithm fits the MLP-parameters on the training adjustment set , using backpropagation, or some if its later variants. You can now use cross validation on the training generalization set to compare the generalization ability of different MLP topologies. You can plot the number of parameters for each MLP topology versus the accuracy as computed on the training generalization set . Model selection means choosing the MLP that yields the best trade-off between the number of parameters and the accuracy as computed on the training generalization set . Finally, you evaluate your best MLP using the independent test set.
