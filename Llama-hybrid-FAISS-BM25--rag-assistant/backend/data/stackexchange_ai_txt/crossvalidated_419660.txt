[site]: crossvalidated
[post_id]: 419660
[parent_id]: 
[tags]: 
Retrofitting GloVe word embeddings on smaller (<20k) records

I am training my own word embeddings in R using GloVe algorithm. The sample data is less than 20k records and are product reviews obtained from an ecommerce website. In the current approach using word embeddings created from these reviews, I perform tSNE to reduce the dimensions from 50 to 2. However, I notice that the words which should had been semantically similar are actually dispersed quite a lot on the 2d scatter plot (post using tSNE). Multiple iterations with changes in hyperparameters do not mitigate this problem. Recently, I came across a package in Python called Mittens, which performs retrofitting of embeddings using pretrained ones from the Stanford GloVe website. I used the approach and in a way ended up modifying my existing embeddings with pretrained ones from the above source. This led to tighter/closer grouping for some of the words from my existing vocabulary/reviews. But not for all possible words. I would like to know if retrofitting using the Mittens approach is the correct way to proceed. I have a hint of optimisim, given at least some expected grouping of words can now be actually seen. Open to suggestions and comments in this regard. Thank you in advance.
