[site]: crossvalidated
[post_id]: 246910
[parent_id]: 246896
[tags]: 
Firstly, let's pretend we have just started training so that: \begin{equation} n = 0 \end{equation} and \begin{equation} \omega_{kj}(0) = \alpha\delta_jy_k \end{equation} because this is the first time step. If we go one training step further to \begin{equation} n = 1 \end{equation} then: \begin{equation} \omega_{kj}(1) = \alpha\delta_jy_k + \eta\omega_{kj}(0) \end{equation} and one more to \begin{equation} n = 2 \end{equation} \begin{equation} \omega_{kj}(2) = \alpha\delta_jy_k + \eta\omega_{kj}(1) \end{equation} and substitute: \begin{equation} \omega_{kj}(2) = \alpha\delta_{j2}y_{k2} + \eta(\alpha\delta_{j1}y_{k1} + \eta\omega_{kj}(0)) \end{equation} You can see that if we continued this trend, even a time step of \begin{equation} n = 200 \end{equation} would still slightly be influenced by the very first weight gradient at time step 0 (but very very slightly). This is the concept of momentum: velocity with a memory of past velocities. This concept is directly related to physics. If I push a block 10m/s forward (think of that as my first time step) and then I push it at -20m/s (my second time step); then via momentum this would be: new speed = -20m/s + u (100m/s) where u is friction (or in our case eta ). So if friction is 0.1, my final speed will actually be -20 + (0.1)(100) = -10. I am closer to -20 but I am not completely there. If I push the block again at -20m/s my result is: -10 + (0.1)(-20) = -12, and I am closer to the speed I wish to attain. In the concept of neural networks, this means if I am going down the gradient and searching for the minimum, I do not want to drastically change directions at each time step because some directions could lead to valleys or local minima. Therefore I want to go the direction in which most of my weight gradients push me. Changing this direction usually requires a large number of weight gradients against my current gradient 'flow'.
