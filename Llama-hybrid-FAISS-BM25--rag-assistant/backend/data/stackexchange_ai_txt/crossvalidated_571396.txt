[site]: crossvalidated
[post_id]: 571396
[parent_id]: 571331
[tags]: 
Answering given the clarification in the comments. Bayesian logistic regression model is $$\begin{align} p_i &= g^{-1}(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_k X_k) \\ y_i &\sim \mathsf{Bernoulli}(p_i) \end{align}$$ with some priors for the $\beta_1,\beta_2,\dots,\beta_k$ parameters. $\beta_i$ are parameters of the model, and $X_i$ are the features. We are estimating the parameters, hence in the Bayesian setting, we are picking priors for them. We are not estimating the data, because the data was observed. It doesn't matter that much $X_i$ 's are binary or not, you pick the priors for the parameters, given what you expect a priori for the parameters to be. Now correcting what I just said, it is not exactly that you should not care at all what $X_i$ 's are. Notice that in the Bayesian setting using something like Gaussian priors corresponds to using $\ell_2$ regularization , while using Laplace priors to $\ell_1$ regularization . In Ridge regression or Lasso, it is usually recommended to scale all the features because there is a single regularization parameter, i.e. we use the same prior for each parameter. For the same reason, using the same prior for parameters corresponding to different features that are differently scaled may accidentally bias your results. Usually, binary features would have different scales than many other features, so their parameters may need different priors, but in the same sense, each parameter needs an individual prior. So there is nothing special about binary features. You may also find interesting the Gelman's et al paper The prior can generally only be understood in the context of the likelihood that discusses how our knowledge about the data influence the modeling decisions.
