[site]: crossvalidated
[post_id]: 559067
[parent_id]: 559042
[tags]: 
Your example is one of purely observation-driven modeling. And your observations indicate that there is a relationship between your feature and your target. Yes, it's a weak association. But that it is overfitting is not evident from the data you have observed! You can't very well accuse LOOCV of failing to detect overfitting that you only know is there because you know the true data generating process (DGP), i.e., you have information that is not available to the LOOCV method. Also, "overfitting" is not a Boolean attribute. On the one hand, we never know the true DGP outside simulations, so we can never truly say there is overfitting: anything could conceivably have an influence on our outcome. On the other hand, if you believe in "tapering effect sizes", we will never be able to capture all influences, so we will always have underfitting. Thus, it makes more sense to think of "overfitting" as a continuum. How much worse does adding a predictor make my model for future expected losses (which we again will only be able to estimate, unless we know the true DGP)? Thus, we have to think about how much signal there is in our data. In the present case, the overfitting is quite weak, and as Dikran says , it is hard to distinguish a weak effect from no effect whatsoever. And adding this feature will only have a very small effect on future predictions, so the overfitting, measured on a continuum, is small. Per above, you are modeling purely based on the observations here. Essentially, such a model has no predilection towards a simpler model, like the one without an effect of the feature. There are various ways of including such a predilection. Essentially, we would bias our models towards simplicity, and per the bias-variance tradeoff, this may very well improve future predictions. In the terminology above, our overfitting, if present, would be weaker. We could explicitly run a Bayesian model with a prior on the impact of the feature. As you write, we could include elements of NHST, only accepting the more complex model if the improvement in fit is statistically significant. Or we would use the "one standard error rule", which is very often used in cross-validation .
