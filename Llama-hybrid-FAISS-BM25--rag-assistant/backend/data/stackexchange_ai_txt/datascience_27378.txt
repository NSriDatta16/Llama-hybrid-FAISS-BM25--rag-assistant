[site]: datascience
[post_id]: 27378
[parent_id]: 
[tags]: 
Clarifying my understanding of on-policy RL (online SARSA)

I want to clarify I have understood how SARSA works in nuances. Consider an original definition taken from ON-LINE Q-LEARNING USING CONNECTIONIST SYSTEMS. G. A. Rummery & M. Niranjan. CUED/F-INFENG/TR 166. September 1994 (which is the first publication where SARSA wss mentioned, according to a Wikipedia article). The authors proposed an update rule which "... differs from normal Q-learning in the use of the Qt+1 associated with the action selected, rather than the greedy max(Qt+1 | a) used in Q-learning." (A citation from page 6.) Please note that this is essential to on-policy nature of the algorithm. The term SARSA mentioned in the footnote for this definition. And a later pseudo-code of SARSA update used in many readings: begin initialize Q[S,A] arbitrarily observe current state s select action using a policy based on Q repeat forever: carry out an action a observe reward r and state s' select action a' using a poicy based on Q Q[s,a] end-repeat end source: http://www.cse.unsw.edu.au/~cs9417ml/RL1/algorithms.html I want to understand if I must use exactly the same Q function (and policy) to get A and A'. If I update Q function in each iteration, it follows that the next action in a subsequent iteration will be derived using the latest Q updated, while the previous action was obtained using the former Q.\ On the other hand, I really can make A and A' with exactly the same Q, and only after that update the Q. So I will always consider A and A' derived using the same function. Which is more orthodox / correct?
