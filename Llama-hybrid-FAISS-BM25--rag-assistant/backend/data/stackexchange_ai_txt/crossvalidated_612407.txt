[site]: crossvalidated
[post_id]: 612407
[parent_id]: 612400
[tags]: 
There's no need to pre-test for multicollinearity in a study like this. That can be an issue in large-scale studies with many potential predictor variables, so it gets a lot of attention in machine learning courses. Even if you found multicollinearity, how would you change the modeling strategy for your studies? All that multicollinearity will do here is inflate the variance estimates for individual coefficients.* It probably won't hurt predictions from the models at all, as the high individual variance of one coefficient will typically be offset by corresponding covariances with other coefficients. If you are worried about multicollinearity, fit a model and then use the vif() function in the R car package to calculate the variance inflation factors (VIF) directly from the model. As this page explains, the VIF produced by that function are appropriate both for categorical predictors and for models other than ordinary least squares, situations where the usual VIF calculations are inappropriate. *If the collinearity is perfect then there might be a problem, but that just means that some of your predictors are exact linear combinations of others. Software will often just remove such a predictor from the model before it does the fitting.
