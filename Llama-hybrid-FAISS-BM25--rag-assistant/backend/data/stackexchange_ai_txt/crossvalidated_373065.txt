[site]: crossvalidated
[post_id]: 373065
[parent_id]: 373055
[tags]: 
When the optimization does occur through partial derivatives, in each turn does it change both w1 and w2 or is it a combination like in few iterations only w1 is changed and when w1 isn't reducing the error more, the derivative starts with w2 - to reach the local minima? In each iteration, the algorithm will change all weights at the same time based on gradient vector. In fact, the gradient is a vector. The length of the gradient is as same as number of the weights in the model. On the other hand, changing one parameter at a time did exist and it is called coordinate decent algorithm , which is a type of gradient free optimization algorithm . In practice, it may not work as well as gradient based algorithm. Here is an interesting answer on gradient free algorithm Is it possible to train a neural network without backpropagation?
