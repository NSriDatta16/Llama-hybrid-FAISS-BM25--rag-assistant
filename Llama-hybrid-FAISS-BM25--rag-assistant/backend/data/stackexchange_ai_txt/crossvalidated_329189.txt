[site]: crossvalidated
[post_id]: 329189
[parent_id]: 329183
[tags]: 
PCA finds the eigen vectors of X T X . These eigen vectors form a basis such that the first eigen vector represents the dimension which accounts for the highest amount of variation within X. The second eigenvector represents the dimension accounting for the second greatest amount of variation, etc. So, by projecting the data onto a basis made from the first n eigen vectors, we create an approximation of the data set, and where each new dimension added accounts for an increasingly small amount of the detail of variation. Thus, in a 1,000 dimensional space, the first three dimensions might account for 99% of the variation of the data, and we might want to project onto only those three dimensions. This wikipedia article covers the details reasonably well.
