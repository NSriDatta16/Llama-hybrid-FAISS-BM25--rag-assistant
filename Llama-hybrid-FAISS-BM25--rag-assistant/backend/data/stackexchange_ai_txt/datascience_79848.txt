[site]: datascience
[post_id]: 79848
[parent_id]: 79844
[tags]: 
Training a neural network involves optimizing a large set of parameters. This optimization step is commonly known as backpropgation (a.k.a backprop) via a form of gradient descent. Backprop via gradient descent allows a network to adjust its learnable parameters (i.e., weights) such that the loss (difference between the forward pass output and the actual output) reduces. In each epoch (1 pass over the entire dataset), very small adjustments (learning rate) are applied to the weights so that it eventually converges to an a a solution (a global optimum). However, finding a global optimum requires huge amounts of data. We never may know how much data we need until it is able to find a global optimum, therefore, in practice, we have multiple epochs (multiple passes over the data) to try to find this global optimum. In short, gradient descent is a very data-hungry process and hence why we require multiple epochs (or iteration in your case)
