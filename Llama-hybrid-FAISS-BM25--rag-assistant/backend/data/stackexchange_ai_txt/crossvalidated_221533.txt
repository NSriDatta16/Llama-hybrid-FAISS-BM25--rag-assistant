[site]: crossvalidated
[post_id]: 221533
[parent_id]: 221530
[tags]: 
The beauty of logistic regression is that it outputs probabilities. So just sort the subjects by their predicted probability of offending and pick the 20 greatest. I don't know what you're asking. I think you have the vocabulary mixed up here. Multicollinearity isn't something you do; it's a condition of a dataset. A categorical feature comprises levels, not variables. What exactly do you mean by "VIF"? To put it bluntly, stepwise model selection is an obsolete method. There are much better ways to do variable selection, such as the lasso, and you should be careful not to assume you need to do variable selection in the first place, because variable selection always carries the risk of throwing away useful information, and there are modeling techniques that can handle a lot of uninformative features, such as random forests (and lasso-regularized logistic regression, for that matter). As I mentioned earlier, logistic regression produces probabilities, not 0s and 1s. Making an ROC curve just means coercing the probabilities to 0 and 1 with a varying threshold. But it seems more to the point to just give the investigators the complete list of subjects sorted by their probability of offending. No, the idea of a validation set is just to allow you to tune some aspect of the models with the test set (such as, in the case of the lasso, the penalty size). Then you can examine the model's predictions in the validation set without any optimistic bias from overfitting the tuning procedure to the test set. That depends entirely on how you're getting your data. Just don't throw away data you already have to balance your classes. That would be counterproductive. Edit: Oh, when you wrote "sampling" here, you probably meant "resampling". No, don't do that.
