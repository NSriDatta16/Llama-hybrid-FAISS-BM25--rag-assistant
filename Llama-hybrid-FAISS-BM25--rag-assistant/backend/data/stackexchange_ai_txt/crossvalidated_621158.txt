[site]: crossvalidated
[post_id]: 621158
[parent_id]: 618226
[tags]: 
Great question. This has been shown to be possible, for example in Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (2020) ! We use a simplified form of position embeddings where each “embedding” is simply a scalar that is added to the corresponding logit used for computing the attention weights. Specifically, in their implementation, they use single scalar values added to the dot products in the attention layer (which can be interpreted as your idea of concatenating and then handling this dimension separately). Take a look at the HuggingFace implementation .
