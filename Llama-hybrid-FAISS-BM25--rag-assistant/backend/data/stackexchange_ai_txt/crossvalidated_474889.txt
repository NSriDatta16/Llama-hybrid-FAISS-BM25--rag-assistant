[site]: crossvalidated
[post_id]: 474889
[parent_id]: 
[tags]: 
Understanding reparameterization trick and training process in variational autoencoders

I am trying to understand variational autoencoders, particularly the sampling component and the reparameterization trick. I understand that instead of using a fixed determinstic latent representation as in traditional autoencoders, variational autoencoders involve computing mean and standard deviation vectors. These vectors are then used to sample latent vectors, which in turn can generate new data. However, I am trying to understand where sampling fits in the training process, if at all? My understanding is that the stochastic aspect is only relevant for generating new data (after training is complete), but not for training the encoder/decoder networks. Is that correct? Or does sampling also occur during training the variational autoencoder model, and if so how does this work?
