[site]: crossvalidated
[post_id]: 230276
[parent_id]: 
[tags]: 
How to compute the gradients for activation maximization in neural network?

I have a question regarding the Activation Maximization technique for neural networks. Activation Maximization is a technique used to visualize the filters of a neural network: Erhan, Dumitru, et al. "Visualizing higher-layer features of a deep network." University of Montreal 1341 (2009). The idea is to find an artificial sample that maximize the activation of an hidden unit and take this sample as representation of the filter. Since the weights are fixed after training, the activation of an unit only depends on x : h(x) . To approximate a local maxima, we can use gradient ascent. There are two things that I don't get here: Do we use the input only b + Wx or the activation function sigmoid(b+Wx) ? How can we compute the gradients in which to move x ?
