[site]: datascience
[post_id]: 31385
[parent_id]: 31382
[tags]: 
You are facing a very common problem: handling imbalanced data. For neural networks, typical procedures are: Having the proper metrics: global accuracy should not be used. Oversampling the minority class: randomly generate replicas of the minority class until the imbalance disappears.You can also perform data augmentation on the minority class. Synthetic data can be generated from the feature space, using SMOTE algorithm, but I don't know how it applies to neural networks. Under-sampling the majority class: randomly remove instances of the majority class. It can deteriorate the performance on the majority class Include class-weights in the loss function: the idea is to penalize the misclassification of the minority class. The weights are usually inversely proportional to the occurrence frequency of each class. Using different learning rates per class: you can use a bigger learning rate for the majority class, thus the net stops learning earlier from the majority class than from the minority class I'd recommend a combination of 1 (always), 2 and 4. For a higher insight in this topic, which is of very importance, I recommend reading: https://dl.acm.org/citation.cfm?id=1592322 https://arxiv.org/pdf/1710.05381.pdf
