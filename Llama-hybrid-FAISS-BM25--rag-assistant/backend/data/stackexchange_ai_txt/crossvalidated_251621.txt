[site]: crossvalidated
[post_id]: 251621
[parent_id]: 
[tags]: 
How Convolution Layer and Max pooling Work

1 model.add(ZeroPadding2D((1, 1), input_shape=(3, 48, 48), dim_ordering='th')) 2 model.add(Convolution2D(4, 3, 3, activation='relu', dim_ordering='th',init='he_uniform')) 3 model.add(ZeroPadding2D((1, 1), dim_ordering='th')) 4 model.add(Convolution2D(4, 3, 3, activation='relu', dim_ordering='th',init='he_uniform')) 5 model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), dim_ordering='th')) 6 model.add(ZeroPadding2D((1, 1), dim_ordering='th')) 7 model.add(Convolution2D(8, 3, 3, activation='relu', dim_ordering='th',init='he_uniform')) 8 model.add(ZeroPadding2D((1, 1), dim_ordering='th')) 9 model.add(Convolution2D(8, 3, 3, activation='relu', dim_ordering='th',init='he_uniform')) 10 model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), dim_ordering='th')) above is an example of a pretty simple keras model The convolution layer at #2 produces as output 4 activation maps, which where learnt from 4, 3x3 kernels. Does the max pool layer at #5 combine these 4 activation maps into a single one? Also, would it make sense to change #4 to use 8 or 16 kernels? This doesn't make sense to me because i've never seen a CNN example where the # of kernels changes from one layer to the next. It makes sense to me to change the # of kernels between #5 and #7 because the Max pool layer combines the separate activation maps into one. Any intuition on why/how this work?
