[site]: crossvalidated
[post_id]: 177209
[parent_id]: 
[tags]: 
What is a reasonable model measurement for a binary classification on an imbalanced data set? Accuracy? ROC?

We analyzed whether or not user comments contain customer needs or not. The comments are manually labeled by experts (comments with needs = 330, comments with no needs = 1600). We used different algorithms for classification (like Random Forests, Random Trees, Bayes Net, Naive Bayes) and different sampling methods (undersampling, oversampling, SMOTE) as well as 10-fold-cross-validation. We are very happy with the precision and recall of different combinations, but we are struggling with finding a good/suitable/reasonable measurement for comparing the results. So far we would just go with accuracy, but it doesn't take a lot of things into account. Should we go with a ROC Analysis? Or is there another reasonable measurement for our models?
