[site]: crossvalidated
[post_id]: 616773
[parent_id]: 
[tags]: 
How to perform statistics and machine learning on categorical, nominal data with large cardinality

I'm working with a dataset in which each row is an event with a number of categorical properties, most of which are also nominal. Each of these properties has around 5-10 possible values, but some columns can have more then 200. There is a column which denotes if an event had a positive, negative or neutral outcome and a column with a certain result code. I already made an analysis on a purely statistical basis of what values have larger or smaller chance to be positive or negative or have a certain result code. This works great when looking at a single property, but when combining them, there are too many categories and they only have a handful of data points in them, making finding statistically significant differences difficult to find. There are two things I want to do with this data. First, I want to see what combinations of parameters have the highest or lowest chance to have a positive result. For this, I'm not sure what to do as there are no numerical columns to use to calculate correlations etc. Summing over certain sets where combinations of parameters are the same (so all events where column A has the same value, column B has the same value etc), but I think that the sets would become too small as there is also a lot of missing data. I also want to use machine learning so the outcome of certain combinations of parameters can be estimated, but again, I'm not sure what to use because of the categorical data. I think one-hot encoding would result in too many features (300+ that are mostly 0) and since the features are nominal, a single column with different numbers would create a bias in the data. Any insights?
