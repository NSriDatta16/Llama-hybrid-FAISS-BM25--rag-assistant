[site]: crossvalidated
[post_id]: 535428
[parent_id]: 
[tags]: 
Why Accuracy increase only 1% after data augmentation NLP?

i have small dataset 4840 samples (60% negative ,28% positive,12% negative) i use data augmentation on training set (70%train 30% test) and i have about 2000 samples for each class while test is unbalanced (800 neutral ,400 positive ,200 negative). I use synonym replace from wordnet and insert words with contextual embedding takeed from nlpaug library. i use 10-folds cross validation the permonace doesn't increase very much . without data aug i have 86% accuracy 84 f1 score , with data aug 87% accuracy 85 f1 . p.s. I try different data augmentation the result are quite similar i have this value of train and val loss , I was expecting overfitting. The train and val loss value after 10 epochs val_loss = 0.22798433965072035 train_loss = 1.6610426288098097 what is the problem ?
