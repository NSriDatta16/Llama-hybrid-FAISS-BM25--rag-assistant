[site]: datascience
[post_id]: 76977
[parent_id]: 76444
[tags]: 
How about this? https://pypi.org/project/keras-self-attention/ I use it but I have not tried it without the LSTM layer yet. Also, you have to use keras directly, not tensorflow.keras. Here is a working model I am running right now. model = keras.models.Sequential() model.add(keras.layers.LSTM(cfg.LSTM, input_shape=(cfg.TIMESTEPS, cfg.FEATURES), return_sequences=True)) model.add(SeqSelfAttention(attention_activation='sigmoid')) model.add(keras.layers.Dense(cfg.DENSE)) model.add(keras.layers.Dense(OUTPUT, activation='sigmoid')) Also, look here, there is quite a bit https://awesomeopensource.com/projects/attention-mechanism
