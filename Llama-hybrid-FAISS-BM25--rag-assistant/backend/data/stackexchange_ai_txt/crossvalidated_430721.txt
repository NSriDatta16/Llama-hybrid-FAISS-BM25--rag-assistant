[site]: crossvalidated
[post_id]: 430721
[parent_id]: 
[tags]: 
Connection between Stochastic Neighbor Embedding and MDS

In the original SNE paper the authors mention a connection between the SNE objective function and an MDS-like stress function in the regime $\sigma_i \rightarrow \infty$ , as follows. When $\sigma_i^2$ is very large, it can be shown that SNE is equivalent to minimizing the mismatch between squared distances in the two spaces, provided all the squared distances from an object $i$ are first normalized by subtracting off their antigeometric mean $g_i^2$ : $$\textrm{Mismatch} = \sum_{ij} \big[(d_{ij}^2 - g_i^2) - (\hat{d}_{ij}^2 - \hat{g}_i^2) \big]^2$$ with $$d_{ij}^2 = \lVert x_i - x_j \rVert^2 / \sigma^2,\quad g_i^2 = -\log \sum_{k \ne i} \frac{\exp(-d_{ij}^2)}{n -1}$$ and $\hat{d}_{ij}^2$ and $\hat{g}_i^2$ defined similarly for the embedding points $\{y_i\}$ . My question is, how exactly is this mismatch function derived and what does it represent ? It seems strange because they say "when $\sigma_i^2$ is very large" so one might expect that they're talking about a limit, but then there's still a $\sigma$ in the expression obtained. Also, for $\sigma_i = \sigma$ constant, the expression inside the brackets is exactly $-\log(p_{ij}) + \log(q_{ij})$ and I don't see where the square could come from. What is going on here?
