[site]: datascience
[post_id]: 25293
[parent_id]: 
[tags]: 
Dropout without the averaging

The final step in dropout regularization is to multiply the weights by the dropout probability. This is motivated by analogy to bagging: averaging the weights of multiple nets. But it isn't truly that because of Jensen's inequality. The effect of the averaging is basically just to shrink the weights by some factor. Why average at all? Has anyone tried simply not averaging, and controlling the magnitude of the parameters with e.g. L2 regularization? One would still get the benefits of preventing codependence, but one would need to regularize some other way.
