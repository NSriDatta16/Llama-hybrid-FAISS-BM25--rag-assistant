[site]: crossvalidated
[post_id]: 130160
[parent_id]: 130122
[tags]: 
Since you used cross-validation, MSE is the error you want to look at because it's the error from your testing set, and approximation of the true testing error. If you use other algorithms with cross-validation, MSE is still appropriate. You would only want to use an adjustment to MSE--like AIC, BIC, adj. R^2--if you only ran your models on the training set without any sort of validation. For classification use the error rate, which is the average of $I(y_i \neq \hat{y_i})$. Source: "An Introduction to Statistical Learning" by Gareth James
