[site]: crossvalidated
[post_id]: 295567
[parent_id]: 
[tags]: 
Picking the right prediction model

I am having a hard time trying to make/pick a prediction model in R. The data: I have information on 40 different players, with all their recorded performances(training loads) over the last season. These instances are not evenly spaced over time, and each instance has 24 variables: 1 column with player-id's 1 column with the instance date 2 columns with binary variables: injured/not injured, and gets injured/not (the instance before the player is injured is the 'gets injured' instance). 4 measured numerical independent variables (including sprint and total distance), and 16 numerical variables that are made using the first 4. The goal: give a risk percentage that a player injured (possibly if the player exceeds distance x) in the next /match/event (i.e. a future, not yet recorded event). The problems: My suspicion is that the cumulative load on a player causes him to have an injury, so the model should take previous instances (load) of th√°t player into account, but not of other players. The resulting model should however be generalizeable and able to give a risk for a previously unsean player (if there is enough load information available on that player) Getting injured is a rare event. I was thinking towards a series of time series, but I do not know how to split/combine the different players with records on different dates in order to make the generalized model. I also read about sliding window, could that be a good solution? I have also tried the zelig package, but this does not result in any actionable outcomes (probabilities 0 or 1 occurred). Question: which predictive modelling technique takes this interrelateability into account yet allows for generalizeability in predicting rare events?
