[site]: stackoverflow
[post_id]: 4216725
[parent_id]: 4216108
[tags]: 
For a published book, that's pretty terrible code! (You can download all the examples for the book from here ; the relevant file is chapter4/nn.py .) No docstring. What is this function supposed to do? From its name, we can guess that it's generating one of the nodes in the "hidden layer" of a neural network, but what role do the wordids and urls play? Database query uses string substitution and so is vulnerable to SQL injection attacks (especially since this is something to do with web searching, so the wordids probably come from a user query and so may be untrustedâ€”but then, maybe they are ids rather than words so it's OK in practice but still a very bad habit to get into). Not using the expressive power of the database: if all you want to do is to determine if a key exists in the database then you probably want to use a SELECT EXISTS(...) rather than asking the database to send you a bunch of records which you're then going to ignore. Function does nothing if there was already a record with createkey . No error. Is that correct? Who can say? The weighting for the words is scaled to the numbers of words, but the weighting for the urls is the constant 0.1 (perhaps there are always 10 URLs, but it would be better style to scale by len(urls) here). I could go on and on, but I better not. Anyway, to answer your question, it looks as though this function is adding a database entry for a node in the hidden layer of a neural network . This neural network has, I think, words in the input layer, and URLs in the output layer. The idea of the application is to attempt to train a neural network to find good search results (URLs) based on the words in the query. See the function trainquery , which takes the arguments (wordids, urlids, selectedurl) . Presumably (since there's no docstring I have to guess) wordids were the words the user searched for, urlids are the URLs the search engine offered the user, and selectedurl is the one the user picked. The idea being to train the neural net to better predict which URLs users will pick, and so place those URLs higher in future search results. So the mysterious line of code is preventing nodes being created in the hidden layer with links to more than three nodes in the input layer. In the context of the search application this makes sense: there's no point in training up the network on queries that are too specialized, because these queries won't recur often enough for the training to be worth it.
