[site]: crossvalidated
[post_id]: 484918
[parent_id]: 
[tags]: 
Is it possible that accuracy testing and the accuracy training are 1.00 in binary classification

I am working to improve classification results with more ML algorithm. I get 100 percent accuracy in both test and training set. I used GradientBoostingClassifier, XGboost , RandomForest and Xgboost with GridSearchCV My daset shape is (222,70), for the 70 features i have 25 binary features and 44 continious features. My dataset looks like this: My code for Xgboost with GridSearchCV looks like this : from sklearn.model_selection import GridSearchCV import xgboost as xgb X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15,random_state=141) steps=[('scaler', StandardScaler()), ('XGB',xgb.XGBClassifier(objective= 'binary:logistic'))] from sklearn.pipeline import Pipeline pipeline = Pipeline(steps) # define the pipeline object. parameteres = {'XGB__learning_rate':[0.01,0.1, 0.25,0.5,0.7,0.8,1], 'XGB__n_estimators':[5,8,9,10,11,20,100], 'XGB__max_depth':[1,2,3,5,8,10,20]} grid = GridSearchCV(pipeline, param_grid=parameteres, cv=5,verbose=True) grid.fit(X_train, y_train) print ("score = %3.2f" %(grid.score(X_test,y_test))) print (grid.best_params_) The result looks like this : score = 1.00 {'XGB__learning_rate': 0.01, 'XGB__max_depth': 1, 'XGB__n_estimators': 5} And the result with GradientBoostingClassifier and RandomForest looks like this GradientBoostingClassifier ****Results**** Accuracy test: 100.0000% Accuracy trainning: 100.0000% Log Loss: 9.992007221626415e-16 confusion matrix matrix: [[18 0] [ 0 16]] RandomForestClassifier ****Results**** Accuracy test: 100.0000% Accuracy trainning: 100.0000% Log Loss: 9.992007221626415e-16 confusion matrix matrix: [[18 0] [ 0 16]] How to interpret this result ? It is possible that my classifiers are overfitting the training set ?
