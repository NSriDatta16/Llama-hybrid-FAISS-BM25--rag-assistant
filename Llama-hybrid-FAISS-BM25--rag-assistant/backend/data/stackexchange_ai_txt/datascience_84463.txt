[site]: datascience
[post_id]: 84463
[parent_id]: 
[tags]: 
What does this formula in Glorot & Bengio mean?

In this paper, on page 5, we find the formula $$Var(z^i)=Var(x)\prod_{i'=0}^{i-1}n_{i'}Var(W^{i'})$$ I am really struggling to understand what is meant by this formula. I think at least some of the following are true: We're dealing with a linear neural network, i.e. no activation functions. $z^i$ is the output of one layer of the network, so for the very first hidden layer (closest to the input) of this network we would have: $$Var(y)=nVar(x)Var(W)$$ where $y$ is the output vector of the first hidden layer, $x$ is the input vector and $W$ is the matrix of weights connecting that first layer to the input. However, it's entirely unclear to me what these variances are. I thought maybe $Var(y)$ just meant the empirical variance of the vector $y$ , i.e. the sum of the squares of the differences of the elements of $y$ from the mean of $y$ , and likewise for $Var(x)$ and $Var(W)$ , where the latter is just the variance of all of the entries of $W$ . But under this interpretation the formula turns out to be false numerically, so I'm at a bit of a loss to understand what this equation is supposed to mean.
