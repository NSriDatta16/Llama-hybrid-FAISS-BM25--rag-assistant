[site]: crossvalidated
[post_id]: 354258
[parent_id]: 353214
[tags]: 
I will try to answer the questions separately below but would also refer you to a question I have asked on the subject some years ago, where - I believe - the setting is similar to what you describe. I will intentionally not repeat what is covered in that thread. Which of these ways is used in practice or which of these makes the most sense? What are the requirements for training data for each of the above approaches (provided they both make sense in the first place). In brief, for the event detection setting I have tried both the multiple HMM and single HMM approaches and did not get very far. Conceptually HMM's are more suited for settings where the states themselves are latent / hidden and you do not actually have intuition of what they may / may not represent; how many states there are etc. Put more succinctly, an HMM is an unsupervised learning algorithm with a temporal dimension. In most cases it is just a Gaussian Mixture model with a state-transition component in the likelihood function. A very good reference on this is Pattern Recognition and Machine Learning by Chris Bishop, chapters 9 and 13. The Baum-Welch training uses only the observations and the state model is fitted to the observations, i.e. there is no clear interpretation of what the states actually represent. Therefore it cannot be used for the single HMM approach, am I correct? Baum-Welch is just the EM algorithm in this context, you use it to fit a single HMM, for multiple HMM's you would need to repeat the process. You can also try to fit a HMM (as you can any model) by using a stochastic optimisation process like MCMC, etc. In an HMM there is no clear representation of what the states represent not due to Baum-Welch but due to the unsupervised nature of the approach. In other words, you are just temporally clustering similar observations, taking also into account how they transition between themselves. Theoretically, there is no guarantee that the results of the clustering will align with your implicit labelling (event / no event). You can force it to do so by feature-engineering, but then you are trying to fit a solution to a problem it is not originally meant for IMHO. If I wanted to use the single-model approach (provided it makes sense at all) I would need to actually know the hidden states of the system for the training sequences, am I correct? If you knew the hidden states, they are not hidden. The very discussion of event/non-event resonates with supervised labelling. One commonly encountered issue in this context is the large imbalance in the number of event versus non-event observations. Events are usually rare occurrences (like in my question). If it is possible to label the training sequence(s) with the state (or what I think is the state), i.e. an in-action state and not-in-action state, or even action-is-starting, in-action, action-is-ending and non-action states, how would I train the single HMM? As mentioned above, form the moment you are talking about labelling, you would be better off, especially if your events are fixed lengths, by using a classifier with temporal features (e.g. going back x number of timesteps). Even more suitable are the weight sharing neural network algorithms like recurrent neural networks as you point out or convolutional neural networks (not just useful for image processing) with varying window/convolution sizes.
