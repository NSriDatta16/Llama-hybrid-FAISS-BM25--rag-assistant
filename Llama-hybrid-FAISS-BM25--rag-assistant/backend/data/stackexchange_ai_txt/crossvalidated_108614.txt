[site]: crossvalidated
[post_id]: 108614
[parent_id]: 
[tags]: 
Regression in $p>n$ setting: how to choose regularization method (Lasso, PLS, PCR, ridge)?

I am trying see whether to go for ridge regression , LASSO , principal component regression (PCR), or Partial Least Squares (PLS) in a situation where there are large number of variables / features ($p$) and smaller number of samples ($n This my understanding: Ridge regression shrinks the regression coefficients, but uses all coefficients without making them $0$. LASSO also shrinks the coefficients, but also makes them $0$, meaning that it can do variable selection too. Principal component regression truncates the components so that $p$ becomes less than $n$; it will discard $p-n$ components. Partial least square also constructs a set of linear combinations of the inputs for regression, but unlike PCR it uses $y$ (in addition to $X$) for dimensionality reduction. The main practical difference between PCR and PLS regression is that PCR often needs more components than PLS to achieve the same prediction error ( see here ). Consider the following dummy data (the actual data I am trying to work with is similar): #random population of 200 subjects with 1000 variables M Implementation of four methods: require(glmnet) # LASSO fit1=glmnet(M,y, family="gaussian", alpha=1) # Ridge fit1=glmnet(M,y, family="gaussian", alpha=0) # PLS require(pls) fit3 The best description of the data is: $p > n$, most of times $p>10n$; Variables ($X$ and $Y$) are correlated with each other with different degrees. My question is which strategy may be best for this situation? Why?
