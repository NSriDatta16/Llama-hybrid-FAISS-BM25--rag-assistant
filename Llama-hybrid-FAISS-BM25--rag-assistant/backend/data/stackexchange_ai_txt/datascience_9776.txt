[site]: datascience
[post_id]: 9776
[parent_id]: 9769
[tags]: 
I don't think that your assumption $w' = w^T$ holds. Or rather is not necessary, and if it is done, it is not in order to somehow automatically reverse the calculation to create the hidden layer features. It is not possible to reverse the compression in general, going from n to smaller m, directly in this way. If that was the goal, then you would want a form of matrix inversion, not simple transpose. Instead we just want $w_{ij}$ for the compressed higher-level feature representation, and will discard $w'_{ij}$ after auto-encoder is finished. You can set $w' = w^T$ and tie the weights. This can help with regularisation - helping the autoencoder generalise. But it is not necessary. For the autoencoder to function it doesn't actually matter what activation function you use after the layer that you are pre-training, provided the last layer of the autoencoder can express the range of possible inputs. However, you may get varying quality of results depending on what you use, as normal for a neural network. It is quite reasonable to use the same activation function that you are building the pre-trained layer for, as it is the simplest choice. Using an inverse function is possible too, but not advisable for sigmoid or tanh, because e.g. arctanh is not defined 1, so likely would not be numerically stable.
