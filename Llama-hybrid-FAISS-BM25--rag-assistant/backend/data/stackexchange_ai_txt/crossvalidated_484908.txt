[site]: crossvalidated
[post_id]: 484908
[parent_id]: 
[tags]: 
Deriving simple linear regression

So we're looking for the coefficients $\beta_0, \beta_1$ such that we minimize $\sum_{i=1}^n\epsilon_i^2$ in $$Y_i = \beta_0 + \beta_1X_i + \epsilon_i$$ Meaning, $\beta_0$ and $\beta_1$ such that $$\sum_{i=1}^n\epsilon_i^2 = \sum_{i=1}^n\big(Y_i - \beta_0 - \beta_1X_i \big)^2$$ is minimal. We do that by solving a system of equations that we get by equating the partial derivatives with zero: $$\frac{d}{d\beta_0}\sum_{i=1}^n\big(Y_i - \beta_0 - \beta_1X_i \big)^2 = 0$$ $$\frac{d}{d\beta_1}\sum_{i=1}^n\big(Y_i - \beta_0 - \beta_1X_i \big)^2 = 0$$ This gives us two equations: $$-2\sum_{i=1}^n\big(Y_i-\beta_1X_i - \beta_0\big) = 0$$ $$-2\sum_{i=1}^nX_i\big(Y_i-\beta_1X_i - \beta_0\big) = 0$$ which further reduce to $$\sum_{i=1}^nY_i - \beta_1\sum_{i=1}^nX_i - n\beta_0 = 0$$ $$\sum_{i=1}^nX_iY_i - \beta_1\sum_{i=1}^nX_i^2 - \beta_0\sum_{i=1}^nX_i = 0$$ And I have managed to solve this system myself and I got the following solution for $\beta_1$ : $$\beta_1 = \frac{\sum_{i=1}^nX_iY_i - n\bar{X_n}\bar{Y_n}}{\sum_{i=1}^nX_i^2 - n\bar{X_n}^2}$$ This solution matches with the solution given in my textbook. However, my textbook also adds one more equality for which it gives no explanation. So the textbook version of the final solution is this: $$\beta_1 = \frac{\sum_{i=1}^nX_iY_i - n\bar{X_n}\bar{Y_n}}{\sum_{i=1}^nX_i^2 - n\bar{X_n}^2} = \frac{\sum_{i=1}^n(X_i-\bar{X_n})(Y_i-\bar{Y_n})}{\sum_{i=1}^n(X_i-\bar{X_n})^2}$$ At first I thought, since they showed no work, it must be very simple to derive the RHS from the LHS. It may be. But I've tried it for a while now and I just can't seem to do it. Tried rewriting the sample means as averages and all kinds of stuff but I either get right back where I started or the expression gets too complicated so I have to start all over again. Any ideas? Thanks.
