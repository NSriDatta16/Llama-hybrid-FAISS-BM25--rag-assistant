[site]: crossvalidated
[post_id]: 423480
[parent_id]: 
[tags]: 
Is this a valid non-parametric approach for computing false discovery rate (FDR)?

Summarizing the method in the reference below as I understand it: For each of $N$ "gene sets" (where a gene set represents a single null hypothesis significance test), compute a test statistic $T_i$ . For each of $N$ gene sets, also compute $K$ random test statistics $R_k$ using $K$ random permutations of the data labels. For each test statistic $T_i$ , compute the proportion of instances where $R \geq T_i$ (across all $N * K$ permutations). $R$ is a vector of the random test statistics. For each test statistic $T_i$ , compute the proportion of gene sets where $T \geq T_i$ . $T$ is a vector of the observed test statistics Finally, compute the FDR by dividing the vector of proportions from step 3 by the vector of proportions from step 4. Could someone kindly explain why this would be a valid approach for computing the FDR given that the quotient of two proportions does not necessarily have an upper bound of 1. Am I missing something? I would also would appreciate any help in understanding the reasoning behind this approach: why does the quotient from step 5 correspond to the FDR? References: Subramanian, A., Tamayo, P., Mootha, V. K., Mukherjee, S., Ebert, B. L., Gillette, M. A., ... & Mesirov, J. P. (2005). Gene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles. Proceedings of the National Academy of Sciences, 102 (43), 15545-15550.
