[site]: crossvalidated
[post_id]: 502214
[parent_id]: 
[tags]: 
RNN Loss in Sentiment Analysis

I am currently reading on RNNs and Backprop through Time. With MLPs using SGD, we did Backprop after every training sample. With RNNs, one method to avoid exploding gradients is to cut an input sample into several samples and do BPTT for each of these "cutted" samples. I've read about BPTT and RNNs in the Dive into Deep Learning book and my lecture, both of which used text prediction/generation as an example use case. The total loss is the mean loss for all timesteps in a input sequence. For text generation, that makes sense to me because we can validate whether the next predicted word by the model corresponds to the word in out input sequence. However, I do not understand how this metholodgy applies to other use cases, e.g. sentiment analysis. Consider movie reviews which are of different length. We do not want to predict a next word, instead we want to predict positive or negative. In BPTT, we would for each timestep (input word) calculate the loss. However, as long as the RNN does not change its mind within the sequence, we will just sum up the same output over and over again. Is that how it really works in this use case? I do not understand why we want the output per timestep and not just the finaly output, in this case, on the other hand, we need this time-dependent view in RNNs. Any help on understanding the loss function in contexts other than next-word prediction is much appreciated. Thank you!
