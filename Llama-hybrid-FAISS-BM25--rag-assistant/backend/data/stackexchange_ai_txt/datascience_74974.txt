[site]: datascience
[post_id]: 74974
[parent_id]: 32839
[tags]: 
Finally, I find time to answer this question whose answer was found in a well-known online course provided by Pr. Boyd for convex optimisation . In that course, he refers to applications of optimisation. One of its applications is penalty function approximation. As a brief answer, just define your penalty for the parameters you want and add it to the cost function, it will be like multi-objective optimisation which should be optimised using scalarization. For simplicity, consider what we have in l1/l2 regularisation methods for avoiding overfitting. Simply, add this to the current cost function. But, consider your cost function to be smooth. For instance, I've added the example he has provided. As you can see, the log-barrier limits the range of errors for a particular variable, and values more/less than one will be penalised significantly while inputs in the range $(-1, 1)$ won't. As you can see, there are other methods too. In today's usual neural networks, people employe l2 norm for tasks like classification to penalise mislabeled items..
