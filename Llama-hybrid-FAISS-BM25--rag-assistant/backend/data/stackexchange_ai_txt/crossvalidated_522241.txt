[site]: crossvalidated
[post_id]: 522241
[parent_id]: 
[tags]: 
Is an overfitted model likely to get better with more data?

I've been dealing with some regularization techniques for preventing overfitting in neural networks. I'm wondering: Is an overfitted model without modifying anything likely to get better with more data? If I started with a capable model that had been generated using a fixed dataset, and using optimization to adjusted the parameters so that it was fitting both the generalizable form and the noise, aka overfitted, then I increased the training set size significantly, and then iterated the same optimizer over the model to make it fit the larger set, would it recover or improve its ability to generalize? Bonus: To continue the thought, can overfitting be thought of as either too little data or too much complexity of the model? How should I think about this?
