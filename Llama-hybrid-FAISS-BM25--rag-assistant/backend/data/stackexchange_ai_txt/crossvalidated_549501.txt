[site]: crossvalidated
[post_id]: 549501
[parent_id]: 
[tags]: 
Statistics question: Why is the standard error, which is calculated from 1 sample, a good approximation for the spread of many hypothetical means?

I'm re-learning statistics and got confused by the idea of taking the standard error from 1 sample's worth of data (standard deviation of sample divided by the square root of sample size, i.e. $\frac{s}{\sqrt{n}}$ ). As I understand it, the standard error is the spread of many sample means in an attempt to gauge how precise (not accurate) our estimate of the population mean is, but what if there's just the one sample? Usually experiments can't or just aren't repeated and only have 1 sample from a population, no? So, how can I pretend to know the spread of many means from samples of a population that I only did hypothetically? If this is an example of a frequentist's versus a Bayesian approach to statistics, I'd just like to learn the frequentists side of the argument (just to understand what it is), please. Thanks in advance!
