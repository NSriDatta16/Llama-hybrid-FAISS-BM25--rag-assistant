[site]: crossvalidated
[post_id]: 519231
[parent_id]: 519219
[tags]: 
There are some different mindsets between classical statistics and machine learning, where modern machine learning can have huge amount of data. And as you said, the whole problem is an "optimization problem", where people put little or no assumptions to the model. And the golden standard is the performance on a large testing set. If we go back and review all the nice properties (confidence interval, p value, etc.) they are coming from strong assumptions about the data and model. With less data, people are willing to do more work in math (say Gaussâ€“Markov theorem for OLS) to make the results nice and solid. But modern machine learning people are more practical and do not care too much about the assumption and statistically significant, but focusing on the model will "work" on large testing data. Note deep neural network can have billions of parameters and the number of parameters can even much bigger than number of data points. (which may not really acceptable from classical statistics perspective) So, AIC and BIC also not working there.
