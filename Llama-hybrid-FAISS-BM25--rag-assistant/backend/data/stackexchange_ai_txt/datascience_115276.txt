[site]: datascience
[post_id]: 115276
[parent_id]: 
[tags]: 
CNN model why is ReLu used in Conv1D layer and in the first Dense Layer?

I have a problem. I have a CNN model which is used for an NLP problem. This is written in Python. I have questions about this, which I can't find an answer to. Why is ReLu used inside the Conv1D layer and not Softmax ? Why is ReLu used again as activation function in the first Dense-Layer and why Softmax afterwards ? model1 = Sequential() model1.add( Embedding(vocab_size ,embed_size ,weights = [embedding_matrix] #Supplied embedding matrix created from glove ,input_length = maxlen ,trainable=False) ) model1.add(Conv1D(256, 7, activation="relu")) model1.add(MaxPooling1D()) model1.add(Conv1D(128, 5, activation="relu")) model1.add(MaxPooling1D()) model1.add(GlobalMaxPooling1D()) model1.add(Dense(128, activation="relu")) model1.add(Dense(number, activation='softmax')) print(model1.summary()) ```
