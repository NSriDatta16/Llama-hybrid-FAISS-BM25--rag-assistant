[site]: crossvalidated
[post_id]: 262678
[parent_id]: 
[tags]: 
ReLU derivative - second order effects

I am reading the Deep Learning Book, where there is a section on generalisations of the ReLU (section 6.3.1). It states: The second derivative of the rectifying operation is 0 almost everywhere, and the derivative of the rectifying operation is 1 everywhere that the unit is active. This means that the gradient direction is far more useful for learning than it would be with activation functions that introduce second-order eï¬€ects. I then read a short and simple paper about second order effects, which confirmed my memories that second order effects can help to optimisation to run more efficiently. I was then wondering why the book so sweepingly said that the gradient direction of activation functions without second order effects are more useful than those with second order effects? Is it that the first order gradient (of the ReLU in this case) must therefore encode all gradient information, and so are alone as efficient as can be? The fact that the ReLU's second order derivative is zero of course means that gradient information is stored solely in it's first derivative, given that the second order's zero value doesn't aid gradient descent calculations, as zero terms cannot alter the weights. Can anybody explain the authors' thoughts here, or perhaps recommend any literature regarding the second order effects of activation functions?
