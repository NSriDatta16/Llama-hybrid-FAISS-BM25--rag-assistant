[site]: crossvalidated
[post_id]: 517189
[parent_id]: 471489
[tags]: 
I'm a scientist, not a statistician, but given that it has been almost a year since this question was first asked and it still has not received any answers, I will do my best to provide some insight. Revisions/corrections to my answer are welcome as I have also spent considerable time and effort trying to figure out this paper. I'll mention here that I too have been pleased by this method's performance and will encourage anyone reading this answer to give it a shot if you have not already. To start from the top: importance sampling is useful when you are interested in a probability distribution $f(x)$ but are unable to sample from it using MCMC due to computational and/or practical constraints. Fortunately, you can instead sample from an approximating distribution $g(x)$ and use those samples to make inferences about $f(x)$ . To do so, simply adjust the weight of each sample to $f(x)/g(x)$ . In principle, this adjustment corrects both 1) the overrepresentation of samples that are more prevalent in the sampling distribution $g(x)$ than in the target distribution $f(x)$ , and 2) the underrepresentation of samples that are rare in $g(x)$ but common in $f(x)$ . The problems that are commonly described about importance sampling arise from this second set of samples. Depending on the overlap between the sampling distribution and the target distribution, cases that are rare in $g(x)$ but common in $f(x)$ will have huge importance weights and will strongly influence any inferences you make about the composition of the target distribution $f(x)$ . Often, the tails of the resulting distribution are erratic. Based on my understanding, the primary contribution of the PSIS paper is to observe that these weights follow the Pareto principle, sometimes called the "80/20 rule" (although the numbers in this case are arbitrary). Therefore, rather than take these weights at face value, the authors demonstrate that you can fit them to a generalized Pareto distribution, thus smoothing over their values and removing many of these issues observed with importance sampling in general. So what does this have to do with leave-one-out cross-validation (LOO-CV)? To recap LOO-CV, you are typically interested in gathering uncertainty statistics about your model by iterating through your dataset, removing a data point, and repeating the analysis (e.g. MCMC sampling). Most of us would describe the tediousness of this process, namely the resampling step, as a "computational and/or practical constraint", and the authors of this paper appear to agree. Their solution is to skip resampling entirely and instead use PSIS to reweigh the samples obtained the first time with the complete dataset, thus saving considerable amounts of time while still providing the uncertainty statistics of interest. As they demonstrate, this provides results that are nearly identical to classical LOO-CV, but with a far lower computational footprint. To summarize PSIS-LOO: Use the complete data set to generate a large pool of samples from the sampling distribution $g(x)$ . Iterate through each data point and remove it from the data set. Use Pareto-smoothed importance sampling to reweigh the samples obtained in step one to this new target distribution $f(x)$ . After repeating for the full dataset, compute the parameter statistics using LOO-CV as you previously would. Again, if you are a statistician reading this and you are thinking "this answer is wrong", then I would appreciate any corrections, revisions, or addendums.
