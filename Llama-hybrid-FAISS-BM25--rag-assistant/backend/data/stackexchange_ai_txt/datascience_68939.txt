[site]: datascience
[post_id]: 68939
[parent_id]: 68927
[tags]: 
Yes, you are right that when number of observations are very large, k fold cross validation (CV) are less useful. Let's look at why this is so: 1) Very high number of observations imply high training time for model and validation. Already the number of observations is large for the model to be trained and validated and now we are demanding it to be done k times. This is a huge burden on resources that is why in the Deep Learning regime we don't generally follow k fold CV as the data needed for training good neural networks are very high compared to traditional ML algorithms. 2) The higher the number of observations, the higher is the number of data chosen for cross validation set. This inherently makes it less likely that the sampled data points do not represent the original distribution. As you would know, the more data we sample, the better we approximate the original distribution. Because of these reasons, k fold CV is inefficient when the number of observations is very high so a hold out set for CV will do the job.
