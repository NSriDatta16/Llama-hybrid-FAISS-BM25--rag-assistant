[site]: crossvalidated
[post_id]: 341868
[parent_id]: 
[tags]: 
Confusions about how to choose an action in deterministic policy gradient

I read "Continuous Control with Deep Reinforcement Learning". it says The DPG algorithm maintains a parameterized actor function μ(s|θμ) which specifies the current policy by deterministically mapping states to a specific action. I thought DPG simply chose the action with maximum $\mu(s|\theta_\mu)$(with noise for exploration). But when I read "Deterministic Policy Gradient Algorithms", it seems to say a different story: To ensure that our deterministic policy gradient algorithms continue to explore satisfactorily, we introduce an off-policy learning algorithm. The basic idea is to choose actions according to a stochastic behavior policy (to ensure adequate exploration), but to learn about a deterministic target policy (exploiting the efficiency of the deterministic policy gradient) According to my understanding, it says to choose an action as the stochastic policy gradient, but to learn the policy parameters using $\nabla_\theta Q(s,a)$ I'm not sure whether I've misunderstood something, but now I get confused -- how does DPG select an action exactly? or both above cases work fine? Thanks in advance.
