[site]: crossvalidated
[post_id]: 433378
[parent_id]: 433373
[tags]: 
The only real answer is - whenever you want to, as long as you can justify it. A reasonable case can be made for not adjusting at all. A reasonable case can be made for doing different things in different cases. This is more generally true of statistical analysis than most people think (see Statistics as Principled Argument by Robert Abelson for a lot more First, recognize that 5% is arbitrary. Why not 1%? Why not 10%? Why is the usual power set at 80% or 90% instead of 95% or 99%? Second, taking a tiny bit of a Bayesian view, how sure are we that the null is false? Except for some categorical tests with small populations, the null is never exactly true. If you (say) compare the heights of people with even and odd social security numbers and somehow get data for the entire population of the USA, the means will be different. Maybe in the 4th or 5th decimal place, but they won't be identical. But, in most research, we are pretty sure that the null isn't even approximately true. The whole structure is an unfortunate legacy of Fisher's situation: He was testing different treatments of plots of land at Rothamstead. In this case, the notion that the null might be approximately true was sensible. Is treatment A better than treatment B? He had no real idea - so, he tested. But, often, we do know. If we are testing a medical treatment, for instance, we can be pretty sure it has some effect. The more certain we are, a priori that the null is not close to true, the less inclined we should be to adjust p values. Also, if recruiting more subjects and doing the measurements is expensive, we may think twice about adjusting. But if we have "big data" then it's no problem. Third, remember that decreasing type I error will either increase type II error or require a bigger sample. Is a type I error worse than a type II error? The usual choices of 5% and 20% indicate that we think so. But sometimes type II error is much much worse. Suppose we have a drug that we think might reverse some disease which is terminal and currently untreatable. Then a type I error means giving an ineffective drug to a dying person while a type II error means letting someone die who could be cured. Finally, remember that the whole significance testing apparatus is increasingly criticized, with varying degrees of severity. I'm a pretty harsh critic (you may have figured that out by now!) but who am I? Just some guy on Cross Validated. But the American Statistical Association has (to a much lesser degree) joined the critics.
