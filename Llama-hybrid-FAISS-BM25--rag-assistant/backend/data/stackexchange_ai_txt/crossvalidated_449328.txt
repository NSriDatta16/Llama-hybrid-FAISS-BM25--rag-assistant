[site]: crossvalidated
[post_id]: 449328
[parent_id]: 449322
[tags]: 
This is not a full answer. In the question I mentioned one way how I can imagine a small bias being amplified. In this question I write it down in more detail. I consider this way of amplification a bit trivial and wonder whether there are more reasons why Amazon's recruitment tool was considered biased, especially why it was biased because AI was being used and amplified the bias in the data. Say that some model is fitted to predict a probability that a 'candidate in a particular class' is being hired/successful or not (like logistic regression). In these probabilities there might be 'only' a slight bias against women due to being based on biased history of hiring. We could plot this as a distribution by ranking the classes that are occurring in the population according to how high the probability of success is and as a function of this probability describe how many people are in a related class. Below this is done for men and women separately with hypothetical beta distributions (parameters 3,3 and 2.75,3.25) and for these women have a slight disadvantage and, on average, have a ~8% less probability to be hired. Sidenote: these distributions might in practice/reality be more like discrete distributions The big problem with these numbers occurs when some classification is being made in the tails. E.g. if this model would be used in proactive recruitment and is used to scan profiles on large databases of potential recruits. For such purpose many candidates are being scanned and we do not want a large false positive error rate, so say that we set a boundary at 0.9 probability that a recruit would be successful. In that case we see that suddenly there are double as many male candidates as female candidates. Is there something else? For me the problems in this way of recruitment are not in the inner mechanics of some algorithm and some black box algorithm that we do not understand that is making magical mistakes and wrongly selecting candidates because it is misinterpreting words like "woman's chess club" (the estimates of probability might be very accurate, so no mistake there) or can't handle unbalanced data . But instead it is due to the dichotomous classification which only selected people from the tails. That is a human choice and not intrinsic to the fact of using an algorithm or human. When Amazon is replacing the recruitment tool by humans then it might just again become biased (or even more biased). Among high potential recruits a human might just as well place mostly men (or at least that appears to be what they did in history). It are the human recruiters that, for certain given properties (that relate to a high probability success), bias men in favor of women. This bias has not been amplified, it is only filtered out by the strict choice of only recruiting high potential candidates. Are there other modes that amplify bias? Edit, interaction with unbalanced data and the effect of selecting tails After writing this answer, I realize now that there is an interaction in this 'comparison of tails' and 'unbalanced data'. In the example above I use two distributions with different means, but it would also occur if the distributions have different variance (or it will especially occur in those cases and the means don't need to differ at all). This difference in variation could occur when some machine learning algorithm is trained on mostly male resume's which makes it very good at classifying men (e.g. relating mostly to activities that correlate with being men like certain sports activities or membership of gender specific clubs). Such training on men is creating a large variance in the predicted probabilities (the model is good at classifying men), and being able to assign to men more often extreme probability categories. Then the scores that are generated by the model could look like: On average, the model/algorithm does not assign a different probability of recruitment success to women, but it might place women more often in the middle categories which might be less interesting to recruiters.
