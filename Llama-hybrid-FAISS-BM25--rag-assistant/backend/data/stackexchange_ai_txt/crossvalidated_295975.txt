[site]: crossvalidated
[post_id]: 295975
[parent_id]: 294489
[tags]: 
Let's sketch your problem as: $$ f(\{X_t: t \leq T \}) = X_{T+1} \tag{1}$$ that is, you are trying to machine learn a function $f(x)$. Your feature set is all the data available until $T$. In a somehow overloaded notation I wanted to highlight the fact that if we look at $X$ as a stochastic process, it'd be convenient to impose that $X$ is adapted to a filtration ( an increasing stream of information ) - I'm mentioning filtrations here for completeness sake. We can also look at equation $1$ as trying to estimate ( here ): $$ E[X_{T+1} | X_T, X_{T-1}, ..] = f(\{X_t: t \leq T \}) $$ In the simplest case that pops in my head - the OLS linear regression - we have: $$ E[X_{T+1} | X_T, X_{T-1}, ..] = Xb + e $$ I am suggesting this line of thought to bridge statistical learning and classic econometrics. I am doing so because, no matter how you estimate (linear regression, random forest, GBMs, ..) $ E[X_{T+1} | X_T, X_{T-1}, ..]$, you will have to deal with the stationarity of your process X, that is: how $ E[X_{T+1} | X_T, X_{T-1}, ..]$ behaves in time. There are multiple definitions of stationarity that try to give us a flavor of time-homegeneity of your stochastic process, i.e how the mean and variance of the estimator of your expected value behave as you increase the forecasting horizon. In the worst case scenario, where there is no sort of homogeneity, every {X_t} is drawn from a different random variable. Best case scenario, iid. We are in-between the worst and best case scenario: autocorrelation impacts the type of stationarity a stochastic process displays: the autocovariance function $\gamma(h)$, where $h$ is the time gap between two measurements, characterizes weakly stationary processes. The autocorrelation function is the scale-independent version of the autocovariance function ( source , source ) If the mean function m(t) is constant and the covariance function r(s,t) is everywhere finite, and depends only on the time difference τ = t − s, the process {X(t), t ∈ T} is called weakly stationary, or covariance stationary ( source ) The weakly stationary framework should guide you on how to treat your data. The key takeaway is that you can not put auto-correlation under the rug - you have to deal with it: You increase the granularity of the time mesh: You throw away data-points (less granularity and a lot less data to train your model) but auto-correlation is still biting you on the dispersion of $E[X_{T+1} | X_T, X_{T-1}, ..]$ and you'll see lot of variance in your cross-validation You increase the granularity of the time mesh: sampling, chunking and cross-validation are all much more complex. From a model point of view, you'll have to deal with auto-correlation explicitly.
