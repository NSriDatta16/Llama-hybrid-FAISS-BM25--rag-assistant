[site]: crossvalidated
[post_id]: 324258
[parent_id]: 
[tags]: 
Understanding decision boundary of logistic regression

I am currently attempting to code out a simple example of logistic regression to better understand the concept behind it. I generated the following classification example: The code I use to optimise my decision boundary is the gradient descent algorithm which I implemented like this: def gradient_descent_iterate(alpha, x, y, ep, max_iter): # Set-up of variables converged = False iter = 0 m = x.shape[0] t0 = np.random.random(x.shape[1]) t1 = np.random.random(x.shape[1]) # Calculate the cost J = -(1/m)*sum([(-y[i]*log(1/(1+exp(t0 + t1*x[i]))) + (1-y[i])*log(1-(1/(1+exp(t0 + t1*x[i]))))) for i in range(m)]) # Calculate and update the gradient while(not converged): # for each iteration, compute the gradient grad0 = (1/m) * sum([(t0 + t1*x[i] - y[i]) for i in range(m)]) grad1 = (1/m) * sum([(t0 + t1*x[i] - y[i])*x[i] for i in range(m)]) # Save to temp temp0 = t0 - alpha*grad0 temp1 = t1 - alpha*grad1 # Update theta t0 = temp0 t1 = temp1 #Calculate MSE in each iteration e = (1/2*m)*sum([(t0 + t1*x[i] - y[i])**2 for i in range(m)]) #Check for early convergence if(abs(J-e) Which outputs soon after: Optimizer has converged, iterations 200 ! Theta0 = [ 0.46931187] , Theta1 = [ 0.44905758] My biggest problem is now to understand what exactly this tells me. I understood that the decision boundary is used to return h(x)>0 or h(x) 0.46931187 + 0.44905758 * -0.4708 = 0.2 which is not Nevertheless, when plotting I still get the following classifier, which looks okay to me:
