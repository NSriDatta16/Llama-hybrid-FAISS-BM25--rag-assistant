[site]: datascience
[post_id]: 120428
[parent_id]: 
[tags]: 
Does double cross-validation make sense?

We do the following: split data_all into K folds, each consisting of data_train_k and data_test_k where k = 0, ... K-1. for each k in 0, ... K-1, split data_train_k into M folds each consisting of data_train_k,m and data_eval_k,m where m = 0, ... M-1. Then perform hyperparameter tuning via cross-validation using the M folds. We validate the optimized model using data_test_k . Now we have K tuned models with cross-validated metric, like: k | avg eval score on M folds | std of score on M folds | test score 0 | 0.92 | 0.05 | 0.93 ... K-1|0.89 | 0.03 | 0.88 compute average test_score - this is $\hat S$ , the final estimate of test score. Is it a proper solution? Can we use it to compare different model architectures? And finally, is the estimate unbiased?
