[site]: crossvalidated
[post_id]: 576502
[parent_id]: 
[tags]: 
Is "vanilla" random forest regression appropriate when dependent variable is a rate (i.e. per 100 people)?

To add a little more context, I am working with a dataset from which I want to predict the population-normalized count of emergency department visits on county level, with 50+ independent variables. The dataset itself contains raw counts of the ED visits, as well as the county populations, so I have a bit of freedom in terms of how I perform the normalization. I am really struggling to determine an appropriate methodology to analyze this now, however. I know with more traditional generalized linear models, careful attention needs to be given to the distribution of the dependent variable. I assume the best way to do this in a general circumstance would be a Poisson regression with an offset, though I do not know how this translates to tree-based ML methodologies, which I am specifically interested in. With this in mind, would a "vanilla" random forest regressor (or gradient boosting, alternatively) be appropriate to use here without giving these details too much thought? I have done some digging online for an answer to this, with mixed results. I've seen that sklearn has different criterion parameters - such as Poisson - when implementing an RF, but I don't know if that is really what I need. Would regular squared error still work? Also, as a side note, part of my analysis intends to assess a random forest/gradient boosting methodology, specifically, for this application - which is why I would prefer not to use a different type of model if I can get away with it.
