[site]: datascience
[post_id]: 91221
[parent_id]: 91209
[tags]: 
When layers are "frozen" it generally means that their weights are not updated when backpropagation happens. So, technically, there is no difference between: Using a "frozen" VGG16 and training some fully connected layers and Using the VGG16 embeddings and training some fully connected layers In practice, if you did both and compared the results, you may see differences if you haven't accounted for the fully connected layers' random initialisation.
