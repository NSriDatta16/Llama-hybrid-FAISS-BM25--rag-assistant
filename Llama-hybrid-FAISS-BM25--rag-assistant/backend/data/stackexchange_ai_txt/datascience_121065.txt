[site]: datascience
[post_id]: 121065
[parent_id]: 97200
[tags]: 
From your description you represent your [2x2] image as a vector of dimension 4 and then you compute binary cross entropy between y_pred and y_true. First of all, cross entropy compares distribution $q$ relative to $p$ , where $q$ is often used as the estimate and $p$ as ground truth in machine learning. $$ H(p, q) = -\text{E}_{p}[\log q] \geqslant H(p). $$ In this sense, $p,q$ are distributions and we don't need them to be binary. However, notice that the minimum value of cross entropy is the entropy of the ground truth. So, in your application, if you do not have binary target , you won't get zero BinaryCrossentropy even if your y_pred is identical to your y_true. Furthermore, your vectors represent the [2x2] image do not sum up to 1, so the above minimum cannot apply. Why usual binary cross entropy has minimum $0$ ? The answer is that BinaryCrossentropy forces the ground truth $p$ to be binary, i.e. 0 or 1. In this case, the entropy $H(p)$ is zero so the minimum of $H(p,q)$ is zero. In summary, answering your question, you didn't apply cross entropy function correctly as you apply it to two vectors rather than two probability distributions. if you do not have binary target values in your application, normalize the vector to get probability distributions and then apply cross entropy. However, in this case you will not get 0 even if your y_pred is completely the same as your y_true.
