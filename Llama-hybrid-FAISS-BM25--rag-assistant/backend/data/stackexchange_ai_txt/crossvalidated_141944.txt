[site]: crossvalidated
[post_id]: 141944
[parent_id]: 141870
[tags]: 
Toni, It has always been my impression that the theoretical backing of basic inferential statistics is typically the central limit theorem, which motivates both the mean and variance calculations you suggested, not necessarily by deeming them the most likely, but by arguments of the asymptotic correctness of these approximations. MLE is really designed to answer a slightly different question: given a collection of data, and a set of (parametrized) assumptions, what configuration of the parameters maximizes the likelihood of my data? Of course, it is no coincidence that these values agree with the ones above, given that we expect them to agree asymptotically. As for Bayesian Inference, I differentiate this once again by the nature of the question being asked. Here we no longer ask, which configuration of the parameters makes our data most likely, but rather, which configuration of the parameters is most likely. One can intuitively, or symbolically, arrive quickly to the fact that we don't have enough assumptions to answer this question. For this reason a prior distribution on the parameters is necessary. (A common alternate view of Bayesian inference begins with the prior, simply trying to incorporate prior beliefs on statistical inference in a measured way). All in all, if I could only have one index on a canon of mathematical theory, I would index by the question being asked/answered. Answers are pretty useless without their questions, and when you go to apply this stuff, you'll typically start with a question. Sorry for the vague answer, it was just a very overarching question, but I think that the organization of one's understanding of a subject is incredibly important.
