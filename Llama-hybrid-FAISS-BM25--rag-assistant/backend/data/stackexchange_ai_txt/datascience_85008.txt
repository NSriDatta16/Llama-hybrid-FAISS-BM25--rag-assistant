[site]: datascience
[post_id]: 85008
[parent_id]: 
[tags]: 
Model inclines because of imbalanced data

In life, some events are rare and most cases are normal. So I am wondering, to detect the rare cases, shall we use an imbalanced dataset with more historical rare cases? Taking the German Credit Data, as an example. It contains data on 20 variables and the classification whether an applicant is considered a Good or a Bad credit risk for 1000 loan applicants. 70% are Good. 30% Bad. With this original data set, I assume the model would incline to better recognize the normal cases (because more normal cases in the data) If a balanced dataset is used, i.e. the number of good credits equals to that of bad credits, a final model will be good at predicting both ‘Good’ and ‘Bad’. But if we want to use machine learning to recognize the rare events, e.g. in this case, the bad credit customers. Shall we use an imbalanced dataset (for example 70% Bad credits, 30% Good in total 1000 records), that contains much more bad credit customers, than good ones, so the final model is good at recognizing the bad customers? Or a balanced dataset is always necessary (also the only right way). Can anybody please shed some light on this? Thank you. Link to data: https://online.stat.psu.edu/stat857/sites/onlinecourses.science.psu.edu.stat857/files/german_credit/index.csv
