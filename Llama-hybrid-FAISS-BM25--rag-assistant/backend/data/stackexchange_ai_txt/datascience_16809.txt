[site]: datascience
[post_id]: 16809
[parent_id]: 16800
[tags]: 
Picking up answer from Ricardo Cruz : The reason is that DecisionTreeClassifier is not a deterministic classifier. Specifically, when splitting between two features that lead to the same decrease of the metric, DecisionTreeClassifier picks one at random. The fluctuations observed in the question are caused by this. The corollary of this observation is that scikit-learn's Random Forest uses bootstrap for 3 different things: items (samples different items) features (samples different features) splitting (samples different splittings) Freezing items (using bootstrap=False ) and features (using max_depth=None ) is not sufficient to freeze splitting . The only way to freeze splitting is by using a fixed random_state . Notice that using the same random_state alone is not enough to freeze everything, i.e. foo10 = RandomForestClassifier(10, random_state=1) foo100 = RandomForestClassifier(100, random_state=1) foo200 = RandomForestClassifier(200, random_state=1) do not give the same score. Thus, the only way to guarantee that the classifier gives the same results for any n_estimators is to initialize it as RandomForestClassifier(n_estimators, max_features=None, bootstrap=False, random_state=1) This is not particularly helpful for classification per se, but allows to better understand what is being done under the hood.
