[site]: crossvalidated
[post_id]: 634777
[parent_id]: 
[tags]: 
How do you count degrees of freedom in multivariate distributions?

When dealing with a multivariate distribution (e.g., a multivariate t-distribution), how does dimensionality play into degrees of freedom? Let's say you have N measurements of a d-dimensional vector, assume that we have a well-behaved (non-singular) shape parameter matrix that we are estimating from the data,and we want to use a multivariate t-distribution to understand the uncertainty in our estimate of the mean. Is df simply N-1, as in the univariate case? Or is each component of each measurement and the mean a degree of freedom, leading to df = d*(N-1)? Or is there some more complicated calculation given that you are estimating d*(d+1)/2 independent entries of the covariance? EDIT: it seems like the above is still ambiguous. Let me try and be more explicit: I have a set of $N$ measurements $X = \{\mathbf{x}_1, \, ...,\, \mathbf{x}_N\}$ of a multivariate normally distributed $p$ -vector with true mean $\mathbf{\mu}$ and true covariance $\Sigma$ , both unknown and with no informative prior. I can use the sample mean as an estimator for the true mean, $\hat{\boldsymbol{\mu}} = \frac{1}{N} \sum_i \mathbf{x}_i$ . I want to know the distribution of the random variable $\boldsymbol{\delta} = \hat{\boldsymbol{\mu}} - \boldsymbol{\mu}$ , the error from the true mean. At the risk of being pedantic, by distribution, I mean the probability density of $\boldsymbol{\delta}$ were I to resample another $N$ vectors from the same original normal distribution many times. I brought up multivariate $t$ -distributions because from my understanding, this distribution ought to be a multivariate $t$ -distribution in the form \begin{equation} p(\boldsymbol{\delta}| X) = \frac{\Gamma[(\nu + p)/2]}{\Gamma(\nu/2) \nu^{p/2} \pi^{p/2} (\det S)^{1/2}}\left[ 1 + \frac{1}{\nu}\boldsymbol{\delta}^T S^{-1} \boldsymbol{\delta} \right]^{-(\nu+p)/2}, \end{equation} where $\nu$ is the "degrees of freedom" and $S$ is a shape parameter matrix that is proportional to the sample covariance $\hat\Sigma$ with proportionality depending on $\nu$ (expressible alternatively in terms of $N$ ). At this point, I have answered my original question by finding a result (In "Bayesian Data Analysis", 3rd Edition, Gelman, bottom of pg. 73) that corroborates my simulated data: that $\nu = N-p$ , and that $S = \frac{1}{N(N-p)} \sum_i (\mathbf{x}_i - \hat{\boldsymbol{\mu}})(\mathbf{x}_i - \hat{\boldsymbol{\mu}})^T= \frac{N-1}{N(N-p)}\hat\Sigma$ . So even though I have solved my specific research problem at this point, I'm trying to understand why $\nu = N - p$ . My (admittedly sophomoric) understanding of degrees of freedom is that they reflect the number of unconstrained random variables contributing to the shape of a distribution. But if we're talking about $N$ measurements of a $p$ -vector, that's $N\cdot p$ random variables. Specifying the mean (also a $p$ -vector) applies $p$ constraints. So thinking of each component of each vector as a random variable leads to a final count of $\nu = N p - p = p(N-1)$ . Alternatively, thinking of each $p$ -vector as a single random variable leads to the simpler $\nu = N-1$ , the result typically used in calculating the sample covariance as an estimator for the covariance. Neither of these results is $N-p$ , which seems to require some hybrid philosophy where each vector observation is somehow just one degree of freedom, while specifying the mean constrains $p$ degrees of freedom. But $N-p$ is clearly empirically correct if you go and simulate data and assess its distribution. So my question is: whence comes this result $\nu = N-p$ ? What is the logic? And as a corollary, why is $N-1$ always used as the degrees of freedom in the estimation of sample covariances (the classic formula being $\hat\Sigma = \frac{1}{N-1} \sum_i (\mathbf{x}_i - \hat{\boldsymbol{\mu}})(\mathbf{x}_i - \hat{\boldsymbol{\mu}})^T$ ), instead of $N-p$ ? Why does the shape parameter we plug into the $t$ -distribution use a different denominator than $\hat\Sigma/N$ ?
