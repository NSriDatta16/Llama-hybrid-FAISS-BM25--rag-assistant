[site]: datascience
[post_id]: 100702
[parent_id]: 100679
[tags]: 
Correlation analysis is very useful for a number of reasons, and not just for SVM classification. In the case of feature selection, if two features in your dataset are highly correlated it means there is redundancy in your feature set (i.e. both features are explaining similar sources of variation in your data). By eliminating redundant features you can reduce the complexity of your model, since you will have fewer parameters to optimise. It can also inform feature extraction. If you find there is a lot of correlation amongst your features, then your dataset may have a lower effective dimensionality than the current feature set you're working with. A technique such as PCA might allow you to extract a new smaller set of features that capture most of the variance in your original feature set, again eliminating redundant features and reducing the number of parameters that must be optimised (IMPORTANT: Methods like PCA must be performed on normalised data, otherwise features with larger value ranges will be prioritised). This can also act as a form of regularisation - removing noise-based variance from your data to avoid model overfitting. As for your follow-up questions: Sure, as mentioned above, it can show which features in your dataset are redundant and therefore probably don't need to be included for the classification task. As mentioned above also, this would be a sign that there is redundancy between those two features, and you may want to only use one. No, features that are poorly correlated with other features are not necessarily noise. While I'm sure there are additional reasons beyond those listed here, this is hopefully enough to demonstrate the importance of performing correlation analysis when taking on a classification (and regression) task!
