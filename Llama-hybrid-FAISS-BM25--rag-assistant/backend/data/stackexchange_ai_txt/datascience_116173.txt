[site]: datascience
[post_id]: 116173
[parent_id]: 
[tags]: 
How to use MinMaxScaler when X_train and X_test are different sizes

I'm using an LSTM through Keras to predict a time series. My inputs are the previous measurements of one time series(v1) at time 0-59 seconds and the goal is to predict the measurement of a different variable(v2) at time 60 (based on those previous 60 seconds of v1 measurements). The training actually performs really well, but I'm hitting an error when it comes to normalizing the test data. I've seen all posts about using .fit_transform() on x_train and only using .transform() on x_test, and this works when I train on half the data and test on the other half, but I'm running into an error with MinMaxScaler because when I train on 70% of the data (resulting in a matrix of size 1920x60 for x_train) and testing on the remaining 30% (resulting in a matrix of size 960x60): I've got a scaler set up for the v1 (X_train) and a different one set up for the v2 (Y_train), but when I go to scale X_test using the .transform() only of the v1 scaler, I get the following error: ValueError: X has 960 features, but MinMaxScaler is expecting 1920 features as input. I understand the logic behind this (MinMaxScaler saves the min/max it was fit on in the .fit_transform step), but how are people getting around this issue? Am I missing something completely? TIA
