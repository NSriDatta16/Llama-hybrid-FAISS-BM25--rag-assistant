[site]: crossvalidated
[post_id]: 479619
[parent_id]: 
[tags]: 
Mean Squared Error changes according to scale of value in machine learning regression problem

I am working on a machine learning regression problem and I have chosen the metrics Mean Absolute Error (MAE) and 'Mean Squared Error (MSE). I have 3 features and two of them have values in the range 0.007 - 0.009 and the third feature's values range from 1.18 to 1.19. The predicted value/output should also lie in the range 0.007 - 0.009. When I implement a simple linear regression model using scikit learn in Python, I get the MSE to be about 2.037727147668752e-07. However I noticed if I multiplied all my features and the value to be predicted by say 100, the MSE changed to 0.0024. Can someone please explain If the MSE is a metric that is to be used on a relative scale, how do I interpret it? Does it mean an error of 0.002 means that if my actual value is 0.008, my predicted value is 0.008 +/- 0.002 = 0.006 or 0.01? This is a large difference between the actual and predicted values, are there any specific regression machine learning models that work well for this kind of problem? Will normalizing the data or scaling it help improve performance and if so, why? I noticed that MAE remained constant regardless of the scale. Is this an absolute error measure? What other metric can I use to evaluate the performance of my model?
