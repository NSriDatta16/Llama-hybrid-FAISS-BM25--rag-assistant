[site]: crossvalidated
[post_id]: 509966
[parent_id]: 
[tags]: 
For imbalanced datasets, is it necessary to use undersampling or upsampling if one evaluates the performance of ML classifier using PR curves?

Previously, I would have assumed evaluating the classification performance of Decision tree and SVM with a PR curve would obviate the need for under/over-sampling since it doesn't evaluate the true negatives, but this question here Optimising for Precision-Recall curves under class imbalance implies training with under/up-sampling and then testing on imbalanced datasets (as I would be doing) could lead to varying results. I didn't gather a conclusion here as to why this occurred and am wondering if I should still upsample even if I evaluate it on an unbalanced test set.
