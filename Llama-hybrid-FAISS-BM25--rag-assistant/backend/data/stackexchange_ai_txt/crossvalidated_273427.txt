[site]: crossvalidated
[post_id]: 273427
[parent_id]: 
[tags]: 
Controller architecture for neural architecture search

In "Neural architecture search with reinforcement learning" ( https://arxiv.org/pdf/1611.01578.pdf ), the authors use a recurrent neural network as the controller to generate hyperparameters, but do not describe the architecture of the controller in detail. How are the predictions for each hyperparameter made? The paper says that softmax is used and hyperparameters are generated sequentially (first filter width/height, then stride width/height, etc., then repeating for the next layer). Thus, what architecture allows predictions to be made sequentially in such a way? How are the skip connection sigmoids incorporated into this architecture?
