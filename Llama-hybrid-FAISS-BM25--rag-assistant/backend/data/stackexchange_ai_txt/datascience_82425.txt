[site]: datascience
[post_id]: 82425
[parent_id]: 82414
[tags]: 
It all depends on the nature of the data in relation to the labels. For instance, if all that you need to appropriately classify the input sequence is to know the values at certain fixed points, then a mere multi-layer perceptron (MLP) could do. However, if in order to properly classify it was needed to take a look at the trends, maybe the MLP would not behave so well (this would depend on the actual data, of course). If could be the case that the labels depend on some local patterns in the daily values. In that case, maybe a 1D convolutional network could do well, because local pattern detection is precisely their inductive bias. Recurrent networks' inductive bias is inherently sequential, and therefore fair well when the prediction can be obtained when looking value after value. Of course, people normally use LSTMs or GRUs instead of vanilla RNNs due to the vanishing gradient problem. Finally, self-attention networks, which are feedforward, are currently the state of the art is natural language processing. These graph neural networks in disguise can obtain in general better text representations than LSTMs. So, summing up : it is perfectly possible to use a feedforward network on sequential data.
