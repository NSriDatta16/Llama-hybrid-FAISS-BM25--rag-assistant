[site]: datascience
[post_id]: 108465
[parent_id]: 
[tags]: 
Computing precision in case of multi-label classification

When evaluating a multi-label model for precision by averaging the precision of each sample, would it be appropriate to a) ignore those samples where no prediction is being made? Or is it more appropriate to b) consider precision as 0 for such samples. Eg: Ground-Truth: [[1, 1, 0], [1, 0, 0], [0, 1, 1]] Prediction: [[1, 1, 0], [0, 0, 0], [0, 0, 1]] Precision by averaging over each sample: a) (1 + 0.5) / 2 ? OR b) (1 + 0 + 0.5) / 3 ?
