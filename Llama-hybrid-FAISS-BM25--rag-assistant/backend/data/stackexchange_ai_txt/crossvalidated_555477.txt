[site]: crossvalidated
[post_id]: 555477
[parent_id]: 555193
[tags]: 
If you are using the lmfit Python package, and specifically the lmfit.Model interface to data fitting, then calculating the "the range of acceptable outputs" is pretty straightforward with the eval_uncertainty method. You can use that with (for example) matplotlib's fill_between method to visualize the range. A simple example might look like import numpy as np import matplotlib.pyplot as plt from lmfit.models import LinearModel # make up something nearly linear and slightly noisy: x = np.arange(51) y = 4 + 0.07*x + 0.0003*x*(x-30) + np.random.normal(size=len(x), scale=0.1) mymodel = LinearModel() # create slope and intercept parameters, with initial values. See notes params = mymodel.make_params(intercept=y.min(), slope=(y.max()-y.min())/(x.max()-x.min())) result = mymodel.fit(y, params, x=x) print(result.fit_report()) dely = result.eval_uncertainty(sigma=3) plt.plot(x, y, 'o', label='data') plt.plot(x, result.best_fit, label='fit') plt.fill_between(x, result.best_fit-dely, result.best_fit+dely, color="#ABABAB", label=r'1- $\sigma$ uncertainty band') plt.legend() plt.show() This will print out a report of [[Model]] Model(linear) [[Fit Statistics]] # fitting method = leastsq # function evals = 7 # data points = 51 # variables = 2 chi-square = 0.76282188 reduced chi-square = 0.01556779 Akaike info crit = -210.330374 Bayesian info crit = -206.466723 [[Variables]] slope: 0.07622481 +/- 0.00104910 (1.38%) (init = 0.07931648) intercept: 3.86210965 +/- 0.03043598 (0.79%) (init = 3.907146) [[Correlations]] (unreported correlations are and produce a plot like this: which shows the (correlation-included) 1-sigma confidence band for the best fit. Now, it should be said that such a fit for a linear model (and here, linear means "linear in the parameters" so any polynomial or even any model that can be linearized) you do not need iterative methods such as used by lmfit -- you could do this all analytically by regression methods. lmfit allows you to easily extend that to non-linear methods. Linear regression does not require starting values, but the non-linear solvers do, so lmfit needs them. The guesses for these values do not need to be very good for truly linear data, so the crude values here are sufficient. If embedding into a real or more complex analysis process, you might want to do that better.
