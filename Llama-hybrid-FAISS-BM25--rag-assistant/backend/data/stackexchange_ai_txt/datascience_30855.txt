[site]: datascience
[post_id]: 30855
[parent_id]: 9852
[tags]: 
As pointed out by Jacob Panikulam, Universal Approximation Theorem gives you the answer. So... Yes. But please do note that anyway this is valid for GLMs, SVMs... So... Would the Neural Network not tent towards being the same as the Bayesian Network? Yes, it does, as does with any other model. Note that this does not happen the other way around, Bayesian Networks cannot model certain types of dependencies, e.g. circular dependencies. Also, all Bayesian Network structure learning algorithms assume faithfulness, so they perform pretty badly with certain types of independent and conditionally dependent variables: see Is there a Probabilistic Graphical Model for this Situation? Caution Note: Please do not mistake this with the fact that you should only learn about and use neural networks . Introduction to Statistical Learning thoroughly explains why.
