[site]: crossvalidated
[post_id]: 521211
[parent_id]: 
[tags]: 
changing the step size/learning rate in artificial intelligence problems

The vast majority of update equations usually come in the form of: $\text{new estimate} \longleftarrow \text{old estimate} + \text{step size}[\text{current reward} - \text{old estimate}]$ The step size in the simples of problems is commonly written as $\alpha = \dfrac{1}{n}$ . But in more nonstationary problems (where the interaction between the agent and the environment changes from time to time), it is wise to change this step size (also known as learning rate). Two conditions have been placed as to what would qualify as an acceptable $\alpha$ . They must satisfy the following conditions: $\displaystyle\sum \alpha = \infty$ and $\displaystyle\sum \alpha^2 . It is easy to see that $\alpha = \dfrac{1}{n}$ satisfies both conditions. My question is: Why are these conditions in place? Can you kindly explain the intuition why these conditions are used. Also, is there another $\alpha$ , aside from $\dfrac{1}{n}$ that is commonly used in practise? Thanks
