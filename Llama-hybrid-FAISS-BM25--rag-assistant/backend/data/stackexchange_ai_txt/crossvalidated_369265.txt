[site]: crossvalidated
[post_id]: 369265
[parent_id]: 189652
[tags]: 
For machine learning models that include coefficients (e.g. regression, logistic regression, etc) the main reason to normalize is numerical stability. Mathematically, if one of your predictor columns is multiplied by 10^6, then the corresponding regression coefficient will get multiplied by 10^{-6} and the results will be the same. Computationally, your predictors are often transformed by the learning algorithm (e.g. the matrix X of predictors in a regression becomes X'X) and some of those transformations can result in lost numerical precision if X is very large or very small. If your predictors are on the scale of 100's then this won't matter. If you're modeling grains of sand, astronomical units, or search query counts then it might.
