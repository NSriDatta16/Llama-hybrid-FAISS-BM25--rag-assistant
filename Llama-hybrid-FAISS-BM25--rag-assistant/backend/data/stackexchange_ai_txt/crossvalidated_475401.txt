[site]: crossvalidated
[post_id]: 475401
[parent_id]: 65128
[tags]: 
As was already pointed out by the answer of cebeleites, inner and outer CV loop have different purposes: the inner CV loop is used to get the best model, the outer CV loop can serve different purposes. It can help you to estimate in a more unbiased way the generalisation error of your top performing model. Additionally it gives you insights into the "stability" of you inner CV loop: are the best performing hyperparameters consistent with regard to the different outer folds? For this information you pay a high price because you are repating the optimization procedure k-times (k-Fold outer CV). If your goal is only to estimate the generalization performance, I would consider another way described below. According to this paper from Bergstra and Bengio: Random Search for Hyper-Parameter Optimization (4000 citations, as of 2019): Goal: make a hyperoptimization to get the best model and report / get an idea about its generalization error Your available data is only a small portion of a generally unknown distribution. CV can help by giving you a mean of expectations rather than a single expectation. CV can help you in choosing the best model (the best hyperparameters). You could also skip CV here at the cost of fewer informations (mean of expectation on different datasets, variance). At the end you would choose the top performing model out of your inner loop (for example random search on hyperparameters with / without CV). Now you have your "best" model: it is the winner of the hyperoptimization loop. In practice there will be several different models that perform nearly equally good. When it comes to report your testing error, you must be careful: " However, when different trials have nearly optimal validation means, then it is not clear which test score to report, and a slightly different choice of λ [single fixed hyperparameter set] could have yielded a different test error. To resolve the difficulty of choosing a winner, we report a weighted average of all the test set scores, in which each one is weighted by the probability that its particular λ(s) is in fact the best. " For details, see the paper. It involves calculating the test error of each model you evaluated in the hyperoptimization loop. This should be cheaper than a nested CV! So: this technique is an alternative to estimate generalization errors from a model selected out of a hyperoptimization loop! NB: in practice, most people just do a single hyperoptimization (often with CV) and report the performance on the test set. This can be too optimistic.
