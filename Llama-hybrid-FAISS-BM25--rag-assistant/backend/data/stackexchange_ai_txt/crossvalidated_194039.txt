[site]: crossvalidated
[post_id]: 194039
[parent_id]: 
[tags]: 
Explanatory power of variable

To clarify from the very beginning, I expect relations between my variables to be very non-linear, so usual correlation and PCA approach may not work here. Also, for simplicity all the variables are 1-dimensional here. Let's say I have a variable $Y$ being my target, and variables $X_1, X_2, \dots$ measured at the same conditions. I want to know how much each of them adds to the explanation of $Y$. Some example to explain what I am looking for: if I need to decide whether $X_1$ or $X_2$ is better alone, I can just plot graph of $Y$ vs each of them. Let's say graph vs $X_1$ looks like a curve with a bit of noise around it, and the one vs $X_2$ looks like a symmetric cloud. From that qualitatively I would conclude that $X_1$ has more information about $Y$ than $X_2$. There are two obvious drawbacks of this method: Cannot be applied beyond 2d Even in case of 1d and 2d it is often hard to interpret which situation is better. Thus, I'd rather have a quantitative criterion which would tell me the explanatory power of a set of variables $X_{i_1}, X_{i_2}, \dots$ w.r.t. $Y$. Of course, I can take any model (such as neural net) and look at the fitting error of this model, however such method is certainly model-dependent, whereas I am looking for a model-independent approach. Mathematician by background I thought of using average conditional variance as a measure of fit, that is $$ \Bbb E[\mathsf{Var}(Y|X_{i_1}, X_{i_2})]. $$ Good think about this method: it is model-free, and intuitively working. At the same time, I am not sure how to evaluate it over discrete observations of the continuous data. Also, I guess there are classical methods of approaching this issue.
