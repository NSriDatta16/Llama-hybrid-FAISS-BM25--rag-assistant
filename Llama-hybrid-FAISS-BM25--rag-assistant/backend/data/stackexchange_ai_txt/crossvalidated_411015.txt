[site]: crossvalidated
[post_id]: 411015
[parent_id]: 409802
[tags]: 
In the standard Transformer model as introduced by Vaswani et al. it is not possible, because generating a word is always conditioned on the previously decoded words, so there is no other option than generating words one by one. Recently, there appeared several papers on so-called non-autoregressive models which parallelize the decoder as well, but the problem is that the words "do not know" what the previous words are, so there is a big problem with fluency. Another problem is that you need to somehow estimate how long the target sentence will be because you cannot wait until a symbol. There were several papers dealing with that: Gu et al., 2017 use latent variables for fertility in the middle, but they have to sample from the model and re-score the outputs with the standard Transformer to get reasonable results. Lee et al. (2018) use a simple classifier to estimate target sentence length. They also have a second non-autoregressive decoder that is used for iterative refinement of the generated sentence and gain better fluency. Libovick√Ω and Helcl (2018) used an ASR-like approach and avoided the target length estimation using CTC loss. Most recently, Ghazvininejad et al. (2019) used BERT-like training. There is also a blog post summarizing it .
