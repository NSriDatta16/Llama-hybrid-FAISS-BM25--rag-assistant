[site]: crossvalidated
[post_id]: 343071
[parent_id]: 
[tags]: 
Algorithm for Debiasing Word Embeddings

I'm taking Andrew Ng's course on Sequence Models on Coursera, and he has a pretty fascinating discussion of how to "debias" word embeddings by removing the learned gender component (or race component, etc) of various words. I have some questions about the intuition behind the algorithm, and I've posted a screenshot below that provides an overview of that algorithm. (This question was inspired by our homework, but won't directly help me with a homework question.) (NOTE: While writing this question I found some of the answers myself, which I've posted as an answer. But I'm still interested in additional feedback.) What's not on the slide is that first he gets a bias vector $g$ by calculating $$g = e_{woman} - e_{man}$$ where $e_{x}$ is the embedding for the word $x$. I think because $e_{woman}$ comes first in the equation, $g$ will be positively correlated with femininity and negatively correlated with masculinity. I think that $g$ is basically an abstract "axis" of gender - I say abstract because most components in $g$ will be nonzero, so it isn't an axis along a single embedding dimension. Using $g$, he demonstrates that learned word embeddings can pick up a gender component. For example the cosine similarity of $g$ with some names is: marie 0.315597935396 sophie 0.318687898594 ronaldo -0.312447968503 Getting similar cosine similarities between $g$ and other words shows why we would want to eliminate the gender component of these words (which would cause these cosine similarities to drop to 0, I think): technology -0.131937324476 fashion 0.0356389462577 teacher 0.179209234318 engineer -0.0803928049452 To perform the debiasing, we calculate $e^{bias\_component}$ for each embedding and just subtract it from the embedding. To calculate this, we get the vector projection of the word we want to debias ($e$) onto $g$. I'm not good at linear algebra, and while I think I understand the equations for vector projection, I don't fully understand the intuition of what the resulting vector represents. My best intuition is that $e^{bias\_component}$ is the amount that, if we subtract it from $e$, will make $e$ orthogonal to our abstract gender axis. However, while I don't doubt that embeddings can pick up gender bias in general, it isn't clear to me what proof we have that a specific word did so, or to what extent. The existence of $e^{bias\_component}$ doesn't seem like such a proof - we have chosen to project $e$ onto $g$ to obtain $e^{bias\_component}$, but we will do that for every word. Basically, it seems that we are just assuming that gender bias is uniformly distributed among our words. My other question concerns the value of $g$. Since it is equal to $e_{woman} - e_{man}$, if we want to eliminate the gender component of a word, shouldn't we be setting $g$ to $1/2 * (e_{woman} - e_{man})$ to get the midway point between the two vectors?
