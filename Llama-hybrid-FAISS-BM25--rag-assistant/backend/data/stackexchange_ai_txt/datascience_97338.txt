[site]: datascience
[post_id]: 97338
[parent_id]: 97332
[tags]: 
There are different methods, it depends what kind of sequential task. Conditional Random Fields are typically used for sequence labeling tasks like POS tagging or NER . n-gram models can be used for standard language models, but n-grams can also be used as features in various tasks where order matters. However the larger $n$ the more data is required, so it's rare to go beyond $n=5$ and this is a limitation for tasks which require long distance relations to be represented. In order to reasonably capture semantics, the traditional approach is to deploy a chain of components: POS tagging , syntactic parsing (e.g. dependency parsing ), then semantic role labeling . This would result in an explicit semantic representation of the sentence.
