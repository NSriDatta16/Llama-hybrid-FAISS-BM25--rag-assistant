[site]: crossvalidated
[post_id]: 427750
[parent_id]: 99137
[tags]: 
I see two possibilities: You could conceivably use the MAPE directly as an objective function. Zheng (2011), International Journal of Machine Learning and Cybernetics ) could perhaps be adapted to yield a smooth approximation to the MAPE, so you get a nice gradient. You could try to get full predictive densities from your model and then derive the one-number summary of that distribution that minimizes the expected MAPE . Yes, both of these do not address feature selection. You may want to take a look at this for a few ideas on this aspect.
