[site]: crossvalidated
[post_id]: 163983
[parent_id]: 
[tags]: 
Understanding Gaussian Process Regression via infinite dimensional basis function view

It is often said that gaussian process regression corresponds (GPR) to bayesian linear regression with a (possibly) infinite amount of basis functions. I am currently trying to understand this in detail to get an intuition for what kind of models I can express using GPR. Do you think that this is a good approach to try to understand GPR? In the book Gaussian Processes for Machine learning Rasmussen and Williams show that the set of gaussian processes described by the parameterised exponential squared kernel $$k(x,x';l)= \sigma_p^2\exp\left(-\frac{(x-x)^2}{2l^2}\right)$$ can be equivalently described as bayesian regression with prior belief $w \sim \mathcal{N}(0,\sigma_p^2 I)$ on the weights and an infinite amount of basis functions of the form $$\phi_c(x;l)=\exp\left(-\frac{(x-c)^2}{2l^2}\right) $$ Thus, the parameterisation of the kernel could by fully translated into a parameterisation of the basis functions. Can the parameterisation of a differentiable kernel always be translated into parameterisation of the prior and the basis functions or are there differentiable kernels where e.g. the number of the basis functions depends on the configuration? My understanding so far is that for a fixed kernel function k(x,x') Mercer's Theorem tells us that $k(x,x')$ can be expressed as $$k(x,x')=\sum_{i=1}^\infty \lambda_i\phi_i(x)\phi_i(x')$$ where $\phi_i$ is a function either into the reals or the complex numbers. Thus, for a given kernel the corresponding bayesian regression model has prior $w \sim \mathcal{N}(0,\text{diag}([\lambda_1^2,\ldots]))$ and basis functions $\phi_i$. Thus, every GP can even by formulated as bayesian linear regression model with diagonal prior. However, if we now use mercers theorem for every configuration of a parameterised kernel $k(x,x',\theta)$ that is differentiable at every $\theta$ the corresponding eigenvalues and eigenfunctions might by different for every configuration. My next question is about the inverse of mercers theorem. Which sets of basis functions lead to valid kernels? And the extension Which sets of parameterised basis functions lead to valid differentiable kernels?
