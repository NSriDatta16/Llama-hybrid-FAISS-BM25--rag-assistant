[site]: crossvalidated
[post_id]: 245502
[parent_id]: 
[tags]: 
Why should we shuffle data while training a neural network?

In the mini-batch training of a neural network, I heard that an important practice is to shuffle the training data before every epoch. Can somebody explain why the shuffling at each epoch helps? From the google search, I found the following answers: it helps the training converge fast it prevents any bias during the training it prevents the model from learning the order of the training But, I have the difficulty of understanding why any of those effects is caused by the random shuffling. Can anybody provide an intuitive explanation?
