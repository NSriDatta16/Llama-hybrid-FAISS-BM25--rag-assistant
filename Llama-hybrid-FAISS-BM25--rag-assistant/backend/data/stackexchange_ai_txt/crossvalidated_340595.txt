[site]: crossvalidated
[post_id]: 340595
[parent_id]: 340555
[tags]: 
The three algorithms don't attempt to solve the same problem, so choosing between them will depend on what problem you ARE trying to solve. BFGS attempts to solve a general nonlinear optimization problem without any constraints. You can think of it as an approximation to Newton's method, where the approximation is a clever "estimation" of the Hessian that updates with each iteration. L-BFGS-B is a variant of BFGS that allows the incorporation of "box" constraints, i.e., constraints of the form $a_i \leq \theta_i \leq b_i$ for any or all parameters $\theta_i$. Obviously, if you don't have any box constraints, you shouldn't bother to use L-BFGS-B, and if you do, you shouldn't use the unconstrained version of BFGS. ETA: Jim points out in comments above that L-BFGS-B uses a limited memory version of BFGS as well as incorporating box constraints. This could be important for your application, or not. I haven't run into memory constraints in a long time, but that is a function of my application area and work environment, and, as Jim observes, "it is widely used[in machine learning] because it is more memory efficient than plain vanilla BFGS". PORT is a particular implementation of the Levenberg-Marquardt algorithm, and solves a nonlinear least squares problem. Note that in this case the objective function is fixed, namely, $\sum_{i=1}^N(y_i-f(\theta,x_i))^2$, although you still get to specify $f$. Obviously if this is not your objective function you can't use PORT! But if it is, then you should use L-M in preference to the less-specialized BFGS algorithms.
