[site]: crossvalidated
[post_id]: 239363
[parent_id]: 235862
[tags]: 
You can also use another network to advise how the parameters should be updated. There is the Decoupled Neural Interfaces (DNI) from Google Deepmind. Instead of using backpropagation, it uses another set of neural networks to predict how to update the parameters, which allows for parallel and asynchronous parameter update. The paper shows that DNI increases the training speed and model capacity of RNNs, and gives comparable results for both RNNs and FFNNs on various tasks. The paper also listed and compared many other non-backpropagation methods Our synthetic gradient model is most analogous to a value function which is used for gradient ascent [2] or a value function used for bootstrapping. Most other works that aim to remove backpropagation do so with the goal of performing biologically plausible credit assignment, but this doesn’t eliminate update locking between layers. E.g. target propagation [3, 15] removes the reliance on passing gradients between layers, by instead generating target activations which should be fitted to. However these targets must still be generated sequentially, propagating backwards through the network and layers are therefore still update- and backwardslocked. Other algorithms remove the backwards locking by allowing loss or rewards to be broadcast directly to each layer – e.g. REINFORCE [21] (considering all activations are actions), Kickback 1 , and Policy Gradient Coagent Networks [20] – but still remain update locked since they require rewards to be generated by an output (or a global critic). While Real-Time Recurrent Learning [22] or approximations such as [17] may seem a promising way to remove update locking, these methods require maintaining the full (or approximate) gradient of the current state with respect to the parameters. This is inherently not scalable and also requires the optimiser to have global knowledge of the network state. In contrast, by framing the interaction between layers as a local communication problem with DNI, we remove the need for global knowledge of the learning system. Other works such as [4, 19] allow training of layers in parallel without backpropagation, but in practice are not scalable to more complex and generic network architectures.
