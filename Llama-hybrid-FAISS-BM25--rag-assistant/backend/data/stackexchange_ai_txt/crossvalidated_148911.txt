[site]: crossvalidated
[post_id]: 148911
[parent_id]: 
[tags]: 
Combining different raters scores

In my experiment two raters evaluate 100 products, both expressing an "innovation score" on a likert scale (range is 1 to 5, where 1 is "not innovative" and 5 is "disruptive innovation"). I calculated a weighted Cohen's Kappa and the inter-rater agreement is very good (k=0.7). Now I need to combine the two ratings into one single score for each product. I saw that many studies (dealing with other measures) are combining scores using their average. I would rather use the sum of the two ratings? Could you suggest me what is best (between mean score, sum, etc..) and why? Could you also provide some reference I could use to support my choice?
