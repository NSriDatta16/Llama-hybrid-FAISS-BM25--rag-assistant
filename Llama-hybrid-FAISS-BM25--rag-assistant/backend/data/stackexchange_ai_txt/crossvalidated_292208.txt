[site]: crossvalidated
[post_id]: 292208
[parent_id]: 292179
[tags]: 
Nested cross-validation is a statistical concept of estimating the performance of a statistical (machine learning) model. The outer cross-validation splits the data in train + test and estimates the model performance on each fold. To minimize variance of random partitioning within folds, this is repeated x times. Now you could use the default parameter settings each time you train a model on the training set. However, default settings of a model are likely not optimal for each data set out in the wild. Hence, another cross-validation is performed (= the inner resampling) which has the aim to find the best parameter set of a learner for each fold . This parameter set is then applied to the actual training set of the outer resampling procedure, assuming that the model achieves better results using this non-standard parameter configuration. This concept of finding the best parameter set is also referred to as "tuning a model". You could do a grid search and try all possible parameter configurations or you do a random search and pick randomly a parameter combination y times and see what comes out. The more random tries, the higher your probability to find the "optimal" configuration. However, it is unlikely that this is found by a random search, especially if you have numeric hyperparameters. Nevertheless, using a several hundred iterations of a random search will most likely find a better configuration than the model default. Be aware that nested cross-validation is computational intense as you are fitting thousands of models (several hundred iterations in inner + ususally 500 - 1000 iterations in outer (folds*reps)). Therefore it is highly recommended to parallelize your code. Here is a full parallelized example which also extracts the tuning results (see extract argument in resample ). This works on a binary response task (which is not given here). lrn_rf
