[site]: crossvalidated
[post_id]: 474459
[parent_id]: 422890
[tags]: 
The mask is simply to ensure that the encoder doesn't pay any attention to padding tokens. Here is the formula for the masked scaled dot product attention: $$ \mathrm{Attention}(Q, K, V, M) = \mathrm{softmax}\left(\frac{QK^T}{\sqrt{d_k}}M\right)V $$ Softmax outputs a probability distribution. By setting the mask vector $M$ to a value close to negative infinity where we have padding tokens and 1 otherwise, we ensure that no attention is paid to those tokens.
