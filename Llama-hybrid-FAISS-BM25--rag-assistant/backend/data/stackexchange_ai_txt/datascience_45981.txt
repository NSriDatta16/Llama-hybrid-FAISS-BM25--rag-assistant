[site]: datascience
[post_id]: 45981
[parent_id]: 45974
[tags]: 
The difference between macro and micro averaging for performance metrics (such as the F1-score) is that macro weighs each class equally whereas micro weights each sample equally. If the distribution of classes is symmetrical (i.e. you have an equal number of samples for each class), then macro and micro will result in the same score. As an example for your binary classification problem, say we get a F1-score of 0.7 for class 1 and 0.5 for class 2. Using macro averaging, we'd simply average those two scores to get an overall score for your classifier of 0.6, this would be the same no matter how the samples are distributed between the two classes. If you were using micro averaging, then it would matter what the distribution was. Say that class 1 made up 80% of your data, the formula would then be 0.7*80% + 0.5*(100%-80%) which would equal 0.66, since each sample is weighed equally and as a result the score is representative of the data imbalance. If class 1 made up 50% of your data, the formula would shift to 0.7*50% + 0.5*(100%-50%) which would be 0.6, the same as the result from macro averaging. If your data was perfectly balanced, then macro and micro averaging will both result in the same score. If not, there's still a chance that they result in the same score depending on the exact distribution of scores (or if your estimator has the same performance for all classes involved).
