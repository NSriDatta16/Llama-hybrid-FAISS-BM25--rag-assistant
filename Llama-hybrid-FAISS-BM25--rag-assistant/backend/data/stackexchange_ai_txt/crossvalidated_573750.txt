[site]: crossvalidated
[post_id]: 573750
[parent_id]: 
[tags]: 
Using target values in both (X and y) arguments of fit(X, y)

This question is based on SQL Server Machine Learning Services Ski Rental tutorial . We have a dataframe df : index Year Month Day RentalCount WeekDay Holiday Snow 0 2014 1 20 445 2 1 0 1 2014 2 13 40 5 0 0 2 2013 3 10 456 1 0 0 3 2014 3 31 38 2 0 0 4 2014 4 24 23 5 0 0 .. ... ... ... ... ... ... ... 448 2013 2 19 57 3 0 1 449 2015 3 18 26 4 0 0 450 2015 3 24 29 3 0 1 451 2014 3 26 50 4 0 1 452 2015 12 6 377 1 0 1 [453 rows x 7 columns] We want to predict RentalCount: # Store the variable we'll be predicting on. target = "RentalCount" # Generate the training set. Set random_state to be able to replicate results. train = df.sample(frac=0.8, random_state=1) # Select anything not in the training set and put it in the testing set. test = df.loc[~df.index.isin(train.index)] # Initialize the model class. lin_model = LinearRegression() # Fit the model to the training data. lin_model.fit(train[columns], train[target]) In the tutorial, columns are ['Month', 'Day', 'RentalCount', 'Weekday', 'Holiday', 'Snow'] (everything except Year ). This means that both arguments of lin_model.fit(train[columns], train[target]) will contain the 'RentalCount' values. Since it is the target variable, shouldn't it only be included in the target (2nd) argument? With: lin_predictions = lin_model.predict(test[columns]) lin_mse = mean_squared_error(lin_predictions, test[target]) print(lin_mse) from sklearn.metrics import r2_score r2_score(test[target],lin_predictions) I can see that mse is low, and r2 high ( 1.63e-27 and 1.0 ), but is this correct? I tried removing RentalCount from columns, but I only get worse scores ( 35003.54 and 0.224 ) Notebook with data
