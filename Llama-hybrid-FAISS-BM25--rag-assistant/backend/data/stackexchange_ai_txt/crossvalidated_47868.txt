[site]: crossvalidated
[post_id]: 47868
[parent_id]: 47821
[tags]: 
I don't have any references on this either, but maybe the following thoughts will be helpful. First, why would you need all $w_i$ to be non-negative? If you need a black box that classifies your data, why would you care about how the box works? If you are studying your data, the negativity of $w_i$ is a property of the data reflected in the SVM classifier, then why deny it? Answering your question, I'd suggest the following two approaches. Add constraints $w_i \geq 0$ to the original SVM optimization problem. I don't know if there are any SVM libraries that would allow you to do that. If your dataset is not very large, you can solve this new optimization problem yourself using any general solver for quadratic problems. Note that, generally speaking, the properties of your hyperplane will not be the same as those of the common SVM hyperplane. Get $w, b$ by solving the original SVM optimization problem and if some $w_i$ are negative, transform your data using one of the following approaches. For each $w_i Shift and rotate your data to get all $w_i \geq 0$. First, substract $b/||w||$ from your data -- now your hyperplane passes through the origin. Rotate your data to point $w$ into the positive quadrant of $R^n$. For example, if $||w|| = d$, you can calculate the angle $\alpha$ between $w$ and $w' = (d, 0, ..., 0)$, and then rotate your dataset by $\alpha$. Note that in this case you will get a classifier with all $w'_i = 0$ except $w'_1 = d$.
