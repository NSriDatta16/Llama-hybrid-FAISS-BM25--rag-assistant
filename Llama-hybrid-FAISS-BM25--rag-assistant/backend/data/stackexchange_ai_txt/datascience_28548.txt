[site]: datascience
[post_id]: 28548
[parent_id]: 
[tags]: 
Interpretation of tuning parameters (shrinkage and nrounds) in XGBoost

I'm using XGBoost for a multiclass classification problem in R. I'm trying different combinations of the shrinkage parameter and number of iterations to try and settle on optimal values for these parameters (using cross-validation). However, I'm having trouble with exactly interpreting the significance of the parameters. The XGBoost algorithm isn't completely a black box for me - I know how boosting works at a high-level. The questions I have though are as follows: Does a low shrinkage (e.g. 0.01) and high nrounds (e.g. 500) yield the same results as a high shrinkage (e.g. 0.3) and low nrounds (e.g. 50)? Does nrounds correspond to the number of weak classifiers being fit? The xgb.cv function in R returns test mean errors and standard deviations for each of the iterations. For the nth iteration, is this the mean over all iterations less than equal to n over all the folds? Apologies if I'm interpreting this incorrectly. Also, what is the significance of the standard deviation here? Is there some blog post/link I can refer to for a comprehensive guide on tuning XGBoost parameters?
