[site]: datascience
[post_id]: 27613
[parent_id]: 
[tags]: 
Best machine learning algorithm for understanding specific conditional structures

Being still a bit new to neural networks, I wish to use some form of machine learning but I am not sure which one is most suited. I have a toy-example of what I am working on. Tips are much appreciated. Assuming there are chunks of data (not pixels of an image :)) which all are based on a root variable, being 'a'. This 'a' is the 'thing' and the other variables are aspects. Also assuming it's a finite set of things and that each thing 'a' has four ('b', 'c' and 'd') aspects. I wish to make a program that learns that thing ('a':1) often occurs with ('b':0.2), ('c' :0.8), and ('d':0.3). There is a catch however, I want there to be a certain conditional structure underlying the co-occurrence of a and the respective values of (b, c, d). For instance: ('a':1) often (prob. 0.7) occurs with ('b':0.2), ('c':0.8), and ('d':0.3) ('a':1) sometimes (prob. 0.6) occurs with ('b':0.2), ('c':0.5), and ('d':0.7) ('a':1) sometimes (prob. 0.4) occurs with ('b':0.3), ('c':0.4), and ('d':0.2) ('a':1) rarely (prob. 0.2) occurs with ('b':0.5), ('c':0.3), and ('d':0.8) For a given ('a':2) this would all be different. There is no implied necessity for the values of b, c and d to sum to 1. They are mere placeholder values representing aspects (0.2 could be green, 0.8 purple, etc.). I want to make sure the program understands that there is an inherent dependency between b, c and d, as well as an 'a':x given (b, c, d). If anyone has suggestions based on this, I would be very grateful. Thanks! EDIT: I am working in python and was thinking about dictionaries as the data-structure, but if there are better solutions (also depending on the algorithm perhaps) I am all ears.
