[site]: crossvalidated
[post_id]: 632323
[parent_id]: 632316
[tags]: 
Because $(X,Y)$ has a uniform distribution over the triangle shown, the expectation of $Y$ conditional on $X$ evidently splits the lower and upper boundaries of the triangle, shown as the dotted line $y = 1 - x/2:$ That's the regression of $Y$ on $X.$ Because it happens to be a linear function, it's also the (Ordinary Least Squares, or "OLS") linear regression. We can prove this from first principles. The density function (supported in the blue triangle of the diagram) is $$f_{X,Y}(x,y) = 2\mathcal I(0\le x\le 1,\ 1-x \le y\le 1).$$ We will need the first and second moments, as always, so let's calculate them now. By symmetry $X$ and $Y$ have the same expectation, $$E[Y] = E[X] = \iint x f_{X,Y}(x,y)\,\mathrm d x\, \mathrm d y = 2 \int_0^1\int_{1-x}^1 x \,\mathrm d x\, \mathrm d y = \frac{2}{3}.$$ Similarly $$E[Y^2] = E[X^2] = \frac{1}{2}$$ and $$E[XY] = \frac{5}{12}.$$ The least squares objective is the average squared deviation between $Y$ and $\alpha + \beta X$ for unknown parameters to be determined: $$\Lambda = \iint (y - (\alpha + \beta x))^2\,f_{X,Y}(x,y)\,\mathrm d x\,\mathrm dy.$$ This is a differentiable function of the parameters (which can be any real numbers) and the differentiation can be carried out under the integral sign, telling us that $$\Lambda_\alpha = 2\iint y - (\alpha + \beta x)\,f_{X,Y}(x,y)\,\mathrm dx\,\mathrm dy = 2\left(E[Y] - \alpha - \beta E[X]\right)$$ and $$\Lambda_\beta = 2\iint x(y - (\alpha + \beta x))\,f_{X,Y}(x,y)\,\mathrm dx\,\mathrm dy = 2\left(E[XY] - \alpha E[X] - \beta E[X^2]\right).$$ Equating both with zero gives all possible critical points. Plugging in the expectations computed previously gives $$0 = 2\left(\frac{2}{3} - \alpha - \beta \frac{2}{3}\right)$$ and $$0 = 2\left(\frac{5}{12} - \frac{2}{3} - \beta \frac{1}{2}\right).$$ This system of linear equations (the Normal equations of OLS) has the unique solution (easily found) $$(\alpha,\beta) = (1, -1/2),$$ as we saw in the diagram.
