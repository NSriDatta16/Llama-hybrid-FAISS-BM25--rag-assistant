[site]: crossvalidated
[post_id]: 548633
[parent_id]: 
[tags]: 
Is this an application of sampling statistics?

Suppose there are is a bake sale and one of the participants prepares 20 cakes. You try 6 of this participant's cakes, and unfortunately you don't like any of his cakes. Question: Can you use statistics to determine "the number of cakes you need to try before you can be statistically sure that you won't like any of this participant's cake"? In this case, is this an application of "sample size determination", where you need to find the sample size with respect to the population of 20 cakes (let's assume that the population consists of only the cakes that are present at the bake sale)? Would this sample size somehow determine on "how similar the cakes are from one another"? For example : suppose you make a histogram of the cakes according to how many grams of sugar are in each cake - suppose you don't like sweet cakes, and this participant on average uses a lot of sugar - the resulting distribution would look right skewed, and in general you would be less likely to enjoy this participant's cakes? (But let's assume that the participant does not know that you like cakes with less sugar, and you don't know that the participant likes to use a lot of sugar in their cakes) I tried to illustrate this scenario in the R programming language (suppose the participant brought 1000 cakes to the bake sale): amount_of_sugar_in_cakes = rnorm(1000,37,5) hist(amount_of_sugar_in_cakes, labels = TRUE, xlab = "Amount of Sugar in Cake grams", ylab = "Number of Cakes", breaks=100, main = "Distribution of Sugar in Cakes: You Dislike Anything with more than 27 grams of Sugar") In this kind of question - can statistics be used to determine the minimum number of cakes you need to try (at some significance level) before you can conclude that you will not like any of this participant's cakes? Or is this just a simple math proportion ? I.e If the participant has 20 cakes and I try 10 randomly selected cakes, there is still a 50% chance the participant might have a cake that I might like. If I try 15 of the cakes, now there is only a 25% chance of there being a cake that I might like - and of course, if I eat all 20 cakes and I did not like any of them, now there is a 0% chance of there being a cake that I did not like. If there is some statistical formula that can be used to calculate the sample size, does some aspect of the data need to have a normal distribution for this formula to work? For example, suppose instead of cakes, now I am at a large clothing store - it is too difficult to define the items in this clothing store by some variables (which variables to choose) and then see if these variables have a normal distribution. Furthermore, it might not be possible to quantify my preferences for certain types of clothing according to some variable, as I had done with the amount of sugar in cakes - I might not even know what types of clothing I like. If someone were to look at the inventory of all items in this clothing store, randomly shuffle this inventory list and then randomly show me a few items, and ask me whether or not I like these items : is there any statistical formula that can be used to tell me the minimum number of items I need to see before I can make a statistical conclusion about my preferences? e.g. if I see 10% of all items in this store and I like 40% of these items (assume "like" is a binary variable: 1 = like, 0 = not like) - is there any formula that will now say : I have seen enough items and I can now believe that on average, I will like 40% of the items available in this store? Thanks Note: I know that preferences and likings are very subjective - I was just trying to use them to illustrate my question on sampling size.
