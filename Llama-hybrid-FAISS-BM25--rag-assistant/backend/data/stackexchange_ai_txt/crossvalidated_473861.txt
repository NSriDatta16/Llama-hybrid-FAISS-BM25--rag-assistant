[site]: crossvalidated
[post_id]: 473861
[parent_id]: 473858
[tags]: 
Attention is a generalized pooling method with bias alignment over inputs. I would suggest not to dwell on that if you understand the maths. I think it's a bad phrasing that you're meant to parse "[bias [alignment over inputs]]" (not a compound of "bias" and "alignment"). Bias here is taken to mean "inductive bias", not parameters that bias an output. The rest is correct. I'd call "attention layer" all the parameters and the computation in general thanks to which you obtain the output (the weighted sum).
