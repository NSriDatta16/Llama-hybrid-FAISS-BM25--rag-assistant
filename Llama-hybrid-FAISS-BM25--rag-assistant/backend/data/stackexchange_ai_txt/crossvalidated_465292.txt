[site]: crossvalidated
[post_id]: 465292
[parent_id]: 461218
[tags]: 
Yes, as you exactly said squeezing operation is nothing but reshaping operator. Let me explain multi-scale with dilated convolutions which you can understand it with the GIF below: Dilated convolution is a specific type of convolution where we expand our support without loss of coverage. Thanks to the expansion via dilation, we can reach further information. This is a way of constructing multi-scale architectures which harness multiple levels information from different contexts. In RealNVP, like other type of flows, the problem comes from spatial dimension. If it's too large, it'd be a computational burden. To avoid it, they use divide-by-conquer style approach which is handled by applying a channel-wise mask and squeeze it to reduce the input dimensionality to the coupling layers. After coupling,you can easily invert the reshaping operations. Multi-scale here can also be understood as parallelism in this context. They also explain why they did squeezing operation below: At each layer, as the spatial resolution is reduced, the number of hidden layer features in s and t is doubled. All variables which have been factored out at different scales are concatenated to obtain the final transformed output (Equation 16). Well, it may be worse. This depends. The main strength of flow models is also their weakness. Layers need to be designed such that determinant of the Jacobian must be computed efficiently. Also, factoring out the dimensions by half may help learning better. You can think factoring as implicit regularizer. If we would apply the vector with dimensionality D(which is can be high )directly may lead to curse of dimensionality. Hope this helps.
