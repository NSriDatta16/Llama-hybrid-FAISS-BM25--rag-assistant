[site]: crossvalidated
[post_id]: 194357
[parent_id]: 
[tags]: 
State of the art for bagging/model averaging?

If I estimate a collection of models predicting $Y$ by $\hat{Y}$, which methods are out there to combine these forecasts? Which methods work well/best (and why?) to improve prediction accuracy? My interest is of theoretical nature, for frequentist and bayesian approaches alike. I am aware that this question is very open, but I want to gain an overview. Consequently, references to further sources or survey papers, book chapters, ... are also highly appreciated! Edit : Bagging, boosting and stacking in machine learning was poposed as an answer to this question. I was very thankful for the link, and I recommend everyone interested in this question to read the post and its answers if they haven't done so already. However, the post does not answer my question: I am interested in specific methods for model averaging and bagging. The aforementioned post elaborates on the differences between the concepts of 'boosting', 'bagging', and 'stacking' rather than giving explicit different implementations. (e.g., model averaging use weights for each model. What I want to enquire with this post is the ways in which these weights can be obtained.)
