[site]: crossvalidated
[post_id]: 434003
[parent_id]: 
[tags]: 
In XGBoost with a f1_score, is the iteration with a lower or higher score the better iteration?

In the following XGBoost script the output states iteration 0 with score 0.0047 is the best score. I would expect iteration 10 with score 0.01335 to be the better score? Output Start xgb.train [0] train-F1_score:0.005977 eval-F1_score:0.00471 Multiple eval metrics have been passed: 'eval-F1_score' will be used for early stopping. Will train until eval-F1_score hasn't improved in 10 rounds. [1] train-F1_score:0.01074 eval-F1_score:0.006875 [2] train-F1_score:0.011652 eval-F1_score:0.008973 [3] train-F1_score:0.014231 eval-F1_score:0.010479 [4] train-F1_score:0.016044 eval-F1_score:0.010811 [5] train-F1_score:0.01645 eval-F1_score:0.010806 [6] train-F1_score:0.0179 eval-F1_score:0.011724 [7] train-F1_score:0.018575 eval-F1_score:0.011996 [8] train-F1_score:0.020258 eval-F1_score:0.011704 [9] train-F1_score:0.020238 eval-F1_score:0.012464 [10] train-F1_score:0.021241 eval-F1_score:0.01335 Stopping. Best iteration: [0] train-F1_score:0.005977 eval-F1_score:0.00471 eval def f1_eval(predt: np.ndarray, dtrain: xgb.DMatrix) -> Tuple[str, float]: y = dtrain.get_label() # convert the predicted values from {predt E R | 0 0.5, 1, 0) return "F1_score", sklearn.metrics.f1_score(y_true=y, y_pred=predt_binary) param param = {'max_depth': max_depth, 'objective': 'binary:logistic', 'disable_default_eval_metric': 1, 'scale_pos_weight': scale_pos_weight, 'max_delta_step': '2'} train bst = xgb.train(param, dtrain, num_round, watchlist, feval=f1_eval, evals_result=evals_result, early_stopping_rounds=early_stopping_rounds)
