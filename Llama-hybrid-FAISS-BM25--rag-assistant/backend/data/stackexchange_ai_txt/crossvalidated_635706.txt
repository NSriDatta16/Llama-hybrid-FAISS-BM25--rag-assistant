[site]: crossvalidated
[post_id]: 635706
[parent_id]: 635419
[tags]: 
As stated in Elements of Statistical Learning (same page you mention) the SVM hinge loss $L(y, f(x)) = [1 - y f(x)]_+$ is minimized in expectation by the minimizing function $$ f^*(x) = \mathrm{sign} \left[ \mathbb{P} ( Y = 1 \vert x) - \frac{1}{2} \right] . $$ This is equivalent to selecting the mode of the posterior class probabilities, i.e. to selecting the class with the higher probability: We have $\mathbb{P}(Y=1 \vert x) > \mathbb{P}(Y=-1 \vert x)$ if and only if $\mathbb{P}(Y=1 \vert x) > 1/2$ in the binary classification setting. The optimal $f^*$ is the theoretical solution of the loss minimization problem, when we impose no restrictions on the shape. For the SVM we however choose $f(x) = w^T x + b$ as a separating hyperplane, so the SVM decision function will only come somewhat close to the theoretical $f^*$ . Additionally, the SVM does not estimate probabilities, but looks for a good separation of the classes. The hinge loss gives the optimization no incentive to represent probabilities with $f(x)$ , as only the signs of these values are relevant for an optimal solution. The minimizer of the SVM hinge loss thus reflects the posterior probabilities only in the sense that it indicates which probability is higher. Stated differently, in most cases where $f(x) > 0$ it should hold that $\mathbb{P}(Y=1 \vert x) > \mathbb{P}(Y=-1 \vert x)$ . But the value of $f(x)$ will not give you any info on the exact probability $\mathbb{P} (Y = 1 \vert x)$ .
