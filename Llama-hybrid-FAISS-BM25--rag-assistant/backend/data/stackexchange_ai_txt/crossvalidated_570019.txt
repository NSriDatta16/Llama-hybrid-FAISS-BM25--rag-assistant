[site]: crossvalidated
[post_id]: 570019
[parent_id]: 
[tags]: 
Why does test MSE always decrease with increasing training size (and decreasing test size)?

Context: I am trying to find the best predictive model for a dataset with 1000 observations. The problem is I am not sure what the best training and test size should be. So what I did was that I ran a loop to vary the training size from 1 to 999 (and the rest as test set, size from 999 to 1) for my predictive models and find the one with the lowest MSE. Problem: It seems that for Ridge Regression, Lasso Regression, Bagging and Random Forest procedures, the models that gives the lowest MSE for each method all have training size close to 999. The plot of training size against MSE can be seen below (I omitted the first 900 since their MSE is way too high, but generally there is a downward trend in MSE as training size increase). Why is this the case?
