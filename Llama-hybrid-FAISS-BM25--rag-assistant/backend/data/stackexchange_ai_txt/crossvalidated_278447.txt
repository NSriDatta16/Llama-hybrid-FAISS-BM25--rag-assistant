[site]: crossvalidated
[post_id]: 278447
[parent_id]: 278385
[tags]: 
To a certain extent, this is already done. Look at deep learning in image recognition. You can actually interpret the different layers in the network as learning different visual features in the data, e.g., straight lines vs. round shapes etc. That is, the network itself "decides" which patterns in the data encode different features, which network nodes "should" "concentrate" on which features, and how to put the results together to output whether the image is a dog or a car. So if you call "how to put different detected features together" an algorithm, deep learning already is a meta-algorithm. And of course, other ML techniques work similarly, especially those in pattern recognition, whether it is how to isolate, detect and assemble features in handwriting recognition or in matching payments to invoices. However, we are still far away from truly "self-learning" algorithms. Humans, when given images, can pretty much deduce that they should try to understand what the images depict. Computers that are given big heaps of bytes don't know whether they should recognize visual patterns, or detect natural language, or forecast loan defaults. They don't yet understand "meaning". (And I don't think they ever will.)
