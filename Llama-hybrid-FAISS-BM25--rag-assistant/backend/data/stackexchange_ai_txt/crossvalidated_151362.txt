[site]: crossvalidated
[post_id]: 151362
[parent_id]: 7400
[tags]: 
There are plenty of distance measures between two histograms. You can read a good categorization of these measures in: K. Meshgi, and S. Ishii, “Expanding Histogram of Colors with Gridding to Improve Tracking Accuracy,” in Proc. of MVA’15, Tokyo, Japan, May 2015. The most popular distance functions are listed here for your convenience: $L_0$ or Hellinger Distance $D_{L0} = \sum\limits_{i} h_1(i) \neq h_2(i) $ $L_1$ , Manhattan, or City Block Distance $D_{L1} = \sum_{i}\lvert h_1(i) - h_2(i) \rvert $ $L=2$ or Euclidean Distance $D_{L2} = \sqrt{\sum_{i}\left( h_1(i) - h_2(i) \right) ^2 }$ L $_{\infty}$ or Chybyshev Distance $D_{L\infty} = \max_{i}\lvert h_1(i) - h_2(i) \rvert $ L $_p$ or Fractional Distance (part of Minkowski distance family) $D_{Lp} = \left(\sum\limits_{i}\lvert h_1(i) - h_2(i) \rvert ^p \right)^{1/p}$ and $0 Histogram Intersection $D_{\cap} = 1 - \frac{\sum_{i} \left(\min(h_1(i),h_2(i) \right)}{\min\left(\vert h_1(i)\vert,\vert h_2(i) \vert \right)}$ Cosine Distance $D_{CO} = 1 - \sum_i h_1(i)h2_(i)$ Canberra Distance $D_{CB} = \sum_i \frac{\lvert h_1(i)-h_2(i) \rvert}{\min\left( \lvert h_1(i)\rvert,\lvert h_2(i)\rvert \right)}$ Pearson's Correlation Coefficient $ D_{CR} = \frac{\sum_i \left(h_1(i)- \frac{1}{n} \right)\left(h_2(i)- \frac{1}{n} \right)}{\sqrt{\sum_i \left(h_1(i)- \frac{1}{n} \right)^2\sum_i \left(h_2(i)- \frac{1}{n} \right)^2}} $ Kolmogorov-Smirnov Divergance $ D_{KS} = \max_{i}\lvert h_1(i) - h_2(i) \rvert $ Match Distance $D_{MA} = \sum\limits_{i}\lvert h_1(i) - h_2(i) \rvert $ Cramer-von Mises Distance $D_{CM} = \sum\limits_{i}\left( h_1(i) - h_2(i) \right)^2$ $\chi^2$ Statistics $D_{\chi^2} = \sum_i \frac{\left(h_1(i) - h_2(i)\right)^2}{h_1(i) + h_2(i)}$ Bhattacharyya Distance $ D_{BH} = \sqrt{1-\sum_i \sqrt{h_1(i)h_2(i)}} $ & hellinger Squared Chord $ D_{SC} = \sum_i\left(\sqrt{h_1(i)}-\sqrt{h_2(i)}\right)^2 $ Kullback-Liebler Divergance $D_{KL} = \sum_i h_1(i)\log\frac{h_1(i)}{m(i)}$ Jefferey Divergence $D_{JD} = \sum_i \left(h_1(i)\log\frac{h_1(i)}{m(i)}+h_2(i)\log\frac{h_2(i)}{m(i)}\right)$ Earth Mover's Distance (this is the first member of Transportation distances that embed binning information $A$ into the distance, for more information please refer to the above mentioned paper or Wikipedia entry. $ D_{EM} = \frac{\min_{f_{ij}}\sum_{i,j}f_{ij}A_{ij}}{sum_{i,j}f_{ij}}$ $ \sum_j f_{ij} \leq h_1(i) , \sum_j f_{ij} \leq h_2(j) , \sum_{i,j} f_{ij} = \min\left( \sum_i h_1(i) \sum_j h_2(j) \right) $ and $f_{ij}$ represents the flow from $i$ to $j$ Quadratic Distance $D_{QU} = \sqrt{\sum_{i,j} A_{ij}\left(h_1(i) - h_2(j)\right)^2}$ Quadratic-Chi Distance $D_{QC} = \sqrt{\sum_{i,j} A_{ij}\left(\frac{h_1(i) - h_2(i)}{\left(\sum_c A_{ci}\left(h_1(c)+h_2(c)\right)\right)^m}\right)\left(\frac{h_1(j) - h_2(j)}{\left(\sum_c A_{cj}\left(h_1(c)+h_2(c)\right)\right)^m}\right)}$ and $\frac{0}{0} \equiv 0$ A Matlab implementation of some of these distances is available from my GitHub repository . Also, you can search for people like Yossi Rubner, Ofir Pele, Marco Cuturi, and Haibin Ling for more state-of-the-art distances. Update: Alternative explanation for the distances appears here and there in the literature, so I list them here for sake of completeness. Canberra distance (another version) $D_{CB}=\sum_i \frac{|h_1(i)-h_2(i)|}{|h_1(i)|+|h_2(i)|}$ Bray-Curtis Dissimilarity, Sorensen Distance (since the sum of histograms are equal to one, it equals to $D_{L0}$ ) $D_{BC} = 1 - \frac{2 \sum_i h_1(i) = h_2(i)}{\sum_i h_1(i) + \sum_i h_2(i)}$ Jaccard Distance (i.e. intersection over union, another version) $D_{IOU} = 1 - \frac{\sum_i \min(h_1(i),h_2(i))}{\sum_i \max(h_1(i),h_2(i))}$
