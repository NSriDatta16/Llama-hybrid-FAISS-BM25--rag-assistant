[site]: crossvalidated
[post_id]: 422492
[parent_id]: 264715
[tags]: 
Disclaimer: this answer possibly belongs in the comments, but I don't have enough reputation points to post comments. 1,2. To help you answer questions 1. and 2. yourself, I have two pointers for you. First is this paper: A Survey of Discretization Techniques: Taxonomy and Empirical Analysis in Supervised Learning . Of course, which exact method to use depends on your use case; as the authors note in the conclusion section: "A researcher/practitioner interested in applying a discretization method should be aware of the properties that define them in order to choose the most appropriate in each case. The taxonomy developed and the empirical study can help to make this decision." Second is the Bayesian Blocks method, which you can learn more about from this 2012 blog post , which heavily relies on this paper . As noted in the blog: "The adaptive-width bins lead to a very clean representation of the important features in the data. More importantly, these bins are quantifiably optimal, and their properties can be used to make quantitative statistical statements about the nature of the data." (For a pre-built example in Python, you can refer to its implementation in the scikit-hep project .) It's not necessary to do so if you have methods that deal with continuous variables and produce good results for your problem. However, sometimes binning may be the best/most practical approach (for example when you want to learn a Bayesian network from your data), in which case you can try one of the methods from the links I shared above. As already pointed out in a previous answer, this would probably not be a good idea due to the correlation/redundancy that will be present between the original data and what's derived from it by binning.
