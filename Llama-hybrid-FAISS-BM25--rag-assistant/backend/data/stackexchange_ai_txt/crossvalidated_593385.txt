[site]: crossvalidated
[post_id]: 593385
[parent_id]: 558122
[tags]: 
In A Gentle Introduction to Batch Normalization for Deep Neural Networks , Jason Brownlee suggests "The stability to training brought by batch normalization can make training deep networks less sensitive to the choice of weight initialization method". In other words, batch normalisation acts as a form of regularisation, reducing model variance - including the variance due to different weight initialisations. Another way of describing this is in terms of the loss surface. As neural networks are complex, they can have a complex loss surface, with lots of local minima. So a slight change in the initial weights values can lead to a different local minimum (in other words, a different solution). Santurkar et al. How Does Batch Normalization Help Optimization? found that batch normalisation leads to a smoother loss surface, so there are fewer local minima. Therefore models with different weight initialisations are more likely to converge to the same solution when trained.
