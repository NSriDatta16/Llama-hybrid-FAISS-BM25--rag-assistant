[site]: crossvalidated
[post_id]: 212798
[parent_id]: 163600
[tags]: 
As can be understood from the above answers, pre-training was 'fashioned out' when multiple things happened. However, I do want to distill my understanding of it: Long time ago in 2010, everyone cared about pre-training. Here is a great paper on the subject that I did not see brought up. Slightly before before Alex Krizhevsky, Ilya Sutskever and Geoff Hinton published their imagenet paper, people still believed features mattered but were focused mostly on unsupervised learning and even self taught learning to manufacture those features. It is not hard to see why - the building blocks of neural networks at the time were not as robust and converged very slowly to useful features. Many times they even failed spectacularly. Pre training was useful when you had ample data you could get a good initialization for SGD. When relu was brought up, networks converged faster. When leaky relu and more recent solutions were brought up, neural nets became more robust machines when it comes to converging to a viable result. I highly recommend that you play with an excellent neural networks demo this talented googler wrote , you will see what I am talking about. Getting to our main point, that is not to say that some form of Pre-training is not important in deep learning. If you want to get state of the art results you have to perform pre-processing of the data (ZCA for example) and properly choose the initial weights - this is a very good paper on the subject . So you see, pre-training changed in form to pre-processing and weights initialization but remained in function and it became more elegant. As a final note, machine learning is very fashionable. I am personally betting like Andrew Ng that unsupervised and self-taught learning will be dominant in the future so don't make this a religion :)
