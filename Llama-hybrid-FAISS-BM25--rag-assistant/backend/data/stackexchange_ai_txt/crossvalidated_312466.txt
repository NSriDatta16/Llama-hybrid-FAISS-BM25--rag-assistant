[site]: crossvalidated
[post_id]: 312466
[parent_id]: 
[tags]: 
Does PLS have a corresponding objective function to PCA's?

Paraphrased from Understanding Machine Learning by Shalev-Shwartz: Let $\mathbf{x}_1, \dots, \mathbf{x}_m \in \mathbb{R}^d$, $\mathbf{W}$ an $n \times d$ matrix with real entries, and $\mathbf{U}$ a $d \times n$ matrix with real entries, with $n What is the corresponding objective funtion for partial least squares (PLS), if there is one? From what I'm getting from Introduction to Statistical Learning (ISL) by James et al. (anyone who's read this book would know it doesn't go into much mathematical detail), my guess would be if $y_1, \dots, y_m\in \mathbb{R}$, the objective function is finding the $n \times d$ matrix $\mathbf{V}$ and the $n \times 1$ matrix $\boldsymbol\beta$ which minimize $$\sum_{i=1}^{m}\left[y_i-(\mathbf{V}\mathbf{x}_i)^{T}\boldsymbol\beta\right]^{2}$$ since ISL states: Like PCR, PLS is a dimension reduction method, which first identifies a new set of features $Z_1, \dots, Z_M$ $(M$ corresponding to $n$ in this case) that are linear combinations of the original features, and then fits a linear model via least squares using these $M$ new features. Am I interpreting this correctly?
