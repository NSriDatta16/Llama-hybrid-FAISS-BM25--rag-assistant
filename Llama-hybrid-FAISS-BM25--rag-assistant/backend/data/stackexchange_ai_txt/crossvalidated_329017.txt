[site]: crossvalidated
[post_id]: 329017
[parent_id]: 
[tags]: 
Why is the optimal policy non-stationary in the case finite-horizon problems, whereas it is stationary in the case of infinite-horizon problems?

I have difficulty understanding the meaning of stationary policy in the RL (MDP) setting. Specifically, let's assume stationary dynamics $$P(s_{t+1}=j|s_t=i,a) = P (s_{k+1}=j|s_k=i,a) \ \forall t,k,i,j,a$$ In other words, given a fixed policy, the probability of transitioning from state $i$ to state $j$ under some action $a$ does not change over time. We know that a stationary policy will always choose the same action in the same state, independent of time, while a non-stationary policy can choose different action in the same state, depending on time. I do not understand why in the case of finite-horizon problems the optimal policy is non-stationary while in the case of infinite horizon problems the optimal policy is stationary. If this is truly the case, why most RL algorithms use stationary policies in episodic settings (i.e. finite-horizon)? Furthermore, in reality, a lot of environments are non-stationary and it makes more sense to use a non-stationary policy instead of a stationary one. Again, why most RL algorithms use stationary policies in these cases, too?
