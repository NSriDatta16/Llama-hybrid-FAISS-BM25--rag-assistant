[site]: crossvalidated
[post_id]: 437502
[parent_id]: 285231
[tags]: 
I'm going to disagree with the premise that unbalanced data isn't a problem in machine learning. Perhaps less so in regression, but it certainly is in classification. Imbalanced Data is relevant in Machine Learning applications because of decreased performance of algorithms (the research I am thinking of is specifically on classifiers) in the setting of class imbalance. Take a simple binary classification problem with 25:1 ratio of training examples of class A' vs. 'class B'. Research has shown that accuracy pertaining to the classification of class B takes a hit simply because of the decreased ratio of training data. Makes sense, as the less # of training examples you have, the poorer your classifier will train on that data. As one of the commenters stated, you can't make something out of nothing. From the papers I've seen, in multiclass classification problems, it seems you need to get to a 10:1 ratio to start having a significant impact on accuracy of the minority class. Perhaps folks who read different literature than I've seen have different opinions. So, the proposed solutions are: Oversampling the minority class, Undersampling the majority class, or using SMOTE on the minority class. Yes, you can't really create data out of nowhere (SMOTE sort-of does, but not exactly) unless you're getting into synthetic data creation for the minority class (no simple method). Other techniques like MixUp and the like potentially fall into this concept, but I think that they are more regularizers than class imbalance solutions. In the papers I have read, Oversampling > SMOTE > Undersampling. Regardless of your technique, you are altering the relationship between majority and minority classes which may affect incidence. In other words, if you are creating a classifier to detect super-rare brain disease X which has an incidence of 1 in 100,000 and your classifier is at 1:1, you might be more sensitive and less specific with a larger number of false positives. If it is important that you detect those cases and arbiter later, you're ok. If not, you wasted a lot of other people's time and money. This problem eventually will need to be dealt with. So to answer the question: tl/dr: Class-balancing operations like Over/Undersampling and SMOTE (and synthetic data) exist to improve machine learning algorithm (classifier) performance by resolving the inherent performance hit in an algorithm caused by the imbalance itself .
