[site]: crossvalidated
[post_id]: 642027
[parent_id]: 642024
[tags]: 
It depends on the nature of your data. If you have a massive sample size and your variables have low noise, adding more variables will make your model better. If you have a tiny sample size and your variables are very noisy, adding more variables will lead to wild overfitting. When you add in a variable, your are hoping that the information your model gains offsets it's new capacity to overfit the data through the added dimensionality. Imagine I'm trying to predict house price and I have neighborhood quality, proximity to downtown core, floor area, house volume, house height, house weight, and house width. If my dataset is small, I'll probably keep the neighborhood quality, distance to downtown and floor area and ditch the rest, because I'd expect that much of the underlying information about house size (as you call it, X) is covered by the floor area. Alternatively I may consider doing PCA to preserve the information in all these X-related variables but with the lowest added dimensionality possible. But if I had a giant dataset, I'll add all these weird house size-related variables because they still have information in them and I have a sample size big enough to not overfit the data too badly. If you want to know for your specific dataset what's appropriate you may want to consider evaluating regression models built with different numbers of these features through cross validation. I'd recommend taking a look at chapter 2 and 6 (6 especially on model selection) of the Introduction to Statistical Learning book to see the problem here formalized a bit better and practical advice on how to determine the right number of features to include. The book is free here https://www.statlearning.com/ and there are YouTube videos where the authors walk through each of the chapters in detail.
