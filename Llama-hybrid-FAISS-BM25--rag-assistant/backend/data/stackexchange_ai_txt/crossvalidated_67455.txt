[site]: crossvalidated
[post_id]: 67455
[parent_id]: 50012
[tags]: 
The regularization parameter is a form of Tikhonov's regulraization term (which is derived from the point of view of adding a preference to the particular solution when looking for the minimum of residuals)- as there are many ways of regularizing the neural network error function. You are simply trying to reduce the risk of overfitting by "punishing" your network for having big weights values. This $$ a\sum_{i,j} w_{ij}^2 $$ term is simply a weighted square of the weights norm $$ a \left \| w \right \|^2 = \left \| (\sqrt{a}I) w \right \|^2 $$ where $a$ is a constant used for balancing the networks fitting to the data ("left half") and its complexity ("right half"). The notion of network complexity is a complex issue on itself, this is why there have been many ideas for a regularization terms. In general, most of the currently used are methods of Tikhonov's regularization , but these are not the only ones.
