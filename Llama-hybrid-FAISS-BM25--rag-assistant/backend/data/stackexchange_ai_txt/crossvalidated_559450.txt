[site]: crossvalidated
[post_id]: 559450
[parent_id]: 559251
[tags]: 
In my understanding it's a consequence of the high number of variables that neural networks tends to require when tackling interesting problems. For simple tasks gradient-free methods work very well and are quite capable of beating gradient-based methods, as many of them deal with non-convex functions/local optima better than the grad-based methods and that tends to be the biggest issue for low dimensional problems. However, as the number of dimensions/model variables increases, two things happen: Local optima cease to be optima and become saddles instead. To be, say, a local minimum a zero-gradient point must be a minimum with respect to every dimension. If you have a million of these, it is practically guaranteed that it won't be a minimum in at least one. Modern gradient-based methods deal with saddles reasonably well, so as models scale up the functions become effectively convex for them. A random perturbation of a solution candidate becomes increasingly unlikely to happen to have a direction similar to that of the gradient. That means that in grad-free methods that rely on such perturbations a lot of them will have to be made before the solution candidates move in the direction of the gradient, as opposed to just performing a random walk. Most grad-free methods fall into this category, and accordingly take a performance hit as models scale up. The exception to that rule are the methods of the evolutionary strategies family. The main idea of these is to accumulate the information about the gradient from multiple perturbations and skew the distribution of subsequent perturbations in a way that makes them more likely to be aligned with the gradient. Those perform reasonably well on deep learning tasks [1]. They require roughly a few times more resources than the amount required by the gradient-based family to do the same job, but offer a superior performance on deceptive problems and improved horizontal scalability. I think the main reason why this approach never attracted mainstream attention is because the amount of parallel hardware required to get to the point where they compare favorably to grad-based is available to very few people in the world. It's been a while, but from what I recall the breakeven point is somewhere in the hundreds of GPUs region. [1] https://eng.uber.com/deep-neuroevolution/
