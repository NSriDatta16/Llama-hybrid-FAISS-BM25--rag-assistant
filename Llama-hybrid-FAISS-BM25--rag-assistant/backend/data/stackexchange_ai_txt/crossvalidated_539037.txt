[site]: crossvalidated
[post_id]: 539037
[parent_id]: 539013
[tags]: 
The question gives a nice formulation of the problem, because it is very general (for instance, it does not assume a group's performance is an additive function of the "scores" of its members), yet is still simple enough to analyze if we look at it right. To get you started, here are some notation and concepts. The key idea is that you are sampling a population of groups, not people. Let this population be $\Omega,$ which therefore has $\binom{N}{n}$ elements. Associated with each group $\omega\in\Omega$ is the group's score, $f(\omega).$ For this analysis I am supposing $f$ is measured without error. (Errors can be accommodated, but that will complicate the model.) We have to track which groups $\omega$ contain which individuals $j.$ To this end it's convenient to define the indicator $$\mathcal{I}(\omega,j) = \left\{\begin{aligned} 1, & \quad j\in \omega \\ 0, & \quad\text{otherwise.}\end{aligned}\right.$$ The "sought measure" of individual $j$ is the sum of the scores of their groups. (Maybe it should be the average, in case groups can be of varying sizes -- but the analysis of the average is similar.) This value is a parameter of the population $\Omega,$ one for each individual: $$\tau_j = \sum_{j\in\omega\in\Omega} f(\omega) = \sum_{\omega\in\Omega} f(\omega)\,\mathcal{I}(\omega, j).$$ The sample is some kind of random sample. If it is obtained without replacement--you never measure the same group twice--the sampling process can be described by a vector $\mathbf{X}=(X_\omega)$ of random variables, one for each $\omega\in\Omega,$ indicating the sample's members: $X_\omega=1$ when $\omega$ is in the sample and otherwise $X_\omega = 0.$ The measure of $j$ in the sample is the sum of $f(\omega)$ over all sampled $\omega$ containing $j.$ This can be expressed as $$t_j(\mathbf X) = \sum_{\omega\in\Omega} f(\omega)\mathcal{I}(\omega,j)X(\omega).$$ It is a linear combination of the $X(\omega),$ whence its expectation is $$E\left[t_j(\mathbf X)\right] = E\left[\sum_{\omega\in\Omega} f(\omega)\mathcal{I}(\omega,j)X(\omega)\right] = \sum_{\omega\in\Omega} f(\omega)\mathcal{I}(\omega,j)E[X(\omega)].$$ You have sampled in such a way that every individual appears the same number of times, $k,$ in the sample. Let's suppose you also sampled "fairly" in the sense that every individual was sampled without any favoritism. This is a form of exchangeability in the sense that the distribution of $\mathbf X$ is the same as the distribution achieved when any two individuals are switched. This implies (among other things) that all the components $X(\omega)$ have the same expectation. Since these components sum to the sample size $M,$ that common expectation must be $$E[X(\omega)] = \frac{M}{\binom{N}{n}}.$$ In this case we can use the sample statistics $t_j$ to estimate the measures by rescaling them appropriately. After all, substituting this expectation into the preceding equation gives $$E\left[\frac{\binom{N}{n}}{M}\,t_j(\mathbf X)\right] = \frac{\binom{N}{n}}{M}\sum_{\omega\in\Omega} f(\omega)\mathcal{I}(\omega,j)\frac{M}{\binom{N}{n}} = \sum_{\omega\in\Omega} f(\omega)\mathcal{I}(\omega,j) = \tau_j.$$ That is, these statistics $\hat\tau_j = \binom{N}{n}t_j/M$ are unbiased estimators of the $\tau_j.$ There are no problems with negative scores or with individuals appearing to make negative contributions to groups: this analysis considers the most general possible score function $f.$ As a tiny example, with $N=6$ individuals (identified with the integers $1,2,\ldots, N$ ) working in groups of $n=3$ I generated the following values of $f.$ [123] [124] [125] [126] [134] [135] [136] [145] [146] [156] -9 -4 1 6 -3 0 3 -1 0 -3 [234] [235] [236] [245] [246] [256] [345] [346] [356] [456] 0 3 6 4 6 6 9 12 15 24 Groups are enclosed in brackets [] and beneath each group $\omega$ is its score $f(\omega).$ This score is a (highly) nonlinear function of the member identifiers, arranged so that on the whole, members with smaller identifiers tend to work in groups with smaller scores. Taking $500$ independent samples of size $M=4$ in a simulation (so that each person participated $k=2$ times), I observed the following distributions of the estimators $\hat\tau_1,\ldots, \hat\tau_N.$ (An example of one of these samples consists of the groups [236] , [156] , [145] , and [234] . All other samples in the simulation were obtained from this one by permuting the individual identifiers.) The vertical red bars plot the true scores $\tau_j.$ (For instance, the true score $\tau_1$ shown in the upper left panel can be computed from the preceding array of all scores as $\tau_1 = -9-4+1+6-3+0+3-1+0-3 = -10,$ which is the sum of all scores with "1" in their headers.) You can see the sample estimates do tend to be centered around the true scores -- but not necessarily symmetrically so, due to the nonlinear nature of $f.$ The next step would be to work out the variances (and covariances) of these estimators -- but that would depend on the specifics of your sampling plan, so I will stop here. The methods can be found in Stephen Thompson, Sampling (any edition).
