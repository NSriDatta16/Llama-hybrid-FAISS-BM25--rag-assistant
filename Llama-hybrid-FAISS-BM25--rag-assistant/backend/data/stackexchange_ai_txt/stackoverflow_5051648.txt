[site]: stackoverflow
[post_id]: 5051648
[parent_id]: 5051500
[tags]: 
most efficient (query performance and memory consumption) By this you probably mean something that is well balanced between the two. Also, I think that the data insertion must be fast. The easiest and maybe sufficient solution would be to use plain array IMO as it is most memory efficient non-compressed form you can store the data. Thus each array element contains the timestamp, id and value . When you query the data with two timestamps begin and end , you determine the locations of the timestamps in the array using binary search . Then, you traverse all the elements and fetch only the ones with id-s of data sources you are interested in. The array elements must be of course ordered by timestamps. The data takes O(n) memory, where the number of log entries is n. Data insertion is O(1) Data retrieval should be something like O(2*log(n) + n*m), where n is the number of elements. If you have more data sources you want to include in the query, then you can store the data source ID-s in a set, thus the complexity would be O(2*log(n) + n*log(m)). There are of course other possibilities that can involve storing the transactions in trees, hashtables or something that mixes these with lists to get more detailed balance between performance/memory consumption. Also, the problems arise, when the amount of logs is large. In that case, you should split the array into files and store the begin/end timestamps the files contain the logs. Then the implementation gets a little bit more complex. Hopefully this helps somehow you to decide the best data structure / implementation for your solution.
