[site]: crossvalidated
[post_id]: 524341
[parent_id]: 524238
[tags]: 
The best way to proceed--both from your and your readers' perspectives--is to repeat your entire modeling process on multiple bootstrap samples of your dataset (re-samples with replacement of the same size as your dataset) and report a summary of those results. That's explained in the context of LASSO for linear regression in Section 6.2 of Statistical Learning with Sparsity , but directly extends to elastic net for logistic regression if you use an appropriate measure for cross-validated selection of $\alpha$ (the ridge versus LASSO tradeoff) and $\lambda$ (penalization), like deviance (log loss). Don't rely on improper scoring rules like accuracy for building your model. The idea of the bootstrap principle is that resampling with replacement from your dataset mimics the process of taking your dataset from the underlying population. So, for example, the (co)variances of coefficient values among models based on multiple bootstrap samples provides an estimate of the (co)variances of coefficient estimates from the model you built on the full data set. Similar arguments apply to any measure of modeling performance. The third and fourth paragraphs of this answer outline the approach in the context of a question on elastic net. This can directly provide further evaluation of modeling bias via the optimism bootstrap . With any predictor-selection method, you will typically find that the retained predictors will differ among bootstrapped samples. That's OK, as you would find the same behavior if you took multiple full samples from the population. Use that result as a caution against the idea that LASSO or elastic net always return the "most important" predictors. Finally, remember that the above estimates the performance of your modeling process , not necessarily that of your model. It provides no protection, for example, if you have a limited data sample that doesn't reliably represent the underlying population of interest.
