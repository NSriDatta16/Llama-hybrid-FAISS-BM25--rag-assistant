[site]: crossvalidated
[post_id]: 177630
[parent_id]: 177453
[tags]: 
Your training target class distribution is highly skewed towards "NorthAmerican". If classes are not very easy to separate, the RF model will end up predicting the target of all new samples as "NorthAmerican". If you believe training class distribution should not be the prior expectation for future samples, you can e.g. assume uniform probability of any class, such that the posterior prediction of the random forest model is not tainted by the skewed training data. In practice, you can incorporate such a flat expectation with bootstrap stratification. If you train 500 trees. You can bootstrap 250 samples from each target class, 25 $\cdot$ 11 = 2750 samples in total for each tree. I simulated 1000 times how many "NorthAmerican" targets would be selected at least once in the RF model. It is very unlikely to include less than 90% of the "NorthAmerican" samples. If you increase number of trees you will get close to 100%. In short: Stratify only, if reasonable. Don't worry about not utilizing the "NorthAmerican" class examples, you will. Here's a thread on how to use stratified random forests in R. simulation of stratified sampling hist( #plot histogram main="Simulating 1000 stratified RF models", xlab="how many northamerican's will be included in a least once tree", x = replicate(1000,#simulate 1000 times length(unique(unlist( #get unique "Northamricans in each forest" replicate(500, #simulate stratified bootstrapping in 500 trees sample(41524,250,replace =T),simplify = F) ))) ) )
