[site]: crossvalidated
[post_id]: 17427
[parent_id]: 17109
[tags]: 
The information is commonly defined as $h(x) = \log p(x)$. There is some nice theory explaining that $\log_2 p(x)$ is the amount of bits you need to code $x$ using $p$. If you want to know more about this read up on arithmetic coding . So how can that solve your problem? Easy. Find some $p$ that represents your data and use $-\log p(x)$ where $x$ is a new sample as a measure of surprise or information of encountering it. The hard thing is to find some model for $p$ and to generate your data. Maybe you can come up with an algorithm that generates matrices which you deem 'probable'. Some ideas for fitting $p$. If you are only looking at 5x5 matrices, you only need $2^{25}$ bits to store all possible matrices, so you can just enumerate all of them and assign a certain probability to each. Use a restricted Boltzmann machine to fit your data (then you'd have to use the free energy as a substitute for information, but that's okay), Use zip as a substitute for $-\log p(x)$ and don't care about the whole probability story from above. It's even formally okay, because you use zip as an approximation to Kolmogorov complexity and this has been done by information theorists as well leading to the normalized compression distance , Maybe use a graphical model to include spatial prior beliefs and use Bernoulli variables locally. To encode translational invariance, you could use an energy based model using a convolutional network . Some of the ideas above are quite heavy and come from machine learning. In case you want to have further advice, just use the comments.
