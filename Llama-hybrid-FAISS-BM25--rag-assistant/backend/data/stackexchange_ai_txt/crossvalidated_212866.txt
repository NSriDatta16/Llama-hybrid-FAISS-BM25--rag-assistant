[site]: crossvalidated
[post_id]: 212866
[parent_id]: 
[tags]: 
On the Bayesian setup in inference

I've been trying to get into the chapter 4 in Lehmann's Theory of point estimation , but I can't seem to understand his presentation of the Bayesian setup. He starts of by the introduction below and after a few examples of uses of Bayesian estimators he outlines the idea (after dots in my photo). I don't know what he means by: $EL(\Theta,d)$. In my opinion there should be two expectations there since we want to find d to minimize (1.1), I can't see how minimizing the one above being sufficient. I've tried with "Law of total expectation" and Fubini but nothing has been satisfactory. I have a similar problem with a theorem which comes right after the second paragraph.
