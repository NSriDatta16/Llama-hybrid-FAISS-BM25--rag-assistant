[site]: crossvalidated
[post_id]: 192021
[parent_id]: 192007
[tags]: 
First of all, let me describe the meaning of over-fitting in general. Overfitting means your model not only fits the relationship between the dependent variables and independent variable, but also fits random noise into it. Here is a good example of underfitting, right fitting and overfitting. Fitting such a over fitting model will result in a very low error in predicting your training data (or you can image that you are using the model fitted by the data to predict the same data, of cause the more complex the model the lower the error) but a very high error when you predict NEW data (testing data). Error can be defined as $\sum(\hat{y} - y)^2$ , where $\hat{y}$ is fitted value. In general, I don't think any of the methods you mentioned in your question would help you to prevent or detect over-fitting in a linear regression model. For example, if you are fitting a linear model between the area of house (Y, in $m^2$ ) and the price of the house (X, in $k$ dollars). The model is like $Y = \alpha + \beta X + \epsilon,$ where $\epsilon \sim N(0, \sigma^2)$ Then, for example, the sum of parameter is $\hat{\alpha} + \hat{\beta} + \hat{\sigma}$ , if I understand your question correctly. However, if you change the unit of price of the house from $k$ dollars to million dollars, your $\hat{\beta}$ will change to $\hat{\beta}/1000$ . Thus the sum of the parameter reduce to $\hat{\alpha} + \hat{\beta}/1000 + \hat{\sigma}$ . But you cannot say either of the model is more over-fitting than the other one even the sum of parameters changes. What I usually use to prevent over-fitting is cross validation . Cross validation means that you split your data into several subsets. For each subset, you use it as testing set while using others as training set to fit a model and use it to predict the testing set and calculate the prediction error for this testing set. Then you average your prediction errors among all the testing set, you'll get a cross validation error. Or for simple cases, I would use adjusted $R^2$ from the output of lm in r. Adjusted $R^2$ takes into the complexity of your model into account. Complexity will tend to reduce the adjusted $R^2$ .
