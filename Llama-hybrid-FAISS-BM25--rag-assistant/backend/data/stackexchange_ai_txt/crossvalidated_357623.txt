[site]: crossvalidated
[post_id]: 357623
[parent_id]: 
[tags]: 
In which scenarios are the in-sample error and training error NOT the same?

In Elements of Statistical Learning , Chapter 7 (pages 228-229), the authors define the optimism of the training error rate as: $$ op\equiv Err_{in}-\overline{err} $$ With the training error $\overline{err}$ defined as: $$ \overline{err} = \frac{1}{N}\sum_{i=1}^{N}{L(y_i,\hat{f}(x_i))} $$ And the in-sample error $Err_{in}$ defined as: $$ Err_{in} = \frac{1}{N}\sum_{i=1}^{N}{E_{Y^0}[L(Y_{i}^{0},\hat{f}(x_i))|\tau]} $$ They also specify that: The $Y^0$ notation indicates that we observe N new response values at each of the training points $x_i, i = 1, 2, . . . ,N$. It is counterintuitive that $\overline{err}$ and $Err_{in}$ are not always the same, but according to the text and to this answer , this is because the response $Y_0$ is not deterministic and can change from one realization to another. This makes sense in the abstract, but in practice I don't get it. All of the supervised learning methods (regression models, NNets, SVM, Tree based methods,...) I can think of will, after training, return the same output for the same input. Any randomness will come from the initial values of the model + the output of the training, but once the training is complete, the model is deterministic. In fact, a model which returned a non deterministic output based on the same input seems to defeat the purpose of supervised learning and would be very hard to manage in production. Under which scenarios does the output of a trained model have a random component? Are there any specific algorithms for which this is the case?
