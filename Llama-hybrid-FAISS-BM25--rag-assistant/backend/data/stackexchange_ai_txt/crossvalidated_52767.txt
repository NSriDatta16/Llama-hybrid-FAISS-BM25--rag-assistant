[site]: crossvalidated
[post_id]: 52767
[parent_id]: 47846
[tags]: 
1. How to decide number of Hidden States (although HMM says we don't need to know, we just have to make a guess, even for making guess what should be the best criteria) The number of hidden states is problem dependent. For example in speech recognition and synthesis, 3 and 5 states are commonly used. The reason for using these is that speech is a highly variable data. So the distribution at different instants of speech sounds (phonemes) varies with time and each state models the different distributions. 2. Once define hidden states let say 5, then how to define initial probabilities for each hidden state and the transitional probabilities among each other... An HMM can be defined by (A, B, $\pi$ ), where A is a matrix of state transition probabilities, B is a vector of state emission probabilities and $\pi$ (a special member of A) is a vector of initial state distributions. The following steps are taken to estimate these parameters: For the A and $\pi$ parameters, randomly initialise the HMM (between 0 and 1) Initialise the B parameter by uniformly segmenting the training data and estimating the global mean and variance. The B parameter deals with the mean and variances of each state Re-estimate and refine the parameters using the Baum-Welch algorithm. This is a variant of the well-known Expectation-Maximation (EM) algorithm. References: Rabiner, L. 1989. A tutorial on hidden Markov models and selected applications in speech recognition. Baum, L.E., T. Petrie, G. Soules and N. Weiss. 1970. A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains. The Annals of Mathematical Statistics Vol. 41, No. 1, pp. 164-171. Dempster, A. P., N. M. Laird, and D. B. Rubin. 1977. Maximum likeli- hood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B (Methodological) 39 (1), pp. 1-38.
