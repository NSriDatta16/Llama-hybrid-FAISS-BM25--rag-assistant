[site]: datascience
[post_id]: 124525
[parent_id]: 124524
[tags]: 
First I will answer your questions: The tokenization strategy is constrained by how the alignment is done. If you need word-level representations for the alignment, then the tokenization should be at word-level. For the encoder part: either an LSTM or a Transformer encoder (like BERT), where all the input sentence is condensed on a fixed-size representation. For the decoder part: either an LSTM or a Transformer decoder. This is a design decision. You choose. However, based on the literature (see this ), I would suggest normalizing flows. You state that "Most text-VAE-based models do not support standard training methods such as backpropagation". I don't agree with that. While this is true for Generative Adversarial Networks (GAN) due to the discrete junction between generator and discriminator, in the case of textual VAEs there is no such problem. The problem with textual VAEs is the need to cram all the information from the input sentence in a fixed-sized representation. This is the same problem that the first LSTM-based attention-less encoder-decoder machine translation models had. Bahdanau's and Luong's attention came to mitigate this problem. If you want to obtain competitive results, you would also need a mechanism that can give the decoder access to more than a fixed-length representation, either attention or something else. Aligning word representation spaces is the base of most unsupervised machine translation approaches. It is challenging, and the results are only good enough to bootstrap a back-translation loop. My guess is that aligning sentence representation spaces is much more difficult, because you cannot apply the same assumptions as with the word-level spaces Probably the most promising things would be to build on top of the ideas of the variational machine translation approaches proposed on the literature (e.g. this ). However, given that a supervised approach to variational machine translation did not take off, I have doubts that an unsupervised one would give good results.
