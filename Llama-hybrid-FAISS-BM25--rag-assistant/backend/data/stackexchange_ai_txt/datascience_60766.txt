[site]: datascience
[post_id]: 60766
[parent_id]: 60764
[tags]: 
When you attempt to train your model without sampling- keeping the imbalanced classes, your model is learning that the easiest way to classify the data is to label everything negative. From an accuracy perspective (total number of correctly classified for each class divided by the total number of instances) your model will have an accuracy of $\frac{128487}{128575}$ or 99%. Essentially it extremely underfits your data to be all one class. Oversampling corrects the imbalance, and makes your algorithm work a little bit harder to figure out the true shape of the data. Lumping everything into one category won't work. You could have also corrected your imbalance by undersampling the negative class. Typically the rule of thumb is undersampling when you have tens of thousands to hundreds of thousands of rows, and oversampling when your data is smaller (tens of thousands or less). Here is a good reference for dealing with class imbalances in machine learning.
