[site]: crossvalidated
[post_id]: 616263
[parent_id]: 
[tags]: 
How to properly put lower/upper limits on a Bayesian posterior

I have a situation where I compute a posterior probability using the vanilla Bayes' Theorem: $P(A|B) \propto P(B|A)P(A)$ As a concrete example, $P(B|A)$ is a Normal distribution establishing the likelihood that a measured value plus random noise is consistent with a known expectation value (and variance), and $P(A)$ is a uniform prior that the measured value is a false positive. In this situation, large outliers will have a posterior probability of zero. Suppose I'd like to say "I can't really be so sure that the outlier is due to noise that I understand (the Normal distribution), so I want to hedge my bets and always leave the posterior with a tiny, minimum probability. Naively, I can just clip/clamp the posterior to lie between $[\epsilon,1-\epsilon]$ , but this won't pass the sniff test of people (like you) who clearly know much more than I do about probability, and will tell me "this is prior information so include it as a prior!" I just don't know enough about this field to see how to do it on the right-hand side of the equation. I'd love some guidance.
