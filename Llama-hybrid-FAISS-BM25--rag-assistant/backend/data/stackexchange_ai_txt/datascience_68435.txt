[site]: datascience
[post_id]: 68435
[parent_id]: 
[tags]: 
When/how should I use the validation set for hyper-parameter sweeps for neural networks?

I know similar questions have been asked so many times, but I couldn't find the answer to this one particularly, at least not in a way that satisfied me. I am very confused about how to use validation sets. I know they are used to perform hyper parameter sweeps but I'm not quite sure in what way. For example, suppose I am trying to decide between a neural network that has 1 hidden layer and one which has 2, and I have a 50/25/25 train/validation/test split of my data. I will not be performing k-fold CV for reasons which are inherent to my problem. Should I be using the validation set during training (to prevent overfitting) and the same validation set after the fact (once when the models are completely trained) to determine which model is better? When do I use the validation set to compare models? And then once I get to the test set, I know I am supposed to train on the training data and validation set together. But should I actually keep the validation set separate from the training set during training of the finalized model to prevent overfitting? Why would I use the separate validation set during training of the individual models to prevent overfitting, but not use it for that purpose (and make it more training data) on the final model?
