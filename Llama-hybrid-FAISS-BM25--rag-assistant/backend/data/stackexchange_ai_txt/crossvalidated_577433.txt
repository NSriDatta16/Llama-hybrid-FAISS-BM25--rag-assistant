[site]: crossvalidated
[post_id]: 577433
[parent_id]: 577431
[tags]: 
Interesting question. Let's break this down into: Why squared error, why mean squared error, and then why root mean squared error. I think that should answer your question. Why squared error (SE) Squared error happens to be a proper scoring rule , which is a really desirable property for your loss function to have (feel free to read up on proper scoring rules by searching this site). However, the squared error can grow simply by just adding more data. So if I have two data sets (maybe one from yesterday and one from today), and they are of different sizes, I could be fooled into thinking my model is doing poorly simply because I had more data today than yesterday. Which leads me to... Why mean squared error (MSE) Taking the mean of the squared eliminates this problem of different data sizes. By taking the average loss, we retain the nice properties of the proper scoring rule, but now can compare the loss of a model on different data sets of possibly different sizes. But the interpretation of MSE is kind of hard. If $y$ is measured in dollars, what is a dollar squared? Which leads me too... Why root mean squared error (RMSE) MSE has weird units, but if we took the square root of MSE the result would be on the scale of $y$ . This makes interpretation a little easier. In summation: SE is a proper scoring rule. We like that To prevent misleading inflation of the error due to sample sizes, we take the average of SE, or MSE MSE is hard to interpret, so instead we take the square root of MSE to get RMSE and have the error units on the same scale as the outcome.
