[site]: datascience
[post_id]: 128280
[parent_id]: 128264
[tags]: 
Standardizing names may not be your only issue here. You should understand the significance of each column before trying to join them in some way as they may not represent the same ground truths. With any ML model or statistical methods - a common phrase you may have already heard is garbage in, garbage out . I would suggest figuring out which attributes of each dataset are of most importance to you and which you think may contribute to your model. Remove insignificant columns: You may find that some columns are completely useless like the img column in the vgchartz data - this seems to be a URL which probably won't provide much use to you in your model without additional pre-processing, it's also not present in any of your other data. It would be wise to familiarize yourself with statistical feature selection techniques . You should go through cleaning each dataset individually to end up with the remaining columns, with normalized shapes (i.e. deconstructing the reviews object from the Open-Critic data, possibly create an average of the reviews from all in the object) and types (i.e. total_shipped is a string of 76.55m which can be converted into 76550000). Modify columns with data imbalances This can be done either by removing the columns with a majority of null values, or look at data imputation. Once each of your datasets are cleaned and normalized, you can start considering joining the data based on similarities, further normalizing values as you join them, be weary of things like currencies and what dates actually represent - are they release date? last updated? Hope this helps, let me know if you have any other questions!
