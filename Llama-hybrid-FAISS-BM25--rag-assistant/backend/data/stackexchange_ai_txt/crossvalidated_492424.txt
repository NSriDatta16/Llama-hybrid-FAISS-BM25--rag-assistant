[site]: crossvalidated
[post_id]: 492424
[parent_id]: 
[tags]: 
Understanding the decision boundary using a tanh activation function in a Neural Network

I was hoping that someone might be able to explain to me a bit why the decision boundary looks the way it does. I believe that this has to do with SGD and this reflects the derivative of the tanh. Could someone explain in more detail for me? Like, when you only have one hidden layer it seems that the decision boundary is linear. Just as an explanation, this a neural network made up of two neurons and one hidden layer. I'm kind of curious why the number of neurons causes the decision boundary to divide these points further into different areas of the graph, but the boundary becomes more curved.
