[site]: datascience
[post_id]: 33312
[parent_id]: 33309
[tags]: 
In general, you can't. You can find a matrix that will minimize $||\tilde{A}^TB-C||$, and another that minimizes $||\tilde{A}-A||$, but, in general, the two matrices you find won't be the same. In fact, the matrix that solves the second problem is always $\tilde{A} = A$, and, if $||{A}^TB-C||$ is not the minimum of the first function, then your problem doesn't have a solution. However , what would a machine learning practicioner do? Instead of finding a matrix that solves both of the problems (that might not exist), you can aim to solve a combination of the two problems. You can aim to find the matrix that minimizes $$||\tilde{A}^TB-C||^2 + ||\tilde{A}-A||^2 $$ or, in general, the matrix that minimizes $$||\tilde{A}^TB-C||^2 + \lambda ||\tilde{A}-A||^2 $$ for some $\lambda >0$. If $\lambda \approx 0$, then you are just solving the first problem, and if $\lambda >>0$, then you are just solving the second one. And , this is just a quadratic optimization problem, that can be solved using gradient descent and will provide a unique global maximum.
