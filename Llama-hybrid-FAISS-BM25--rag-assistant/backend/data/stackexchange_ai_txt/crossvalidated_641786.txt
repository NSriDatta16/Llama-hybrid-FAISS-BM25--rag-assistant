[site]: crossvalidated
[post_id]: 641786
[parent_id]: 472314
[tags]: 
The following idea can be salvaged from this paper "Grokking Modular Arithmetic" https://arxiv.org/pdf/2301.02679.pdf . We will construct a two-layer neural network which computes modular reduction exactly. Just as user @Dave's idea, it is based on the Fourier analysis. The difference here is that we will use a finite (but large) number of frequencies. Let $W^{(1)}=(w^{(1)}_{kn})_{k,n}$ be a $K \times N$ and $W^{(2)}=(w^{(2)}_{kn})_{k,r}$ be a $K \times p$ matrix defined by \begin{align} w_{kn}^{(1)} &= \frac{1}{\sqrt K}\cos(\frac{2k\pi}{p}\frac{n}{2} + \frac{\varphi^{(1)}_k}{2}),\text{ for any }k \in [K],\,n \in [N],\\ w_{kr}^{(2)} &= \cos(-\frac{2k\pi}{p}r - \varphi_k^{(2)}),\text{ for any }k \in [K],\,r \in [p], \end{align} the $\varphi_k^{(\ell)}$ are angles which will be specified later. Here $N \ge p$ and we are interested in computing $n \mod p$ for any integer $n \in [0,N]$ . Let $x\in \mathbb R^N$ be the one-hot encoding of $n$ . Then, the preactivations of $x$ are given by the vector $h(x) :=W x =(w_{kn})_{k \in [K]} \in \mathbb R^K$ is the $n$ column of $W$ . It is clear that \begin{eqnarray} h(x)_k^2 = w_{kn}^2 = \frac{\cos(\frac{2k\pi}{p}n + \varphi_k^{(1)}) + 1}{2K}, \end{eqnarray} where we have used the trigonometric identity $\cos^2 u = (\cos(2u) + 1)/2$ . Finally, one computes the $r$ th output of the network as \begin{eqnarray} \begin{split} \Phi(x)_r &= \frac{1}{2K}\sum_k \cos(-\frac{2k\pi}{p}r - \varphi_k^{(2)})\\ &\quad + \frac{1}{2K}\sum_k \cos(-\frac{2k\pi}{p}r - \varphi_k^{(2)})\cos(\frac{2k\pi}{p}n + \varphi_k^{(1)}). \end{split} \end{eqnarray} Recall the trigonometric identity \begin{eqnarray} \cos(\frac{a+b}{2})\cos(\frac{a-b}{2}) = \frac{\cos a + \cos b}{2}. \end{eqnarray} Set $a+b=2u$ and $a-b=2v$ , we get $a=u+v$ and $v=a-b$ , and so \begin{eqnarray} \cos u \cos v = \frac{\cos(u+v) + \cos(u-v)}{2}. \end{eqnarray} Letting $v=-(2\pi k/p)r - \varphi_k^{(2)}$ and $u=(2\pi k/p)n + \varphi_k^{(1)}$ , we get \begin{eqnarray} \begin{split} \Phi(x)_r &= (W_2 h(x))_r = \frac{1}{2K}\sum_k \cos(\frac{2k\pi}{p} r + \varphi_k^{(2)})\\ &\quad + \frac{1}{4K}\sum_k \cos(\frac{2k\pi}{p} (n-r) + \varphi^{(1)}_k - \varphi_k^{(2)})\\ &\quad + \frac{1}{4K}\sum_k \cos(\frac{2k\pi}{p} (n+r) + \varphi^{(1)}_k + \varphi_k^{(2)}). \end{split} \end{eqnarray} Now let $\varphi_1^{(1)},\ldots,\varphi_K^{(1)}$ be iid uniform on $[-2\pi,2\pi]$ and take $\varphi_k^{(2)} = \varphi_k^{(1)}$ for all $k$ . Then, first and third terms are proportional to quantities of the form $$ \frac{1}{K}\sum_{k=1}^K \cos(\frac{2k\pi}{p}s + a \varphi_k^{(1)}) = O_{\mathbb P}(\sqrt{\log (N)/K}) = o_{\mathbb P}(1),\text{ for }K \gg \log N, $$ uniformly on all different choices of $s \in [-2N,2N] \cap \mathbb Z$ and $a \in \{1,2\}$ . We have used concentration of a Lipschitz function of subGaussian random variables (here, $\log N$ is the "entropy cost" due to union-bound and the Lipschitz constant of the LHS as a function of the vector of the $\varphi^{(1)}_k$ 's is $1/\sqrt K$ ). On the other hand, one simplifies the 2nd term like so \begin{eqnarray} \begin{split} \frac{1}{4K}\sum_{k=1}^K \cos(\frac{2k\pi}{p}(n-r)) &= \frac{\delta_p(n,r)}{4},\\ \text{ where }\delta_p(n,q) &= \begin{cases} 1,&\mbox{ if }n=r\mod p,\\ 0,&\mbox{ else,} \end{cases} \end{split} \end{eqnarray} provided $K \ge p$ . We conclude that for $K \ge p$ and $K \gg \log N$ , it holds w.h.p that \begin{eqnarray} \Phi(x)_r = \frac{\delta_p(n,r)}{4} + O(\sqrt{\frac{\log N}{K}}) \simeq \frac{\delta_p(n,r)}{4}. \end{eqnarray}
