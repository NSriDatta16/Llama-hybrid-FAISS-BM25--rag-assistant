[site]: crossvalidated
[post_id]: 358924
[parent_id]: 
[tags]: 
Anomaly Detection on Multiple Correlated Timeseries data

I'm just trying to put together a simple algorithm to detect anomalies on multiple correlated timeseries data. Imagine the following timeseries data points: t1=a1,b1,c1 t2=a2,b2,c2 t3=a3,b3,c3 t4=a4,b4,c4 t5=a5,b5,c5 .... Assumption is a, b, c are highly correlated. I'd decompose the individual timeseries of a, b, c using STL decomposition and I would get the residual timeseries (with seasonal and noise removed) respectively. Let's say they are: A,B,C (Residual timeseries). t1=A1,B1,C1 t2=A2,B2,C2 t3=A3,B3,C3 ... To pass the data to the anomaly detector, is it practical to average the every data point and use this as a final residual data. So the final data becomes: t1=AVG(A1,B1,C1) t2=AVG(A2,B2,C2) t3=AVG(A3,B3,C3) ... What are the pros and cons and any suggestions to solve this type of problem?
