[site]: crossvalidated
[post_id]: 255846
[parent_id]: 255823
[tags]: 
After reading the further explanations in the comments (which should definitely be moved to the question itself), I feel that using the same starting point $M_0$ for the MCMC chain is not necessarily damaging to the approximation if A large enough burn-in (or warm-up) chunk of the simulations is not used for the approximation; The total number of MCMC simulations is such that convergence (up to some degree of precision) is guaranteed. My reasoning is the same as with regular Monte Carlo approximations to functions. Using the same simulations for different entries of the function is reducing the variability of the approximation and produces smoother versions of the approximated functions. As we wrote in our book Introducing Monte Carlo Methods with R (2010, Section 5.4.1), If $h(x)$ can be written as $\mathbb{E}[H(x,Z)]$ but is not directly computable, a natural Monte Carlo approximation of $h$ is $$ \hat h(x) = {1\over m} \sum_{i=1}^m H(x,z_i),$$ where the $Z_i$'s are generated from the conditional distribution $f(z|x)$. This approximation yields a convergent estimator of $h(x)$ for every value of $x$ (that is, it provides a pointwise convergent estimator), but its use in optimization setups is not recommended because, since the sample of $Z_i$'s changes with every value of $x$, using an iterative optimization algorithm over the $x$'s will result in an unstable sequence of evaluations of $h$ and thus in a rather noisy resolution to $\arg\max h(x)$.
