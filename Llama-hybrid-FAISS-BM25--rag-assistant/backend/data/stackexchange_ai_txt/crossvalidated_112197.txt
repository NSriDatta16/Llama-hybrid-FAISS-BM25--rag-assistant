[site]: crossvalidated
[post_id]: 112197
[parent_id]: 112144
[tags]: 
I think what's a little tricky is this: in a Student T test for comparing whether two populations have the same sample mean, you have a null hypothesis that the two populations have the same sample mean. Notice that this is as far as you have to specify the null hypothesis, you don't need to specify what both means are for the null hypothesis, just the difference. Because the actual value of the two means does not affect the distribution that results in the t-quantity, only their difference. Here, the situation is quite different. The null hypothesis that both entropies are 0, and the null hypothesis that both entropies are some other value, are very different. If the entropy is truly 0, then you should always measure zero entropy in every test (i.e. there's zero variance). I would take a Bayesian approach instead: assuming uniform priors (on the multnomial distribution parameters, not the entropy) and a measured entropy, you can come up with the distribution of the true entropy using the likelihood function. Note that this basically boils down to a series of questions about the multinomial distribution and frequency counts. That is, you can do the problem by getting a distribution over the multinomial parameters by adding the probabilities over all the permutations of frequency counts (since they all yield the same entropy). Once you have that distribution, you can convert that to a distribution over entropy. If you do that once for each die, then you'll two distributions. You can then get the distribution of the difference. You can see how sharply peaked that distribution is away from 0, and that will tell you about whether the difference in entropy is significant.
