[site]: stackoverflow
[post_id]: 394512
[parent_id]: 345085
[tags]: 
edit: Jack Ganssle has a decent discussion in his book on embedded systems, "The Firmware Handbook" . FYI: If you have accuracy and performance constraints, Taylor series should not be used to approximate functions for numerical purposes. (Save them for your Calculus courses.) They make use of the analyticity of a function at a single point , e.g. the fact that all its derivatives exist at that point. They don't necessarily converge in the interval of interest. Often they do a lousy job of distributing the function approximation's accuracy in order to be "perfect" right near the evaluation point; the error generally zooms upwards as you get away from it. And if you have a function with any noncontinuous derivative (e.g. square waves, triangle waves, and their integrals), a Taylor series will give you the wrong answer. The best "easy" solution, when using a polynomial of maximum degree N to approximate a given function f(x) over an interval x0 Chebyshev approximation ; see Numerical Recipes for a good discussion. Note that the Tj(x) and Tk(x) in the Wolfram article I linked to used the cos and inverse cosine, these are polynomials and in practice you use a recurrence formula to get the coefficients. Again, see Numerical Recipes. edit: Wikipedia has a semi-decent article on approximation theory . One of the sources they cite (Hart, "Computer Approximations") is out of print (& used copies tend to be expensive) but goes into a lot of detail about stuff like this. (Jack Ganssle mentions this in issue 39 of his newsletter The Embedded Muse .) edit 2: Here's some tangible error metrics (see below) for Taylor vs. Chebyshev for sin(x). Some important points to note: that the maximum error of a Taylor series approximation over a given range, is much larger than the maximum error of a Chebyshev approximation of the same degree. (For about the same error, you can get away with one fewer term with Chebyshev, which means faster performance) Range reduction is a huge win. This is because the contribution of higher order polynomials shrinks down when the interval of the approximation is smaller. If you can't get away with range reduction, your coefficients need to be stored with more precision. Don't get me wrong: Taylor series will work properly for sine/cosine (with reasonable precision for the range -pi/2 to +pi/2; technically, with enough terms, you can reach any desired precision for all real inputs, but try to calculate cos(100) using Taylor series and you can't do it unless you use arbitrary-precision arithmetic). If I were stuck on a desert island with a nonscientific calculator, and I needed to calculate sine and cosine, I would probably use Taylor series since the coefficients are easy to remember. But the real world applications for having to write your own sin() or cos() functions are rare enough that you'd be best off using an efficient implementation to reach a desired accuracy -- which the Taylor series is not . Range = -pi/2 to +pi/2, degree 5 (3 terms) Taylor: max error around 4.5e-3, f(x) = x-x 3 /6+x 5 /120 Chebyshev: max error around 7e-5, f(x) = 0.9996949x-0.1656700x 3 +0.0075134x 5 Range = -pi/2 to +pi/2, degree 7 (4 terms) Taylor: max error around 1.5e-4, f(x) = x-x 3 /6+x 5 /120-x 7 /5040 Chebyshev: max error around 6e-7, f(x) = 0.99999660x-0.16664824x 3 +0.00830629x 5 -0.00018363x 7 Range = -pi/4 to +pi/4, degree 3 (2 terms) Taylor: max error around 2.5e-3, f(x) = x-x 3 /6 Chebyshev: max error around 1.5e-4, f(x) = 0.999x-0.1603x 3 Range = -pi/4 to +pi/4, degree 5 (3 terms) Taylor: max error around 3.5e-5, f(x) = x-x 3 /6+x 5 Chebyshev: max error around 6e-7, f(x) = 0.999995x-0.1666016x 3 +0.0081215x 5 Range = -pi/4 to +pi/4, degree 7 (4 terms) Taylor: max error around 3e-7, f(x) = x-x 3 /6+x 5 /120-x 7 /5040 Chebyshev: max error around 1.2e-9, f(x) = 0.999999986x-0.166666367x 3 +0.008331584x 5 -0.000194621x 7
