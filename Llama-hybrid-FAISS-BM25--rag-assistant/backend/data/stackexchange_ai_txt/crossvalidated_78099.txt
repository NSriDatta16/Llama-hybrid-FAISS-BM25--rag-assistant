[site]: crossvalidated
[post_id]: 78099
[parent_id]: 78063
[tags]: 
I'm aware of two related similar approaches in statistics. Trimmed means: when computing the mean, you drop the smallest and largest observations of your data (e.g. the top and bottom $1%$ each; you should do this symmetrically!) Winsorization: similar to the trimmed mean, you only modify extreme observations. However, instead of dropping them, you replace them with the largest/smallest non-extreme observation. This often works slightly better than trimming. For more detailed examples, see Wikipedia: https://en.wikipedia.org/wiki/Trimmed_estimator https://en.wikipedia.org/wiki/Winsorising Note that this works good for some statistics such as when computing the mean. The trimmed / winsorized mean is often a better estimate of the true mean than the artihmetic average. In other cases, it may ruin your statistics. For example when computing variance, trimming will always underestimate your true variance. Winsorization, assuming that indeed some of the extreme observations are faulty, will work a bit better then (it will probably still underestimate, but not by as much). I don't see how replacing the extreme values with the mean would fit in here. However, there is another practice that is related: missing value imputation . Assuming that your outlier is flawed, worthless data, so your remove it. When you then perform imputation, a typical substitute value would be the mean or mode: https://en.wikipedia.org/wiki/Imputation_%28statistics%29
