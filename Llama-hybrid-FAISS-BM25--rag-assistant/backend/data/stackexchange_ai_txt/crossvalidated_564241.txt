[site]: crossvalidated
[post_id]: 564241
[parent_id]: 
[tags]: 
Is it cheating to use least squares to obtain a prior for Bayesian regression?

Say that I have the model $Y=AX + E$ where $Y,X \in \mathbb{R}^{n\times m}$ and that $A$ is an unknown $n \times n$ matrix and E is a matrix of observation noise. When I place an uninformed prior on the parameters in $A$ , NUTS sampler (specifically in PyMC) tends to throw an error that the gradients vanish. When I placed an informed prior (for example, a solution given by least squares), I get no such issue. So I am wondering if this style of prior is considered "cheating" in any way?
