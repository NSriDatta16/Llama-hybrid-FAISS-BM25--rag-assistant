[site]: crossvalidated
[post_id]: 225348
[parent_id]: 
[tags]: 
bayesian classification unknown domain

Suppose I am building a naive Bayes model to classify text messages as either spam or legit. I am training my model using a dataset containing both classes and for which I know the domain (the number of possible words) Obviously, when I will deploy or validate my model, it will encounter new words that occurred 0 times in the training dataset. Using Laplace smoothing (adding all counts by k and dividing by the sum of all occurrence counts plus k times our domain), I can avoid having 0 conditional probabilities However, since the domain will not always be known, should my model increment the count matrix and apply smoothing each time it encounters a new word? wouldn't that be bad performance wise? Is there an alternative to Laplace smoothing that doesn't require a prior knowledge of the domain ?
