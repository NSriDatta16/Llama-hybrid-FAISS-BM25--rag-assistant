[site]: crossvalidated
[post_id]: 177617
[parent_id]: 173060
[tags]: 
I'd say there are three components that are essential in defining big data: the direction of analysis, the size of the data with respect to the population, and the size of the data with respect to computational problems. The question itself posits that hypotheses are developed after data exists. I don't use "collected" because think the word "collected" implies for a purpose and data often exists for no known purpose at the time. The collecting often occurs in big data by bringing existing data together in service of a question. A second important part is that it's not just any data for which post hoc analysis, what one would call exploratory analysis with smaller datasets, is appropriate. It needs to be of sufficient size that it's believed that estimates gathered from it are close enough to population estimates that many smaller sample issues can be ignored. Because of this I'm a little concerned that there is a push right now in the field toward multiple comparison corrections. If you had the whole population, or an approximation that you have good reason to believe is valid, such corrections should be moot. While I realize that it does occur that sometimes problems are posed that really do turn the "big data" into a small sample (e.g. large logistic regressions), that comes down to understanding what a large sample is for a specific question. Many of the multiple comparison questions should instead be turned to a effect size questions. And, of course, the whole idea you'd use tests with alpha = 0.05, as many still do with big data, is just absurd. And finally, small populations don't qualify. In some cases there is a small population and one can collect all of the data required to examine it very easily and allow the first two criteria to be met. The data needs to be of sufficient magnitude that it becomes a computational problem. As such, in some ways we must concede that "big data" may be a transient buzz word and perhaps a phenomenon perpetually in search of strict definition. Some of the things that make "big data" big now will vanish in a few short years and definitions like Hadley's, based on computer capacity, will seem quaint. But at another level computational problems are questions that aren't about computer capacity or perhaps about computer capacity that can never be addressed. I think that in that sense the problems of defining "big data" will continue in the future. One might note that I haven't provided examples or firm definitions of what a hard computational problem is for this domain (there are loads of examples generally in comp sci, and some applicable, that I won't go into). I don't want to make any because I think that will have to remain somewhat open. Over time the collected works of many people come together to make such things easy, more often through software development than hardware at this point. Perhaps the field will have to mature more fully in order to make this last requirement more solidly bounded but the edges will always be fuzzy.
