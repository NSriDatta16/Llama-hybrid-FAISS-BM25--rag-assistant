[site]: crossvalidated
[post_id]: 484312
[parent_id]: 484292
[tags]: 
A potentially bigger problem than data leakage between test and training sets is the unreliability of test/train splits of small data sets. You need many thousands of cases for that to be reliable. Otherwise you are throwing away information by limiting the size of the training set, and you are getting imprecise estimates of model validity by having too small a test set. As you have a well-defined algorithm for building your model, consider internal validation by bootstrapping instead. This answer outlines the procedure. Even if you continue to use a train/test split for your modeling, repeating the entire modeling process including the original train/test split on multiple bootstrap samples of your data will evaluate how much of a problem is imposed by any data leakage. In your case you don't appear to have a problem with data leak in your cross validation. All you have done is to combine raw readings into a type of average, the MAV, without any attempt to standardize the readings within each sensor at that point of the analysis . There might be some standardization later on within your parameter search, but so far as I can tell (I'm not fluent in sklearn ) that seems to be done appropriately. That's different from the situation described in the page you link . There each of the predictors was standardized from the beginning to put them on the same relative scale, as is needed for principal-component analyses and penalized regressions (ridge, LASSO). As the degree of transformation of any predictor needed for the standardization will vary from sample to sample, that can be a problem if your (wise) intent is to repeat the entire modeling process (including the standardization) in each CV fold or bootstrap sample. You haven't fed your parameter search pre-standardized data, just an average without any change of scale, so you shouldn't have that problem.
