[site]: crossvalidated
[post_id]: 327574
[parent_id]: 
[tags]: 
State of the Art Status of Deep Boltzmann Machine and Pretraining

I have been reading some old papers by Hinton on deep Boltzmann machine and deep belief networks, but I wonder what the current status is regarding these models: Are DBM and DBN totally outdated? I can understand they do not work as well as CNN and RNN in terms of NLP and computer vision tasks, but are they useful in other machine learning tasks such as dimension reduction or generative model or density estimation for tabular data? Is the pretraining step needed for multilayer perceptrons? I feel all the reasons that make DBM and DBN difficult to train also apply to a multilayer perceptron, and their argument on why pretraining can prevent overfitting in supervised task makes sense on multilayer perceptron as well.
