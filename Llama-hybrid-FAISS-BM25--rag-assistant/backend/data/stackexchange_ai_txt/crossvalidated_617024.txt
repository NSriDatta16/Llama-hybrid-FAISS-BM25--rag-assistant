[site]: crossvalidated
[post_id]: 617024
[parent_id]: 616940
[tags]: 
You may find yourself interested in Benavoli et al. (JMLR 2017). In addition to making a case for the Bayesian methods the authors like, the paper goes through more classical methods from frequentist statistics that you may prefer. I do not see the relevance of the model construction when it comes to testing. While the fact that you are using different data sources to construct the models might have enormous implications for your work, when it comes to testing, you just have two models that you want to compare, no different than if you wanted to compare a random forest to a neural network. Benavoli et al. (JMLR 2017) measure their models using classification accuracy, but their techniques do not seem to depend on the measure of performance on which the models are compared. If you have reason to want to investigate the ROCAUC, do feel free to use that in the methods in the article. If you need to do power calculations, simulation can be a powerful tool for these kinds of tests where analytical solutions are not straightforward or necessarily even possible. REFERENCE Benavoli, Alessio, et al. "Time for a change: a tutorial for comparing multiple classifiers through Bayesian analysis." The Journal of Machine Learning Research 18.1 (2017): 2653-2688.
