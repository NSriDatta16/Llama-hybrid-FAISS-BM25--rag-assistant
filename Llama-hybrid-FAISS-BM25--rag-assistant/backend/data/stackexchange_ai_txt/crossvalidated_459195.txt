[site]: crossvalidated
[post_id]: 459195
[parent_id]: 
[tags]: 
Optimizing hyperparameters of network with extremely long training time

As an example, let's say i am using a very deep fully convolutional autoencoder to segment lung scans. Input image resolutions will be large, since the features i hope to segment (things like early stage tumors) will be small and detail is important. In addition, lots of noise means lots of layers. Training time will be excessive. When accuracy is critical, so is hyperparameter tuning. There are methods like gaussian optimization that will be much faster than random or grid searches, but even training the full model a single time will be expensive - gaussian optimization will require training several times. My question is, what methodology or "best practice" do we have for situations like this? I read this paper about scaling CNNs, and from what i gathered, you can optimize a less complex model, procedurally scale the model until it is at the complexity you require, and it will be close to optimized. Use a gaussian optimizer once or twice to optimize the complex model further, and your accuracy very well may be superior. Although this is a large ask, i like to think it is very important, and this post could be an extremely useful resource. Many of my associates have encountered this roadblock, and i have not seen a single educational resource that addresses it.
