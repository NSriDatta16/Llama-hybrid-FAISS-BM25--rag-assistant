[site]: crossvalidated
[post_id]: 370004
[parent_id]: 369998
[tags]: 
In my understanding, the expected value of a random variable is not necessarily a good description of it. This depends on what you mean by "description". The expectation has a number of interpretations, all of which might or might not be "good" for you. In frequentist terms, it is the long-run average of a data-generating process . If you draw from a random variable $X$ an infinite number of times, the average of the observations will converge on $E(X)$ . Mathematically, it is a weighted average of the possible outcomes (even in the continuous case if you squint at it). The more probable an outcome, the greater its weight. The expectation is also the center of mass of the probability distribution . This description is appealing in higher dimensions (where you can think of the data as occupying a "blob" in space), and is analogous to the center of mass in physics. Finally, the expectation is a location parameter . This means that a change in the expectation of the distribution represents a shift in the density of the distribution. If you change the expected value, it's like you are picking the density of the distribution up off the graph, and just dropping it elsewhere, without otherwise modifying its shape. The "not necessarily a good description" criticism is probably related to the fact that, in highly skewed or heavy-tailed distributions, very few observations are actually near the expected value point. This is valid, but probably not something we have the luxury to care about. As I mention below, we don't really have an alternative. I am wondering why do we assume that the expected value of loss is considered a good description of the random variable? It is a location parameter. Smaller loss is good. If the location of the loss distribution is lower, then loss on average is smaller. This is what we want. It's relatively easy to compute. The fact that it's linear is especially helpful. The alternative location parameters (median, mode, ...?) are not so easy to compute, and are arguably less representative than the mean. We use it everywhere else anyway. In economics and decision theory, some of the easiest utility functions to work with imply that agents minimize expected loss (or equivalently maximize expected gain). This is what it comes down to: we can compute it, it works for the most part, and there isn't a clear alternative.
