[site]: datascience
[post_id]: 84817
[parent_id]: 84810
[tags]: 
Here are the Options: Oversampling- sure, there are some possibly good ones like SMOTE etc. Just apply it after Train test split to avoid leakage. Undersampling - reducing the 30000 to a certain number where what is left is representative of the Information you Need to classify this class. You could, for example, apply some unsupervised learning to see which clusters inside this 30k are available and then only sample from These clusters until you have 300 examples. Or apply other undersampling techniques. GANS- finally even tough really powerfull in certain Scenarios they are also VERY expensive. I would advise you to try GANS as your final Resort since it will take time for the Network to generate good examples. CONCLUSION: Maybe you expected a decisive yes or no for GANS, but the truth About it is, its an Experiment. It might work, and it might not. Just like there are situations where NN are terrible. After update: Theoretically you can apply most of them without any labels, just mark These 300 Points as one label and see what you get. Ofcourse without a clean Goal in mind you could justify any Output as reasonable. SMOTE has many variants. SMOTE should be treated as a conservative density estimation of the data, which makes the conservative assumption that the line segments between close neighbors of some class belong to the same class. Sampling from this rough, conservative density estimation absolutely makes sense, but does not work necessarily, depending on the distribution of the data. There are more advanced variants of SMOTE carrying out more proper density estimation. Here is a repo with a lot of smote variations . Here is also git repo for tabular GANS data Augmentation, this should be easy to consume
