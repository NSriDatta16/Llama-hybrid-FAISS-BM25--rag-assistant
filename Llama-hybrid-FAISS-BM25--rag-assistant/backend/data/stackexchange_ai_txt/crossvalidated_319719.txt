[site]: crossvalidated
[post_id]: 319719
[parent_id]: 319627
[tags]: 
For index set $\mathcal{T}$, is custom in some time series analysis texts to denote by $\{\mu_t\}_{t \in \mathcal{T}}$ the time-deterministic component of your series $\{\widetilde{Y}_t\}_{t \in \mathcal{T}}$. Making this more explicit, one may write any trend-stationary $\{\widetilde{Y}_t\}_{t \in \mathcal{T}}$ as the decomposition \begin{align} \widetilde{Y}_t = \mu_t + Y_t + \varepsilon_t, \end{align} where $\mu_t:\mathcal{T} \to \mathbb{R}$ is purely deterministic (i.e., purely a function of time), the component $ Y_t$ is a purely non-deterministic (i.e., stochastic) trend term and is weakly stationary , and $\varepsilon_t$ is your iid white noise term. Now since stationarity is a property of stochastic sequences and $\mu_t$ is purely-deterministic, you don't care for it. Instead, you define $Y_t = \widetilde{Y}_t - \mu_t$, which will be purely non-deterministic. You then calculate auto-covariances for this transformed process. Since $\mu_t$ is non-stochastic, these auto-covariances will be identical to those of $\widetilde{Y}_t$. In practice of course, you don't know $\mu_t$. Instead, you give it some functional form with parameters $\theta$, for example $\mu_t = \theta_0 + t\theta_1$ and estimate those parameters to obtain $Y_t$ from the observations $\widetilde{Y}_t$. Alternatively (and that is what the autocovariance estimator in your post is based upon), you assume that $\mu_t = \mu$ for all $t \in \mathcal{T}$, and you simply estimate it by the mean of $\widetilde{Y}_t$. Once you have obtained the autocovariance estimates $\hat{\gamma}(h)$ for lag lengths $h$ that you deem relevant, you can also transform them into autocorrelation estimates by defining $\hat{\rho}(h) = \hat{\gamma}(h)/\hat{\gamma}(0)$.
