[site]: crossvalidated
[post_id]: 388372
[parent_id]: 387285
[tags]: 
After a brief communication with the author of the paper (Olivier Gascuel), I received the explanation below. In short: PLS-DA predicts a value between 0 and 1, which classifies the observation by comparing the value to a threshold: all values above the threshold, e.g. 0.5, are classified as of class 1, else 0. As the threshold, 0.5 could be chosen continuously with infinite precision, the value of N is infinite as well. This means that the paper I provided by O. Gascuel is not fit for my intended use. Comments regarding Vapnik-Chervonenkis dimensionality by @cbeleites are supported by Olivier. Dear Linus, that’s an ancient paper… but basically this paper deals with the case where the number of classifiers is finite (e.g. decision trees with categorical variables). In the (usual) case where the number of classifiers is infinite (e.g. linear discrimination, as the weights are continuous and infinitely many) another approach must be used, typically based on the Vapnik-Chervonenkis (VC) dimension. But since 1992 I’m pretty sure that new approaches have been proposed, especially in the recent period with the incredible mood for machine learning. Hope this helps, regards, Olivier.
