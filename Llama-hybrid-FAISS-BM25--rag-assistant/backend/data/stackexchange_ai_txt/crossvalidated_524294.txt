[site]: crossvalidated
[post_id]: 524294
[parent_id]: 524075
[tags]: 
What you refer to as $P(X=\text{head})$ is in fact an expected value $E(p)$ of a prior distribution that you've chosen for $p$ . You noticed correctly that the distribution can take only two values $0.4$ and $0.6$ , so $0.58$ is an impossible result for such distribution. There is nothing wrong with this. Bernoulli distribution can take only two values $0$ and $1$ , and its expected value is a real number in the unit interval. The expected value of a discrete distribution, like binomial, or Poisson can be a real number, no matter that those are distributions for integer-valued things. This has nothing to do with Bayesian statistics, it's how random variables and expected values work. It’s worth pointing out that the expected value of the prior distribution for $p$ is not the “subjective probability”, it’s just a numerical summary of the prior distribution that you assumed for the parameter $p$ . This summary may, or may not make sense to describe the distribution. Your prior is the whole distribution, not its expected value. If your prior probability distribution is $$ \begin{align} P(p=0.6)&=0.9 \\ P(p=0.4)&=0.1\text{,} \end{align} $$ and based on this you need to make a single bet, then you could simply bet on $0.6$ as it has a higher probability. There is no reason why you would bet for the impossible $0.58$ average. Expected value just tells you what is $p$ on average , nothing more than this. This choice of the prior for $p$ is rather strange, but I understand that this is a made-up example. As you noticed, the distribution can take only two values. If you plug in the prior into the Bayes theorem $$ P(A|B) = \frac{P(B|A)\,P(A)}{P(B)} $$ the result would also need to be defined in terms of those two values. The more usual choice of a prior for probability $p$ would be a distribution over the whole range of possible values for probabilities, e.g. uniform over $(0, 1)$ , or beta distribution . Such distributions assign non-zero probabilities to all the possible values of $p$ , so you could update the prior with the data (through the likelihood ), and come up with an estimate. Usually, you have no way of knowing for certain that the parameter is a particular real number, so both such prior makes more sense, and problems, as described by you, don't arise. If you collect the data, the more data you have , the closer the result would be to what you observed in the data. Given that you used a reasonable prior (that assigns non-zero probabilities to the possible values of the parameter), this methodology enables you to estimate the distribution of possible values of the parameters given the data and the prior. What is probability? Bruno de Finetti once said that "probability does not exist". Probability is an abstract, mathematical concept. There are no "probabilities" in the wild. If you toss a coin , an unlimited number of different factors make it hard to predict. Each of those factors alone can be described in terms of laws of physics, but together they form a chaotic system. We assign a probability to simplify all those factors to a single number, but there is no "probability" that is responsible for the result of the coin toss. Bayesians reason in terms of subjective probabilities , the probability is a number between zero and one that measures how much I believe in something. If I needed to make a bet, the number would help me quantify the amount of the bet that would be reasonable based on my assumptions .
