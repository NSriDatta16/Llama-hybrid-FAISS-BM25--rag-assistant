[site]: stackoverflow
[post_id]: 3548951
[parent_id]: 
[tags]: 
Storing 200 million variable length strings causes fragmentation/table bloat

I am attempting to store over 200 million key value pairs. The value for over 50% of the keys will change over the course of the week, and about 5% of the rows will be permanently removed. Using traditional SQL solutions, this has proven to cause a large amount of fragmentation, causing table bloat (4x the original table size), and some performance issues. It takes considerable down time to resolve this fragmentation in SQL. We have used both reindexing and reorganizing techniques, but both have failed to keep up with the fragmentation. In addition I need to replicate this data to 2 other systems, which has also proven to be quite problematic. The Table design is simple: key NVARCHAR(50) value VARCHAR(MAX) We are considering using other technologies like MongoDB, but fear we will run into similar fragmentation issues. Does anyone have any suggestions on how we can come at this problem in a different way that might limit the fragmentation?
