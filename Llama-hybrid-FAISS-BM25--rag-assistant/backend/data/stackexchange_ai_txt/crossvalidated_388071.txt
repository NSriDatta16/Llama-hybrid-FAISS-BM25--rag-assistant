[site]: crossvalidated
[post_id]: 388071
[parent_id]: 387257
[tags]: 
You ask " if I am making a rookie mistake somewhere". By far, the biggest rookie mistake you're making, is not providing us with a sample dataset. If your data are proprietary, build a synthetic dataset or use an open one (there are open datasets also for industrial applications). Too time-consuming? Tough luck. Most results/best practices in machine learning depend on the dataset, so without seeing it, we cannot help you much. Actually, I'm not even sure this is a suitable question for this site. At the very least, tell us more about the problem: image classification (I guess)? How many classes? Balanced? Unbalanced? Which are the frequencies in the dataset? Show us the ROC curve (yes, you can plot ROC curves for multiclass problems too) . Having said that, here are a few things that stick out: You say that 80% accuracy "is not satisfying". Have you tried estimating the Bayes error rate? It may be that, with your dataset, and the features you're using, it's impossible to increase the accuracy significantly above 80%. Which is the human accuracy for this task, on a random sample? PS the human performing the task must not already know the right labels, of course (thus you may not be the right candidate for this "human test"). The architecture seems some weird variation over VGG-19. I guess you know already that this is not even remotely optimal for image classification: if not, read about Inception, ResNet, ResNeXt or NASNet. BatchNorm tends not to play nicely with Dropout. Are you sure that you need both of them for all the convolutional layers? The fact that you have low dropout rates for all layers except the FC ones, may indicate that by eliminating some (possibly all) the dropout layers after the convolutional layers, you'll get statistically insignificant differences in results. I'd rather use RMSProp or Adam (AdamW if you were using weight decay, but you're not), rather than Adadelta. But I bet it won't make a big difference. What may make a difference, instead, is using SGD with Nesterov momentum and a good learning rate schedule. You don't show us the model.fit , so it's impossible to know if you made some silly mistakes with minibatches and/or validation. You shouldn't control accuracy anyway. I know you're an industrial Data Scientist and accuracy is the only metric your stakeholders/internal customers understand, but that's your problem and you have to figure out a solution to that. It doesn't change the fact that controlling validation accuracy is a crappy way to perform model selection/tune hyperparameters. You should control validation accuracy, or you'll be in for a nasty surprise when you'll apply your model to data never seen before (thus, not the data you used to select the current architecture). Have a look at Why is accuracy not the best measure for assessing classification models? Good accuracy despite high loss value How is it possible that validation loss is increasing while validation accuracy is increasing as well "The model is being trained on synthesized data and applied to real world images". This has the potential to be a HUGE issue: distribution shift/label shift is a real thing, and neural networks trained on the MNIST handwritten digits dataset don't even learn to recognize handwritten digits (except those in the MNIST test dataset). Thus I would be very afraid of deploying in production a model trained on data which don't come from the actual data distribution. Finally, why do you halve the number of channels after maxpooling? Usually you either double or keep constant . See the VGG-1X architectures here .
