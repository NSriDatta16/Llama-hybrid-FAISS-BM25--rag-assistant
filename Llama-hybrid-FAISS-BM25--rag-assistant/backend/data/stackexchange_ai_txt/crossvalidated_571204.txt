[site]: crossvalidated
[post_id]: 571204
[parent_id]: 34357
[tags]: 
Actually, no assumption is made Q-learning regarding the transition function other than that it exists and is a probability distribution. The definition of a Q-function is given by $$Q(s, a) = \mathbb{E}_{\tau \sim (\pi, p)}[r(s, a) + \gamma v_\pi(S') | S_t=s, A_t=a]\;$$ where $v_\pi$ is the value function of policy $\pi$ , and $\tau$ is the trajectory which we assume to be generated according to the policy $\pi$ and transition function $p$ . What this essentially means is that the Q-function is the expected (discounted) returns of taking action $a$ in state $s$ and following $\pi$ thereafter to generate the rest of the trajectory until termination. Now, $\pi$ is used to generate actions whilst the transition function $p$ determines the state transition dynamics, i.e. it gives us $p(s' | s, a)$ (that is, the probability of transitioning to state $s'$ given that we took action $a$ in state $s$ ). You can see that this definition, as I stated at the beginning of the answer, makes no underlying assumptions about whether or not $p$ is stochastic or deterministic. Now suppose the grid is a stochastic environment, an agent can move up/left/right with 1/3 probability. What you are describing here is not totally clear (at least to me), but I would not describe this as an instance of a stochastic environment. A clearer example would be that, if the action 'up' is taken, then with probability $x$ the agent actually moves up, and with probability $1-x$ the agent moves instead to the right (for some $x \in (0, 1)$ ).
