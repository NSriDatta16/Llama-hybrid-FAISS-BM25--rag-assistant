[site]: crossvalidated
[post_id]: 415790
[parent_id]: 415727
[tags]: 
Perhaps what the author was getting as is distinguishing between the posterior distribution and an individual's posterior distribution. Suppose we knew as a fact that some of our parameters followed a given distribution before the data was distributed. Then based on those parameters, we observed some data conditional on those parameters and we knew the form of that conditional distribution. We could then apply Bayes theorem and know the probability distribution of said parameters, conditional on the data we saw. On the other hand, traditional Bayesian statistics tells us that our uncertainty can be a prior distribution. That's fine and I think most modern statisticians have zero problem with that concept. However, it's sometimes swept under the rug that the final output is still conditional on the original prior, and generally speaking there's no reason to believe the next person will have the same prior as you. To illustrate, consider the following R/psuedocode to demonstrate: # Simulate mean first, then simulate data simData = function(n = 10){ # Simulate uniform(0,1) rand_unif = runif(1, min = 0, max = 1) # Mu is either -1 or 1 with probability 0.5 if(rand_unif > 0.5){ mu = 1 } else{ mu = -1 } #Simulate output = rnorm(n, mu = mu, sd = 1) return(output) } Now if I showed you this code in advance and run the function, you can easily compute the probability that mu = 1 in the given simulation, despite never seeing it. This will unquestionable be correct (up to numerical error). On the other hand, suppose you know the code up to # Mu is some value simData = function(n = 10){ mu = cliffsFavoriteNumber #Simulate output = rnorm(n, mu = mu, sd = 1) return(output) } People other than me don't know the distribution on cliffsFavoriteNumber . Maybe Joe says "I don't think Cliff would choose a big number and it's gotta be a positive integer, so I'd say the prior for mu is a rounded exponential". Bob thinks "Cliff loves multiples of 9, so I'd say uniform prior on 9, 18, ..., 81". The computations they can compute from there are not mathematically wrong, but they condition on different people's prior believes and as such end up with very different answers. Thus, it's a little misleading for Joe to say something like " The posterior distribution is ...". It's more precise to say something like "Conditional on believing that the prior distribution on mu is a rounded exponential, the posterior distribution will be ...".
