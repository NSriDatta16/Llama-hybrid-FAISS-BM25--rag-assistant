[site]: crossvalidated
[post_id]: 113443
[parent_id]: 113417
[tags]: 
Let $X$ and $Y$ denote random variables such that $E[X^2]$ and $E[Y^2]$ are finite. Then, $E[XY]$, $E[X]$ and $E[Y]$ all are finite. Restricting our attention to such random variables, let $A$ denote the statement that $X$ and $Y$ are independent random variables and $B$ the statement that $X$ and $Y$ are uncorrelated random variables, that is, $E[XY] = E[X]E[Y]$. Then we know that $A$ implies $B$, that is, independent random variables are uncorrelated random variables. Indeed, one definition of independent random variables is that $E[g(X)h(Y)]$ equals $E[g(X)]E[h(Y)]$ for all measurable functions $g(\cdot)$ and $h(\cdot)$). This is usually expressed as $$A \implies B.$$ But $A \implies B$ is logically equivalent to $\neg B \implies \neg A$, that is, correlated random variables are dependent random variables. If $E[XY]$, $E[X]$ or $E[Y]$ are not finite or do not exist, then it is not possible to say whether $X$ and $Y$ are uncorrelated or not in the classical meaning of uncorrelated random variables being those for which $E[XY] = E[X]E[Y]$. For example, $X$ and $Y$ could be independent Cauchy random variables (for which the mean does not exist ). Are they uncorrelated random variables in the classical sense?
