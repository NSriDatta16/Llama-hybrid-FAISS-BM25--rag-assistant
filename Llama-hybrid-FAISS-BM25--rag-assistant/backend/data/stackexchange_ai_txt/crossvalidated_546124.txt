[site]: crossvalidated
[post_id]: 546124
[parent_id]: 545207
[tags]: 
The intuitive, short version is that policy gradient methods directly try to find a policy that maximizes the agent's rewards conditioned on the starting state $S_0$ (or in general, the starting state distribution). Thus, when you are computing the returns from a state $S_t$ , you add the discounting factor $\gamma^t$ to the return from that state since it is $t$ time-steps away from $S_0$ . For a full derivation, you can refer to page 56 of the solutions manual , which also admits that the book is a bit unclear on this.
