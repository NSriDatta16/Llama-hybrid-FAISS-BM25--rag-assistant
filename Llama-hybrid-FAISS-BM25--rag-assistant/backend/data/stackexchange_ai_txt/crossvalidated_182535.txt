[site]: crossvalidated
[post_id]: 182535
[parent_id]: 
[tags]: 
Calibrating a Gaussian Process

In Thoughts on Massively Scalable Gaussian Processes (or any other introduction to Gaussian Processes), authors claim that calibrating a Gaussian process is just maximizing: $$\log(p|y,\theta)\propto-\frac{1}{2}[y^T(K_{\theta}+\sigma^2I)^{-1}y + \log|K_{\theta}+\sigma^2I|]$$ Where the optimization takes place over $\theta$. Does it mean that the kernel can be chosen by the algorithm itself ? If I just plug a list of kernels (linear, polynomial, exponential) and select the best fit over their parameters, I am selecting the best model ? By best I mean the model from which I can expect the highest out of sample performance. This way of thinking is very far from what is done when calibrating SVMs, where all the hyperparameters of the kernel (and the kernel itself) are selected by cross-validation.
