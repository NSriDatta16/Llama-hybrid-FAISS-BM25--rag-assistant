[site]: crossvalidated
[post_id]: 457307
[parent_id]: 
[tags]: 
Normalization functions in RNN LSTM

I've read somewhere that the tanh function was introduced in order to combat the problem of vanishing exploding gradient. However not many sources explain why exactly. How I understand it is that during backpropagation through time we are multiplying derivatives from multiple layers, so without normalization the values would make the gradient provide huge or minimal weight updates. Most sources however never mention that tanh is explicitly used for that but the whole cell mechanism can prevent vanishing gradient. So is the tanh used just as normalization function so that the model has better performance or is it used for gradient?
