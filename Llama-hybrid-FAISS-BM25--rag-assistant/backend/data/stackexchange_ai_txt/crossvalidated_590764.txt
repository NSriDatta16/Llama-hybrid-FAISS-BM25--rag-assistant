[site]: crossvalidated
[post_id]: 590764
[parent_id]: 
[tags]: 
Reporting results of random seeds and bootstrapping (stats)

I am trying to compare my model's (a neural network) results with human-scored results on the same type of data. However, there are no ground truth labels (humans disagree on classifying these results), so Cohen's kappa is used to get the measure of agreement. I hypothesize that my model is performing as well as humans at classifying this data, and this seems apparent when you eyeball boxplots of the two samples. However, I need a more rigorous test. My test sample (n=400) is not the same sample as the human results (n Given the smaller human sample, I believe that I should bootstrap both samples to try to approximate more closely the population statistics of the unseen data. But I'm not sure where to go from there. Should I be comparing the sample distributions? Should it be one or two-tailed (given that I'm hoping to perform as well or better)? Etc. I am not making any normality assumptions, so I guess a non-parametric test would be appropriate. Additionally, and less important than the result above, I wanted to compute statistics on how much the random seed influences the results, so I ran the final model on 5 different seeds. The S.D. value of the median of the (5) sample kappas sets is tiny. But that then opened up another question: when I'm bootstrapping my results above, should I only bootstrap against 1 of these sets? And why?
