[site]: datascience
[post_id]: 124627
[parent_id]: 
[tags]: 
Random Forest overfitting to unbalanced data set

I am working on an unbalanced classification problem. I have have 2000 points which are positive, and 6000 points as -ve (chosen randomly from 100k universe of -ve points universe). Although I have ~40 features, I am using top 15 (in order of RF feature importance). After I split my data into X_train, X_test, y_train, y_test I. I train both logistic regression (without regularisation) and RF (with hyperparameter tuning using RandomizedSearchCV with random_grid={ 'max_depth': [4,10,12],'max_features': ['auto', 'sqrt'],'min_samples_leaf': [1, 2, 4],'min_samples_split': [2, 5, 10],'n_estimators': [200, 400, 600, 800]} and RandomForestClassifier(class_weight='balanced') My first observation is: The AUC on RF is 0.99 which clearly indicates overfitting. The LR also gives a fairly high AUC of 0.92 but not as high as RF When I test on a completely new data set (although I do not have true labels), with some domain knowledge, I can say that the LR model gives much better results. This could not be a data leakage problem because LR gives much better results. Another observation is that LR probabilities are distributed to the tail end also however RF there are none after 0.8 My questions are: Why RF is getting overfit in spite of hyperparameter tuning? What are the options?
