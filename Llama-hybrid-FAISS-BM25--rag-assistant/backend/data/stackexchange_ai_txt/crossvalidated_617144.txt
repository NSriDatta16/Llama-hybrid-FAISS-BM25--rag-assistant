[site]: crossvalidated
[post_id]: 617144
[parent_id]: 
[tags]: 
Is ReLU activation function unsuitable for input layer if the input data has high inter-example correlation?

After making a neural network using ReLU as the activation function throughout, I had a look at the input layer activations and noticed that about 10% of the neurons are dead on initialization (never activated). At first I assumed it was caused by the large magnitude of my inputs and/or large input layer weights, but scaling those down made no difference (which in hindsight seems obvious for ReLU). Then I realized that the input feature vectors have high inter-example correlation, and that the linear part of the input layer is essentially taking the dot/inner product of this correlated bundle of vectors, with other random vectors. From this perspective, it seems unsurprising that some of those random vectors will have a negative dot product with the input vectors from the entire example set, and that there's nothing I can do to fix this other than changing the activation function, or de-correlating the input feature vectors by preprocessing. Am I missing something, or are ReLU-activated input neurons unsuitable when the dataset's inputs are highly correlated between examples? Or should I perhaps just subtract the population mean input from all the input vectors to at least center the population at the origin? (Or perhaps learn a bias parameter for each input feature before feeding it in?)
