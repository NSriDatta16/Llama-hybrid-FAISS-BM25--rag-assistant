[site]: crossvalidated
[post_id]: 568855
[parent_id]: 568758
[tags]: 
Let's simulate this and see what happens. set.seed(2022) # Define sample size # N Your proposed methodology results is weaker performance, not stronger. I think I see why you would expect it to be better, however. If you know that the average amount by which the model misses the truth is the RMSE (not quite what RMSE means, but it's close), then adjust the prediction, right? The trouble is that, since we don't know the truth when we make predictions for real (if we did, there would not be anything to predict), we have to guess about adding or subtracting the value by which we are adjusting the prediction. If we guess right, then we improve performance, but if we guess wrong, we lower performance. Interestingly, having equal numbers of right and wrong guesses seems not to cancel out to result in equal performance. Further, we can overshoot the true value, even if we move in the right direction. For instance, if the prediction is lower than the observation and we guess that we should increase the prediction, we might wind up with a larger residual. As an example, consider an $RMSE$ adjustment of $10$ , for a true value of $5$ and a prediction of $4$ . We guess that our prediction is low, so we add $10$ to get an adjusted prediction of $14$ , but now our residual has a magnitude of $9$ instead of $1$ . NOW LET'S PROVE IT My simulation assumes a model of $y_i = \beta_0 + \beta_1x +\epsilon_i$ for $iid$ $\epsilon_i$ . $$RMSE = \sqrt{\mathbb E\left[\epsilon_i^2\right]} = \sqrt{\mathbb E\left[\left(Y_i - \hat Y_i\right)^2\right]}$$ I am going to drop the square root so I don't have to keep writing it, but if we show that $MSE_{original} , then $RMSE_{original} . Let $\tilde Y_i = \hat Y_i + Z_i$ be an adjusted prediction, where $Z_i$ takes $\pm RMSE$ with equal probability. Note that $Z_i$ has positive variance and zero expected value, so its second moment is equal to its variance. $\hat Y_i$ and $Z_i$ are independent, so when we get to the step in the proof where we calculate $\mathbb E\left[\hat Y_iZ\right]$ , we can take that as $\mathbb E\left[\hat Y_i\right]\mathbb E\left[Z\right]$ . The $MSE$ of the adjusted predictions is $\mathbb E\left[ \left( Y_i - \tilde{Y_i} \right)^2 \right]$ . $$ = \mathbb E\left[ Y_i^2 - 2Y_i\tilde Y_i + \tilde Y_i^2 \right]\\= \mathbb E\left[ Y_i^2 -2Y_i(\hat Y_i + Z_i) + (\hat Y_i + Z_i)^2 \right]\\= \mathbb E\left[ Y_i^2 - 2Y_i\hat Y_i -2Y_iZ_i + \hat Y_i^2 + 2\hat Y_i Z_i + Z_i^2 \right]\\= \mathbb E\left[ Y_i^2 - 2Y_i\hat Y_i+\hat Y_i^2 -2Y_iZ_i+2\hat Y_iZ_i + Z_i^2 \right]\\= \mathbb E\left[ (Y_i -\hat Y_i)^2-2Y_iZ_i+2\hat Y_iZ_i + Z_i^2 \right]\\= \mathbb E\left[ \left( Y_i - \hat{Y_i} \right)^2 \right] - 2\mathbb E\left[Y_i Z_i\right] + \mathbb E\left[\hat Y_i Z_i\right] + \mathbb E\left[Z_i^2\right] $$ Since $Y_i$ is a constant, we can pull it out of the expectation. $$ = \mathbb E\left[ \left( Y_i - \hat{Y_i} \right)^2 \right] - 2Y_i \mathbb E\left[Z_i\right] + \mathbb E\left[\hat Y_i Z_i\right] + \mathbb E\left[Z_i^2\right] \\= \mathbb E\left[ \left( Y_i - \hat{Y_i} \right)^2 \right] - 2Y_i \mathbb \times 0 + \mathbb E\left[\hat Y_i\right]\mathbb E\left[Z\right] + \mathbb E\left[Z_i^2\right] \\= \mathbb E\left[ \left( Y_i - \hat{Y_i} \right)^2 \right] - 2Y_i \mathbb \times 0 + \mathbb E\left[\hat Y_i\right]\times 0 + \mathbb E\left[Z_i^2\right] \\= \mathbb E\left[ \left( Y_i - \hat{Y_i} \right)^2 \right] + \mathbb E\left[Z_i^2\right] \\>\mathbb E\left[ \left( Y_i - \hat{Y_i} \right)^2 \right] = MSE_{original}\\\square $$
