[site]: crossvalidated
[post_id]: 423307
[parent_id]: 
[tags]: 
Softmax in last layer - error rises but when using sigmoid error decreases

I wrote a neural network from scratch in Python. It has 1 hidden layer which uses tanh activation function. I train it on Iris and MNIST datasets. When I use Sigmoid in the last layer results are very similar to Tensor Flow implementation. If I change Sigmoid to Softmax, learning error rises instead of falling. I got stuck at it. This is how I calculate learning error: product = labels*pred loss = -(np.sum((np.log(product[product!=0]))) / self.m[kind]) + reg This is my back propagation loop: // iterating from last layer to the first one for i in range(n_depth, 0, -1): if i == n_depth: # last layer dz = (self.cache_a[i] - self.labels[kind]) else: # i = 1; s1 contain n1 neurons; (n1, m) = (n1, n2) x (n2, m) * (n1, m) dz = np.dot(self.weights[i+1].T, prev_dz)*self.d_tanh(self.cache_z[i]) prev_dz = dz self.cache_dw[i] = np.dot(dz, self.cache_a[i-1].T) / self.m[kind] self.cache_db[i] = np.sum(dz, axis=1, keepdims=True) / self.m[kind] # no regularization if(self.regularization): reg = (self.reg_lambda * self.weights[i]) / self.m[kind] else: reg = 0; self.weights[i] = self.weights[i] - self.alpha * (self.cache_dw[i] + reg) self.bias[i] = self.bias[i] - self.alpha * self.cache_db[i] #end for Softmax: def softmax(self, z): """softmax - multi-class, single-label """ t = np.exp(z - np.max(z)) return t / np.sum(t) #end If I remove - np.max(z) from softmax the error increases faster. Errors with - np.max(z) : Last train error: 6.20518290954117 Last cv error: 5.505586272828439 Errors and learning curve without - np.max(z) : Last train error: 13.856890930850874 Last cv error: 15.972665792136079 - np.max(z) "> Forward propagation loop: for i in range(1, len(self.network_map)+1): # cache_z.shape = (n-next + 1, 1) # b's shape is (i+1,1) and it's added to the new matrix before activation fn is applied # it is broadcasted to all results along m axis layer_z = np.dot(weights[i], self.cache_a[i-1]) + bias[i] # Save layerZ in cache for back prop self.cache_z.append(layer_z) if i == len(self.network_map): # Last is softmax/sigmoid layer_a = self.softmax(layer_z) #layer_a = self.sigmoid(layer_z) else: # Non-last are tanh layer_a = self.tanh(layer_z) np.nan_to_num(layer_a, False) self.cache_a.append(layer_a) #end for Most likely the error is somewhere in back prop. but I am not sure. Can you spot it or give me some tips? Here is the whole class: https://github.com/al1357/ml/blob/master/neural_network.py
