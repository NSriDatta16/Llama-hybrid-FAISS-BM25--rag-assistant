[site]: crossvalidated
[post_id]: 615539
[parent_id]: 614382
[tags]: 
I have kind of figured out a (almost) correct answer to my question so I will post it here and leave room for others to weigh in to improve it. Answer to the first question Apparently, there is no consensus as to the definition of the standard error of the weighted mean. Even different statistical softwares use different definitions. However, the most coherent answer that I keep seeing is this for an unbiased estimation of the standard error on a weighted mean: $$ se= \frac{s_w}{\sqrt{\sum_i^n w_i}} $$ where the $s_w$ is the unbiased estimator of the standard deviation of the random variable $X$ and $\sum_i^n w_i$ is the sum of the individual weights that contribute to the unbiased estimation of $X$ . The following link is a statistical note that compares how it is computed in SPSS vs WinCross and SPSS uses the sum of weights as the denominator (which happens to be almost the same as the sample size $n$ in their example). So in the example I provided in my question, the sum of weights is $\sum_i^n w_i = 92$ . Answer to the second question I came up with the following formulas for recursive computation of the weighted mean, weighted standard deviation and the standard error on the weighted mean: Given that the current known data points are $n$ and the next data point that triggers the update is denoted as $n+1$ , we can express the weighted stats like so: Recursive weighted mean: $$ \bar{x}_{w,n+1} = \frac{(\sum_{i=1}^{n} w_i) \bar{x}_{w,n} + w_{n+1} \times x_{n+1}}{\sum_{i=1}^{n} w_i + w_{n+1}} $$ Recusrive weighted standard deviation $$ s_{w,n+1} = \sqrt{\frac{(\sum_{i=1}^{n} w_i) \times (s_{w,n}^2 + [\bar{x}_{w,n} - \bar{x}_{w,n+1}]^2) + w_{n+1} (x_{n+1} - \bar{x}_{w,n+1})^2}{\sum_{i=1}^{n} w_i + w_{n+1}}} $$ Standard error of the weighted mean $$ se_w = \frac{s_{w,n+1}}{\sqrt{\sum_{i=1}^{n} w_i + w_{n+1}}} $$ Python's statsmodels implemented a class that computes all sorts of weighted statistics including the standard deviation and standard error (method under the name std_mean here in their source code. As we can see from their implementation, their unbiased estimator of the standard error with degres of freedom parameter set to $1$ is the formula that I wrote above. This answers my first question as to what should I take as a denominator when computing the unbiased estimation of the standard error on my weighted mean. Using Python I was able to verify the implementation of the above estimators using recursive definitions vs statsmodels 's weighted stats function knowing the full history of data like so: import numpy as np from statsmodels.stats.weightstats import DescrStatsW def update_weighted_mean_se(current_sum_weights, current_weighted_avg, current_weighted_std, new_weight, new_x): ''' Update the weighted statistics (mean, weighted standard deviation and weighted standard error) given the previous sum of weights, previous weighted mean, previous weighted standard deviation, new weight, and new x value. ''' # new weighted mean and weighted standard deviation recursively new_sum_weights = current_sum_weights + new_weight new_weighted_avg = (current_sum_weights*current_weighted_avg + new_weight*new_x) / new_sum_weights new_weighted_std = np.sqrt((current_sum_weights*(current_weighted_std**2 + (current_weighted_avg-new_weighted_avg)**2) + new_weight*(new_x-new_weighted_avg)**2)/new_sum_weights) # new standard error on the weighted mean se_w = new_weighted_std / np.sqrt(new_sum_weights) return new_weighted_avg, new_weighted_std, se_w # define the x measurements and their weights x = np.array([10, 12, 15.2, 12.5, 11]) w = np.array([100, 120, 108, 80, 98]) # calculate the unbiased estimators of avg, std and se (with ddof=1) sum_w = np.sum(w) avg_w = np.sum(w * x) / sum_w std_w = np.sqrt(np.sum(w*(x-avg_w)**2) / (sum_w-1)) se_w = std_w / np.sqrt(sum_w) # add new values and compute weighted stats iteratively new_x_array = np.array([20, 30]) new_weights_array = np.array([200, 150]) for new_x, new_w in zip(new_x_array, new_weights_array): avg_w, std_w, se_w = update_weighted_mean_se(sum_w, avg_w, std_w, new_w, new_x) sum_w+=new_w # verify new weighted stats using the formula (with ddof=1) weighted_stats = DescrStatsW(np.concatenate([x, new_x_array]), weights=np.concatenate([w, new_weights_array]), ddof=1) print('iterative weighted avg = %0.5f' %avg_w) print('iterative weighted std = %0.5f' %std_w) print('iterative weighted se = %0.5f' %se_w) print('statsmodels weighted avg = %0.5f' %weighted_stats.mean) print('statsmodels weighted std = %0.5f' %weighted_stats.std) print('statsmodels weighted se = %0.5f' %weighted_stats.std_mean) >>> OUTPUT: iterative weighted avg = 17.12570 iterative weighted std = 6.88164 iterative weighted se = 0.23521 statsmodels weighted avg = 17.12570 statsmodels weighted std = 6.88539 statsmodels weighted se = 0.23534 My implementation yields the correct weighted average but the standard deviation (and by extention the standard error) are only accurate up to $2$ or $3$ decimal points. This means that my implementation of the standard deviation is not exactly the same as statsmodel 's and there is room for improvement. I wonder if this is just a matter of numerical precision.
