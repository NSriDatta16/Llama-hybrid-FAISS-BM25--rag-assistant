[site]: crossvalidated
[post_id]: 151091
[parent_id]: 149859
[tags]: 
to answer your first question: LDA expects document-word counts of the text corpus (see below) as input, that is, which word occured how often in which document. In the sample you linked, only the book "Moby Dick" is used as a corpus. So in order to obtain several "documents", the book is simply split into several "chunks". Then these chunks (think of them as chapters, or paragraphs) have to be lexicalized in order to have the aforementioned input for LDA. Now concerning an intuitive LDA explanation, let me try :) Let's say you have 3 encyclopedias on the following topics Animals Finance Cars Now as a writing exercise, you randomly (but non-uniformly) select 1000 words from these three books and write a blog post, the next day you do the same (select random words, write blog post) and so on. Now you want to assign several tags to your blog post, basically summarizing what topics they are about. Unfortunately you cannot remember which encyclopedias you used, only how many. If words in your stories often co-occur together, it is quite likely they belong to the same topic (encyclopedia in our thought experiment). So you can come up with a first estimate of these 3 topics. Then, if you analyze how probable it would be to select the same 1000 words required for each one of your stories from the topics you estimated in the step before, you can estimate how likely it is that this story is about that topic. These two estimations to which topics do the words belong? from which topics were the documents created? influence each other and you repeat them until you feel confident that your estimate is stable. This is what a LDA does for you. I hope that helps a bit.
