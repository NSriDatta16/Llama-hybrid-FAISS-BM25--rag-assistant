[site]: crossvalidated
[post_id]: 251158
[parent_id]: 
[tags]: 
Matrix-Based Approach to Mini-batch SGD for Feedforward Neural Networks

I'm attempting the final problem in Chapter 2 of Michael Nielsen's Neural Networks and Deep Learning book . The network in question is a simple 3-layer feedforward neural network with sigmoid activation functions for the neurons in the hidden and output layers. When calculating the gradient, it appears I need to manage derivatives of the cost function with respect to each weight and bias parameter separately for each training example, for each stage of backpropagation until the final step; this leads to some rather ugly code. Is there any way to get around this?
