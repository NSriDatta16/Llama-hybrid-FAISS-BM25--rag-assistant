[site]: crossvalidated
[post_id]: 597969
[parent_id]: 
[tags]: 
Better understanding classification with unbalanced test data from a mathematical perspective

Suppose I want to get a model, such as a neural network, to correctly classify pictures of cats and dogs and I know that the test set contains around $1\%$ of cats and $99\%$ of dogs. My intuition is that if I have very few training data, it would be better to use a training dataset that is highly unbalanced. This is because with few data the model doesn't stand a chance trying to 'understand' what is a cat or a dog, and in this case it would work better by being highly biased towards the highly represented class. It would simply learn to assume a target distribution and behave accordingly. My intuition also says that, on the opposite, if I had a very large training set there would be no need to use an unbalanced train set, since the model can 'understand' the properties of each class and does not need to rely on a priori assumptions on the distribution of the test dataset. So the question could be, if I used, let's say, $50$ -million pictures of cats and dogs each for training set, would I get a worse result than if I presented it with $99$ -million dogs and $1$ -milliom cats? Also, I believe that this would very likely depend on the model used (I assume the answer for a neural network may differs from the answer for SVM).
