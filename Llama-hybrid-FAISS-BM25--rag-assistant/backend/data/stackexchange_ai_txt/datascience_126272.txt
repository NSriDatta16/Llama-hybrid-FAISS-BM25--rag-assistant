[site]: datascience
[post_id]: 126272
[parent_id]: 
[tags]: 
Similarity search with text and tabular data

If I have two documents, D1 and D2 and a function f which computes the (normalized) document embedding E1 = f(D1) then I can get the cosine similarity by E1 @ E2 (with "@" being the dot-product). This is a fast and easy to way to get top N most similar documents. Now, say I have additional data like e.g publishing date and price (just to take an example) that I want to incorporate in the similarity; are there any "usual" way to compute the similarity between D1 and D2 with this additional data in "one go"? I could make one vector consisting of [text_embedding, price, day_in_year] , normalize that vector and again taking the dot product between these new vectors. I do see several issues with this one being the text-embedding consists of e.g 1024 dimensions thus it would clearly dominate the dot product thus the price/publish date might not make such a big difference (if any). I could of course make some weighted similarity/distance between the text, the price and the publishing date, but since I would like to store those vectors in a vectordatabase such that I can do some some similarity search (e.g with pgvector ), it is really not my first choice . It seems like multimodal embedding ( example ) is what I'm looking for, but I can't find that much litterature on how it is done.
