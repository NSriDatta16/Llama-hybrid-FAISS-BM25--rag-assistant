[site]: crossvalidated
[post_id]: 395078
[parent_id]: 
[tags]: 
How to detect and quantify a structural break in time-series (R)

Background So first some background to gauge the level of understanding I might have. Currently completing MSc thesis, statistics has been a negligible part of this although I do have a basic understanding. My current question makes me doubt what I can/should do in practice, reading more and more online and in literature seems to be counterproductive. What am I trying to achieve? So for my thesis I joined a company and the general question I am trying to answer there is essentially how a predictive process is affected by the implementation of certain system (which affects the data used for the predictive process). The desired outcome in this is an understanding of: Is there a noticeable change? (e.g. statistical proof) How large is the change? (in mean and variance) What factors are important in this predictive process (Also how the influence of factors changes from before > after the break) To answer 1 and 2 I obtained historical data in the form of a time-series object (and more but irrelevant at this stage). The software I use is R . Data The data encompasses a weighted score for each day (2.5yrs), indicating how bad the predictive process performed (deviation from the actual event). This one time-series object contains the weighted score for the predictions that occurred from one hour before up until the actual occurrence of the event (1hr interval) for these 2.5 years (so each day has one weighted score for this interval). Likewise, there are multiple time series constructed for other intervals (e.g. 1-2, 2-3 hrs etc.) myts1 Process until now Now I had understood that for question 1 I can apply a test for a structural break to determine if and when the break occurred (with a known break date). For this I use strucchange package in R and utilize the breakpoints function. However, the CUSUM (for unkown break date) test was also recommended by my supervisor. Unsure what is best here? EDIT: I see Andrew's supF test conducts the Chow's test for all possible breaks. Then rejects if the maximum of the F (or Chow) statistics become too large. (Found - perform chow test on time series ) Code to obtain a break date using struccchange library(strucchange) test2 Using this I can obtain a break date and a 95% CI , which tells me that a break has occurred. However, this break is in the mean since the formula is myts1~1, reflecting a regression on a constant. If I understand this correctly, the residuals are the demeaned values of myts1 and therefore I am looking at a change in the mean. The plot visualizes the data with the breakdate and a confidence interval. Questions Q0: Before starting this analysis I was wondering if I should be concerned with how these prediction errors are distributed and correct for certain characteristics? It seems a rather stable process apart from the break occurring and some outliers. Q1: How can I calculate a change in variance ? I can imagine a change in variance could also occur at a different point in time than the mean? Is it correct to say a break in the variance is also a break in the mean, but then a break in the mean of the squared demeaned series? Not much to find about this. Q2: Given I have now obtained sufficient evidence of a break in mean and variance, how can I quantify this change? e.g. the variance has shifted from X to Y after the break date? Is it as simple as splitting the the time-series along the break date and summarizing statistics about both parts? Q3: If I rerun the break analysis for other time intervals, how do I compare how the change in mean and variance evolves for the different prediction horizons. Is this yet again a simply summary of the statistics or is there a test that assesses how different the errors are? addition Q3:## In creating these time series, prediction errors up to 10 hours before the predicted event occurs are considered. Taking one day as an example: predictions are seperated into 1 hour bins (creates 10 bins), then within each bin, all predictions are summarized into a weighted average value(weighed based on a different variable). This means that for each day there is one weighted score per bin (total of 10). Translating this to the time series object that I provided in this post (myts1, covering the last hour) yields the following: A time series in which each point corresponds to the weighted average value for that day in the given time interval. Essentially each bin contains 975 separate days with an average weighted value for each (purely historical). My thoughts on this part: I added an image containing 9 bins out of 10, which clearly shows that the break becomes less noticeable further back in time. Given these 10 time series, I rerun the "Score-CUSUM" (mean/variance) test for each. From there can be determined at which hour the effect of this system becomes "noticeable" (as in absolute change in mean/variance) and usable from an operational point of view. Q3.1 Does it make sense to analyze the time series in this way? I assume it does not matter that I re-run the SCORE-CUSUM test 10 times? Q3.1 How do I deal with a 95% CI that spans 6 months when segmenting the break? (found in bins 4hrs out) Q3.2 Should I be concerned in comparing the different models (errors) across these 10 time intervals? I hope my explanation sufficient, can provide more information if necessary. EDIT: I have added a csv file (seperated by ;) in columnar format, this also includes the number of events that occurred each day, however, there seems to be no correlation when plotted. Link: https://www.dropbox.com/s/5pilmn43bps9ss4/Data.csv?dl=0 EDIT2: Should add that the actual implementation occured around timepoint 2018 day 136 in the timeseries. EDIT3: Added the second prediction interval of hour 1 to 2 as a TS object in R on pastebin: https://pastebin.com/50sb4RtP (limitations in characters of main post)
