[site]: datascience
[post_id]: 93369
[parent_id]: 
[tags]: 
Interpreting one-class SVM

I am new to SVM (one-class) and was practically investigating it. Got some weird result that I can not explain. Let me demonstrate by some small reproducible code and visualization: from sklearn.svm import OneClassSVM import numpy as np import pandas as pd import plotly.express as px training_data = np.random.normal(2,.5,(10000,1)) ###testing data### Y1=np.random.normal(5,1,(1000,1)) Y2=np.random.normal(4,1,(1000,1)) Y3=np.random.normal(3,1,(1000,1)) Fitting model clf = OneClassSVM(gamma='auto').fit(training_data) getting score for each data point pred_training_score=clf.score_samples(training_data) pred_y1_score=clf.score_samples(Y1) pred_y2_score=clf.score_samples(Y2) pred_y3_score=clf.score_samples(Y3) getting prediction### pred_training=clf.predict(training_data) pred_y1=clf.predict(Y1) pred_y2=clf.predict(Y2) pred_y3=clf.predict(Y3) ####visualize the prediction on training data df=pd.DataFrame(np.array([list(range(len(training_data))),training_data.reshape(-1),pred_training,pred_training_score]).T) df.columns=['counter','values','labels','score'] fig = px.scatter(df, x="counter", y="values", color="labels",hover_data=["score"]) fig.show() ####training data visualization output ####Visualize testing data1 (with mean 5) df1=pd.DataFrame(np.array([list(range(len(Y1))),Y1.reshape(-1),pred_y1,pred_y1_score]).T) df1.columns=['counter','values','labels','score'] fig = px.scatter(df1, x="counter", y="values", color="labels",hover_data=["score"]) fig.show() ####testing data (mean 5) visualization output This is great. Because I trained with data with mean 2 and test with mean 5 and almost all test data has been classified with anomaly (label: -1) . The visualization of training data says that a soft margin has been computed and labeled data points near 2 as normal and other are abnormal/anomaly. Now the interesting thing happens if I increase the spread of the data-points by increasing the standard deviation. So lets increase that and train again: training_data = np.random.normal(2,1,(10000,1)) Now visualize the labeling of the training data: The data points which are very close to 2 has been classified as anomaly. Why is that? This gets worse if I increase the SD again: training_data = np.random.normal(2,1.5,(10000,1)) and the visualization: Could you please explain this type of behavior?
