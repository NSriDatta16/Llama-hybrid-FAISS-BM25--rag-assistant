[site]: crossvalidated
[post_id]: 570548
[parent_id]: 570536
[tags]: 
Prior probability vs distribution is the same distinction as between probabilities (in general) and probability distributions . Probability is a number between 0 and 1, the probability distribution is a function $f(x)$ that maps some values $x$ to corresponding probabilities. Probabilities are usually not very interesting because they refer to binary events probability that $A$ is true $P(A)$ , while probability distributions generalize this concept e.g. probability that after rolling a $K$ -sided dice you'd observe $4$ as a result, $P(X=4)$ . This introduces another concept: random variables . If those things are not familiar to you, I recommend a probability theory course or a handbook . Bayes theorem is $$ \underbrace{p(\theta \mid X)}_\text{posterior} = \frac{\overbrace{p(X \mid \theta)}^\text{likelihood} \, \overbrace{p(\theta)}^\text{prior}}{\underbrace{p(X)}_\text{normalizing constant}} $$ Prior is the unconditional probability of $\theta$ that you know a priori , given the data in likelihood it is updated so that you end up with conditional posterior . Next, you could use this posterior as a prior with new data, this is called Bayesian updating . See also Help me understand Bayesian prior and posterior distributions .
