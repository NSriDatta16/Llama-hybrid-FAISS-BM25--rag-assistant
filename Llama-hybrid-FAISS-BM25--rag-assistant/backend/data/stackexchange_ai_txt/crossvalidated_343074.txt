[site]: crossvalidated
[post_id]: 343074
[parent_id]: 340643
[tags]: 
Not quite. What matters is the limiting probability of being in each state, which for many MDPs is independent of the starting state. Assume the limiting probabilities given a policy $\pi$ are a vector $p_{\pi}(s;s_0)$ conditional upon the starting state $s_0$, then: $$\sum_s v_{\pi}(s)p_{\pi}(s;s_0) = 0$$ i.e, the expected value of the value functions (normalized by subtracting the gain, as you have done) has to equal 0. Note that for many chains "limiting probabilities" really are "long run average frequency of state occupation", not probabilities, as it may be, for example, that state A cannot be occupied on odd stages (e.g., stage # 1029111) but can be on even stages. The "average reward" criterion implies (for well-behaved problems) that everything that happens when transiting to the steady state has no impact on the determination of the optimum policy, as its contribution to the average reward goes to zero as the stage $n \to \infty$. This is somewhat of an inverse to the "discounted reward" criterion, in which, if there are an infinite number of transitions to the steady state, it's the steady state that has no impact on the determination of the optimum policy.
