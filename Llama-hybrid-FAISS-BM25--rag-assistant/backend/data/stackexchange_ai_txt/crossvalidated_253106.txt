[site]: crossvalidated
[post_id]: 253106
[parent_id]: 
[tags]: 
Is it worth exploring OOB error to optimize random forest?

The first time I used Random Forest, it gave me an accuracy of 81% with these parameters: n_estimators:10, max_features: sqrt(20), min_samples_split: 2 and min_samples_leaf: 1 Then I decided to optimize. 1) I did a random search. Accuracy: 89.76%, n_estimators: 10, max_features:8, min_samples_split:6, min_samples_leaf:1. This took: 19 minutes. 2) Then I read that it is a good idea to uses OOB error to find optimal values for n_estimators and max_features. So I did it. The plot showed that with less than 100 trees the OOB error is high and inestable. And that max_features: 4 to 10 is good. 3) So I ran once more the random search. But I specified to look for trees over 100. The best parameters are: n_ estimators: 127, max_features: 7, min_samples_split: 5 and min_samples_leaf: 1. Accuracy: 91,56%. Time: 6 hours!!!!! As you can see the increase in accuracy is 3% more. But it took 6 hours. So is it worth increasing the number of trees(n_estimators) from 10 to 127?
