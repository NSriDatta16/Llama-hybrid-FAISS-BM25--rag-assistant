[site]: datascience
[post_id]: 49474
[parent_id]: 
[tags]: 
Chinese Word Segmentation

Following this paper https://www.aclweb.org/anthology/D17-1079 , I'm stuck with Paragraph 2 "Baseline Segmentation Model". In this section they basically explain how they feed the chinese word embeddings into the bi-LSTM model. In particular they use this formula for $e^f_i$ (forward word representation): $$ e^f_i = concat_1(e_{w_i}, e_{w_{i−1}w_i}), = tanh(W_1[e_{w_i}; e_{w_{i−1}w_i}]) $$ where $w_i$ is the $i$ -th character in a sentence $\{w_1,w_2,\dots,w_N\}$ , where $N$ is the sentence length. We have a corresponding embedding $e_{w_i}$ and $e_{w_{i−1}w_i}$ for each character unigram $w_i$ and character bigram $w_{i−1}w_i$ . I don't really get what they are doing here and how can I replicate it with Keras?
