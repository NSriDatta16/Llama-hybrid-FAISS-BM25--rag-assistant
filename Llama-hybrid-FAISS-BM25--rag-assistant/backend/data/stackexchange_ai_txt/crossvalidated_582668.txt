[site]: crossvalidated
[post_id]: 582668
[parent_id]: 582657
[tags]: 
I am not sure what you mean by "shape", because you want to also differentiate e.g. between a constant and a constant at zero. But, maybe, this could be an approach: Think of a family of possible models, e.g. all quadratic polynomials, then create a fitted model $m_i$ for each of your tuples $\mathbf y_i = (y_i^1,\ldots, y_i^n)$ . Finally, cluster those models w.r.t. their parameters. E.g., let's stick with the example of quadratic polynomials. Each model $m_i$ will be given by three parameters $a_{ik}, k=0,1,2$ according to $m_i = a_{i0} + a_{i1}x + a_{i2}x^2$ . Clustering those $\mathbf a_i\in\mathbb R^3$ might give you already some first "shape classification". But, of course, two increasing, equal-linear-shape functions $y_i = a_{i0} + a_{i1}x$ and $y_k = a_{k0} + a_{k1}x$ could still be in different clusters if e.g. $a_{i0}$ and $a_{k0}$ are very different. So you could also think about clustering the projections of the $\mathbf a_i$ to some coordinate subspaces. But this all depends on what your notion of "shape" really is. As an alternative, you might want to consider the curves as images, similar to the digits in MNIST, add a lot of fake functions $\mathbf y_i$ which you label according to their shape, and then apply some standard image classification model (e.g. CNN) to this augmented dataset, which then should classify your data properly into your shape categories. But maybe, this is a little bit of an exaggeration.
