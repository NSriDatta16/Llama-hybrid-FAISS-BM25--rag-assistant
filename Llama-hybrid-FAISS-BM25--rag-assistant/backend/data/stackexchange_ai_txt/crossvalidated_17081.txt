[site]: crossvalidated
[post_id]: 17081
[parent_id]: 17066
[tags]: 
The ESL , as already mentioned by Peter Flom, is an excellent suggestion (note that my link is to the author's homepage where the book can be obtained as a pdf-file for free). Let me add a couple of more specific things to look for in the book: Table 10.1 (page 351) gives the authors assessment of certain characteristics of Neural Nets, SVM, Trees, MARS, and k-NN kernels, which somehow appear to be the methods the authors want to include in a list of "off-the-shelf" methods. Chapter 10 treats boosting, which I found missing in the list of methods in the poll cited by the OP. Gradient boosting seems to be one of the better performing methods in a number of examples. Chapter 9 treats generalized additive models (GAMs), which adds to the logistic regression model (top ranked in the poll ) the flexibility of non-linear additive effects of the predictors. GAMs would not be nearly as easy to use as logistic regression with all the smoothing parameters that have to be chosen if it wasn't for nice implementations like the one in the R package mgcv . Add to the book the Machine Learning Task View for R, which gives some impression of what the many machine learning packages can actually do, though there is no real comparison. For Python users I imagine that scikit.learn is a good place to look. How much "out-of-the-box" or "off-the-shelf" a method is, is very much determined by how well the implementation deals with automatic adaptation to the data situation versus leaving the detailed tuning to the user. In my mind, mgcv for R is a good example that makes the fitting of a reasonably good generalized additive model really easy and basically without any need for the user to "hand-tune" anything.
