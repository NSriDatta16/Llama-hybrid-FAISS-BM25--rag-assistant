[site]: datascience
[post_id]: 86995
[parent_id]: 86993
[tags]: 
XGBoost iteratively trains many trees (=boosting). So there is not only one tree. For example you can plot a single tree, see: https://machinelearningmastery.com/visualize-gradient-boosting-decision-trees-xgboost-python/ A basic decision tree algorithm creates just one tree. If you apply pruning to the tree not all features would be present in the tree. The first split would be the one with the highest importance, ...
