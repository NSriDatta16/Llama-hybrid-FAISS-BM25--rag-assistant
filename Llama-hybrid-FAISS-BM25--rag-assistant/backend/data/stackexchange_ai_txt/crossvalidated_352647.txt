[site]: crossvalidated
[post_id]: 352647
[parent_id]: 
[tags]: 
What is the motivation or objective for adopting Kernel methods? Is kernel trick a feature engineering method?

I come to know that kernel methods can be used in not only SVM but also many machine learning algorithms. I understanding that in SVM, the reason for using kernel trick is that some data are linearly inseparable, and people think the data may be linearly separable in a certain higher dimensional space. While it is difficult or impossible to map the original feature vectors to a higher-dimensional space. Therefore a kernel function can be used to latently do the mapping and the dot product in higher-dimensional space together. So I think the motivation for using kernel methods in SVM is: to convert a linear SVM to a nonlinear SVM, which can classify linearly inseparable data . Q1: So I am wondering what are the purposes of using Kernel methods in other ML algorithms in general? Is it the same: to classify linearly inseparable data with an originally linear model? Q2: I am trying to have a knowledge structure in my mind, so Which super category do kernel methods fall into? Since the kernel functions operate on feature vectors, I couldn't help but relate kernel methods to Feature Engineering(feature selection, feature extraction like PCA, LDA, etc.). Can I state this: kernel methods is a kind of nonlinear Feature Engineering methods ?
