[site]: datascience
[post_id]: 9883
[parent_id]: 
[tags]: 
Non-linear data preprocessing before mini-batch gradient descent

I found some realisation of pretty interesting ML algorithm. https://github.com/EderSantana/DeepEEG We have dataset X, matrix of spacial filter W, matrix of time filter V, and matrices U and B - parameters of classyfier. $Xfilt_i=ln(W*(X_i*v)^2))$ $Z=(Xfilt)^T * U+B$ $Y\_pred = argmax(softmax(Z))$ So, quite simple logistic regression classification. But most inreresting thing was the way authors tune W,V,U and B matrices. W was initialized by CSP algorith. V was initialized by ones, U and B by zeros. And then cross-entropy cost function optimised (mini-batch gradient descent) respect to this parameters. And accuracy, achived by this way pretty high. My quiestion is, why we need non-linearity here (squaring and log) $Xfilt_i=ln(W*(X_i*v)^2))$ ? PS In experiments optimisation algorithm did not converge without this nonlineraity.
