[site]: crossvalidated
[post_id]: 524801
[parent_id]: 
[tags]: 
What is biased? The model, the data or both?

I wonder if we look at current state-of-the-art approaches for NLU like BERT if these approaches may incorporate an inherent bias as well. It may seem quite evident that if BERT produces biased output it is simply because it's picking up the latent structure of its training input, i.e., the bias lies in the data. But then again, how do we know an architecture (in this case BERT) does not favor a structural/sociatal bias, even if we had no bias (whatever that means) in the data, which could be understood as a form of bias as well? What if BERT itself is biased, gets biased data and produces even worse results (i.e., what if both the model and data are biased)? I read this paper, which is talking about a bias in the training data. I'm trying to extend that thought a little bit and would love your view on this.
