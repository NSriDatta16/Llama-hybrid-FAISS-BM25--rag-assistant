[site]: crossvalidated
[post_id]: 414845
[parent_id]: 
[tags]: 
MCMC in Gibbs: Threshold Multivariate Linear Regression

I have a problem regarding what I expected to be a rather straightforward modelling approach: I want to investigate multivariate linear regression parameters and a thresholding parameter which defines 2 different regimes for the data. For simulated data my sampler converges extremely fast to the true threshold value and Bayesian inference suggests virtually no estimation uncertainty which makes me wonder if I did something wrong. Why is identification of a threshold so seemingly easy? Edit: My initial question turned out to be way to naive, instead, I ask: when is identification easy and what can I do to identify this cases? Suppose you have data of the form $Y\in\mathbb{R}^{T \times N}$ , $X\in\mathbb{R}^{T \times K}$ and $r\geq0$ . The data can be separated by a threshold $r$ such that $$Y^0 :=\{Y: |X_{i,L}| for some (known) column $L$ of $X$ . Similar, $$Y^1 :=\{Y: |X_{i,L}|\geq r\} \text{ and } X^1 :=\{X: |X_{i,L}|\geq r\}.$$ The respective size of the partitioned matrices is $Y^0 \in\mathbb{R}^{T^0 \times N}$ and $Y^1 \in\mathbb{R}^{T^1 \times N}$ with $T^0 + T^1 =T$ . The underlying data-generating process takes the form: $$Y^i = X^i\beta^i+U^i \text{ for } i\in\{0,1\} \text{ where }U^i \sim MN(0, \Sigma^i).$$ $r$ is not known ex-ante, so the set of parameters to estimate is $\theta:=\{\beta^0, \beta^i, \Sigma^0, \Sigma^1, r\}$ . Obviously, conditional on $r$ , standard inference from Bayesian multivariate linear regression models applies, but for the sake of completeness I provide the corresponding setup: The likelihood takes the form $$L(Y|\theta,X) \propto \prod\limits_{i\in\{0, 1\}}|\Sigma^i|^{-T^i/2}\exp{\left(-1/2\text{tr}\left({\Sigma^i}^{-1}{U^i}'U^i\right)\right)}$$ Priors are chosen to be conditional conjugate for $\beta^i, \Sigma^i$ : $$p(\Sigma^i)\sim IW(V, v) \text{ and } p(\beta^i|\Sigma^i)\sim MN(B, \Sigma^i)$$ The prior for $r$ is strictly positive, assume for now it is uniform such that $p(r) \sim U[0,b)$ with some $b>0$ . Posterior inference: A standard Gibbs sampling scheme applies for the posterior when conditioning on $r$ (See here (Wikipedia) for the conditional posterior distributions). Basically, given an initial (or sampled) value of $r$ I do the following: - Separate the data $Y$ and $X$ (some consistency check necessary to ensure $T^0$ and $T^1$ are large enough). - Inverse Wishart draw to draw from $\Sigma^0$ and $\Sigma^1$ - Multivariate normal draw to sample from $\beta^0|\Sigma^0$ and $\beta^1|\Sigma^1$ . Now, the part where I want to make use of a random walk metropolis hastings (RWMH) step within Gibbs: sampling from $p(r|\beta^0, \beta^1, \Sigma^0, \Sigma^1, Y, X)$ : RWMH requires evaluation of the (non-normalized) conditional posterior distribution which can be evaluated as follows: $p(r| \beta^0, \beta^1, \Sigma^0, \Sigma^1, Y, X) \propto L(Y|\theta,X).$ (The prior for $r$ does not show up due to the uniform assumption. Therefore, given a proposal draw $\log (r^\text{new})$ sampled from $N(\log(r^\text{old}),c^2)$ where $r^\text{old}$ is the recent (initial) value, I evaluate the likelihood ratio of $r^\text{old}$ and $r^\text{new}$ : Acceptance ratio $$\alpha(r^\text{new}|r^\text{old}) = \min\left(1, \frac{L(Y|r^\text{new}, \beta^0, \beta^1, \Sigma^0, \Sigma^1, X)r^\text{new}}{L(Y|r^\text{old}, \beta^0, \beta^1, \Sigma^0, \Sigma^1, X)r^\text{old}}\right)$$ Likelihood $L(Y|r^\text{new}, \beta^0, \beta^1, \Sigma^0, \Sigma^1, X)$ depends on $r^\text{new}$ due to different segmentation of $Y$ and $X$ as shown above. Values of $\beta^0, \beta^1, \Sigma^1, \Sigma^0$ are kept constant, though, for both evaluated likelihoods due to the conditioning. Therefore, letting $\tilde T^i$ the sample size of $Y^i$ based on the new segmentation due to $r^\text{new}$ and $\tilde U^i$ the corresponding residuals $\tilde Y^i - \tilde X^i\beta^i$ , I get: $$\log\left(\frac{L(Y|r^\text{new}, \beta^0, \beta^1, \Sigma^0, \Sigma^1, X)r^\text{new}}{L(Y|r^\text{old}, \beta^0, \beta^1, \Sigma^0, \Sigma^1, X)r^\text{old}}\right) = \frac{(T^0 - \tilde T ^0)}{2} |\Sigma^0| + \frac{(T^1 - \tilde T ^1)}{2} |\Sigma^1| + \frac{1}{2}\text{tr}({\Sigma^0}^{-1}({U^0}'U^0-{\tilde U^0}'\tilde U^0) + {\Sigma^1}^{-1}({U^1}'U^1-{\tilde U^1}'\tilde U^1 )) + log(\frac{r^\text{new}}{r^\text{old}})$$ Estimation and simulation I simulate data based on the description above and let my sampler run to draw inference regarding all unknown parameters exactly the way described above. I sample 2000 draws (+ 2000 burn in) and discard every 2nd draw ot reduce autocorrelation. My tuning parameter $c$ for the proposal distribution is set such that the acceptance ratio is roughly 15%. The resulting draws for $r$ look as follows (true value is $r=0.4$ ): However, setting r_true=0.32 suddenly prevents the sampler from correctly identifying the threshold. Why? # Simulate data set.seed(3010) T $beta_0 B sample[[i+1]] $Sigma_0 Sigma U_0 $beta_1 B sample[[i+1]] $Sigma_1 Sigma U_1 b | r_proposal $beta_0 tilde_U_1 beta_1 log_acceptance_ratio $Sigma_1))*(T_1 - tilde_T_1) + log(det(sample[[i+1]]$ Sigma_0))*(T_0 - tilde_T_0) + 1/2*sum(diag(solve(sample[[i+1]] $Sigma_1)%*%(crossprod(U_1, U_1) - crossprod(tilde_U_1, tilde_U_1)))) + 1/2*sum(diag(solve(sample[[i+1]]$ Sigma_0)%*%(crossprod(U_0, U_0) - crossprod(tilde_U_0, tilde_U_0)))) + log(r_proposal/r_old) acceptance_ratio
