[site]: crossvalidated
[post_id]: 460901
[parent_id]: 460858
[tags]: 
In a regression problem, you can have predictors with the most different scales. for example, you may have a predictor that ranges between 0 and 1 alongside another that ranges in the thousands, but still, the first one may be an index with a very strong influence on your outcome variable $y$ , much more than the second one. You don't know in principle. But, let's say they have an equal share of the predicted value $\hat y$ . The variance of $\hat y$ can be decomposed (following this fundamental property of variance) in the quadratic form of the parameters $\hat \beta$ and the variance of the predictors. This means that if we want our two predictors to contribute equally to the variability of $\hat y$ , their parameters must be inversely proportional to the respective predictor st. deviation. If they only had equal variance, their parameter would have been the same value. So, if you scale the predictors to have equal variance, their parameter will be proportional to the influence they have on $\hat y$ (more or less, collinearity also plays a role, but LASSO generally leaves out collinear predictors). But why is this so important for LASSO method? Because LASSO applies a regularization on all the parameters of the model, which means from a bayesian perspective, that it applies a prior with the same scale on all the parameters. So for the model to make a sensible selection of the predictors, and a good shrinkage of the parameters, the predictors must have a scale proportional to their importance. For all these reasons, if you don't have prior knowledge of which predictors have more or less effect on $\hat y$ , you just scale all them to have equal variance, otherwise you may get a model dangerously biased towards higher-scale predictors.
