[site]: crossvalidated
[post_id]: 239343
[parent_id]: 238895
[tags]: 
Different learners / classifiers, by their nature, assume that the response function is of a specific character. Consider this figure (copied from the Scikit-learn page on classifier comparison ; click on it for a larger version): In particular, notice that Linear SVM (the third column from the left) outputs parallel bands with varying possible orientations, Decision Tree and Random Forrest models (the sixth and seventh from the left) chop the space into regions with borders strictly parallel to the axes, and QDA (far right column) induces a curve from the family of conic sections. (All the classifiers assume the pattern of the response function has some certain nature, but those are the easiest to see / most distinctive in the figure.) The result of this fact is that a learner will do best if the kind of pattern it is designed to work with approximates the true pattern better than any other learner you try. As @hxd101 notes, "If we have infinite data and unlimited computational power, any function can be approximated with a [sufficiently] complicated [version of any] model [type]". However, short of that, the best matching learner will tend to outperform. To answer your question explicitly, the converse of the aforementioned facts is that, for whatever learner you are using, the function that will be easiest to learn is the one that perfectly matches what the learner is designed to detect. Or put in a different way, there will be no single function type that is easiest to learn irrespective of the model type you use to try to learn it. Feature engineering will help if it moves the pattern towards the best matching type or makes the information in the data easier for the model to extract.
