[site]: crossvalidated
[post_id]: 422706
[parent_id]: 422667
[tags]: 
Fake data for illustration. Suppose you have data from Likert scores (1 worst through 5 best) as follows, where Group A seems to have somewhat higher scores than Group B. table(a); table(b) a 1 2 3 4 5 12 38 46 73 81 b 1 2 3 4 5 20 37 68 73 52 summary(a); summary(b) a: Min. 1st Qu. Median Mean 3rd Qu. Max. 1.000 3.000 4.000 3.692 5.000 5.000 b: Min. 1st Qu. Median Mean 3rd Qu. Max. 1.0 3.0 3.5 3.4 4.0 5.0 Welch 2-sample t test. Because you have moderately large sample sizes (250 in each group) it seems reasonable to do a Welch two-sample t test on the (non-normal) Likert scores. Results from R below show P-value about 0.01 so it seems reasonable to say that the Likert average 3.7 for Group A is significantly greater than average 3.4 for Group B. t.test(a,b) Welch Two Sample t-test data: a and b t = 2.7127, df = 497.97, p-value = 0.006905 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: 0.0805107 0.5034893 sample estimates: mean of x mean of y 3.692 3.400 If you want to make an overall observation of the population, you might say that the average Likert value is $.88(3.69) + .12(3.40) = 3.66.$ To the extent that you may believe that Likert values imitate valid numerical assessments, this average may be useful. Wilcoxon test. Some people believe it is better to use a Wilcoxon rank sum test to compare Likert scores for two groups. (Likert data are not numerical, but they are ordinal.) Again there is a significant difference between (medians of) Groups A and B; the P-value is below 1%. wilcox.test(a,b)$p.val [1] 0.004490969 Chi-squared test of homogeneity. You could also look at a chi-squared test to see if probabilities of counts in the five Likert categories differ between groups A and B. (Categories are treated as nominal.) The two groups have significantly different probabilities for the various Likert scores. The P-value is below 5%. DTA = rbind(tabulate(a), tabulate(b)) ab.out = chisq.test(DTA); ab.out Pearson's Chi-squared test data: DTA X-squared = 12.582, df = 4, p-value = 0.01351 We review the 'observed' counts used, and make sure that 'expected' counts all exceed 5. Also we note that there are a couple of Pearson residuals, possibly large enough to try to interpret. ab.out $obs [,1] [,2] [,3] [,4] [,5] [1,] 12 38 46 73 81 [2,] 20 37 68 73 52 ab.out$ exp [,1] [,2] [,3] [,4] [,5] [1,] 16 37.5 57 73 66.5 [2,] 16 37.5 57 73 66.5 ab.out$resi [,1] [,2] [,3] [,4] [,5] [1,] -1 0.08164966 -1.456986 0 1.778104 [2,] 1 -0.08164966 1.456986 0 -1.778104 The residuals with largest absolute values are for Likert score 5, where Group A had more (81) than the expected number under the null hypothesis (66.5), while Group B had fewer than expected. To a lesser degree Group B had more Likert 3's than Group B. The chi-squared statistic (12.6) is the sum of the squares of the Pearson residuals, so one might say that these differences for Likert scores 3 and 5 made important contributions to this significantly large statistic. Note: In case it is of interest, my fake data were arbitrarily simulated as follows. Vectors p are proportions for Likert scores; R turns them into probabilities. set.seed(818) a = sample(1:5, 250, rep=T, p = c(1, 2, 3, 4, 4)) b = sample(1:5, 250, rep=T, p = c(1, 2, 4, 4, 3))
