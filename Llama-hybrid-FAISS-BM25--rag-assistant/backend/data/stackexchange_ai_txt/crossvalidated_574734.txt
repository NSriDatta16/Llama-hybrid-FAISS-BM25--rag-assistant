[site]: crossvalidated
[post_id]: 574734
[parent_id]: 
[tags]: 
Are these Bayesian Inference?

I am trying to understand what is and what is not considered Bayesian inference. Let say I am to estimate a parameter or a vector of parameters say $\theta$ and I have data on some features of the distribution say the first two moments $\hat{m_1}$ and $\hat{m_2}$ . I think it is OK to use Bayes theorem to derive a sort of a posterior as follows $p(\theta|\hat{m_1},\hat{m_2})\propto p(\hat{m_1},\hat{m_2}|\theta)p(\theta)$ where $p(\hat{m_1},\hat{m_2}|\theta)$ is the joint distribution of $\hat{m_1}$ and $\hat{m_1}$ conditional on $\theta$ . Which one of these is not Bayesian inference? use the actual distribution of $p(\hat{m_1},\hat{m_2})$ in the above formula use the Asymptotic distribution of $p(\hat{m_1},\hat{m_2})$ in the above formula How about the following when I have a frequentist estimate of $\theta$ and its distribution? $p(\theta|\hat{\theta})\propto p(\hat{\theta}|\theta)p(\theta)$ Addendum : Here is an example to clarify the question: Suppose I want to learn about the parameters of the distribution of income given my data and let's assume it follows a parametric distribution with parameter $\theta$ . Often the data on income distribution is available only in the form of some summary statistics (e.g. deciles or Lorenz ordinates). for simplicity let say only 2 summary statistics of $\hat{m_1}$ and $\hat{m_2}$ are available. It is possible to obtain either the actual or more often asymptotic distribution of these summary statistics as a function of unknown $\theta$ . Let's denote this with $p(\hat{m_1},\hat{m_2}|\theta)$ . (1) and (2) above asks whether this can be used like a likelihood and proceed with the Bayesian inference. I personally think this is a legitimate Bayesian analysis and I have seen papers doing such things. Now if the above is legitimate Bayesian inference and the objective of Bayesian analysis is to incorporate prior information then I can use the above logic and easily make any frequentist estimator Bayesian without MCMC or any high power methods. I can calculate $\hat{\theta}$ (the frequentist estimate of $\theta$ ) and the frequentist analysis often gives me the (asymptotic) distribution of $\hat{\theta}$ as a function of $\theta$ which I denote with $p(\hat{\theta}|\theta)$ . I can combine this (often a normal distribution) with prior $p(\theta)$ and do a simple Bayesian analysis. This does not seem right but what is wrong with it or what is that I am missing? Addendum2: The latter may not as simple as I thought since the variance of the asymptotic normal distribution could depend on $\theta$ in a complicated way.
