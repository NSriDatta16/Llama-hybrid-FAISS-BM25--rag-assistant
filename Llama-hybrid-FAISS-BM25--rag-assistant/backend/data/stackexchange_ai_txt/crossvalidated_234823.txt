[site]: crossvalidated
[post_id]: 234823
[parent_id]: 234791
[tags]: 
Your logic is more suited to the Bayesian rather than classical statistical setting. Note that in your interval the randomness applies to the lower and upper bounds of the interval, as these are considered to vary over (infinitely many) random samples. Thus your probability statement about the interval should really be read as $\Pr( [L(y), U(y)] \mbox{ contains } y^* | \mu, \sigma)$. In words, the chance that any random sample's prediction interval contains a new value (generated from the same distribution as the data), which depends on unknown but fixed parameters. So you don't have an interval about $y^*_q$. I suggest you look at recent clarification of correct interpretation of classical intervals (eg p-values) in Wasserstein and Lazar (2016), on behalf of the American Statistical Association. Some good references about long running debate. Bayesians are often pointing out the way in which Frequentist (classical statistical) intervals are misinterpreted. Many people provide an interpretation more akin to a Bayesian posterior probability, which for you would require the posterior predictive distribution $\Pr(y^*| y) = \int_\theta p(y^*|\theta) p(\theta|y) d\theta$, where $\theta=(\mu,\sigma)$. You could then take any summary statistic, such as quantiles from this distribution. An excellent mathematical intro is provided by Gelman, Carlin, Stern and Rubin's seminal book "Bayesian Data Analysis". Alternatively investigate topics of "tolerance intervals" (classical stats needs extra contortions to get probability statements about confidence bounds) or "quartile regression" to model quantiles.
