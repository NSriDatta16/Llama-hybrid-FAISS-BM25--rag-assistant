[site]: crossvalidated
[post_id]: 63414
[parent_id]: 63386
[tags]: 
As with many questions on definitions, answers need to have an eye both on the underlying principles and on the ways terms are used in practice, which can often be at least a little loose or inconsistent, even by individuals who are well informed, and more importantly, variable from community to community. One common principle is that a statistic is a property of a sample, and a known constant, and a parameter is the corresponding property of the population, and so an unknown constant. The word "corresponding" is to be understood as quite elastic here. Incidentally, precisely this distinction and precisely this terminology are less than a century old, having being introduced by R.A. Fisher. But A set-up of sample and population doesn't characterise all our own problems. Time series are one major class of examples in which the idea is rather of an underlying generating process, and something like that is arguably the deeper and more general idea. There are set-ups in which parameters change. Again, time series analysis provides examples. To the main point here, we don't in practice think of all the properties of a population or process as parameters. If some procedure assumes a model of a normal distribution, then the minimum and maximum are not parameters. (Indeed, according to the model, the minimum and maximum are arbitrarily large negative and positive numbers any way, not that that should worry us.) I would say that for once Wikipedia is pointing in the right direction here, and practice and principle are both respected if we say that a parameter is whatever we are estimating . This helps too with other questions that have caused puzzlement. For example, if we calculate a 25% trimmed mean, what we are estimating? A reasonable answer is the corresponding property of the population, which in effect is defined by the estimation method. One terminology is that an estimator has an estimand, whatever it is estimating. Starting with some Platonic idea of a property "out there" (say the mode of a distribution) and thinking how to estimate that is reasonable, as is thinking up good recipes for analysing data and thinking through what they imply when regarded as inference. As often in applied mathematics or science, there is a twofold aspect to a parameter. We often think of it as something real out there which we are discovering, but it is also true that it is something defined by our model of the process, so that it has no meaning outside the context of the model. Two quite different points: Many scientists use the word "parameter" in the way that statisticians use variable. I have a scientist persona as well as a statistical one, and I would say that is unfortunate. Variables and properties are better words. It is remarkably common in wider English usage that parameter is thought to mean limits or bounds, which may stem from some original confusion between "parameter" and "perimeter". A note on the estimand point of view The classical position is that we identify a parameter in advance and then decide how to estimate it, and this remains majority practice, but reversing the process is not absurd and can be helpful for some problems. I call this the estimand point of view. It has been in the literature for at least 50 years. Tukey (1962, p.60) urged that "We must give even more attention to starting with an estimator and discovering what is a reasonable estimand, to discovering what is it reasonable to think of the estimator as estimating." A similar point of view has been elaborated formally in considerable detail and depth by Bickel and Lehmann (1975) and informally with considerable lucidity by Mosteller and Tukey (1977, pp.32-34). There is also an elementary version. Using (say) sample median or geometric mean to estimate the corresponding population parameter makes sense regardless of whether the underlying distribution is symmetric, and the same goodwill can be extended to (e.g.) sample trimmed means, which are regarded as estimators of their population counterparts. Bickel, P.J. and E.L. Lehmann. 1975. Descriptive statistics for nonparametric models. II. Location . Annals of Statistics 3: 1045-1069. Mosteller, F. and J.W. Tukey. 1977. Data Analysis and Regression. Reading, MA: Addison-Wesley. Tukey, J.W. 1962. The future of data analysis . Annals of Mathematical Statistics 33: 1-67.
