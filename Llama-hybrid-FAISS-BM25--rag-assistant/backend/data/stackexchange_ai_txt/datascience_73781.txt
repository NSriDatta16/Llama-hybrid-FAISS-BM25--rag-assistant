[site]: datascience
[post_id]: 73781
[parent_id]: 73778
[tags]: 
Text is a 1D sequence, but is typically treated as a sequence of embedding vectors. So yes it is in some sense 2D input. But the embedding dimension doesn't really have any spatial meaning; adjacent dimensions aren't any more related than any others. There is no invariance across the embedding dimension either; the same values in one part of the embedding don't mean the same thing. So the assumptions a 2D convolution don't make sense for this type of input.
