[site]: crossvalidated
[post_id]: 638822
[parent_id]: 
[tags]: 
Error rate vs Empirical risk - What's the difference between practical and theoretical terms for performance of neural networks?

Motivation I am currently reading the following book: Understanding machine learning by Shalev-Shwartz and Ben-David. The book uses statistics terminology in its machine learning theory, and it is not clear to me how to reconcile the stat terms with terms used in coding practice. (I do not restrict my question to definitions from the book, they should only serve as a reference point for what I mean by stat terms.) I would like to be precise when I use the following terms that I encounter in statistics or in practice: Question Define : error, loss, risk, empirical risk error rate, accuracy Using context : I divide the sample dataset into train set and test set,then train on the train set and do the empirical risk (?) assessment on the test set. In practice, we often call that error rate. However the error in statistics is simply the distance between a single prediction and the correct prediction, risk has to do something with an expected value. To be more specific about my motivation (this is not part of my question) I would like to paraphrase agnostic PAC learnability such that the terms align both with usual statistic definitions and e.g. common pytorch coding practices. For this reason, I need to be precise with the terms above. Can you help me on this quest?
