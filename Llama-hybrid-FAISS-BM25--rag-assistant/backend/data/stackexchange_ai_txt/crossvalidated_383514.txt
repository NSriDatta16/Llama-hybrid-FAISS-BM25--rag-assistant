[site]: crossvalidated
[post_id]: 383514
[parent_id]: 380123
[tags]: 
Why do we need a critic at all? I just can't see where the critic suddenly came from and what it solves. The critic solves the problem of high variance in the reward signal. If you run the same (likely-stochastic) policy over and over in an (also probably stochastic) environment, you will get different amounts of cumulative reward all the time. Meanwhile, the critic gives a (hopefully good) estimation of the cumulative reward without any variance between rollouts. Why not use the same parameters for the actor and the critic? It seems to me they are actually to approximate he same thing: "How good is choosing action a from state s" You can certainly share some of the parameters in the networks of the actor and the critic. However, you can't literally use the same exact parameters all the way through because they're neural networks which compute different things. Why did we replace $Q^{\pi_{\theta}}(s,a)$ with an approximation by different parameters $w$ , $Q_w(s,a)$ . What benefit does this separation introduce? $Q^{\pi_{\theta}}(s,a)$ is the Q-value of the policy $\pi_\theta$ . There is no straightforward way to compute this for free. $Q_w(s,a)$ is our approximation of that function.
