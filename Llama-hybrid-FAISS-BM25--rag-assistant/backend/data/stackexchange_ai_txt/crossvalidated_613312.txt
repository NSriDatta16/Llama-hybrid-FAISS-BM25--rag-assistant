[site]: crossvalidated
[post_id]: 613312
[parent_id]: 
[tags]: 
Robbins-Monro condition and Exponential Moving Average

I have been working with incremental parameter update, mainly incremental sample mean update. That is, for $k$ -th iteration, the parameter $\theta^{(k)}$ is updated as $$ \theta^{(k)} = \theta^{(k-1)} + \alpha^{(k)} (\hat{\theta}^{(k)} - \theta^{(k-1)}) $$ where $\hat{\theta}^{(k)}$ is the estimation target (or $k$ -th observation of estimation) and $\alpha^{(k)}$ is the stepsize for $k$ -th update. In most literature, $\alpha^{(k)}$ is chosen to satisfy the Robbins-Monro conditions which is $$ \lim_{k \to \infty} \alpha(k) = 0 \\ \sum_{k=1}^{\infty} \alpha(k) = \infty \\ \sum_{k=1}^{\infty} \alpha^{2}(k) = 0 $$ since these conditions ensure the convergence. Typically it is implemented as $\alpha(k)=k^{-p}$ where $p \in (0.5, 1]$ . However, in other literatures, I have seen using constant stepsize, which is often refered as an exponentially weighted mean or exponential moving average. Although it clearly violates Robbins-Monro conditions, it seems to work fine with most of them especially with non-stationary system cases It is intuitively understandable since constant stepsize allows forgetting old values, however, I am curious if there is some mathematical derivation or theoretical studies of the validness of exponential moving average rather than 'it just works.' Also, can I assume that it is always better to use constant stepsize than decreasing stepsize in, for example, deep learning situation where layrer inputs are non-stationary? Moreover, if the stepsize selection differs depend on the situation, is there another considerable choice of stepsize that is also widely used?
