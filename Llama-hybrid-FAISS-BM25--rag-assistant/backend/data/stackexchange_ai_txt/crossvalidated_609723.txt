[site]: crossvalidated
[post_id]: 609723
[parent_id]: 
[tags]: 
Can XGBoost handle a custom objective where the 2nd derivative can be negative?

I am going through the introduction to XGBoost page, and there is a section where they derive the optimal value of the leaf node, for a given tree structure. To quote the specific section, In this equation, $w_j$ are independent with respect to each other, the form $G_j w_j + \frac{1}{2} (H_j + \lambda) w_j^2$ is quadratic and the best $w_j$ for a given structure $q(x)$ and the best objective reduction we can get is: $$ w^*_j = -\frac{G_j}{H_j + \lambda}$$ .... For context, $G_j w_j + \frac{1}{2} (H_j + \lambda) w_j^2$ is the summand of the objective, $G_j$ is the sum of sample gradients in leaf $j$ WRT leaf values evaluated at existing trees, $H_j$ is the sum of Hessian terms in leaf $j$ , $w_j$ is the leaf value, and $\lambda$ is the L2-regularization constant. Clearly $w^*_j$ is the stationary point of $w_j$ in the objective. Now, the second derivative of the objective is $H_j + \lambda$ . But a stationary point is only a minimum if the second derivative is positive. In this case, their equation for $w^*_j$ is only a minimizer of the objective if $H_j > 0$ (let's ignore $\lambda$ for simplicity here). If $H_j$ is negative, or zero, then $w^*_j$ may, in fact, be a maximizer or a saddle point. I find that the second derivative of most typical objective functions is strictly positive. The common examples that come to mind are MSE ( $1$ ), cross-entropy ( $p (1 - p)$ where $p = \sigma(y)$ ), Poisson objective ( $\frac{t}{y^2}$ where $t$ is the target). Which is a happy coincidence. But, as soon as we introduce a custom objective with a 2nd derivative that can be negative or zero, then XGBoost can no longer handle this. Is this true? The reason I am asking this is because I am currently trying to train a model with a custom objective with Hessian that is sometimes $ . And I am getting the wrong behavior for very obvious cases. I even contrived a dataset where the leaf values should all be pushed to $-\infty$ , and are instead being pushed to $+\infty$ . One "hacky" solution I found, that actually worked, is to replace the Hessian with its absolute value. Though I can't theoretically justify why it works. Is XGBoost unable to handle custom objectives with negative second derivatives? Or am I fundamentally misunderstanding something?
