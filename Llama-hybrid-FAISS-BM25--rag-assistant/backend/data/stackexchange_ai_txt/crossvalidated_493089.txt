[site]: crossvalidated
[post_id]: 493089
[parent_id]: 492995
[tags]: 
Lognormal data is often encountered in practice. Here is a session in R that begins with $n_1 = 1000$ observations from a lognormal distribution. At the start, the sample mean is $\bar Y_1 = 1.70$ and the sample standard deviation is $S_1 = 2.49.$ Then we go through several iterations, removing boxplot outliers at each step. [Computations in R.] set.seed(2020) y1= rlnorm(1000) summary(y1); length(y1); sd(y1) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.04217 0.49016 0.94409 1.69558 1.89374 40.56466 [1] 1000 [1] 2.493404 y2 = y1[y1 With successive 'outlier' removals, the sample size has fallen to 911, 874, 863, and finally, $n_5 =$ 857. So I'm down to less than 86% of my original data, with no end of removals yet in sight. The sample mean has decreased from 1.70 for the full sample to 0.975, and the standard deviation from 2.49 to 0.674. One has to wonder what population the final sample in this sequence might represent. Certainly, not the same population that the original sample came from. Here are boxplots for the original sample (at left) and the multiply-truncated sample y5 (right). boxplot(y1,y2,y3,y4,y5, col="skyblue2") To be clear, there is nothing unusual about the original sample above. Here are boxplots for 20 samples of size 1000 from the same population. Every one of the 20 samples has multiple boxplot outliers. set.seed(1021) m = 20; n = 1000 x = rlnorm(m*n); gp = rep(1:20, each=n) boxplot(x ~ gp, col="skyblue2", main="Boxplots of 20 Lognormal Samples") A simulation with 100,000 samples of size 1000 shows that the average number of (first pass) boxplot outliers per sample is about 76. set.seed(1234) nr.out = replicate(10^5, length(boxplot.stats(rlnorm(1000))$out)) mean(nr.out) [1] 77.53626 If an outcome is known to have resulted from equipment failure or data entry error, then, of course, it should be excluded. Also, there may be times when you know a value is simply impossible (e.g., a negative height or a human lifetime over 900 years) and needs to be ignored. But I think it is a serious mistake to establish an automated process for 'outlier' removal--particularly an iterative one. Sometimes real data show surprisingly good or disastrously bad news. It's a good idea to pay attention when that happens.
