[site]: datascience
[post_id]: 109197
[parent_id]: 
[tags]: 
What makes differences in each head in the multiheaded attention in transformer?

What makes differences in each head in the multiheaded attention in transformer? As they are fed and trained in the exact same way, except the initialization of weights are different for each head to produce different sets of (Q,K,V) in each head. Such multi-headed design to me seems no difference than ensembling multiple models that are initialized differently. Many sources claim that the multi-head attention 'can help capture meaning in different contextual subspace' without further substantiation or supporting proofs. Honestly I've been quite fed up with all those vague descriptions in the data science world that they make claim without mathematical rigor. I think I'm looking for more rigorous explanation as to why "multi-head attention 'can help capture meaning in different contextual subspace'" when they are simply an ensemble of identical models but weights randomly initialized?
