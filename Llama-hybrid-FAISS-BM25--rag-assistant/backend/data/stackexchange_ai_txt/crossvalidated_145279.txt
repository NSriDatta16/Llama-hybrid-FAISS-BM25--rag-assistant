[site]: crossvalidated
[post_id]: 145279
[parent_id]: 
[tags]: 
Best Random Forest model converging to bagging: What does it mean?

I am performing a grid search to tune the Random Forest parameter $m$ (at $ntree$ fixed). I have $p=79$ variables, and the best model, in terms of $OOB_{error}$, turns out to be a model with $m=76$ variables ($OOB_{error} = 0.137$). So, $m$ is roughly equal to $p$, which means that the best RF model is (almost) equivalent to Bagging... What does it mean? Especially, what conclusions can be drawn about my set of predictors/observations? For comparison, the $OOB_{error}$ for the model with $m=\sqrt{p}$ is $0.167$, so, the difference is not huge, but still... EDIT: the $OOB_{error}$ continuously decreases as $m$ increases, until it gets to $m=76$. Then, for $m=77$ and $m=78$, it slightly increases ($0.1397$,$0.1405$, respectively). The language I am using is R, randomForest package.
