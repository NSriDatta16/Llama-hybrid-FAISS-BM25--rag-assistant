[site]: crossvalidated
[post_id]: 579266
[parent_id]: 
[tags]: 
Why isn't RandomSearchCV returning the optimum parameters for the XGBoost Model, and how can I avoid Overfitting?

I have a dataset for energy consumer customers and binary target variables with which I want to predict the churn for the customers. Counts of target values Not Churn 0: 14153 Churn 1: 1520 I have trained XGBoost model with tree different strategies, but unfortunately model overfit the training dataset I split the data into 80/20 and trained the XGBoost model with the following parameters model = xgb.XGBClassifier( learning_rate = 0.1, max_depth = 6, n_estimators = 500, n_jobs = -1, min_child_weight = 9 ) Result: Accuracy: 0.9132431742791528 Precision: 0.7323943661971831 Recall: 0.13941018766756033 AUC Test: 0.709404 AUC Train: 0.989078 Same model but with StratifiedKFold StratifiedKFold(n_splits=5, random_state=13, shuffle=True) AUC Train: 0.986 AUC Test: 0.712 --- UPDATED TEXT 3. Same Model but with additional parameters to add complexity parameters = {'subsample': 0.8, 'scale_pos_weight': 1, 'n_estimates': 1100, 'min_child_weight': 1, 'max_depth': 12, 'learning_rate': 0.01, 'gamma': 4.0, 'colsample_bytree': 0.60} AUC Train: 0.7238 AUC Test: 0.6568 The Question here is following obviously, the model overfits the training example. How can I reduce the overfitting? I was reading the article that adding more parameters to the model would probably do that, but how can I select parameters? this would be the hit and trial method. What is the best way? I got the best parameters from RandomSearchCV for the third model, but with those values, I got AUC-test and Train 0.5 both, which is strange because it is supposed to give me the best parameters and the best AUC score. After getting unsuccessful results, I updated parameter values on my own and didn't use the Best_parameters_ given by Random CV. Why isn't it providing me with the best parameters?
