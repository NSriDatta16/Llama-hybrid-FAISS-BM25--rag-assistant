[site]: crossvalidated
[post_id]: 568083
[parent_id]: 
[tags]: 
Am I using confidence intervals correctly here?

I run a stochastic computer program. What I’m doing is testing one strategy against what might be considered a default strategy. At the start of a run, there are 100 individuals using the test strategy and 100 using the default strategy. A strategy is successful if the quantity of individuals using the test strategy is significantly greater than the starting quantity of 100 by the end of the run. (There are always 200 individuals: test quantity + default quantity = 200. So when test quantity gains, default quantity loses.) Each of the 10000 runs will have perhaps 300 time-steps. I will graph a quantity for each of the 300 time-steps, and on the graph, this quantity for each time-step is averaged over the 10000 runs. Most individual values for quantity will be either 0 or 200, and this is especially true for values closest to the last time-step (e.g., 0, 200, 0, 200, 200, 75). So the distribution of values for a particular time-step may be sort of V-shaped. And I will be most interested in reporting whether the average quantity for the last time-step significantly exceeds 100. Since I don't know the population's standard deviation, I use a student’s t distribution. For 95% confidence intervals, it’s $mean \pm standard\_error*t_{df}$ . And $t_{df}$ for 10,000 runs for 95% is 1.96. So, it’s $mean \pm standard\_error*1.96$ . And the average quantity at the end of a run is significantly greater than 100 if $mean-standard\_error*1.96$ exceeds 100. Does this look right? Thank you so much for your time. Also, I am willing to pay for a person's time. But I have found that the feedback and insight on this website probably exceed freelancer sites. I truly do appreciate your time.
