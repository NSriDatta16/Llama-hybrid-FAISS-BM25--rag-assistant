[site]: datascience
[post_id]: 128245
[parent_id]: 
[tags]: 
Can my LSTM model learn feature engineering on its own?

I have a timeseries dataset and I am training an LSTM model on it to perform multiclass classification. My dataset has 7 columns => x1,x2,x3....x7 And has 4 labels => f1,f2,f3,f4 Since I have domain knowledge over my dataset, I know exactly what feature engineering needs to be done. for example, I need to create 4 new features from my current features by applying some rules on each row:- newx1 is created by => if (x2==x3) then 1, else 0 newx2 is created by => if (x1==x4 and x1>x5) then 1, else 0 newx3 is created by => if ((x1-x6)/x1>x7) then 1, else 0 newx4 is created by => if ((x6-x1)x1/>x7) then 1. else 0 I am getting 100% accuracy on test data if I train my LSTM model on newx1, newx2, newx3, newx4. However, training it on the original features (x1,x2....x7) I am getting reduced accuracy of 85-90% accuracy on test data. The problem I am trying to solve requires me to have greater than 99% accuracy, so it won't suffice to have just 90% accuracy. I was wondering if somehow my LSTM model can learn the rules for feature engineering on its own or do I have to change my model ? Note: I can not apply the feature engineering rules manually because I am training the LSTM model on multiple datasets and each dataset will require its own feature engineering rules. I want to keep it as generic as possible. LSTM model :- def create_lstm_model(MaxTimeslice, H, LR, num_classes, dropout_rate=0.1, l2_reg=0.001): ip = Input(shape=(MaxTimeslice, H)) x = LSTM(32, return_sequences=True, dropout=dropout_rate, kernel_regularizer=l2(l2_reg))(ip) x = LSTM(16, dropout=dropout_rate, kernel_regularizer=l2(l2_reg))(x) x = Dense(units=16, activation='relu')(x) multiclass_output = Dense(units=num_classes, activation='softmax')(x) model = Model(inputs=ip, outputs=multiclass_output) model.compile(loss="categorical_crossentropy", metrics=["accuracy"], optimizer=RMSprop(learning_rate=LR)) return model ```
