[site]: crossvalidated
[post_id]: 271648
[parent_id]: 271480
[tags]: 
This will be a rather long elaboration on @hxd1011 answer. Suppose we observe $$D = (1,1),(2,2),(3,3),â‹¯,(1000,1000)$$ There is a mathematical justification why we choose $y = x$ over some high degree polynomial. It's called bayesian Occam's Razor. From Bayes theorem $$p(m|D) \propto p(D|m)p(m)$$ where $p(m|D)$ is a probability that we are dealing with a model $m$ having observed data $D$. Likelihood Now consider some complex model $m_C$ that can be fitted to many observed datasets, we have $$\sum_{D' \in \mathbb{D}}p(D'|m_C) = 1$$ so if $|\mathbb{D}|$ is large, we expect each probability of observing a dataset $p(D'|m_{C})$ to be, on average, smaller than for some simpler model $m_S$. So since $y=x$ is a simple model, and observed data is coherent with it, we assign it higher probability, i.e. $$p(D|m_S) > p(D|m_C) \implies p(m_S|D) > p(m_C|D)$$ assuming the same prior on models $p(m_C) \simeq p(m_S)$ Prior Of course, from the whole set of simpler models that fit our data well, we still prefer some of them more than others. For example, the observed data also fits a model $m_{S2}$ as $$y = \begin{cases} -1 &\mbox{if } x = 1001 \\ x & \mbox{otherwise}\end{cases} $$ and here the prior $p(m)$ comes into play, where we usually prefer "more natural" models, of course this is less mathematical and more subjective.
