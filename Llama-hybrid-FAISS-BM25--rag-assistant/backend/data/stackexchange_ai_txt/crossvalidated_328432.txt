[site]: crossvalidated
[post_id]: 328432
[parent_id]: 328395
[tags]: 
I think you're making the common mistake of treating logistic regression as a classifier. We don't have false negatives or positives because those require an assignment of labels. We are instead modeling the probabilities of a success so all we have are the modeled probabilities for the observations where the event happened, and where the event didn't happen. But depending on our threshold, a probability of $\hat y_i = 0.7$ may lead to either a positive or negative label. In this light, $c_i$ doesn't actually represent a penalty for false negatives. Our loss (ignoring multiplicative constants) can be rewritten as $$ \sum \limits_{i \, :\, t_i=1} \log \hat y_i + \sum \limits_{i \, :\, t_i=0} \log (1 - \hat y_i). $$ This means for each observation where the event happened (i.e. $t_i=1$) we get a contribution of $\log \hat y_i$, and analogously we get $\log (1-\hat y_i)$ for observation where the event did not happen (i.e. $t_i=0$). If we add a $c_i$ term as you did, then we get $$ \sum \limits_{i \, :\, t_i=1} c_i \log \hat y_i + \sum \limits_{i \, :\, t_i=0} \log (1 - \hat y_i). $$ The effect is not that we're forcing the model to minimize false negatives, but rather we are affecting the contribution to the loss of the observations where the event happened. If we're maximizing then $c_i$ being large means our model is going to be encouraged to assign larger probabilities to the observations with $t_i=1$ even if the probabilities for $t_i=0$ observations suffer (although no finite $c_i$ will ever allow for $\hat y_i = 1$ when $t_i=0$, and in general any fixed $c_i$ can be overpowered by a really poorly aligned $t_i$ and $\hat y_i$). That does mean that for an a priori fixed threshold we'll likely see the false negative rate go down, although that's not because we're directly penalizing it but rather we're just encouraging our probabilities to be bigger. Similarly, this same modification will result in a relative increase in the true positive rate but for the exact same reason.
