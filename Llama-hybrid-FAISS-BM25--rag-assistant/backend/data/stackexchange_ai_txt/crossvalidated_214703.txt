[site]: crossvalidated
[post_id]: 214703
[parent_id]: 214684
[tags]: 
Say we're measuring some fixed physical quantity (e.g. the mass of an object). But, the measurement is noisy, meaning it's affected by varying processes other than the signal of interest (e.g. noise in the electronics of the weighing scale). In this case, the measurement will be different each time we weigh the object, even though the the true weight is fixed. As you mention, the precision of the measurement is reduced as the variance increases. On the other hand, consider a method like PCA, where we want to find directions of high variance in the data (and discard directions with low variance). In this case, the intuition is that the high variance directions are more 'relevant'. This intuition works in some cases but not others. A case where it works: We have a multidimensional signal that varies along a small set of directions. The signal is corrupted by white noise that's spread out across all dimensions (unlike the signal). The noise magnitude is lower than the signal magnitude. In this case, discarding low variance directions would reduce the noise but keep the signal. Or, say we're interested in the relationship between two variables $X$ and $Y$. In this case, we're interested in the extent to which $X$ and $Y$ change together. If $X$ and $Y$ have larger covariance relative to their individual variances, we could say that $X$ contains more information about $Y$ (and vice versa). Note that the reverse is not necessarily true if they're nonlinearly related. In the general case, see mutual information . Variance has some connections to the information theoretic concept of entropy , which measures how 'spread out' a distribution is. In this case, you could think of them both as being measures of uncertainty. For unimodal distributions, the entropy scales with the variance (but you can break this relationship in other cases). Another way to interpret entropy: Say we're drawing values from some distribution, and we want a way to represent each of the values. That is, to encode them (e.g. as a bit string). Entropy is the average length (in bits) you'd need to perfectly encode the values, using the shortest possible code. Very loosely, this says that more information is required to describe things that are more uncertain.
