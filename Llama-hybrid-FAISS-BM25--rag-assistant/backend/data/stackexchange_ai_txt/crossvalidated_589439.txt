[site]: crossvalidated
[post_id]: 589439
[parent_id]: 589432
[tags]: 
One of the issues when you introduce nonlinearities and interactions is that the change resulting in a change in a variable of interest depends on the starting value of that variable of interest and of the other variables in the model. For instance, consider a model like $\hat y = x_1-x_2+x_1^2x_2$ . If you want to know by how much $y$ changes upon changing $x_1$ by one unit, take the derivative. $$ \dfrac{ \partial \hat y }{ \partial x_1 } = 1 + 2x_1x_2 $$ You cannot answer the question (with a single number) unless you know $x_1$ and $x_2$ . Neural networks are no different. We can take partial derivatives (making use of the chain rule) and interpret those derivatives as slopes just like normal. However, those slopes are likely to depend on the values of all variables (just like above), including the variable of interest. Consequently, there is no simple interpretation like, “When $x_1$ increases one unit, our predicted $y$ increases by $\hat\beta_1$ units.” If $A$ is an activation function, a simple neural network with two features and two neurons (with activation function $A$ ) in the hidden layer is: $$ \hat y = \hat b_{2,1} + \hat w_{2,1}A\bigg(\hat b_{1,1}+\hat w_{1,1}x_1 + \hat b_{1,3}x_2\bigg) + \hat w_{2,2} A\bigg(\hat b_{1,2} + \hat w_{1,2}x_1 + \hat w_{1,4}x_2\bigg) $$ For a nonlinear activation function $A$ , the partial derivatives with respect to $x_1$ and $x_2$ can involve both $x_1$ and $x_2$ , meaning that you must know the point where you want to talk about changes in order to talk about changes.
