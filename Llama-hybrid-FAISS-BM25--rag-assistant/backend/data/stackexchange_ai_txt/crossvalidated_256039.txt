[site]: crossvalidated
[post_id]: 256039
[parent_id]: 
[tags]: 
What is the difference between the posterior density, and the posterior "ordinate" in the context of Bayesian modelling

This is the first time I've come across the term posterior "ordinate" and I'm not sure what it means. I understand what the posterior density of some model parameters given some data refers to, but this seems to be different. I came across it reading this paper: http://www.tandfonline.com/doi/abs/10.1198/016214501750332848 has anyone ever heard of it before. Given the context it is used in, could it just refer to the un-normalised posterior density? UPDATE: The context of this is for calculating the marginal likelihood of bayesian statistical models, and their Bayes factors. It is related to this question Bayesian Information Criterion and Basic Marginal likelihood identity To calculate the Bayes factor between two models $M_1$ and $M_2$ we need to calculate their marginal likelihoods $m(\boldsymbol{y}|M_i)$ $$ m(\boldsymbol{y}|M_0) = \int f(\boldsymbol{y}|M_0,\boldsymbol{\theta}_0)\pi(\boldsymbol{\theta}_0|M_0) dx $$ which can be written as $$ m(\boldsymbol{y}|M_0) = \frac{f(\boldsymbol{y}|M_0,\boldsymbol{\theta}_0)\pi(\boldsymbol{\theta}_0|M_0)}{\pi(\boldsymbol{\theta}_0|\boldsymbol{y},M_0)}$$ which the writer calls the basic marginal likelihood identity though I haven't heard of it elsewhere. To explain the variables, $\boldsymbol{y}$ seem to be the "data", while $\boldsymbol{\theta}_0$ are the model parameters. I would call $f(\boldsymbol{y}|M_0,\boldsymbol{\theta}_0)$ the likelihood (data|parameters), and $\pi(\boldsymbol{\theta}_0|M_0)$ seem to be the priors on the model parameters. The thing on the bottom looks very much like the joint posterior distribution of the model parameters, however he calls that the posterior ordinate instead. Since I've gone into detail, I would like to expand my question. The paper is essentially deriving a way to calculate marginalised likelihoods from the normal output that a Metropolis-Hastings sampler would produce, i.e a representative sample of the joint posterior $\pi(\boldsymbol{\theta}_0|\boldsymbol{y},M_0)$. However, to do draw this sample, and run the MetroHastings sampler, we must have a way to calculate $\pi(\boldsymbol{\theta}_0|\boldsymbol{y},M_0)$ at some point $\boldsymbol{\theta}_0$. Though it's possible we don't have the proper normalising constant, which is where my previous assumption came from. But essentially, if we can directly calculate the joint posterior, then surely using this basic marginal likelihood identity to calculate the marginal likelihood is as trivial as putting the numbers in. However, this doesn't seem to be the case and they use a much more complicated way which I am trying to look into.
