[site]: crossvalidated
[post_id]: 366590
[parent_id]: 
[tags]: 
How to demonstrate that a generation process outperforms some others, given a parameter?

Dear statistical stackexchanger, I would like to evaluate whether an algorithm outperforms others for a task of increasing and increasing complexity. I am measuring the impact of the selected algorithm (in a set of algorithms {A1, A2...}) on the performance (number of seconds to complete the task) depending on the difficulty level of the task it faces (difficulty level L1, L2...). For each difficulty level, I generated 50 tasks (i.e. 50 tasks of level L1, 50 tasks of level L2 and so on). I am measuring the performance of the algorithms on each of these instances. If it helps, each algorithm is applied exactly on the same set of (randomly-generated) instances. When averaging the measured values per level, one of my algorithms (let's call it A12) does better than the others for all difficulty levels. While good explanations can be brought for that, I want to show that, statistically, A12 does better than the others in general , and not just that A12 got a bit lucky. Moreover, I would like to show that A12 does better and better (wrt the other algorithms) as the difficulty level increases . As a first attempt, I computed and drew the standard deviation. Bad luck, this deviation is relatively high : the time for solving certain problems is very random (e.g. some instances of low difficulty levels can take much longer than others than others to be solved. Though, this instance is long for all algorithms). Do you have an advice on the statistical method I could be using for testing whether A12 does better than others, please? Is there a statistical method that I could be using for testing whether A12 does better and better with regard to others when the difficulty level increases? Thank you very much for any help! Best regards, Loïs Vanhée
