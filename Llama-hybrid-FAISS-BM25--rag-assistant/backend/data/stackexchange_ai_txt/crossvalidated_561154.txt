[site]: crossvalidated
[post_id]: 561154
[parent_id]: 561147
[tags]: 
I must admit I am not awfully experienced with stacking, however I've done a good amount of reading so here's my two cents. To be clear: by "Under such case, can stacking of A and B further improve the performance", I assume you mean you're training some model C on the outputs of both A and B. My response: You have two base learners, one of which is undoubtedly superior to other overall. However, a meta-learner trained on both, A and B, could still outperform A (the top base performer). The reason is that if A and B are different (different algorithms, different hyperparameters, trained on different rows/features etc.), they could have different biases. So even though on average A is better, it could be the case that it's not flat out better in every scenario. Imagine you have a dataset of some measurement made in males (20% of data) and females (80% of data). A could be much better than B on average, but let's say B is really good at the male rows. This situation-specific superiority of B would likely be drowned out when looking at the overall performance like you are, BUT a meta-learner could be able to pick up on the fact that when male, B tends to be more accurate but broadly A is better. My point here, is that even though one model could be better overall, there is the possibility that the models A and B have biases different enough that a meta-learner could discern these orthogonal biases and learn to rely more on one than the other in certain situations (leading to perhaps only a very marginal improvement in performance). However, if the biases/errors between A and B are not very different, then you almost certainly won't get performance improvement by stacking. For example, if A is a tuned RF and B is an unoptimized RF trained on some random subset of the data (there's no reason at ALL to think that B would have any unique insight that A doesn't have), then there's no reason stacking would help.
