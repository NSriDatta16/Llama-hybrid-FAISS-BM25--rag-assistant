[site]: crossvalidated
[post_id]: 48441
[parent_id]: 
[tags]: 
Time series prediction with non-constant sampling interval

I have some data which can be modelled as such: each data sample $S$ is a series of discrete signal values $S(t_n) \in \{-1, 1\}$ measured at times $(t_{n, S})_{1 \leq n \leq N_S}$. The number of signal measurements $N_S$ per sample and the times at which the measurements were made all vary from sample to sample: $N_{S} \neq N_{S'}$ and $t_{n, S} \neq t_{n, S'}$ in the general case. In other words, each data sample is a series of "yes or no" answers asked at various times. I have some training data. Now, given a data sample $S$, I would like to predict the answer to the next question, in other words: the value of $S(t_{N_S + 1})$. Even better would be to have the probability of a "no" answer: $P(S(t_{N_S + 1}) = -1)$. I have no idea how to do this. Any hint? Starters? EDIT: if the signals were measured always at the same times ($t_{n, S} = t_{n, S'}$), and there were a constant number of signal measurements for each data sample ($N_S = N$), then I could produce for each data sample $S$ a vector of $N-1$ dimensions that would contain the $N-1$ first signal measurements: $S = [1, -1, -1, 1, 1, \dots]$. Measurement $S(t_N) \in \{-1, 1\}$ would be the label associated to sample $S$. Then, the problem would boil down to a classification/regression problem that can be solved e.g: with linear SVM. Unfortunately, the time differences between each signal measurement are important for the experiment.
