[site]: datascience
[post_id]: 128112
[parent_id]: 
[tags]: 
How to deal with a heavily imbalanced test dataset?

Both my train data and test data were imbalanced. So I tried SMOTE for training. Before Smote: 0 2774 1 171 2 841 3 261 4 1758 5 282 6 249 7 2697 8 683 Name: count, dtype: int64 After SMOTE, all of them have the same amount of data. But when I train and test my tuned XGBoost, test set performs poorly on minority classes (since data is also imbalanced there). What else can I try? precision recall f1-score support 0 0.87 0.90 0.88 555 1 0.71 0.59 0.65 34 2 0.91 0.91 0.91 168 3 0.70 0.81 0.75 52 4 0.84 0.82 0.83 352 5 0.85 0.80 0.83 56 6 0.63 0.84 0.72 50 7 0.95 0.92 0.94 540 8 0.95 0.86 0.90 137 accuracy 0.88 1944 macro avg 0.82 0.83 0.82 1944 weighted avg 0.88 0.88 0.88 1944
