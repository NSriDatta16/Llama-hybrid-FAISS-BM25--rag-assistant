[site]: datascience
[post_id]: 97309
[parent_id]: 97307
[tags]: 
I guess the observation you need is that a neural network is typically a function $f \colon \mathbb{R}^2 \to \mathbb{R}$ or something along those lines, whereas the XOR function is a map from $\{0, 1\}^2$ to $\{0, 1\}$ . It's easy to construct a function that, when restricted to $\{0, 1\}^2$ , is the XOR function, but is continuous on $\mathbb{R}^2$ . Consider $$\begin{align*} g \colon \mathbb{R}^2 &\to \mathbb{R}, \\ (x, y) &\mapsto x + y - 2xy.\end{align*}$$ Certainly $g(0, 0) = 0$ , $g(1, 0) = 1$ , $g(0, 1) = 1$ and $g(1, 1) = 0$ , so $g|_{\{0, 1\}^2} = \operatorname{XOR}$ . But we can see that $g$ is a continuous function (on $\mathbb{R}^2$ ). We don't care what it does on almost the entirety of $\mathbb{R}^2$ ; it just needs to agree on the four points we care about. It's a fun exercise to try and explicitly construct a neural network which computes the XOR function. You should be able to do it fairly easily with ReLU activation; it might be slightly simpler to do it with a couple of layers. If it's still not clear, I can expand further.
