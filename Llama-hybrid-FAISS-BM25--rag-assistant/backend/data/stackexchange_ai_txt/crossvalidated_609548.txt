[site]: crossvalidated
[post_id]: 609548
[parent_id]: 609545
[tags]: 
It depends on what loss or error function you use when you code the network. If your loss is the sum of squared errors (SSE), then divide this value by the number of predictions being made. If your loss is the mean squared error (MSE), then you already have the MSE and do not need to transform the value at all. If your loss is the root mean squared error (RMSE), square this to get the MSE. If you have a loss function that is unrelated to squared errors, such as mean absolute error (MAE) or crossentropy loss in a classification problem, the MSE cannot be recovered from such a value, so you need more information to get the MSE (such as the predictions that you can then feed into your own calculation of the MSE). Watch out for if the loss value reported for your neural network applies to in-sample or out-of-sample predictions. $$ SSE = \overset{N}{\underset{i=1}{\sum}}\left( y_i -\hat y_i \right)^2 $$ $$ MSE = \dfrac{1}{N} \overset{N}{\underset{i=1}{\sum}}\left( y_i -\hat y_i \right)^2 $$ $$ RMSE =\sqrt{ \dfrac{1}{N} \overset{N}{\underset{i=1}{\sum}}\left( y_i -\hat y_i \right)^2 } $$ $$ MAE = \dfrac{1}{N} \overset{N}{\underset{i=1}{\sum}}\left\vert y_i -\hat y_i \right\vert $$ $$ \text{Crossentropy Loss}\\= -\dfrac{1}{N} \overset{N}{\underset{i=1}{\sum}}\left[ y_i\log(\hat y_i) + (1-y_i)\log(1-\hat y_i) \right]\\ y_i\in\{0,1\}\\ \hat y_i\in[0,1] $$ $y_i$ are the observed values, $\hat y_i$ are the predicted values, and $N$ is the number of predicted values. For the crossentropy loss, it is often a convention to take $0\log(0)=0$ .
