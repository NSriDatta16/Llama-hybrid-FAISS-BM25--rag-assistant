[site]: crossvalidated
[post_id]: 344304
[parent_id]: 344189
[tags]: 
Your confusion is understandable. Surely, if you already know $p(\theta|X)$, why would you need to draw samples of $\theta$ under this distribution? The answer is usually that the distribution is multivariate, and you want to marginalize over some dimensions of $\theta$ but not others. So for instance, $\theta$ might be a vector of 10 parameters, and you're interested in the marginal distribution $p(\theta_1|X)=\int p(\theta|X)d\theta_{2:10}$. The integrals required to do this marginalization are often very hard to compute exactly. They may be analytically intractable, and (deterministic) numerical integration is often cumbersome in high dimensions. This is where MCMC can help. As long as you know $p(\theta|X)$ up to a constant of multiplication, you can generate samples of $\theta$ that follow this distribution. Then, given a sufficient number of such samples, you can simply look at the distribution of sampled values of $\theta_1$ (e.g. by making a histogram), and those samples will approximate the desired marginal distribution. Compared to numerical integration methods, MCMC is more efficient because it spends more time exploring parts of the distribution where more of the probability mass is concentrated. Also, many MCMC algorithms (such as the classic Metropolis Hastings algorithm) only require that you know the target distribution up to a constant of proportionality, which is helpful if you don't know the normalization constant required to make the distribution proper (which is very often the case, because to compute that constant itself often requires computing a multivariate integral just as complex as the one you're interested in). Edit: it occurred to me that this perhaps doesn't fully answer your first question. The answer to this is that MCMC only requires that you can calculate the posterior probability (density) of a certain parameter value (up to a constant of proportionality). So all you need is a function where, if you put a parameter value in, it gives you its probability under the target distribution (or a value proportional to that probability). That is the sense in which the target distribution must be 'known'. But you don't need to know anything else about it. You can be blissfully ignorant about the mean & covariance of the distribution, or about the little squiggles and bumps that it has here or there, or any number of other things (although some of those things can be helpful to know in order to make MCMC run more smoothly).
