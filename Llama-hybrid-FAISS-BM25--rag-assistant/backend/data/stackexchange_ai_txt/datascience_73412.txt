[site]: datascience
[post_id]: 73412
[parent_id]: 
[tags]: 
PCA targeted away from some subspace

Is there an existing technique allowing to do PCA maximizing not the variance per se, but the variance away from some direction? Imagine I have high-dim data with two different labels L1, L2 and I want to do PCA on L1-labeled data in such a way that it contains as little as possible information about the variance of the L2-labeled data. The simplest way would be to do normal PCA on L2 labelled data, take subspace S generated by first several components and then do another normal PCA on the projection of L1-labelled data on the S* subspace (orthogonal to S). I am skeptical about this approach though because one can still have some important (fo L2) information in S subspace. E.g. it would be good if one had some control on the angle of the subspace one does second PCA on, rather then requiring it to be strictly orthogonal.
