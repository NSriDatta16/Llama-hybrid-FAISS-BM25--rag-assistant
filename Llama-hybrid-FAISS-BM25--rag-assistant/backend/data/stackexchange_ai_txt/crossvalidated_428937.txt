[site]: crossvalidated
[post_id]: 428937
[parent_id]: 
[tags]: 
MLE and Cross Entropy for Conditional Probabilities

I'm trying to understand the relationship between maximum likelihood estimation for a function of the type $p(y^{(i)}|x^{(i)};\theta)$ and the related cross entropy minimization. For a single variable this is straight forward. I am using the notation from "Deep Learning" by Goodfellow et al. $\hat{\theta}_{ML} = \operatorname{argmax}_{\theta} \frac{1}{N}\sum_{i=1}^{N} \log p_{model}(x^{(i)};\theta)$ . Writing the empirical distribution of the data as $\hat{p}_{data}$ we can re-write the function inside the argmax as $E_{\hat{p}_{data}}[\log p_{model}(x;\theta)]$ . Maximizing this function is equivalent to minimizing the cross entropy $H(\hat{p}_{data},p_{model})$ . Ok, easy enough. I'm confused how this generalizes to the case where we are maximizing the likelihood of a model of the type $p_{model}(y^{(i)}|x^{(i)};\theta)$ as we might do in a supervised learning framework. Trying to follow the same line of reasoning we should be able to derive a statement that maximizing the likelihood $\hat{\theta}_{ML} = \operatorname{argmax} \frac{1}{N} \sum_{i=1}^{N} \log p_{model}(y^{(i)}|x^{(i)};\theta)$ is equivalent to minimizing the cross entropy between $\hat{p}_{data}(Y|X)$ and $p_{model}(Y|X)$ . However $H(\hat{p}_{data}(Y|X),p_{model}(Y|X))$ is a random variable w.r.t. $X$ so the analogy doesn't hold. My best guess is that maximizing the likelihood in this scenario is equivalent to minimizing the expected cross entropy between $\hat{p}_{data}(Y|X)$ and $p_{model}(Y|X)$ taken over the empirical distribution of $X$ . Written out this would be $E_{\hat{p}_{data}(X)}[H(\hat{p}_{data}(Y|X),p_{model}(Y|X))]$ . Any insight would be greatly appreciated.
