[site]: datascience
[post_id]: 11785
[parent_id]: 11770
[tags]: 
Let s1 to s4 be the signs (+1 or -1) of the regression coefficients of f1 to f4. Then the predicted value for a data item with columns s1*Inf to s4*Inf will be (as near as precision allows it) 1. You go to the extreme of the parameter space in the right direction (as told by the coefficients signs). Inverting the signs and doing the same should give you a data item that is predicted to be 0. Example, code in R Create sample data set. set.seed(12345) d2 = data.frame(f1=runif(10), f2 = runif(10), f3 = runif(10), output=sample(c(0,1),10,TRUE)) Fit model: m2 = glm(output~f1+f2+f3, data=d2, family="binomial") Look at model parameters: coef(m2) ## (Intercept) f1 f2 f3 ## 5.258817 -4.848013 -15.370838 4.890984 This means as f1 and f2 increase, the fitted probability of zero increases because their signs are negative. As f3 increases, the fitted probability of one increases because it has a positive sign. So.... predict(m2, newdata=data.frame(f1=-Inf, f2=-Inf, f3=Inf), type="response") should be one, which it is, and: predict(m2, newdata=data.frame(f1=Inf, f2=Inf, f3=-Inf), type="response") should be zero, which it is, within tolerance. Zeroes and ones in logistic regression responses are like plus or minus infinity in a linear regression, so if you think back to a simple X-Y fit, you get an infinite response to a linear regression when you go to +Inf on the X-axis if the slope is positive or -Inf on the X-axis if the slope is negative. Transform that to a logistic regression and then you get to +1 or 0 at the appropriate signed infinities of the covariates.
