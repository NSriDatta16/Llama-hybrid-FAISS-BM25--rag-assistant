[site]: crossvalidated
[post_id]: 282701
[parent_id]: 
[tags]: 
Logistic regression gradient descent converge

I have built a logistic regression in python. The Betas are calculated through the gradient descent (ascent actually) method. The change in loglikelihood between iterations is used as a measure to see how close the betas are to their 'true' values. At what point can it be said that the model has converged? I.e. how small does the change in loglikelihood need to be before one can say that the betas have been found? 1e-10, 1e-100, or 1e-5? This is important because setting a too small threshold will result in the need for many iterations (i.e. takes much time). Too large a threshold will result in the model being calculated quicker, but less precise.
