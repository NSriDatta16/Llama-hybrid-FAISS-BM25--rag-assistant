[site]: crossvalidated
[post_id]: 578311
[parent_id]: 578097
[tags]: 
A good setup for this problem is as an errors-in-variables model . We have observations, $x_i$ , of underlying variables, $\bar x_i$ , with noise $\epsilon_i$ : $$ x_i = \bar x_i + \epsilon_i. $$ We are looking for coefficients $a_i$ and $b$ such that $$ \bar x_0 = \sum_{i=1}^N a_i \bar x_i + b. $$ This looks a lot like a linear regression problem! The only difference is that there is noise in all of the $x_i $ variables, which needs to be taken into account. For your two questions: Note that the balance won't necessarily hold for any individual sample. Even if we know the correct coefficients, $$ \sum_{i=1}^N a_i x_i + b - x_0 = \sum_{i=1}^N a_i \epsilon_i - \epsilon_0 $$ This can have large variance for large $N$ or large $a_i$ values. However, if we take $K$ such samples, then averaging over $K$ , $$ \frac{1}{K} \sum_{k=1}^K \left( \sum_{i=1}^N a_i \epsilon_i^k - \epsilon_0^k \right) $$ The variance will scale as $1 / K$ . We can use an errors-in-variables model to infer the values of the coefficients. the values of $a_i$ will tell us how much each $x_i$ contributes relative to $x_0$ . (A note of caution here is that, while the choice of $x_0$ was arbitrary, it is important that it contributes to the sum significantly enough.) a confidence interval or a posterior distribution over $b$ gives how likely $b = 0$ .
