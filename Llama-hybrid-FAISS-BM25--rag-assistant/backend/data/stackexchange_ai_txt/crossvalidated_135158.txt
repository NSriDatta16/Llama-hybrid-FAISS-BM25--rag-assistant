[site]: crossvalidated
[post_id]: 135158
[parent_id]: 
[tags]: 
Likelihood maximization: MCEM algorithm versus MCMC algorithm

Hello Everyone this is my first question. I am a particle physicist and I am doing some empirical studiues on parameters estimation using different methods (this might give me some handle to study on the systematic effects associated with different methods). My current problem is to write a quite general analysis program to do maximum likelihood estimation, for a general model for the data. I already have a working MCMC for bayesian analysis (used in other kind of problem), that I could "adapt" for the current problem (plugging in flat priors for the parameters the posterior would just be proportional to the likelihood). Since I need to permorm systematic studies on simulated data (where I know the answer aready) running one or several chains could be rather long, especially when the parameters are many (10 or more), I thought that the use of EM algorithms (or better MCEM) could help me in maximizing the likelihood. Here follows the log-likelihood I am using to analyze the gamma-ray peaks in a spectrum in order to determine the activities of radioactive isotopes: $$\mathcal{L}(\alpha,\underline{\beta_{L}},\underline{\beta_{R}})= \sum_{k}{\left[ G_{k}\log[(\alpha\varepsilon_{k}+s_{k}\beta_{T_{k}})t]+R_{k}\log(\beta_{R_{k}}t)+R_{k}\log(\beta_{L_{k}}t) - \alpha\varepsilon_{k}t - (s_{k}+1)\beta_{T_{k}}t \right]}$$ Where the index $k$ runs over every line under analysis. $G_{k}$, $R_{k}$ and $L_{k}$ are the measured number of counts in three different regions across the line $k$. The parameters are $\alpha$ (the activity of the radionucleide), $\beta_{L_{k}}$ and $\beta_{R_{k}}$ which are respectively the background rates in the left and in the right control regions. In the signal region of each gamma-line the expected count rate is given by $(\alpha\varepsilon_{k}+s_{k}\beta_{T_{k}})t$, where $\beta_{T_{k}}=(\beta_{L_{k}}+\beta_{R_{k}})$. The other parameters like $t$, $\varepsilon_{k}$ and $s_{k}$ must be considered just given constants. As you can see in this case the model is quite simple as it is just described by three Poisson distributions for each line under analysis. For the MCEM implementation I consider the latent variables $S_{k}$ and $B_{k}$, that are observed in the experiment only as a sum: $G_{k}=S_{k}+B_{k}$. Therefore for I get the log-likelihood of the complete model is given by for given by: $$\log{P(\{G_{k}\},\{S_{k}\},\{L_{k}\},\{R_{k}\}|\alpha,\underline{\beta_{L}},\underline{\beta_{R}})} = \log{P(\{G_{k}\},\{S_{k}\}|\alpha,\underline{\beta_{L}},\underline{\beta_{R}}))} + \log{P(\{R_{k}\},\{L_{k}\}|\underline{\beta_{L}},\underline{\beta_{R}}))} = \sum_{k}{\left[ S_{k}\log\left(\frac{\alpha\varepsilon_{k}}{\beta_{T_{k}}s_{k}}\right) + G_{k}\log(\beta_{T_{k}}t) + L_{k}\log(\beta_{L_{k}}t) + R_{k}\log(\beta_{R_{k}}t) - \alpha\varepsilon_{k}t - (s_{k}+1)\beta_{T_{k}}t \right]}$$ The conditional distribution of $S_{k}$ is then just a binomial: $$P(S_{k}|G_{k},\alpha,\beta_{L_{k}},\beta_{R_{k}}) = \binom{G_{k}}{S_{k}}\left( \frac{\alpha\varepsilon_{k}}{\alpha\varepsilon_{k}+\beta_{T_{k}}s_{k}} \right)^{S_{k}}\left( \frac{\beta_{T_{k}}s_{k}}{\alpha\varepsilon_{k}+\beta_{T_{k}}s_{k}} \right)^{G_{k}-S_{k}}$$ For this easy model I don't actually need a MC estimation of the function $G(\theta|\theta^{(i)}$ as it is just sufficient to calculate the contditional expectation of $E(S^{(i)}_{k}|G_{k})$ with the parameters $\alpha^{(i)}_{k}$, $\beta^{(i)}_{L_{k}}$ and $\beta^{(i)}_{R_{k}}$: $E(S^{(i)}_{k}|G_{k}) = G_{k}\left( \frac{\alpha^{(i)}\varepsilon_{k}}{\alpha^{(i)}\varepsilon_{k}+\beta^{(i)}_{T_{k}}s_{k}} \right)$ The same think might be done sampling from the binomial and using the sample mean of the outcomes of $S_{k}$. Also the M-step results rather easy as it is possible to find algebraic solution of linar equations for $\alpha^{(i+1)}$, $\beta^{(i)}_{L_{k}}$ and $\beta^{(i)}_{R_{k}}$ (I skip their explicit determination since it is quite stritforward). Since I am not so advanced in statistics and statistical methods the first thing I would ask you is whether my reasoning is correct (I have mostly followed this paper from Wei and Tanner). The second question I would ask is whether there is a "sampling way" (sorry for a not proper language) to perform the M-step instead of using numerical methods. The reason of this question is that for each model that I use (simple or complicated) I would like to have an algorithm which goes toward the maximum of the likelihood in steps without the need to calculate numerically gradients and variances matrixes.
