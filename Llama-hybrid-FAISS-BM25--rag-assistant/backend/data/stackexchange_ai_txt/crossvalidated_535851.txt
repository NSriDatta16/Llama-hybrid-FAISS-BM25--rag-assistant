[site]: crossvalidated
[post_id]: 535851
[parent_id]: 
[tags]: 
How does gradient descent help SVM learn a linearly separable hyperplane?

So I see the Perceptron Algorithm applied to learning an SVM, where $\theta$ is the normal vector to the linearly separating hyperplane. How does the update $$\theta^{t+1}\leftarrow\theta^t+\alpha y_ix_i$$ There's this visual example, but I'm not sure how updating $\theta$ , causes it to adjust in the correct direction. Help push the hyperplane to learn a linear separation between the points $x_n$ , where $y_i$ and $x_i$ are misclassified points?
