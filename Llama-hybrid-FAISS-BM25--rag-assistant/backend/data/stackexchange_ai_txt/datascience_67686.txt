[site]: datascience
[post_id]: 67686
[parent_id]: 67680
[tags]: 
The definition of your customized training is too vague to be able to implement it: what is the "outcome" of a layer? How do you "train a layer with the outcome of another layer"? Also, note that, while it is possible to freeze layers selectively during training (i.e. not applying any update during the optimization step), the gradients will always propagate backward through layers that are connected. Nevertheless, there is a viable approach that may be somewhat similar to the intuition behind your proposal: residual connections . As shown in the figure above (taken from the original paper ), residual connections are simply connections from early layers to further points in the network by simply adding them. They allow gradients to be more easily propagated. You could create your autoencoder so that the result of each decoder layer gets added the result of the encoder layer of matching size.
