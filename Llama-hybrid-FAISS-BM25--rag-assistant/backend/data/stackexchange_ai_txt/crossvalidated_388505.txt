[site]: crossvalidated
[post_id]: 388505
[parent_id]: 
[tags]: 
Types of Machine Learning algorithms and architectures where only a fraction of the weights are trained per each training step?

I need help for literature review I'm doing: Does anyone know of a paper where a large number ( Over 50% ) of the weights are turned off at each training step ( besides dropout ) ? Or, if there are any key words/phrases that you suggest querying ? An example of this is pretty much any word embedding architectures, where only a small percentage of the weights are trained during each time-step (the contexts, labels, and negative samples). Another example is something like pathNet, where may of the parameters are frozen after training on a specific task, in order to transfer to a new task. Even if you don't know the papers off hand, would appreciate any ideas you have for words or phrases to query.
