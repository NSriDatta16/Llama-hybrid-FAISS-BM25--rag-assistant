[site]: datascience
[post_id]: 121824
[parent_id]: 
[tags]: 
How does relu appears in first layer gradient of backpropagation?

I'm following Stanford's Natural language processing course in Coursera. I'm learning about "Continuous bag of words" model Where neural network with one relu(first layer) and one softmax(second layer) is involved. The gradient of the J with respect to W1 somehow goes like this: $$\delta J/\delta W_1 = 1/m * Relu(W_2^T(\hat{Y} - Y))X^T$$ . But according to the general formula of backpropagation, isn't that supposed to be: $$\delta J/\delta W_1 = 1/m*(W_2^T(\hat{Y} - Y)X^T \odot [Z_1 > 0])$$ , where $[Z_1 > 0]$ is just the derivative of the first layer. My question: How are these two equivalent. I tried some many ways to connect these!
