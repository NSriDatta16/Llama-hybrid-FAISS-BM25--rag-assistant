[site]: crossvalidated
[post_id]: 440760
[parent_id]: 285006
[tags]: 
In context of tidy data, one bootstraps on samples(rows) and one bootstraps on both samples(rows) and variables(columns). They, as far as I know, always bootstrap in rows. Here are the rules for "tidy" originally put forth by Hadley Wickham [ 1 , 2] : Each variable forms a column. Each observation forms a row. Each type of observational units forms a table. So the question becomes "what is the advantage of bootstrapping on columns" . It gives you what bootstrapping always gives, but applied to the column space: robust characterization. when a column is important, and excluded, error is much larger and vis versa. This can add emphasis on giving higher weight to higher importance variables, and given that tree-weights are inverse to error, this can reduce the impact of less important variables. Accelerated compute: when you operate on less data, ceteris paribus, your algo runs faster. If you make each tree with 75% of the columns, then they construct faster. Update: A classic CART-model looks at the along edges of the hyper-rectangle to find the location where a binary split of the domain best improves the measure of goodness. This is the stump. It then repeats the process for every sub-parcel of the domain until stopping criteria is met. A CART-model is a weak learner. In a random forest, the parallel ensemble of CART-models, one is trying to aggregate weak learners to overcome their bias. Because there is more than one element required for an "ensemble" the ensemble can depart from classic CART and do things like bootstrap in row and column spaces. By column-wise bootstrapping, we are only looking at a subset of the axes for splitting. By row-wise bootstrapping, we are looking at a subset of points when evaluating the split metric. The value of the approach is data-dependent. Mileage varies, as always. There are, I have heard, some substantial compute-speedups to be found, but they only work if the whole column is treated as a single entity. I'm thinking about Gini importance or variance based trees. You can compute the score for the whole, and a subset, and then in O(1) get the score for the other subset. This doesn't work if you are bootstrapping on the rows(samples) but only on the columns(variables). So the trade-off made is that the rows are resampled once per tree and the columns are resampled at each split. This allows faster compute per tree, faster per split, and allows bootstrapping on both.
