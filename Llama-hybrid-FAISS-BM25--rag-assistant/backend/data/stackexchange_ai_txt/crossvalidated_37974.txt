[site]: crossvalidated
[post_id]: 37974
[parent_id]: 37973
[tags]: 
What you are seeing is called "Regression towards the Mean" and is completely expected. Any time that there is variability in the data (and yours looks like it has a bunch) then the prediction values will on average be between the overall mean and the observed values. The plot you created, of the outcome vs the predicted values is not commonly done, for the reasons that you are seeing, it tends to confuse more than enlighten. It is more common to plot the predicted values against the residuals as this plot will show more randomness if the model is reasonable. Edit to address comment below csgillespie's example has been criticized for only including 1 predictor when the original question included 4. Here is some quick R code that can be run to show the same patterns with 4 predictors: # simulated data, no relationship df1 Notice that the plots look very similar to those in the original question. Notice also that in the plot of the original outcome vs. the fitted values that the points (and more so their trend) tend to fall between the $y=x$ line and the mean line. This is the idea of regression towards the mean as originally described by Galton See Here . The points are not randomly scattered about the $y=x$ line like the original poster assumed and what would happen without the regression towards the mean, rather they follow a linear trend along a line that represents proportianality between the $y=x$ line and the overall mean line, just as predicted by Galton. The term regression towards the mean (and variants) is sometimes used for other concepts (some closer related to the original than others) as can be seen in the above article, and that may be where some of the confusion comes from.
