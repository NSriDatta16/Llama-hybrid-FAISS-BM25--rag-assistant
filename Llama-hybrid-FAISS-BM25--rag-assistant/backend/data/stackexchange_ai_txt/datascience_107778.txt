[site]: datascience
[post_id]: 107778
[parent_id]: 107521
[tags]: 
Someone, such as Albert Einstein, spends his/her entire life coming up with a solution to a problem, such as Special Relativity. Then within a single class, maybe an hour and a half, a teacher teaches you and you now have the same knowledge. That's basically what happens in distilling knowledge. The teacher model does the hard work by learning the relationship between inputs and outputs. The student model learns that relationship from the teacher network. A prime example is DeiT in which the authors trained vision transformer with CNN teacher and achieved competitive performance with the original vision transformer ViT that needed about 300M images for pretraining.
