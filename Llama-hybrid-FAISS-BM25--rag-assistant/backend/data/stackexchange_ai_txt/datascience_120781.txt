[site]: datascience
[post_id]: 120781
[parent_id]: 
[tags]: 
Dynamic batching and padding batches for NLP in deep learning libraries

This is the usual way we train modern deep learning models for NLP, e.g. with Huggingface libraries where we have a fix length for the input no. of tokens/subwoords unit. https://huggingface.co/docs/transformers/pad_truncation In the follow example, we have 5 sentences of various length and all of them are padded to the max length set at 1024. The first part of my question is with regards to GPU memory usage and pad, when we train a model with batches of data with padded inputs, would the padded tokens hog up the GPU RAM ? Even if the model don't compute them since they will return zeros, it's still rather wasteful. Or does PyTorch / Tensorflow or other lower-level tensor libraries reoptimize the batch such that the pads don't take up memory? If so, any pointers to code/docs on this? There are instances where the batches can be ordered in a way to arrange batches with similar length to go together, esp. at the start of model training, e.g. https://discuss.huggingface.co/t/are-dynamic-padding-and-smart-batching-in-the-library/10404/15 Instead of doing padding, are there existing code for some sort of dynamic batching without sorting, is there a way to keep an offset of all the input sentences EOS token and pack the batch into something that looks like this : Are there examples of the above batch packing in other deep learning libraries? Or in native Pytorch/Tensorflow/JAX?
