[site]: crossvalidated
[post_id]: 548769
[parent_id]: 548767
[tags]: 
Linear regression is $$ y = \mathbf{x} \cdot \boldsymbol{w} + b $$ that’s exactly the same as a single-layer network with linear activation. Linear regression is usually trained using ordinary least squares , but alternatively you could use an optimization algorithm such as gradient descent . Same as with other neural networks, gradient descent can be used for all data, in mini-batches, or one sample at a time (stochastic gradient descent). It’s just single layer, so you don’t need back propagation.
