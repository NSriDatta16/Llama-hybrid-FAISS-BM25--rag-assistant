[site]: crossvalidated
[post_id]: 44740
[parent_id]: 44727
[tags]: 
@FrankHarrell is correct that percent accuracy isn't the loss function that logistic regression is trying to optimize. So there could be situations where the best model according to the (quasi) binomial likelihood isn't also the best one according to percent accuracy. Edited to add : He's also right in the comments below that setting a cutpoint has serious problems. What I've proposed below is a workaround that gets at the intuition of percent accuracy but avoids setting an arbitrary threshold between the underlying continuous model predictions. On the other hand, percent accuracy seems like a perfectly reasonable loss function as well, and it might be worth knowing how logistic regression performs with it. Percent accuracy can be more intuitive, and it isn't as susceptible to outliers and the occasional prediction that was off by a very large amount. Finding this value is pretty straightforward. First, find the probabilities the model assigns to each outcome: probabilities If your glm flipped a bunch of biased coins for each response, it would give heads this percent of the time. Then all you need to do is find the proportion of the time that the coin will come up the wrong way. The simplest way is probably: 1 - mean(abs(probabilities - binary.outcome)) You can prove to yourself that this gives the right answer by simulating the biased coinflips yourself with rbinom .
