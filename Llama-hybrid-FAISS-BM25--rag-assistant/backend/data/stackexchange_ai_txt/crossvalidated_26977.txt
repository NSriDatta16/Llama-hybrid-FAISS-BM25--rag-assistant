[site]: crossvalidated
[post_id]: 26977
[parent_id]: 26930
[tags]: 
1) Are there any particular assumptions regarding the errors for logistic regression such as the constant variance of the error terms and the normality of the residuals? Logistic regression models don't have "errors" in the traditional sense. It is both counter-intuitive and methodologically inconsistent. The model outputs are fitted probabilities or risks whereas the observed outcomes are 0/1 event indicators. Methodologically, you would tend to under-emphasize domains of very high or very low fitted probabilities (contributing very small amounts to the residual distance) whereas the model fitting algorithm places considerably higher importance on such regions. Squared distance is generally a poor way of calibrating a logistic regression model. An alternative goodness of fit test is the Hosmer-Lemeshow test in which the fitted values are used to create binned partitions based on deciles of fitted risk. You can read about this test in Alan Agresti's Categorical Data Analysis or the book Logistic Regression by Hosmer and Lemeshow. Another process is to use the Studentized Residuals where the mean variance relationship is used to reweight residuals by their fitted inverse variance. For logistic regression this is $$r_{stud} = \frac{Y - \mu}{\sqrt{\mu(1-\mu)}}$$ 2) Also typically when you have points that have a Cook's distance larger than 4/n, do you remove them? If you do remove them, how can you tell if the model with the removed points is better? I never remove points based on sensitivity analyses. If I do a random sample of 100 people and their income and 1 person happens to be a billionaire, then my safest assumption is that that 1 billionaire represents 1/100th of the population.
