[site]: crossvalidated
[post_id]: 494760
[parent_id]: 247260
[tags]: 
PCA is not designed for noise removal purpose. It is designed to REDUCE DIMENSIONS . As a large number of features are difficult to handle. PCA just lets you to approximate your data. Think PCA as a tuning knob . You can smoothly decide how much approximation you want by tuning it and which is impossible to achieve if you work directly with original given features. Because you cannot directly decide which feature to keep and which feature to eliminate to approximate your data at a desired level. Because the original features have no order of their priority or usability on which you can decide on which one to keep and which one to eliminate. That's why PCA comes into the place. The main difference between the original dimensions and principle components is that, if you are working with original dimensions, you can think that the dimensions were already available before any data points were even plotted. So what wrong can happen? The problem is after plotting the data points these points can be positioned randomly and they may not allow us to eliminate some of the original dimensions directly to approximate it. As depending on the positions of the data points in many cases none of the original feature dimensions may be able to capture good variations notably. So the situation will be like, to capture a decent amount of variations of data you may need to keep a large number of original dimensions. Which is not efficient. So what's the remedy? One of the remedies is using PCA ! In PCA we do the opposite. Here the data points are already there. Now we will be placing the new dimensions (principle components) one by one with the target of capturing most of the variations available at that stage which are not still captured by the previous principle component(s) we have already plotted. Hence, the first PC covers the maximum possible variations (variation is measured by variance) possible to be captured by a single PC . The second PC captures the variations of data less than the first PC and those variations were missed out by the first PC . The third PC again does less than the second PC and so on. So, these principle components are already sorted based on how useful they are or how much variance they can capture. Each of the principle components has two properties - eigenvector and eigenvalue. The measure of the captured variations is nothing but the eigenvalue of that PC and the direction of that PC is just the eigenvector of it. As PC s are also axes and so each of them must have a direction. So in your case, as you have said in the comment that after applying PCA the performance has improved, this is just because when you are eliminating some of the PC s of lower variances i.e, of lower eigenvalues, this action may be helping the model to generalize well. Because PC s of higher eigenvalues are capturing the more generalized features. As you are taking more and more PC s, the specialized features are also being added. If you take all of them the 100% of the data-variations will be restored like the original dimensions. So removing removing some PC s with lower eigenvalues actually acting as some sort of regularization and your model is only learning the more general features and not being confused by very fine detail which are likely not the general properties of that class. This is how overfitting is being prevented upto a certain level. But again this doesn't assure you that those very fine example-specific details are noise. Noise can be embedded even with other PC s as well. Because PCA doesn't know which is noise and which is information. As it is just a linear transformation. All the PC axes can be represented by some linear combinations of existing original dimensions. So based on the variation levels of different types noises they can be captured by different PC s. So it is not a guaranteed way to remove noise although noise may be reduced if the eliminated PC s are involved in capturing those noise. But with noise you may also lose information as well.
