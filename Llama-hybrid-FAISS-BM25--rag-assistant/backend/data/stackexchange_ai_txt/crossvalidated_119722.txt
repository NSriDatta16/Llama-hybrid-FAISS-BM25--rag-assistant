[site]: crossvalidated
[post_id]: 119722
[parent_id]: 119714
[tags]: 
As expected: it depends on what you want. In terms of generalization performance, typically the performance differences are minor. That said, minimizing the $l_1$-norm has the extremely attractive feature of yielding sparse solutions (the support vectors are a subset of the training set). When doing ridge regression, just like in least-squares SVM, all training instances become support vectors and you end up with a model the size of your training set. A large model requires a lot of memory (obviously) and is slower in prediction.
