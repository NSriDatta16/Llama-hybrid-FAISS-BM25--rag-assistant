[site]: crossvalidated
[post_id]: 540073
[parent_id]: 
[tags]: 
In cases where neural attention is used for machine translation, how they deal with translating sentences that have different lengths?

So attention and transformer models can be used for machine translation. Sometimes, a sentence in one language might consist of 5 words, but in the target language it consists of 8 words (so for example a word in the source language might be two words in the target language) How do K, Q, V in the attention models work to deal with this problem? Are there several values for that corresponding key? (in this case, several words corresponding to one concept?)
