[site]: crossvalidated
[post_id]: 304275
[parent_id]: 
[tags]: 
Machine learning regression when you only know the product of the truth values

Suppose I want to model an unknown function $f$ knowing a set of pair $\{ x_i, y_i\}_i^N$ where $x_i\in \mathbb{R}^D$ and $y_i\in \mathbb R$ such that $y_i=f(x_i)$. This is the classical problem of regression. My problem is more complicated. I know $\{ x_i^{(1)}, x_i^{(2)}, z_i\}_i^N$ such that $f(x_i^{(1)}) \times f(x_i^{(2)}) = z_i$. (The problem can be generalized to $g(f(x_i^{(1)}), f(x_i^{(2)})) = z_i$ where $g$ is known, in my actual problem it is $g(a, b) = \sqrt{ab}$). Using the L2 metric I should minimize $\sum_i|\hat f(x_i^{(1)}) \times \hat f(x_i^{(2)}) - z_i|^2$ where $\hat f$ is my algorithm (let say a neutral network). Question 1. Has this kind of problem a name? Question 2. How practically implement that? This is similar to a convolution since you are reusing the same network (not just one layer) on different inputs (you can represent your inputs as element of $\mathbb{R}^{2D}$. Using a package as tensorflow is it possible to duplicate a NN (let say a simple feed forward with few layers) but to say the the weights are the same in the sense that they are really the same variable?
