[site]: crossvalidated
[post_id]: 371046
[parent_id]: 
[tags]: 
Stochastic gradient descent (SGD) on data with weights

Mostly deep learning model training is on data with a unit weight. In this case, every mini-batch of a fixed size, say, 32, contains exactly the same total weight (32) for each update. This is the common case in most research papers of not all. However, what if the data records are not equally weighted? I have a dataset in which some records are of importance 1000 and others just 1-10. Pulling them randomly in a fixed sized mini-batch will result in different total weights in one batch to another. This will make the gradient of some batch gigantically large and also change the overall importance of samples. One way to handle this is to "flatten" all the data records, for instance, duplicating a record of weight 10 to be 10 separate records and shuffled together with others with weight 1. But this will make the overall input data size 10x or more in size. Is there any other mathematically smart way to deal with this issue? Any reference will be appreciated!
