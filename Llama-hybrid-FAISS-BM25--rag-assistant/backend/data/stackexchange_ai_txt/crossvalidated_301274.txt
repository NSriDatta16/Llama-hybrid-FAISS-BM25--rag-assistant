[site]: crossvalidated
[post_id]: 301274
[parent_id]: 274536
[tags]: 
I would suggest you study Word Embeddings a bit. You need to understand the mathematical significance of going from a one-hot encoded space to a dense vector space. However to answer your questions (based on what I understood): 1) Assuming you are talking about English, use some pre-trained Word2Vec model and simply convert all the tags to some fixed vector length, say 10 or 50. 2) Not sure what adding vectors would yield. You might be better off just multiplying all pairs of Doc 1 tags with Doc 2 tags, and then adding their cosine similarities. Like if Doc 1 has 5 tags and Doc 2 has 6 tags, then 30 pairs similarities can be found and those can be used.
