[site]: crossvalidated
[post_id]: 409665
[parent_id]: 
[tags]: 
Penalization term for unfairness

I am reading [1], where the researchers do a logistic regression, but add to the loss function the following penalization term for fairness $ R^{AVD}_{FP}(\theta; S) = \left\lvert \dfrac{\sum\limits_{x \in \mathcal{D}_{00}} \theta^T x}{\lvert \mathcal{D}_{00} \rvert} - \dfrac{\sum\limits_{x \in \mathcal{D}_{10}} \theta^T x}{\lvert \mathcal{D}_{10} \rvert} \right\rvert $ where FP=False Positive, $x$ are the predictors, $\theta$ are the parameters to learn, and $\mathcal{D}_{ab}$ is a partition of the dataset defined as follows: $\mathcal{D}_{ay} = \{(x^i, y^i) \in \mathcal{D}: x^i_1 = a, y^i = y\}$ where $x_1$ , the first predictor, is a "protected" attribute, for example, race. We want a prediction model that is not sensible to $x_1$ . For example, we want to classify the probability that a person will commit a heist independently if the person is white or black. But I am not understanding the penalization term. It would make sense if the sum was over the misclassified points. But there is no such concept there? Also because the penalization is added to the loss function, so during the learning phase, where of course there is not yet such a concept as misclassified or not. For example, the first sum seems to be over all the samples for which the protected attribute is 0, and the true label is 0. The $\theta^T x$ can be negative (true negative) or positive (false positive!). How does it make sense? [1] Bechavod, Y., & Ligett, K. (2017). Penalizing Unfairness in Binary Classification. Retrieved from http://arxiv.org/abs/1707.00044
