[site]: datascience
[post_id]: 86541
[parent_id]: 86537
[tags]: 
I give you that this is a weird way of displaying the data, but the accuracy is the only field that don't fit the schema. For example: precision recall f1-score support 0 0.84 0.97 0.90 160319 1 0.67 0.27 0.38 41010 As explained in How to interpret classification report of scikit-learn? , the precision, recall, f1-score and support are simply those metrics for both classes of your binary classification problem. The second part of the table: accuracy 0.82 201329 The accuracy is the overall accuracy of the model (note that accuracy is not a measure that is relative to a certain class, but a performance across all classes). The macro average for the precision and recall score is just the harmonic mean of the two classes. ie: recall macro avg = (recall_class_1 + recall_class_0) / 2 The weighted average is just the average metric of the two classes weighted by the support/size-of-sample. ie: recall weighted avg = (support_class_0 * recall_class_0 + support_class_1 * recall_class_1) / (support_class_0 + support_class_1) This is a pretty long-winded way of saying the accuracy is just the overall accuracy. It has nothing to do with the column f1-score that it happens to be under.
