[site]: crossvalidated
[post_id]: 373369
[parent_id]: 373167
[tags]: 
One thing to remember is that ML is an instrumental goal. Ultimately, we don't want to predict out of stock events, we want to prevent out of stock events. Predicting out of stock events is simply a means to that end. So as far as Type II errors are concerned, this isn't an issue. Either we continue to have OOSE, in which case we have data to train our model, or we don't, in which the problem that the model was created to address has been solved. What can be a problem is Type I errors. It's easy to fall into a Bear Patrol fallacy, where you have a system X that is built to prevent Y, you don't see Y, so you conclude that X prevents Y, and any attempts to shut X down are dismissed on the basis "But it's doing such a good job preventing Y!" Organizations can be locked into expensive programs because no one wants to risk that Y will come back, and it's difficult to find out whether X is really necessary without allowing that possibility. It then becomes a trade-off of how much you're willing to occasionally engage in (according to your model) suboptimal behavior to get a control group. That's part of any active exploration: if you have a drug that you think is effective, you have to have a control group that isn't getting the drug to confirm that it is in fact effective.
