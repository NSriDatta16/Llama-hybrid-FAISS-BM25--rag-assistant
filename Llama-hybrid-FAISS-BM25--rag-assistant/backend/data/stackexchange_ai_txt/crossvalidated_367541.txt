[site]: crossvalidated
[post_id]: 367541
[parent_id]: 226005
[tags]: 
Late on the post, but to help future generations I will respond. In short, the answer is no. Statistical significance does not tell the researcher anything about a model's predictive ability. The definition of p-values makes this clear. A p-value represents how likely it is that you the researcher observed a given data set under the assumption that the null hypothesis is true. So even if you reject the null, you really cannot say anything about which model is better at predicting, only that what you observed is more or less likely given the null. A super common mistake is that people assume rejecting the null implies that the alternative is a better model, which is simply not the case. Perhaps a more complete answer would be this: p-values are not a valid metric for model comparison. To compare models effectively, you need methods like AIC or BIC. One could also ditch frequentism all together and use Bayesian estimation techniques to construct Bayes factors, which would allow you to compare models. One last (and to some controversial) comment on p-values. Consider this, given enough data, you will obtain significance. This is due to the (silly) nature of the null-hypothesis, which states that an effect is exactly zero. In a vast majority of cases, this is almost certainly not true. There will be some non-zero effect, just a small one. So what p-value thresholds do in practice is set up straw-man requirement that can always be met with enough data. Thus, given that you can always achieve significance, it should be unsurprising that p-values do not really tell you anything about model performance. As far as I can tell, they really do not tell you much of anything. As scientists, we care about estimation and prediction and p-values do not help us do either...
