[site]: crossvalidated
[post_id]: 353325
[parent_id]: 
[tags]: 
How do you calculate interrater reliability for multiple items?

I have responses rated on 12 binary categories, treating the categories as separate items on the same measure. There are 10 raters, so Fleiss's Kappa seems appropriate. How would you compute reliability for the whole measure? Would you calculate it for each individual item and then take the average of all coefficients as the overall reliability? So for instance I have 5 responses and code them on categories A, B, and C. I would compute Kappa separately for each category, but then how to summarize the reliability across categories? Credit to this question for the sample data structure. Rater 1: A B C Response 1 | 1 0 1 Response 2 | 1 0 0 Response 3 | 0 0 1 Response 4 | 1 1 0 Response 5 | 0 1 0 Rater 2: A B C Response 1 | 1 0 1 Response 2 | 1 0 0 Response 3 | 0 0 1 Response 4 | 1 1 0 Response 5 | 0 1 0 Rater 3: A B C Response 1 | 1 0 0 Response 2 | 0 1 0 Response 3 | 1 0 1 Response 4 | 1 0 0 Response 5 | 1 1 1 This seems very straightforward, yet all examples I've found are for one specific rating, e.g. inter-rater reliability for one of the binary codes. This question and this question ask essentially the same thing, but there doesn't seem to be a conclusion.
