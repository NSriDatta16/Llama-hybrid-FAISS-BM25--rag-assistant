[site]: crossvalidated
[post_id]: 284676
[parent_id]: 284663
[tags]: 
Fantasy particles were first introduced by Tielman, using a technique for training RBMs called persistent contrast divergence. See: Training restricted Boltzmann machines using approximations to the likelihood gradient- Tieleman . and Using Fast Weights to Improve Persistent Contrastive Divergence -Tieleman, et. al. From, A Practical Guide for training Restricted Boltzmann Machines -- Hinton : A more radical departure from CD1 is called “persistent contrastive divergence” (Tieleman, 2008). Instead of initializing each alternating Gibbs Markov chain at a datavector, which is the essence of CD learning, we keep track of the states of a number of persistent chains or “fantasy particles”. Each persisitent chain has its hidden and visible states updated one (or a few) times after each weight update. The learning signal is then the difference between the pairwise statistics measured on a mini- batch of data and the pairwise statistics measured on the persistent chains. Typically the number of persistent chains is the same as the size of a mini-batch, but there is no good reason for this. The persistent chains mix surprisingly fast because the weight-updates repel each chain from its current state by raising the energy of that state (Tieleman and Hinton, 2009). See also https://www.cs.toronto.edu/~hinton/csc2535/notes/lec4new.pdf
