[site]: crossvalidated
[post_id]: 57149
[parent_id]: 57133
[tags]: 
The AIC and BIC optimize different things. AIC is basically suitable for a situation where you don't necessarily think there's 'a model' so much as a bunch of effects of different sizes, and you're in a situation you want to get good prediction error. As such, as the sample size expands, the AIC choice of model expands as well, as smaller and smaller effects become relevant (in the sense that including them is on average better than excluding them). BIC on the other hand basically assumes the model is in the candidate set and you want to find it. BIC tends to hone in on one model as the number of observations grows, AIC really doesn't. As a result, at large $n$, AIC tends to pick somewhat larger models than BIC. If you're trying to understand what the main drivers are, you might want something more like BIC. If that's less important than good MSPE, you might lean more toward AIC.
