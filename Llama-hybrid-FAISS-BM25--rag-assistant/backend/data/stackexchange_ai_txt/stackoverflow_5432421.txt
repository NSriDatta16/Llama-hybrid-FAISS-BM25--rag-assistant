[site]: stackoverflow
[post_id]: 5432421
[parent_id]: 5432244
[tags]: 
The keyword is DISTANCE or "string distance" . and also, " Paragraph similarity " You seek to implement a function which would express as a scalar, say a percentage as suggested in the question, indicative of how similar a string is from another string. Plain string distance functions such as hamming or Levenstein may not be appropriate, for they work at character level rather than at word level, but generally these algorithms convey the idea of what is needed. Working at word level you'll probably also want to take into account some common NLP features, for example ignore (or give less weight to) very common words (such as ' the ', ' in ', ' of ' etc.) and maybe allow for some forms of stemming. The order of the words, or for the least their proximity may also be of import. One key factor to remember is that even with relatively short strings, many distances functions can be quite expensive, computationally speaking. Before selecting one particular algorithm you'll need to get an idea of the general parameters of the problem: how many strings would have to be compared? (on average, maximum) how many words/token do the string contain? (on average, max) Is it possible to introduce a simple (quick) filter to reduce the number of strings to be compared ? how fancy do we need to get with linguistic features ? is it possible to pre-process the strings ? Are all the records in a single language ? Comparing Methods for Single Paragraph Similarity Analysis , a scholarly paper provides a survey of relevant techniques and considerations. In a nutshell, the the amount of design-time and run-time one can apply this relatively open problem varies greatly and is typically a compromise between the level of precision desired vs. the run-time resources and the overall complexity of the solution which may be acceptable. In its simplest form, when the order of the words matters little, computing the sum of factors based on the TF-IDF values of the words which match may be a very acceptable solution. Fancier solutions may introduce a pipeline of processes borrowed from NLP, for example Part-of-Speech Tagging (say for the purpose of avoiding false positive such as " SAW " as a noun (to cut wood), and " SAW " as the past tense of the verb "to see". or more likely to filter outright some of the words based on their grammatical function), stemming and possibly semantic substitutions, concept extraction or latent semantic analysis .
