[site]: crossvalidated
[post_id]: 303025
[parent_id]: 303022
[tags]: 
In batch processing, the gradient is evaluated for several different input/output values, with each observation yielding a different vector. We then average together the gradient vectors over all observations in the batch and take a single step in the resulting direction. The benefit of this is that the gradient estimate using a single point only may be very noisy. By averaging together the gradient over many samples we obtain a less noisy estimate. I'll also point out that having too large of a batch can also be a bad thing. Sometimes the noise in the gradient direction can kick you out of local minima and help you get to a better solution. This is problem specific, though, so when training a NN, you should experiment with many different batch sizes. This paper goes into further detail about this.
