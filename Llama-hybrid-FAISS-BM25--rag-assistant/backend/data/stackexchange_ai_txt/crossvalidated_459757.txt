[site]: crossvalidated
[post_id]: 459757
[parent_id]: 
[tags]: 
Validation loss extremely high with good accuracy

I have tried creating a CNN for the famous MNIST challenge, but while I achive good accuracy on the validation set during training (I have around 99% after 20 epochs), the validation loss is behaving oddly: the loss is pretty normal (it goes down smoothly), but validation loss seems to be random values. Also, while I know accuracy on the test set should be lower than on the validation set, it goes from 99% to 96%. Any idea why these things are happening? I suppose I've written something poorly in my code, because that doesn't seem like a normal behavior. Here is what I wrote: import tensorflow as tf import sklearn import numpy as np from tensorflow import keras import matplotlib.pyplot as plt import pandas as pd mnist=keras.datasets.mnist (X,y),(X_test,y_test)=mnist.load_data() X_valid,X_train=X[:5000]/255.0,X[5000:]/255.0 y_valid,y_train=y[:5000],y[5000:] X_mean=X_train.mean(axis=0,keepdims=True) X_std=X_train.std(axis=0,keepdims=True)+1e-7 X_train=(X_train-X_mean)/X_std X_test=(X_test-X_mean)/X_std X_valid=(X_valid-X_mean)/X_std X_train=X_train[...,np.newaxis] X_valid=X_valid[...,np.newaxis] X_test=X_test[...,np.newaxis] model=keras.models.Sequential([ keras.layers.Conv2D(32,3,activation="relu",padding="SAME"), keras.layers.Conv2D(64,3,activation="relu",padding="SAME"), keras.layers.MaxPooling2D(), keras.layers.Flatten(), keras.layers.Dropout(0.25), keras.layers.Dense(128,activation="relu"), keras.layers.Dropout(0.5), keras.layers.Dense(10,activation="softmax") ]) model.compile(loss="sparse_categorical_crossentropy", optimizer="nadam", metrics=["accuracy"]) history=model.fit(X_train,y_train,epochs=20,validation_data=(X_valid,y_valid)) pd.DataFrame(history.history).plot(figsize=(8,5)) plt.grid(True) plt.gca().set_ylim(0,1) plt.show() model.evaluate(X_test,y_test) Here is what I obtained after executing the code: Train on 55000 samples, validate on 5000 samples Epoch 1/20 55000/55000 [==============================] - 7s 136us/sample - loss: 0.1904 - accuracy: 0.9442 - val_loss: 5.0653 - val_accuracy: 0.9834 Epoch 2/20 55000/55000 [==============================] - 7s 125us/sample - loss: 0.0799 - accuracy: 0.9761 - val_loss: 1.9537 - val_accuracy: 0.9880 Epoch 3/20 55000/55000 [==============================] - 7s 125us/sample - loss: 0.0598 - accuracy: 0.9823 - val_loss: 5.4052 - val_accuracy: 0.9864 Epoch 4/20 55000/55000 [==============================] - 7s 126us/sample - loss: 0.0530 - accuracy: 0.9848 - val_loss: 0.5642 - val_accuracy: 0.9888 Epoch 5/20 55000/55000 [==============================] - 7s 122us/sample - loss: 0.0423 - accuracy: 0.9866 - val_loss: 10.1840 - val_accuracy: 0.9916 Epoch 6/20 55000/55000 [==============================] - 7s 122us/sample - loss: 0.0375 - accuracy: 0.9879 - val_loss: 15.6639 - val_accuracy: 0.9902 Epoch 7/20 55000/55000 [==============================] - 7s 121us/sample - loss: 0.0334 - accuracy: 0.9895 - val_loss: 7.7230 - val_accuracy: 0.9892 Epoch 8/20 55000/55000 [==============================] - 7s 121us/sample - loss: 0.0312 - accuracy: 0.9905 - val_loss: 40.1243 - val_accuracy: 0.9892 Epoch 9/20 55000/55000 [==============================] - 7s 125us/sample - loss: 0.0287 - accuracy: 0.9909 - val_loss: 0.0332 - val_accuracy: 0.9904 Epoch 10/20 55000/55000 [==============================] - 7s 123us/sample - loss: 0.0253 - accuracy: 0.9923 - val_loss: 14.4594 - val_accuracy: 0.9920 Epoch 11/20 55000/55000 [==============================] - 7s 126us/sample - loss: 0.0241 - accuracy: 0.9926 - val_loss: 9.4999 - val_accuracy: 0.9888 Epoch 12/20 55000/55000 [==============================] - 7s 125us/sample - loss: 0.0239 - accuracy: 0.9928 - val_loss: 0.0350 - val_accuracy: 0.9928 Epoch 13/20 55000/55000 [==============================] - 7s 126us/sample - loss: 0.0210 - accuracy: 0.9932 - val_loss: 1.8624 - val_accuracy: 0.9906 Epoch 14/20 55000/55000 [==============================] - 7s 122us/sample - loss: 0.0227 - accuracy: 0.9936 - val_loss: 23.4051 - val_accuracy: 0.9918 Epoch 15/20 55000/55000 [==============================] - 7s 119us/sample - loss: 0.0227 - accuracy: 0.9933 - val_loss: 18.1206 - val_accuracy: 0.9924 Epoch 16/20 55000/55000 [==============================] - 7s 125us/sample - loss: 0.0202 - accuracy: 0.9938 - val_loss: 23.3301 - val_accuracy: 0.9910 Epoch 17/20 55000/55000 [==============================] - 7s 126us/sample - loss: 0.0173 - accuracy: 0.9944 - val_loss: 0.0534 - val_accuracy: 0.9902 Epoch 18/20 55000/55000 [==============================] - 7s 125us/sample - loss: 0.0175 - accuracy: 0.9946 - val_loss: 7.3290 - val_accuracy: 0.9906 Epoch 19/20 55000/55000 [==============================] - 7s 127us/sample - loss: 0.0179 - accuracy: 0.9942 - val_loss: 67.2724 - val_accuracy: 0.9904 Epoch 20/20 55000/55000 [==============================] - 7s 126us/sample - loss: 0.0186 - accuracy: 0.9946 - val_loss: 73.3262 - val_accuracy: 0.9892 As for the evaluation on the test set: 10000/10000 [==============================] - 0s 46us/sample - loss: 31.8384 - accuracy: 0.9668 Here's the image of the loss/accuracy over the epochs: Any idea what's happening there? Did I make a beginner's mistake or is this a bug?
