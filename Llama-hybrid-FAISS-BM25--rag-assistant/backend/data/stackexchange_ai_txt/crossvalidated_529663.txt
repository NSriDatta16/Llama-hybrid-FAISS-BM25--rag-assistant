[site]: crossvalidated
[post_id]: 529663
[parent_id]: 
[tags]: 
How can we incorporating uncertainty about our data into Bayesian inference?

I want to use a Bayesian approach to estimate the parameter $\theta$ of a binomial distribution $\textsf{Binomial}(\theta,n)$ with the number of Bernoulli experiments $x_i \in \{0,1\}$ being known and fixed to $n$ . In the textbook example, the binomial likelihood only takes the sum of positive vs. negative outcomes of $x_i$ into account: \begin{equation} f(x|\theta) = \theta^{\sum x_i} (1-\theta)^{n-\sum x_i} \end{equation} Together with a uniform prior $f(\theta)=\textsf{Uniform}(0,1)$ , the posterior becomes the Beta distribution \begin{equation} f(\theta|s) = \textsf{Beta}(\theta|s+1,n-s+1) . \end{equation} , where $s=\sum x_i$ . Unfortunately, $x_i$ is not directly observable. Instead, we are given $\pi_i$ as features to e.g. a neural network, which then provides a probabilistic mapping $f(x_i|\pi_i)$ . This probability represents uncertainty about the true, latent value $x_i$ . How can we incorporate this uncertainty about the data into the estimate? E.g. if $f(x_i|\pi_i)=0.51\ \forall\ 0\leq\ i , we would expect the MAP estimate of $\theta$ to be a lot further off from $1$ than if $f(x_i|\pi_i)=0.91$ throughout.
