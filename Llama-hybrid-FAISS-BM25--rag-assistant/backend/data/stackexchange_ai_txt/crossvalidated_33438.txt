[site]: crossvalidated
[post_id]: 33438
[parent_id]: 33308
[tags]: 
Some of the distinctions I have identified so far were hidden in (one of) the original papers by Neil Lawrence. There are two versions of "A Sparse Bayesian Compression Scheme - The Informative Vector Machine" [Kernel Workshop at NIPS 2001], one on the Microsoft research site , and one on Laurence's site . In the MS version there is an extra sentence with the statement "the selected data points are close to the decision boundary, a characteristics shared with the SVM". So my original view that the IVM vectors represented the 'middle' was wrong. The other point is that it is a compression scheme in the sense that it is looking for a 'sparse representation of the data set ", so that the IVM also seeks to retain those vectors that provided the most information during the analysis, so they can be re-used, as a data set and any computation repeated. This ability to reduce the size of the data set is useful when the computation is bigO(M^3) The RVM does select the 'middle' vectors (and their weights) which act on the basis functions (e.g. Gaussians) ( see Ch7.2 'Relevance Vector Machines' in Bishop's 'Pattern recognition and machine learning'). OK, so the explanation is a bit of a hand waving description and isn't fully complete, but hopefully it will help those who aren't as relaxed with compact matrix formulations. More feedback would still be welcome.
