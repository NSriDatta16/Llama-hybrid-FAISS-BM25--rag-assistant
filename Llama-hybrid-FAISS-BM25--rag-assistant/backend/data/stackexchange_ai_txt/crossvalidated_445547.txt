[site]: crossvalidated
[post_id]: 445547
[parent_id]: 445497
[tags]: 
One concern I would have about this approach is that I believe it's been shown that powerful enough decoders may essentially ignore the stochasticity of the latent space, so your encodings may not need to be very good. That said, let's assume you got a good reconstruction with a weak decoder. I would say this is doable in that case. I would think a good measure would be to just measure the mean Kullback-Leibler Divergence between your candidate outlier and either some representative subset of known good encodings of that class (e.g. training data - but if you can take a peek at decent production embeddings, should be fine too), or to approximate the whole latent cluster's mean and covariance matrix and KLD against that . It's more or less the same approach as training the VAE in the first place, only you're comparing against the cluster parameters as your ideal, rather than the (0, 1) Standard Normal parameters. The first approach seems like it could be a bit more robust, but is far more of a pain to actually crunch the numbers for, the latter introduces the risk that your estimate of the cluster parameters is off. Finally, if you're feeling like something really cheap and simple and likely to go off the rails, you could just use a plain Euclidean distance from the mean, but that's really hacky.
