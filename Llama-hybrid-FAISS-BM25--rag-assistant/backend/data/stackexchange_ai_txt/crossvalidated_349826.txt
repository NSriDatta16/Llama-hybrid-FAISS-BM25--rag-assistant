[site]: crossvalidated
[post_id]: 349826
[parent_id]: 349815
[tags]: 
In the specific case the parameter space $\Theta$ is finite or countably infinite$$\Theta=\{\theta_1,\theta_2,\ldots\}$$ the posterior loss associated with the indicator loss is equal to the probability of being wrong $\mathbb{P}(\hat{\theta}\ne\theta|x)$ and it is minimised when the posterior probability of being correct $\mathbb{P}(\hat{\theta}=\theta|x)$ is maximised. This means that $\hat{\theta}$ is the mode of the posterior distribution or the MAP. However, this association of MAP and $0-1$ loss is a "folk theorem" in that it is incorrect in most settings, i.e., it does not hold for continuous parameter spaces where $\mathbb{P}(\hat{\theta}=\theta|x)=0$ for all $\hat{\theta}$'s and it further conflicts with the results of Druihlet and Marin (BA, 2007), who point out that the MAP ultimately depends on the choice of the dominating measure. (Even though the Lebesgue measure is implicitly chosen as the default.) For instance, Evans and Jang posted an arXiv paper in 2011 where they discuss the connection between MAP, least relative surprise (or maximum profile likelihood) estimators, and loss functions. The core of the matter is that neither MAP estimators, nor MLEs are really justified by a decision-theoretic approach, at least in a continuous parameter space. And that the dominating measure [arbitrarily] chosen on the parameter space impacts the value of the MAP, as demonstrated by Druihlet and Marin in 2007. They start in the finite case with the loss function $$ \mathrm{L}(\theta,d) = \mathbb{I}\{\Psi(\theta) \ne d) / \pi_\Psi(\Psi(\theta)) $$ where they consider the estimation of the transform Ψ(θ) by d, inversely weighted by the marginal prior on this transform. In the special case of the identity transform, this loss function leads to the MLE as the Bayes estimator. In the general case, the Bayes estimator is the maximum profile likelihood estimator (LRSE). However, this loss function does not generalise to countably infinite (and obviously continuous) parameter spaces and in such settings the authors can only provide LRSEs as limits of Bayes procedures. The loss function adopted in the countable case is for instance $$ \mathrm{L}(\theta,d) = \mathbb{I}\{\Psi(\theta) \ne d\} / \max\{\eta,\pi_\Psi(\Psi(\theta))\} $$ with the bound decreasing to zero. In the continuous case, the indicator does not work any longer, thus the choice made by the authors is to discretise the space Ψ(Θ) by a specific choice of a partition of balls whose diameters λ go to zero. In the spirit of Druihlet and Marin, this choice depends on a metric (and further regularity conditions). Furthermore, the LRSE itself $$ \max_{\psi}\pi_\psi(\psi|x)/\pi_\psi(\theta) $$ does depend on the version chosen for the densities (if not on the dominating measure), unless one imposes everywhere the Bayes equality $$ \pi_{\psi}(\psi|x)/\pi_\psi(\theta)=f(x|\psi)/m(x) $$ everywhere, when $$ f(x|\psi)=\int_{\{\theta;\Psi(\theta)=\psi\}}f(x|\theta)\pi(\theta)\mathrm{d}\theta $$ and $$ m(x)=\int f(x|\theta)\pi(\theta)\mathrm{d}\theta $$ in the spirit of our Savage-Dickey paradox paper . Robert Bassett and Julio Deride arXived a paper in 2016 discussing the position of MAPs within Bayesian decision theory. “…we provide a counterexample to the commonly accepted notion of MAP estimators as a limit of Bayes estimators having 0-1 loss.” The authors mention my book The Bayesian Choice stating this property without further precautions and I completely agree to being careless in this regard! The difficulty stands with the limit of the maximisers being not necessarily the maximiser of the limit. The paper includes an example to this effect, with a prior as above, associated with a sampling distribution that does not depend on the parameter. The sufficient conditions proposed therein are that the posterior density is almost surely proper or quasiconcave. See also an alternate characterisation of MAP estimators by Burger and Lucka as proper Bayes estimators under another type of loss function , albeit a rather artificial one. The authors of this arXived paper start with a distance based on the prior; called the Bregman distance, which may be the quadratic or the entropy distance depending on the prior. Defining a loss function that is a mix of this Bregman distance and of the quadratic distance $$ ||K(\hat u-u)||^2+2D_\pi(\hat u,u) $$ produces the MAP as the Bayes estimator. One might still wonder about the dominating measure but both the loss function and the resulting estimator are clearly dependent on the choice of the dominating measure… (The loss depends on the prior but this is not a drawback per se.)
