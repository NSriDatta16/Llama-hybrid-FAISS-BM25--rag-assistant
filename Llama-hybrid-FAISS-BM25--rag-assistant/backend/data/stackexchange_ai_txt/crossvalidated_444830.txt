[site]: crossvalidated
[post_id]: 444830
[parent_id]: 444821
[tags]: 
Great question - you've hit a great realization about PCA. Even though the important feature in the model is X2 because of the large coefficient, because X2 has such low variance, the PCA is essentially ignoring it by giving it a low weight. PCA is great for capturing the greatest variance among variables, but that is a different task than regression, prediction, or inference. More generally speaking, PCA is a statistical process that has the main task of identifying the linear combination of variables that contain the most variance while being linearly uncorrelated with one another. If X denotes the design/predictor matrix and y the outcome label or target variable, then PCA is a function purely on X , therefore it tells you nothing about X 's relationship to y . So technically, you shouldn't be able to conclude anything about y positive or negative, based on PCA since PCA's internal algorithm is independent of Y. PCA can create denser, more informative features for you to build a regression model around y , but PCA on its own is neither a prediction nor causal inference tool. So the example you provided actually shows you PCA is not optimal neither for causal inference nor prediction, because it ignores the variable with the highest coefficient, and the weights provided by PCA do not give information about the direction of coefficient. If your equation were y = -3 $X_1$ + 100 $X_2$ and everything else stayed the same, then the PCA would still assign the largest weight to $X_1$ , a positive weight value, even though the relationship between $X_1$ and y is negative.
