[site]: crossvalidated
[post_id]: 584551
[parent_id]: 185275
[tags]: 
I disagree with the Minitab blog and many of the common criticisms of $R^2$ . After all, $R^2$ is just a function of the sum of squared residuals: $SSRes = \sum_{i=1}^n\big(y_i - \hat y_i\big)^2$ . $$ R^2 = 1-\dfrac{SSRes}{\sum_{i=1}^n\big(y_i - \bar y\big)^2} $$ This equation is equivalent to other common definitions of $R^2$ , such the the squared correlation between $X$ and $Y$ in simple linear OLS regression and the squared correlation between true and predicted values in both simple and multiple OLS linear regression. Consequently, any criticism of $R^2$ is also a criticism of the sum of squared residuals and the mean squared residual, $MSRes = \dfrac{SSRes}{n}$ , which often gets called the mean squared error or $MSE$ . A major (and valid) criticism of all of these metrics is that they can be driven to be perfect by overfitting to the data. If we hit every $y_i$ point, then every residual is zero, the $SSRes$ is zero, and the $R^2$ is one. Consequently, we want to have some way to penalize the model for overfitting. There are two main avenues for doing this: sticking to in-sample (training) data and testing on some holdout (test or validation data) data. To penalize the model for perhaps overfitting, a common in-sample approach is to tweak $R^2$ and use adjusted $R^2$ . $$R^2_{adj}=1 - \dfrac{\dfrac{SSRes}{n-p}} {\dfrac{\sum_{i=1}^n\big(y_i - \bar y\big)^2}{n-1}}$$ . Here, $p$ is the number of parameters in the regression (including the intercept). An alternative to using an in-sample metric like $R^2_{adj}$ is to keep some data that the model has not seen and test out the model on this holdout data. If the model has overfitted, we expect poor performance on the holdout data. The usual metrics applied to holdout data would work well here. Beyond just considering functions of squared residuals, we might be interested in absolute residuals or percent deviation between truth and prediction. A more advanced variant of out-of-sample validation, beyond the scope of this question, uses bootstrapping to estimate by how much you have overfit. I briefly describe the method here. But you had asked about nonlinear models. Analogues to $R^2$ for nonlinear models are hard to determine, as the degrees of freedom used in place of the $n-p$ term are not as clear as in linear regression. Consequently, using out-of-sample checks (or bootstrap validation) might be the way to go. Many in-sample metrics can be calculated on out-of-sample data. I will list a few below along with pros and cons. $SSRes$ Pros : Easy to calculate Cons : Hard to interpret, since it can grow large just by having many observations $MSE$ Pros : Easy to calculate, related to the variance of the error term Cons : The relationship to variance can range from unhelpful to downright misleading if the error is not Gaussian or does not have a constant variance; the units are squared $RMSE: \text{Root Mean Squared Error}$ (This is just the square root of the MSE.) Pros : Related to the standard deviation of the error term; easy to calculate; in the same units of $y$ Cons : The relationship to standard deviation can range from unhelpful to downright misleading if the error is not Gaussian or does not have a constant standard deviation $R^2$ Pros : Related to comparing your predictions to the predictions of a baseline model Cons : Out-of-sample (and even in-sample when a regression is fit by a method other than least squares), $R^2$ lacks its usual "proportion of variance explained" interpretation; it is easy to think in term of letter grades in school where $R^2=0.6$ is a $\text{D}$ that makes us sad, even though such a value might be spectacular performance ( For reasons I discuss in detail here, I disagree with the exact implementation of out-of-sample $R^2$ in the common Python machine learning package sklearn . That implementation compares your performance to a model that always guesses the out-of-sample mean, which is supposed to be a model that you cannot access (since the out-of-sample data are not for training).) $MAPE: \text{Mean Absolute Percentage Error}$ Pros : Handles data on different scales, where missing by $5$ might be a big deal when the true value is $10$ but less of a big deal when the true value is a billion Cons : Overestimates and underestimates are not penalized equally; you have to divide by zero if a true value is zero; many others, as described on the Wikipedia article on MAPE , though the link also mentions some alternatives
