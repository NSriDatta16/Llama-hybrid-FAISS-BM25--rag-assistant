[site]: crossvalidated
[post_id]: 507601
[parent_id]: 507591
[tags]: 
The shape of LSTM inputs needs to be a 3D tensor of dimensions, [batch_size, time_steps, num_features]. Your response variable can either be a separate Numpy array of shape [batch_size, time_steps, 1] or it can be included with the feature; you just need to split it off when passing in training data. It is worth noting, batch size defaults to 32 in Keras/TF. On the topic of changing the number of nodes in the LSTM layer, I don't see why it would help. Passing in a single observation of a single variable will have the same effect on each node, each node would not weigh the same input differently making the number of them irrelevant. When you BPTT each node would affect the loss the same as they are all replicates. Think of the acyclic graph of layers here, the input gets sent to the same four LSTM nodes, and then condensed back into a single node. I don't think it's a shaping error but may have something to do with the "return_sequences" parameter of the LSTM layer. Look further into the functionality of this, and see if you are using it correctly, it defaults to "False". TensorFlow has very useful blog post titled "Time series forecasting" which reviews data preprocessing for LSTM networks, and the LSTM documentation is also useful.
