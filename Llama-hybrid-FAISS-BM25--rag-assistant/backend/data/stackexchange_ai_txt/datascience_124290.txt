[site]: datascience
[post_id]: 124290
[parent_id]: 
[tags]: 
Interpreting large discrepancies between Specificities & the # of Extraneous Variable Models selected by a variable selection algorithm

I am going to preface my question by saying that this problem of interpretation I have run into is in the context of me doing my part as a collaborator on a statistical learning paper for the first time. Here it the link to the GitHub Repository for this project. My role is to run the several benchmark variable selection algorithms on a large set of synthetic datasets in R, calculate their performance metrics, and analyze and interpret those performance metrics. I am currently interpreting the performance metrics of the 1st benchmark, Lasso, and they exhibit two things which I don't think should be possible at the same time. They have an extremely high mean Specificity of 0.867, or equivalently, an extremely low False Positive Rate of 0.133, but they also have a very large number of overall models selected which are categorized as Extraneous Variable Models (221,269 of the 260,000 total models fit on the 260k synthetic datasets, so 85.1% of them), where an extraneous variable model is a regression equation with all of the true positives included and at least one false positive included in it as well. I did not assume that the False Positive Rate would equal this percentage of Extraneous Variable Models, but, I did not expect them to differ to such an extent. Just to make the problem as clear as possible, here is a screenshot of the Workbook I printed the performance metrics out to from R and store them in: The way I wrote the part of the R code which counts which models are overspecified (extraneous variable models), underspecified (omitted variable models), and correctly specified, by using the TPR, TNR, FPR, and/or FNR was done in a way in that makes the concurrence of these two metrics puzzling: # Number of Underspecified Regression Specifications Selected by LASSO N_Under = sum( (TPR 0) ) The only possibility I can think of is that almost all of the Extraneous Variable Models selected by Lasso only have 1 extra variable included, which keeps the average false positive rate low despite the percentage of models with at least one false positive is not low. p.s. Here is the code before the code snippet included that shows how the TPR, TNR, FPR, and FNR are created: ### Count up how many Variables Selected match the true ### structural equation variables for that dataset in order ### to measure LASSO's performance. # all of the "Positives", i.e. all the Structural Regressors glmnet_NPs Also, Structural_Variables, Nonstructural_Variables, Variables.Selected, and Variables.Not.Selected are all lists that look like this: > head(Structural_Variables, n = 4) [[1]] [1] "X8" "X10" "X23" [[2]] [1] "X12" "X18" "X23" [[3]] [1] "X11" "X13" "X16" [[4]] [1] "X17" "X19" "X24"
