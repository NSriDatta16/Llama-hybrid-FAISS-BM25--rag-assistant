[site]: datascience
[post_id]: 25311
[parent_id]: 24276
[tags]: 
Let's sum up what you are doing: You intend to apply a Neural Network on a dataset of X samples with 1,503 features to get the most accurate prediction of price (i.e. continuous variable). You are worrying that your engineered features may only contribute marginally to the performance. Your concern may be unnecessary If your overall goal is to get the most accurate prediction, I believe dimensionality reduction such as PCA (as mentioned by OP) is not the way to go here. With every feature you create you may explain a fraction of the variance, even if they are highly correlated. Worst case is, that it is useless. If your network is complex enough, i.e. your architecture can ensure that relationships between the variables can be accounted for completely, you will catch everything that's there and your chosen algorithm and setup is capable of. Any variable that's not important will not play a role in your prediction as weights will simply be (close to) zero. However, if you follow this way, ensure that you have a robust method to measure whether you are overfitting or not, such as K-fold cross validation. You may also want to look into regularization techniques suchas L1, L2 and DropOut (neural networks).
