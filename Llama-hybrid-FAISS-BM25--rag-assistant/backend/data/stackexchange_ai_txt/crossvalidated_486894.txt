[site]: crossvalidated
[post_id]: 486894
[parent_id]: 485553
[tags]: 
HuggingFace's Tokenizers are just tokenizers, i.e., they do not make any embeddings. The encode_plus method only returns one-hot vector, so need to train embeddings on your own. (Either explicitly using an embeddings layer or implicitly in the first projection matrix of your model.) Even with HuggingFace's Tokenizers, you can create a vocabulary as large as you want. tokenizer = ByteLevelBPETokenizer() tokenizer.train(["wiki.test.raw"], vocab_size=20000) This snippet will create a byte-level BPE-based vocabulary with 2000 entries, however, you need to estimate the vocabulary parameters on some training data. Machine translation and pre-trained contextual representations typically use vocabularies of tens of thousands of units, so that infrequent words get split into smaller units (using e.g., the WordPiece or the BPE algorithm) and the following layers will learn how to treat them together. This ensures that there are no out-of-vocabulary tokens and all vocabulary units get updated reasonably frequently during training. On the other hand, wod2vec is able to learn embeddings for many words. The models trained with word2vec will probably generalize for words that are in the embeddings table but were never seen during the task-specific training. The disadvantage is that you cannot easily finetune the embeddings for your task, because it is very unlikely that your task-specific training data will contain all 3M unique word forms, so you can update them accordingly.
