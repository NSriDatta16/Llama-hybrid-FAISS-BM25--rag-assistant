[site]: crossvalidated
[post_id]: 483467
[parent_id]: 483465
[tags]: 
Let's handle 2. first. As you guessed, the logit transformation of $\pi$ is designed so that the regression formula has no restriction on its values; any value will be mapped into $(0,1)$ . The same is true for the log transformation of $\lambda$ : $\lambda$ must be positive, and using log transformation allows the regression formula to take any value, positive or negative. The log part of both transformations also means we get a multiplicative model rather than an additive, which often makes more sense for counts and proportions. And, on top of all that, there are mathematical reasons that these transformations for these particular distributions lead to slightly tidier computation and are the defaults, though that shouldn't be very important reason. Now for the orthogonal functions. These aren't saying $f_1$ is orthogonal to $f_2$ ; that's up to the data to decide. They are saying that $f_1$ is a quadratic polynomial in $x^{(1)}$ , and that it's implemented as a weighted sum of orthogonal terms rather than a weighted sum of $x$ , $x^2$ . What the orthogonal polynomials actually are depends on the data, but let's pretend the data are evenly spaced on $[-1,1]$ and they're the Chebyshev polynomials $T_0(x)=1,\, T_1(x)=x,\, T_2(x)=2x^2-1,\, T_3(x)=4x^3-3x$ . If we were just doing maximum likelihood this wouldn't matter at all. Suppose the ML estimate based on the powers of $x$ was $-0.1+2.7x-3x^2+4.5x^3$ . We can rewrite this in terms of the orthogonal polynomials: clearly the coefficient of $T_3$ has to be 4.5/4 to make the $x^3$ match, and the rest will take calculation. It turns out to be $-1.6T_0+6.075T_1-1.5T_2+1.125T_3$ . These are the same polynomial , it's just a different way of writing the same model, and in this case (and nearly always with modern computers) the collinearity isn't anywhere near strong enough to cause numerical rounding problems. With Bayesian inference, though, there's the question of priors. It makes more sense to put independent priors ( $\alpha_j$ and $\beta_k$ in the paper) on the coefficients of orthogonal polynomials than to put independent priors on the coefficients of $x$ , $x^2$ , $x^3$ . So, my assumption is that the orthogonal polynomials were chosen so that the relatively flat ( $N(0,10^2)$ ) independent priors on their coefficients made sense.
