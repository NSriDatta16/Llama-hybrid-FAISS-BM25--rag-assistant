[site]: datascience
[post_id]: 72436
[parent_id]: 47966
[tags]: 
I implemented various architectures for transfer learning and observed that models containing BatchNorm layers (e.g. Inception, ResNet, MobileNet) perform a lot worse (~30 % compared to >95 % test accuracy) during evaluation (validation/test) than models without BatchNorm layers (e.g. VGG) on my custom dataset. Furthermore, this problem does not occurr when saving bottleneck features and using them for classification. There are already a few blog entries, forum threads, issues and pull requests on this topic and it turns out that the BatchNorm layer uses not the new dataset's statistics but the original dataset's (ImageNet) statistics when frozen: Assume you are building a Computer Vision model but you donâ€™t have enough data, so you decide to use one of the pre-trained CNNs of Keras and fine-tune it. Unfortunately, by doing so you get no guarantees that the mean and variance of your new dataset inside the BN layers will be similar to the ones of the original dataset. Remember that at the moment, during training your network will always use the mini-batch statistics either the BN layer is frozen or not; also during inference you will use the previously learned statistics of the frozen BN layers. As a result, if you fine-tune the top layers, their weights will be adjusted to the mean/variance of the new dataset. Nevertheless, during inference they will receive data which are scaled differently because the mean/variance of the original dataset will be used. cited from http://blog.datumbox.com/the-batch-normalization-layer-of-keras-is-broken/ What fixed the problem for me, was to freeze all layers and then unfreeze all BatchNormalization layers to make them use the new dataset's statistics instead of the original statistics: # build model input_tensor = Input(shape=train_generator.image_shape) base_model = inception_v3.InceptionV3(input_tensor=input_tensor, include_top=False, weights='imagenet', pooling='avg') x = base_model.output # freeze all layers in the base model base_model.trainable = False # un-freeze the BatchNorm layers for layer in base_model.layers: if "BatchNormalization" in layer.__class__.__name__: layer.trainable = True # add custom layers x = Dense(1024, activation='relu')(x) x = Dropout(0.5)(x) x = Dense(train_generator.num_classes, activation='softmax')(x) # define new model model = Model(inputs=input_tensor, outputs=x) This also explains the difference in performance between training the model with frozen layers and evaluate it with a validation/test set and saving bottleneck features (with model.predict the internal backend flag set_learning_phase is set to 0 ) and training a classifier on the cached bottleneck features. More information here: Pull request to change this behavior (not-accepted): https://github.com/keras-team/keras/pull/9965 Similar thread: https://stackoverflow.com/questions/50364706/massive-overfit-during-resnet50-transfer-learning
