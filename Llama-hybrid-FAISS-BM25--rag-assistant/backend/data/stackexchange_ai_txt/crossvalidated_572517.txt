[site]: crossvalidated
[post_id]: 572517
[parent_id]: 
[tags]: 
Logistic regression with SGD getting low error with high learning rate

The thing is that I am implementing LR with SGD. Here is my cross error function: def crossErr(x,y,w): sz = len(x) error = 0 for i in range(sz): error += np.log( 1 + np.exp(-y[i]*(w.T.dot(x[i])))) return error/sz Where x are the set of features, w the weights and y the labels. Then I have the gradient of the cross error as follows (same parameters as above): def gradLR(x,y,w): sz = len(x) err =0 for i in range(sz): err += (y[i]*x[i])/(1 + np.exp(y[i]*w.T.dot(x[i]))) return -1*err/sz As last this is my SGD_LR function: def sgdRL(X,Y,lr,it): epoch = 0 n_groups = len(X)//16 w1= np.array([ 0 ,0 ,0]) w_old = w1 norm=1 k=0 mX = np.array_split(X,n_groups) mY = np.array_split(Y,n_groups) while k 0.01: np.random.seed(k) p = np.random.permutation(len(mX)) w1 = w_old for o in range(n_groups): index= p[o] w_new = w_old - lr*gradLR(mX[index],mY[index],w_old) w_old = w_new norm = np.linalg.norm(w1 - w_old) epoch+=1 k+=1 return w_new,epoch Where X,Y are the data set, lr is the learning rate, it is the number of epochs. The algorithm split the data set in minibatchs and update the weighs every time it use a minibatch to compute de gradient. Everytime it iterate on all minibatch,I count 1 epoch, and the I shuffle the data set and begin a new epoch. The thing I am doing is very simple: I am genenerate a cloud of 100 points and classify them with respect to some arbitrary line, and no noise is included. Then I train my model with this data set and use a sample with 1000, with the same seed (I mean same distribution) points as a test set. The thing is that if a choose a high learning rate, I obtain a very low error (I have come to use lr=1000) That is counter-intuitive, based on theory. What could be happening? The source code can be found at: https://github.com/Dazckel/Cositas/blob/main/proof.py
