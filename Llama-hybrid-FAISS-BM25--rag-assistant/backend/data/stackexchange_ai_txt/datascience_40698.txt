[site]: datascience
[post_id]: 40698
[parent_id]: 
[tags]: 
Best practice for short sentences in a deep learning network

In a deep learning network (CNN or RNN), we might use word embeddings such as FastText, Glove, etc. to represent the input text. My question is: If I'm working on a data from Twitter, and I have a variety of text lengths, as in the picture below (example): sometime I have a few sentences of a length larger than 150 and the average length of the rest of the sentences is 48. Here, what I noticed in some implementations online that they pad the short sentences with "PAD" word to increase their size to reach the length of the largest sentence (Red spaces in the picture), where this PAD word is filled with a random value. Is it a good practice to do so? If my dataset contains 5000 tweets, and I have about 20 tweets with a length larger than 150 , the random value will affect the classification task with a high ratio since most of the sentences will be padded with a random value.
