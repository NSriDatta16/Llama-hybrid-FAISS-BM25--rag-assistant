[site]: datascience
[post_id]: 112004
[parent_id]: 112001
[tags]: 
Not sure if this is the right place for this question, but the answer is simple so here we go: What it says is that multi-head attention allows the model to attend to information from different subspaces in different places. Hence the 'multi' part. It can look in multiple places at the same time. With a single attention head you can only look at one place at the same time. In order to still provide the single attention head with all information, the different places are averaged. Because this averaging occurs, it prevents the model from looking into multiple places (note that this is intentional). Perhaps a more intuitive example: If you are a government interested in the happiness of your people, and you have a population of say, 20. A method would be to hire 20 happiness inspectors, and assign 1 inspector to each person in your population. However, if you can/want to hire only 1 inspector, they will need to look at some aggregate of the happiness over the whole population to still get an idea of the happiness score. Thus, the 20 scores are averaged, and given to the inspector (let's skip how we get these scores in this example). The inspector sees only the average, and thus knows nothing about individual happiness scores > the averaging prevents - or inhibits - the inspector from seeing the individual scores. (not specifically an expert in the field of attention mechanisms, so forgive me some minor technical mistakes)
