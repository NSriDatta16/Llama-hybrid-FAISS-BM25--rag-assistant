[site]: crossvalidated
[post_id]: 588800
[parent_id]: 350924
[tags]: 
Eventhough this question was posted 4 years ago, I will make a post since it seems that there is a misconception in the comments of the other post. The question "So, the fact that Gamma is the sampling distribution of the MLE estimator, has nothing to do with the fact that we use the InverseGamma as a prior on the model parameter?" From the perspective of jbowman's answer then the answer to the above question seems to be no. But there is actually a deeper explanation. As LeastSquaresWonderer mentions in his question the maximum likelihood estimator of the variance is gamma distributed: \begin{align*} \sigma^2_{MLE} := \sum^N_{i=1} \frac{X_i^2}{N} \sim \Gamma\left(\frac{N}{2},\frac{2 \sigma^2}{N}\right) \ . \end{align*} The missing part is that you are looking at the distribution of $\sigma^2_{MLE}$ , but in Bayesian statistics you are treating the parameters them selfs as the random variables. Thus in the above expression you want to have $\sigma^2$ on the left hand side treating it as a random variable. For a gamma distributed random variable $X \sim \Gamma(k,\theta)$ we have for $c>0$ that $cX \sim \Gamma(k,c\theta)$ in our case we replace $c$ with $\frac{1}{\sigma^2 \sigma^2_{MLE}}$ then we get: \begin{align*} \frac{1}{\sigma^2 \sigma^2_{MLE}} \sigma^2_{MLE} = \frac{1}{\sigma^2} \sim \Gamma\left(\frac{N}{2},\frac{2}{N\sigma^2_{MLE}}\right) \ . \end{align*} Now if a random variable $X \sim \Gamma(k,\theta)$ is gamma distributed then $\frac{1}{X}\sim \Gamma^{-1}(k,1/\theta)$ which is the inverse gamma distribution. Thus we get: \begin{align*} \sigma^2 \sim \Gamma^{-1}\left(\frac{N}{2},\frac{N\sigma^2_{MLE}}{2}\right) \ . \end{align*} This together with the fact that the inverse gamma distribution is a conjugate prior of a normal distribution motivates the use of the inverse gamma as a prior.
