[site]: crossvalidated
[post_id]: 133231
[parent_id]: 133229
[tags]: 
Sounds like you want to do a simulation. In that case you know the population parameter and the proportion of confidence intervals that include that true parameter value is your coverage probability. To quantify the the comment by @HarveyMotulsky: In a given scenario (sample size, number, distribution and correlation of independent/explanatory/right-hand-side/x-variables, strength of effects, marginal variance of the dependent/explained/left-hand-side/y-variable, ...) you would expect about 5% of the replications to fall outside your 95% confidence interval. So if you have 100 replications, you would expect to base your estimate of the coverage on only 5 replications. That is obviously not enough. For 95% confidence intervals I tend to use 20,000 replications; that way I expect to base my estimate of the coverage on 1,000 replications in which the true hypothesis is rejected. Once you have done so for one scenario, you change one of the characteristics (e.g. change the sample size), and do the simulation again. You continue doing that until you have a good idea of under what conditions your test fails and under what conditions your test is fine. You would not expect a simple conclusion like "the confidence interval is right", because any statistic will fail if you use it on extreme enough data. Instead, the purpose for a simulation study should be to find the conditions under which the test performs acceptable, the conditions under which the test will not perform acceptable, and the "grey zone" where the user should be extra careful when using this test.
