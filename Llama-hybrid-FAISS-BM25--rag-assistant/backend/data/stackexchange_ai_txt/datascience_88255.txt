[site]: datascience
[post_id]: 88255
[parent_id]: 88245
[tags]: 
Have a look at the weights of your model at each step and the gradients that are being applied. In many cases the gradients are of order 10^-10 or smaller, meaning that the weights of the model basically do not change at all. The reason for this is that a neural network is sensitive to the scale of the data. It is therefore often good practice to scale your input variables, e.g. on a 0-1 scale. Simply dividing each column in the input by their max value using X_train /= X_train.max(axis=0) allows me to reach 90%+ training accuracy after 100 epochs (depending on the initialization of the weights). You can take the scaling even further by using something like MinMaxScaler or StandardScaler from scikit-learn.
