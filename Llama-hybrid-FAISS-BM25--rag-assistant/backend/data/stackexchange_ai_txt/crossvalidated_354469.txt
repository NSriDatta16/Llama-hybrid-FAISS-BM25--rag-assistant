[site]: crossvalidated
[post_id]: 354469
[parent_id]: 354447
[tags]: 
Traditionally DAG's refer to probabilistic graphical models, specifically used by Bayesian networks (BN's), which have fallen out of fashion (blown out of water) by neural nets. While directionality is clear, causality has additional implications and is a debatable aspect of Bayesian Networks (see statistical indistinguishability of DAG's etc.). Basically you can get identical Bayesian scores for different DAG's while structure learning of BN's. There is also the concept of DAG's for deep learning, see an example here . Some call these DAG networks, where the layers can be arranged as a directed acyclic graph. DAG networks can have a more complex architecture where layers can have inputs from, or outputs to, multiple layers. Finally, anytime you hear anyone having strong claims about DAG's and causality, take whatever they say with a pinch of salt(!).
