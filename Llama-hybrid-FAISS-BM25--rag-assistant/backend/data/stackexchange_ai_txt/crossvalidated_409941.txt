[site]: crossvalidated
[post_id]: 409941
[parent_id]: 409936
[tags]: 
First of all, with RNN you need much less parameters , since you condition only on the current input and the hidden state returned by previous RNN cell. You only need parameters for those signals, not for the all the inputs. This should be obvious if you think of how RNN cells are defined. Quoting myself As described by Andrey Karpathy , the basic recurrent neural network cell is something like $$ h_t = \tanh(W_{hh}h_{t-1} + W_{xh}x_t) $$ so it takes previous hidden state $h_{t-1}$ and current input $x_t$ , to produce hidden state $h_t$ . Notice that $W_{hh}$ and $W_{xh}$ are not indexed by time $t$ , we use the same weights for each timestep. In simplified python code , the forward pass is basically a for-loop: for t in range(timesteps): h[t] = np.tanh(np.dot(Wxh, x[t]) + np.dot(Whh, h[t-1])) Second, because of the above fact, RNNs can take as inputs sequences of arbitrary length . This is impossible for classical architectures using only the dense layers, since they need fixed size of inputs. Notice however, that nowadays we also have other neural network architectures that can deal with inputs of arbitrary length, that don't use RNNs, for example Transformer network (Vaswani et al, 2017) .
