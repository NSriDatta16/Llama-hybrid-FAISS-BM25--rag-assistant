[site]: datascience
[post_id]: 780
[parent_id]: 41
[tags]: 
Some good answers here. I would like to join the discussion by adding the following three notes : The question's emphasis on the volume of data while referring to Big Data is certainly understandable and valid, especially considering the problem of data volume growth outpacing technological capacities' exponential growth per Moore's Law ( http://en.wikipedia.org/wiki/Moore%27s_law ). Having said that, it is important to remember about other aspects of big data concept. Based on Gartner 's definition (emphasis mine - AB): " Big data is high volume , high velocity , and/or high variety information assets that require new forms of processing to enable enhanced decision making, insight discovery and process optimization." (usually referred to as the " 3Vs model "). I mention this, because it forces data scientists and other analysts to look for and use R packages that focus on other than volume aspects of big data (enabled by the richness of enormous R ecosystem ). While existing answers mention some R packages, related to big data, for a more comprehensive coverage , I'd recommend to refer to CRAN Task View "High-Performance and Parallel Computing with R" ( http://cran.r-project.org/web/views/HighPerformanceComputing.html ), in particular, sections "Parallel computing: Hadoop" and "Large memory and out-of-memory data" .
