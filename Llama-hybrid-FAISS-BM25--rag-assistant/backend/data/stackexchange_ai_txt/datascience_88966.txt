[site]: datascience
[post_id]: 88966
[parent_id]: 88936
[tags]: 
The example you cited (using x^2 instead of x) is the idea more popular outside deep learning community, called feature engineering. The trend in neural network modeling is instead to, Play with weights (w) and fine tune them. Not change the input vector (x) but feed it to the network directly. If a single layer neural network is not good enough, add more layers. Introduce non-linearities using activation functions. And in general, not hand-roll features (like x^2) but let neural networks discover such features.
