[site]: crossvalidated
[post_id]: 408086
[parent_id]: 407799
[tags]: 
Random Forest models 'overfit' by definition, however this seldom has an effect on their predictive power. When you are passing a training sample down it's corresponding random forest model, the sample will end up in the exact same terminal nodes it ended up during training. Therefore, the above left plot depicts the deviation of the sample value from the mean values of samples which share a terminal leave node with it. Random Forests are a form of NN model and thus the conventional definition of overfitting, i.e. performance of the model on train vs. test samples, does not really make sense, as the later will almost always be lower. Overfitting is generally not an issue in context of Random Forests and increasing the node size, pruning the trees, etc. might actually hurt the model, as it can increase the bias. Selecting meaningful model features is another topic but again, Random Forests are relatively robust against noisy features, as the uninformative ones will simply be not selected during a split. There exist many ways of feature selection (e.g. forward/backward selection based on certain variable importance measures) but those are mostly used for model interpretation purpose.
