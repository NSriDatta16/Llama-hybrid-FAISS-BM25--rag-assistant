[site]: crossvalidated
[post_id]: 628802
[parent_id]: 
[tags]: 
Applying Xavier initialization to model causes model to not train properly and therefore output meaningless text

I have a Transformer implementation that I'm working on. The Transformer model is the original encoder-decoder sequence2sequence model introduced in the 2017 paper. I wasn't originally using any form of initialization and decided to apply Xavier initialization. What I found was that the model wasn't training properly and was outputting meaningless text. The gradients also seem rather small and then collapse to zero, as shown in the image below (top is with Xavier, bottom is without): The way that I applied initialization is as follows: def xavier_init_model(model): for param in model.parameters(): try: nn.init.xavier_uniform_(param) # Also tried `xavier_normal_` except ValueError: pass model = Model() model.apply(xavier_init_model) Hyperparameters and regularization were kept the same, so I'm wondering if initialized parameters require something different? I followed the learning rate schedule used in the original paper (a form of exponential decay with warmup) and also used a Dropout rate of 0.1. If anyone knows any resources that might point to initialized models requiring different regularization or hyperparameter settings that I'd appreciate that too.
