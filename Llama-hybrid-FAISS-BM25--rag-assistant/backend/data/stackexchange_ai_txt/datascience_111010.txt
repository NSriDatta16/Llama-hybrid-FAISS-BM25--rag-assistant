[site]: datascience
[post_id]: 111010
[parent_id]: 110980
[tags]: 
Even if both expressions are often considered the same in practice, it is crucial to draw a line between "reuse" and "fine-tune". We reuse a model to keep some of its inner architecture or mechanism for a different application than the original one. For example, we can reuse a GPT2 model initialy based on english to adapt it to another language like chinese, which means deep changes from the initial model to the new one. On the other hand, we fine tune a model to improve an already existing application or a slight different one, by changing specific hyperparameters or use better algorithms (for instance, using AdamW instead of Gradient Descent). There are plenty of methods in NLP to improve existing models, that's why we can consider it as a different area. It could be regarded as a semantic issue, but I think it is interesting not to be confused between both expressions.
