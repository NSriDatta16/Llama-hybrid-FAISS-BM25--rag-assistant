[site]: crossvalidated
[post_id]: 304472
[parent_id]: 
[tags]: 
What should regularization loss look like?

New to ML and Deep Learning. I am trying my hands on some image classification using resnet50. Here are some of the graphs I see on tensorboard: As you can see, my total loss is mostly being influenced by the regularization loss. Is this normal? How big should the regularization loss be compared to the cross entropy loss? I know that the cross-entropy loss should ideally come down eventually during training. What about the regularization loss? It is okay for it to shoot up? What would an ideal regularization loss graph look like? I tried the training with various parameters, and for a lot more steps as well. I can see the cross entropy loss coming down slowly, but the regularization loss is not, and hence my total loss is mostly going up.
