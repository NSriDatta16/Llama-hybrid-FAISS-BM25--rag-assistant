[site]: datascience
[post_id]: 97661
[parent_id]: 78055
[tags]: 
I am a bit confused about your question because word2vec basically outputs a word vector/embedding for each word which is completely independent of the context of that word in the document. Word2vec is a very simple and nice embedding technique Google came up with that uses a neural network to make word embeddings by using either a Skip-gram or Continuous Bag of Words approach (CBOW). If you would like more details you should read their paper. To overcome this issue with word2vec - that its embeddings do not learn context - there are two approaches. One, a model named ELMO (it uses LSTMs) and the other BERT (uses Transformers). These are the state of the art now as they can learn context and long-range dependencies. I don't have experience with R so I do not know if these can be implemented in R or if there is a package for them, but they work with most deep learning packages like PyTorch or TensorFlow.
