[site]: crossvalidated
[post_id]: 211544
[parent_id]: 
[tags]: 
Which, if any, machine learning algorithms are accepted as being a good tradeoff between explainability and prediction?

Machine learning texts describing algorithms such as gradient boosting machines or neural networks often comment that these models are good at prediction, but this comes at the price of a loss of explainability or interpretability. Conversely, single decision trees and classical regression models are labelled as good at explanation, but giving a (relatively) poor prediction accuracy compared to more sophisticated models such as random forests or SVMs. Are there machine learning models commonly accepted as representing a good tradeoff between the two? Is there are any literature enumerating the characteristics of algorithms which allow them to be explainable? EDIT: To make the notion of 'explainable' clearer, in interpreting a regression model, it is possible to make statements to the effect of 'an increase of x units of predictor i will lead to a y increase in the dependent variable'. By contrast, it is difficult to make a statement like that based on the output of a neural network. The first scenario is an example of a model being 'explainable'. This claim is made of linear regression in both Hastie, Tibshirani, Friedman's Elements of Statistical Learning (2008) and Kuhn, Johson's Applied Predictive Modelling (2013), although on review the actual word used 'interpretable'. The tradeoff is that a regression model has far stricter assumptions than a neural network. For example, to give an accurate prediction, there must be little correlation between predictor variables, and the relationships between predictor variables and the dependent variables must be linear. If the data doesn't meet these (and other requirements), the predictions will not be as accurate. In contrast, there are models such as neural networks which require fewer assumptions (e.g. they may tolerate non-linearity far better), and achieve a better prediction accuracy, at the cost of losing 'explainability' as described above. An example is given in the Compressive Strength Case Study in Kuhn, Johnson's Applied Predictive Modelling (2013), where the models the text had earlier labelled as most explainable - CART, linear regression - achieved the worst prediction accuracy on the data. A second set of data was also discussed in the text - and again neither C&RT nor linear regression were considered 'high performers' compared to, for example, boosted trees.
