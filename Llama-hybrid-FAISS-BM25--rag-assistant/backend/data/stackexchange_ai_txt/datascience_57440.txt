[site]: datascience
[post_id]: 57440
[parent_id]: 55503
[tags]: 
To me, it would make intuitive sense to visualize/analyze the data before imputing the missing values, as imputation will skew distributions and may lead to false assumptions about the real data before imputation. I think there's a crucial detail to answer here which might deepen your analysis a bit. How are you planning to imput the missing values? That might well depend on two things: the actual distribution of the values. Depending on the distribution, different techniques may be appropriate for each case: most common value, mean, some machine learning algorithm to predict the missing values based on other data... But to decide which method to use, you have to first understand the raw data . how many values are missing, which you covered later. On the other hand, I can see a case for not imputing values if the % of missing values is high enough to affect the data's distribution. That's correct. Think of it this way: if the percentage of missing values is too high, then you have no basis on which to accurately fill the missing values. You might just be inventing too much data. How will that play later? Are you adding something valuable, or are you putting in too much invention on your own just to save that one attribute? Hope this helps!
