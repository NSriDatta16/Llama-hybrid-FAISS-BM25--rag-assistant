[site]: crossvalidated
[post_id]: 137271
[parent_id]: 137244
[tags]: 
Although an exact probability cannot be computed (except in special circumstances with $n \le 2$), it can be numerically calculated quickly to high accuracy. Despite this limitation, it can be proven rigorously that the runner with the greatest standard deviation has the greatest chance of winning. The figure depicts the situation and shows why this result is intuitively obvious: Probability densities for the times of five runners are shown. All are continuous and symmetric about a common mean $\mu$. (Scaled Beta densities were used to ensure all times are positive.) One density, drawn in darker blue, has a much greater spread. The visible portion in its left tail represents times that no other runner can usually match. Because that left tail, with its relatively large area, represents appreciable probability, the runner with this density has the greatest chance of winning the race. (They also have the greatest chance of coming in last!) These results are proven for more than just Normal distributions: the methods presented here apply equally well to distributions that are symmetric and continuous. (This will be of interest to anyone who objects to using Normal distributions to model running times.) When these assumptions are violated it is possible that the runner with greatest standard deviation might not have the greatest chance of winning (I leave construction of counterexamples to interested readers), but we can still prove under milder assumptions that the runner with greatest SD will have the best chance of winning provided that SD is sufficiently large. The figure also suggests that the same results could be obtained by considering one-sided analogs of standard deviation (the so-called "semivariance"), which measure the dispersion of a distribution to one side only. A runner with great dispersion to the left (towards better times) ought to have a greater chance of winning, regardless of what happens in the rest of the distribution. These considerations help us appreciate how the property of being the best (in a group) differs from other properties such as averages. Let $X_1, \ldots, X_n$ be random variables representing the runners' times. The question assumes they are independent and Normally distributed with common mean $\mu$. (Although this is literally an impossible model, because it posits positive probabilities for negative times, it can still be a reasonable approximation to reality provided the standard deviations are substantially smaller than $\mu$.) In order to carry out the following argument, retain the supposition of independence but otherwise assume the distributions of the $X_i$ are given by $F_i$ and that these distributional laws can be anything. For convenience, also assume the distribution $F_n$ is continuous with density $f_n$. Later, as needed, we may apply additional assumptions provided they include the case of Normal distributions. For any $y$ and infinitesimal $dy$, the chance that the last runner has a time in the interval $(y-dy, y]$ and is the fastest runner is obtained by multiplying all relevant probabilities (because all times are independent): $$\Pr(X_n \in (y-dy, y], X_1 \gt y, \ldots, X_{n-1} \gt y) = f_n(y)dy(1-F_{1}(y))\cdots(1-F_{n-1}(y)).$$ Integrating over all these mutually exclusive possibilities yields $$\Pr(X_n \le \min(X_1, X_2, \ldots, X_{n-1})) = \int_{\mathbb R} f_n(y)(1-F_1(y))\cdots(1-F_{n-1}(y)) dy.$$ For Normal distributions, this integral cannot be evaluated in closed form when $n\gt 2$: it needs numerical evaluation. This figure plots the integrand for each of five runners having standard deviations in the ratio 1:2:3:4:5. The larger the SD, the more the function is shifted to the left--and the greater its area becomes. The areas are approximately 8:14:21:26:31%. In particular, the runner with the largest SD has a 31% chance of winning. Although a closed form cannot be found, we can still draw solid conclusions and prove that the runner with the largest SD is most likely to win. We need to study what happens as the standard deviation of one of the distributions, say $F_n$, changes. When the random variable $X_n$ is rescaled by $\sigma \gt 0$ around its mean, its SD is multiplied by $\sigma$ and $f_n(y)dy$ will change to $f_n(y/\sigma)dy/\sigma$. Making the change of variable $y=x\sigma$ in the integral gives an expression for the chance of runner $n$ winning, as a function of $\sigma$: $$\phi(\sigma) = \int_{\mathbb R} f_n(y)(1-F_1(y\sigma))\cdots(1-F_{n-1}(y\sigma)) dy.$$ Suppose now that the medians of all $n$ distributions are equal and that all the distributions are symmetric and continuous, with densities $f_i$. (This certainly is the case under the conditions of the question, because a Normal median is its mean.) By a simple (locational) change of variable we may assume this common median is $0$; the symmetry means $f_n(y) = f_n(-y)$ and $1 - F_j(-y) = F_j(y)$ for all $y$. These relationships enable us to combine the integral over $(-\infty, 0]$ with the integral over $(0,\infty)$ to give $$\phi(\sigma) = \int_0^{\infty} f_n(y)\left(\prod_{j=1}^{n-1}\left(1-F_j(y\sigma)\right)+\prod_{j=1}^{n-1}F_j(y\sigma)\right) dy.$$ The function $\phi$ is differentiable. Its derivative, obtained by differentiating the integrand, is a sum of integrals where each term is of the form $$y f_n(y) f_i(y\sigma)\left(\prod_{j\ne i}^{n-1}F_j(y\sigma) - \prod_{j\ne i}^{n-1}(1-F_j(y\sigma))\right)$$ for $i=1, 2, \ldots, n-1$. The assumptions we made about the distributions were designed to assure that $F_j(x) \ge 1-F_j(x)$ for $x\ge 0$. Thus, since $x=y\sigma\ge 0$, each term in the left product exceeds its corresponding term in the right product, implying the difference of products is nonnegative. The other factors $y f_n(y) f_i(y\sigma)$ are clearly nonnegative because densities cannot be negative and $y\ge 0$. We may conclude that $\phi^\prime(\sigma) \ge 0$ for $\sigma \ge 0$, proving that the chance that player $n$ wins increases with the standard deviation of $X_n$. This is enough to prove that runner $n$ will win provided the standard deviation of $X_n$ is sufficiently large. This is not quite satisfactory, because a large SD could result in a physically unrealistic model (where negative winning times have appreciable chances). But suppose all the distributions have identical shapes apart from their standard deviations . In this case, when they all have the same SD, the $X_i$ are independent and identically distributed: nobody can have a greater or lesser chance of winning than anyone else, so all chances are equal (to $1/n$). Start by setting all distributions to that of runner $n$. Now gradually decrease the SDs of all other runners, one at a time. As this occurs, the chance that $n$ wins cannot decrease, while the chances of all the other runners have decreased. Consequently, $n$ has the greatest chances of winning, QED .
