[site]: crossvalidated
[post_id]: 292237
[parent_id]: 292221
[tags]: 
Let's think the problem as follows. Say $X=(X_1,X_2,..X_n)$ and $Y$ is a binary variable standing for the population : $Y=0$ means first population, $Y=1$ means second population. The null hypothesis can be expressed in several equivalent ways: $H_0$: the populations are the same $H_0$: the distribution of $X$ given $Y=0$ is the same as the distribution of $X$ given $Y=1$ $H_0$: $X$ and $Y$ are independent $H_0$: for any function $f$ into $\{0,1\}$, $f(X)$ and $Y$ are independent I don't know much about random forests, but they may be thought as an all purpose predictor that avoids over-fitting. If we idealize them quite a bit: it is something capable of detecting any kind of relationship between $Y$ and any kind of features $X$ without over-fitting. It is possible to try something based on this. Split the original dataset into a training set and a test set. Then: train a random forest $f$ that predicts $Y$ from $X$ on the training set. make a simple chi-squared independence test (with risk $\alpha$) between $f(X)$ and $Y$ on the test set This test is quite conservative. If the random forest is a poor method, at worst outputting a dumb $f(X)$, then it will reject $H_0$ with a probability less than $\alpha$ anyway (when $H_0$ is true). The over-fitting would not even be a problem since we use a test and a training set. However, the power of the test directly depends on the intelligence of the random forest method (or any predictor used). Note that you can use several possible predictors: like plain old logistic regression first, then logistic regression with some cross features, then a few decision trees, then a random forest... But if you do so you should adjust $\alpha$ to the number of tests to avoid "false discoveries". See: Alpha adjustment for multiple testing
