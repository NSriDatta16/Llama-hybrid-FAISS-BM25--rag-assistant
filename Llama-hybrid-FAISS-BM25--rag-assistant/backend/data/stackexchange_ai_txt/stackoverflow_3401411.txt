[site]: stackoverflow
[post_id]: 3401411
[parent_id]: 3400890
[tags]: 
First to your actual question - "C" has no debugging options to do with I/O performance or any other kind of performance. Your IDE, debugger, or OS might, although I'm afraid I don't know the details of any. Stupid question - do all the loops produce the same amount of output? Maybe the first 500 are small. It could be that 500 loops is how long it takes to fill the disk write cache (at one or more levels - process, OS, hardware), and after that the program is I/O bound. Can't really say whether that's likely, without knowing the amounts of data involved. Try writing 1GB of data to a file, and time it, to get a very rough idea of what sustained rate is plausible. If 0.07 seconds per pair, times the amount of data per pair, works out faster than that rate, then your initial fast rate is a one-time-only special offer: the disk will have to catch up sooner or later. Beyond that, think more about what your output is actually doing , that you don't detail. Writing in a straight line? Seeking back and forth? Inserting records into an ordered array on disk, so that each write has to move on average 50% of the data written so far? Different access patterns obviously have very different expected performance over time. I focus on output rather than input, on the assumption that the read cache is useless, so that your read speeds will be fairly consistent throughout. That's not necessarily the case, but if the computer can't predict your access patterns then it's a pretty good approximation. Even so, 300000 * 5 seconds is over 400 hours. This is enough time for any mere mortal computer to write its entire hard disk many times over. So you'd have to be doing something quite odd for raw write speed to be all there is to it.
