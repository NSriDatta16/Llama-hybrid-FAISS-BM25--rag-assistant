[site]: crossvalidated
[post_id]: 523097
[parent_id]: 
[tags]: 
Normalization of possibly not fully representative data

I am trying to train a classification RNN model on a sequence of table medical data, but I stuck with the normalization problem. I realized that I cannot simply use MinMaxScaler, because of 3 problems: outliers, but I could fight them or use RobustScaler instead. I am not sure that some features in my dataset include all possible ranges. Like I have max(feature_A) == 10, but with the data update, it could become 20. And if I'll preprocess data the same way I will get bad prediction results. Some features do not have a limit at all and will only increase with time, like how many years patients were treated, for example. I could suppose that this value is !>100years, for example, but if my mean value is 10 years, it will squeeze feature values a lot. My dataset is pretty large, like millions of observations, thus there is a pretty good chance that it is representative, though. But I am concerned with the small-time range, like all those observations are for the 2 years only, thus, some feature values (like how many years patients were treated) could still grow their bounds. How should I handle this? My concerns example: import pandas as pd from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler() #### like, initial state df1 = pd.DataFrame({'A': [1, 2, 3, 4, 5], 'B': [10, 40, 60, 80, 100]}) """ output: A B 0 1 10 1 2 40 2 3 60 3 4 80 4 5 100 """ scaler.fit_transform(df1) """ output: array([[0. , 0. ], [0.25 , 0.33333333], [0.5 , 0.55555556], [0.75 , 0.77777778], [1. , 1. ]]) """ #### new data arrived, while preprocessing is the same df2 = pd.DataFrame({'A': [1, 2, 3, 4, 5, 10, 10], 'B': [10, 40, 60, 80, 100, 120, 140]}) """ output: A B 0 1 10 1 2 40 2 3 60 3 4 80 4 5 100 5 10 120 6 10 140 """ # now 5 in "A" scaled to 0.4 instead of 1, same in "B" scaler.fit_transform(df2) """ output: array([[0. , 0. ], [0.11111111, 0.23076923], [0.22222222, 0.38461538], [0.33333333, 0.53846154], [0.44444444, 0.69230769], [1. , 0.84615385], [1. , 1. ]]) """ PS: I've duplicated this question in different communities (question in ai got most of views): https://datascience.stackexchange.com/questions/94095/normalization-of-possibly-not-fully-representative-data https://ai.stackexchange.com/questions/27678/normalization-of-possibly-not-fully-representative-data
