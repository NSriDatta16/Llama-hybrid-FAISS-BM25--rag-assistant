[site]: datascience
[post_id]: 53201
[parent_id]: 
[tags]: 
Overfitting CNN models

I tried to develop a number of CNN architectures to train on a 1000-point subset of the "cat-dog" Kaggle training set (meaning, by the way, that all 1000 data points were labeled). I used a 700-150-150 train-validate-test split, and used the following code on an Xception architecture: pre_xception_model = keras.applications.Xception(include_top=False, weights='imagenet', input_tensor=None, input_shape=(224,224,3), pooling=None, classes=2) for layer in pre_xception_model.layers: layer.trainable = False dropout = Dropout(0.5)(pre_xception_model.output) flatten = Flatten()(dropout) output = Dense(2, activation='softmax')(flatten) xception_model = Model(pre_xception_model.input, output) xception_model.compile(Adam(lr=.0001, decay=1e-6), loss='categorical_crossentropy', metrics=['accuracy']) aug = ImageDataGenerator(rotation_range=20, zoom_range=0.15, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.15, horizontal_flip=True, fill_mode="nearest") batches = 20 xception_model.fit_generator(aug.flow(X_train, y_train, batch_size = batches), steps_per_epoch = len(X_train) // batches, validation_data = (X_valid, y_valid), validation_steps = 4, epochs = 10, verbose = 1) xception_prob = xception_model.predict(X_test, verbose=1) xception_predict = xception_prob.argmax(axis=-1) cm_xception = confusion_matrix(y_test[:,1], xception_predict) plot_confusion_matrix(cm_xception, cm_plot_labels, title='Confusion Matrix') print(f'\nAccuracy = {(cm_xception[0,0]+cm_xception[1,1])/150}\n') This produced , and the .predict() produced . I'm at a loss as to explain the overfitting; can someone here help me as to where I went wrong on this?
