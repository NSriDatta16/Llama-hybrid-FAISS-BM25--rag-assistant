[site]: crossvalidated
[post_id]: 435162
[parent_id]: 
[tags]: 
Higher accuracies for larger k at cross validations?

I am fitting an artificial neural network with Python's scitkit-learn . The data source is experimental data from my study. Objective is identifying an optimal parameterization, plus I want to track the results for each run (something like a gridsearch, but I want to do it manually). I realized that the randomness of the method introduces some uncertainties that disallow a proper sorting by scores. Parameterization A could achieve slightly higher scores than B, but the next time it turns out the other way around. Consequently I increased k in the kfold cross-validation to obtain more robust results. Now I am confused to find that the overall accuracy increased with higher k, i.e. Root-Mean-Squared-Error is going down. cross_validate(model, X, y, scoring='neg_mean_squared_error', cv=3) I ran that line 5 times an received the following RMSEs (lower = better): --> 0.630, 0.634, 0.633, 0.620, 0.616 (mean: 0.626, std: 0.008) cross_validate(model, X, y, scoring='neg_mean_squared_error', cv=15) --> 0.561, 0.553, 0.568, 0.544, 0.548 (mean: 0.548, std: 0.010) Why is the error going down for the 15fold cross-validation? Why is the standard deviation not going down? Update: I have found another strange thing. I used to use cross_val_score to evaluate my model like so: cross_val_score(model, X_train, y_train, scoring="r2", cv=5) cross_val_score(model, X_test, y_test, scoring="r2", cv=5) This, however, gives significantly worse results than calculating score_results = cross_validate(model, X, y, scoring="r2, cv=5) When using cross_validate instead of cross_val_score the statistics indicate that the training data scores way better than the test data. Plus, the overall accuracy is better. How could that be?
