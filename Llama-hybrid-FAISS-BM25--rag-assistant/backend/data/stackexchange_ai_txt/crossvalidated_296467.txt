[site]: crossvalidated
[post_id]: 296467
[parent_id]: 
[tags]: 
Is there a normalization method that does not require the memory storage of past inputs?

I am attempting to conduct natural language processing on website traffic, specifically trying to use supervised ML to identify possible malicious attacks. However, the problem is that it would take too much storage to use something similar to bag of words, because every time a new request would come in, the normalization method would have to be refreshed (and therefore, all past inputs would have to be stored in memory), and all the representation of all past inputs would have to be adjusted. Is there a method by which a standard normalization method (that does not depend on a comprehensive input base) could be applied to incoming text statements, before being fed into a neural network?
