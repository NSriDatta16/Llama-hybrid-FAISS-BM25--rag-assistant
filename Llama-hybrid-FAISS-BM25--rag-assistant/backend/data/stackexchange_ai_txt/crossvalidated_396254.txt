[site]: crossvalidated
[post_id]: 396254
[parent_id]: 396037
[tags]: 
What binomial distribution describes, is the distribution of "successes" in $n$ Bernoulli trials, each having probability of success $p$ . The most popular handbook example, is that you have a biased coin with probability of tossing heads equal to $p$ , you throw it $n$ times and count the total number of heads tossed. In single coin toss the variance is $p(1-p)$ , if you toss it twice, it is $p(1-p) + p(1-p)$ by the property of variance of uncorrelated variables that $\mathrm{Var}(A+B) = \mathrm{Var}(A) + \mathrm{Var}(B)$ , if you toss it $n$ times, it varies by $\sum_{i=1}^n p(1-p) = np(1-p)$ . Say that you record results of tossing some coin $n$ times, then your friend tosses it $m$ times. You repeat this experiment many times and in each row of your notebook you record the successes by you in one column and your friends results in the second column. In the end you observe that on average you observed $np \pm x$ heads, while your friend $mp \pm y$ heads. If you summed together the results rowwise, on average you would observe $np+mp = (n+m)p$ heads for both of you. How much would the sum vary? It wouldn't be $(n+m)p \pm x$ , neither it wouldn't be $(n+m)p \pm y$ , since in both cases this would mean that the additional tosses cannot make the result more extreme. This wouldn't make sense. Say that you never tossed more then $k$ heads, while your friend never tossed more then $r$ heads, then obviously if you sum the number of heads tossed by both of you, the result can be greater then $k$ or $r$ .
