[site]: crossvalidated
[post_id]: 388049
[parent_id]: 
[tags]: 
One-hot-encoding gives untractable amount of classes

I'm performing regression on the price of bycicles based on their brand, model and submodel. These features are hierarchical: one model belongs only to one brand but one brand can have many models. Similarly, one submodel belongs only to a model but the model can have many submodels. I tried one-hot encoding the combination of [brand-model-submodel] but I get almost 45k classes. Ironically, the resulting matrix is so big that I can't even apply pca because I run out of memory. Many machine learning models also run out of memory when they try to fit the data, so I really need to reduce the dimensionality somehow. I was thinking of there is a way to only branch out a class if the prices are really different from those of their siblings. Other than that, how would you solve it? What alternatives do I have? I'm using python by the way, in case you can point me to concrete resources.
