[site]: crossvalidated
[post_id]: 459145
[parent_id]: 458841
[tags]: 
Transformers require quadratic memory with the length of the text, using too long texts might result in memory issues. E.g., in Huggingface's implementation, most of the models do not accept sequences longer than 512 subwords. Making pre-trained Transformers work efficiently for long texts is an active research area, you can have a look at a paper called DocBERT to have an idea of what people are trying. But it will take some time until there is a nicely packaged open-source solution.
