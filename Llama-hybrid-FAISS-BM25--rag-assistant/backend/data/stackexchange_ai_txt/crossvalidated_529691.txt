[site]: crossvalidated
[post_id]: 529691
[parent_id]: 529669
[tags]: 
Am I correctly understand, that in general case, not only in my task, for instance, accuracy plot depending from num of epochs should monotonously increase, but in the end, when num of epochs will be big, accuracy should stuck at some value? Similarly, for loss plot, but in that case, loss should monotonously decrease, but also should stuck at some value? There is no reason why an accuracy plot should monotonically increase or a loss plot should monotonically decrease. For some machine learning models and optimization strategies this will hold true (for example logistic regression with Newton's Method and appropriate hyperparameters will converge to the global minimum and therefore the loss and accuracy plots will monotonically converge towards static values). But for a neural network using an optimiser with minibatches and/or momentum this will rarely hold true. Additionally, dropout will also make monotonic convergence unlikely as each batch is trained on a different network subset; you're crippling the network to help it generalise and in doing so its loss and accuracy will oscillate. Which model is better? On the one hand, in first plot, there is no overfitting, but train and validation accuracy are not so big, as in second plot. But in the second plot, we have overfitting. Any network will tend towards overfitting given enough epochs, but early stopping can be employed to select the weights from the epoch which has the lowest underfitting and overfitting. Epoch ~65 in the second model appears to have a validation loss similar to the validation loss at the final epoch of the first model. I'd argue that when early stopping is employed both networks perform similarly. The first model is more stable, which is desirable, but the second network is able to more closely fit the training set, suggesting it has a larger capacity to learn which is also desirable. One school of throught for designing good networks is to design a network which can overfit the training data and then you tame the overfitting using regularization techniques and early stopping. It's dangerous for someone to claim one of those two networks is better than the other considering they both display different properties. I'd recommend exploring regularization in the second network. I heard, that depending on after which layer (conv or linear) I will put the dropout layer, I should assign different values for p. Moreover, this value should increase with num of linear layer. Is it true? What is the rule? Where I can read about it? There is no 'ideal' value for $p$ . Every network is different and different complex architectures can benefit from different amounts and types of dropout. Experiment with various values of $p$ and tune it based on validation set performance. I will say, however, that standard dropout is preferable for dense layers while 2D spatial dropout (dropping channels instead of individual units) is preferable for convolutions. Am I correct, that if I will put larger value for p, say 0.9, the regularization effect will be more, that if p=0.5? Absolutely not. Maximum regularization is achieved for $p=0.5$ as described by analysis performed in this paper: http://papers.nips.cc/paper/4878-understanding-dropout.pdf . Increasing the dropout level above 50% can sever too many connections between layers and limit the network's ability to learn. Additionally, it has been discovered that lower dropout rates around $0.1$ - $0.2$ are preferable for convolutional layers. But, again, there is no 'rule' for optimal dropout, just guidelines for what has worked well for other networks.
