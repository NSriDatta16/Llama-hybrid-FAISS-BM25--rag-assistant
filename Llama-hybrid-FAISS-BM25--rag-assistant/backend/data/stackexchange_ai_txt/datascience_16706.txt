[site]: datascience
[post_id]: 16706
[parent_id]: 
[tags]: 
What does "batch" and "batch_size" mean in word2vec skip-gram model?

I am reading tensorflow documentation now and I can not understand what does "batch" and "batch_size" mean in explanation skip-gram model. Please, can someone explain me? Here this paragraph: Recall that skip-gram inverts contexts and targets, and tries to predict each context word from its target word, so the task becomes to predict 'the' and 'brown' from 'quick', 'quick' and 'fox' from 'brown', etc. Therefore our dataset becomes (quick, the), (quick, brown), (brown, quick), (brown, fox), ... of (input, output) pairs. The objective function is defined over the entire dataset, but we typically optimize this with stochastic gradient descent (SGD) using one example at a time (or a 'minibatch' of batch_size examples, where typically 16 batch_size
