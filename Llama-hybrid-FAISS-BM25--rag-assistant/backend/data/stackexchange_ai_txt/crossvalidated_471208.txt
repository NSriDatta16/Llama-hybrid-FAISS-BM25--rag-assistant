[site]: crossvalidated
[post_id]: 471208
[parent_id]: 
[tags]: 
Error bars in repeated k-fold cross-validation

Suppose I want to compute the expectation of the loss $L$ based on Repeated K-fold Cross-Validation (KFCV). Just to be precise by repeated KFCV I mean the following: I repeat the $K$ cross-validation on $R$ independent re-shuffles of the data set. Notation $r=1,2,\ldots,R$ are the replicates $k=1,2,\ldots, K$ are folds (in each replicate) $\{L_k^{(r)}\}$ is the sequence of estimated loss, where $L_k^{(r)}$ is the loss computed on the $k$ -th fold. Let's arrange the output of the algorithm in a $K \times R$ matrix like this $$ H = (L_k^{(r)}) = \begin{pmatrix} L_1^{(1)} & L_1^{(2)} & \cdots & L_1^{(R)}\\ L_2^{(1)} & L_2^{(2)} & \cdots & L_2^{(R)}\\ \vdots & \vdots & \ddots & \vdots \\ L_K^{(1)} & L_K^{(2)} & \cdots & L_K^{(R)}\\ \end{pmatrix} $$ Estimated losses within the same column of $H$ are expected to be correlated, while elements along each row should not be correlated. The estimated expected loss in the $r$ -th replicate is given by $$ \bar L^{(r)} = \frac{1}{K} \sum_k L_k^{(r)}, $$ and this is an unbiased estimate of such expectation. However, the variance of $\bar L^{(r)}$ is inflated by the fact that the components of the sum are correlated. As far as I can understand, a better approximation is to average across replicate: $$ \hat{L} \; = \; \frac{1}{R} \sum_r \bar{L}^{(r)} $$ This $\hat{L}$ is still unbiased, but its variance is smaller than that of $\bar L^{(r)}$ because it's an average of independent quantities. What is not clear to me is how to approximates error bars around $\hat{L}$ that show up in any plot of the cross-validation output. It seems kind of strange, but I had to go through software source codes to see how these bars are produced, and I got mixed answers. I am a bit confused when talking about variance in cross-validation. I'd like to be corrected if I am wrong. But an error bar about the estimated loss, in my opinion should be based on a confidence interval for the expected loss. Now one possible way is to come up with the "average +/- standard error" rule, which requires an estimate of the standard error. The only think that makes sense to me is to compute the standard error of $\hat{L}$ $$ s^2=\frac{1}{R-1} \sum_r \left(\bar L^{(r)} - \hat{L} \right)^2, $$ Based on this I would approximate the standard error se $(\hat{L})=s/\sqrt{R}$ , and consider error bars such as $\hat{L} \; \pm$ se $(\hat{L})$ . Is this correct?
