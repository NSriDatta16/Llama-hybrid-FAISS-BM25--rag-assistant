[site]: datascience
[post_id]: 63713
[parent_id]: 63678
[tags]: 
Most of the commonly used machine learning algorithms like Decision Trees and Logistic regression convert the categorical variables into one-hot encoding. For e.g., if you have 20 states then you create 20 variables which has only 1 entry non-zero at a time. In such cases, you can reduce the dimension of the input space by training an embedding matrix E just like word embeddings are learned, e.g., word2vec. You can use a hidden dimension of say 15, to reduce your input space. To find the embedding E , you create a neural network which predicts the embedding of the disease given the embedding of state and symptoms. Training this model with the above objective through back-propagation will allow the embedding matrix capture structure present in the state in the given data. Once the training is over, the i^{th} row of E gives the new representation of your i^{th} state.
