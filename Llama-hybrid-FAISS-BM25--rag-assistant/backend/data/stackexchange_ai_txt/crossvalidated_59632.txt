[site]: crossvalidated
[post_id]: 59632
[parent_id]: 59630
[tags]: 
How to interpret a test accuracy higher than training set accuracy. Most likely culprit is your train/test split percentage. Imagine if you're using 99% of the data to train, and 1% for test, then obviously testing set accuracy will be better than the testing set, 99 times out of 100. The solution here is to use 50% of the data to train on, and 50% to evaluate the model. Accuracy on the training set might be noise, depending on which ML algorithm you are using. The training set accuracy doesn't evaluate the correctness of your model on unseen rows. One strategy is to ignore the training set accuracy. To get a clearer picture of which hyper-parameter choices to your model such (train/test split, iterations, convergence criteria, learning rate alpha, etc) are most responsible for your model having superior accuracy on test set, then run your model 100 times for every hyper parameter choice, then average the differences between training accuracy and testing accuracy. Another strategy is to bag up your models into a list of N-models all of which were trained on a 50/50 train test split. Then all of the models have access to all of the data, and yet also none of the models have the ability to observe more than 50% of the training data. The average result is the correct one, then your training accuracy and testing accuracy will be much closer to equal.
