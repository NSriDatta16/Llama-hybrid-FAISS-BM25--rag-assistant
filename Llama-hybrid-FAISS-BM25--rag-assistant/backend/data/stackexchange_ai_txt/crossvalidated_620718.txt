[site]: crossvalidated
[post_id]: 620718
[parent_id]: 
[tags]: 
Understanding the difference between MLE and MSE

When using MSE in linear regression, I understand that we aim to minimize the average of the square errors between the predicted value $f(x)$ and the actual label y. My understanding is that we aim to reduce the geometric difference between $f(x)$ and the real value $y.$ I think this comes from $f(x)-y.$ My initial thought was that like MSE, the logistic loss function also tries to minimize the distance between the data points and the prediction. Especially when in lecture a fitted sigmoid curve was plotted to the data set for extreme input values, the predicted values match the dataset almost perfectly. Now I have two questions: I thought the output from the logistic model the outputs are probabilities. So how can I even map them on the data set, where the y-labels are units. If not geometric difference, what is essentially measured by the logistic loss function. What discrepancy is the logistic loss function aiming to quantify and minimize in the context of binary classification problems?
