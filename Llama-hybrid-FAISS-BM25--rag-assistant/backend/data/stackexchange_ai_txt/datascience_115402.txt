[site]: datascience
[post_id]: 115402
[parent_id]: 115319
[tags]: 
This is a pretty clear case of overfitting. Your testing loss is much higher than your training loss, which means your model has memorized the training data. This is confirmed by the fact that accuracy is 0.515 - hardly better than a coin flip. It might help to add some regularization to your model. Some of the most common techniques in deep learning are reducing model size, adding dropout, augmenting data, adding L1 or L2 terms to your loss function, or stopping training early when you notice that validation loss starts to increase rather than decrease.
