[site]: crossvalidated
[post_id]: 308703
[parent_id]: 300120
[tags]: 
They are two different metrics to evaluate your model's performance usually being used in different phases. Loss is often used in the training process to find the "best" parameter values for your model (e.g. weights in neural network). It is what you try to optimize in the training by updating weights. Accuracy is more from an applied perspective. Once you find the optimized parameters above, you use this metrics to evaluate how accurate your model's prediction is compared to the true data. Let us use a toy classification example. You want to predict gender from one's weight and height. You have 3 data, they are as follows:(0 stands for male, 1 stands for female) $y_1 = 0, x_{1w}= 50kg, x_{2h} = 160cm$; $y_2 = 0, x_{2w} = 60kg, x_{2h} = 170cm$; $y_3 = 1, x_{3w} = 55kg, x_{3h} = 175cm$; You use a simple logistic regression model that is $y = \frac{1}{1+e^-(b_1*x_w+b2*x_h)}$ How do you find $b_1$ and $b_2$? you define a loss first and use optimization method to minimize the loss in an iterative way by updating $b_1$ and $b_2$. In our example, a typical loss for this binary classification problem can be: $$-\sum_{i=1}^{3}y_{i}log(\hat{y_i}) + (1-y_{i})log(1-\hat{y_i})$$ We don't know what $b_1$ and $b_2$ should be. Let us make a random guess say $b_1$ = 0.1 and $b_2$ = -0.03. Then what is our loss now? $\hat{y}_1 = \frac{1}{(1+e^{-(0.1*50-0.03*160)})} = 0.549834 = 0.55$ $\hat{y}_2 = \frac{1}{(1+e^{-(0.1*60-0.03*170)}}= 0.7109495 = 0.71$ $\hat{y}_3 = \frac{1}{(1+e^{-(0.1*55-0.03*175)}}= 0.5621765 = 0.56$ so the loss is $(-log(1-0.55) + -log(1-0.71) - log(0.56))$ = 2.6162 Then you learning algorithm (e.g. gradient descent) will find a way to update $b_1$ and $b_2$ to decrease the loss. What if b1=0.1 and b2=-0.03 is the final b1 and b2 (output from gradient descent), what is the accuracy now? Let's assume if $\hat{y} >= 0.5$, we decide our prediction is female(1). otherwise it would be 0. Therefore, our algorithm predict $y_1 = 1, y_2 = 1$ and $y_3 = 1$. What is our accuracy? We make wrong prediction on $y_1$ and y2 and make correct one on $y_3$. So now our accuracy is $\frac{1}{3}$ = 33.33% PS: In Amir's answer, back-propagation is said to be an optimization method in NN. I think it would be treated as a way to find gradient for weights in NN. Common optimization method in NN are GradientDescent and Adam.
