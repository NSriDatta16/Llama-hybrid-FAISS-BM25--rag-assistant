[site]: crossvalidated
[post_id]: 630662
[parent_id]: 
[tags]: 
RNN weight and state matrices

Implementations of RNN in NLP tasks, like those in https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-2/ , are done using matrices, that are used to store the inputs, outputs, memory(state) and weights of the RNN. Since a matrix has a predefined size, I understand that this size limits the variable size to a maximum sentence lenght that the RNN can deal with. Is it correct? Assuming that W is the weight matrix applied to the neurons of the hidden-to-hidden layer, why is W defined as a square matrix of dimensions "HxH", with "H" being the number of hidden layers (or the maximum number of hidden layers)? Why isnÂ´t it only a vector of "H" size with one weight for each hidden layer? Thanks!
