[site]: crossvalidated
[post_id]: 633039
[parent_id]: 632935
[tags]: 
I agree with some other answers in that this could be a limitation of the toolset (the human mind), but, I think, perhaps not as simple a one as some people being too dumb to understand Bayesian inference. The causal hypotheses themselves are attractive for the human mind to reason about. Take competitive contexts with lots of sources of the data (and of the hypotheses), such as scientific articles. Any part of the new (second hand) information is likely to be biased in a way and to a degree that would be revealed only much later. If and when that happens, one might also wish to "unlearn" what they learned from an apparently flawed source. A computerized model can perhaps "forget" a Bayesian update (even if not organized as a Bayesian model itself - it can do so by replaying what it already learned minus the update to be forgotten), but the human mind cannot do that. Having hypotheses explicitly articulated has two benefits: Explicit hypotheses allow replication attempts. That allows detection of bad data. The isolated "causal principles" behind each data set make it somewhat easier to detect the types of causal discourse that were heavily predicated on the particular bit of faulty data. This helps to reduce the cognitive setback sustained from the previously believed data. I don't doubt that Bayesian analysis can be the perfect tool for perfect data, or for uniformly imperfect data. But: If some of your input data was actually provided by your adversaries, or if a very large proportion of the data ultimately turns out to be noise, AND you are a human, then a more structured methodology could conceivably end up more successful or at least more popular in the long run.
