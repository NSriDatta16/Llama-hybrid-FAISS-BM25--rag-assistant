[site]: crossvalidated
[post_id]: 2737
[parent_id]: 2686
[tags]: 
I have heard of 'time-based boxcar' functions which might solve your problem. A time-based boxcar sum of 'window size' $\Delta t$ is defined at time $t$ to be the sum of all values between $t - \Delta t$ and $t$. This will be subject to discontinuities which you may or may not want. If you want older values to be downweighted, you can employ a simple or exponential moving average within your time based window. edit: I interpret the question as follows: suppose some events occur at times $t_i$ with magnitudes $x_i$. (for example, $x_i$ might be the amount of a bill paid.) Find some function $f(t)$ which estimates the sum of the magnitudes of the $x_i$ for times "near" $t$. For one of the examples posed by the OP, $f(t)$ would represent "how much one was paying for electricity" around time $t$. Similar to this problem is that of estimating the "average" value around time $t$. For example: regression , interpolation (not usually applied to noisy data), and filtering . You could spend a lifetime studying just one of these three problems. A seemingly unrelated problem, statistical in nature, is Density Estimation . Here the goal is, given observations of magnitudes $y_i$ generated by some process, to estimate, roughly, the probability of that process generating an event of magnitude $y$. One approach to density estimation is via a kernel function . My suggestion is to abuse the kernel approach for this problem. Let $w(t)$ be a function such that $w(t) \ge 0$ for all $t$, $w(0) = 1$ (ordinary kernels do not all share this property), and $w'(t) \le 0$. Let $h$ be the bandwidth , which controls how much influence each data point has. Given data $t_i, x_i$, define the sum estimate by $$f(t) = \sum_{i=1}^n x_i w(|t - t_i|/h).$$ Some possible values of the function $w(t)$ are as follows: a uniform (or 'boxcar') kernel: $w(t) = 1$ for $t \le 1$ and $0$ otherwise; a triangular kernel: $w(t) = \max{(0,1-t)}$; a quadratic kernel: $w(t) = \max{(0,1-t^2)}$; a tricube kernel: $w(t) = \max{(0,(1-t^2)^3)}$; a Gaussian kernel: $w(t) = \exp{(-t^2 / 2)}$; I call these kernels, but they are off by a constant factor here and there; see also a comprehensive list of kernels . Some example code in Matlab: %%kernels ker0 = @(t)(max(0,ceil(1-t))); %uniform ker1 = @(t)(max(0,1-t)); %triangular ker2 = @(t)(max(0,1-t.^2)); %quadratic ker3 = @(t)(max(0,(1-t.^2).^3)); %tricube ker4 = @(t)(exp(-0.5 * t.^2)); %Gaussian %%compute f(t) given x_i,t_i,kernel,h ff = @(x_i,t_i,t,kerf,h)(sum(x_i .* kerf(abs(t - t_i) / h))); %%some sample data: irregular electric bills sdata = [ datenum(2009,12,30),141.73;... datenum(2010,01,25),100.45;... datenum(2010,02,23),98.34;... datenum(2010,03,30),83.92;... datenum(2010,05,01),56.21;... %late this month; datenum(2010,05,22),47.33;... datenum(2010,06,14),62.84;... datenum(2010,07,30),83.34;... datenum(2010,09,10),93.34;... %really late this month datenum(2010,09,22),78.34;... datenum(2010,10,22),93.25;... datenum(2010,11,14),83.39;... %early this month; datenum(2010,12,30),133.82]; %%some irregular observation times at which to sample the filtered version; t_obs = sort(datenum(2009,12,01) + 400 * rand(1,400)); t_i = sdata(:,1);x_i = sdata(:,2); %%compute f(t) for each of the kernel functions; h = 60; %bandwidth of 60 days; fx0 = arrayfun(@(t)(ff(x_i,t_i,t,ker0,h)),t_obs); fx1 = arrayfun(@(t)(ff(x_i,t_i,t,ker1,h)),t_obs); fx2 = arrayfun(@(t)(ff(x_i,t_i,t,ker2,h)),t_obs); fx3 = arrayfun(@(t)(ff(x_i,t_i,t,ker3,h)),t_obs); fx4 = arrayfun(@(t)(ff(x_i,t_i,t,ker4,0.5*h)),t_obs); %!!use smaller bandwidth %%plot them; lhand = plot(t_i,x_i,'--rs',t_obs,fx0,'m-+',t_obs,fx1,'b-+',t_obs,fx2,'k-+',... t_obs,fx3,'g-+',t_obs,fx4,'c-+'); set(lhand(1),'MarkerSize',12); set(lhand(2:end),'MarkerSize',4); datetick(gca()); legend(lhand,{'data','uniform','triangular','quadratic','tricube','gaussian'}); The plot shows the use of a few kernels on some sample electric bill data. Note that the uniform kernel is subject to the 'stochastic shocks' which the OP is trying to avoid. The tricube and Gaussian kernels give much smoother approximations. If this approach is acceptable, one only has to choose the kernel and the bandwidth (in general that is a hard problem, but given some domain knowledge, and some code-test-recode loops, it should not be too difficult.)
