[site]: datascience
[post_id]: 67333
[parent_id]: 67246
[tags]: 
The problem you are describing is known as wake word detection or trigger word detection. I'm sure you could use a CNN to classify a chunked Mel -spectrogram of your audio (see also librosa ). As training labels you would simply use 0 for timestamps with no wake word (no "pizza") and 1 for timestamps with the wake word. Alternatively to classifying all timestamps of one chunk, you could also train for just the the center frame of each spectrogram chunk (makes things easier). In any case, you will have to make sure that your dataset is at least mildly balanced, i.e., you'll have to have enough wake word and non-wake word instances. One way to achieve this, is to overlay recordings of background noise with recordings of wake and non-wake words. There are some tutorials that detail how to do this, e.g. this YouTube video , this article or this GitHub repo . Note that all these approaches use RNN s for the task. However, it has been argued by Bia et al. that a temporal convolutional network (TCN) architecture (in essence a CNN skip connections and dilation ) may work equally well or better for such tasks as the one you describe and is probably easier to train. Hopefully this answer will give you some points to start from.
