[site]: datascience
[post_id]: 94622
[parent_id]: 27665
[tags]: 
In neural networks, activation functions such as the logistic (sigmoid) and the hyperbolic tangent functions map any real values to a compact range of values. For example, the sigmoid function, S(x)= 1/(1+ e^(-x) ) maps a set of real values x to between 0 and 1. To attain these boundaries of either 0 or 1, large magnitude negative or positive values of x are required. Therefore, a neuron is said to be saturated when extremely large weights cause the neuron to produce values (gradients) that are very close to the range boundary. If the gradient is constantly 0, no learning will take place in the neural network. Likewise, if the gradient is constantly 1, it most likely means that the neuron is over-fitting on training data and will likely perform poorly on test data.
