[site]: crossvalidated
[post_id]: 479577
[parent_id]: 
[tags]: 
Information loss in recurrent neural networks?

I am reading Deep Learning by Goodfellow and he seems to imply that the latter structure for a RNN will lead to information loss over the first structure. In both of the pictures, we have a RNN. x is the exogenous variable and h is the hidden layer. o is the prediction. As you can see, in the second structure, we feed our prediction (output of the output layer) into the next time step, as opposed to the output of the hidden layer. What is the intuition behind the information loss that occurs with the second structure? Is it because at the second time step, the neural network has less input variables and this implies less information? First Structure Second Structure
