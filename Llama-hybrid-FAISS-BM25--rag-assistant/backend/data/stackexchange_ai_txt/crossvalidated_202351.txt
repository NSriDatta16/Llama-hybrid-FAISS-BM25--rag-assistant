[site]: crossvalidated
[post_id]: 202351
[parent_id]: 
[tags]: 
Text classification with word2vec and neural nets [spacy.io, keras]

I have about 300.000 messages with bodies and titles at hand. ~20% are of them labeled positive. Right now, I run the word2vec feature generation with spacy.io (excellent library btw.), generating 300 features for title and body respectively. That is fed into a Keras MLP with the following design. model = Sequential() model.add(Dense(600, input_shape=(600,))) model.add(PReLU()) model.add(Dropout(0.5)) model.add(Dense(1200)) model.add(PReLU()) model.add(Dropout(0.5)) model.add(Dense(600)) model.add(PReLU()) model.add(Dropout(0.5)) model.add(Dense(10)) model.add(PReLU()) model.add(Dropout(0.5)) model.add(Dense(1)) model.add(Activation('sigmoid')) model.compile(loss='mse', optimizer=Adam(),class_mode='binary') For training, I use a batch size of 256/512. With that setup, I'm achieving a test-set AUC (10% CV) of ~0.91. What would be most promising steps to further improve classifier performance? Other NN setup (e.g. CNN), does RNN e.g. LSTM make even sense on word vectors? NN layer design, activation functions, other regularization than dropout What gains could be expected roughly from exhaustive hyperparameter search? I can't really spend a downtime of one week for the production machine right now. Would it make sense to implement 1-AUC as loss, as this is our business goal? What do you think about extending word2vec embeddings with POS or NER tags? I think main downside would be that this will not work anymore with spacy vectorization. Do you see a way to procedurally sort out bad data? I expect that some of the labels are plainly wrong, which may put an upper limit on the performance Additionally, some of the samples might not even contain proper messages Is it naive to use the NN as classifier and would an exhaustive evaluation of other classifiers be advisable?
