[site]: crossvalidated
[post_id]: 239468
[parent_id]: 238496
[tags]: 
A RNN is a Deep Neural Network (DNN) where each layer may take new input but have the same parameters. BPT is a fancy word for Back Propagation on such a network which itself is a fancy word for Gradient Descent. Say that the RNN outputs $\hat{y}_t$ in each step and \begin{equation} error_t=(y_t-\hat{y}_t)^2 \end{equation} In order to learn the weights we need gradients for the function to answer the question "how much does a change in parameter effect the loss function?" and move the parameters in the direction given by: \begin{equation} \nabla error_t=-2(y_t-\hat{y}_t)\nabla \hat{y}_t \end{equation} I.e we have a DNN where we get feedback on how good the prediction is at each layer. Since a change in parameter will change every layer in the DNN (timestep) and every layer contributes to the forthcoming outputs this needs to be accounted for. Take a simple one neuron-one layer network to see this semi-explicitly: \begin{align*} \hat{y}_{t+1} =& f(a+bx_t+c\hat{y}_t)\\ \frac{\partial}{\partial a}\hat{y}_{t+1} = & f'(a+bx_t+c\hat{y}_t)\cdot c\cdot \frac{\partial}{\partial a}\hat{y}_{t} \\ \frac{\partial}{\partial b}\hat{y}_{t+1} = & f'(a+bx_t+c\hat{y}_t)\cdot (x_t+c\cdot\frac{\partial}{\partial b}\hat{y}_{t})\\ \frac{\partial}{\partial c}\hat{y}_{t+1} = & f'(a+bx_t+c\hat{y}_t)\cdot (\hat{y}_t+c\cdot\frac{\partial}{\partial c}\hat{y}_{t})\\ \iff\\ \nabla \hat{y}_{t+1} =& f'(a+bx_t+c\hat{y}_t)\cdot \left(\begin{bmatrix}0\\x_t\\\hat{y}_t \end{bmatrix} + c \mathbin{\color{red}{\nabla \hat{y}_{t}}} \right) \end{align*} With $\delta$ the learning rate one training step is then: \begin{equation} \begin{bmatrix}\tilde{a}\\\tilde{b}\\\tilde{c}\end{bmatrix} \leftarrow \begin{bmatrix}a\\b\\c\end{bmatrix} + \delta (y_{t}-\hat{y}_{t})\nabla \hat{y}_t \end{equation} What we see is that in order to calculate $\nabla \hat{y}_{t+1}$ you need to calculate i.e roll out $\nabla \hat{y}_{t}$. What you propose is to simply disregard the red part calculate the red part for $t$ but not recurse further. I assume that your loss is something like \begin{equation} error=\sum_t(y_t-\hat{y}_t)^2 \end{equation} Maybe each step will then contribute a crude direction which is enough in aggregation? This could explain your results but I'd be really interested in hearing more about your method/loss function! Also would be interested in a comparison with a two timestep windowed ANN. edit4: After reading comments it seems like your architecture is not an RNN. RNN: Stateful - carry forward hidden state $h_t$ indefinitely This is your model but the training is different. Your model: Stateless - hidden state rebuilt in each step edit2 : added more refs to DNNs edit3 : fixed gradstep and some notation edit5 : Fixed the interpretation of your model after your answer/clarification.
