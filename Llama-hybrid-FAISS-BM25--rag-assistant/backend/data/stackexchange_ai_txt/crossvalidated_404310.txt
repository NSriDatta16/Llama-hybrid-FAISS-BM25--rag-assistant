[site]: crossvalidated
[post_id]: 404310
[parent_id]: 404305
[tags]: 
Your understanding of the effect isn't quite right. If these test scores are random variables that are conditionally independent over time (given their student-wise means), then across all students we expect as many scores to increase or decrease on the second measurement (i.e. about 50% do better the second time and 50% do worse). But consider what happens if we look at the top 1% of students from the first test. Some of their good performance is likely due to stable factors like their intelligence and proficiency at the subject, and this will carry over from one test to the next. That is, these factors determine their mean performance. But part of their good scores on the first test is going to be due to luck. Statistically, this means that the random variability in their test scores turned out to their advantage on this particular test. This puts them in the upper tail of the distribution for that test. But this luck does not carry over to the next test. They could get lucky again, and this probability hasn't changed, but overall it's more likely that they will have average luck (because that's the most likely outcome for everyone). So on the next test, the scores of these students are likely to be close to their average performance (as they typically are). Only because we singled them out based on their previous performance, it seems like these great scorers have a tendency to do worse on their second test. The opposite effect happens if we singled out the worst 1% from the first test. They likely got pretty bad luck on that first test, but given another try most of them will have average luck (because, again, average luck is more likely overall), so it will seem like their scores have a tendency to increase. If we look at the test scores from all students though, we will see that they have the same distribution for the first and second tests, with the same proportion of values near the mean or far away from it. In short, random variables with typical distributions (e.g. Normal) are always more likely to take on values close to their means, whether you look at their first, second or N-th observation. Occasionally, though, they'll take on a value far from the mean, somewhere in the tails of their distribution. If you look only at those times, then it seems like whenever they are very high, they tend to be lower the next time, and vice versa. But this effect doesn't really exist, and the real explanation is that values near the mean are always more likely.
