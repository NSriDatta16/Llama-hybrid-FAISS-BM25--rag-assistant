[site]: crossvalidated
[post_id]: 446738
[parent_id]: 446443
[tags]: 
That's linear algebra, not specific to perceptrons or machine learning. Let's start with a simple, two-dimensional case, with axes $x_1$ and $x_2$ . Your class boundary can be defined by a straight line: $$x_2 = a x_1 + b$$ This is a so-called explicit line equation. It has one problem: What if the boundary is a vertical line? In that case $a$ would need to be infinte. You can circumvent this problem by swapping the coordinates and writing: $$x_1 = c x_2 + d$$ A vertical line is then described by $x_1 = d$ , i.e. taking $c=0$ . However, here you have the same problem with the horizontal line, because $c$ would have to be infinite. In the first equation, the horizontal line would be written down as $x_2 = b$ , i.e. setting $a$ to zero. The implicit equation, which is easy to derive from the above two, unifies the notation into a general form and is not susceptible to that problem: $$w_1 x_1 + w_2 x_2 + b = 0$$ You can get the first explicit equation from it by setting $w_1 = a$ and $w_2 = -1$ , and the second equation by setting $w_1 = -1$ , $w_2 = c$ , and $b=d$ . A horizontal line can be written down by setting $w_1 = 0$ , and a vertical one by setting $w_2 = 0$ . Using vector notation, with $w = [w_1, w_2]$ and $x = [x_1, x_2]$ , the implicit equation can be written more compactly as: $$w \cdot x + b = 0$$ This can be generalized to any number of dimensions.
