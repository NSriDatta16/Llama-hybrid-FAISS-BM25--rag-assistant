[site]: crossvalidated
[post_id]: 626273
[parent_id]: 626271
[tags]: 
There is no assumption about uncorrelated features in logistic regression. For instance, people will run logistic regression on the MNIST digit pixels without giving it a second thought and achieve decent performance north of $90\%$ accuracy. While this is hardly world-class performance (deep learning approaches can score more like $99.97\%$ accuracy), that’s a lot of digits identified correctly. However, some of the pixels are quite correlated. I understand the appeal of dropping some correlated predictors: preserve most of the “information” available in the features yet drop a parameter that risks overfitting. However, there are a few drawbacks to this. Unless there is perfect multicollinearity, the dropped feature will contain unique information. Yes, dropping a parameter can help to quell overfitting, but that can come at the expense of underfitting! Imagine dropping a feature due to overfitting concerns and wanting strong predictive ability, only to find yourself unable to get a good fit because a useful feature has been dropped from the model. It is possible to overfit a feature selection step. If we consider overfitting to be fitting to coincidences in the data that do not generalize, a selection of particular features can be specific to a particular set of training data, and you might get a rather different selection of features in a different split of the data. Our Frank Harrell talks about the instability of feature selection in one of his machine learning presentations . If you start including interaction terms, which are implicit in many sophisticated machine learning models like neural networks and random forests and can be included in logistic regression models when those interactions are made explicit, dropping a variable misses the opportunity to have it participate in the interactions. There are techniques, such as regularization, that can reduce overfitting yet do not totally remove the variable from the analysis. Therefore, it is far from a given that you should remove a feature that is correlated with other features.
