[site]: crossvalidated
[post_id]: 417002
[parent_id]: 412257
[tags]: 
I’ve used TF-IDF and LDA on non-text, and am not aware of pitfalls in general. That doesn’t mean there aren’t any pitfalls, though! And, of course, any bag-of-words approach is throwing away your time series’ ordering, which can dramatically affect meaning. In your use case, there are a couple of thoughts that come to mind: It seems that there are no amounts associated with your events? In textual use, you tend to have word counts within documents. So you are working with just presence/absence data, which is even less information from the original time series. Your solution appeals to me, and you can make the analogy that each object is a “document” and the events that happened to it are “words” and you’re looking to categorize your objects (“documents”) by “topic”. But some folks won’t accept that and will demand a technique they are familiar with, instead of an off-brand usage. I’d investigate your logistic regression more. I’ve heard of successful logistic regressions with 10,000 variables. They used Feature Hashing to compress the huge number of highly sparse features down to 10,000. Have you tried an association rules approach? Do certain bigrams (adjacent pair of events) have high association with your target output? Be careful, though, since slicing your data more and more finely can cause associations “by chance” — look at enough cases, especially with small numbers, and you will eventually find something.
