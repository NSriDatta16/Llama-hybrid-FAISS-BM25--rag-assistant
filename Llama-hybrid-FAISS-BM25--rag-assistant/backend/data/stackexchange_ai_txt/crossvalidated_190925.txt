[site]: crossvalidated
[post_id]: 190925
[parent_id]: 190921
[tags]: 
PCA sounds like a good thing to do within the groups of correlated predictor variables. You could use the rule-of-thumb for each PCA run and set the number of PCs required from each run based on the number of eigenvalues greater than unity, $\lambda_j>1$. For example, your first PCA run has 50 predictors, hence a $50 \times 50$ correlation matrix $\mathbf{R}$, and maybe 7 eigenvalues are greater than 1, hence #PC=7. The second PCA run has 30 predictors, so a $30 \times 30$ $\mathbf{R}$ correlation matrix for which maybe 3 eigenvalues were greater than one, hence #PC=3. For these two PC runs you now have 10 PCs which are uncorrelated predictors. Unfortunately, a lot of beginners(students) want to throw everything into a single model to solve everything in one run. However, you should think about breaking up your models and dependent variables, where you might regress each $y$ on each set of PCs from the intended PCs (relevant predictors which were reduced via PCA). Don't initially plan on using one unifying model. Instead, plan on breaking up a large regression into small regression models. If you had only one $y$ or several multivariate $y$'s, you could regress them on all of the PCs from all the PCA runs, but if you have different $y$-variables associated with each set of (original) predictors, then run those regressions using the $y$ on the PCs you extracted via PCA. Don't make one large unifying model -- try to break things up into many models, because they will have their own peculiarities and different goodness-of-fit characteristics.
