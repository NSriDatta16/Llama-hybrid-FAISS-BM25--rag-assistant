[site]: crossvalidated
[post_id]: 553366
[parent_id]: 553355
[tags]: 
As you noticed, defining complexity can be challenging. For models such as linear regression, complexity is the function of the number of parameters. It is much harder to define it for other models. What would be the complexity for the random forest, neural networks, or for non-parametric models like $k$ -NN? This is a non-trivial question. Questions like this were considered by researchers studying (lack of) bias-variance trade-off and I recommend you to check the papers mentioned in the linked answer . Those papers focus rather on a more abstract idea of model capacity , where the models with more capacity would be able to learn more complicated patterns, up to being able to fit completely random data. As you noticed, $k$ -NN with $k=1$ "learns" $N$ different "patterns" from the data, one per sample, with $k=N$ it can only make one prediction (the average). In such a case, we could say that $k$ controls model's capacity, or complexity, nonetheless that it doesn't have parameters.
