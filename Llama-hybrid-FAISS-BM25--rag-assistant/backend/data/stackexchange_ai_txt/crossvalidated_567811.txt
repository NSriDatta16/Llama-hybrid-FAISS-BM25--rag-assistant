[site]: crossvalidated
[post_id]: 567811
[parent_id]: 
[tags]: 
Current best practices for detecting Unit Root in time series data?

I'm trying to get an overview of what are the current best practices for detecting unit roots in time series data. The main approach I came across is the Augmented Dickey Fuller (ADF) test, which tests for unit roots in three models: \begin{align} \Delta y_t =& \alpha + \beta t + \gamma X_{t-1} + \delta_1 \Delta y_{t-1} + \dots + \varepsilon_t \\ \Delta y_t =& \alpha + \gamma X_{t-1} + \delta_1 \Delta y_{t-1} + \dots + \varepsilon_t \\ \Delta y_t =& \gamma X_{t-1} + \delta_1 \Delta y_{t-1} + \dots + \varepsilon_t \end{align} If I understand this test correctly, the only reason to not only use the most complex model is that the power to reject H0: $\gamma = 0$ is very low if the complex model is misspecified (i.e., no drift / linear trend). However, from here on, there seem to be different ways to execute this. One is to go through a kind of flow-chart of consecutive hypothesis tests from the most complex model to the least complex one (see e.g., PDF page in this book ). Another one seems to use model selection, e.g., using an information criterion, and then perform the hypothesis test using the selected model (see e.g., this paper ). My question is whether there is any agreement based on theory and/or simulation studies which approach works best? In addition, is the ADF even considered best practices? What are, e.g., econometricians using in 2022 to detect a unit root?
