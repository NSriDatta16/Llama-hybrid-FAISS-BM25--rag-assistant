[site]: datascience
[post_id]: 33532
[parent_id]: 
[tags]: 
Does out-of-bag sampling make Random Forests inherently less robust than other classifiers?

All of the popular evaluation metrics (ROC-AUC, Confusion Matrices, etc.) require two lists as parameters: a list of the actual y labels associated with some arbitrary group of training examples ( x's ), and a parallel list of the predicted labels given to those x's by the model. To construct such lists, you must separate a testing/validation data set from the training set you feed the model on. However, random forests automatically partition 1/3 of the training set you give it to calculate the out of bag score. I don't believe you can stop the bagging process that causes this, since I think it is vital to how the random forest functions. Because the RF model never sees 1/3 of the training set (due to bagging), do RFs create a less thorough image of the dataset then, say, a neural network would whenever a testing set is reserved for evaluation?
