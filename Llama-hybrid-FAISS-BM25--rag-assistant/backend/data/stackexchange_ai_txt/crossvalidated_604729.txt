[site]: crossvalidated
[post_id]: 604729
[parent_id]: 
[tags]: 
Computing mean and standard deviation of a set of time series data

Consider a set of $M$ time series described by $D = \{d^1,\ldots,d^M\}$ where each $d^i$ is a time series of the same length, say $N$ . I want to create a plot with the mean time series and the standard deviation. What I initially thought was to compute the mean and standard deviation of the collection of points at each time step $n=1,\ldots, N$ then just plot that. But this point-wise approach seems like an overly naive approach for time-series data: the "mean" trajectory formed this way certainly isn't the average trajectory found in $D$ (similarly when computing the standard deviation). I am wondering if there is a better approach that computes the mean and standard deviation trajectory-wise rather than in the above described point-wise way. I hope I'm making myself clear.
