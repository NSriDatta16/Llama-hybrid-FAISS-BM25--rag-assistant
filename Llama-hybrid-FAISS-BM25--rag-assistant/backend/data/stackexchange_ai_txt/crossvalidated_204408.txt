[site]: crossvalidated
[post_id]: 204408
[parent_id]: 204397
[tags]: 
I could be wrong but I have always thought the difference to be in the assumption of independence. In the Gambler's fallacy the issue is the misunderstanding of independence. Sure over some large N number of coin tosses you will be around a 50-50 split, but if by chance you are not then the thought that your next T tosses will help even out the odds is wrong because there each coin toss is independent of the previous. Regression towards the mean is, where I see it used, some idea that draws are dependent on previous draws or a previous calculated average/values. For example let use NBA shooting percentage. If player A has made on average 40% of his shots during his career and starts off a new year by shooting 70% in his first 5 games its reasonable to think that he will regress to the mean of his career average. There are dependent factors that can and will influence his play: hot/cold streaks, teammate play, confidence, and the simple fact that if he were to maintain 70% shooting for the year he would absolutely annihilate multiple records that are simply impossible physical feats (under the current performance abilities of professional basket ball players). As you play more games your shooting percentage will likely drop closer to your career average.
