[site]: datascience
[post_id]: 110506
[parent_id]: 107462
[tags]: 
A simple, intuitive explanation- think of each latent dimension as a measure of some (very abstract) quality or property of a word. The value a word's coordinate has in that dimension describes how aligned or opposed that word is to that property. Words have a higher similarity when they are aligned with the same abstract concepts and opposed to the same ones. When you average the coordinates over the words in a sentence, what happens to the coordinates that many words agree upon? In a very, very simple example of a two word sentence and a two dimensional embedding, say the first word has the embedding $(1, -1)$ and the second $(1, 1)$ . The average of the two is $(1, 0)$ . Since the words agree on the first dimension, the average is large. Since they disagree on the second, they cancel each other out. Adding in a third word with embedding $(1,0)$ results in the same average. All 3 words are positively associated with the first abstract concept. For the second abstract concept, one word is positively associated, one is negatively, and the last is neutral. These all combine to be neutral. So the embedding average in a sense captures the associations that words agree on, and when words disagree the average becomes more neutral to the concept. For tasks where the ordering of the words is not important but the collective associations are, this is very convenient- it allows representing sentences or collections of words of arbitrary length in the same exact way as individual words, and the embeddings are directly comparable. Two sentences are similar when the average of the similarities between the individual words is large. Take a sentence with words $(x_1, x_2)$ and one with $(y_1, y_2)$ . Computing their average embeddings followed by the dot product of the embeddings can be written as $$ $$ But since inner products are linear, we can rewrite this as $$ \frac{1}{4}( + + + )$$ Which shows that sentence similarity calculated in this way is equivalent to the average similarity when selecting a word in the first sentence and one from the second. This gives another way to think about the averaged embeddings in a way that directly relates to the similarities between the individual words. An interesting and important connection pops up when we consider the distance between the embeddings- this takes into account the pairwise similarity between words within the sentences as well as between. This becomes equivalent to something called Maximum Mean Discrepancy which has a great number of important properties when examining whether two samples came from the same distribution or not.
