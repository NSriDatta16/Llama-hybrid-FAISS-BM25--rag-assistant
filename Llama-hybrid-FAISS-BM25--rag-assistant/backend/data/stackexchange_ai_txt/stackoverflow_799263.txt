[site]: stackoverflow
[post_id]: 799263
[parent_id]: 799239
[tags]: 
This article on IBM goes into some detail on how the Web crawler uses the robots exclusion protocol and recrawl interval settings in the Web crawler To quote the articles. The first time that a page is crawled, the crawler uses the date and time that the page is crawled and an average of the specified minimum and maximum recrawl intervals to set a recrawl date. The page will not be recrawled before that date. The time that the page will be recrawled after that date depends on the crawler load and the balance of new and old URLs in the crawl space. Each time that the page is recrawled, the crawler checks to see if the content has changed. If the content has changed, the next recrawl interval will be shorter than the previous one, but never shorter than the specified minimum recrawl interval. If the content has not changed, the next recrawl interval will be longer than the previous one, but never longer than the specified maximum recrawl interval. This is about their web crawler but is very useful in reading while building your own tool.
