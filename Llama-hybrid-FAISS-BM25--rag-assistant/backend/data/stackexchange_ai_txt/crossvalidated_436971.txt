[site]: crossvalidated
[post_id]: 436971
[parent_id]: 
[tags]: 
Is it always possible to find the feature map from a given kernel?

Each positive definite kernel $k(x, x')$ used in machine learning/statistics has an equivalent representation as a dot product of the feature map representation $\phi(x)$ of each input i.e. \begin{align} k(x, x') = \phi(x)^T\phi(x') \end{align} My question is given a kernel expression, is it always possible to find the corresponding feature map? For example, we know that the corresponding feature map of gaussian kernel is an infinite dimensional vector. ( Feature map for the Gaussian kernel ) Any pointers (including research papers) are welcome.
