[site]: crossvalidated
[post_id]: 15072
[parent_id]: 
[tags]: 
Predicting a maximum value with little data

My problem is i'm trying to figure out how many servers might be required to handle a theoretical maximal load of data requests. To do that I need to know what the maximum number of requests in a second might be. I have a fairly limited set of data that 156,000 transactions occurred in one hour. This is the maximum recorded transactions for a rolling 60 minute period over 4 months. 4500 devices have access to the server and each client-server transaction takes and average of 45 seconds. I was wanting to predict what the maximum number of client-server transactions in a single second might be with 95% certainty. Each server can service a maximum of 15 concurrent transactions before they start to experience lag. It's a long time since I've done any stats so if someone could point me in the right direction then that'd be brilliant. I figured I might need to know the max/min/average transaction time to be able to get to the variance but i'm probably just babbling now. Thanks a million.
