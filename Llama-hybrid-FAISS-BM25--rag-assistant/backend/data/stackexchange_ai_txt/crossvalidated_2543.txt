[site]: crossvalidated
[post_id]: 2543
[parent_id]: 146
[tags]: 
If your data is high dimensional and noisy, and you don't have a large number of sample, you run into the danger of overfitting. In such cases, it does make sense to use PCA (which can capture a dominant part of data variance; orthogonality isn't an issue) or factor analysis (which can find the true explanatory variables underlying the data) to reduce data dimensionality and then train a regression model with them. For factor analysis based approaches, see this paper Bayesian Factor Regression Model , and a nonparametric Bayesian version of this model that does not assume that you a priori know the "true" number of relevant factors (or principal components in case of PCA). I'd add that in many cases, supervised dimensionality reduction (e.g., Fisher Discriminant Analysis ) can give improvements over simple PCA or FA based approaches, because you can make use of the label information while doing dimensionality reduction.
