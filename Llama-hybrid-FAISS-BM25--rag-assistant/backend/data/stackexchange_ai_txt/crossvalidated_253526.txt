[site]: crossvalidated
[post_id]: 253526
[parent_id]: 199263
[tags]: 
It is important to remember that topic models such as LDA were primarily developed for unsupervised text summarization. So often, there is not a "best" choice for how many top words to show. Most research papers on topic models tend to use the top 5-20 words. If you use more than 20 words, then you start to defeat the purpose of succinctly summarizing the text. A tolerance $\epsilon > 0.01$ is far too low for showing which words pertain to each topic. A primary purpose of LDA is to group words such that the topic words in each topic are highly probable within that topic. If such a low threshold is chosen, then many, many words will appear in each topic, again defeating the purpose of succinct text summarization. To extract the most probable words, you would be better off choosing a threshold of $\epsilon > 0.9$ or maybe $\epsilon > 0.8$. The issue of seeing wordless topics in general when using Gensim is probably because Gensim has its own tolerance parameter "minimum_probability". This parameter defaults to 0.01 (this is explained in the Gensim LDA documentation ). If you want to see all the words per topic, regardless of their low probability of appearing in the topic, you can set minimum_probability = 0. For LDA, you are best off using the normalized probabilities (using "get_topic_terms" function through the ldamodel) because they are the most interpretable. I am not intimately familiar with how Gensim estimates the topic-word probabilities, but the unnormalized values are probably a result of Bayesian estimation where it's not relevant to directly estimate the denominator because (as you've said) it's just normalization.
