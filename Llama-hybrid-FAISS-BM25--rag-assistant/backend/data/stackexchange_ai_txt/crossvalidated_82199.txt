[site]: crossvalidated
[post_id]: 82199
[parent_id]: 82177
[tags]: 
The logistic function is a convenience. The modern inspiration for artificial neural networks comes from A logical calculus of the ideas immanent in nervous activity. The proposed neuron is based on a realistic binary neuron, the threshold logic unit. This is the Heaviside function. $ H(x) = \left\{ \begin{array}{lr} 1 , w x +b > t\\ 0 , \text{otherwise}\\ \end{array} \right.$ Assembling nodes with this activation function in a many-to-one mapping is called a perceptron. This is one of the first ANN algorithms that saw wide-spread use. The main reason was because a simple training algorithm was discovered. The update rule is similar to gradient descent of ordinary least squares. $w(t+1) = w(t) + \alpha (y_{true} - y_{pred})x$ One of several reasons that the perceptron fell out of favor is that the perceptron is not a universal function approximator. The perceptron can only learn a linear separation boundary. The XOR function for instance cannot be learned by the perceptron. $ X = \left[ \begin{array}{ccc} 0 & 0\\ 1 & 0 \\ 0 & 1 \\ 1 & 1 \end{array} \right]$ $ Y = \left[ \begin{array}{ccc} 1\\ 0\\ 0\\ 1\end{array} \right]$ When MLPs came into the picture, so did backprop. MLPs are universal function approximators. We could use binary neurons with MLPs but the error is high, it takes longer to converge and requires more neurons. The recent trend is towards Rectified Linear Units regularized with Dropout. Our modern understanding of real neural networks is the stochastic binary neuron, which is a probabilistically activated Heaviside function. Dropout can be seen as mimicking the regularizing behaviour of the stochastic binary neuron. It can also been seen as a form of ridge regression with a diag(covariance) prior. The deep connection between all three is fascinating.
