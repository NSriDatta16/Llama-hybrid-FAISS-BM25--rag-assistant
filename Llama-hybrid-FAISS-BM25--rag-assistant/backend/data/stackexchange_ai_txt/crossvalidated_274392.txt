[site]: crossvalidated
[post_id]: 274392
[parent_id]: 
[tags]: 
Deep Autoencoder in TensorFlow to learn pseudo-sentence embeddings

I am stuck with a problem since days which is to find a right set-up for an Autoencoder that learns dense representations of sparse textual representations, word order is not relevant which is why I did not use skip-gram or CBOW. I would like to use these embeddings as features for another neural network. Data structure: Vocabulary Size (columns): 4495 Unique Pseudo-Sentences (rows): 1452789 (Each sentence, basically a list of numerically encoded tags, is one-hot-encoded given the vocabulary and thus a vector of length 4495 which contains a 1 for very tag in the vocabulary, and 0 respectively) The resulting matrix is pretty sparse, on average each pseudo-sentence has 11 tags. Target: Learn a dense embedding for each pseudo-sentence Measures: I am currently observing 4 measures across training Accuracy Prediction Recall RMSE I adapted the TF Autoencoder Example from here: https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/autoencoder.py I changed from sigmoid to rely activation, except of the output (final decoded) layer and tried different set-up around 2 and 3 hidden layers: [1024, 256, 64] [2048, 256, 32] [1024, 128] [1024, 64] [512, 32] ... learning rates: {0.03, 0.01, 0.003} batch_sizes: {512, 1024} My biggest problem so far is that accuracy is stuck around .70 and .82, but even more crucial is my ridiculously low recall of ~0.007 which is decreasing during learning. Loss is decreasing and the precision is very good ~0.99. Recall and Precision are calculated by taking a threshold, currently 0.8 against which I evaluate the predictions. That means every value below that threshold is taken as 0 (word not in sentence) and every equal or above that threshold is mapped to 1 (word appears in sentence). I know, this is quite arbitrary, but somehow I have to find out whether large accuracy just stems from predicting 0 for all due to the large sparsity. I have to get recall up with the embeddings, but I don't like to test even more different architectures since I don't believe that this is the main problem. Has anyone some helpful suggestions? Maybe changing to another loss function or trying a deeper architecture? Thanks in advance!
