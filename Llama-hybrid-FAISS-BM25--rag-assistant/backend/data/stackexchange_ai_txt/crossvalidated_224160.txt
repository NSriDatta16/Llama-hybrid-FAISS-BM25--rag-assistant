[site]: crossvalidated
[post_id]: 224160
[parent_id]: 
[tags]: 
Gaussian Processes Probabilistic Least Squares Classifier - Leave one Out Cross-Validation Means, Variances Shape?

I am currently working on implementing some of the algorithms covered in Rasmussen and Williams' book, and stuck on a particular part in chapter 5 ( link to the chapter here ). In particular, on page 117, in equation 5.12, there seem to be major explanations missing there, the most important of which being that one would expect both the LOO-CV mean and variance at a single training point to be a single value - the point I'm stuck on/need help clarifying. EDIT: I should clarify that below, the subscript $_{i}$ indicates that it is the $i^{th}$ row that is removed before inversion, and subscript $_{ii}$ that the $i^{th}$ row and column are removed before inversion. The syntax is misleading, but can be found appropriately in some papers, as well as directly implied by the preceding and subsequent paragraph where these equations are (by way of explanations of efficiency of inverting the covariance matrix over and over). However, $$\mu_i = y_i - \frac{[K^{-1}y]_i}{[K^{-1}]_{ii}}$$ is, to my understanding, an $m \times m$ matrix, where $m = n-1$, and $n$ is the size of the square covariance matrix. The same goes for the other equation, $$\sigma_i^2 = \frac{1}{[K^{-1}]_{ii}} $$ which also produces an $m \times m$ matrix as opposed to a single value. I'm sure that I have misunderstood something (possibly quite fundamental/basic), or am missing some implied knowledge of some sort that would solve my supposed problem. Any help greatly appreciated!
