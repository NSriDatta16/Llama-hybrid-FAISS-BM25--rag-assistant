[site]: crossvalidated
[post_id]: 496236
[parent_id]: 
[tags]: 
Is there any rationale for rules of thumb for maxlag selection?

I understand that the optimum number of lags can be selected using some information criterion (e.g. Akaike (AIC), Schwartz Bayesian Information Criterion (SBIC) etc.). However, in order to select the optimal number of lags we also usually need to specify the maximum number of lags to be considered by the criteria which can affect the results. For example, consider the example below where selecting different maximum lag to be considered produces widely different results: install.packages("tsDyn") require("tsDyn") data(barry) test1 Now I actually know that there are some rules of thumb that can be used here. For example: A common one that I heard often is to set maximum lag to natural log of the number of periods $\ln T$ (see for example this thread on research gate). This cross-validated answer just recommends for cointegration 1 for yearly data 4 for quarterly data and so on for higher frequencies. However, these answers never mention where these rules of thumb come from or give any rationale behind them. I think this has to be ultimately based on sample size. For example, AIC is given as: $${\text{AIC} =2k-2\ln({\hat {L}})}$$ where $k$ is the number of parameters estimated by the candidate model and $\hat{L}$ the maximum likelihood of the candidate model. Now as far as I understand the candidate models would still require approximately 30 observations per independent parameter (but this is again rule of thumb) as other parametric models giving us another potential rule of thumb $\text{maxlag}= T/30$ , but again this is not rigorous just an alternative rule of thumb. Hence my questions are: Are the rules of thumb mentioned above based on/can be derived from some rigorous theory? If the answer to above is negative is there any rigorous way of determining maximum number of lags to consider?
