[site]: stackoverflow
[post_id]: 4542670
[parent_id]: 4542600
[tags]: 
There's not much of a difference. If I've done my math right your's is on average N/16 slower than the authors but pleanty of cases exist where your implementation will be faster. Edit: I'll call your implementation Y and the author's A Both proposed solutions has O(N^2) as worst case and they both have a best case of O(N) when all elements are the same value. EDIT: This is a complete rewrite. Inspired by the debat in the comments I tried to find the average case for random N random numbers. That is a sequence with a random size and a random distribution. What would the average case be. Y will always run U times where U is the number of unique numbers. For each iteration it will do N-X comparisons where X is the number of elements removed prior to the iteration (+1). The first time no element will have been removed and on average on the second iteration N/U will have been removed. That is on average ½N will been left to iterate. We can express the average cost as U*½N. The average U can be expressed based on N as well 0 Expressing A becomes more difficult. Let's say we use I iterations before we've encountered all unique values. After that will run between 1 and U comparisons (on average that's U/") and will do that N-I times. I*c+U/2(N-I) but whats the average number of comparisons (c) we run for the first I iterations. on average we need to compare against half of the elements already visited and on average we've visited I/2 elements, Ie. c=I/4 I/4+U/2(N-I). I can be expressed in terms of N. On average we'll need to visited half on N to find the unique values so I=N/2 yielding an average of (I^2)/4+U/2(N-I) which can be reduced to (3*N^2)/16. That is of course if my estimation of the averages are correct. That is on average for any potential sequence A has N/16 fewer comparisons than Y but pleanty of cases exists where Y is faster than A. So I'd say they are equal when compared to the number of comparisons
