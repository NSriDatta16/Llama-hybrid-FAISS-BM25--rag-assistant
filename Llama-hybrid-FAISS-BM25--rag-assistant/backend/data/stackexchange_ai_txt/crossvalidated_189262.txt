[site]: crossvalidated
[post_id]: 189262
[parent_id]: 189256
[tags]: 
Generally, bias is when you are tying to estimate something, but because of how you go about estimating the thing, your expected estimate is different from the true value. Take your sampling example. You are collecting data to attempt to learn something about the population you are sampling from. The sample is bias when it, for some reason, does not correctly reflect the population you're drawing from for purposes of what you are trying to learn. That is, if you took sample after sample in many universes, what you learned from the different samples would still be wrong. In learning, a model has high bias when averaging over training datasets drawn from many different universes, the model still does not reflect the true process. This tends to happen when the model is underfit. For example, if the truth is a parabola, and you're fitting a line, no matter how many times you train the model or collect more data, you're still going to be wrong. So, blur your eyes a little, and the concepts are mostly the same.
