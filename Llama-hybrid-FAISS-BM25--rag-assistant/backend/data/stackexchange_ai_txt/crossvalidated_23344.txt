[site]: crossvalidated
[post_id]: 23344
[parent_id]: 23340
[tags]: 
As a rule of thumb, small and/or sparse networks generalise better. You can let your training algorithm weed out unecessary connections within a fixed-size network by applying some form of weight decay, or you can apply an algorithm that aims to optimise network architecture/topology itself through removing unecessary inputs, hidden nodes or connections. Have a look at these references for ideas and starting points for further research, or look into the use of evolutionary algorithms to design, prune and optimise architectures. Castellano, G., Fanelli, A.M. (2000) 'Variable selection using neural-network models', Neurcomputing (31) Ji C., Psaltis D. (1997) 'Network Synthesis through Data-Driven Growth and Decay', Neural Networks Vol. 10, No. 6, pp. 1133-1141 Narasimha P.L. et al (2008) 'An integrated growing-pruning method for feedforward network training', Neurocomputing (71), pp. 2831-2847 Schuster, A. (2008) 'Robust ArtiÔ¨Åcial Neural Network Architectures', International Journal of Computational Intelligence (4:2), pp. 98-104
