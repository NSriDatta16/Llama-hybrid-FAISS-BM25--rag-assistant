[site]: datascience
[post_id]: 47669
[parent_id]: 47665
[tags]: 
Normally a convolutional neural network will get flattened into a single column vector after the convolutions and then maybe be processed by dense layer. In this model, the convolution $1\times1$ is used as an output layer. It will have $C$ channels like every other layer, but it is not dilatated. Hence, you can use this layer as the input to other convolutional neural networks. The kernel size will not influence the channels. Imagine an RGB image with 4 by 4 pixels. If we have a $2\times 2$ convolution with $2\times 2$ stride we will get an output of dimension $3\times 2 \times 2$ (without padding). Hence the channels do not change. If we have $K$ filters we will get $K$ times a $3\times 2 \times 2$ output. If the kernel size is $4\times 4$ with a stride of $2\times 2$ we will get a $3\times 1 \times 1$ (without padding) output for each of the $K$ filters. The kernel size only influences how large the receptive field of the convolutions are. Hence, it only influences how the layer is scaling the individual dimensions of the width and height (by using RGB images as an example). If you flatten the output of a layer you will always reduce its dimensionality to $1$ . In the dilated convolutional neural network they do not have a layer that flattens the input.
