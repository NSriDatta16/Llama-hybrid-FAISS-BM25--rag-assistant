[site]: crossvalidated
[post_id]: 434563
[parent_id]: 364361
[tags]: 
You should always use a sampling approach in logistic regression. When facing an unbalanced dataset, which means there is a huge size difference between the event (response, positive..) vs non-event (no response, negative...) data. When the target event is rare, a representative sample is unlikely to have enough target events to build a good predictive model. Fortunately, the amount of information in a data set with a categorical outcome (such as a response to a marketing campaign) is determined not by the total number of cases in the data set, but by the number of cases in the rarest outcome category. Oversampling One approach is to oversampling . While oversampling reduces analysis time, it also introduces some biases. You need to correct these biases so that the results are applicable to the population. For example, you might choose a sampling of data that contains all of the events and only a subset of the nonevents, which will make event and non-event data sizes similar. Again, this kind of analysis introduces biases that you need to correct so that the results are applicable to the population. Splitting the data for training, validation, and test. (You probably know this.) Bias correction. The effect of oversampling is the response ( $logit(\hat{p})$ ) surface for a logistic regression model is shifted linearly, and oversampling does not affect the slopes, but it does make the intercepts too high or too low. To correct the bias, or offset follow the equation $$Offset = \ln \frac{(\pi_0 \rho_1)}{(\pi_1 \rho_0)}$$ $\pi_0$ =proportion of non-events in the population; $\pi_1$ =proportion of events in the population. $\rho_0$ =proportion of non-events in the sample; $\rho_1$ =proportion of events in the sample. The output results should be ( $logit(\hat{p})-$ offset)
