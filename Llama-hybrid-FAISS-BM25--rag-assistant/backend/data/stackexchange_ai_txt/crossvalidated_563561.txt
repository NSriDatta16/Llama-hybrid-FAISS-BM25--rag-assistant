[site]: crossvalidated
[post_id]: 563561
[parent_id]: 563552
[tags]: 
Gibbs sampling is possibly the first MCMC algorithm implemented for mixture models (Gelber, Gelman and Goldhirsch, 1989), inspired from the data augmentation of Tanner and Wong (1987) and ultimately from the EM algorithm. It takes advantage of the latent variable structure in producing nice, low dimension, and natural conditionals that can be simulated quite efficiently, much more than a default MCMC algorithm like random walk Metropolis-Hastings. However, here is a slide from my MCMC course , where I illustrate the potential pitfall of using plain Gibbs for a mixture of two Gaussians with unknown means, namely that it may get trapped in a local mode because the latent variables are practically if not theoretically stuck at a fixed value.
