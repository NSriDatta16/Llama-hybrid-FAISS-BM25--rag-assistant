[site]: crossvalidated
[post_id]: 422634
[parent_id]: 
[tags]: 
Intensity normalization with segmented ROI on black background

I have a set of segmented ROIs .jpgs against a black background. Pixels in the ROI range from 0 to 255 in grayscale (1 channel only). I want to normalize these intensities in order to feed it into a CNN. When I initially just do a simple subtraction by mean and divide by the std, the output ROI image is essentially bright white. Am I doing this correctly? When I instead try to subset the image to only nonzero pixels, find the mean and std, the ouput image is nearly as bright, with tremenedous loss of detail. Here is how I did it: import cv2 import numpy as np from matplotlib import pyplot as plt image = cv2.imread("/media/sf_share/out/slice080.jpg") subset_mean = np.mean(np.where ( image > 0)) subset_std = np.std(image) transformed = (diff_im - subset_mean) / subset_std plt.imshow(transformed, cmap = 'gray', interpolation = 'bicubic') plt.show() Visually, the output image has lost a considerable amount of the detail that makes it unique. I understand that this is an expected output of the transformation, but the details of the images make it important. Any idea what I can do? Am I doing this properly?
