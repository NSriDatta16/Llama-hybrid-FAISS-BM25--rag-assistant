[site]: crossvalidated
[post_id]: 469810
[parent_id]: 
[tags]: 
How to Create a Model Which Can Produce Accurate Predictions?

My code and output are attached below. I create a simulation dataset with only 2 categorical features (each has values of 0 or 1). The labels (0 or 1) for each categorical combination are generated according to Bernoulli trials with a pre-defined success probability. Then I fit the dataset with a logistic regression model. But I found the prediction probability for each combination is not close to the pre-defined success probability (i.e, 0.1 vs 0.076). I tried linear regression and got the similar result. For such a simple dataset, I don't think fitting a non-linear model is necessary. My question is: is there any way to improve the probability prediction using linear models? import numpy as np import pandas as pd from sklearn import linear_model # Create a simulation dataset with 2 features and dummy encoding np.random.seed(2000) n_samples = 1000 p1, p2, p3, p4 = 0.10, 0.30, 0.20, 0.70 data = [] ys = [] # For x1=0, x2=0, generate 1,000 records with 0/1 label via bernouli trial of p=0.1 for i in range(n_samples): y = np.random.binomial(1, p1) ys.append(y) data.append([y,0,0]) avg_y1 = np.mean(ys) print("p1:{:.2f}, avg_y1:{:.3f}".format(p1, avg_y1)) ys = [] # For x1=0, x2=1, generate 1,000 records with 0/1 label via bernouli trial of p=0.3 for i in range(n_samples): y = np.random.binomial(1, p2) ys.append(y) data.append([y,0,1]) avg_y2 = np.mean(ys) print("p2:{:.2f}, avg_y2:{:.3f}".format(p2, avg_y2)) ys = [] # For x1=1, x2=0, generate 1,000 records with 0/1 label via bernouli trial of p=0.2 for i in range(n_samples): y = np.random.binomial(1, p3) ys.append(y) data.append([y, 1,0]) avg_y3 = np.mean(ys) print("p3:{:.2f}, avg_y3:{:.3f}".format(p3, avg_y3)) ys = [] # For x1=1, x2=1, generate 1,000 records with 0/1 label via bernouli trial of p=0.7 for i in range(n_samples): y = np.random.binomial(1, p4) ys.append(y) data.append([y, 1,1]) avg_y4 = np.mean(ys) print("p4:{:.2f}, avg_y4:{:.3f}".format(p4, avg_y4)) d1 = pd.DataFrame(data, columns=['y','x1','x2']) X = d1[['x1','x2']] y = d1.y clf = linear_model.LogisticRegression(fit_intercept=True).fit(X, y) print("p1:{:.2f}, avg_y1:{:.3f}, pred_p1:{:.3f}".format(p1, avg_y1, clf.predict_proba(np.array([[0,0]]))[0][1])) print("p2:{:.2f}, avg_y2:{:.3f}, pred_p2:{:.3f}".format(p2, avg_y2, clf.predict_proba(np.array([[0,1]]))[0][1])) print("p3:{:.2f}, avg_y3:{:.3f}, pred_p3:{:.3f}".format(p3, avg_y3, clf.predict_proba(np.array([[1,0]]))[0][1])) print("p4:{:.2f}, avg_y4:{:.3f}, pred_p4:{:.3f}".format(p4, avg_y4, clf.predict_proba(np.array([[1,1]]))[0][1])) print("coef:{}, intercept:{}".format(clf.coef_, clf.intercept_)) #p1:0.10, avg_y1:0.118, pred_p1:0.076 #p2:0.30, avg_y2:0.289, pred_p2:0.332 #p3:0.20, avg_y3:0.195, pred_p3:0.238 #p4:0.70, avg_y4:0.698, pred_p4:0.653 #coef:[[1.33225542 1.79422588]], intercept:[-2.49334945] ```
