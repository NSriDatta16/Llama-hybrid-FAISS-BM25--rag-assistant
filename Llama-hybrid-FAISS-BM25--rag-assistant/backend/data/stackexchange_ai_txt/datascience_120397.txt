[site]: datascience
[post_id]: 120397
[parent_id]: 81283
[tags]: 
Here is my own proposal below: class Qlearning: def __init__(self, learning_rate, gamma, state_size, action_size): self.state_size = state_size self.action_size = action_size self.learning_rate = learning_rate self.gamma = gamma self.reset_qtable() def update(self, state, action, reward, new_state): """Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]""" delta = ( reward + self.gamma * np.max(self.qtable[new_state, :]) - self.qtable[state, action] ) q_update = self.qtable[state, action] + self.learning_rate * delta return q_update def reset_qtable(self): """Reset the Q-table.""" self.qtable = np.zeros((self.state_size, self.action_size)) And here is a full example to illustrate how it works on a simple RandomWalk1D environment (full notebook is here ): # ## Dependencies from typing import NamedTuple from enum import Enum import numpy as np from numpy.random import default_rng from tqdm import tqdm import matplotlib.pyplot as plt import matplotlib.patches as mpatches import pandas as pd import seaborn as sns sns.set_theme() # ## Parameters class Params(NamedTuple): total_episodes: int # Total episodes learning_rate: float # Learning rate gamma: float # Discounting rate seed: int # Define a seed so that we get reproducible results n_runs: int # Number of runs action_size: int # Number of possible actions state_size: int # Number of possible states epsilon: float # Exploration probability params = Params( total_episodes=50, learning_rate=0.3, gamma=0.95, seed=42, n_runs=100, action_size=None, state_size=None, epsilon=0.1, ) # Set the seed rng = np.random.default_rng(params.seed) # ## The environment class Actions(Enum): Left = 0 Right = 1 class RandomWalk1D: """`RandomWalk1D` to test the Q-learning algorithm. The agent (A) starts in state 3. The actions it can take are going left or right. The episode ends when it reaches state 0 or 6. When it reaches state 0, it gets a reward of -1, when it reaches state 6, it gets a reward of +1. At any other state it gets a reward of zero. Rewards: -1 +1 States: Environment inspired from `ReinforcementLearning.jl`'s tutorial: https://juliareinforcementlearning.org/docs/tutorial/ """ def __init__(self): self.observation_space = np.arange(0, 7) self.action_space = [item.value for item in list(Actions)] self.right_boundary = 6 self.left_boundary = 0 self.reset() def reset(self): self.current_state = 3 return self.current_state def step(self, action): if action == Actions.Left.value: new_state = np.max([self.left_boundary, self.current_state - 1]) elif action == Actions.Right.value: new_state = np.min([self.right_boundary, self.current_state + 1]) else: raise ValueError("Impossible action type") self.current_state = new_state reward = self.reward(self.current_state) is_terminated = self.is_terminated(self.current_state) return new_state, reward, is_terminated def reward(self, observation): reward = 0 if observation == self.right_boundary: reward = 1 elif observation == self.left_boundary: reward = -1 return reward def is_terminated(self, observation): is_terminated = False if observation == self.right_boundary or observation == self.left_boundary: is_terminated = True return is_terminated env = RandomWalk1D() params = params._replace(action_size=len(env.action_space)) params = params._replace(state_size=len(env.observation_space)) print(f"Action size: {params.action_size}") print(f"State size: {params.state_size}") # ## The learning algorithm: Q-learning class Qlearning: def __init__(self, learning_rate, gamma, state_size, action_size): self.state_size = state_size self.action_size = action_size self.learning_rate = learning_rate self.gamma = gamma self.reset_qtable() def update(self, state, action, reward, new_state): """Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]""" delta = ( reward + self.gamma * np.max(self.qtable[new_state, :]) - self.qtable[state, action] ) q_update = self.qtable[state, action] + self.learning_rate * delta return q_update def reset_qtable(self): """Reset the Q-table.""" self.qtable = np.zeros((self.state_size, self.action_size)) # ## The explorer algorithm: epsilon-greedy class EpsilonGreedy: def __init__(self, epsilon, rng=None): self.epsilon = epsilon if rng: self.rng = rng else: self.rng = default_rng() def choose_action(self, action_space, state, qtable): """Choose an action `a` in the current world state (s).""" # First we randomize a number explor_exploit_tradeoff = self.rng.uniform(0, 1) def sample(action_space): return self.rng.choice(action_space) # Exploration if explor_exploit_tradeoff Which will produce the following rewards and number of steps to end the episode plot: And the following plot of the Q-values learned: Also, if that helps, here are three other implementations of Q-learning in Python: From Shangtong Zhang's implementation of Sutton & Barto's book Reinforcement Learning: An Introduction (2nd Edition) : https://github.com/ShangtongZhang/reinforcement-learning-an-introduction/blob/master/chapter06/cliff_walking.py#L128 From Thomas Simonini's Deep Reinforcement Learning Course: https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Q%20learning/FrozenLake/Q%20Learning%20with%20FrozenLake.ipynb From Tim Miller's (University of Melbourne) book Introduction to Reinforcement Learning : https://gibberblot.github.io/rl-notes/index.html From the Dive into Deep Learning book: https://d2l.ai/chapter_reinforcement-learning/qlearning.html#implementation-of-q-learning
