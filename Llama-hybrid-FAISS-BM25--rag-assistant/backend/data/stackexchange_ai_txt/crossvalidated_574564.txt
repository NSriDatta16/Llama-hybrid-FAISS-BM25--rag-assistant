[site]: crossvalidated
[post_id]: 574564
[parent_id]: 
[tags]: 
Comparing impact of training data size - what testing data size?

I am training a classifier using BERT and want to check how the accuracy changes with increasing training data size. Up until now, I have 1k annotated training samples and tested the accuracy for different subsizes of this set (200, 400, 600, 800, 1000) and divided the training and test data with a 80:20 ratio. The problem that occurred to me is that in my case I was always using different testing samples in order to assess the accuracy. However, if I understood correctly, the best approach would be to keep a constant test data set across all subsets of testing. My question now: Is this thinking correct? If yes, would I then choose 20% of the whole dataset (e.g. 1000*0.2 = 200) for all 5 training sizes (200, 400, 600, 800, 1000) when reporting accuracy?
