[site]: crossvalidated
[post_id]: 622208
[parent_id]: 
[tags]: 
Can LSTMs/RNNs' Final Hidden States Serve as Effective Sequence Embeddings for Document or Sentence Analysis?

Can the final hidden state of LSTMs/RNNs be utilized as a sequence embedding for documents or sentences? These embeddings could then be applied to tasks like classification or clustering to leverage the spatial relations between them. The learning process can be self-supervised, for example, by using X: [ ,"I","ate","an","apple","yesterday"] Y: ["I","ate","an","apple","yesterday", ] as training data, and considering the option of using pretrained word2vec embeddings to enhance the quality of sequence embeddings. The loss function used is Cross Entropy. Since separate RNNs will be trained for each sequence, the concern of overfitting is eliminated, allowing the RNN to be trained for a larger number of epochs with the intention of purposely overfitting on the data. One potential concern I observe currently is that due to the random weight initialization, the hidden states may always differ for the same sequence. However, this does not necessarily imply that they will exhibit a lower similarity score (cosine) in higher dimensions.
