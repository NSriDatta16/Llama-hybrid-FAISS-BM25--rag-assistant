[site]: crossvalidated
[post_id]: 491757
[parent_id]: 491709
[tags]: 
Not sure what kind of models you are using exactly, but in deep learning it often happens that the gradients are not defined everywhere (e.g. ReLU activation function). Also, Adagrad , a well-known optimisation strategy in deep learning, explicitly takes subgradients into account.
