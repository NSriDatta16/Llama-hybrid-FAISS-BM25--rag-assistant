[site]: crossvalidated
[post_id]: 330565
[parent_id]: 330559
[tags]: 
It's not that it is necessarily better than $\text{sigmoid}$. In other words, it's not the center of an activation fuction that makes it better. And the idea behind both functions is the same, and they also share a similar "trend". Needless to say that the $\tanh$ function is called a shifted version of the $\text{sigmoid}$ function. The real reason that $\text{tanh}$ is preferred compared to $\text{sigmoid}$, especially when it comes to big data when you are usually struggling to find quickly the local (or global) minimum, is that the derivatives of the $\text{tanh}$ are larger than the derivatives of the $\text{sigmoid}$. In other words, you minimize your cost function faster if you use $\text{tanh}$ as an activation fuction. But why does the hyperbolic tangent have larger derivatives? Just to give you a very simple intuition you may observe the following graph: The fact that the range is between -1 and 1 compared to 0 and 1, makes the function to be more convenient for neural networks. Apart from that, if I use some math, I can prove that: $$\tanh{x} = 2σ(2x)-1$$ And in general, we may prove that in most cases $\Big|\frac{\partial\tanh (x)}{\partial x}\Big| > \Big|\frac{\partial\text{σ} (x)}{\partial x}\Big|$.
