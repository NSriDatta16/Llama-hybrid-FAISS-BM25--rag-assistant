[site]: crossvalidated
[post_id]: 179928
[parent_id]: 179657
[tags]: 
The MaxEnt algorithm heavily favors distributions as close to uniform as possible. Therefore, given the constraint that the average is $4$, it is more optimal to add more mass to $6$ rather than $4$ in order for the posterior to stay close to uniform. Why such tendency? Perhaps it has something to do with the shortest message possible. It's simple to encode distributions close to the uniform. Why? Well, the uniform distribution is at the extreme end of this: instead of encoding $(\frac{1}{6}, \frac{1}{6}, \frac{1}{6}, \frac{1}{6}, \frac{1}{6}, \frac{1}{6})$ one gets away with encoding roughly $[6, \frac{1}{6}]$. I.e. $\frac{1}{6}$ $6$-times. So what MaxEnt does is akin to Occam's razor/minimum message length and the like: through the space of all possible explanations (posteriors) it finds the simplest one which explains the data. And then it tells you that the simplest one is most likely: so if I were to encode each posterior as a binary string, the entropy of each string could itself be renormalised as some probability measure. The shortest string is then just the mode of such entropy-probability-measure on all strings. My intuition called for $4$ to have more mass but perhaps it was just a fallacy of my own intuition: given the above reasoning I find it plausible that guessing $6$ is the right thing to do if I am rewarded only when I guess a toss correctly. On the other hand, it seems that conditioning on the most probable hypothesis is short-sighted: it ignores all other gains to be made from other hypotheses which are not likely, but plausible. Therefore a very important question arises: when conditioning on the mode of the entropy-probability-measure is equivalent to taking the expectation with respect to entropy-probability-measure? I suspect that MaxEnt distribution is simply the expectation of this entropy-probability-measure, so always. As for the loss functions which would force me to guess $4$, it is a well-known result that if my loss-function is a mean-squared error, I should guess the expectation of the posterior, which is, of course, $4$. I am yet to connect this with MLE's... So to be continued/edited.
