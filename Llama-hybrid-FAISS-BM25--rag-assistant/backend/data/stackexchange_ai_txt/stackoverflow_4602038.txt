[site]: stackoverflow
[post_id]: 4602038
[parent_id]: 
[tags]: 
Creating a unique list from dataset too big to fit in memory

I have a list of 120 million records of around 40/50 bytes each which is about 5.5/6 gigabytes of raw memory space not including any extra storage required to keep an array in memory. I'd like to make sure this list is unique. The way I have tried to do it is create a Hashset and add all the entries to it one by one. When I get to about 33 million records I'm out of memory and the list creation slows to a crawl. Is there a better way to sort this massive list of entries in a timely manner? The only solution I can think of is using an Amazon EC2 High-Memory Quadruple Extra Large Instance for an hour. Thanks
