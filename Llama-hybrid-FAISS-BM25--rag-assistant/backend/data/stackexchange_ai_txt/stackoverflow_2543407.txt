[site]: stackoverflow
[post_id]: 2543407
[parent_id]: 2543325
[tags]: 
OK, I'll go with the brute force DIY approach (however it's possible there's already some test module with file checking API - I just never bumped into one as flexible/generic as what we needed and wrote ourselves and never felt the compelling need to search deeper :). I'll describe a fairly generic testing setup, you may want/need only very specific file testing aspects of it. What we do in this case is literally what your functional spec above states, as part of overall testing framework: Have a testing library with two methods (among others) - test_file_identical() and test_grep_file() . If you need help writing those two, please drop a comment and i'll offer some hints (we use different comparators, including a combination of -e , comparisons of various stat attributes, comparing content strings of tested file vs. benchmark file obtained via File::Slurp and doing grep of the file, line by line or via slurped contents for small files, including comparison of massaged grep results to a benchmark file. Have your tests cases organized into sub-directories (or tarballs), one per test, and each test consists of 2 directories - input files and expected output files. Have the test engine script loop over test cases (which for us are meta-described either by a Perl datastructure or better yet, an XML file so business analysts can fiddle with them if needed). If a test case specifies that the test needs to match (exactly or via grep), the test engine finds appropriate files (either hard-coded names, or via name pattern specified in test case), apply those file testing methods mentioned in the first bullet point
