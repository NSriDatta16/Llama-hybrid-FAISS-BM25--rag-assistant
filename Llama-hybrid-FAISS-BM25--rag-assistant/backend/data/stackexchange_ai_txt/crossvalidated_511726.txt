[site]: crossvalidated
[post_id]: 511726
[parent_id]: 
[tags]: 
Different usage of the term "Bias" in stats/machine learning

I think I've seen about 4 different usages of the word "bias" in stats/ML, and all these usages seem to be non-related. I just wanted to get clarification that the usages are indeed non-related. Here are the 4 I've seen: (1) "Bias"-variance tradeoff: Here, bias is used to characterize the model error due to simplified assumptions of the model, e.g., using linear regression when linear regression is not complex enough to capture the trends of the data. (2) "Bias" of an estimator: the difference between the expectation of an estimator and its true value. (3) "Bias" as in the offset term: e.g., in simple linear regression with $y = \hat{\beta}_1x + \hat{\beta}_0$ , I sometimes see the offset term, $\hat{\beta}_0$ , referred to as the "bias" term. e.g., in Role of the bias term in regression (4) "Biased" data: I think this might be similar to (1), but here it's referring to the data and not the model. I typically see this in experimental studies where the experiment was conducted in a setting that captures a limited portion of the trend. e.g., you could have some response that's sinusoidal, $e.g., y = \sin(x)$ , and the experiment was only performed for $x \in [0, \pi/2]$ , which is a monotonically increasing period. I think these are the 4 usages that I've seen, but if there are others, please do share. The usage of "bias" in these 4 settings are un-related, correct? It's a little confusing to see the same term show up many times in different settings in the same field.
