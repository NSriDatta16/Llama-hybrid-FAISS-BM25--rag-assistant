[site]: crossvalidated
[post_id]: 307036
[parent_id]: 
[tags]: 
How important is basis expansion for deep nets?

If deep neural nets are considered to be universal function approximators, is basis expansion really necessary? Or would this be case-specific? For example, if one has three quantitative X variables, would there be any advantage in expanding the number of variables by introducing interactions, polynomials, etc.? This seems to have good utility in e.g. RFs and SVM, but I'm unsure of whether this would be a good strategy for neural nets. If this is perhaps too broad or vague, could someone point me to some pertinent information on basis expansion and feature engineering in the context of deep nets?
