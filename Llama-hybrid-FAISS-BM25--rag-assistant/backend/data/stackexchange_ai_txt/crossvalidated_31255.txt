[site]: crossvalidated
[post_id]: 31255
[parent_id]: 
[tags]: 
Absolute error loss minimization

From Robert (The Bayesian Choice, 2001) , it is proposed that the Bayes Estimator associated with the prior distribution $\pi$ and the multilinear loss is a $(k_2/(k_1+k_2))$ fractile of $\pi(\theta|x)$. The proof follows that $$ E^\pi[L_{k_1,k_2}(\theta,d|x)] = k_1\int\limits_{-\infty}^{d} (d-\theta)\pi(\theta|x)d\theta + k_2\int\limits_{d}^{+\infty}(\theta-d)\pi(\theta|x)d\theta $$ Then, using the identity $$ \int\limits_{c c) $$ I would get to $$ E^\pi[L_{k_1,k_2}(\theta,d|x)] = k_2P^\pi(\theta>d|x) - k_1P^\pi(\theta But he gets to $$ E^\pi[L_{k_1,k_2}(\theta,d|x)] = k_1\int\limits_{-\infty}^{d} P^\pi(\theta y|x)dy $$ And then takes the derivative in $d$. What am I'm missing, and why he does this last step? Thanks!
