[site]: crossvalidated
[post_id]: 44535
[parent_id]: 44518
[tags]: 
The advantage (and disadvantage ) of k -NN is that it keeps all training data. If your classes are strongly overlapping and "smeared" out, parametric models (SVM, Bayes etc) can have trouble remembering local groupings, as by their nature they summarize information in some way. With SVMs, sometimes the right choices of kernel and margin can circumvent this problem. The price you pay when using k -NN in general is two things: If you have a lot of data, prediction becomes costly. Basically, all training data might be involved in decision making (which gives the advantage above). If you have only a very moderate number of dimensions ( However, if the dimensionality is much higher, any index usually becomes useless (because the union of per-dimension matches becomes too large, resulting in long scans). For categorical attributes, inverted lists can still help you here. Also, as Kyle pointed out, the distance measure becomes crucial, and sometimes no useful distance measure will be available (cf. the curse of dimensionality ). Most parametric classifiers do not have this problem. Because your dimensionality is very low, k -NN might just be the right choice for you.
