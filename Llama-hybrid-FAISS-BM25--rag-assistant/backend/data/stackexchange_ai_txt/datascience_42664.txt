[site]: datascience
[post_id]: 42664
[parent_id]: 
[tags]: 
Calibrate the predicted class probability to make it represent a true probability?

Let's say that we have a simple binary classification model (a neural network -- NN) for classifying input images as "dog" ( $y=1$ ) or "not dog" ( $y=0$ ). Let's assume that the NN has one "sigmoid output" $\hat{y}$ . After training, we feed an image of a dog and the output of the NN is e.g. $\hat{y} = 0.8$ . As I understand it, this value of "0.8" is the probability ("confidence") that the particular image is a "dog", according to the model, and the prediction is that "it's a dog" (because 0.8 > 0.5). But is the $\hat{y}$ really a (true) probability? What I mean is: if we feed the NN with many different images, and say for 100 of them the model gives the same $\hat{y} = 0.8$ . For this $\hat{y}$ to be a true probability, I would expect 80 of these images to actually be dogs and 20 not dogs. To check this we would look at the $y$ (assuming that they exist). In the case when all the 100 images are actually dogs, our model made correct prediction (since it correctly labeled all of them as dogs, because 0.8 > 0.5 for all of the 100 images), but the $\hat{y}$ is not a true probability prediction, because it should be 1. This is actually the accuracy, $100/100 = 1$ . On the other hand, if out of 100 images 80 are actually dogs and 20 are not dogs, then the model prediction has accuracy $80/100 = 0.8$ , because it incorrectly classified all 100 images as dogs, again, but only 80 are actually dogs. However now the $\hat{y} = 0.8$ represents the true probability. In other cases, the accuracy can be e.g. 0.9, meaning that the predicted $\hat{y} = 0.8$ is not a correct probability, but is closer to it than in the first case. So, first of all, is what I am saying correct? Second, does the " calibration " fix this issue, i.e. making $\hat{y}$ = accuracy allows the $\hat{y}$ to be interpreted as true probability?
