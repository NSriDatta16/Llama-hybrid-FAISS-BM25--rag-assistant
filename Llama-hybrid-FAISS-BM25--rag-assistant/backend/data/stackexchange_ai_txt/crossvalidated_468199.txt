[site]: crossvalidated
[post_id]: 468199
[parent_id]: 447353
[tags]: 
This is a Bayesian regression problem where you have imposed a prior on the regression coefficients and you are seeking to maximise the conditional expectation function $\mathbb{E}(Y|X=x)$ . You are using a polynomial regression model conditional on the parameters $\beta_1$ and $\beta_2$ , with the true regression function: $$\mathbb{E}(Y|X=x, \beta_1, \beta_2) = \beta_1 x + \beta_2 x^2.$$ You can find the conditional expectation function explicitly using the law of iterated expectation . $$\begin{aligned} \mathbb{E}(Y|X=x) &= \mathbb{E} \Big( \mathbb{E}(Y|X=x, \beta_1, \beta_2) \Big) \\[6pt] &= \mathbb{E} ( \beta_1 x + \beta_2 x^2 ) \\[6pt] &= x \cdot \mathbb{E}(\beta_1) + x^2 \cdot \mathbb{E}(\beta_2) \\[6pt] &= x \cdot 0 + x^2 \cdot 0 \\[6pt] &= 0. \\[6pt] \end{aligned}$$ Since both of your priors have zero expectation, we can see that $\mathbb{E}(Y|X=x) = 0$ for all $x \in \mathbb{R}$ . This means that any value maximises the function.
