[site]: datascience
[post_id]: 32034
[parent_id]: 32032
[tags]: 
from sklearn.metrics import recall_score If you then call recall_score.__dir__ (or directly read the docs here ) you'll see that recall is The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives If you go down to where they define micro , it says 'micro' : Calculate metrics globally by counting the total true positives, false negatives and false positives Here, the true positives are $2$ (the sum of the terms on the diagonal, also known as the trace), while the sum of the false negatives plus the false positives (the off-diagonal terms) is $3$ . As $2/5=.4$ , the recall (using the micro argument for average ) is indeed $.4$ . Note that, using micro , precision and recall are the same. The following, in fact, returns nothing: from numpy import random from sklearn.metrics import recall_score, precision_score for i in range(100): y_pred = random.randint(0, 3, 5) y_true = random.randint(0, 3, 5) if recall_score(y_pred, y_true, average='micro') != precision_score(y_pred, y_true, average='micro'): print(i)
