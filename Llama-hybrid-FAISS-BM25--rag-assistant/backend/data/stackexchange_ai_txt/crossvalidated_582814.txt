[site]: crossvalidated
[post_id]: 582814
[parent_id]: 
[tags]: 
Non-negative weights in Logistic Regression

I am working on a credit scorecard model based on Logistic Regression, with output being the odds of default. There are multiple variables used, all categorical in nature. Even if there are numerical variables, we use binning to convert it to a categorical variable. The final output is later scaled to arrive at a figure which is easily understandable. Now, with that transformation, we calculate individual sub-scores for each variable then add all of them to get the credit score for an observation. To make sure that the individual sub-scores are all non-negative, I have been told to adjust all the weights of the Logistic Regression model based on the following logic: Say there are three categories for a variable, each category will get assigned certain weights, say b1, b2, b3 , we then transform each of the weights by subtracting the minimum of the three weights, so b1 will be changed to b1-minimum(b1, b2, b3) . So my question is this correct way to make sure that all the sub-scores are positive? Edit: I am giving an example of sub-score calculation for one variable called TENOR Variable Category Coefficient Adjusted Coefficient Score TENOR (35, Inf] -0.568920702 0 62 TENOR (-Inf,35] 0 0.568920702 55 The Score is derived as the sum of an offset value and ( multiplier * Adjusted Coefficient ). The values of multiplier and the offset depends on how we would like the scores to be interpreted as, for example, a decrease of 15 in score should double the odds of default.
