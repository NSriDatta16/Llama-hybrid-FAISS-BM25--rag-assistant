[site]: datascience
[post_id]: 96436
[parent_id]: 
[tags]: 
Should I restandardize training data during retraining?

I am running a simple keras deep learning which i will train once and then retrain every month as new data becomes available. My data is made up of monetary values so i will first standardize my data using StandardScaler() however once new data comes in and i want to retrain can i use the same StandardScaler object? Because lets assume that the new data has a maximum datapoint higher than my current maximum and will thus alter the entire dataset standardization. Should i re-standardize or can i use the same standardization for the new data?
