[site]: crossvalidated
[post_id]: 469931
[parent_id]: 469924
[tags]: 
I find it hard to tell, what exactly the question is. "Deep Learning" and "Machine Learning" are not disjunct terms. Deep Learning is a special case of Neural Network learning which is a special case of Machine Learning. As Ryan Volpi explained, different parts of Machine Learning lend differently to parallelisation. Neural Networks often perform well on GPUs and with them Deep Learning. A different question is the meaning of n-jobs in Python's scikit-learn and whether that addresses GPUs. We find the answer in the scikit-learn FAQ on https://scikit-learn.org/dev/faq.html : FAQ: Will you add GPU support? Answer: No, or at least not in the near future. The main reason is that GPU support will introduce many software dependencies and introduce platform specific issues. scikit-learn is designed to be easy to install on a wide variety of platforms. Outside of neural networks, GPUs donâ€™t play a large role in machine learning today, and much larger gains in speed can often be achieved by a careful choice of algorithms. And then there is the question of "Is is necessary to use CUDA?". CUDA is a parallel computing platform and programming model developed by NVIDIA for general computing on GPUs. So at this time CUDA is a requirement to use NVIDIA GPUs for machine learning/neural networks but often you do not have to use CUDA yourself but rather software that makes use of it for you.
