[site]: crossvalidated
[post_id]: 282064
[parent_id]: 282041
[tags]: 
Averaging your data can only hurt in this case. Although you could average the samples across each day, this won't provide any benefit. A standard linear regression model aggregates the data automatically. Averaging the samples for each day would decrease the variance but would also reduce the total number of observations, resulting in no net gain in power. Furthermore, averaging time points might actually lose data . I've wondered about this before myself so I made a toy example in R, and simulated data similar to your problem's setup. Example code noise = 2 sim1 Method 1: Model each data point separately. We get a power (true positive rate) of 0.54882, and a false positive rate of 0.04961 as desired. Method 2: Average points for each day. We get a power (true positive rate) of 0.25541, and a false positive rate of 0.05038 as desired. Method 1 significantly outperforms method 2 on these simulated data. Maybe someone else can give a more rigorous mathematical explanation of why this is true. You can probably exclude the time of day from your regressors, as long as it's not interacting with any of the other variables to produce effects that are concealed right now. However, it will be impossible to explain more of the variance by excluding the time of day; removing variables can only decrease a model's performance on the data used to train it. Hope this helps :)
