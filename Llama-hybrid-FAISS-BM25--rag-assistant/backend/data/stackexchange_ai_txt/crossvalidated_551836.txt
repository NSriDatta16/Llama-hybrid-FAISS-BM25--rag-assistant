[site]: crossvalidated
[post_id]: 551836
[parent_id]: 381045
[tags]: 
The answer to the first question above is satisfactory, so I will elaborate on the second question. (a) Yes, this is possible by choosing data-independent priors $P_1$ to $P_K$ , letting each bound hold with probability $1-\delta/K$ , and then applying the union bound, so all of the bounds hold simultaneously with probability $1-\delta$ , adding a term $\log(K)$ from the $\log\delta$ term. It is also possible to do this using a prior which is a mixture distribution, see e.g. Theorem A.1 from [1]. However, it is still crucial that each $P_k$ is independent of the data. (b) If we are using a different set of data than we use to evaluate the bound, this is fine. This could either be drawn from the same distribution, or a different (but ideally similar) one. See e.g. [2] amongst many other works. I also mention further work in e.g. [3] which uses a differentially-private mechanism to learn learn a prior from the data, adding an extra term to the bound. This is based on the idea that a differentially-private prior is "nearly" independent of the data. [1] Wenda Zhou, Victor Veitch, Morgane Austern, Ryan P. Adams, Peter Orbanz: Non-vacuous Generalization Bounds at the ImageNet Scale: a PAC-Bayesian Compression Approach. ICLR (Poster) 2019. [2] John Shawe-Taylor, Emilio Parrado-Hern√°ndez, Amiran Ambroladze: Data Dependent Priors in PAC-Bayes Bounds. COMPSTAT 2010: 231-240 [3] Gintare Karolina Dziugaite, Daniel M. Roy: Data-dependent PAC-Bayes priors via differential privacy. NeurIPS 2018: 8440-8450
