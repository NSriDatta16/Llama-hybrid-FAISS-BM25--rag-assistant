[site]: crossvalidated
[post_id]: 523689
[parent_id]: 
[tags]: 
Time series linear regression: does the violation of strict exogeneity imply correlatedness?

If we have the linear regression model $$y_t = \mathbf{x}^\prime_t \mathbf{\beta} + u_t$$ for time series data, for which we know that the strict exogeneity assumption is violated, i.e., we know that $$\mathbb{E}(u_t \mid X) \not= 0,$$ then we have that the OLS estimator is biased because $$\mathbf{\hat{\beta}_{OLS}} = (X'X)^{-1}X'\mathbf{y} = \mathbf{\beta} + (X'X)^{-1}X'\mathbf{u} = \mathbf{\beta} + \left(\sum_{t=1}^n\mathbf{x_t}\mathbf{x_t'}\right)^{-1}\sum_{t=1}^n\mathbf{x_t}u_t$$ \begin{equation*} \begin{split} \mathbb{E}\left(\mathbf{\hat{\beta}_{OLS}}\right) &= \mathbf{\beta} + \mathbb{E}\left(\left(\sum_{t=1}^n\mathbf{x_t}\mathbf{x_t'}\right)^{-1}\sum_{t=1}^n\mathbf{x_t}u_t\right) \\ &\neq \mathbf{\beta} + \mathbb{E}\left(\left(\sum_{t=1}^n\mathbf{x_t}\mathbf{x_t'}\right)^{-1}\right)\mathbb{E}\left(\sum_{t=1}^n\mathbf{x_t}u_t\right) \\ &= \mathbf{\beta} \end{split} \end{equation*} Where there is an inequality because $\left(\sum_{t=1}^n\mathbf{x_t}\mathbf{x_t'}\right)^{-1}$ and $\sum_{t=1}^n\mathbf{x_t}u_t$ are not independent. I wonder how this indepence is a result of the violation of strict exogeneity. I understand that correlatedness between these two terms implies violation of strict exogeneity but can anyone explain to me why the opposite is also true? So, why does the violation of strict exogeneity imply correlatedness of these two terms?
