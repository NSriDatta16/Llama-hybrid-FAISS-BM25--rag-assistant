[site]: datascience
[post_id]: 51991
[parent_id]: 51989
[tags]: 
CNNs require a large number of images to be trained properly. The most common workaround when having a small training set is to use a pre-trained model , which has been trained on a huge dataset (e.g. ImageNet). Through this you take a highly effective model, drop its final layers (which are used for classification) and keep all the previous (which are used for feature extraction). Then you add your own final layers and only train them . This allows you to train a model with much less parameters (because most of them have already been trained), which can be done on smaller datasets. Another thing you can do is data augmentation. This usually consists of affine transformations (rotations, translations, scaling, shifting, etc.), flipping the image, brightness/contrast adjustments, noise induction, etc. The problem with this is that strong augmentations might ruin the image . This limits you in terms of how many and how strong augmentors you use. That being said, in terms of image classification a tiny dataset could be considered one with a few hundred samples. If you have just one image per class, I'm not sure that you can ever effectively train a CNN without overfitting. My suggestion is, if possible, obtain more data .
