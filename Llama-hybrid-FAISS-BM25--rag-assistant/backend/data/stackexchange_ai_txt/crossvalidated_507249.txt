[site]: crossvalidated
[post_id]: 507249
[parent_id]: 
[tags]: 
JS-divergence vs KL-divergence as objective for GAN?

I would love to get some help with that interview question I failed to answer correctly: In what sense is the JS-divergence prefferable over KL-divergence $D_{KL}(p_{data}||p_{model})$ as an objective for GANs? Hint: Analyze the case of a non-zero $p_{data}(x)>0$ , with near-zero small $p_{model}(x)$
