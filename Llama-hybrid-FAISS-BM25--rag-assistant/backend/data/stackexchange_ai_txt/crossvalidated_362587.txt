[site]: crossvalidated
[post_id]: 362587
[parent_id]: 362573
[tags]: 
I'm not sure because I don't have access to Comprehensive Meta-Analysis (CMA) , which is what I'm guessing you're referring to by "The comprehensive meta-analysis", but, guessing: if you enter standard errors, then you're assuming the standard error for each study is known exactly if you enter sample sizes, then you're assuming the standard errors are not known, but are proportional to $1/\sqrt{n}$ for each study; the baseline (expected standard error for a hypothetical study with $n=1$ (!)) is estimated. As Wolfgang Viechtbauer says in this answer : the [sample size approach] will assume that the sampling variances are known up to a proportionality constant (what is estimated as the residual variance by the function). That is different than what we typically want to do in meta-analyses (where the sampling variances are pretty much known). WV considers the issue more generally here and here . My answer would be: if in doubt, use the "known standard errors" approach , because (1) it's more standard/accepted by meta-analysis experts such as WV and (2) if the standard errors are estimated reliably, it will use more of the information that's known about the studies. However, I would consider the "sample size weighted" approach if: (1) I didn't have standard errors or (2) I had reason to be skeptical about the accuracy of the SE estimates; for example, if your studies are super-small/noisy, then assuming that their standard errors are proportional to $1/\sqrt{n}$ might actually be more accurate (because you get to use the whole data set to estimate the proportionality constant).
