[site]: crossvalidated
[post_id]: 327188
[parent_id]: 282030
[tags]: 
Topology is not good enough. You may try with more hidden fully-connected layers, but what you really should do is to use a DCGAN, i.e., a GAN that uses convolutional layers. Try with at least 3 or 4 convolutional layers in both D and G. You need to know how CNNs work, though, but that's the way to go. MNIST is composed of images that are a lot more simple than images in your data set. You don't even have to convert them to gray-scale, but you may, of course. You should use batch normalization, but not on the first conv layer in D, and not on logits in G. Dropout is not necessary, but you may experiment with it. Also, use Leaky ReLU instead of plain ReLU in GANs. You can select 0.2 as slope in the negative region. Training 1:1 should be fine. Batch size can be larger, like 32, 64, 128, but 10 is okay, too. That's not the problem. For Adam optimizer, try with learning rate of 0.0002, and beta1 of 0.5. When calculating D loss for real images, instead of 1.0 for labels, feed it with 0.9. You can always train for more epochs, if it helps. Data set size may be a problem, but try and see what you get. Check out: Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks My project on GitHub that's similar to yours My project contains what you need, but try to solve this problem on your own first. If you aren't familiar with CNNs, here's something to get you started: http://cs231n.github.io/convolutional-networks/ https://en.wikipedia.org/wiki/Convolutional_neural_network You can tell us how it went. ;)
