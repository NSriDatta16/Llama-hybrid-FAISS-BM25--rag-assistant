[site]: crossvalidated
[post_id]: 488388
[parent_id]: 
[tags]: 
Rule of thumb for Data Requirements when Designing a Neural Network for Deep Learning

I'm designing an MLP classifier and I've been noticing that: Using a very shallow network, or one whose at least one layer has a small number of neurons yields bad performance Using a deep network each with possibly a number of neurons more than thirty times the number of features in the data provides no apparent advantage over much simpler models. While I've commonly heard that Deep Learning requires a large training data set, I'm failing to find research papers to properly quote on this. Even further, I'm failing to find any rule of thumb for how much data should be required to have a large enough training set given a proposed topology or a rule for how deep and wide could a network be given the amount of data available. Is there any?
