[site]: datascience
[post_id]: 74118
[parent_id]: 
[tags]: 
How can I improve my model on a very very small dataset?

I am starting as a PhD student and we want to find appropriate materials (with certain qualities) from basic chemical properties like charge, etc. There are a lot of models and datasets in similar works, but since our work is pretty novel, we have to make and test each data sample ourselves. This makes the data acquisition very very slow and very expensive. Our estimated samples will be 10-15 samples for some time, until we can expand it. Now I want to use this samples to make a basic predictive model, but with as much 'good generalization' as possible. I will use this model to screen other possible candidates from a large pool of properties to find best probable materials, and will then proceed to make them for testing. Now I clearly don't expect performances anywhere near 95% or so, but I want a working model with predictive capability that will actually help me to find some of the best probable material candidates, so we can expand our work. I am uncertain if I can (or rather should) use some of regular ML methods like dataset splitting and cross-validation. So I would appreciate your thoughts. Since our data size is minuscule, I have been searching for ways to improve its robustness. These our my ideas: 1- Use an ensemble model to avoid overfitting and avoid skewed biases (using algos like elasticnet, SVM, random forests, etc). 2- Setting heavy regularization to avoid certain biases that can arise from small data. 3- Using algos that arrive at the minimum periphery faster. I would appreciate any suggestions on how I can improve this model as much as possible, to reach the best generalization performance. I have also thought about synthetic data generation a lot. Do you have any suggestions on how I can go about it?
