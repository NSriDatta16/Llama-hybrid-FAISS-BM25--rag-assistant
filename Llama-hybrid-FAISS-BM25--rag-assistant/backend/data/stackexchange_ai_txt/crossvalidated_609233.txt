[site]: crossvalidated
[post_id]: 609233
[parent_id]: 312877
[tags]: 
Did you try Bayesian Neural Network ? This can be built based on the TensorFlow Probability library. Here I found a blog post that describes how to do it with CNN. BNN can consider two types of uncertainties, namely: Epistemic Uncertainty and Aleatoric Uncertainty. Epistemic Uncertainty: This type of uncertainty occurs when your model lacks the necessary training data. So, the parameter estimation cannot be determined super confidently. Adding more data reduces this type of uncertainty. Aleatoric Uncertainty: This type of uncertainty is introduced directly from the training data. What if your data itself is noisy? It can be due to labeling errors, or the data can naturally contain uncertainty, for example: when you are working with sensor data, for the same input, different outputs can be generated naturally due to some imperfection. No matter how many data samples you add, you are still in this type of uncertainty. So we have to make the model aware of this as well. According to Wikipedia : In mathematics, uncertainty is often characterized in terms of a probability distribution. From that perspective, epistemic uncertainty means not being certain what the relevant probability distribution is, and aleatoric uncertainty means not being certain what a random sample drawn from a probability distribution will be. With the help of BNN, you can make your model aware of both of them, such that it knows when it doesn't know and doesn't give any wrong prediction with embarrassingly high confidence. But like any other system, it is not perfect. Following are the fallbacks I experienced while working with it: You should not keep an extremely high hope. But at least you can hope for a better uncertainty estimation than a regular Non-Bayesian Neural Network. It is slower than its Non-Bayesian counterpart. I worked on text classification with this type of network. What I found, this network is good at preventing high confidence for completely nonsensical utterings but it struggles if you intentionally generate some adversarial inputs like, for text classification, if you query the model with a valid but incomplete sentence (which as a human we understand that it is also an OOD, but BNN sometimes fails in this case). Some say you can approximate BNN with Dropout.
