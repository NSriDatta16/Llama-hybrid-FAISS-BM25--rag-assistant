[site]: crossvalidated
[post_id]: 327583
[parent_id]: 
[tags]: 
Regression models: Speed versus accuracy

This is probably a broad question, but I am interested to know if there are any studies that throw some light on this question: If I have to train regression models (in the least-squared sense), how do various modeling techniques (e.g., OLS, L1/L2 regularised least-squares, neural networks, etc.) compare with respect to each other from a computational speed vs model quality (some measure of out-of-sample prediction error)? Put another way, for a given data set, CPU resources, how much additional processing time would I need to train a broad class of neural networks to get the same/better performance as a simple OLS/L2 regularised regression?
