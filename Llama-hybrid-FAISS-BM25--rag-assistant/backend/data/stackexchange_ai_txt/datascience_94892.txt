[site]: datascience
[post_id]: 94892
[parent_id]: 92382
[tags]: 
To add what @noe has written which is very true, Complexity of KNN always depends on 1) number of dimension 2) the distance function If you have many many number of dimensions one can use a dimension reduction algorithm and perform the KNN on top. The dimension reduction can be something very simple for example a PCA where you do KNN on first top N components or something more complex as an auto-encoder where there are many embedding layers. If you are thinking about the real-time scenario the de-coding also will add into the running time so you have see which combination of a dimension-reduction + knn gives you the best performance. You talked about "production" - well, bare in mind in production you won't train a model. That means you are supposed to have a trained model (all distances among the training point) (not an N*N-1/2) and just calculate the distance between "test/incoming" observation with you training subjects. So it will be N times calculation if you have N training points. Of course I also recommend Dask which has a built-in KNN implementation e.g. here and here .
