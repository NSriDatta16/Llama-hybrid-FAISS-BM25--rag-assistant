[site]: datascience
[post_id]: 85643
[parent_id]: 69041
[tags]: 
To answer your specific questions: AdditiveAttention() and Attention() layers, are (loosely but not exactly) based on Bahdanau and Luong's attentions, respectively. They use post-2018 semantics of queries, values and keys. To map the semantics to the Bahdanau or Luong's paper, you can consider the 'query' to be the last decoder hidden state. The 'values' will be the set of the encoder outputs - all the hidden states of the encoder. The 'query' 'attends' to all the 'values' If you run thru' the library code you will see that the query is first expanded over the time axis and then there is a dense layer that determines the weights of w1, w2. These weights are applied to the expanded query and the values and then they are added and another weight 'v' is finally applied. A softmax of this is taken to return the attention weights and then these attention weights are multiplied with the 'values' and added to return the context. This is Bahdanau's additive logic However, while analysing tf.keras.layers.Attention Github code to better understand how to use the same, the first line I could come across was - "This class is suitable for Dense or CNN networks, and not for RNN networks". Since you are using RNN, I would use this layer with caution. In general, all these ready-to-use layers are mostly for self-attention and if you want to create a transformer like model where you are doing away with the RNN altogether and want too use only attention to represent the sequence, you can consider these classes. If you still want to use the same, you can go ahead and try the following: ##Input 1 = the last decoder hidden state: stminus1 ##Input 2 = All hidden states of the encoder: lstm_out ##Apply Bahdanau additive attention and give me the ##output = context context = tf.keras.layers.AdditiveAttention()([stminus1, lstm_out]) You can now use the context additionally to strengthen the predictions. However I would strongly recommend you write your own attention layer in less than half a dozen lines of code. See for e.g.: https://stackoverflow.com/questions/63060083/create-an-lstm-layer-with-attention-in-keras-for-multi-label-text-classification/64853996#64853996
