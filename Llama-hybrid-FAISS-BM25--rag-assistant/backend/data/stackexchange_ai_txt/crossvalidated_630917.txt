[site]: crossvalidated
[post_id]: 630917
[parent_id]: 
[tags]: 
Best practices when training an xgboost model as part of a larger model?

I would like to train a model end-to-end that uses the output from an xgboost model as an input. I've successfully implemented full-batch gradient descent into my pipeline with jax, following this guide: JAX vs PyTorch: Automatic Differentiation for XGBoost . However, I am training the rest of my model with stochastic gradient descent (ideally with Adam, if that matters). What are the best practices for end-to-end training here? Should I be computing xgboost outputs for every training example, then storing the gradient/Hessian until the end of each epoch and passing that back to xgboost? Or is there a principled way of updating the xgboost model along with everything else on each SGD step?
