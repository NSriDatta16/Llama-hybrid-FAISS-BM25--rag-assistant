[site]: datascience
[post_id]: 41288
[parent_id]: 37234
[tags]: 
Do I understand correctly that a traditional binary approach to counting numbers is exactly what the Multi-Hot is? We can imagine a byte as a vector of 8 components, and each entry is either 0 or 1.0 I strongly believe so. This means I can't use it for 255 distinct categories (or for relatively unrelated categories) Is the effect as bad in Multi-Hot encoding, in particular in a binary approach? I found a pretty interesting answer. It seems that binary can actually be used with classification tasks really well! Have a look at the table on the last page of paper "A Comparative Study of Categorical Variable Encoding Techniques for Neural Network Classifiers" 2017 This would mean we can save the number of input neurons BY AN INCREDIBLE AMOUNT, instead of using traditional one-hot encoding. Personally, I can get it intuitively: Label-encoding tells us to set a neuron with different values: 1,2,3,4... It's really easy for the network to linearly-interpolate from 1 to 2 and from 2 to 3, by using fractions. Thus, there is a really strong precidence between such input values, and the network will easily pick up on that. So we can't use Label-encoding for categories. Contrary to that, Binary encoding exhibits a more "integer-like" behavior. In other words, it's not as blatantly evident how to linearly interpolate from 1 to 2 in binary form (from 0001 to 0010), which resembles one-hot approach too :)
