[site]: crossvalidated
[post_id]: 541806
[parent_id]: 539351
[tags]: 
Let us think about what you are attempting to ask. If we define $X=x\in\chi, \chi$ being the sample space, as observed, then in Bayesian thought, it is a constant. It is an observable. Instead of using the language of parameters and data, we can think in terms of observables and unobservables. There is no randomness here. $\theta$ is normally a random variable in the parameter space, but it is now a constant. It is crucial that we know how it became so. It appears from the language of your posting that we are conditioning on it in the likelihood function so that, for our purposes, the likelihood is now $.82^1$ . So it is not a random variable either. So it is senseless to talk about a probability when everything is a constant. It would be like discussing the probability that $2+2=4$ . It isn’t impossible to discuss this, but it is difficult for several reasons. First, the interpretation can change depending on the axioms used to derive Bayes rule. For example, if we are conditioning on $\theta=.82$ , then de Finetti’s axioms would require a prior with mass only on .82. What if we were using some other axiomatization and the mass of the prior was zero at the point we conditioned it? Cox’s axioms would find that problematic as well. Savage’s might not if we allowed for time inconsistency, though why you would change your mind on the prior and not the likelihood is beyond me. We also need a better definition of what a constant is. For example, conditioning some parameters on constants is not that unusual in Bayesian thinking. Sometimes you do know one of them. There is another case, though, that wrenches up even the Frequentist toolset. To give an example, the speed of light is known precisely. However, as distance is now normed against the speed of light, distance is uncertain. We used to measure the speed of light with uncertainty; we now measure distance with uncertainty. Let us imagine we get out our carefully built scientific equipment and decide to measure out five kilometers for our morning run. Our equipment is accurate to within plus or minus twenty meters. When our device measures five kilometers, we know it is somewhere within 4980 and 5020 meters in reality. It is close enough. If this is part of our measuring, we could condition on it being five kilometers as it is close enough for our purposes. It is also definitely wrong. Because distance is a value in the real numbers, the probability that our actual distance is five kilometers when it registers five kilometers is a measure zero event. Our conditioning is wrong with certainty. A second issue with this type of conditioning problem is a non-mathematical issue. If, instead, we were running a wrecking ball and hit our intended building, plus or minus twenty meters, we could be hitting the wrong building. At the same time, we have conditioned our uncertainty away. Had our wrecking ball been run by a robot, a la E.T. Jaynes, we would have no way to know our decision process was bad. On the surface, you may think that would not matter, but de Finetti’s coherence wrecks that idea if we are gambling money. A bad constant could create a Dutch Book. As I see it, there is no randomness in your problem. We observed the outcome; it is a certainty. We observed the parameter. It is being treated as a certainty. We are being bigoted in our conditioning in that we are saying there is no uncertainty. What is probability in the face of perfect certainty? What do you mean by your question?
