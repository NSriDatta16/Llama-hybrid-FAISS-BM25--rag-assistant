[site]: crossvalidated
[post_id]: 188616
[parent_id]: 
[tags]: 
How can we calculate ROC AUC for classification algorithm such as random forest?

As at In classification with 2 - classes, can a higher accuracy leads to a lower ROC - AUC? , AdamO said that for random forest ROC AUC is not available, because there is no cut-off value for this algorithm, and ROC AUC is only calculable in the case if the algorithm returns a continuous probability value (and only 1 value) for an unseen element. But in R and Python, it is very often, such as pROC::auc in R, or roc_auc_score in sklearn in python, we can calculate ROC AUC after we have predicted results, i.e. ROC AUC is available for all algorithms. Summary: Why ROC AUC is not available for random forest (in case of classify True/False label) the idea of AUC is: if you change the cut-off value, the assignment will change. For instance, in regression, a predict value for an object is 0.75 and the cut-off value is 0.8, it will be assign to False (0) and if the cut-off value is 0.6 it will be assigned to True (1). But for randomForest, the assignment never change. For instance with the function predict_proba , sklearn will return a list of probability for each class, not only 1 probability So, letâ€™s say sklearn returns for an unseen element True False 0.21 0.19 Whatever cut-off value changes, the assignment will always be True and never change.
