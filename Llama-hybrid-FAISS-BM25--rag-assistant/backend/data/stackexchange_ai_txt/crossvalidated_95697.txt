[site]: crossvalidated
[post_id]: 95697
[parent_id]: 95672
[tags]: 
Geoffrey Hinton definitely uses multiple layers of RBM's in some cases. I believe he coined the term Deep Belief Network. The point of the associative memory is that those layers, which are unsupervised, discover more optimal features for the feed forward layer that follows than just treating the input as a bag of bits. In the case of image recognition, the first RBM layer might start to detect edges. The second would start to detect larger features. Finally, the larger scale features would become apparent. In the more specific case of images of faces, this leads to a set of faces that act similarly to "eigen faces". This allows the next layers (i.e. the feed forward neural network) to have an input that is essentially a combination of these eigen faces, which make for better features for the feed forward network. There are some additional techniques, such as training with drop out, that Hinton uses to increase predictive power.
