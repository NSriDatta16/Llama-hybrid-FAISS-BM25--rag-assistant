[site]: crossvalidated
[post_id]: 627166
[parent_id]: 627165
[tags]: 
It depends on the implementation. Naively, the $k$ NN algorithm just searches in prediction time through the training data to find the $k$ nearest neighbors, so you only need to store the train dataset. You could speed up the search by memoizing the nearest neighbors, then the size grows with the size of the memory. If you found the neighbors in advance for all the training cases, then you are correct but this does not guarantee that all the test cases will be included in this set (so it's disputable how much value would it have). There are also approximate nearest neighbors algorithms with different time and storage requirements. So the basic case is $O(Nd)$ , and the other cases depend on implementation details.
