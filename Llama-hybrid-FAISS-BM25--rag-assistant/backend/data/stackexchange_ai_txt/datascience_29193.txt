[site]: datascience
[post_id]: 29193
[parent_id]: 29192
[tags]: 
LSTMs would run into problems beyond 500 time steps. original paper by Hochreiter and Schmidhuber : "LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps." Example: Your video is 60 seconds long. I would start by encoding 5 frames per second using a CNN (use resnet (SEnets have better performance but i havent read the paper) till the layer just before final affine layer which feeds into the softmax encode your images. Use these encodings as inputs in a GRU (Hence a 300 time steps for this GRU) and use its final state to make the class prediction (use a separate loss for each label?)
