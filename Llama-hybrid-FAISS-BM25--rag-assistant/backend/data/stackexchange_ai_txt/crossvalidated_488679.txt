[site]: crossvalidated
[post_id]: 488679
[parent_id]: 488664
[tags]: 
Like @astel says, KNN is a bad choice for high dimensional data because of the curse of dimensionality(everything seems close to everything else in high dimensional spaces because the "volume" of the space increases exponentially). Not to mention, you have 21 million data points, which makes it time consuming to figure out the nearest neighbors, computationally. If you really want to try a non-parametric model, like you said, random forests are a good choice, but what I have found is that based on the depth of the individual trees, these models themselves can consume a lot of memory. If you have access to a cluster of machines to do your work, I recommend trying Spark's MlLib . You could also reduce the dimensionality of the data, and that might or might not help, and depends on the data. Parametric models like logistic regression, or SVM(with kernels if you want non-linearity) might also be easier to run using sklearn.
