[site]: crossvalidated
[post_id]: 323398
[parent_id]: 
[tags]: 
Determining important features from large number of features

From a large number of time series, I've generated an even larger number of features for each one. The result is a NxM matrix for times series N and feature M. Using Matlab, I could feed this dataset into the Classification Learner and generate a nice model of the data but I want to be a bit more sophisticated as I'm sure a majority of the M features are irrelevant (or correlated). I understand that using Principle Component Analysis, the calculated eigenvectors give insight into which features contribute the most to the variability in each Principle Component (at least, that's my perhaps naive understanding), but I feel like picking the features like this may be a small part of a bigger picture. Are there any systematic approaches I can use to reduce this data? Perhaps some books I should read on this topic? Thank you
