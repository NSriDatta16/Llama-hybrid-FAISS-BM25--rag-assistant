[site]: crossvalidated
[post_id]: 643429
[parent_id]: 643422
[tags]: 
The question is interesting, albeit somewhat ill-posed. Bayesians are generally comfortable with the idea of some point $\theta_0$ in parameter space $\Theta$ being the true parameter of a given parametric model $p_{X|\theta}$ . Your prior probability $\pi(\theta)$ over $\Theta$ then describes your confidence regarding $\theta_0$ 's location, and with every new piece of information $x_{1}, x_{2}, \ldots, x_{T}$ that you update your prior with, $\pi(\theta|x_{1}, x_{2}, \ldots, x_{T})$ becomes narrower and narrower until it concentrates over $\theta_{0}$ . But the concentration of $\pi(\theta|\cdot)$ stands at the end of this process, not the beginning. In other words, we should be talking about the posterior concentrating over the "truth", not the prior (for a seminal paper discussing posterior concentration , see Ghosal et al. ). In principle, you could have a point prior $\pi(\theta) = \delta(\theta-\theta_0)$ (where $\delta$ is a zero-variance Gaussian ). But in that case any further Bayesian updating is pointless: you already know the "true" $\theta$ with absolute certainty and every $\theta \neq \theta_{0}$ has zero probability mass, which no amount of data will ever "undo" .
