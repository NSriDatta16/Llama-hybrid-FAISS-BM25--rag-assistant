[site]: crossvalidated
[post_id]: 589703
[parent_id]: 
[tags]: 
Autoencoder accuracy with standardized data

I want to make an autoencoder over the data that I originally standardized (that is, the data is now normally distributed ~ N(0,1)). The activation functions I use in the linear autoencoder is ReLu. As I know that about 95% of the data after standardization is in the interval of $ $ only by using the Relu function $max(0,x)$ will not be able to guess all the data. One idea that came to my mind is to use bias, but as far as I understand, bias works inside the activation function, that is, $y = f(w^Tx + b)$ , not $y = f(w^Tx) + b$ what kind of implementation in Pytorch. Does anyone have another solution to this problem?
