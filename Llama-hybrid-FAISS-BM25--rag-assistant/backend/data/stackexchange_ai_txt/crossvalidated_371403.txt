[site]: crossvalidated
[post_id]: 371403
[parent_id]: 371402
[tags]: 
It does not have a standard name. In different areas of statistics, it has different names. In the neural networks and deep learning community, it is called the sigmoid function. This is confusing for everyone else, because sigmoid is just a fancy way of saying "S-shaped" and this function is not unique among S-shaped functions; for example, $\tanh$ is also S-shaped and widely used in neural networks, yet it is not commonly termed "sigmoidal" in neural network literature. In the GLM literature, this is called the logistic function (as in logistic regression). If the logit function is $$\text{logit}(p)= \log\left(\frac{p}{1-p}\right)= \log(p)-\log(1-p)=x$$ for $p\in(0,1)$ , then $$\text{logit}^{-1}(x)= \frac{\exp(x)}{1 + \exp(x)}= \frac{1}{1+\exp(-x)}= p$$ for $x\in\mathbb{R}$ . This is the reason some people call $\text{logit}^{-1}$ the inverse logit or anti-logit function. (Thanks, Glen_b!) Rarely, I've seen the name expit used; as far as I can tell, this is a back-formation from the word logit but never really caught on. (Thanks, CliffAB!)
