[site]: crossvalidated
[post_id]: 432836
[parent_id]: 
[tags]: 
Bag of Tricks for Efficient Text Classification. How are parameters among features and classes in this model?

In the Bag of Tricks paper they state about how linear classifiers do not share parameters between features and classes. Taking a look at the math behind it I can see that they do not share parameters. The paper goes on to say that the sentence vector which is obtained can be used as a hidden variable can be used again. Reading online about this I have seen that people say that is can be used as a hidden variable that can be shared between features. I cannot find any math or more verbose explanation about the model to better understand this. Any clarification appreciated. Edit: The averaging of the word vectors to make a sentence vector is apparently what is being used to represent the sharing of parameters between the features because they compared this to the bag of words model which does not do this. This is my interpretation and what I understood from the github link I posted below which had a simple model file which you can follow. This is a link to the proof for not sharing features. This is a link to the medium blog where they describe the text representation as a hidden variable. This is a link to a github of someone who made their own implementation of the model which I looked through to get a better understanding.
