[site]: crossvalidated
[post_id]: 352598
[parent_id]: 350936
[tags]: 
I think it is worth considering the use of generalised additive models (GAMs). GAMs are able to encapsulate non-linear relations between the response variable and the outcome variables and are straight-forward to explain. They are well-understood and widely used within the Statistics community. In totally informal manner: GAMs are practically GLMs with a out-of-the-box, semi-automated basis expansion module strapped in. No need to define quirky $x^{\frac{1}{4}}, \sin(2\pi x)$, etc. transformations, the best non-linear relation will be automatically selected. GAM are great to visualise the (potentially) varying influence of $x$ on the outcome $y$. There is an abundance of good resources on using GAMs online (e.g. here and here ), in print (e.g. here and here ) and CV has literally dozens of insightful questions and answers on generalized additive models. R has two extremely good GAM packages gam and mgcv . I would suggest you start with mgcv as a matter of convenience. I would suggest you also look at the FAT/ML (Fairness, Accountability, and Transparency in Machine Learning) initiative. It has some great novel ideas. In relation with GAMs I would point you to the 2017 invited talk by Rich Caruana on " Friends Donâ€™t Let Friends Deploy Black-Box Models "; it shows an application of an extension of GAMs (called GA2M) that is used instead of standard ML techniques (random forests) and gives results of similar accuracy but also being fully interpretable.
