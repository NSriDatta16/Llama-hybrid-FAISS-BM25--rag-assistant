[site]: crossvalidated
[post_id]: 286522
[parent_id]: 
[tags]: 
Is it necessary to standardize data for neural networks?

I'm learning about neural networks and I've been trying to figure out wether it's a good idea to normalize/standardiza data before training. From what I've read there's divided opinions about this, some sources says it will imrpove the result/make the training faster while other sources state there's no real need for it. I guess the general answere is "it depends" on the situation. Here's my use case and I'd like to hear what you think is the better approach: I have a dataset with about 15 independent parameters of different types, some are categorical strings ("cat","dog,"horse"...,"elefant"), others are categorical integers (1,2,3,4...,10) and yet others are continous numerical data (1.32,3.123,2.132...,1.234). Some values are missing for some of the parameters (sometimes most of the values are missing). In my current preprossecing I do this: Transforms categoriacal strings to integers starting from 1: ("cat","dog,"horse"...) -> (1,2,3...) Categorical integers and numerical data are left as is. Replaces all missing values (NA) with 0 The training is done in R using rxNeuralNet and the output is a continous value (regression). From what I've read there are ways to standardize the data which might improve the performance (scaling, centering to zero mean etc). In this case, would it be meaningful to do this preprocessing?
