[site]: crossvalidated
[post_id]: 563464
[parent_id]: 
[tags]: 
Regularization: How do you penalize weights of some exact value?

Assume we have a loss function of the form: L(f(X; theta),T) where X is the input dataset and T is the target dataset? Then you would update the the paramters by doing p = p - p.grad where p.grad is the gradient achieved through gradient descent (not important here) then I understand that L2 norm is a method of regularisation used to penalize large weights. ~L = L(f(X;theta),T) + (lambda/2) * (frobenius norm) consequently, p = p - lr * p.grad - lambda * p But what if I want to discourage weights of some exact magnitude. What If I want to penalize weights that are near +-0.75 more than others? Is that possible? How would the new loss function look like? edit: Could I simply make a function of theta that increases as theta approaches 1? then that would magnify lambda (the scaling factor of the penalty) for values near 1.
