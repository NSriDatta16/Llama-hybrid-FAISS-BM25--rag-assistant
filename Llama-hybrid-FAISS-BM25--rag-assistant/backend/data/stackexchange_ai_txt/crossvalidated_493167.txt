[site]: crossvalidated
[post_id]: 493167
[parent_id]: 232897
[tags]: 
So, first of all, this is an answer based on the one by @cbeleites above, this one here , and the question itself (all these contributions helped me understand). There is nothing original in it, and although it makes sense to me, I am still a student in this topic so I am not 100% sure of it. Therefore, any feedback is appreciated. However, it gives a specific example, so I think it might be useful to understand the concepts expressed above. We as model builders need to deliver a model and an estimate of its performance. Similarly, if we buy an amperometer, together with the object we are sold a manual containing its specifications. I assume that SVM with C parameter in the range [0.1,10] is a good model and that accuracy is a good performance measure for the case at hand. We want to select the best C parameter between: 0.1, 1, 10. These are the steps and their interpretation: 1- We first implement nested cross-validation on all data (see here for an example using sklearn). Aim of this step is to estimate the performance/accuracy of my final model (which I haven't fitted yet), or rather of my fitting procedure. Call it $a$ . $a$ will be an average over outer folds of the accuracy from different but equivalent models (i.e. models having different parameters and hyperparameter C in {0.1, 1, 10}) 1b- before proceeding further I should check model stability and that I am not overfitting, as explained here . 2- I now implement cross-validation on all data to determine the best C. I assume I get C= $c$ . 3- I finally use all data to fit my SVM with C= $c$ . This is the best model which I can achieve so I deliver it. I will also tell my customer that it will have an accuracy of $~a$ , which I have estimated in step 1.
