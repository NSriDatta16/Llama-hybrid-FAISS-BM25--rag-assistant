[site]: crossvalidated
[post_id]: 488979
[parent_id]: 
[tags]: 
Slicing series with t-test

I am trying to develop a system for automated performance regression detection. Usually this boils down to setting range for the performance counters manually (e.g. no more than 5% drop in throughput vs. baseline/average of N last results) but I wanted something that could also detect smaller drops that stay in the results for many observations. I could do different thresholds for floating averages etc., but I was thinking about something statistically more sound. My naive approach was to analyze the last N values (up to last detected change) and do a t-test for x 1 .. x k vs. x k+1 .. x n for each k (requiring the subset to have at least few values) at 95% confidence level. I was wondering why I am getting many false positives on real data, so I've generated random series from normally distributed data with the same mean and I've observed the same. I think that my request to see no false positives is contradicting the set confidence level - is that correct? Can you suggest a better approach, besides increasing the confidence level? Am I completely off path? False positives would be more frequent since in fact I will have 10s of counters for each nightly run and the chances multiply.
