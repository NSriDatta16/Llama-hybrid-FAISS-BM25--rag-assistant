[site]: datascience
[post_id]: 54806
[parent_id]: 
[tags]: 
Word embedding of a new word which was not in training

Let's say I trained a Skip-Gram model (Word2Vec) for my vocabulary of size 10,000. The representation allows me to reduce the dimension from 10,000 (one-hot-encoding) to 100 (size of hidden layer of the neural network). Now suppose I have a word in my test set which was not in my training vocabulary. What is a reasonable representation of the word in the space of dimension 100? For me, it seems that I cannot use the neural network I trained to come up with the word embeddings.
