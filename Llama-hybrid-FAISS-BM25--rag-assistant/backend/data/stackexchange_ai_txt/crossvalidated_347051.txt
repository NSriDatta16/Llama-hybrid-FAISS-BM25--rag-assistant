[site]: crossvalidated
[post_id]: 347051
[parent_id]: 
[tags]: 
What is the difference between Lasso regression in glmnet (in R) and Sklearn lasso (in Python)?

A similar post was discussed here regarding Ridge Regression: What are the differences between Ridge regression using R's glmnet and Python's scikit-learn? My question is what is this difference for Lasso? In R my data gives a corresponding lambda value of $0.02$ from the cross-validation plot. This is also almost the same value that is returned from Python as well. However, the coefficients returned from R return several variable coefficients while Python returns $0$'s for the coefficients. The expression which Python lasso optimizes is: $\frac{1}{ 2\cdot n_{samples}} ||y - Xw||^2_2 + \lambda ||w||_1$. While R minimizes: $\frac{1}{ n_{samples}} ||y - Xw||^2_2 + \lambda ||w||_1$. At first I thought it would be a simple factor of $2$ between the two $\lambda$'s, however, this does not seem to be the case. I am new to coding in general, so any ideas would be appreciated. Ok, so as an edit I am adding code with just some simple toy data. Both R and Python still give different values for the Lambda chosen. Although if you notice the coefficient plot it looks to be shifted by some factor... this is also the case in my full dataset. Code for R: #initialize libraries library(xlsx) library(glmnet) library(plotmo) #Data Input x And the Python Code: import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import Lasso, LassoCV from __future__ import division import time x=np.array([[-1,-4],[0,0],[1,16]]) y=np.array([[-2.2],[0],[3.8]]).reshape(3,) #set alphas to be tested, note this is in logspace alphas = np.logspace(-7,5,num=1000,base=np.e) #plot coefficent size vs log(alpha) graph. lasso = Lasso(max_iter=10000, normalize=False, fit_intercept=False) coefs = [] for a in alphas: lasso.set_params(alpha=a) lasso.fit((x), (y)) coefs.append(lasso.coef_) ax = plt.gca() ax.plot(np.log(alphas), coefs) ax.set_xscale('linear') plt.axis('tight') plt.xlabel('log(alpha)') plt.ylabel('weights') print("Computing regularization path using the coordinate descent lasso...") t1 = time.time() model = LassoCV(alphas=None,cv=3, normalize=False, fit_intercept=False).fit((x), (y)) t_lasso_cv = time.time() - t1 # Display results m_log_alphas = np.log(model.alphas_) plt.figure() #To properly show, the ymin and ymax may need adjusted ymin, ymax = 0, 25 plt.plot(m_log_alphas, model.mse_path_.mean(axis=-1), 'k', label='Average across the folds', linewidth=2) plt.axvline(np.log(model.alpha_), linestyle='--', color='k', label='alpha: CV estimate') plt.legend() plt.xlabel('log(alpha)') plt.ylabel('Mean square error') plt.title('Mean square error on each fold: coordinate descent ' '(train time: %.2fs)' % t_lasso_cv) plt.axis('tight') plt.ylim(ymin, ymax) plt.xlim(-10,10) #Calculate Lasso coefficents from proper alpha. Skipping for now, as we are just comparing 1 alpha value. #lassocv = LassoCV(eps=.000000001,max_n_alphas=1000, cv=N, max_iter=100000, normalize=False, fit_intercept=False) #lassocv.fit(x, y) #minimumalpha = lassocv.alpha_ lasso=Lasso(alpha=.1,normalize=False,fit_intercept=False,max_iter=100000) lasso.fit(x, y) print(lasso.coef_)
