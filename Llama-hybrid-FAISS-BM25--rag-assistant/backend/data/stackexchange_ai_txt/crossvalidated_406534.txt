[site]: crossvalidated
[post_id]: 406534
[parent_id]: 
[tags]: 
Theoretical grounding for ease of training with a prior

If we have a neural network that learns the generative model for P(A, B, C) And now, we want to learn the generative model for P(A, B, C, D) Is there any theory that says learning P(A,B,C) and then composing it with P(D | A,B,C) is faster than learning P(A,B,C,D) from scratch?
