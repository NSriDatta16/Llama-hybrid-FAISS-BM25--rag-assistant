[site]: crossvalidated
[post_id]: 548389
[parent_id]: 548295
[tags]: 
You have two sets of posterior samples that you want to compare. The problem is, you haven't defined the joint distribution of the two sampled quantities. Why? because the two quantities are derived from independent models/data. The two models have been developed independently, so there is no assumed/modelled link between your two sampled quantities. Hence assuming independence of the two sets of samples is not unreasonable, in which case the proposed 'arbitrary pairings' approach does give you what you're looking for. Having said that, it's worth bearing in mind the assumptions you're making here. If the two models had explanatory variables/parameters in common then you could have fitted a single combined model for both datasets, and in that case the posterior samples for your two quantities would potentially no longer be independent. An alternative approach which I've found helpful is the following. What is each of your 5000 prediction error samples telling you? It's telling you about the performance of the model with fixed parameters equal to those of the specific sample - but you wouldn't actually use such a fixed-parameter model for prediction. So, you could instead construct a performance metric which is a property of the fitted model rather than a property of a specific sample. For example, you could define prediction error for Walmart store no. 1 to be the difference between its average sales and the expected average sales where the expectation is over the posterior predictive distribution. Or whatever metric is most meaningful in your line of work. I have done something similar using metrics derived from the ROC curve. This approach is briefly discussed in the book Bayesian Data Analysis , section 7.3, subsection "Evaluating predictive error comparisons": Sometimes it may be possible to use an application-specific scoring function that is so familiar for subject-matter experts that they can interpret the practical significance of differences. For example, epidemiologists are used to looking at differences in area under receiver operating characteristic curve (AUC) for classification and survival models Finally, you compare the metrics you've calculated for your two models.
