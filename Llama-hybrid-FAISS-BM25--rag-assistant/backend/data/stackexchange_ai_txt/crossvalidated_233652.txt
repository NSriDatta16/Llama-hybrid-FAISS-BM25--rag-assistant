[site]: crossvalidated
[post_id]: 233652
[parent_id]: 233627
[tags]: 
Let's suppose that we observe $y_1, \dots, y_n$ where $y_i = \mu + \varepsilon_i$ for iid errors $\varepsilon_i$ with mean 0 and variance $\sigma^2$. I'm just denoting the constant value of $f(x_i)$ for each $i$ by $\mu$ here. We want to predict the label $y_0$ for some new point $x_0$, which we could do by (1) taking the nearest point to our test point, say $y_1$, or (2) we could average the $y_i$ over all $n$. Both are unbiased since $E(y_1) = E(\bar y) = \mu$. But their variances differ: $Var(y_1) = \sigma^2$ while $Var(\bar y) = \sigma^2/n$, so $\bar y$ is the better estimator. So if the regression function really is constant for all $n$ of these points, then we'll get a much better estimator by averaging rather than just using one of them. In practice it's not so simple, though, because we don't think the regression function is truly constant, so it may be that the expected value of farther away points is too different and that we don't want to include them. This is all captured by choosing $k$.
