[site]: crossvalidated
[post_id]: 177919
[parent_id]: 122294
[tags]: 
If you haven't yet, check out this page which covers SARSA with LFA: http://artint.info/html/ArtInt_272.html Sutton's book is really confusing in how they describe how to set up your feature space F(s,a), but in the web page above, they describe it in a simple example. Applying the architecture of theta and F(s,a) from that page to Sutton's algorithm works very well. Suppose you have 4 possible actions in a state. Create a reward Q distribution (in this case a 4-value array), with one value for each possible action in the given state. Iterate over each action, and for that action, populate the feature space based on what that action will do to/for the agent. For example, if the agent is directly below a wall, and the chosen action is 'up', there should be a 1 for the feature 'is the agent about to try to move into a wall'. Likewise, for action='right' and wall to the right, the same feature would be a 1, etc. for all other possibilities. You've probably moved past this problem a while ago, but if not, hope this helped!
