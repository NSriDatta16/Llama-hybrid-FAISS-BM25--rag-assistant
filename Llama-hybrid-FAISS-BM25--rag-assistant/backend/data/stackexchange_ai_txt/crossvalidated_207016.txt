[site]: crossvalidated
[post_id]: 207016
[parent_id]: 207011
[tags]: 
The CART model is fundamentally piece-wise constant. The ensemble variations (rf and gbm) can have more smoothness but are still piece-wise constant in element. They can approximate and, in a cousin-manner to neural networks, are universal approximators but are not the best non-local generalizers. The CART basis, like Navier-Stokes for the continuity assumption [1] , is only valid when the piece-wise constant model is valid. Like NS when the big-box average is no longer valid over the infinitesimal element, interpolation or extrapolation using CART when the constant mean model is not the appropriate approximation, breaks down. (It must be late, I'm think of NS in terms of the big-box model. I blame Dr. J , formerly known as 'jaws', one of the most excellent teachers.) If you train them over a restricted domain, one with a region of the domain missing, for $$ f \left( x_1, x_2 \right) = \frac {x_1} {x_2} $$ and then try to interpolate, the interpolation is going to have issues. In regions where: $$ \frac{\partial f\left(x_{1},x_{2} \right)}{\partial x_{1}} = 0 $$ and $$\frac{\partial f\left(x_{1},x_{2} \right)}{\partial x_{2}} = 0 $$ either by taking very small changes in independent variables, or where $x_{2}$ is really big compared to $x_{1}$ and $x_{1}$ doesn't change sign, then the approximation is going to have less interpolation error.
