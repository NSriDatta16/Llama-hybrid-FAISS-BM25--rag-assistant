[site]: crossvalidated
[post_id]: 56771
[parent_id]: 
[tags]: 
Kernel PCA (in R)

I am attempting to use the kernel PCA features in kernlab but am having trouble understanding the output. In particular, it's unclear what scale the results are in when I try to project the original data unto the resulting eigenvectors. Here is an example: require(kernlab) x = data.frame(a=1:10, b=10:1, c=sample(1:10,10), d=sample(1:10,10)) x = as.data.frame(lapply(x, scale, scale=F)) kpc = kpca(~., data=x, kernel="polydot", kpar=list(scale=1, offset=0, degree=2), features=1) Printing x and the data from x projected unto the eigenvectors shows: print(x) a b c d # 1 -4.5 4.5 1.5 -2.5 # 2 -3.5 3.5 -4.5 -3.5 # 3 -2.5 2.5 -1.5 -4.5 # 4 -1.5 1.5 0.5 -1.5 # 5 -0.5 0.5 -0.5 -0.5 # 6 0.5 -0.5 -2.5 4.5 # 7 1.5 -1.5 3.5 1.5 # 8 2.5 -2.5 2.5 3.5 # 9 3.5 -3.5 4.5 2.5 #10 4.5 -4.5 -3.5 0.5 print(rotated(kpc)) [,1] #1 68.18682 #2 -109.41619 #3 -24.34769 #4 30.26417 #5 24.81883 #6 41.32570 #7 -20.17489 #8 -37.57738 #9 -93.93368 #10 120.85431 I'm having a tough time understanding the scale of this result. Does rotated not scale the projection? In particular, the eigenvectors that PCA returns should be length 1. We don't know the eigenvectors in feature space, however, so well we can return is the projection of our data in that feature space using the kernel trick. The results below don't seem to be on the correct scale, however.
