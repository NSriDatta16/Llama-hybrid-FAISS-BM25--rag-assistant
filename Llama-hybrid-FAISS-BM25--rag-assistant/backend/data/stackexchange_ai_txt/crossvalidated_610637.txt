[site]: crossvalidated
[post_id]: 610637
[parent_id]: 
[tags]: 
Is unit deviance (statistics) equivalent to the loss function (machine learning)

In this page from scikit learn, about GLM , the notion of unit deviance is introduced as loss function (from the machine learning perspective). I want to know if there is equivalence between these two notions: unit devance vs. loss function. For linear regression, there is equivalence, since the loss function is the squared error and it is equivalent to maximizing the Gaussian distribution in MLE (Maximum Likelihood Estimation). For logistic regression, I struggled to understand, since the deviance is: $$2({y}\log \frac{y}{\hat{y}}+({1}-{y})\log \frac{{1}-{y}}{{1}-\hat{y}})$$ Whereas the classic log loss is: $$y \log(p) + (1-y) \log(1 - p)$$ And this post seems to confirm that the deviance is the same as log loss. As for Gamma regression, I found this post discussing the loss function for gamma in XGBoost and the result is different from the one seen in scikit-learn.
