[site]: crossvalidated
[post_id]: 417242
[parent_id]: 417241
[tags]: 
Is it possible to train a 3 layer multilayer perceptron (mlp) in a binary classification problem with just 135 observations with out massive over fitting? Yes, it is. At least 10 observations / per parameter (in case of mlp: weights), can somebody confirm this rule? No, that's not so simple. While I appreciate such heuristics I also take that for what they are, heuristics. If you have more degrees of freedom (basically flexibility) in your model than you have samples then it can, theoretically, memorize the data, which might lead to catastrophic overfitting (not always though). Thing is, the number of parameters is not exactly the same as the degrees of freedom of a model. You can artificially induce constrain your parameters. This is called regularization (see our questions and answers on it regularization ). On MLPs, regularization can be used explicitly (you want your model to reduce a mis-classification cost plus a sum of the absolute values of the weights, i.e. $\ell$ -1 regularization, so it has less freedom on its selection of weights), but there are other implicitly defined regularizations (for example, batch learning). The biggest artificial neural networks are in an overparametrized regime, where they have way more parameters than the number of samples they'll ever be exposed to. See the top performing networks on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) for example. An early top performer, VGG-19 has 143,667,240 trainable parameters . ImageNet has less than 1 million samples.
