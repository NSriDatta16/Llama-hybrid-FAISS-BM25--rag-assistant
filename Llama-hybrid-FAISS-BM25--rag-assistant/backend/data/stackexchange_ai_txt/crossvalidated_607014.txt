[site]: crossvalidated
[post_id]: 607014
[parent_id]: 604021
[tags]: 
Your LLM will give you a categorical distribution as output, over which you can sample, and thus use RL to estimate the gradient... What you are suggesting looks more like a GAN which instead of using a discriminator, you have a ordinal-NN, over which you take the gradient to maximize the ordinal output... however, this is unstable and usually the generator (LLM) will have a very easy time maximizing the output of the discriminator, which is known as mode alignment... You can probably fine tune a LLM with a loss like: $$ \nabla L(\theta) = \nabla(-D_{kl}(\pi_{\theta}(y|x)||\pi_{orig}(y|x))) + \nabla D(\pi_{\theta}(y|x)|x) $$ where the second term is the gradient flowing from the conditional discriminator (which maximization should give you more human-like response), and the first one is just a penalization term to not go too far from the pre-trained model However, in my opinion, this will just make the LLM overfit the discriminator...
