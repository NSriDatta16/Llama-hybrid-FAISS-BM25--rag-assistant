[site]: crossvalidated
[post_id]: 309597
[parent_id]: 248715
[tags]: 
For CNN architecture usually the input is (for each sentence) a vector of embedded words [GloVe($w_0$), GloVe($w_1$), ..., GloVe($w_N$)] of fixed length N. So commonly you preset the maximum sentence length and just pad the tail of the input vector with zero vectors (just as you did). Marking the beginning and the end of the sentence is more relevant for the RNN, where the sentence length is expected to be variable and processing is done sequentially. Having said that, you can add a new dimension to the GloVe vector, setting it to zero for normal words, and arbitrarily to (say) 0.5 for BEGIN, and 1 for END and a random negative value for OOV. As for unknown words, you should ;) encounter them very rear, otherwise you might consider training the embeddings by yourself.
