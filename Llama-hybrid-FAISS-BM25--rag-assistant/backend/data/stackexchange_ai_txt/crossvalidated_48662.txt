[site]: crossvalidated
[post_id]: 48662
[parent_id]: 
[tags]: 
Reasonable assumption in Bayesian linear regression

I'm learning bayesian linear regression from the book Bayesian Data Analysis . Here is my questions. Notation (follow the book): $X$ stands for explanatory variables, with parameter $\psi$; $\bf{y}$ for responsible variables, conditional on $X$ and regression parameter $\theta$. Why do we need linear regression? It seems we can make predictions on $\tilde{y}$ directly, given the past values of $\bf{y}$. i.e. build a posterior predictive distribution $p(\tilde{y} | \bf{y})$. In the book, one of the assumptions is (Pg.354 Ch.14): The distribution of $X$ is assumed to provide no information about the conditional distribution of $\bf{y}$ given $X$; that is, the parameters $\theta$ determining $p(y|X,\theta)$ and the parameters $\psi$ determining $p(X|\psi)$ are assumed independent in their prior distributions. Is this a reasonable assumption? I think we can't say $\psi$ and $\theta$ are independent in the prior, but conditional independent after we observed the variables $X$ and ${y}$. i.e. they are independent in the posterior. The last question is about the factorization of joint posterior. $$ p(\psi, \theta |X, y) = p(\psi|X)p(\theta|X,y) $$ According to the author, we can analyze the second factor by itself, with no loss of information: (Pg.355 Ch.14) $$ p(\theta|X,y) \propto p(\theta)p(y|X,\theta) $$ How to get the above formula? My way: \begin{align} p(\theta|X,y) &= \frac{p(X,y|\theta)p(\theta)}{p(X,y)} \\ &\propto p(X,y|\theta)p(\theta) \end{align} In addition, what does the "information loss" refer to?
