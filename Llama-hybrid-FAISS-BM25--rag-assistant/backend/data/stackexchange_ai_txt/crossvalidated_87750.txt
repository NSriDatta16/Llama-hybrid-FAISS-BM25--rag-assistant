[site]: crossvalidated
[post_id]: 87750
[parent_id]: 
[tags]: 
What does the term "Estimation error" mean?

I was reading some notes on machine learning when I came across the following sentence: First, we may have a large estimation error . This means that, even if the true relationship between x and y is linear, it is hard for us to estimate it on the basis of a small (and potentially noisy) training set $S_n$. Our estimated parameters $\hat{\theta}$ will not be entirely correct. The larger the training set is, the smaller the estimation error will be. I think the notes try to give a intuitive explanation of what the term means, but I was not sure if I understood it completely. Does having an estimation error mean that the way that we are estimating our parameters is wrong even if we are choosing from the correct set of classifiers? Say that our data was truly linear, but we still failed to to separate it. Is that a correct example? Also, any answer relating it to the concepts of bias and variance (or overfitting and underfitting, will be greatly appreciated!) I am looking for both a intuitive explanation and if there exists, a rigurous mathematical one for what that term means, it would be awesome!
