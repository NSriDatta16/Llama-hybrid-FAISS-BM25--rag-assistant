[site]: crossvalidated
[post_id]: 376672
[parent_id]: 
[tags]: 
When data size is very large, would it still matter which model to use?

I'm new to machine learning models and do not have an opportunity to run real business models. Doing some studies on classification models using SK-learn (SVM, Logistic, tree based models and etc.) I wonder if I have really large data size to do the training, would it still be a matter what model I choose to fit the data and then use on test data? My naive guess is the more data you have, the less likely the type of the model matters. Also, if we really have huge amount of data, we probably cannot run many alternative models to see which one is better, because it may take days to fit even just one model. Therefore my naive conclusion is, in real business where people running model against their huge customer base, they won't have the needs of trying many model options or grid search many parameter settings but still can come up well performed model. Is my naive understanding valid? :)
