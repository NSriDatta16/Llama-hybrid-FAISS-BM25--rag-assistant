[site]: datascience
[post_id]: 77312
[parent_id]: 
[tags]: 
Does the performance of GBM methods profit from feature scaling?

I know that feature scaling is an important pre-processing step for creating artificial neural network models. But what about Gradient Boosting Machines , such as LightGBM, XGBoost or CatBoost? Does their performance profit from feature scaling? If so, why and how?
