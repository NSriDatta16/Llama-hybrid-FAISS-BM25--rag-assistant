[site]: datascience
[post_id]: 128258
[parent_id]: 128257
[tags]: 
When you train any model, you need to train it on a specific task and its associated loss function. Masked-language modelling is the token-level task used for training transformer encoders like BERT and RoBERTa on unlabeled text. While the transformer encoder architecture enables it to "operate bidirectionally", you still need a loss that exercises such bidirectionality so that the model is actually trained. Therefore, you can't have a BERT model (nor BERT contextual embeddings) unless you train it in some task like masked language modelling.
