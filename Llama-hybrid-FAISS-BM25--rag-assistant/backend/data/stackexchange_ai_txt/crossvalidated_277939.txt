[site]: crossvalidated
[post_id]: 277939
[parent_id]: 277836
[tags]: 
One way to do this would be to first perform a supervised classification with Random Forest, and then extract the important features. R has a very nice Random Forest implementation - https://cran.r-project.org/web/packages/randomForest/randomForest.pdf A very "Cliffs Notes" version of what I would do is: library(randomForest) rf Then extract the importance features using: rf$importance The most "descriptive" predictors will have the highest percent increase mean squared error (%IncMSE), as the error will increase as they are removed during the RF tree-generation. Like I said, this is a very "Cliffs Notes" version of the pipeline, but it should guide you in a good direction if you chose to pursue it. (EDIT: I am an R programmer so I included R code. However, I am pretty sure there are other languages that have Random Forest implementation if you are not comfortable in R.)
