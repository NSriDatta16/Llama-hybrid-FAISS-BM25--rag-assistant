[site]: crossvalidated
[post_id]: 465709
[parent_id]: 
[tags]: 
Why does validation loss so much higher than training?

I am trainning a LSTM model that seems to do well on the training set. However, for some reason, after the first few iterations, the validation loss shoots up significantly. I am not even how a loss of 184 is even possible for softmax and cross-entropy loss. Does this look like a bug or expected hehavior? Training for 5 epoch(s)... Epoch: 1/5 Loss: 5.659843604564667 time_per_batch: 0.024110078811645508 Epoch: 1/5 Loss: 5.194439644813538 time_per_batch: 0.024190187454223633 validation loss: 5.250472049864512 Epoch: 2/5 Loss: 5.014158352967855 time_per_batch: 0.023919343948364258 Epoch: 2/5 Loss: 4.939968596100807 time_per_batch: 0.02540421485900879 validation loss: 5.1598842238623 Epoch: 3/5 Loss: 4.84823331800667 time_per_batch: 0.023817777633666992 Epoch: 3/5 Loss: 4.872153713107109 time_per_batch: 0.023148298263549805 validation loss: 184.54528227306548 Epoch: 4/5 Loss: 4.829729948172698 time_per_batch: 0.023070335388183594 Epoch: 4/5 Loss: 4.780962432026863 time_per_batch: 0.023934125900268555 validation loss: 77.25420267619783 Epoch: 5/5 Loss: 4.766845889671429 time_per_batch: 0.023708105087280273 Epoch: 5/5 Loss: 4.764066659212112 time_per_batch: 0.023237943649291992 validation loss: 435.22167629665796 Network Architecture: class RNN(nn.Module): def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5, pretrained_embedding = None): """ Initialize the PyTorch RNN Module :param vocab_size: The number of input dimensions of the neural network (the size of the vocabulary) :param output_size: The number of output dimensions of the neural network :param embedding_dim: The size of embeddings, should you choose to use them :param hidden_dim: The size of the hidden layer outputs :param dropout: dropout to add in between LSTM/GRU layers """ super(RNN, self).__init__() # TODO: Implement function self.n_layers = n_layers self.hidden_dim = hidden_dim self.vocab_size = vocab_size self.output_size = output_size self.embedding_dim = embedding_dim self.embd = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim) if pretrained_embedding is not None: nn.Embedding.from_pretrained(torch.from_numpy(pretrained_embedding)) self.lstm = nn.LSTM(input_size=embedding_dim, num_layers=n_layers, hidden_size=hidden_dim, dropout=dropout, batch_first=True) self.dropout = nn.Dropout(dropout) self.bn = nn.BatchNorm1d(num_features=hidden_dim) self.fc = nn.Linear(in_features=hidden_dim, out_features=100) self.fc2 = nn.Linear(in_features=100, out_features=output_size) # self.logsoftmax = nn.LogSoftmax(dim=1) def forward(self, nn_input, hidden, print_shape=False): """ Forward propagation of the neural network :param nn_input: The input to the neural network :param hidden: The hidden state :return: Two Tensors, the output of the neural network and the latest hidden state """ # TODO: Implement function batch_size = nn_input.size(0) if print_shape: print(f"vocab_size: {self.vocab_size}, hidden_dim: {self.hidden_dim}, " f"nn_input shape: {nn_input.shape}, output_size: {self.output_size}") print(f'batch_sz: {batch_size}, embd_dim : {self.embedding_dim}') x = self.embd(nn_input.long()) x, hidden = self.lstm(x, hidden) x = x[:,-1,:] x = self.bn(x) x = self.fc(x) x = self.dropout(x) x = self.fc2(x) #x = self.logsoftmax(x) # return one batch of output word scores and the hidden state return x, hidden def init_hidden(self, batch_size): ''' Initialize the hidden state of an LSTM/GRU :param batch_size: The batch_size of the hidden state :return: hidden state of dims (n_layers, batch_size, hidden_dim) ''' # Implement function # initialize hidden state with zero weights, and move to GPU if available weight = next(self.parameters()).data #print(weight.shape) if (train_on_gpu): hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(), weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda()) else: hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(), weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()) return hidden
