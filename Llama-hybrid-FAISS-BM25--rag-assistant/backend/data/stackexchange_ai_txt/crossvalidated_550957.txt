[site]: crossvalidated
[post_id]: 550957
[parent_id]: 550928
[tags]: 
First, let us review what stacking is. Then we can answer the question. Stacking is a machine learning variation of meta-analysis. I don't claim any originality for this answer from the geeksforgeeks site , which says, "There are many ways to ensemble models, the widely known models are Bagging or Boosting. Bagging allows multiple similar models with high variance are averaged to decrease variance. Boosting builds multiple incremental models to decrease the bias, while keeping variance small. Stacking (sometimes called Stacked Generalization) is a different paradigm. The point of stacking is to explore a space of different models for the same problem. The idea is that you can attack a learning problem with different types of models which are capable to learn some part of the problem, but not the whole space of the problem. So, you can build multiple different learners and you use them to build an intermediate prediction, one prediction for each learned model. Then you add a new model which learns from the intermediate predictions the same target. This final model is said to be stacked on the top of the others, hence the name. Thus, you might improve your overall performance, and often you end up with a model which is better than any individual intermediate model. Notice however, that it does not give you any guarantee, as is often the case with any machine learning technique. " Note my bolding of text above; no guarantees it works. Moreover, they outline a procedure for stacking: "How stacking works? We split the training data into K-folds just like K-fold cross-validation. A base model is fitted on the K-1 parts and predictions are made for Kth part. We do for each part of the training data. The base model is then fitted on the whole train data set to calculate its performance on the test set. We repeat the last 3 steps for other base models. Predictions from the train set are used as features for the second level model. Second level model is used to make a prediction on the test set." Also, see this other article same website . Now the answer Stacking uses different models for the same data. To combine results from different data sets would be a more general meta-analysis, and there appears to have been some work on that topic. The OP asks Once we have these models, does it make sense to apply them to different data types/structures? It makes sense to go through the same procedure from scratch. That is, if the problem changes, the models needed will be different. If the problem doesn't change and there is merely more of the same type of data, then the analysis can be repeated on that larger data set.
