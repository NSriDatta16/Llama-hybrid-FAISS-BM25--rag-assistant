[site]: datascience
[post_id]: 123165
[parent_id]: 
[tags]: 
When training deep learning model which is better, training with sampled data Vs. training on shorter epoch

I am running multiple hyperparameter optimization trials therefore trying to find a way to reduce time consumption. Two ways that I could think of are search hyperparameter on subset of data. search hyperparameter on shorter epoch on full data (shuffling on each epoch). My take on this is #1 will be beneficial if there are redundant data and subset of data represents population well. But if full data is already complex thus not easily represented by subset #2,#1 would be similar since total training batch would be similar. Does anyone have answer to this with possible reference that proves its statement with mathematical rigor?
