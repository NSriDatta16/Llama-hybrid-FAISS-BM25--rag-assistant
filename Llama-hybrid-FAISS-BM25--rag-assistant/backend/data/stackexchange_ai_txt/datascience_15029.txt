[site]: datascience
[post_id]: 15029
[parent_id]: 
[tags]: 
GridSearchCV with SVM estimator AUC score not reproduced on SVM run

I've run GridSearchCV to determine 'best parameters' for a linear SVM, and then passed these [in a dictionary along with non-tuned parameters] into a new SVM. I've printed out the parameter dictionaries and confirmed that the output of the grid search feeds the parameters correctly into the new SVM ( sklearn.svm.SVC() ), and yet where the grid search achieved AUC of 0.89, on the same dataset I achieve just 0.5, and "no predicted samples" . To confirm, I set the param_grid to a 1x1 grid (i.e. no searching, just running 5-fold cross-val using SVM estimator) and it still "does well" only to drop to 0.5 AUC score afterwards. Can anyone please help diagnose this disparity? (Note that this code has a reduced logspace parameter space, to reduce running time after I found the most performant values) import numpy as np from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV def SVMParamTune(model_class, X, y, params): from sklearn.svm import SVC C_range = np.logspace(-2, -2, 1) gamma_range = np.logspace(3, 3, 1) param_grid = dict(gamma=gamma_range, C=C_range) # If the configuration contains other parameters, use these # These are the 'constants' to parameterising the SVM for param_name in params: if param_name not in param_grid.keys(): pass # param_grid[param_name] = params[param_name] cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42) # NB the SVC estimator provides a `score` method, # meaning GridSearchCV uses mean accuracy (from this estimator's score) grid = GridSearchCV(SVC(), param_grid=param_grid, scoring='roc_auc', cv=cv, n_jobs=10, verbose=10) grid.fit(X, y) print("The best parameters are %s with an AUC score of %0.2f" % (grid.best_params_, grid.best_score_)) print(grid.best_params_) for param in params: if param in grid.best_params_.keys(): params[param] = grid.best_params_[param] print(params) return params This is run, I see a good result, yet the result doesn't make it out of the tuning process for reasons unknown. m_params = SVMParamTune(m_class, X=data[0], y=data[1], params=m_params) The class ( sklearn.svm.SVC , stored in m_class ) is then run with these updated parameters, and yet gives different results model_run = m_class(**m_params) Switching StratifiedShuffleSplit to StratifiedKFold to be consistent with the latter SVM run made negligible difference so cannot be the cause. Update returning the model itself also gives the same changed AUC score - the model itself cannot be reproduced, on the same data...
