[site]: crossvalidated
[post_id]: 505729
[parent_id]: 505510
[tags]: 
There are a few solutions from a Fequentist perspective, but I'd like to offer a Bayesian perspective. The Model It would be useful to model the data generating process as a mixture of binomials; one with 0 bias and the other with some non-zero bias left to be estimated. You can then partially pool the data from all the coins to help estimation. Here is the model I'm using in math... $$ \mu \sim \operatorname{Beta}(1,1)$$ $$ \kappa \sim \operatorname{Half-Cauchy}(0,1) $$ $$ p \sim \operatorname{Beta}(1,1) $$ $$ b_i \sim \operatorname{Beta}(\mu \times \kappa, (1-\mu) \times \kappa) $$ $$ y_i \sim p \operatorname{Binom}(n_i, b_i) + (1-p)\operatorname{Binom}(n_i, 0.5)$$ Here, $p$ is the probability of producing a biased coin, $b_i$ is the probability of heads for the biased coins, which is assumed to come from a beta distrbution with parameters $\alpha = \mu \times \kappa$ and $\beta= (1-\mu) \times \kappa$ . If I were to write this in Stan, it might look like data{ int n; int heads[n]; int coin[n]; } parameters{ real prob_of_bias; vector [n] b; real mu; real kappa; } model{ prob_of_bias ~ beta(1,1); mu ~ beta(1,1); kappa ~ cauchy(0, 1); // the prior below takes a parameterization in terms of mu and kappa // and turns it into alpha beta parameterization b ~ beta_proportion(mu,kappa); for (i in 1:n){ target+=log_mix(prob_of_bias, binomial_lpmf(heads[i] | 200, b[i]), binomial_lpmf(heads[i] | 200, 0.5)); } Here is a simulation example. The probability of generating a biased coin is 0.77. Here is the simulation code n = 100 #Number coins is_biased = rbinom(n, 1 , 0.77) b = 0.5 + is_biased*(rbeta(n, 90, 10) - 0.5) x = rbinom(n, 100, p) #100 flips d = tibble(coin = factor(1:n), heads = x) Fitting this model, we see the estimated posterior means of each parameter recover their true values to within 1 decimal place (which is fine because the credible intervals capture the truth). Your Questions The question about the fraction of strongly biased coins has now been turned into a question of estimating $p$ from the model -- which is the probability of drawing a biased coin. We can obtain a variety of credible intervals but perhaps the easiest would be to just compute quantiles of the posterior for $p$ . Additionally, we can examine the probability that any given coin is biased by computing the posterior probability directly. Appending the following generated quantities block to the Stan model generated quantities{ matrix[n,2] ps; for (i in 1:n) { vector[2] pn; // log-probability that there is bias pn[1] = log(prob_of_bias) + binomial_lpmf(heads[i] | 200, b[i]); // log-probability that there is no bias pn[2] = log1m(prob_of_bias) + binomial_lpmf(heads[i] | 200, 0.5); // posterior probabilities for bias and no bias ps[i,] = to_row_vector(softmax(pn)); } Will produce a matrix in which the rows correspond to coins and the columns are probabilities of being biased and not being biased respectively. As an example, the first few coins from my simulation are 52 83 90 85 59 The middle 3 are biased coins. The first 5 rows of the matrix ps rounded to 4 decimal places are P(bias) P(No bias) [1,] 0 1 [2,] 1 0 [3,] 1 0 [4,] 1 0 [5,] 0 1 In this case, the probabilities are quite large, but when the bias is smaller these probabilities might be more reasonable (e.g. 30% and 70%). In such a case, you have a probabilistic model for bias and can leverage techniques from that literature (or from Bayesian decision theory, setting some sort of utility function) to decide if a coin is biased or not. I don't expect you to actually use this approach, and I'm sure similar approaches can be made with Expectation Maximization. Just thought I would throw my in two cents in case someone else found it interesting. Additionally, if this is something you are interested in, I can add details as needed.
