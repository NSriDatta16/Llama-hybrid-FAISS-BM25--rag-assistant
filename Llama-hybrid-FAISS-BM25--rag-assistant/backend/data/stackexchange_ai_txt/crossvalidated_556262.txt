[site]: crossvalidated
[post_id]: 556262
[parent_id]: 
[tags]: 
Differential entropy of a multivariate log-normal distribution

Let $Y$ be a multivariate log-normal random variable. As such, $\ln(Y)$ follows a multivariate normal distribution, which I denote by $\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})$ . I already know the normal distribution parameters ( $\boldsymbol{\mu}$ and $\boldsymbol{\Sigma}$ ), and I can calculate the mean and covariance matrix of the log-normal distribution with the appropriate equations ( like the ones shown on Wikipedia ). Question : How can I obtain the differential entropy of the multivariate log-normal distribution? Is the differential entropy of the corresponding normal distribution equivalent? If so, I could use this formula to calculate it: $$ H(Y) = \frac{1}{2}\ln{(2\pi e)^N\det{\boldsymbol{\Sigma}}} $$ where $N$ is the dimensionality of the multivariate distributions. Conceptually, the differential entropy is the "measure of average surprisal of a random variable [following a continuous probability distribution]". My reasoning is that converting a normal distribution to a log-normal one (and vice-versa) does not add or remove information. Therefore, the entropy of the log-normal distribution should remain the same as that of the corresponding normal distribution. Is that correct?
