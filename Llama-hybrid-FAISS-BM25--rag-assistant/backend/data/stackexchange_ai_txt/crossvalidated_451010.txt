[site]: crossvalidated
[post_id]: 451010
[parent_id]: 
[tags]: 
Matrices K and V in the decoder part in the transformer model?

There is something I do not get in the illustrated transformer article from Jay Alamar ( http://jalammar.github.io/illustrated-transformer/ ). In the decoder side paragraph, he said The encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. How the hell do we compute those K and V? . If i well understood, the output of the encoder is a matrice (Number of words x Embedding Length). So where those K and V come from please?
