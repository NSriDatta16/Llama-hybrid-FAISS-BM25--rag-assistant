[site]: crossvalidated
[post_id]: 302121
[parent_id]: 190315
[tags]: 
Regarding your question about whether reweighting training samples is equivalent to multiplying the loss in one of the two cases by a constant: yes, it is. One way to write the logistic regression loss function is $$\sum_{j=1}^J\log\left\{1+\exp\left[-f\left(x_j\right)\right]\right\}+\sum_{k=1}^K\log\left\{1+\exp\left[f\left(x_k\right)\right]\right\}$$ where $j$ and $k$ denote respective positive and negative instances, and $f(\cdot)$ is the logistic classifier built from features $x$. If you want to give more weight to your negative instances, for example, you might wish to modify your loss as $$\sum_{j=1}^J\log\left\{1+\exp\left[-f\left(x_j\right)\right]\right\}+\sum_{k=1}^Kw\log\left\{1+\exp\left[f\left(x_k\right)\right]\right\}$$ for some $w>1$. This loss function is minimized by software implementations of weighted logistic regression, but you could also arrive at the same answer by upweighting your negative instances by a factor of $w$ and fitting a standard logistic regression (for example, if $w=2$, then you create 2 copies of each negative instance and fit). Some further details on this kind of approach here . And there is a general warning about what happens to parameter standard errors here , but this may not be such a concern if you're solely doing prediction.
