[site]: crossvalidated
[post_id]: 86991
[parent_id]: 
[tags]: 
Reason for not shrinking the bias (intercept) term in regression

For a linear model $y=\beta_0+x\beta+\varepsilon$, the shrinkage term is always $P(\beta) $. What is the reason that we do not shrink the bias (intercept) term $\beta_0$? Should we shrink the bias term in the neural network models?
