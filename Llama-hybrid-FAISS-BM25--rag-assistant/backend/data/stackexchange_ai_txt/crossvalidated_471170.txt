[site]: crossvalidated
[post_id]: 471170
[parent_id]: 
[tags]: 
How is modeling the time series error/variance, e.g. ARCH or GARCH models, different from modeling time varying forecast intervals?

I'm having a hard time understanding the intuitive difference between modeling the volatility or variance of a time series as it is done in ARCH and GARCH models: $$Y_t = c+\epsilon_t+\phi_1Y_{t-1}+...+\phi_p Y_{t-p}+ \epsilon_t$$ and then we model $\epsilon_t$ as: $$\epsilon_t=\sigma_tz_t$$ with $z_t$ white noise, and $\sigma_t$ is modeled itself as its own time series process: $$\sigma_{t+h}=f(\sigma_t,\sigma_{t-1},\sigma_{t-2},...,\epsilon_t,\epsilon_{t-1},\epsilon_{t-2},...)$$ On one hand. On the other hand, using a sophisticated mean time series model where the forecast intervals are estimated "properly" as opposed to using some simplifying assumptions, so that there width is time dependent, e.g. when using FB Prophet and simulating the intervals using MCMC to get intervals that vary with the seasonality. Or better still, forecasting full densities, so that the entire shape of the distribution is time dependent, and therefore $\epsilon_t$ and $\sigma_t$ are time dependent as well. It seems to me that although the math and the terminology are different, whether we model the variance as its own explicit time series, or whether we just make sure that we model the forecast intervals as best as we can (using MCMC or by forecasting full densities, parametric or non parametric), the end result is the same: A time dependent uncertainty that is conditional on the previous uncertainty estimates. What am I missing? Why is volatility modeling considered a separate topic from simulating forecast intervals or modeling full forecast densities?
