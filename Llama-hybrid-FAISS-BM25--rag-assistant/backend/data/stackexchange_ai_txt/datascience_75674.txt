[site]: datascience
[post_id]: 75674
[parent_id]: 75656
[tags]: 
You can use word embeddings take a look at word2vec and glove. There are many good tutorials on word2vec, but very briefly word2vec represents a word as a vector, so that you can do a similarity comparison, and also allows addition and subtraction of vectors. The embeddings are already available, for example here: https://nlp.stanford.edu/projects/glove/ Here is an example, using glove embeddings: import numpy as np from sklearn.metrics.pairwise import cosine_similarity word_to_embeddings = dict() with open("/path/to/glove.twitter.27B.25d.txt") as f: for line in f: word = line.split()[0] embeddings = np.asarray(line.split()[1:], dtype='float32') word_to_embeddings[word] = embeddings w = word_to_embeddings sim_1 = cosine_similarity([w["chess"]], [w["cheese"]])[0][0] print(f"chess and cheese sim: {sim_1}") sim_2 = cosine_similarity([w["chess"]], [w["board"] + w["game"]])[0][0] print(f"chess and board + game sim: {sim_2}") sim_3 = cosine_similarity([w["football"]], [w["handball"]])[0][0] print(f"football and handball sim: {sim_3}") sim_4 = cosine_similarity([w["python"]], [w["programming"]])[0][0] print(f"python and programming sim: {sim_4}") sim_5 = cosine_similarity([w["c++"]], [w["programming"]])[0][0] print(f"c++ and programming sim: {sim_5}") Result: chess and cheese sim: 0.4501191973686218 chess and board + game sim: 0.7286249995231628 football and handball sim: 0.7467300891876221 python and programming sim: 0.7076062560081482 c++ and programming sim: 0.8262864351272583 The results are not going to be exactly what you expect, for example Python could also refer to a snake. You might want to play around with different embeddings, because they can give different results.
