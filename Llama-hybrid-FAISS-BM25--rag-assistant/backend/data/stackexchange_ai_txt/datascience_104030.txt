[site]: datascience
[post_id]: 104030
[parent_id]: 
[tags]: 
Samples to use when calculating SHAP values

I'm new to data science and I'm learning about SHAP values to explain how a Random Forest model works. I have an existing RF model that was trained on tens of millions of samples over a few hundred features. Also, the model tries to predict if a sample belongs to Class A or B, where the proportion is heavily skewed towards Class A, like 1000:1. I've read that calculating the SHAP values can be slow when there are a lot of samples and features. And I've come across shap_values = explainer.shap_values(X) where X can be the entire dataset, or just X_train , or X_test . So my questions are I guess the SHAP values will be different for X , X_train and X_test , but the general feature importance and their contribution to the prediction would still be very similar? Because I'm working with such a large and skewed dataset, I'm thinking of just calculating the SHAP values of a sampled dataset. If I take just 1% of samples belonging to Class A and B each, thus preserving the proportion (I'm afraid just randomly sampling the entire dataset would result in too few Class B samples), would this give me a reasonable global interpretation of the model? The model was trained some time ago using a dataset that may no longer be available to me. Can I calculate the SHAP values using a more recent dataset that is similar to the old data, or do I need to use the exact dataset that was used to train the model?
