[site]: crossvalidated
[post_id]: 577610
[parent_id]: 577599
[tags]: 
Logistic regression is for a response variable with discrete classes (TRUE/FALSE). It's fit by the maximum likelihood method, which uses an assumed probability distribution of model errors to calculate the joint probability that the data could have been produced by the model. Because "logistic regression" was one option of many in the calculator menu and this example is for continuous numeric data, I would guess that the calculator is applying a general purpose nonlinear optimization method to find the model parameters that minimize the sum of squares error. Unlike maximum likelihood, minimizing sum of squares error can be applied to any curve fitting problem without making deliberate choices about probability distributions. If this is true, it's not a true logistic regression, but rather curve fitting with nonlinear optimization applied to a logistic function. The most commonly-used algorithms for minimizing the sum of squares error of a nonlinear function are gradient-based. They're actually not limited to minimizing model error; they can minimize any smooth function you can define. They require a starting guess for the parameters, calculate a direction to change the parameters to reduce error, and take a step in that direction, and iterate until a local minimum is found. The most intuitive algorithm to accomplish this is steepest descent, which simply takes steps in the direction that most strongly reduces the error. It turns out that this isn't the most robust way to do it, so more commonly used algorithms are Conjugate Gradient and Levenberg-Marquardt.
