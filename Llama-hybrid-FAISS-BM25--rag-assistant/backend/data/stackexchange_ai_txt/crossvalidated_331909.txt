[site]: crossvalidated
[post_id]: 331909
[parent_id]: 331886
[tags]: 
This will be difficult if there's a class imbalance, where you have about the same or more non-clustered data than clustered data. If this is the case, normal K-means will suffer from poor choice of initial conditions and most of the time you'll end up with clusters in the non-clustered region. The usual fix for this is to just throw in more clusters in hopes that they will absorb the non-clustered data, but this will be painfully slow. So to make K-means work, you'll need to manually carve out a region of clustered vs non-clustered points. This might not be too bad: To make this scalable, first try hitting your data with PCA and investigate if there's any obvious separation between clustered/non-clustered in the first few components. You could also try tSNE if your data is highly nonlinear. Then start manually tagging points as clustered/non-clustered. If you're using tSNE for example, try to manually tag cluster and non-cluster points. Since tSNE preserves distances, try to pick clustered/non-clustered points that are closed to each other (i.e. on the "boundary" of region between clustered/non-clustered points). If PCA does a good job, you can define non-clustered points by isolating the quadrant they are in, and in effect completely throwing those points out. Otherwise, using the above tagging scheme, you can then train an SVM to find a separating hyperplane. You could also use any reasonable classifier, even small random forests. The nice thing about an SVM is it will give you an obvious separating hyperplane. After that, predict your cluster points, and throw out the predicted non-clusters. If you really do have solid separation between clustered/non-clustered points, the above scheme should work reasonably well after tagging say, a few hundred points.
