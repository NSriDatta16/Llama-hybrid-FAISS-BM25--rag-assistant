[site]: crossvalidated
[post_id]: 553278
[parent_id]: 
[tags]: 
When do Efficient Decision Algorithms for 1D Anomaly Detection exist (compared to threshold-tests)?

I'm tasked with investigating whether machine learning algorithms can be used to efficiently identify if a certain type of anomaly is present in the temporal spacing of incoming network packets. Identifying the anomaly using threshold tests takes a fairly long time, since it is pretty small. However my initial experiments with ML-algorithms didn't show a lot of promise in reducing this detection time, so I'm trying to mathematically determine wether ML would even garner a significant performance increase. The problem can be modeled as follows: Let R and c be random variables from $\mathbb{R}^+$ with known probability distributions and c How many times does X need to be measured to determine within reasonable certainty whether X = R or X = R + c? Under which circumstances do decision algorithms with higher efficiency than simple threshold-tests exist? The second question is the important one. This sounds like the kind of problem where a lot of research in its general direction has already been done. Could you give me some recommended reading or refer me to theorems that deal with this type of problem? My mathematical intuition would suggest that if R and c are normal, threshold-tests are always the most efficient decision algorithms. However it is also fairly easy to come up with situations where this isn't the case (e.g. if the probability density of R has sharp peaks, detecting their movement could be done with higher precision than for the whole of R)
