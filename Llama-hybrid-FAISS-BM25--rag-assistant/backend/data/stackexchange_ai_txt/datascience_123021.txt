[site]: datascience
[post_id]: 123021
[parent_id]: 61095
[tags]: 
I will answer part of your question directly, "Is it practical to compress time series with losses using a neural network if the compression time does not matter". You try to use an autoencoder to compress data - however, autoencoders without correct regularization methods (or more advanced ideas) are prone to finding simple solutions (you don't put any constraint on the network, so it will very likely converge to the simplest solution it can find that minimizes your loss) that might not generalize well to other data points. I suggest that you split the dataset to train and test and check how well your autoencoder performs on the test set. If you care about unseen data points (sometimes you don't but in 99% of the use cases you do) the next step, after you probably see that your results are sub-optimal, is to look into something like VAE (variational autoencoders). All that being said, if you have another method that is not neural, it's mostly safer to stick to that (maybe look into SVD-based approaches, I am not an expert). Neural networks tend to be unpredictable and you probably should not add another failure point to the system unless you have to. Another plus of "classical" approaches is that it often comes with some theoretical guarantees and information about the amount of variance you get to keep (i.e., "how much" you lose, not what information though, it's "how much" - I find the "what" part often impractical and complicated to interpret). Neural networks are very practical to find a good representation for downstream tasks but I am not sure it's a requirement from your side (and it often requires some method of self-supervision/proxy task, e.g. word embeddings).
