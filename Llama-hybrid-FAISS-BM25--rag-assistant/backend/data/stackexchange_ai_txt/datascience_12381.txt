[site]: datascience
[post_id]: 12381
[parent_id]: 8208
[tags]: 
So let me clarify your query: you have trained a random forest model to classify a dataset into multiple classes, e.g. A, B, C, or D. Now, you want to understand for each different class label (e.g. A), which features contributed to it being classified as class A vs. not class A, and so on. To find the importance of a variable in random forest, each variable is permuted among all trees and the difference in out of sample error of before and after permutation is calculated. The variables with highest difference are considered most important, and ones with lower values are less important. this method gives importance for the entire multinomial classification, i.e. A vs. B/C/D, B vs. A/C/D, etc. and not just for one label. So you can't claim that feature1 is more important to identify class A, but feature 2 is more important to identify Class B. I've been using the random forest package implemented in R and it does not seem to provide any way to decipher the internal details on the one-vs-all categorization. If you're really interested in doing just that, then you should fit a set of random forest models each for classifying class A vs. the rest, class B vs. the rest, etc. The variable importance of these separate models would give you the feature importance for classifying each label. You can use the same library to fit these models using scikit-learn. You may refer to their help page here - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html
