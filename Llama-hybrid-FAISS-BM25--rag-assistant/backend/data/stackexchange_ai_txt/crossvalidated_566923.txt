[site]: crossvalidated
[post_id]: 566923
[parent_id]: 566921
[tags]: 
To possibly make this less mysterious, consider a Normal mean and consider just scaling it. With data from $N(\mu,1)$ , $\bar X_n\sim N(0,1/n)$ has MSE $1/n$ $\alpha\bar X_n\sim N(\alpha\mu, \alpha^2)$ has MSE $$(1-\alpha)^2\mu^2+\alpha^2n= (\mu^2-2\alpha\mu^2+(\mu^2+1/n)\alpha^2) $$ As $\alpha$ decreases from 1, the MSE decreases at first, because the increase in bias is outweighed by the decrease in variance, then the trend reverses. If you know, say, $\mu\in [-0.6,0.6]$ , you can pick a value of $\alpha$ that reduces the bias for all $\mu$ in that range, with the biggest reduction being at $\mu=0$ . The small reduction in MSE is real, but it goes away pretty fast as $n$ increases, and requires knowing a bound on $\mu$ . It also requires that MSE is what you care about -- that you're happy to trade variance for bias, and give up on confidence interval coverage. In three or more dimensions you can get reduced MSE in a somewhat related way for all values of $\mu$ (though it still goes away as $n$ increases) using the James-Stein estimator.
