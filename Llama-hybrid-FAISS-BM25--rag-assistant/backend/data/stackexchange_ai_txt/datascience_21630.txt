[site]: datascience
[post_id]: 21630
[parent_id]: 21629
[tags]: 
Each hidden layer represents a nonlinear application of a function on the inputs of the previous layer. The first layers perform a function on our data-inputs and the last layer is responsible for either using softmax for classification purposes or be an affine layer for regression. The in-between layers are necessary in order to stack complexity and therefore avoid having to create a layer of very high complexity. In fact, in theory, two level deep neural network can represent any function. But how complex would this function need to be? A layer of a very high complexity would be able to map our raw inputs into our final classification or regression task. However what would be simpler to do is to let the intermediate hidden layers learn useful features regarding the given task and then the next hidden layer could exploit these features in its own way. And then the next hidden layer could use these newly transformed features in its own way and so on. So stacking nonlinear functions one on top of the other is more clever than requiring a single nonlinear function to do all the hard work. There is a limit on how many layers you can stack together for the main reason that these hidden layers get trained by backpropagation which uses the chain rule of derivation in order to pass the deltas of the error back to the weights of the layers and tune them but a long series of these operations could end-up with exploding or vanishing gradients which means really poor performance or a model that cannot be trained.
