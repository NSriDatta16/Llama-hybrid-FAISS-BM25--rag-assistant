[site]: crossvalidated
[post_id]: 387447
[parent_id]: 386254
[tags]: 
Your working is correct, but your method has the disadvantage that you have set the prior probabilities of your hypothesis to values that are fixed by your prior distribution for the parameter. In Bayesian hypothesis testing we usually want to have the freedom to vary the prior probabilities of the overall hypotheses (i.e., classes of parameter values), while keeping the form of the prior distribution when conditioning on particular hypotheses. We can do this in the present case by formulating the more general model: $$\begin{equation} \begin{aligned} X_1,...,X_n | \lambda &\sim \text{IID Pois}(\lambda), \\[10pt] \pi(\lambda | H_0) &\propto \text{Ga}(\lambda|\alpha, \beta) \cdot \mathbb{I}(\lambda \leqslant \lambda_0), \\[10pt] \pi(\lambda | H_1) &\propto \text{Ga}(\lambda|\alpha, \beta) \cdot \mathbb{I}(\lambda > \lambda_0). \\[10pt] \end{aligned} \end{equation}$$ In this generalised model we can vary the prior probability $\phi = \pi(H_0)$ while maintaining the gamma form for the parameter value (truncated under each hypothesis so that its support is only over the class of parameters in that hypothesis). This allows us to conduct the hypothesis test for any chosen prior probability of the hypotheses. Implementing the generalised hypothesis test: Bayesian hypothesis testing is usually done by calculating Bayes' factor . This allows you to find the posterior probability of the hypotheses under any specified prior probabilities for the two hypotheses in your test. First we will confirm your derivation of the likelihood and posterior. In this case, you have likelihood function: $$L_{\mathbf{x}}(\lambda) \propto \prod_{i=1}^n \text{Pois}(x_i|\lambda) = \prod_{i=1}^n \frac{\lambda^{x_i}}{x_i!} \exp(-\lambda) \propto \lambda^{\sum x_i} \exp(-n\lambda).$$ Combining this with the prior gives you the (hypothesis-conditional) posterior distributions: $$\pi_n(\lambda|H_0) \propto \text{Ga}\Big( \lambda \Big| \alpha + \sum_{i=1}^n x_i, \frac{\beta}{1 + n \beta} \Big) \cdot \mathbb{I}(\lambda \leqslant \lambda_0),$$ $$\pi_n(\lambda|H_1) \propto \text{Ga}\Big( \lambda \Big| \alpha + \sum_{i=1}^n x_i, \frac{\beta}{1 + n \beta} \Big) \cdot \mathbb{I}(\lambda > \lambda_0).$$ This confirms that your derivation of the posterior is correct, though we have generalised your model to allow variation of $\phi$ separately from $\alpha$ and $\beta$ . From here we obtain the Bayes factor: $$\begin{equation} \begin{aligned} BF(\mathbf{x}) &\equiv \frac{p(\mathbf{x}|H_1)}{p(\mathbf{x}|H_0)} \\[6pt] &= \frac{\int \pi_n(\lambda|H_1) \ d\lambda}{\int \pi_n(\lambda|H_0) \ d\lambda} \cdot \frac{\int \pi(\lambda|H_0) \ d\lambda}{\int \pi(\lambda|H_1) \ d\lambda} \\[6pt] &= \frac{\int_{\lambda_0}^\infty \pi_n(\lambda|H_1) \ d\lambda}{\int_0^{\lambda_0} \pi_n(\lambda|H_0) \ d\lambda} \cdot \frac{\int_0^{\lambda_0} \pi(\lambda|H_0) \ d\lambda}{\int_{\lambda_0}^\infty \pi(\lambda|H_1) \ d\lambda} \\[6pt] &= \frac{\Gamma(\alpha + \sum x_i)-\gamma(\alpha + \sum x_i, \beta \lambda_0 /(1+n\beta))}{\gamma(\alpha + \sum x_i, \beta \lambda_0 /(1+n\beta))} \cdot \frac{\gamma(\alpha, \beta \lambda_0)}{\Gamma(\alpha)-\gamma(\alpha, \beta \lambda_0)}. \\[6pt] \end{aligned} \end{equation}$$ Using the Bayes factor you have: $$\frac{\mathbb{P}(H_1|\mathbf{x})}{\mathbb{P}(H_0|\mathbf{x})} = \frac{\mathbb{P}(H_1)}{\mathbb{P}(H_0)} \cdot BF(\mathbf{x}) = \frac{1-\phi}{\phi} \cdot BF(\mathbf{x}).$$ In your case you have decided to reject $H_0$ if this posterior ratio exceeds one (i.e., if the alternative has higher posterior probability than the null), and you have implicitly constrained the prior probability of the null hypothesis to $\phi = \gamma(\alpha,\beta \lambda_0)/\Gamma(\alpha)$ . The advantage of expressing things in our more generalised form is that we can choose any prior probability $\phi$ , irrespective of the other model parameters.
