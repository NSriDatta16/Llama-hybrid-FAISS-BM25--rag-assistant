[site]: crossvalidated
[post_id]: 485201
[parent_id]: 
[tags]: 
Loss function depending on the derivative of a neural network with respect to the input in tensorflow

I have a neural network $x \mapsto f(x, \theta)$ , and I can access predictions in my code with out = model(X) . Imagine that I have a loss function $l(x,y) = (y-\frac{\partial f(x, \theta)}{\partial x}\cdot C)^2$ where $C$ is a constant vector. I think that I can approximate the derivative and write the code like this eps = 0.001 pred = ... # compute the gradient with a formula like (f(x(1+eps))-f(x(1-eps)))/2x*eps # and make the dot product with C loss = tf.math.squared_difference(y, pred) # loss for one data point Is it possible to compute the gradient directly with respect to the input such that the output is still a function of $\theta$ and can be optimized by tensorflow? If so, what should I write in the line pred = ... ? Thanks
