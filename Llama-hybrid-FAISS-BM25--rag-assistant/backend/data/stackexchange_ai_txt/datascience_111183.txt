[site]: datascience
[post_id]: 111183
[parent_id]: 111176
[tags]: 
It looks to me like topic modeling methods would be a good candidate for this problem. This option has several advantages: it's very standard with many libraries available, and it's very efficient (at least the standard LDA method) compared to calculating pairwise similarity between documents. A topic model is made of: a set of topics, represented as a probability distribution over the words. This is typically used to represent each topic as a list of top representative words. for each document, a distribution over topics. This can be used to assign the most likely topic and consider the clusters of documents by topic, but it's also possible to use some subtle similarity between the distribution. The typical difficulty with LDA is picking the number of topics. A better and less known alternative is HDP , which infers the number of topics itself. It's less standard but there are a few implementations (like this one ) apparently. There are also more recent neural topic models using embeddings (for example ETM ). Update Actually I'm not really convinced by the idea to convert the data into a graph: unless there is a specific goal to this, analyzing the graph version of a large amount of text data is not necessarily simpler. In particular it should be noted that any form of clustering on the graph is unlikely (in general) to produce better results than topic modelling: the latter produces a probabilistic clustering based on the words in the documents, and this usually offers a quite good way to summarize and group the documents. In any case, it would possible to produce a graph based on the distribution over topics by document (this is the most natural way, there might be others). Calculating a pairwise similarity between these distributions would represent closely related pairs of documents with a high-weight edge and conversely. Naturally a threshold can be used to remove edges corresponding to low similarity edges.
