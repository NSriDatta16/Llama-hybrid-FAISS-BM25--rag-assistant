[site]: crossvalidated
[post_id]: 404165
[parent_id]: 402559
[tags]: 
Question 1 : You are incorrect that "we don't need any distributional assumption for the biomarkers in logistic regressions." A single-predictor logistic regression specifically assumes that the log-odds of the binary outcome are linearly related to the values of the predictor. So if you are using, say, RNAseq data as predictors you will get different results (for coefficients and p -values) if you use sequence counts instead of log-transformed counts. It is not at all surprising that logistic regression, with that strong parametric assumption, and the non-parametric rank-based Wilcoxon test are giving different p -values. Logistic regression might be more powerful (better ability to detect true significant associations) when the linearity assumption is met, but not when the assumption is violated. The validity of the linearity assumption might differ among predictors. That said, you should be wary of using any set of single-predictor tests to select components for your composite score. Logistic regression has an inherent omitted-variable bias such that if you omit any predictor related to outcome from a model you will bias the coefficients of the includes predictors. See this answer and its links as one of many on this site that discuss these dangers. Question 2 : Based on the above, your Method 3 has substantial problems as it relies on a whole set of logistic regressions each of which omits many predictors related to outcome. The fourth method would be preferable, but a related approach described below could be even better, depending on the scale of your problem. Question 3 : Although AUC is better than some measures of model performance, it has significant drawbacks for model comparison. The best way to evaluate a model that predicts a probability of an outcome is to use a proper scoring rule like the Brier score . You also need to be thorough in how you perform your comparisons. You should be evaluating each entire model-building process starting from the initial data, with bootstrapping or cross-validation, particularly when your modeling used the outcomes to select the predictors. Alternate approaches : These depend on whether you are evaluating a few dozen potential predictors (as in some clinical studies) or thousands of them (as in RNAseq studies). In the first case you should consider approaches like those recommended by Harrell's Regression Modeling Strategies . Chapter 11 of the second edition is a clinical case study that illustrates how to perform data reduction (including linear and nonlinear principal components), selection among modeling variations, backward variable selection from a full model to simplify, and model evaluation for logistic regression. In the second case you should be using a principled way to select and weight the predictors for a composite score. LASSO comes immediately to mind. This can be thought of as starting with the best individual predictor but then adding additional predictors in a way that avoids the overfitting seen in standard stepwise approaches . An Introduction to Statistical Learning provides one accessible presentation in Chapter 6 with a worked example for standard linear regression, but the glmnet() function illustrated there (of the R package having the same name) also allows for logistic regression. This would provide you with something similar to your Method 4 in Question 2, but with a more reliable basis. You might also consider the Elastic Net , a combination of LASSO and ridge regression , that minimizes the instability in LASSO predictor selection when there are multiple correlated predictors. Statistical Learning with Sparsity describes Elastic Net starting in Chapter 4. Elastic Net can also be implemented via glmnet() . A final warning: if you are going to use any of these linear regression approaches you need to document the linear relationship between the predictors and the log-odds of outcome. I suspect that a failure of that linear relationship for some of your candidate predictors led to your original question about different results with logistic regression and Wilcoxon test results, so you don't want to face that problem again farther down the road.
