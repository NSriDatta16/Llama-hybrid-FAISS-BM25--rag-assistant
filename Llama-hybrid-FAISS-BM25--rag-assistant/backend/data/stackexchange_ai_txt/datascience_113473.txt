[site]: datascience
[post_id]: 113473
[parent_id]: 113180
[tags]: 
I ended up with the transformer model because: It makes no assumptions about the temporal/spatial relationships across the data. This is ideal for processing a set of objects (for example, StarCraft units). To generate features, I iterated over the time range (every x hours) and got all vehicles that are in service at the time point t1 and plan to be in service at the time point t2 (t1 + y hours). Then I calculated features relative to these two time points: how long t1 vehicles have already spent in service, in how many hours t2 vehicles plan to arrive etc. Now we have bags of vehicles with different time features. The target variable is the status for all vehicles at the time point t2 (in service/not in service, a vector of zeros and ones for each bag). As the number of vehicles is different in each bag, I created a custom loss function and an attention mask to mask padding.
