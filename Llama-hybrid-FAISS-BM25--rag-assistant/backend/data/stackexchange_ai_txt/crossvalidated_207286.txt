[site]: crossvalidated
[post_id]: 207286
[parent_id]: 207274
[tags]: 
The "probability loss" function has sometimes been called the "linear score" in the literature. Although it looks appealing, this loss function is improper, which means that it does not set the incentive to forecast the true probability that $y_i = 1$. For details, see p. 366 of Gneiting and Raftery ("Strictly Proper Scoring Rules, Prediction, and Estimation", Journal of the American Statistical Association , 2007). In practice, impropriety means that a silly forecaster (who, for example, skews his probabilities towards the extremes of zero and one) may obtain a better probability loss than a reasonable forecaster. The following example, based on R code, illustrates this point. First, set a random seed ( set.seed(1) ) and fix a sample size ( n ) Simulate an arbitrary vector of true probabilities: p_true Now, draw a vector of binary observations which follow these probabilities: y Suppose Anne is an omniscient forecaster, and knows the true probabilities p_true . Her probability loss (on average over cases) can be computed as follows: loss_Anne By contrast, consider a second forecaster (Bob) who makes overconfident predictions, according to the following formula: p_wrong = 0.5)) . The formula means that Bob skews the "small" probabilities (less than 50 percent) towards zero, and the "large" probabilities (more than 50 percent) towards one. Bob's average loss is given by loss_Bob Running this code on my PC, I find that loss_Anne is about $0.33$, whereas loss_Bob is about $0.29$. Thus, a perfect forecaster (Anne) loses to an overconfident forecaster who deliberately skews his probabilities towards the extremes of zero and one. Thus, the probability loss should not be used for model comparison, as it will generally not select the true model (even asymptotically). Instead, a strictly proper scoring function like the logarithmic loss or Brier score should be used. Again see the reference mentioned above.
