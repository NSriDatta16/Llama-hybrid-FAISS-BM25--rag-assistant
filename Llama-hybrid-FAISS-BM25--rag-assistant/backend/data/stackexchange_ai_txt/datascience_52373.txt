[site]: datascience
[post_id]: 52373
[parent_id]: 52367
[tags]: 
"Tokens" are usually individual words (at least in languages like English) and "tokenization" is taking a text or set of text and breaking it up into its individual words This is by far the simplest definition you can get about tokens. Consider a sentence as follows:- "Data is the new oil". Now humans can interpret the same sentence in a number of ways like - "We can call data as the new oil", "Data can also be called as the new oil". Now all these sentences almost mean the same and we as humans understand the sentence by reading the sentence word by word and associate the meaning it creates by putting it together. As Deep learning is closely associated with replicating human-like methods, tokenization is one such method of simplifying the meaning of individual words and then creating a sense out of the sentence. Read further about this at Tokenization tutorial
