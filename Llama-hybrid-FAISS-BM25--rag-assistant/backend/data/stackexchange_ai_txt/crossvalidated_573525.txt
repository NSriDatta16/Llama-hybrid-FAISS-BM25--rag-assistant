[site]: crossvalidated
[post_id]: 573525
[parent_id]: 470804
[tags]: 
A word embedding is a learned look up map i.e. every word is given a one hot encoding which then functions as an index, and the corresponding to this index is a n dimensional vector where the coefficients are learn when training the model. A positional embedding is similar to a word embedding. Except it is the position in the sentence is used as the index, rather than the one hot encoding. A positional encoding is not learned but a chosen mathematical function. $\mathbb{N}\rightarrow\mathbb{R}^n$ .
