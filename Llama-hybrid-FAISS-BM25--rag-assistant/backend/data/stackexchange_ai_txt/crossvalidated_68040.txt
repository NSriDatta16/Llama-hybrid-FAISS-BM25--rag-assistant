[site]: crossvalidated
[post_id]: 68040
[parent_id]: 62368
[tags]: 
Other than the issue of what mechanism caused the missing data in the first place (missing at random, missing not at random, missing completely at random) the problem with using imputation by regression is that it does not allow for uncertainty in the imputed values - and hence standard errors will be too small. You could extend the idea by allowing uncertainty by adding "noise" to the predictions, say based on the residual variance of the regression and doing this several times to obtain a distribution of plausible values. You could further extend the idea by allowing for uncertainty in the regression parameters too - eg. using a Bayesian regression model. What you then have is several completed datasets, each one containing different plausible values for those that were missing. This is the basis for "multiple imputation" using chained equations, and this is essentially how particular methods in the MICE package are implemented. Models are then run on each set of completed data and the results pooled according to well-established rules (Rubin's rules). A further issue to consider is that there should be a correspondence between the imputation and the model of interest (this is known as "congeniality" in the multiple imputation literature). There is a huge literature on this. A great starting point is the classic book by Little and Rubin: Statistical Analysis with Missing Data, 2nd Edition (Wiley Series in Probability and Statistics) 2002
