[site]: crossvalidated
[post_id]: 236840
[parent_id]: 
[tags]: 
Different probability density transformations due to Jacobian factor

In Bishop's Pattern Recognition and Machine Learning I read the following, just after the probability density $p(x\in(a,b))=\int_a^bp(x)\textrm{d}x$ was introduced: Under a nonlinear change of variable, a probability density transforms differently from a simple function, due to the Jacobian factor. For instance, if we consider a change of variables $x = g(y)$ , then a function $f(x)$ becomes $\tilde{f}(y) = f(g(y))$ . Now consider a probability density $p_x(x)$ that corresponds to a density $p_y(y)$ with respect to the new variable $y$ , where the sufﬁces denote the fact that $p_x(x)$ and $p_y(y)$ are different densities. Observations falling in the range $(x, x + \delta x)$ will, for small values of $\delta x$ , be transformed into the range $(y, y + \delta y$ ) where $p_x(x)\delta x \simeq p_y(y)δy$ , and hence $p_y(y) = p_x(x) |\frac{dx}{dy}| = p_x(g(y)) | g\prime (y) |$ . What is the Jacobian factor and what exactly does everything mean (maybe qualitatively)? Bishop says, that a consequence of this property is that the concept of the maximum of a probability density is dependent on the choice of variable. What does this mean? To me this comes all a bit out of the blue (considering it's in the introduction chapter). I'd appreciate some hints, thanks!
