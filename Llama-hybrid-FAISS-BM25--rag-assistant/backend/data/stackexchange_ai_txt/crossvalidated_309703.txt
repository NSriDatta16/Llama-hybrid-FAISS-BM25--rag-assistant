[site]: crossvalidated
[post_id]: 309703
[parent_id]: 
[tags]: 
How to adjust unbalanced dataset (>99% y=1, <1% y=0)

I am working on a fairly standard data science project for a Company, where I have been tasked with answering the question "What factors are most important when determining whether or not a potential customer will turn into a buying customer". The product being offered here is legal software. The Company automatically collects data on all users / companies that visit their website, and as a result I have a dataset with 200K rows, where each row represents a prospective customer but only 1K rows are purchasing customers (y=1) whereas 199K did not purchase (y=0). The data has ~70 feature variables. I think it would be helpful for my analysis to have a more balanced dataset. I have a few ideas for how I can go about doing this: Randomly remove 189K of the 199K rows, leaving a dataset that is still unbalanced, but much more manageable (90% / 10%, instead of 99.5% / 0.5%). Remove the 189K rows with the most missing values (this dataset is loaded with missing values, in fact every single row has at least a couple missing values). Filter rows based on one of the columns. For example, if a feature variable "# of times visited the site" is == 1 roughly 50K times, and in all 50K of those rows, the customers did not purchase (y=0), then toss these rows. Any other thoughts would be appreciated. As a reminder, this is a project on variable importance, not necessarily on building the most accurate model. I understand that the two are tied to a sense, but ultimately I need to provide the best predictor variables as my deliverable. Thanks! EDIT - I'm approaching the project in a few ways - (1) I am fitting ML classification models that handle missing values particularly well in R (decision trees, GBM, random forest) and examining their variable importance plots. For (1), I don't think adjusting the dataset is important. Also (2) I am running statistical tests for each individual dependent variable with the y variable. For integer-value feature variables, I am computing linear regression fits, logistic regression fits, correlations (point biserial correlation), and Mann Whitney U Statistic (my data is not normally distributed, so I did not do a t-test / 2 sample mean test). For categorical-value feature variables, I am computing Chi Squared P Value and Cramer's V statistic between each variable and the y variable. I think for these statistical tests, having a more balanced dataset is helpful. In particular, wouldn't tests like the mann-whitney test, and the chi-squared test, provide better results with a more balanced dataset?
