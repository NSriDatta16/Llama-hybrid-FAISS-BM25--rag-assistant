[site]: crossvalidated
[post_id]: 572947
[parent_id]: 572116
[tags]: 
You can start with the Why is accuracy not the best measure for assessing classification models? thread. it gets the average correctness of the predictions and in some cases its result can be different from reality What do you understand by reality here? Say that you split the data on train and test set, you can calculate train accuracy and test accuracy. The training accuracy can differ from test accuracy, for example, because the model is overfitted to the training set. But if you applied the model in the wild, the test accuracy might differ from the prediction time accuracy if your test set differed from the prediction time data. Notice that this would apply to any metric, not only accuracy. We can hope that the metrics calculated on the data we have can be extrapolated to the unseen data, but you can never be sure. This is not about accuracy. Is it right to say that since accuracy averages, the real result could be composed by both half very distant and half very close values from reality? Hard to say what you mean here. Say that you have a hot-dog vs not-a-hot-dog classifier. The classification can be either correct (e.g. hot-dog classifier as a hot-dog) or not (e.g. hot-dog classified as not-a-hot-dog). You cannot say that some of those classifications are "more correct" than others because it is binary. To measure something like "how far from reality" you would need a continuous value, so you could calculate some distance. For example, if your model predicts probabilities, that hot-dogs should be predicted with high probabilities in general, where some edge cases (e.g. blurry picture, or a non-standard hot-dog) would have lower probabilities. That is one of the reasons accuracy is a rather rudimentary metric, as it looks only at the hard classifications and doesn't consider the scores (probabilities) predicted by the model. Is it right to say that accuracy might suggest a good result is achieved, while in reality the result is a disaster?" How do you define "good"? If you define "good" as "having high accuracy" then by definition it would be correct. As said above, accuracy looks only at hard classifications so it will not differentiate between two classifiers in terms of how well-calibrated the probabilities they return (so how "sure" they are about their predictions). It has also many issues discussed in the link above. But again, in general, you would have this problem with any metric. Each metric measures how "good" is a result under some definition of "good". Different metrics would consider different things as "good" and there is no single metric that would agree with all the other metrics. There is no one-size-fits-all metric.
