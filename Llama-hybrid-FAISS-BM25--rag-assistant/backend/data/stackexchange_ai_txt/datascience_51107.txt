[site]: datascience
[post_id]: 51107
[parent_id]: 51092
[tags]: 
I am not clear on how you convert your N rows into a single row. Do you concatenate all of them? This is an option but could become problematic over longer sequence lengths. A straightforward method could be to simply average the features across time-steps. Although you lose causal information, this has been shown to work surprisingly well for word embeddings. Another standard approach is to use hidden markov models. Very simply, a hidden markov model assumes a latent state space which generates an output (in this case your feature) at every time-step conditioned on the previous state. https://en.wikipedia.org/wiki/Hidden_Markov_model Today the practice is to use sequence neural models such as LSTMs and GRUs which can hold memory over longer time-steps. https://en.wikipedia.org/wiki/Long_short-term_memory I suggest your read up on these methods and select the one that suits your purpose :)
