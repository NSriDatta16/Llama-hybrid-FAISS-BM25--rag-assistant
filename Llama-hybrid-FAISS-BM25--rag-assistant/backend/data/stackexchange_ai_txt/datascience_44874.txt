[site]: datascience
[post_id]: 44874
[parent_id]: 44839
[tags]: 
I think I have a solution. In special cases I do the average of two crossentropy loss, one assuming wrong and other assuming right ("half correct") taking in consideration the probabilities of the output softmax of the model. Again ONLY IN THE "SPECIAL CASES" :-) . It is in keras TF so we have to do all things in "declarative vectorial programming" (no cycles or if's ) : def my_loss(y_pred, y_true): y_pre_indexes = K.argmax(y_pred, axis=1) y_test_indexes= K.argmax(y_true, axis=1) #identify special cases #True special cases - False all others TN = K.tf.logical_or( K.tf.logical_or (K.tf.logical_and(K.equal(y_pre_indexes,0),K.equal(y_test_indexes,1)), K.tf.logical_and(K.equal(y_pre_indexes,1),K.equal(y_test_indexes,0))) , K.tf.logical_or (K.tf.logical_and(K.equal(y_pre_indexes,3),K.equal(y_test_indexes,4)), K.tf.logical_and(K.equal(y_pre_indexes,4),K.equal(y_test_indexes,3)))) #1.00 special cases - 0.00 all athoers (float) TN_float = K.cast(TN, K.floatx()) #1 special cases - 0 all athoers TN_int=K.tf.cast(TN_float,K.tf.int64) #0 special cases - 1 all others TN_int_inverse=(TN_int*(-1))+1 # make a tensor with all 0's but the predictions of special cases (in indexes) classes_only_TN=TN_int*y_pre_indexes # make 0 the special cases in y_test indexes real_classes_remove_TN=TN_int_inverse*y_test_indexes # sum the two and we get a simulated y_test with the special cases correct and all other normal simulated_index=classes_only_TN+real_classes_remove_TN #make this simulated y_test one_hot to feed crossentropy simulated_one_hot=(K.tf.one_hot(simulated_index,5)) #get the result of crossentropy assuming special cases correct (the simulated y_test) crosentr_result_sim=K.categorical_crossentropy(y_pred, simulated_one_hot) #get the result of crossentropy assuming special cases wrong (the real y_test) crosentr_result_real=K.categorical_crossentropy(y_pred, y_true) # make the float 1.00 that identify special cases be 0.5, all othes continue 0.0 TN_float_half=TN_float/2 #The result of testing with the simulated y_test will be devided bt 2 only on special cases # and 0 for all others crosentr_result_sim_only_half_TN=crosentr_result_sim*TN_float_half # Have a tensor with (0.5 for special cases and 1 for all others) TN_float_inverse=((TN_float*(-1))+2)/2 # devide the result of special cases by 2 only on special cases on the real y_test crosentr_result # maintain all others crosentr_result_real_half_TN=crosentr_result_real*TN_float_inverse #sum the two above: on simulated y_test we have all 0's except special cases wue have half of the value # on real y_test we have all the real values of loss except special case where we have half final_result_loss=crosentr_result_real_half_TN+crosentr_result_sim_only_half_TN #sum the two and we have average(half correct answer) only for special cases # in result we lower the error on special cases ... and global return final_result_loss [EDIT] I have tested this loss with some tries and in global it does not improve the winning positions in the confusion matrix (on the final validation set - after test set)... Then I also try to change the target values (on train and test) like this : def convert_probs(arg): test= arg for i in range(len(test)): if test[i][0] == 1: test[i] = [0.8, 0.2, 0, 0, 0] elif test[i][1] == 1: test[i]=[0.20,0.80,0,0,0] elif test[i][3] == 1: test[i]=[0,0,0,0.8,0.2] elif test[i][4] == 1: test[i]=[0,0,0,0.2,0.8] return test and, with this transformation of the target distribution, use the kl_divergence loss function in the model but again it does not improve in relation to have normal target probabilities (with 1's on the right class) and using categorical cross entropy. So I will stick to the normal modeling of the problem and not force the model to go to the right "quadrants" of the confusion matrix.
