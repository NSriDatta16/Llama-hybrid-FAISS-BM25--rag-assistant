[site]: datascience
[post_id]: 49075
[parent_id]: 49071
[tags]: 
Generally speak, no. Deep learning models struggle to compete when it comes to tabular data. If we head over to kaggle were people compete to build the best model we find that usually the best performing non-ensemble models for this kind of data are gradient boosting trees. More specifically it tends to be either XGBoost or more often now days it is LightGBM that performs best. Both are highly optimized implementations of gradient boosting trees. Feature engineering and parameter tuning are both important to get extra performance. But the gap does not shrink since you would get the same improvements if you did it on a gradient boosting model instead. Actually other models tend to gain more from feature engineering than neural networks since one of the strengths of neural networks is that they perform a sort of automatic feature engineering when they are trained. Neural networks usually shine in the domain of unstructured data such as text and images.
