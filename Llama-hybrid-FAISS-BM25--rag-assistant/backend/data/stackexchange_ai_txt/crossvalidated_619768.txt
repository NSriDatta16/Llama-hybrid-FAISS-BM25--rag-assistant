[site]: crossvalidated
[post_id]: 619768
[parent_id]: 619754
[tags]: 
One thing that usually works quite well is to use a Bayesian approach, because you can exactly write down your hypothesized data-generating-process as your likelihood, set priors (if you don't really want priors, make them vague). This approach often even has very good small sample properties as a frequentist method. Importantly, fitting a Bayesian model and then transforming model parameters deals really well with things like inference about differences of proportions (as opposed to, say, log-odds ratios). In general, something like Stan let's you write down almost any model (as long as you don't have discrete parameters that can't easily be integrated out). Alternatively, higher level-interfaces like brms should actually be able to handle a relatively simple case like this one. E.g. code like the following could work, if you want to answer the question of this difference in a new randomly drawn member of the population: library(tidyverse) library(brms) example % as_tibble() %>% mutate(delta = V2/pseudo_samples - V1/pseudo_samples) quantile(preds$delta, probs=c(0.5, 0.025, 0.975)) # One example run gave: # 50% 2.5% 97.5% # 0.1203225 -0.6870735 0.8045677 preds %>% ggplot(aes(x=delta)) + geom_density(fill=rgb(0.1,0.1,1.0,0.5))
