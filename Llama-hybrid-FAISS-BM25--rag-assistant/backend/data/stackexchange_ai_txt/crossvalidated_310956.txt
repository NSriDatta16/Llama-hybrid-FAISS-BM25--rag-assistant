[site]: crossvalidated
[post_id]: 310956
[parent_id]: 310952
[tags]: 
You can use a prior distribution over the classes. Let us assume that your model computes a vector of class probabilities $v$ . You can define a vector of prior probabilities $\pi$ and then compute your class probabilities to be proportional to $v \circ \pi$ , where $\circ$ denotes an element-wise product. So the probability that your observation belongs to class $c$ is proportional to $v_c\pi_c$ . If you want a proper distribution you just need to renormalize. In your example, if you want your predictions to be slightly biased to class 1, you can define $\pi=(0.4, 0.3, 0.3)$ , for instance. If you think about it, in the binary case this is what you are implicitly doing this when you change the threshold. Let us say you establish the following rule: if your probability vector is $v$ and your decision function is $f(x)$ , then $$ f(x)= \begin{cases} 2 & v_2\geq \theta \\ 1 & \mbox{otherwise} \end{cases} $$ for some $\theta \in (0,1)$ . Then this is equivalent (at least when it comes to making the decision) to computing the class probabilities to be proportional to $(\frac{v_1}{1-\theta}, \frac{v_2}{\theta})$ , so you would be defining $\pi=(\frac{1}{1-\theta}, \frac{1}{\theta})$ . You can also learn the value of $\pi$ from your data. For instance, you can compute the proportion of each class and use that as prior probabilities. For a more principled way of incorporating this kind of prior assumptions into your model, you might want to look at Bayesian inference.
