[site]: datascience
[post_id]: 98262
[parent_id]: 69021
[tags]: 
There are many options to spend this up: Get a better CPU. Distribute the process across a cluster since each document is independent. Reduce the size of the vocabulary. If only the top-n most popular words are used, it greats reduces the size of the data. Reduce the size of the embedding space. Switch to doc2vec so the document themselves are a learned embedding.
