[site]: datascience
[post_id]: 53282
[parent_id]: 20098
[tags]: 
You might find the following references useful: Section 4.1.2 Advantage Normalization : They mention that normalizing the advantage is a trick useful for training. It usually results in faster learning. Learning values across many orders of magnitude : They give an algorithm for normalization of rewards, and give detailed experimentation on the Atari environments. The basic idea is that the rewards can vary over a large range of magnitudes, and the function approximators being used in RL (such as neural networks) are usually not invariant to the scale of the input. So normalization becomes a key step. Do check this paper for more details.
