[site]: crossvalidated
[post_id]: 489727
[parent_id]: 
[tags]: 
What does the normalization factor mean in Bayesian learning?

Suppose we want to learn the distribution of a random variable $x$ . Assuming it's from a Gaussian distribution: $x\sim\mathcal{N}(\theta,1)$ with constant variance 1, we can learn the parameter $\theta$ from some data by using Bayesian learning. The prior distribution of $\theta$ is set to be a Gaussian $$p(\theta) = \mathcal{N}(\theta;0,1)$$ The likelihood of an observed data point would then be $$p(x|\theta) = \mathcal{N}(x;\theta,1)$$ The posterior distribution is obtained by multiplying the prior and the likelihood and normalizing: $$p(\theta|x)=\frac{p(\theta)p(x|\theta)}{\int p(\theta)p(x|\theta) d\theta}=\frac{p(\theta)p(x|\theta)}{p(x)}$$ What does the normalization factor $p(x)$ mean in the above posterior distribution equation? It's denoted as " $p(x)$ " but can't be the true distribution of $x$ as the prior and likelihood function are completely hypothetical, meaning the whole joint distribution $p(x,\theta)$ is hypothetical as well. So how do we interpret $p(x)$ ? Disregarding this unexplainable " $p(x)$ ", the remaining learning process makes good sense to me. As we observe more and more instances of $x\in\mathcal{D}$ from its true distribution, the posterior mean deviates from the prior mean and approaches the maximum likelihood estimate of $\theta=\mathop{\arg\min}_\theta p(\mathcal{D}|\theta)$ . Imagine the true distribution $p(x)=\delta(x-1)$ , i.e. $x$ always equals to 1. the posterior mean should approach to 1 in the limit of infinite observations instead of approaching prior mean of 0. All is well until I came across a disturbing description on p.74 in "Bishop C M. Pattern recognition and machine learning[M]. springer, 2006.", which states: Consider a general Bayesian inference problem for a parameter $\theta$ for which we have observed a data set $\mathcal{D}$ . The following result $$\mathbb{E}_\theta[\theta]=\mathbb{E}_\mathcal{D}[\mathbb{E}_\theta[\theta|\mathcal{D}]]$$ where $$\mathbb{E}_\mathcal{D}[\mathbb{E}_\theta[\theta|\mathcal{D}]]=\int \Big\{ \int \theta p(\theta|\mathcal{D})d\theta\Big\}p(\mathcal{D})d\mathcal{D}$$ $$\mathbb{E}_\theta[\theta]=\int \theta p(\theta)d\theta$$ says that the posterior mean of $\theta$ , averaged over the distribution generating the data, is equal to the prior mean of $\theta$ . I can see this equation is correct, but how does this tell us anything about Bayesian learning? The author clearly indicates $p(\mathcal{D})$ is the real distribution which we want to learn, however, going through the constructed example above, I found it is not the case. What am I missing here?
