[site]: crossvalidated
[post_id]: 634838
[parent_id]: 633394
[tags]: 
I think you have the right idea. One can easily think scenarios where first fitting a multiclass classifier and then reducing the output to binary will work better than directly fitting a binary classifier. Just as a dummy example. I generated some data where normal (N) samples are in the origin and different anomalies (A0,..) in clusters around it; and fitted logistic regression model both using the original labels (left) and using binarized labels (right). The multiclass model can find a good decision boundary, whereas the binary classifier is completely lost as the data cannot be separated linearly. I suppose this was what you were thinking. This proves that multiclass classifier can sometimes work better than binary classifier. I don't think this is in general true though. Couple of things to consider come to mind: The multiclass model is more complex. If the anomalies could be separated e.g. by linear decision boundary, the multiclass approach would add unnecessary complexity and make the model prone to overfitting. Related to this, maybe some of the anomaly groups contain only couple of samples. In this case some of the decision boundaries would be very poorly estimated by the multiclass classifier. Instead of using a multiclass classifier, you can just use a more complex model and solve the binary classification problem directly. The only way to know which approach will work best for your particular problem is usually just to test different models. The idea of first fitting multiclass model and then reducing the output to binary might work, or not. If nothing else, the output contains more information; maybe someone cares which type of anomaly it is.
