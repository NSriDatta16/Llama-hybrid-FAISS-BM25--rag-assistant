[site]: crossvalidated
[post_id]: 305399
[parent_id]: 305350
[tags]: 
This can be viewed in two ways. First, you can see L2 regularization as a prior on the weights to be distributed near 0. Since you have prior information on the possible distribution of weights, this rules out many improbable solutions. If your prior was correct, this will theoretically increase the chance that you get a reasonable answer -- one which will generalize and not overfit. This is sort of like Occam's razor -- in Occam's razor, we prefer short solutions to long solutions. Here, we prefer small weight solutions to large weight solutions. The second way to see it is that a neural network is a function approximator which is extremely flexible and has a huge model capacity. By flexible I mean that it is relatively easy from an optimization standpoint to tweak the weights a little and arrive at your desired function. L2 regularization helps by lowering model capacity (too much of which causes overfitting) while leaving the network flexible enough to perform well even with the added regularization.
