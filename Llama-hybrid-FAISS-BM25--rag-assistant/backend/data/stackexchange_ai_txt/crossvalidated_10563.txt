[site]: crossvalidated
[post_id]: 10563
[parent_id]: 10517
[tags]: 
The only way to "prove" that data comes from a certain distribution (without an infinite number of samples) is to know precisely how that data is generated. For example, if you know that the data came from the magnitude of a circular bivariate normal random variable, it has a Rician distribution. Or if the data came from the time between events in a Poisson process, then it has an exponential distribution. Lacking a precise definition of the generating process, there are a number of empirical measures you can use to determine the underlying distribution. First, look at the data itself: Is it discrete or continuous? Is it supported on (-inf,inf), [0,inf),(0,1), or another interval? This knowledge can be used to narrow down the possible univariate parametric distributions that could fit your data. Examples include the Gaussian distribution, Cauchy, Exponential, Gamma, Generalized Extreme Value, Rician, Wrapped Cauchy, Von Mises, Binomial, and Beta. Once you have determined the support of the distribution, test the potential univariate distributions with an information criterion - such as Akaike information criterion (AIC) or Bayesian information criterion (BIC). These balance the number of parameters in a given distribution with the likelihood of the data fitting a given distribution. Visually check the best-scoring distribution(s) to see if they appear to fit the data. An alternative is to construct a kernel density estimate of the data. This is basically a sophisticated version of creating a histogram of the data, where a small Gaussian (or other) distribution is placed at each data point, and the estimated distribution is constructed from the sum of these. For more information, see Kernel Density Estimation . This has the advantage of being able to fit arbitrary distributions in the data, but sampling from this distribution has a large computational cost, especially with large data sets. Another option is to construct a Gaussian Mixture Model (GMM) from the data, where a small number of Gaussian distributions are used to approximate the underlying distribution. For more information, see Mixture Models . The method appropriate for your application depends on the application itself. If you can determine the distribution from the generating process, great, estimate the parameters and your done! If not, the next-best scenario is finding a univariate parametric distribution that accurately describes the data. Lacking that, mixture models, KDEs, or other methods can be used to approximate the distribution.
