[site]: crossvalidated
[post_id]: 180537
[parent_id]: 177191
[tags]: 
It looks like any exploratory process performed without having a hypothesis beforehand is prone to generate spurious hypotheses. I would temper this statement and express it a little differently: Choosing a hypothesis to test based on the data undermines the test if one doesn't use the correct null hypothesis. The thrust of the Nature article is, essentially, that it's easy for analysts to kid themselves into ignoring all of the multiple comparisons they're implicitly making during exploration. Nature quotes Andrew Gelman, but doesn't mention his paper with Eric Loken about just this topic. An excerpt: When criticisms of multiple comparisons have come up in regards to some of the papers we discuss here, the researchers never respond that they had chosen all the details of their data processing and data analysis ahead of time; rather, they claim that they picked only one analysis for the particular data they saw . Intuitive as this defense may seem, it does not address the fundamental frequentist concern of multiple comparisons. Another: Itâ€™s not that the researchers performed hundreds of different comparisons and picked ones that were statistically significant. Rather, they start with a somewhat-formed idea in their mind of what comparison to perform, and they refine that idea in light of the data. They saw a pattern in red and pink, and they combined the colors. Succinctly: There is a one-to-many mapping from scientific to statistical hypotheses. And one more, emphasis mine: In all the cases we have discussed, the published analysis has a story that is consistent with the scientific hypotheses that motivated the work, but other data patterns (which, given the sample sizes, could easily have occurred by chance) would naturally have led to different data analyses (for example, a focus on main effects rather than interactions, or a different choice of data subsets to compare) which equally could have been used to support the research hypotheses. The result remains, as we have written elsewhere, a sort of machine for producing and publicizing random patterns. In short, it's not that EDA leads to a "spurious hypothesis"; it's that testing a hypothesis with the same dataset that prompted the hypothesis can lead to spurious conclusions. If you're interested in conquering this obstacle, Gelman has another paper arguing that many of these problems disappear in a Bayesian framework, and the paper with Loken references "pre-publication replication" as anecdotally described in the first section of this paper .
