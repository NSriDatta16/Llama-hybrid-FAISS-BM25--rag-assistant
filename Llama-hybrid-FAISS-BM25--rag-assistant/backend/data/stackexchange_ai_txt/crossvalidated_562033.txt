[site]: crossvalidated
[post_id]: 562033
[parent_id]: 
[tags]: 
Can ReLU Activation Functions create Non-Convex Loss Functions in Neural Networks?

Can ReLU Activation Functions create Non-Convex Loss Functions in Neural Networks? From another question on Stackoverflow ( https://math.stackexchange.com/questions/287716/the-composition-of-two-convex-functions-is-convex/287725 ), I learned that the composition of two convex functions will always be convex: We want to prove that for $x, y \in \Omega$ , $(g \circ f)\left(\lambda x + (1 - \lambda) y\right) \le \lambda (g \circ f)(x) + (1 - \lambda)(g \circ f)(y)$ . We have: \begin{align} (g \circ f)\left(\lambda x + (1 - \lambda) y\right) &= g\left(f\left(\lambda x + (1 - \lambda) y\right)\right) \\ &\le g\left(\lambda f(x) + (1 - \lambda) f(y)\right) & \text{(} f \text{ convex and } g \text{ nondecreasing)} \\ &\le \lambda g(f(x)) + (1 - \lambda)g(f(y)) & \text{(} g \text{ convex)} \\ &= \lambda (g \circ f)(x) + (1 - \lambda)(g \circ f)(y) \end{align} My Question: Take a Neural Network in which all Activation Functions are ReLU. We know that the ReLU function is a convex function - we also know that the general structure of Neural Networks are based on multiple compositions of Activation Functions. Thus, when we put these two logical arguments together : How is it possible for the Loss Function of some Neural Network (where all activation functions are ReLU) to be Non-Convex - when compositions of Convex Functions are necessarily Convex? Can someone please explain this? Thanks!
