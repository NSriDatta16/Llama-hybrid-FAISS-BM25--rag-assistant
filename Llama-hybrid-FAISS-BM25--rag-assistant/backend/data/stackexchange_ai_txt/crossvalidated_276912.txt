[site]: crossvalidated
[post_id]: 276912
[parent_id]: 276907
[tags]: 
Efficiency is a big factor here, and will always be regardless of how fast computers get. Say you want to fit a normal distribution to some 1D data $\{ X_1, \ldots X_N\}$, but we know that in the future we may want to add more data to this dataset and update our fit. One way to do this is store all the data and recalculate whenever new data arrives - but that means our storage and time taken to calculate the parameters will grow $O(N)$. If instead we just store $\sum_{i=1}^N X_i$, $\sum_{i=1}^N X^2_i$ and $N$, then we have all the data we need to update our parameter estimates later when new data arrives, and our storage and runtime only grow $O(1)$. Depending on the application and dataset sizes involved that can be a make or break issue. I'm not really familiar with this, but looking on Wikipedia it seems that "...according to the Pitman–Koopman–Darmois theorem, among families of probability distributions whose domain does not vary with the parameter being estimated, only in exponential families is there a sufficient statistic whose dimension remains bounded as sample size increases" . My gut feeling has always been that the existance of sufficient statistics in a problem is usually a pretty special case and indicates a "nice" distribution. A lot of the time I'm working with whole datasets instead. It depends on the problem. Generally, once you've found the factorisation $f_\theta(x) = h(x)g_\theta(T(x))$, to estimate $\theta$ you can just throw away the $h(x)$ part and just use the $g_\theta(T(x))$ to do whatever you like - so, if you're doing max likelihood, you just need to differentiate and set to zero $g_\theta(T(x))$ instead of $f_\theta(x)$, which usually ought to make the problem easier. I find that's really my main use of sufficient stats day to day - not as an answer to a problem in and of themselves, but as a way to simplify and speed up steps as part of another solution.
