[site]: crossvalidated
[post_id]: 441379
[parent_id]: 
[tags]: 
Binomial Logistic-Normal Updating

I've been considering how sports with binary outcomes might be modelled e.g. the probability of a tennis player winning a point on serve. In text books the usual Bayesian approach uses the beta-binomial. The updating rule is equivalent to starting with some pseudo observations for successes and failures and adding on new successes and failures as they occur, from which you calculate your new probability of winning that particular point. I've read an alternative is the binomial logistic-normal where the log of the odds (logit) approaches a normal distribution. What would my Bayesian updating rule be here for each new success or fail? I assume the odds get multiplied by a different factor each point (adding to the logit)? I don't yet have the ability in statistics to understand GLMs or GLMMs properly but I'm still curious to know the updating rule as I could plot graphs for p and the logit, calculate moments, and get some intuition for how it compares to Beta-Binomal as n gets bigger. On page 164 of 'The Mathematics of tennis' http://www.strategicgames.com.au/book.pdf the authors use an updating rule which is the following: "For player i the proportion of initial serving statistics $X_i$ is combined with actual serving statistics $Y_i$ to give updated serving statistics $Z_i$ at any point within the match." "n represents the total number of points played and c is a constant." $Z_i = e^{\!-n/c}X_i + (1âˆ’e^{\!-n/c} )Y_i$ Is this binomial logistic-normal or something different? (I emailed the one contactable author but he never got back to me.)
