[site]: crossvalidated
[post_id]: 519922
[parent_id]: 
[tags]: 
What does the term "regularization" refer to specifically?

Initially I thought that "regularization" referred to specific methods to reduce overfitting by putting a penalizer term in the cost function that uses a norm eg. L1, L2 norm. But recently I've seen people use the the term "regularization" to refer to any method that reduces overfitting eg. I've heard people call random dropout in neural networks as a regularization method. So which usage of the term is correct?
