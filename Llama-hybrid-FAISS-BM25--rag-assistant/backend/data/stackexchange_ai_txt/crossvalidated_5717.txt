[site]: crossvalidated
[post_id]: 5717
[parent_id]: 5713
[tags]: 
It looks like I can't add a long comment directly to Dr. Simpson's answer. Sorry I have to put my response here. I really appreciate your response, Dr. Simpson! I should clarify my arguments a little bit. What I'm having trouble with the partialling business is not a theoretical but a practical issue. Suppose a linear regression model is of the following form y = a + b * Sex + other fixed effects + residuals I totally agree that, from the theoretical perspective, regardless how we quantify the Sex variable, we would have the same residuals. Even if I code the subjects with some crazy numbers such as male = 10.7 and female = 53.65, I would still get the same residuals as r1 and r2 in your example. However, what matters in those papers is not about the residuals. Instead, the focus is on the interpretation of the intercept a and other fixed effects in the model above, and this may invite problem when partialling. With such a focus in mind, how Sex is coded does seem to have a big consequence on the interpretation of all other effects in the above model. With dummy coding ( options(contrasts = c("contr.treatment", "contr.poly")) in R), all other effects except 'b' should be interpreted as being associated with sex group with code "0" (males). With effect coding ( options(contrasts = c("contr.sum", "contr.poly")) in R), all other effects except b are the average effects for the whole population regardless the sex. Using your example, the model simplifies to y = a + b * Sex + residuals. The problem can be clearly seen with the following about the estimate of the intercept a : > summary(m1) Call: lm(formula = Size ~ Sex, data = dat) ... Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 180.9526 0.9979 181.332 summary(m2) Call: lm(formula = Size ~ Sex, data = dat) ... Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 175.4601 0.7056 248.659 Finally it looks like I have to agree that my original argument (3) might not be valid. Continuing your example, > options(contrasts = c("contr.sum", "contr.poly")) > m0 summary(m0) Call: lm(formula = Size ~ 1, data = dat) ... Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 175.460 1.122 156.4 It seems that including Sex in the model does not change the effect estimate, but it does increase the statistical power since more variability in the data is accounted for through the Sex effect. My previous illusion in argument (3) may have come from a dataset with a huge sample size in which adding Sex in the model didn't really change much for the significance of other effects. However, in the conventional balanced ANOVA-type analysis, a between-subjects factor such as Sex does not have consequence on those effects unrelated to the factor because of the orthogonal partitioning of the variances?
