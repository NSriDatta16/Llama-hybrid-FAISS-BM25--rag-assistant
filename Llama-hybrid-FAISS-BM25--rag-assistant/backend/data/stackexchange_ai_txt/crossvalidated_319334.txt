[site]: crossvalidated
[post_id]: 319334
[parent_id]: 
[tags]: 
Power spectral density of the output of a linear time invariant system with a weakly-stationary process

Please note that I have no background whatsoever in Fourier analysis and very little in time series . This is exercise 2.10 in Theodoridis' Machine Learning : Show that the autocorrelation of the output of a linear system, with impulse response $w_{n}$, $n \in \mathbb{Z}$, is related to the autocorrelation of the intput WSS process, via $$r_d(k) = r_u(k) * w_k * w^{*}_{-k}$$ I don't even know where to get started with this exercise. Here is the information I've gathered: Some notation/terms to define : WSS = Wide-Sense Stationary (or weakly stationary) $r_d(k) = \mathbb{E}[d_nd_{n-k}]$ $r_u(k) = \mathbb{E}[u_nu_{n-k}]$ $w^{*}$ means the complex conjugate of $w$ ( I think ) $*$ refers to the convolution sum. For example, if $\dots, w_0, w_1, \dots$ are parameters and $(u_n)$ a time series, $$w_n * u_n = \sum_{i=-\infty}^{+\infty}w^{*}_i u_{n-i}$$ I'm not sure how this works when you have two convolutions ( are convolutions associative? ). It is worth noting that this exercise is used as an intermediate step to the proof of theorem Theorem 2.1 . The power spectral density of the output, $d_n$, of a linear time invariant system, where it is excited by a WSS stochastic process, $u_n$, is given by $$S_d(\omega) = |W(\omega)|^2S_u(\omega)$$ The proof is solved by using the result of the exercise above and then, apparently, using the "well-known" properties of the Fourier transform implying that $$r_u(k) * w_k \mapsto S_u(\omega)W(\omega)$$ $$w^{*}_{-k} \mapsto W^{*}(\omega)$$ I opened one of my Time Series texts and found this theorem in the text by Shumway and Stoffer (4th ed.) on p. 175 (with very different notation), but I don't understand the proof, given my lack of time series and Fourier analysis background. Could someone please assist with helping me start exercise 2.10 , addressing my concerns in bold above?
