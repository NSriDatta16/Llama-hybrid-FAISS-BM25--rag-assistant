[site]: datascience
[post_id]: 121143
[parent_id]: 121140
[tags]: 
Certainly, the original language model by Bengio et al, 2003 worked with "normal NNs". However, they worked by simply concatenating word embeddings and then applying the transformation $y = b + Wx + U \mathsf{tanh}(d + Hx)$ . This kind of language model presents some problems: Scalability: the longer you want the context window to be, the larger the matrix multiplications you need. Training efficiency: you cannot train for all the output words of a sequence, that is, you can only train one output word at a time. Therefore, each sequence in the training data leads to multiple training data points (one per each possible location of the context window within the sequence). Order-dependent representations (lack of generalization): the representations learned for a word appearing at a specific position within the context window cannot be generalized to other positions. Modern language models are all RNNs or Transformers, which certainly don't have the aforementioned problems (although Transformers do have problems scaling the context window due to the quadratic memory requirements of attention).
