[site]: crossvalidated
[post_id]: 157820
[parent_id]: 157794
[tags]: 
The most common approach to find feature importance, is to employ a generalized linear model and check its performance by turning off features. The method is described here: http://uk.mathworks.com/help/stats/feature-selection.html?refresh=true . Another approach, that I usually prefer, is to use random forests to compute the feature importance based on their splits and OOB samples. You can find more information at: http://uk.mathworks.com/help/stats/ensemble-methods.html#bsx62vu , and in order to calculate the variance importance, you just set the 'OOBVarImp' as 'On' in the treebagger function If you still want to use a neural network, and given that your features are standardized, the method that you have to follow is called Sensitivity Analysis (as you have it in your tags). It does exactly what you want, but probably you will have to code it yourself. I would refer you for more information about implementing it to: http://www.mathworks.com/matlabcentral/answers/5342-sensitivity-analysis-ann http://www.mathworks.com/matlabcentral/answers/194647-how-to-compute-sensitivity-analysis-in-neural-network-model https://beckmw.wordpress.com/2013/10/07/sensitivity-analysis-for-neural-networks/
