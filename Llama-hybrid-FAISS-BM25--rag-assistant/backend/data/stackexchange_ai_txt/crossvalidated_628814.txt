[site]: crossvalidated
[post_id]: 628814
[parent_id]: 628557
[tags]: 
Properly set up k-fold cross validation and properly set up out-of-bootstrap are both pessimistically biased, i.e. they tend to underestimate predictive performance. There are variants of out-of-bootstrap that try to correct for this optimistic bias, e.g. .632-[out of] bootstrap or .632+-[out of] bootstrap. In situations with heavy overfitting, .632 can become optimistically biased , while .632+ is then supposed to become equal to normal out-of-bootstrap (i.e. should retain the pessimistic bias). (For cross validation, adjustment of $k$ affects the size of the bias). Both k-fold CV and out-of-bootstrap provide provide unbiased estimates (if properly set up) of the performance of the surrogate models that are actually tested. The pessimistic bias is caused by the (on average) lower performance of models trained on fewer actual cases. Out-of-bootstrap often has larger pessimistic bias than typical k-fold cross validation with k between 5 and 10, which implies that duplicate cases do not contribute as much information as real new cases. Simulation studies comparing (repeated) k-fold CV with out-of-bootstrap on the same data: Kim: Estimating classification error rate: Repeated cross-validation, repeated hold-out and bootstrap https://doi.org/10.1016/j.csda.2009.04.009 Beleites et al : Variance reduction in estimating classification error using sparse datasets https://doi.org/10.1016/j.chemolab.2005.04.008 [bootstrap] there would be higher chances for the sample to contain individuals revolving around the mean Out-of-bootstrap bootstraps the training cases, testing is done with the left-out cases. So your argument would turn around and you'd argue for a narrower calibration range* which is more frequently tested with out-of-calibration range cases. Since models tend not to predict well outside their calibration range (aka extrapolation), you should expect a pessimistic bias. BUT: the bootstrap draws cases without bias , i.e. each case has - regardless of its outcome value - the same probability as any other case of ending up in the test subset (≈ 33 %, more precisely $\frac{1}{e}$ ) or in the training set (≈ 67 %), and also the probability of being duplicate, triplicate, etc. in the training subsets are equal across cases. Also cross validation yields equal probabilities of ending up in training ( $\frac{k - 1}{k}$ ) or test subsets ( $\frac{1}{k}$ ). It actually enforces this within each run of $k$ folds. With out-of-bootstrap, you need to do more replicates to get a sufficient approximation to equal distribution of cases. Thus, you're just as likely to get a surrogate model that is trained on the outside cases and evaluated with cases in the center of the training data (where prediction tends to be better) or a surrogate model that has more compact, central training subset and is evaluated with "outside" test cases for which scenario we'd expect worse performance. Side note 1: One may argue that the additional variance of the bootstrap is good in the sense that it is a more realistic simulation of getting real test cases. OTOH, in my field, we work a lot with designed experiments that deliberately use uniform sampling of the calibration range or input space, and cross validation corresponds more closely to that. Side note 2: I'd recommend to also repeat $k$ -fold cross validation to measure instability. Side note 3: For cross validation, there exist variants that try to ensure very similar distributions of cases across all folds also in calibration space, e.g. Venetian blinds, but also block-wise CV that would deliberately set aside consecutive blocks (which allows to check how much "inner" vs. "outer" blocks differ in performance. AFAIK, they are rarely used, though. * Calibration range is the range of the dependent variable/outcome covered by the training data. This is a technical term in analytical chemistry/chemometrics, where much emphasis is put on ensuring that predictive models are not operated outside their calibration range.
