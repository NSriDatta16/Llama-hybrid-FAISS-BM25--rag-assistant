[site]: crossvalidated
[post_id]: 499060
[parent_id]: 
[tags]: 
Using adaptive LASSO penalty for a logistic regression

So I have this code for running adaptive LASSO in R library(glmnet) train = data.matrix(train) test = data.matrix(train) train_X = subset(train, select = -c(Category)) train_y = subset(train, select = c(Category)) test_X = subset(test, select = -c(Category)) test_y = subset(test, select = c(Category)) n = nrow(train) lambdas = c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.5, 2.0, 3.0, 4.0, 5.0, 8.0, 10.0) ridge $lambda.min) y_pred = predict(alasso, test_X, s = alasso$ lambda.min, type="link") The target is a Bernoulli and what's coming back in y_pred, might be correct? But I don't know how to interpret it. 0 1 [1,] 1.42888292 -1.42888292 [2,] 1.41375071 -1.41375071 [3,] -0.75950680 0.75950680 [4,] 1.43786886 -1.43786886 [5,] 1.37787921 -1.37787921 [6,] 0.27880898 -0.27880898 [7,] 1.47372331 -1.47372331 [8,] 1.41583441 -1.41583441 This is what's in y_pred. Seems like maybe some sort of weightage for class 0 or 1? I don't really know. I think setting family='multinomial' is fine the only options are family = c("gaussian", "binomial", "poisson", "multinomial", "cox", "mgaussian") according to the docs. I had to set family="binomial" for the initial ridge regression, because the returned coefficients can't be applied to adaptive lasso when it's set to 'multinomial' cause it's a list.
