[site]: crossvalidated
[post_id]: 524882
[parent_id]: 524078
[tags]: 
First, as far as your "optimization" goes, it is not really "optimization," and further, "there is an app for that": Use "jitter". It will be better than your approach because it does not add as much noise. The purpose of adding noise is not to validate any assumptions; instead, it just allows to to see the data density more clearly when the data are discrete, as in your case. Second, as far as the assumptions go, every problem you see in the residual plot is a function of the discreteness/boundedness of your DV. And for that reason alone, your linearity and homoscedasticity assumptions are obviously violated, for essentially the same reasons that they are violated for binary DVs. Also obviously, normality is violated as well for the same reasons. There is no reasons to look at kurtosis to tell you that, although the somewhat large kurtosis does alert you to the fact that there are rare, extreme response values in your data. You can improve the diagnostic value of your plots by adding the LOESS smooth (of the original, not jittered) data to the jittered (predict, resid) plot to better diagnose deviation from linearity. Deviation from a flat line indicates nonlinearity. To better diagnose constant variance, you can add the LOESS smooth (of the original, not jittered) data to the jittered (predict, abs(resid)) plot. Here again, deviation from the flat line indicates a problem, although the problem is heteroscedasticity in this plot. You might be able to get by with just using ordinary linear regression if these plots look ok, but it will definitely be sub-optimal. In particular, the predicted conditional distributions are normal when you do that, but your distributions are in reality, discrete and highly skewed. So your predictions will be scientifically unrealistic. Another, probably better option, especially considering that there are plenty of observations available for estimating the additional parameters, is ordinal regression. It also makes assumptions, but you can assess their validity by estimating an even more highly parameterized model such as multinomial logistic regression, then by comparing fit measures such as penalized likelihood, and also by comparing estimated conditional distributions. If the estimated conditional distributions are similar for both models, then the simpler ordinal regression model wins, according to Occam's razor.
