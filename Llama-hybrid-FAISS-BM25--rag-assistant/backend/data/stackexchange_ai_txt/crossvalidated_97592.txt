[site]: crossvalidated
[post_id]: 97592
[parent_id]: 97580
[tags]: 
$k$-fold and $v$-fold are the same thing. It's when you divide your data set randomly into $v$ equal parts. You then train your learning algorithm on $v-1$ parts and test on the remaining piece (e.g. compute the misclassification rate). This gives you an estimate of the error rate of your procedure. Repeat this many times and compute the average of the results. Btw, the extreme case is when $v$ is equal to the number of samples. This is called leave-one-out cross-validation and is typically considered to be the best thing you can do unless there are computational constraints.
