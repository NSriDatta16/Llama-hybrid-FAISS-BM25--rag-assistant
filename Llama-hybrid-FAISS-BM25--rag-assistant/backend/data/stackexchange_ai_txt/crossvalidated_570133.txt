[site]: crossvalidated
[post_id]: 570133
[parent_id]: 569922
[tags]: 
I initially used a GLM (because LMA data are neither normal nor heteroskedastic [maybe you meant "homoskedastic"?]) That might not be necessary. The distributional issues have to do with errors around the model predictions, not the raw data. we supplemented LMA measurements with average values for some species That's not typically a good way to deal with missing data. See Stef van Buuren's Flexible Imputation of Missing Data for the advantages of creating multiple imputed data sets so that you have the best chance of avoiding bias while including the variance arising from filling in missing data. Example of anova results for 3 models. The problem is that depending on the models, the significant effects are different ... Check the documentation for how anova() works with those model objects. It's possible that it uses a sequential Type I method so the order of variables in the model might affect the apparent "significance" of the individual predictors. It's not clear why you are doing separate models for each of cat_TCT , specimen_TCT and combined_TCT . It's generally better to start with as many predictors in a model as reasonable without overfitting, even if some predictors are somewhat correlated. (If predictors are linearly dependent or close to that, you should choose a linearly-independent set.) I don't know why you are having troubles with ML versus REML. You can't compare AIC values among models having different fixed effects that are fit via REML . Work through one step at a time to see the source of the error, instead of combining updates with anova() .
