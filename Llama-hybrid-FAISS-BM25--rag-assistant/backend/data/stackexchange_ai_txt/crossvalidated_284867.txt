[site]: crossvalidated
[post_id]: 284867
[parent_id]: 284842
[tags]: 
Per Wikipedia : The mode of a continuous probability distribution is the value x at which its probability density function has its maximum value, so the mode is at the peak. The mode is important in statistics and machine learning because we typically want to find the maximum of a function, e.g., the maximum of the likelihood function over a parameter space, and that is exactly the mode. Unfortunately, functions can have more than one mode: If our algorithm is looking for "the" mode, then it will end up in one of these, but the actual parameter value of the maximizer will be highly variable. However, having many modes (= global maxima) is not really a very common problem. What is more common is having many local maxima: This function has only a single mode (= global maximum). However, it has many local maxima. Most optimization algorithms search for local maxima, not global ones - so in this case, they may end up in a local-but-not-global maximum. (And the parameter value of the local maximizer will still be highly variable.) This gets more and more prevalent and problematic in higher dimensions, which is one aspect of the curse of dimensionality. Typically, you start your optimization algorithm multiple times with different starting values, and then take the largest maximum achieved over all runs. This also gives you a feeling for how variable the parameter value is. And you try to get some analytical results on how smooth the function you are investigating is.
