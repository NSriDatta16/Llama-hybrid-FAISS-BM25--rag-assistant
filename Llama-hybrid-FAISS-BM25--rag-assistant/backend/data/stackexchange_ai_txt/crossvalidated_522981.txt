[site]: crossvalidated
[post_id]: 522981
[parent_id]: 
[tags]: 
DL: Fixing random seeds or not?

I recently started a project to interpret medical images by classifying them into certain classes. During the experiments I tried out different hyperparameters. To make the different configurations comparable I fixed the random seeds using pytorch-lightningÂ´s pl.seed_everything(seed=42) . As I was quite happy with the current results after iterating different hyperparameters a couple of rounds, I tried out the performance on different random seeds that also initialize the neural network. The results vary greatly when moving away from the seed I used initially. It seems that during my iterations I overfitted the fixed random seed. TLDR : Is there a way to try out different hyperparameters for neural networks without overfitting a fixed random seed while still being able to compare different runs without the need to repeat a single configuration N times?
