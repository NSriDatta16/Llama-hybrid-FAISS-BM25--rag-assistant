[site]: datascience
[post_id]: 44635
[parent_id]: 
[tags]: 
Does sum of embeddings make sense?

Referring to the LightFM model from paper Metadata Embeddings for User and Item Cold-start Recommendations , the model tries to learn $d$ -dimensional user and item feature embeddings $e_f^U$ and $e_f^I$ for each feature $f$ ( $U$ is the set of users, $I$ is the set of items). The latent representation of user $u$ is given by the sum of its features' latent vectors: $$ \mathbf{q_u} = \sum_{j\in{f_u}}\mathbf{e_j^U} $$ The same holds for item $i$ : $$ \mathbf{p_i} = \sum_{j\in{f_i}}\mathbf{e_j^I} $$ Does it really make sense to sum the latent embeddings to represent a set of features (user or item)?
