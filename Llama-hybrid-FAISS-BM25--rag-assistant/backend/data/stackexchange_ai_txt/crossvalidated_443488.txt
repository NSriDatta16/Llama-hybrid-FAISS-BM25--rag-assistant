[site]: crossvalidated
[post_id]: 443488
[parent_id]: 443486
[tags]: 
I think there is good news and bad news on this... The bad news is that each and every choice of algorithm implicitly makes assumptions on the underlying probability distribution of the data (more precisely, it usually makes assumptions about the structure of $E[Y|X]$ where $Y$ is the target variable and $X$ is the variable responsible for producing the feature vectors). Since we cannot actually say anything about it there is no other way than testing many different mode classes in order so select the best one. Even worse: there is a theorem (called no free lunch theorem) that tells us that for every model, we can generate a very weird dataset so that this mode becomes arbitrarily close to guessing, i.e. we can even mathematically prove that there is no such thing as a “magical mode selection device”, the only thing we can do is testing... The good news is that in a “reasonable “ ML setup ( The only reason to exclude a model class (that comes to my mind) is -as you said- performance. If you deal with DNA like “rectangular” problems (millions of features, little amount of rows) then models based on decision trees will probably take forever. If it is millions of rows and few columns on the other hand, SVMs cannot deal with that so well. Hope this helps...
