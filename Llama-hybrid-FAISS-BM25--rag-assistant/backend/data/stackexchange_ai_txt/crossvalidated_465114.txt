[site]: crossvalidated
[post_id]: 465114
[parent_id]: 
[tags]: 
Forecasting Validation

question on validating forecast models. The traditional approach ( https://otexts.com/fpp2/forecasting-on-training-and-test-sets.html ) is to hold-out n number of samples from the end of your time series, and then run the model to predict those n time periods (multi-step forecast on the test set). (let's say I have 100 daily data points, and I hold out 10 days at the end) Doing this - and lets say I've fit an ARIMAX model for the sake of argument - I get very poor results between the actual values held out and those predicted for the holdout period. However, if instead during the 10-day validation period I re-build the model 1-day at a time and predict 1 day ahead, I get very good results when compared to the actual values. What is the difference - except computational effort? I should clarify here that I don't mean re-estimate the parameters - I mean fit the model to 91 days, and predict the next day, then 92 days, and predict the next day, and so on... The goal of the model is to predict 1-day into the future anyway, so I'm confused about why we don't just re-build the model stepwise during the hold-out period instead of trying to predict all 10-days at a time. I'm adding some Python code to help explain: n = 10 #number of days in hold-out periods v_actu = y[-n:].reset_index(drop=True) # the actual y values v_preds = [] #empt array to hold 1-step ahead predictions y2ci = [] # empty array to hold 1-step conf. intvs. m1 = pmd.ARIMA(order=(1,1,2), seasonal_order=(1,1,0,5)) for i, elem in enumerate(v_actu): m1_fit = m1.fit(y[:-(n-i)], exogenous=x_vars_all[:-(n-i)], suppress_warnings=True, stepwise=True) y2_pred = m1_fit.predict(n_periods=1, exogenous = pd.DataFrame(x_vars_all[-n:].reset_index(drop=True).loc[i]).T, return_conf_int=True) v_preds.append(y2_pred[0]) y2ci.append(y2_pred[1])
