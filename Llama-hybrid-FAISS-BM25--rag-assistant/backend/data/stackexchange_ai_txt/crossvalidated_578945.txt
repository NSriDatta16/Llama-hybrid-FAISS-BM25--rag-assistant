[site]: crossvalidated
[post_id]: 578945
[parent_id]: 578915
[tags]: 
You are correct, mutual information between X and Y cannot be larger than either the entropy of X or the entropy of Y. Ever. To debug where you've gone astray: instead of passing in a 2D matrix, try passing in pairs of vectors as separate X and Y arguments, and see if you can make sense of the results you get that way. Also pay attention to the method argument; empirical entropy estimates can be very biased unless you have a huge dataset; but it looks like corrections for this are supported.
