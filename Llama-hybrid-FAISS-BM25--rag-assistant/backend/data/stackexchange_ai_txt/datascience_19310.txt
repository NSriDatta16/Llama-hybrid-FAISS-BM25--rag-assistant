[site]: datascience
[post_id]: 19310
[parent_id]: 19276
[tags]: 
When considering how to clean the text, we should think about the data problem we are trying to solve. Here are few more step for preprocessing which can improve your features. 1.) Use Good tokenizer(textblob,stanford tokenizer) 2.) Try Lemmatization , stemming always not perform well in case news article. 3.) word segmentation 4.) Normalization (equivalence classing of terms) For selecting model 1.) In your example above, we classified the document by comparing the number of matching terms in the document vectors. In the real world numerous more complex algorithms exist for classification such as Support Vector Machines (SVMs) , Naive Bayes and Decision Trees , Maximum Entropy . 2.) You can think your problem as making clusters of news and getting semantic relationship of source news from these cluster. You can try topic modelling( LDA and LSA ) and Doc2vec/word2vec technique for getting vector for document/word and then use these vectors for classification task. Further if you are confuse to select a appropriate model for problem , you can read from this link Choosing Machine Learning Algorithms: Lessons from Microsoft Azure
