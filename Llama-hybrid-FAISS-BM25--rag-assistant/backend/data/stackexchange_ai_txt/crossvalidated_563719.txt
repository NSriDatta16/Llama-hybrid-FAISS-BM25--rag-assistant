[site]: crossvalidated
[post_id]: 563719
[parent_id]: 
[tags]: 
Measurement system analysis

Let's say I'm responsible for checking the length of shoes on a production line. Our production system isn't great, so a shoe that should be 25 cm long might actually be a slightly different length (25.4, 24.8, 26.0, etc.) And I am measuring with a set of rulers that are not great, so I could measure 25.0 cm when the actual shoe length is different (24.9, 25.1 cm, etc.). When I'm measuring shoes on the production line, I might pick up any of the 5 rulers in the set. How can I answer the question, "What is the smallest real difference in length that I can resolve with these not-great rulers?" Assume I only have time to measure each shoe on the production line once, so I can't eliminate the non-biased error by making multiple measurements. Also I'm the only inspector, so we can ignore variability due to different operators. For example, could I take a shoe, measure it with my 5 not-great rulers, calculate the round-robin difference between each ruler to all the others, repeat for several other shoes of different lengths, average all the length differences, and say that this average is the smallest difference in length I can resolve? (Also is there a name for this kind of analysis, maybe a flavor of gage R&R? Knowing the name would help me read up on suggested approaches, since I know this is not a novel problem.)
