[site]: datascience
[post_id]: 671
[parent_id]: 
[tags]: 
Linearly increasing data with manual reset

I have a linearly increasing time series dataset of a sensor, with value ranges between 50 and 150. I've implemented a Simple Linear Regression algorithm to fit a regression line on such data, and I'm predicting the date when the series would reach 120. All works fine when the series move upwards. But, there are cases in which the sensor reaches around 110 or 115, and it is reset; in such cases the values would start over again at, say, 50 or 60. This is where I start facing issues with the regression line, as it starts moving downwards, and it starts predicting old date. I think I should be considering only the subset of data from where it was previously reset. However, I'm trying to understand if there are any algorithms available that consider this case. I'm new to data science, would appreciate any pointers to move further. Edit: nfmcclure's suggestions applied Before applying the suggestions Below is the snapshot of what I've got after splitting the dataset where the reset occurs, and the slope of two set. finding the mean of the two slopes and drawing the line from the mean. Is this OK?
