[site]: crossvalidated
[post_id]: 432611
[parent_id]: 306273
[tags]: 
My first question would be, why would you use deep learning for tokenization (performance and accuracy-wise)? Apart from some corner cases it's a pretty straightforward task. Existing statistics-based systems and rule-based methods are fast and quite accurate, unlike deep learning approaches which are computationally expensive and require large datasets to be trained. Squeezing extra, say 0.5% from tokenization for an arbitrary task in NLP may not be worth the effort. Check out Moses tokenizer or SpaCy's tokenizer , if you haven't given them a try yet.
