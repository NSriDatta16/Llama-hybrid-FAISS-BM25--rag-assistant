[site]: datascience
[post_id]: 52352
[parent_id]: 
[tags]: 
Why maximize ELBO in the variational autoencoder?

For a variational autoencoder, we have that: $$\mathcal{L}(x,\theta,\phi) := \mathbb{E}_{z \sim q_\phi(z|x)}[\log p_{\theta}(x|z)] -KL[q_{\phi}(z|x) ||p(z)] $$ This is called the variational lower bound or evidence lower bound (ELBO). But I think what we're actually trying to maximize is the log-likelihood of our data: $$\log p_{\theta}(x) = \mathcal{L}(x,\theta,\phi) + KL[q_{\phi}(z|x)||p_{\theta}(z|x)]$$ There are a few things I'm unsure about, in increasing order of difficulty. For the actual loss function of a VAE, we use $-\mathcal{L}$ , more or less. Of course, it's expensive to actually calculate the expectation, which is why we use a single $z$ sample each time, yes? We are told to treat $p(z)$ as being $\mathcal{N}(0,1)$ , but I don't see what causes it to become normal. We're just told to plug in $\mathcal{N}(0,1)$ for $p(z)$ when calculating the loss, and all this seems to do is ensure that $q_\phi(z|x)$ gets closer to it. We know nothing about $p(z)$ , right (besides the fact that if all $p_{\theta}(z|x)$ are close to some distribution, then so is $p_{\theta}(z)$ )? How should we think about that KL divergence in the second formula? It's usually pointed out that for fixed $p_{\theta}$ , maximizing $\mathcal{L}$ is equivalent to minimizing the KL div. But $p_{\theta}$ is not fixed; it's being learned. We could also improve $\log p_{\theta}(x)$ (our ultimate goal) by making that divergence worse , couldn't we? What justifies optimizing $\mathcal{L}$ alone (other than tractability)? Basically, I'm confused by the common explanation that what we really want is to minimize that second KL divergence, and also that the best way to do that is to maximize the ELBO.
