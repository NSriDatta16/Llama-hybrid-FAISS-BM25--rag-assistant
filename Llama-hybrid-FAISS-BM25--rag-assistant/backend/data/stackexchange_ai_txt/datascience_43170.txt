[site]: datascience
[post_id]: 43170
[parent_id]: 
[tags]: 
Show importance of variables from a data set without a response variable? Use PCA?

I am trying to find a way to statistically show that some variables in my data set are more important than others to determine its classification. I have an example data set with three variables from 200 different WiFi boxes located in different buildings owned by a company. Variable A: Average speed of internet each user gets (Ex: Don't want the internet speed to be too low, which is about "3" or below, but for most people they won't notice much of a difference if the speed is above "5". ) Variable B: Average count of how many users are using the WiFi throughout the day Variable C: Average percentage of resources used from the machine at that location The problem: According to the company, all 200 routers are not providing good enough service to the users and must be replaced (Let's say replacing a 2.4 GHz WiFi router with a router that can do WiFi 5GHz). They say this because they are only looking at Variable_C and seeing that a large majority of the router's resources are being used (Let's say ~80%). I argue that not all routers need to be replaced. What people care about is if their internet speed is high enough, not that the router is using a large amount of its resources. If a router is using 80%+ of its resources and still delivering 5+ internet speed, the router is fine and replacing it is a waste of money because most people wouldn't notice a speed increase larger than "5". Is there a statistical method that I can use to show that Variable A&B are more important variables for classification because there are many examples where Variable A&B can vary wildly while Variable_C stays near constant 100%? My steps to try to find a solution To get a quick look at how my variables relate to each other, I created a pairplot in seaborn. From this I saw that some of my data is extremely skewed. Variable_A looks to be a decent normal distribution. Variable_B is extremely skewed and looks like a Poisson distribution. Variable_C is left skewed. I've read that when data is extremely skewed, it can be good to perform a natural log transformation to make the data approximately conform to normality. The new log transformed data pairplot looks like this. From this pairplot I can see some relationships between the variables. It seems that when the amount of users at a location increases, the user enjoyment decreases. I don't see any relationship between user enjoyment and percentage of resources used. I then created a pearson correlation matrix to get numerical values for each of the variables. The correlation plot solidifies what I saw from the log transformed pairplot. From here I am unsure how to proceed. I don't have a regression model I am trying to create because I have no response variable. All my data points have been pre-classified as "needing to be replaced". I just want to show that Variable_A and Variable_B are more important in determining whether something needs replaced instead of prioritizing Variable_C. I presented my problem to a friend and he recommended I use Principal Component Analysis to get the most important features from the data. I studied math in school and learned briefly about the eigenvalues/eigenvectors of PCA, but I never really took any Statistics classes. The PCA examples I learned about were about facial recognition and cancer classifying using PCA and kmeans. I am trying to learn more everyday about different topics in statistics but I am moving slower than I'd like. If PCA is the correct path for my problem, I have the following code which produces the chart and graph below: from sklearn.decomposition import PCA import math pca = PCA(n_components=2, svd_solver='full') pca.fit(log_site_transform) pca_site = pca.transform(log_site_transform) pca.explained_variance_ratio_ components = pd.DataFrame(pca.components_, columns = list(log_site_transform.columns)) def get_important_features(transformed_features, components_, columns): """ This function will return the most "important" features so we can determine which have the most effect on multi-dimensional scaling """ num_columns = len(columns) # Scale the principal components by the max value in # the transformed set belonging to that component xvector = components_[0] * max(transformed_features[:,0]) yvector = components_[1] * max(transformed_features[:,1]) # Sort each column by it's length. These are your *original* # columns, not the principal components. important_features = { columns[i] : math.sqrt(xvector[i]**2 + yvector[i]**2) for i in range(num_columns) } important_features = sorted(zip(important_features.values(), important_features.keys()), reverse=True) print("Features by importance:\n", important_features) print(pca.explained_variance_ratio_) print(get_important_features(pca_site, pca.components_, log_site_transform.columns.values)) def draw_vectors(transformed_features, components_, columns): """ This funtion will project your *original* features onto your principal component feature-space, so that you can visualize how "important" each one was in the multi-dimensional scaling """ num_columns = len(columns) # Scale the principal components by the max value in # the transformed set belonging to that component xvector = components_[0] * max(transformed_features[:,0]) yvector = components_[1] * max(transformed_features[:,1]) ax = plt.axes() for i in range(num_columns): # Use an arrow to project each original feature as a # labeled vector on your principal component axes plt.arrow(0, 0, xvector[i], yvector[i], color='b', width=0.0015, head_width=0.05, alpha=0.85) plt.text(xvector[i]*1.1, yvector[i]*1.1, list(columns)[i], color='k', alpha=0.75) return ax plt.figure() plt.style.use('seaborn-poster') sns.set_palette('muted', color_codes=True); sns.set() ax = draw_vectors(pca_site, pca.components_, log_site_transform.columns.values) T_df = pd.DataFrame(pca_site) T_df.columns = ['component1', 'component2'] T_df['color'] = 'b' #T_df.loc[T_df['component1'] > 10, 'color'] = 'g' #T_df.loc[T_df['component2'] > 10, 'color'] = 'r' plt.xlabel('Principal Component #1') plt.ylabel('Principal Component #2') plt.xlim(-2, 3) plt.scatter(T_df['component1'], T_df['component2'], color=T_df['color'], alpha=0.5) Can this be seen as a solution? print(pca.explained_variance_ratio_) log_site_transform.columns.values)) [0.73449626 0.24027837] print(get_important_features(pca_site, pca.components_, Features by importance: [(2.18326388651478, 'Variable_B'), (1.850445828906843, 'Variable_A'), (0.3147679640995589, 'Variable_C_%')] None I still need to read more in depth about PCA, but one goal of it is to show "the largest proportion of variance explained"? Is this showing that Variable A&B have large affects on the Principal Components and Variable_C has little? Am I making this a lot more complicated than it needs to be to show that Variable_C should not be prioritized as a key identifier for optimal performance?
