[site]: crossvalidated
[post_id]: 328965
[parent_id]: 320919
[tags]: 
RNNs before LSTM/GRU used to be unstable because what they were doing through was essentially multiplication of hidden state with some weights for every timestep, which means it's an exponential operation. And as we know, exponentiation is very unstable: $$0.99^{200} \approx 0.134$$ $$1^{200} = 1$$ $$1.01^{200} \approx 13$$ LSTM/GRU cells solve this problem by turning multiplication into addition. You have a cell state, and instead of multiplying you either add or subtract from it. However there's still some paths through which the gradient might become unstable, and the bigger the net is, the more probable it is that you're gonna run into this problem.
