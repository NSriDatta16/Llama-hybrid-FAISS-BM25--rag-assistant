[site]: datascience
[post_id]: 76811
[parent_id]: 
[tags]: 
Does stronger regularization always improve performance on testing set?

I am using the Sklearn logistic regression function to do a binary classification task on texts. I did the task using three different inputs: Bag-Of-Words, TF-IDF, Doc2vec embeddings. The question is that for the Bag-Of-Words, stronger regularization improves the performance on testing set. However, for TF-IDF and Doc2vec embedding, stronger regularization doesn't improve the performance on testing set. Actually the performance downgrades with stronger regularization. Why this is the case?
