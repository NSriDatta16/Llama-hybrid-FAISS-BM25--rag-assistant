[site]: crossvalidated
[post_id]: 612825
[parent_id]: 612638
[tags]: 
This is not really a theorem about stochastic dominance: it's a property of areas. It comes down to this lemma, which will be applied in the last two paragraphs: When $f:\mathbb R\to\mathbb R$ is an integrable function with non-zero norm $|f|=\int |f(x)|\,\mathrm dx \lt \infty$ and $\mathcal A$ is a set of positive measure $|\mathcal A| = \int_{\mathcal A}\mathrm dx \gt 0$ on which the values of $f$ all exceed some positive number $\epsilon \gt 0,$ then there exists an increasing (measurable) function $u$ for which the transformed function $f\circ u$ has a positive integral, $$\int_\mathbb{R}f(u(x))\,\mathrm dx \gt 0.$$ The idea is to make the image of $u$ focus on $\mathcal A$ while practically skipping over everything else: the integral is then at least $\epsilon$ (the minimum value of $f$ on $\mathcal A$ ) times the measure of $\mathcal A$ -- plus any negative contributions elsewhere. By limiting the latter we wind up with a positive integral. In this illustration, the set $\mathcal A$ is highlighted in orange along the horizontal axis and the area under $f$ over the region $\mathcal A$ is shaded. One such function $u$ is obtained by inverting the (strictly) increasing function $$v(y) = \int_{-\infty}^y \mathcal{I}_\mathcal{A}(x) + \delta(1-\mathcal{I}_\mathcal{A}(x))\,\mathrm dx$$ for a positive $\delta$ to be determined. ( $\mathcal I$ is the indicator function.) This illustration graphs $v$ for $\delta = 0.05.$ Its slopes are $1$ (orange) and $0.05$ (gray). The Fundamental Theorem of Calculus and the rule of differentiating inverse functions show the inverse $u=v^{-1}$ is (a) differentiable with (b) derivative equal to $1$ on $\mathcal A$ and $1/\delta$ elsewhere. Writing $v(\mathcal A)^\prime$ for the complement of $v(\mathcal A)$ within the image of $v$ (which is $\mathbb R$ itself), use the standard integral inequalities (Holder's, for instance) and the change of variables formula for integrals to deduce $$\begin{aligned} \int f(u(x))\,\mathrm dx &= \int_{v(\mathcal A)} f(u(x))\,\mathrm dx + \int_{v(\mathcal A)^\prime} f(u(x))\frac{|u^\prime(x)|}{|u^\prime(x)|}\,\mathrm dx\\ &\ge \int_{v(\mathcal A)} f(u(x))\,\mathrm dx - \left(\sup_{x\in v(\mathcal A)^\prime} \frac{1}{|u^\prime(x)|}\right)\left|\int f(u(x))|u^\prime(x)|\,\mathrm dx\right|\\ &\ge |\mathcal A|\epsilon - \delta|f|. \end{aligned}$$ Taking $\delta = |\mathcal A|\epsilon / (2|f|)$ produces a strictly positive value, proving the lemma. This illustration of the graph of $f\circ u$ shows how the horizontal axis has been squeezed at all places where $f\lt \epsilon,$ thereby giving the entire integral a positive value. Making $\delta$ sufficiently close to zero will effectively eliminate the dips in the graph below $\epsilon.$ As a corollary, applying the lemma to $-f$ shows that when there is a set of positive measure on which $f$ has negative values below $-\epsilon \lt 0,$ then there is an increasing function $u$ for which $f\circ u$ has a negative integral. Consequently, if for all increasing (measurable) functions $u$ the integral in the lemma is positive, it follows that the set of places where $f$ has a negative value has measure zero. That's the heart of the matter. Let's pause to notice two things. The first is technical: in this construction of $u,$ $u^{-1}$ is also almost everywhere differentiable and therefore continuous and measurable, allowing us to focus on such "nice" functions. The second is probabilistic: when $F_X$ is the distribution function of a random variable $X$ -- that is, $F_X(x)=\Pr(X\le x)$ -- and $u$ is an increasing (measurable) function with an increasing (measurable) inverse $u^{-1},$ then the distribution function of $u^{-1}(X)$ is $$F_{u^{-1}(X)}(y) = \Pr(u^{-1}(X)\lt y) = \Pr(X \le u(y)) = F_X(u(y)).$$ That is, $F_{u^{-1}(X)} = F_X\circ u.$ Now observe that when $F$ and $G$ are distinct distribution functions for a random variable $X$ and $u$ is an increasing (measurable) function, $$E_G[u^{-1}(X)] - E_F[u^{-1}(X)] = \int F(u(x)) - G(u(x))\,\mathrm dx = \int (F-G)(u(x))\,\mathrm dx.$$ (For the elementary proof see Expectation of a function of a random variable from CDF for instance. It's just an integration by parts.) Proof of the theorem Applying the corollary to the function $f = F-G$ (which has a nonzero norm since $F$ and $G$ are distinct), under the assumption $f$ has finite norm, shows that when all such integrals are positive, the set on which $F-G$ is negative has measure zero: that is, $G$ stochastically dominates $F,$ QED. We can eliminate the finite-norm assumption by noting that $F-G$ can have an infinite norm only by diverging at infinity: it cannot have vertical asymptotes. (The values are differences of probabilities, whence they are bounded by $\pm 1.$ ) Consequently we can approximate $F-G$ on an expanding sequence of compact sets, such as the intervals $(-n,n)$ for $n=1,2,3,\cdots,$ and apply a limiting argument. But that should be viewed as a technicality, because the underlying idea remains the same, as expressed in the lemma.
