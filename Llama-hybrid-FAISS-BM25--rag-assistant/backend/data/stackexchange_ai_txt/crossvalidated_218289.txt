[site]: crossvalidated
[post_id]: 218289
[parent_id]: 218241
[tags]: 
Here is where I would start. Bayesian inference for non-linear regression model Let $y_{i,t}$ be the observation at time $t$ with temperature $\theta_i$ where $i=1,2$ (since there are two temperatures) and $t=1,\ldots,T$ (for simplicity I'm assuming observations are taken at the same times, but this could easily be modified for situations where this is not the case.) Assume an additive normal error, i.e. $$ y_{i,t} \stackrel{ind}{\sim} N(\alpha e^{\beta/\theta_i}, \sigma^2). $$ A Bayesian approach to estimation requires a prior over the parameters, i.e. $p(\alpha,\beta,\sigma^2)$, and then the posterior is $$ p(\alpha,\beta,\sigma^2|y) \propto p(\alpha,\beta,\sigma^2) \prod_{i=1}^2 \prod_{t=1}^{T} N\left(y_{i,t}; \alpha e^{\beta/\theta_i} t, \sigma^2\right). $$ which will likely need to be estimated computationally, e.g. Markov chain Monte Carlo. This posterior can provide point estimates, e.g. $E[\alpha|y]$, and uncertainties, e.g. $V[\alpha|y]$. Forecasts can be obtained for a new temperature $\tilde{\theta}$ and time $\tilde{t}$. Assuming the response is independent of the previous data given the parameters, the forecast distribution is $$ p(\tilde{y}|y) = \int \int \int N\left(\tilde{y};\alpha e^{\beta/\tilde{\theta}} \tilde{t}, \sigma^2\right) p(\alpha,\beta,\sigma^2|y) d\alpha d\beta d\sigma^2 $$ which will also likely need to be estimated computationally. Although this forecast distribution will have more uncertainty when you are extrapolating, the extrapolations will still be heavily influenced by the model which may or may not be very good in the extrapolated regions and there will be no way for you to tell. Turning the model into a standard regression model If the observations are positive and it is reasonable to consider a multiplicate rather than additive error, you can turn this problem into a standard regression problem. Then, we could assume $$ y_{i,t} = \alpha e^{\beta/\theta_i} t e^{\epsilon_{i,t}}$$ where $e^{\epsilon_{i,t}}$ is the multiplicative error. If we take logarithms, then $$ \log y_{i,t} = \log(\alpha) + \beta \frac{1}{\theta_i} + \epsilon_{i,t}. $$ If we assume $\epsilon_{i,t} \stackrel{ind}{\sim} N(0,\sigma^2)$, this is a standard simple linear regression model where $\log(\alpha)$ is the intercept and $\beta$ is the slope for inverse temperature. This model can be trivially fit using any regression software. If you are using a Bayesian approach, then you can also trivially get uncertainty on $\alpha$ rather than $\log(\alpha)$ by taking samples of $\log(\alpha)$ and exponentiating. If you are using the standard prior ($p(\log(\alpha),\beta,\sigma^2) \propto 1/\sigma^2$), the forecast distribution for $\log(\tilde{y})$ is a Student $t$ distribution and can be found in most Bayesian textbooks. The statement about extrapolation above still applies. If it is reasonable, I would certainly opt for this second approach.
