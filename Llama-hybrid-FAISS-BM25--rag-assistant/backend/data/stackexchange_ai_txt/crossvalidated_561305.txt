[site]: crossvalidated
[post_id]: 561305
[parent_id]: 561164
[tags]: 
Why do we use Gradient Descent instead of Random Search for optimizing the loss functions in Neural Networks? We do use both at the same time currently. Meaning that, there is already a degree of random search even if we use stochastic gradient decent in training neural networks, i.e., random initialisation and in reinforcement learning via random search in game trees. For supervised deep learning, this is prominently studied by The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks arXiv . A random initialisation is acting as a random search. In reinforcement learning, deep Q-learning is achieved actually with the synergy of Monte Carlo Tree Search . In short, random search algorithms are already a practical part of training deep learning models. Completely gradient-free optimisation for deep learning, there was a different discussion here .
