[site]: crossvalidated
[post_id]: 327501
[parent_id]: 323779
[tags]: 
It seems that there is no scientific consensus on it. I think that Monte Carlo analogy by @Matthew is not perfect. E.g. if you had a neural network and you did a grid search of learning rate and momentum, then using the same random seed would lead to the same initialization which seems a good idea, so I agree with your intuition. Key point here is different. If variance coming from random seed is significant compared to variance coming from different choice of hyper-parameter, then grid search may not have sens. For different seeds it may find different optimal hyper-points. If such a case occurs, you may want to perform repeated cross validation, for more see this site So the final answer is: if fixing seed matters, something is going wrong.
