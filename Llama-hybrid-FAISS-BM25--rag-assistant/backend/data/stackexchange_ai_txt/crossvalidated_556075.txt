[site]: crossvalidated
[post_id]: 556075
[parent_id]: 556074
[tags]: 
This is explained by "independence of the errors". If two models have independent error terms, they can cancel each other out, and the result may be better predictions. If the errors are dependent, the predictions will not be improved (both models make the same mistakes). Independent errors are more likely with completely different models, for example, based on different columns, or different training samples, or using a different algorithm. A random forest combines many different trees (these are losers by themselves), where all of them are grown on different subsets of columns and features. This way, the errors will be more or less independent and the result is an accurate model. Combining different algorithms (a random forest with a neural network for example) is known as stacking.
