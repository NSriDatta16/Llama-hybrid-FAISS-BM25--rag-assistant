[site]: crossvalidated
[post_id]: 383848
[parent_id]: 19952
[tags]: 
Here are some general solution sketches that also work for high-dimensional distributions: Train an f-GAN with reverse KL divergence, without giving any random input to the generator (i.e. force it to be deterministic). Train an f-GAN with reverse KL divergence, move the input distribution to the generator towards a Dirac delta function as training progresses, and add a gradient penalty to the generator loss function. Train a (differentiable) generative model that can tractably evaluate an approximation of the pdf at any point (I believe that e.g. a VAE, a flow-based model, or an autoregressive model would do). Then use some type of optimization (some flavor of gradient ascent can be used if model inference is differentiable) to find a maximum of that approximation.
