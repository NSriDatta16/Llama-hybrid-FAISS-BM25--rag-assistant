[site]: crossvalidated
[post_id]: 341828
[parent_id]: 198629
[tags]: 
I think the best thing to do is discard terms like "over-fitting" and "over-training" entirely and just consider bias and variance . Keep in mind that when people say a model is "overfit," they typically mean that the model's variance is higher than the value of variance corresponding to minimum out-of-sample loss . From there, think about how each of the hyperparameters of your model affect both bias and variance. As an example, consider a single-hidden-layer feedforward neural network with MSE loss (and bear in mind that $\hat{MSE}=\hat{\text{var}}+\hat{\text{bias}^2}$). We'll assume that the training dataset is stationary, and that all observations are drawn from the same multivariate distribution. 1) as $N_{iter}$ (number of training iterations) increases , $\hat{\text{var}}$ increases , and $\hat{\text{bias}^2}$ decreases . I believe this is what "over-training" refers to. 2) As $N_{obs}$ (number of observations in the training set) increases , $\hat{\text{var}}$ decreases , and $\hat{\text{bias}^2}$ is unaffected . Clearly, as long as our assumptions hold, larger $N_{obs}$ is always better. In practice, non-stationarity due to temporal changes in the relationships between the predictors and response can create situations where some finite value of $N_{obs}$ is optimal. 3) as $P$ (the number of parameters, or connection weights, in the network) increases , $\hat{\text{var}}$ increases , and $\hat{\text{bias}^2}$ is unaffected . 4) as $\lambda$ (the L2 regularization penalty) increases , $\hat{\text{var}}$ decreases , and $\hat{\text{bias}^2}$ increases . My preferred approach -- although expensive -- is to determine the topology of the network first (and thus fix $P$, or at least parameterize it in terms of the number of predictors), and then adjust $\lambda$ (or whatever parameter governs your chosen form of regularization) such that the model converges in a way that generalizes well out-of-sample even for arbitrarily large $N_{iter}$. In the second link I provided above there is a plot of bias, variance, and MSE in the context of ridge regression. It's worth noting that the minimum value of $\hat{MSE}$ does not necessarily coincide with the minimum of $\hat{\text{var}}$. Therefore, targeting the minimum $\hat{\text{var}}$ (and it's $\hat{\text{var}}$ that's generally associated with "overfitting") doesn't necessarily yield you the optimal model in terms of $\hat{MSE}$ when $\hat{\text{bias}^2}>0$. Note also that increasing regularization hyperparameters like $\lambda$, while having the benefit of decreasing $\hat{\text{var}}$, typically have the (generally undesirable) side effect of increasing $\hat{\text{bias}^2}$. Hence the bias-variance tradeoff . On a somewhat separate note, you can decrease $\hat{\text{var}}$ without increasing $\hat{\text{bias}^2}$ by ensembling (bagging) multiple neural networks trained on the same data with different initializations. But there's no free lunch to be found because the computational cost scales linearly with the number of networks you train.
