[site]: datascience
[post_id]: 106594
[parent_id]: 106589
[tags]: 
The problem seems to be that you are dividing by 10 (k) at each iteration, I can think to try to calculate the average, this is incorrect and probably it is what is causing you to see a very low metric value. It would be simpler and correct, to only store the values for the metric in each iteration at the validation set and finally just calculate the average and std of the list in which you stored the values. Try: import numpy as np from sklearn.model_selection import KFold from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score from sklearn.metrics import precision_recall_fscore_support as score k = 10 kf = KFold(n_splits=k, random_state=None) acc_score = [] pr_score = [] rc_score = [] model = clf for train_index , test_index in kf.split(X, Y): x_train , x_test = X.iloc[train_index,:],X.iloc[test_index,:] y_train , y_test = Y[train_index] , Y[test_index] model.fit(x_train,y_train) pred_values = model.predict(x_test) acc = accuracy_score(pred_values , y_test) acc_score.append(acc) pr = precision_score(pred_values, y_test) pr_score.append(pr) rc = recall_score(pred_values, y_test) print(f'accuracy of each fold - {acc_score}') print(f'Avg accuracy :{np.mean(acc_score)} ± {np.std(acc_score)} std') print(f'Precision:{np.mean(pr_score)} ± {np.std(pr_score)} std') print(f'Recall:{np.mean(rc_score)} ± {np.std(rc_score)} std')
