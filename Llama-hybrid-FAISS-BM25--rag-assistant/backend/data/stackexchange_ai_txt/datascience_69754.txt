[site]: datascience
[post_id]: 69754
[parent_id]: 69737
[tags]: 
I would suggest also to give a shot to gensim. It's pretty quick compare to other self-written 'top_n retrieval by cosine-similarity' functions. Save your embeddings in a .txt or .csv file and then load it using the command 'load_word2vecformat'. Once you load the model you can use the function 'similar_by_word' or 'similar_by_vector' to retrive the n closest vectors. from gensim.models import KeyedVectors model = KeyedVectors.load_word2vec_format(datapath('model_file'), binary=False) top_10 = model.similar_by_word('cat') Just pay attention when you save the file to include as first line the number of embeddings, 1M in your case, and the vectors dimension. It's just the gensim standard format, the file should look like this: 1000000, 300 '.', -0.0001, -0.0001, ... ',', -0.0001, -0.0001, ... 'a', -0.0001, -0.0001, ...
