[site]: crossvalidated
[post_id]: 626488
[parent_id]: 
[tags]: 
Why is serial correlation a problem in GLMs?

I have learned that serial correlation is a problem in linear regression as the idiosyncratic error terms are correlated which leads to the variance function of the coefficients to be misspecified and inefficient. The proof of this seems fairly straight forward, as shown by Ben Lambert on Youtube. I was wondering, given that GLMs, aside from when data are continuous and a normal likelihood is used, don't have an idiosyncratic error, is serial correlation still a problem; and if it is, how can it be shown mathematically?
