[site]: crossvalidated
[post_id]: 35377
[parent_id]: 35353
[tags]: 
Note that neither p-values or AIC were designed for stepwise model selection, in fact the assumptions underlying both (but different assumptions) are violated after the first step in a stepwise regression. As @PeterFlom mentioned, LASSO and/or LAR are better alternatives if you feel the need for automated model selection. Those methods pull the estimates that are large by chance (which stepwise rewards for chance) back towards 0 and so tends to be less biased than stepwise (and the remaining bias tends to be more conservative). A big issue with AIC that is often overlooked is the size of the difference in AIC values, it is all to common to see "lower is better" and stop there (and automated proceedures just emphasise this). If you are comparing 2 models and they have very different AIC values, then there is a clear preference for the model with the lower AIC, but often we will have 2 (or more) models with AIC values that are close to each other, in this case using only the model with the lowest AIC value will miss out on valuable information (and infering things about terms that are in or not in this model but differ in the other similar models will be meaningless or worse). Information from outside the data itself (such as how hard/expensive) it is to collect the set of predictor variables) may make a model with slightly higher AIC more desirable to use without much loss in quality. Another approach is to use a weighted average of the similar models (this will probably result in similar final predictions to the penalized methods like ridge regression or lasso, but the thought process leading to the model might aid in understanding).
