[site]: datascience
[post_id]: 19366
[parent_id]: 18903
[tags]: 
LightGBM is a great implementation that is similar to XGBoost but varies in a few specific ways, especially in how it creates the trees. It offers some different parameters but most of them are very similar to their XGBoost counterparts. If you use the same parameters, you almost always get a very close score. In most cases, the training will be 2-10 times faster though. Why don't more people use it then? XGBoost has been around longer and is already installed on many machines. LightGBM is rather new and didn't have a Python wrapper at first. The current version is easier to install and use so no obstacles here. Many of the more advanced users on Kaggle and similar sites already use LightGBM and for each new competition, it gets more and more coverage. Still, the starter scripts are often based around XGBoost as people just reuse their old code and adjust a few parameters. I'm sure this will increase once there are a few more tutorials and guides on how to use it (most of the non-ScikitLearn guides currently focus on XGBoost or neural networks).
