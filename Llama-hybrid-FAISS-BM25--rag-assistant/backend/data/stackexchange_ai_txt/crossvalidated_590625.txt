[site]: crossvalidated
[post_id]: 590625
[parent_id]: 590501
[tags]: 
Your problem can be formulated more rigorously as an Optimal Experimental Design problem. Rather than maximizing a sum of mutual informations, your initial goal is to maximize the mutual information $I(X,Z)$ where $Z = [Z_1, Z_2, \dots , Z_K]$ . You did not specify over what you are optimizing, but I assume it is over some parameters/inputs on which $p(Z_i)$ implicitly depends. The chain rule for the mutual information yields $$ I(X,Z) = \sum_{i=1}^K I(X;Z_i|Z_{i-1}, \dots , Z_1) $$ Hence, computing $\rm{max}_Z I(X,Z)$ can be rewritten as $$ \rm{max}_{Z_1} \Biggl[ I(X;Z_1) + \rm{max}_{Z_2} \biggl[ I(X;Z_2|Z_1) + \rm{max}_{Z_3} \bigl[ I(X;Z_3|Z_1,Z_2) + [\dots] \bigr] \biggr] \Biggr] $$ which is, as you explained, an intractable problem, since the algorithmic complexity scales exponentially with $K$ . The classical solution is to perform so-called myopic optimization, i.e. to optimize for only one manipulation at a time while taking into account previous manipulations but not future ones: $$ \sum_{i=1}^K \rm{max}_{Z_i} I(X;Z_i|Z_{i-1},\dots,Z_1) $$ which is not as optimal as optimizing directly $I(X;Z)$ , but which makes the problem computationally tractable. You can have a look at our recent preprints, in which we propose a particle-filtering based solution for mutual information optimization: Gontier, C., Surace, S. C., & Pfister, J. P. (2022). Efficient Sampling-Based Bayesian Active Learning. arXiv preprint arXiv:2201.07539.
