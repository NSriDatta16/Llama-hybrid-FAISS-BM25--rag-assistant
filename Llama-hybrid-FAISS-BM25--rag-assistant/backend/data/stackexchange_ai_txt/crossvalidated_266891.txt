[site]: crossvalidated
[post_id]: 266891
[parent_id]: 266864
[tags]: 
PCA rests on the singular value decomposition of the covariance matrix ( $\text{Cov}(\text{Data})$ ): The covariance matrix is a Gramian matrix , and all Gramian matrices can be expressed as $A^\top A$ . $A^\top A$ matrices have wonderful properties: Symmetry Positive semidefinite- ness Real and positive eigenvalues The trace is positive (the trace is the sum of eigenvalues) The determinant is positive (the determinant is the product of the eigenvalues) The diagonal entries are all positive Orthogonal eigenvectors Diagonalizable as $Q\Lambda Q^T$ It is possible to obtain a Cholesky decomposition . Rank of $A^TA$ is the same as rank of $A$ . $\text{ker}(A^TA)=\text{ker}(A)$ The trace is the sum of variance values in the diagonal of $\text{Cov}(\text{data})=A^\top A$ . This is just the structure of the covariance matrix with the elements $$E[(X_i-\mu_i)(X_i-\mu_i)]$$ along the diagonal. The trace is the sum of eigenvalues . Scrappy proof in R: > set.seed(0) # To replicate results > data = matrix(rnorm(50), nrow = 10) # Made-up toy matrix 10 x 5 > covariance = cov(data) # Covariance of the data matrix > SVD_d = svd(covariance)$d # Eigenvalues of the covariance matrix > sum(diag(covariance)) # Trace of the covariance matrix: [1] 4.242387 > sum(SVD_d) # Sum of eigenvalues: [1] 4.242387
