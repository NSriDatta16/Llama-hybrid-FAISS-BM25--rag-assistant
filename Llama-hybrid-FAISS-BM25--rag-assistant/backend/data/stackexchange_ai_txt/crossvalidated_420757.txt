[site]: crossvalidated
[post_id]: 420757
[parent_id]: 413251
[tags]: 
VI is an approximate Bayesian technique, I think just because it has the word “inference” in its name you shouldn’t confuse it as a whole new school of thought. I would first say that I disagree with you saying that a “Bayesian technique maximises posterior probability” as really all Bayesian techniques are doing is evaluating the posterior distribution over parameters given the data. Once you have a posterior distribution you can maximise what you like, for example my choosing a MAP estimate of the parameter, but there’s plenty of other things you can do that people might consider more principled like intergrating out the parameter which has nothing to do with maximisation. Secondly if you aren’t satisfied by “maximising the lower bound of the evidence” you might be better off thinking about it as equivalently minimising the KL divergence between the variational posterior and the true one. This way you can see that VI is explicitly Bayesian, as you are trying to make an approximation that is as close to the true posterior distribution as possible.
