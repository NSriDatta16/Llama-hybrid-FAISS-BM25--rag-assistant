[site]: datascience
[post_id]: 113316
[parent_id]: 113313
[tags]: 
Vanishing & Exploding Gradient problem happens in case of deep neural network. In NN when we have to update weights & biases for each layer we calculate the partial derivate with respect to y_hat at each layer (Back Propogation Algorithm) . Because in this case weights are multiplied in chain with each other. As Sigmoid is used in last layer it will only be just gradient and does not have impact of other layer while initial layer will be multiplies by weights of earlier layer leading to Vanishing Gradient problem. So Sigmoid in last layer does not lead to Vanishing Gradient problem and you can use it safely.
