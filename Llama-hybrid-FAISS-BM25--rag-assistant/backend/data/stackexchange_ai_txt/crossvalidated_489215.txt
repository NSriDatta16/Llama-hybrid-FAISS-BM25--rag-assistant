[site]: crossvalidated
[post_id]: 489215
[parent_id]: 
[tags]: 
Is it always better to average out parameter uncertainty?

Setup If we have a data set $y_1, \ldots, y_t := y_{1:t}$ , and we're trying to predict $y_{t+1}$ , a Bayesian would try to use the posterior predictive distrbution $$ p(y_{t+1} \mid y_{1:t}) = \int p(y_{t+1} | y_{1:t}, \theta)p(\theta | y_{1:t})d\theta . $$ as opposed to plugging in some estimator of the parameter into the conditional likelihood: $$ p(y_{t+1} \mid y_{1:t}, \hat{\theta}). $$ When asked why the former is superior to the latter, people often reply "because it takes into account parameter uncertainty." Question Now, assuming the first is unavailable, and you only care about prediction score, is it always better to average out parameter uncertainty? For instance, what if we average over with the prior distribution: $$ \check{p}(y_{t+1} \mid y_{1:t}) := \int p(y_{t+1} \mid y_{1:t}, \theta)p(\theta) d\theta? $$ Letting $f(y_{t+1})$ be the "true" prediction density; can we say anything about information theoretic quantities like $$ \text{KL}\left[ f(y_{t+1}) || \check{p}(y_{t+1} \mid y_{1:t})\right] \overset{\text{?}}{\le} \text{KL}\left[ f(y_{t+1}) || p(y_{t+1} \mid y_{1:t}, \hat{\theta})\right]? $$ I doubt the above is true for any $\theta$ . I can show it's true on average: $$ \text{KL}\left[ f(y_{t+1}) || \check{p}(y_{t+1} \mid y_{1:t})\right] \le E_{\theta} \left\{ \text{KL}\left[ f(y_{t+1}) || p(y_{t+1} \mid y_{1:t}, \theta )\right] \right\}. $$ Proof: by Jensen's $\log\check{p}(y_{t+1} \mid y_{1:t}) \ge E_{\theta}\left[\log p(y_{t+1} \mid y_{1:t}, \theta) \right]$ . Take $E_{f(y_{t+1})}(\cdot)$ on both sides, and then switch the order of integration.
