[site]: crossvalidated
[post_id]: 50007
[parent_id]: 
[tags]: 
Omitted variable bias in linear regression

I have a philosophical question regarding omitted variable bias. We have the typical regression model (population model) $$ Y= \beta_0 + \beta_1X_1 + ... + \beta_nX_n + \upsilon, $$ where the samples are from $(Y,X_1,...,X_n)$, and then a bunch of conditions by which the OLS estimates behave quite well. Then we know that, if we omit one of the main variables, $X_k$, this might bias the estimates of $\beta_0, \beta_1, ..., \beta_{k-1}, \beta_{k+1}, ..., \beta_n$. This would affect, at least, the estimated effect of the rest of the variables on $Y$, and also the hypothesis tests about $\beta_1, \beta_2, ...$, as the predicted values are not reliable. The thing is, we don't know which variables are in the true population model. Instead, we have a bunch of candidates from which we should analyze and find out the most appropriate subset. This process of variable selection uses OLS estimates and hypothesis tests again. Based on that, we reject or include different variables. But since each candidate model is omitting relevant variables (you will never be able to find the true model), wouldn't these decisions be based on biased results? Why then, should we trust them? (I'm thinking of forward stepwise method, for instance, where you pick one variable then add the rest. You compare the models doing inference, and I'm thinking that omitted variables may be disturbing everything.) I was never too worried about this topic until I began thinking of it, and I'm sure I'm wrong somewhere.
