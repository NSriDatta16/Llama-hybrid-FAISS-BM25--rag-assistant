[site]: crossvalidated
[post_id]: 171686
[parent_id]: 171451
[tags]: 
I am not going to attempt to provide final answers to your question; I believe the topic is more than addressed after the comprehensive response given by Glen. However, and apropos of his comment about a Bayesian approach, I'd like to post some illustrations about the way our preconceptions about the "fairness" of the coin (or the experiment in general) affects the posterior probability density , i.e. the $p\,(\theta\,\vert\,\text{Data})$, where $\theta$ stands for the probability of heads in the coin toss. Luckily, we have a conjugate prior distribution for the binomial case that occupies us - the beta distribution, facilitating the calculation of the posterior distribution. First scenario - The Fair-Minded Player: We walk into the game (not a very exciting game, but still...), and we have absolutely no reason to assume that there is foul play going on. Things being by nature less than perfect, we have it in our mind that the coin is fair- ish . In other words, we think that the probability of heads, $\theta$, falls somewhere around $\frac{1}{2}$. Later, the unexpected single tail out of $6$ tosses, will force us to move the posterior probability of $\theta$ to the left (the arrows indicate the influence of the data on the prior distribution): Second Scenario - The Shrewd Player: We strongly suspect from insider's leaked information that the game is markedly biased towards tails, and we not only are about to make a killing, but also in need to further reinforce our conviction after the first round, doubling down our bet: Third Scenario - Losing Your Shirt: We've never played before, but we have read a manual, and we feel ready. All signs clearly indicate that the coin is markedly biased towards $heads$, a mistake that we will soon start to correct at a high $\ $\$ cost: Fourth Scenario - No Idea Whatsoever: It's a good thing that the $\beta(1,1)$ distribution turns into a $U\,(0,1)$ to address this scenario, where only the likelihood will influence the - posterior probability of $\theta$. As brought up to my attention, a Jeffreys prior is close and possibly more correct : So I hope this provides a bit of a light-hearted visual depiction of our approach to estimating the chances of this game being rigged, perhaps encapsulating more of a real scenario than calculations of the type pbinom(1, 6, 0.5) . If you want the code in R, and the credits to a great video with Matlab illustrations, I posted it here .
