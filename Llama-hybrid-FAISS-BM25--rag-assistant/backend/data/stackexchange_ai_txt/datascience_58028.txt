[site]: datascience
[post_id]: 58028
[parent_id]: 57809
[tags]: 
The differences between UNET and the deeper networks like RDRCNN and Tiramisu is that the deeper networks have Batch Normalization layers. They also include dropout. I thought it might be the bias terms in the convolutional layers, but the bias for all those layers was set to 'False'. I tried regularization, changing the momentum value of the Batch Norm layers, and removing the Batch Norm layers closest to the output. However, the only thing that worked was removing all the Batch Norm layers entirely or removing all the dropout entirely. It seems this is a data issue on my end. Certain images are exacerbating a common problem with using batch normalization and dropout together. For some, this may introduce too much noise into the network, leading to incorrect predictions.
