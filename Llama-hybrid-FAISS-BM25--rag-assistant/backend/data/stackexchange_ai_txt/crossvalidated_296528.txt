[site]: crossvalidated
[post_id]: 296528
[parent_id]: 
[tags]: 
Bayesian Decision Theory - Self Study

Consider a naive Bayes classier with a binary class $Y ∈ {0, 1}$ and three binary features $X_1, X_2, X_3$ ∈ {0, 1}. You are given a set $D$ of $n$ training examples, i.e. D={$(x^{(1)}_1, x^{(1)}_2, x^{(1)}_3, y^{(1)}), . . . ,(x^{(n)}_1, x^{(n)}_2, x^{(n)}_3, y^{(n)})$}. Suppose for some example the posterior is $P(Y = 1 | X_1 = x_1, X_2 = x_2, X_3 = x_3) = p$ and assume that there is a cost $c_m$ for misclassication (that is, classifying a sample with label 1 as having label 0 or vice versa). For each prediction you have the choice between trusting the naive Bayes classier or asking a human expert with cost $c_h Which of these two choices minimizes the expected cost as a function of p? My solution: $ \begin{equation} \text{Conditional distribution $P(Y=y|X)$} = \begin{cases} p, & \text{if}\ y=1 \\ 1-p, & \text{if y= -1} \end{cases} \end{equation}$ Action set A = $\{-1,1,H\}$ $ \begin{equation} \text{Cost-function $C(y,a)$} = \begin{cases} c_m . \mathbf{1}_{y \neq a}, & \text{if a $\in$ {1,-1}}\ \\ c_h & \text{if a = H} \end{cases} \end{equation}$ In the Bayesian optimal decision framework we want to pick the action which minimizes the expected loss: $a^* = argmin_{a \in A} \mathbb{E}[C(y,a)|x]= argmin_{a \in A} \sum_y (c_m.\mathbb{1}[y \neq a]\mathbb{1}[a \neq H]+c_h.\mathbb{1}[H=a])P(y|x)$ By analyzing each cost separately: $\begin{equation} \text{ $\mathbb{E}[C(y,a)|x]$} = \begin{cases} (1-p).c_m, & \text{if a=1} \\ p.c_m, & \text{if a= -1} \\ c_h, & \text{if a= H} \\ \end{cases} \end{equation}$ But I dont know how to conclude from here which make me doubt about my solution. Can somebody help? Thanks a lot
