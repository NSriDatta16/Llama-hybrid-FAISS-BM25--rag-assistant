[site]: crossvalidated
[post_id]: 291185
[parent_id]: 290845
[tags]: 
Regarding the first two questions Accuracy wise, are there any negative effects of having too many features in a logistic regression model? Yes, if you have too many features and too little examples you might end up over-fitting the model, you could use l1-regularization to ensure that even if you feed your model a lot of features some of the weights will be 0, however this requires adjusting a cost parameter. Using dummy variables after one hot encoding also drastically increases the number of columns, does this too have a negative effect on accuracy? Using dummy variables per say should not cause problems to the model, however increasing the number of features significantly might lead to over-fitting.
