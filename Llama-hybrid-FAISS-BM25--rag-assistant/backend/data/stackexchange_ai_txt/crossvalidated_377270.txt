[site]: crossvalidated
[post_id]: 377270
[parent_id]: 
[tags]: 
improving classification accuracy of the dataset as a whole by considering classifier distributions

Overview I'm new to machine learning so apologies if I misuse terms. I have an idea to improve my classification analysis that I feel is not terribly unique, but I can not find a reference to such a procedure with my limited knowledge. In short, I am using ML classification to try to partition my data. That is, I don't care for any given sample what class it is, but I want to know the fraction of each class in my dataset. Example problem Consider a binary classification, where I have some signal $S$ and background $B$ . I train up my classifier, and get the following confusion matrix: +---+-----+-----+ | | S | B | +---+-----+-----+ | S | 0.9 | 0.1 | | B | 0.2 | 0.8 | +---+-----+-----+ That is, 90% of signal is correctly identified and 80% of background. Now, I give my classifier a set of testing data with a 20/80 S/B split, and this classifier tells me that my data consists of $$ \begin{pmatrix}S_0 \\ B_0\end{pmatrix} = \begin{pmatrix} 0.9 & 0.2 \\ 0.1 & 0.8 \end{pmatrix} \begin{pmatrix} S \\ B \end{pmatrix} = \begin{pmatrix} 0.34 \\ 0.66 \end{pmatrix} $$ Not super great. Improvement #1 I think I am allowed to use my knowledge of the category leakage (expressed in the confusion matrix) in order to correct my population estimates. I.e., $$ \begin{pmatrix}S_1 \\ B_1\end{pmatrix} = \begin{pmatrix} 0.9 & 0.2 \\ 0.1 & 0.8 \end{pmatrix}^{-1} \begin{pmatrix}S_0 \\ B_0\end{pmatrix} $$ which in this case gives back the input signal and background perfectly. In a real case of course the mismatch won't be exactly the same as the confusion matrix on the training data, but it must still be better than taking the numbers at face value, right? Improvement #2 Let's say I modify my classifier so that, for a given input sample, if the classification probability is below some threshold (e.g. 10%), classify as "Uknown". Now I have a new confusion matrix: +---+------+------+------+ | | S | B | U | +---+------+------+------+ | S | 0.9 | 0.02 | 0.08 | | B | 0.05 | 0.75 | 0.2 | +---+------+------+------+ and when I give it the same 20/80 split, I get $$ \begin{pmatrix}S_0 \\ B_0 \\ U_0 \end{pmatrix} = \begin{pmatrix}0.22 \\ 0.604 \\ 0.176\end{pmatrix} $$ Now to estimate the true signal and background fractions, I could perform e.g. a $\chi^2$ minimization of the different weights in the confusion matrix to my test output. I.e. minimize $|\mathbf{\epsilon}|^2$ in $$ \begin{pmatrix}S_0 \\ B_0 \\ U_0 \end{pmatrix} = \mathbf{M_c} \begin{pmatrix} S_1 \\ B_1 \end{pmatrix} + \vec{\mathbf{\epsilon}} $$ where $\mathbf{M_c}$ is the confusion matrix (transposed). Improvement #3 For any sample I give to the classifier, I get a score that tells me the probability that the sample is signal. Using my training data (or better, a different, independent set of training data), I can build PDFs for the score distribution separately for signal and background events. When I want to evaluate a test dataset, I get the distribution of scores from the classifier and fit that to the weighted sum of my signal and background PDFs, and the resulting weights give me the fraction of signal and background in the test dataset. Conclusion Is this sort of approach standard, and where can I find more information? If it's not already standard, are these approaches valid, and likely to add anything useful in a real-world scenario?
