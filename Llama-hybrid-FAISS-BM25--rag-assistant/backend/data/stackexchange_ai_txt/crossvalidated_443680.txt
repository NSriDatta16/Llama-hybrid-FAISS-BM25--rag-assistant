[site]: crossvalidated
[post_id]: 443680
[parent_id]: 443653
[tags]: 
Scikit learn's implementation follows the original implementation of Breiman. At every split, a subset of features is selected (size depends on $mtry$ parameter). For each feature, the algorithm computes the drop in squared loss (eq. to variance) at every split point. The split (over all features) returning the best loss drop is then selected. The drop in the loss is simply the squared loss of the current estimator minus the one that the new estimator would achieve by splitting at a given point. The package should also be able to perform this step by only evaluating a subsample of all split points in case of large dataset, and also to evaluate the splits after binning the continuous variables in order to increase speed (like XGboost does). If you need more specific details, you can read "Understanding Random Forests" , which is the PhD thesis of one of the authors of the sklearn Random Forest package.
