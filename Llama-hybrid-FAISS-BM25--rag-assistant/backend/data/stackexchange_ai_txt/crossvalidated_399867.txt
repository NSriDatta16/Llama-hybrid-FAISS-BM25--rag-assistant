[site]: crossvalidated
[post_id]: 399867
[parent_id]: 399848
[tags]: 
It is possible to combine the prediction from several survival models by using techniques from ensemble learning. In fact, a Random Survival Forest is already an ensemble of survival trees whose predictions are aggregated to form the final prediction of the entire forest. You can use a similar approach when combining predictions from different survival models, which would be called an heterogeneous ensemble. If you have models that just output a single risk score, which would be the case for all Cox-like models, you just have to aggregate a list of numbers. The simplest way is to just compute an average; or, you can train an additional (linear) survival model on top of the risk scores to learn how to fuse individual predictions – this is called stacking . Even more sophisticated you could consider pruning models that either perform poorly or whose predictions are strongly correlated – this is called ensemble selection or ensemble pruning . For an example for the latter, you can checkout the paper Heterogeneous ensembles for predicting survival of metastatic, castrate-resistant prostate cancer patients . The most important consideration for an ensemble to be better than its individual members is that their predictions are uncorrelated. If predictions are strongly correlated, all models would essentially give you the same answer, thus, aggregation would not give a different answer. In fact, uncorrelated predictions are usually more important than the prediction performance of individual models, as long as individual models perform better than random (concordance index > 0.5).
