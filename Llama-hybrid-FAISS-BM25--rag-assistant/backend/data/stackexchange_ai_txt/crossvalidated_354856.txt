[site]: crossvalidated
[post_id]: 354856
[parent_id]: 
[tags]: 
Unscaling Principal Components Regression Coefficients to Match Linear Regression

I am working with a dataset. We can call it dataset . portfolio USSW2 USSW5 USSW10 USSW30 2017-10-31 -0.430 0.01510 0.0171 0.0055 -0.0023 ... Each of the values are absolute changes in daily value. Since the original values are quoted in percentages, the values in this dataset represent daily change in percentage quoted. So, a "portfolio" value of 1.0 could correspond to a true portfolio value change of 3.0% to 4.0%. Let's assume the covariates are highly correlated. I mean very highly correlated, as such: USSW2 USSW5 USSW10 USSW30 USSW2 1.00 USSW5 0.94 1.00 USSW10 0.85 0.97 1.00 USSW30 0.76 0.90 0.97 1.00 Yes, I realize there is a multicollinearity problem. Let's ignore that for the moment. I can run a regression on this data (using R) using a standard lm() command. > test.reg1 |t|) (Intercept) -0.001107 0.022498 -0.049 0.96080 USSW2 0.542981 3.171097 0.171 0.86424 USSW5 -13.689734 5.008679 -2.733 0.00692 ** USSW10 6.976394 5.817408 1.199 0.23207 USSW30 7.052272 3.460557 2.038 0.04307 * --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 0.2882 on 174 degrees of freedom Multiple R-squared: 0.2887, Adjusted R-squared: 0.2724 F-statistic: 17.66 on 4 and 174 DF, p-value: 3.505e-12 I can interpret the Beta values (estimates) of the regression as such: When USSW2 increases by 1, portfolio increases by 0.542981 When USSW5 increases by 1, portfolio decreases by -13.689734 ... If I am correct in this, then the interpretation of the result is really simple and useful to me. I can understand how one variable (or multiple variables) affect the portfolio variable in my regression. I can also split the dataset, 80:20, into training and testing. This way, I can test for MSEP (mean squared error of prediction) of my standard linear regression. # set up training and testing datasets > range.train train.data test.data fit.train1 predictions1 fit1.ActPred mse1 Everything up to this point is standard, except for the fact that my model is not real great. Please remember this is a toy version of the real dataset I am working with, so using the methods I am using really do make sense. Next, I run Principal Component Regression (PCR) on the same dataset. There is a very nice R package that does this for me (including rescaling). In my understanding, PCR is doing the following: Running PCA on my covariates to determine loadings on PCs. Running regression using PCs (not input variables) as new covariates. This sounds pretty good! Maybe I will get a better result if I effectively group my covariates prior to regressing. I give it a try, holding my the number of components steady at 3: > library(pls) > pcr.a print(summary(pcr.a)) Data: X dimension: 179 4 Y dimension: 179 1 Fit method: svdpc Number of components considered: 3 VALIDATION: RMSEP Cross-validated using 10 random segments. (Intercept) 1 comps 2 comps 3 comps CV 0.3388 0.3443 0.3050 0.3073 adjCV 0.3388 0.3436 0.3043 0.3061 TRAINING: % variance explained 1 comps 2 comps 3 comps X 92.268 99.21 99.86 portfolio 2.804 24.87 28.25 From this, I can get the percentage of variance explained by my target 3 PCs (28.25%), which is comparable to my adjusted R-squared value from my standard regression (27.24%). If I run the train/test protocol using this method to predict "future" values, I can get the MSEP for PCR as well. MSEP(PCR) = 0.2578109 , while MSEP(reg) = 0.2749278 . This is also slightly better. Now I want to get beta values in the same way I did before. I use a built-in function which is able to extract the same beta value structure from the PCR routine: > options(warn=-1); print(jack.test(pcr.a)); options(warn=0) Response portfolio (3 comps): Estimate Std. Error Df t value Pr(>|t|) USSW2 -0.0262841 0.1456076 9 -0.1805 0.86075 USSW5 -0.2910241 0.1986047 9 -1.4653 0.17687 USSW10 -0.0033343 0.1071678 9 -0.0311 0.97586 USSW30 0.3935101 0.1300165 9 3.0266 0.01432 * --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Since the values are scaled in the method, I can't interpret the values in the same way as linear regression. How can I transform my beta estimates in this PCR routine so that they can be understood in the same way as the linear regression?
