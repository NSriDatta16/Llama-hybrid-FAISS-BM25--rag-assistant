[site]: datascience
[post_id]: 37372
[parent_id]: 37368
[tags]: 
It completely depends on what you're classifying. Using character embeddings for semantic classification of sentences introduces unnecessary complexity, making the data harder to fit. Although using n-grams would help the model deal with word derivatives. Classifying words based on their derivative would be a task that would require character embeddings. If you're asking whether it would be useful to train a model to embed characters like you would with word2vec- then no. And in fact would probably yield bad results. We use embeddings to implicitly encode that two data points are close together and therefore should be treated more similar to the model. The letter 'd' shouldn't be semantically closer to 'e' than 'q'.
