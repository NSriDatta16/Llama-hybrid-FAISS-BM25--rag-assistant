[site]: crossvalidated
[post_id]: 561720
[parent_id]: 
[tags]: 
Poor model performance on certain out-of-sample data

We're noticing poor model performance on certain out of sample products. We have trained a ML model on about 2000 different products in a few markets. Our predictors include a) product characteristics b) historical sales, revenue, marketing expenses etc. c) Location and demographic control variables such as median income Target variable is price / value of the product. When we fit the model and calculate our overall error, it's about 15%. For certain properties, the ones that are very high in value / prestigious or in sparse locations, the model doesn't do a good job of prediction. The error is quite high 25-40%. Additional info: Is this a time series problem? We do not have time series data, but, we refresh the data monthly. How much data (rows) do you have? 2000 rows * 15 columns (numeric and categorical) Are you training a single model on all different products? Yes, single model for different products with sales data from different markets. Location/market-specific variables are included in the model. Are you using cross validation to tune your model? How exactly are you doing your train/test split? X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,0:14], df.iloc[:,-1], test_size=0.1, random_state=42) Training an xgboost model with RandomizedSearchCV . model = RandomizedSearchCV(model, param_distributions=params, n_iter=50, scoring="neg_mean_absolute_error", cv=5, verbose=1, n_jobs=1, return_train_score=True, error_score='raise') where params, params = {"objective":"reg:squarederror", "colsample_bytree": xgbest.colsample_bytree, # fraction of cols to sample "gamma": xgbest.gamma, # min loss reduction required for next split "learning_rate": xgbest.learning_rate, "max_depth": xgbest.max_depth, # controls model complexity and overfitting "n_estimators": xgbest.n_estimators, "subsample": xgbest.subsample} # % of rows to use in training sample We need to handle this variance in our model and lower the error rates. What are some ways to reduce the variance error of our model? Is this a case of model overfitting and how might we tackle this problem?
