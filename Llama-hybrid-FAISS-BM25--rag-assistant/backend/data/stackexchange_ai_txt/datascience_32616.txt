[site]: datascience
[post_id]: 32616
[parent_id]: 32614
[tags]: 
That's partially right. Apart from the weights (and bias) you also have the so called hyper-parameters . Depending on your architecture, you may have different ones. For a simple feedforward neural network, the hyperparameters are: number of neurons number of layers learning rate eta (η) regularization penalty lambda (λ) momentum number of epochs batch size dropout etc These hyperparameters are (usually) user-tuned. The weights (and bias) are self-trained by the network, based on optimization. Keep in mind that hyperparameters like the number of neurons or layers are immediately connected with the underlying model of your data, therefore they are an index of your dataset's complexity.
