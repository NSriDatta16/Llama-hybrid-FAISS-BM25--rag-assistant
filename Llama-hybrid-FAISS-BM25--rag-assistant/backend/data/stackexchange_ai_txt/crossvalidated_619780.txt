[site]: crossvalidated
[post_id]: 619780
[parent_id]: 619769
[tags]: 
First of all, intuitively, it is an expected value , because we want to know what is the "average uncertainty" of a random variable. It also tells you about the "number of bits on average required to describe the random variable". If you didn't weigh the probabilities, the "average" wouldn't make sense. Let's re-write the entropy as an expected value of the information content of the $x_i$ values $$ H(X) = E[I(X)] = \sum_i p(x_i) I(x_i) $$ With your formula, this would be just a (normalized) sum $\sum_i I(x_i)$ , why would you care about the sum? You don't care about the "total information content" but the average. If you get a random message $x_i$ , you can use the expected value to make a rough guess on how many bits you need to encode it. The probability that you observe a particular $x_i$ value is $p(x_i)$ , so that should be its contribution to the average. It is just an expected value of the function (that tells us about the information content) of a random variable. But Shannon (1948) also listed the formal requirements he wanted the entropy to follow $H$ should be continuous in the $p_i$ . If all the $p_i$ are equal, $p_i = \frac{1}{n}$ , then $H$ should be a monotonic increasing function of $n$ . With equally likely events there is more choice, or uncertainty, when there are more possible events. If a choice be broken down into two successive choices, the original $H$ should be the weighted sum of the individual values of $H$ . and, as he proves, only the expected value satisfies them.
