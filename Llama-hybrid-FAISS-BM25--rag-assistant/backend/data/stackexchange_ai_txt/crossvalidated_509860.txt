[site]: crossvalidated
[post_id]: 509860
[parent_id]: 
[tags]: 
Attempting to implement a vectorized version of one of the backpropagation equations

I'm trying to implement a neural net that can perform backpropagation on a whole minibatch rather than iterating through each element. Here is what I want to do: Given two matrices $\delta_l$ of shape (batch size, # of neurons in in layer $l$ ). Where each element $(k, i)$ corresponds to the error for the neuron $i$ in layer $l$ at minibatch $k$ . Each element looks like this $\dfrac{\partial C}{\partial z^l_{i}}$ . $a_{l-1}$ of shape (batch size, # of neurons in layer $l-1$ ). Where the each element $(k, j)$ corresponds to the activation of neuron $j$ in layer $l-1$ at minibatch $k$ . Each element $a^{l-1}_j$ is equal to (calculations not shown) $\dfrac{\partial z^l_i}{\partial w^l_{ij}}$ for some arbitrary neuron $i$ in layer $l$ . Note that the notation I am using for weights is $w^{\text{layer}}_{\text{where it is going, where it is coming from}}$ . The reason these two matrices are used is because using the chain rule, we can see that for any weight $w^l_{ij}$ , we can compute it's derivative with respect to the cost by $ \dfrac{\partial C}{\partial z^l_{i}} \dfrac{\partial z^l_i}{\partial w^l_{ij}}$ Let $m$ be the number of neurons in layer $l$ and $n$ be the number of neurons in layer $l-1$ . Using these two matrices, I want to compute a matrix that represents the gradient of the weights, which looks like the following: $$\nabla W = \begin{bmatrix} \dfrac{\partial C}{\partial z^l_1} a^{l-1}_1 & \cdots & \dfrac{\partial C}{\partial z^l_1} a^{l-1}_n \\ \vdots &\ddots & \vdots \\ \dfrac{\partial C}{\partial z^l_m} a^{l-1}_1 & \cdots & \dfrac{\partial C}{\partial z^l_m} a^{l-1}_n \\ \end{bmatrix}$$ $$ = \begin{bmatrix} \dfrac{\partial C}{\partial z^l_1} \dfrac{\partial z^l_1}{\partial w_{1,1}^l} & \cdots &\dfrac{\partial C}{\partial z^l_1} \dfrac{\partial z^l_1}{\partial w_{1,n}^l} \\ \vdots & \ddots & \vdots \\ \dfrac{\partial C}{\partial z^l_m} \dfrac{\partial z^l_m}{\partial w_{m,1}^l}& \cdots & \dfrac{\partial C}{\partial z^l_m} \dfrac{\partial z^l_m}{\partial w_{m,n}^l}\\ \end{bmatrix}$$ Is there way I can use these two matrices and compute the gradient of the weights that connect layers $l$ and $l-1$ together? I can think of a way to do it with NumPy operations (Using tiling and transposing), but I can't think of simpler, more elegant way to do it. Perhaps I'm just looking at this whole thing wrong and should go around representing my error and activations in another way.
