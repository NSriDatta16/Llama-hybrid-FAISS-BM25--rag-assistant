[site]: datascience
[post_id]: 10474
[parent_id]: 10471
[tags]: 
If I understand you correctly, you want to err on the side of overestimating. If so, you need an appropriate, asymmetric cost function. One simple candidate is to tweak the squared loss: $\mathcal L: (x,\alpha) \to x^2 \left( \mathrm{sgn} x + \alpha \right)^2$ where $-1 def loss(x, a): return x**2 * (numpy.sign(x) + a)**2 Next let's generate some data: import numpy x = numpy.arange(-10, 10, 0.1) y = -0.1*x**2 + x + numpy.sin(x) + 0.1*numpy.random.randn(len(x)) Finally, we will do our regression in tensorflow , a machine learning library from Google that supports automated differentiation (making gradient-based optimization of such problems simpler). I will use this example as a starting point. import tensorflow as tf X = tf.placeholder("float") # create symbolic variables Y = tf.placeholder("float") w = tf.Variable(0.0, name="coeff") b = tf.Variable(0.0, name="offset") y_model = tf.mul(X, w) + b cost = tf.pow(y_model-Y, 2) # use sqr error for cost function def acost(a): return tf.pow(y_model-Y, 2) * tf.pow(tf.sign(y_model-Y) + a, 2) train_op = tf.train.AdamOptimizer().minimize(cost) train_op2 = tf.train.AdamOptimizer().minimize(acost(-0.5)) sess = tf.Session() init = tf.initialize_all_variables() sess.run(init) for i in range(100): for (xi, yi) in zip(x, y): # sess.run(train_op, feed_dict={X: xi, Y: yi}) sess.run(train_op2, feed_dict={X: xi, Y: yi}) print(sess.run(w), sess.run(b)) cost is the regular squared error, while acost is the aforementioned asymmetric loss function. If you use cost you get 1.00764 -3.32445 If you use acost you get 1.02604 -1.07742 acost clearly tries not to underestimate. I did not check for convergence, but you get the idea.
