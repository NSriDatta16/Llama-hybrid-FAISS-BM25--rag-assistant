[site]: crossvalidated
[post_id]: 561195
[parent_id]: 561164
[tags]: 
One limitation of random search is that searching over a large space is extremely challenging; even a small difference can spoil the result. Émile Borel's 1913 article "Mécanique Statistique et Irréversibilité" stated if a million monkeys spent ten hours a day at a typewriter, it's extremely unlikely that the quality of their writing would equal a library's contents. And of course we understand the intuition: language is highly structured (not random), so randomly pressing keys is not going to yield a coherent text. Even a text that is extremely similar to language can be rendered incoherent by a minor error. In terms of estimating a model, you need to estimate everything correctly simultaneously . Getting the correct slope $\hat{\beta}_1$ in the model $y = \beta_0 + \beta_1 x +\epsilon$ is not very meaningful if $\hat{\beta}_0$ is very very far from the truth. In a larger number of dimensions, such as in a neural network, a good solution will need to be found in thousands or millions of parameters simultaneously. This is unlikely to happen at random! This is directly related to the curse of dimensionality . Suppose your goal is to find a solution with a distance less than 0.05 from the true solution, which is at the middle of the unit interval. Using random sampling, the probability of this is 0.1. But as we increase the dimension of the search space to a unit square, a unit cube, and higher dimensions, the volume occupied by our "good solution" (a solution with distance from the optimal solution less than 0.05) shrinks, and the probability of finding that solution using random sampling likewise shrinks. (And naturally, increasing the size of the search space but keeping the dimensionality constant also rapidly diminishes the probability.) The "trick" to random search is that it purports to defeat this process by keeping the probability constant while dimension grows; this must imply that the volume assigned to the "good solution" increases, correspondingly, to keep the probability assigned to the event constant. This is hardly perfect, because the quality of the solutions within our radius is worse (because these solutions have a larger average distance from the true value). You have no way to know if your search space contains a good result . The core assumption of random search is that your search space contains a configuration that is “good enough” to solve your problem. If a “good enough “ solution isn’t in your search space at all ( perhaps because you chose too small a region), then the probability of finding that good solution is 0. Random search can only find the top 5% of solutions with positive probability from among the solutions in the search space . You might think that enlarging the search space is a good way to increase your odds. While it might make the search region contain an extremely high quality region, but the probability of selecting something in that region shrinks rapidly with increasing size of the search space. High-quality model parameters often reside in narrow valleys. When considering hyperparameters, it's often true that the hyperparameter response surface changes only gradually; there are large regions of the space where lots of hyperparameter values are basically the same in terms of quality. Moreover, a small number of hyperparameters make large contributions to improving the model; see Examples of the performance of a machine learning model to be more sensitive to a small subset of hyperparameters than others? But in terms of estimating the model parameters , we see the opposite phenomenon. For instance, regression problems have likelihoods that are prominently peaked around their optimal values (provided you have more observations than features); moreover, these peaks become more and more "pointy" as the number of observations increases. Peaked optimal values are bad news for random search, because it means that the "optimal region" is actually quite small, and all of the "near miss" values are actually much poorer in comparison to the optimal value. To make a fair comparison between random search and gradient descent, set a budget of iterations (e.g. the $n=60$ value derived from random search). Then compare model quality of a neural network fit with $n$ iterations of ordinary gradient descent & backprop to a model that uses $n$ iterations of random search. As long as gradient descent doesn't diverge, I'm confident that it will beat random search with high probability. Obtaining even stronger guarantees rapidly becomes expensive. You can of course adjust $p$ or $q$ to increase the assurances that you'll find a very high quality solution, but if you work out the arithmetic, you'll find that $n$ rapidly becomes very large (that is, random search becomes expensive quickly). Moreover, in a fair comparison, gradient descent will likewise take $n$ optimization steps, and tend to find even better solutions than random search. Some more discussion, with an intuitive illustration: Does random search depend on the number of dimensions searched?
