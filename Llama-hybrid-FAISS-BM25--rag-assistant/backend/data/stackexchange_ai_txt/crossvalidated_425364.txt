[site]: crossvalidated
[post_id]: 425364
[parent_id]: 
[tags]: 
Regression tree splitting (CART, Scikit Learn)

I am working with a Random Forest using scikit-learn and still have some questions and thoughts I am not sure about regarding the splits in regression trees: 1. It seems to me that scikit-learn's RandomForestRegressor and DecisionTreeRegressor use just one feature at each node in order to split the data set and decide the subsequent node for future samples. Is this true? 2. The number of features and - in case not all features are considered - the features which are to be considered while looking for the best split at each node, are randomly chosen. That would mean that the overall best split won't occur at some nodes, because the respective feature possibly wasn't even considered. Is this true? 2.2. I read Breiman's CART (1984) but I can't remember the random consideration of features at each node being mentioned there. I could have possibly missed it. Is that something from the original CART algorithm or was this later included in modified algorithms? If (1.) is true: Are there any regression tree algorithms which make splits based on multiple features? (Not considering features for splits, but setting the thresholds for splits based on multiple features)
