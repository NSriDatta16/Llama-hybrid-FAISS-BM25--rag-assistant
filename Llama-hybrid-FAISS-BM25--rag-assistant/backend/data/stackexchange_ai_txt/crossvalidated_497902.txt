[site]: crossvalidated
[post_id]: 497902
[parent_id]: 497899
[tags]: 
Traditionally, machine learning approaches maximize the log-likelihood instead of the actual likelihood. That is, instead of maximizing $p(Y|X, \theta)$ wrt $\theta$ , we instead maximize $$\log p(Y | X, \theta) = \log \prod_i p(y_i | x_i, \theta) = \sum_i \log p(y_i | x_i, \theta).$$ Because the logarithm turns products into summation, the computation of the log likelihood is a sum of large negative numbers, as opposed to a product of small positive numbers. This leads to better numerical stability compared to the actual likelihood, as you have noticed. Equivalently, you can also minimize the negative of the log likelihood, which is exactly the same as maximizing the log likelihood. From your post, I'm guessing that what you are trying to do is roughly (in pseudocode): likelihood = 1 for datapoint in dataset: likelihood *= density(datapoint) log_likelihood = log(likelihood) log_likelihood.backward() what you should be doing instead is: log_likelihood = 0 for datapoint in dataset: log_likelihood += log(density(datapoint)) log_likelihood.backward()
