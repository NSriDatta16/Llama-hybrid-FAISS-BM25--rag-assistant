[site]: crossvalidated
[post_id]: 281606
[parent_id]: 280442
[tags]: 
The holdout is considered the validation set, not the test set. The test set gives you an idea of how well your model performs on unseen data, ie how well your model represents reality. That said, ideally you can catch overfitting before you expose your model to the test set. This is where the validation (holdout) set comes in. By systematically setting aside a validation set to compare, in this case, 90% of the training data to, you benefit in at least two ways: If there is large fluctuation in error between validation sets, there may be overfitting Using the average of each training/validation split provides you with a better sense of the model's true accuracy. Ultimately, this gives you an overfitting-proof assessment, as your total model accuracy is less dependent on the arbitrary choice of your training set
