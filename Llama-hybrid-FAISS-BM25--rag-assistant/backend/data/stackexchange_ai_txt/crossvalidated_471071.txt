[site]: crossvalidated
[post_id]: 471071
[parent_id]: 
[tags]: 
What is the right model structure for spike-and-diminish feature?

I am asked to build a model for detecting anomalies/monitoring the usage of some machines. The x-axis is time in minutes, and the y-axis is the resource usage of a machine (you can think of something like CPU usage, or RAM usage, but not always the case), my data consists of many series that monitoring different performance metrics. The photo below is an example. We can see the underlying process goes like: - the metric (monitoring the usage of a machine) remains at 0 when no task is run; - while a task is performed, the machine starts to heat up causing the monitoring metric to go up; - there may be some consistent pattern in each spike; - the metric dial downs until the next call. For the record, we are not given the time when each task is performed, so we don't have exogenous variables that give the cycle, we only have multiple metrics that monitor the same system. And we are not intending to incorporate periodic features (at least at the moment) because the timing of the tasks run on the system could change, I am only showing the graph with the nicest pattern. And when it does change, it is not really counted as a system anomaly. But what counts as an anomaly in our definition is strange patterns or extreme outliers. And For this task, I am only interested in what happens (sequentially) during a repeated but random event assuming we don't know when it comes. (i.e., extract/model some local patterns). My questions are: assuming there are different tasks running on the same system at different time, and the patterns that different tasks trigger may be different, what are the approaches for this sort of time series clustering? similarly, is there any "state" -ish model that could track the types of events given a subsequence of data? what are the better ways in modeling heteroskedasticity for this sort of data? assuming we want to decompose the data structure into long-run short-run, with obviously long-run trend being the 0, how do I model spike that is not normally distributed around the LR trend. does an error correction model make sense here as each data will be "corrected" to its trend on long run? We are modeling using multivariate data (multiple metrics on the same system), and we have tried VAR (vector autoregression). What else could we try on besides VAR and VECM? also we cannot use deep learning methods (such as LSTM or variational encoder) considering the model training resources are limited, and we are interested in building a base model that yields insight rather than for prediction. (you don't need to answer all, cause we are still only looking for clues to invest our time on, more like a brainstorming process).
