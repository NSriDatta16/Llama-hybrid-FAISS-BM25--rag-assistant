[site]: datascience
[post_id]: 115351
[parent_id]: 
[tags]: 
Why compare multiple machine learning algorithms and then decide which algorithm to use for fine tuning?

I have a problem. There is a dataset A, which deals with a classification problem. And for this dataset, several different baseline algorithms have been defined and computed. In addition, three models were used: Logistic Regression, XGBoost and RandomForest. Now my question is this, why use different algorithms (Logistic Regression, XGBoost and RandomForest) and investigate which one is the better algorithm? Is it because the algorithms have different strengths and perform better depending on the data set? Algorithm Accuracy Precision Recall F1-Score Baseline 1 0,20 0,20 0,20 0,20 Baseline 2 0,20 0,20 0,20 0,20 Logistic Regression 0,53 0,52 0,28 0,36 RandomForest 0,65 0,64 0,63 0,63 XGBoost 0,50 0,61 0,55 0,58 For example, RandomForest, gave the best result and then the hyperparameters are adjusted.
