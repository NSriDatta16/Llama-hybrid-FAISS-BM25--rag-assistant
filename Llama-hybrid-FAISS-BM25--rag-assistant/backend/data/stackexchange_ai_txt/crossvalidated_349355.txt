[site]: crossvalidated
[post_id]: 349355
[parent_id]: 349343
[tags]: 
Consider a matrix where the rows correspond to documents, and the columns correspond to words. In this matrix - exactly as you say - the question of whether to use tf or tf-idf exactly corresponds to the question of whether to scale each column by some constant (which happens to be the idf ). The question is just how this scaling (or its absence) affects the method chosen for this matrix. If the cosine similarity between rows is used, then the scaling matters - cosine similarity is not invariant to scaling. Without idf , the numbber of times "the" is used, can dominate the similarity, for example. Note that cosine similarity was used in some IR (information retrieval) systems. For random forests and linear models, for example, scaling doesn't matter. For linear models with regularization, or when interpreting the coefficients of linear models, scaling can matter a great deal. Regarding your specific question, you might want to use regularized SVM , in which case the weight of "the" can matter. If dimension reduction via SVD is used, scaling can matter. Scaling can matter, for practical reasons, to NNs.
