[site]: crossvalidated
[post_id]: 414358
[parent_id]: 
[tags]: 
Why are weights being used in (generalized) dice loss, and why can't I?

Generalized dice loss is advocated as optimizing mIoU directly in semantic segmentation problems (especially those with a severe class imbalance), as opposed to other loss functions like multinomial cross-entropy which only serve as a "proxy". The paper on generalized dice loss uses weights inversely proportional to labels area, in order to better predict labels with generally small regions. mIoU actually weights each label equally, since it is just an average of IoUs over all labels. Why then does generalized dice loss still need to use weights? What's more, in my own implementation, if I use weights, then my loss will be exclusively 1, since the ratio between numerator and denominator is too small: def generalized_dice_loss(onehots_true, logits): onehots_true, logits = mask(onehots_true, logits) probabilities = tf.nn.softmax(logits) weights = 1.0 / ((tf.reduce_sum(onehots_true, axis=0)**2) + 1e-3) weights = tf.clip_by_value(weights, 1e-17, 1.0 - 1e-7) numerator = tf.reduce_sum(onehots_true * probabilities, axis=0) #numerator = tf.reduce_sum(weights * numerator) denominator = tf.reduce_sum(onehots_true + probabilities, axis=0) #denominator = tf.reduce_sum(weights * denominator) loss = 1.0 - 2.0 * (numerator + 1) / (denominator + 1) return loss Omitting the weights yields workable loss, but then my network only predicts the three or four biggest out of 21 classes. I thought that even without weighting, dice loss would be a good solution to class imabalanced problems, but it only makes the problem worse; if I use multinomial cross-entropy, the network predicts far more classes. I have comparetively little ground truth though, so maybe the network has taught itself to just aim for a high IoU on the big classes and ot even bother guessing the smaller ones? Why can't I use the weights? And if I could, what would that add? --------------------------------------------------------------------------------- For reference, my weights after clipping them to [epsilon, 1 - epsilon], look like this: tf.Tensor( [4.89021e-05 2.21410e-10 5.43187e-11 1.00000e+00 1.00000e+00 4.23855e-07 5.87461e-09 3.13044e-09 2.95369e-07 1.00000e+00 1.00000e+00 2.22499e-05 1.00000e+00 1.73611e-03 9.47212e-10 1.12608e-05 2.77563e-09 1.00926e-08 7.74787e-10 1.00000e+00 1.34570e-07], shape=(21,), dtype=float32) which seems fine to me, though they are pretty small. The numerators ( tf.reduce_sum(onehots_true * probabilities, axis=0) , prior to their weighting) look like this: tf.Tensor( [3.42069e+01 0.00000e+00 9.43506e+03 7.88478e+01 1.50554e-02 0.00000e+00 1.22765e+01 4.36149e-01 1.75026e+02 0.00000e+00 2.33183e+02 1.81064e-01 0.00000e+00 1.60128e+02 1.48867e+04 0.00000e+00 3.87697e+00 4.49753e+02 5.87062e+01 0.00000e+00 0.00000e+00], shape=(21,), dtype=float32) tf.Tensor(1.0, shape=(), dtype=float32) which also looks reasonable, since they're basically the labels' respective sizes times the network's certainty about them (which is likely low in the beginning of training). The denominators ( tf.reduce_sum(onehots_true + probabilities, axis=0) , prior to weighting) also look fine: tf.Tensor( [ 14053.483 25004.557 250343.36 66548.234 6653.863 3470.502 5318.3926 164206.19 19914.338 1951.0701 3559.3235 7248.4717 5984.786 7902.9004 133984.66 41497.473 25010.273 22232.062 26451.926 66250.39 6497.735 ], shape=(21,), dtype=float32) These are large, but that is to be expected since the class probabilities of a pixel sum to 1, and therefore the sum of these denominators should more or less equal the amount of pixels with ground truth. However, summing the numerators gives a very small sum (~0.001, though occasionally it's in a single digit range) while the denominator sums to very large values. This results in my final loss being exclusively 1, or something really close to that. I do not understand why the weights make the ratio so small. It makes sense to me that if I scale the numerators and denominators by the same (albeit very tiny) amount, that the ratio should be kept at least more or less the same. Printing the numerators and denominators even show they have the same order of magnitude as their weighted counterparts.
