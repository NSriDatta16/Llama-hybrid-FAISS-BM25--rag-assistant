[site]: crossvalidated
[post_id]: 525319
[parent_id]: 
[tags]: 
Plug-in principle with kernel density estimate

The plug-in principle says that to estimate a statistical functional of the form $$ T(\mu) = \int f(x)\ d\mu(x) $$ we can replace $\mu$ with the empirical distribution $\mu_n$ depending on data $X_1,\ldots,X_n \sim \mu$ (i.e. taking a Monte Carlo average). However, say that $\mu$ has a density, so we could instead replace $\mu$ with a kernel density estimate (KDE) $\hat{\mu}_n$ to get an estimate $$ \hat{T}_n(\mu) = \int f(x) \hat{\mu}_n(x)\ dx \, . $$ My questions are the following: Do estimators of this form, that depend on a KDE (or some other estimate of the distribution), have a name? Or are they still simply referred to as plug-in estimators? An example of this would be the Nadaraya-Watson estimator, but I'm interested more generally in estimating other quantities of interest, say the 4th moment. How do the estimators depending on the KDE compare to those depending on the empirical distribution (usual plug-in estimator)? For the second question I would imagine that certain kernels could provide better estimates if, for example, the density $\mu$ is known to be twice-differentiable.
