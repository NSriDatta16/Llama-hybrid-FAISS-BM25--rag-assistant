[site]: crossvalidated
[post_id]: 95277
[parent_id]: 95263
[tags]: 
Here's a demonstration in r of how @gung's answer applies in logistic regression. First, a function for simulating data and printing logistic regression summaries using continuous and binned versions of the predictor. Outcome probability is a function of the uniformly distributed predictor. Change n to suit your sample size and runif to suit your theoretical distribution if you like. thesims=function(n=30,seed=1){set.seed(seed);marks=runif(n,0,100) admitted=rbinom(n,1,marks/100);binned=c();for(i in 1:n) {if(marks[i] With the default $n=30$ and set.seed(1) , the continuous predictor is clearly significant $(z=2.77,p=.006)$, but the binned predictor is very far from it $(z=.01,p=.994)$. This is a pretty extreme difference, but all other seeds will still produce a weaker result for the binned predictor. Here's a way of visualizing the difference in the same simulated data as above. With continuous data: require(ggplot2);ggplot(data.frame(marks,admitted),aes(x=marks,y=admitted))+ scale_x_continuous('Marks')+scale_y_continuous('Probability of admission',lim=0:1)+ geom_point(position=position_jitter(width=0,height=.04))+ stat_smooth(method='glm',family='binomial') Things look as they should...but with binned data: ggplot(data.frame(admitted,binned),aes(x=binned,y=admitted))+ scale_x_continuous('Binned marks')+scale_y_continuous('Probability of admission',lim=0:1)+ geom_point(position=position_jitter(width=0,height=.04))+ stat_smooth(method='glm',family='binomial') Fitted probabilities of zero or one are bad news. Again, this is an extreme example, but you will generally find wider confidence bands with binned data. This is to be expected because you're effectively throwing away useful information by binning.
