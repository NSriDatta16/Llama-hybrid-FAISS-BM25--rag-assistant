[site]: datascience
[post_id]: 3726
[parent_id]: 2673
[tags]: 
This is the fundamental challenge to all data modeling. We don't just want to memorize the the link between a given input and a given output (otherwise you wouldn't be modeling data, you'd be memorizing 1:1 connections with a dict / hash / relational database table / etc). We want to capture the underlying pattern in the data from only looking at the training data. Let's expand a little on your gravity example. You have your 10 training samples showing the start and ending position of an object dropped. For consistency, let's say the object was dropped the moment the object's location was initially recorded and the ending location was recorded at some precise time interval later (but before the object hit the ground). Let's also say the model (neural network in this case) managed to precisely learn the expected change in location since it just comes down to subtraction in one axis. You can show it another 10, 100, 1000 examples that all leverage the connection found and your model will keep performing well. Why not keep going to 10k, 100k, or even more samples? Theoretically, if you managed to isolate the connection and run the experiment the same way each time, your model will always work. But realistically, something is going to eventually change in the system. You hire a new lab assistant who tends to press the 'record location' button well after having dropped the object (giving the object more initial velocity, which you won't notice having only recorded location). Maybe you lost your initial ball and had to use something else which is lighter and catches the wind more (so it goes slower). .... the longer you run the experiment, the more small changes will creep into your system. Eventually these changes will alter the connection enough to make your initial model wrong. When modeling data, we want to capture the underlying patterns and acknowledge that the model only matters as long as those underlying patterns stay relevant. It's not really about the number of samples. It's about the connections / the model itself. The number of samples just happens to be one of the better proxies we have - the more samples you use, the more confident you have some underlying pattern. 'Statistical validity' is one stab at solving this, though it's validity is still up for question in the era of big data. There is plenty of work done trying to solve for how to gain confidence in good generalization in neural networks specifically, but it's still very much an open question. For a different example, if you're looking at user behavior, you'll see differences between day and night; weekdays and weekends; summer and winter; year of a person's life; culture a person grew up in... even if you prove you found a pattern in your initial sample, the system will eventually change and it's up to luck whether the connection(s) you found are a part of the system that changed or a part of the system that didn't.
