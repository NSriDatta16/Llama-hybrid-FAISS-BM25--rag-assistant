[site]: crossvalidated
[post_id]: 108756
[parent_id]: 108676
[tags]: 
Let us be clear that the "variance" under discussion appears to be a random variable derived from a finite portion of a time series. Specifically, the raw $k^\text{th}$ moment of $\mathrm{X} =(X_1, X_2, \ldots, X_N)$ is $$\mu_k(\mathrm{X}) = (X_1^k+X_2^k+\cdots+X_N^k)/N,$$ which is a random variable, and the variance is $$\text{var}(\mathrm{X}) = \mu_2(\mathrm{X}) - \mu_1^2(\mathrm{X}),$$ which also is a random variable. Similarly we may define moments $\mu_{jk}$ of the bivariate series $(X_i,Y_i)$ and from those compute a covariance. All these definitions make sense even when either series is constant (although then the moments and variance may reduce to numbers rather than random variables). To show that counterexamples exist even when $X$ and $Y$ have positive covariance, let the $Y_i$ be bounded by $0$ and $1$, let $\mathrm{Y}$ have nonzero variance, pick $0 \lt \varepsilon \lt 1$, and define $$X_i = 1 + \varepsilon Y_i \ge 0.$$ By construction there is perfect (unit) correlation between each $X_i$ and $Y_i$ as well as between $\mu_k(\mathrm{X})$ and $\mu_k(\mathrm{Y})$ for any $k\gt 0$; certainly the covariances are positive. Yet, since $Z_i=X_iY_i = Y_i + \varepsilon Y_i^2$, $$\text{Var}(\mathrm{Z}) = \text{Var}(\mathrm{Y}) + 2\varepsilon\mu_1(\mathrm{Y}^3) + \varepsilon^2 \mu_1(\mathrm{Y}^4) \gt \text{Var}(\mathrm{Y}) \gt \varepsilon^2 \text{Var}(\mathrm{Y}) = \text{Var}(\mathrm{X}),$$ disproving the conjecture in the question. The same analysis (coupled with the fact that $\mu_1(\mathrm{Y}^4)\lt \mu_1(\mathrm{Y}^2)$) demonstrates that for sufficiently large $\varepsilon\gt 1$, the inequality must be reversed. Thus there is no necessary inequality relating $\text{Var}(\mathrm{X})$ and $\text{Var}(\mathrm{Z})$.
