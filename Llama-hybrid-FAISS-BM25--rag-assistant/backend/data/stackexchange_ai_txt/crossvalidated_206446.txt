[site]: crossvalidated
[post_id]: 206446
[parent_id]: 
[tags]: 
How can I estimate a state-action matrix for q-learning when I do not have complete knowledge of all possible spaces and actions?

In this example of q-learning the "state-action" matrix R can be easily defined since there is a limited number of possible actions in each state and they are easy to identify. This example is very simple and I was able to make a simulaton using Python within minutes I read the article. However, suppose that you do not know the entire "state-action" matrix R and consider the following complications/hypothesis: Available actions in each state are stochastic. Probabilities of what actions might be available in a certain state can be estimated by making the agent exploring the state-space. You do not have a full knowledge of all the possible states (say for instance it is not practical to try to gather it). You know the outcome you want but there isn't any, as far as you can tell, a clear deterministic strategy to get the best outcome. That is, such strategy might exist but it may not be obvious and cannot be "hard coded" For instance in the example of q-learning I mentioned, the optimal strategy can be "hard coded" using a sequence of if-else statement. My question is, how could I get an estimate of the R matrix? Could you provide me with some of the most standard approaches (say 2 at most, although 1 is enough) used in these cases, so that I can try to make a simulation? I wonder if Markov Chains could be useful, but I'm not really sure. I understand that this problem is a variation of the one in the example here however I really cannot get my head around it.
