[site]: crossvalidated
[post_id]: 534836
[parent_id]: 534830
[tags]: 
Because (unlikely sensitivity/recall) precision depends on the prevalence of each class, this does not even hold for the two-class case. This will in general depend a lot on the specifics of the situation, especially on whether there is a boundary area where one class is more likely but the other class(es) as still possible. The bigger this boundary area is, the more it matters that you train and evaluate with the same class proportions as in the final test set. E.g. for image classification and some types of fraud detection, the correct answer is usually very clear and the boundary area is relatively small. In those scenarios, it is often quite unproblematic to train on different distributions (e.g. making it 50:50 because this makes it easier for the neural network to learn). In contrast, for predicting based on age, sex and blood pressure whether someone will get a myocardial infarction in the next 10 years, it will be large (it's entirely plausible that people who are according to our data completely identical get very different outcomes), in which case the covariate distribution in training and validation data matters enormously. One idea could be to think of plausible distributions. Then, you could find hyperparameters that give a good average performance across the plausible distributions and don't result in too bad a worst-case performance.
