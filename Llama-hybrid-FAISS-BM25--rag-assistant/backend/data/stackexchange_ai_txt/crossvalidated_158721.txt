[site]: crossvalidated
[post_id]: 158721
[parent_id]: 158712
[tags]: 
You are correct that if you try to directly optimize the SVM's accuracy on training cases, also called the 0-1 loss, the gradient vanishes. This is why people don't do that. :) What you're trying to do, though, isn't really an SVM yet; it's rather just a general linear classifier. An SVM in particular arises when you replace the 0-1 loss function with a convex surrogate known as hinge loss ; this amounts to the idea of margin maximimization which is core to the idea of an SVM. This loss function is (almost) differentiable; the only issue is if any outputs are exactly at the hinge point, which (a) happens with probability zero under most reasonable assumptions and (b) then you can just use either 0 or 1 as the derivative (or anything in between), in which case you're technically doing subgradient descent. Since you're talking about backpropagation, I'll assume you're at least a little familiar with optimizing neural networks. The same problem occurs with neural network classifiers as well; this is why people use other loss functions there too.
