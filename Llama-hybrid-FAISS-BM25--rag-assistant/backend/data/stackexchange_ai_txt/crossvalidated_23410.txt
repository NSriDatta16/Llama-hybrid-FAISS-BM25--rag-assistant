[site]: crossvalidated
[post_id]: 23410
[parent_id]: 23409
[tags]: 
Oh well, gotta answer my own question. Quotting "Data Mining: Practical Machine Learning Tools and Techniques" , ..postpruning does seem to offer some advantages. For example, situations occur in which two attributes individually seem to have nothing to contribute but are powerful predictors when combinedâ€”a sort of combination-lock effect in which the correct combination of the two attribute values is very informative whereas the attributes taken individually are not. Most decision tree builders postprune; it is an open question whether prepruning strategies can be developed that perform as well. So basically while building a whole decision tree (rather that a subset of it as in pre-prunning) we may often come up with powerful "combined predictors" which are plausible to notice only when the whole tree (rather than its subset) is built. Moreover, this is the recommeded approach and forward pruning is rarely used at all.
