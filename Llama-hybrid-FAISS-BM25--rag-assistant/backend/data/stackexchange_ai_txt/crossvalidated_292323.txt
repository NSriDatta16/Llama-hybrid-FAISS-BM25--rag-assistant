[site]: crossvalidated
[post_id]: 292323
[parent_id]: 292291
[tags]: 
Two important points that are not directly related to your question: First, even the goal is accuracy instead of interpretation, regularization is still necessary in many cases, since, it will make sure the "high accuracy" on real testing / production data set, not the data used for modeling. Second, if there are billion rows and million columns, it is possible no regularization is needed. This is because the data is huge, and many computational models have "limited power", i.e., it is almost impossible to overfit. This is why some deep neural network has billions of parameters. Now, about your question. As mentioned by Ben and Andrey, there are some options as alternatives to regularization. I would like to add more examples. Use simpler model (For example, reduce number of hidden unit in neural network. Use lower order polynomial kernel in SVM. Reduce number of Gaussians in mixture of Gaussian. etc.) Stop early in the optimization. (For example, reduce the epoch in neural network training, reduce number of iterations in optimization (CG, BFGS, etc.) Average on many models (For example, random forest etc.)
