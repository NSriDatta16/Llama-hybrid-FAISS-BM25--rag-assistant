[site]: crossvalidated
[post_id]: 552847
[parent_id]: 
[tags]: 
Is early stopping a reasonable method to prevent overfitting in Machine Learning algorithms?

I'm new to Machine learning and have just come across early stopping criteria. I understand it uses a validation set to measure how the accuracy (or any score) improves over iteration (while training). However, what confuses me is that most ML algorithms I came across, such as SVM, Linear Regression etc are convex optimization problems, i.e, the local optimum is the global optimum. Does early stopping really work on those problems? Also, Is early stopping criteria useful for algorithms such as Decision Trees, Random Forests, Adaboost, etc, (Tree-based methods) where we may not ideally find the best optimum?
