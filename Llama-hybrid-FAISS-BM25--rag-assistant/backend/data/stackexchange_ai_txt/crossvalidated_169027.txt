[site]: crossvalidated
[post_id]: 169027
[parent_id]: 120200
[tags]: 
One approach to combine results from multiple models in a manner that reflects how well each model works on the dataset under analysis is Bayesian model averaging. One simple way of approximating full Bayesian model averaging is to use weights proportional to $e^{-\text{BIC}_m/2}$ (or using the effective number of parameters Ã  la DIC for a hierarchical model) for model $m=1,\ldots,M$. These weights approximate the posterior model probabilities, if a-priori all models are equally likely. Other weights are also possible, e.g. you might a-priori favor more complex models to get a AIC like behavior in the model averaging. You can either average estimates (on a suitable scale) using these weights (and there are straighforward formulae published for combining uncertainty assuming a reasonably normal posteriors from each model) or perhaps in a more Bayesian fashion sample from the posteriors in proportion to these weights.
