[site]: datascience
[post_id]: 116160
[parent_id]: 
[tags]: 
What is the difference (interpretation) between the partial R^2 and the SHAP value for a linear regression model?

To calculate the coefficient of partial determination R 2 for a given variable: We calculate the R 2 with and without that variable and substract them. This implies fitting a different model with and without that variable. I don't know if for this calculation it's better to use adjusted R 2 s or unadjusted ones. The procedure to calculate the SHAP value is more convoluted: The average of the output variation for random inputs on the other variables is calculated, varying just one variable, two, three... and in different orders. This process can be repeated from different staring inputs and take the grand average. SHAP is a black-box method useful for any model but it's very slow. Sampling some of the data and interpolation may be used to accelerate its calculation. But intuitively... How are the interpretation of the partial R 2 and the SHAP value different when applied to a linear regression model? https://online.stat.psu.edu/stat462/node/138/ https://christophm.github.io/interpretable-ml-book/shapley.html#shapley https://www.aidancooper.co.uk/a-non-technical-guide-to-interpreting-shap-analyses/
