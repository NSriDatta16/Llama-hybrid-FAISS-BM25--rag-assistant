[site]: datascience
[post_id]: 90184
[parent_id]: 
[tags]: 
How does an autoencoder 'fill in the blanks' in the context of a recommender system?

My understanding is that an autoencoder takes an input, produces a lower dimensional representation of the input, which should explain the original features in the dataset, and then reconstructs the dataset from this lower dimensional representation. I can see how this is useful in the context of dimensionality reduction. However, in the context of a recommender system, given that the 'missing ratings' are going to be present on both sides of the autoencoder - how does the autoencoder learn to predict these missing ratings? Let us consider for example a recommender system that uses binary scores (1 if the user has interacted with an item, and 0 if they have not). Given that we have the same 1s and 0s in both the input and the output (maybe slightly different if we use some kind of dropout), how is it that the 0s are replaced by 'predicted scores'?
