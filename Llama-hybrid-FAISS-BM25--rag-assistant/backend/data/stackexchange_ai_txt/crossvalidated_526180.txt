[site]: crossvalidated
[post_id]: 526180
[parent_id]: 
[tags]: 
Distance Measures between Prior and Posterior

Suppose that I want to measure the distance between a discrete posterior distribution $p(x|Data)$ and each discrete prior distribution $p(x)$ . When I have full analytical knowledge of both the posterior and prior distributions I assume I can calculate the Kullback-Leibler divergence $KL(p(x|Data),p(x))$ , https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence . However, in the case where it is not easy to make calculations in order to derive analytically the posterior, and we only have $MCMC$ samples from the $p(x|Data)$ , I think that it is rational to use the Wasserstein distance https://en.wikipedia.org/wiki/Wasserstein_metric , for calculating the distance between the posterior and prior. Essentially we will simulate samples $x_{1},...,x_{n}$ from the prior $p(x)$ and calculate the Wasserstein between those sample and the $MCMC$ posterior sample $x_{1}^{MCMC},...,x_{n}^{MCMC}$ Does this procedure seems reasonable, or there are other methods for calculating the distance between the posterior and prior in the concept that I described?
