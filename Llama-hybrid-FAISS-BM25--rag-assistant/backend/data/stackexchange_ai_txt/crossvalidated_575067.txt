[site]: crossvalidated
[post_id]: 575067
[parent_id]: 575008
[tags]: 
This maybe should be considered a duplicate of this question , but that doesn't directly address model-comparison/averaging concerns. Gelman's standardize() function is directed to comparisons of regression coefficient magnitudes within a single model. He writes in a blog post : For comparing coefficients for different predictors within a model, standardizing gets the nod. (Although I don’t standardize binary inputs. I code them as 0/1, and then I standardize all other numeric inputs by dividing by two standard deviation, thus putting them on approximately the same scale as 0/1 variables.) For comparing coefficients for the same predictors across different data sets, it’s better not to standardize–or, to standardize only once. Baguley discusses the slipperiness of standardized effect sizes when the denominator starts changing under your feet. That said, there's no need for centering or scaling in your application, with one potential exception discussed later. You don't seem to be interested in comparisons among regression coefficients within any model, and all your models are based on a single data set. Whether you express the values in their original scales or in transformed scales, each of your models will provide coefficients in the corresponding scale. Just be consistent for all models. With respect to centering, Frank Harrell says: "I almost never use centering, finding it completely unncessary and confusing." Some "main effect" coefficients for predictors involved in interactions might be easier to interpret if continuous predictors are centered, but model predictions for specific scenarios will be the same regardless. See this question for an outline of the considerations, and this question for extensive discussion. Sample-dependent scaling, like division by a multiple of the standard deviation, will slightly complicate application of the model to new data samples. It also means that coefficients are expressed in terms of standard-deviation units rather than in the original predictor units. I prefer seeing results in units, say, of "yield change per degree of temperature" rather than "yield change per standard deviation of whatever temperature data happened to be used for building the model." That said, such results can all be re-expressed however you want to see them. Some analyses run into numerical problems with data on widely different scales when things like exponentiations are involved. In analyses like Cox survival models where this occurs, the software typically knows to do that internally, then re-transform to original scales. So, again, there's no need for you to pre-transform unless you are writing your own code for such an analysis. The possible exception: scaling is important if any of your models uses a penalized approach like ridge regression or LASSO, where predictors are assumed to be on comparable scales so that their coefficients are penalized fairly. Then, however, you have to think very hard about how you want to deal with binary or multi-level categorical predictors, as explained on this page . A final warning: you seem to be engaged in some type of automated model selection. That's dangerous . At least, document the reliability of your automated approach by repeating it on multiple bootstrapped samples of your data and applying the models to the full data set, as in the optimism bootstrap.
