[site]: crossvalidated
[post_id]: 308239
[parent_id]: 308232
[tags]: 
There's always a lurking subtlety in treatments of bias and variance, and it's important to pay careful attention to it when studying. If you re-read the first few words of ESL in a section from that chapter, the authors to pay it some respect. Discussions of error rate estimation can be confusing, because we have to make clear which quantities are fixed and which are random The subtlety is what is fixed, and what is random . In traditional treatments of linear regression, the data $X$ is treated as fixed and known. If you follow the arguments in ESL, you will find that the authors are also making this assumption. Under these assumptions, your example does not come into play, as the only remaining source of randomness in from the conditional distribution of $y$ given $X$. If it helps, you may want to replace the notation $Err(x_0)$ in your mind with $Err(x_0 \mid X)$. That is not to say that your concern is invalid, it is certainly true that the selection of training data does indeed introduce randomness in our model algorithm, and a diligent practitioner will attempt to quantify the effect of this randomness on their outcomes. In fact, you can see quite clearly that the common practices of bootstrapping and cross-validation explicitly incorporate these sources of randomness into their inferences. To derive an explicit mathematical expression for the bias and variance of a linear model in the context of a random training data set, one would need to make some assumptions about the structure of the randomness in the $X$ data. This would involve some suppositions on the distribution of $X$. This can be done, but has not become part of the mainstream expositions of these ideas.
