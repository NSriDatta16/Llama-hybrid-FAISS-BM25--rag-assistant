[site]: crossvalidated
[post_id]: 533942
[parent_id]: 
[tags]: 
Overfitting in text classification task with word2vec

I'm trying to complete a text classification task with word2vec, the steps I took are: preprocess the text in my dataset; split the dataset into training set(70%) and test set(30%); train wrod2vec model with the text in the training set; transfer all text(both in training and test set) into word2vec embeddings; approach Naive Bayes, Logistic regression, and random forest to do the classification. RandomizedSearchCV was used to search for the optimal parameters. use learning curves(use the data from the training set) to detect if the classifiers overfit or not. The accuracy of all classifiers is similar, approx. 73%. However, all the learning curves of the classifiers showed that they were overfitted. Below I give the learning curves: Could anyone explain the situation? If there are some solutions for my problems? Thanks in advance!
