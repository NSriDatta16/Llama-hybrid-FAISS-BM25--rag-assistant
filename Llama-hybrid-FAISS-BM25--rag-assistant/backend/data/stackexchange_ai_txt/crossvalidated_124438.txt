[site]: crossvalidated
[post_id]: 124438
[parent_id]: 
[tags]: 
Maximum likelihood estimator for minimum of exponential distributions

I am stuck on how to solve this problem. So, we have two sequences of random variables, $X_i$ and $Y_i$ for $i=1,...,n$. Now, $X$ and $Y$ are independent exponential distributions with parameters $\lambda$ and $\mu$. However, instead of observing $X$ and $Y$, we observe instead $Z$ and $W$. $Z=\min(X_i,Y_i)$ and $W=1$ if $Z_i=X_i$ and 0 if $Z_i=Y_i$. I have to find closed-forms for the maximum likelihood estimators of $\lambda$ and $\mu$ on the basis of $Z$ and $W$. Further, we need to show that these are global maxima. Now, I know that the minimum of two independent exponentials is itself exponential, with the rate equal to the sum of rates, so we know that $Z$ is exponential with parameter $\lambda+\mu$. Thus our maximum likelihood estimator is: $\hat{\lambda}+\hat{\mu}=\bar{Z}$. But I'm stuck with where to go from here. I know that $W$ is a Bernoulli distribution with parameter $p=P(Z_i=X_i)$, but I don't know how to go about converting this into a statement about one of the parameters. For example, what would the MLE $\bar{W}$ be estimating in terms of $\lambda$ and/or $\mu$? I understand that if $Z_i=X_i$, then $\mu=0$, but I'm having a hard time figuring out how to come up with any algebraic statement, here. UPDATE 1: So I have been told in the comments to derive the likelihood for the joint distribution of $Z$ and $W$. So $f(Z,W)=f(Z|W=1)\cdot p+f(Z|W=0)\cdot (1-p)$ where $p=P(Z_i=X_i)$. Correct? I don't know how else to derive a joint distribution in this case, since $Z$ and $W$ are not independent. So this gives us, $f(Z_i,W_i)=p\lambda e^{-\lambda z_i}+(1-p)\mu e^{-\mu z_i}$, by the definition of $W$ above. But now what? This doesn't get me anywhere. If I go through the steps of calculating the likelihood, I get: (using $m$ and $n$ as the sample sizes for each part of the mixture...) $L(\lambda,\mu)=p^m\lambda^m e^{-\lambda \sum{z_i}}+(1-p)^n\mu^n e^{-\mu \sum{z_i}}$ $\log L=m\log p+m\log\lambda-\lambda \sum{z_i}+n\log(1-p)+n\log\mu-\mu \sum{z_i}$ If I take the partial derivatives, this tells me that my MLE estimates for $\lambda$ and $\mu$ are just the average of the $Z$'s conditional on $W$. That is, $\hat{\lambda}=\frac{\sum{Z_i}}{m}$ $\hat{\mu}=\frac{\sum{Z_i}}{n}$ and $\hat{p}=\frac{m}{n+m}$
