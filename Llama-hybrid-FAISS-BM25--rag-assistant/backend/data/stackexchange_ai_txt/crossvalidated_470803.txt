[site]: crossvalidated
[post_id]: 470803
[parent_id]: 470797
[tags]: 
So what you have done is done cross-validation to find out the best hyper parameter for the data. What CV does is to predict the model on the fold not used in training, and this step is supposed to reduce overfitting. If you choose a hyper parameter that follows the training data too closely, it will perform badly on the test fold, making it less likely to be chosen. If you look under model_listworks$rfmodel you will see what is the mtry chosen. In this part of the code: probsrf2 = predict(model_listworks$rfmodel, train, type = "prob") You are essentially take the model that is trained on all the training data, and predicting it again on the same data, hence you get a very high AUC, because it's the same data used in the training. The training CV AUCs will be under model_listworks, like you have done, model_listworks$rfmodel for random forest. These are basically the AUC calculated using the test fold.
