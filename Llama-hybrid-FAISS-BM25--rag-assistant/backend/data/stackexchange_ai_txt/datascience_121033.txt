[site]: datascience
[post_id]: 121033
[parent_id]: 
[tags]: 
How to perform classification on time-series data in real-time, at periodic intervals?

I have a multivariate dataset for binary classification. Each series in the dataset has 1000 rows with eight features. The following is a description of a sample series in the dataset. The values in the table below are random and do not represent the actual data. |----------------|----------|----------|----------| ... |----------| | Time (seconds) | Feature1 | Feature2 | Feature3 | ... | Feature8 | |----------------|----------|----------|----------| ... |----------| | 1 | 100 | 157 | 321 | ... | 452 | |----------------|----------|----------|----------| ... |----------| | 2 | 97 | 123 | 323 | ... | 497 | |----------------|----------|----------|----------| ... |----------| | ... | ... | ... | ... | ... | ... | |----------------|----------|----------|----------| ... |----------| | 1000 | 234 | 391 | 46 | ... | 516 | |----------------|----------|----------|----------| ... |----------| We can consider each row to be logged every second. The training dataset is completely available offline. At the time of deployment, the data will appear in real-time, i.e., after the first second, only one row will be available, two rows will be available after two seconds, and so on. Typically, time series models provide the classification output at the end of the entire sample time series. However, I want to generate classification outputs online and periodically. For example, a classification output at every n seconds. I checked multiple relevant blogs, and it seems a sliding-window based LSTM model may fit the purpose. However, there are concerns about overfitting with such a model, as discussed here: Sliding window leads to overfitting in LSTM? Since my training examples are also limited, I am looking for alternative solutions. For instance, currently, I have about 100 training examples with 1000 x 8 rows each, 50 in each class. What are some other approaches to solving the problem?
