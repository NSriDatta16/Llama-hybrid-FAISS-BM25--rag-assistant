[site]: crossvalidated
[post_id]: 319536
[parent_id]: 319535
[tags]: 
Yes this does have a name: .... wait for it .... Perceptron. Jokes aside, let me explain a little further. When you throw away the non-linearities (like sigmoid, ReLu, ...) in neural networks (as you said) the output of each neuron is just the dot product of the input at the previous layer. Thus you will only be able to express linear problems, and the neural network will no longer be a "universal function approximator". In fact you can show that each multi-layer perceptron without non-linearities, can be reduced to a single-layer preceptron. If you still want to implement it you can do this in any deep learning library (e.g., Keras, Tensorflow, Theano, ...).
