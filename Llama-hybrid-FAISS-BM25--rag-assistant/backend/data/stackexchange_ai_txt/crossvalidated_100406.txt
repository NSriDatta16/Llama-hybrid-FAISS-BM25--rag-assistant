[site]: crossvalidated
[post_id]: 100406
[parent_id]: 100358
[tags]: 
Sure enough, you have "perfect" prediction with the interaction term; subset(my.data,A==1&B=='no') yields all 1 s for X . The Bayesian alternative you've already chosen is one way to go in handling this. As Avitus and Scortchi have suggested, Firth's (1993) method of penalizing the model to reduce bias is another. Here's how that performs by default (had to convert the data back to numeric to get it to run): require(logistf);summary(logistf(X~A*B,lapply(my.data,as.numeric))) Model fitted by Penalized ML; Confidence intervals and p-values by Profile Likelihood coef se(coef) lower 0.95 upper 0.95 Chisq p (Intercept) 8.7500000 13.253712 -14.45312 31.640625 10.176059 0.00142276 A -2.5489062 6.982541 -14.16579 13.233453 6.302140 0.01205923 B -2.3105651 6.984261 -13.93029 14.597185 4.436414 0.03518007 A:B 0.7167941 3.728474 -7.71979 6.651535 1.552630 0.21274756 Likelihood ratio test=18.18618 on 3 df, p=0.0004026211, n=49 Wald test = 1.084365 on 3 df, p = 0.7808497 Scortchi's answer suggests the hlr package offers yet another option (among others I won't review here), but I haven't been able to make it work for these data... Reference Firth, D. (1993). Bias reduction of maximum likelihood estimates. Biometrika, 80 (1), 27â€“38. Retrieved from http://www.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf .
