[site]: datascience
[post_id]: 39657
[parent_id]: 39643
[tags]: 
Let us look at the neural network as a graph (calculation graph). Each calculation node, forwards or backwards, costs the same (correct me if I'm wrong on that point). The neurons are the nodes, and the inputs/outputs to/from each neuron are the vertices. Calculating f(theta) (or f(theta,… theta+epsilon,… theta)) takes a single pass over all the graph. O(v+e), using forward propagation. Calculating backpropagation also takes a single pass over all the graph O(v+e). Thus, calculating derivatives using backpropagation, will cost 2*O(v+e), one for forward and one for back propagation. Also, thus, Calculating derivatives for each variable using the approximation will cost at least one pass over the graph (two for the two sided formula of the derivative), meaning O(v+e) per variable, costing a total of N*O(v+e) for all the variables. When N is large, backpropagation becomes much faster.
