[site]: crossvalidated
[post_id]: 115306
[parent_id]: 115246
[tags]: 
Yes, they are orthogonal. To see that your last expression equals zero, write it in the vector notation: $$\sum_{i,j=1}^n \alpha_i \beta_j K_{ij} = \boldsymbol \alpha^\top \boldsymbol K \boldsymbol \beta=0,$$ because $\boldsymbol \alpha$ and $\boldsymbol \beta$ are two different eigenvectors of $\boldsymbol K$. It is a standard linear algebra result: $$\boldsymbol K \boldsymbol \alpha = \lambda_1 \boldsymbol \alpha,\, \boldsymbol K\boldsymbol \beta = \lambda_2 \boldsymbol \beta \\ \Rightarrow \boldsymbol \beta^\top \boldsymbol K\boldsymbol \alpha = \boldsymbol \beta^\top \lambda_1 \boldsymbol \alpha = (\boldsymbol \beta^\top \lambda_1 \boldsymbol \alpha)^\top = \boldsymbol \alpha^\top \lambda_1 \boldsymbol \beta = \frac{\lambda_1}{\lambda_2}\boldsymbol \alpha^\top \lambda_2 \boldsymbol \beta = \frac{\lambda_1}{\lambda_2}\boldsymbol \alpha^\top \boldsymbol K\boldsymbol \beta = \frac{\lambda_1}{\lambda_2}(\boldsymbol \alpha^\top \boldsymbol K\boldsymbol \beta)^\top = \frac{\lambda_1}{\lambda_2}\boldsymbol \beta^\top \boldsymbol K\boldsymbol \alpha,$$ so if $\lambda_1 \ne \lambda_2$ then $\boldsymbol \beta^\top \boldsymbol K\boldsymbol \alpha=\boldsymbol \alpha^\top \boldsymbol K\boldsymbol \beta=0$. Note, however, that covariance matrix in the target space often cannot even be defined, because target space can be infinite-dimensional (this is the case e.g. with Gaussian kernel). Meaning that your $\mathbf a$ and $\mathbf b$ can be infinite-dimensional, making the above computations somewhat sloppy... What I think is more important, is that $\boldsymbol \alpha$ and $\boldsymbol \beta$ are orthogonal -- this means that kernel PCs have zero correlation. See my answer here for more details: Is Kernel PCA with linear kernel equivalent to standard PCA?
