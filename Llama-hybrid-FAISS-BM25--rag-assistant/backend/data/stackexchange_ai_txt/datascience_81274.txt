[site]: datascience
[post_id]: 81274
[parent_id]: 
[tags]: 
Multiclass Classification and log_loss

I hope I can make this clear with few lines of code/explanation. I've a 16K list of texts, labelled over 30 different classes that were ran through different classifiers; my Prediction and the Ground truth match on average at 94%. I am now after measuring something extra (not sure what should I measure on top of an F1_score minimum as I'm still learning, though) and I came across the log_loss from sklearn, which result I understand it range between 0 and 1. When ran against my prediction, however, the results is 1.48xxx, which is in fact higher. In trying to understand what was wrong. I have explored the result of ComplementNB.predict_proba that is required for the log_loss, and the value matches the one of my prediction array. Below some code: from sklearn.metrics import log_loss y = ... # This is my array of value that is my source of truth labels = numpy.unique(y) label_ary = [idx for gt in y for idx, lbl in enumerate(labels) if gt == lbl] print(f'The log loss is {log_loss(label_ary, clf.predict_proba(X.toarray()))}') Whether I use label_ary or y, in both the circumstance I am obtaining the same value, meaning that some conversion inside the log_loss is already happening. I'm not sure whether it me misinterpreting the results, or the specific of the function. What am I doing wrong? Thanks
