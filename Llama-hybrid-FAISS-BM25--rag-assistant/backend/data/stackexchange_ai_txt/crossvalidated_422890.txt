[site]: crossvalidated
[post_id]: 422890
[parent_id]: 
[tags]: 
Why do we use masking for padding in the Transformer's encoder?

I'm currently trying to implement a PyTorch version of the Transformer and had a question. I've noticed that many implementations apply a mask not just to the decoder but also to the encoder. The official TensorFlow tutorial for the Transformer also states that the Transformer uses something called "MultiHead Attention (with padding masking)." I'm just confused, why are masks applied to the padding in the encoder sequence?
