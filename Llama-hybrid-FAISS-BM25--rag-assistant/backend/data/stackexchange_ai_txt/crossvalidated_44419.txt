[site]: crossvalidated
[post_id]: 44419
[parent_id]: 44418
[tags]: 
I would try using EM. Suppose that your exponential distribution is X, and your gamma distribution is Y. We will learn their distributions in a Bayesian way since both are exponential families.) Now, for each training example $z$, we will find $x$ and $y$ such that $z=x + y$ where we assume that $x$ was drawn from $X$ and $y$ was drawn from $Y$. Then, we will update the parameters in a Bayesian way. To find $x$ and $y$, we run a simple bounded minimizer, and minimize the log-likelihood of $P(X=x, Y=y)$. (Ideally, we would use the posterior predictive distribution, but it's probably best to get this working first.) To update the parameters, we let the mean of $X$ be $E(x)$, the mean of $Y$ is $E(y)$, and $E(\log(Y)) = E(\log(y))$. This is the standard Bayesian update. You will probably want a different parametrization of the Gamma distribution, but that is a different question I think. I've had pretty good results with this kind of approach. Convergence can be made faster by throwing out some of the old data. (Above, the calculation of expectations relies on totals and counts â€” throwing out data is scaling these quantities.)
