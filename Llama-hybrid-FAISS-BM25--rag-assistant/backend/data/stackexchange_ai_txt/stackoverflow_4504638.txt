[site]: stackoverflow
[post_id]: 4504638
[parent_id]: 4503181
[tags]: 
Had to make this an answer as it's simply too long for a comment. I'm basically working in this field right now so I feel I have some knowledge. Obviously from my standpoint I'd recommend working with audio rather than images. I also recommend using MFCCs as your feature extraction (which you can think of as coefficients which summarise/characterise specific sub-bands of audio frequency [because they are]). GMMs are the go. To perform this task you must have some (preferably a lot) of labelled/known data, otherwise there is no basis for the machine learning to take place. A technicality which you may find useful: 'Then, during testing, you submit a query MFCC vector to the GMM, and it will tell you which species it thinks it is.' More accurately, you submit a query to each GMM (which if you're using them correctly, each gives you a likelihood score [probability] of that particular feature vector being emitted by that probability distribution). Then you compare all the likelihood scores you receive from all the GMMs and classify based on the highest you receive. UBMs Rather than "filtering out" noise, you can simply model all background noise/channel distortion with a UBM (Universal Background Model). This model consists of a GMM trained using all the training data available to you (that is, all the training data you used for each class). You can use this to get a 'likelihood ratio' ( Pr [x would be emitted by specific model] / Pr [x would be emitted by background model (UBM)]) to help remove any biasing that can be explained by the background model itself.
