[site]: datascience
[post_id]: 120937
[parent_id]: 
[tags]: 
Dealing with rich vocabulary and a low average frequency of words in NLP

What is the best way to deal with a dataset that has a rich vocabulary and a low average frequency of words that is showing low validation accuracy? While reading online I saw many people recommending removing stop-words and stemming while others suggested using a TF-IDF vectorizer. Which solution will have the greater impact?
