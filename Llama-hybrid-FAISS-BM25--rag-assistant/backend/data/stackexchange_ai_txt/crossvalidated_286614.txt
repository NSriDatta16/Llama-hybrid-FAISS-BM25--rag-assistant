[site]: crossvalidated
[post_id]: 286614
[parent_id]: 
[tags]: 
Expected value of logarithm of distribution

I'm studying variational inference and I'm checking an example from Bishop, Pattern recognition and machine learning (2006). In the page 470 (10.1.3) there is an example with the univariate Gaussian. There is a dataset $ \mathcal{D} = \{x_1, \ldots, x_N \} $ of observed values which are drawn from a Gaussian (independently) so the likelihood is: $$ p(\mathcal{D}\mid \mu,\tau)= \left( \frac{\tau}{2\pi}\right)^{(N/2)} \exp \left\{-\frac{\tau}{2} \sum_{n=1}^N{(x_n - \mu)^2)} \right\}$$ and the priors are: $$ p(\mu\mid\tau) = \mathcal{N}(\mu \mid \mu_0 \mid (\lambda_0\tau)^{-1}) $$ $$ p(\tau) = \operatorname{Gam}(\tau \mid a_0, b_0)$$ we try to aproximate the posterior distribution using a factorized variational aproximation $ q(\mu,\tau) = q_\mu(\mu)q_\tau(\tau)$ The optimum factors can be obtained from the general result of VI (maximizing the lower bound) so we have for the (log of) optimum factor $ \ln q_\mu^*(\mu) = \DeclareMathOperator{\E}{\mathbb{E}} \E_\tau[ \ln p(\mathcal{D}\mid \mu,\tau) + \ln p(\mu\mid\tau)] + \text{const} \tag{1}$ $$ \ln q_\mu^*(\mu) = -\frac{\E[\tau]}{2} \{\lambda_0(\mu-\mu_0)^2 + \sum_{n=1}^N{(x_n - \mu)^2)}\} + \text{const} \tag{2}$$ Then we can see that that is a Gaussian $ \mathcal{N}(\mu \mid \mu_N \mid \lambda_N^-1) $ with $$ \mu_N= \frac{\lambda_0\mu_0+N\overline{x}}{\lambda_0 + N}$$ $$\lambda_N = (\lambda_0 + N) \E[\tau]$$ My question is how do I go from $(1)$ to $(2)$ should I just apply the definition of Expected value? but what if is not the distribution but the logarithm of the distribution.
