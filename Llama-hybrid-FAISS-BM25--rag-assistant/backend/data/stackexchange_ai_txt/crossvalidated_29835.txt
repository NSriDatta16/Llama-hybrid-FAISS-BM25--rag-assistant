[site]: crossvalidated
[post_id]: 29835
[parent_id]: 
[tags]: 
Frequentism and priors

Robby McKilliam says in a comment to this post: It should be pointed out that, from the frequentists point of view, there is no reason that you can't incorporate the prior knowledge into the model. In this sense, the frequentist view is simpler, you only have a model and some data. There is no need to separate the prior information from the model Also, here , @jbowman says that frequentists use regularization by a cost/penalty function, while bayesians can make this a prior: Frequentists realized that regularization was good, and use it quite commonly these days - and Bayesian priors can be easily interpreted as regularization. So, my question is, can frequentists in general incorporate into their models what Bayesians specify as priors? Taking the regularization as an example, is the cost/penalty function really integrated into the model, or is this a purely artificial means of adjusting the solution (as well as making it unique)?
