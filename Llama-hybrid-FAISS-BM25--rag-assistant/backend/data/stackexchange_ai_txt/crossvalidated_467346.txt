[site]: crossvalidated
[post_id]: 467346
[parent_id]: 
[tags]: 
PPO: Reinforcement learning algorithm with multiple, seperate outputs

I'm implementing the PPO algorithm on a problem where initially I only had a single discrete action. For this I have a shared neural network base, with an output head for the discrete output and an output head for the value prediction. Now one of the actions I can take is parameterisable with a continuous argument. I'm planning to simply add another output head to the network with a normal distribution that has a learned mu and sigma, as in other PPO implementations I've seen. My question is how to deal with the network parameter update steps now that I have two components to each action? A discrete action from a categorical distribution, and a continuous action from a normal distribution. For example, I'm not sure how to take the ratio of new action logprobs to old action logprobs. Should I calculate this once for the discrete output and once for the categorical output, and then average them? Similarly I calculate the entropy of the categorical distribution and subtract this from my loss. Should I calculate the entropies of both distributions, average them, and subtract the average from the loss instead?
