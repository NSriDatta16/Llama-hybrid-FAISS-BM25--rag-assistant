[site]: stackoverflow
[post_id]: 1859094
[parent_id]: 1859078
[tags]: 
It depends on how high-level your development is. If you develop for embedded systems, this means a lot of hardware knowledge (close to a EE level). If you are in some specialized area, like low-level 3D graphics programming for games, you should know ins and outs of specific graphics cards. If you are doing web or desktop applications, probably not so much. But in all development, you probably should know the fundamentals. E.g., Where are the bottlenecks in von-Neumann architecture. How does CPU cache works (important in multi-threading). How OS scheduling will be different on single-processor multi-processor CPUs (again, important in multi-threading). The way IO works and why when you write to a file does not necessarily mean you data is immediately persistent. How slow is IO and why most database applications are IO-bound. Why network is even slower and less reliable than IO (and wireless network even more so). On the other hand, I don't think knowing the specifics like what memory-mapped IO is, or knowing the difference between NAND and NOR flash is really important for an average* (desktop/web) developer. Even the knowledge of the architecture of a modern CPU is probably going to be a science in itself seeing how complex they've got in the past years, not to mention that the code emitted by modern compilers is becoming more difficult to predict (the article linked shows that it's harder now to outsmart the compiler at low-level optimizations). It's similar to being a car mechanic was easier a few decades ago, today not that many people will try to fix their cars by themselves. **Definition of "average" may vary.*
