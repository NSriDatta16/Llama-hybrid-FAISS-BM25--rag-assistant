[site]: crossvalidated
[post_id]: 416419
[parent_id]: 
[tags]: 
Use of Hyper-Parameter Tuning in Deep Learning in Practice

[While this question on Cross-Validated might look similar to my question, I am asking something different.] I have read several books and dozens of blogs on Deep Learning (DL) but it is very rare to see hyper-parameter optimization (HPO) incorporated into DL models, which is quite unlike classical algorithms, such as random forests, which are deemed incomplete without HPO. Is it because the values of the parameters are more intuitive (or have rules of thumb etc) in DL, making HPO redundant? Or is it because of the excessive computational cost of HPO process in DL that everyone skips it? Is it common to do HPO with DL in production code in the industry?
