[site]: crossvalidated
[post_id]: 258033
[parent_id]: 
[tags]: 
I cannot differentiate my loss function, what is the best method for optimizing the weights in my neural network?

Suppose the output of my network is $y \in [0, 1]$ for a given input x. The loss function to be minimized is $f(x) = -\Sigma (w_i y_i + | y_i - y_{i-1}|)$, with real weights $w_i$. The dependency of one output to the other means I can't calculate derivatives and plug this into a backpropagation algorithm. (I've tried and failed.) I see two questions: Have I given up on calculus too early? Is it possible to differentiate $f(x)$ for use in backpropagation? If not, what efficient methods can I use to find an optimal solution? Right now I randomly tinker with weights and biases, and accept them if I find a new minimum.
