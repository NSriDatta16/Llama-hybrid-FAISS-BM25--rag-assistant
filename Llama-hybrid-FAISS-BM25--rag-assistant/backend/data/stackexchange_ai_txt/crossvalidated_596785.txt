[site]: crossvalidated
[post_id]: 596785
[parent_id]: 596783
[tags]: 
Class imbalance isn’t much of a problem , and many of the apparent issues come from using a surprisingly poor “accuracy” metric . Depending on how much you care about this class and topic, you might want to press your instructor on why class imbalance is an issue at all. Moreover, SMOTE does not seem to be very good at what it aims to do , even. So should you use SMOTE when you run a logistic regression? Probably not, but that’s really because you probably shouldn’t use it for any kind of machine learning. I would not consider logistic regression special, however, in using SMOTE, and I would expect the full-credit answer on your exam to be that SMOTE is totally compatible with logistic regression (even if I only consider this the full-credit answer and not the correct answer). You also mention undersampling. Since class imbalance isn’t really a problem, there is no need to discard precious data to fix a non-problem. (Yes, Dikran Marsupial gives an interesting example of when class imbalance really is a problem. That’s quite different from what most in machine learning mean when they talk about class imbalance being a problem. I might even argue that example to be a matter of experimental design rather than of model evaluation, the latter of which is where practitioners like your instructor seem to be under the impression that class imbalance poses problems.) EDIT The bounty message mentions wanting a reputable source. Frank Harrell’s Regression Modeling Strategies textbook gets into this and has many references to the primary literature. Further, he has (at least) two blog posts on the topic: (1) (2) . EDIT 2 I contest the example your professor gave about a naïve classifier with an accuracy of $95\%$ is giving a good accuracy score. Where I think people get in trouble with this is interpreting accuracy as being akin to $R^2$ in regression, since both are proportions (and that isn’t even totally true for $R^2$ ). Their logic seems to be that, since $R^2 = 0.95$ probably would be considered strong performance, accuracy of $95\%=0.95$ should be considered impressive, too. The trouble is that all $R^2$ and accuracy have in common is that they are (sort of) proportions. A reasonable view of $R^2$ is that it is a comparison of the square loss of the model and the square loss of a “must-beat” naïve model . A reasonable counterpart for $R^2$ in terms of classification accuracy could be a comparison of model error rate to the error rate of a model that naively guesses the majority class every time. Subtracting from one the ratio of these two error rates then would be analogous to $R^2$ , and the performance of the model given by your professor would be zero, correctly highlighting the fact that the model basically does nothing, despite an accuracy score of $95\%$ that (misleadingly) looks like a high $\text{A}$ in school that makes us happy. Why such a metric is not more widespread is a mystery to me. Indeed, even a good UCLA page gives this metric (granted, in a different (but equivalent) form as something like $R^2$ for a classification problem, yet I rarely see it discussed.
