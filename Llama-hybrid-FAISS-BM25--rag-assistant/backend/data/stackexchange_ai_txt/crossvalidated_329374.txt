[site]: crossvalidated
[post_id]: 329374
[parent_id]: 329370
[tags]: 
I am interpreting your question as backpropagation just referring to a method of computing derivatives. Some people also interpret it to include the actual gradient descent algorithm using the gradient thus computed. @Sid 's answer focuses just on the gradient descent algorithm, and does not address backpropagation as a method for computing derivatives. My answer does. Backpropagation is the reverse mode (a.k.a. reverse accumulation, a.k.a. reverse pass) of automatic differentiation (a.k.a algorithmic differentiation) applied to neural networks. The reverse mode of automatic differentiation is commonly applied to compute the gradient of objective function, among other things, in nonlinear optimization. This can support steepest descent (gradient descent), conjugate gradient or Quasi-Newton methods, among others. If followed by forward pass automatic differentiation of the gradient (this is called forward over reverse), it can be used to compute a Hessian or Hessian-vector products for use in a Newton method. The forward mode can be used to compute the Jacobian of constraints for nonlinear optimization. There are whole books on (reverse mode of) automatic differentiation. Here are a few links: https://en.wikipedia.org/wiki/Automatic_differentiation https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation Step-by-step example of reverse-mode automatic differentiation http://www.cs.cmu.edu/~wcohen/10-605/notes/autodiff.pdf There are also many libraries and software tools to support use of automatic differentiation.
