[site]: crossvalidated
[post_id]: 364773
[parent_id]: 154798
[tags]: 
The existing answers are excellent and comprehensively answer the question. Just want to add that filters in Convolutional networks are shared across the entire image (i.e., the input is convolved with the filter, as visualized in Franck's answer). The receptive field of a particular neuron are all input units that affect the neuron in question. The receptive field of a neuron in a Convolutional network is generally smaller than the receptive field of a neuron in a Dense network courtesy of shared filters(also called parameter sharing ). Parameter sharing confers a certain benefit on CNNs, namely a property termed equivariance to translation . This is to say that if the input is perturbed or translated, the output is also modified in the same manner. Ian Goodfellow provides a great example in the Deep Learning Book regarding how practitioners can capitalize on equivariance in CNNs: When processing time-series data, this means that convolution produces a sort of timeline that shows when diﬀerent features appear in the input.If we move an event later in time in the input, the exact same representation of it will appear in the output, just later. Similarly with images, convolution creates a 2-D map of where certain features appear in the input. If we move the object in the input, its representation will move the same amount in the output. This is useful for when we know that some function of a small number of neighboring pixels is useful when applied to multiple input locations. For example, when processing images, it is useful to detect edges in the ﬁrst layer of a convolutional network. The same edges appear more or less everywhere in the image, so it is practical to share parameters across the entire image.
