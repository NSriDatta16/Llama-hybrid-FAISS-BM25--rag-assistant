[site]: crossvalidated
[post_id]: 554004
[parent_id]: 553995
[tags]: 
Dataset requests are off-topic, but there is an interesting statistical take. I will simulate some data in R. set.seed(2021) N If you just run linear regression on “x”, your performance will be awful. However, if you run a complex algorithm like a neural network, perhaps with more data, you can get a decent fit to the parabolic shape. If, however, you know to expect the parabolic relationship, the regression on “x^2” has excellent performance. Some of the appeal of machine learning is that we can delegate this “figure out the features” to algorithms, rather than doing it by hand. The universal approximation theorem in neural networks says that, for “decent” functions and with some technical considerations, single-layer neural networks can approximate anything. If you visualize the architecture of such a neural network, you will see that the final output is a linear combination of the hidden layer (plus an activation function). That is, a neural network does feature extraction and then applies a linear regression (or generalized linear model). If you’re able to figure out the features in that hidden layer the way that you could have figured out that my simulation included a quadratic term, your linear regression (or other GLM, such as logistic) will do just as well as the neural network. Some of the trouble with finding a dataset where a (generalized) linear model does not work at all is that there is likely to be some linear component.
