[site]: crossvalidated
[post_id]: 240033
[parent_id]: 240019
[tags]: 
A critical distinction is whether you want to: [Most common case]: Construct an estimate of performance on new subjects (drawn from the same population as your data). Construct an estimate of performance on new observations from the same subjects as in your sample. The far more common case is case number (1). Eg., how well do you predict heart attacks for someone that's coming into the emergency room? And if you're in case (1), you almost certainly should do (a) subject-wise cross-validation rather than (b) record-wise cross-validation. Doing record-wise validation in case (1) likely will lead to unreasonably high, bogus estimates of performance on new subjects. I don't precisely understand what you're trying to do (and perhaps it is self-study so the question isn't entirely realistic). I don't know what case you're in. If you're in the much less common case (2), record wise validation may be ok. A general theme in statistics is to think carefully about what's independent and what's correlated. Generally speaking, an independent observation tends to be a different subject. If you want to predict performance on new subjects, you must test on subjects you didn't train on! Why subject-wise cross validation rather than record-wise? In typical settings, repeated observations of the same individual are correlated with each other even after conditioning on features. Hence with record-wise cross-validation, your test set isn't independent of your training set! In the extreme case of perfect correlation, you'd have the exact same observations in the training set and the test set! You'd be training on the test set! The performance measured in cross-validation would not be predictive of performance on new subjects. For example, this recent paper calls record-wise cross-validation, ``Voodoo Machine Learning." What to do with so few subjects though... Perhaps some commenters more experienced with cross-validation than me could chime-in, but to me, this looks like a possible candidate for $k=n$ (aka leave out one cross-validation)? To maximize data for training, something you could do is leave out one subject for cross validation. Each iteration, test on a different hold-out subject and train on all the others. If the subjects are all very different, you may effectively have close to $n = 38$ and may want to put as many independent subjects in the training set.
