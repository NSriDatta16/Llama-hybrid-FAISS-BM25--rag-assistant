[site]: datascience
[post_id]: 37508
[parent_id]: 
[tags]: 
Huge performance discrepancies at each run, with the same CNN architecture

I have set up a CNN architecture with 3 Convolutional layers with pooling (except the last one), a fully connected layer and a logistic regression layer. All the layers, except the last one, have a ReLU activation function to train on the MNIST dataset. I have followed LeNet tutorial When I train my model, I have some weird behaviours. One time out of five (approximately), my network will early-stop at the second epoch with an error of 89% (so, quite bad) and sometimes, the same network will yield a test performance of ~1.3%, which is great in my opinion. This is totally random and this is weird. I thought of two reasons on why this might happen : Weight initializations might not be right . However, I have double-checked the weights initialization for each layer type. While the LogReg layer initializes its weights at 0, the two other layers (FullyConnected & ConvPool) initialize theirs weights according to a normal distribution whose parameters are np.random.normal(loc = 0, scale = np.sqrt(1.0 / fan_out),size = filter_shape) There is an error in the SGD step . I double-checked my SGD step with the tutorial's, and I can't spot any discrepancies. Moreover, I don't understand why it would work sometimes, and be broken at others. Is there any other reasons I might have not considered?
