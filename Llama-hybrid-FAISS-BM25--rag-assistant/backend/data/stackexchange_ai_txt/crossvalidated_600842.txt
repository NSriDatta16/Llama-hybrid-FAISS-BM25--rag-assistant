[site]: crossvalidated
[post_id]: 600842
[parent_id]: 600592
[tags]: 
Case of simple hypotheses Yes the likelihood for independent events is multiplicative . $$\mathcal{L}(\theta ; x_1, x_2)= \mathcal{L}(\theta ; x_1) \cdot \mathcal{L}(\theta ; x_2)$$ And as a consequence the likelihood ratio is multiplicative as well. $$\frac{\mathcal{L}(\theta_1 ; x_1, x_2)}{\mathcal{L}(\theta_2 ; x_1, x_2)}= \frac{\mathcal{L}(\theta_1 ; x_1)}{\mathcal{L}(\theta_2 ; x_1)} \cdot \frac{\mathcal{L}(\theta_1 ; x_2)}{\mathcal{L}(\theta_2 ; x_2)}$$ In the case of log likelihood and log of the likelihood ratio, the multiplicative property changes into an additive property. $\log(ab) = \log(a) + \log(b)$ Be aware that the multiplication is only true for independent events. Multiple measurements are not always independent. For instance, the second measurement can be made as a consequence of the first measurements, and this leads to the stopping problem . Case of a composite alternative hypothesis 1 It might be that you are using the likelihood ratio test for testing a null hypothesis $H_0: \theta = \theta_0$ against an alternative composite hypothesis $H_a: \theta \neq \theta_0$ and you are using the likelihood ratio $$\lambda_x = \frac{\mathcal{L}(\theta_0;x)}{\mathcal{L}(\hat\theta_{ML};x)}$$ Here you can again multiply the likelihood ratio values (or sum the log values), but you need to consider that you computed two times parameters and the distribution of the ratio will be different. Typically the distribution of the logarithm of the likelihood ratio, multiplied with -2, is considered to be (approximately) chi-squared distributed with $\nu$ degrees of freedom (where $\nu$ is the number of free parameters that are used to fit) and now you will have the sum of two such values which will be chi-squared distributed with $2\nu$ degrees of freedom. Using the product of the likelihood ratios is similar to Fisher's method of combining p-values. (Which involves multiplication of the p-values and comparing with a chi-squared distribution) Case of a composite alternative hypothesis 2 In your explanation you have a composite hypothesis for both hypotheses. You can still multiply likelihood functions and ratios. You can consider the two different datasets as if it was a single dataset that you fit with twice as many parameters. A question is however what use your likelihood ratio has. Often you use it for a hypothesis test and you can know some distribution for the likelihood distribution given that null hypothesis is true. In your example, hypotheses of lognormal versus normal, it may not be easy to figure out this distribution. But, this is a problem that is unrelated to combining ratios and already occurs with a single dataset. and conclude that all the data would be 7.75 times more likely to occur with sampling from a normal distribution than with sampling from a lognormal distribution? This is not the interpretation of the likelihood ratio when it is computed with composite hypotheses. In the case of composite hypotheses there are nuisance parameters where you fit the cases that maximise the likelihood. Compare for instance with a Bayesian analysis where those parameters could have a prior distribution. In that case you would have a likelihood ratio that can be interpreted as the relative probabilities for the data to occur with the two different hypotheses. Case of information interpretation. In the comments a situation is described that seems like model selection and one might use the likelihood as part of the Akaike information criterion (AIC). In this case, AIC, it might make sense to add up the average log likelihood, the log likelihood divided by the number of observations. It is the average log likelihood that approaches the cross entropy and it might make more sense to add up the entropy rather than the likelihood (which differ by a constant, the number of observations). If the number of observations are different for the different datasets then using this way of combining the likelihoods will be of influence.
