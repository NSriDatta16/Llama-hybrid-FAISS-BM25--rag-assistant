[site]: crossvalidated
[post_id]: 346894
[parent_id]: 126546
[tags]: 
Your problem seems to be well suited for online learning. You can use stochastic or mini-batch gradient descent to train a neural network continuously over time. In stochastic gradient descent, you take one gradient descent step each time you get a new training example. On mini-batch, you do it each time you gather a batch of n training examples. Since your local minima changes over time, your neural net will continuously adapt its weights as newer data comes in, fitting the new local minimum. You can also play around with the step size. The larger it is, the more it will effectively weight more recent data (e.g. the easier to escape from the older local minimum) but could also become super unstable. Worth tuning as a hyperparameter and running backtests in time. You can watch this video by Andrew Ng for an example of online learning: https://www.youtube.com/watch?v=dnCzy_XKGbA
