[site]: datascience
[post_id]: 98345
[parent_id]: 98337
[tags]: 
Cross-validation is a method to obtain to obtain a reliable estimation of the performance. The performance is obtained as the average across the CV "folds" because this way it doesn't depend on a single test set, i.e. the impact of chance is minimized. In the case of hyper-parameter selection, the goal is not only to evaluate but also to select the hyper-parameters values based on this evaluation. This turns the CV process into a training stage, because it is used to determine something about the model. When the goal is to select the best hyper-parameters among a set of possible assignments of their values, the method is run across all the CV "folds" for every possible assignment, and then the average performance is also obtained for every assignment . At the end of the CV process the assignment which corresponds to the maximum average performance is selected. Now that the parameters are fixed, one still has to determine the true performance on a fresh test set because the high performance among the parameters assignments could be due to chance. This is why a model is trained again with these parameters (usually using the whole training data), then applied to a fresh test set to obtain the final performance. Notice that everything in CV is done the same way across the "folds": the same method(s) are run for every fold, and the results are always obtained across all "folds". In particular one should never select the best model or parameters by picking the maximum "fold".
