[site]: crossvalidated
[post_id]: 47571
[parent_id]: 47132
[tags]: 
The question, as asked, is impossible to answer correctly. It depends on the structure inside your data. Is there a distance measure on your data such that the average of the closest n-points almost always give a good estimate of the values? Is there a global structure to the data such as one that would be reflected in a linear or more complex model of the same type? Or are there a number of independent but possibly overlapping clusters in the data, each with their own structure? If each of your 20 features can realistically be considered independently for their effect, then learning an additive model might be perfect, but if certain combinations of features change the picture entirely then you need a different kind of learning algorithm. In order to determine the best supervised learning algorithm for your data you need someone who is skilled in using the different algorithms and is familiar with their characteristics to do some exploratory analysis on your data. BUT.... An adequate learning algorithm is a completely different question. Sight unseen, I can tell you that a suitably sized back-propagation neural network, without the non-linearity on the output of the top neuron, will probably do you just fine unless you are extraordinarily unfortunate and have a deceptive collection of data where an inferior solution has a wide basin of attraction. I can also tell you that random forests are robust and tolerant of many different kinds of structure, though not necessarily the most efficient approach.
