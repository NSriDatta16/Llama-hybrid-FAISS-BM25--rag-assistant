[site]: crossvalidated
[post_id]: 553422
[parent_id]: 
[tags]: 
Prior predictive distribution with an improper prior for a Poisson likelihood

I have recently started exploring some bayesian statistics and I cannot seem to understand something about improper priors. In particular, the set up consists of a Poisson likelihood $ p(X|\theta) = \frac{\theta^x e^{-\theta}}{x!}$ and some improper prior on $\theta$ , $p(\theta) = \frac{1}{\sqrt{\theta}}$ . Now to find the prior predictive distribution we use the following: $$p(X) = \int_{0}^{\infty}p(X|\theta)p(\theta)d\theta = \int_{0}^{\infty}\frac{\theta^{x-\frac{1}{2}} e^{-\theta}}{x!}d\theta = \frac{\Gamma{(x+\frac{1}{2})}}{x!}\int_{0}^{\infty}\frac{1}{\Gamma{(x+\frac{1}{2})}}\theta^{ (x+\frac{1}{2}) -1} e^{-\theta}d\theta = \frac{\Gamma{(x+\frac{1}{2})}}{x!}$$ . If $p(X)$ is a probability distribution it must equal to one when summed over its support, which does not happen here. Therefore, I wanted to ask whether that is happening because of the structure of the prior or if there is perhaps something else that I'm missing. I have seen some examples of discrete likelihoods with finite support where we can't use improper priors, which makes sense, however, I'm confused why we can use an improper prior in this case even though we get a posterior density in a recognisable form when at the same time the prior predictive does not sum up to 1.
