[site]: datascience
[post_id]: 57574
[parent_id]: 57570
[tags]: 
In general, you need to wrap the feature selection filter method within a cross validation scheme. Here is an example of such an algorithm, assuming you are also tuning hyperparameters. Split the data into training and testing. Split the training set only into k folds. For each iteration of hyperparameter tuning, use cross validation, i.e. do : For each of the k folds, for iterations i = 1 to k, do : 1) Let the validation set be fold i, and the "inner training" set be all folds not equal to fold i. 2) Using your filtering method, select some subset of your features using some cutoff p on the inner training set. For example, I might filter out variables that have absolute correlation with the target variable y less than p = 0.3. 3) Fit a model. 4) Predict the validation set = fold i. Score the model, i.e. evaluate your function you are trying to optimizer like log loss, mean squared/absolute error, Brier, etc. Save the score. end After you are done with your cross validation for one set of hyperparameters, take the mean score over all of the k folds. If the score is better than a previous set of hyperparameters found in a prior iteration, save these hyperparameters. Note: you can choose to have p, your filter cutoff, also be a hyperparameter here. For simplicity I kept it constant but in general you will probably do better if you tune the filtering cutoff as well. end Using the same (or best, if you let it be a hyperparameter and optimized over it in step 3) filter cutoff p, and the best hyperparameters that you found in the cross validation process in step 3, use the training set created in step 1, and fit your "final" model. That is, filter your training set with the cutoff value p using the exact same filtering method you did in step 3. Then, fit a model to this data set using the optimal hyper parameters, also found in step 3. Predict the test set using your fitted model in step 4. Score the model. Finally, repeat the entire process again but this time, don't do any filter selection method (omit the filtering process). Use the exact same seeds in your cross validation and at the initial split in step 1 to make sure you are splitting your data the same way from when you used filtering. Now, compare test set scores and see for yourself if there is any benefit to using your filtering method. In general, I am not a fan of filter based methods because they all introduce some arbitrary cutoff = p and also are all univariate (they don't take into consideration other variables in the model, just in isolation). That being said, they are fast and they might work for your problem. EDIT: Also, depending on how much data you have and how variable your validation scores are, multiple repeats of the entire algorithm above (steps 1 through 5) might be necessary in order to obtain valid comparisons between using filtering and not using filtering. You would repeat the algorithm above say, N times, once with filtering and once without for a total of 2N times. You would then calculate an average of averages (find the average score over N repeats using filtering, and find the average score over the same N repeats with no filtering). Then, you could compare scores, form confidence intervals, etc. for a more trustworthy (but also more time consuming) comparison. Hope this helps.
