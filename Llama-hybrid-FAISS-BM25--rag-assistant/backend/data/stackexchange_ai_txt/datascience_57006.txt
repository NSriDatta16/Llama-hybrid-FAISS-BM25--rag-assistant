[site]: datascience
[post_id]: 57006
[parent_id]: 57005
[tags]: 
The softmax function is used in the last layer of CNN network. Softmax is an activation function like tanh and ReLU, the difference is that this technique can interpret the incoming inputs as output probabilities. The method guarantees that the output probabilities will be in a range of 0 and 1, and the sum of them is 1, thus the scores are interpretable as a percentage rate for each class. The function uses this formula, also you can use these lines of code to compute softmax: logits = [2.0, 1.0, 0.1] exps = [np.exp(i) for i in logits] sum_of_exps = sum(exps) softmax = [j/sum_of_exps for j in exps] print(softmax)
