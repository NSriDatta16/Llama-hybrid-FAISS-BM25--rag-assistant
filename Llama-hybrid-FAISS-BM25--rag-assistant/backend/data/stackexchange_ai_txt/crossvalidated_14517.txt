[site]: crossvalidated
[post_id]: 14517
[parent_id]: 13078
[tags]: 
If you don't know the value of $N$, then we simply integrate it out, the same as any other nuisance parameter. We know that $N$ is discrete, and as long as your standard deviation is not zero, then you know that you have at least $2$ observations, so $N\geq 2$. So we write out the posterior as: $$p(\mu,\sigma|\overline{x},s^2,I)=\sum_{N=2}^{\infty}p(\mu,\sigma,N|\overline{x},s^2,I)$$ $$=\sum_{N=2}^{\infty}p(N|\overline{x},s^2,I)p(\mu,\sigma|N,\overline{x},s^2,I)=\frac{\sum_{N=2}^{\infty}p(N|\overline{x},s^2,I)p(\mu,\sigma|N,\overline{x},s^2,I)}{\sum_{N=2}^{\infty}p(N|\overline{x},s^2,I)}$$ The last form is written to show that we basically have a weighted average of $p(\mu,\sigma|N,\overline{x},s^2,I)$ for different $N$. This term is the standard posterior distribution you get in most cases, so I won't discuss it here. I will focus on the first term, which is the weights. Now we can re-arrange it using Bayes theorem to get: $$p(N|\overline{x},s^2,I)\propto p(N|I)p(\overline{x},s^2|N,I)$$ From @JMS comment, he seems to think that $p(\overline{x},s^2|N,I)$ does not depend on $N$, so that we are basically weighting by the prior $p(N|I)$. This is incorrect, unless we have very little prior information about $\mu$ and $\sigma$ . I will show this with a specific example, followed by my attempt at explaining why this is the case. Now we can further decompose this as: $$p(\overline{x},s^2|N,I)=\iint p(\overline{x},s^2,\mu,\sigma|N,I)\,d\mu\, d\sigma$$ $$=\iint p(\overline{x},s^2|\mu,\sigma,N,I)p(\mu,\sigma|N,I)\,d\mu\,d\sigma$$ $$=\iint p(\overline{x}|\mu,\sigma,N,I)p(s^2|\mu,\sigma,N,I)p(\mu,\sigma|N,I)\,d\mu\,d\sigma$$ The last line follows from the know fact that for normal with known mean and variance, sample mean is independent of sample standard deviation. We also have: $$p(\overline{x}|\mu,\sigma,N,I)=\sqrt{\frac{N}{2\pi\sigma^2}}\exp\left(-\frac{N}{2\sigma^{2}}[\overline{x}-\mu]^2\right)$$ $$\frac{Ns^2}{\sigma^2}\sim \chi^2_{N-1}\implies p(s^2|\mu,\sigma,N,I)=\frac{1}{\Gamma[\frac{N-1}{2}]}\left(\frac{Ns^2}{2}\right)^{(N-1)/2}\sigma^{-(N-1)}\exp\left(-\frac{Ns^2}{2\sigma^2}\right)$$ Now if I was to specialise to the prior $p(\mu,\sigma|N,I)\propto \sigma^{-\alpha-1}\exp\left(-\frac{\beta}{\sigma^2}\right)$ we get: $$p(\overline{x},s^2|N,I)\propto\frac{\Gamma[\frac{N-1}{2}+\alpha]}{\Gamma[\frac{N-1}{2}]}\left(\frac{Ns^2}{2}\right)^{(N-1)/2}\left(\frac{Ns^2}{2}+\beta\right)^{-(N-1+\alpha)/2}$$ Where factors independent of $N$ have been absorbed into the "$\propto$" symbol. Note that this still depends on $N$ unless we use the Jeffrey's prior $\alpha=\beta=0$. Hence, if you know something about the parameters of the normal distribution, you can use this information to estimate $N$ from $s^2$ and $\overline{x}$. Now where is the intuition behind this? I don't really know, but I'll have a go, as I was initially quite surprised myself at this result, but on a bit of reflection it does make sense. We know from the chi-square pivotal quantity, that we can roughly expect $Ns^2\approx (N-1)\sigma^2$. Now if we use an informative prior for $\sigma^2$ like I did then we have $\sigma^2\approx \frac{\alpha}{2\beta}$ (this is the prior mean for $\sigma^2$). But we can then use this to solve for $N$, and we get $$N\approx\left(1-\frac{2s^2\beta}{\alpha}\right)^{-1}=\left(1-\frac{s^2}{\hat{\sigma}^{2}_{PRIOR}}\right)^{-1}$$ Now why doesn't this work for the Jeffreys prior? Well, when we use the Jeffreys prior, the prior estimate is undefined so the above equation is arbitrary for Jeffreys prior. Additionally, we essentially estimate $\sigma^2\approx s^2$ with the Jeffreys prior and so we already have $Ns^2\approx (N-1)\sigma^2$ independently of the value of $N$ - hence we cannot use this to help us estimate $N$. You can see this also in the form of $p(\overline{x},s^2|N,I)$ as it essentially has the heuristic form: $$\frac{\text{normalisation with }\mu,\sigma\text{ estimated from N obs and prior information}}{\text{normalisation with }\mu,\sigma\text{ estimated from N obs}}$$ So if you are in some sort of sequential situation, you can get estimates for $N$ after the receiving the first mean and variance.
