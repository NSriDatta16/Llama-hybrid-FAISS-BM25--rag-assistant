[site]: crossvalidated
[post_id]: 498005
[parent_id]: 
[tags]: 
Is it a good idea to use a linear model (like logistic regression) to generate new features for a non linear model (like random forest)?

The setting is a 2-class classification problem. We have too many features, some of them not very informative and with many zeros. We are thinking in ways of selecting the best features, and PCA (in the full dataset or maybe in groups of related features) is one of the alternatives. But I thought if there was another way of generating linear combinations of features that not only takes in consideration the intrinsic variance, but also the relationship with the target. Like a target-PCA, if that existed. And an approximation of this idea could be what I ask in the main question: Could it be a good idea to use a linear classifier like logistic regression or SVM to generate linear combinations of features that, in a way, are optimizing the information gain with respect to the target? Because I think that if, given a subset of variables, a hyperplane can give a good separation of the classes, the equation of the hyperplane, considered as a feature, have more predictive power than any of the individual features, so maybe you could substitute the group of features with the new one and give all this generated features to the last model (the random forest) as the inputs. EDIT: There is a very similar question to this one, that someone has suggested: Non-perpendicular hyperplane decision trees It's closely related to what I was thinking. Thanks everyone!!
