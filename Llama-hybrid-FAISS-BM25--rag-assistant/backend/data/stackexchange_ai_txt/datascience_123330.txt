[site]: datascience
[post_id]: 123330
[parent_id]: 123328
[tags]: 
The typical approach to have neural networks generate discrete outputs is to make them generate a probability distribution over the space of options. Then, you can choose the option with the highest probability, or use a different sampling strategy (i.e. beam search in text generation). To generate a probability distribution over N options, you project the output of your model (i.e. an LSTM) to an N-dimensional space and then apply the softmax function to normalize the N components and make them add up to 1. The N-dimensional vector before the softmax is referred to as unnormalized log probabilities or "logits". The usual loss function for this kind of discrete output is the categorical cross-entropy. This kind of approach is used not only for text generation, but also in multi-class classification.
