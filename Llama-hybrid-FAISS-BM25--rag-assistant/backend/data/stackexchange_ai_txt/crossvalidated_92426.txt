[site]: crossvalidated
[post_id]: 92426
[parent_id]: 92393
[tags]: 
The equations for linear regression can be performed in primal form (e.g. the regular normal equations), where there is one parameter per input variable, or dual form (e.g. kernel ridge regression with a linear kernel) where there is one parameter per training example. This means you can choose which form to use depending on which is more efficient, if $N >> d$ then use the normal equation, which is $O(N)$, if $d >> N$ then use kernel ridge regression with a linear kernel, where $d$ is the number of attributes. Bayesian linear regression is to GP with a linear covariance function what linear regression is to kernel ridge regression with a linear kernel. If you use a quadratic kernel, you can achieve the same result by replacing it with a linear kernel and adding additional attributes that are the pairwise products of all of the existing attributes. You can then use the same trick to get O(N) complexity. However this becomes impractical if d is too large as this causes a very rapid increase in the number of product terms.
