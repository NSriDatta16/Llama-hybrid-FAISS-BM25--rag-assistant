[site]: crossvalidated
[post_id]: 326278
[parent_id]: 325215
[tags]: 
Thinking in terms of decision theory can help you make continuous analogues out of the quantities of sensitivity and specificity. Bishop pp. 40 illustrates a binary classification problem along a single (continuous) input dimension, using a stylized representation of joint probabilities $p(c,C_k)$ for each of 2 classes: If you are classifying a continuous $x$ into one of two classes $C_1, C_2$ then you are effectively drawing a decision boundary/surface $\hat{x}$ in such a way that the input space for $x$ is divided into decision regions $R_1, R_2$. The true decision boundary $x_0$ is unknown. The analogous terms to "true positive" and "true negative" would be the probability of correctly classifying a point in either $C_1$ or $C_2$, depending on what those $C_k$ actually represent. In the chart above, they are represented by white regions under the curve. To evaluate them, The probability of correctly classifying a point in $C_1$ is $\int_{R_1} p(x,C_1)dx$ The probability of correctly classifying a point in $C_2$ is $\int_{R_2} p(x,C_2)dx$ You could also calculate the related "false positive" and "false negative" quantities, which are represented by the coloured red, green & blue areas under the curve: The probability of incorrectly classifying a point in $C_1$ as $C_2$ is $\int_{R_2} p(x,C_1)dx$ The probability of incorrectly classifying a point in $C_2$ as $C_1$ is $\int_{R_1} p(x,C_2)dx$ Then sensitivity and specificity would simply be the appropriate ratios of those regions, based on which of $C_1,C_2$ represents the "positive" and which represents the "negative": $\text{sensitivity} = \frac{\text{true positives}}{\text{true positives} + \text{false negatives}}$ $\text{specificity} = \frac{\text{true negatives}}{\text{true negatives} + \text{false positives}}$ This isn't just a theoretical observation. In our example, the green & blue areas under the curve cannot be reduced, but it should be possible to minimise the red area by minimising the expected loss. This actually solves for the optimal decision boundary/surface by setting $\hat{x} = x_0$, and is the foundation of many machine learning classification algorithms.
