[site]: datascience
[post_id]: 38680
[parent_id]: 
[tags]: 
What does "Model recursive loss convergence" mean?

I trained a very simple neural network to figure out some features from an image. it's something like a reverse graphic. The neural Network will receive an image and try to estimate its features. The features can completely describe the image(like inverse graphic-see p.s.2). So we can use them to reconstruct the image using a "rendering function". Obviously, the network has some error and the image cannot be reconstructed exactly. I fed the ' Reconstructed Image ' to the model and got some new ' Estimated Features '. I repeated this and after some time the 'Estimated Features' has converged to some value. Not the ' True Value ' for sure but something. What does this repeated estimation convergence and also the loss convergence of this repeated process mean? can it be useful for anything? for example, for training the Neural Network model in a more efficient way. This is the plot of the convergence of the estimated parameters and the real value of each of them. Colors represent each Param. lines are estimated values in each feedback loop iteration and '+' is the real value. p.s.1: the loss is too high cause I wanted to show how it converges to something. with more training, the Loss decrease and the distance get's really small. p.s.2: For the reverse graphic, you can imagine a set of features like (x, y, width, height, thickness) which can be used to draw a rectangle. The rendered image then can feed into a model to find the estimated features of (_x, _y, _width, _height, _thickness) p.s.3: The 5th feature's True and estimated value is 0 for all samples
