[site]: crossvalidated
[post_id]: 254150
[parent_id]: 254134
[tags]: 
This is only true when using temporal difference learning alone, i.e. Q-learning. In that setting you are learning the optimal state-action-value function Q* and then taking actions that maximize Q*. If instead, you learned V*, you know the real value of the state that you are in if you followed an optimal policy but that doesn't help you make a decision which action to choose (because V is not a function of a). If you knew the model, you could see what reward you would get for each action and what state you would end up in, thus effectively calculating Q* from V*.
