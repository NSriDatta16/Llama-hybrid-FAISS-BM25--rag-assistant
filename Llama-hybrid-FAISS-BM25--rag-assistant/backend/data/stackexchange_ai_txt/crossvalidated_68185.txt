[site]: crossvalidated
[post_id]: 68185
[parent_id]: 68167
[tags]: 
I do have some suggestions about some alternatives to solve your clustering problem (which i bring out at the end of this message). However, to answer your question first, based on what you describe, there is nothing that stops you from applying pca to your data (I am assuming your features are not binary). For pca, think of voxels as observations and the attributes/features (ex: [1;0;0;1;1] for syn_data_1/position-1/voxel-1) as variables. There is a function called pca in matlab to do principal component analysis. In your case, use the following code snippet: [princomp, newVoxelRep, princompVar] = pca( syn_data' ); Assuming syn_data is numFeatures x numVoxel matrix, the outputs are the following: princomp: is a numFeatures x numFeatures matrix wherein each column contains the representation of a principal component in the original/input feature space (these are the eigen vectors of the covaraince matrix you use in PCA). The columns of this matrix are ordered in descending order of principal component variance. Also, the columns are unit-vectors and mutually orthogonal and hence can be viewed as the axes of a coordinate space. newVoxelRep: is a numVoxel x numFeatures matrix wherein each row contains the representation of a voxel in the new coordinate space formed by the principal components as coordinate-axis. princompVar: is a numFeatures x 1 vector containing the variances (or eigen values) of the principal components (or eigen vectors). Now you can look at the variances of the principal components (in "princompVar") and knock-off/drop the ones that are too low (people typically keep the top k principal components which cover say 98% of the variance). If you want to visualize your high-dimensional data to get a rough sense of the clustering tendency of your data, you can project the data onto the top two (given by newVoxelRep(:,1:2)) or top three (given by newVoxelRep(:,1:3)) principal components and plot them in matlab as shown below: plot( newVoxelRep(:,1), newVoxelRep(:,2), '.' ); % 2D plot3( newVoxelRep(:,1), newVoxelRep(:,2), newVoxelRep(:,3), '.' ); % 3D Please refer to this lecture notes for the math behind pca. Having answered your question, i do have some concerns about whether pca is the right approach to take to solve your clustering problem. PCA is a dimensionality reduction technique (through there are other uses of it) and the dimensionality of your feature space isnt that high. PCA yields the best way (under certain assumptions) to reduce the dimensionality of the data while preserving the distances between the points as much as possible. So, whether you decide to reduce the dimensionality of your data or not, end of the day, you have to use a clustering algorithm to actually solve your clustering problem. On the contrary, if your intention behind doing pca, is to find a way visualize the data in 2D or 3D (by projecting the data onto the top two or three principal components as i described above) to get a rough visual sense of the clustering tendency of the data then doing pca is a good first attempt. However, even in this scenario, i would like to bring it to your notice, that pca, being a linear dimensionality reduction technique, it does not perform well if your data lies on a low-dimensional "non-linear" manifold embedded in a high-dimensional space (ex: imagine a spiral in 3D which is a one-dimensional non-linear manifold embedded in 3D space). With pca you are geometrically fitting an ellipsoid to your data and if your data isnt ellispodal looking then it will perform poorly. In these scenarios, you should look into non-linear dimensionality reduction techniques such as Locally Linear Embedding, Laplacian Eigen-maps, Diffusion-Maps etc. Please refer to these lecture notes [ 1 , 2 ] for more information on non-linear dimensionality reduction. For clustering, you may want to look into one of the several clustering algorithms such as k-means (for spherical clusters), Gaussian Mixture model clustering (for gaussian looking clusters) using the expectation-maximization (EM) framework, and mean-shift (for arbitrary-shaped clusters). If you have reasons to assume that your data lies on a low-dimensional non-linear manifold embedded in a high-dimensional space then you may also want to explore the literature on spectral clustering wherein they first apply a (graph-based) non-linear dimensinality reduction technique to the data and then use a clustering algorithm to cluster it. Also, i would like to bring to your notice that in the literature, people have used clustering algorithms with feature-vector = [pixel/voxel position, color/texture] to segment images and volumes. Please refer to the following paper based on mean-shift clustering, as a first look: Comaniciu, D.; Meer, P., " Mean shift: a robust approach toward feature space analysis ," IEEE Transactions on Pattern Analysis and Machine Intelligence, , vol.24, no.5, 2002.
