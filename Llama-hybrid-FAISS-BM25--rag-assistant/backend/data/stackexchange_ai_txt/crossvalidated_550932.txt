[site]: crossvalidated
[post_id]: 550932
[parent_id]: 354484
[tags]: 
In my opinion, classical boosting and XGBoost have almost the same grounds for the learning rate. I personally see two three reasons for this. A common approach is to view classical boosting as gradient descent (GD) in the function space ([1], p.3). As for the simplest univariate GD, we need to define a learning rate that guarantees that we make small steps s.t. we don't overshoot the minimum. Then, XGBoost makes use of the 2nd order Taylor approximation and indeed is close to the Newton's method in this sense. While using the learning rate is not a requirement of the Newton's method, the learning rate can sometimes be used to satisfy the Wolfe conditions. Learning rate provides shrinkage. Each weak learner is weak , meaning that it can't perfectly reconstruct the residuals (and it shouldn't: otherwise we would overfit). So, the learning rate reduces the effect of each such erroneous weak learner: we are going in the right direction (opposite to the gradient of the loss) but we are not allowed to have steps as big as we would take with strong learners. This is similar to stochastic gradient descent: we are forced to keep the learning rate small as our gradient estimations are imprecise. The effect is even more pronounced in XGBoost where extra tree randomization is added through row/column sampling. The subsequent weak learners provide small corrections step-by-step. These corrections accumulate with algorithm iterations and if you don't do early stopping at some point, you'll often fit the training set perfectly (i.e. you are likely to overfit). (Added to the original answer). Regularization (I almost forgot the one which is the most relevant to the question). Smaller learning rate allows adding more weak learners until the test error starts increasing([1], p.15). By averaging over more weak learners a potentially lower generalization error can be achieved. Again, the effect is even more evident for XGBoost, where column/row sampling reduces correlation between the weak learners, which adds some "bagging" effect to the boosting model. Then, could you please provide more detailed derivations of the equations in your post? It seems strange that the learning rate ends up in the denominator. [1]Friedman, Jerome H. "Greedy function approximation: a gradient boosting machine." Annals of statistics (2001): 1189-1232.
