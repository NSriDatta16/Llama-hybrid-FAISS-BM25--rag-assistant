[site]: crossvalidated
[post_id]: 396173
[parent_id]: 
[tags]: 
How to approximately sample an unknown non-parametric joint distribution given a complete set of partial conditional distributions?

This question is related somewhat to Bayesian networks. In a BN, you have a DAG (directed acyclic graph). By supplying the root nodes with a sample, you can then follow the directed arcs to sample each of their children, and children's children, etc. until you have sampled the whole graph. But what about a scenario where I have a complete, undirected (and thus cyclic) graph? For example, if I have 4 discrete variables $a, b, c, d$ , and I have the complete set of empirical non-parametric partial conditional distributions $C = \{ p(a|b), p(a|c), p(a|d), p(b|a), p(b|c), ... \}$ , how could I best sample a random vector $V_n = \{a_n, b_n, c_n, d_n\}$ satisfying all partial conditional distributions $C$ ? Since there is no notion of a set of "starting nodes", the cycles become an issue. Here are two ideas I have so far: Generate a vast number of random $V_n = \{ p(a), p(b), p(c), p(d) \}$ and compute some approximation of likelihood for each like $\sim MLE(V_n) = \prod_{p(x|y)\in C, (x,y)\in V_n}{p(y)p(x|y)}$ , and then stochastically select a winner by sampling over $\sim MLE(V_n)$ . This seems very inefficient and unlikely to generalize well if the variable count and/or number of factors begins to increase substantially. A more complex stochastic method where you start with sample $V_{n_0} = \{ p(a), p(b), p(c), p(d) \}$ , and iteratively attempt to converge toward the actual joint distribution by randomly selecting and replacing one of the variables, lets say for example $c$ , using only the conditionals $p(c|\ast)$ in $C$ to sample a new $c_1$ , and thus creating a new sample vector $V_{n_1} = \{ a_0, b_0, c_1, d_0 \}$ . Then we could compare the likelihood estimates $\sim MLE(V_{n_0})$ and $\sim MLE(V_{n_1})$ and choose the more likely (deterministic or stochastic choice?) as the starting point for our next iteration. Hopefully this would converge after some reasonable number of iterations (even if the variable count and/or number of factors was high). However, how many iterations to take (or when to conditionally stop) is unclear to me. Would either of my two ideas work at all? What better ways are there to efficiently generate quality samples approximating a non-parametric joint distribution given the complete set of partial conditional distributions? Maybe some other types of acceptance/rejection methods?
