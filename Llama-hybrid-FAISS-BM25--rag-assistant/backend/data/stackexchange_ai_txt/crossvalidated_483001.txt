[site]: crossvalidated
[post_id]: 483001
[parent_id]: 
[tags]: 
How to calculate uncertainty from test set size

I'm training a machine learning model and trying to determine how large my test set should be. I'm not using any k-fold cross-validation, just the test set. I believe the only benefit of increasing the test set size is to reduce the uncertainty of the accuracy statistics calculated. Is this correct? To get the uncertainty, is that just the square root of the test set size, N? For example, if I have 1,000 samples and I put 10% into my test set, I would have N=100. Let's say I get 70% correct. Because 1/sqrt(100) = 0.1, my accuracy be 0.7±0.1, or 70±10%. Is this correct? Let's say I'm deciding between putting 10% and 20% of my data in the test set. Is it true to say that the only benefit of putting 20% into my test set is that uncertainty in my accuracy goes from 0.1 to 1/sqrt(200) = 0.071? If I have 100M examples but I am perfectly happy with a 0.1 uncertainty in my accuracy (this is theoretical, just trying to make sure I understand the basics), is there any reason not to do 99,999,800 in training, 100 in validation, and 100 in testing?
