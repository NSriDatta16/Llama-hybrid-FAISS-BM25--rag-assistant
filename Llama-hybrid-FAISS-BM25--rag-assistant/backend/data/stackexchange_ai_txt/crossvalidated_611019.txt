[site]: crossvalidated
[post_id]: 611019
[parent_id]: 
[tags]: 
Regression predictor from count of categorical variables?

Let's say I have the following strings and associated target variables: PVADDHJ 98.58 LMIJLFPA 98.89 PNI 97.86 YZDYI 100.98 OXFBI 100.99 OPGWQJ 102.43 JDUKN 100.76 ZWDTXHZ 100.09 URJFIT 98.05 VWYWBWUIR 99.76 Python code to generate this: import numpy as np chars = list('ABCDEFGHIJKLMNOPQRSTUVWXYZ') n = 10 for i in range(n): np.random.seed(i) n_chars = np.random.randint(3,10) rand_chars = "".join(np.random.choice(chars, size=n_chars)) rand_target = np.random.normal(100,1) print(f"{rand_chars:15s} {rand_target:.2f}") The above example is random, but let say I have an actual dataset where there is a relationship between the strings and the target variable. Using counts of the individual characters (or just a binary value for each character to indicate whether it's included), I want to predict the values of the target variable. One way to do this would be a linear regression with one hot encoding for each character and solve for the value of each variable plus a constant. If this were binary classification, I could use logistic regression with one hot encoding and I believe Naive Bayes would also work. However, I'm wondering if there is something similar to Naive Bayes that would work for regression in this case, maybe starting with an overall distribution of the target values, and then refining it based on which characters are in the string. I think this would lead to different predictions than the one hot encoding approach. Updating to provide some more context: I'm learning about sentiment analysis and I'm curious whether simple counts of certain words are sufficient over more advanced models. I'm trying to figure out how to treat situations that include words that are somewhat contradictory to each other with respect to the target value. The example above is just an attempt to illustrate the idea in a simple way.
