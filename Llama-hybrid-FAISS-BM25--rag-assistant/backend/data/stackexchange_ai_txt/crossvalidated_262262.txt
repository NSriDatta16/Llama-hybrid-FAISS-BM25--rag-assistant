[site]: crossvalidated
[post_id]: 262262
[parent_id]: 
[tags]: 
Understanding Bayesian statistics

OK, so I have no problem with the Bayes' theorem itself and I do think that I get a lot of the simpler maths around Bayesian statistics. However, I am struggling with the idea of treating population parameters as random variables. Say someone tosses a coin. Before the toss, I estimate the probability to be 0.5. Coin lands, I don't know the result. My frequentist soul tells me that it is either heads or tails, but not a random variable with associated probability. In the long run, I can treat my estimate as the expected fraction of correct guesses, but for that one particular toss it is not applicable. Our intuition is more Bayesian in the sense that even though the coin has landed, as long as we don't know the result, we still tend to think that the probability is 0.5 (we treat the reality of the coin toss as a random variable). However, this is relative: should someone get a peek at the coin and tell us: "I bet 50$ that it's a head!" we would not bet, would we? (in a way, it is the reverse of the famous xkcd strip). I guess my question is naive, but here it comes: how do you deal with this paradox?
