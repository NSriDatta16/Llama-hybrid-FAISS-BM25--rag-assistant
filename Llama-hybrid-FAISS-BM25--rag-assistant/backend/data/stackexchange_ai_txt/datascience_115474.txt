[site]: datascience
[post_id]: 115474
[parent_id]: 115473
[tags]: 
In most data science scenarios, thousands of features are not relevant. Just a few are enough, but it depends on the data. In general, some data preprocessing is necessary to take the most relevant features. This could be done thanks to a correlation map. https://medium.com/@szabo.bibor/how-to-create-a-seaborn-correlation-heatmap-in-python-834c0686b88e The features with correlated values around 0 with other features could be removed, and the strongly correlated features could be merged into one. If you want to have also a clearer view of your data, you can apply a dimensionality reduction algorithm to squeeze your data into 2 or 3 dimensions, and get clusters of similar features. It also works for grouping similar samples. https://umap-learn.readthedocs.io/en/latest/clustering.html One last tip: start with smaller samples with fewer features to reduce the processing time and build an efficient model quickly. Then increase them to cover all features and samples.
