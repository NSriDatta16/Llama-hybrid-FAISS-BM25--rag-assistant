[site]: crossvalidated
[post_id]: 354531
[parent_id]: 
[tags]: 
Neural network performance degradation when input is standardised and mean-centered

In an effort to do some debugging of my simple multi-layer-perceptron regression problem, I have one input feature $x$ and one output feature $x$, meaning that the network should simply output whatever was input: $$f(x) \rightarrow x$$ For this very simple case: I have only 16 examples, a single hidden layer with 100 neurons, and I use MSE as the loss function, training in keras/tensorflow . When batch normalization is configured I get constant training error (which I haven't figure out yet either), so there's no batch normalization. EDIT: I should note that I use ReLU for activation. When I train the network, I get a very small error that is essentially zero - this is good. However, when I transform the input so that it is mean centered and normalized to the standard deviation of the variable across the examples (vector $\boldsymbol{x}$), such that the model aims to train: $$f(\frac{x - mean(\boldsymbol{x})}{std(\boldsymbol{x})}) \rightarrow x$$ The post-training error is much worse - from relatively negligible to approximately the mean value of $\boldsymbol{x}$ (which is a several orders of magnitude error increase). Why would this be? I have been assuming that this standardization of a variable does not negatively impact the 'predictiveness' of the variable, but rather hopefully helps speed up convergence. If perhaps it was just a case of increased model complexity, then I would expect increasing the number of neurons would help - but even when increasing the number of neurons in my hidden layer 100x, the error remains the same. I'm now unsure of whether it would be appropriate to remove standardization from my initial, more complex, multivariate model that I'm trying to investigate. Any ideas?
