[site]: crossvalidated
[post_id]: 488844
[parent_id]: 
[tags]: 
Self Attention for Variable Length Sequence Classification

I have a problem that is not particularly unique, but I'm still having trouble to figure out exactly how it's usually done. My training set is of the form $\mathcal{T}=\{(t_i\in \mathbb{R}^{[n,m]\times 128} ,l_i\in \mathbb{B})\}_{i=1}^N$ . So the input is variable length multivariate time series and the label is binary. I have some insight that self attention should be useful since the classification problem is related to the periodic behaviour of the input sequence. This paper ( RepNet ) from CVPR 20 used a self-attention network (transformer) for analysis of a periodic signal with good results so my insight is coming mostly from here. The problem I have is how to use a transformer network like torch.nn.TransformerEncoder for variable length input. This article explains that BERT models expect fixed length input so there is usually a padding character appended. My main question is to find a method to do self attention on sequences of variable length, if it's possible. Thanks
