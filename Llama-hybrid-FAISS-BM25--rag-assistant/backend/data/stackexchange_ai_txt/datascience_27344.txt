[site]: datascience
[post_id]: 27344
[parent_id]: 24786
[tags]: 
1. About clustering of Values Since Values has a specific meaning in that problem, you might want to select the categories arbitrarily setting ranges yourself. (using the same example as the above answer, this would look like: 1-3 : Bad, 4-6 : Average, 7-10 : Good). In this case, if the range of answers is not pre-defined (so it's not like high-school grades, but more like people's weight), make sure to use ranges of same size. Another approach would be to use a distance based algorithm for this purpose (e.g. kmeans), so that the ranges of the clusters won't be stable but automatically selected to minimize distances. It is similar as the t-shirts example in this video (starting 1.35) but with one variable. In both cases though you will have to set the number of clusters yourself. In the second case this can be done easier by visualizing the data and see if the clusters make sense. Since this is your output variable (and this procedure will be performed only once) I think you should not do this important step automatically but use your judgement before selecting the final grouping. 2. About feature selection: There are plenty feature selection algorithms you can choose from. This is a review paper that can give you some ideas. SVM-RFE (Support Vector Machines with Recursive Feature Elimination) is a very strong combination of feature selection technique with classification that usually works well. You can deal with categorical attributes by turning them into dummy variables. A Decision Tree Classifier could also do the job. They do the classification based on automatically generated rules, based to some criterion (e.g. entropy). You can see the importance of each attribute for the classification (GINI importance) There are very good implementation for both suggested approaches in python's scikit learn package: SVM and RFE Decision Tree Classifier Food for thought: I don't know what is the exact problem you are trying to solve, but have you thought of using a Decision Tree Regressor directly to the data? You can use Values as the target-variable and the rest of attributes as your dataset. The end-leaves of the created Decision Tree will be the "Values clusters" and each one of the samples will be classified to one of them. (Also available in scikit learn toolbox here )
