[site]: crossvalidated
[post_id]: 482269
[parent_id]: 482260
[tags]: 
The question is rather broad, though I guess that is the intention with an interview question :) How to measure performance of linear regression? You question then goes on to talk about training and test sets, and of course this could be part of the answer, however I would back up a bit and suggest that performance should really be in terms of whatever question the model was built to answer. If asked this question in a real world setting by someone who had fitted a model and came to me for advice, the first thing I would ask is: What question is the model trying to answer ? In any applied setting be it traditional statistical analysis, "modern" data science, or whatever, context is vitally important. As a large part of this I would want to know if the model is going to be used for inference or prediction. With inference, there is almost always a underlying causal question: For example, does wearing seatbelts cause road traffic accident casualties to decrease? Does smoking reduce poor outcomes in those who test positive for COVID-19? In such cases one of the most important aspects of a linear regression is variable selection. It is important not to condition on mediators, or to over-adjust for confounders, but the inclusion of competing exposures is helpful. If there are several questions, as is often the case in data science settings, such as "which of these variables are most important for understanding the outcome" - the usual (mistaken) approach is some stepwise procedure based on p-values, AIC, BIC etc. This is a terrible idea because it completely overlooks the causal relationships and is based on arbitrary thresholds (even with AIC/BIC this really boils down to p-value thresholds). The key point is that, for each variable of interest, a different model is usually needed, because, for example, a variable that is a confounder for one association, will be a mediator for another, and should be excluded in the former and included in the latter. It should be obvious to anyone who reads published papers that this is the "Table 2 Fallacy" where researchers simply put everything in multivariable linear regression models and report all the estimates and p-values! Once we are happy with the set of variables we would want to look at the usual diagnostics to assess whether the assumption of linearity holds. Other assumptions such as observation independence and variance homogeneity might also be considered. Then we might want to think about prediction. People often think that prediction and inference are two separate things, and they are, but the problem is that many people who think they are only interested in prediction are actually interested in inference as well. For instance, right now there are teams of data scientists and machine learning practitioners busy trying to apply their methods to COVID-19 datasets (I was one of them until quite recently), and for the most part all the discussion was around prediction. Yet, after you build a model and find that (and I am making this up for confidentiality reasons) vaping is strongly predictive of better outcome in those aged under 35 who test positive for COVID-19, there ALWAYS follows questions from clinicians about interpreting model output as a causal effect - either with a view to promoting some kind of intervention, or to explain an "apparent paradox". But the regression models were not built with causal inference in mind. This type of thing is happening all the time, and it is utterly bonkers. If there is genuinely no interest in inference then the "blind" methods of train, test, (cross) validate can of course be used and things like MSE are of course good metrics. However, if there is some element of causality involved then it is important to consider the issues I described above.
