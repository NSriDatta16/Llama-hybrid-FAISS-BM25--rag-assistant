[site]: datascience
[post_id]: 94769
[parent_id]: 64835
[tags]: 
In order to compare models, you need to define a single metric for the quality of the model. You don't specify but I assume your model is a multiclass classifier: out of 45 labels, for each example, it needs to pick the right one. You can calculate the F1 score of your model for each label vs the rest: how often is it able to correctly predict label 0 vs not label 0? (This is called the "one versus all" strategy.) This will give you 45 F1 scores for each model: one F1 score for each label. The plot you show suggests that all labels are more or less equally important, so you can average the F1 scores for each model and the will give you an indication of the quality of each model. If the importance of the labels varies, use a weighted average with the weight according to the importance. Note that the plot you show just says each model predicts label 0 about 28 times, but there is no info on how often this was correct...
