[site]: crossvalidated
[post_id]: 531213
[parent_id]: 531165
[tags]: 
Firstly, it seems like for some reason survival analysis using ML techniques does not seem to get done much. That seems weird, when you think about all the possible applications like, say, time to customer churn, time to equipment failure and so on, which would seem like interesting topics. One of the more "obvious" approaches that I've seen used a few times is to use any neural network that you might use for such a tasks, if the goal were e.g. classification or regression, but to instead of outputting probabilities for each category or a single regression estimate to output the parameters of a survival distribution. I'm not sure I've seen this published, although it feels like someone must have done so and I probably just have not seen it. The datasets I've used it on are proprietary, so I don't have any nice walk-throughs. This, obviously, does not really (or at least it's not obvious to me how) work for Cox regression/Kaplan-Meier type estimates due to their semi-parametric nature. However, this would e.g. work quite well for the Weibull distribution , you would have two outputs from the neural network that you would treat as the log-scale $\log \lambda$ and log-shape $\log k$ parameters. You would then use something like $$- \log k + \log \lambda - (k-1) ( \log x - \log \lambda) + (x/\lambda)^k$$ as your loss function for observations with an event (minus the log of the probability density function) and $$ (x/\lambda)^{k} $$ for observations that are right censored (minus the log of the complementary cumulative distribution function). It may very well be that the various re-parameterizations used in the statistical literature / in statistical software would turn out useful here, too. Defining a neural network with two outputs is easy enough with e.g. the keras , PyTorch or fastai (on top of PyTorch) libraries in python, or the keras or torch packages in R. For what architecture to use before that, you can take inspiration from what people are generally doing for tabular data (assuming your inputs are tabular) including e.g. the (prediction) model architecture used in fastai (see e.g. the code accessible via the documentation or the excellent book using the library) or Dragonnet if you want to look into causal inference problems. Thus, the main thing you need to do to get this to work is to code up the custom loss function (I'm not aware that any libraries offer this as a default).
