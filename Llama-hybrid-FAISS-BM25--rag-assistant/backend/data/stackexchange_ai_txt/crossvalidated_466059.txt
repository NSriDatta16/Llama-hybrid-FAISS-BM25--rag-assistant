[site]: crossvalidated
[post_id]: 466059
[parent_id]: 
[tags]: 
How to proof that the bayes optimal classifier is optimal for a continuous domain

Exercise 3.7 from the book »Understanding Machine Learning: From Theory to Algorithms«, Shalev-Shwartz and Ben-David, states the following: The Bayes optimal predictor: Show that for every probability distribution $\mathcal{D}$ over the domain $\mathcal{X} \times \{0, 1\}$ , the Bayes optimal predictor $f_D$ is optimal, in the sense that for every classifier $g : \mathcal{X} \to \{0, 1\}$ , $L_\mathcal{D}(f_\mathcal{D}) \leq L_\mathcal{D}(g)$ . The bayes optimal predictor $f_\mathcal{D} : \mathcal{X} \to \{0, 1\}$ is defined as $$ f_\mathcal{D}(x) = \begin{cases}1 : \text{if}\ \mathbb{P}[y = 1 | x] \geq 0.5 \\ 0 : \text{otherwise}\end{cases} $$ and $$ L_\mathcal{D}(h) = \underset{(x, y) \sim \mathcal{D}}{\mathbb{P}}[h(x) \neq y] = \mathcal{D}(\{(x, y) \mid h(x) \neq y\}) $$ is the true risk of a classifier $h$ . I was able to convince myself that, by construction of the Bayes optimal classifier, for any particular $x' \in \mathcal{X}$ the »local risk« of $f_\mathcal{D}$ is less than that of $g$ , where by »local risk« I mean $$ \mathcal{D}(\{(x, y) \mid x = x', f_\mathcal{D}(x') \neq y\}) \leq \mathcal{D}(\{(x, y) \mid x = x', g(x') \neq y\}). $$ If $\mathcal{X}$ is discrete, I'm also able to expand this knowledge about any particular point $x'$ to the whole domain $\mathcal{X} \times \{0, 1\}$ , because then I could rewrite $\{(x, y) \mid h(x') \neq y\}$ as $\bigcup_{i = 1}^{\infty}\{(x_i, y) \mid h(x) \neq y\}$ (for any $h$ ) and use the $\sigma$ -additivity of $\mathcal{D}$ to conclude $L_\mathcal{D}(f_\mathcal{D}) \leq L_\mathcal{D}(g)$ . However, how does this work for a continuous $\mathcal{X}$ ?
