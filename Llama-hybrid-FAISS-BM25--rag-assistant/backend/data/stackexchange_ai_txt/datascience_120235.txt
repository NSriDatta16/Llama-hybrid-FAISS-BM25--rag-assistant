[site]: datascience
[post_id]: 120235
[parent_id]: 120112
[tags]: 
What you described sounds just like the usual TD(0) learning of state value functions. If you only have state values (including the next state value) which by definition they must follow a specific bootstrapped (non-optimal) behavior policy $\pi_b$ , then you don't have direct criterion to improve your policy in the absence of a model under Q-learning's assumption. In fact all kinds of RL algos ultimately try to find an optimal policy and follow the generalized policy iteration (GPI) idea which inherently utilizes action value function Q indexed on both state and action. Mathematically if you want to solve the Bellman optimality equation (foundation of Q-learning) which is only expressed directly in terms of optimal state value function and actions allowed at that state you'll still end up with a greedy optimal policy solution in terms of the same Q function indexed on both state and action.
