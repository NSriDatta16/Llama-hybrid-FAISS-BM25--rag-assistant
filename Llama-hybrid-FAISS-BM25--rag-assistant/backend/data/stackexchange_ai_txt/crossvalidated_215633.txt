[site]: crossvalidated
[post_id]: 215633
[parent_id]: 
[tags]: 
How flexible are autoencoders for non-linear dimensionality reduction?

I've been starting to play around with autoencoders for feature extraction and dimensionality reduction, and am wondering how critical input feature definitions are for success. For example, some of my features record how often "events" have a certain property, with the total number of events varying between cases. I'm wondering if it would make a difference to represent that as the number of events that have the property, or the fraction of events which have the property, given that I'll have another feature which represents the total number of events. To put it another way, how flexible are autoencoders in doing functional transformations of the input features? Can they (effectively) multiply or divide? Do log transforms? (etc.) Is there a particular network architecture which makes this easier (e.g. minimal number of layers, particular activation functions, etc.)? If, in practice, they do have difficulty with non-linear feature transformations, are there any rules of thumb on how to choose feature representations to best facilitate dimensionality reduction?
