[site]: crossvalidated
[post_id]: 618850
[parent_id]: 618834
[tags]: 
To expand upon my comment/answer your question: When you do standard out-of-the-box logistic regression, you are calculating the maximum likelihood estimators $\hat{\beta}_{0}, \hat{\beta}_{1}$ which are given by $$argmin_{\beta_{0}, \beta_{1}} \sum_{i=1}^{N}y_{i}\ln\sigma(\beta_{0} + \beta_{1}x_{1}) + (1-y_{i})\ln (1 - \sigma(\beta_{0} + \beta_{1}x_{1}))$$ where I'm using $\sigma(x)$ to denote $\frac{1}{1+e^{-x}}$ For what's to come, it will be instructive to note that this is equivalent to writing: $$argmin_{\beta_{0}, \beta_{1}} \sum_{i=1}^{N}y_{i}\ln p(y_{i}|x_{i}, \beta_{0}, \beta_{1})=argmin_{\beta_{0}, \beta_{1}} \ln P(Data|\beta_{0}, \beta_{1})$$ If instead of having a maximum-likelihood estimator but wanted to come up with a posterior distribution for $(\beta_{0}, \beta_{1})$ , you would do this using Bayes' rule $$p(\beta_{0}, \beta_{1}| D) = \frac{p(D|\beta_{0}, \beta_{1})p(\beta_{0}, \beta_{1})}{\int p(D|\beta_{0}, \beta_{1})p(\beta_{0}, \beta_{1})d\beta_{0}\beta_{1}}$$ You already have an expression for the likelihood term, albeit it's a bit more complicated if you don't take the log, it's given by $$\prod_{i=1}^{N}\sigma(\beta_{0}+ \beta_{1}x_{i})^{y_{i}}(1 - \sigma( \beta_{0}+\beta_{1}x_{i}))^{1-y_{i}}$$ and you will have to choose some suitable prior $p(\beta_{0}, \beta_{1})$ that corresponds to your intuition. So in theory, I've written down an analytic expression for your posterior function, but this isn't entirely trivial to evaluate (which is why Bayesian inference and MCMC are a discipline unto themselves). The denominator is not something you can calculate analytically, you'll have to solve it numerically. Note however that you don't technically need to know the posterior, you just need to be able to sample from it, as what you care about is getting a posterior for the quantity $\frac{\beta_{0}}{\beta_{1}}$ . Because $(\beta_{0},\beta_{1})$ will be correlated, even if you had access to the joint posterior it would be non-trivial to calculate the distribution of their ratio. A more pragmatic way to proceed would be to sample $(\beta_{0},\beta_{1})$ pairs from the distribution, calculate the ratios every time and take an average and std over many such ratios. This only requires you to be able to sample from the posterior, and there is a whole literature on how to sample from un-normalised distributions. In this case, that means treating your posterior as $$ p(\beta_{0}, \beta_{1}) = \alpha \cdot \prod_{i=1}^{N}\sigma(\beta_{0}+ \beta_{1}x_{i})^{y_{i}}(1 - \sigma( \beta_{0}+\beta_{1}x_{i}))^{1-y_{i}}$$ In conclusion, you can see why people tend to solve this problem by bootstrapping
