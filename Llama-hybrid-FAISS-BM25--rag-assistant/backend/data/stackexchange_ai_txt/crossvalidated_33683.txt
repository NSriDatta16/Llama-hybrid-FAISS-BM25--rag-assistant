[site]: crossvalidated
[post_id]: 33683
[parent_id]: 
[tags]: 
Combining results from (GBM or any other) model based on samples from a very large database

How would you combine results of model performed on random samples of a very large dataset? I need to model a very large database in R (~75 million rows) that can not be loaded directly into memory. I am still at the planning phase. My first idea was to divide the dataset in smaller datasets using random sampling without replacement. Then I could run the random forest model on the datasets by batch, and then combine the results. I think this approach would give reasonable results. I was wondering if it would be possible to do the same approach with the GBM model? I read the documentation and each iteration depends on the previous one (unlike random forest) and thus I don't know how the results could be combined in the end. Do you know the statistical principles used to merge model results in such a parallel implementation? I am interested in general methods, not only specific to GBM.
