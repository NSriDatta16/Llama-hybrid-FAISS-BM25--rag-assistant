[site]: crossvalidated
[post_id]: 267756
[parent_id]: 267739
[tags]: 
That's a bit oversimplified. The shrinkage in a mixed-effects regression is weighted by overall balance between "classes"/"groups" in the random-effects structures, so it's not that you don't get to to choose, but rather that your group size and strength of evidence chooses. (Think of it as like a weighted grand mean). Moreover, mixed-effects models are very useful when you have a number of groups but only very little data in each group: the overall structure and partial pooling allows for better inferences even within each group! There are also LASSO (L1-regularized), ridge (L2-regularized), and elastic net (combination of L1 and L2 regularization) variants of mixed models. In other words, these things are orthogonal. In Bayesian terms, you get mixed-effects shrinkage via your hierarchical/multilevel model structure and regularization via your choice of prior on the distribution of model coefficients. Perhaps the confusion arises from the frequent use of regularization in "machine learning" (where prediction is the goal) but the frequent use of mixed-effects in "statistics" (where inference is the goal), but that's more a side effect of other aspects of common datasets in such areas (e.g. size) and computational concerns. Mixed-effects models are generally harder to fit, so if a regularized fixed-effect model that ignores some structure of the data is good enough for the predictions you need, it may not be worthwhile to fit a mixed-effects model. But if you need to make inferences on your data, then ignoring its structure would be a bad idea.
