[site]: crossvalidated
[post_id]: 464201
[parent_id]: 
[tags]: 
BERT masking scheme

From BERT paper, "The masked words were not always replaced by the masked tokens [MASK] because the [MASK] token would never appear during fine-tuning" What does this mean? If i just mask 15% words at random and then train my model and use it for fine-tuning, what would happen?
