[site]: crossvalidated
[post_id]: 611754
[parent_id]: 280209
[tags]: 
It would be elegant to have some kind of AIC/BIC-style penalty on a decision tree. However, the typical way to penalize a machine learning model for being overly complex is to try it on some data that were not used to develop the model, with the thinking being that, if the model is so complex that it allows for fitting to the noise (coincidences in the data), the model will not have such a great fit on the new data and will have a poor score. When you see something like a train/test split, the test data are being used for this very evaluation. There are drawbacks to such a strategy. For instance, what if you have some good or bad luck with your holdout data and just get one score that is particularly high or low? Cross validation is a possible remedy for this. As another example, having a holdout set limits the previous data available for training. Bootstrap validation is a possible remedy for this.
