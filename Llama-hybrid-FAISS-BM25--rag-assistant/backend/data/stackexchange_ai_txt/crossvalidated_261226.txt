[site]: crossvalidated
[post_id]: 261226
[parent_id]: 259427
[tags]: 
A Bayesian Approach Let $X_i$ for $i=1,\ldots n$ be a series of IID Bernoulli random variables with parameter $p$. Let us represent our uncertainty of the parameter $p$ by assuming it follows the Beta distribution with hyperparameters $\alpha$ and $\beta$. The likelihood function is Bernoulli and the Beta distribution is a conjugate prior for the Bernoulli distribution, hence the posterior follows the Beta distribution. Furthermore, the posterior is parameterized by: $$ \hat{\alpha} = \alpha + \sum_{i=1}^n X_i \quad \quad \hat{\beta} = \beta + n - \sum_{i=1}^n X_i$$ Consequently: \begin{align*} \mathrm{E}[p \mid X_1, \ldots, X_n] &= \frac{\hat{\alpha}}{\hat{\alpha} + \hat{\beta}}\\ &= \frac{\alpha + \sum_{i=1}^n X_i }{\alpha + \beta + n} \end{align*} Thus if you see 10 failures, your expectation of $p$ is $\frac{\alpha}{\alpha + \beta + 10}$, and if you see 20 failures, your expectation of $p$ is $\frac{\alpha}{\alpha + \beta + 20}$. The more failures you see, the lower your expectation of $p$. Is this a reasonable argument? It depends on how you feel about Bayesian statistics, whether you're willing to model uncertainty over some parameter $p$ using the mechanics of probability. And it depends on how reasonable is your choice of a prior.
