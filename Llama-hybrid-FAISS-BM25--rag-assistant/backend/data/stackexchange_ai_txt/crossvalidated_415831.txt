[site]: crossvalidated
[post_id]: 415831
[parent_id]: 
[tags]: 
Understanding bayesian inference for parameter estimation

I am reading this article an am a bit confused about one of the terms. My understanding is we are trying to estimate a gaussian distribution Θ from which a certain value we are interested in is drawn. We make an initial assumption, our "prior", about such distribution and label it p(Θ). We know (by bayes' theorem) that p(Θ|data) ~ p(data|Θ)p(Θ). Where p(Θ|data) is our updated believe after observing some real world-data about the value we are interested in. What I don't fully get is the meaning of p(data|Θ). This to me reads as "probability of observing the real value given it is drawn from its distribution". But why would this be a probability? Is it a probability simply because we assume our measurements would be noisy/subject to stochastic?
