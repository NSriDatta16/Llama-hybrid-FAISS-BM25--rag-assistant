[site]: crossvalidated
[post_id]: 149294
[parent_id]: 149231
[tags]: 
In reinforcement learning, the agent learns the value function, not the reward function. The value of an action is it's overall utility; for example, an action may bring a high reward, but lead to low-value states, making it low-value. What the quote says is that sometimes the world is complicated: some problems have state-action spaces that are too large to enumerate, so we don't have explicit transition probability and reward functions. In such a case the agent would have to learn some generalization of the true value function.
