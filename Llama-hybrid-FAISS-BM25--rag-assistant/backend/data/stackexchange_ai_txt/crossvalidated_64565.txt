[site]: crossvalidated
[post_id]: 64565
[parent_id]: 64496
[tags]: 
Applying a filter to the data is clearly a case of selective inference (a.k.a. double dipping, data snooping, etc.) If you are removing true null hypotheses, you will have an anti-conservative FDR. If you are removing true alternative hypotheses, you will have a conservative FDR. The filtering criterion you are using seems "fair" a-priori in the sense it does not seem to be correlated with the truthfulness of a hypothesis. On the other hand, the fact that you are making more and more discoveries suggests that high response values, might also be correlated with a difference between groups. I thus suspect your filtering step will have increased the FDR of the BH procedure. Try testing it using a simulation. Also, could it be you are using an analysis that assumes homoscedasticity where the variances actually grow with the means?. If this is the case, try some non-parametric test to compare the groups. Finally, back to the original question: I guess it should be possible to find the optimal number of hypotheses to test using the BH procedure. You will naturally need to define a generalized notion of power (say, average power over hypotheses). You will need to know the proportion of true null hypotheses and the power of each marginal test.
