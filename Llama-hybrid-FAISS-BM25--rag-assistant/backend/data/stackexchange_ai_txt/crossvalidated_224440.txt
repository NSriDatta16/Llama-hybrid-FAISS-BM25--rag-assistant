[site]: crossvalidated
[post_id]: 224440
[parent_id]: 224432
[tags]: 
Certainly there's complete linear dependence between the existing variables $(x_1,x_2)$ and their difference ($x_3=x_1-x_2$). One way to see this is that you can write any one of the predictors as a linear combination of the other two. Specifically, $x_3$ is defined that way, but you can also write each of the others that way. [See the third paragraph of the Wikipedia article on multicollinearity ] Note in particular, that if you add some constant $\delta$ to the coefficient of $x_3$ and also add the same constant to the coefficient of $x_2$ and subtract it from the coefficient of $x_1$, the fit will be unchanged. As a result, you cannot uniquely identify the parameters in the larger model - any optimal solution is accompanied by an infinity of other solutions. Without some form of constraint/regularization, you can't estimate all three at once.
