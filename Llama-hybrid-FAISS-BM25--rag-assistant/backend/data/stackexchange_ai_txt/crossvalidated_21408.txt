[site]: crossvalidated
[post_id]: 21408
[parent_id]: 7742
[tags]: 
This may come a wee late, but the question should be rephrased: as defined by Jaynes , maximum entropy is a way to construct a prior distribution that (a) satisfies the constraints imposed by $E$ and (b) has the maximum entropy, relative to a reference measure in the continuous case: $$ \int -\log [ \pi(\theta) ] \text{d}\mu(\theta)\,. $$ Thus, (Jaynes') maximum entropy is clearly part of the Bayesian toolbox. And the maximum entropy prior does not provide the prior distribution that is closest to the true prior, as suggested by Ashok's question. Bayesian inference about a distribution $Q$ is an altogether different problem, handled by Bayesian non-parametrics (see, e.g., this recent book by Hjort et al.). It requires to have observations from $Q$, which does not seem to be the setting of the current question...
