[site]: crossvalidated
[post_id]: 347333
[parent_id]: 344518
[tags]: 
Below are two papers that target this question: Trial-by-trial data analysis using computational models Estimating Internal Variables of a Decision Makerâ€™s Brain: A Model-Based Approach for Neuroscience The following answer is based on the explanation given in the first reference. To estimate the agent policy trail-by-trial, first we should assume some learning model $M$ (a learning process that the agent uses), for instance, one can assume Q learning. In this case, the model parameters are the learning rate $\alpha$ and temperature $\beta$. By Bayes' rule we can estimate the posterior probability distribution of the parameters model $P(\theta_M|D,M)$ by the following: $P(\theta_M|D,M) \propto P(D|M,\theta_M) \cdot P(\theta_M |M)$. Since we are interested in $\hat \theta_M = \max_{\theta_M} P(\theta_M|D,M)$, the propotion is enough. The first term can be calculated using the model specification and the second term is a prior that is determined by the knowledge about the model. Then the model parameter that best fit the data $\hat \theta_M$ can be calculated analytically for simple models by maximizing over the posterior probability with respect to $\theta_M$. If calculating the derivations with respect to $\theta$ is intractable, one can use Monte Carlo based methods. With the ability to calculate the data likelihood, we can draw any combination of possible $\theta_M= $ as select the combination with the highest likelihood. For instance, assume we observe an agent in a 2-arms bendit setup {Right(R),Left(L)}, and that the learning model (in this case the action value) is updated as follows: $Q_{t+1}(c_t) = Q_t(c_t)+ \alpha (r_t-Q_t(c_t))$, and the action probability $P(c_t)$ taken according to the following: $P(c_t|Q_t(L), Q_t(R)) = \frac{\exp(\beta \cdot Q_t(L))}{\exp(\beta \cdot Q_t(L))+ exp(\beta \cdot Q_t(R))}$. Then, given the observed sequences $c_{1:T}$ and $r_{1:T}$, its likelihood can be calculated by: $P(D|M,\theta_M) = \Pi_t P(c_t|Q_t(L),Q_t(R))$. $Q_t$ is determined by the reward sequence $r_{1:T}$ and the equation for $Q_t$ given before. Here it would be easier to use Monte Carlo Method to estimate $\hat \theta_M$.
