[site]: datascience
[post_id]: 62396
[parent_id]: 62391
[tags]: 
There are two ways to perform the PCA: Compute the eigenvalue decomposition of the covariance matrix $\Sigma$ Compute the singular value decomposition of the data matrix $X$ Numerically, you can do both by calling svd() on either of them, as for positive semi-definite matrices (like $\Sigma$ ) svd() gives you the eigenvalue decomposition. There is a difference, though, when it comes to interpreting the result: The singular values in s are the eigenvalues of $\Sigma$ , i.e. the variances along the PCs The singular values in s are the singular values of $X$ , i.e. the square roots of the variances along the PCs In sklearn they go with method 2. Hence they need to square the singular values to compute the coverage. In coursera they go with method 1 so no need to square s . In the slide you show and in the video you linked they just sum the values up. Without having run your code, my guess would be that if you change the line calculated_coverages = ((s ** 2) / (len(s) -1)).cumsum() to calculated_coverages = (s / (len(s) -1)).cumsum() you will get better results. Addendum: On second thought I'm not sure how StandardScaler() impacts the results of the PCA either. When comparing, make sure that it is applied in both your implementation of the PCA and the implementation provided by sklearn (and maybe leave a comment if this mattered or not, pretty please ;)).
