[site]: crossvalidated
[post_id]: 114565
[parent_id]: 114560
[tags]: 
Principal component analysis will provide you with a number of principal components $W$; these components will qualitatively represent the principal and orthogonal modes of variation in your sample. You will use (some) of these $W$ to project your original dataset $X$ to a lower dimensional subspace $T$. This is your new dataset and the PCs are in effect an axis system over which we can represent data $X$ in a compact form. Now, as @RobertKubrick mentions you need to make sure that information from your testing dataset is not "leaked" into your training dataset. If this takes place then you will utilize information that "should be unknown" during your prediction; your error estimates will be wrong. The generalization of your model will suffer. For your case in particular you should do the following: Calculate the principal components $W$s on the training dataset and then utilize the training sample $W$ to reduce the dimensions of the testing dataset. I say this because: if you merged both your training and testing dataset to calculate your PC you will evidently utilize information from the testing set. This is clearly wrong. If you did two independent PCAs you will be comparing data registered on different axes (if anything princ. components are not sign-identifiable so the estimated parameters from them will also have the same issue). The axes over which you project your data should be the same, otherwise you are in a typical " orange-apples situation ". Clearly if you do $k$-fold cross validation, or something similar (eg. jack-knifing) that you will need to calculate new principal components $W$ each time. I.T. Jolliffe's Principal Component Analysis is a standard and great reference on PCA; I would strongly recommend it.
