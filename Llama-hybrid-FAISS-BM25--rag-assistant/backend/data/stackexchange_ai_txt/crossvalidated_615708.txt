[site]: crossvalidated
[post_id]: 615708
[parent_id]: 615531
[tags]: 
You have not told us what you consider a valid kernel, so I can't comment on that. But I can give you a reason why one likes to consider positive semi-definite kernels and not just positive definite ones. Just consider the following kernel: $$ K:\mathbb{R}\times\mathbb{R}\rightarrow\mathbb{R}\quad,\quad K(x,y)=xy.$$ This may be just the canonical inner product on $\mathbb{R}$ but defines a decent real reproducing kernel Hilbert-space (RKHS) and $K$ is its reproducing kernel. The kernel even has a name: linear kernel. But notice that the kernel matrix for the points $x=1$ and $y=2$ is $$ \begin{pmatrix} 1 & 2\\2 & 4\end{pmatrix}$$ hence semi-definite. This is no accident. Consider the canonical feature map of this RKHS. This is the map which associates with a point $x\in\mathbb{R}$ the function (= vector in the RKHS) $k_x$ with the property that $k_x(z)=K(x,z)=xz.$ It is not difficult to see that for any two points $x,y$ the respective functions $k_x$ and $k_y$ are scalar multiples of each other. Since the canonical features span the RKHS, you can conclude that the feature space is one-dimensional and in a one-dimensional space each Gram-Matrix (=Kernel matrix) has at most rank one. This generalizes to feature spaces of arbitrary but finite dimension, where the kernel is defined on an infinite set. If your feature space is $n$ -dimensional you pick $n+1$ points and the according feature vectors will be linearly dependent and their Gram/Kernel Matrix semi-definite. To conclude, insisting on positive definite Kernels, would exclude finite dimensional RKHS, which does not make a lot of sense. And of course if you want all Kernel matrices invertible, on infinite domains of definition, you need a Kernel with infinite dimensional feature space.
