[site]: datascience
[post_id]: 118392
[parent_id]: 55901
[tags]: 
The following is conjecture, not fact. If you look at how much each scalar in the the positional embedding vector changes as a function of position... you'll find that many of the scalars barely change at all. You can visualize this with any positional embedding plot, where the x axis is usually the [512] length of the vector, and the y axis is the position of the token. For example, this image is from Jay Alammar's well regarded "The Illustrated Transformer" Let's try to do this mathematically as well. The implementation of PE's that Jay references is at this Google GitHub repo: https://github.com/tensorflow/tensor2tensor/tree/23bd23b9830059fbc349381b70d9429b5c40a139 Running the function on a PE/WE of length 512 and max sentence length of 128, let's look at how much the final value in the vector actually changes from the first position, to the 64th position, to the final position. Answer: not much. print(signal[0, 0, -1]) print(signal[0, 63, -1]) print(signal[0, 127, -1]) tf.Tensor(1.0, shape=(), dtype=float32) tf.Tensor(0.99998015, shape=(), dtype=float32) tf.Tensor(0.99991935, shape=(), dtype=float32) Ditto for a value 16 steps away from the final location: print(signal[0, 0, -16]) print(signal[0, 63, -16]) print(signal[0, 127, -16]) tf.Tensor(1.0, shape=(), dtype=float32) tf.Tensor(0.9984067, shape=(), dtype=float32) tf.Tensor(0.9935305, shape=(), dtype=float32) I saw elsewhere that BERT's WEs are typically roughly the range of [-2, 2], so adding a 0.007 delta from the PE would not move the WE very much at the -16th position. So what I think is probably happening is that only ~256 of the PE vector's values are actually moving around as a function of the position... the rest are ~constant. Then the learned WE (Transformers don't use prelearned WE like word2vec or glove), figures out to only use the other ~256 elements. So really... it's conceptually a concat. notebook here https://colab.research.google.com/drive/14RGALTsPIYGAuIByXGutK-aYN-PikWzF
