[site]: crossvalidated
[post_id]: 443245
[parent_id]: 
[tags]: 
In the following queuing problem what assumption led the author to assume the probability of each task happening at 1/180

I am going through this algorithms and data structure course which implements a queue DS to simulate a printing queue. Following is the solution described: To model this situation we need to use some probabilities. For example, students may print a paper from 1 to 20 pages in length. If each length from 1 to 20 is equally likely, the actual length for a print task can be simulated by using a random number between 1 and 20 inclusive. This means that there is equal chance of any length from 1 to 20 appearing. If there are 10 students in the lab and each prints twice, then there are 20 print tasks per hour on average. What is the chance that at any given second, a print task is going to be created? The way to answer this is to consider the ratio of tasks to time. Twenty tasks per hour means that on average there will be one task every 180 seconds: 20 / 1 ℎ × 1 ℎ / 60 × 1 / 60 = 1 / 180 For every second we can simulate the chance that a print task occurs by generating a random number between 1 and 180 inclusive. If the number is 180, we say a task has been created. Note that it is possible that many tasks could be created in a row or we may wait quite a while for a task to appear. That is the nature of simulation. You want to simulate the real situation as closely as possible given that you know general parameters. I have doubt in the assumption that probability of each task is 1/180. Shouldn't probability for each task 1/20 since there are 20 tasks in total? It could be the considered average per second rate but how is this probability of each task happening every second?
