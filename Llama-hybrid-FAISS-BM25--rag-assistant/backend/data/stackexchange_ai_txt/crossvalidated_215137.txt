[site]: crossvalidated
[post_id]: 215137
[parent_id]: 
[tags]: 
k-fold crossvalidation and independent models evaluation

I have a question about using k-fold cross-validation. From what I have read, there are several steps for using it. Shuffle my data. Train my model for each fold on training data and test it using testing data. I get k different success-rates for the evaluation of the model so I take a mean and interpret it as the model's average performance. Now for another model I repeat all steps, I get another average performance and I can compare these two models. Am I right? Now comes the tricky part. Do I shuffle the data ahead of evaluation for each model? I've tried searching this and haven't run into any solid mathematical argument. I think that both not-shuffling and shuffling before each model evaluation makes sense. Could you point me to some article?
