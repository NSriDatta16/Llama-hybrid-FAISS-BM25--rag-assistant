[site]: crossvalidated
[post_id]: 34936
[parent_id]: 34823
[tags]: 
As other answers correctly state, the reported probabilities from models such as logistic regression and naive Bayes are estimates of the class probability. If the model were true, the probability would indeed be the probability of a correct classification. However, it is quite important to understand that this could be misleading because the model is estimated and thus not a correct model. There are at least three issues. Uncertainty of estimates. Model misspecification. Bias. The uncertainty is just the everywhere present fact that the probability is just an estimate. A confidence interval of the estimated class probability could provide some idea about the uncertainty (of the class probability, not the classification). If the model is wrong $-$ and face it, it is $-$ the class probabilities can be quite misleading even if the class predictions are good. Logistic regression can get the class probabilities wrong for two fairly well separated classes if some data points are a little extreme. It might still do a fine job in terms of classification. If the estimation procedure (intentionally) provides a biased estimate , the class probabilities are wrong. This is something I see with regularization methods like lasso and ridge for logistic regression. While a cross-validated choice of the regularization leads to a model with good performance in terms of classification, the resulting class probabilities are clearly underestimated (too close to 0.5) on test cases. This is not necessarily bad, but important to be aware of.
