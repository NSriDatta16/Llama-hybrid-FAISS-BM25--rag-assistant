[site]: datascience
[post_id]: 99549
[parent_id]: 98330
[tags]: 
Overall, consider the algorithm as working by (after subsampling etc) choosing each word in turn to be the target word and, for each, defining the context words as (up to) $l$ words either side of the target word, where $l$ is a fixed number. Answering your questions from this perspective: the target word is not chosen from the context window, rather the other way round, context words are defined relative to the target word. it is not that being in the centre is important, rather for each word (in turn considered the target word), the context window effectively defines which words are considered (in some sense) influenced by that target word. The choice of $l$ words either side can be varied, e.g. be made larger or smaller or asymmetric. various research looks at this but there isn't (as far as I know) any conclusively better approach or theoretical basis for what is optimal. the target words aren't chosen "arbitrarily", they can be thought of as the central object, with context words defined relative to them. So (subject to subsampling, dropping rare words etc) every single word in the corpus - and so the vocabulary- is at some point considered a target word. If it weren't, it's ``input" embedding would not be updated by the algorithm and not learn anything (i.e. just stick as its random initialisation). Overall, the word2vec (and GloVe) algorithms effectively factorise co-occurrence statistics (specifically pointwise mutual information ), so each embedding of a word can be thought of as a compressed (dimensionality-reduced) encoding of the distribution of words that occur around it , as learned by the algorithm as it trawls over the corpus. This paper explains how that works. (There are follow up works if of interest).
