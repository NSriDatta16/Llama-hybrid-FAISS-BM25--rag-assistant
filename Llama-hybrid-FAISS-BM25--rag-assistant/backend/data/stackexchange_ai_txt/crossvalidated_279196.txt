[site]: crossvalidated
[post_id]: 279196
[parent_id]: 
[tags]: 
Understanding Deep Belief Networks!

I have implemented Stacked Autoencoder in tensorflow and I was thinking of implementing Deep Belief Networks using Stacked RBM's. I had started reading about DBN's from various websites and through books as well. Though they are structurally similar but its how they work is different. I had examined various codes and pseudo code specially DBN Pseudo code from university of montreal. I noticed that during unsupervised pretraining of the RBM's none of them are trying to minimize the error with an optimizer. In stacked autoencoder I have seen people minimizing the error or the cost function but not in DBN. And yes when it comes to fine tuning that every one was minimizing the error. My question is why not minimizing the error in pre-training, is it because of Gibbs sampling that is being done during pre-training?
