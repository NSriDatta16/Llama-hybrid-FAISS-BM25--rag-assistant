[site]: crossvalidated
[post_id]: 490416
[parent_id]: 
[tags]: 
LogisticRegression with GridSearchCV not converging

I'm trying to find the best parameters for a logistoic regression but I find that the "best estimator" doesn't converge. Is there a way to specify that the estimator needs to converge to take it into account? Here is my code. # NO PCA cv = GroupKFold(n_splits=10) pipe = Pipeline([('scale', StandardScaler()), ('mnl', LogisticRegression(fit_intercept=True, multi_class="multinomial"))]) param_grid = [{'mnl__solver': ['newton-cg', 'lbfgs','sag', 'saga'], 'mnl__C':[0.5,1,1.5,2,2.5], 'mnl__class_weight':[None,'balanced'], 'mnl__max_iter':[1000,2000,3000], 'mnl__penalty':['l1','l2']}] grid = GridSearchCV(estimator = pipe, param_grid=param_grid, scoring=scoring, n_jobs=-1, refit='neg_log_loss', cv=cv, verbose=2, return_train_score=True) grid.fit(X, y, groups=data.groups) # WITH PCA pipe = Pipeline([( ('scale', StandardScaler()), ('pca', PCA()) ('mnl', mnl)]) param_grid = [{'pca__n_components':[None,15,30,45,65] 'mnl__solver': ['newton-cg', 'lbfgs','sag', 'saga'], 'mnl__max_iter':[1000,2000,3000], 'mnl__C':[0.5,1,1.5,2,2.5], 'mnl__class_weight':[None,'balanced'], 'mnl__penalty':['l1','l2']}] grid = GridSearchCV(estimator = pipe, param_grid=param_grid, scoring='neg_log_loss', n_jobs=-1, refit=True, cv=cv, verbose=2) grid.fit(X, y, groups=data.groups) On the first case, the best estimator found is with an l2-lbfgs solver, with 1000 iterations, and it converges. The second one, the best estimator found is with saga solver and l1 penalty, 3000 iterations. I feel it has to do with the solver... but anyways, is there a straightforward way to state that it has to converge to accept it as best?
