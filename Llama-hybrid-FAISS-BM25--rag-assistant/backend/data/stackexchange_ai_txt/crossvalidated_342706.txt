[site]: crossvalidated
[post_id]: 342706
[parent_id]: 
[tags]: 
Number of parameteres of a NN in relation to the inputs

When talking about neural networks, is there any established theorem or common finding, about the relation between number of parameters of the network, and number of inputs (or input features) to the network? Let's say the size of my input files is "d" = 7800, and I have "n" = 8700 samples. That is approximately 68M total input features. I have read in [1] that there exists a 2-layer neural network with p = 2n + d parameters, that can learn any training set. At least that is what I understand from the sentence "(...) 2-layer network that can express any labeling of any sample of size n in d dimensions." In the paper I think that by "learning" they refer to memorizing. Anyhow: Is there any rule like this when it comes to generalization? Or at least, a minimum number parameters that a network should have in order to learn the specific function between the space of the inputs and the label space? References [1] Zhang.C, Understanding deep learning requires rethinking generalization.
