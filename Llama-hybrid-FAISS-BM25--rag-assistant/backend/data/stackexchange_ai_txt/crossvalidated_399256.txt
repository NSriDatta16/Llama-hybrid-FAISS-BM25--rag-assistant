[site]: crossvalidated
[post_id]: 399256
[parent_id]: 347548
[tags]: 
If I read it correctly they're using only times between 9:30-16:00 ( ~510 minuets), dropping just about everything that doesn't conform, then chunking each day up into roughly 300 units per day's trading hours ( ~1.7 minuets of data per sample), and retain the closing price from the last 60 minuets, all before the quoted line... context can be helpful... so if things are adding up then maybe a tap is about 102 seconds, if so five of'em would be about 8.5 minuets of data. However, I could be totally wrong... I've tried to parse that paragraph and can totally see how by the time the author is up-to the quoted line it's very much a dash of this or that , not to pinch too hard at their ego but considering how well referenced the surrounding text is it's totally understandable to be a bit lost with that sentence. There's a solid Khan Academy on standard error of the mean (a.k.a. the standard deviation of the sampling distribution of the sample mean!) which may help in sorting out the what the authors where eluding to. As to how they are normalizing their test data, it's stated more than once in the linked to paper that they are using raw inputs to a network that outputs probability of movement, direction, and magnitude... Well I think that's what they where getting at... They get deeper into the pre-processing in section [3.3] Preprocessing on page 6 where this network's architecture is described in more detail, also see figures 1 and 2 on that page as those show how the data flows through. I may have to come back to this and take a second crack at dissecting their research, but hopefully some of this was able to gain ya some traction.
