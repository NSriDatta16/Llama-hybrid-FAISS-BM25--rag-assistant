[site]: crossvalidated
[post_id]: 376917
[parent_id]: 376906
[tags]: 
I think there is a lot of confusion in this question (caused of course by authors that describe what they think linear regression means very bad/imprecise/up to wrong). First of all we are given some data $(x_i, y_i)_{i=1,...,N}, x_i \in \mathbb{R}^d, y_i \in \mathbb{R}$ and we want to "make sense of it in form of a linear equation" (see below for a precise formulation). Now it may be the case that this model does absolutely not match the data, for example, if $d=1$ then it could be that $y_i = \sin(x_i)$ or so... Nevertheless one could use linear regression in order to write down a (shitty!) model for that but what you are looking for is the following version of linear regression: Assume (some things about the data) then the linear regression model is the bestest model ever. Now we are going to make this precise. First of all we assume that there is a probability space $\Omega$ and random variables $X_i : \Omega \to \mathbb{R}^d$ and $Y_i : \Omega \to \mathbb{R}$ and $\epsilon_i : \Omega \to \mathbb{R}$ and we assume that there are (as you call them) 'true' $\beta \in \mathbb{R}^d, b \in \mathbb{R}$ such that $$Y_i = \beta X_i + b + \epsilon_i$$ (as functions from $\Omega$ to $\mathbb{R}$ ) and we assume that there is a 'true' $\omega_0 \in \Omega$ such that $$x_i = X_i(\omega_0)$$ and $$y_i = Y_i(\omega_0)$$ and the $\epsilon_i$ are independent from the $X_i$ and the $\epsilon_i$ are iid. $\mathcal{N}(0, \sigma^2)$ distributed. These assumptions mean that the data we are confronted with really comes from these random variables and they satisfy some relations. Then we can execute an algorithm in order to find approximations $\hat{\beta}, \hat{b}$ of $\beta, b$ such that when we are confronted with a new, unseen $x$ , the equation $$\hat{y} = \hat{\beta} x + \hat{b}$$ will give the best (concerning some measurement, namely in average) approximation for the 'true' $y$ that belongs to that $x$ . 1) Now we have to ask: What do these people mean when they write $\delta_i$ , $\epsilon_i$ , ...? They for sure do hardly know what the term 'random variable' really means from a mathematical point of view or they know it and jut ignore it, hence, they just use it for any symbol in their mind that is somewhat related to some kind of error. I guess that they mean $$\text{their}~ \delta_i = \hat{y}_i - y_i$$ i.e. given the current parameters $\hat{\beta}, \hat{b}$ , what is the error to the $i$ -th true training answer? This is a very concrete real number, not a random variable and this (well, the sum of the squares of them) is what you minimize in linear regression. When they write that they "minimize" something involving $\epsilon_i$ then we do not know what they mean: these are random variables that we cannot even change!!! How should this be minimized? Hence, I think that you are confused for the right reason: Whatever they write in the context of approximizing $\beta, b$ , they almost always mean $\hat{y}_i - y_i$ . 2) I have not seen such a book yet... I think it stems from the following: either the author does not know something about mathematics and precision (or does not give a sh*t about it) or he/she does not want to exhaust the audience with these (absolutely important) details... However, there are some questions in this direction here on se, see here or here and so forth... (shamelessly referring to questions and answers of myself here but probably you can find many more). 3) What do you mean by residiuals? Are you referring to the random variables $\epsilon_i$ or are you referring to $\hat{\beta}X_i + \hat{b} - Y_i$ ? I hardly doubt that the latter are normally distributed or so because this depends on the distribution on $X_i$ alone and these can have any distribution as long as they are in line with the corresponding $Y_i$ ! 4) Lack/ignorance of mathematical knowledge or they actually want to describe something else I guess... For example: One can analyze confidence intervals (i.e. we want to leave the perspective of one single line and for a fresh new unseen $x$ we want to give lower and upper bounds $y_l, y_u$ such that with abc% probability, $y_l \leq y \leq y_u$ ). Then uncertainty needs to come into play again.
