[site]: crossvalidated
[post_id]: 237577
[parent_id]: 
[tags]: 
How can an autoencoder unroll the swiss roll?

I'm trying to unroll the swiss roll (from 3D to 2D) using an autoencoder, but it keeps getting stuck in local optima: the swiss roll ends up squashed rather than unrolled. It's no better than using PCA, basically. If I just have one hidden layer with 2 units, the autoencoder just projects the swiss roll down to 2D, which squashes it (like PCA), and the decoder can't recover the unrolled version. So I tried to stack 3 hidden layers like so: ouputs (3 units) \ | | decoder hidden 3 (5 units) | | / hidden 2 (2 units) \ | | encoder hidden 1 (5 units) | | / inputs (3D) But for some reason this does not work any better. Since I'm pretty sure the weights could be set manually to make it work fine, I guess this means that training is stuck in a local minimum. I have tried tying the weights ($W_{out} = {W_1}^T$, and $W_3 = {W_2}^T$) and adding $\ell_2$ regularization, but to no avail. Any idea on what I should try next? Edit I'm using the ELU activation function in all hidden layers, and no activation function in the output layer. I've successfully used this same architecture (with more neurons) to compress MNIST, so it works fine with more dimensions.
