[site]: crossvalidated
[post_id]: 531184
[parent_id]: 
[tags]: 
How are percentiles used for value clipping in the quantisation of neural networks?

According to Quantization for Neural Networks webpage, Quantization maps a floating point value $x \in [\alpha, \beta]$ to a b -bit integer $x_q \in [\alpha_q, \beta_q]$ . In practice, the quantization process will have chance to have $x$ that is outside the range of $[\alpha, \beta]$ , thus the quantized value $x_q$ will also be outside the range of $[\alpha_q, \beta_q]$ . If the integer type is signed INTb and $[\alpha_q, \beta_q] = (-2^{b-1}, 2^{b-1}-1)$ , or unsigned UINTb and $[\alpha_q, \beta_q] = (0, 2^b-1)$ , programming languages that have fixed type-precisions will clip the values that are outside the range. There is an approach of value clipping that works based on the percentiles. It is mentioned in Trained quantization thresholds for accurate and efficient fixed-point inference of deep neural networks paper by Jain, Gural, Wu and Dick: IBM's FAQ uses percentile initialization to determine clipping thresholds My question is what is the process of useing percentiles for value clipping? I appreciate the detailed explanation (preferably with an example).
