[site]: datascience
[post_id]: 110883
[parent_id]: 
[tags]: 
expanding 1x1xn global average pooling output to the size of the input map HxWxn

I am trying to recreate the model in this paper : BiSeNetV2 There is one module called Context Embedding Block, in which Global Average Pooling is used to embed global information into the feature maps using a residual connection. In the figure provided in the paper, after global pooling and a 1x1 convolution, it mentions a broadcast operation that I traced back to this idea from another paper. Now, I find myself in front of a relatively easy thing, which is taking a vector of 1x1 feature maps and emulate a feature map of HxW size by repeating the 1x1 element over its entirety. My question is: does it make sense to use Bilinear Upsampling to do so, or will such operation produce a different output from what I have in mind? And moreover, if I were to use Bilinear Upsampling, how should I choose the scale factor in order to accomodate for different input sizes?
