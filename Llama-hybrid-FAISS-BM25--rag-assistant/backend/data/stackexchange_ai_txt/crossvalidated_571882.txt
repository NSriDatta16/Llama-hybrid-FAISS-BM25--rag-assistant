[site]: crossvalidated
[post_id]: 571882
[parent_id]: 571872
[tags]: 
There's no reason to believe that the loss must converge to 0. That might suggest you're overfittingâ€”the capacity of your model is too high, and it's memorizing training instances. For the sake of analogy, consider a normal autoencoder whose latent dimension is smaller than the feature space. You can minimize the reconstruction error, but never get it to 0 (assuming that the data don't actually live on a lower-dimensional manifold). The auto encoder is still usable and of value.
