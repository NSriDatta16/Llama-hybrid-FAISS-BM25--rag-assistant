[site]: datascience
[post_id]: 53611
[parent_id]: 53474
[tags]: 
About the number of layers The reason can be understood by looking at the architecture of a CNN and an LSTM and how the might operated on time-series data. But I should say that the number of layers is something that depends heavily on the problem you are trying to solve. You might be able to solve an ECG classification using few LSTM layers, but for activity recognition from videos you will need more layers. Putting that aside, here's how a CNN and an LSTM might process a time series signal. A very simple signal where after three positive cycles you get a negative cycle. For a CNN to see this simple pattern it takes 4 layers in this example. When a CNN processes a time series input, a convolution outputs does not know about the previous outputs (i.e. they are not connected). However an LSTM can do that just using a single layer as they can remember temporal patterns up to 100s of time steps. Because one output is based on the current input as well as the previous inputs the model has seen. I am not saying this is the only reason, but it is probably one of the main factors why CNNs require more layers and LSTMs don't for time series data. About vanishing gradients and overfitting Vanishing gradient is likely to become a problem within a single layer than across layers. That is when processing many sequential steps the knowledge about the first few steps will likely to disappear. And I don't think sequential models are likely to overfit on time-series data if you regularize them correctly. So this choice is probably more influenced by the architecture/capabilities of the models than by the vanishing gradient or overfitting.
