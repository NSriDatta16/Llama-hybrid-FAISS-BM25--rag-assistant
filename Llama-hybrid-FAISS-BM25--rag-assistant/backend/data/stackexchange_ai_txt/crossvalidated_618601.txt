[site]: crossvalidated
[post_id]: 618601
[parent_id]: 618559
[tags]: 
In addition to @Nuclear Hoagie's nice answer (+1). Finding out that there is no difference between the arms of A and B is fine . Detecting positive differences can actually be quite hard when working in an already optimized environment or setting. To quote Kohavi et al. (2009) Online Experimentation at Microsoft : " Evaluating well-designed and executed experiments that were designed to improve a key metric, only about one-third were successful at improving the key metric! " That said, do note that we often have heterogeneous treatment effects, especially within the context of A/B tests where the concept of an Average Treatment Effect might be ill-defined (for example, if we have mobile & desktop users from different demographics put together). In such cases, looking at a CATE (Conditional Average Treatment Effect) might be more enlightening, Imai & Ratkovic (2013) Estimating treatment effect heterogeneity in randomized program evaluation is considered pretty much a classic if one wants to start on the subject.
