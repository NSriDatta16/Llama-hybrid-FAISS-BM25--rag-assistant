[site]: crossvalidated
[post_id]: 434088
[parent_id]: 434023
[tags]: 
Pragmatically, computation cost is harder to measure, since the cost varies depending on the hardware. Should everyone test their method on a standard machine? Would you measure time or memory? Would you distinguish between full precision ops and half precision ops? Should you test on CPU, GPU, TPU, etc? Of course when it is relevant (robotics, autonomous vehicle, or other "real time" applications), people do indeed report details about computational cost. And outside of these applications, when two algorithms are comparable in performance, if one runs substantially faster, that is often mentioned and accepted as a real advantage. Theoretically, preferring a model with fewer parameters can be justified by appealing to Occam's razor, or Solomonoff 's universal prior, which dominates every other computable prior. For computation time, there is Juergen Schmidhuber's "Speed Prior", although I don't think the mathematical justification for that is nearly as strong. Also it's definitely not as well known.
