[site]: datascience
[post_id]: 55807
[parent_id]: 51214
[tags]: 
The accuracy is varying and not linear, meaning that if the number of components increase, the accuracy will not necessarily increase. This is not unexpected. Choosing the right number of components requires balancing the extra information given by the additional dimensions and the useless noise and redundancy present therein. Even though PCA is a linear transform, and even if you used a linear classifier, the dependence of the classifier performance on the number of components is generally very non-linear. Two options (or alterations thereof) are common: Pick a predefined level of variance to keep (usually 90% or 95%, at least for me) and just stick with it. Think of it as regularization, rather than as a hyper-parameter. Treat it as a hyper-parameter and optimize over a validation set for the number of components to keep $k$ . This is essentially what you are doing already. It's perfectly reasonable to simply choose the $k$ that gave the best performance (on a held-out set).
