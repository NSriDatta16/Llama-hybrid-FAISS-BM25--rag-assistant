[site]: crossvalidated
[post_id]: 301888
[parent_id]: 250517
[tags]: 
In my master thesis I used (among others) a Random Forest, which also uses the bagging technique. Below the part from my thesis related to bagging. You have different models than I had, but the idea is the same. You take subsamples of your training data and fit your model on these subsamples. Each model makes a certain classification per observation in your testing data. Lastly, you make the final classification per observation by checking which class got the most 'votes'. So to answer your questions: -You train your models and then use bagging -There is no real relation between bagging and your models. Bagging is an estimation technique (AFAIK) which can be applied to all type of models. Lastly, I think that XGBoost incorporates bagging automatically (not sure though), so don't forget to check the documentation. Thesis part: Bagging is the application of the bootstrap method applied to high-variance machine learning algorithms. Bagging uses multiple machine learning algorithms in order to improve the accuracy and stability of the individual algorithms. Furthermore, it reduces variance and the probability of overfitting. The CART algorithm for example has a high variance due to the fact that it depends heavily on the training set that is used. A couple of extra outliers in the training sample can change the whole tree. The bagging procedure applied to the CART algorithm is as follows: 1) Create $B$ sub-samples; 2) Fit a CART tree on each different sub-sample to obtain the ensemble of trees $\{CART_b\}^B_1$; 3)Use each individual tree to make a prediction; 4)The final prediction is obtained by taking the average prediction (for regression trees) or to take the value that is predicted the most (for classification trees). By using many different trees, the effects of outliers will be averaged out and hence, the predictions are less volatile than the predictions of a single CART tree.
