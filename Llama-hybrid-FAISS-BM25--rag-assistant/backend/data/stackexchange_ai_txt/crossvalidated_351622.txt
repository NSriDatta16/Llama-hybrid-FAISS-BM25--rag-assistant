[site]: crossvalidated
[post_id]: 351622
[parent_id]: 351609
[tags]: 
Learning to complete the task in only 81 actions is easy, because the underlying policy is also very easy: only choose an action (teleport to location X) if the corresponding location is unsearched (which it can observe directly). The reward contribution of new_location_status provides that. Learning to never hesitate requires some sort of baseline penalty. Note that if it chooses to stay where it is, under the first reward function it pays no travel cost and gets no reward, for a result of 0; under the second reward function, the goal_dist hasn't changed, and so it gains a positive reward for hesitating, which is larger the closer it is to the goal! Learning to complete the task in only 81 traveled distance is hard, because it requires combining information about the current location and the observation, and then requires path planning to not get stuck in any dead ends that require a hop to escape. Let's ignore the path planning problem in the hopes that the RL agent will eventually memorize it through the state values, and just focus on whether or not we're communicating enough information that the model will pick up on close jumps being much better than far jumps. max_travel_distance is, presumably, a constant, but even if it's a variable dependent on the location that won't matter too much (as it's twice as high in the corner, with the maximum distance, as it is in the center, with the minimum distance), and so you can view the reward as simply adding two factors--the distance, with weight $-\alpha$, and whether or not the target is unsearched, with weight $1$. But it seems to me like a sensible value of $\alpha$ is something like $1/2$ or $1/\sqrt{2}$. It should be less than $1$ so that it's worthwhile to travel at all (instead of just being indifferent between doing nothing and the optimal path), but we know that the optimal path doesn't involve any jumps higher than length $1$, and so jumps of length longer than $1$ should be discouraged (while still earning points on each 'optimal' step, such that it's still worth jumping out of a dead end you find yourself in). Also, setting this too high will discourage movement at all; it may be the case that $1/2$ leads to it taking a long time for the agent to escape the region near the center. So try the subgoal-only reward again, but with a more sensible penalty for distance.
