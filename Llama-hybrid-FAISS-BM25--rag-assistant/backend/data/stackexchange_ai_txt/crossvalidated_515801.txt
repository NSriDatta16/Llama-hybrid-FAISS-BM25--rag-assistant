[site]: crossvalidated
[post_id]: 515801
[parent_id]: 
[tags]: 
Bayesian inferenceâ€”interpretation of P(X)

Let's assume we have a simple neural network model for which we want to use Bayesian inference. $X$ - is the data we have seen so far $W$ - is the weights space of our neural network. In order to better understand Bayesian inference, i would like to understand the intuitive meaning of each component. Some of the component have a clear intuitive meaning, but i am not sure regarding $P(X)$ . The Bayes formula is: $$ P(W|X) = \frac{P(X|W) \times P(W)}{P(X)} $$ The components are: $P(W|X)$ - the posterior - This represents the weights distribution of our NN model. $P(W)$ - the prior: This represent the prior probability for each weight. We can assume normal distribution for simplicity $P(X|W)$ : the likelihood. This represents our model prediction i.e. what is the probability of the data give the current weights of the model $P(X)$ : the data probability - ? My question is what is the intuitive meaning of $P(X)$ ? What does it represent? Let's take a simple example: We have dataset of pairs $(x,y)$ which is drawn from $y = \sin(x) + \mathrm{noise}$ . We choose normal distribution as our prior $P(W)$ . We have a neural network that learns this function. In this case, what is the meaning of each component (especially $P(X)$ )?
