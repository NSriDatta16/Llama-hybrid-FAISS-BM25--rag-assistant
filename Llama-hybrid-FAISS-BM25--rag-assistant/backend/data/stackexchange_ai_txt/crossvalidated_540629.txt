[site]: crossvalidated
[post_id]: 540629
[parent_id]: 540624
[tags]: 
This is indeed a reasonable approach from a machine learning perspective, and I did something similar in my Weighted Least-Squares Support Vector Machine implementation (see this paper ) so that the range of hyper-parameter values that you need to search is more compact and the optimal value less dependent on the number of training samples. I suspect the reason it is not as commonly seen in more statistically based models (rather than models from a more machine learning oriented source) is that Bayesian model selection schemes would require the overall loss, rather than the per-pattern loss, as might AIC or BIC. However, different samples have different sampling variations (noise) so you may want to retune $C$ for different training samples, even if they are of the same size (as there is no telling that the value for the first sample of data was not an "outlier" in the distribution of optimal values).
