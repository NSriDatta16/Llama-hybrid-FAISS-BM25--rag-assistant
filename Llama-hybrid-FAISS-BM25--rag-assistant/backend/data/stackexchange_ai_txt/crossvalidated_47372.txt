[site]: crossvalidated
[post_id]: 47372
[parent_id]: 47367
[tags]: 
If you have access to LASSO and your predictors are all numeric then that is a good choice as Peter mentioned. If you have a massive number of predictors as experienced often in fields like marketing - then this can be computationally too expensive. In that case, a tree can be used, but a random forest or gradient boosted regression tree would likely be better choices as variable importance is more robust (for the same reason boosted and bagged trees are expected to be more stable). The party package in R might be another good choice, as the conditional inference trees and associated forests and variable importance measures are purported to be less biased. Party VI Outside of statistics, Googling "Feature Selection" might give you more ideas.
