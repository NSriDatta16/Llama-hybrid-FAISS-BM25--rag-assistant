[site]: crossvalidated
[post_id]: 473220
[parent_id]: 
[tags]: 
Bayesian Statistics: Derivation of the expected value of the posterior predictive distribution

I am reading the Gelman, et. al., book Bayesian Data Analysis and they talk about the expected value of the posterior predictive distribution as follows. However, they did not really seem to derive this in the book itself. I was hoping someone could help fill in the gaps. I am talking about p.41 of section 2.5. The claim is that: $$ \mathrm{E}(\tilde{y} | y)=\mathrm{E}(\mathrm{E}(\tilde{y} | \theta, y) | y)=\mathrm{E}(\theta | y)=\mu_{1} $$ and $$ \begin{aligned} \operatorname{var}(\tilde{y} | y) &=\mathrm{E}(\operatorname{var}(\tilde{y} | \theta, y) | y)+\operatorname{var}(\mathrm{E}(\tilde{y} | \theta, y) | y) \\ &=\mathrm{E}\left(\sigma^{2} | y\right)+\operatorname{var}(\theta | y) \\ &=\sigma^{2}+\tau_{1}^{2} \end{aligned} $$ where $\tilde{y}$ is the predicted distribution for a new data point. The distribtion for $y$ is normal, so conjugacy probably plays a roll here. The thing that was confusing me was how $\mathrm{E}(\mathrm{E}(\tilde{y} | \theta, y) | y)=\mathrm{E}(\theta | y)$ .
