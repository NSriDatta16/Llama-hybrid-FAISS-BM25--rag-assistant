[site]: crossvalidated
[post_id]: 563467
[parent_id]: 
[tags]: 
Training a Neural Network on Two Interrelated Tasks

I'm working on a project where I'm interested in comparing the performance of a Neural Network to human performance on a multisensory perceptual task. The task is quite easy, but I'm more interested in assessing the internal representations of the network. The human task is a multisensory 'rate-categorization' paradigm in which humans are simultaneously presented with visual flicker and audio white-noise flutter at different rates (e.g. 5, 7, 9, 11/second; rates may be congruent between sense or incongruent). At the beginning of each trial, the human is told which sense they will be reporting the rate in - thus, if it the trial is a visual trial, the participant will attempt to correctly choose the rate of the visual stimulus. The idea of the experiment was to provide evidence for a certain computational model which describes how multisensory integration (i.e. where the stimuli have congruent rates) improves performance, as well as some other idiosyncracies. Naturally it's quite easy to tell a human which stimuli to be responding to, but I'm unsure how to implement this with a Neural Network. I'm going to have two input streams ('audio' and 'visual') of binary code representing stimulus ON and OFF. I then need to make sure the NN is reporting the rate of the correct sense. Are there any ideas on how I could do this? I've thought about: i) Using some sort of attention mechanism to guide the Networks attention to the relevant input stream. ii) Having another single node input stream which indicates which sense to attend to. I'm unsure how to implement this in Tensorflow/Keras though, as concatenating layers requires the same shape. iii) Or perhaps somehow through the training process?
