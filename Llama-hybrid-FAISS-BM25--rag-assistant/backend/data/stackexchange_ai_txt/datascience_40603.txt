[site]: datascience
[post_id]: 40603
[parent_id]: 26068
[tags]: 
Both facilitate the input of information (not only from a neighbouring point, as is typical for all networks, but also) from an "earlier" point in the network. What "earlier" means in each case is (one of the) main differences. In case of ResNets this extra input happens by allowing info to "jump" from a point at a given depth to deeper point (layer) in the network (both corresponding to a single original input). If you consider the inputs are stacked vertically on the left of the network and the deeper network layers as proceeding from left to right horizontally, this would correspond to a "skipping" of some info from left to right at a given height. This helps information from the shallower depths of the network keep "alive" at further depths into the network. (Combats vanishing gradients etc) In the case of a RNN this extra input takes the form of allowing a "jump" vertically at a certain horizontal position ie at a given depth (but varying input) This way the processing of a given input at some depth in the network can glean some information, not just from its input "ancestor" but also those of some "neigbouring" inputs. This can be especially useful if the inputs represent "sequences" which are related to one another like words in a sentence or notes in a melody - here the vertical is often considered a "temportal" (sequence) dimension
