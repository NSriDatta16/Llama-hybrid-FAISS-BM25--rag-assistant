[site]: crossvalidated
[post_id]: 483287
[parent_id]: 479835
[tags]: 
It is relatively easy to understand the Bayesian bootstrap in a "large-but-finite" sample space prior. Suppose that $X_i$ takes values in a finite set $\mathcal X$ where the size of $\mathcal X$ is thought of as very large but finite --- say, $\mathcal X$ is the collection of all real numbers which can be represented by floating point numbers on your computer. Clearly, for the vast majority of practical purposes, we lose nothing by restricting attention to distributions on $\mathcal X$ versus distributions on $\mathbb R$ . Since $\mathcal X$ is large but finite, any distribution on $\mathcal X$ is represented by some vector $\pi = (\pi_x : x \in \mathcal X)$ , and we can place a Dirichlet prior on it: $\pi \sim \mathcal D(\alpha, \ldots, \alpha)$ . The posterior distribution of $\pi$ will also be Dirichlet, with shape $\pmb \alpha = (\alpha_x : x \in \mathcal X)$ where $\alpha_x = \alpha$ if $x$ is not observed and $\alpha_x = 1 + \alpha$ if $x$ is observed exactly once. In general we have $\alpha_x = n_x + \alpha$ if we observe ties where $n_x$ is the number of observations equal to $x$ . Now suppose we get our sample of $X_i$ 's and we do not observe any ties. We get the Bayesian bootstrap in the limiting case $\alpha \to 0$ . The values $x$ we do not observe in the sample have $\pi_x \to 0$ in distribution as $\alpha \to 0$ , so those get ignored. This makes it clearer that the posterior does depend on the data --- the data tells us which support points of $\mathcal X$ the posterior will assign non-zero probability to. So the data is actually quite important. Edit Vis-a-vis the comments: The reason Rubin chose this prior was specifically to match Efron's bootstrap to the extent possible. The goal was actually to criticize the bootstrap, as Rubin felt that the prior was absurd. At some point, his attitude seems to have changed, as later work by him and his collaborators use the Bayesian bootstrap. Yes, $n_x = 1$ for the Bayesian bootstrap with probability 1 whenever the truth is continuous. But you can define a Bayesian bootstrap on discrete spaces as well, and there you might have $X_i = X_{i'}$ for some $(i,i')$ in which case the shape associated to the shared value would be $2$ rather than $1$ . The event $X_i = X_{i'}$ is what I would call a "tie." This never happens in theory with continuous distributions, but it happens all the time with real "continuous" data. You can't use the uniform prior with $\alpha = 1$ and get any sensible answer, at least within the context of my motivation. What happens in the large-but-finite $\mathcal X$ setting is that it now depends how big $\mathcal X$ is --- if $\mathcal X$ is very large then the posterior will actually not carry very much information about the distribution, because the posterior will say that the majority of the mass in $\pi$ is still on the elements of $\pi$ which have not been observed. Unlike the Bayesian bootstrap, how severe this is would depend on precisely what $\mathcal X$ looks like. The role of sending $\alpha \to 0$ is that it kills all the values in $\mathcal X$ that we did not observe; you don't get that if $\alpha = 1$ instead. The point is that the "correct" way to think of a $\mathcal D(0,1,1)$ distribution is that $\pi_1 = 0$ almost surely and $(\pi_2, \pi_3) \sim \mathcal D(1,1)$ .
