[site]: crossvalidated
[post_id]: 508399
[parent_id]: 507553
[tags]: 
It reads as if you want to produce a single classification for each variable-length sequence. This is similar to classifying images of variable size, just in 1D instead of 2D. You can feed the signal through a 1D convolutional deep neural network that will use adaptive pooling ( PyTorch / TensorFlow docs) to compress time to a fixed-length representation just before the fully-connected layers/readout layer. This is how Torchvision's CNN implementations deal with variable size image inputs. An alternative, somewhat more complicated approach is to feed your signal piece by piece to a recurrent neural network (e.g., an LSTM). If you read the output only at the last timepoint, you will get a single classification per sequence, regardless of the sequence's length ( a Matlab example ).
