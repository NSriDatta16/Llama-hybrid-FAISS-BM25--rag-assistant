[site]: crossvalidated
[post_id]: 468797
[parent_id]: 468782
[tags]: 
The answer by Thomas Lumley provides a good explanation for why there is a problem. TO put it in other words: If you are fine no matter which model you pick before seeing the data, you are still not necessarily fine if you pick the model based on the data, because models are only fine on average across all possible data realizations (and different ones may have different problems for different data realizations). There's some more examples, e.g.: Switching between t-test and Wilcoxon-rank-sum test based on whether a test for normal residuals is signficant is known the inflate the type I error. This happens even when the residuals are perfectly normal so that either test would be valid. Changing the analysis (random effects model assuming no carry-over or analysis of only the first period) in a cross-over clinical trial depending on whether a test for carry-over is significant is known to inflate the type I error (even if there is in truth no carry-over so that both analyses are valid). Switching between a fixed effects and a random effects meta-analysis model depending on a test for heterogeneity of effects is known to inflate the type I error, even when both analyses would be valid. Doing linerazing transformations based on a test of linearity (e.g. testing for the significance of a quadratic term in a model) inflates the type I error. There's probably a huge number of further examples one can find. This is all very consistent with the statistical literature that has long pointed out that doing post-model-selection inference as if no model-selection had occurred, leads to problematic properties of the inference.
