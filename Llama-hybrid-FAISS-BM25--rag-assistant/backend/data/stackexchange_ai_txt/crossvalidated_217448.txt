[site]: crossvalidated
[post_id]: 217448
[parent_id]: 216142
[tags]: 
To expand on Kilian's answer: Categorical variables have different implications if meant to be inputs or targets . I your case, they are inputs. Apart from the encoding method, you may want to be careful that the (continuous) output is not heavily influenced by them (i.e. a strong change in a categorical input may lead to a bump in the output). The number of hidden units is a difficult task. For testing, go with the architecture you have (4-3-1), although I'd rather tend to something like 4-9-1... In any case, if you want to optimize, you need to iterate and try different hidden layer's sizes. A recommendation is to start with 1 or 2 nodes and iterate upwards, then graph your results and see if there's a point where more nodes do not help anymore. In many time series forecasting problems you want to include previous information. Consider adding the previous' months sales information in some form. Of course you'd have more inputs nodes then. If there are no recurrent neural networks available, you can incorporate this information in normal MLP as normal inputs. Have in mind that the results change in every run, since (most commonly) the weights are randomly(!) initialized, thus (potentially) leading to different results. You can check this easily: Train, train again and compare the results. This can give you a hint about the goodness of other parameters.
