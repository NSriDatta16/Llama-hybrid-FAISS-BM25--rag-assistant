[site]: crossvalidated
[post_id]: 517651
[parent_id]: 
[tags]: 
Adaboost + SVM vs GBM + hange function of SVM

We know that SVM is equivalent to the hange loss function : $$\min\limits_{w,b}\ \Big(1 - y(w\cdot x + b)\Big)^+ + \lambda||w||^2.$$ Then there are two ways of classification boosting based on SVM: AdaBoost with weaker learner SVM; GBM with weak learner SVM (the gradient of hange loss function). First, I believe that AdaBoost is not a special case of GBM (AdaBoost is not to fit the negative gradient of loss function). Then is there any relation between above two algrithms or which one is better?
