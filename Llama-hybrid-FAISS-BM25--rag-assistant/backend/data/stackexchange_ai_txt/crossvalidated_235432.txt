[site]: crossvalidated
[post_id]: 235432
[parent_id]: 215654
[tags]: 
AIC is an estimate of twice the model-driven additive term to the expected Kullback-Leibler divergence between the true distribution $f$ and the approximating parametric model $g$. K-L divergence is a topic in information theory and works intuitively (though not rigorously) as a measure of distance between two probability distributions. In my explanation below, I'm referencing these slides from Shuhua Hu. This answer still needs a citation for the "key result." The K-L divergence between the true model $f$ and approximating model $g_{\theta}$ is $$ d(f, g_{\theta}) = \int f(x) \log(f(x)) dx -\int f(x) \log(g_{\theta}(x)) dx$$ Since the truth is unknown, data $y$ is generated from $f$ and maximum likelihood estimation yields estimator $\hat{\theta}(y)$. Replacing $\theta$ with $\hat{\theta}(y)$ in the equations above means that both the second term in the K-L divergence formula as well as the K-L divergence itself are now random variables. The "key result" in the slides is that the average of the second additive term with respect to $y$ can be estimated by a simple function of the likelihood function $L$ (evaluated at the MLE), and $k$, the dimension of $\theta$ : $$ -\text{E}_y\left[\int f(x) \log(g_{\hat{\theta}(y)}(x)) \, dx \right] \approx -\log(L(\hat{\theta}(y))) + k.$$ AIC is defined as twice the expectation above (HT @Carl), and smaller (more negative) values correspond to a smaller estimated K-L divergences between the true distribution $f$ and the modeled distribution $g_{\hat{\theta}(y)}$.
