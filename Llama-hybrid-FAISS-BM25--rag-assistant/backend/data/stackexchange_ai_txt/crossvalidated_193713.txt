[site]: crossvalidated
[post_id]: 193713
[parent_id]: 
[tags]: 
Estimating unconditional variance in time series

Consider a time series process with a well-defined, finite unconditional variance. Given a realization of the process (a time series) and a model for it, there are at least two ways of estimating the unconditional variance: Use the model to derive an analytical expression of the unconditional variance as a function of error variance and model parameters, and substitute sample estimates for the theoretical quantities.* Use sample variance as if the data were $i.i.d.$** Question: When should 1. be preferred over 2. and vice versa? A partly overlapping question is this one . I simulated some AR(1) processes and discovered that 1. and 2. give very similar results. The kernel densities of the estimated variances over 1000 repetitions are nearly identical, and this holds across different sample sizes and different values of the autoregressive coefficient. I then simulated some GARCH(1,1) processes and found that 2. is a much more robust alternative. While the median estimates due to 1. and 2. over 1000 repetitions were almost the same, 1. had many more extreme values that were apparently due to occasionally poor estimation of the GARCH model parameters. So far it looks as if the simpler alternative 2. fares quite well compared with a more sophisticated alternative 1... *Example: for an AR(1) process, the unconditional variance is $\frac{\sigma^2}{1-\varphi_1^2}$ where $\sigma^2$ is the error variance and $\varphi_1$ is the autoregressive coefficient. To make the estimator operational, $\sigma^2$ and $\varphi_1$ have to be substituted by their sample counterparts due to, say, full maximum likelihood estimator for the AR(1) model. ** That is, $\widehat{\text{Var}}(X)=\frac{1}{n-1}\sum_{t=1}^T (x_t-\bar{x})^2$.
