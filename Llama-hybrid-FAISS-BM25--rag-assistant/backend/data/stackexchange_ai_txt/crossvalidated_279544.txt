[site]: crossvalidated
[post_id]: 279544
[parent_id]: 
[tags]: 
Naive Bayes vs. logistic regression

I'm working with credit scoring models. Here's what I know: Let Y be the binary outcome variable, $Y \in \{0,1\}$ where $Y = 1$ is the outcome of default and $X = (X_{1},...,X_{m})$ be the random vector of predictor variables. A credit score $\text{s}(\textbf{x})$ can be represented as a linear combination of coefficients $\beta_{0},...,\beta_{m}$, where m is the number of predictor variables. i.e. $\text{s}(\textbf{x}) = \beta_{0}+\sum_{j=1}^{m}\beta_{j}$. These parameters can be estimated in various ways such as OLS or MLE. We can often represent $\text{s}(\textbf{x})$ as a log-odds score: $\text{s}(\textbf{x}) = \frac{P(Y=0|\textbf{X}=\textbf{x})}{P(Y=1|\textbf{X}=\textbf{x})}$ In both Naive Bayes and logistic regression, we use the log-odds interpretation. In Naive Bayes , the score can be interpreted using weights of evidence with the assumption that the covariates $X_{j}$ in $\textbf{X}$, a vector of categorical variables and Bayes' Theorem. Hence, $\text{s}(\textbf{x}) = w_{0}+\sum_{j=1}^{m}w(x_{j})$. For logistic regression , we condition on the coefficients when using the log-odds interpretation of the credit score: $\text{s}(\textbf{x}) = \beta_{0}+\boldsymbol{\beta} \cdot \textbf{x} = \text{log}(\frac{P(Y=0|\textbf{X = x},\beta_{0},\boldsymbol{\beta})}{P(Y=1|\textbf{X=x},\beta_{0},\boldsymbol{\beta})})$ My question is -- what is the difference between these two? Surely we can just write the score in Naive Bayes as a linear combination of $\beta$s as well from the definition of a linear scorecard? Is the difference due to a difference in parameter estimation?
