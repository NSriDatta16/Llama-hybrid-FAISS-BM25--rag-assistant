[site]: crossvalidated
[post_id]: 587045
[parent_id]: 
[tags]: 
Understanding feature importance for collinear features with tree-based models

I'm trying to understand how collinearity affects feature importance for tree-based models. My understanding is that tree-based models naturally overcome multicollinearity for the purposes of prediction, but there seems to be conflicting information about how this affects feature importance calculations. In my particular case I am working with a gradient boosting machine (LightGBM) on a classification task. I have features that I know are correlated, and am using SHAP for feature importance. The results seem to show the collinear features being ranked as equally important, so I just want to make sure that I am not missing anything, since I've heard that collinear features can mess with feature importance calculation. Thank you! Relevant prior reading: Permutation-based importance has limitations: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-9-307 https://www.mdpi.com/2079-9292/9/5/761/htm It looks like SHAP assumes independence: Are SHAP values potentially misleading when predictors are highly correlated? SHAP also seems to have a setting for when feature independence can't be assumed: https://github.com/slundberg/shap/issues/1098
