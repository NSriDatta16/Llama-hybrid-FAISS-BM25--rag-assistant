[site]: crossvalidated
[post_id]: 531367
[parent_id]: 451184
[tags]: 
Have a look at SHapley Additive exPlanations , which has a game theoretic basis ("How do you 'optimally' assign credit to different players in a team for the team outcome?" - here variables are treated like players). For xgboost and similar algorithms (e.g. LightGBM), the calculations - that involve considering all orders in which you could give features importance - can be done quite efficiently so that this approach has become quite popular. SHAP values have their flaws (like probably most other current explainability approaches): see e.g. this paper or the work discussed in this podcast episode . More generally, there's a great online book on explainable ML that's worth looking into. Chapters 5.9 and 5.10 deal with SHAP, but there's also chapters on LIME etc., which are of course also well-known approaches.
