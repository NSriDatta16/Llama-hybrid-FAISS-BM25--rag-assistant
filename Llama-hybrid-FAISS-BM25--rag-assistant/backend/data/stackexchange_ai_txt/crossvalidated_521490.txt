[site]: crossvalidated
[post_id]: 521490
[parent_id]: 140537
[tags]: 
I would like to point out one point that the answers above seems to have missed about vanishing gradient in RNN. What people mean by vanishing gradient should be understood differently from the original meaning in DNN. But first we need to make some notation. Let $h_0 \neq 0$ , the recursive formula for Elman Recurrent Neural Network is \begin{align*} h_t &= f_h(U_hx_t + W_hh_{t-1} + b_h) \\ \hat y_t &= f_y(W_y h_t +b_y) \end{align*} For $1\leq t \leq T$ , as $T$ is the total of time steps. Denote $E_t$ as the error between real value $y_t$ and $\hat y_t$ , then the total loss is $L = \sum_{i=1}^T E_t$ . Due to shared weight nature of RNN, finding partial derivative of $L$ w.r.t to $W_{hh}$ obliges you to find $\frac{\partial E_t}{\partial W}$ for each $W$ w.r.t each time-stamp $i . Then if you look at the paper where most of what our current understand about the exploding/vanishing gradient is based upon: This term [ $\frac{\partial E_t}{\partial W}$ at $i$ ] tends to become very small in comparison to terms for which $\tau$ is close to $t$ . This means that even though there might exist a change in $W$ that would allow a, to jump to another (better) basin of attraction, the gradient of the cost with respect to $W$ does not reflect that possibility. What this means when $||W||$ is small, some partial derivative at time-stamp $i$ of some component $E_t$ might get lost due to their time distance. Resulting in a gradient descent algorithm that pays too much attention to the surrounding (usually bumpy) loss surface that not necessarily go down in the long run. So what people usually mean by vanishing gradient in RNN is only by long component that contain distance information of RNN, not the system as a whole.
