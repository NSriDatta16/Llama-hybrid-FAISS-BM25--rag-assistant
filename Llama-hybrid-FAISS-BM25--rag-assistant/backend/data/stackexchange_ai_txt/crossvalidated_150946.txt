[site]: crossvalidated
[post_id]: 150946
[parent_id]: 
[tags]: 
How to estimate variance of classifier on test set?

I have a binary classification task for which I want to compare two different classification methods as well as hyper-parameters for each. I have used k-fold cross-validation (k = 5) to obtain k estimates of my performance metric (average miss-rate over a given range of false positive rate) to give an approximate mean and variance. This revealed method A to be superior to B on average, although the best performance of method B roughly matched the worst performance of method A. For example, method A might have achieved a miss rate of 0.68 (lower is better) with a stddev of 0.02, whereas method B achieved 0.72, also with a stddev of 0.02. For my task, it is standard procedure to report a single performance number on the test set. When I choose the hyper-parameters according to cross-validation, the performance that method A and method B each achieve on the test set are virtually identical. However, I fear this is simply due to the variance of the two methods, and perhaps method A would be better on average if I could sample more training and testing sets. The numbers are quite far from those that I saw in cross-validation, suggesting a mismatch between the training and testing distributions. QUESTION: Is there a principled way to estimate the variance of the classifiers using the testing distribution? I thought of applying the k classifiers from cross-validation to the whole test set (or training new classifiers after re-sampling k training sets by bootstrapping) and looking at the variance of the results. However, I'm concerned that this will be specific to the testing set that I have, not estimate a property of the testing distribution. Perhaps I should divide the testing set into k random partitions? Although these would each be relatively small, would that be inefficient?
