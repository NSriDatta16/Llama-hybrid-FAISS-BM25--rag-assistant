[site]: crossvalidated
[post_id]: 530820
[parent_id]: 530812
[tags]: 
You do not know the population variance. You calculated the sample variance and expect that value to be close to the population variance, but you do not know $\sigma^2$ . Therefore, you may have underestimated the variance. To offset this, we use a test statistic with a heavier tail than the normal distribution. The way the math works out for a test statistic of $\dfrac{\bar{x}-\mu_0}{s/\sqrt{n}}$ , the test statistic is distributed as $t_{n-1}$ . As you get a larger and larger $n$ , you expect to have a tighter estimate of $\sigma^2$ , so if you underestimated the variance, you expect to underestimate by less and less. The tails of the test statistic, therefore, can get lighter. In the limit, as $n\rightarrow\infty$ , you know the population variance, and there is a notion of the $t_n$ distribution converging to $N(0,1)$ as $n\rightarrow\infty$ . For this reason, one might argue that, for a very large (intentionally vague terminology) $n$ , the $z$ -test might be a reasonable approximation of the $t$ -test. I am not sure that I have seen anyone do this, however, and with any modern data science software having t-test tools built in, I see little reason to do this particular approximation. This is why your colleague argues that $t$ -testing is appropriate here, not $z$ -testing.
