[site]: crossvalidated
[post_id]: 237860
[parent_id]: 
[tags]: 
Censoring? Am I training my model on an event that might not have happened yet?

I'm working on a paper at the moment, and I have gone into one of those self-doubt loops/spirals. My supervisors assure me that this is the right way, but i'm sceptical. I am identifying doctors at risk of a negligence complaint. I have doctor demographic data and the details of the complaint. I've built out my features and have an outcome class True/False where that doctor has had a complaint or not had a complaint to train the classifiers(svm, naive bayes, cart, c5). What is not sitting well is that i have the year the doctor came onboard, for example joined=2015. So the likelihood of a complaint for a doctor with 1 year of practice is not the same as one that has been working for 10 years. I am worried that i am training my data on doctors that might be high risk, but they just havent had the chance to be negligent yet. So they are classed as a "False", but give them a couple of years and they would be a "True". My #1 question is: How do i work around this (Censoring i assume)? I know i should be deleting some of the instances where the doctors have joined recently, but Im unsure as to how to determine that cut-off? My #2 question is: The data i have covers complaints for the last 5 years. Therefor I would have thought that my model predicts whether Doctor X is likely to committ an offense within a 5 year period. Is that the correct hypothesis? Thanks B My data: [Years Experiance at complaint, sum of complaints] [0,1] [1,17] [2,20] [3,14] [4,17] [5,24] [6,18] [8,22]... [48,1] Mean: Average experiance at complaint 16years StDev: 10 years experiance
