[site]: datascience
[post_id]: 17491
[parent_id]: 
[tags]: 
Explaining GRU equations from two sources

I really confused about how GRU computation really works. I am not really good at math btw. I read the original source of GRU equations from Cho, et al. (2014), and also a Colah's blog post on the same topic. And somehow, GRU equations at Colah's blog post Understanding LSTM Networks are different than equations in the Cho's original paper Learning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translation . These are the equations from Colah's blog: $$z_t = \sigma(W_z \cdot [h_{t-1},x_t])$$ $$r_t = \sigma(W_r \cdot [h_{t-1},x_t])$$ $$h_t = (1-z_t) \ast h_{t-1} + z_t \ast \tilde{h}_t$$ $$\tilde h_t = tanh(W \cdot [r_t \ast h_{t-1}, x_t]_j)$$ These are the equations from the original paper: $$z_j = \sigma([\mathbf W_z \mathbf x]_j + [\mathbf U_z \mathbf h_{(t-1)}]_j)$$ $$r_j = \sigma([\mathbf W_r \mathbf x]_j + [\mathbf U_r \mathbf h_{(t-1)}]_j)$$ $$h_j^{(t)} = z_jh_j^{(t-1)} + (1-z_j) \tilde h_j^{(t)}$$ $$\tilde h_j^{(t)} = \phi([\mathbf W \mathbf x]_j + [\mathbf U(\mathbf r \odot h_{(t-1)})]_j)$$ Can someone explain if Colah's equations are the same as the equations in the paper version?
