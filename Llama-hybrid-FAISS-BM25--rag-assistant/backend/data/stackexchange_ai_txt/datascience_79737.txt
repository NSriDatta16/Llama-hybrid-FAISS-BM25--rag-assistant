[site]: datascience
[post_id]: 79737
[parent_id]: 
[tags]: 
Can someone explain to me the structure of a plain Recurrent Neural Network?

I have seen pictures of RNNs and LTSMs, and they usually look like this: Here the task is to take a sentence and make a prediction of some sort. What are each of the green squares? Are each of them layers, or does each green square have it's own set of layers and neurons? Additionally, in such a scenario, the input data is the entire sequence of words. When dealing with time series data, would the input be the entire data set since the entire dataset is a sequence? If you have 1million observations, you would need to construct 1million green squares?
