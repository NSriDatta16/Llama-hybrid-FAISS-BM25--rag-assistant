[site]: datascience
[post_id]: 73772
[parent_id]: 72390
[tags]: 
I understood it wrong ,here is the paper which discuss using multiple data set for the same classifier- http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.141.3142&rep=rep1&type=pdf They conclude- " We theoretically and empirically analyzed three families of statistical tests that can be used for comparing two or more classifiers over multiple data sets: parametric tests (the paired t-test and ANOVA), non-parametric tests (the Wilcoxon and the Friedman test) and the non-parametric test that assumes no commensurability of the results (sign test). In the theoretical part, we specifically discussed the possible violations of the testsâ€™ assumptions by a typical machine learning data. Based on the well known statistical properties of the tests and our knowledge of the machine learning data, we concluded that the non-parametric tests should be preferred over the parametric ones." I recently learned about 5x2cv paired t test procedure to compare the performance of two models. Please refer below- http://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_5x2cv/ It Implements the 5x2cv paired t test proposed by Dieterrich (1998) to compare the performance of two models.
