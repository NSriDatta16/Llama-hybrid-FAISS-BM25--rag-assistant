[site]: crossvalidated
[post_id]: 311428
[parent_id]: 
[tags]: 
Probabilistic model without predictors (inspired by Bayesian regression)

I have an exercise where I have to use a probabilistic model to fit the data. Here is the original problem (Problem 1) : (Problem 2 with solution) At the lecture we learnt to derive Bayesian linear regression solution. We have dataset $\textbf{x} = (x^{(1)},...,x^{(N)}) $ and $\textbf{y} = (y^{(1)},...,y^{(N)}) $, parameters $\textbf{w} = (w_0,w_1)^T $. We also have $\phi(x) = (1,x)^T$ and $ y = h^w(x) + \epsilon$ with $\epsilon \sim N(0,\sigma^2)$ Then we have the Bayesian solution : The question is how to use insight from Problem 2 to solve Problem 1. As I realize problem 1 is just problem 2 version without any predictors $x$ (in which $\mu = w_0$ and $e^{(i)} = \epsilon$ ). My best guess would be $\mu = y^{(N+1)} = \text{Mean}(y^{(i)})$ but I'm not sure how to prove it
