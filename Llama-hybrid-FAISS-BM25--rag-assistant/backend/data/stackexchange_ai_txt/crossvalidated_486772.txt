[site]: crossvalidated
[post_id]: 486772
[parent_id]: 486715
[tags]: 
One method you could try is Isolation Forsts. The method works by randomly selecting variables, then randomly selecting a cut-off point for the selected variable and doing this until observations are all “isolated”. This can be repeated to het an ensemble of trees. The easier to separate an observation the more likely (according to this method) the observation is to be an outlier. The resulting trees can be used to give each observation an anomaly score, with values close to one more likely to be anomalies. Below is text copied from https://en.m.wikipedia.org/wiki/Isolation_forest . The authors took advantage of two quantitative properties of anomalous data points in a sample: Few - they are the minority consisting of fewer instances and Different - they have attribute-values that are very different from those of normal instances Since anomalies are "few and different", they are easier to “isolate” compared to normal points. Isolation Forest builds an ensemble of “Isolation Trees” (iTrees) for the data set, and anomalies are the points that have shorter average path lengths on the iTrees. One thing to consider is the curse of dimensionality applies here, again from the Wikipedia article: One of the main limitation to standard, distance-based methods is their inefficiency in dealing with high dimensional datasets: The main reason for that is, in a high dimensional space every point is equally sparse, so using a distance-based measure of separation is pretty ineffective. Unfortunately, high-dimensional data also affects the detection performance of iForest, but the performance can be vastly improved by adding a features selection test like Kurtosis to reduce the dimensionality of the sample space.
