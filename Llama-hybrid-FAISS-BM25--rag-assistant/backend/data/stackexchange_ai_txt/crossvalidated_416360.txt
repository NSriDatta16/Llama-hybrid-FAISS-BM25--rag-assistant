[site]: crossvalidated
[post_id]: 416360
[parent_id]: 
[tags]: 
KL divergence of a uniform prior and a custom posterior

So I was reading the Google's paper on VQ-VAE and have stumbled upon the derivation of KL divergence of the uniform prior and the given distribution: $$q(z=k \mid x)=\left\{\begin{array}{ll}{1} & \text{for } k=\operatorname{argmin}_j \left\|z_e(x)-e_j\right\|_2 \\ 0 & \text{otherwise}\end{array}\right.$$ In the paper it is stated that the KL divergence of the distributions is equal to $\log K$ . I understand that the KL is constant but how the $\log K$ derived is pretty unclear to me. Also if the $q(z|x)$ is a one-hot vector then how the Kullbackâ€“Leibler distance is even calculated if the distribution contains zero elements. I know we can smooth the distribution bu still. Here is the link to the paper: arxiv.org/pdf/1711.00937 .
