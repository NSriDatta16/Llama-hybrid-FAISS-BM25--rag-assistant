[site]: crossvalidated
[post_id]: 490808
[parent_id]: 
[tags]: 
Do we need to truncate test dataset for seq2seq LSTM?

I am running a summarization model which uses a seq2seq biLSTM with an attention mechanism. It is a standard practice to truncate the input dataset during training to 400 - 500 tokens. My question is, during generation on the test dataset (or validation dataset), do I need to truncate that dataset as well?
