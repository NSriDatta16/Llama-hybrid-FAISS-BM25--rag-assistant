[site]: crossvalidated
[post_id]: 391567
[parent_id]: 
[tags]: 
Effects of class imbalance on nn batch training

Say I have a binary classification task, where the positive class (1) is only 1% of the whole data set. Intuitively I can understand why this could be bad for the classifier as the model may learn to just predict everything to be the negative class (0). From my experience and online search results, there seems like to be multiple ways dealing with this (also depends on your model and your evaluation metric. I found gradient boosting e.g. xgboost etc actually did not perform too bad with imbalanced data, but neural net seems like would get affected a lot more). For example, upsampling the positive class or downsample the negatives, or use class weights which penalize the misclassification on the minority class more. I'm trying to build a neural network model (cnn) on this data set, I did not do any sampling in my approach. I used class weights and performed stratified cross validation so each train set and validation fold contains the same percentage of positive class. My question is, I'm doing batch training (e.g. with batch size 128), most of the batches would only contain the negative class. How would it affect model learning? Can someone give an intuitive explanation or with equations? I would guess it's harmful to the model training as in many batches the model only sees negative class, which would lead the model params biased towards class 0 and eventually slows down training if it does not make training much more difficult. If it's the case, I think what I could do is to make sure each batch contains some percentage of positive class by just random sampling.
