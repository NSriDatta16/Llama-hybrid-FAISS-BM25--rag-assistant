[site]: crossvalidated
[post_id]: 466278
[parent_id]: 
[tags]: 
Machine Learning Feature Importance Method Disagreement (SHAP)

I am interested in reasons as to why different feature importance methods might give different feature rankings. In particular, Shapley values vs other methods such as weight/gain from OOB score. Consider the example below using the California house price dataset. Here for illustrative purposes I have fitted a gradient boosting model. Please note that this example is illustrative, and I am interested in how the two methods could disagree assuming that the gradient boosting model is tuned with no bias/variance issues. The feature importance calculated when constructing the gradient boosting model is given below: However the Shapley values calculated give different feature ranking all together. My thinking is that it is due to the different calculation methods. If the Shapley value of a feature is calculated as the average change in the prediction that a coalition (subset of features) receives when a feature is added (i.e. weighted and summed over all possible feature value combinations), then perhaps it should not agree with other feature importance methods (as it is averaged over all feature combinations). Does anyone know why feature importance calculation methods would not agree? Any literature on this topic people can provide would be greatly appreciated.
