[site]: crossvalidated
[post_id]: 327592
[parent_id]: 
[tags]: 
Convolutional Neural Network Scale Sensitivity

For the sake of example, lets suppose we're building an age estimator, based on the picture of a person. Below we have two people in suits, but the first one is clearly younger than the second one. (source: tinytux.com ) There are plenty of features that imply this, for example the face structure. However the most telling feature is the ratio of head size to body size : (source: wikimedia.org ) So suppose we've trained a CNN regression to predict the age of the person. In a lot of the age predictors that I've tried, the above image of the kid seems to fool the predictions into thinking he's older, because of the suit and likely because they rely primarily the face: I'm wondering just how well can a vanilla CNN architecture infer the ratio of head to torso? Compared to a regional RCNN, which is able to get bounding boxes on the body and head, will the vanilla CNN always perform worse? Just before global flattening in the vanilla CNN (i.e. just after all convolutions), each output has a corresponding receptive field, which should have a sense of scale. I know that faster RCNN exploits this by making bounding box proposals exactly at this stage, so that all prior convolutional filters automatically train to all scales. So, I would think that the vanilla CNN should be able to infer the ratio of head to torso size? Is this right? If so, is the only benefit of using a faster RCNN framework to exploit the fact that may have been pre-trained on detecting people?
