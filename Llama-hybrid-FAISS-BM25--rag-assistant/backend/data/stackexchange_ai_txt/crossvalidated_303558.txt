[site]: crossvalidated
[post_id]: 303558
[parent_id]: 287773
[tags]: 
In addition to the good points covered by the answer above, it is worth going back to basics and considering the needed complexity of the predictive signal. It is amazing how often an additive model competes well with regard to predictive discrimination. In other words, non-additive effects (interactions) are often weaker than main effects. In many problems, lack of fit is caused more by nonlinear effects of predictors than by non-additive effects. In the neural network framework this relates to how many layers are really needed for the problem at hand. The use of prior knowledge results in better predictions. For example, one often should not "waste parameters" by allowing interactions to be as important as main additive effects. A Bayesian modeler would put a prior on interaction effects that reflects this. Putting more emphasis on effects likely to be stronger results in less overfitting and better predictions.
