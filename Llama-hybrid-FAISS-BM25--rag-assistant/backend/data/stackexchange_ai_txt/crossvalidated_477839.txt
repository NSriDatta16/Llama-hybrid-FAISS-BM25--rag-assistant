[site]: crossvalidated
[post_id]: 477839
[parent_id]: 147587
[tags]: 
The term "non-parametric" is a bit of a misnomer, as generally these models/algorithms are defined as having the number of parameters which increase as the sample size increases. Whether a RF does this or not depends on how the tree splitting/pruning algorithm works. If no pruning is done, and splitting it based on sample size rules (e.g. split a node if it contains more than 10 data points) then a RF would be non-parametric. However, there are other "parametric" methods like regression, which become somewhat "non-parametric" once you add in feature selection methods. In my view the process of feature selection for linear/logistic regression is very similar to tree based methods. I think a lot of what the ML community has done is fill in the space of how to convert a set of "raw inputs" into "regression inputs". At the basic level, a regression tree is still a "linear model" - but with a transformed set of inputs. Splines are in a similar group as well. Regarding assumptions, the ML models are not "assumption free". Some of the assumptions for ML would be things like "validation error is similar to the error for a new case" - that is an assumption about the distribution of the errors! The choice of how to measure "error" is also an assumption about distribution of errors - eg using squared error vs absolute error as the measure you are minimising (eg normal vs laplace distribution). Whether to treat/remove "outliers" is also a distributional assumption (eg normal vs cauchy distribution). I think instead that ML output just doesn't bother checking if "underlying assumptions" are true - more based on checking if the outputs "look good/reasonable" (similar to the IT culture of testing...does input+process=good output?). This is often better because "modelling assumptions" (eg the error terms are normally distributed) may not uniquely characterise any algorithm. Further the predictions might also not be that different if we change assumptions (eg normal vs t with 30 degrees of freedom). However, we see that the ML community has discovered a lot of the practical problems that statisticians knew about - bias-variance trade off, the need for large datasets to fit complex models (ie regression with n One aspect that I think ML has done better is the notion of reproducibility - a good model should work on multiple datasets. The idea of test-train-validate is a useful way to bring this concept to the practical level.
