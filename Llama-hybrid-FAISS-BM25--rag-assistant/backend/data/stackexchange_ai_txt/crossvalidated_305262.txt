[site]: crossvalidated
[post_id]: 305262
[parent_id]: 
[tags]: 
Why normalize data to the range [0,1] in autoencoders?

When people use autoencoders, they usually normalize the data such that the values are normalized to the range [0,1]. Why is that? Why not use zero-mean unit variance normalization for example? I read on a Quora answer that this range gives you more choice of loss functions, but I don't really understand why. Any ideas?
