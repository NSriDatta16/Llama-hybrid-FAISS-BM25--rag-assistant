[site]: datascience
[post_id]: 90809
[parent_id]: 
[tags]: 
What is the best way (cheapest / fastest option) to train an model on massive dataset (400GB+, 100m rows x 200 columns)?

I have a 400GB data set that I want to train a model on. What is the cheapest method to train this model? The options I can think of so far are: AWS instance with massive RAM and train CPU (slow, but instances are cheap). AWS instance with many GPUs and using Dask + XGBoost to distribute (fast, but expensive instances, and I don't even think there would be an instance large enough to handle). I have just assumed XGBoost is the best package since its tabular data, but if another gradient boosted tree package would be better at handling this, that would be acceptable as well. Any help would be greatly appreciated.
