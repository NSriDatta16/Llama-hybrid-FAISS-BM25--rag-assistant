[site]: datascience
[post_id]: 85690
[parent_id]: 76175
[tags]: 
The approach itself looks fine. The conversion of words to embeddings is just necessary step in a image captioning exercise & since we have the sequence numbers already in your use case, they are in a sense are already encoded and ready to be used (as in a time-series non NLP prediction, like stock market predictions etc). By itself that should not make a difference. You may want to re-look at your labelled data to see if it has enough information and features for the model to train itself. Also see if the loss function can be more optimized
