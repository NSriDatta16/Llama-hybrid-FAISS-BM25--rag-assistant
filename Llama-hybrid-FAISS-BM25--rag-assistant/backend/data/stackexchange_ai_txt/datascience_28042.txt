[site]: datascience
[post_id]: 28042
[parent_id]: 28022
[tags]: 
Parse using regular expressions I work a project where we get thousands of data files per day from on the order of 10 different systems. The filenames are all a jumble, and have a tendency to change over time. This is a job for regular expressions. The primary thing I use to organize is functional groupings. I simply use a small set of indicators common to each file set. In my case, the server that aggregates and sends the data files to me appends its name to the filename (zs2101, or something like that). So then I search file names for this limited set of regular expressions (I am currently using 20). In your situation, it seems like the client name in the file headers is something you could search for an use for the base of your organization Then, I divide files by date of generation of the data. Each file comes with a timestamp. When I read files into my archive, I find the date field in every file and convert it to a standard format, and then change the filename to reflect that standard format. Now I build a directory tree for each server name, organized by date (a folder for each year, and a sub-folder for each month, in my case). My recommendation is that you use regular expressions to organize your data. These are deterministic in the sense that if you receive a strange file header or malformed filename, you know where it will end up (in my case, there is an 'Undetermined' folder that accepts everything that doesn't match a server or date regex). The problem with k-means is that if you get something you didn't plan for, it can be pretty hard to tell where the clustering algorithm will put it, leading to lost data. Good luck, hope this helps.
