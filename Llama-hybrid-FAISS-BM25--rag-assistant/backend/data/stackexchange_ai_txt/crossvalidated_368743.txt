[site]: crossvalidated
[post_id]: 368743
[parent_id]: 
[tags]: 
Can I use ReLU in autoencoder working with negative inputs?

I am using autoencoders to compress data and I see all examples on the internet using ReLU activation with image datasets. However, I am planning to use a dataset that has negative values and I was just thinking about how ReLU outputs zero if any Z values are negative. I was thinking that RELU would not be a good activation function to get my latent encoded data because of this. Would using an activation function such as sigmoid or tanh as the activation for my encoded vector be better so I always get a value?
