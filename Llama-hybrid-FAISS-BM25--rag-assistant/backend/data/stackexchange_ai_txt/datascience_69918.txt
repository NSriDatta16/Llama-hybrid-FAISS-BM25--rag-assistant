[site]: datascience
[post_id]: 69918
[parent_id]: 
[tags]: 
Reward(t) vs. Reward(t+1) ? Reinforcement Learning, Q-learning

In "Reinforcement Learning, An Introduction; Richard S. Sutton and Andrew G. Barto" is written on page 70: 3.1 The Agent–Environment Interface At each time step t, the agent receives some representation of the environment’s state State(t), and on that basis selects an action A(t). One time step later , in part as a consequence of its action , the agent receives a numerical reward R(t+1) , and finds itself in a new state S(t+1). [4] We use R(t+1) instead of R(t) to denote the reward due to A(t) because it emphasizes that the next reward and next state, R(t+1) and S(t+1), are jointly determined. Unfortunately, both conventions are widely used in the literature. In case of trading a reward for opening a SELL(-1)/NONE(0)/BUY(1) could look like this: reward = self.direction * (self.price[t+1] - self.price[t]) We step t+1 into the future... However, i often see people doing it different: Q-Trader for t in xrange(l): action = agent.act(state) # sit next_state = getState(data, t + 1, window_size + 1) reward = 0 if action == 1: # buy agent.inventory.append(data[t]) print "Buy: " + formatPrice(data[t]) elif action == 2 and len(agent.inventory) > 0: # sell bought_price = agent.inventory.pop(0) reward = max(data[t] - bought_price, 0) total_profit += data[t] - bought_price print "Sell: " + formatPrice(data[t]) + " | Profit: " + formatPrice(data[t] - bought_price) He's clearly using the current Price(t) for OPEN/CLOSE. And for the CLOSE that makes sense to me, as he's closing the trade NOW at Price(t) and NOT at Price(t+1), so the reward is Price(current)-Price(open)... I somehow get confused because in one case the next value is used, in the other case the current. For opening a position the first way (t+1) makes more sense to me "self.direction * (self.price[t+1] - self.price[t])" as we are interested in the next value. But for closing it, we need the current value. Question: Can i use both versions, OPEN rewards are R(t+1) and CLOSE rewards are R(t)? Is this a valid approach? Thanks!
