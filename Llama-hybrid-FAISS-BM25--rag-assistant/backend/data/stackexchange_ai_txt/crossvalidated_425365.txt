[site]: crossvalidated
[post_id]: 425365
[parent_id]: 425364
[tags]: 
It seems to me that scikit-learn's RandomForestRegressor and DecisionTreeRegressor use just one feature at each node in order to split the data set and decide the subsequent node for future samples. Is this true? Yes, this is pretty much standard. They would basically evaluate all features to determine the best one regarding node variance (or equiv. MSE) and then pick one. The number of features and - in case not all features are considered - the features which are to be considered while looking for the best split at each node, are randomly chosen. That would mean that the overall best split won't occur at some nodes, because the respective feature possibly wasn't even considered. Is this true? Yes, when I mentioned "all features" above, I was referring to all features "available" for that tree. More specifically regarding "That would mean that the overall best split won't occur at some nodes" -- sure, that's right. If it weren't, there would be no point in a "Random" Forest since all trees would be the same and you would likely overfit terribly. 2.2. I read Breiman's CART (1984) but I can't remember the random consideration of features at each node being mentioned there. CART is a decision tree algorithm. The randomization is per Random Forest algorithm. I.e., the random aspect comes from the fact that the trees don't have access to all features but only a random subset. If (1.) is true: Are there any regression tree algorithms which make splits based on multiple features? (Not considering features for splits, but setting the thresholds for splits based on multiple features) Not sure about this one, but how would you split on multiple features? That's essentially like 2 or more splits then, so you can do it consecutively.
