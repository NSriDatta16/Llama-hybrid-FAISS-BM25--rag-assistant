[site]: crossvalidated
[post_id]: 629679
[parent_id]: 
[tags]: 
Null hypothesis for ANOVA for regression

Context: I know the "classic" ANOVA framework: we have $n$ groups and $k$ measurements of a variable $X$ for each group. Let $µ_1$ , $µ_2$ , ..., $µ_n$ be the mean of $X$ in each group. Hypothesis $H_0$ : $µ_1 = µ_2 = ... = µ_n$ Hypothesis $H_1$ : $\exists i,j, \rm{\ such\ that \ }µ_i \neq µ_j$ Based on the effective samples, we can compute the value of " $F$ ", which is finally bigger or smaller than a certain threshold (calculated with $k$ , $n$ , and for example $\alpha=5\%$ ). This allows us to reject or not reject $H_0$ . This is the classic ANOVA. I often read about "ANOVA for regression", for testing the linearity / the validity of a linear model, but the hypothesis $H_0$ is rarely clearly described. Question : how can we use the framework described in the first paragraph (which is about testing whether $µ_1 = µ_2 = ... = µ_n$ or not) to test the relevance of a linear model? What is the $H_0$ hypothesis in this case? Concrete example: let's say we want to show linearity between X and Y, here is some input data: For $x_0 = 2$ , we have 4 measurements $y_{0,0} = 3.12$ , $y_{0,1} = 3.27$ , $y_{0,2} = 3.45$ , $y_{0,3} = 3.28$ . For $x_1 = 3$ , we have 4 measurements $y_{1,0} = 4.15$ , $y_{1,1} = 4.37$ , $y_{1,2} = 4.01$ , $y_{1,3} = 3.98$ . For $x_2 = 4$ , we have 4 measurements $y_{2,0} = 5.35$ , $y_{2,1} = 5.58$ , $y_{2,2} = 5.11$ , $y_{2,3} = 5.67$ . We can do a linear regression $y = a + bx$ , find $a$ , $b$ , $R^2$ , etc. But how to use "ANOVA for regression" in this example? Here an hypothesis like $H_0$ : $µ_1 = µ_2 = µ_3$ does not make any sense.
