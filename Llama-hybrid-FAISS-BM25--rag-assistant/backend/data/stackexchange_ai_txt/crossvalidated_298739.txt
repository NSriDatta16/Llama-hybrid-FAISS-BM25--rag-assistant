[site]: crossvalidated
[post_id]: 298739
[parent_id]: 202014
[tags]: 
There are counter examples to Bayesian reasoning, see Counterexamples to a likelihood theory of evidence . In this, the authors make the point that when comparing two hypotheses, neither one represents the data (i.e., the truth). In CHALLENGES TO BAYESIAN CONFIRMATION THEORY It is written that, "Given this history of competing traditions in induction and confirmation rising and falling over the centuries, it seems only prudent to expect that the Bayesian approach will recede from its present prominence and once again be merely one of several useful instruments for assessing inductive inference relations. The goal of this chapter is to review the weaknesses recounted in the literature that may drive this decline." Correct me please if I am wrong, but my impression is that perhaps if one has a model that needs a prior to obtain acceptable results, one might get better results faster by spending the same time to find a different model that describes the data accurately without the need to assume a prior. For example, if $s\approx\frac{1}{2}a t^3$ doesn't seem to work well, we could spend our time adjusting the results with a prior, or, we could merely search for and find how distance is covered from a motionless starting point when uniformly accelerated $s\approx\frac{1}{2}a t^2$. Note the approximately equal sign. If that isn't good enough then we would not enforce a prior, just Lorentz transform it into a more accurate, relativistic form. Now, this is perhaps a purist approach. However, even if our reasoning is perfectly accurate a priori , we still need $n$-tuple post hoc validations before anyone will believe it. That is the Catch-22 . Bayesian inference lacks the assurances that only $n$-tuple post hoc validations can provide, and, even post hoc reasoning is only temporizing, whilst we seek the Black Swan .
