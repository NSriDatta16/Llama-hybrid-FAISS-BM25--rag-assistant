[site]: stackoverflow
[post_id]: 1862929
[parent_id]: 1209574
[tags]: 
Has arbitrary-precision arithmetic affected numerical analysis software? I feel that most numerical analysis software keeps on using the same floats and doubles. There are several unfortunate reasons that arbitrary-precision (ap) is not used more extensively. Lack of support for important features: missing values for NaN/Infinities, no complex numbers or special functions, lack or buggy implementation of rounding modes (round half-even not implemented in GMP), lack of handlers for important events (loss of significant digits, overflow, underflow...ok,this isn't even implemented in most standard libraries). Why is this important ? Because without that you must invest much energy to formulate your problem in arbitrary precision (ever written a complex number library or special functions in ap ?), you can't reproduce your double result because ap lacks the features you need to track the changes. 99,9% of all programmers aren't interested in numerics at all. One of the most asked question here is: "Why is 0.1+0.1 NOT 0.2 ???? HELP !!!" So why should programmers invest time to learn a specific ap implementation and formulate their problem in it ? If your ap results diverge from the double results and you have no knowledge of numerics, how do you find the bug ? Is double precision too inexact ? Has the ap library a bug ? WHAT IS GOING ON ?! Who knows.... Many numeric experts who does know how to compute discourage the use of ap. Frustated by the hardware implementations of FP they insist that reproducability is anyway "impossible" to implement and input data has almost always only few significant digits. So they mostly analyze the precision loss and rewrite the critical routines to minimize it. Benchmark addiction. Wow, my computer is FASTER than others. As the other commentators rightly remarked, ap is much slower than hardware supported floating-point datatypes because you must program it with the integer datatypes per hand. One of the imminent dangers of this attitude is that the programmers, totally unaware of the problems, choose solutions who spit out totally impressive nonsense numbers. I am very cautious about GPGPU. Sure, the graphic cards are much, much faster than the processor, but the reason for that is less precision and accuracy. If you use floats (32bit) instead of doubles(64bit), you have much less bits to compute and to transfer. The human eye is very fault-tolerant, so it does not matter if one or two results are off-limits. Heck, as hardware constructor you can use imprecise, badly rounded computations to speed up your computations (which is really ok for graphics). Throw off those pesky subnormal implementation or rounding modes. There is a very good reason why processors aren't so fast as GPUs. I can recommend William Kahans page link text for some information about the problems in numerics.
