[site]: crossvalidated
[post_id]: 268433
[parent_id]: 218752
[tags]: 
In addition to @Bhagyesh_Vikani: Relu behaves close to a linear unit Relu is like a switch for linearity. If you don't need it, you "switch" it off. If you need it, you "switch" it on. Thus, we get the linearity benefits but reserve ourself an option of not using it altogther. The derivative is 1 when it's active. The second derivative of the function is 0 almost everywhere. Thus, it's a very simple function. That makes optimisation much easier. The gradient is large whenever you want it be and never saturate There are also generalisations of rectified linear units. Rectified linear units and its generalisations are based on the principle that linear models are easier to optimize. Both sigmoid/softmax are discouraged (chapter 6: Ian Goodfellow) for vanilla feedforward implementation. They are more useful for recurrent networks, probabilistic models, and some autoencoders have additional requirements that rule out the use of piecewise linear activation functions. If you have a simple NN (that's the question), Relu is your first preference .
