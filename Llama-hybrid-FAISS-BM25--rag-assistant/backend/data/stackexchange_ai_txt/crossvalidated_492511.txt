[site]: crossvalidated
[post_id]: 492511
[parent_id]: 492163
[tags]: 
Even with only 100 observations, then assuming that the data are missing at random or missing completely at random, it is likely that a pricipled approach to missing data such as multiple imputation will provide much better resuts that removing rows/columns or any kind of single value imputation. The general approach to multiple imputation is: create several complete datasets, let's say $m$ , using whatever multiple imputation alogorithm you choose. A common rule of thumb is that you set $m$ to be the average percentage of missing values in the dataset. perform the glm model on each complete dataset. That is, you would run $m$ models. pool the results of the analyses. With a glm regression model you would simply average all the estimates of interest to find the pooled estimate and use Rubin's rules, which incorporate uncertainty both within, and between, imputations to compute standard errors.
