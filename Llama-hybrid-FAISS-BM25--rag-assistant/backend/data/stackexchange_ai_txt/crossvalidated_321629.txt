[site]: crossvalidated
[post_id]: 321629
[parent_id]: 318786
[tags]: 
Going off of your NIPS workshop link, Yee Whye Teh had a keynote speech at NIPS on Bayesian Deep Learning (video: https://www.youtube.com/watch?v=LVBvJsTr3rg , slides: http://csml.stats.ox.ac.uk/news/2017-12-08-ywteh-breiman-lecture/ ). I think at some point in the talk, Teh summarized Bayesian deep learning as applying the Bayesian framework to ideas from deep learning (like learning a posterior over the weights of a neural network), and deep Bayesian learning as applying ideas from deep learning to the Bayesian framework (like deep Gaussian processes or deep exponential families). There are of course ideas that straddle the line between the two concepts, like variational autoencoders. When most people say Bayesian deep learning, they usually mean either of the two, and that's reflected in the accepted papers at the workshop you linked (along with the workshop the previous year). While the ideas go back to Neal's work on Bayesian learning of neural networks in the 90's ( http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.446.9306&rep=rep1&type=pdf ), and there's been work over the years since then, probably one of the more important recent papers would be the original variational autoencoder paper ( https://arxiv.org/pdf/1312.6114.pdf ).
