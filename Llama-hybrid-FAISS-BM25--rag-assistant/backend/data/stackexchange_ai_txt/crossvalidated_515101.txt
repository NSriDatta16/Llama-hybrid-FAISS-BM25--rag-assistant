[site]: crossvalidated
[post_id]: 515101
[parent_id]: 515041
[tags]: 
Stephan makes good, although brief, points. Assuming the clinician is giving a probability and not a prediction of the form "This person will die, this person will not" there are a couple ways to compare the two models. Brier Score: The brier score is a quadratic scoring rule. It is the mean squared error between the observed outcomes and the probabilities each method assigns to those outcomes. Lower is better in this case. Calibration: If you assign a probability of 75% to 100 subjects, then 75 out of the 100 subjects should get the outcome. A proabbalistic model which is poorly calibrated gives probabilities which are either too extreme or not extreme enough. Neither are preferable. You want a well calibrated model. Discrimination: See area under the receiver operating characteristic curve. This metric can be insensitive to actual improvements from model to model and so a lack of improvement here does not necessarily mean the alternative model did not improve the prediction quality. However, from what I know about how humans understand probability, I'm fairly confident a logistic regression would show improvement in this particular metric over a clinician. R squared: There are several ways to compute r squared for probabilistic models. I recommend Nagelkerke's r squared. Many of these are discussed in Frank Harrell's book Regression Modelling Strategies . I recommend taking a look at that book.
