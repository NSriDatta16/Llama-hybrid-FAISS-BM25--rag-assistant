[site]: crossvalidated
[post_id]: 250941
[parent_id]: 250937
[tags]: 
The relationship is as follows: $l(\beta) = \sum_i L(z_i)$. Define a logistic function as $f(z) = \frac{e^{z}}{1 + e^{z}} = \frac{1}{1+e^{-z}}$. They possess the property that $f(-z) = 1-f(z)$. Or in other words: $$ \frac{1}{1+e^{z}} = \frac{e^{-z}}{1+e^{-z}}. $$ If you take the reciprocal of both sides, then take the log you get: $$ \ln(1+e^{z}) = \ln(1+e^{-z}) + z. $$ Subtract $z$ from both sides and you should see this: $$ -y_i\beta^Tx_i+ln(1+e^{y_i\beta^Tx_i}) = L(z_i). $$ Edit: At the moment I am re-reading this answer and am confused about how I got $-y_i\beta^Tx_i+ln(1+e^{\beta^Tx_i})$ to be equal to $-y_i\beta^Tx_i+ln(1+e^{y_i\beta^Tx_i})$. Perhaps there's a typo in the original question. Edit 2: In the case that there wasn't a typo in the original question, @ManelMorales appears to be correct to draw attention to the fact that, when $y \in \{-1,1\}$, the probability mass function can be written as $P(Y_i=y_i) = f(y_i\beta^Tx_i)$, due to the property that $f(-z) = 1 - f(z)$. I am re-writing it differently here, because he introduces a new equivocation on the notation $z_i$. The rest follows by taking the negative log-likelihood for each $y$ coding. See his answer below for more details.
