[site]: crossvalidated
[post_id]: 617396
[parent_id]: 616623
[tags]: 
What is the penalty value for PELT? PELT segments the data $y_{1:n} = (y_1, \dots, y_n)$ with change points $(\tau_0, \tau_1, \dots, \tau_{m+1})$ (where $\tau_0 = 0, \tau_{m+1} = n$ ) to make segments $y_{(\tau_0 + 1):\tau_1}, y_{(\tau_1 + 1):\tau_2}, \dots, y_{(\tau_m + 1):\tau_{m+1}}$ . In the paper where PELT is described , it says that it chooses the segmentation to minimise the function $$\beta m + \sum_{i=1}^{m+1}C\left(y_{(\tau_i + 1):\tau_{i+1}}\right),$$ so $\beta$ is a penalty term for each change that we add, in order that we won't just put too many changes, and $C$ is a cost function (the default $C$ is twice the negative normal log-likelihood). If you choose $\beta = 2\log(n)$ then you are asking PELT to choose the change points to minimise the Bayesian Information Criterion (BIC) for a model with 1 parameter fit per segment. Each change point adds two parameters to the model (1 for the segment parameter and 1 for the new change point). As you get lots of data, the BIC is likely to be lowest for the correct model, it's a heuristic to balance the good fit of model complexity with a penalty for complicated models. How to choose an optimal range? It really depends on your application and what cost function you're using. Without seeing the data and PELT's proposed change points it's hard to say whether the change points are over/underfitting to the data. In general the BIC is a good starting point but it generally chooses too many change points. Having the penalty as a function of $\log(n)$ will help to keep the number of change points in check. I would just experiment with different penalty values on the type of data that you're using. If you wanted a scheme of penalties to try you could start with $2\log(n)$ and then keep doubling it until the change points fit for the application you have in mind. In my answer to this question I just experimented with larger penalties until it was only selecting the larger mean shifts in the data.
