[site]: crossvalidated
[post_id]: 505482
[parent_id]: 505469
[tags]: 
Your proposed methodology is flat-out invalid if you don't use a train/test/validation split. The point of the holdout dataset (I'll call this the test split even though some call it the validation split) is that you tune your hyperparameters on the validation set and pick the best hyperparameters, and finally you evaluate a single model with chosen hyperparameters on that test set which has never been seen or used in this process. By not having a holdout set, you're effectively allowing both your neural network and the other standard approaches to cheat in terms of data leakage from the test set. Even if your results are better, you can't dismiss the potential criticism that your method simply allows more data leakage and more cheating.
