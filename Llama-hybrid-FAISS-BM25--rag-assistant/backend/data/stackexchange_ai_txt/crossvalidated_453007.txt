[site]: crossvalidated
[post_id]: 453007
[parent_id]: 452819
[tags]: 
Based upon your question, it appears you wish to know when we can forego traditional hypothesis testing given a sufficient number of rejections of $H_{0}$ . To begin, I want to note that frequentist tests cannot tell us anything with certainty. In the post you referenced, you posed a question about a drug's effectiveness on reducing the incidence of a particular disease. It is worth noting that frequentist routines, even if conducted repeatedly, cannot definitively answer this type of question. We must make a declaration of uncertainty. Inference proceeds with statements regarding how likely , or unlikely , our observed effect is, if $H_{0}$ is true . Suppose a new drug purports to reduce the the onset of chronic heart failure. One group of 30 patients receives a new drug, while another group of 30 patients receives a placebo. After one year, the proportion of patients experiencing heart failure is lower in the treatment group. Suppose 10 out of 30 patients receiving the new drug experience heart failure after the observation period, compared with 20 out of 30 patients in the control group. It appears the drug reduced the onset of heart failure in the treatment group. Now suppose a new sample was drawn and 28 out of 30 patients did not experience any symptoms related to heart failure, compared with only 10 out of the 30 patients taking the placebo. We could concoct many stories from this observed effect. Maybe patients in the former test were more likely to transcend physiological hardship during the observation period. Based upon one test, this is a likely explanation. However, treated patients in the latter test showed even more improvement . It is still possible that the treatment group showed more willingness to improve their health throughout the observation period. But now, this explanation is less likely . I am only using this hypothetical example to illustrate a point. My question is: If results start to pile up in one rejection region after a lot of runs, how long will we believe in the plausibility of 0 and thus the relevance of ? Because we often work with samples in practice, we can never confirm nor deny the plausibility of $H_{0}$ , which is a statement about a population quantity. Note, the p -value is not evidence of the truth of $H_{0}$ . Suppose in our second example we obtained a $p$ -value of .03. This is the probability of observing a result this extreme, or a result even more extreme, if the null were true . In other words, if the null were true , an observed effect this extreme, or even more extreme, would occur 3 times out of 100. Even in repeated testing, we are not assigning probabilities to the truth or falsity of the stated null. Moreover, would it be reasonable to reject (or accept) 0 after one test (as we often see being done in practice)? Yes. Hypothesis testing involves explicit statements about population parameters. The number of tests is irrelevant. The conclusions we draw from a single test are up to us to decide. Frequentist methods cannot answer questions with respect to how the data is favoring the null. Large p -values are not indicative of the truth of $H_{0}$ . Bayesian approaches may be more applicable in this scenario. See this post for a brief discussion.
