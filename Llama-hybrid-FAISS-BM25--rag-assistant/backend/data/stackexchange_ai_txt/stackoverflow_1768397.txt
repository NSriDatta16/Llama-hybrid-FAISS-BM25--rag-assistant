[site]: stackoverflow
[post_id]: 1768397
[parent_id]: 1767865
[tags]: 
Edit: I've revised my method, abandoning scipy.nanmean in favor of masked arrays. If it is unclear what the code is doing at any point, first try putting print statements in. If it is still unclear, feel free to ask; I'll try my best to explain. The trick part is getting the t-values merged. (That was done with numpy array's searchsorted method.) Playing with numpy has led me to believe that its speed advantages may not exist until the datasets get quite big (maybe you'll need at least 10,000 rows per data set). Otherwise, a pure python solution may be both easier to write and faster. Here are the toy datasets I used: % cat set1 1, 10, 50 2, 13, 58 3,9,43 4,14,61 7, 4, 82 % cat set2 1, 12, 48 2, 7, 60 3,17,51 4,12,57 7,10,88 8,15,92 9,6,63 And here is the code: #!/usr/bin/env python import numpy as np filenames=('set1','set2') # change this to list all your csv files column_names=('t','a','b') # slurp the csv data files into a list of numpy arrays data=[np.loadtxt(filename, delimiter=',') for filename in filenames] # Find the complete list of t-values # For each elt in data, elt[a,b] is the value in the a_th row and b_th column t_values=np.array(list(reduce(set.union,(set(elt[:,0]) for elt in data)))) t_values.sort() # print(t_values) # [ 1. 2. 3. 4. 7. 8. 9.] num_rows=len(t_values) num_columns=len(column_names) num_datasets=len(filenames) # For each data set, we compute the indices of the t_values that are used. idx=[(t_values.searchsorted(data[n][:,0])) for n in range(num_datasets)] data2=np.ma.zeros((num_rows,num_columns,num_datasets)) for n in range(num_datasets): data2[idx[n],:,n]=data[n][:,:] data2=np.ma.masked_equal(data2, 0) averages=data2.mean(axis=-1) print(averages) # [[1.0 11.0 49.0] # [2.0 10.0 59.0] # [3.0 13.0 47.0] # [4.0 13.0 59.0] # [7.0 7.0 85.0] # [8.0 15.0 92.0] # [9.0 6.0 63.0]]
