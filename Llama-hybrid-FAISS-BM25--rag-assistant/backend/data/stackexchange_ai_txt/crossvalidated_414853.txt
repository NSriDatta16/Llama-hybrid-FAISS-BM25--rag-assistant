[site]: crossvalidated
[post_id]: 414853
[parent_id]: 
[tags]: 
Detecting outliers in time series with fitting curve

I have a collection of time series that I know follow a particular type of non-linear functions. For each time series I use the LM algorithm to obtain a fitting curve. I have already checked that the error (difference between residual and fitted values) is normally distributed. I want to detect outliers in my fitting curve. I used Grubb's test and it works really well when there is a single outlier, but if there are two or more the test misses all of them because of the masking effect: each time series contains 17-23 pairs, so the standard deviation of the sample is greatly affected by outliers. The good news is that I can generate virtually as many time series (with their respectives fitting curves) as I want, so I can have a very good estimation of the standard deviation. Using the Grubb's test but replacing the sample standard deviation by this estimator I can find all of the outliers of the curve. I would like to express formally that when the standard deviation is known, this modification of the Grubb's test is better and overcomes the masking/swamping effect.
