[site]: crossvalidated
[post_id]: 324478
[parent_id]: 324356
[tags]: 
Another approach is to train autoencoders for your first N classes, and then train another network $f$ which maps trained autoencoder weights to vectors $W_i$, where $W_i$ is the $i$th row in the weight matrix in the FC layer before the softmax (this vector would correspond to the $i$th class being picked). Now in order to add a new class, you can simply train an autoencoder on just that class, and use $f$ to predict the vector $W_{N+1}$. Now you can concatenate it onto your weight matrix, and perhaps fine-tune a bit. This doesn't require retraining on all the data (although finetuning may improve performance), and the cost is constant with respect to $N$. Also see the extremely relevant paper "Learning to Model the Tail"
