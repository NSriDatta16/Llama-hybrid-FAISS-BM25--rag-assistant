[site]: crossvalidated
[post_id]: 140879
[parent_id]: 
[tags]: 
Standardizing dimension reduction output

I understand that data is (typically) standardized (i.e. zero mean and unit variance) before dimension reduction technique such as PCA/LDA is applied. In addition to this, would it ever make sense to scale the output of the dimension reduction algorithm? To provide context, i have a high dimensional data extracted from video. This data is normalized (subtract from mean and divide by stddev) and fed to a dimension reduction algorithm. The data resulting from this dimension reduction will be used to fit a further complicated model. To initialize some hyper-parameters for this model, it will be better if the data falls within a particular range. This is particularly relevant if my model's initialization is agnostic about the dimension reduction algorithm used. I see that some of the dimension reduction algorithms produce output that has extremely different stddevs. It would be easier if I can scale this PCA/LDA output to a pre-defined range (say between -1 and +1). But it is not clear whether this (second time) scaling would adversely alter the true characteristics of the data. I see that normalization itself is a touchy subject and whether it should be done or not depends on the algorithm. But my context is for numerical convinience. It appears as though I could re-normalize but wanted to double check.
