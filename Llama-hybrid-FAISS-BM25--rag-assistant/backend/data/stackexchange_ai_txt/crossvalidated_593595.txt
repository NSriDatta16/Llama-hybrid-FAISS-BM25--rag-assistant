[site]: crossvalidated
[post_id]: 593595
[parent_id]: 591569
[tags]: 
The key point to consider here is that the reason for the primary train/test split is the test dataset should not be used at any point in the model selection/training process. It's kept to one side, and only used once the final model has been built. It is then used to test that model to evaluate how well the model should perform when deployed. This strategy helps ensure the model does not overfit the test data, so the evaluation results should be representative of how the model performs when deployed (assuming of course that the test data is representative of "real" data). Attempting to use k-fold cross validation and then averaging the results (ensembling) as you suggest means that all the data is used during model training, so there is no independent test set that can be used for model evaluation. While using the strategy you suggest could lead to a better model - both having more training data and ensembling multiple results should result in a better performing model - the lack of an independent test set means you can't tell if the model really is better, or if it just overfitted to the test data. K-fold cross validation is used on the training set, usually either for hyperparameter tuning or for model selection. However, I don't see any reason why you can't build an ensemble using k-fold splits of the training set to generate the training data for each ensemble member. You then still have the test data to evaluate your ensemble model.
