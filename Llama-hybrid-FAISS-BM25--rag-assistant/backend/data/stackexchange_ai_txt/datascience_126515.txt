[site]: datascience
[post_id]: 126515
[parent_id]: 
[tags]: 
Neural network does not overfit my data. (Primarily linear function)

I am using TensorFlow and Keras. My goal is to approximate a primarily linear function that is partially nonlinear, such that a linear regression yields a Mean Absolute Error (MAE) of 0.13. All targets and inputs are scaled to have a mean of 0 and a standard deviation of 1. However, there is a nonlinear component, so I decided to employ a neural network to tackle the task. The issue is that, regardless of the width or depth of my neural network, it fails to improve the loss function beyond the level achieved by a simple linear regression model (or slightly better). I tested my network on purely random standardized values, and it perfectly overfits them. The perplexing part is that I can't overfit a much simpler function, even when I reduce my training data to 4000 samples and use extremely deep and wide networks with various architectures. This led me to believe that most ReLU perceptrons are 'deactivated' in the beginning, as the main error comes from the linear part. However, when the network needs to learn the nonlinear part, there are no spare parameters left. To address this, I decided to implement something similar to gradient boosting. I trained a linear regression model and calculated residuals (the nonlinear part). Then, I attempted to predict the residuals using a neural network with the same inputs as in the main task. Unfortunately, it behaved like a pure linear model and did not improve the error at all. The MAE remained at 0.13, irrespective of the width or depth of the network. I have been investigating this issue for two weeks and have tried various approaches, but nothing seems to work. The picture attached illustrates that attempting to predict residuals with different network architectures and hyperparameters converges to the same asymptote of 0.13, showing no improvement beyond the initial error.[![enter image description here][1]][1] My code to train the neural net part is as follows: def build_model(hp): n_hidden_layers = hp.Int("n_hidden_layers", min_value=2, max_value=4) n_neurons_per_layer = hp.Int("n_neurons_per_layer", min_value=100, max_value=600) model = keras.Sequential() for layer in range(n_hidden_layers): model.add(layers.Dense(n_neurons_per_layer, kernel_initializer="he_normal", activation = "relu",name=f"Hidden{layer}")) model.add(layers.Dense(residuals_trainY_2024.shape[1], name="output")) optimizer = keras.optimizers.SGD(learning_rate = 0.01) model.compile(optimizer=optimizer, loss=tf.keras.losses.MeanAbsoluteError()) return model random_search_tuner = kt.RandomSearch(build_model, objective="val_loss", max_trials=1, overwrite=True, directory="my_dir", project_name="ML_GAP_proj_redisuals_1", seed=42) random_search_tuner.search(trainX_2024, residuals_trainY_2024, epochs=n_epochs, batch_size=batch_size, verbose = 0, callbacks = [keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.7, patience=30, min_lr=0.0001), EarlyStopping(monitor='loss', min_delta=0.0005, verbose=1, patience=5), EarlyStopping(monitor='val_loss', patience=60), TensorBoard(log_dir, profile_batch=1000000000) I would greatly appreciate any insights or ideas on how to resolve this issue. While I could resort to using XGBoost, my primary motivation is educational, and I'm keen on understanding how to address this within a neural network context. Thanks! [1]: https://i.stack.imgur.com/N2l6n.png
