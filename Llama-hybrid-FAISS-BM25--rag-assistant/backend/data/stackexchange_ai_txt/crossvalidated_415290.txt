[site]: crossvalidated
[post_id]: 415290
[parent_id]: 415282
[tags]: 
When you perform Bayesian analysis explicitly you need to assume a prior; that is right. However, from a Bayesian perspective, there are many cases where you implicitly assume a prior distribution; for instance, the regularization term, $\lambda$ , in ridge regression can be associated with normal distribution on parameters; or Lasso with Laplace distribution, etc. This is not restricted to linear regression. In many ML methods, Bayesian pespective associates regularization with some prior distribution over the model parameters. Neural networks are not an exception. For example, a neural net with cross-entropy loss + regularization can be thought of as log-likelihood + log-prior, as in MAP estimates.
