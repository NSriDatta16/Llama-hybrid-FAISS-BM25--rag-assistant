[site]: crossvalidated
[post_id]: 590717
[parent_id]: 589724
[tags]: 
Some years ago, I spent a lot of time solving a similar problem. Let me say that accurate function approximation with neural nets can be really hard. First of all, in my experience, standard optimization methods used in deep learning (e.g., for image recognition and language modelling) are not very well-suited to this kind of problem, especially if the capacity of the network (number of neurons and layers) is not very large. I would start by trying to reproduce the identity mapping with the algorithm you are currently using: take your 25 input features and use them as both input and output and see what happens. You might easily find that your training algorithm is not even able to accurately reproduce the identity mapping. Another problem might be caused by multicollinearity in the outputs, which can seriously impair the convergence properties of the training algorithm (basically, you have ill-conditioned matrices somewhere in the net). Try to train a separate network for each of the 19 outputs and see what happens. Try custom parameter updating rules. The one that worked best for me was parameter update = -sign(gradient) * indicator(gradient and momentum have the same sign). I found this in a paper that tried thousands of rules. This rule was the best one. Updating only one layer per iteration may also help. Increase the batch size as the loss decreases. With your problem, I would say, start from 50 and increase to something around 10000 in the end. Decrease the learning rate as the loss decreases.
