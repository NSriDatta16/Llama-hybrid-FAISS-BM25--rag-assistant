[site]: crossvalidated
[post_id]: 447961
[parent_id]: 447952
[tags]: 
Given the data matrix $X$ , consider it's covariance matrix $cov(X)=\Sigma$ and obtain the matrix $V$ of eigenvectors from $\Sigma$ . This matrix $V$ is what you call the loadings . This set of eigenvectors define an orthogonal change of basis matrix that maximize the variance from $X$ . This means that, if I project $X$ into the subspace generated by $V$ I will obtain a matrix $U$ by simply solving $XV=U$ . This matrix $U$ is what you call the scores . The loadings in PCA are obtained in a way such that the first loading maximizes the variance, and each subsequent loading maximizes the remaining variance subject to the constraint that it is orthogonal to the previous loadings. Summing up, $V$ is orthogonal, so this means that $V^t=V^{-1}$ So we have that $$XV=U \rightarrow X=UV^{-1}=UV^t$$ Additionally, the variability explained by each loading is meassured in the associated eigen values, so for instance, if we define $\lambda_1$ as the first eigen value (associated to $v_1$ the first eigen vector),..., $\lambda_p$ the last eigenvalue associated to $v_p$ the last eigenvector, one can compute the percentage of variability explained by the first loading as $$\frac{\lambda_1}{\lambda_1+\ldots+\lambda_p}$$ Check this little example using R: data(iris) # Load the known iris dataset X = as.matrix(iris[,1:4]) # select the numerical variables in the dataset Sigma = cov(X) # Obtain the covariance matrix from X loadings = eigen(Sigma)$vectors # Obtain the loadings as the eigenvectors from sigma scores = X%*%loadings # Obtain the scores as the projection of X into the loadings var_explained = eigen(Sigma) $value/sum(eigen(Sigma)$ value) X_2 = scores%*%t(loadings) X_3 = scores%*%solve(loadings) head(X_2) head(X_3) # Calculating it with R fit = prcomp(X, center=T, scale=F) summary(fit) loading_pca = fit $rotation # loading score_pca = fit$ x # score
