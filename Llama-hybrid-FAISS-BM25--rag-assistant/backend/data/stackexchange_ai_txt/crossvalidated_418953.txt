[site]: crossvalidated
[post_id]: 418953
[parent_id]: 
[tags]: 
How to perform SHAP explainer on a system of models

I have developed a model with Autoencoder + XGBoost. Autoencoder is used to reduce dimensionality and then passed on to XGBoost for prediction. I would like to understand the feature importance of the root variables in the prediction. SHAP on XGBoost is giving me the importance of encoded variables from AE. Is there a way to treat these two models together as a system of models and perform SHAP on top of both of these to retrieve feature importance of the raw variable?
