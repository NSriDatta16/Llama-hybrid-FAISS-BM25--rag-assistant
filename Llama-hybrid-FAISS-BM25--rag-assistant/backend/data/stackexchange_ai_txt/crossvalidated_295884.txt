[site]: crossvalidated
[post_id]: 295884
[parent_id]: 285334
[tags]: 
This is normal and exactly what you should expect. Often, deep models are exquisitely sensitive to the initial parameters. It would appear that they are exquisitely sensitive to the first step taken, even. This is because the loss function is non-convex and optimization procedures have difficulty finding any kind of global minimum--the existence and locatability-by-an-optimization-algorithm of which would make your models the same every time. This is therefore not necessarily an indication of uncertainty in your model. What to do? (1) You might as well try 10 different random initializations or train it 10 different times with the same initialization and take the one that gives you best results . (Generally when DL is applied, it seems that practitioners are measuring Performace on massive held out sets--CV isn't even feasible.) (2) Read Chapter 8 of Deep Learning by Goodfellow et al. before returning to the answers to this post. It is very interesting and might give insight into what you're seeing. Here is a revealing diagram shown there. Devising an optimization procedure that will arrive at the same solution in high dimensions regardless of where it begins is an area of active research, compounded by the difficulties shown just above; besides trying to navigate a nonconvex topology, things like the amount of training iterations might now dictate where you land depending on where you start. Even if you weren't initializing differently each time, you still could find very different models. You may not even be ending up at different local minimima; in high dimensions, you could land at a saddle point, or plateau, etc, etc. Sometimes, Ã  modeler stop training because the training error plateaus but the norm of the gradient continues to increase , suggesting that it hasn't even reached a critical point! Ultimately, training deep models can take months for these reasons and appears to be part art. If you're looking for a model that trains the same way every time, consider one of the myriad models with convex losses (e.g., SVM, LR). Perhaps you don't have enough data to train a very deep model...perhaps with enough data the training would be more stable..but I don't think this is necessarily the case.
