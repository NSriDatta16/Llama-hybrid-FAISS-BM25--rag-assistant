[site]: crossvalidated
[post_id]: 459432
[parent_id]: 
[tags]: 
Multiclass gradient boosting: how to derive the initial guess, how to predict a probability

I have some questions regarding multiclass boosted-tree-algorithmus. Currently, I apply xgBoost as implemented in R to solve a multi-classification problem. According to StatQuest, for a simple two-classes case, the initial guess is: p = (exp(log odds)) / (1 + exp (log odds)) ( https://www.youtube.com/watch?v=jxuNLH5dXCs ) I could find no answer regarding how in multi-classification the initial guess is derived. Furthermore, I suspect that the predict()-function in R for the method XGBoost utilizes some sort of softmax-function to predict probability values for single estimates. I tried to understand the code but I did not really comprehend it. Can you give a clear example of how to calculate such a probability using boosted trees? Does it relate to some sort of softmax-output or does it somehow relate to the sum of weights of those trees that agreed on the majority class? I read different opinions about the last question and would love to have a final answer. Thank you!
