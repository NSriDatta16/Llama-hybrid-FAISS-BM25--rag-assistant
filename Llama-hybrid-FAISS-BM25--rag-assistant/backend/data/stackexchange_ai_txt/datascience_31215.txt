[site]: datascience
[post_id]: 31215
[parent_id]: 31212
[tags]: 
If you plug this kind of data into a standard network, e.g. an MLP, you will usually hope that the model actually extracts this information itself. You could introduce a dummy variable that encodes this information, but you run the risk that the model learns to just follow the dummy variable and doesn't learn its own powerful abstractions and feastures from the data. [EDITED: ] An example could be to create a weight based variable that is normalised to other physical characteristics, such as DV = weight / (height + waist circumfrence) . This should then, according to your assumptions, scale nicely with the output obesity. Another way that people might include prior information into a model, is to use a Probabilistic modelling, which incorporates ideas from Bayesian statistics. You can do things such as define a prior distribution for your outputs, conditioned on your inputs - essentially allowing you to provide information to the model (such as weight being correlated with obesity, ceteris paribus ) - this then nudges the model to go along these lines. If you would like to get into it, there are already some great libraries to make it really easy: Stan - which has interfaces to many languages: Python, R, State etc. Edward - probabilistic programming with modern GPU accelration and deep learning integration (Tensorflow and Keras). A Linear regression example A Bayesian Neural Network examples This seems to be a nice overview of some of the methods and tools, but I haven't read through it all. Another way that I could suggest is to play with the architecture of your model and the idea of auxilliary models . Have a look at my recent answer on a question talking about the Inception model from Szegedy et al. . The idea is that you have branches coming off the model at train time only, which also make predictions and produce error to be backpropagated through the preceding weights. You could make a side model that predicts the obesity, based on e.g. the input weights and perhaps some related features extracted from the first layer of your neural network. This would make the idea or importance of this relationship more prominent during training and the weights would subsequently be tuned to take that into account. At test time, you simply ignore these auxilliary branches.
