[site]: crossvalidated
[post_id]: 457217
[parent_id]: 
[tags]: 
Bayesian linear regression KL divergence

$$y_i \sim N(w_0 + w_1x_i, \sigma^2_j)$$ $$\mathbf{w} \sim N(0,\alpha^2 I) $$ Data is $D$ , posterior distribution $p(\mathbf{w}|D)$ is approximated according to mean-field approximation $$p(\mathbf{w}|D) \approx \prod_{d=0}^1 q(w_d)= \prod_{d=0}^1 N(w_d | \mu_d, \sigma^2_d)$$ How to calculate the KL divergence of: $$ \text{KL} \left[q_{\lambda}(\mathbf{w}) | p(\mathbf{w})\right]$$ I know that analytical solution of KL for two univariate gaussians would be: $$-\frac{1}{2} + \log(\frac{\sigma_1^2}{\sigma_2^2}) + \frac{\sigma_2^2 + (\mu_1-\mu_2)^2}{2\sigma_1^2}$$ But not sure what to do here since $p(\mathbf{w}|D)$ itself has two components.
