[site]: crossvalidated
[post_id]: 625028
[parent_id]: 625008
[tags]: 
In cases where the Ordinary Least Squares (OLS) has a single solution, and given that you have a suitable learning rate, gradient descent will always take you towards it, whether it is too small or big in some dimensions. Though, this situation could still slow down the learning, and makes you more vulnerable because that suitable learning rate is harder to choose. In neural networks, this is not the case. Depending on the size, it may have hundreds of thousands of local minima, maybe even more gradient plateaus and you may end up anywhere at the end if your features do not behave well enough. Not to mention that these plateaus are partly constructed by the activations in neural networks, which is in line with your comment that it's multiple layers of regressions (with activations in-between). In practice, there may still be numerical issues, however, in OLS depending on your feature ranges, and the effective range of your chosen data type (e.g. you can't tackle numbers like $2e+1000$ using a float data type), or the gradients will not behave very well, because of these numerical anomalies. In that case, maybe your only weapon is to standardize. Apart from the case where the two uses the same optimization algorithm, OLS has more options because the optimization problem is much more simpler, like second-order methods (newton's method), cholesky decomp etc. which are not available to solving general neural nets (yet).
