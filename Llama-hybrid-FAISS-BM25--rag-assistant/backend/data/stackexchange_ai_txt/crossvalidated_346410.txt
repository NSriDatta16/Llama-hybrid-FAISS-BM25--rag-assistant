[site]: crossvalidated
[post_id]: 346410
[parent_id]: 294434
[tags]: 
I am not completely sure, but here is my understanding. First of all, to your first question "How are these models transforming the features into higher dimensional, sparse space?". In the random forest model, suppose there are $n$ decision trees and for each tree $T_i$, there are $M_i$ leaves(not labels). When an instance goes through the random forest model, it ends up being 1 leaf in each tree and the final result is the majority vote of the leaves' value. Now, we come up a boolean variable $l_{i_m}$ to denote if the instance ends up in tree $T_i$ and leaf $m$. In this way, we will have $\sum_{i = 1}^{n} M_i$, binary variables, which is usually greater than the number of original predictors, after the random forest transformation. Since an instance can be only one leaf in a tree, most of the binary variables will be 0, which explains "sparse". Next step, we fit a logistic regression(linear model) using only with binary variables as inputs.
