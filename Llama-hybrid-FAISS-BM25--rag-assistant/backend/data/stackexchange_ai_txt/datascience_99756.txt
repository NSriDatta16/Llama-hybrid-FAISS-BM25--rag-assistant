[site]: datascience
[post_id]: 99756
[parent_id]: 17860
[tags]: 
IMO, this notation is confusing and is better to think in terms of probability distributions. I will tackle the case of a finite MDP to simplify things. The reward function on a MDP depends only on current state and actions: $$r(s,a)=\mathbb{E}[R_{t+1}|s,a]\\ r(s,a)=\sum_{r\in\mathcal{R}}p(r|s,a)r\\\\$$ If we now condition on next states by using the law of total probability we get: $$r(s,a)=\sum_{r\in\mathcal{R}}\sum_{s'\in\mathcal{S}}p(r|s,a,s')p(s'|s,a)r\\\\$$ Then we observe that $p(r,s'|s,a)=p(s'|s,a)p(r|s,a,s')$ : $$r(s,a)=\sum_{r\in\mathcal{R}}\sum_{s'\in\mathcal{S}}p(r,s'|s,a)r\\\\$$ In summary, you can consider the joint distribution by averaging over all next states and their probabilities, that would explain the need for $P_{ss'}^a$ when you try to express the reward function in terms of next states probabilities.
