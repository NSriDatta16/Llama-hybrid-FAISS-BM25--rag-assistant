[site]: crossvalidated
[post_id]: 489595
[parent_id]: 486708
[tags]: 
I agree that overfitting is not possible when the data-generating process is deterministic. However, this is not "too good to be true" because generalization is still a problem. Consider that we can take our model $\hat{f}$ to be a Lagrange polynomial (or any other "look-up-table"-like interpolator) of whatever order is necessary to get 100% accuracy on all data. Each time you give me another $\{x,y\}$ , I'll simply increase the complexity of my model by adding some new terms - i.e. raise the order of my polynomial $\hat{f}$ . With a deterministic $f$ , one can perhaps call this "perfect fitting." But we know for generalization reasons that such a model probably won't work well outside the training data on which "over/underfitting" are defined. However, sometimes when people say "overfitting" they also mean "won't generalize well" in which case nothing can save you. We can't guarantee perfect generalization performance in any situation unless we get to sample every possible $\{x,y\}$ (infinitely often in the stochastic case) which is really not much different than saying you already know $f$ . Edit I feel like you know the above already, and that your confusion stems from this: "If there is a range models to choose from, the highly flexible ones will have low bias and high variance and will tend to overfit. The inflexible ones will have high bias and low variance and will tend to underfit." That concept makes sense when talking about performance on a specific set of data points. It doesn't hold when considering all possible data points ("generalization performance"). There is nothing about a "highly flexible" model that will definitively cause low bias for inputs it wasn't trained on. So I took your definition of under/overfitting to mean "on the training data." (I mean, even the word "fit" implies that). If you meant "in generalization" then the fallacy in your reasoning is the above quoted text. Also, from wikipedia on the Bias-Variance Trade-Off: "It is an often made fallacy to assume that complex models must have high variance (and thus low bias); High variance models are 'complex' in some sense, but the reverse needs not be true." I think the key is to understand that for generalization performance, low bias comes from model correctness , not complexity. Unprincipled complexity only reduces "bias" if you're talking about training set performance. This is not the precisely defined bias $E(f - \hat{f})$ in the bias-variance decomposition, which involves an expectation taken over all possible inputs. Thus, I think your underlying confusion was thinking that highly flexible models have low bias in the expected value (generalization) sense, while that is only true if the expected value is approximated by a sample mean over the training set (on which we define the word "fit"). A sort of corollary to this idea is that if you have a huge, encompassingly representative amount of training data, then a massively complex model (like those of modern deep learning) can lower bias on a sample mean error that closely approximates the actual mean. But it should be noted that most of the successful massive models aren't full of "unprincipled complexity" - they often take advantage of crucial structures inherent to the data (e.g. using convolution on images, etc). Moreover, understanding the surprising generalization ability of massive deep models is still a point of research to this day (and research on the many ways that generalization ability can silently fail too, e.g. adversarial inputs).
