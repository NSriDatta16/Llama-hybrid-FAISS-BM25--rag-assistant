[site]: datascience
[post_id]: 66021
[parent_id]: 41353
[tags]: 
You could take pretrained embedder on multiple languages, then check distances between the encodings. There's unofficial pypi port of Facebook's LASER . It's langauge-agnostic and pretrained on both en and fr . from laserembeddings import Laser laser = Laser() sentence_en = 'My name is Hendrik' sentence_fr = 'Je suis Hendrik' en_embedding = laser.embed_sentences([sentence_en], lang='en')[0] fr_embedding = laser.embed_sentences([sentence_fr], lang='fr')[0] Embeddings are 1024-element NumPy array. You can calculate some metric between embeddings i.e. euclidean. import numpy as np distance = np.linalg.norm(en_embedding - fr_embedding) The good thing is you have defined similarity in your DB, so you can learn the threshold for your distance metric and check execatly how well it fares.
