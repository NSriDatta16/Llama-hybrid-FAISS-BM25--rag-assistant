[site]: datascience
[post_id]: 120317
[parent_id]: 23789
[tags]: 
First Question: XGBoost converts weak learners to strong learners. What's the advantage of doing this? Combining many weak learners instead of just using a single tree? Just to get the vocabulary straight: XGBoost is a library for different coding languages. It uses the Gradient Boosting algorithm. Also, it is not correct that Gradient Boosting uses decision trees . It can additionally use other Machine Learning algorithms such as regressions . What is the Gradient Boosting algorithm? Gradient boosting, in turn, is a subset of many, different boosting algorithms. The basic idea behind it is that the next model should be built in such a way that it further minimizes the loss function of the ensemble. In the simplest cases, the loss function simply describes the difference between the model's prediction and the actual value. Suppose we train an AI to predict a house price. The loss function could then simply be the mean squared error between the actual price of the house and the predicted price of the house. Ideally, the function approaches zero over time and our model can predict correct prices. New models are added as long as prediction and reality no longer differ, i.e. the loss function has reached the minimum. Each new model tries to predict the error of the previous model. Let's go back to our example with house prices. Let's assume a property has a living area of 100m², four rooms, and a garage and costs 200,000€. The gradient boosting process would then look like this: Training a regression to predict the purchase price with the features of living space, the number of rooms, and the garage. This model predicts a purchase price of 170,000 € instead of the actual 200,000 €, so the error is 30,000 €. Training another regression that predicts the error of the previous model with the features of living space, number of rooms, and garage. This model predicts a deviation of 23,000 € instead of the actual 30,000 €. The remaining error is therefore 7,000 €. These steps are repeated until the remaining error is as small as possible or even zero. Second Question: Random Forest uses various samples from trees to create a tree. What's the advantage of this method instead of just using a singular tree? The Random Forest consists of a large number of these decision trees , which work together as a so-called ensemble. Each individual decision tree makes a prediction, such as a classification result, and the forest uses the result supported by most of the decision trees as the prediction of the entire ensemble. Why are multiple decision trees so much better than a single one? The secret behind the Random Forest is the so-called principle of the wisdom of crowds. The basic idea is that the decision of many is always better than the decision of a single individual or a single decision tree . This concept was first recognized in the estimation of a continuous set. In 1906, an ox was shown to a total of 800 people at a fair. They were asked to estimate how heavy this ox was before it was actually weighed. It turned out that the median of the 800 estimates was only about 1% away from the actual weight of the ox. No single estimate had come that close to being correct. So the crowd as a whole had estimated better than any other single person. This can be applied in exactly the same way to the Random Forest . A large number of decision trees and their aggregated prediction will always outperform a single decision tree .
