[site]: crossvalidated
[post_id]: 257756
[parent_id]: 
[tags]: 
Is ReLU activation function more common than logistic?

In the first lecture of Prof. Hinton coursera course (artificial neural networks for machine learning), he says that logistic activation function is probably the commonest activations function. But in deeplearningbook.org they say that "most neural networks today are based on ReLU". Which one is true? ReLU is the first choice or logistic units?
