[site]: crossvalidated
[post_id]: 606606
[parent_id]: 
[tags]: 
Should I destandardize the errors from training the neural network?

So, I am learning a bit about Neural Networks. I have built a code in PyTorch for regression, and I have standardized both the features and the target variables following this answer. My question is that should I destandardize the loss value before plotting the elbow curve to observe the train and validation errors across the epochs. I destandardize the predictions on the test dataset to make some plots. But is that necessary for the elbow curve as well? Adding code to make my question a bit more understandable, I guess?? ... x_train = StandardScale(x_train, x_scaler) # Just indicating that scaling is done here y_train = StandardScale(y_train, y_scaler) ... #Similarly for validation and test datasets ... dl = DataLoader(params) #for all train, test, val model = net(params) optimizer = SGD(model.params(), other-params) for epoch in range(0, num_epochs): for i, (x_train, y_train) in enumerate(dl): # This loop is run for both train and val at each epoch. optimizer.zero_grad() output = model(x_train) loss = criterion(output, y_train) loss.backward() optimizer.step() running_loss += loss.item() ... ... ... epoch_loss = running_loss / len(dl) history[phase].append(epoch_loss) #history is a dictionary containing training and validation phase losses fig = plt.figure() #Plotting the elbow plot plt.plot(history["train"]) plt.plot(history["val"]) y_pred = model(x_test) y_pred = StandardScaler.inverse_transform(y_pred) # This step is not done for the elbow plot and that is my question. fig1 = plt.figure() # Plotting the performance on test data plt.plot(y_pred) plt.plot(y_test)
