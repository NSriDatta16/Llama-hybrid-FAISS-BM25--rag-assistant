[site]: crossvalidated
[post_id]: 433491
[parent_id]: 433016
[tags]: 
1) almost so. The proportions can be considered as conditional probability of finding a white person, when looking for someone randomnly in each state. You may then define a joint probability function for the varibles "county" and "skin color". This means that, if you want to turn your list of proportions in a unique distribution, you have to include proportions of habitants of each county, so that your data will look a lot more like: | 1st county | 2nd county | 3rd county white | .2 .1 .2 black | .2 .2 .1 Note that totals by columns are not equal, and that overall total is 1. You may also put all these probabilities in one vector, before processing them. If you fix the counties populations in your simulation to be the real ones, the similarity measure using this distributions will reflect only the differences between proportions of white people, giving more weight to more populous counties. If you don't want this, you may just consider each of your proportion as a probability value of a Bernoulli distribution, and comupute 58 similarities of those distribution, before averaging them. 2) it depends. There are infinite possibilities, and most of them are probably just fine. First ones that come to my mind are Chi-squared statistic and Cramer's phi, but also Kullbackâ€“Leibler divergence makes some sense here. Although you have to consider that KL divergence consideres one distribution to be the true one, and the other one to be an approximation of it. So if you consider your simulation as a kind of model to describe how people of different races distributes, you may want to use KL divergence (as well as chi squared or Cramer's phi) to describe how near your model gets to real data. But I would rather use an average of multiple simulations, or some estimate of expected outcome of your model. I can't understand now how your method work, but if you expect convergence in your simulation, then KL probably makes sense. Histogram intersection is one other feasible method. Cosine similarity is a less theorically founded similarity measure, not that much related to probability at all. You may want to use it as well as MSE or countless others. Cosine similarity works in a way that is usefull in other kinds of applications in particular and I don't see any reason why you should choose this measure over others, but still it could work just fine. About EMD, you wrote: 1) With Kullback-Leibler divergence and Cosine Similarity, the value changes if i reshuffle both the arrays and compute both the metrics again but with Earth movers distance its not the case. but this is just wrong, it is rather the opposite. If you shuffle your data (provided that you shuffle both your vectors of probabilities keeping them paired) all the methods I've discussed since now give the same results, you can verify it by looking into their formulas. EMD instead needs (and it is the only one since now) a metric to evaluate distance between various counties. Of course actual distance between, let's say, the capitals of the counties, immediately comes to mind. So, if you want to take into account the geographical position of data, you can choose this metric (and you need to use their coordinates of course). If your simulation doesn't take into account geographical distances, I would rather not use it.
