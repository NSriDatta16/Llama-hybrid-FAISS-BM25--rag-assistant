[site]: datascience
[post_id]: 122419
[parent_id]: 
[tags]: 
Best algorithm to capture non-randomness in series of numbers

Suppose we have a series of integer numbers which are not truly random (For example, the numbers at a given position in decimal pi representation). Is there a machine learning algorithm that can capture this? Thanks for your time.
