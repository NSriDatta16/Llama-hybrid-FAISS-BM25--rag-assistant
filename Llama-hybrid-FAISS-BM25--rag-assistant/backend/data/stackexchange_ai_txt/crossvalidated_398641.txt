[site]: crossvalidated
[post_id]: 398641
[parent_id]: 398638
[tags]: 
+1 for "sometimes seems a bit overwhelming". It really depends (as Harrell clearly states; see the section at the end of Chapter 4) whether you want to do confirmatory analysis ( $\to$ reduce your predictor complexity to a reasonable level without looking at the responses, by PCA or subject-area considerations or ...) predictive analysis ( $\to$ use appropriate penalization methods). Lasso could very well work OK with 100 predictors, if you have a reasonably large sample. Feature selection will be unstable, but that's OK if all you care about is prediction. I have a personal preference for ridge-like approaches that don't technically "select features" (because they never reduce any parameter to exactly zero), but whatever works ... You'll have to use cross-validation to choose the degree of penalization, which will destroy your ability to do inference (construct confidence intervals on predictions) unless you use cutting-edge high-dimensional inference methods (e.g. Dezeure et al 2015 ; I have not tried these approaches but they seem sensible ...) exploratory analysis: have fun, be transparent and honest, don't quote any p-values. For the particular use case you have now described (a bunch of your predictors essentially represent a cumulative distribution of the dose received by different fractions of the heart), you might want to look into varying-coefficient models (a little hard to search for), which basically fit a smooth curve for the effect of the CDF (these can be implemented in R's mgcv package).
