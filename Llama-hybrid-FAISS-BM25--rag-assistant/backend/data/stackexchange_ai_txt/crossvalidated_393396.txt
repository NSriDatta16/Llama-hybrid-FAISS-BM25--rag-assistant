[site]: crossvalidated
[post_id]: 393396
[parent_id]: 393316
[tags]: 
I know of two interpretations. The first was said by Tim: We have observed $X$ successes out of $Y$ trials, so if we believe the trials were i.i.d. we can estimate the probability of the process at $X/Y$ with some error bars, e.g. of order $1/\sqrt{Y}$ . The second involves "higher-order probabilities" or uncertainties about a generating process. For example, say I have a coin in my hand manufactured by a crafter gambler, who with $0.5$ probability made a 60%-heads coin, and with $0.5$ probability made a 40%-heads coin. My best guess is a 50% chance that the coin comes up heads, but with big error bars: the "true" chance is either 40% or 60%. In other words, you can imagine running the experiment a billion times and taking the fraction of successes $X/Y$ (actually the limiting fraction). It makes sense, at least from a Bayesian perspective, to give e.g. a 95% confidence interval around that number. In the above example, given current knowledge, this is $[0.4,0.6]$ . For a real coin, maybe it is $[0.47,0.53]$ or something. For more, see: Do We Need Higher-Order Probabilities and, If So, What Do They Mean? Judea Pearl. UAI 1987. https://arxiv.org/abs/1304.2716
