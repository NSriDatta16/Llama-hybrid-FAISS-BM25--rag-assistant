[site]: crossvalidated
[post_id]: 573425
[parent_id]: 485910
[tags]: 
Consider encoder part of transformer. If there is no feed-forward layer, self-attention is simply performing re-averaging of value vectors. In order to add more model function, i.e. element-wise non-linearity transformation of incoming vectors, to transformer, we add feed-forward layer to encoder part of transformer.
