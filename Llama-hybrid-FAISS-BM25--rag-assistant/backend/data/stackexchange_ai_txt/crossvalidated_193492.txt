[site]: crossvalidated
[post_id]: 193492
[parent_id]: 
[tags]: 
State of the art in general learning from data in '69

I'm trying to understand the context of the famous Minsky and Papert book "Perceptrons" from 1969, so critical to neural networks. As far as I know, there were no other generic supervised learning algorithms yet except for perceptron: decision trees started to become actually useful only in late '70s, random forests and SVMs are '90s. It seems that the jackknife method was already known, but not k-cross validation (70s) or bootstrap (1979?). Wikipedia says the classical statistics frameworks of Neyman-Pearson and Fisher were still in disagreement in '50s, despite that the first attempts at describing a hybrid theory were already in '40s. Therefore my question: what were the state-of-the-art methods of solving general problems of predicting from data?
