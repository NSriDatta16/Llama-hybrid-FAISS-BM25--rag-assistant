[site]: datascience
[post_id]: 128140
[parent_id]: 
[tags]: 
Is there any standard or heuristic for deciding on the dimensions and filters of a convolution layer for image processing?

I reposted this from StackOverflow since it does not meet StackOverflow's guideline to focus on programming and coding questions. Link to the original question. I want to find ways other than trial and error to select the number of filters and the kernel size of my convolutional layers. The best I could come up with is to start with a kernel size of three and keep doubling it to see the effect on the network performance: 3, 6, 9, 12, etc. I could try to keep selecting odd-numbered filters as suggested here: Deciding optimal kernel size for CNN. As for the number of layers and filters, I start with 64 filters as my default and, as usual, keep increasing or decreasing based on the network performance. After that, I do trial and error on the number of layers. The order in which I tune a parameter (kernel size first, layer count first, or filter count first) might skew my tests and prevent me from picking other combinations. If I try to experiment with more combinations, I would take too much time to train all the models. I don't want to modify two or more parameters at once since I can't tell which one causes an improvement or decrease in model performance. Is there a better way?
