[site]: crossvalidated
[post_id]: 616948
[parent_id]: 616899
[tags]: 
This is a nice example of a situation where the all too conventional all-or-none description of the results as 'significant' or 'not significant' is unhelpful, as you have sensibly noticed. I agree with Harvey Motulsky's answer, but would add a few considerations. First, note that a p-value of 0.04 is only ever fairly weak evidence against the null hypothesis. Where your the sample size is small the false positive error rate does not increase (assuming the statistical model is appropriate), but a 'significant' result will often come from a sample that severely exaggerates the true effect. Next, note that the individual binary data points carry relatively little information, and so even though the p-value falls within the conventionally 'significant' range, the likelihood function from your data will be relatively widely spread and so if you were to do a Bayesian analysis (maybe you should!) you would find that the posterior probability distribution would not be moved very much from your prior. (All of that is to say that a a result of p=0.04 is says that you have fairly weak evidence against the null.) Finally, note that the reliability of a statistical test is only as good as the statistical model is well matched to the actual data generating and sampling systems. Given your thoughtful question, you might find my explanations of the differences between p-value evidence considerations and error rate accountings will clarify the issues. You will find it interesting in any case: A reckless guide to p-values: local evidence, global errors .
