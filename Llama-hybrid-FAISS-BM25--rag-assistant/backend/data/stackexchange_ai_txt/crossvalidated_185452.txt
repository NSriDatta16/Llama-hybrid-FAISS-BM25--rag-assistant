[site]: crossvalidated
[post_id]: 185452
[parent_id]: 
[tags]: 
Forms of the Reward function in Reinforcement Learning: A vector, a matrix, a linear combination?

Personally, one of the most intuitive forms of the reward function in reinforcement learning is the form $R(s,a)$ in a matrix. In this case, $s$ is a state and $a$ is an action. This way if an agent is in state $s$ and plans to do an action $a$, then it is easy to see in the matrix $R(s,a)$ the corresponding value of the reward function. My questions are: (a) if the reward function is a vector of dimension $n$, where $n$ is the number of states, what is the interpretation of this? (b) Also, if the in case of infinite state space, $R$ is most likely a linear combination of features (in features-based representation reinforcement learning), $R = \sum \alpha_i \phi_i$, where $\phi_i$ are the features and $\alpha_i$ are the weights. How would one interpret this while the agent is learning? The features seem to change as time goes by, doesn't it? (versus the action and the state in the case of $R(s,a)$ when everything is constant.)
