[site]: datascience
[post_id]: 122909
[parent_id]: 
[tags]: 
How to Justify Anomalies Detected by Unsupervised Anomaly Detection Models?

I'm working on an unsupervised anomaly detection project involving a large sensor dataset, where I aim to identify anomalies without the aid of labeled data. While I've implemented several unsupervised machine learning models for this task, such as Isolation Forest and K-Nearest Neighbors (KNN), I'm facing challenges in providing solid justifications for why the predicted values are classified as anomalies. Since I don't have access to labeled data for validation, I find it challenging to explain the rationale behind the model's decisions. My stakeholders and domain experts require interpretability in the anomaly detection process to trust and act upon the model's output effectively. Therefore, I'd like advice on the best approach to validating the predictions. I'm open to empirical approaches, as well as tactical advice.
