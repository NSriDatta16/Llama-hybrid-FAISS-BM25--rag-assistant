[site]: datascience
[post_id]: 126373
[parent_id]: 
[tags]: 
How to read the "predicted_true" Metric of an Azure ML experiment?

I followed along to Explore Automated Machine Learning in Azure Machine Learning which had me create a regression experiment using data from https://aka.ms/bike-rentals (731 samples; 12 features; 1 label (rentals)). The best model was a Voting Ensemble of two algos. The metrics have a predicted_true graph which I had trouble understanding even after reading this : The true values are binned along the x-axis and for each bin the mean predicted value is plotted with error bars. There are 9 bins here and there are 9 corresponding Average Predicted Values s (APVs) (one for each bin) (see table below to confirm this). But the blue line makes it seem like there's a whole continuum of Average Predicted Value s. Is the blue line just "connecting the dots" (i.e. the 9 APVs)? If there are only 9 APVs, then what's the meaning of the error bars (light blue regions)? What's going on on the right hand side? With the dip in the blue line and dip and arrowhead in the light blue region? The error bars dip to 0 at 4 different bins which coincide with the 4 bins that have 0 Bin errors in the table below, but the rest of the error bar doesn't seem to coincide with the other Bin errors values. E.g., for the 1,525.5 ~ 1,894.85 bin (the 4th bin from the right), the error bars seem to be around 1.5k in the graph, but the Bin error for that bin is 258.847455318977 in the table below. When viewed as table, the Predicted vs True data looks like this and here it is in an actual table Bin edges Bin counts Bin averages Bin errors 43-48.11 1 113.6912279 0 48.11-417.46 22 296.3723139 135.1174633 417.46-786.81 11 709.1647262 183.4718475 786.81-1156.16 21 994.882296 265.2700464 1156.16-1525.5 9 1293.40242 161.7881036 1525.5-1894.85 4 1692.202127 258.8474553 1894.85-2264.2 1 2151.197189 0 2264.2-2633.55 4 1548.323188 503.2679452 2633.55-2827 1 2281.890262 0
