[site]: crossvalidated
[post_id]: 49921
[parent_id]: 49915
[tags]: 
The "Mean Average Precision" (sometimes abbreviated mAP or MAP) might be what you want. It's pretty commonly used for evaluating information retrieval systems and is fairly straightforward to compute. First, calculate the average precision for a given query. To do this, rank the documents and compute the precision after retrieving each relevant document. For example, suppose that four documents are relevant to this query, and our system returned the following: Relevant document Irrelevant document Relevant document Relevant document Irrelevant document Irrelevant document. Relevant document The first relevant document is at position one, and the precision there is 1/1 = 1.0 The next relevant document is at position 3; two of the three documents seen so far are relevant, so our precision here is 2/3. Document 4 is relevant too and the precision score here is 3/4. The final relevant item is at position seven, giving us a precision of 4/7. Find the mean of these precision scores (1/4*(1 + 2/3 + 3/4 + 4/7) = ~0.747) to get the average precision for this query. The mean average precision is just the mean of these averages across all the queries in your evaluation set. As for choosing a precision-recall trade off, that's largely up to you. The $F_1$ score gives them equal weight; you can interpret the $\beta$ in $F_\beta$ as giving $\beta$ times more weight to recall than precision. I believe that some studies indicate that users prefer precision to recall, but I would bet that it depends a lot on the application and use-case. I certainly don't need google to show me every webpage about cats, but do want all the sites on the first page to be relevant. On the flip side, it might be more important to return every possibly-relevant document if you're doing discovery for a court case.
