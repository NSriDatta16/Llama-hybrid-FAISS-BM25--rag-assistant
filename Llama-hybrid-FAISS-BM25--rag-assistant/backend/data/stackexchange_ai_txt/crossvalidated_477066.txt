[site]: crossvalidated
[post_id]: 477066
[parent_id]: 477049
[tags]: 
I don't think so. Consider the case where we are trying to use a Gaussian Process to estimate an expensive, deterministic function (an emulator - not to be confused with the type of thing you run GBA games on). In essence to make an emulator, we run a computationally expensive computer model (e.g. a big physics simulation, a climate model or whatever) a handful of times and use the observed computer model runs to build a GP. Using the GP allows us to predict the output from our expensive model at just a fraction of the computational effort. They're pretty accurate too. Within sample, the error will be exactly $0$ because we have constructed an interpolator . The emulator perfectly represents the simulator at the training points . Out of sample there will be some error. The GP emulator typically leverages the Bayesian framework so priors are placed over all hyperparameters of the GP. Therefore any point estimate derived from the posterior will exhibit some kind of bias. Although there are special cases of the GP prior which exhibit no bias!
