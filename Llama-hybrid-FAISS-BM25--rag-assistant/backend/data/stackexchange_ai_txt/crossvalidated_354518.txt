[site]: crossvalidated
[post_id]: 354518
[parent_id]: 
[tags]: 
Kernel function from polynomial basis functions

In chapter 3 & 6 of Bishop's Pattern Recognition and Machine Learning, he showed that the equivalent kernel based on eqn (3.62) $$ k(x,x') = \beta \phi(x)^T (\alpha I + \beta \Phi^T \Phi )^{-1}\phi(x')$$ and the standard kernel based on the feature space mapping eqn (6.10), $$ k(x,x') = \phi(x)^T \phi(x') = \sum_{j=1}^M \phi_j(x)\phi_j(x') $$ both of which have a sinc like behavior corresponding to Figures 3.11(left) and 6.1(left), see below. This is quite impossible given that when $x=0$, all $\phi_j(x)=x^j=0$, which implies $\phi(x)^T\phi(x')=0\ \forall x'$. Is my assumption that $\phi_j(x)=x^j$ wrong? My understanding is that polynomial basis are global functions, so how can one arrive at the conclusion that its kernel have a local behavior, it's not obvious to me at this point.
