[site]: crossvalidated
[post_id]: 382127
[parent_id]: 382112
[tags]: 
Feature importance measurements are strongly associated with the underlying model. That is because different models can utilise a feature differently. Some (like a gradient boosting machine (GBM)) will recursively partition it while others (like a generalised additive model (GAM)) might fit a spline on it. In addition, the way that an algorithm accounts for interactions between variables might be completely different; for example, in a random forest (RF) interactions are essentially detected in an automated way as deeper trees are grown by sequentially examining different features, while in standard linear model (LM) we would need to explicitly define it. That said, all is not lost, the feature importance from an GBM algorithm does hold some relevance for a GAM model or a neural network (NN) as well. It is totally unreasonable to expect that a given algorithm most important features are useless for the learning task of another algorithm. They will contain relevant information but there is little guarantee that these features will present the optimal set of features for another type of learner too. We can definitely use them as a first educated guess. A final note: especially for a deep neural network (DNNs) where we commonly have hundreds (if not thousands) of input features it is not always coherent to examine individual variable importance. DNNs are strong exactly because they can synthesize features internally and train themselves on those internal features. For instance, within a image classifier the value of a single pixel is often immaterial, it is the convolution layers that make the big difference. Overall, we might wish to exclude a particular feature (or better yet replacing it with pure noise), retrain our DNN on this new dataset and then compare the performance difference but this method can turn out to be very computationally expensive for anything more than simple cases. There are some further techniques for DNNs that give feature importance for a DNN (e.g. Sundararajan et al. " Axiomatic Attribution for Deep Networks " (ICML 2017)) but once again those are particular to the learner used.
