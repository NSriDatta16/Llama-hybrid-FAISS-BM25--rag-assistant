[site]: datascience
[post_id]: 77030
[parent_id]: 76872
[tags]: 
BERT uses both masked LM and NSP (Next Sentence Prediction) task to train their models. So one of the goals of section 4.2 in the RoBERTa paper is to evaluate the effectiveness of adding NSP tasks and compare it to just using masked LM training. For the sake of completeness, I will briefly describe all the evaluations in the section. First, they compare SEGMENT-PAIR+NSP and SENTENCE-PAIR+NSP both models use masked LM + NSP training and they find that using individual sentences hurts performance on downstream tasks i.e., SEGMENT-PAIR+NSP performs better than SENTENCE-PAIR+NSP. Second, they remove the NSP task (hence they take continuous sentences) and train the model with only masked LM. And they add small variations by allowing the sampled input sentence to cross the document boundary in one case but not in another. They report that removing the NSP loss matches or slightly improves downstream task performance by comparing DOC-SENTENCES and FULL-SENTENCES with SEGMENT-PAIR+NSP and SENTENCE-PAIR+NSP. And that single document (DOC-SENTENCES) performs slightly better than packing sequences from multiple documents (FULL-SENTENCES)
