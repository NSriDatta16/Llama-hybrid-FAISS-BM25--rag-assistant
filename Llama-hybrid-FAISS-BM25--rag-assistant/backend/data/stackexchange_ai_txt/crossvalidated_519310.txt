[site]: crossvalidated
[post_id]: 519310
[parent_id]: 519294
[tags]: 
From Wikipedia : A Bayesian network (also known as a Bayes network, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). Bayes' rule is used for inference in Bayesian networks, as will be shown below. A better name for a Bayesian network would be directed probabilistic graphical model , and the main purpose of a probabilistic graphical model is to efficiently represent the conditional independencies in a joint probability distribution. The fundamental idea to understanding the conditional independencies represented by Bayesian networks is the Markov assumption : The Markov condition, sometimes called the Markov assumption, is an assumption made in Bayesian probability theory, that every node in a Bayesian network is conditionally independent of its nondescendents, given its parents. Stated loosely, it is assumed that a node has no bearing on nodes which do not descend from it. In a DAG, this local Markov condition is equivalent to the global Markov condition, which states that d-separations in the graph also correspond to conditional independence relations. This also means that a node is conditionally independent of the entire network, given its Markov blanket. So, in your diagram, using the Markov assumption, the following conditional independencies can be deduced: $\text{Storm} \perp \emptyset \ | \ \emptyset$ $\text{Burglar} \perp \text{Cat} \ | \ \text{Storm}$ $\text{Cat} \perp \text{Burglar} \ | \ \text{Storm}$ $\text{Alarm} \perp \text{Storm} \ | \ \{\text{Cat},\text{Burglar}\}$ Where $\text{A} \perp \text{B} \ | \ \text{C}$ reads as " $\text{A}$ is conditionally independent of $\text{B}$ given $\text{C}$ ", and $\emptyset$ is the empty set. The joint probability distribution is: $$ p(\text{Storm},\text{Burglar},\text{Cat},\text{Alarm}) = p(\text{Alarm}|\text{Storm},\text{Burglar},\text{Cat}) \cdot p(\text{Burglar}|\text{Storm},\text{Cat}) \cdot p(\text{Cat}|\text{Storm}) \cdot p(\text{Storm}) $$ Given the conditional independencies that were deduced above, then this joint distribution can be simplified to: $$ p(\text{Storm},\text{Burglar},\text{Cat},\text{Alarm}) = p(\text{Alarm}|\text{Burglar},\text{Cat}) \cdot p(\text{Burglar}|\text{Storm}) \cdot p(\text{Cat}|\text{Storm}) \cdot p(\text{Storm}) $$ Next, using Bayes' rule , we want to find: $$ p(\text{Alarm}|\text{Storm}) = \frac{p(\text{Alarm},\text{Storm})}{p(\text{Storm})} $$ Using the law of total probability , we know that: $$ \begin{align} p(\text{Alarm},\text{Storm}) &= \sum_{\text{Burglar}} \sum_{\text{Cat}} p(\text{Storm},\text{Burglar},\text{Cat},\text{Alarm}) \\ &= \sum_{\text{Burglar}} \sum_{\text{Cat}} p(\text{Alarm}|\text{Burglar},\text{Cat}) \cdot p(\text{Burglar}|\text{Storm}) \cdot p(\text{Cat}|\text{Storm}) \cdot p(\text{Storm}) \\ &= p(\text{Storm}) \sum_{\text{Burglar}} p(\text{Burglar}|\text{Storm}) \sum_{\text{Cat}} p(\text{Alarm}|\text{Burglar},\text{Cat}) \cdot p(\text{Cat}|\text{Storm}) \end{align} $$ So: $$ p(\text{Alarm}|\text{Storm}) = \sum_{\text{Burglar}} p(\text{Burglar}|\text{Storm}) \sum_{\text{Cat}} p(\text{Alarm}|\text{Burglar},\text{Cat}) \cdot p(\text{Cat}|\text{Storm}) $$ An efficient way of computing $p(\text{Alarm}|\text{Storm})$ as it is shown above is belief propagation .
