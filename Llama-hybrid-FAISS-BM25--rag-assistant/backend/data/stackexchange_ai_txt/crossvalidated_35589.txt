[site]: crossvalidated
[post_id]: 35589
[parent_id]: 30358
[tags]: 
I work as a project manager & software engineer at a small, research oriented software company. We have recently completed a text categorization project and made many experiments on the way. We work with a Lineer SVM implementation I coded in C# based on Platt's "Sequential Minimal Optimization" Algorithm and later improvements for the linear case. The items are product definitions being sold on the internet. Our categories is a two level tree with about 15 first level nodes and 200 leaves. Here is a two sentence summary of what I have learned: Success depends on the quality of the training set much more than the exact methodology. Although it is true that SVM performs somewhat better than Decision Trees, Nearest Neighboors and any other "simple" approach, selecting the kernel, optimizing the system parameters make little difference in terms of success rate. Our experiments included the comparison of the two approaches you wrote about. Although my common sense still tells me that first approach should perform better, there was no statistically significant difference in terms of success rate in our experiments: very likely, when a product is classified, either both approaches classify it correctly, of neither. For the RAM usage, I don't know about R to make concrete suggestions. However I can give some general advice, in case you have not already implemented these steps: Remove stop words Remove uncommon words (i.e. words that occur in less than say 10 documents) Format and Stem all the words so that COME, come, came, coming, etc. are mapped to the same term. (There are free libraries for this) SVM needs much memory during training. Still, 24 Gb seems more than enough to me. (Again, I don't know anything about the R implementation) However here is an experiment you may perform, which if succeeds will make training much faster, and will consume much less resources: If you have 300 categories, construct 300 different training sets and Support Vector Machines. For each category let your training set consist of 100 positive samples and say 400 randomly selected negative samples (find the optimum number by experimentation). You will have 300 different term-document matrices but each will be of much less size. When a document is to be classified, ask each SVM and return the category with the maximum value.
