[site]: crossvalidated
[post_id]: 139398
[parent_id]: 
[tags]: 
Methods for compiler benchmarking?

In computer science, statistical knowledge is quite low in general. You rarely see standard deviation in papers. Now I am trying to improve at least my skills, but I find myself utterly confused about the wealth of methods available. Here is my typical use case: I am working with compilers, so I come up with an optimization, which should make programs faster. For evaluation I have a benchmark suite (e.g. SPEC 2006 ) with a set of programs. I run it n times with and without my optimization. What should I do now? Compute sample-means per benchmark program. Gives me speedups per programs. Hopefully, all positive. In this field often small. A 1% improvement is great (for C/C++ compilers on Intel CPUs). Compute standard deviation per program. Hopefully, I do not see them overlapping. Compute p-value using T-Student. As far as I understood, I should use the "unpaired/independent", "equal variance", "two-tailed" variant. If p-value is high, then I run the benchmark suite again with higher n . Would it make sense to do more (e.g. confidence intervals)? How do I correctly summarize the different programs? Mean of means? Instead of the mean , some people use the best measurement per program. The reason is that a compiler optimization is used once to create a single program, which is evaluated n times. Thus any variance is never an effect of the optimization we want to evaluate. On the other hand you can argue that the optimization can influence the size (strength?) of the variance, which might be interesting to measure. I never tried a bayesian approach.
