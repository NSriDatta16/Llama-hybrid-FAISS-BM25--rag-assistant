[site]: crossvalidated
[post_id]: 430238
[parent_id]: 430221
[tags]: 
You want something called a Posterior Predictive Distribution . Your Bayesian model is a generative model. In essence, this means that if you knew the probability of the coin landing on heads (call it $\theta$ ), then you could generate predictions from your likelihood. Using the posterior to draw multiple $\theta$ and then passing them to the likelihood allows us to make predictions. Let's see this in action. Let's assume we put a beta prior on our coin, observed some data, and now our posterior is something like $\operatorname{Beta}(10,12)$ . Here is some python code to generate predictions or how many heads we might see in 100 flips of the same coin. from scipy.stats import beta, binom import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Define the posterior to be a Beta(10,12) posterior = beta(a=10, b=12) def likelihood(theta, flips): # Binomial Likelhood return binom(n=flips, p=theta).rvs() # Pass the posterior draws to the likelihood predictions = likelihood(theta=posterior.rvs(1000), flips = 100) plt.hist(predictions, edgecolor='white', bins = np.arange(10,100,10)); The result is the following: We can then summarize our simulations using the mean (or some other summary function). So for what we've drawn here, we estimate that the number of heads in 100 flips would be approximately 45 or 46, and our prediction interval (depending on how you define it) is 23--46
