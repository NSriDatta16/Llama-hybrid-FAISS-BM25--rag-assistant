[site]: datascience
[post_id]: 60958
[parent_id]: 
[tags]: 
Will it be more computationally expensive to have multipl 2d tensors or 1 3d tensor

Odd question but I am busy creating a Genetic Algorithm that optimizes the weights on a Neural Network instead of using good old fashion 1st-order optimization (Gradient/Adam) What I have is x as a 2d vector (rows, columns) x = tf.placeholder(tf.float32, shape=[None, total_input]) Since I need multiple neural networks instead of my hidden layer or output layer being 2d it needs to turn into a 3d vector. fully_connected_1 = tf.Variable(tf.truncated_normal([size_in, size_out, total_networks], stddev=0.1), name="W") acttivision1 = tf.nn.relu(tf.matmul(input_tensor, w) + b) I can now either have my next layer into multiple 2d vectors: for neural_networks in layer1: fully_connected_2 = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=0.1), name="W") activison2 = tf.nn.relu(tf.matmul(acttivision1 , w) + b) or keep the 3d fashion. fully_connected_2 = tf.Variable(tf.truncated_normal([size_in, size_out,total_neural_networks], stddev=0.1), name="W") activison2 = tf.nn.relu(tf.matmul(acttivision1 , w) + b) As you have noticed the code is not 100% accurate as I am not even sure whether relu activision function will output a 3d vector. I know it is going to be resource-intensive but the question is which one will be less computationally expensive. Also if you have a better idea please share. This is the only 2 solutions I can come up with.
