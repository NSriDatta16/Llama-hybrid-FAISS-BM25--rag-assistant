[site]: crossvalidated
[post_id]: 427825
[parent_id]: 425039
[tags]: 
A ridge/ $\ell_2$ penalty achieves what you're asking for. Consider the case of duplicate features $X_1 = X_2$ as mentioned in the question. For any choice of some value $c$ , all solutions where $\beta_1 X_1 + \beta_2 X_2 = c$ will have the same cost/likelihood (assuming weights for all other features are held constant). This is why ordinary logistic regression has no unique solution in this case. But, among the continuum of equivalent choices for $\beta_1$ and $\beta_2$ , the $\ell_2$ norm of the weights is minimized when $\beta_1 = \beta_2$ . Thus, $\ell_2$ penalized logistic regression would set these weights to be equal. Contrary to what the question states, an $\ell_2$ penalty does not result feature selection or sparse weights as lasso or elastic net penalties do. But, it does encourage weights to have smaller magnitude than the maximum likelihood solution (called shrinkage). If you want sparse weights, use the elastic net. The $\ell_1$ component of the elastic net penalty will encourage sparsity, while the $\ell_2$ component will encourage similar features to have similar weights (for the same reason as above).
