[site]: datascience
[post_id]: 94815
[parent_id]: 
[tags]: 
Can a machine learning model be used as some kind of compression?

I'm trying to understand how machine learning is working. I read a lot and now came into my mind that it could be missuses in a practical way. I also hope that this question is on topic here. Please correct me if I have some wrong assumptions: All models require sample data The learning process is some kind of optimization of its functions aka neutrons The learning process is iterative to find the best values of the parameters to maximize the expected output of the neurons To make this process faster you do some kind of reduction of the data, so that in the end the neutrons do not need to look at every part of the input You split up the training data in two buckets to find out of the result became better for the optimization (I would call this verification, not sure how this is really called) If your verification data are bad or equal to the input data than the results can just be as good as your input data If this is true or at least most parts I got this idea: When I set the input data to the same as the output data, then the model is doing nothing, but for equal data I get always the same data right? So when I train the model as in the table: input output 0 3 1 1 2 4 3 1 4 5 5 9 6 2 7 6 8 5 Then the model would return PI based on the correct count of digits (of cause for non learned indexes that would be random) I absolutely know that this makes no sense at all, but I'm just curious if I understand it correctly that you could "compress" data in a ML-Model.
