[site]: datascience
[post_id]: 90553
[parent_id]: 
[tags]: 
Should a filter learned within a residual block be different form its vanilla CNN counterpart?

I have a very basic CNN using Conv2d with multiple layers and activations, each layer $\ell$ has parameters $w_\ell$ inducing a mapping $f_\ell(x,w_\ell)$ . Now I decide to introduce skip connections to each layer separately , meaning the output of each layer is now $x+f_\ell(x,w_\ell)$ . Should I expect any significant difference if I were to try to visualize each kernel? My intuition is that in theory any change to the filter wouldn't stem from the fact we are learning the residual part of the function rather than the function itself: the identity part could be seen as a convolution with a kernel that has one pixel set to 1 and the rest are 0, so $w_\ell$ shouldn't differ too much before and after but one pixel. So if there is any difference, would it only stem from the learning stability and us being able to get better filters? I might be wrong, am I new to Res-Nets and trying to wrap my head around how they act. Note by "visualizing filters" I don't mean anything specific, we can see the weights as numerical values or look at the images the kernels induce.
