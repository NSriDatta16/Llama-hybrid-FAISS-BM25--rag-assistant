[site]: crossvalidated
[post_id]: 20872
[parent_id]: 20866
[tags]: 
The hyper-parameters should be expected to change somewhat, if only because scaling the target will also scale the noise variance. Maximising the evidence is not really a proper Bayesian thing to do, ideally you should place an appropriate prior over the hyper-parameters and integrate them out as well, however in practice most people don't do this due to the computational expense involved. I suspect if you integrated the hyper-parameters out rather than maximising the evidence, then both models would give similar performance. The trouble with poor performance with a small scaling factor may be due to the starting point for the hyper-parameter search being a long way from the optimal value, on a plateau, or worse near a local minima. Changing the scale will change the noise variance, which might be enough to move the initial hyper-parameter values to somewhere nearer the optimal solution. Try maximising the evidence starting from different (random?) initial points and see whether they call converge to the same solution. Model selection for GPs is not straightforward, especially by evidence maximisation. The evidence is noisy (it is based on a small sample of data, so small changes in the particular sample used can give susbtantial changes in the evidence) and may well have local minima etc. Caveat: I haven't used Gaussian process regression much, but most of the comments above stem from my experiments with Gaussian Process CLassification.
