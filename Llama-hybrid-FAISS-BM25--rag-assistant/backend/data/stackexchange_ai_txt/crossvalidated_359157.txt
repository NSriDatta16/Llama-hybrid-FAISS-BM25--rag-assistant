[site]: crossvalidated
[post_id]: 359157
[parent_id]: 
[tags]: 
Normalization and train/dev/test split for multi-dimensional time series

We have multi-dimensional sensor data (30 different measurements) measured every ten seconds on over 500 assets. These assets were put into production at different times in the past five years. Some of these assets have failed in the past. (For our purposes, a failed asset does not make it back into the dataset after it gets fixed.) The assets are all not similar, so their sensor readings could be in different ranges. Our goal is to estimate the probability of asset failure in the next 30 days using machine/deep learning. For data normalization, is it reasonable to normalize each asset's data independently? For example, in the case of standard scaling, say we use the first 3 months of data from each asset (all our assets are functional for at least the first 3 months) to compute mean and standard deviation, and then use those to normalize all the data points for that asset? We can do the same for each asset independently. The idea is that a single model can then work well for all the assets. How can we do train/dev/test set splits? Should we do time-based split, so the training data is from the earliest time period (say a year), dev set should be a few months of data at least 30 days after the last training data point and test data should be at least 30 days after the last dev set data point? Or can we just split by assets? This has the concern of potentially using "future data" to predict the past, but can we think of assets as independent?
