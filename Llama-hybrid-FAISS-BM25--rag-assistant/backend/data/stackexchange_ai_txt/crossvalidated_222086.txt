[site]: crossvalidated
[post_id]: 222086
[parent_id]: 
[tags]: 
Strategy to help classifiers cope with ambiguous examples

I have a machine learning problem where sometimes the training data will correctly have two or more similar/same training examples with different class labels. As an over-simplified example, let us say we are trying to identify radio stations based on the songs that they play. The training data may look like: Turn around - Radio 1 All out of love - Radio 1 Changes - Radio 2 Dreamer - Radio 2 Turn around - Radio 2 Using this example, Radio 1 and Radio 2 play the same songs occassionally, but this may obviously cause confusion when given to the classifier. As a strategy to avoid this situation, I've come up with the following: 1) Split the training set into halves. 2) Train the classifier on the first half. 3) Test the classifier using the second half. 4) For those songs that the classifier makes a mistake on during testing, re-label the class as 'ambiguous'. 5) Train a second more robust classifier using the testing data with re-labelled examples where necessary. Are there any fundamental issues with that sort of bootstrapping approach? Alternately, are there other techniques (clustering maybe) that can be used to identify and remove (or re-label as 'ambiguous') those training examples that are similar and thus likely to cause classification errors? I'm using Random Forest classifiers but general solutions are welcome.
