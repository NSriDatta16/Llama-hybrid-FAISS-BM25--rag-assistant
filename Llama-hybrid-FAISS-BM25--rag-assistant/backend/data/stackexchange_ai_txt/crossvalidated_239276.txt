[site]: crossvalidated
[post_id]: 239276
[parent_id]: 
[tags]: 
Vectorized gradients for neural networks: matrix multiplication

In Stanford's 231N course , they discuss computing a vectorized form of the gradient of the loss function w.r.t. the matrix $W$ as the derivative of the matrix multiplication $WX$ w.r.t. $W$ multiplied with the derivative of the loss function w.r.t. the "upstream layer" input. This makes sense to me in terms of the chain rule and the computational graph explained in the course. What I dont understand is they show the derivative of $WX$ w.r.t. $W$ as $X$. Is the derivative of a vector ($WX$) w.r.t to matrix more complex than just $X$?
