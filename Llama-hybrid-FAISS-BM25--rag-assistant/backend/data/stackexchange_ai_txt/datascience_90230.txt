[site]: datascience
[post_id]: 90230
[parent_id]: 90114
[tags]: 
Neural networks work as a great encoder/decoder for any task you give them. To create a good representation of data, you require sparse representations. The dead neurons actually contribute to that. ReLU actually help the process. In fact, a recent paper actually justified that ReLU is even better than LeakyReLU which you mentioned above. They start with the problem that in order to create a decision boundary, you need to disentangle data. But, in order to disentangle data what you need is a non-continuous function so the data can be mapped to disjointed area in the manifold. That is only possible by a non-continuous function. They even go further to explain that tanh, sigmoid and others work due to the floating point precision which induces the non-continuous nature to even these functions. Here's a Medium article and the original paper .
