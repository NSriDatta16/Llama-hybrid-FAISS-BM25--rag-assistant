[site]: crossvalidated
[post_id]: 482284
[parent_id]: 481324
[tags]: 
Because Transformers are black-box models, it is hard to say, what the keys and values really are, but the motivation is that might want to retrieve something else than what is your search criterion. Imagine something like SQL-like query: get phone numbers of people that have a similar name to "Jindrich". "Jindrich" is a query , the criterion for the search. But you do not want similar names from the database, you want the phone numbers. Phone numbers are the values in this case. The keys are the names already in the phonebook. The projection for the keys and values in the Transformer model can be understood as extracting a relevant piece of information from the hidden states. E.g., in the Transformer Base architecture, the hidden states are 512-dimensional, but the "extracted" keys and values are only 64-dimensional. Regarding the multiplication: For simplicity, let's assume we have just one query vector $q$ (and not the full matrix $Q$ ). First, you compute a similarity score for each of the keys: $$ \alpha = \mathrm{softmax}(qK/\sqrt{d}) = \mathrm{softmax}\left( \frac{(qk_o, qk_1, \ldots, qk_n)}{\sqrt d} \right) $$ The distribution $\alpha$ is a single-dimensional vector, that only tells you how much each of the keys $k_i \in K$ is relevant for the query $q$ . In other words, it says at what positions you should retrieve, but you need something to retrieve and these are the values: $$\alpha V = \sum_{i=0}^m \alpha_i \cdot v_i $$
