[site]: crossvalidated
[post_id]: 610716
[parent_id]: 
[tags]: 
Loss function weighting in regression when the target varies orders of magnitude between groups

I have a dataset with 200 groups, and 50-300 observations per group. The target I'm trying to predict is a strictly positive financial metric, which varies 5+ orders of magnitude between groups but is roughly the same order of magnitude within every group. In addition some of the input predictors match the (magnitude) of the target closely. I want to fit a single model to make predictions from any group. I've tried different approches to dealing with this problem, such as log-scaling the targets. So far the models I've produced have significantly worse evaluation metrics on groups with small targets. I have tried Mean Average Precision Error, and this still tends to result in a model that produces good outputs for groups with big targets and useless predictions for groups with small targets. One thing I'd like to consider is adjusting the weights of a loss function like MSE. For an error of \$1000, if group A has targets in the range \$1000-\$5000, and group B has targets in the range \$10,000,000 - \$25,000,000, the prediction error isn't equally meaningful. Does it make sense to do something like an (inverse) power law transform to calculate weights for each group, and increase the weighting of obersvations from low-magnitude groups in the loss function? I guess it will be subjective, but how do I choose how much higher weighted group A should be than group B - i.e., the parameter of the power law transform?
