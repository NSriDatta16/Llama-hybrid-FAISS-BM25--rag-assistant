[site]: crossvalidated
[post_id]: 239798
[parent_id]: 239782
[tags]: 
It looks like this is a system of ordinary differential equations (ODEs) in time $t$, where equation (1) corresponds to $$T_j\frac{dA_j}{dt} = I_j-A_j[t] + \sum_iW_{ij}O_i[t]$$ and equation (2) corresponds to $$O_j[t] = \sigma(\,A_j[t]-\mathrm{threshold}\,)$$ where $$\sigma(x)=\dfrac{1}{1 - e^{-x}}$$ is the standard logistic curve , also known as "sigmoid activation", in the (artificial?) neural network community. This is a nonlinear system of first order ODEs, but the essence of the "time constant" can be understood from the following simplified form $$\tau\frac{dx}{dt}=c-x[t]$$ Here the system will asymptotically approach the steady state $x=c$, such that for initial condition $x[0]=x_0$ we have $$x[t]=c+(x_0-c)e^{-t/\tau}$$ From these equations, we can see that the time constant $\tau$ gives the timescale of evolution \begin{align} t\ll\tau &\implies x[t]\approx x_0 \\ t\gg\tau &\implies x[t]\approx c \end{align} In this simple example with scalar $x$ and $\tau$, the time constant is not particularly important, as it essentially just changes the "units" of time measurement (e.g. seconds vs. years). In your ODE system, with vector $\vec{A}$ and $\vec{T}$, the time constants can be much more significant, as they can represent variations in the evolution timescales of different components of $\vec{A}$. For example, if $T_1=10^{-3}$ and $T_2=10^3$, then $A_1$ will be very "fast" relative to $A_2$. (An ODE system with widely varying evolution timescales is called stiff. A nice discussion of this can be found on the SciComp SE site here .) As for the second question, the $I_j-A_j$ term is exactly analogous to my simplified example above. That is, the linear part of each ODE in (1) represents an asymptotic approach of $A_j[t]$ to the steady state $I_j$. Notice that this term is uncoupled as well as linear. So in the absence of feedback from other "neurons" $A_{j\neq i}$, each neuron will go to a steady-state background $A_j=I_j$.
