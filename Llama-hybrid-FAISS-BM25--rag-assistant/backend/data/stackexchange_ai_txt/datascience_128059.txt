[site]: datascience
[post_id]: 128059
[parent_id]: 128039
[tags]: 
Not a complete answer, but some clarifications to help frame things. Models that are arithmetically based can still produce your relationship: if the scaling is $\tilde{x}_i := (x_i - b_i)/m_i$ , then $x_1 - x_2 > 0$ is equivalent to $m_1 \tilde{x}_1 - m_2 \tilde{x}_2 + b_1 - b_2 > 0$ . This is just another linear relationship between the new $\tilde{x}_i$ , so linear regressions or neural networks (with intercepts/biases) should be fine at finding it. Models with regularization will care; that's one of the reasons we scale: to put regularization penalties on the same scale. But in your example, where $x_2$ has a much smaller scale but your relationship is still just $x_1 > x_2$ , this may cause some problems: $m_1 \gg m_2$ , and so the regularization penalty will apply differently on the estimated coefficients of $x_1$ and $x_2$ , which I don't think is desirable. Tree-based models (the usual ones anyway) won't care at all: the new relationship looks exactly the same to a tree; they don't care about scaling, only relative ordering. (The original relationship isn't trivial for a tree to approximate though.) Finally, the actual fitting procedure might care: the backpropagation in neural networks tends to work best when parameters are roughly in the same range, so again in your example with $m_1\gg m_2$ , the scaled data might actually be harder to learn from.
