[site]: crossvalidated
[post_id]: 483810
[parent_id]: 483796
[tags]: 
This seems to be pretty standard. With logistic regression as the base estimator, the adaptive boosting stops adding value after very few iterations. I put together a little notebook to illustrate, using sklearn s AdaBoostClassifier , which allows you to set your own base_estimator .) Note that unlike in gradient boosting, you can theoretically get a nonlinear model at the end, because the sigmoid link function gets applied to each of the base models before averaging predictions, not after summing. However, the examples in the notebook don't show any strongly nonlinear results. I suspect this is just because logistic regression fits too well, so that the misclassified points are "balanced" in such a way that the later iterations don't have much effect.
