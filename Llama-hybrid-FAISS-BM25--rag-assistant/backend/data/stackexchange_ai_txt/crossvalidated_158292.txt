[site]: crossvalidated
[post_id]: 158292
[parent_id]: 158272
[tags]: 
You could use a backward feature selection algorithm: provide all possible features to your classifier, assess its performance (for instance with the auroc you mentioned). Then remove the features one by one and each time assess the performance of your classifier and the p-value between your previous classifier (or the initial one) and the current one. When the p-value is significant, you stop removing features, this is your best model. To assess which feature to remove, you can have a look at the feature importance if you use a random forest classifier or the features weight if you use a logistic regression.
