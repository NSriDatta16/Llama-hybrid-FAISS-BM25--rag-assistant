[site]: crossvalidated
[post_id]: 576568
[parent_id]: 
[tags]: 
Lightgbm, time-series and spikes repeated on a yearly basis

I have a data set (time-series) with the shape { $2190$ x $63$ }. There are 63 variables, 2 products ( $A$ and $B$ ) worth of 3 years of daily data, thus I have $1095$ observations per product and total of $2190$ observations. Important note : Product $A$ has spikes every 30th of December. I've built a lightgbm model which overall works just fine. I've chosen lightgbm because the number of products will be much larger, increasing the size of the dataset. Let's say $20$ products, $1095$ observations per product, $63$ variables, which totals $20*1095=21900$ rows and $63$ columns. My problem : min_data_in_leaf affects how well spikes of the product $A$ are forecasted. As far as I can tell, when you have a relatively large dataset { $21900$ x $63$ } (it can go WELL beyond it), min_data_in_leaf should be large enough to make sure that your model is not overfitting. The problem is, no matter how many products will be included (expanding my data), large min_data_in_leaf will fail to capture these spikes because to distinguish them from other observations I need to specify a small value of min_data_in_leaf . At least to my understanding. My thoughts on it : I should stick to tuning other parameters that control overfitting and keep min_data_in_leaf fairly low; Maybe I need to do more feature engineering? (I've tried, unsuccessfully) Any help is greatly appreciated! Visualisation. 1 year of data (I have 3) + forecasts : My parameters (early stopping is on) : lgb_params = {'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'rmse', 'learning_rate': 0.01, 'subsample': 0.8, 'subsample_freq': 1, 'feature_fraction': 0.6, # 'max_depth': -1, 'num_leaves': 100, 'min_data_in_leaf': 30, 'max_bin': 100, 'n_estimators': 10000, 'boost_from_average': False, 'verbose': -1}
