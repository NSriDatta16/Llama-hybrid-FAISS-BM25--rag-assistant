[site]: crossvalidated
[post_id]: 298604
[parent_id]: 298600
[tags]: 
Grid search is a method for hyperparameters optimization amongst others. It tends to be well known as it doesn't depend on much hypotheses. For instance if you use a gradient descent method with a cost function that isn't strictly convex you might end up in a local minimum, which shouldn't happen in a grid search. Nonetheless, depending on your model (and on your cost function) you can use other methods such as the bayesian optimization , or hyper-gradient-descent . There are many ways to perform hyperparameter tuning. Take care though about overfitting since if you perform a huge grid search for instance you might end up on hyperparameters that fit a lot your training set without being as good on test sets. For a linear regression though you probably won't need to tune your hyperparameters, you'd better search for an approximate of the inverse of the matrix containing your coefficients. EDIT: I forgot to mention that if you have to perform a grid search, try to do it with a higher stepsize first and then zoom upon the region of the hyperparameters space where you performed best. It will save you a fine amount of time and computation!
