[site]: crossvalidated
[post_id]: 397339
[parent_id]: 397335
[tags]: 
Unlike classical tests in an ergodic setting, such test statistics are known to produce "mixed signals" quite regularly, see e.g. https://onlinelibrary.wiley.com/doi/full/10.1002/jae.733 or https://www.sciencedirect.com/science/article/pii/S0304407603002793 for evidence in a cointegration setup. This may open up the possibility to suitable combine the test statistics to achieve more robust power over nuisance parameters, see e.g. https://www.sciencedirect.com/science/article/pii/S0304407612000280 in a unit root setup or (self-promotion alert!) https://onlinelibrary.wiley.com/doi/full/10.1111/j.1467-9892.2012.00814.x in the cointegration case. From a more pragmatic view, I do not find it too suprising that different tests often produce differing results. After all, why would so many tests still be in use after such a long time if they always produced the same result? In that case, the community would have long ago settled on one test based on some auxiliary criterion, e.g., ease of computation, who was around first, which test had the most famous author, which was coded most conveniently in some click-and-point software or nice package, etc.?
