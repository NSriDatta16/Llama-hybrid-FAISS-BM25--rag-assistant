[site]: datascience
[post_id]: 97138
[parent_id]: 
[tags]: 
What can we learn from PCA on non linear data?

Suppose we have dataset with 10 features which are not linear: import numpy as np from sklearn.decomposition import PCA import matplotlib import matplotlib.pyplot as plt v1 = np.random.rand(100) print (type(v1)) v2 = 2**v1 v3 = 3**v1 + np.matmul(v1, v1) v4 = 4**v1 + np.matmul(v2, v3) v5 = 5**v1 + np.matmul(v1, v3) v6 = 6**v1 + np.matmul(v1, v4) v7 = 7**v1 + np.matmul(v2, v2) v8 = 8**v1 + np.matmul(v4, v5) v9 = 9**v1 v10 = 10**v1 v = [v1,v2, v3, v4,v5, v6,v7, v8, v9,v10] pca = PCA() pca.fit(v) pca.explained_variance_ratio_ PC_values = np.arange(pca.n_components_) + 1 plt.plot(PC_values, pca.explained_variance_ratio_, 'ro-', linewidth=2) plt.title('Scree Plot') plt.xlabel('Principal Component') plt.ylabel('Proportion of Variance Explained') plt.show() I know that PCA is used to find the linear correlation. But what can we learn from that example? Can we use the PCA results and use only the first component of the PCA to train-predict our model? Does the result shown here is valid (correct for further processing)?
