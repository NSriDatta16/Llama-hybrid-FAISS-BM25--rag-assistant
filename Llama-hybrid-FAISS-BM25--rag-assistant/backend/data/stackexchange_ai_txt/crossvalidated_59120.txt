[site]: crossvalidated
[post_id]: 59120
[parent_id]: 59079
[tags]: 
There are a few approaches that might get you moving in the right direction. First you could train your logistic regression via iterative learning and this would keep the solution confined to employing the likelihood function to estimate the regression parameters. First, estimate the logistic regression and look at the errors it made. If you want to improve the false positive rate, then assign a higher weight to the observations that the model incorrectly identified as belonging to the positive class. Then rerun the logistic regression using the weighted observations. Do this iteratively until the false positive rate satisfies your objective. See the post on Case weighted logistic regression for a discussion on implementing case weighted logistic regression in R. To be honest, I am not sure how well this approach will work in general, and it is not really an efficient optimization method. It is, however, simple to implement and easy to try. If you are willing to move outside of relying solely on the likelihood function to control classifications, then you could always optimize the threshold parameter used to partition the observations into negative and positive classes. I believe this threshold parameter is defaulted to 0.5, with any output less than 0.5 being classified as a negative class and any output above 0.5 being classified as a positive class. After the logistic regression weights are estimated you could move onto optimizing this threshold parameter using the true negative rate, true positive rate, precision, recall, F-measure, ect. as your objective function. Practical Neural Network Recipes in C++ has a decent discussion of augmenting the classification threshold to control different error rates (the discussion is about neural nets, but it applies to logistic regression models as well). A variety of optimization techniques could be used to solve this problem.
