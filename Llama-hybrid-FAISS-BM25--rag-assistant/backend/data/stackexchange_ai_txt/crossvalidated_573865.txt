[site]: crossvalidated
[post_id]: 573865
[parent_id]: 
[tags]: 
Features are Relevant for Regression but not necessarily for Classification - what to make of this?

I have used the R Boruta package to check for feature relevance in predicting log returns of financial time series, the targets being the log returns themselves (for regression) and the sign of log returns (for classification, either 1 or -1). In both cases the features used are exactly the same. For the regression case all features are deemed relevant whilst for the classification case only a subset of the features are deemed relevant. What am I to make of this? My intuition tells me that in the classification case the subset that is relevant are those features which are "closest" to the current price of the time series and therefore are important in determining the decision boundary whilst the features "further away" are thus irrelevant for classification purposes. On the other hand, for regression, all features are relevant because small log returns imply the features near the boundary are important and large log returns also similarly imply the importance of those features that are "further away." Is this a valid assumption to make?
