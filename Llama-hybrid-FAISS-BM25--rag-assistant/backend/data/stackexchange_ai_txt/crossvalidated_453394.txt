[site]: crossvalidated
[post_id]: 453394
[parent_id]: 452842
[tags]: 
It depends what kind of neural network you are using. A convolutional neural network (CNN) with data augmentation before training should be able to handle this problem, but a multilayer perceptron (MLP), which is the kind of thing you'd see in a beginner tutorial, may not be. A CNN does seem to recognize features, starting with lower level visual features such as edges and curves and building up to more complex features like pointed ears and rounded faces. Convolution matrices are key to this feature recognition, but you wouldn't see this in a MLP. Regarding the problem of recognizing cats in the right half of the picture when training on pictures in the left half, it's common to use data augmentation to generate variations of pictures, so that wouldn't be a problem. If you have enough cat pictures, I suspect it may work even without data augmentation. In any kind of neural network, weights can be trained to handle lower intensity signals. Where this may become a problem is if the cat is black and the photo is terribly underexposed: if you can't see the cat, neither can the neural net. The problem with this understanding is that the distribution of pixels that the neural net learns is not pixel-wise, in the sense that pixels are not independent. It's a joint distribution of all the pixels in the photo, because a pixel's value is only meaningful when looked at in the context of nearby pixels. This is where the convolution matrices in a CNN come in: higher-level features of an image depend on the combination of adjacent pixels, or adjacent lower-level features. Adding more layers to a CNN would allow you to build ever more complex visual features.
