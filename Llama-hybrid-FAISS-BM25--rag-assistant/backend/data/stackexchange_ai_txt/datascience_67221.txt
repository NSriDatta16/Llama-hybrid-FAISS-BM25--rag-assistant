[site]: datascience
[post_id]: 67221
[parent_id]: 42121
[tags]: 
I'll go through your questions one by one: is feature selection more important in KNN than in other algorithms? I don't think it is more important for kNN than for other kinds of algorithms. If a particular feature is not predictive in a neural network, the network will just learn to ignore it. But in KNN, it seems like it could make the prediction worse, right? Correct. Neural Networks are "smarter" algorithms, they have internal weights that adjust to minimize a cost function. Less important features will be attributed comparatively lower importance with respect to highly predictive variables. This doesn't happen in kNN, in which prediction is based exclusively on distance between datapoints - and no information about relative importance of variables can be deduced from it. In a less extreme example, what if a feature is weakly predictive? Rather than normalize all my features so they have an equal weight, wouldn't I want to make the highly predictive features have more weight than less predictive ones? sklearn allows to manipulate kNN weights. But this weights distribution is not endogenous to the model (such as for Neural Networks, that learn that autonomously) but exogenous, i.e. you have to specify them, or find some methodology to attribute these weights a priori , before running your kNN algorithm. If you can do that, and you have good methodological reasons for it, then changing variables' weights can improve your model, but I'd be careful about it (you can easily overfit).
