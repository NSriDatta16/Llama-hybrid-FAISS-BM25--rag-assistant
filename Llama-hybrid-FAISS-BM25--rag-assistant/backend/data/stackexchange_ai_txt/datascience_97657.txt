[site]: datascience
[post_id]: 97657
[parent_id]: 85065
[tags]: 
Just think of Skip-Gram (with negative sampling) as a simple binary logistic classifier. The data is a collection of nearby word pairs $(w,c)$ extracted from a large corpus. For each of those $k$ negative samples are formed by drawing a context word $c'$ from a noise distribution . The model has two layers of parameters without a non-linear function between them (equivalent to matrix multiplication of layer parameters) and a sigmoid function on the output (not softmax). Input and output layers have one node per word and the middle layer has the dimension of embeddings $d$ (e.g. 500). For each word pair $(w, c)$ , feed a one-hot vector representing $w$ and predict 1 at the output node representing $c$ and 0 at each of the negative output nodes $c'$ . Each input word $w$ corresponds to $d$ parameters in the first layer and each context word $c$ has $d$ parameters in the second layer. These are embeddings. Since each word is considered as $w$ or $c$ at different times, each dictionary word has two $d$ -dimensional embeddings (one in each layer), typically only one is used.
