[site]: datascience
[post_id]: 116700
[parent_id]: 107512
[tags]: 
xgboost simply displays the importance of the functions of the dataset on which it was trained, no more. If some functions are so bad that they are not included in any of the trees, then their importance will be 0. You can submit a dataset of size (a x b), and specify n_estimators=1,max_depth=1 and you will see that 1 function will have an importance of 1, all the others 0, because that they didn't even have a chance to get into the trees. You can use some feature selection algorithms to get rid of unnecessary functions, such as boruta, recursive feature elimination, etc.
