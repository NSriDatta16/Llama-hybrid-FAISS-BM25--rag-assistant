[site]: crossvalidated
[post_id]: 548241
[parent_id]: 548235
[tags]: 
Hypothesis testing: why $\mu > 0$ (or even $\mu > \epsilon$ ) "seems easierâ€ to substantiate than $\mu \neq 0$ ? It seems easier because the one-sided t-test and two-sided t-test have different sensitivity for different values. The two-sided t-test has sensitivity split for both positive and negative values. The one-sided t-test has sensitivity for only the positive or only the negative values, and because of that will be 'easier' in substantiating a result (but only for a single side) Below is a graph of the sensitivity or power for the two tests as a function of the true mean of the distribution. (Power is the probability that an observation is a positive result). You can see that the one-sided test is not everywhere 'easier' than the two-sided test. It is only 'easier' for the positive values. Also note that the tests have an equal $5\%$ probability/frequency of a positive result when in reality the effect is negative (when the true mean is equal to zero). So if the true mean is equal to zero then the two hypothesis tests are equally 'easy' in making a false claim of substantiating an alternative hypothesis (like $\mu \neq 0$ or $\mu > 0$ ). But they are still different. They will make these (false) claims for different observations and for a specific observation they are not equally 'easy'. This can occur because there is no unique way to compute p-values and associated hypothesis tests. The p-values based on different methods can be different for a particular observation but on average (for all possible observations) they will be equal. Below you see a simulation of $10 000$ samples of size $n=5$ drawn from a standard normal distribution. We plot the observed (unbiased) sample standard deviation $s$ and the observed sample mean. Along with it we plot the rejected sampled based on a two-sided t-test and a one-sided t-test. The amount of rejected samples is in both cases the same , namely $5\%$ . But, the rejected samples are different for the cases. The one-sided t-test is more sensitive to values on one side. The two-sided t-test splits the sensitivity to two sides. So again, that is why the one-sided test may seem more 'easy' but that is because of the choice to place the sensitivity in a different region. It is only 'easier' for that region. So it may occur that you reject $H_0: \mu \leq 0$ with an alternative hypothesis $H_a: \mu > 0$ , but you can't reject with the same data $H_0: \mu = 0$ with an alternative hypothesis $H_a: \neq 0$ . When such situation occurs then it is seemingly a paradox. But, It is not purely the data that rejected the $H_0$ . It is also your choice and it is also the alternative hypothesis that 'helps' rejecting the null hypothesis. The same hypothesis, with the same data, can be or not be rejected, depending on your arbitrary rejection criteria. Other examples (from this website) were different tests reject a hypothesis for different observations are: ANOVA with F-test vs Tukey's range test . The ANOVA test and Tukey's procedure test the same null hypothesis 'equality of means' but have different p-values for different observations because they relate to different statistics and have sensitivity in different regions. One looks at the largest difference between samples, the other at the variance. Mann-Whitney U test versus t-test can be used to test equality of two means. They have different p-values because the one uses the t-statistic based on the ratio of the difference in the means and the standard deviation, the other computes a statistic based on how often a value in the one sample is larger than a sample in the other sample. Fisher exact test with pooled versus non-pooled data . In that question the Fisher exact test had a surprising behaviour in the power being different when the data was split into groups. But, effectively this was due to using different regions where the tests are sensitive.
