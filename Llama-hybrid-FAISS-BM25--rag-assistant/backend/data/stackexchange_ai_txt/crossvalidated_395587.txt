[site]: crossvalidated
[post_id]: 395587
[parent_id]: 
[tags]: 
Pattern Recognition and Machine Learning (Bishop) - How is this log-evidence function maximized with respect to $\alpha$?

In the book Pattern Recognition and Machine Learning , the author writes the log-evidence function (equation 3.86 in page 167): ln $p(\textbf{t}| \alpha, \beta) = \frac{M}{2}$ ln $\alpha$ + $\frac{N}{2}$ ln $\beta$ - $E (\textbf{m}_N)$ - $\frac{1}{2}$ ln| $\textbf{A}$ | - $\frac{N}{2}$ ln( $2\pi$ ) where $E (\textbf{m}_N) = \frac{\beta}{2} || \textbf{t} - \Phi \, \textbf{m}_N ||^2 + \frac{\alpha}{2} || \textbf{m}_N ||^2 $ (equation 3.82) Then, he differentiates the log-evidence with respect to $\alpha$ and sets to zero (to maximize), getting: $0 = \frac{M}{2\alpha} - \frac{1}{2} || \textbf{m}_N ||^2 - \frac{1}{2} \sum_i{\frac{1}{\lambda_i + \alpha}}$ (equation 3.89) The last term is $\frac{1}{2} \frac{d}{d \alpha}$ ln | $\textbf{A}$ |, it is not important in this question and I have already checked it. The thing is that, as far as I understand, the log-evidence has been differentiated as if $\textbf{m}_N$ was constant with respect to $\alpha$ , which is not the case, because in page 153 we can see its definition: $\textbf{m}_N = \beta \;\textbf{S}_N \; \Phi^T \textbf{t}$ (equation 3.53) where $\textbf{S}_N ^{-1} = \alpha \textbf{I} + \beta \; \Phi \Phi^T$ (equation 3.54) I would appreciate any help to understand the derivation of 3.89 by differentiation of 3.86 with respect to $\alpha$ .
