[site]: crossvalidated
[post_id]: 613328
[parent_id]: 472597
[tags]: 
To my mind there are a few reasons for this: We don't know how to train kernel methods efficiently on datasets are large as we can for neural networks Similarly, neural networks allow more parallel computation Mathematically, many kernel methods (such as kernel ridge regression) satisfy a representer theorem, which says that the learned function lives in the span of a finite set of evaluations of the kernel. I would guess that this is quite restrictive, especially in high dimensions (e.g., for inner product kernels), and that neural networks don't have this drawback. (It's worth saying that what I've called a drawback above for kernel machines is actually a great strength when it comes to theoretical analysis. It might just not help practically)
