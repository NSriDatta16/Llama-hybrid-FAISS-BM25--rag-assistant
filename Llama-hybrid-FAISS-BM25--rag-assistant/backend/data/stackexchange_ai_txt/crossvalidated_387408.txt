[site]: crossvalidated
[post_id]: 387408
[parent_id]: 387401
[tags]: 
If you have the whole population, you are not really doing any inference of a variable, that only happens when you are taking a sample. Let's say you are using a model that predicts weight based on height, so it's $$w = a\cdot h + \epsilon$$ Where $\epsilon$ is some error. Somehow you have collected data on the whole population of the planet. Or even better, any person that ever lived. But then, what's a person? Already, you have some interesting questions here. But let's stick to the plan. For the whole population, you estimate an 'a'. Then we sample down to 1 million people. If it's a random sample, you can establish limits on how far your inference on this sample lies from the true value. In frequentist statistics, you assume a 'true' value of $a$ , which is going to be superclose to the inference on the whole population, and its going to be very close to the inference on the smaller population as well. Under assumptions on the error, the variance of an estimator will be proportional to 1 over the square of 1 million for the sample, and 1 over the square of whatever the size of the whole population for that original inference. This follows from the Central Limit Theorem. So both are close to the 'true' value and they are going to be close to each other as well. I mentioned frequentist, so now I have to mention some other viewpoint as well, but the inference in Bayesian statistics is going to be pretty much equal as well, allthough perhaps you are not really assuming a 'true' value for $a$ , but rather updating your belief after measuring all those people. But the math still holds pretty much and if you do it with the sample it will be extremely close to the inference on the whole population. Regardless of the estimator variance, a more interesting point here is that the model is clearly not the truth. There is no true value for $a$ , it's just a simplification that you may trust to use for your usecase. This holds for any model, however sophisticated it may appear. Another thought, if you have big data it is often the case that you have a lot of data relative to the number of variables that you are estimating. At that scale, using the Central Limit Theorem for deriving estimator variances is sometimes missing the larger point, as in the previous paragraph, your model is wrong and you already know it. For example if you use a simple linear regression such as above, with a population of 1m people, your estimator variance is in the order of 1 over square root of 1 million, that's 0.001. So your report is going to be, " $a$ is contained in the interval $[1.234, 1.235]$ . The significance is through the roof.". But at that point, a better question may be, how well does this model actually predict weight from height? And you apply cross-validation and things like that, and it's going to look like machine learning more.
