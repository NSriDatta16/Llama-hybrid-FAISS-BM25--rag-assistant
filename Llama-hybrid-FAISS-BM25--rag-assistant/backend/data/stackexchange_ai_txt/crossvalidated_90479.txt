[site]: crossvalidated
[post_id]: 90479
[parent_id]: 
[tags]: 
SVD & ICA -- or why doesn't the other rotation matrix in SVD solve for independent components?

When data are a linear mixture of non-gaussian sources, it can be shown that with a rotation, an independent rescaling of each of the rotated axes, and a second rotation you can recover the original, independent axes. Singular value decomposition [SVD] does exactly these three operations -- i.e., it decomposes a matrix X into U , S , and V [i.e, X = USV' ], where U & V are rotation matrices. SVD even finds that first rotation [i.e., the PCs] and the scaling for axes [the eigenvalues of the covariance matrix], but the second rotation isn't the rotation to the independent axes. I know the dimensions of the second rotation are wrong, but still... So here's my [possibly very naive] question: why doesn't SVD find this second rotation to recover the original, independent axes? There's been a lot posted on how SVD relates to PCA, which makes sense -- but how does SVD relate to ICA? Why can't you recover the unmixing matrix with SVD? or is this just the wrong question to ask... EDIT: After reading about this a bit more, I feel like I have a good intuition for why SVD, despite performing the necessary matrix operations [rotate, rescale, rotate], doesn't find the independent components. Both rotation matrices are only sensitive to the correlations in the data -- the U & V matrices are, respectively, the eigenvectors of XX' and X'X . Thus, these two rotations are only sensitive to second-order correlations in the data -- they are not sensitive to higher-order moments [e.g., kurtosis], and these higher-order moments are necessary to find independent projections. This seems satisfying to me. If others have insights, please let me know!
