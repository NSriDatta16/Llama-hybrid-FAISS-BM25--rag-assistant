[site]: crossvalidated
[post_id]: 452377
[parent_id]: 
[tags]: 
Why correlation changed to 100% after normalising variables as ratios?

There are historical prices for GOLD and SP500. But they are measured in USD which is volatile on its own and also produces false trend because of inflation. We don't care about "USD" and would like to simplify things, so let's switch measure unit from "USD" to basket of "0.5 GOLD + 0.5 SP500". Now at any given moment we measure prices of GOLD and SP500 not in USD, but in such GOLDSP500 basket. And there's problem - the correlation changes . Why? Example Let's suppose a = GOLD and b = SP500 : # Historical prices of a and b in USD a = [1, 3, 5, 2] b = [3, 2, 2, 3] # Historical prices for ab basket in USD a_b = (a + b) ./ 2 # New prices of a, b measured in basket, or ratios a_r = a ./ a_b b_r = b ./ a_b Question 1 : The strange thing is that the correlation for rates will be always = -1. For cor(a, b) = -0.84 and cor(a_r, b_r) = -1.0 . I understand why it's changed - because the basket is not independent, but why it's always = -1 for any two random arrays? Question 2 : Is that more or less a reasonable way to normalize price time series? Or it's fundamentally broken and should't be used at all? Is there a better way to normalize it? Plots of a, b (gold and blue), a_b (black) and scatter plots of a to b and a_r to b_r
