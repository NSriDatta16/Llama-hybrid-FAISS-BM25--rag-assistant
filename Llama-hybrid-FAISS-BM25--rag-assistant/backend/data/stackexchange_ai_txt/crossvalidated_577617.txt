[site]: crossvalidated
[post_id]: 577617
[parent_id]: 
[tags]: 
Why xgboost BDT model constructed with histogram method depends on the training data ordering?

I was using Python xgboost to train some models (with binary logistic) on some data (50k in total) and I used the histogram tree method for the training (tree_method="hist"). By accidence I shuffled the events in the data and compared the resulting bdt. It turned out that the models built are slightly different depending on the order of events and the prediction on validation set (different than training set) could vary up to 10%. As a double check I also used the lightgbm and this effect is also presented. It seems that this is a problem of histogram approximation because if I use the exact method in xgboost (tree_method="exact") then this problem disappears. Does anyone know why the bdt model based on histogram method depends on the event order? I tried to look for the reference paper but was totally lost. I also tried to look at the github code but sadly I understood nothing (sorry my major is physics). PS1 If there are relevant tags please add it. PS2 The following code is a toy model to show this effect. import numpy as np import xgboost as xgb """ xgb version : 1.6.1 np version : 1.22.3 """ n_sig = 1000 n_bkg = 10000 n_test_sig = 250 n_test_bkg = 250 sig_1 = np.random.randn(n_sig,3) sig_2 = np.random.randn(n_sig,3) bkg = np.random.uniform(-10,10,(n_bkg,3)) test_sig = np.random.randn(n_test_sig,3) test_bkg = np.random.uniform(-10,10,(n_test_bkg,3)) data_1 = np.concatenate([sig_1,sig_2,bkg]) label_1 = np.concatenate([np.ones(n_sig,dtype=np.int8),np.ones(n_sig,dtype=np.int8),np.zeros(n_bkg,dtype=np.int8)]) data_2 = np.concatenate([sig_1,bkg,sig_2]) label_2 = np.concatenate([np.ones(n_sig,dtype=np.int8),np.zeros(n_bkg,dtype=np.int8),np.ones(n_sig,dtype=np.int8)]) data_test = np.concatenate([test_sig,test_bkg]) dm_1 = xgb.DMatrix(data_1,label=label_1) dm_2 = xgb.DMatrix(data_2,label=label_2) dm_test = xgb.DMatrix(data_test) hp = {"objective":"binary:logistic", "tree_method":"hist", "eta":0.3, "validate_parameters":False} bdt_1 = xgb.train(hp,dm_1,20) bdt_2 = xgb.train(hp,dm_2,20) pred_1 = bdt_1.predict(dm_test) pred_2 = bdt_2.predict(dm_test) print(f"whether predictions are the same: {np.all(pred_1==pred_2)}")
