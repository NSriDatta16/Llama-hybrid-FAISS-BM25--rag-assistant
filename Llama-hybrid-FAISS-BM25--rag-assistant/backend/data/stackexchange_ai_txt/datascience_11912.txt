[site]: datascience
[post_id]: 11912
[parent_id]: 
[tags]: 
Choosing regularization method in neural networks

When training neural networks, there are at least 4 ways to regularize the network: L1 Regularization L2 Regularization Dropout Batch Normalization plus of course other things like weight sharing and reducing the number of connections, which might not be regularization in the strictest sense. But how would one choose which of those regularization methods to use? Is there a more principled way than "just try everything and see what works"?
