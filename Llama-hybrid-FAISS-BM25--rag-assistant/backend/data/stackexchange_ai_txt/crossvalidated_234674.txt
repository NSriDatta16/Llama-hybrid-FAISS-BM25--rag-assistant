[site]: crossvalidated
[post_id]: 234674
[parent_id]: 234657
[tags]: 
First, I would have different perspective for machine learning. What you mentioned, Andrew Ng's Coursera lecture and Kaggle competition are not 100% of machine learning but some branches that targeted on practical applications. Real machine learning research should be the work that invent the random forest / SVM / gradient boosting model, which is fairly close to statistics / math. I would agree machine learning practitioners focus more on accuracy comparing to statisticians / economists. There are reasons that people interested in getting better accuracy, rather than "inference about the true distribution." The major reason is the way we collect data and use the data has been changed over past decades. Statistics was established for hundred years, but in the past, no one would think about you have billions of data for training and other billions of data for testing. (For example, number of images on Internet). Therefore, with relatively small amount of data, assumptions from domain knowledge are needed to do the work. Or you can think about to "regularize" the model. Once the assumptions were made, then there are inferences problems about the "true" distribution. However, if we carefully think about it, can we make sure these assumptions are true, and the inferences are valid? I would like cite George Box: All models are wrong but some are useful Now, let's back to think about the practical approach to put more emphasis on accuracy than assumption / inference. It is a good approach, when we have huge amount of data. Suppose we are building a model for all images contain human faces on pixel level. First, it is very hard to propose the assumptions on pixel level for billion of images: no one has that domain knowledge. Second, we can think about all possible ways to fit the data, and because the data is huge, all the models we have may not be sufficient (almost impossible to over fit). This is also why, "deep learning / neural network" got popular again. Under the condition of big data, we can pick one model that really complex, and fit it as best as we can, and we may still OK, because our computational resources are limited, comparing to all the real data in the word. Finally, if the model we built are good in huge testing data set, then they are good and valuable, although we may not know the underline assumption or true distribution. I want to point out the word "inference" has different meanings in different community. In statistics community, it usually means getting information of the true distribution in parametric or non-parametric way. In machine learning community, it usually means computing certain probabilities from a given distribution. See Murphy's Graphical Models Tutorial for examples. In machine learning, people use the word "learning" to represent "getting the parameters of the true distribution", which is similar to the "inference" in statistics community. So, you can see, essentially, there are many people in machine learning are also doing "inference". In addition, you may also think about people in academia like to "re-brand their work and re-sell": coming up with new terms may be helpful to show the novelty of the research. In fact, there are many overlaps among artificial intelligence, data mining and machine learning. And they are closely related to statistics and algorithm design. Again there are no clear boundaries for doing "inference" or not.
