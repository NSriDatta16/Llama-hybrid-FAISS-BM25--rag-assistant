[site]: crossvalidated
[post_id]: 476856
[parent_id]: 
[tags]: 
Calculating P(x | Ck) for Naive Bayes Classifier

I understand the theory behind Bayes Theorem and the theory behind the Naive Bayes Classifier but I am having a hard time understanding the specific implementations described at https://nicolovaligi.com/naive-bayes-tensorflow.html and https://www.kaggle.com/sklasfeld/tensorflow-naive-bayes-classifier/notebook My understanding is that p(x | Ck) means "The probability that a paragraph will contain word x given that it belongs to class k ". To calculate the p(x | Ck), the authors do the following: Take all paragraphs associated with a class Convert each paragraph into a vector of numbers (e.g. using the tensorflow universal sentence encoder) Calculate the mean, variance of the vectors (paragraphs) associated with each class. This provides a continuous probability density function (PDF), which is nice, but I am expecting them to do the following instead: Create a set of all words across the training data, across all classes. Iterate over the paragraphs of each class. For each word found in step 1, record a value of 1 if the paragraph contains the word; otherwise, record a value of 0 . Use the aforementioned sum to generate a discrete P(x | Ck) per word. Obviously it is nicer to have a single continuous PDF than multiple discrete probabilities per word but this sounds more correct to me. I have two questions: Is the authors' approach an acceptable way to calculate P(x | Ck)? If so, why? Is my approach an acceptable way to calculate P(x | Ck)? Can we do better?
