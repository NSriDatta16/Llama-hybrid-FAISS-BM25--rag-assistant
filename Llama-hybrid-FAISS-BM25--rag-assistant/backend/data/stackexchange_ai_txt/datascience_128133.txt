[site]: datascience
[post_id]: 128133
[parent_id]: 128123
[tags]: 
Cross-attention appeared in CAT 1 paper where, for a given image, authors proposed the use of self-attention itself on, first, the 'inner patches' (local information) followed by self-attention between the patches - 'cross patches'. CrossViT 2 then used this term and proposed dual-branch transformer architecture to get better image representations . They process small-patches and large-patches in separate branches and fuse them by using 'cross-attention fusion'. References: 1 Lin, Hezheng, et al. " Cat: Cross attention in vision transformer. " 2022 IEEE International Conference on Multimedia and Expo (ICME). IEEE, 2022. 2 Chen, Chun-Fu Richard, Quanfu Fan, and Rameswar Panda. " Crossvit: Cross-attention multi-scale vision transformer for image classification. " Proceedings of the IEEE/CVF international conference on computer vision. 2021. [Update1] As I was watching Sir Karpathy's video on " Let's build GPT ", I noticed he mentioned following simple statement at one point: "self-attention" just means that the keys and values are produced from the same source as queries. In, "cross-attention", the source of keys and values is different from that of queries.
