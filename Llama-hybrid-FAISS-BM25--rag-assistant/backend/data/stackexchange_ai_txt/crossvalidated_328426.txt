[site]: crossvalidated
[post_id]: 328426
[parent_id]: 
[tags]: 
Classification based on explanatory variables being within ranges?

I'm trying to build a classification model where observations are classified based not on some linear combination of their explanatory variables, but instead based on whether the variables are all within some (unknown) ranges. So, for example, if I had two independent variables, and my observations (independent, followed by dependent variables) are: (0, 5): 0 (2, 7): 1 (3, 2): 0 (4, 4): 1 (3, 9): 0 (6, 6): 0 then perhaps the model would learn that the dependent variable is 1 iff the first independent variable is between 1 and 5, and the second independent variable is between 3 and 8. (I only want "and"s in the model; I'm not looking for any cases where variable 1 is between a and b OR variable 2 is between c and d.) In essence, the model should find some hyperrectangle in the independent variables with all edges parallel to the axes, where 0s are expected to be outside the rectangle and 1s are expected to be inside it. (In the example data, the data points are separable; this is not the case in my real problem. In the spirit of an SVM or logistic regression, the model should do the best it can.) My first stab at this involved trying to do least-squares regression with a hypothesis function made up of two sigmoids for each independent variable (one "forward", one "backward"), parameterized to slide back and forth, all of which were multiplied together. Maybe my scipy-fu is weak, but above four variables, I started having convergence issues with that approach. Is there some tried-and-true way to solve this kind of classification problem? (Even just a name for this kind of problem would be helpful!) [This is a blatant cross-post from Data Science , after that post didn't get much engagement; if this behavior is frowned upon here, I'll delete.]
