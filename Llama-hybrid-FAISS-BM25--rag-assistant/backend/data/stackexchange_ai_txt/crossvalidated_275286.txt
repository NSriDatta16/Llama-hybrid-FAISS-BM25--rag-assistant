[site]: crossvalidated
[post_id]: 275286
[parent_id]: 274018
[tags]: 
I'll provide a few comments in the language of machine learning that will hopefully lead you to a helpful logical conclusion. First, minimizing that quadratic objective is like solving a least squares problem (if this is not obvious, try proving it as an exercise). Second, for any least squares problem, if the features are orthogonal, then estimating the coefficients seperately or sequentially (like doing exactly one round of coordinate descent) is equivalent to estimating them jointly. (If this isn't obvious, then suppose the features are orthogonal. Do you see this means $A$ must be diagonal? That means each entry of the solution does not depend on the others). So now the question is: How can we solve the same problem, but with a diagonal matrix in place of $A$? Third, the $\ell_2$ norm is orthogonally-invariant, so if you left or right multiply whatever is inside the norm by an orthogonal matrix (which is interpreted as a rotation), you can just solve that problem then back out that orthogonal transformation at the end. Since $A$ is symmetric positive semi-definite, we can get those orthogonal matrices from the eigenvalue decomposition of $A$ (aka by "diagonalizing" $A$). Back to statistics: This process is sometimes referred to as whitening or pre-whitening though I believe that there is a lack of concesus as to the usage of this term. Put simply and loosely, in the eigenspace of $A$, the columns/rows of $A$ can be viewed as totally separate and unrelated pieces of information.
