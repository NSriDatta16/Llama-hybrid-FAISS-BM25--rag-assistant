[site]: crossvalidated
[post_id]: 521889
[parent_id]: 521835
[tags]: 
Parsimony usually a desirable feature in a machine learning model - this relates to the idea of Occam's Razor, that among models that produce equally good (or bad) results, you should generally prefer the simpler one. This is explicitly performed in certain model selection approaches such as the Minimum Description Length, which describes the "cost" of a model as related both to how well the model fits the data, and the complexity of the model. For two models that both perform poorly on a dataset, the simpler model is preferred. Overfit models typically require extra parameters that add "cost" to a model with no discernable benefit, so you're usually better off with an underfit model that yields similar error. In a way, an overfit model is like a student who memorizes the sequence of answers on a multiple choice practice test, applies that same sequence to the real test, and fails. The underfit model, on the other hand, is the student who simply picks "C" for every answer, and does as poorly on the real test as they did on the practice. The overfit student is "surprised" by their poor performance, and did a lot of work for absolutely no benefit. The underfit student knows their strategy won't work well, but can implement that strategy very easily. Both students fail the test, but at least one knew ahead of time and didn't waste a lot of effort for nothing.
