[site]: crossvalidated
[post_id]: 362823
[parent_id]: 362711
[tags]: 
Remember that R's cos() works in radians. So 1:3000 is a lot of cycles, all squeezed together tightly. (Do plot(x,y) to see the training data you are giving to the deep learning model.) However, I think the root problem you are having is that "x" is your only predictor variable. You train on x values of 1 to 2000, then ask it what 2500 to 3000 might give. That is rather hard. I think you might get better results if you split the training data randomly; see h2o.splitFrame() . Then, e.g. it might train on x=1,2,4,5,6,8 and you will ask it to predict x=3 and x=7. It might be able to learn the relationship between x=3 and the nearby x values. BTW, more generally, the first thing I would try is to get rid of all your arguments, and use a default model. H2O default settings are generally good. I suppose I might increase epochs, from the default 10, given that the data is not too large: m = h2o.deeplearning( "C1", "C2", as.h2o(traincos), as.h2o(validcos), epochs=1e3 ) And then add one tuning argument at a time, to see if it makes things better or worse. Rather than messing with learning rates, or the more exotic parameters, the first tuning I might try is a deeper network. E.g. hidden=[200,200,200] instead of the default [200,200] . UPDATE: I did a few experiments. I also got poor results, at least until I tried all the ideas together. Here are some of the things I tried, and what happened: Choosing A Tighter Region Instead of x = 1:3000 I tried: x = seq(0, 30, length.out = 3000) This gives just over 3 full cycles. This was the thing that made the biggest difference. Deeper Network: epochs = 1000, activation = "Tanh", hidden = c(40,40,40,40,40) The switch to Tanh was just because I got numerical instability. This didn't really help when x was 1:3000 , but did when x was just 3 cycles. However I didn't experiment further to narrow down what is optimal. It did take 600 to 700 epochs to get the best validation error. Some Hints: x2 = x^2 x3 = x^3 x4 = x^4 x5 = x^5 x^3, x^5 and x^7 should be enough to give a good prediction from -PI to +PI (See http://pages.pacificcoast.net/~cazelais/187/maclaurin-sin.pdf ). Pretending to be honest I tried just the first 5 powers. I then later also added x^6 and x^7. The default model with these hints was no better than without them. But they did help the deeper model, and the deep model was better with x^6 and x^7 than without. With the hints up to x^7, and using the deep network, and using just 3 cycles, this was the best result I got: (green is on training data, blue is on validation data, red is the test data.) Using splitFrame I only briefly experimented with this; judging from the score history, it was not learning very well. However I only tried it when x=1:3000 , so let's try it with my last experiment... The split code: x = seq(0,30,length.out = 3000) y = cos(x) x2 = x^2 x3 = x^3 x4 = x^4 x5 = x^5 x6 = x^6 x7 = x^7 data = as.h2o(data.frame(y, x, x2, x3, x4, x5, x6, x7)) parts = h2o.splitFrame(data, c(0.67, 0.167)) train = parts[[1]] valid = parts[[2]] test = parts[[3]] The split ratios mean I am still training on 2000 values, using 500 for validation, and 500 for test. Then the model building: m_deep = h2o.deeplearning(2:ncol(data), "y", training_frame = train, validation_frame=valid, epochs = 1000, activation = "Tanh", hidden = c(40,40,40,40,40) ) m_deep h2o.performance(m_deep, test) plot(m_deep) You will notice the MSE is now nicely low, and the score history graph looks satisfying too. Finally predictions and plotting: m=m_deep # The -1 in next few lines means exclude the answer pt = as.data.frame(h2o.predict(m, train[,-1])) ptdata = data.frame(as.data.frame(train$x),y=pt) pv = as.data.frame(h2o.predict(m, valid[,-1])) pvdata = data.frame(as.data.frame(valid$x),y=pv) p = as.data.frame(h2o.predict(m, test[,-1])) pdata = data.frame(as.data.frame(test$x),y=p) plot(as.data.frame(data[,c(2,1)]), type="p",pch=".") points(ptdata, col="green",pch=".") points(pvdata, col="blue",pch=".") points(pdata, col="red",pch=".") The chart shows predictions on unseen data is basically as good as on training data. It does show that it is not very good at predicting the extreme values (as it never saw enough examples of them?):
