[site]: crossvalidated
[post_id]: 625553
[parent_id]: 
[tags]: 
KDE-like technique to learn a continuous distribution from samples subject to specific noise

There's a continuous-valued random variable $X$ with distribution $f_X$ . Normally, we're given a bunch of i.i.d. samples $X_1, \ldots, X_n$ , and we try to give an estimate $\hat{f}_X$ of the distribution using something like KDE. However, what if we're given noisy samples $Y_1,\ldots,Y_n$ , where the conditional distribution $f_{Y|X}$ is known . Then given the noisy samples, how do we get a decent estimate $\hat{f}_X$ ? As an example, if samples were messed up by additive standard Gaussian noise, then we'd have $f_{Y|X}(y|x) = \mathcal{N}(x, 1; y)$ . To be clear, I know that if the noise distribution is really bad (e.g. maps every $X$ to the same $Y$ ), then there's no hope in recovering the original distribution. So make any reasonable assumptions you want about the noise distribution so that you can give an answer. My guess is that the solution might either involve deconvolution, or else some kind of Expectation Maximization. For example, one procedure I guess might work is: Initialize a guess for $\hat{f}_X$ (e.g. uniform) Assume $\hat{f}_X$ is the actual distribution, and find posterior distributions on all of sample $X_i$ 's given the $Y_i$ 's. Sample each of those posterior distributions to get guesses $\tilde{X}_i$ at what each of the $X_i$ 's are. Use KDE on the guesses $\tilde{X}_i$ to update what you think $\hat{f}_X$ should be. Keep repeating steps 2-4 until convergence. And perhaps, for steps 3-4, you can sample multiple times and average together all the KDE estimates to get your final $\hat{f}_X$ .
