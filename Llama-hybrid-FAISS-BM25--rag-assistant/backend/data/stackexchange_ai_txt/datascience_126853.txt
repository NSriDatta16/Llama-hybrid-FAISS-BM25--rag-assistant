[site]: datascience
[post_id]: 126853
[parent_id]: 
[tags]: 
How to add a new item in the embeddings vocabulary?

Imagine you have trained a model containing an Embedding layer. Your model performs well and you're happy with your embedding. Then, suddenly, you want to add a new item in your vocabulary. In other words you want to compute the embedding of this new item. Basically an Embedding layer is a lookup table, used to turn positive integers into dense vectors of fixed size, and now you want to consider a new integer that was not there during the training. How can you do this without retraining the model from scratch? Does it make sense (after some matrix shape adjustement) to relaunch the training freezing all the parameters except the ones used to embed the new item?
