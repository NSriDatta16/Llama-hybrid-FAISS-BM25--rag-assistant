[site]: datascience
[post_id]: 88780
[parent_id]: 
[tags]: 
Unbiased Predictions for all Distinct Training Subsets

Suppose I have a data set $\left(X_i \in \chi, y_i \in \zeta \right)$ where $X_i$ and $y_i$ correspond to instances and labels, and $\chi$ and $\zeta$ correspond to the space where $X_i$ and $y_i$ exist. For example, if I'm trying to use blood pressure ( $X$ ) to predict resting heart rate ( $y$ ), I might have the dataset $\left((X_i, y_i) = (120, 60), (130, 58), (118, 72), ... \right)$ . Furthermore, using assumptions about the heart rate and blood pressure of people, I can say $\chi = [50, 90] $ and $\zeta = [95, 180]$ where "[]" is a closed interval in $\mathbb{R}$ . In otherwords, I know that nobody has a resting heart rate lower than 50 and higher than 90, and I also know nobody has a resting blood pressure lower than 95 and greater than 180. Here's the question. Suppose I partition my set $\zeta$ into $k$ distinct subsets $\zeta_1, \zeta_2, ... \zeta_k$ . These subsets, for example, might be $\zeta_1 = [95, 130], \zeta_2 = [130,160], \zeta_3 = [160,180]$ (i.e. different groups of people based on blood pressure: low, medium and high blood pressure). Suppose for each subset of labels, I want my machine learning model to make unbiased predictions . I should note that a model that makes unbiased predictions on $\zeta$ as a whole (trained with a regular RMSE loss function) can still make biased predictions on each $\zeta_i$ (it might overpredict the blood pressure of people with low blood pressure and underpredict the blood pressure of people with high blood pressure, for example). What is the best loss function to use to prevent this from happening, and make unbiased predictions in each label region $\zeta_i$ ? I've tried training with the loss function $L = L_0 + \alpha \sum_{j=1}^k \frac{1}{|\zeta_j|}\sum_{y_i \in \zeta_j} (\hat{y}_i - y_i)$ where $L_0$ is the MSE loss function and $\hat{y}$ are the predictions, and while this does a decent job, it can often lead to unstable training conditions. In addition, it requires using many instances in a single gradient descent step (note the summation over instances).
