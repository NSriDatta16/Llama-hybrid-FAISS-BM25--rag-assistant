[site]: crossvalidated
[post_id]: 462880
[parent_id]: 462874
[tags]: 
Unfortunately, this is not ok. If you follow the proposed procedure, the final error estimate (using the test set) will be downwardly biased, and the model will appear to generalize better than it actually will. This is because data from the test set has already been used to select the hyperparameters. To obtain an (asymptotically) unbiased estimate of generalization performance, the test set must be independent from data used to fit the model, including hyperparameter tuning. If the dataset is small, use nested cross validation. This will properly maintain independence of the training/validation/test data, but yield lower variance error estimates compared to simple holdout (single training/validation/test split). To reduce the variance even further, use repeated nested cross validation (i.e. repeat nested cross validation multiple times, partitioning the data randomly each time, then average the error across repetitions).
