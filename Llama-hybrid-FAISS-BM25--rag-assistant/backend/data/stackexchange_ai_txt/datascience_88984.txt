[site]: datascience
[post_id]: 88984
[parent_id]: 
[tags]: 
How does Google's Universal Sentence Encoder deal with out-of-vocabulary terms?

It seems to output embeddings even for random jibberish, and the similarity is even high for this particular pair of jibberish. np.inner(embed('sdasdasSda'), embed('sadasvdsaf')) array([[0.70911765]], dtype=float32) I'm wondering how sentences are tokenized and what preprocessing steps are done internally. Also, how is the embedding model trained? As I understand it, they use a Deep Averaging Network, which is another neural network applied on the average of the individual word embeddings?
