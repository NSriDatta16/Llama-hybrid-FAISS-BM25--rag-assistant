[site]: crossvalidated
[post_id]: 593776
[parent_id]: 
[tags]: 
How to apply transfer learning to Lasso model (or other linear models)?

I am familiar with the use of Transfer Learning to Neural Networks models, but I am wondering whether it would be possible to apply it to linear models, and in particular to parsimonious models such as Lasso. More formally, I have an initial dataset $D$ containing $N$ observations $(y^{(i)}, \mathbf{X}^{(i)})_{1 \leq i \leq N}$ , where $y^{(i)}$ is a (scalar) response variable, and $\mathbf{X}^{(i)} \in \mathbb{R}^{p}$ is a vector of predictors. This dataset is used to fit a linear model $\mathcal{L}$ , which writes in vector notation: $y = \mathbf{\alpha}^T \mathbf{X}$ , where $\mathbf{\alpha} \in \mathbb{R}^{p}$ is the vector of model coefficients (note that $\mathbf{\alpha}$ has many zero, or at least very small, components if I use a Lasso regularization). Then, I have a second dataset $D'$ containing $M$ observations $(y, \mathbf{X})$ . Typically this second dataset is much smaller than the initial one: $M \ll N$ . The objective is now to fit a new linear model $\mathcal{L}'$ on $D'$ , while reusing the initial model $\mathcal{L}$ . $\mathcal{L}'$ could write: $y = \mathcal{T}(\mathbf{\alpha})^T \ \mathbf{X}$ , where $\mathcal{T}(\mathbf{\alpha})$ is a function of the initial model coefficients, that also depends on the new dataset $D'$ . The new model coefficients $\mathcal{T}(\mathbf{\alpha})$ could be a nonlinear function of $\mathbf{\alpha}$ , but it could also depend linearly on $\mathbf{\alpha}$ : $\mathcal{T}(\mathbf{\alpha}) = \mathbf{\beta} \mathbf{\alpha} $ with $\mathbf{\beta} \in \mathbb{R}^{p \times p}$ . I have a few questions about how to construct the new model coefficients $\mathcal{T}(\mathbf{\alpha})$ , and once again I would like to emphasize that my main concern is related to the very low-data regime, where $M \ll p , N$ . Would a nonlinear $\mathcal{T}(\mathbf{\alpha})$ be preferable to a linear one? In particular, I am thinking of incorporating any type of available prior knowledge or inductive bias into the nonlinearity of $\mathcal{T}(\mathbf{\alpha})$ . This is a very broad question, but any suggestion of reference to literature would be welcome. In the case where $\mathcal{T}(\mathbf{\alpha}) = \mathbf{\beta} \mathbf{\alpha} $ is linear, there is nothing that guarantees that the new model coefficients would still be sparse. Is there any type of method to impose a structure on the $\mathbf{\beta}$ matrix, such that there would a guarantee on the resulting $\mathcal{T}(\mathbf{\alpha})$ to still be sparse? In particular, how to preserve the same sparsity structure as the original $\mathbf{\alpha}$ ? If I have some prior knowledge about the general structure of $\mathbf{\beta}$ , are there any Bayesian method that could help me to incorporate this prior information into $\mathbf{\beta}$ , and would this help to deal with the very low-data regime? How would you perform cross-validation to select the best possible $\mathbf{\beta}$ ? $M$ could be really small, so I am thinking of something like "leave one out" or the "jackknife", to keep as many observations as possible for the training itself.
