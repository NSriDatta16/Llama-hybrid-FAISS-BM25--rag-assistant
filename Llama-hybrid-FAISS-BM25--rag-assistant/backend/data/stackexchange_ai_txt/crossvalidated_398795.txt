[site]: crossvalidated
[post_id]: 398795
[parent_id]: 398322
[tags]: 
Just adding to @Fabian Werner’s answer - do you remember doing Riemann Sums rule in an intro to integration? Well that too was a set of evenly partitioned if statements which you use to calculate the area under the function. If you draw a 1D function and draw the partitions evenly what you will find is that in areas where the function has little gradient, neighboring partitions can be merged together without a great loss in accuracy. Equally, in partitions with high gradient adding more partitions will significantly improve the approximation. Any set of partitions will approximate the function but some are clearly better than others. Now, moving to CART models - we see data in the form of noisy points from this function and we are asked to approximate the function. By adding too many partitions we can overfit and essentially perform a nearest neighbor type model. To avoid this we limit the number of partitions our model can use (usually in the form of max depth and min samples per split). So now where should we place these splits? That is the question addressed by the splitting criteria. Areas with higher “complexity” should receive more splits as a rule of thumb and that is what gini, entropy, etc. endeavour to do. Making predictions are just if-else statements but in the context of machine learning that is not where the power of the model comes from. The power comes from the model's ability to trade off over and under fit in a scalable manner and can be derived in a consistent probabilistic framework with theoretical guarantees in the limit of data. Finally, if we take a similar abstracted view of ML models we can say neural networks, kernel methods, Monte Carlo approaches and many more are simply addition and multiplication. Unfortunately, that is not a very useful view of the literature.
