[site]: crossvalidated
[post_id]: 34693
[parent_id]: 34611
[tags]: 
The described difference is IMHO bogus. You'll observe it only if the distribution of truely positive cases (i.e. reference method says it is a positive case) is very unequal over the folds (as in the example) and the number of relevant test cases (the denominator of the performance measure we're talking about, here the truly positive) is not taken into account when averaging the fold averages. If you weight the first three fold averages with $\frac{4}{12} = \frac{1}{3}$ (as there were 4 test cases among the total 12 cases which are relevant for calculation of the precision), and the last 6 fold averages with 1 (all test cases relevant for precision calculation), the weighted average is exactly the same you'd get from pooling the predictions of the 10 folds and then calculating the precision. edit: the original question also asked about iterating/repeating the validation: yes , you should run iterations of the whole $k$ -fold cross validation procedure: From that, you can get an idea of the stability of the predictions of your models How much do the predictions change if the training data is perturbed by exchanging a few training samples? I.e., how much do the predictions of different "surrogate" models vary for the same test sample? You were asking for scientific papers : search terms are iterated or repeated cross validation. Papers that say "you should do this": Dougherty, E. R.; Sima, C.; Hua, J.; Hanczar, B. & Braga-Neto, U. M.: Performance of Error Estimators for Classification Current Bioinformatics, 2010, 5, 53-67. is a good starting point. For spectroscopic data, I did some simulations Beleites, C.; Baumgartner, R.; Bowman, C.; Somorjai, R.; Steiner, G.; Salzer, R. & Sowa, M. G.: Variance reduction in estimating classification error using sparse datasets. Chem.Intell.Lab.Syst., 2005, 79, 91 - 100. preprint I use it regularly, e.g Beleites, C.; Geiger, K.; Kirsch, M.; Sobottka, S. B.; Schackert, G. & Salzer, R.: Raman spectroscopic grading of astrocytoma tissues: using soft reference information.Anal Bioanal Chem, 2011, 400, 2801-2816 Underestimating variance Ultimately, your data set has finite (n = 120) sample size, regardless of how many iterations of bootstrap or cross validation you do. You have (at least) 2 sources of variance in the resampling (cross validation and out of bootstrap) validation results: variance due to finite number of (test) sample variance due to instability of the predictions of the surrogate models If your models are stable, then iterations of $k$ -fold cross validation were not needed (they don't improve the performance estimate: the average over each run of the cross validation is the same). However, the performance estimate is still subject to variance due to the finite number of test samples. If your data structure is "simple" (i.e. one single measurement vector for each statistically independent case), you can assume that the test results are the results of a Bernoulli process (coin-throwing) and calculate the finite-test-set variance. out-of-bootstrap looks at variance between each surrogate model's predictions. That is possible with the cross validation results as well, but it is uncommon. If you do this, you'll see variance due to finite sample size in addition to the instability. However, keep in mind that some pooling has (usually) taken place already: for cross validation usually $\frac{n}{k}$ results are pooled, and for out-of-bootstrap a varying number of left out samples are pooled. Which makes me personally prefer the cross validation (for the moment) as it is easier to separate instability from finite test sample sizes.
