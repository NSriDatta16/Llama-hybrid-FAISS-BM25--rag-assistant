[site]: crossvalidated
[post_id]: 616325
[parent_id]: 616314
[tags]: 
I don't think this is immediately possible with XGBoost as you would have to write a multi-output / multi-parameter boosting variant of it. At best, XGBoost (and other usual boosting routines learners) are able to do multi-output predictions (for example estimating the parameters for a Gamma distribution) by having one model for each target and then putting meta-estimators on top. That said, what you primarily want is NGboost and the concept of "natural gradients" (NGs). NGs allow us to take into account the geometry of the underlying probability space, they are implemented in the package ngboost . In a standard NGBoosts application, we use Maximum Likelihood Estimation as our scoring function to try and minimize the Kullback-Leibler (KL) divergence between the current distribution and the target distribution. NGBoost explores a divergence (scoring function) -specific statistical manifold where each point in the manifold corresponds to a probability distribution. This is a significant change to standard point estimation and allows our booster to output a full probability distribution over the entire outcome space instead of a point estimate. We can then use the NGBoost provided pred_dist function such that we get estimated distributional parameters. With ngboost for example, for a Gamma distribution we estimate the $\alpha$ (shape) and $\beta$ (rate) parameters aiming to minimize the KL-divergence. In contrast, XGBoosts (and other boosters) if minimising a Gamma distributed target focus directly on the mean Gamma deviance ( mean_gamma_deviance of the point estimates.)
