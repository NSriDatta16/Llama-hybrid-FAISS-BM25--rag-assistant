[site]: datascience
[post_id]: 93173
[parent_id]: 
[tags]: 
Explanation of random forest performance difference to when using categories and when using dummy variables

I have some hand coded feature which is a category with values "High", "Low", and "Normal". I created this feature myself and my problem performance (classification) increased dramatically when using it with expanding these by dummy variables. Now since I'm trying random forest, I thought I change "High, Low, Normal" to 1, -1, 0 instead. Now the same model doesn't learn at all. I thought it should become easier actually for it to split. Does this have to do with me putting normal to 0? Thank you for any explanation helping me to understand this.
