[site]: crossvalidated
[post_id]: 398310
[parent_id]: 398261
[tags]: 
Is there some reason for working with a different inner product other than just trying for some non-linear decision boundary? Yes. SVM finds linear boundaries, so it can't separate classes of some datasets. Using kernels you can use SVM as if it operated in higher-dimensional space into which you embed your datapoints (since this embedding might be nonlinear, it might happen that embedded classes become linearly separable). This is called the kernel trick . What are some desirable properties for an inner product for a given problem? Unfortunately I don't think there is single general answer for that - in practice you'd need to find a kernel that separates the classes well, but does not overfit (for example RBF kernel is known to be able to represent any decision boundary if you choose appropriate parameters, but it might not generalize well then). Can someone give me an example of a problem in which a specific kernel is a better notion of inner product than the dot product? One simple example would be two concentric circles. I have collected a couple examples in this and that notebook.
