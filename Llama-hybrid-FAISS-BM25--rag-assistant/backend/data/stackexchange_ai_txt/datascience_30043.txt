[site]: datascience
[post_id]: 30043
[parent_id]: 29991
[tags]: 
I am not sure what you actually want to do, but if you want to simulate the Markov chain, than you really have to bring probabilities into play. Thus, in each step, you would use the transition matrix to determine the probability for each of the possible target states - A, B and C in your example. You would then draw a value - let me call it U - from a uniform distribution on [0,1] and determine the next state based on that value. In your example, you would proceed from C to A if U is between 0 and 0.7 (and therefore with probability 0.7), to B if U is between 0.7 and 0.8 (and therefore with probability 0.8 - 0.7 = 0.1) and stay at C if U is between 0.8 and 1.0 (with probability 1.0 - 0.8 = 0.2)
