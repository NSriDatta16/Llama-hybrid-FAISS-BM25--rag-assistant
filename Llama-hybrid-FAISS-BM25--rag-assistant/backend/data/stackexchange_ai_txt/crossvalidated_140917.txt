[site]: crossvalidated
[post_id]: 140917
[parent_id]: 
[tags]: 
Why don't we train neural networks to maximize linear correlation instead of error?

Recently a project I've been a part of has involved training neural networks so that we maximize the Pearson correlation between actual and predicted values. So this came to my mind: why don't we change the mathematical workings of, say, gradient descent so that instead of minimizing RMSE, we maximize $r$? If we can make the network predict with a high correlation, all we have to do is chain a linear function to the predictions and we have good prediction.
