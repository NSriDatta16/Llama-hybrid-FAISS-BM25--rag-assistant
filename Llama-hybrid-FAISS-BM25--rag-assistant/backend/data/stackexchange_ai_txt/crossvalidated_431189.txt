[site]: crossvalidated
[post_id]: 431189
[parent_id]: 
[tags]: 
Network learns bias during the first iterations if parameter initialization is not good

Andrej Karpathy in his blog post " A Recipe for Training Neural Networks " states that initialization is important for convergence. I get that but when he says: init well. Initialize the final layer weights correctly. E.g. if you are regressing some values that have a mean of 50 then initialize the final bias to 50. If you have an imbalanced dataset of a ratio 1:10 of positives:negatives, set the bias on your logits such that your network predicts probability of 0.1 at initialization. Setting these correctly will speed up convergence and eliminate “hockey stick” loss curves where in the first few iteration your network is basically just learning the bias. Why an incorrect initialization causes the network to learn the bias only during the first iterations? How is this related to bias?
