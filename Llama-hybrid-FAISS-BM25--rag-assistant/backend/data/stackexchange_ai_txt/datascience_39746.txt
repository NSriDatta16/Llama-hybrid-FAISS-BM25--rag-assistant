[site]: datascience
[post_id]: 39746
[parent_id]: 39742
[tags]: 
In short there are no rules, and no "best way". Any combined feature for statistical learners can be judged on two basic criteria: Does it make sense in the domain of the problem? Does it improve the metrics that you care about in the model? If you want to compare two radically different features, then you will probably need to introduce some kind of scaling factor. In which case you can make comparisons based on e.g. number of standard deviations away from the mean, or whether or not both features are in the same quartile of their respective distributions. Such a comparison does not rely on the units of the features. If the units are different, it doesn't make sense to subtract them (e.g. trade volume vs orderbook size). Features as fed into a statistical classifier or regression system are effectively unit-less. Given that, you can make any mathematical conversion that you like. Although some will make more sense in the domain of your problem than others. Once inside your model, there is every chance that the numbers will be compared, multiplied, added, subtracted in all sorts of combination. This is definitely true of neural networks for example. If it helps you can think of any normalisation, or even the weights of your model, as having units that also normalise the units of your inputs to all be compatible. Using number of standard deviations away from the mean is a simple way to achieve this, but not the only way. The exceptions to this approach would be models that are more like scientific models, where units are carefully tracked and a fixed formula applied. Even those commonly have constants which do unit conversion. If either features can be 0, what's the best way to compare them? Depends on the features, and on your classifier, but if possible, and working with linear or logistic regression, I would do the following: Optionally, transform each feature non-linearly so that its distribution follows a bell curve that looks roughly like a Normal distribution. Normalise both features to mean 0, standard deviation 1. Subtract one from the other. Normalise the new feature. If the two units you have combined are correlated, then the new feature may have an interesting distribution - worth plotting it and taking a look. There other equally valid approaches too. In part it will depend on the ML model you intend to use. Normalising as above works well for linear or logistic regression, neural networks (I would let the NN find the derived feature here though, unless I thought it was really important due to domain knowledge), kNN or other models that might use simple distance metrics.
