[site]: crossvalidated
[post_id]: 403520
[parent_id]: 
[tags]: 
Top principal components versus most significant random forest variables

I was working on making a supervised learning model starting with a database of about 100 features and 1000 data entries. My goal is to predict a certain target variable. I tried three different methods to do dimension reduction: #1 Principal component analysis, followed by scree plot criteria led to about 10 main PCs with cumulative variance percentage reaching 90 for the PC set. #2 Regression analysis starting with all 100 variables and then the top 10 variables according to their p-values were chosen. #3 Random forest on all 100 variables and then the top 10 according to variable importance plot were chosen. Then starting with the 10 variables from each of the top 3 methods I redid the random forest regression. Results with #3 and #2 feature sets were quite similar in RMSE and $R^2$ , while #1 feature set was drastically under-performing. I know that these dimension reduction methods might not be ideal. But my question is about why I got such results. Why is it that principal components, which I expect to capture the most important information, under-perform compared with the other two feature sets, while the model (RF) and number of features are the same?
