[site]: datascience
[post_id]: 99723
[parent_id]: 
[tags]: 
Transformers (BERT) vs LSTM on Sentiment Analysis/NER - dataset sizes comparison

I am aware (continuously learning) of the advantages of Transformers over LSTMs. At the same time, I was wondering from the viewpoint of size of the data needed, contrast of those two techniques , supposing I want to train for a downstream task, (classification or NER for instance), in which case would I need more data to achieve a specific result (although I am fully aware we never know in advance for a task how much data we need). Presuming a result of N% (supposing that threshold is achievable for both LSTM and BERT), which architecture (LSTM or BERT) would require a bigger dataset (regardless of the size, I am aware dataset size is task-dependent and subject to change) to reach that point. Does BERT need a bigger dataset to achieve "good results" (an empirical observation would help me) or a, say, bidirectional LSTM?
