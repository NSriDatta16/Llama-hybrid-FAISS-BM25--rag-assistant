[site]: datascience
[post_id]: 25259
[parent_id]: 
[tags]: 
What is Compatible Function Approximation theorem in reinforcement learning?

I am following David Silver's RL course. In the policy gradient section, I found this slide that I would like have an explanation of. What are these two conditions? What is the logic behind the first derivative equality? Is it just that we assume these two derivatives should be equal since there should be some kind of connection with direction of value function approximation gradient and our policy like-hood ? Then what is the epsilon value? What is this mean square value? Q value with w parameters means the Q values function new approximation. And the other Q value is the value got by following our policy right? So is this OFF policy value approximation or ON policy? I think this is on policy since normally in the Q value update we take the max.
