[site]: crossvalidated
[post_id]: 476131
[parent_id]: 
[tags]: 
Preparing **probabilities** data for mixed effect modeling

I have two related problems i cant seem to find an online solution and would really appreciate any direction. I am modeling probabilities* as a function of different predictors, across 35 subjects. X1 - could be 1,2,3 (numeric) X2 - could be 1,2,3 (numeric) X3 - could be 1 or 2 (factor) (all three predictors are orthogonal by design) Here is my model: M = glmmTMB(P ~ X1 + X2 + X1:X2 +(1|subject),data=DF, beta_family(link = "logit")) Problem 1: Average or raw data? (1) Should i use the raw data as it is or should i average per condition and model the averaged data frame? by average i mean average the data frame by X1,X2 and X3 and get one value per subject per combination of conditions. Each method gives me different results. For example, when averaging by X1 X2 and X3 - the model has significant main effect of X2 and interaction X1:X2. But when i only average the data by X1 and X2 (ignoring X3) - the interaction is far from significant. And there is no effect of X3 in any version. Important to note that even though X1 and X2 are orthogonal, the levels of X2 in the raw data appear in different frequency, i.e. X2==3 is 3 times more often than X2==1 . Problem 2. scaling the predictors for the raw data: Because I only take trials in which subjects answered correctly (80-90%), there could be some between-subject variation. So my solution was to go through the data frame and scale X1 and X2 for each subject separately. What I am not sure about is: (2.1) is this the right way? (2.2) do I need to then scale again across all the data? [remember this is a mixed effect model] (2.3) should I maybe only demean and not scale at all? Thank you very much in advance!
