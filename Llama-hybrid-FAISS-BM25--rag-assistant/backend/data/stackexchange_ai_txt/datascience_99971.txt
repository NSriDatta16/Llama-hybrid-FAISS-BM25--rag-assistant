[site]: datascience
[post_id]: 99971
[parent_id]: 31796
[tags]: 
Truncated Backpropagation Through Time (truncated BPTT) is a widespread method for learning recurrent computational graphs. Truncated BPTT keeps the computational benefits of Backpropagation Through Time (BPTT) while relieving the need for a complete backtrack through the whole data sequence at every step. More on Unbiasing Truncated Backpropagation Through Time From the paper On the difficulty of training Recurrent Neural Networks From stats.exchange answer by itzjustricky The truncated part of Truncated Backpropagation through Time simply refers to at which point in time to stop calculating the gradients for the backpropagation phase. Let's say you truncate after $k$ steps then the difference is you calculate the below instead. $$ \frac{\partial{L}}{\partial A}\approx\sum_{t=T-k}^{T}\frac{\partial{L}}{\partial h_{t}}\frac{\partial^{+}{h_{t}}}{\partial A}=\sum_{t=T-k}^{T}(\frac{\partial{L}}{\partial h_{t}}\odot f'(Ah_{t-1}+Bx_{t}))h_{t-1}^{T}. $$ Where $\frac{\partial^{+}}{\partial{A}}$ is the "immediate" partial wrt A, i.e. the one that assumes all terms other than an explicit A are constant.
