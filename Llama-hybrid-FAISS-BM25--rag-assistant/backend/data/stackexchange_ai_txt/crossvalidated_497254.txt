[site]: crossvalidated
[post_id]: 497254
[parent_id]: 496855
[tags]: 
Let me start with the linear regression case. Consider: $$ Xb = y $$ where $X$ is a $n\times p$ matrix, $y$ is a $n$ -vector, and $b$ is a $p$ -vector. If and only if there exists $b$ that satisfies this equation, we can achieve the zero training error for the linear regression. "If" part is trivial. "Only if" part can be proven by noting that there is a non-zero residual and thus squared sum is not zero. Unfortunately $p \ge n$ is not sufficient due to the possibility that some equations are contradictory, as pointed out by @user21060 in the comment. $p \ge n$ is not necessary either. To see this, imagine the case where $y$ is constant to zero; we can achieve zero training error by $b=0$ . Perhaps a general condition for $Xb=y$ to be possible is that $$ \mathrm{rank}(X) = \mathrm{rank}([X, y]) $$ There can be better way to state this, but you can search for the linear algebra and the conditions regarding the solution existence for linear equations. In the logistic regression context, consider: $$ Xb = 2 y - 1 $$ Note that the right hand side converts $y$ from $\{0,1\}$ to $\{-1,1\}$ . Similar to the case of linear regression, we can achieve the zero training error if this equation has a solution. But this is not necessary condition since all we need is that $Xb$ has the correct sign and we don't need them to be exactly equal to $1$ or $-1$ . It would be possible to refine the condition by considering the hyperplane separation . The hyperplane separation theorem states roughly that If two sets are disjoint and convex, then there exists a hyperplane separating them. The separating hyperplane is very close to what we need to achieve zero training error. This is because, separating hyperplane would imply that $$ x_i \cdot v \ge c \;\;\;\; \text{for all positive cases} $$ $$ x_i \cdot v \le c \;\;\;\; \text{for all negative cases} $$ If we have a separating hyperplane with the strict inequalities, then we can achieve zero training error. You can look into the conditions for achieving the strict hyperplane separation, but I cannot find one immediately.
