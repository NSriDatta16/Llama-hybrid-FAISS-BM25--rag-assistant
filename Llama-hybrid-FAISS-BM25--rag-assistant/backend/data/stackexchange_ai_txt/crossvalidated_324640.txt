[site]: crossvalidated
[post_id]: 324640
[parent_id]: 324635
[tags]: 
Well, correlation is a wide topic. Generally, we define correlation of two random variables X and Y as $$\rho = corr(X,Y) = \frac{cov(X,Y)}{\sqrt{Var(X}\sqrt{Var(Y)}}$$ By Cauchy-Schwarz inequality, $cov(X,Y) = E\Big[(X-E(X))(Y-E(Y))\Big] \le \sqrt{Var(X)Var(Y)}=\sqrt{E\Big[(X-E(X))^2\Big]E\Big[(Y-E(Y))^2\Big]}$, so, $-1\le\rho \le 1$. In linear model, we say that the two predictors (for example we use education level and working experience to predict the response salary) are positively correlated if $\rho > 0$, while negatively correlated if $\rho Special Case 1: If the correlation is 0, we may say the predictors are uncorrelated, but be very careful that uncorrelated does not necessarily means independent. However, for normally distributed data (linear regression assumes the error term is of $N(0,\sigma^2)$ distribution), it has the nice property that uncorrelated implies independent. Special Case 2: If the correlation is 1 or -1, then we say the two variables are perfectly correlated, so we can say that $X$ is a linear combination of $Y$, i.e., $X= \lambda Y$ for some real value $\lambda$. Finally, for edification only, for data reduction, we typically look for the variables having great correlation in the correlation matrix, since it may represent high collinearity. This is the starting point to do PCA for example. So, you may consult books, Wiki, or other posts for more deduction of proofs and specific information. Hope it helps.
