[site]: crossvalidated
[post_id]: 22273
[parent_id]: 
[tags]: 
Why normalise the standard deviation of neural network input?

I understand why the mean of the input to a neural network is normalized, to avoid the numerical problems with very large and very small numbers. Also, it's nice if the bias node is around the same magnitude as the other input data. But why is the standard deviation usually normalized? Of course higher standard deviation means that the variations have larger effect, but won't the weights adapt to that anyway? Thanks!
