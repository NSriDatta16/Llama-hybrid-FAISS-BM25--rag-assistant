[site]: crossvalidated
[post_id]: 172332
[parent_id]: 171932
[tags]: 
In logistic regression (I do it univariate for easier typing) you try to explain a binary outome $y_i \in \{0,1\}$ by assuming that it is the outcome of a Bernouilli random variable with a success probability $p_i$ that depends on your explanatory variable $x_i$, i.e. $p_i=P(y_i=1|_{x_i})=f(x_i)$, where $f$ is the logistic function: $f(x)=\frac{1}{1+e^{-(\beta_0+\beta_1 x)}}$. The parameters $\beta_i$ are estimated by maximum likelihood. This works as follows: for the $i$-th observation you observe the outcome $y_i$ and the success probability is $p_i=f(x_i)$, the probability to observe $y_i$ for a Bernouilli with success probability $p_i$ is $p_i^{y_i}(1-p_i)^{(1-y_i)}$. So, for all the observations in the sample, assuming independence between observations, the probability of observing $y_i, i=1,2, \dots n$ is $\prod_{i=1}^np_i^{y_i}(1-p_i)^{(1-y_i)}$. Using the above definition of $p_i=f(x_i)$ this becomes $\prod_{i=1}^nf(x_i)^{y_i}(1-f(x_i))^{(1-y_i)}=$. As the $y_i$ and $x_i$ are observed values, we can see this as a function of the unknown parameters $\beta_i$, i.e. $\mathcal{L}(\beta_0, \beta_1)=\prod_{i=1}^n\left(\frac{1}{1+e^{-(\beta_0+\beta_1 x_i)}}\right)^{y_i}\left(1-\frac{1}{1+e^{-(\beta_0+\beta_1 x_i)}}\right)^{(1-y_i)}$. Maximimum likelihood finds the values for $\beta_i$ that maximise $\mathcal{L}(\beta_0, \beta_1)$. Let us denote this maximum $(\hat{\beta}_0, \hat{\beta}_1)$, then the value of the likelihood in this maximum is $\mathcal{L}(\hat{\beta}_0, \hat{\beta}_1)$. In a similar way, if you would have used two explanatory variables $x_1$ and $x_2$, then the likelihood function would have had three parameters $\mathcal{L}'(\beta_0, \beta_1, \beta_2)$ and the maximum would be $(\hat{\beta}'_0, \hat{\beta}'_1, \hat{\beta}'_2)$ and the value of the likelihood would be $\mathcal{L}'(\hat{\beta}'_0, \hat{\beta}'_1, \hat{\beta}'_2)$. Obviously it would hold that $\mathcal{L}'(\hat{\beta}'_0, \hat{\beta}'_1, \hat{\beta}'_2) > \mathcal{L}(\hat{\beta}_0, \hat{\beta}_1)$, whether the incerase in likelihood is significant has to be 'tested' with e.g. a likelihood ratio test. So likelihood ratio tests allow you te 'penalize' models with too many regressors . This is not so for AUC ! In fact AUC does not even tell you whether your 'success probabilities' are well predicted ! If you take all possible couples $(i,j)$ where $y_i=1$ and $y_j=0$ then AUC will be equal to the fraction of all these couples that have $p_i If adding 1 explanatory variable does not change anything to the ranking of the probabilities of the subjects, then AUC will not change by adding an explanatory variable. So the first question you have to ask is what you want to predict : do you want to distinguish between zeroes and ones or do you want to have 'well predicted probabilities' ? Only after you have answered this question you can look for the most parsimonious technique. If you want to distinguish between zeroes and ones then ROC/AUC may be an option, if you want well predicted probabilities you should take a look at Goodness-of-fit test in Logistic regression; which 'fit' do we want to test? .
