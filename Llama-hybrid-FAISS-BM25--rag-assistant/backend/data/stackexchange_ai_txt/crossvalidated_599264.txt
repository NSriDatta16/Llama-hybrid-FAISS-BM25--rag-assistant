[site]: crossvalidated
[post_id]: 599264
[parent_id]: 
[tags]: 
Why is Gradient Accumulation not used frequently in training large models compared to using bigger batch sizes?

So I was currently going through various implementations of models and they set a large batch size of around 256 , running this is google collab is very memory intensive, so I decided to use a batch size of 4 with a gradient accumulate of 64 which theoretically should achieve the same performance, but I don't understand why this is not used as a standard , is there some inherent disadvantage of using gradient accumulate ? If not then I don't understand the reason , repositories of deep learning projects from Google and Facebook use large batch sizes and not gradient accumulate.
