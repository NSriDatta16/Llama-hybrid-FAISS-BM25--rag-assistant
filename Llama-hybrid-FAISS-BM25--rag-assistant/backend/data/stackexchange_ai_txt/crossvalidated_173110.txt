[site]: crossvalidated
[post_id]: 173110
[parent_id]: 173056
[tags]: 
I'll try to be incredible clear with my terminology. As you did, we'll focus on one coin, $X \sim Bernoulli(p)$, so $Pr(X=1) = p$. Bayesians and frequentists both view $X$ as a random variable and they share the same views about the probability distribution $Pr(X)$. However, Bayesians also use probability distributions to model their uncertainty about a fixed parameter, in this case $p$. If we now let $x_1, x_2, \dots \sim Bernoulli(p)$ and define $h_n = \sum_{i=1}^n x_i$, as you pointed out $$ \lim_{n\rightarrow \infty} \frac{h_n}{n}= p. $$ This is relevant because $h_n/n$ is the MLE for $p$. However notice that for any positive numbers $a,b$ (in fact they don't even need to be positive): $$ \lim_{n\rightarrow \infty} \frac{h_n+a}{n+a+b}= p. $$ One draw back of the estimator $h_n/n$ is that for small $n$ this might be crazy. The most extreme example of this is when $n = 1$, our estimate of $p$ will be $0$ or $1$. What if we set $a=b=5$ and use the second estimate. If we get a $1$ on the first flip our updated estimate is $6/11$, greater than $50\%$ but not as extreme as $1$. This more restrained estimate can be easily derived by expressing our uncertainty about $p$ in the form of a prior (and eventually posterior) distribution. If you would like to look up this example in depth this is known as the Beta-Binomial . It involves putting a Beta prior on the parameter of a Binomial Distribution, and taking the expectation of the resulting posterior.
