[site]: crossvalidated
[post_id]: 397247
[parent_id]: 
[tags]: 
Q: Implications of autocorrelation in a Dickey-Fuller unit root test

I am conducting a Dickey-Fuller unit root test on the FEDFUNDS series from the FRED database. My estimation period is quarterly from 1955(1) to 2007(4) (taking the average of the monthly rates within each quarter). Using the DF regression / AR model: $$\Delta y_t = \delta + y_{t-1} + c_1 \Delta y_{t-1} + \epsilon_t$$ where $\delta$ is the constant term. Theoretically, the series should exhibit stationarity, however using a normal t-test I find that the series rejects the null hypothesis of a unit root (-2.88 against a critical value of -2.86 at 5 % significance (David and MacKinnon 1993)), but the model exhibits autocorrelation. Does this mean I cannot use my econometric findings? If I conduct a Likelihood ratio test setting both $\delta = c_1 = 0$ I get a likehood ratio of 8.26 which is within the critical value of 9.13 however the model still exhibits autocorrelation . Am I missing something? In my theoretical model and using existing litterature I could support the claim that the effective federal funds rate should be a stationairy process but my econometric analysis indicate otherwise. EDIT: Linking to my data in my Github repository which I just created. I have taken the average EFFR of each quarter to represent quarterly data from the FEDFUNDS series as stated previously.
