[site]: crossvalidated
[post_id]: 140537
[parent_id]: 
[tags]: 
Why do RNNs have a tendency to suffer from vanishing/exploding gradient?

Why do recurrent neural networks (RNNs) have a tendency to suffer from vanishing/exploding gradient? For what a vanishing/exploding gradient is, see Pascanu, et al. (2013). On the difficulty of training recurrent neural networks, section 2 ( pdf ).
