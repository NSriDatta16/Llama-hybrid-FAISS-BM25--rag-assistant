[site]: crossvalidated
[post_id]: 3597
[parent_id]: 3595
[tags]: 
I'll second @suncoolsu comment: The dimensionality of your data set is not the only criterion that should orient you toward a specific software. For instance, if you're just planning to do unsupervised clustering or use PCA, there are several dedicated tools that cope with large data sets, as commonly encountered in genomic studies. Now, R (64 bits) handles large data pretty well, and you still have the option to use disk storage instead of RAM access, but see CRAN Task View High-Performance and Parallel Computing with R . Standard GLM will easily accomodate 20,000 obs. (but see also speedglm ) within reasonable time, as shown below: > require(MASS) > n X df system.time(glm(y ~ ., data=df)) user system elapsed 0.361 0.018 0.379 To give a more concrete illustration, I used R to process and analyse large genetic data (800 individuals x 800k SNPs , where the main statistical model was a stratified GLM with several covariates (2 min); that was made possible thanks to efficient R and C codes available in the snpMatrix package (in comparison, the same kind of model took about 8 min using a dedicated C++ software ( plink ). I also worked on a clinical study (12k patients x 50 variables of interest) and R fits my needs too. Finally, as far as I know, the lme4 package is the only software that allow to fit mixed-effects model with unbalanced and large data sets (as is the case in large-scale educational assessment). Stata/SE is another software that can handle large data set . SAS and SPSS are file based software, so they will handle large volumes of data. A comparative review of software for datamining is available in Data Mining Tools: Which One is Best for CRM . For visualization, there are also plenty of options; maybe a good start is Graphics of large datasets: visualizing a million ( reviewed in the JSS by P Murrell), and all related threads on this site.
