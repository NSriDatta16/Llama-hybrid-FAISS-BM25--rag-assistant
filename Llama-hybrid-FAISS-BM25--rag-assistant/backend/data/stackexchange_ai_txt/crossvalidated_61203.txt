[site]: crossvalidated
[post_id]: 61203
[parent_id]: 61201
[tags]: 
In an RKHS framework, any function $f(\cdot)$ can be minimized with a hilbert-norm minimizing solution, just by a linear combination of the kernels evaluated at the rest of the data points and $x$ itself. i.e, in detail, if you dont know the result of the function $f's$ Hilbert-norm minimizing minimizer, all you need to do in a reproducing kernel Hilbert space, (RKHS) is that you take the reproducing p.s.d kernel $K(\cdot,\cdot)$ which is a function of two arguments (pairwise), where in one argument you fix $x$; the point where you would like to evaluate $f(\cdot)$ such that $x$ is the minimizer and then in the other argument you choose each of the other points apart from $x$ that you have in your dataset, as $K(x,\cdot)$ and then the "Representer Theorem" ensures that there exists a set of $\alpha's$ which go along with $K(x,\cdot)$ such that the result $f(x)$ "can be" represented/${evaluated}/admits$ the following property: $f(x)=\sum{K(x,\cdot)}$ where the placeholder $\cdot$ takes the rest of the data points and the summation is over each of those points.
