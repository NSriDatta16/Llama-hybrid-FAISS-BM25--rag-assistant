[site]: crossvalidated
[post_id]: 342287
[parent_id]: 
[tags]: 
Is the MLE problem for Gaussian Process Regression convex?

Log marginal likelihood for Gaussian Process as per Rasmussen's Gaussian Processes for Machine Learning equation 2.30 is: $$\log p(y|X) = -\frac{1}{2}y^T(K+\sigma^2_n I)^{-1}y - \frac{1}{2}\log|K+\sigma^2_n I|-\frac{n}{2}\log2\pi$$ Implementations typically apply gradient descent by default to maximize this over some family of $K$, where that is typically a sum/product of constant kernels, RBF kernels, periodic kernels, etc It is not clear how convex this optimization problem is, in general. If we sample some data $X$, $y$ from a 'nice' process (i.e. preventing crazily-contrived examples of non-convexity that would never occur in real life data), is this problem convex for common kernels? (sums/products of constant, RBF, exponential sine-squared, etc)
