[site]: datascience
[post_id]: 94533
[parent_id]: 94531
[tags]: 
For simplicity, I will talk about the classification problem, but regression follows the same rules. You are given some data: part of it with known labels and others with unknown labels. The "classical" goal is to determine the missing labels based on known labels. The data is naturally split into two chunks: (a) data without labels -- we call it a test set. (b) data with labels -- in some textbooks, it is called the train set, but the train set is used for another thing as well (see below), so I would call it labeled data to avoid confusion. You want to train a selected model on some data and apply the model to the test set. The data which is used for training is called the training set. So, a naive scheme is Train the model on the train set Predict the labels on the test set. However, before making predictions on the test set, you need to measure how good the model is. We modify our approach to Train the model on the train set Verify that the model performs well Predict the labels on the test set To verify the model performance, we do the following: given a set called a validation set, we predict the labels on the validation set, and then check the accuracy. Note that the validation set must contain labeled data only. Why not to use the train set for validation? This approach is prone to overfitting: deep enough random forest will predict each entry in the train set accurately polynomial with a high enough degree will approximate all the points with zero error KNN with k=1 will have each node guessed accurately on the train set because the closest node is itself. etc. To fix this issue, we make a train set and a validation set disjoint. Usually, we randomly split the labeled data into two chunks. Note that the test set contains another data. In your question, you refer to the validation set as a test set. Some authors do this, but it is very misleading. For increasing K-value in K-fold, we manipulate the bias-variance tradeoff. It is another concept, and you can read about it, for example, here: https://medium.com/@karenovna.ak/part-ii-evaluating-a-predictive-model-cross-validation-and-bias-and-variance-tradeoff-9874b836cd2e
