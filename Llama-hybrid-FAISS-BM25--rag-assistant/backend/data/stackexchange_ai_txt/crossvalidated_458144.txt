[site]: crossvalidated
[post_id]: 458144
[parent_id]: 458140
[tags]: 
Some hints: 500k rows with 100 columns do not impose problems to load and prepare, even on a normal laptop. No need for big data tools like spark. Spark is good in situations with hundreds of millions of rows. Good random forest implementations like ranger (available in caret ) are fully parallelized. The more cores, the better. Random forests do not scale too well to large data. Why? Their basic idea is to pool a lot of very deep trees. But growing deep trees eats a lot of resources. Playing with parameters like max.depth and num.trees help to reduce computational time. Still, they are not ideal. In your situation, maybe 20 minutes with ranger on a normal laptop would be sufficient. (A rough guess). library(ranger) n With higher max.depth , quite a lot of additional time will be required.
