[site]: datascience
[post_id]: 31466
[parent_id]: 24577
[tags]: 
You should use a Seq2Seq (which uses LSTM/GRU/RNNs) architecture for the task of cleaning text. The encoder network would take in the "noisy" sequence and the decoder would generate the "cleaned" sequence. Seq2Seq models is often used for Neural Machine Translation (NMT). The next thing is you will need a very large data set. Fortunately this shouldn't be too hard to generate noise injections (of real words, symbols, fake words, letter omissions etc) into sequences of text. A good guide for a similar task is in the following post. The implementation uses Keras and has all of the code. https://towardsdatascience.com/how-to-create-data-products-that-are-magical-using-sequence-to-sequence-models-703f86a231f8
