[site]: datascience
[post_id]: 113177
[parent_id]: 
[tags]: 
Do I need training data in multiple languages for a multilingual transformer?

I am attempting to train a transformer which can categorize sentences into one of n categories. This model should be able to work with a number of different languages - English and Arabic in my case. Do I need to have labelled training data in both English and Arabic to fine tune a pretrained transformer, such as BLOOM , or can I fine tune the model using only English samples, and then the model should also work well on Arabic samples for my classificaiton task, since the fine tuning only trained the classification head? My thoughts are that the pretraining of this model should allow it to transform the same input texts in English and Arabic to the same (or similar) embedding, which the classification head would have learned to then predict these embeddings accurately through the fine tuning.
