[site]: datascience
[post_id]: 114382
[parent_id]: 114372
[tags]: 
Bad news: In general, feature selection is (well-known) NP-Hard problem - we never know which feature subset is best unless we exhaustively search all possible combinations, which is usually not feasible in practice. The best feature subset is dependent on the model, e.g. a linear model may perform best on a subset different from LightGBM. This comes from the fact that a (statistical) model... embodies a set of statistical assumptions , so different model may favor different features. It depends on the evaluation metric as well (obviously). To sum up, there is no free lunch . In reality, we use a number of heuristics to do the job. You may refer to Wiki's feature selection page for an overview. Some best practices: Try to engineer as many features as possible, and try different selection approaches. Try many models, preferably different classes (e.g. linear, tree-based, neural...) Ensemble of models always performs better than individual one. The above points apply to machine learning problems in general, including your particular problem.
