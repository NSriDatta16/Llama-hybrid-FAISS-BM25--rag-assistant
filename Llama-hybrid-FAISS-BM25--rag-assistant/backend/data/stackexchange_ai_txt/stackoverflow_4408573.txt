[site]: stackoverflow
[post_id]: 4408573
[parent_id]: 4394183
[tags]: 
First my opinions, then some analysis Opinions Denormalisation is perceived to help reading data because common use of the word denormalisation often include not only breaking normal forms, but also introducing any insertion, update and deletion dependencies into the system. This, strictly speaking, is false , see this question/answer , Denormalisation in strict sense mean to break any of the normal forms from 1NF-6NF, other insertion, update and deletion dependencies are addressed with Principle of Orthogonal Design . So what happens is that people take the Space vs Time tradeoff principle and remember the term redundancy (associated with denormalisation, still not equal to it) and conclude that you should have benefits. This is faulty implication, but false implications do not allow you to conclude the reverse. Breaking normal forms may indeed speed up some data retrieval (details in analysis below), but as a rule it will also at the same time: favour only specific type of queries and slow down all other access paths increase complexity of the system (which influences not only maintenance of the database itself, but also increases the complexity of applications that consume the data) obfuscate and weaken semantic clarity of the database main point of database systems, as central data representing the problem space is to be unbiased in recording the facts, so that when requirements change you don't have to redesign the parts of the system (data and applications) that are independent in reality. to be able to do this artificial dependencies should be minimised - today's 'critical' requirement to speed up one query quite often become only marginally important. Analysis So, I made a claim that sometimes breaking normal forms can help retrieval. Time to give some arguments 1) Breaking 1NF Assume you have financial records in 6NF. From such database you can surely get a report on what is a balance for each account for each month. Assuming that a query that would have to calculate such report would need to go through n records you could make a table account_balances(month, report) which would hold XML structured balances for each account. This breaks 1NF (see notes later), but allows one specific query to execute with minimum I/O . At the same time, assuming it is possible to update any month with inserts, updates or deletes of financial records, the performance of the update queries on the system might be slowed down by time proportional to some function of n for each update . (the above case illustrates a principle, in reality you would have better options and the benefit of getting minimum I/O bring such penalties that for realistic system that actually updates data often you would get bad performance on even for your targeted query depending on the type of actual workload; can explain this in more detail if you want) Note: This is actually trivial example and there is one problem with it - the definition of 1NF. Assumption that the above model breaks 1NF is according to requirement that values of an attribute ' contain exactly one value from the applicable domain '. This allows you to say that the domain of the attribute report is a set of all possible reports and that from all of them there is exactly one value and claim that 1NF is not broken (similar to argument that storing words does not break 1NF even though you might have letters relation somewhere in your model). On the other hand there are much better ways to model this table, which would be more useful for wider range of queries (such as to retrieve balances for single account for all months in a year). In this case you would justify that improvement by saying that this field is not in 1NF. Anyway it explains why people claim that breaking NFs might improve performance. 2) Breaking 3NF Assuming tables in 3NF CREATE TABLE `t` ( `id` int(10) unsigned NOT NULL AUTO_INCREMENT, `member_id` int(10) unsigned NOT NULL, `status` tinyint(3) unsigned NOT NULL, `amount` decimal(10,2) NOT NULL, `opening` decimal(10,2) DEFAULT NULL, PRIMARY KEY (`id`), KEY `member_id` (`member_id`), CONSTRAINT `t_ibfk_1` FOREIGN KEY (`member_id`) REFERENCES `m` (`id`) ON DELETE CASCADE ON UPDATE CASCADE ) ENGINE=InnoDB CREATE TABLE `m` ( `id` int(10) unsigned NOT NULL AUTO_INCREMENT, `name` varchar(255) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB with sample data (1M rows in t, 100k in m) Assume a common query that you want to improve mysql> select sql_no_cache m.name, count(*) from t join m on t.member_id = m.id where t.id between 100000 and 500000 group by m.name; +-------+----------+ | name | count(*) | +-------+----------+ | omega | 11 | | test | 8 | | test3 | 399982 | +-------+----------+ 3 rows in set (1.08 sec) you could find suggestions to move attribute name into table m which breaks 3NF (it has a FD: member_id -> name and member_id is not a key of t) after alter table t add column varchar(255); update t inner join m on t.member_id = t.id set t.name = m.name; running mysql> select sql_no_cache name, count(*) from t where id between 100000 and 500000 group by name; +-------+----------+ | name | count(*) | +-------+----------+ | omega | 11 | | test | 8 | | test3 | 399982 | +-------+----------+ 3 rows in set (0.41 sec) notes: The above query execution time is cut in half , but the table was not in 5NF/6NF to begin with the test was done with no_sql_cache so most cache mechanisms were avoided (and in real situations they play a role in system's performance) space consumption is increased by approx 9x size of the column name x 100k rows there should be triggers on t to keep the integrity of data, which would significantly slow down all updates to name and add additional checks that inserts in t would need to go through probably better results could be achieved by dropping surrogate keys and switching to natural keys, and/or indexing, or redesigning to higher NFs Normalising is the proper way in the long run. But you don't always have an option to redesign company's ERP (which is for example already only mostly 3NF) - sometimes you must achieve certain task within given resources. Of course doing this is only short term 'solution'. Bottom line I think that the most pertinent answer to your question is that you will find the industry and education using the term 'denormalisation' in strict sense, for breaking NFs loosely, for introducing any insertion, update and deletion dependencies (original Codd's quote comments on normalisation saying: ' undesirable (!) insertion, update and deletion dependencies', see some details here ) So, under strict definition, the aggregation (summary tables) are not considered denormalisation and they can help a lot in terms of performance (as will any cache, which is not perceived as denormalisation). The loose usage encompasses both breaking normal forms and the principle of orthogonal design , as said before. Another thing that might shed some light is that there is a very important difference between the logical model and the physical model . For example indexes store redundant data, but no one considers them denormalization, not even people who use the term loosely and there are two (connected) reasons for this they are not part of the logical model they are transparent and guaranteed not to break integrity of your model If you fail to properly model your logical model you will end up with inconsistent database - wrong types of relationships between your entities (inability to represent problem space), conflicting facts (ability to loose information) and you should employ whatever methods you can to get a correct logical model, it is a foundation for all applications that will be built on top of it. Normalisation, orthogonal and clear semantics of your predicates, well defined attributes, correctly identified functional dependencies all play a factor in avoiding pitfalls. When it comes to physical implementation things get more relaxed in a sense that ok, materialised computed column that is dependent on non key might be breaking 3NF, but if there are mechanisms that guarantee consistency it is allowed in physical model in the same way as indexes are allowed, but you have to very carefully justify it because usually normalising will yield same or better improvements across the board and will have no or less negative impact and will keep the design clear (which reduces the application development and maintenance costs) resulting in savings that you can easily spend on upgrading hardware to improve the speed even more then what is achieved with breaking NFs.
