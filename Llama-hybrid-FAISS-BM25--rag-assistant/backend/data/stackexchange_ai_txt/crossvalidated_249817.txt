[site]: crossvalidated
[post_id]: 249817
[parent_id]: 239435
[tags]: 
In general, for most models you will want to take categorical columns and convert them into dummy variables for each category. So, if you have a column with categories (A,B,C), then you would create three columns (col A, col B, col C) in binary, indicating whether the category exists for that particular line item. Additionally, people often might drop one of these columns to be used as the baseline (intercept in regression models). Decision trees/Random Forest/Gradient Boosting are advantageous in the fact that this does NOT need to be done, as the model can interpret each category as a way to split decisions within the tree. Although, creating dummies (as described above) in combination with feature selection can be helpful when you want to remove noise and better generalize the model. Please note that in R, the majority of models will interpret categorical variables as is (without dummies), as the advice I am giving is for the typical machine learning models in python (scikit-learn, xgboost).
