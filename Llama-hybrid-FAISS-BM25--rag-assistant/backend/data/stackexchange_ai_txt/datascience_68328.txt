[site]: datascience
[post_id]: 68328
[parent_id]: 68327
[tags]: 
Generally the activation is part of the model and gets applied for each neuron, so definitely before the error calculation. What the activation function is depends on what task you are solving and where the neuron of interest is. In principle the activation function $f$ would go to the calculation of the outcome $$ y = f(Wx + b)$$ For output neurons, if you are doing classification, then $f$ should map between 0 and 1, since you'll interpret the outcome as a probability. For regression $f$ could be just the identity. For hidden (i.e. non-output neurons), you definitely want to use a non-linear $f$ . The reason is that the neural network would otherwise be equivalent to a regular linear model. So the non-linear activations are needed to harvest the expressive power of neural networks. For deep learning the most popular $f$ for hidden neurons would probably be the rectified linear unit (relu) $$ f(x) = \max(0,x)$$
