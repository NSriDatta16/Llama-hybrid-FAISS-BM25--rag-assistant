[site]: crossvalidated
[post_id]: 637757
[parent_id]: 
[tags]: 
Diebold-Mariano Revisited: what is a reasonable parameter count for information criteria when the model is complex?

Diebold (2015) wrote a follow-up paper/essay reflecting on how his work with Mariano to develop the Diebold-Mariano test has been abused over the years. One of the main points in the follow-up paper is the advantage of looking at model performance over the entire sample and then penalizing according to the parameter count to quell concerns about overfitting, as opposed to training on a subset of the data and testing performance on a holdout set, such as with Bayesian/Schwarz or Akaike information criteria. $$ \text{BIC}=\text{SIC} = k\log(T) - 2\log(\hat L)\\ \text{AIC} = 2k - 2\log(\hat L) $$ Here, $\hat L$ is the (estimated) model likelihood, $T$ is the sample size, and $k$ is the parameter count. The $\hat L$ is straightforward enough to calculate. However, $k$ seems to become unclear when we deviate from traditional methods. For instance, if we estimate the parameters using some kind of cross-validation-tuned penalized method (think ridge, LASSO, or elastic net), sure, we can count the number of arameters being estimated to use as $k$ , but that seems unfair, as we did more than estimate the parameters. We also estimated the best way to estimate the parameters by tuning the hyperparameter. If we use a neural network and all of the hyperparameters that go along with that (e.g., early stopping), then I really have no idea what to do. How do we know the parameter count in these and similar situations? Is this framework discussed by Diebold not compatible with modern machine learning techniques? REFERENCES Diebold, Francis X. "Comparing predictive accuracy, twenty years later: A personal perspective on the use and abuse of Dieboldâ€“Mariano tests." Journal of Business & Economic Statistics 33.1 (2015): 1-1.
