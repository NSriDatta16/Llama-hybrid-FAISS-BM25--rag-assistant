[site]: crossvalidated
[post_id]: 604543
[parent_id]: 604539
[tags]: 
There's three common options for this: Train 3 separate models. Easy and straightforward to do, but does not exploit potential information across the different targets (which could help neural networks find good internal representations of the data). Train a single model that separately outputs multiple targets, which have different loss functions. I.e. one output is label1, another label2 and another label3, which might be trained e.g. using categorical-cross-entropy, categorical-cross-entropy and binary-log-loss, respectively. This is usually easiest done using neural networks and any packages (like PyTorch/ torch , fastai , keras etc.) accommodate this out-of-the-box, you just need to look it up in the documentation. That's occasionally hard, because people don't talk about this in a consistent manner, but keywords like "multi-target" should help (multi-label is sometimes also used, but is also used sometimes for having one categorical variable with multiple levels). Models other than neural networks need a lot of manual tinkering to make this work, so would not be my first try. Treat all combinations of the levels as your target ("pug-sunny-not-a-tree", "pug-rainy-tree", "husky-snowy-not-a-tree" etc.). Then, you can just train any model to predict these combined categories and break the output apart thereafter. The downside is that you get a lot of categories, so unless you have a lot of data this may be quite inefficient (and some categories not occurring in the training data may be a problem).
