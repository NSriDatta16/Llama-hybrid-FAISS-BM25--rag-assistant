[site]: datascience
[post_id]: 44166
[parent_id]: 44161
[tags]: 
It can be found, assuming a proper learning rate, a suitable threshold, and binary cross-entropy cost, since it translates this into a convex problem, in which we have one global optimum. We don't have closed form solution for logistic regression, but through gradient descent we can get to this optimum arbitrarily close. I'd suggest running a logreg from scikit-learn or a familiar library.
