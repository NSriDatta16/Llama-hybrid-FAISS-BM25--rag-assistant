[site]: datascience
[post_id]: 51157
[parent_id]: 
[tags]: 
If we are using batch normalization as the first layer, can we forego standard scaling of inputs?

It is common practice to use the standard scaler on the inputs before feeding it to a deep learning architecture. I was wondering whether it is necessary if the first layer is a batch normalization layer.
