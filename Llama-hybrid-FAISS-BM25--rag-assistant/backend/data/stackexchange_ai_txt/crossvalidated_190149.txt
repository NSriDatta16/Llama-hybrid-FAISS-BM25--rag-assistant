[site]: crossvalidated
[post_id]: 190149
[parent_id]: 
[tags]: 
Smooth Non Normally Distributed Data

I have ~16,000 probability sets of goals scored [maximum of 12] like below [some % are rounded hence do not add up to 100%]: Goals 0 1 2 3 4 5 6 7 8 9 10 11 12 ------------------------------------------------------------------------- 1: 9 15 24 23 14 8 4 2 0.5 0.2... AVERAGE TEAM 2: 29 25 26 11 - - - 9 - - - 3: 4 9 32 33 10 10 2 1 - - - 4: 22 36 17 16 9 - - - - - - BAD TEAM 5: 3 10 11 22 23 19 9 1 1 - - GOOD TEAM Tacky illustrative examples of objective [on the gentle side]: 2: 27 29 24 10 1 - 1 7 1 - - 3: 4 11 30 31 13 8 3 1 - - - 4: No problem other than 5 & 6 goals should perhaps greater than 0% 2: Is a perfect example: 9% for 7 goals but 0% for 4, 5, 6 & 8 goals is a serious problem. 29%, 25%, 26% is a smaller problem too that would greatly benefit from a light touch of smoothing. 3: It is hard to tell, should 2 & 3 goals peak that high? 4 & 5 goals probably should not be the same expected frequency. The data is in excel. I am not looking for best practise, just some simple methods that would scale well based on the number of sets. It would be nice if it was a little less hacky / tacky than "three median/mean methods." Obviously a lot of interference might make pretty graphs [not the aim] but is not statistically sound and would not hold up in the future. Meddling with expected frequencies feels intuitively bad practise, and could be frowned upon. I respect that if that is community consensus, as it is now without smoothing data, the expected frequencies work BUT are worse than actually using the mean. This should not be case for soccer but maybe the 7% tail above 5.5 is not that bad. However I would like to try gently smoothing the data to see if I can improve things so that at least using frequencies is equals predictive power of using mean. Edit: The objective would be to smooth each set [row] individually. The first row is merely there to illustrate the average distribution and that any smoothing should really maintain a positive skew. Each set is expected frequencies of a given teams performance for their next match against an average team. Whatever the smoothing, the aim would be that it is actually a better representation of the raw expected frequencies. The smoothing desired would be anything that could be achieved using excel functions / formulas [even if it requires 100+ helper columnns, not an issue, performance is the real inhibitor].
