[site]: crossvalidated
[post_id]: 627199
[parent_id]: 
[tags]: 
Why are approaches that approximate a random forest with a single decision not more popular?

I understand that random forests yield better performance than standard decision trees, but are less interpretable, because they do not generate a single tree. In this question , several users provided models that approximate a random forest using a single decision tree. A limitation with these approaches is that they require the user to generate data to approximate the random forest's functional space. However, this question references a paper titled "Seeing the Forest Through the Trees: Learning a Comprehensible Model from an Ensemble" that seems to provide a solution without having to generate any data, and yields similar performance to random forests. If this paper's performance is close to a random forest's without having to sacrifice interpretability, it seems like it would be better to use in most cases. That is, unless interpretability is irrelevant, which would be unlikely in most real-world applications. My question is why is this paper's approach not more popular? Optimal decision trees have become popular in literature recently, but there is still a gap in the performance of optimal decision trees to random forests. On the other hand, the approach in the "Seeing the Forest Through the Trees" paper does not seem to have as significant of a gap between the proposed model's performance to random forests. Are there certain limitations to this paper's approach that optimal decision trees and random forests do not have?
