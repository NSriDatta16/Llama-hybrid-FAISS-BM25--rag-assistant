[site]: crossvalidated
[post_id]: 583080
[parent_id]: 
[tags]: 
A question about serial correlation

I'm new to Time Series and I'm confused by some assumptions relating the classical regression model and time series. Econometrics, Fumio Hayashi's book, when introducing the classical regression model, makes some caveats for the case of time series. For example, in page 45: But, as emphasized in Section 1.1, the strict exogeneity assumption is not satisfied in time-series models typically encountered in econometrics, and serial correlation is an issue that arises only in time-series models. Moreover, the linear model is given by $$y_i = x_i'\beta + \varepsilon_i, \quad i = 1,...,n$$ and the Assumption 1.4 (spherical error variance) is: $$E[\varepsilon_i^2|X]=\sigma^2 > 0, \quad E[\varepsilon_i \varepsilon_j |X] =0 $$ where $X = [\mathbf{x}_1,...,\mathbf{x}_K]_{n\times K}$ matrix. As said earlier, the most time series models do not satisfy this hypothesis, because of the serial correlation ( $E(\varepsilon_i \varepsilon_j)\neq 0$ ). But for me, I can't see that most time series have serial correlation. For example, AR(1) processes are: $$y_t = \rho y_{t-1} + \varepsilon_t$$ with $(\varepsilon_t)$ being i.i.d.. So, I think that in this case, I don't have serial correlation, because the error is i.i.d. And in many other time series models (MA, ARMA) it seems to me that we have i.i.d errors. So in what cases (examples) do we have serial correlation?
