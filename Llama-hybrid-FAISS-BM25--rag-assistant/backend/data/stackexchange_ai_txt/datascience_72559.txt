[site]: datascience
[post_id]: 72559
[parent_id]: 
[tags]: 
Different activation function in same layer of a Neural network

My question is that what will happen if I arrange different activation functions in the same layer of a neural network and continue the same trend for the other hidden layers. Suppose I have 3 relu units in the starting and 3 tanh after that and other activation function I the same hidden layer and for the other hidden layers I am scaling all the nodes with the same scale (decreasing/increasing) and the arrangement and order of the activation function is not changing.
