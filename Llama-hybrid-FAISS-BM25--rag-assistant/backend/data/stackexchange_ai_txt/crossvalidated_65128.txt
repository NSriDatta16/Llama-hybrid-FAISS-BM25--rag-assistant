[site]: crossvalidated
[post_id]: 65128
[parent_id]: 
[tags]: 
Nested cross validation for model selection

How can one use nested cross validation for model selection ? From what I read online, nested CV works as follows: There is the inner CV loop, where we may conduct a grid search (e.g. running K-fold for every available model, e.g. combination of hyperparameters/features) There is the outer CV loop, where we measure the performance of the model that won in the inner fold, on a separate external fold. At the end of this process we end up with $K$ models ($K$ being the number of folds in the outer loop). These models are the ones that won in the grid search within the inner CV, and they are likely different (e.g. SVMs with different kernels, trained with possibly different features, depending on the grid search). How do I choose a model from this output? It looks to me that selecting the best model out of those $K$ winning models would not be a fair comparison since each model was trained and tested on different parts of the dataset. So how can I use nested CV for model selection? Also I have read threads discussing how nested model selection is useful for analyzing the learning procedure. What types of analysis /checks can I do with the scores that I get from the outer K folds?
