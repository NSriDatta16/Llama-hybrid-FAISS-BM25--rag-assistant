[site]: crossvalidated
[post_id]: 310910
[parent_id]: 
[tags]: 
Bayesian Model Suddenly Overfits

I am building a Bayesian model and have a trouble of (sudden) overfitting. The example figure that shows the issue is here: Until around 1250 iteration, the log-likelihood goes up little by little, but it soon starts increasing rapidly, which might be a sign of overfitting. I'm building a language model, so the strangeness of this log-likelihood is clearer if we check perplexity. Perplexity is defined as $${\rm Perplexity} = p( \mathbf{w} | \mathcal{M}) = \exp \left( \frac{-{\rm loglik}}{{\rm Num\ obs.}} \right),$$ which means the probability of observing words $\mathbf{w}$ under the model $\mathcal{M}$. Perplexity can be interpreted as the geometric mean of possible choices of words, and the model works well if the perplexity is low. My model's perplexity is almost $0$ and model works "too" well. My questions are, Does overfitting occur in the full Bayesian model? (no optimization, and all parameters including hyper-parameters are estimated using MCMC sampling) What can I do to fix the model?
