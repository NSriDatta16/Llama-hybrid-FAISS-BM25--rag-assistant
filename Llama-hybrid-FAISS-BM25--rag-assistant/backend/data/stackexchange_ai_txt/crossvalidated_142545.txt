[site]: crossvalidated
[post_id]: 142545
[parent_id]: 142533
[tags]: 
P-values give information about differences between two groups of results ("treatment" vs "control", "A" vs "B", etc.) that sample from two populations. The nature of the difference is formalized in the statement of hypotheses -- e.g. "mean of A is greater than mean of B". Low p-values suggest that the differences are not due to random variation, while high p-values suggest that differences in the two samples cannot be distinguished from differences that might arise simply from random variation. What is "low" or "high" for a p-value has historically been a matter of convention and taste rather than established by rigorous logic or analysis of evidence. A prerequisite for using p-values is that the two groups of results are really comparable, namely that the only source of difference between them is related to variable you are evaluating. As a exaggerated example, imagine that you have statistics on two diseases in two time periods -- A: mortality from cholera among men in British prisons 1920-1930, and B: infection by malaria in Nigeria 1960-1970. Computing a p-value from these two sets of data would be rather absurd. Now, if A: mortality from cholera among men in British prisons who are not treated vs. B: mortality from cholera among men in British prisons treated with re-hydration, then you have the basis for a solid statistical hypothesis. Most often this is accomplished through careful experiment design, or careful survey design, or careful collection of historical data, etc. Also, the differences between the two results must be formalized into hypotheses statements involving sample statistics -- often sample means, but could also be sample variances, or other sample statistics. It's also possible to create hypotheses statements comparing the two sample distributions as a whole, using stochastic dominance. These are rare. The controversy over p-values centers on "what is really significant" for research? This is where effect sizes come in. Basically, effect size is the magnitude of the difference between the two groups. It's possible to have high statistical significance (low p-value -> not due to random variation) but also low effect size (very little difference in magnitude). When effect sizes are very large, then allowing somewhat high p-values may be OK. Most disciplines are now moving very strongly toward reporting effect sizes, and reducing or minimizing the role of p-values. They also encourage more descriptive statistics about the sample distributions. Some approaches, including Bayesian Statistics, do away with p-values all together. My answer is condensed and simplified. There are many articles on this topic you can consult for more details, justifications, and specifics, including these: Using Effect Sizeâ€”or Why the P Value Is Not Enough Effect sizes and p-values: what should be replicated and what should be reported It's the Effect Size, Stupid
