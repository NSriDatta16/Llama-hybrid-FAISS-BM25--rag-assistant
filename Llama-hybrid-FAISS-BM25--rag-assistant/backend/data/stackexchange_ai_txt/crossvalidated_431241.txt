[site]: crossvalidated
[post_id]: 431241
[parent_id]: 430343
[tags]: 
First of all, ARI is one of the standard measures used everywhere. So if you omit it, there is a good chance reviewers will demand that you add it, and reject your paper. B³ is a fairly exotic measure that I have not seen anybody actually use. It is meaningless if the values of measure A are higher than those of measure B. Because that is apples and oranges. Conversely, some fairly simple transformations can affect these values. For example. The Rand index (RI) will always be higher than ARI, despite them measuring the same quantity, because ARI take the RI relative to an expected value. So B³>ARI is a useless observation, you must never compare different measures. Do you actually observe different rankings? I.e. give two results R1, R2, do you observe $B³(R1)\gg B³(R2)$ but $ARI(R1)\ll ARI(R2)$ on the same data set? Which essentially brings us to a point why you should pay attention to ARI . If your result has an ARI close to 0 (such as 0.1), then it means your result will be almost as good if you randomly permute all labels. But then how can it be any good? Does B³ do any such adjustment? What is the B³ of a random result? It is correct that ARI is not perfect. But are the other measures actually better there? I doubt so. Yes, the adjustment of ARI uses a fairly simple assumption for adjustment (that breaks down for constant labels), but it serves the purpose of making the Rand index more interpretable well. For some uncommon special cases, it may be desirable to use different adjustments. In such cases, I would then suggest to use the minimum of all adjustments. So your score can only become worse. Non-locality is an unfortunate property but not crucial for most users. It this mostly is relevant for hierarchical approaches: while you can locally evaluate Rand to check if a split yields better results, you cannot find the best agreement with respect to ARI that easily. The reasonable approach in this situation is of course to maximize the Rand index, and then adjust this final result. In conclusion, I suggest that you: Always give the standard ARI index. A low ARI does indicate a poor result. If you want, add additional adjustments with other null models. Give the NMI, because that is the other standard measure (it also has issues; in particular a high NMI does not guarantee a good result, because a random result can score high). Be explicit about which version of NMI you use! Give the AMI, the adjusted mutual information. This is a similar adjustment that aims at normalizing a random MI to be 0. Include trivial baselines, such as random permutation of the true labels, all objects labeled 0, random subsampling of k "centers" and assigning each to its nearest "center" etc. If you can't substantially beat these baselines, your method is not good (and you will be surprised by how common this is!)
