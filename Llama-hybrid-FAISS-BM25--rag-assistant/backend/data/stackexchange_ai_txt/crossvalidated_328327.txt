[site]: crossvalidated
[post_id]: 328327
[parent_id]: 328313
[tags]: 
The quantity $p(y|\theta_\text{ml}(y))$ is the marginal likelihood for the most favourable prior, which is not a true prior since it depends on $y$. It simply achieves the upper bound. Taking the supremum over all $y$'s does not make sense from a Bayesian perspective, since it should be conditional on $y$. Now if one considers the Normal-Normal case, with prior $\theta\sim\mathcal{N}(0,1)$ and observation $X\sim\mathcal{N}(\theta,1)$, we have $$p(y|\theta_\text{ml}(y))=\frac{1}{\sqrt{2\pi}}\qquad \text{and}\qquad m(y)=\frac{1}{\sqrt{4\pi}}\exp\{-y^2/4\}$$ Hence $$\sup_{y\in \mathcal{Y}} \frac{p(y\,\vert\,\theta_\text{ml}(y))}{m(y)}=\infty$$ which is not very surprising in that $p(y|\theta_\text{ml}(y))$ will correspond to an overfit of the model, while $m(y)$ includes a penalty Ã  la Schwarz (BIC). This may also connects with Lindley's paradox.
