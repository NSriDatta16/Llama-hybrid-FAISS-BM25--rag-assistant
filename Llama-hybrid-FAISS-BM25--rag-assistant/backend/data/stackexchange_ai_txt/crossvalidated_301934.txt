[site]: crossvalidated
[post_id]: 301934
[parent_id]: 301727
[tags]: 
You are stating you have empirically computed $P(X_t | X_{t-i}), 1 \leq i \leq 3$ and need $P(X_t | X_{t-1}, X_{t-2}, X_{t-3})$ where $X_t$ is the letter appearing at time $t$. By the conditional probability definition: $$P(X_t | X_{t-1}, X_{t-2}, X_{t-3}) = \frac{P(X_t, X_{t-1}, X_{t-2}, X_{t-3})}{P(X_{t-1} | X_{t-2}, X_{t-3}) P(X_{t-2}|X_{t-3}) P(X_{t-3}) }$$ First, you don't know every $P(X_t, X_{t-1}, X_{t-2}, X_{t-3})$. But you can approximate using a kind of Laplace smoothing, ie, initially giving every case one dummy sample to prevent zero probabilities, and then count all the data you have. Even so, they grow exponentially like @combo refers for the translation of the Higher-Order Markov Chain, and some memory tricks might be needed. Second, you could simplify and consider that $P(X_{t-1} | X_{t-2}, X_{t-3}) \approx P(X_{t-1} | X_{t-2})$ or do a similar smoothing as before for $P(X_{t-1}, X_{t-2}, X_{t-3})$. Anyway, let's not forget these are simplifications. They might not work in your application.
