[site]: datascience
[post_id]: 91059
[parent_id]: 91041
[tags]: 
To the title question, yes, repeated k-fold makes sense with random forests; and to the question body, no, the results will not generally be the same as repeated model fits with one k-fold split. The reason is that fixing one k-fold split and then repeatedly fitting random forests (with different random seeds) still only gives each forest access to $(k-1)/k$ of the data at a time. It may be easiest to think about the case when the number of trees is astronomical: the random choices for the bagging get averaged out, to the point where different random seeds don't actually matter: the forests converge to the same result, given a training split. Then the average of the forests' scores are the same for each of the splits, and so you average just $k$ scores. Compare that to repeated $k$ -fold, where each of the forests converges, but are all on different training sets, and so the average happens with more variety. Whether that has a sizable impact, or in which direction, is harder to say. Repeated $k$ -fold seems like it should give more stable results, even when the number of trees is something more reasonable, because the forests are less correlated.
