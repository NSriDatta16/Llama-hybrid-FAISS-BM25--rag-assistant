[site]: crossvalidated
[post_id]: 369538
[parent_id]: 330559
[tags]: 
It all essentially depends on the derivatives of the activation function, the main problem with the sigmoid function is that the max value of its derivative is 0.25, this means that the update of the values of W and b will be small. The tanh function on the other hand, has a derivativ of up to 1.0, making the updates of W and b much larger. This makes the tanh function almost always better as an activation function (for hidden layers) rather than the sigmoid function. To prove this myself (at least in a simple case), I coded a simple neural network and used sigmoid, tanh and relu as activation functions, then I plotted how the error value evolved and this is what I got. The full notebook I wrote is here https://www.kaggle.com/moriano/a-showcase-of-how-relus-can-speed-up-the-learning If it helps, here are the charts of the derivatives of the tanh function and the sigmoid one (pay attention to the vertical axis!)
