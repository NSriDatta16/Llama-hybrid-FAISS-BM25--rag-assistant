[site]: crossvalidated
[post_id]: 603517
[parent_id]: 
[tags]: 
Much better results when standardizing features to train LSTMs

I have a data set of time series. Each time series represents trajectories of the same path taken. So, the time series captures acceleration in $x$ , $y$ and $z$ direction, respectively for the rotation. In addition to that environmental properties are collected such as temperature, humidity, air pressure, light spectrum values, magnetic field and so on. I am training the following LSTM to recognise its current position on the path (column for Positions (y): model_2_0 = tf.keras.Sequential([ layers.LSTM(64, activation="relu", return_sequences = True, input_shape= (win_length, num_features)), layers.Dropout(0.2), layers.LSTM(64, activation="relu", return_sequences=True), layers.Dropout(0.2), layers.LSTM(64, activation="relu"), layers.Dropout(0.2), layers.Dense(23, activation="softmax") ], name=model_name) # Save summary # Compile model model_2_0.compile(loss=tf.keras.losses.CategoricalCrossentropy(), # optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003), optimizer=tf.keras.optimizers.Adam(learning_rate=0.00003), metrics=["accuracy"]) history_2_0 = model_2_0.fit(X_train, y_train, epochs=25, validation_data=(X_valid, y_valid), #batch_size=1024, batch_size=200, verbose=1) Now, if I standardize the features, the accuracy is much better so that the val accuracy in the first epoch is 78% (val loss 0.6) and in the last 90.8% (val loss 0.47) with a test accuracy of 90.7% (loss 0.47). If I normalize the timeseries (min max normalization), the increase of the val accuracy in the first epochs is not that great compared to standardization. So, I am not quite sure whether there is a problem with standardizing the data in my context and whether I train my net correctly. So, should I normalize or standardize the data given the context?
