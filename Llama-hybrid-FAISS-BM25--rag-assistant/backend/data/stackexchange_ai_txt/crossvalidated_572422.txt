[site]: crossvalidated
[post_id]: 572422
[parent_id]: 
[tags]: 
Estimating the likelihood of a Dirichlet process

I am not sure if what I'm trying to achieve makes sense or is even possible, but I'd like to do MLE on a Dirichlet process mixture model. My reasoning is the following: If we can write out the likelihood of some probability density function, then we should be able to just plug that into an autograd framework like pytorch, and just optimize the parameters. Sure, we could do a fully Bayesian approach like MCMC, but that seems like overkill when I don't need posterior distributions. I just want to specify a hierarchical model, write out the likelihood, and optimize the parameters. This has worked for me so far, but I'd like to implement a DPMM model so I get the nice automatic inference of the number of clusters. I haven't been able to find an actual likelihood of a Dirichlet process, but, if I go through the stick breaking process and sample from beta distributions with $\text{Beta}(1, \alpha)$ , this is all completely differentiable in pytorch. Of course, to optimize the parameters, I can't randomly sample from the beta distribution, but I need to go the other way: I need to have a realization of the stick breaking process, and then figure out the likelihood of this particular realization. I've worked out how to "undo" the stick breaking process, and get the original probabilities out, and, in my mind, I should be able to get the entire likelihood out with the beta distribution. For instance, in Pytorch, I did this import torch import torch.nn as nn k = 50 # Truncate the mixture components to some k alpha = 0.1 # I want few clusters in my toy example # The mean parameters of the GMM, each component will correspond to one of # the probabilites $\pi_i$ mu = nn.Parameter(torch.rand(k)) # The $\pi_i$ 's of the DP. To make sure that it's differentiable and easy # to work with in pytorch, these are unbounded real numbers here. I'll # convert them to "Dirichlet draws" with a softmax later on pi = nn.Parameter(torch.rand(k)) # Estimate the likelihood of the DP # I add a random 0.01 at the end, otherwise, the last piece of the stick corresponds # to beta probability =1, which is not defined. So, this simulates the "infinite" # part of the stick pi_padded = torch.cat([pi, torch.tensor([0.01])]) # Convert the parameters to a probability distribution, drop the "infinite" part pi_prob = torch.softmax(pi_padded, dim=0)[:-1] # Extract the draws from the Beta distribution, probabilities between zero and one betas = inverse_stick_breaking(pi_prob) # Estimate the actual likelihood of the DP log_prob = torch.distributions.Beta(1, alpha).log_prob(betas).sum() Here, I define def inverse_stick_breaking(pi): pi = pi.clone() pi[1:] = pi[1:] / (1 - pi[:-1].cumsum(dim=0)) return pi However, this doesn't seem to work. The reason it doesn't work is that this method assigns the same likelihood to my assignment probabilities, regardless the distribution. E.g. >>> inverse_stick_breaking(torch.tensor([0.8, 0.1, 0.09])) tensor([0.8000, 0.5000, 0.9000]) >>> torch.distributions.Beta(1, 0.1).log_prob(inverse_stick_breaking(torch.tensor([0.8, 0.1, 0.09]))).sum() tensor(-2.7631) will yield the same likelihood as >>> inverse_stick_breaking(torch.tensor([0.4, 0.5, 0.09])) tensor([0.4000, 0.8333, 0.9000]) >>> torch.distributions.Beta(1, 0.1).log_prob(inverse_stick_breaking(torch.tensor([0.4, 0.5, 0.09]))).sum() tensor(-2.7631) This doesn't make sense to me, since if I set $\alpha=0.1$ , then we should prefer larger probability draws, forcing most of the concentration to the first component. The second probability distribution ([0.4, 0.5, 0.09]) should be much less likely than the first probability distribution. Or, even if the order of the components doesn't matter (e.g. [0.8, 0.1, ...] is equivalent to [0.1, 0.8, ...] in terms of likelihoods -- which it shouldn't be I guess), the more uniform distribution should definitely be less likely. Clearly, I am misunderstanding something, and this isn't the right way to do things. What have I got wrong? Is it even possible to write out the analytical form for the DP? Any help would be greatly appreciated!
