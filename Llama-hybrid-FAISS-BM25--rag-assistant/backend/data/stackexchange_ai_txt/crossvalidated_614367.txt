[site]: crossvalidated
[post_id]: 614367
[parent_id]: 
[tags]: 
Estimating conditional effect in generalized linear regression model (GLM)

I am trying to understand if it's possible to estimate conditional effect in a GLM using an approach similar to how we would in a linear model. Specifically in a linear model, I can use a residual method to estimate the conditional effect ( $\beta^\ast$ ) and standard error of variable $g$ in this linear model $E(y) = X \beta + g \beta^\ast$ , where $X$ is a matrix of covariates and intercept. The method is described as follows: Fit a covariate-only model $E(y) = X \beta$ and get estimates $\hat{\beta}$ Calculate the residuals $E(\tilde{y}) = y - X \hat{\beta}$ . Project out the covariates from g by $\tilde{g} = g - X (X^T X)^{-1}X^T g $ . Or equivalently regressing out $X$ from $g$ in linear model. Refit a model $E(\tilde{y}) = \tilde{g} \beta^\ast $ , which should give the same slope and (very close) standard error estimate as the estimates from fitting the original full model. I attempted to apply this approach to GLM model using offset but found this residual method gives different estimates compared to the full model result. For example, for logistic regression, I tried the following using statsmodel in python: Fit $logit(\mu) = X \beta$ , get estimates $\hat{\beta}$ and the weight in logistic regression at convergence, where the weight is $\text{weight}_i = \hat{\mu_i} (1-\hat{\mu_i})$ . Calculate the linear predictor $\eta = X \hat{\beta}$ . Project out the covariates from g by $\tilde{g} = g - X (X^T W X)^{-1}X^T Wg $ , where $W$ is a diagonal matrix of individual weight. Fit the logistic model $logit(y) = \tilde{g} \beta^\ast$ with the linear predictor $\eta$ as offset. However, I observed very different results from the correct estimates in the full model. Given that we regress out the MLE estimates from $g$ and include its prediction to offset $y$ , I would have thought that the effect-size estimate for $g$ would match its estimate under the joint model. What am I misunderstanding? See the example result output below: import statsmodels.api as sm import numpy as np import numpy.linalg as npla from numpy import random # simulate under logistic model n = 100 random.seed(1) X1 = random.normal(size=n).reshape((n,1)) # covariate g = random.normal(size=n).reshape((n, 1)) # variable of primary interest intercept = 0.5 X = np.hstack([np.ones_like(X1), X1, g]) beta = np.array([intercept, 1.0, 2.0]).reshape([3,1]) eta = X @ beta prob = 1 / (1 + np.exp(-eta)) y = random.binomial(n=1, p=prob) # fit full model y ~ X1 + X2 mod_full = sm.GLM(y, X, family=sm.families.Binomial()).fit() # covariate only model: y ~ X1 covar = X[:, 0:2] mod_covar = sm.GLM(y, covar, family=sm.families.Binomial()).fit() mod_null_eta = mod_covar.get_prediction(covar, which="linear").predicted mod_null_mu = mod_covar.get_prediction(covar, which="mean").predicted glm_weight = mod_null_mu * (1 - mod_null_mu) w_half_X = np.diag(np.sqrt(glm_weight)) @ covar w_X = np.diag(glm_weight) @ covar # regress out covar from X2 projection_covar = covar @ npla.inv(w_half_X.T @ w_half_X) @ w_X.T # nxn X2_resid = g - projection_covar @ g # fit model y ~ X2_resid with offset mod_X2 = sm.GLM(y, X2_resid, family=sm.families.Binomial(), offset=mod_null_eta).fit() The result from full model is: ============================================================================== coef std err z P>|z| [0.025 0.975] ------------------------------------------------------------------------------ const 0.0779 0.285 0.274 0.784 -0.480 0.636 x1 1.3113 0.417 3.146 0.002 0.494 2.128 g 2.5512 0.552 4.622 0.000 1.469 3.633 ============================================================================== The result under residual method is: ============================================================================== coef std err z P>|z| [0.025 0.975] ------------------------------------------------------------------------------ g 2.1360 0.433 4.934 0.000 1.288 2.984 ============================================================================== ```
