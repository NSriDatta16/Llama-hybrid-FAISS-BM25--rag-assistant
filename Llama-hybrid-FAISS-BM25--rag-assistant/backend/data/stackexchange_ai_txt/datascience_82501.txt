[site]: datascience
[post_id]: 82501
[parent_id]: 58487
[tags]: 
The answer given so far is wrong. 1.) In general, adding more and more layers does not lead to better training accuracy. 2.) The issue is certainly not overfitting, as Andrew Ng is talking only about the training set. The answer becomes true if you add layers, where each additional layer $l$ is represented by a mapping $f^{(l)}_{w}: \mathbb{R}^{n_{l}} \rightarrow \mathbb{R}^{n_{l}}$ for which weights $w$ exist, such that $f^{(l)}_{w}$ is the identity map. In such a case, you can add arbitrary many layers to an existing neural network architecture and in theory optimizing this new network cannot be worse than without the additional layers (since all additional layers can become the identity map). Potentially these new layers could improve the training accuracy. In practice this is not observed (see the ResNet paper). The reasons are mainly 1.) The employed solvers do not deliver the global optimal solution. 2.) Adding more layers makes the gradient computation more prone to numerical errors due to the involved chain rule, e.g. have a look here .
