[site]: crossvalidated
[post_id]: 498012
[parent_id]: 498005
[tags]: 
It looks like Partial Least Squares (PLS) is what you call "target-PCA" - originally this is for regression, but there are versions for classification. One problem with what you propose here is that you will need to be careful when later using something like cross-validation for assessing the quality of your classifier, because if you use the whole dataset for feature generation, cross-validation of the later random forest will be misleading. (This can be dealt with cross-validating the whole process, but that's more difficult and computationally more cumbersome.) I'd be surprised if information reduction before random forest is better than random forest on the full information - I don't know of any results that would suggest that such an operation in advance somehow helps the random forest, although the possibility that it does in your situation cannot be excluded (if you have enough data you could leave some aside and compare). Another issue is that logistic regression or SVD generate features in order to optimise their own way of classifying - why should it be better to use these features with another method that was set up to do something else?
