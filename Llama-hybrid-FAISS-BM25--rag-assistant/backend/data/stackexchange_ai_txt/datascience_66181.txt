[site]: datascience
[post_id]: 66181
[parent_id]: 66015
[tags]: 
I think clustering is the way to go, but first you have to preprocess data in the proper way. I would start encoding your observations using an Autoencoder for dimensionality reduction . This DL technique allows you to compress your data, or to take a "summary of it". The compressed representation of your data is often use as a preliminary step in outlier detection tasks, and I think your problem is highly compatible with this approach. If you don't want to use Deep Learning, you can recur to other dimensionality reduction techniques. PCA might work, for example. ( t-SNE is stochastic, and I don't recommend it outside data visualization in lower dimensions.) Once you have represented your data in a dense, lower dimensional space, there's a number of things you can do. Employ some measure of distance between observations in this compressed space. If your goal is to understand how different two observations are, that would be the main thing to do. The most common and simple is Euclidean distance , but there's an awful lot of measures you can use (Manhattan, Minkowski, Mahalanobis, ... you name it!). You can run clustering techniques. The one I use almost any time is, of course, k-Means clustering . With this, you can try to identify groups of more similar observations, this can give you hints on how to spot "good" and "bad" observations. An alternative is DBSCAN clsutering, that allows you to classify some observations as outliers (the drawback of this model is that you have many hyperparameters to tweak.) Additionally, you can train a classifier that is fed with the compressed representation of your data. Labeling observations of a dataset as "good" and "bad" could be time consuming, but it might be worthwile if you don't just want to check how different two customers are, but also get a straight classification.
