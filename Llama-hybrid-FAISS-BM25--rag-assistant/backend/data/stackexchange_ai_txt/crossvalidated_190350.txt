[site]: crossvalidated
[post_id]: 190350
[parent_id]: 
[tags]: 
Antisymmetric components in Gaussian kernel

In Bishop's Pattern Recognition and Machine Learning , he states on page 80 in reference to the squared Mahalanobis distance $(\mathbf{x} - \boldsymbol{\mu})^\top\boldsymbol{\Sigma}^{-1}(\mathbf{x} - \boldsymbol{\mu})$ that: the matrix $\boldsymbol{\Sigma}$ can be taken to be symmetric, without loss of generality, because any antisymmetric component would disappear from the exponent. My understanding was that all covariance matrices must be symmetric, so could someone please explain what this statement means?
