[site]: crossvalidated
[post_id]: 635327
[parent_id]: 
[tags]: 
Learning the gradient descent stepsize

I'm trying to apply reinforcement learning to learn the optimal stepsize in every gradient descent step. Currently I'm only considering simple quadratic problems of the form f(x)=x'Qx with Q diagonal and condition number 10. I'm using the SAC implementation of SB3 in Python. However, even when trying to overfit to one simple function, training is very unstable (see picture included). I'm currently using the reward log10(1/i^2) with i the current number of gradient descent steps to try the minimize the number of iterations needed. The model can perform better compared to exact linesearch and using a constant stepsize (see picture included), but it heavily depends on the number of training steps taken. Is there any way to make the training more stable? Is another algorithm more suited for this kind of problems? I also used TD3, but it was twice as slow compared to SAC. Would model-based RL maybe be a better choice as the exact model of the environment is known? Also at around 800k training steps, the agent seems to completely forget what it has learned before. Does this has to do with the replay-buffer size being too small? It is currently set at 5e6, which allocates around 120GB of memory.
