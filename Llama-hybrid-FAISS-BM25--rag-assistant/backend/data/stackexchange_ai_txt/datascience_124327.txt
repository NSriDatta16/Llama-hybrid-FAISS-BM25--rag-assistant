[site]: datascience
[post_id]: 124327
[parent_id]: 124324
[tags]: 
For simplicity in getting something running, you can allocate sections of the token sequence for a transformer into designated sections for your various fields. Then the masked language modelling will handle the learning of those tasks jointly together for you, as long as every sample has the additional features you want, or you are fine trying it out with a "missing token" when the fields are missing. By allocate, I mean break up the N-length token sequence typically for natural language, sacrificing the token sequence length of the text for the other attributes. This avoids having to deal with multi-task training of multiple classifiers by using the exact same setup, just changing how the data is fed to the model. I have seen this approach used in research literature, specifically in attempting to leverage the data sample's extra features or attributes.
