[site]: crossvalidated
[post_id]: 112239
[parent_id]: 112224
[tags]: 
In a feed-forward neural network with a single hidden layer there's two activation functions. For some randomly selected batch sized $n$ of your training matrix, $X\in R^{n \cdot m}$, recall that the feed-forward step is given by $$A= F_1(W_1X + B_1)$$ $$\text{Pred} = F_2(W_2A + B_2)$$ The next step in training the model is back propagation. You compute a bunch of $\delta$s, representing the errors, starting with the log loss. $$\delta_2 = \frac{\sum(\text{Truth}\cdot \log(\text{Pred}) + (1 - \text{Truth}) \cdot \log(1 - \text{Pred})}{n}$$ $$\delta_1 = (W_2^T\delta_2)\cdot\nabla_wF_2(A)$$ Then update your weights, using some step-size $\alpha$. $$W_2 := W_2 + \frac{\alpha}{n} A^T\delta_2$$ $$B_2 := B_2 + \frac{\alpha}{n}\delta_2$$ $$W_1 := W_1 + \frac{\alpha}{n} X^T\delta_1$$ $$B_1 := B_1 + \frac{\alpha}{n}\delta_1$$ This is repeated until $\delta_2$ is sufficiently small. Typically you monitor this on a held-out dataset. Note that since you're using the sigmoid function there's a simple result for the calculation of gradient of $F$. $$\nabla_w F(x) = \nabla_w\frac{1}{1+e^{-wx}}\Rightarrow F(x) (1 - F(x))$$ Weight initialization of $W$s is on the interval $[0, \frac{1}{\sqrt{m}}]$ and $B$s as 0. There's a fancier way to initialize $W$, but it's probably not worth the effort with a simple network. There is a boatload of room improvement over this simple model. Structurally some easy wins are convolutional inputs (given that this is an image recognition task) and rectified linear hidden units (which don't suffer from the vanishing gradient problem). During training dropout, Nesterov's accelerated gradient and an $\ell_2$ norm can all make dramatic improvements. If you don't want to implement all of this, I'd recommend looking into Lua Torch's gpu neural network library and Theano / Pylearn2 , then you only need to implement the prediction method. There's also a lot of interesting research on neural networks by Hinton and Bengio .
