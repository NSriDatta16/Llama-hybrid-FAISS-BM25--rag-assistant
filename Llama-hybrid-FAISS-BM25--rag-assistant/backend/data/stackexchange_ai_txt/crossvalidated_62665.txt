[site]: crossvalidated
[post_id]: 62665
[parent_id]: 62621
[tags]: 
Neither precision nor recall tell the full story, and it is hard to compare a predictor with, say, 90% recall and 60% precision to a predictor with, say, 85% precision and 65% recall - unless, of course, you have cost/benefit associated with each of the 4 cells (tp/fp/tn/fn) in the confusion matrix . An interesting way to get a single number ( proficiency , aka uncertainty coefficient ) describing the classifier performance is to use information theory : proficiency = I(predicted,actual) / H(actual) i.e., it tells you what fraction of the information present in the actual data was recovered by the classifier. It is 0 if either precision or recall is 0 and it is 100% if (and only if) both precision and recall are 100%. In that it is similar to F1 score , but proficiency has a clear information-theoretical meaning while F1 is just a harmonic average of two numbers with a meaning. You can find paper, presentation and code (Python) to compute the Proficiency metric here: https://github.com/Magnetic/proficiency-metric
