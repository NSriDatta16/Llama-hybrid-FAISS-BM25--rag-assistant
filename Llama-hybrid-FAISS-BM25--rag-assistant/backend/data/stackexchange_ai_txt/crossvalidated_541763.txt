[site]: crossvalidated
[post_id]: 541763
[parent_id]: 
[tags]: 
Variational Autoencoders and Probabilistic Graphical Models

I am just getting started with the theory on variational autoencoders (VAE) in machine learning and I keep reading that VAEs belong to the category of Probabilistic Graphical Models (PGMs). As I understand it, this is because of the way we treat the input data x and the encoding vector z of latent variables, i.e. as a joint distribution p_θ( x , z ), where θ is the vector of network parameters.\ However, I was wondering whether there was a way to exploit the very useful property of PGMs, namely to infer a graph structure (whether directed or undirected) representing correlations (or causal relationships in the case of DAGs) among variables in the input data. Is this possible once the latent variable vector is selected in the encoding process and would the encoder network in the VAE help in the selection of the graph structure?
