[site]: crossvalidated
[post_id]: 525710
[parent_id]: 525679
[tags]: 
I believe the second-to-last paragraph of page 253 gives a method for making corollaries involving ReLU networks: take three ReLU neurons to generate a "spike" activation function: $s(x) := \DeclareMathOperator{relu}{ReLU} \relu(x-1) - 2 \relu(x) + \relu(x+1)$ ( W|A link ) This function is bounded, continuous, and non-constant and so satisfies theorems 1 and 2 of the paper (but discontinuous derivatives, so not theorems 3 and 4); and the space of relu-network functions contains the space of "spike"-network functions by grouping neurons into threes as above.
