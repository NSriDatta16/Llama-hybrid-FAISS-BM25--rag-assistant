[site]: crossvalidated
[post_id]: 621397
[parent_id]: 
[tags]: 
How do you chose the standard deviation for a Gaussian kernel in KDE?

The routine explanation of a KDE plot is: You chose a kernel (let's say a Gaussian) You center a kernel at each data point You average all kernels I get that (2) means that a Gaussian curve is "plotted" so that the mean of that individual Gaussian distribution is the data point. But how do you come up with the standard deviation for that individual Gaussian kernel? The explanations I found so far seem to gloss over this point. I initially assumed that the standard deviation is the one computed from the sample but I saw some explanations that seem to contradict it. In particular I mean the example given in the Wikipedia article on "KDE" . There they give the example of a sample: np.array([-2.1, -1.3, -0.4, 1.9, 5.1, 6.2]) and place on each data point a Gaussian kernel with variance 2.25. However, the variance of that sample is 9.932. So how do they come up with this value for the variance? I wish I could find a single worked example (preferably with python code) that makes each step explicit rather than hiding it behind an API so that I can understand this. EDIT : I am asking about the standard deviation of the individual kernel (in the case of a Gaussian kernel), not about the band-width. Not sure why I am pointed to questions discussing the band-width.
