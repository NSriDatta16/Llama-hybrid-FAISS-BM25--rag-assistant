[site]: datascience
[post_id]: 27630
[parent_id]: 
[tags]: 
Classify based on features being in ranges?

I'm trying to build a classification model where instances are classified based not on some linear combination of their features, but instead based on whether the features are all within some (unknown) ranges. So, for example, if I had two features, and my instances (features, followed by known classification) are: (0, 5): 0 (2, 7): 1 (3, 2): 0 (4, 4): 1 (3, 9): 0 (6, 6): 0 then perhaps the model would learn that the first feature should be between 1 and 5, and the second feature should be between 3 and 8. (I only want "and"s in the model; I'm not looking for any cases where feature 1 is between a and b OR feature 2 is between c and d.) In essence, the model should find some hyperrectangle in feature space with all edges parallel to feature axes, where 0s are expected to be outside the rectangle and 1s are expected to be inside it. (In the example data, the data points are separable; this is not the case in my real problem. In the spirit of an SVM or logistic regression, the model should do the best it can.) My first stab at this involved trying to do least-squares regression with a hypothesis function made up of two sigmoids for each feature (one "forward", one "backward"), parameterized to slide back and forth, all of which were multiplied together. Maybe my scipy-fu is weak, but above four features, I started having convergence issues with that approach. Is there some tried-and-true way to solve this kind of classification problem? (Even just a name for this kind of problem would be helpful!)
