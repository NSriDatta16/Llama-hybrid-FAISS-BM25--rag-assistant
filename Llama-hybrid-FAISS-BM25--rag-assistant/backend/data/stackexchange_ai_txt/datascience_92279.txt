[site]: datascience
[post_id]: 92279
[parent_id]: 
[tags]: 
Compare distance between embeddings in different dimensions

I am working on a problem with CNNs. After the convolutional layers, comes a "flatten". One could interpret that as a representation of the input image in some high-dimensional continuous space. (In some way similar to word embeddings in NLP) Suppose we have 2 architectures for the same input that result in representations in $\mathbb{R}^N$ and $\mathbb{R}^M$ with $N \neq M$ . Now suppose we have two images $a$ and $b$ which we pass through both neural nets and get their respective representations, lets call them $a_N$ for image $a$ in $\mathbb{R}^N$ , $b_N$ for image $b$ in $\mathbb{R}^N$ , and so on. Is there any distance measure $d(x,y)$ so that $d(a_N,b_N)$ and $d(a_M,b_M)$ are comparable? What I thought so far is calculating the N-dimensional and M-dimensional representation of many image samples from the same category and then use t-SNE so that I take them both to the same dimension. One could also do the projection from the space of highest dimension to the other. Does anyone know some proven technique to do that, or could give some advice?
