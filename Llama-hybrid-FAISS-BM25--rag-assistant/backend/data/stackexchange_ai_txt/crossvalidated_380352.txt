[site]: crossvalidated
[post_id]: 380352
[parent_id]: 380350
[tags]: 
Ben - welcome to the show! A few things: A) you want an estimate of the generalization error using the 150 data points you have. B)Using 10 fold cross validation you would be able to estimate that (and you would end up using all of the data you have). It might seem counter intuitive, but it is a kosher. C) You dont want to test repeated ly on a static test set - very unkosher. The model learns the test set, but gives you a underestimate of the generalization error. D) here is a recipe. Take the data set and split them randomly into 10 equal chunks. Number them 1 to 10. Round 1, keep the chunk 1 as test set, use chunk 2 to fine tune any parameters and use chunks 3 to 10 as training. Compute the error on test. Round 2, keep chunk 2 as test set, and chunk 3 as validation, use the rest of the chunks for training. Compute error on chunk 2 (your new test set). Repeat 10 times (proceeding to round 3 etc as above) and average the 10 errors on the test set. You now ahve a robust estimate of generalization error. You could use a similar approach with leave one out. If you still feel nervous, revisit the first step and again using a different random seed split the data into 10 and repeat. You should see consistent results. If you are in python world take a look at sklearn it has a class for cross validation. To your other question - interpretability. Depends on the model you are using. Linear SVMs will ofer some interpretability. CNNs not as much, but you can focus on the components that are most predictive or start with features you think are most intuitive and see if they perform well. And to your yet other question - which precise model specification should I go with as a result of the sample? Pick the specification with the lowest generalization error and estimate that on the entire data set. Score the new sample.
