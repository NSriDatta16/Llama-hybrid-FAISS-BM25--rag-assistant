[site]: crossvalidated
[post_id]: 441284
[parent_id]: 440746
[tags]: 
The point of hyperparameter tuning is to find a combination of $\gamma, C$ which generalize well, a compromise between over- and under-fitting the data. It seems like you've just picked two arbitrary values $\gamma, C$ and discovered that they don't generalize well. This isn't surprising, because there are lots of tuning parameter choices which are not good choices. Overfitting is possible even if the train and test data have the same distribution. For an example of this in the case of a $k$ -NN classifier, see compare the differences between Figures 2.2 and 2.3 in Elements of Statistical Learning (Hasti et al) and the supporting text. It's possible to overfit any dataset arbitrarily well. Suppose you have a simple 1-dimensional regression problem, where the outcome is measured with some noise. You can choose a basis -- indeed, any basis that is at least as large as the number of training data -- that will interpolate all of the training data, but I'd be shocked if this model has any kind of value at predicting out-of-sample data. This is because your model is fitting the noise as well as the signal. Why do you expect an SVM to be any different? Regarding your edit, your remark what I found bizarre was the combination of perfect performance on the training data, and effectively completely imperfect performance on the testing data is the textbook definition of overfitting. For example, the textbook Elements of Statistical Learning discusses this exact point throughout chapter 7, particularly pp. 228-230 which has the subheading " Optimism of the Training Error Rate." Scaling the data is very common, especially in the case of SVM. Read on for an explanation of why. Thinking about the difference in terms of the kernel function is illustrative. An RBF kernel SVM has a single hyperparameter $\gamma$ and is usually written as $$ k(x_1, x_2) = \exp\left(-\gamma \| x_1 - x_2 \|_2^2\right) $$ but we can rewrite this as $$ k(x_1, x_2) = \exp\left(- (x_1 - x_2)^\top (\gamma I) (x_1 - x_2)\right) $$ If wish we generalize this beyond a single scalar $\gamma$ , then we can write $\Gamma$ for some P(S)D matrix. $$ k(x_1, x_2) = \exp\left(- (x_1 - x_2)^\top \Gamma (x_1 - x_2)\right) $$ Perhaps we could simply scale each feature individually, in which case $\Gamma$ is diagonal with all non-negative entries. In this case, we might choose the diagonal elements of $\Gamma$ such that each adjusts the measurements of the features to be on a similar scale, such as $z$ -scores. In this case, we've exactly recovered a kernel which uses standard deviations as a scale factor and controls bandwidth with a scalar parameter $\gamma$ . $$ k(x_1, x_2) = \exp\left(-\gamma (x_1 - x_2)^\top \begin{bmatrix} \sigma_1^{-2} & 0 & 0 & \cdots \\ 0 & \sigma_2^{-2} & 0 & \cdots \\ 0 & 0 & \sigma_3^{-2} & \cdots \\ \vdots & \vdots & \vdots & \ddots \end{bmatrix} (x_1 - x_2)\right) $$ This final kernel can be readily achieved by scaling the data by $\sigma$ first and then estimating an SVM for some values of $\gamma, C$ . It's plausible because it suppresses the (arbitrary) choice of units. It's easy to see why. Suppose you have 2 features, and the first feature has values in the range $[10^{-4}, 10^{-2}]$ and the second feature has values in the range $[10^2, 10^4]$ . The first feature is almost irrelevant because at either extreme, the contribution of the first feature makes only a tiny difference in the norm $\| x_1 - x_2 \|_2^2$ Of course, even if you scale your data, you'll still need to find the hyperparameters $\gamma, C$ which provide good fit to your data. It's not always true that scaling improves results (see: scaling for SVM destroys my results ), but it's a common enough finding because it tends to be the case that relevant features are present and important to the outcome.
