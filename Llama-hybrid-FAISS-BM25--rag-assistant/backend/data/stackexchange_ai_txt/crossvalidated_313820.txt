[site]: crossvalidated
[post_id]: 313820
[parent_id]: 313768
[tags]: 
Your question is ill-posed, it doesn't make sense to "infer" a prior. Let's say you have a likelihood $p(x|\theta)$, where $x$ is the data and $\theta$ are some parameters. In bayesian inference, the objective is to find the distribution of the parameters given the data, $p(\theta|x)$, which is the posterior. In order to do this, you first posit a prior distribution over the parameters $p(\theta)$. We then have from bayes rule that $p(\theta|x)\propto p(x|\theta)p(\theta)$. "Inference" is usually reserved for the process of using data to learn something about the parameters of a model. Note that the prior doesn't involve the data at all, so it doesn't make sense to talk about inferring it. Also note that empirical bayes isn't really related to reference/Jeffreys' priors (not Jerry's priors). In an empirical bayes setup you already posit a prior distribution, and use the data to set the hyperparameters of this prior. The point of reference/Jeffrey's priors (and objective bayesian inference) is to construct a prior using just the assumed likelihood.
