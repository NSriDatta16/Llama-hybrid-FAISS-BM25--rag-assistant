[site]: datascience
[post_id]: 57128
[parent_id]: 57120
[tags]: 
BPTT is the process of backpropagating the gradients calculations (chain rule) of a loss function at each time step in a recurrent network. The parameters $U$ (input weights) and $W$ (recurrent connection weights) will have a contributions to the gradient at each time step so the total gradient will be the sum of each contribution. The vanishing gradient is a generic issue in neural networks of having decaying gradients values over the parameters of the network, meaning that updates for some parameters will be very small and hard to propagate. This gradient-based learning problem may occur with deep networks, vanilla recurrent networks (because of the large product of contributions to the gradient over timesteps) and often raised by the inherent nature of some activation functions.
