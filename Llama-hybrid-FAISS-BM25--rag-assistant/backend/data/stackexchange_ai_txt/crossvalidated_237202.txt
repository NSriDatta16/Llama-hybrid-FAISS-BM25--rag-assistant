[site]: crossvalidated
[post_id]: 237202
[parent_id]: 
[tags]: 
Does the property of equivariance to translation of convolution layers help to learn translation-invariant features?

In some texts, people mention that the reason why convolutional neural networks are able to learn translation-invariant features are related to the property that convolution layers are equivariant to translation, which means the convolution of the inputs being shifted is the same as the shifted output of the convolution of the original inputs. But I am a little bit confused about this, since the output signals of the convolution layer gets still shifted anyway ("equivariant" is not "invariant", the latter case not only preserves the magnitude, but also preserve the location of the output signal), so there should be some change in the later outputs unless the the coefficients of the connection to any of the nodes in this layer to be the same. I also read that the pooling layer may help with this translation issue, but it is only approximately invariant to very small local translation. What if I move the image of a cat from one corner of the image to another corner? How is CNN able to realize that it is still a cat but only the position is shifted?
