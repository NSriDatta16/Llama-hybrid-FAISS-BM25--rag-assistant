[site]: crossvalidated
[post_id]: 463472
[parent_id]: 
[tags]: 
Explaining results when changing kernel size in CNN

I trained a CNN on CIFAR10 (implemented in PyTorch) with the following architecture: INPUT: (3,32,32) --> Conv2d layer (kernel: 3x3, stride:1x1, filters: 64) --> Activation: ReLU --> Max pooling (kernel: 2x2) --> Conv2d layer (kernel: 3x3, stride:1x1, filters: 16) --> Activation: ReLU --> Max pooling (kernel: 2x2) --> Fully connected layer of 784 --> Softmax (I used Adam optimize, Xavier initialization, and variable learning rate, training in mini-batches of 100 with 5000 train samples and 1000 test samples) Training for 50 epochs yeilded train accuracy of 77% and test accuracy of 48% . When modifing the model to have kernel of 5x5 in both conv layers (and fitting the linear layer for the required size), the network results was 44% for train accuracy and 50% for the test accuracy . I am trying to explain why the results are getting worse when training with bigger kernel, it seems that, in general, the opposite will make sence: larger kernel can learn as the smaller one and even further. Two other observations I couldn't explain is that the convergence around 50% in the small kernel arrives around 10 epochs and keeps that accuracy for the rest of the training while the larger kernel slightly improves test accuracy during all the training. Also, it seems like the 5x5 kernel generalize better (the gap between train and test acc is smaller). Any ideas? This is the plot of the 3x3 kernel size: and the 5x5 kernel size:
