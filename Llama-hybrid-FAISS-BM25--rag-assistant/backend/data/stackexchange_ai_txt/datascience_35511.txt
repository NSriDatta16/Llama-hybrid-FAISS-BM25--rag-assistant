[site]: datascience
[post_id]: 35511
[parent_id]: 35468
[tags]: 
Often customers don't understand the value behind the performance of machine learning models. 60% precision might sound not so good, since 50% is often related to nearly random for binary problems. However, it might well be a very difficult problem, and 60% might well be very good performance. One way you can show your customer that your model is indeed performing well, is to compare its precision against a human classification . You can: Define a random baseline, and explain why it is 35% and not 50% for your problem. Get a couple of humans from your client side to try the task manually. Let your model perform the task and show the improvement. More often than not, humans perform tasks more precisely than models (if in small scale). So, if your model performs better, hopefully, they will see the value your model is adding to their company.
