[site]: crossvalidated
[post_id]: 266849
[parent_id]: 266729
[tags]: 
First of all i agree with Hossein. Mixing two types of features not necessary leads to better classification results. I've just observed it myself few days ago, where i tried to mix character n-grams with content words for topic classification. Classification separatly on each feature type performed much better as on both together. However, what i would suggest you is to construct an ensemble of classifiers $c_1, c_2, \ldots$, where each $c_i$ is trained on one sort of features (in my example $c_1$ on character n-grams and $c_2$ on content words), rather than using single-mixed feature vectors. Have a look on this article " Ensemble Machine Learning Algorithms in Python with scikit-learn ".
