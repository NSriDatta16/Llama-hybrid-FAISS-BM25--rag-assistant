[site]: datascience
[post_id]: 52132
[parent_id]: 49479
[tags]: 
I recently presented a poster at a conference where we used the same approach you describe for clustering. Generally, I think it's a great approach for clustering, as you can get clusters which have a distance that is relative to an outcome or variable that you are interested in. For some insight, I have a few pointers: 1) When getting the co-occurence in trees, depending on how many subjects you have, this can end up being a very sparse matrix. To make the matrix less sparse you can increase the minimum number of samples required in terminal nodes. 2) After you get the similarity matrix, do a PCA on this, ending with individuals in rows and PCs in columns. Get the distance between individuals in this PCA space, restricting to some top number of components that you find acceptable. I recommend this because the similarity matrices can be huge if you have a lot of cases.
