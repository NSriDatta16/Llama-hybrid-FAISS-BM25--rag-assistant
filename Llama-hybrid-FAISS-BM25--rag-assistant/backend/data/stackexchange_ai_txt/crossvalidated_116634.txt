[site]: crossvalidated
[post_id]: 116634
[parent_id]: 116572
[tags]: 
How to make otherwise black-box models more interpretable is a great question that I often struggle with. Do any of these related questions help? Ideas for outputting a prediction equation for Random Forests Obtaining knowledge from a random forest How to make prediction models more interpretable? You mentioned a lot of variation between trees; I suspect that - by definition - no single tree will be a good representation of the forest. But if the goal is purely to justify/interpret/visualize how the RF makes predictions, there may be simple alternatives. If the number of features is small you could chart how the RF's predictions change over the dataspace. (I've had some success using 2-D and 3-D plots for this with a clinical audience). If the number of features is (slightly) bigger, you could provide a "what-if?" analysis, where users can vary the input values and see the effect on output. And you can always measure the relative contributions of each feature. Anyway, see some great answers to the above questions for more ideas. I think this is certainly an important question.
