[site]: datascience
[post_id]: 54708
[parent_id]: 
[tags]: 
Expected value in Bellman equation

I am reading "reinforcement learning - An introduction" by Sutton and Barto. At pag. 59, there is the Bellman equation for the state-value function $\begin{array}{ll} v_{\pi}(s) &= \mathbb{E}_{\pi}[G_t|S_t=s] \\&= \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1}|S_t=s] \\&= \sum\limits_{a} \pi(a|s) \sum\limits_{s'} \sum\limits_{r} p(s^{'},r|s,a) \left[ r + \gamma \mathbb{E}_{\pi}[G_{t+1}|S_{t+1}=s'] \right] \end{array}$ I didn't understand why the expected value survived in the last expression. The definition of the expected value is $\mathbb{E}[X] = \sum\limits x \cdot p(x)$ , not $\mathbb{E}[X] = \sum\limits \mathbb{E}[x] \cdot p(x)$ I don't know whether my question is clear. In the last equation of the defition of $v_{\pi}(s)$ , I would not have put the expected value inside
