[site]: crossvalidated
[post_id]: 476849
[parent_id]: 476829
[tags]: 
After multiple imputation of data sets (MI) and analyzing each of the imputed sets separately, Rubin's rules do have you take the mean over those imputations as the point estimate. For inference, confidence intervals and so forth, you then determine the overall variance of the point estimate as a combination of within-imputation and between-imputation variances. This paper by Marshall, Altman, et al. summarizes the rules nicely: "For a single population parameter of interest, $Q$ , e.g. a regression coefficient, the MI overall point estimate is the average of the $m$ estimates of $Q$ from the imputed datasets, $\bar Q = \frac{1}{m} \sum_{i=1}^m \hat Q_i$ . The associated total variance for this overall MI estimate is $T = \bar U + \left( 1+\frac{1}{m} \right) B$ , where $\bar U = \frac{1}{m} \sum_{i=1}^m U_i$ is the estimated within imputation variance and $B=\frac{1}{m-1} \sum_{i=1}^m \left( \hat Q_i - \bar Q\right)^2$ is the between imputation variance. Inflating the between imputation variance by a factor $1/m$ reflects the extra variability as a consequence of imputing the missing data using a finite number of imputations instead of an infinite number of imputations. When $B$ dominates $\bar U$ greater efficiency, and hence more accurate estimates, can be obtained by increasing $m$ . Conversely, when $\bar U$ dominates $B$ , little is gained from increasing $m$ ." The paper then goes on to show how to apply these rules to multiple coefficients, situations in which coefficient estimates might not have normal distributions, hypothesis testing, and so on.
