[site]: crossvalidated
[post_id]: 50963
[parent_id]: 
[tags]: 
Independence assumption in maximum entropy models in NLP

I am reading Klein and Manning's notes on Maximum Entropy for Natural Language Processing. On slide 22, they have an equation saying, $P(C|D,\lambda) = \Pi _{(c,d)\in (C,D)} P(c|d,\lambda)$. I am not sure how they obtain this or what it really means. In the coursera video on this topic, Manning says that this follows because the random variables are independent and identically distributed. However, I thought, Maximum Entropy methods are prefered over Naive Bayes because they don't make the "naive" independence assumptions. If someone has a reference for this equation, that will be great.
