[site]: datascience
[post_id]: 10969
[parent_id]: 10876
[tags]: 
You included an important detail in your comment that: With balanced dataset (50% positives and 50% negatives) I have similar results. I think the issue is in the activation function of your neural network. If you used a symmetric activation function in your neural network then the FP and FN rates should be roughly the same in the balanced dataset case. You can test whether this is the source of your issue by flipping the signs of the labels on your data (i.e. relabel positive as negative and negative as positive and see what happens). If the FP and FN rates stay the same even when you switch what the dataset labels positive and negative, then this shows that the large false positive rate is not coming from the data but rather from the probability model itself, in which case check your activation function to see if they are biased towards labelling something as positive.
